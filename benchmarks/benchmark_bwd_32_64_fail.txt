### causal=False, headdim=64, batch_size=32, seqlen=512 ###
Flash2 fwd: 176.17 TFLOPs/s, bwd: 114.03 TFLOPs/s, fwd + bwd: 126.81 TFLOPs/s
Pytorch fwd: 27.12 TFLOPs/s, bwd: 34.59 TFLOPs/s, fwd + bwd: 32.07 TFLOPs/s
### causal=False, headdim=64, batch_size=16, seqlen=1024 ###
Flash2 fwd: 187.04 TFLOPs/s, bwd: 131.43 TFLOPs/s, fwd + bwd: 143.63 TFLOPs/s
Pytorch fwd: 31.38 TFLOPs/s, bwd: 40.30 TFLOPs/s, fwd + bwd: 37.28 TFLOPs/s
### causal=False, headdim=64, batch_size=8, seqlen=2048 ###
Flash2 fwd: 188.44 TFLOPs/s, bwd: 141.10 TFLOPs/s, fwd + bwd: 152.01 TFLOPs/s
Pytorch fwd: 30.27 TFLOPs/s, bwd: 45.05 TFLOPs/s, fwd + bwd: 39.53 TFLOPs/s
### causal=False, headdim=64, batch_size=4, seqlen=4096 ###
Flash2 fwd: 181.03 TFLOPs/s, bwd: 147.00 TFLOPs/s, fwd + bwd: 155.35 TFLOPs/s
Pytorch fwd: 31.35 TFLOPs/s, bwd: 47.37 TFLOPs/s, fwd + bwd: 41.34 TFLOPs/s
### causal=False, headdim=64, batch_size=2, seqlen=8192 ###
Flash2 fwd: 178.71 TFLOPs/s, bwd: 149.32 TFLOPs/s, fwd + bwd: 156.68 TFLOPs/s
Pytorch fwd: 29.51 TFLOPs/s, bwd: 48.54 TFLOPs/s, fwd + bwd: 40.99 TFLOPs/s
### causal=False, headdim=64, batch_size=1, seqlen=16384 ###
Flash2 fwd: 177.16 TFLOPs/s, bwd: 150.39 TFLOPs/s, fwd + bwd: 157.18 TFLOPs/s
Pytorch fwd: 31.92 TFLOPs/s, bwd: 47.07 TFLOPs/s, fwd + bwd: 41.45 TFLOPs/s
### causal=False, headdim=128, batch_size=32, seqlen=512 ###
Flash2 fwd: 187.64 TFLOPs/s, bwd: 105.75 TFLOPs/s, fwd + bwd: 120.81 TFLOPs/s
Pytorch fwd: 39.85 TFLOPs/s, bwd: 52.75 TFLOPs/s, fwd + bwd: 48.28 TFLOPs/s
### causal=False, headdim=128, batch_size=16, seqlen=1024 ###
Flash2 fwd: 200.88 TFLOPs/s, bwd: 122.87 TFLOPs/s, fwd + bwd: 138.20 TFLOPs/s
Pytorch fwd: 50.49 TFLOPs/s, bwd: 66.86 TFLOPs/s, fwd + bwd: 61.19 TFLOPs/s
### causal=False, headdim=128, batch_size=8, seqlen=2048 ###
Flash2 fwd: 200.29 TFLOPs/s, bwd: 133.32 TFLOPs/s, fwd + bwd: 147.40 TFLOPs/s
Pytorch fwd: 52.27 TFLOPs/s, bwd: 79.95 TFLOPs/s, fwd + bwd: 69.45 TFLOPs/s
### causal=False, headdim=128, batch_size=4, seqlen=4096 ###
Flash2 fwd: 194.92 TFLOPs/s, bwd: 139.93 TFLOPs/s, fwd + bwd: 152.20 TFLOPs/s
Pytorch fwd: 55.85 TFLOPs/s, bwd: 87.56 TFLOPs/s, fwd + bwd: 75.34 TFLOPs/s
### causal=False, headdim=128, batch_size=2, seqlen=8192 ###
Flash2 fwd: 191.00 TFLOPs/s, bwd: 142.43 TFLOPs/s, fwd + bwd: 153.59 TFLOPs/s
Pytorch fwd: 53.57 TFLOPs/s, bwd: 91.06 TFLOPs/s, fwd + bwd: 75.89 TFLOPs/s
### causal=False, headdim=128, batch_size=1, seqlen=16384 ###
Flash2 fwd: 191.08 TFLOPs/s, bwd: 143.11 TFLOPs/s, fwd + bwd: 154.17 TFLOPs/s
Pytorch fwd: 58.54 TFLOPs/s, bwd: 89.50 TFLOPs/s, fwd + bwd: 77.75 TFLOPs/s
### causal=False, headdim=256, batch_size=32, seqlen=512 ###
Flash2 fwd: 153.26 TFLOPs/s, bwd: 104.04 TFLOPs/s, fwd + bwd: 114.55 TFLOPs/s
Pytorch fwd: 51.57 TFLOPs/s, bwd: 70.04 TFLOPs/s, fwd + bwd: 63.54 TFLOPs/s
### causal=False, headdim=256, batch_size=16, seqlen=1024 ###
Flash2 fwd: 172.40 TFLOPs/s, bwd: 117.01 TFLOPs/s, fwd + bwd: 128.84 TFLOPs/s
Pytorch fwd: 71.68 TFLOPs/s, bwd: 96.25 TFLOPs/s, fwd + bwd: 87.66 TFLOPs/s
### causal=False, headdim=256, batch_size=8, seqlen=2048 ###
Flash2 fwd: 179.13 TFLOPs/s, bwd: 124.08 TFLOPs/s, fwd + bwd: 136.02 TFLOPs/s
Pytorch fwd: 80.23 TFLOPs/s, bwd: 118.59 TFLOPs/s, fwd + bwd: 104.34 TFLOPs/s
### causal=False, headdim=256, batch_size=4, seqlen=4096 ###
Flash2 fwd: 175.24 TFLOPs/s, bwd: 128.58 TFLOPs/s, fwd + bwd: 139.17 TFLOPs/s
Pytorch fwd: 89.35 TFLOPs/s, bwd: 134.33 TFLOPs/s, fwd + bwd: 117.44 TFLOPs/s
### causal=False, headdim=256, batch_size=2, seqlen=8192 ###
Flash2 fwd: 173.76 TFLOPs/s, bwd: 132.57 TFLOPs/s, fwd + bwd: 142.20 TFLOPs/s
Pytorch fwd: 88.81 TFLOPs/s, bwd: 137.83 TFLOPs/s, fwd + bwd: 119.05 TFLOPs/s
### causal=False, headdim=256, batch_size=1, seqlen=16384 ###
Flash2 fwd: 174.00 TFLOPs/s, bwd: 135.22 TFLOPs/s, fwd + bwd: 144.42 TFLOPs/s
Pytorch fwd: 98.29 TFLOPs/s, bwd: 136.85 TFLOPs/s, fwd + bwd: 123.05 TFLOPs/s
Traceback (most recent call last):
  File "/home/v-feiychen/flash-attention/benchmarks/benchmark_flash_attention.py", line 103, in <module>
    f, b = time_fwd_bwd(
  File "/home/v-feiychen/flash-attention/benchmarks/benchmark_flash_attention.py", line 67, in time_fwd_bwd
    time_f, time_b = benchmark_fwd_bwd(func, *args, **kwargs)
  File "/anaconda/envs/flashattention/lib/python3.9/site-packages/flash_attn-2.0.9-py3.9-linux-x86_64.egg/flash_attn/utils/benchmark.py", line 140, in benchmark_fwd_bwd
    benchmark_backward(
  File "/anaconda/envs/flashattention/lib/python3.9/site-packages/flash_attn-2.0.9-py3.9-linux-x86_64.egg/flash_attn/utils/benchmark.py", line 66, in benchmark_backward
    m = t.timeit(repeats)
  File "/anaconda/envs/flashattention/lib/python3.9/site-packages/torch/utils/benchmark/utils/timer.py", line 266, in timeit
    self._timeit(number=max(int(number // 100), 2))
  File "/anaconda/envs/flashattention/lib/python3.9/site-packages/torch/utils/benchmark/utils/timer.py", line 256, in _timeit
    return max(self._timer.timeit(number), 1e-9)
  File "/anaconda/envs/flashattention/lib/python3.9/timeit.py", line 177, in timeit
    timing = self.inner(it, self.timer)
  File "<timeit-src>", line 6, in inner
  File "/anaconda/envs/flashattention/lib/python3.9/site-packages/flash_attn-2.0.9-py3.9-linux-x86_64.egg/flash_attn/utils/benchmark.py", line 59, in f
    y.backward(grad, retain_graph=True)
  File "/anaconda/envs/flashattention/lib/python3.9/site-packages/torch/_tensor.py", line 396, in backward
    torch.autograd.backward(self, gradient, retain_graph, create_graph, inputs=inputs)
  File "/anaconda/envs/flashattention/lib/python3.9/site-packages/torch/autograd/__init__.py", line 173, in backward
    Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
  File "/anaconda/envs/flashattention/lib/python3.9/site-packages/torch/autograd/function.py", line 253, in apply
    return user_fn(self, *args)
  File "/anaconda/envs/flashattention/lib/python3.9/site-packages/flash_attn-2.0.9-py3.9-linux-x86_64.egg/flash_attn/flash_attn_interface.py", line 185, in backward
    _flash_attn_backward(
  File "/anaconda/envs/flashattention/lib/python3.9/site-packages/flash_attn-2.0.9-py3.9-linux-x86_64.egg/flash_attn/flash_attn_interface.py", line 93, in _flash_attn_backward
    dq, dk, dv, softmax_d, = flash_attn_cuda.bwd(
RuntimeError: CUDA error: invalid argument
CUDA kernel errors might be asynchronously reported at some other API call,so the stacktrace below might be incorrect.
For debugging consider passing CUDA_LAUNCH_BLOCKING=1.