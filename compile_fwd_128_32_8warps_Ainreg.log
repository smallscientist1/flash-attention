

torch.__version__  = 1.12.1


running install
running bdist_egg
running egg_info
writing flash_attn.egg-info/PKG-INFO
writing dependency_links to flash_attn.egg-info/dependency_links.txt
writing requirements to flash_attn.egg-info/requires.txt
writing top-level names to flash_attn.egg-info/top_level.txt
reading manifest file 'flash_attn.egg-info/SOURCES.txt'
reading manifest template 'MANIFEST.in'
adding license file 'LICENSE'
adding license file 'AUTHORS'
writing manifest file 'flash_attn.egg-info/SOURCES.txt'
installing library code to build/bdist.linux-x86_64/egg
running install_lib
running build_py
creating build
creating build/lib.linux-x86_64-cpython-39
creating build/lib.linux-x86_64-cpython-39/flash_attn
copying flash_attn/flash_attn_triton_og.py -> build/lib.linux-x86_64-cpython-39/flash_attn
copying flash_attn/__init__.py -> build/lib.linux-x86_64-cpython-39/flash_attn
copying flash_attn/fused_softmax.py -> build/lib.linux-x86_64-cpython-39/flash_attn
copying flash_attn/flash_blocksparse_attn_interface.py -> build/lib.linux-x86_64-cpython-39/flash_attn
copying flash_attn/flash_attn_interface.py -> build/lib.linux-x86_64-cpython-39/flash_attn
copying flash_attn/bert_padding.py -> build/lib.linux-x86_64-cpython-39/flash_attn
copying flash_attn/flash_attn_triton.py -> build/lib.linux-x86_64-cpython-39/flash_attn
copying flash_attn/flash_blocksparse_attention.py -> build/lib.linux-x86_64-cpython-39/flash_attn
creating build/lib.linux-x86_64-cpython-39/flash_attn/ops
copying flash_attn/ops/__init__.py -> build/lib.linux-x86_64-cpython-39/flash_attn/ops
copying flash_attn/ops/activations.py -> build/lib.linux-x86_64-cpython-39/flash_attn/ops
copying flash_attn/ops/fused_dense.py -> build/lib.linux-x86_64-cpython-39/flash_attn/ops
copying flash_attn/ops/rms_norm.py -> build/lib.linux-x86_64-cpython-39/flash_attn/ops
copying flash_attn/ops/layer_norm.py -> build/lib.linux-x86_64-cpython-39/flash_attn/ops
creating build/lib.linux-x86_64-cpython-39/flash_attn/layers
copying flash_attn/layers/__init__.py -> build/lib.linux-x86_64-cpython-39/flash_attn/layers
copying flash_attn/layers/patch_embed.py -> build/lib.linux-x86_64-cpython-39/flash_attn/layers
copying flash_attn/layers/rotary.py -> build/lib.linux-x86_64-cpython-39/flash_attn/layers
creating build/lib.linux-x86_64-cpython-39/flash_attn/utils
copying flash_attn/utils/__init__.py -> build/lib.linux-x86_64-cpython-39/flash_attn/utils
copying flash_attn/utils/pretrained.py -> build/lib.linux-x86_64-cpython-39/flash_attn/utils
copying flash_attn/utils/distributed.py -> build/lib.linux-x86_64-cpython-39/flash_attn/utils
copying flash_attn/utils/benchmark.py -> build/lib.linux-x86_64-cpython-39/flash_attn/utils
copying flash_attn/utils/generation.py -> build/lib.linux-x86_64-cpython-39/flash_attn/utils
creating build/lib.linux-x86_64-cpython-39/flash_attn/losses
copying flash_attn/losses/__init__.py -> build/lib.linux-x86_64-cpython-39/flash_attn/losses
copying flash_attn/losses/cross_entropy.py -> build/lib.linux-x86_64-cpython-39/flash_attn/losses
creating build/lib.linux-x86_64-cpython-39/flash_attn/modules
copying flash_attn/modules/__init__.py -> build/lib.linux-x86_64-cpython-39/flash_attn/modules
copying flash_attn/modules/block.py -> build/lib.linux-x86_64-cpython-39/flash_attn/modules
copying flash_attn/modules/mlp.py -> build/lib.linux-x86_64-cpython-39/flash_attn/modules
copying flash_attn/modules/mha.py -> build/lib.linux-x86_64-cpython-39/flash_attn/modules
copying flash_attn/modules/embedding.py -> build/lib.linux-x86_64-cpython-39/flash_attn/modules
creating build/lib.linux-x86_64-cpython-39/flash_attn/models
copying flash_attn/models/baichuan.py -> build/lib.linux-x86_64-cpython-39/flash_attn/models
copying flash_attn/models/__init__.py -> build/lib.linux-x86_64-cpython-39/flash_attn/models
copying flash_attn/models/opt.py -> build/lib.linux-x86_64-cpython-39/flash_attn/models
copying flash_attn/models/falcon.py -> build/lib.linux-x86_64-cpython-39/flash_attn/models
copying flash_attn/models/gpt_neox.py -> build/lib.linux-x86_64-cpython-39/flash_attn/models
copying flash_attn/models/gptj.py -> build/lib.linux-x86_64-cpython-39/flash_attn/models
copying flash_attn/models/vit.py -> build/lib.linux-x86_64-cpython-39/flash_attn/models
copying flash_attn/models/llama.py -> build/lib.linux-x86_64-cpython-39/flash_attn/models
copying flash_attn/models/gpt.py -> build/lib.linux-x86_64-cpython-39/flash_attn/models
copying flash_attn/models/bert.py -> build/lib.linux-x86_64-cpython-39/flash_attn/models
running build_ext
building 'flash_attn_2_cuda' extension
creating /home/v-feiychen/flash-attention/build/temp.linux-x86_64-cpython-39
creating /home/v-feiychen/flash-attention/build/temp.linux-x86_64-cpython-39/csrc
creating /home/v-feiychen/flash-attention/build/temp.linux-x86_64-cpython-39/csrc/flash_attn
creating /home/v-feiychen/flash-attention/build/temp.linux-x86_64-cpython-39/csrc/flash_attn/src
Emitting ninja build file /home/v-feiychen/flash-attention/build/temp.linux-x86_64-cpython-39/build.ninja...
Compiling objects...
Allowing ninja to set a default number of workers... (overridable by setting the environment variable MAX_JOBS=N)
[1/37] c++ -MMD -MF /home/v-feiychen/flash-attention/build/temp.linux-x86_64-cpython-39/csrc/flash_attn/flash_api.o.d -pthread -B /anaconda/envs/flashattention/compiler_compat -Wno-unused-result -Wsign-compare -DNDEBUG -O2 -Wall -fPIC -O2 -isystem /anaconda/envs/flashattention/include -I/anaconda/envs/flashattention/include -fPIC -O2 -isystem /anaconda/envs/flashattention/include -fPIC -I/home/v-feiychen/flash-attention/csrc/flash_attn -I/home/v-feiychen/flash-attention/csrc/flash_attn/src -I/home/v-feiychen/flash-attention/csrc/cutlass/include -I/anaconda/envs/flashattention/lib/python3.9/site-packages/torch/include -I/anaconda/envs/flashattention/lib/python3.9/site-packages/torch/include/torch/csrc/api/include -I/anaconda/envs/flashattention/lib/python3.9/site-packages/torch/include/TH -I/anaconda/envs/flashattention/lib/python3.9/site-packages/torch/include/THC -I/usr/local/cuda-11.4/include -I/anaconda/envs/flashattention/include/python3.9 -c -c /home/v-feiychen/flash-attention/csrc/flash_attn/flash_api.cpp -o /home/v-feiychen/flash-attention/build/temp.linux-x86_64-cpython-39/csrc/flash_attn/flash_api.o -O3 -std=c++17 -DTORCH_API_INCLUDE_EXTENSION_H '-DPYBIND11_COMPILER_TYPE="_gcc"' '-DPYBIND11_STDLIB="_libstdcpp"' '-DPYBIND11_BUILD_ABI="_cxxabi1011"' -DTORCH_EXTENSION_NAME=flash_attn_2_cuda -D_GLIBCXX_USE_CXX11_ABI=0
/home/v-feiychen/flash-attention/csrc/flash_attn/flash_api.cpp: In function ‘void set_params_fprop(Flash_fwd_params&, size_t, size_t, size_t, size_t, size_t, size_t, size_t, size_t, size_t, at::Tensor, at::Tensor, at::Tensor, at::Tensor, void*, void*, void*, void*, float, float, bool)’:
/home/v-feiychen/flash-attention/csrc/flash_attn/flash_api.cpp:42:38: warning: ‘void* memset(void*, int, size_t)’ clearing an object of non-trivial type ‘struct Flash_fwd_params’; use assignment or value-initialization instead [-Wclass-memaccess]
   42 |     memset(&params, 0, sizeof(params));
      |                                      ^
In file included from /home/v-feiychen/flash-attention/csrc/flash_attn/flash_api.cpp:11:
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash.h:52:8: note: ‘struct Flash_fwd_params’ declared here
   52 | struct Flash_fwd_params : public Qkv_params {
      |        ^~~~~~~~~~~~~~~~
[2/37] /usr/local/cuda-11.4/bin/nvcc  -I/home/v-feiychen/flash-attention/csrc/flash_attn -I/home/v-feiychen/flash-attention/csrc/flash_attn/src -I/home/v-feiychen/flash-attention/csrc/cutlass/include -I/anaconda/envs/flashattention/lib/python3.9/site-packages/torch/include -I/anaconda/envs/flashattention/lib/python3.9/site-packages/torch/include/torch/csrc/api/include -I/anaconda/envs/flashattention/lib/python3.9/site-packages/torch/include/TH -I/anaconda/envs/flashattention/lib/python3.9/site-packages/torch/include/THC -I/usr/local/cuda-11.4/include -I/anaconda/envs/flashattention/include/python3.9 -c -c /home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_bwd_hdim224_fp16_sm80.cu -o /home/v-feiychen/flash-attention/build/temp.linux-x86_64-cpython-39/csrc/flash_attn/src/flash_bwd_hdim224_fp16_sm80.o -D__CUDA_NO_HALF_OPERATORS__ -D__CUDA_NO_HALF_CONVERSIONS__ -D__CUDA_NO_BFLOAT16_CONVERSIONS__ -D__CUDA_NO_HALF2_OPERATORS__ --expt-relaxed-constexpr --compiler-options ''"'"'-fPIC'"'"'' -O3 -std=c++17 -U__CUDA_NO_HALF_OPERATORS__ -U__CUDA_NO_HALF_CONVERSIONS__ -U__CUDA_NO_HALF2_OPERATORS__ -U__CUDA_NO_BFLOAT16_CONVERSIONS__ --expt-relaxed-constexpr --expt-extended-lambda --use_fast_math --ptxas-options=-v -lineinfo -gencode arch=compute_80,code=sm_80 --threads 4 -DTORCH_API_INCLUDE_EXTENSION_H '-DPYBIND11_COMPILER_TYPE="_gcc"' '-DPYBIND11_STDLIB="_libstdcpp"' '-DPYBIND11_BUILD_ABI="_cxxabi1011"' -DTORCH_EXTENSION_NAME=flash_attn_2_cuda -D_GLIBCXX_USE_CXX11_ABI=0
/anaconda/envs/flashattention/lib/python3.9/site-packages/torch/include/c10/core/SymInt.h(84): warning: integer conversion resulted in a change of sign

ptxas info    : 2 bytes gmem
ptxas info    : Compiling entry function '_Z44flash_bwd_dq_dk_dv_loop_seqk_parallel_kernelI23Flash_bwd_kernel_traitsILi224ELi64ELi64ELi8ELi4ELi4ELi4ELb0ELb0EN7cutlass6half_tE19Flash_kernel_traitsILi224ELi64ELi64ELi8ES2_EELb0ELb0ELb0ELb0EEv16Flash_bwd_params' for 'sm_80'
ptxas info    : Function properties for _Z44flash_bwd_dq_dk_dv_loop_seqk_parallel_kernelI23Flash_bwd_kernel_traitsILi224ELi64ELi64ELi8ELi4ELi4ELi4ELb0ELb0EN7cutlass6half_tE19Flash_kernel_traitsILi224ELi64ELi64ELi8ES2_EELb0ELb0ELb0ELb0EEv16Flash_bwd_params
    0 bytes stack frame, 0 bytes spill stores, 0 bytes spill loads
ptxas info    : Used 246 registers, 696 bytes cmem[0]
ptxas info    : Compiling entry function '_Z44flash_bwd_dq_dk_dv_loop_seqk_parallel_kernelI23Flash_bwd_kernel_traitsILi224ELi64ELi64ELi8ELi4ELi4ELi4ELb0ELb0EN7cutlass6half_tE19Flash_kernel_traitsILi224ELi64ELi64ELi8ES2_EELb0ELb0ELb0ELb1EEv16Flash_bwd_params' for 'sm_80'
ptxas info    : Function properties for _Z44flash_bwd_dq_dk_dv_loop_seqk_parallel_kernelI23Flash_bwd_kernel_traitsILi224ELi64ELi64ELi8ELi4ELi4ELi4ELb0ELb0EN7cutlass6half_tE19Flash_kernel_traitsILi224ELi64ELi64ELi8ES2_EELb0ELb0ELb0ELb1EEv16Flash_bwd_params
    0 bytes stack frame, 0 bytes spill stores, 0 bytes spill loads
ptxas info    : Used 246 registers, 696 bytes cmem[0]
ptxas info    : Compiling entry function '_Z44flash_bwd_dq_dk_dv_loop_seqk_parallel_kernelI23Flash_bwd_kernel_traitsILi224ELi64ELi64ELi8ELi4ELi4ELi4ELb0ELb0EN7cutlass6half_tE19Flash_kernel_traitsILi224ELi64ELi64ELi8ES2_EELb0ELb0ELb1ELb0EEv16Flash_bwd_params' for 'sm_80'
ptxas info    : Function properties for _Z44flash_bwd_dq_dk_dv_loop_seqk_parallel_kernelI23Flash_bwd_kernel_traitsILi224ELi64ELi64ELi8ELi4ELi4ELi4ELb0ELb0EN7cutlass6half_tE19Flash_kernel_traitsILi224ELi64ELi64ELi8ES2_EELb0ELb0ELb1ELb0EEv16Flash_bwd_params
    0 bytes stack frame, 0 bytes spill stores, 0 bytes spill loads
ptxas info    : Used 246 registers, 696 bytes cmem[0]
ptxas info    : Compiling entry function '_Z44flash_bwd_dq_dk_dv_loop_seqk_parallel_kernelI23Flash_bwd_kernel_traitsILi224ELi64ELi64ELi8ELi4ELi4ELi4ELb0ELb0EN7cutlass6half_tE19Flash_kernel_traitsILi224ELi64ELi64ELi8ES2_EELb0ELb0ELb1ELb1EEv16Flash_bwd_params' for 'sm_80'
ptxas info    : Function properties for _Z44flash_bwd_dq_dk_dv_loop_seqk_parallel_kernelI23Flash_bwd_kernel_traitsILi224ELi64ELi64ELi8ELi4ELi4ELi4ELb0ELb0EN7cutlass6half_tE19Flash_kernel_traitsILi224ELi64ELi64ELi8ES2_EELb0ELb0ELb1ELb1EEv16Flash_bwd_params
    0 bytes stack frame, 0 bytes spill stores, 0 bytes spill loads
ptxas info    : Used 242 registers, 696 bytes cmem[0]
ptxas info    : Compiling entry function '_Z44flash_bwd_dq_dk_dv_loop_seqk_parallel_kernelI23Flash_bwd_kernel_traitsILi224ELi64ELi64ELi8ELi4ELi4ELi4ELb0ELb0EN7cutlass6half_tE19Flash_kernel_traitsILi224ELi64ELi64ELi8ES2_EELb0ELb1ELb0ELb0EEv16Flash_bwd_params' for 'sm_80'
ptxas info    : Function properties for _Z44flash_bwd_dq_dk_dv_loop_seqk_parallel_kernelI23Flash_bwd_kernel_traitsILi224ELi64ELi64ELi8ELi4ELi4ELi4ELb0ELb0EN7cutlass6half_tE19Flash_kernel_traitsILi224ELi64ELi64ELi8ES2_EELb0ELb1ELb0ELb0EEv16Flash_bwd_params
    0 bytes stack frame, 0 bytes spill stores, 0 bytes spill loads
ptxas info    : Used 252 registers, 696 bytes cmem[0]
ptxas info    : Compiling entry function '_Z44flash_bwd_dq_dk_dv_loop_seqk_parallel_kernelI23Flash_bwd_kernel_traitsILi224ELi64ELi64ELi8ELi4ELi4ELi4ELb0ELb0EN7cutlass6half_tE19Flash_kernel_traitsILi224ELi64ELi64ELi8ES2_EELb0ELb1ELb0ELb1EEv16Flash_bwd_params' for 'sm_80'
ptxas info    : Function properties for _Z44flash_bwd_dq_dk_dv_loop_seqk_parallel_kernelI23Flash_bwd_kernel_traitsILi224ELi64ELi64ELi8ELi4ELi4ELi4ELb0ELb0EN7cutlass6half_tE19Flash_kernel_traitsILi224ELi64ELi64ELi8ES2_EELb0ELb1ELb0ELb1EEv16Flash_bwd_params
    0 bytes stack frame, 0 bytes spill stores, 0 bytes spill loads
ptxas info    : Used 248 registers, 696 bytes cmem[0]
ptxas info    : Compiling entry function '_Z44flash_bwd_dq_dk_dv_loop_seqk_parallel_kernelI23Flash_bwd_kernel_traitsILi224ELi64ELi64ELi8ELi4ELi4ELi4ELb0ELb0EN7cutlass6half_tE19Flash_kernel_traitsILi224ELi64ELi64ELi8ES2_EELb0ELb1ELb1ELb0EEv16Flash_bwd_params' for 'sm_80'
ptxas info    : Function properties for _Z44flash_bwd_dq_dk_dv_loop_seqk_parallel_kernelI23Flash_bwd_kernel_traitsILi224ELi64ELi64ELi8ELi4ELi4ELi4ELb0ELb0EN7cutlass6half_tE19Flash_kernel_traitsILi224ELi64ELi64ELi8ES2_EELb0ELb1ELb1ELb0EEv16Flash_bwd_params
    0 bytes stack frame, 0 bytes spill stores, 0 bytes spill loads
ptxas info    : Used 246 registers, 696 bytes cmem[0]
ptxas info    : Compiling entry function '_Z44flash_bwd_dq_dk_dv_loop_seqk_parallel_kernelI23Flash_bwd_kernel_traitsILi224ELi64ELi64ELi8ELi4ELi4ELi4ELb0ELb0EN7cutlass6half_tE19Flash_kernel_traitsILi224ELi64ELi64ELi8ES2_EELb0ELb1ELb1ELb1EEv16Flash_bwd_params' for 'sm_80'
ptxas info    : Function properties for _Z44flash_bwd_dq_dk_dv_loop_seqk_parallel_kernelI23Flash_bwd_kernel_traitsILi224ELi64ELi64ELi8ELi4ELi4ELi4ELb0ELb0EN7cutlass6half_tE19Flash_kernel_traitsILi224ELi64ELi64ELi8ES2_EELb0ELb1ELb1ELb1EEv16Flash_bwd_params
    0 bytes stack frame, 0 bytes spill stores, 0 bytes spill loads
ptxas info    : Used 244 registers, 696 bytes cmem[0]
ptxas info    : Compiling entry function '_Z27flash_bwd_convert_dq_kernelI23Flash_bwd_kernel_traitsILi224ELi64ELi64ELi8ELi4ELi4ELi4ELb0ELb0EN7cutlass6half_tE19Flash_kernel_traitsILi224ELi64ELi64ELi8ES2_EEEv16Flash_bwd_params' for 'sm_80'
ptxas info    : Function properties for _Z27flash_bwd_convert_dq_kernelI23Flash_bwd_kernel_traitsILi224ELi64ELi64ELi8ELi4ELi4ELi4ELb0ELb0EN7cutlass6half_tE19Flash_kernel_traitsILi224ELi64ELi64ELi8ES2_EEEv16Flash_bwd_params
    0 bytes stack frame, 0 bytes spill stores, 0 bytes spill loads
ptxas info    : Used 32 registers, 696 bytes cmem[0]
ptxas info    : Compiling entry function '_Z44flash_bwd_dq_dk_dv_loop_seqk_parallel_kernelI23Flash_bwd_kernel_traitsILi224ELi64ELi64ELi8ELi4ELi4ELi4ELb0ELb0EN7cutlass6half_tE19Flash_kernel_traitsILi224ELi64ELi64ELi8ES2_EELb1ELb0ELb0ELb0EEv16Flash_bwd_params' for 'sm_80'
ptxas info    : Function properties for _Z44flash_bwd_dq_dk_dv_loop_seqk_parallel_kernelI23Flash_bwd_kernel_traitsILi224ELi64ELi64ELi8ELi4ELi4ELi4ELb0ELb0EN7cutlass6half_tE19Flash_kernel_traitsILi224ELi64ELi64ELi8ES2_EELb1ELb0ELb0ELb0EEv16Flash_bwd_params
    0 bytes stack frame, 0 bytes spill stores, 0 bytes spill loads
ptxas info    : Used 248 registers, 696 bytes cmem[0]
ptxas info    : Compiling entry function '_Z44flash_bwd_dq_dk_dv_loop_seqk_parallel_kernelI23Flash_bwd_kernel_traitsILi224ELi64ELi64ELi8ELi4ELi4ELi4ELb0ELb0EN7cutlass6half_tE19Flash_kernel_traitsILi224ELi64ELi64ELi8ES2_EELb1ELb0ELb0ELb1EEv16Flash_bwd_params' for 'sm_80'
ptxas info    : Function properties for _Z44flash_bwd_dq_dk_dv_loop_seqk_parallel_kernelI23Flash_bwd_kernel_traitsILi224ELi64ELi64ELi8ELi4ELi4ELi4ELb0ELb0EN7cutlass6half_tE19Flash_kernel_traitsILi224ELi64ELi64ELi8ES2_EELb1ELb0ELb0ELb1EEv16Flash_bwd_params
    0 bytes stack frame, 0 bytes spill stores, 0 bytes spill loads
ptxas info    : Used 246 registers, 696 bytes cmem[0]
ptxas info    : Compiling entry function '_Z44flash_bwd_dq_dk_dv_loop_seqk_parallel_kernelI23Flash_bwd_kernel_traitsILi224ELi64ELi64ELi8ELi4ELi4ELi4ELb0ELb0EN7cutlass6half_tE19Flash_kernel_traitsILi224ELi64ELi64ELi8ES2_EELb1ELb0ELb1ELb0EEv16Flash_bwd_params' for 'sm_80'
ptxas info    : Function properties for _Z44flash_bwd_dq_dk_dv_loop_seqk_parallel_kernelI23Flash_bwd_kernel_traitsILi224ELi64ELi64ELi8ELi4ELi4ELi4ELb0ELb0EN7cutlass6half_tE19Flash_kernel_traitsILi224ELi64ELi64ELi8ES2_EELb1ELb0ELb1ELb0EEv16Flash_bwd_params
    96 bytes stack frame, 104 bytes spill stores, 104 bytes spill loads
ptxas info    : Used 255 registers, 696 bytes cmem[0]
ptxas info    : Compiling entry function '_Z44flash_bwd_dq_dk_dv_loop_seqk_parallel_kernelI23Flash_bwd_kernel_traitsILi224ELi64ELi64ELi8ELi4ELi4ELi4ELb0ELb0EN7cutlass6half_tE19Flash_kernel_traitsILi224ELi64ELi64ELi8ES2_EELb1ELb0ELb1ELb1EEv16Flash_bwd_params' for 'sm_80'
ptxas info    : Function properties for _Z44flash_bwd_dq_dk_dv_loop_seqk_parallel_kernelI23Flash_bwd_kernel_traitsILi224ELi64ELi64ELi8ELi4ELi4ELi4ELb0ELb0EN7cutlass6half_tE19Flash_kernel_traitsILi224ELi64ELi64ELi8ES2_EELb1ELb0ELb1ELb1EEv16Flash_bwd_params
    0 bytes stack frame, 0 bytes spill stores, 0 bytes spill loads
ptxas info    : Used 246 registers, 696 bytes cmem[0]
ptxas info    : Compiling entry function '_Z44flash_bwd_dq_dk_dv_loop_seqk_parallel_kernelI23Flash_bwd_kernel_traitsILi224ELi64ELi64ELi8ELi4ELi4ELi4ELb0ELb0EN7cutlass6half_tE19Flash_kernel_traitsILi224ELi64ELi64ELi8ES2_EELb1ELb1ELb0ELb0EEv16Flash_bwd_params' for 'sm_80'
ptxas info    : Function properties for _Z44flash_bwd_dq_dk_dv_loop_seqk_parallel_kernelI23Flash_bwd_kernel_traitsILi224ELi64ELi64ELi8ELi4ELi4ELi4ELb0ELb0EN7cutlass6half_tE19Flash_kernel_traitsILi224ELi64ELi64ELi8ES2_EELb1ELb1ELb0ELb0EEv16Flash_bwd_params
    16 bytes stack frame, 12 bytes spill stores, 12 bytes spill loads
ptxas info    : Used 255 registers, 696 bytes cmem[0]
ptxas info    : Compiling entry function '_Z44flash_bwd_dq_dk_dv_loop_seqk_parallel_kernelI23Flash_bwd_kernel_traitsILi224ELi64ELi64ELi8ELi4ELi4ELi4ELb0ELb0EN7cutlass6half_tE19Flash_kernel_traitsILi224ELi64ELi64ELi8ES2_EELb1ELb1ELb0ELb1EEv16Flash_bwd_params' for 'sm_80'
ptxas info    : Function properties for _Z44flash_bwd_dq_dk_dv_loop_seqk_parallel_kernelI23Flash_bwd_kernel_traitsILi224ELi64ELi64ELi8ELi4ELi4ELi4ELb0ELb0EN7cutlass6half_tE19Flash_kernel_traitsILi224ELi64ELi64ELi8ES2_EELb1ELb1ELb0ELb1EEv16Flash_bwd_params
    0 bytes stack frame, 0 bytes spill stores, 0 bytes spill loads
ptxas info    : Used 248 registers, 696 bytes cmem[0]
ptxas info    : Compiling entry function '_Z44flash_bwd_dq_dk_dv_loop_seqk_parallel_kernelI23Flash_bwd_kernel_traitsILi224ELi64ELi64ELi8ELi4ELi4ELi4ELb0ELb0EN7cutlass6half_tE19Flash_kernel_traitsILi224ELi64ELi64ELi8ES2_EELb1ELb1ELb1ELb0EEv16Flash_bwd_params' for 'sm_80'
ptxas info    : Function properties for _Z44flash_bwd_dq_dk_dv_loop_seqk_parallel_kernelI23Flash_bwd_kernel_traitsILi224ELi64ELi64ELi8ELi4ELi4ELi4ELb0ELb0EN7cutlass6half_tE19Flash_kernel_traitsILi224ELi64ELi64ELi8ES2_EELb1ELb1ELb1ELb0EEv16Flash_bwd_params
    80 bytes stack frame, 92 bytes spill stores, 112 bytes spill loads
ptxas info    : Used 255 registers, 696 bytes cmem[0]
ptxas info    : Compiling entry function '_Z44flash_bwd_dq_dk_dv_loop_seqk_parallel_kernelI23Flash_bwd_kernel_traitsILi224ELi64ELi64ELi8ELi4ELi4ELi4ELb0ELb0EN7cutlass6half_tE19Flash_kernel_traitsILi224ELi64ELi64ELi8ES2_EELb1ELb1ELb1ELb1EEv16Flash_bwd_params' for 'sm_80'
ptxas info    : Function properties for _Z44flash_bwd_dq_dk_dv_loop_seqk_parallel_kernelI23Flash_bwd_kernel_traitsILi224ELi64ELi64ELi8ELi4ELi4ELi4ELb0ELb0EN7cutlass6half_tE19Flash_kernel_traitsILi224ELi64ELi64ELi8ES2_EELb1ELb1ELb1ELb1EEv16Flash_bwd_params
    0 bytes stack frame, 0 bytes spill stores, 0 bytes spill loads
ptxas info    : Used 248 registers, 696 bytes cmem[0]
ptxas info    : Compiling entry function '_Z25flash_bwd_dot_do_o_kernelILb1E23Flash_bwd_kernel_traitsILi224ELi64ELi64ELi8ELi4ELi4ELi4ELb0ELb0EN7cutlass6half_tE19Flash_kernel_traitsILi224ELi64ELi64ELi8ES2_EEEv16Flash_bwd_params' for 'sm_80'
ptxas info    : Function properties for _Z25flash_bwd_dot_do_o_kernelILb1E23Flash_bwd_kernel_traitsILi224ELi64ELi64ELi8ELi4ELi4ELi4ELb0ELb0EN7cutlass6half_tE19Flash_kernel_traitsILi224ELi64ELi64ELi8ES2_EEEv16Flash_bwd_params
    0 bytes stack frame, 0 bytes spill stores, 0 bytes spill loads
ptxas info    : Used 79 registers, 696 bytes cmem[0]
/anaconda/envs/flashattention/lib/python3.9/site-packages/torch/include/c10/core/SymInt.h(84): warning: integer conversion resulted in a change of sign

[3/37] /usr/local/cuda-11.4/bin/nvcc  -I/home/v-feiychen/flash-attention/csrc/flash_attn -I/home/v-feiychen/flash-attention/csrc/flash_attn/src -I/home/v-feiychen/flash-attention/csrc/cutlass/include -I/anaconda/envs/flashattention/lib/python3.9/site-packages/torch/include -I/anaconda/envs/flashattention/lib/python3.9/site-packages/torch/include/torch/csrc/api/include -I/anaconda/envs/flashattention/lib/python3.9/site-packages/torch/include/TH -I/anaconda/envs/flashattention/lib/python3.9/site-packages/torch/include/THC -I/usr/local/cuda-11.4/include -I/anaconda/envs/flashattention/include/python3.9 -c -c /home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_bwd_hdim224_bf16_sm80.cu -o /home/v-feiychen/flash-attention/build/temp.linux-x86_64-cpython-39/csrc/flash_attn/src/flash_bwd_hdim224_bf16_sm80.o -D__CUDA_NO_HALF_OPERATORS__ -D__CUDA_NO_HALF_CONVERSIONS__ -D__CUDA_NO_BFLOAT16_CONVERSIONS__ -D__CUDA_NO_HALF2_OPERATORS__ --expt-relaxed-constexpr --compiler-options ''"'"'-fPIC'"'"'' -O3 -std=c++17 -U__CUDA_NO_HALF_OPERATORS__ -U__CUDA_NO_HALF_CONVERSIONS__ -U__CUDA_NO_HALF2_OPERATORS__ -U__CUDA_NO_BFLOAT16_CONVERSIONS__ --expt-relaxed-constexpr --expt-extended-lambda --use_fast_math --ptxas-options=-v -lineinfo -gencode arch=compute_80,code=sm_80 --threads 4 -DTORCH_API_INCLUDE_EXTENSION_H '-DPYBIND11_COMPILER_TYPE="_gcc"' '-DPYBIND11_STDLIB="_libstdcpp"' '-DPYBIND11_BUILD_ABI="_cxxabi1011"' -DTORCH_EXTENSION_NAME=flash_attn_2_cuda -D_GLIBCXX_USE_CXX11_ABI=0
/anaconda/envs/flashattention/lib/python3.9/site-packages/torch/include/c10/core/SymInt.h(84): warning: integer conversion resulted in a change of sign

ptxas info    : 2 bytes gmem
ptxas info    : Compiling entry function '_Z44flash_bwd_dq_dk_dv_loop_seqk_parallel_kernelI23Flash_bwd_kernel_traitsILi224ELi64ELi64ELi8ELi4ELi4ELi4ELb0ELb0EN7cutlass10bfloat16_tE19Flash_kernel_traitsILi224ELi64ELi64ELi8ES2_EELb0ELb0ELb0ELb0EEv16Flash_bwd_params' for 'sm_80'
ptxas info    : Function properties for _Z44flash_bwd_dq_dk_dv_loop_seqk_parallel_kernelI23Flash_bwd_kernel_traitsILi224ELi64ELi64ELi8ELi4ELi4ELi4ELb0ELb0EN7cutlass10bfloat16_tE19Flash_kernel_traitsILi224ELi64ELi64ELi8ES2_EELb0ELb0ELb0ELb0EEv16Flash_bwd_params
    0 bytes stack frame, 0 bytes spill stores, 0 bytes spill loads
ptxas info    : Used 246 registers, 696 bytes cmem[0]
ptxas info    : Compiling entry function '_Z44flash_bwd_dq_dk_dv_loop_seqk_parallel_kernelI23Flash_bwd_kernel_traitsILi224ELi64ELi64ELi8ELi4ELi4ELi4ELb0ELb0EN7cutlass10bfloat16_tE19Flash_kernel_traitsILi224ELi64ELi64ELi8ES2_EELb0ELb0ELb0ELb1EEv16Flash_bwd_params' for 'sm_80'
ptxas info    : Function properties for _Z44flash_bwd_dq_dk_dv_loop_seqk_parallel_kernelI23Flash_bwd_kernel_traitsILi224ELi64ELi64ELi8ELi4ELi4ELi4ELb0ELb0EN7cutlass10bfloat16_tE19Flash_kernel_traitsILi224ELi64ELi64ELi8ES2_EELb0ELb0ELb0ELb1EEv16Flash_bwd_params
    0 bytes stack frame, 0 bytes spill stores, 0 bytes spill loads
ptxas info    : Used 246 registers, 696 bytes cmem[0]
ptxas info    : Compiling entry function '_Z44flash_bwd_dq_dk_dv_loop_seqk_parallel_kernelI23Flash_bwd_kernel_traitsILi224ELi64ELi64ELi8ELi4ELi4ELi4ELb0ELb0EN7cutlass10bfloat16_tE19Flash_kernel_traitsILi224ELi64ELi64ELi8ES2_EELb0ELb0ELb1ELb0EEv16Flash_bwd_params' for 'sm_80'
ptxas info    : Function properties for _Z44flash_bwd_dq_dk_dv_loop_seqk_parallel_kernelI23Flash_bwd_kernel_traitsILi224ELi64ELi64ELi8ELi4ELi4ELi4ELb0ELb0EN7cutlass10bfloat16_tE19Flash_kernel_traitsILi224ELi64ELi64ELi8ES2_EELb0ELb0ELb1ELb0EEv16Flash_bwd_params
    0 bytes stack frame, 0 bytes spill stores, 0 bytes spill loads
ptxas info    : Used 246 registers, 696 bytes cmem[0]
ptxas info    : Compiling entry function '_Z44flash_bwd_dq_dk_dv_loop_seqk_parallel_kernelI23Flash_bwd_kernel_traitsILi224ELi64ELi64ELi8ELi4ELi4ELi4ELb0ELb0EN7cutlass10bfloat16_tE19Flash_kernel_traitsILi224ELi64ELi64ELi8ES2_EELb0ELb0ELb1ELb1EEv16Flash_bwd_params' for 'sm_80'
ptxas info    : Function properties for _Z44flash_bwd_dq_dk_dv_loop_seqk_parallel_kernelI23Flash_bwd_kernel_traitsILi224ELi64ELi64ELi8ELi4ELi4ELi4ELb0ELb0EN7cutlass10bfloat16_tE19Flash_kernel_traitsILi224ELi64ELi64ELi8ES2_EELb0ELb0ELb1ELb1EEv16Flash_bwd_params
    0 bytes stack frame, 0 bytes spill stores, 0 bytes spill loads
ptxas info    : Used 242 registers, 696 bytes cmem[0]
ptxas info    : Compiling entry function '_Z44flash_bwd_dq_dk_dv_loop_seqk_parallel_kernelI23Flash_bwd_kernel_traitsILi224ELi64ELi64ELi8ELi4ELi4ELi4ELb0ELb0EN7cutlass10bfloat16_tE19Flash_kernel_traitsILi224ELi64ELi64ELi8ES2_EELb0ELb1ELb0ELb0EEv16Flash_bwd_params' for 'sm_80'
ptxas info    : Function properties for _Z44flash_bwd_dq_dk_dv_loop_seqk_parallel_kernelI23Flash_bwd_kernel_traitsILi224ELi64ELi64ELi8ELi4ELi4ELi4ELb0ELb0EN7cutlass10bfloat16_tE19Flash_kernel_traitsILi224ELi64ELi64ELi8ES2_EELb0ELb1ELb0ELb0EEv16Flash_bwd_params
    0 bytes stack frame, 0 bytes spill stores, 0 bytes spill loads
ptxas info    : Used 252 registers, 696 bytes cmem[0]
ptxas info    : Compiling entry function '_Z44flash_bwd_dq_dk_dv_loop_seqk_parallel_kernelI23Flash_bwd_kernel_traitsILi224ELi64ELi64ELi8ELi4ELi4ELi4ELb0ELb0EN7cutlass10bfloat16_tE19Flash_kernel_traitsILi224ELi64ELi64ELi8ES2_EELb0ELb1ELb0ELb1EEv16Flash_bwd_params' for 'sm_80'
ptxas info    : Function properties for _Z44flash_bwd_dq_dk_dv_loop_seqk_parallel_kernelI23Flash_bwd_kernel_traitsILi224ELi64ELi64ELi8ELi4ELi4ELi4ELb0ELb0EN7cutlass10bfloat16_tE19Flash_kernel_traitsILi224ELi64ELi64ELi8ES2_EELb0ELb1ELb0ELb1EEv16Flash_bwd_params
    0 bytes stack frame, 0 bytes spill stores, 0 bytes spill loads
ptxas info    : Used 248 registers, 696 bytes cmem[0]
ptxas info    : Compiling entry function '_Z44flash_bwd_dq_dk_dv_loop_seqk_parallel_kernelI23Flash_bwd_kernel_traitsILi224ELi64ELi64ELi8ELi4ELi4ELi4ELb0ELb0EN7cutlass10bfloat16_tE19Flash_kernel_traitsILi224ELi64ELi64ELi8ES2_EELb0ELb1ELb1ELb0EEv16Flash_bwd_params' for 'sm_80'
ptxas info    : Function properties for _Z44flash_bwd_dq_dk_dv_loop_seqk_parallel_kernelI23Flash_bwd_kernel_traitsILi224ELi64ELi64ELi8ELi4ELi4ELi4ELb0ELb0EN7cutlass10bfloat16_tE19Flash_kernel_traitsILi224ELi64ELi64ELi8ES2_EELb0ELb1ELb1ELb0EEv16Flash_bwd_params
    0 bytes stack frame, 0 bytes spill stores, 0 bytes spill loads
ptxas info    : Used 246 registers, 696 bytes cmem[0]
ptxas info    : Compiling entry function '_Z44flash_bwd_dq_dk_dv_loop_seqk_parallel_kernelI23Flash_bwd_kernel_traitsILi224ELi64ELi64ELi8ELi4ELi4ELi4ELb0ELb0EN7cutlass10bfloat16_tE19Flash_kernel_traitsILi224ELi64ELi64ELi8ES2_EELb0ELb1ELb1ELb1EEv16Flash_bwd_params' for 'sm_80'
ptxas info    : Function properties for _Z44flash_bwd_dq_dk_dv_loop_seqk_parallel_kernelI23Flash_bwd_kernel_traitsILi224ELi64ELi64ELi8ELi4ELi4ELi4ELb0ELb0EN7cutlass10bfloat16_tE19Flash_kernel_traitsILi224ELi64ELi64ELi8ES2_EELb0ELb1ELb1ELb1EEv16Flash_bwd_params
    0 bytes stack frame, 0 bytes spill stores, 0 bytes spill loads
ptxas info    : Used 244 registers, 696 bytes cmem[0]
ptxas info    : Compiling entry function '_Z27flash_bwd_convert_dq_kernelI23Flash_bwd_kernel_traitsILi224ELi64ELi64ELi8ELi4ELi4ELi4ELb0ELb0EN7cutlass10bfloat16_tE19Flash_kernel_traitsILi224ELi64ELi64ELi8ES2_EEEv16Flash_bwd_params' for 'sm_80'
ptxas info    : Function properties for _Z27flash_bwd_convert_dq_kernelI23Flash_bwd_kernel_traitsILi224ELi64ELi64ELi8ELi4ELi4ELi4ELb0ELb0EN7cutlass10bfloat16_tE19Flash_kernel_traitsILi224ELi64ELi64ELi8ES2_EEEv16Flash_bwd_params
    0 bytes stack frame, 0 bytes spill stores, 0 bytes spill loads
ptxas info    : Used 32 registers, 696 bytes cmem[0]
ptxas info    : Compiling entry function '_Z44flash_bwd_dq_dk_dv_loop_seqk_parallel_kernelI23Flash_bwd_kernel_traitsILi224ELi64ELi64ELi8ELi4ELi4ELi4ELb0ELb0EN7cutlass10bfloat16_tE19Flash_kernel_traitsILi224ELi64ELi64ELi8ES2_EELb1ELb0ELb0ELb0EEv16Flash_bwd_params' for 'sm_80'
ptxas info    : Function properties for _Z44flash_bwd_dq_dk_dv_loop_seqk_parallel_kernelI23Flash_bwd_kernel_traitsILi224ELi64ELi64ELi8ELi4ELi4ELi4ELb0ELb0EN7cutlass10bfloat16_tE19Flash_kernel_traitsILi224ELi64ELi64ELi8ES2_EELb1ELb0ELb0ELb0EEv16Flash_bwd_params
    0 bytes stack frame, 0 bytes spill stores, 0 bytes spill loads
ptxas info    : Used 248 registers, 696 bytes cmem[0]
ptxas info    : Compiling entry function '_Z44flash_bwd_dq_dk_dv_loop_seqk_parallel_kernelI23Flash_bwd_kernel_traitsILi224ELi64ELi64ELi8ELi4ELi4ELi4ELb0ELb0EN7cutlass10bfloat16_tE19Flash_kernel_traitsILi224ELi64ELi64ELi8ES2_EELb1ELb0ELb0ELb1EEv16Flash_bwd_params' for 'sm_80'
ptxas info    : Function properties for _Z44flash_bwd_dq_dk_dv_loop_seqk_parallel_kernelI23Flash_bwd_kernel_traitsILi224ELi64ELi64ELi8ELi4ELi4ELi4ELb0ELb0EN7cutlass10bfloat16_tE19Flash_kernel_traitsILi224ELi64ELi64ELi8ES2_EELb1ELb0ELb0ELb1EEv16Flash_bwd_params
    0 bytes stack frame, 0 bytes spill stores, 0 bytes spill loads
ptxas info    : Used 246 registers, 696 bytes cmem[0]
ptxas info    : Compiling entry function '_Z44flash_bwd_dq_dk_dv_loop_seqk_parallel_kernelI23Flash_bwd_kernel_traitsILi224ELi64ELi64ELi8ELi4ELi4ELi4ELb0ELb0EN7cutlass10bfloat16_tE19Flash_kernel_traitsILi224ELi64ELi64ELi8ES2_EELb1ELb0ELb1ELb0EEv16Flash_bwd_params' for 'sm_80'
ptxas info    : Function properties for _Z44flash_bwd_dq_dk_dv_loop_seqk_parallel_kernelI23Flash_bwd_kernel_traitsILi224ELi64ELi64ELi8ELi4ELi4ELi4ELb0ELb0EN7cutlass10bfloat16_tE19Flash_kernel_traitsILi224ELi64ELi64ELi8ES2_EELb1ELb0ELb1ELb0EEv16Flash_bwd_params
    96 bytes stack frame, 104 bytes spill stores, 104 bytes spill loads
ptxas info    : Used 255 registers, 696 bytes cmem[0]
ptxas info    : Compiling entry function '_Z44flash_bwd_dq_dk_dv_loop_seqk_parallel_kernelI23Flash_bwd_kernel_traitsILi224ELi64ELi64ELi8ELi4ELi4ELi4ELb0ELb0EN7cutlass10bfloat16_tE19Flash_kernel_traitsILi224ELi64ELi64ELi8ES2_EELb1ELb0ELb1ELb1EEv16Flash_bwd_params' for 'sm_80'
ptxas info    : Function properties for _Z44flash_bwd_dq_dk_dv_loop_seqk_parallel_kernelI23Flash_bwd_kernel_traitsILi224ELi64ELi64ELi8ELi4ELi4ELi4ELb0ELb0EN7cutlass10bfloat16_tE19Flash_kernel_traitsILi224ELi64ELi64ELi8ES2_EELb1ELb0ELb1ELb1EEv16Flash_bwd_params
    0 bytes stack frame, 0 bytes spill stores, 0 bytes spill loads
ptxas info    : Used 246 registers, 696 bytes cmem[0]
ptxas info    : Compiling entry function '_Z44flash_bwd_dq_dk_dv_loop_seqk_parallel_kernelI23Flash_bwd_kernel_traitsILi224ELi64ELi64ELi8ELi4ELi4ELi4ELb0ELb0EN7cutlass10bfloat16_tE19Flash_kernel_traitsILi224ELi64ELi64ELi8ES2_EELb1ELb1ELb0ELb0EEv16Flash_bwd_params' for 'sm_80'
ptxas info    : Function properties for _Z44flash_bwd_dq_dk_dv_loop_seqk_parallel_kernelI23Flash_bwd_kernel_traitsILi224ELi64ELi64ELi8ELi4ELi4ELi4ELb0ELb0EN7cutlass10bfloat16_tE19Flash_kernel_traitsILi224ELi64ELi64ELi8ES2_EELb1ELb1ELb0ELb0EEv16Flash_bwd_params
    16 bytes stack frame, 12 bytes spill stores, 12 bytes spill loads
ptxas info    : Used 255 registers, 696 bytes cmem[0]
ptxas info    : Compiling entry function '_Z44flash_bwd_dq_dk_dv_loop_seqk_parallel_kernelI23Flash_bwd_kernel_traitsILi224ELi64ELi64ELi8ELi4ELi4ELi4ELb0ELb0EN7cutlass10bfloat16_tE19Flash_kernel_traitsILi224ELi64ELi64ELi8ES2_EELb1ELb1ELb0ELb1EEv16Flash_bwd_params' for 'sm_80'
ptxas info    : Function properties for _Z44flash_bwd_dq_dk_dv_loop_seqk_parallel_kernelI23Flash_bwd_kernel_traitsILi224ELi64ELi64ELi8ELi4ELi4ELi4ELb0ELb0EN7cutlass10bfloat16_tE19Flash_kernel_traitsILi224ELi64ELi64ELi8ES2_EELb1ELb1ELb0ELb1EEv16Flash_bwd_params
    0 bytes stack frame, 0 bytes spill stores, 0 bytes spill loads
ptxas info    : Used 248 registers, 696 bytes cmem[0]
ptxas info    : Compiling entry function '_Z44flash_bwd_dq_dk_dv_loop_seqk_parallel_kernelI23Flash_bwd_kernel_traitsILi224ELi64ELi64ELi8ELi4ELi4ELi4ELb0ELb0EN7cutlass10bfloat16_tE19Flash_kernel_traitsILi224ELi64ELi64ELi8ES2_EELb1ELb1ELb1ELb0EEv16Flash_bwd_params' for 'sm_80'
ptxas info    : Function properties for _Z44flash_bwd_dq_dk_dv_loop_seqk_parallel_kernelI23Flash_bwd_kernel_traitsILi224ELi64ELi64ELi8ELi4ELi4ELi4ELb0ELb0EN7cutlass10bfloat16_tE19Flash_kernel_traitsILi224ELi64ELi64ELi8ES2_EELb1ELb1ELb1ELb0EEv16Flash_bwd_params
    80 bytes stack frame, 92 bytes spill stores, 112 bytes spill loads
ptxas info    : Used 255 registers, 696 bytes cmem[0]
ptxas info    : Compiling entry function '_Z44flash_bwd_dq_dk_dv_loop_seqk_parallel_kernelI23Flash_bwd_kernel_traitsILi224ELi64ELi64ELi8ELi4ELi4ELi4ELb0ELb0EN7cutlass10bfloat16_tE19Flash_kernel_traitsILi224ELi64ELi64ELi8ES2_EELb1ELb1ELb1ELb1EEv16Flash_bwd_params' for 'sm_80'
ptxas info    : Function properties for _Z44flash_bwd_dq_dk_dv_loop_seqk_parallel_kernelI23Flash_bwd_kernel_traitsILi224ELi64ELi64ELi8ELi4ELi4ELi4ELb0ELb0EN7cutlass10bfloat16_tE19Flash_kernel_traitsILi224ELi64ELi64ELi8ES2_EELb1ELb1ELb1ELb1EEv16Flash_bwd_params
    0 bytes stack frame, 0 bytes spill stores, 0 bytes spill loads
ptxas info    : Used 248 registers, 696 bytes cmem[0]
ptxas info    : Compiling entry function '_Z25flash_bwd_dot_do_o_kernelILb1E23Flash_bwd_kernel_traitsILi224ELi64ELi64ELi8ELi4ELi4ELi4ELb0ELb0EN7cutlass10bfloat16_tE19Flash_kernel_traitsILi224ELi64ELi64ELi8ES2_EEEv16Flash_bwd_params' for 'sm_80'
ptxas info    : Function properties for _Z25flash_bwd_dot_do_o_kernelILb1E23Flash_bwd_kernel_traitsILi224ELi64ELi64ELi8ELi4ELi4ELi4ELb0ELb0EN7cutlass10bfloat16_tE19Flash_kernel_traitsILi224ELi64ELi64ELi8ES2_EEEv16Flash_bwd_params
    0 bytes stack frame, 0 bytes spill stores, 0 bytes spill loads
ptxas info    : Used 79 registers, 696 bytes cmem[0]
/anaconda/envs/flashattention/lib/python3.9/site-packages/torch/include/c10/core/SymInt.h(84): warning: integer conversion resulted in a change of sign

[4/37] /usr/local/cuda-11.4/bin/nvcc  -I/home/v-feiychen/flash-attention/csrc/flash_attn -I/home/v-feiychen/flash-attention/csrc/flash_attn/src -I/home/v-feiychen/flash-attention/csrc/cutlass/include -I/anaconda/envs/flashattention/lib/python3.9/site-packages/torch/include -I/anaconda/envs/flashattention/lib/python3.9/site-packages/torch/include/torch/csrc/api/include -I/anaconda/envs/flashattention/lib/python3.9/site-packages/torch/include/TH -I/anaconda/envs/flashattention/lib/python3.9/site-packages/torch/include/THC -I/usr/local/cuda-11.4/include -I/anaconda/envs/flashattention/include/python3.9 -c -c /home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_hdim192_fp16_sm80.cu -o /home/v-feiychen/flash-attention/build/temp.linux-x86_64-cpython-39/csrc/flash_attn/src/flash_fwd_hdim192_fp16_sm80.o -D__CUDA_NO_HALF_OPERATORS__ -D__CUDA_NO_HALF_CONVERSIONS__ -D__CUDA_NO_BFLOAT16_CONVERSIONS__ -D__CUDA_NO_HALF2_OPERATORS__ --expt-relaxed-constexpr --compiler-options ''"'"'-fPIC'"'"'' -O3 -std=c++17 -U__CUDA_NO_HALF_OPERATORS__ -U__CUDA_NO_HALF_CONVERSIONS__ -U__CUDA_NO_HALF2_OPERATORS__ -U__CUDA_NO_BFLOAT16_CONVERSIONS__ --expt-relaxed-constexpr --expt-extended-lambda --use_fast_math --ptxas-options=-v -lineinfo -gencode arch=compute_80,code=sm_80 --threads 4 -DTORCH_API_INCLUDE_EXTENSION_H '-DPYBIND11_COMPILER_TYPE="_gcc"' '-DPYBIND11_STDLIB="_libstdcpp"' '-DPYBIND11_BUILD_ABI="_cxxabi1011"' -DTORCH_EXTENSION_NAME=flash_attn_2_cuda -D_GLIBCXX_USE_CXX11_ABI=0
/anaconda/envs/flashattention/lib/python3.9/site-packages/torch/include/c10/core/SymInt.h(84): warning: integer conversion resulted in a change of sign

/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_kernel.h(140): warning: variable "MMA_M" was declared but never referenced
          detected during:
            instantiation of "void flash::compute_attn_1rowblock<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params,Is_Q_split_K>(const Params &, int, int, int) [with Kernel_traits=Flash_fwd_kernel_traits<192, 64, 64, 4, false, false, cutlass::half_t, Flash_kernel_traits<192, 64, 64, 4, cutlass::half_t>>, Is_dropout=true, Is_causal=true, Is_even_N=true, Is_even_K=true, Return_softmax=true, Params=Flash_fwd_params, Is_Q_split_K=false]" 
(589): here
            instantiation of "void flash::compute_attn<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &) [with Kernel_traits=Flash_fwd_kernel_traits<192, 64, 64, 4, false, false, cutlass::half_t, Flash_kernel_traits<192, 64, 64, 4, cutlass::half_t>>, Is_dropout=true, Is_causal=true, Is_even_N=true, Is_even_K=true, Return_softmax=true, Params=Flash_fwd_params]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(15): here
            instantiation of "void flash_fwd_kernel<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax>(Flash_fwd_params) [with Kernel_traits=Flash_fwd_kernel_traits<192, 64, 64, 4, false, false, cutlass::half_t, Flash_kernel_traits<192, 64, 64, 4, cutlass::half_t>>, Is_dropout=true, Is_causal=true, Is_even_N=true, Is_even_K=true, Return_softmax=true]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(34): here
            instantiation of "void run_flash_fwd<Kernel_traits,Is_dropout,Is_causal>(Flash_fwd_params &, cudaStream_t) [with Kernel_traits=Flash_fwd_kernel_traits<192, 64, 64, 4, false, false, cutlass::half_t, Flash_kernel_traits<192, 64, 64, 4, cutlass::half_t>>, Is_dropout=true, Is_causal=true]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(183): here
            instantiation of "void run_mha_fwd_hdim192<T>(Flash_fwd_params &, cudaStream_t) [with T=cutlass::half_t]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_hdim192_fp16_sm80.cu(26): here

/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_kernel.h(276): warning: variable "tQrQ" was declared but never referenced
          detected during:
            instantiation of "void flash::compute_attn_1rowblock<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params,Is_Q_split_K>(const Params &, int, int, int) [with Kernel_traits=Flash_fwd_kernel_traits<192, 64, 64, 4, false, false, cutlass::half_t, Flash_kernel_traits<192, 64, 64, 4, cutlass::half_t>>, Is_dropout=true, Is_causal=true, Is_even_N=true, Is_even_K=true, Return_softmax=true, Params=Flash_fwd_params, Is_Q_split_K=false]" 
(589): here
            instantiation of "void flash::compute_attn<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &) [with Kernel_traits=Flash_fwd_kernel_traits<192, 64, 64, 4, false, false, cutlass::half_t, Flash_kernel_traits<192, 64, 64, 4, cutlass::half_t>>, Is_dropout=true, Is_causal=true, Is_even_N=true, Is_even_K=true, Return_softmax=true, Params=Flash_fwd_params]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(15): here
            instantiation of "void flash_fwd_kernel<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax>(Flash_fwd_params) [with Kernel_traits=Flash_fwd_kernel_traits<192, 64, 64, 4, false, false, cutlass::half_t, Flash_kernel_traits<192, 64, 64, 4, cutlass::half_t>>, Is_dropout=true, Is_causal=true, Is_even_N=true, Is_even_K=true, Return_softmax=true]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(34): here
            instantiation of "void run_flash_fwd<Kernel_traits,Is_dropout,Is_causal>(Flash_fwd_params &, cudaStream_t) [with Kernel_traits=Flash_fwd_kernel_traits<192, 64, 64, 4, false, false, cutlass::half_t, Flash_kernel_traits<192, 64, 64, 4, cutlass::half_t>>, Is_dropout=true, Is_causal=true]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(183): here
            instantiation of "void run_mha_fwd_hdim192<T>(Flash_fwd_params &, cudaStream_t) [with T=cutlass::half_t]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_hdim192_fp16_sm80.cu(26): here

/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_kernel.h(140): warning: variable "MMA_M" was declared but never referenced
          detected during:
            instantiation of "void flash::compute_attn_1rowblock<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params,Is_Q_split_K>(const Params &, int, int, int) [with Kernel_traits=Flash_fwd_kernel_traits<192, 64, 64, 4, false, false, cutlass::half_t, Flash_kernel_traits<192, 64, 64, 4, cutlass::half_t>>, Is_dropout=true, Is_causal=true, Is_even_N=true, Is_even_K=true, Return_softmax=false, Params=Flash_fwd_params, Is_Q_split_K=false]" 
(589): here
            instantiation of "void flash::compute_attn<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &) [with Kernel_traits=Flash_fwd_kernel_traits<192, 64, 64, 4, false, false, cutlass::half_t, Flash_kernel_traits<192, 64, 64, 4, cutlass::half_t>>, Is_dropout=true, Is_causal=true, Is_even_N=true, Is_even_K=true, Return_softmax=false, Params=Flash_fwd_params]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(15): here
            instantiation of "void flash_fwd_kernel<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax>(Flash_fwd_params) [with Kernel_traits=Flash_fwd_kernel_traits<192, 64, 64, 4, false, false, cutlass::half_t, Flash_kernel_traits<192, 64, 64, 4, cutlass::half_t>>, Is_dropout=true, Is_causal=true, Is_even_N=true, Is_even_K=true, Return_softmax=false]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(34): here
            instantiation of "void run_flash_fwd<Kernel_traits,Is_dropout,Is_causal>(Flash_fwd_params &, cudaStream_t) [with Kernel_traits=Flash_fwd_kernel_traits<192, 64, 64, 4, false, false, cutlass::half_t, Flash_kernel_traits<192, 64, 64, 4, cutlass::half_t>>, Is_dropout=true, Is_causal=true]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(183): here
            instantiation of "void run_mha_fwd_hdim192<T>(Flash_fwd_params &, cudaStream_t) [with T=cutlass::half_t]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_hdim192_fp16_sm80.cu(26): here

/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_kernel.h(276): warning: variable "tQrQ" was declared but never referenced
          detected during:
            instantiation of "void flash::compute_attn_1rowblock<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params,Is_Q_split_K>(const Params &, int, int, int) [with Kernel_traits=Flash_fwd_kernel_traits<192, 64, 64, 4, false, false, cutlass::half_t, Flash_kernel_traits<192, 64, 64, 4, cutlass::half_t>>, Is_dropout=true, Is_causal=true, Is_even_N=true, Is_even_K=true, Return_softmax=false, Params=Flash_fwd_params, Is_Q_split_K=false]" 
(589): here
            instantiation of "void flash::compute_attn<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &) [with Kernel_traits=Flash_fwd_kernel_traits<192, 64, 64, 4, false, false, cutlass::half_t, Flash_kernel_traits<192, 64, 64, 4, cutlass::half_t>>, Is_dropout=true, Is_causal=true, Is_even_N=true, Is_even_K=true, Return_softmax=false, Params=Flash_fwd_params]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(15): here
            instantiation of "void flash_fwd_kernel<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax>(Flash_fwd_params) [with Kernel_traits=Flash_fwd_kernel_traits<192, 64, 64, 4, false, false, cutlass::half_t, Flash_kernel_traits<192, 64, 64, 4, cutlass::half_t>>, Is_dropout=true, Is_causal=true, Is_even_N=true, Is_even_K=true, Return_softmax=false]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(34): here
            instantiation of "void run_flash_fwd<Kernel_traits,Is_dropout,Is_causal>(Flash_fwd_params &, cudaStream_t) [with Kernel_traits=Flash_fwd_kernel_traits<192, 64, 64, 4, false, false, cutlass::half_t, Flash_kernel_traits<192, 64, 64, 4, cutlass::half_t>>, Is_dropout=true, Is_causal=true]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(183): here
            instantiation of "void run_mha_fwd_hdim192<T>(Flash_fwd_params &, cudaStream_t) [with T=cutlass::half_t]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_hdim192_fp16_sm80.cu(26): here

/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_kernel.h(140): warning: variable "MMA_M" was declared but never referenced
          detected during:
            instantiation of "void flash::compute_attn_1rowblock<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params,Is_Q_split_K>(const Params &, int, int, int) [with Kernel_traits=Flash_fwd_kernel_traits<192, 64, 64, 4, false, false, cutlass::half_t, Flash_kernel_traits<192, 64, 64, 4, cutlass::half_t>>, Is_dropout=true, Is_causal=true, Is_even_N=true, Is_even_K=false, Return_softmax=true, Params=Flash_fwd_params, Is_Q_split_K=false]" 
(589): here
            instantiation of "void flash::compute_attn<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &) [with Kernel_traits=Flash_fwd_kernel_traits<192, 64, 64, 4, false, false, cutlass::half_t, Flash_kernel_traits<192, 64, 64, 4, cutlass::half_t>>, Is_dropout=true, Is_causal=true, Is_even_N=true, Is_even_K=false, Return_softmax=true, Params=Flash_fwd_params]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(15): here
            instantiation of "void flash_fwd_kernel<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax>(Flash_fwd_params) [with Kernel_traits=Flash_fwd_kernel_traits<192, 64, 64, 4, false, false, cutlass::half_t, Flash_kernel_traits<192, 64, 64, 4, cutlass::half_t>>, Is_dropout=true, Is_causal=true, Is_even_N=true, Is_even_K=false, Return_softmax=true]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(34): here
            instantiation of "void run_flash_fwd<Kernel_traits,Is_dropout,Is_causal>(Flash_fwd_params &, cudaStream_t) [with Kernel_traits=Flash_fwd_kernel_traits<192, 64, 64, 4, false, false, cutlass::half_t, Flash_kernel_traits<192, 64, 64, 4, cutlass::half_t>>, Is_dropout=true, Is_causal=true]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(183): here
            instantiation of "void run_mha_fwd_hdim192<T>(Flash_fwd_params &, cudaStream_t) [with T=cutlass::half_t]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_hdim192_fp16_sm80.cu(26): here

/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_kernel.h(276): warning: variable "tQrQ" was declared but never referenced
          detected during:
            instantiation of "void flash::compute_attn_1rowblock<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params,Is_Q_split_K>(const Params &, int, int, int) [with Kernel_traits=Flash_fwd_kernel_traits<192, 64, 64, 4, false, false, cutlass::half_t, Flash_kernel_traits<192, 64, 64, 4, cutlass::half_t>>, Is_dropout=true, Is_causal=true, Is_even_N=true, Is_even_K=false, Return_softmax=true, Params=Flash_fwd_params, Is_Q_split_K=false]" 
(589): here
            instantiation of "void flash::compute_attn<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &) [with Kernel_traits=Flash_fwd_kernel_traits<192, 64, 64, 4, false, false, cutlass::half_t, Flash_kernel_traits<192, 64, 64, 4, cutlass::half_t>>, Is_dropout=true, Is_causal=true, Is_even_N=true, Is_even_K=false, Return_softmax=true, Params=Flash_fwd_params]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(15): here
            instantiation of "void flash_fwd_kernel<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax>(Flash_fwd_params) [with Kernel_traits=Flash_fwd_kernel_traits<192, 64, 64, 4, false, false, cutlass::half_t, Flash_kernel_traits<192, 64, 64, 4, cutlass::half_t>>, Is_dropout=true, Is_causal=true, Is_even_N=true, Is_even_K=false, Return_softmax=true]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(34): here
            instantiation of "void run_flash_fwd<Kernel_traits,Is_dropout,Is_causal>(Flash_fwd_params &, cudaStream_t) [with Kernel_traits=Flash_fwd_kernel_traits<192, 64, 64, 4, false, false, cutlass::half_t, Flash_kernel_traits<192, 64, 64, 4, cutlass::half_t>>, Is_dropout=true, Is_causal=true]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(183): here
            instantiation of "void run_mha_fwd_hdim192<T>(Flash_fwd_params &, cudaStream_t) [with T=cutlass::half_t]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_hdim192_fp16_sm80.cu(26): here

/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_kernel.h(140): warning: variable "MMA_M" was declared but never referenced
          detected during:
            instantiation of "void flash::compute_attn_1rowblock<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params,Is_Q_split_K>(const Params &, int, int, int) [with Kernel_traits=Flash_fwd_kernel_traits<192, 64, 64, 4, false, false, cutlass::half_t, Flash_kernel_traits<192, 64, 64, 4, cutlass::half_t>>, Is_dropout=true, Is_causal=true, Is_even_N=true, Is_even_K=false, Return_softmax=false, Params=Flash_fwd_params, Is_Q_split_K=false]" 
(589): here
            instantiation of "void flash::compute_attn<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &) [with Kernel_traits=Flash_fwd_kernel_traits<192, 64, 64, 4, false, false, cutlass::half_t, Flash_kernel_traits<192, 64, 64, 4, cutlass::half_t>>, Is_dropout=true, Is_causal=true, Is_even_N=true, Is_even_K=false, Return_softmax=false, Params=Flash_fwd_params]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(15): here
            instantiation of "void flash_fwd_kernel<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax>(Flash_fwd_params) [with Kernel_traits=Flash_fwd_kernel_traits<192, 64, 64, 4, false, false, cutlass::half_t, Flash_kernel_traits<192, 64, 64, 4, cutlass::half_t>>, Is_dropout=true, Is_causal=true, Is_even_N=true, Is_even_K=false, Return_softmax=false]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(34): here
            instantiation of "void run_flash_fwd<Kernel_traits,Is_dropout,Is_causal>(Flash_fwd_params &, cudaStream_t) [with Kernel_traits=Flash_fwd_kernel_traits<192, 64, 64, 4, false, false, cutlass::half_t, Flash_kernel_traits<192, 64, 64, 4, cutlass::half_t>>, Is_dropout=true, Is_causal=true]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(183): here
            instantiation of "void run_mha_fwd_hdim192<T>(Flash_fwd_params &, cudaStream_t) [with T=cutlass::half_t]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_hdim192_fp16_sm80.cu(26): here

/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_kernel.h(276): warning: variable "tQrQ" was declared but never referenced
          detected during:
            instantiation of "void flash::compute_attn_1rowblock<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params,Is_Q_split_K>(const Params &, int, int, int) [with Kernel_traits=Flash_fwd_kernel_traits<192, 64, 64, 4, false, false, cutlass::half_t, Flash_kernel_traits<192, 64, 64, 4, cutlass::half_t>>, Is_dropout=true, Is_causal=true, Is_even_N=true, Is_even_K=false, Return_softmax=false, Params=Flash_fwd_params, Is_Q_split_K=false]" 
(589): here
            instantiation of "void flash::compute_attn<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &) [with Kernel_traits=Flash_fwd_kernel_traits<192, 64, 64, 4, false, false, cutlass::half_t, Flash_kernel_traits<192, 64, 64, 4, cutlass::half_t>>, Is_dropout=true, Is_causal=true, Is_even_N=true, Is_even_K=false, Return_softmax=false, Params=Flash_fwd_params]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(15): here
            instantiation of "void flash_fwd_kernel<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax>(Flash_fwd_params) [with Kernel_traits=Flash_fwd_kernel_traits<192, 64, 64, 4, false, false, cutlass::half_t, Flash_kernel_traits<192, 64, 64, 4, cutlass::half_t>>, Is_dropout=true, Is_causal=true, Is_even_N=true, Is_even_K=false, Return_softmax=false]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(34): here
            instantiation of "void run_flash_fwd<Kernel_traits,Is_dropout,Is_causal>(Flash_fwd_params &, cudaStream_t) [with Kernel_traits=Flash_fwd_kernel_traits<192, 64, 64, 4, false, false, cutlass::half_t, Flash_kernel_traits<192, 64, 64, 4, cutlass::half_t>>, Is_dropout=true, Is_causal=true]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(183): here
            instantiation of "void run_mha_fwd_hdim192<T>(Flash_fwd_params &, cudaStream_t) [with T=cutlass::half_t]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_hdim192_fp16_sm80.cu(26): here

/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_kernel.h(140): warning: variable "MMA_M" was declared but never referenced
          detected during:
            instantiation of "void flash::compute_attn_1rowblock<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params,Is_Q_split_K>(const Params &, int, int, int) [with Kernel_traits=Flash_fwd_kernel_traits<192, 64, 64, 4, false, false, cutlass::half_t, Flash_kernel_traits<192, 64, 64, 4, cutlass::half_t>>, Is_dropout=true, Is_causal=true, Is_even_N=false, Is_even_K=true, Return_softmax=true, Params=Flash_fwd_params, Is_Q_split_K=false]" 
(589): here
            instantiation of "void flash::compute_attn<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &) [with Kernel_traits=Flash_fwd_kernel_traits<192, 64, 64, 4, false, false, cutlass::half_t, Flash_kernel_traits<192, 64, 64, 4, cutlass::half_t>>, Is_dropout=true, Is_causal=true, Is_even_N=false, Is_even_K=true, Return_softmax=true, Params=Flash_fwd_params]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(15): here
            instantiation of "void flash_fwd_kernel<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax>(Flash_fwd_params) [with Kernel_traits=Flash_fwd_kernel_traits<192, 64, 64, 4, false, false, cutlass::half_t, Flash_kernel_traits<192, 64, 64, 4, cutlass::half_t>>, Is_dropout=true, Is_causal=true, Is_even_N=false, Is_even_K=true, Return_softmax=true]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(34): here
            instantiation of "void run_flash_fwd<Kernel_traits,Is_dropout,Is_causal>(Flash_fwd_params &, cudaStream_t) [with Kernel_traits=Flash_fwd_kernel_traits<192, 64, 64, 4, false, false, cutlass::half_t, Flash_kernel_traits<192, 64, 64, 4, cutlass::half_t>>, Is_dropout=true, Is_causal=true]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(183): here
            instantiation of "void run_mha_fwd_hdim192<T>(Flash_fwd_params &, cudaStream_t) [with T=cutlass::half_t]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_hdim192_fp16_sm80.cu(26): here

/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_kernel.h(276): warning: variable "tQrQ" was declared but never referenced
          detected during:
            instantiation of "void flash::compute_attn_1rowblock<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params,Is_Q_split_K>(const Params &, int, int, int) [with Kernel_traits=Flash_fwd_kernel_traits<192, 64, 64, 4, false, false, cutlass::half_t, Flash_kernel_traits<192, 64, 64, 4, cutlass::half_t>>, Is_dropout=true, Is_causal=true, Is_even_N=false, Is_even_K=true, Return_softmax=true, Params=Flash_fwd_params, Is_Q_split_K=false]" 
(589): here
            instantiation of "void flash::compute_attn<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &) [with Kernel_traits=Flash_fwd_kernel_traits<192, 64, 64, 4, false, false, cutlass::half_t, Flash_kernel_traits<192, 64, 64, 4, cutlass::half_t>>, Is_dropout=true, Is_causal=true, Is_even_N=false, Is_even_K=true, Return_softmax=true, Params=Flash_fwd_params]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(15): here
            instantiation of "void flash_fwd_kernel<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax>(Flash_fwd_params) [with Kernel_traits=Flash_fwd_kernel_traits<192, 64, 64, 4, false, false, cutlass::half_t, Flash_kernel_traits<192, 64, 64, 4, cutlass::half_t>>, Is_dropout=true, Is_causal=true, Is_even_N=false, Is_even_K=true, Return_softmax=true]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(34): here
            instantiation of "void run_flash_fwd<Kernel_traits,Is_dropout,Is_causal>(Flash_fwd_params &, cudaStream_t) [with Kernel_traits=Flash_fwd_kernel_traits<192, 64, 64, 4, false, false, cutlass::half_t, Flash_kernel_traits<192, 64, 64, 4, cutlass::half_t>>, Is_dropout=true, Is_causal=true]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(183): here
            instantiation of "void run_mha_fwd_hdim192<T>(Flash_fwd_params &, cudaStream_t) [with T=cutlass::half_t]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_hdim192_fp16_sm80.cu(26): here

/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_kernel.h(140): warning: variable "MMA_M" was declared but never referenced
          detected during:
            instantiation of "void flash::compute_attn_1rowblock<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params,Is_Q_split_K>(const Params &, int, int, int) [with Kernel_traits=Flash_fwd_kernel_traits<192, 64, 64, 4, false, false, cutlass::half_t, Flash_kernel_traits<192, 64, 64, 4, cutlass::half_t>>, Is_dropout=true, Is_causal=true, Is_even_N=false, Is_even_K=true, Return_softmax=false, Params=Flash_fwd_params, Is_Q_split_K=false]" 
(589): here
            instantiation of "void flash::compute_attn<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &) [with Kernel_traits=Flash_fwd_kernel_traits<192, 64, 64, 4, false, false, cutlass::half_t, Flash_kernel_traits<192, 64, 64, 4, cutlass::half_t>>, Is_dropout=true, Is_causal=true, Is_even_N=false, Is_even_K=true, Return_softmax=false, Params=Flash_fwd_params]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(15): here
            instantiation of "void flash_fwd_kernel<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax>(Flash_fwd_params) [with Kernel_traits=Flash_fwd_kernel_traits<192, 64, 64, 4, false, false, cutlass::half_t, Flash_kernel_traits<192, 64, 64, 4, cutlass::half_t>>, Is_dropout=true, Is_causal=true, Is_even_N=false, Is_even_K=true, Return_softmax=false]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(34): here
            instantiation of "void run_flash_fwd<Kernel_traits,Is_dropout,Is_causal>(Flash_fwd_params &, cudaStream_t) [with Kernel_traits=Flash_fwd_kernel_traits<192, 64, 64, 4, false, false, cutlass::half_t, Flash_kernel_traits<192, 64, 64, 4, cutlass::half_t>>, Is_dropout=true, Is_causal=true]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(183): here
            instantiation of "void run_mha_fwd_hdim192<T>(Flash_fwd_params &, cudaStream_t) [with T=cutlass::half_t]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_hdim192_fp16_sm80.cu(26): here

/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_kernel.h(276): warning: variable "tQrQ" was declared but never referenced
          detected during:
            instantiation of "void flash::compute_attn_1rowblock<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params,Is_Q_split_K>(const Params &, int, int, int) [with Kernel_traits=Flash_fwd_kernel_traits<192, 64, 64, 4, false, false, cutlass::half_t, Flash_kernel_traits<192, 64, 64, 4, cutlass::half_t>>, Is_dropout=true, Is_causal=true, Is_even_N=false, Is_even_K=true, Return_softmax=false, Params=Flash_fwd_params, Is_Q_split_K=false]" 
(589): here
            instantiation of "void flash::compute_attn<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &) [with Kernel_traits=Flash_fwd_kernel_traits<192, 64, 64, 4, false, false, cutlass::half_t, Flash_kernel_traits<192, 64, 64, 4, cutlass::half_t>>, Is_dropout=true, Is_causal=true, Is_even_N=false, Is_even_K=true, Return_softmax=false, Params=Flash_fwd_params]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(15): here
            instantiation of "void flash_fwd_kernel<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax>(Flash_fwd_params) [with Kernel_traits=Flash_fwd_kernel_traits<192, 64, 64, 4, false, false, cutlass::half_t, Flash_kernel_traits<192, 64, 64, 4, cutlass::half_t>>, Is_dropout=true, Is_causal=true, Is_even_N=false, Is_even_K=true, Return_softmax=false]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(34): here
            instantiation of "void run_flash_fwd<Kernel_traits,Is_dropout,Is_causal>(Flash_fwd_params &, cudaStream_t) [with Kernel_traits=Flash_fwd_kernel_traits<192, 64, 64, 4, false, false, cutlass::half_t, Flash_kernel_traits<192, 64, 64, 4, cutlass::half_t>>, Is_dropout=true, Is_causal=true]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(183): here
            instantiation of "void run_mha_fwd_hdim192<T>(Flash_fwd_params &, cudaStream_t) [with T=cutlass::half_t]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_hdim192_fp16_sm80.cu(26): here

/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_kernel.h(140): warning: variable "MMA_M" was declared but never referenced
          detected during:
            instantiation of "void flash::compute_attn_1rowblock<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params,Is_Q_split_K>(const Params &, int, int, int) [with Kernel_traits=Flash_fwd_kernel_traits<192, 64, 64, 4, false, false, cutlass::half_t, Flash_kernel_traits<192, 64, 64, 4, cutlass::half_t>>, Is_dropout=true, Is_causal=true, Is_even_N=false, Is_even_K=false, Return_softmax=true, Params=Flash_fwd_params, Is_Q_split_K=false]" 
(589): here
            instantiation of "void flash::compute_attn<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &) [with Kernel_traits=Flash_fwd_kernel_traits<192, 64, 64, 4, false, false, cutlass::half_t, Flash_kernel_traits<192, 64, 64, 4, cutlass::half_t>>, Is_dropout=true, Is_causal=true, Is_even_N=false, Is_even_K=false, Return_softmax=true, Params=Flash_fwd_params]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(15): here
            instantiation of "void flash_fwd_kernel<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax>(Flash_fwd_params) [with Kernel_traits=Flash_fwd_kernel_traits<192, 64, 64, 4, false, false, cutlass::half_t, Flash_kernel_traits<192, 64, 64, 4, cutlass::half_t>>, Is_dropout=true, Is_causal=true, Is_even_N=false, Is_even_K=false, Return_softmax=true]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(34): here
            instantiation of "void run_flash_fwd<Kernel_traits,Is_dropout,Is_causal>(Flash_fwd_params &, cudaStream_t) [with Kernel_traits=Flash_fwd_kernel_traits<192, 64, 64, 4, false, false, cutlass::half_t, Flash_kernel_traits<192, 64, 64, 4, cutlass::half_t>>, Is_dropout=true, Is_causal=true]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(183): here
            instantiation of "void run_mha_fwd_hdim192<T>(Flash_fwd_params &, cudaStream_t) [with T=cutlass::half_t]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_hdim192_fp16_sm80.cu(26): here

/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_kernel.h(276): warning: variable "tQrQ" was declared but never referenced
          detected during:
            instantiation of "void flash::compute_attn_1rowblock<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params,Is_Q_split_K>(const Params &, int, int, int) [with Kernel_traits=Flash_fwd_kernel_traits<192, 64, 64, 4, false, false, cutlass::half_t, Flash_kernel_traits<192, 64, 64, 4, cutlass::half_t>>, Is_dropout=true, Is_causal=true, Is_even_N=false, Is_even_K=false, Return_softmax=true, Params=Flash_fwd_params, Is_Q_split_K=false]" 
(589): here
            instantiation of "void flash::compute_attn<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &) [with Kernel_traits=Flash_fwd_kernel_traits<192, 64, 64, 4, false, false, cutlass::half_t, Flash_kernel_traits<192, 64, 64, 4, cutlass::half_t>>, Is_dropout=true, Is_causal=true, Is_even_N=false, Is_even_K=false, Return_softmax=true, Params=Flash_fwd_params]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(15): here
            instantiation of "void flash_fwd_kernel<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax>(Flash_fwd_params) [with Kernel_traits=Flash_fwd_kernel_traits<192, 64, 64, 4, false, false, cutlass::half_t, Flash_kernel_traits<192, 64, 64, 4, cutlass::half_t>>, Is_dropout=true, Is_causal=true, Is_even_N=false, Is_even_K=false, Return_softmax=true]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(34): here
            instantiation of "void run_flash_fwd<Kernel_traits,Is_dropout,Is_causal>(Flash_fwd_params &, cudaStream_t) [with Kernel_traits=Flash_fwd_kernel_traits<192, 64, 64, 4, false, false, cutlass::half_t, Flash_kernel_traits<192, 64, 64, 4, cutlass::half_t>>, Is_dropout=true, Is_causal=true]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(183): here
            instantiation of "void run_mha_fwd_hdim192<T>(Flash_fwd_params &, cudaStream_t) [with T=cutlass::half_t]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_hdim192_fp16_sm80.cu(26): here

/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_kernel.h(140): warning: variable "MMA_M" was declared but never referenced
          detected during:
            instantiation of "void flash::compute_attn_1rowblock<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params,Is_Q_split_K>(const Params &, int, int, int) [with Kernel_traits=Flash_fwd_kernel_traits<192, 64, 64, 4, false, false, cutlass::half_t, Flash_kernel_traits<192, 64, 64, 4, cutlass::half_t>>, Is_dropout=true, Is_causal=true, Is_even_N=false, Is_even_K=false, Return_softmax=false, Params=Flash_fwd_params, Is_Q_split_K=false]" 
(589): here
            instantiation of "void flash::compute_attn<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &) [with Kernel_traits=Flash_fwd_kernel_traits<192, 64, 64, 4, false, false, cutlass::half_t, Flash_kernel_traits<192, 64, 64, 4, cutlass::half_t>>, Is_dropout=true, Is_causal=true, Is_even_N=false, Is_even_K=false, Return_softmax=false, Params=Flash_fwd_params]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(15): here
            instantiation of "void flash_fwd_kernel<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax>(Flash_fwd_params) [with Kernel_traits=Flash_fwd_kernel_traits<192, 64, 64, 4, false, false, cutlass::half_t, Flash_kernel_traits<192, 64, 64, 4, cutlass::half_t>>, Is_dropout=true, Is_causal=true, Is_even_N=false, Is_even_K=false, Return_softmax=false]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(34): here
            instantiation of "void run_flash_fwd<Kernel_traits,Is_dropout,Is_causal>(Flash_fwd_params &, cudaStream_t) [with Kernel_traits=Flash_fwd_kernel_traits<192, 64, 64, 4, false, false, cutlass::half_t, Flash_kernel_traits<192, 64, 64, 4, cutlass::half_t>>, Is_dropout=true, Is_causal=true]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(183): here
            instantiation of "void run_mha_fwd_hdim192<T>(Flash_fwd_params &, cudaStream_t) [with T=cutlass::half_t]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_hdim192_fp16_sm80.cu(26): here

/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_kernel.h(276): warning: variable "tQrQ" was declared but never referenced
          detected during:
            instantiation of "void flash::compute_attn_1rowblock<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params,Is_Q_split_K>(const Params &, int, int, int) [with Kernel_traits=Flash_fwd_kernel_traits<192, 64, 64, 4, false, false, cutlass::half_t, Flash_kernel_traits<192, 64, 64, 4, cutlass::half_t>>, Is_dropout=true, Is_causal=true, Is_even_N=false, Is_even_K=false, Return_softmax=false, Params=Flash_fwd_params, Is_Q_split_K=false]" 
(589): here
            instantiation of "void flash::compute_attn<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &) [with Kernel_traits=Flash_fwd_kernel_traits<192, 64, 64, 4, false, false, cutlass::half_t, Flash_kernel_traits<192, 64, 64, 4, cutlass::half_t>>, Is_dropout=true, Is_causal=true, Is_even_N=false, Is_even_K=false, Return_softmax=false, Params=Flash_fwd_params]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(15): here
            instantiation of "void flash_fwd_kernel<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax>(Flash_fwd_params) [with Kernel_traits=Flash_fwd_kernel_traits<192, 64, 64, 4, false, false, cutlass::half_t, Flash_kernel_traits<192, 64, 64, 4, cutlass::half_t>>, Is_dropout=true, Is_causal=true, Is_even_N=false, Is_even_K=false, Return_softmax=false]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(34): here
            instantiation of "void run_flash_fwd<Kernel_traits,Is_dropout,Is_causal>(Flash_fwd_params &, cudaStream_t) [with Kernel_traits=Flash_fwd_kernel_traits<192, 64, 64, 4, false, false, cutlass::half_t, Flash_kernel_traits<192, 64, 64, 4, cutlass::half_t>>, Is_dropout=true, Is_causal=true]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(183): here
            instantiation of "void run_mha_fwd_hdim192<T>(Flash_fwd_params &, cudaStream_t) [with T=cutlass::half_t]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_hdim192_fp16_sm80.cu(26): here

/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_kernel.h(140): warning: variable "MMA_M" was declared but never referenced
          detected during:
            instantiation of "void flash::compute_attn_1rowblock<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params,Is_Q_split_K>(const Params &, int, int, int) [with Kernel_traits=Flash_fwd_kernel_traits<192, 64, 64, 4, false, false, cutlass::half_t, Flash_kernel_traits<192, 64, 64, 4, cutlass::half_t>>, Is_dropout=true, Is_causal=false, Is_even_N=true, Is_even_K=true, Return_softmax=true, Params=Flash_fwd_params, Is_Q_split_K=false]" 
(589): here
            instantiation of "void flash::compute_attn<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &) [with Kernel_traits=Flash_fwd_kernel_traits<192, 64, 64, 4, false, false, cutlass::half_t, Flash_kernel_traits<192, 64, 64, 4, cutlass::half_t>>, Is_dropout=true, Is_causal=false, Is_even_N=true, Is_even_K=true, Return_softmax=true, Params=Flash_fwd_params]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(15): here
            instantiation of "void flash_fwd_kernel<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax>(Flash_fwd_params) [with Kernel_traits=Flash_fwd_kernel_traits<192, 64, 64, 4, false, false, cutlass::half_t, Flash_kernel_traits<192, 64, 64, 4, cutlass::half_t>>, Is_dropout=true, Is_causal=false, Is_even_N=true, Is_even_K=true, Return_softmax=true]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(34): here
            instantiation of "void run_flash_fwd<Kernel_traits,Is_dropout,Is_causal>(Flash_fwd_params &, cudaStream_t) [with Kernel_traits=Flash_fwd_kernel_traits<192, 64, 64, 4, false, false, cutlass::half_t, Flash_kernel_traits<192, 64, 64, 4, cutlass::half_t>>, Is_dropout=true, Is_causal=false]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(183): here
            instantiation of "void run_mha_fwd_hdim192<T>(Flash_fwd_params &, cudaStream_t) [with T=cutlass::half_t]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_hdim192_fp16_sm80.cu(26): here

/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_kernel.h(276): warning: variable "tQrQ" was declared but never referenced
          detected during:
            instantiation of "void flash::compute_attn_1rowblock<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params,Is_Q_split_K>(const Params &, int, int, int) [with Kernel_traits=Flash_fwd_kernel_traits<192, 64, 64, 4, false, false, cutlass::half_t, Flash_kernel_traits<192, 64, 64, 4, cutlass::half_t>>, Is_dropout=true, Is_causal=false, Is_even_N=true, Is_even_K=true, Return_softmax=true, Params=Flash_fwd_params, Is_Q_split_K=false]" 
(589): here
            instantiation of "void flash::compute_attn<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &) [with Kernel_traits=Flash_fwd_kernel_traits<192, 64, 64, 4, false, false, cutlass::half_t, Flash_kernel_traits<192, 64, 64, 4, cutlass::half_t>>, Is_dropout=true, Is_causal=false, Is_even_N=true, Is_even_K=true, Return_softmax=true, Params=Flash_fwd_params]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(15): here
            instantiation of "void flash_fwd_kernel<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax>(Flash_fwd_params) [with Kernel_traits=Flash_fwd_kernel_traits<192, 64, 64, 4, false, false, cutlass::half_t, Flash_kernel_traits<192, 64, 64, 4, cutlass::half_t>>, Is_dropout=true, Is_causal=false, Is_even_N=true, Is_even_K=true, Return_softmax=true]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(34): here
            instantiation of "void run_flash_fwd<Kernel_traits,Is_dropout,Is_causal>(Flash_fwd_params &, cudaStream_t) [with Kernel_traits=Flash_fwd_kernel_traits<192, 64, 64, 4, false, false, cutlass::half_t, Flash_kernel_traits<192, 64, 64, 4, cutlass::half_t>>, Is_dropout=true, Is_causal=false]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(183): here
            instantiation of "void run_mha_fwd_hdim192<T>(Flash_fwd_params &, cudaStream_t) [with T=cutlass::half_t]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_hdim192_fp16_sm80.cu(26): here

/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_kernel.h(140): warning: variable "MMA_M" was declared but never referenced
          detected during:
            instantiation of "void flash::compute_attn_1rowblock<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params,Is_Q_split_K>(const Params &, int, int, int) [with Kernel_traits=Flash_fwd_kernel_traits<192, 64, 64, 4, false, false, cutlass::half_t, Flash_kernel_traits<192, 64, 64, 4, cutlass::half_t>>, Is_dropout=true, Is_causal=false, Is_even_N=true, Is_even_K=true, Return_softmax=false, Params=Flash_fwd_params, Is_Q_split_K=false]" 
(589): here
            instantiation of "void flash::compute_attn<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &) [with Kernel_traits=Flash_fwd_kernel_traits<192, 64, 64, 4, false, false, cutlass::half_t, Flash_kernel_traits<192, 64, 64, 4, cutlass::half_t>>, Is_dropout=true, Is_causal=false, Is_even_N=true, Is_even_K=true, Return_softmax=false, Params=Flash_fwd_params]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(15): here
            instantiation of "void flash_fwd_kernel<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax>(Flash_fwd_params) [with Kernel_traits=Flash_fwd_kernel_traits<192, 64, 64, 4, false, false, cutlass::half_t, Flash_kernel_traits<192, 64, 64, 4, cutlass::half_t>>, Is_dropout=true, Is_causal=false, Is_even_N=true, Is_even_K=true, Return_softmax=false]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(34): here
            instantiation of "void run_flash_fwd<Kernel_traits,Is_dropout,Is_causal>(Flash_fwd_params &, cudaStream_t) [with Kernel_traits=Flash_fwd_kernel_traits<192, 64, 64, 4, false, false, cutlass::half_t, Flash_kernel_traits<192, 64, 64, 4, cutlass::half_t>>, Is_dropout=true, Is_causal=false]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(183): here
            instantiation of "void run_mha_fwd_hdim192<T>(Flash_fwd_params &, cudaStream_t) [with T=cutlass::half_t]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_hdim192_fp16_sm80.cu(26): here

/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_kernel.h(276): warning: variable "tQrQ" was declared but never referenced
          detected during:
            instantiation of "void flash::compute_attn_1rowblock<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params,Is_Q_split_K>(const Params &, int, int, int) [with Kernel_traits=Flash_fwd_kernel_traits<192, 64, 64, 4, false, false, cutlass::half_t, Flash_kernel_traits<192, 64, 64, 4, cutlass::half_t>>, Is_dropout=true, Is_causal=false, Is_even_N=true, Is_even_K=true, Return_softmax=false, Params=Flash_fwd_params, Is_Q_split_K=false]" 
(589): here
            instantiation of "void flash::compute_attn<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &) [with Kernel_traits=Flash_fwd_kernel_traits<192, 64, 64, 4, false, false, cutlass::half_t, Flash_kernel_traits<192, 64, 64, 4, cutlass::half_t>>, Is_dropout=true, Is_causal=false, Is_even_N=true, Is_even_K=true, Return_softmax=false, Params=Flash_fwd_params]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(15): here
            instantiation of "void flash_fwd_kernel<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax>(Flash_fwd_params) [with Kernel_traits=Flash_fwd_kernel_traits<192, 64, 64, 4, false, false, cutlass::half_t, Flash_kernel_traits<192, 64, 64, 4, cutlass::half_t>>, Is_dropout=true, Is_causal=false, Is_even_N=true, Is_even_K=true, Return_softmax=false]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(34): here
            instantiation of "void run_flash_fwd<Kernel_traits,Is_dropout,Is_causal>(Flash_fwd_params &, cudaStream_t) [with Kernel_traits=Flash_fwd_kernel_traits<192, 64, 64, 4, false, false, cutlass::half_t, Flash_kernel_traits<192, 64, 64, 4, cutlass::half_t>>, Is_dropout=true, Is_causal=false]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(183): here
            instantiation of "void run_mha_fwd_hdim192<T>(Flash_fwd_params &, cudaStream_t) [with T=cutlass::half_t]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_hdim192_fp16_sm80.cu(26): here

/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_kernel.h(140): warning: variable "MMA_M" was declared but never referenced
          detected during:
            instantiation of "void flash::compute_attn_1rowblock<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params,Is_Q_split_K>(const Params &, int, int, int) [with Kernel_traits=Flash_fwd_kernel_traits<192, 64, 64, 4, false, false, cutlass::half_t, Flash_kernel_traits<192, 64, 64, 4, cutlass::half_t>>, Is_dropout=true, Is_causal=false, Is_even_N=true, Is_even_K=false, Return_softmax=true, Params=Flash_fwd_params, Is_Q_split_K=false]" 
(589): here
            instantiation of "void flash::compute_attn<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &) [with Kernel_traits=Flash_fwd_kernel_traits<192, 64, 64, 4, false, false, cutlass::half_t, Flash_kernel_traits<192, 64, 64, 4, cutlass::half_t>>, Is_dropout=true, Is_causal=false, Is_even_N=true, Is_even_K=false, Return_softmax=true, Params=Flash_fwd_params]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(15): here
            instantiation of "void flash_fwd_kernel<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax>(Flash_fwd_params) [with Kernel_traits=Flash_fwd_kernel_traits<192, 64, 64, 4, false, false, cutlass::half_t, Flash_kernel_traits<192, 64, 64, 4, cutlass::half_t>>, Is_dropout=true, Is_causal=false, Is_even_N=true, Is_even_K=false, Return_softmax=true]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(34): here
            instantiation of "void run_flash_fwd<Kernel_traits,Is_dropout,Is_causal>(Flash_fwd_params &, cudaStream_t) [with Kernel_traits=Flash_fwd_kernel_traits<192, 64, 64, 4, false, false, cutlass::half_t, Flash_kernel_traits<192, 64, 64, 4, cutlass::half_t>>, Is_dropout=true, Is_causal=false]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(183): here
            instantiation of "void run_mha_fwd_hdim192<T>(Flash_fwd_params &, cudaStream_t) [with T=cutlass::half_t]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_hdim192_fp16_sm80.cu(26): here

/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_kernel.h(276): warning: variable "tQrQ" was declared but never referenced
          detected during:
            instantiation of "void flash::compute_attn_1rowblock<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params,Is_Q_split_K>(const Params &, int, int, int) [with Kernel_traits=Flash_fwd_kernel_traits<192, 64, 64, 4, false, false, cutlass::half_t, Flash_kernel_traits<192, 64, 64, 4, cutlass::half_t>>, Is_dropout=true, Is_causal=false, Is_even_N=true, Is_even_K=false, Return_softmax=true, Params=Flash_fwd_params, Is_Q_split_K=false]" 
(589): here
            instantiation of "void flash::compute_attn<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &) [with Kernel_traits=Flash_fwd_kernel_traits<192, 64, 64, 4, false, false, cutlass::half_t, Flash_kernel_traits<192, 64, 64, 4, cutlass::half_t>>, Is_dropout=true, Is_causal=false, Is_even_N=true, Is_even_K=false, Return_softmax=true, Params=Flash_fwd_params]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(15): here
            instantiation of "void flash_fwd_kernel<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax>(Flash_fwd_params) [with Kernel_traits=Flash_fwd_kernel_traits<192, 64, 64, 4, false, false, cutlass::half_t, Flash_kernel_traits<192, 64, 64, 4, cutlass::half_t>>, Is_dropout=true, Is_causal=false, Is_even_N=true, Is_even_K=false, Return_softmax=true]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(34): here
            instantiation of "void run_flash_fwd<Kernel_traits,Is_dropout,Is_causal>(Flash_fwd_params &, cudaStream_t) [with Kernel_traits=Flash_fwd_kernel_traits<192, 64, 64, 4, false, false, cutlass::half_t, Flash_kernel_traits<192, 64, 64, 4, cutlass::half_t>>, Is_dropout=true, Is_causal=false]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(183): here
            instantiation of "void run_mha_fwd_hdim192<T>(Flash_fwd_params &, cudaStream_t) [with T=cutlass::half_t]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_hdim192_fp16_sm80.cu(26): here

/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_kernel.h(140): warning: variable "MMA_M" was declared but never referenced
          detected during:
            instantiation of "void flash::compute_attn_1rowblock<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params,Is_Q_split_K>(const Params &, int, int, int) [with Kernel_traits=Flash_fwd_kernel_traits<192, 64, 64, 4, false, false, cutlass::half_t, Flash_kernel_traits<192, 64, 64, 4, cutlass::half_t>>, Is_dropout=true, Is_causal=false, Is_even_N=true, Is_even_K=false, Return_softmax=false, Params=Flash_fwd_params, Is_Q_split_K=false]" 
(589): here
            instantiation of "void flash::compute_attn<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &) [with Kernel_traits=Flash_fwd_kernel_traits<192, 64, 64, 4, false, false, cutlass::half_t, Flash_kernel_traits<192, 64, 64, 4, cutlass::half_t>>, Is_dropout=true, Is_causal=false, Is_even_N=true, Is_even_K=false, Return_softmax=false, Params=Flash_fwd_params]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(15): here
            instantiation of "void flash_fwd_kernel<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax>(Flash_fwd_params) [with Kernel_traits=Flash_fwd_kernel_traits<192, 64, 64, 4, false, false, cutlass::half_t, Flash_kernel_traits<192, 64, 64, 4, cutlass::half_t>>, Is_dropout=true, Is_causal=false, Is_even_N=true, Is_even_K=false, Return_softmax=false]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(34): here
            instantiation of "void run_flash_fwd<Kernel_traits,Is_dropout,Is_causal>(Flash_fwd_params &, cudaStream_t) [with Kernel_traits=Flash_fwd_kernel_traits<192, 64, 64, 4, false, false, cutlass::half_t, Flash_kernel_traits<192, 64, 64, 4, cutlass::half_t>>, Is_dropout=true, Is_causal=false]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(183): here
            instantiation of "void run_mha_fwd_hdim192<T>(Flash_fwd_params &, cudaStream_t) [with T=cutlass::half_t]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_hdim192_fp16_sm80.cu(26): here

/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_kernel.h(276): warning: variable "tQrQ" was declared but never referenced
          detected during:
            instantiation of "void flash::compute_attn_1rowblock<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params,Is_Q_split_K>(const Params &, int, int, int) [with Kernel_traits=Flash_fwd_kernel_traits<192, 64, 64, 4, false, false, cutlass::half_t, Flash_kernel_traits<192, 64, 64, 4, cutlass::half_t>>, Is_dropout=true, Is_causal=false, Is_even_N=true, Is_even_K=false, Return_softmax=false, Params=Flash_fwd_params, Is_Q_split_K=false]" 
(589): here
            instantiation of "void flash::compute_attn<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &) [with Kernel_traits=Flash_fwd_kernel_traits<192, 64, 64, 4, false, false, cutlass::half_t, Flash_kernel_traits<192, 64, 64, 4, cutlass::half_t>>, Is_dropout=true, Is_causal=false, Is_even_N=true, Is_even_K=false, Return_softmax=false, Params=Flash_fwd_params]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(15): here
            instantiation of "void flash_fwd_kernel<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax>(Flash_fwd_params) [with Kernel_traits=Flash_fwd_kernel_traits<192, 64, 64, 4, false, false, cutlass::half_t, Flash_kernel_traits<192, 64, 64, 4, cutlass::half_t>>, Is_dropout=true, Is_causal=false, Is_even_N=true, Is_even_K=false, Return_softmax=false]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(34): here
            instantiation of "void run_flash_fwd<Kernel_traits,Is_dropout,Is_causal>(Flash_fwd_params &, cudaStream_t) [with Kernel_traits=Flash_fwd_kernel_traits<192, 64, 64, 4, false, false, cutlass::half_t, Flash_kernel_traits<192, 64, 64, 4, cutlass::half_t>>, Is_dropout=true, Is_causal=false]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(183): here
            instantiation of "void run_mha_fwd_hdim192<T>(Flash_fwd_params &, cudaStream_t) [with T=cutlass::half_t]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_hdim192_fp16_sm80.cu(26): here

/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_kernel.h(140): warning: variable "MMA_M" was declared but never referenced
          detected during:
            instantiation of "void flash::compute_attn_1rowblock<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params,Is_Q_split_K>(const Params &, int, int, int) [with Kernel_traits=Flash_fwd_kernel_traits<192, 64, 64, 4, false, false, cutlass::half_t, Flash_kernel_traits<192, 64, 64, 4, cutlass::half_t>>, Is_dropout=true, Is_causal=false, Is_even_N=false, Is_even_K=true, Return_softmax=true, Params=Flash_fwd_params, Is_Q_split_K=false]" 
(589): here
            instantiation of "void flash::compute_attn<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &) [with Kernel_traits=Flash_fwd_kernel_traits<192, 64, 64, 4, false, false, cutlass::half_t, Flash_kernel_traits<192, 64, 64, 4, cutlass::half_t>>, Is_dropout=true, Is_causal=false, Is_even_N=false, Is_even_K=true, Return_softmax=true, Params=Flash_fwd_params]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(15): here
            instantiation of "void flash_fwd_kernel<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax>(Flash_fwd_params) [with Kernel_traits=Flash_fwd_kernel_traits<192, 64, 64, 4, false, false, cutlass::half_t, Flash_kernel_traits<192, 64, 64, 4, cutlass::half_t>>, Is_dropout=true, Is_causal=false, Is_even_N=false, Is_even_K=true, Return_softmax=true]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(34): here
            instantiation of "void run_flash_fwd<Kernel_traits,Is_dropout,Is_causal>(Flash_fwd_params &, cudaStream_t) [with Kernel_traits=Flash_fwd_kernel_traits<192, 64, 64, 4, false, false, cutlass::half_t, Flash_kernel_traits<192, 64, 64, 4, cutlass::half_t>>, Is_dropout=true, Is_causal=false]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(183): here
            instantiation of "void run_mha_fwd_hdim192<T>(Flash_fwd_params &, cudaStream_t) [with T=cutlass::half_t]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_hdim192_fp16_sm80.cu(26): here

/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_kernel.h(276): warning: variable "tQrQ" was declared but never referenced
          detected during:
            instantiation of "void flash::compute_attn_1rowblock<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params,Is_Q_split_K>(const Params &, int, int, int) [with Kernel_traits=Flash_fwd_kernel_traits<192, 64, 64, 4, false, false, cutlass::half_t, Flash_kernel_traits<192, 64, 64, 4, cutlass::half_t>>, Is_dropout=true, Is_causal=false, Is_even_N=false, Is_even_K=true, Return_softmax=true, Params=Flash_fwd_params, Is_Q_split_K=false]" 
(589): here
            instantiation of "void flash::compute_attn<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &) [with Kernel_traits=Flash_fwd_kernel_traits<192, 64, 64, 4, false, false, cutlass::half_t, Flash_kernel_traits<192, 64, 64, 4, cutlass::half_t>>, Is_dropout=true, Is_causal=false, Is_even_N=false, Is_even_K=true, Return_softmax=true, Params=Flash_fwd_params]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(15): here
            instantiation of "void flash_fwd_kernel<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax>(Flash_fwd_params) [with Kernel_traits=Flash_fwd_kernel_traits<192, 64, 64, 4, false, false, cutlass::half_t, Flash_kernel_traits<192, 64, 64, 4, cutlass::half_t>>, Is_dropout=true, Is_causal=false, Is_even_N=false, Is_even_K=true, Return_softmax=true]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(34): here
            instantiation of "void run_flash_fwd<Kernel_traits,Is_dropout,Is_causal>(Flash_fwd_params &, cudaStream_t) [with Kernel_traits=Flash_fwd_kernel_traits<192, 64, 64, 4, false, false, cutlass::half_t, Flash_kernel_traits<192, 64, 64, 4, cutlass::half_t>>, Is_dropout=true, Is_causal=false]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(183): here
            instantiation of "void run_mha_fwd_hdim192<T>(Flash_fwd_params &, cudaStream_t) [with T=cutlass::half_t]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_hdim192_fp16_sm80.cu(26): here

/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_kernel.h(140): warning: variable "MMA_M" was declared but never referenced
          detected during:
            instantiation of "void flash::compute_attn_1rowblock<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params,Is_Q_split_K>(const Params &, int, int, int) [with Kernel_traits=Flash_fwd_kernel_traits<192, 64, 64, 4, false, false, cutlass::half_t, Flash_kernel_traits<192, 64, 64, 4, cutlass::half_t>>, Is_dropout=true, Is_causal=false, Is_even_N=false, Is_even_K=true, Return_softmax=false, Params=Flash_fwd_params, Is_Q_split_K=false]" 
(589): here
            instantiation of "void flash::compute_attn<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &) [with Kernel_traits=Flash_fwd_kernel_traits<192, 64, 64, 4, false, false, cutlass::half_t, Flash_kernel_traits<192, 64, 64, 4, cutlass::half_t>>, Is_dropout=true, Is_causal=false, Is_even_N=false, Is_even_K=true, Return_softmax=false, Params=Flash_fwd_params]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(15): here
            instantiation of "void flash_fwd_kernel<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax>(Flash_fwd_params) [with Kernel_traits=Flash_fwd_kernel_traits<192, 64, 64, 4, false, false, cutlass::half_t, Flash_kernel_traits<192, 64, 64, 4, cutlass::half_t>>, Is_dropout=true, Is_causal=false, Is_even_N=false, Is_even_K=true, Return_softmax=false]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(34): here
            instantiation of "void run_flash_fwd<Kernel_traits,Is_dropout,Is_causal>(Flash_fwd_params &, cudaStream_t) [with Kernel_traits=Flash_fwd_kernel_traits<192, 64, 64, 4, false, false, cutlass::half_t, Flash_kernel_traits<192, 64, 64, 4, cutlass::half_t>>, Is_dropout=true, Is_causal=false]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(183): here
            instantiation of "void run_mha_fwd_hdim192<T>(Flash_fwd_params &, cudaStream_t) [with T=cutlass::half_t]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_hdim192_fp16_sm80.cu(26): here

/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_kernel.h(276): warning: variable "tQrQ" was declared but never referenced
          detected during:
            instantiation of "void flash::compute_attn_1rowblock<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params,Is_Q_split_K>(const Params &, int, int, int) [with Kernel_traits=Flash_fwd_kernel_traits<192, 64, 64, 4, false, false, cutlass::half_t, Flash_kernel_traits<192, 64, 64, 4, cutlass::half_t>>, Is_dropout=true, Is_causal=false, Is_even_N=false, Is_even_K=true, Return_softmax=false, Params=Flash_fwd_params, Is_Q_split_K=false]" 
(589): here
            instantiation of "void flash::compute_attn<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &) [with Kernel_traits=Flash_fwd_kernel_traits<192, 64, 64, 4, false, false, cutlass::half_t, Flash_kernel_traits<192, 64, 64, 4, cutlass::half_t>>, Is_dropout=true, Is_causal=false, Is_even_N=false, Is_even_K=true, Return_softmax=false, Params=Flash_fwd_params]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(15): here
            instantiation of "void flash_fwd_kernel<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax>(Flash_fwd_params) [with Kernel_traits=Flash_fwd_kernel_traits<192, 64, 64, 4, false, false, cutlass::half_t, Flash_kernel_traits<192, 64, 64, 4, cutlass::half_t>>, Is_dropout=true, Is_causal=false, Is_even_N=false, Is_even_K=true, Return_softmax=false]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(34): here
            instantiation of "void run_flash_fwd<Kernel_traits,Is_dropout,Is_causal>(Flash_fwd_params &, cudaStream_t) [with Kernel_traits=Flash_fwd_kernel_traits<192, 64, 64, 4, false, false, cutlass::half_t, Flash_kernel_traits<192, 64, 64, 4, cutlass::half_t>>, Is_dropout=true, Is_causal=false]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(183): here
            instantiation of "void run_mha_fwd_hdim192<T>(Flash_fwd_params &, cudaStream_t) [with T=cutlass::half_t]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_hdim192_fp16_sm80.cu(26): here

/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_kernel.h(140): warning: variable "MMA_M" was declared but never referenced
          detected during:
            instantiation of "void flash::compute_attn_1rowblock<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params,Is_Q_split_K>(const Params &, int, int, int) [with Kernel_traits=Flash_fwd_kernel_traits<192, 64, 64, 4, false, false, cutlass::half_t, Flash_kernel_traits<192, 64, 64, 4, cutlass::half_t>>, Is_dropout=true, Is_causal=false, Is_even_N=false, Is_even_K=false, Return_softmax=true, Params=Flash_fwd_params, Is_Q_split_K=false]" 
(589): here
            instantiation of "void flash::compute_attn<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &) [with Kernel_traits=Flash_fwd_kernel_traits<192, 64, 64, 4, false, false, cutlass::half_t, Flash_kernel_traits<192, 64, 64, 4, cutlass::half_t>>, Is_dropout=true, Is_causal=false, Is_even_N=false, Is_even_K=false, Return_softmax=true, Params=Flash_fwd_params]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(15): here
            instantiation of "void flash_fwd_kernel<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax>(Flash_fwd_params) [with Kernel_traits=Flash_fwd_kernel_traits<192, 64, 64, 4, false, false, cutlass::half_t, Flash_kernel_traits<192, 64, 64, 4, cutlass::half_t>>, Is_dropout=true, Is_causal=false, Is_even_N=false, Is_even_K=false, Return_softmax=true]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(34): here
            instantiation of "void run_flash_fwd<Kernel_traits,Is_dropout,Is_causal>(Flash_fwd_params &, cudaStream_t) [with Kernel_traits=Flash_fwd_kernel_traits<192, 64, 64, 4, false, false, cutlass::half_t, Flash_kernel_traits<192, 64, 64, 4, cutlass::half_t>>, Is_dropout=true, Is_causal=false]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(183): here
            instantiation of "void run_mha_fwd_hdim192<T>(Flash_fwd_params &, cudaStream_t) [with T=cutlass::half_t]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_hdim192_fp16_sm80.cu(26): here

/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_kernel.h(276): warning: variable "tQrQ" was declared but never referenced
          detected during:
            instantiation of "void flash::compute_attn_1rowblock<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params,Is_Q_split_K>(const Params &, int, int, int) [with Kernel_traits=Flash_fwd_kernel_traits<192, 64, 64, 4, false, false, cutlass::half_t, Flash_kernel_traits<192, 64, 64, 4, cutlass::half_t>>, Is_dropout=true, Is_causal=false, Is_even_N=false, Is_even_K=false, Return_softmax=true, Params=Flash_fwd_params, Is_Q_split_K=false]" 
(589): here
            instantiation of "void flash::compute_attn<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &) [with Kernel_traits=Flash_fwd_kernel_traits<192, 64, 64, 4, false, false, cutlass::half_t, Flash_kernel_traits<192, 64, 64, 4, cutlass::half_t>>, Is_dropout=true, Is_causal=false, Is_even_N=false, Is_even_K=false, Return_softmax=true, Params=Flash_fwd_params]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(15): here
            instantiation of "void flash_fwd_kernel<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax>(Flash_fwd_params) [with Kernel_traits=Flash_fwd_kernel_traits<192, 64, 64, 4, false, false, cutlass::half_t, Flash_kernel_traits<192, 64, 64, 4, cutlass::half_t>>, Is_dropout=true, Is_causal=false, Is_even_N=false, Is_even_K=false, Return_softmax=true]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(34): here
            instantiation of "void run_flash_fwd<Kernel_traits,Is_dropout,Is_causal>(Flash_fwd_params &, cudaStream_t) [with Kernel_traits=Flash_fwd_kernel_traits<192, 64, 64, 4, false, false, cutlass::half_t, Flash_kernel_traits<192, 64, 64, 4, cutlass::half_t>>, Is_dropout=true, Is_causal=false]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(183): here
            instantiation of "void run_mha_fwd_hdim192<T>(Flash_fwd_params &, cudaStream_t) [with T=cutlass::half_t]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_hdim192_fp16_sm80.cu(26): here

/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_kernel.h(140): warning: variable "MMA_M" was declared but never referenced
          detected during:
            instantiation of "void flash::compute_attn_1rowblock<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params,Is_Q_split_K>(const Params &, int, int, int) [with Kernel_traits=Flash_fwd_kernel_traits<192, 64, 64, 4, false, false, cutlass::half_t, Flash_kernel_traits<192, 64, 64, 4, cutlass::half_t>>, Is_dropout=true, Is_causal=false, Is_even_N=false, Is_even_K=false, Return_softmax=false, Params=Flash_fwd_params, Is_Q_split_K=false]" 
(589): here
            instantiation of "void flash::compute_attn<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &) [with Kernel_traits=Flash_fwd_kernel_traits<192, 64, 64, 4, false, false, cutlass::half_t, Flash_kernel_traits<192, 64, 64, 4, cutlass::half_t>>, Is_dropout=true, Is_causal=false, Is_even_N=false, Is_even_K=false, Return_softmax=false, Params=Flash_fwd_params]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(15): here
            instantiation of "void flash_fwd_kernel<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax>(Flash_fwd_params) [with Kernel_traits=Flash_fwd_kernel_traits<192, 64, 64, 4, false, false, cutlass::half_t, Flash_kernel_traits<192, 64, 64, 4, cutlass::half_t>>, Is_dropout=true, Is_causal=false, Is_even_N=false, Is_even_K=false, Return_softmax=false]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(34): here
            instantiation of "void run_flash_fwd<Kernel_traits,Is_dropout,Is_causal>(Flash_fwd_params &, cudaStream_t) [with Kernel_traits=Flash_fwd_kernel_traits<192, 64, 64, 4, false, false, cutlass::half_t, Flash_kernel_traits<192, 64, 64, 4, cutlass::half_t>>, Is_dropout=true, Is_causal=false]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(183): here
            instantiation of "void run_mha_fwd_hdim192<T>(Flash_fwd_params &, cudaStream_t) [with T=cutlass::half_t]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_hdim192_fp16_sm80.cu(26): here

/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_kernel.h(276): warning: variable "tQrQ" was declared but never referenced
          detected during:
            instantiation of "void flash::compute_attn_1rowblock<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params,Is_Q_split_K>(const Params &, int, int, int) [with Kernel_traits=Flash_fwd_kernel_traits<192, 64, 64, 4, false, false, cutlass::half_t, Flash_kernel_traits<192, 64, 64, 4, cutlass::half_t>>, Is_dropout=true, Is_causal=false, Is_even_N=false, Is_even_K=false, Return_softmax=false, Params=Flash_fwd_params, Is_Q_split_K=false]" 
(589): here
            instantiation of "void flash::compute_attn<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &) [with Kernel_traits=Flash_fwd_kernel_traits<192, 64, 64, 4, false, false, cutlass::half_t, Flash_kernel_traits<192, 64, 64, 4, cutlass::half_t>>, Is_dropout=true, Is_causal=false, Is_even_N=false, Is_even_K=false, Return_softmax=false, Params=Flash_fwd_params]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(15): here
            instantiation of "void flash_fwd_kernel<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax>(Flash_fwd_params) [with Kernel_traits=Flash_fwd_kernel_traits<192, 64, 64, 4, false, false, cutlass::half_t, Flash_kernel_traits<192, 64, 64, 4, cutlass::half_t>>, Is_dropout=true, Is_causal=false, Is_even_N=false, Is_even_K=false, Return_softmax=false]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(34): here
            instantiation of "void run_flash_fwd<Kernel_traits,Is_dropout,Is_causal>(Flash_fwd_params &, cudaStream_t) [with Kernel_traits=Flash_fwd_kernel_traits<192, 64, 64, 4, false, false, cutlass::half_t, Flash_kernel_traits<192, 64, 64, 4, cutlass::half_t>>, Is_dropout=true, Is_causal=false]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(183): here
            instantiation of "void run_mha_fwd_hdim192<T>(Flash_fwd_params &, cudaStream_t) [with T=cutlass::half_t]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_hdim192_fp16_sm80.cu(26): here

/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_kernel.h(140): warning: variable "MMA_M" was declared but never referenced
          detected during:
            instantiation of "void flash::compute_attn_1rowblock<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params,Is_Q_split_K>(const Params &, int, int, int) [with Kernel_traits=Flash_fwd_kernel_traits<192, 128, 64, 8, false, false, cutlass::half_t, Flash_kernel_traits<192, 128, 64, 8, cutlass::half_t>>, Is_dropout=false, Is_causal=true, Is_even_N=true, Is_even_K=true, Return_softmax=false, Params=Flash_fwd_params, Is_Q_split_K=false]" 
(589): here
            instantiation of "void flash::compute_attn<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &) [with Kernel_traits=Flash_fwd_kernel_traits<192, 128, 64, 8, false, false, cutlass::half_t, Flash_kernel_traits<192, 128, 64, 8, cutlass::half_t>>, Is_dropout=false, Is_causal=true, Is_even_N=true, Is_even_K=true, Return_softmax=false, Params=Flash_fwd_params]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(15): here
            instantiation of "void flash_fwd_kernel<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax>(Flash_fwd_params) [with Kernel_traits=Flash_fwd_kernel_traits<192, 128, 64, 8, false, false, cutlass::half_t, Flash_kernel_traits<192, 128, 64, 8, cutlass::half_t>>, Is_dropout=false, Is_causal=true, Is_even_N=true, Is_even_K=true, Return_softmax=false]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(34): here
            instantiation of "void run_flash_fwd<Kernel_traits,Is_dropout,Is_causal>(Flash_fwd_params &, cudaStream_t) [with Kernel_traits=Flash_fwd_kernel_traits<192, 128, 64, 8, false, false, cutlass::half_t, Flash_kernel_traits<192, 128, 64, 8, cutlass::half_t>>, Is_dropout=false, Is_causal=true]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(183): here
            instantiation of "void run_mha_fwd_hdim192<T>(Flash_fwd_params &, cudaStream_t) [with T=cutlass::half_t]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_hdim192_fp16_sm80.cu(26): here

/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_kernel.h(276): warning: variable "tQrQ" was declared but never referenced
          detected during:
            instantiation of "void flash::compute_attn_1rowblock<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params,Is_Q_split_K>(const Params &, int, int, int) [with Kernel_traits=Flash_fwd_kernel_traits<192, 128, 64, 8, false, false, cutlass::half_t, Flash_kernel_traits<192, 128, 64, 8, cutlass::half_t>>, Is_dropout=false, Is_causal=true, Is_even_N=true, Is_even_K=true, Return_softmax=false, Params=Flash_fwd_params, Is_Q_split_K=false]" 
(589): here
            instantiation of "void flash::compute_attn<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &) [with Kernel_traits=Flash_fwd_kernel_traits<192, 128, 64, 8, false, false, cutlass::half_t, Flash_kernel_traits<192, 128, 64, 8, cutlass::half_t>>, Is_dropout=false, Is_causal=true, Is_even_N=true, Is_even_K=true, Return_softmax=false, Params=Flash_fwd_params]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(15): here
            instantiation of "void flash_fwd_kernel<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax>(Flash_fwd_params) [with Kernel_traits=Flash_fwd_kernel_traits<192, 128, 64, 8, false, false, cutlass::half_t, Flash_kernel_traits<192, 128, 64, 8, cutlass::half_t>>, Is_dropout=false, Is_causal=true, Is_even_N=true, Is_even_K=true, Return_softmax=false]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(34): here
            instantiation of "void run_flash_fwd<Kernel_traits,Is_dropout,Is_causal>(Flash_fwd_params &, cudaStream_t) [with Kernel_traits=Flash_fwd_kernel_traits<192, 128, 64, 8, false, false, cutlass::half_t, Flash_kernel_traits<192, 128, 64, 8, cutlass::half_t>>, Is_dropout=false, Is_causal=true]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(183): here
            instantiation of "void run_mha_fwd_hdim192<T>(Flash_fwd_params &, cudaStream_t) [with T=cutlass::half_t]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_hdim192_fp16_sm80.cu(26): here

/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_kernel.h(140): warning: variable "MMA_M" was declared but never referenced
          detected during:
            instantiation of "void flash::compute_attn_1rowblock<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params,Is_Q_split_K>(const Params &, int, int, int) [with Kernel_traits=Flash_fwd_kernel_traits<192, 128, 64, 8, false, false, cutlass::half_t, Flash_kernel_traits<192, 128, 64, 8, cutlass::half_t>>, Is_dropout=false, Is_causal=true, Is_even_N=true, Is_even_K=false, Return_softmax=false, Params=Flash_fwd_params, Is_Q_split_K=false]" 
(589): here
            instantiation of "void flash::compute_attn<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &) [with Kernel_traits=Flash_fwd_kernel_traits<192, 128, 64, 8, false, false, cutlass::half_t, Flash_kernel_traits<192, 128, 64, 8, cutlass::half_t>>, Is_dropout=false, Is_causal=true, Is_even_N=true, Is_even_K=false, Return_softmax=false, Params=Flash_fwd_params]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(15): here
            instantiation of "void flash_fwd_kernel<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax>(Flash_fwd_params) [with Kernel_traits=Flash_fwd_kernel_traits<192, 128, 64, 8, false, false, cutlass::half_t, Flash_kernel_traits<192, 128, 64, 8, cutlass::half_t>>, Is_dropout=false, Is_causal=true, Is_even_N=true, Is_even_K=false, Return_softmax=false]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(34): here
            instantiation of "void run_flash_fwd<Kernel_traits,Is_dropout,Is_causal>(Flash_fwd_params &, cudaStream_t) [with Kernel_traits=Flash_fwd_kernel_traits<192, 128, 64, 8, false, false, cutlass::half_t, Flash_kernel_traits<192, 128, 64, 8, cutlass::half_t>>, Is_dropout=false, Is_causal=true]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(183): here
            instantiation of "void run_mha_fwd_hdim192<T>(Flash_fwd_params &, cudaStream_t) [with T=cutlass::half_t]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_hdim192_fp16_sm80.cu(26): here

/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_kernel.h(276): warning: variable "tQrQ" was declared but never referenced
          detected during:
            instantiation of "void flash::compute_attn_1rowblock<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params,Is_Q_split_K>(const Params &, int, int, int) [with Kernel_traits=Flash_fwd_kernel_traits<192, 128, 64, 8, false, false, cutlass::half_t, Flash_kernel_traits<192, 128, 64, 8, cutlass::half_t>>, Is_dropout=false, Is_causal=true, Is_even_N=true, Is_even_K=false, Return_softmax=false, Params=Flash_fwd_params, Is_Q_split_K=false]" 
(589): here
            instantiation of "void flash::compute_attn<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &) [with Kernel_traits=Flash_fwd_kernel_traits<192, 128, 64, 8, false, false, cutlass::half_t, Flash_kernel_traits<192, 128, 64, 8, cutlass::half_t>>, Is_dropout=false, Is_causal=true, Is_even_N=true, Is_even_K=false, Return_softmax=false, Params=Flash_fwd_params]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(15): here
            instantiation of "void flash_fwd_kernel<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax>(Flash_fwd_params) [with Kernel_traits=Flash_fwd_kernel_traits<192, 128, 64, 8, false, false, cutlass::half_t, Flash_kernel_traits<192, 128, 64, 8, cutlass::half_t>>, Is_dropout=false, Is_causal=true, Is_even_N=true, Is_even_K=false, Return_softmax=false]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(34): here
            instantiation of "void run_flash_fwd<Kernel_traits,Is_dropout,Is_causal>(Flash_fwd_params &, cudaStream_t) [with Kernel_traits=Flash_fwd_kernel_traits<192, 128, 64, 8, false, false, cutlass::half_t, Flash_kernel_traits<192, 128, 64, 8, cutlass::half_t>>, Is_dropout=false, Is_causal=true]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(183): here
            instantiation of "void run_mha_fwd_hdim192<T>(Flash_fwd_params &, cudaStream_t) [with T=cutlass::half_t]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_hdim192_fp16_sm80.cu(26): here

/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_kernel.h(140): warning: variable "MMA_M" was declared but never referenced
          detected during:
            instantiation of "void flash::compute_attn_1rowblock<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params,Is_Q_split_K>(const Params &, int, int, int) [with Kernel_traits=Flash_fwd_kernel_traits<192, 128, 64, 8, false, false, cutlass::half_t, Flash_kernel_traits<192, 128, 64, 8, cutlass::half_t>>, Is_dropout=false, Is_causal=true, Is_even_N=false, Is_even_K=true, Return_softmax=false, Params=Flash_fwd_params, Is_Q_split_K=false]" 
(589): here
            instantiation of "void flash::compute_attn<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &) [with Kernel_traits=Flash_fwd_kernel_traits<192, 128, 64, 8, false, false, cutlass::half_t, Flash_kernel_traits<192, 128, 64, 8, cutlass::half_t>>, Is_dropout=false, Is_causal=true, Is_even_N=false, Is_even_K=true, Return_softmax=false, Params=Flash_fwd_params]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(15): here
            instantiation of "void flash_fwd_kernel<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax>(Flash_fwd_params) [with Kernel_traits=Flash_fwd_kernel_traits<192, 128, 64, 8, false, false, cutlass::half_t, Flash_kernel_traits<192, 128, 64, 8, cutlass::half_t>>, Is_dropout=false, Is_causal=true, Is_even_N=false, Is_even_K=true, Return_softmax=false]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(34): here
            instantiation of "void run_flash_fwd<Kernel_traits,Is_dropout,Is_causal>(Flash_fwd_params &, cudaStream_t) [with Kernel_traits=Flash_fwd_kernel_traits<192, 128, 64, 8, false, false, cutlass::half_t, Flash_kernel_traits<192, 128, 64, 8, cutlass::half_t>>, Is_dropout=false, Is_causal=true]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(183): here
            instantiation of "void run_mha_fwd_hdim192<T>(Flash_fwd_params &, cudaStream_t) [with T=cutlass::half_t]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_hdim192_fp16_sm80.cu(26): here

/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_kernel.h(276): warning: variable "tQrQ" was declared but never referenced
          detected during:
            instantiation of "void flash::compute_attn_1rowblock<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params,Is_Q_split_K>(const Params &, int, int, int) [with Kernel_traits=Flash_fwd_kernel_traits<192, 128, 64, 8, false, false, cutlass::half_t, Flash_kernel_traits<192, 128, 64, 8, cutlass::half_t>>, Is_dropout=false, Is_causal=true, Is_even_N=false, Is_even_K=true, Return_softmax=false, Params=Flash_fwd_params, Is_Q_split_K=false]" 
(589): here
            instantiation of "void flash::compute_attn<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &) [with Kernel_traits=Flash_fwd_kernel_traits<192, 128, 64, 8, false, false, cutlass::half_t, Flash_kernel_traits<192, 128, 64, 8, cutlass::half_t>>, Is_dropout=false, Is_causal=true, Is_even_N=false, Is_even_K=true, Return_softmax=false, Params=Flash_fwd_params]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(15): here
            instantiation of "void flash_fwd_kernel<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax>(Flash_fwd_params) [with Kernel_traits=Flash_fwd_kernel_traits<192, 128, 64, 8, false, false, cutlass::half_t, Flash_kernel_traits<192, 128, 64, 8, cutlass::half_t>>, Is_dropout=false, Is_causal=true, Is_even_N=false, Is_even_K=true, Return_softmax=false]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(34): here
            instantiation of "void run_flash_fwd<Kernel_traits,Is_dropout,Is_causal>(Flash_fwd_params &, cudaStream_t) [with Kernel_traits=Flash_fwd_kernel_traits<192, 128, 64, 8, false, false, cutlass::half_t, Flash_kernel_traits<192, 128, 64, 8, cutlass::half_t>>, Is_dropout=false, Is_causal=true]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(183): here
            instantiation of "void run_mha_fwd_hdim192<T>(Flash_fwd_params &, cudaStream_t) [with T=cutlass::half_t]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_hdim192_fp16_sm80.cu(26): here

/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_kernel.h(140): warning: variable "MMA_M" was declared but never referenced
          detected during:
            instantiation of "void flash::compute_attn_1rowblock<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params,Is_Q_split_K>(const Params &, int, int, int) [with Kernel_traits=Flash_fwd_kernel_traits<192, 128, 64, 8, false, false, cutlass::half_t, Flash_kernel_traits<192, 128, 64, 8, cutlass::half_t>>, Is_dropout=false, Is_causal=true, Is_even_N=false, Is_even_K=false, Return_softmax=false, Params=Flash_fwd_params, Is_Q_split_K=false]" 
(589): here
            instantiation of "void flash::compute_attn<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &) [with Kernel_traits=Flash_fwd_kernel_traits<192, 128, 64, 8, false, false, cutlass::half_t, Flash_kernel_traits<192, 128, 64, 8, cutlass::half_t>>, Is_dropout=false, Is_causal=true, Is_even_N=false, Is_even_K=false, Return_softmax=false, Params=Flash_fwd_params]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(15): here
            instantiation of "void flash_fwd_kernel<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax>(Flash_fwd_params) [with Kernel_traits=Flash_fwd_kernel_traits<192, 128, 64, 8, false, false, cutlass::half_t, Flash_kernel_traits<192, 128, 64, 8, cutlass::half_t>>, Is_dropout=false, Is_causal=true, Is_even_N=false, Is_even_K=false, Return_softmax=false]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(34): here
            instantiation of "void run_flash_fwd<Kernel_traits,Is_dropout,Is_causal>(Flash_fwd_params &, cudaStream_t) [with Kernel_traits=Flash_fwd_kernel_traits<192, 128, 64, 8, false, false, cutlass::half_t, Flash_kernel_traits<192, 128, 64, 8, cutlass::half_t>>, Is_dropout=false, Is_causal=true]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(183): here
            instantiation of "void run_mha_fwd_hdim192<T>(Flash_fwd_params &, cudaStream_t) [with T=cutlass::half_t]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_hdim192_fp16_sm80.cu(26): here

/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_kernel.h(276): warning: variable "tQrQ" was declared but never referenced
          detected during:
            instantiation of "void flash::compute_attn_1rowblock<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params,Is_Q_split_K>(const Params &, int, int, int) [with Kernel_traits=Flash_fwd_kernel_traits<192, 128, 64, 8, false, false, cutlass::half_t, Flash_kernel_traits<192, 128, 64, 8, cutlass::half_t>>, Is_dropout=false, Is_causal=true, Is_even_N=false, Is_even_K=false, Return_softmax=false, Params=Flash_fwd_params, Is_Q_split_K=false]" 
(589): here
            instantiation of "void flash::compute_attn<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &) [with Kernel_traits=Flash_fwd_kernel_traits<192, 128, 64, 8, false, false, cutlass::half_t, Flash_kernel_traits<192, 128, 64, 8, cutlass::half_t>>, Is_dropout=false, Is_causal=true, Is_even_N=false, Is_even_K=false, Return_softmax=false, Params=Flash_fwd_params]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(15): here
            instantiation of "void flash_fwd_kernel<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax>(Flash_fwd_params) [with Kernel_traits=Flash_fwd_kernel_traits<192, 128, 64, 8, false, false, cutlass::half_t, Flash_kernel_traits<192, 128, 64, 8, cutlass::half_t>>, Is_dropout=false, Is_causal=true, Is_even_N=false, Is_even_K=false, Return_softmax=false]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(34): here
            instantiation of "void run_flash_fwd<Kernel_traits,Is_dropout,Is_causal>(Flash_fwd_params &, cudaStream_t) [with Kernel_traits=Flash_fwd_kernel_traits<192, 128, 64, 8, false, false, cutlass::half_t, Flash_kernel_traits<192, 128, 64, 8, cutlass::half_t>>, Is_dropout=false, Is_causal=true]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(183): here
            instantiation of "void run_mha_fwd_hdim192<T>(Flash_fwd_params &, cudaStream_t) [with T=cutlass::half_t]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_hdim192_fp16_sm80.cu(26): here

/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_kernel.h(140): warning: variable "MMA_M" was declared but never referenced
          detected during:
            instantiation of "void flash::compute_attn_1rowblock<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params,Is_Q_split_K>(const Params &, int, int, int) [with Kernel_traits=Flash_fwd_kernel_traits<192, 128, 64, 8, false, false, cutlass::half_t, Flash_kernel_traits<192, 128, 64, 8, cutlass::half_t>>, Is_dropout=false, Is_causal=false, Is_even_N=true, Is_even_K=true, Return_softmax=false, Params=Flash_fwd_params, Is_Q_split_K=false]" 
(589): here
            instantiation of "void flash::compute_attn<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &) [with Kernel_traits=Flash_fwd_kernel_traits<192, 128, 64, 8, false, false, cutlass::half_t, Flash_kernel_traits<192, 128, 64, 8, cutlass::half_t>>, Is_dropout=false, Is_causal=false, Is_even_N=true, Is_even_K=true, Return_softmax=false, Params=Flash_fwd_params]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(15): here
            instantiation of "void flash_fwd_kernel<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax>(Flash_fwd_params) [with Kernel_traits=Flash_fwd_kernel_traits<192, 128, 64, 8, false, false, cutlass::half_t, Flash_kernel_traits<192, 128, 64, 8, cutlass::half_t>>, Is_dropout=false, Is_causal=false, Is_even_N=true, Is_even_K=true, Return_softmax=false]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(34): here
            instantiation of "void run_flash_fwd<Kernel_traits,Is_dropout,Is_causal>(Flash_fwd_params &, cudaStream_t) [with Kernel_traits=Flash_fwd_kernel_traits<192, 128, 64, 8, false, false, cutlass::half_t, Flash_kernel_traits<192, 128, 64, 8, cutlass::half_t>>, Is_dropout=false, Is_causal=false]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(183): here
            instantiation of "void run_mha_fwd_hdim192<T>(Flash_fwd_params &, cudaStream_t) [with T=cutlass::half_t]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_hdim192_fp16_sm80.cu(26): here

/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_kernel.h(276): warning: variable "tQrQ" was declared but never referenced
          detected during:
            instantiation of "void flash::compute_attn_1rowblock<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params,Is_Q_split_K>(const Params &, int, int, int) [with Kernel_traits=Flash_fwd_kernel_traits<192, 128, 64, 8, false, false, cutlass::half_t, Flash_kernel_traits<192, 128, 64, 8, cutlass::half_t>>, Is_dropout=false, Is_causal=false, Is_even_N=true, Is_even_K=true, Return_softmax=false, Params=Flash_fwd_params, Is_Q_split_K=false]" 
(589): here
            instantiation of "void flash::compute_attn<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &) [with Kernel_traits=Flash_fwd_kernel_traits<192, 128, 64, 8, false, false, cutlass::half_t, Flash_kernel_traits<192, 128, 64, 8, cutlass::half_t>>, Is_dropout=false, Is_causal=false, Is_even_N=true, Is_even_K=true, Return_softmax=false, Params=Flash_fwd_params]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(15): here
            instantiation of "void flash_fwd_kernel<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax>(Flash_fwd_params) [with Kernel_traits=Flash_fwd_kernel_traits<192, 128, 64, 8, false, false, cutlass::half_t, Flash_kernel_traits<192, 128, 64, 8, cutlass::half_t>>, Is_dropout=false, Is_causal=false, Is_even_N=true, Is_even_K=true, Return_softmax=false]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(34): here
            instantiation of "void run_flash_fwd<Kernel_traits,Is_dropout,Is_causal>(Flash_fwd_params &, cudaStream_t) [with Kernel_traits=Flash_fwd_kernel_traits<192, 128, 64, 8, false, false, cutlass::half_t, Flash_kernel_traits<192, 128, 64, 8, cutlass::half_t>>, Is_dropout=false, Is_causal=false]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(183): here
            instantiation of "void run_mha_fwd_hdim192<T>(Flash_fwd_params &, cudaStream_t) [with T=cutlass::half_t]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_hdim192_fp16_sm80.cu(26): here

/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_kernel.h(140): warning: variable "MMA_M" was declared but never referenced
          detected during:
            instantiation of "void flash::compute_attn_1rowblock<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params,Is_Q_split_K>(const Params &, int, int, int) [with Kernel_traits=Flash_fwd_kernel_traits<192, 128, 64, 8, false, false, cutlass::half_t, Flash_kernel_traits<192, 128, 64, 8, cutlass::half_t>>, Is_dropout=false, Is_causal=false, Is_even_N=true, Is_even_K=false, Return_softmax=false, Params=Flash_fwd_params, Is_Q_split_K=false]" 
(589): here
            instantiation of "void flash::compute_attn<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &) [with Kernel_traits=Flash_fwd_kernel_traits<192, 128, 64, 8, false, false, cutlass::half_t, Flash_kernel_traits<192, 128, 64, 8, cutlass::half_t>>, Is_dropout=false, Is_causal=false, Is_even_N=true, Is_even_K=false, Return_softmax=false, Params=Flash_fwd_params]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(15): here
            instantiation of "void flash_fwd_kernel<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax>(Flash_fwd_params) [with Kernel_traits=Flash_fwd_kernel_traits<192, 128, 64, 8, false, false, cutlass::half_t, Flash_kernel_traits<192, 128, 64, 8, cutlass::half_t>>, Is_dropout=false, Is_causal=false, Is_even_N=true, Is_even_K=false, Return_softmax=false]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(34): here
            instantiation of "void run_flash_fwd<Kernel_traits,Is_dropout,Is_causal>(Flash_fwd_params &, cudaStream_t) [with Kernel_traits=Flash_fwd_kernel_traits<192, 128, 64, 8, false, false, cutlass::half_t, Flash_kernel_traits<192, 128, 64, 8, cutlass::half_t>>, Is_dropout=false, Is_causal=false]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(183): here
            instantiation of "void run_mha_fwd_hdim192<T>(Flash_fwd_params &, cudaStream_t) [with T=cutlass::half_t]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_hdim192_fp16_sm80.cu(26): here

/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_kernel.h(276): warning: variable "tQrQ" was declared but never referenced
          detected during:
            instantiation of "void flash::compute_attn_1rowblock<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params,Is_Q_split_K>(const Params &, int, int, int) [with Kernel_traits=Flash_fwd_kernel_traits<192, 128, 64, 8, false, false, cutlass::half_t, Flash_kernel_traits<192, 128, 64, 8, cutlass::half_t>>, Is_dropout=false, Is_causal=false, Is_even_N=true, Is_even_K=false, Return_softmax=false, Params=Flash_fwd_params, Is_Q_split_K=false]" 
(589): here
            instantiation of "void flash::compute_attn<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &) [with Kernel_traits=Flash_fwd_kernel_traits<192, 128, 64, 8, false, false, cutlass::half_t, Flash_kernel_traits<192, 128, 64, 8, cutlass::half_t>>, Is_dropout=false, Is_causal=false, Is_even_N=true, Is_even_K=false, Return_softmax=false, Params=Flash_fwd_params]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(15): here
            instantiation of "void flash_fwd_kernel<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax>(Flash_fwd_params) [with Kernel_traits=Flash_fwd_kernel_traits<192, 128, 64, 8, false, false, cutlass::half_t, Flash_kernel_traits<192, 128, 64, 8, cutlass::half_t>>, Is_dropout=false, Is_causal=false, Is_even_N=true, Is_even_K=false, Return_softmax=false]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(34): here
            instantiation of "void run_flash_fwd<Kernel_traits,Is_dropout,Is_causal>(Flash_fwd_params &, cudaStream_t) [with Kernel_traits=Flash_fwd_kernel_traits<192, 128, 64, 8, false, false, cutlass::half_t, Flash_kernel_traits<192, 128, 64, 8, cutlass::half_t>>, Is_dropout=false, Is_causal=false]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(183): here
            instantiation of "void run_mha_fwd_hdim192<T>(Flash_fwd_params &, cudaStream_t) [with T=cutlass::half_t]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_hdim192_fp16_sm80.cu(26): here

/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_kernel.h(140): warning: variable "MMA_M" was declared but never referenced
          detected during:
            instantiation of "void flash::compute_attn_1rowblock<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params,Is_Q_split_K>(const Params &, int, int, int) [with Kernel_traits=Flash_fwd_kernel_traits<192, 128, 64, 8, false, false, cutlass::half_t, Flash_kernel_traits<192, 128, 64, 8, cutlass::half_t>>, Is_dropout=false, Is_causal=false, Is_even_N=false, Is_even_K=true, Return_softmax=false, Params=Flash_fwd_params, Is_Q_split_K=false]" 
(589): here
            instantiation of "void flash::compute_attn<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &) [with Kernel_traits=Flash_fwd_kernel_traits<192, 128, 64, 8, false, false, cutlass::half_t, Flash_kernel_traits<192, 128, 64, 8, cutlass::half_t>>, Is_dropout=false, Is_causal=false, Is_even_N=false, Is_even_K=true, Return_softmax=false, Params=Flash_fwd_params]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(15): here
            instantiation of "void flash_fwd_kernel<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax>(Flash_fwd_params) [with Kernel_traits=Flash_fwd_kernel_traits<192, 128, 64, 8, false, false, cutlass::half_t, Flash_kernel_traits<192, 128, 64, 8, cutlass::half_t>>, Is_dropout=false, Is_causal=false, Is_even_N=false, Is_even_K=true, Return_softmax=false]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(34): here
            instantiation of "void run_flash_fwd<Kernel_traits,Is_dropout,Is_causal>(Flash_fwd_params &, cudaStream_t) [with Kernel_traits=Flash_fwd_kernel_traits<192, 128, 64, 8, false, false, cutlass::half_t, Flash_kernel_traits<192, 128, 64, 8, cutlass::half_t>>, Is_dropout=false, Is_causal=false]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(183): here
            instantiation of "void run_mha_fwd_hdim192<T>(Flash_fwd_params &, cudaStream_t) [with T=cutlass::half_t]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_hdim192_fp16_sm80.cu(26): here

/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_kernel.h(276): warning: variable "tQrQ" was declared but never referenced
          detected during:
            instantiation of "void flash::compute_attn_1rowblock<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params,Is_Q_split_K>(const Params &, int, int, int) [with Kernel_traits=Flash_fwd_kernel_traits<192, 128, 64, 8, false, false, cutlass::half_t, Flash_kernel_traits<192, 128, 64, 8, cutlass::half_t>>, Is_dropout=false, Is_causal=false, Is_even_N=false, Is_even_K=true, Return_softmax=false, Params=Flash_fwd_params, Is_Q_split_K=false]" 
(589): here
            instantiation of "void flash::compute_attn<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &) [with Kernel_traits=Flash_fwd_kernel_traits<192, 128, 64, 8, false, false, cutlass::half_t, Flash_kernel_traits<192, 128, 64, 8, cutlass::half_t>>, Is_dropout=false, Is_causal=false, Is_even_N=false, Is_even_K=true, Return_softmax=false, Params=Flash_fwd_params]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(15): here
            instantiation of "void flash_fwd_kernel<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax>(Flash_fwd_params) [with Kernel_traits=Flash_fwd_kernel_traits<192, 128, 64, 8, false, false, cutlass::half_t, Flash_kernel_traits<192, 128, 64, 8, cutlass::half_t>>, Is_dropout=false, Is_causal=false, Is_even_N=false, Is_even_K=true, Return_softmax=false]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(34): here
            instantiation of "void run_flash_fwd<Kernel_traits,Is_dropout,Is_causal>(Flash_fwd_params &, cudaStream_t) [with Kernel_traits=Flash_fwd_kernel_traits<192, 128, 64, 8, false, false, cutlass::half_t, Flash_kernel_traits<192, 128, 64, 8, cutlass::half_t>>, Is_dropout=false, Is_causal=false]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(183): here
            instantiation of "void run_mha_fwd_hdim192<T>(Flash_fwd_params &, cudaStream_t) [with T=cutlass::half_t]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_hdim192_fp16_sm80.cu(26): here

/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_kernel.h(140): warning: variable "MMA_M" was declared but never referenced
          detected during:
            instantiation of "void flash::compute_attn_1rowblock<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params,Is_Q_split_K>(const Params &, int, int, int) [with Kernel_traits=Flash_fwd_kernel_traits<192, 128, 64, 8, false, false, cutlass::half_t, Flash_kernel_traits<192, 128, 64, 8, cutlass::half_t>>, Is_dropout=false, Is_causal=false, Is_even_N=false, Is_even_K=false, Return_softmax=false, Params=Flash_fwd_params, Is_Q_split_K=false]" 
(589): here
            instantiation of "void flash::compute_attn<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &) [with Kernel_traits=Flash_fwd_kernel_traits<192, 128, 64, 8, false, false, cutlass::half_t, Flash_kernel_traits<192, 128, 64, 8, cutlass::half_t>>, Is_dropout=false, Is_causal=false, Is_even_N=false, Is_even_K=false, Return_softmax=false, Params=Flash_fwd_params]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(15): here
            instantiation of "void flash_fwd_kernel<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax>(Flash_fwd_params) [with Kernel_traits=Flash_fwd_kernel_traits<192, 128, 64, 8, false, false, cutlass::half_t, Flash_kernel_traits<192, 128, 64, 8, cutlass::half_t>>, Is_dropout=false, Is_causal=false, Is_even_N=false, Is_even_K=false, Return_softmax=false]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(34): here
            instantiation of "void run_flash_fwd<Kernel_traits,Is_dropout,Is_causal>(Flash_fwd_params &, cudaStream_t) [with Kernel_traits=Flash_fwd_kernel_traits<192, 128, 64, 8, false, false, cutlass::half_t, Flash_kernel_traits<192, 128, 64, 8, cutlass::half_t>>, Is_dropout=false, Is_causal=false]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(183): here
            instantiation of "void run_mha_fwd_hdim192<T>(Flash_fwd_params &, cudaStream_t) [with T=cutlass::half_t]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_hdim192_fp16_sm80.cu(26): here

/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_kernel.h(276): warning: variable "tQrQ" was declared but never referenced
          detected during:
            instantiation of "void flash::compute_attn_1rowblock<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params,Is_Q_split_K>(const Params &, int, int, int) [with Kernel_traits=Flash_fwd_kernel_traits<192, 128, 64, 8, false, false, cutlass::half_t, Flash_kernel_traits<192, 128, 64, 8, cutlass::half_t>>, Is_dropout=false, Is_causal=false, Is_even_N=false, Is_even_K=false, Return_softmax=false, Params=Flash_fwd_params, Is_Q_split_K=false]" 
(589): here
            instantiation of "void flash::compute_attn<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &) [with Kernel_traits=Flash_fwd_kernel_traits<192, 128, 64, 8, false, false, cutlass::half_t, Flash_kernel_traits<192, 128, 64, 8, cutlass::half_t>>, Is_dropout=false, Is_causal=false, Is_even_N=false, Is_even_K=false, Return_softmax=false, Params=Flash_fwd_params]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(15): here
            instantiation of "void flash_fwd_kernel<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax>(Flash_fwd_params) [with Kernel_traits=Flash_fwd_kernel_traits<192, 128, 64, 8, false, false, cutlass::half_t, Flash_kernel_traits<192, 128, 64, 8, cutlass::half_t>>, Is_dropout=false, Is_causal=false, Is_even_N=false, Is_even_K=false, Return_softmax=false]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(34): here
            instantiation of "void run_flash_fwd<Kernel_traits,Is_dropout,Is_causal>(Flash_fwd_params &, cudaStream_t) [with Kernel_traits=Flash_fwd_kernel_traits<192, 128, 64, 8, false, false, cutlass::half_t, Flash_kernel_traits<192, 128, 64, 8, cutlass::half_t>>, Is_dropout=false, Is_causal=false]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(183): here
            instantiation of "void run_mha_fwd_hdim192<T>(Flash_fwd_params &, cudaStream_t) [with T=cutlass::half_t]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_hdim192_fp16_sm80.cu(26): here

ptxas info    : 2 bytes gmem
ptxas info    : Compiling entry function '_Z16flash_fwd_kernelI23Flash_fwd_kernel_traitsILi192ELi128ELi64ELi8ELb0ELb0EN7cutlass6half_tE19Flash_kernel_traitsILi192ELi128ELi64ELi8ES2_EELb0ELb0ELb0ELb0ELb0EEv16Flash_fwd_params' for 'sm_80'
ptxas info    : Function properties for _Z16flash_fwd_kernelI23Flash_fwd_kernel_traitsILi192ELi128ELi64ELi8ELb0ELb0EN7cutlass6half_tE19Flash_kernel_traitsILi192ELi128ELi64ELi8ES2_EELb0ELb0ELb0ELb0ELb0EEv16Flash_fwd_params
    0 bytes stack frame, 0 bytes spill stores, 0 bytes spill loads
ptxas info    : Used 209 registers, 584 bytes cmem[0]
ptxas info    : Compiling entry function '_Z16flash_fwd_kernelI23Flash_fwd_kernel_traitsILi192ELi128ELi64ELi8ELb0ELb0EN7cutlass6half_tE19Flash_kernel_traitsILi192ELi128ELi64ELi8ES2_EELb0ELb0ELb0ELb1ELb0EEv16Flash_fwd_params' for 'sm_80'
ptxas info    : Function properties for _Z16flash_fwd_kernelI23Flash_fwd_kernel_traitsILi192ELi128ELi64ELi8ELb0ELb0EN7cutlass6half_tE19Flash_kernel_traitsILi192ELi128ELi64ELi8ES2_EELb0ELb0ELb0ELb1ELb0EEv16Flash_fwd_params
    0 bytes stack frame, 0 bytes spill stores, 0 bytes spill loads
ptxas info    : Used 179 registers, 584 bytes cmem[0]
ptxas info    : Compiling entry function '_Z16flash_fwd_kernelI23Flash_fwd_kernel_traitsILi192ELi128ELi64ELi8ELb0ELb0EN7cutlass6half_tE19Flash_kernel_traitsILi192ELi128ELi64ELi8ES2_EELb0ELb0ELb1ELb0ELb0EEv16Flash_fwd_params' for 'sm_80'
ptxas info    : Function properties for _Z16flash_fwd_kernelI23Flash_fwd_kernel_traitsILi192ELi128ELi64ELi8ELb0ELb0EN7cutlass6half_tE19Flash_kernel_traitsILi192ELi128ELi64ELi8ES2_EELb0ELb0ELb1ELb0ELb0EEv16Flash_fwd_params
    0 bytes stack frame, 0 bytes spill stores, 0 bytes spill loads
ptxas info    : Used 173 registers, 584 bytes cmem[0]
ptxas info    : Compiling entry function '_Z16flash_fwd_kernelI23Flash_fwd_kernel_traitsILi192ELi128ELi64ELi8ELb0ELb0EN7cutlass6half_tE19Flash_kernel_traitsILi192ELi128ELi64ELi8ES2_EELb0ELb0ELb1ELb1ELb0EEv16Flash_fwd_params' for 'sm_80'
ptxas info    : Function properties for _Z16flash_fwd_kernelI23Flash_fwd_kernel_traitsILi192ELi128ELi64ELi8ELb0ELb0EN7cutlass6half_tE19Flash_kernel_traitsILi192ELi128ELi64ELi8ES2_EELb0ELb0ELb1ELb1ELb0EEv16Flash_fwd_params
    0 bytes stack frame, 0 bytes spill stores, 0 bytes spill loads
ptxas info    : Used 173 registers, 584 bytes cmem[0]
ptxas info    : Compiling entry function '_Z16flash_fwd_kernelI23Flash_fwd_kernel_traitsILi192ELi128ELi64ELi8ELb0ELb0EN7cutlass6half_tE19Flash_kernel_traitsILi192ELi128ELi64ELi8ES2_EELb0ELb1ELb0ELb0ELb0EEv16Flash_fwd_params' for 'sm_80'
ptxas info    : Function properties for _Z16flash_fwd_kernelI23Flash_fwd_kernel_traitsILi192ELi128ELi64ELi8ELb0ELb0EN7cutlass6half_tE19Flash_kernel_traitsILi192ELi128ELi64ELi8ES2_EELb0ELb1ELb0ELb0ELb0EEv16Flash_fwd_params
    0 bytes stack frame, 0 bytes spill stores, 0 bytes spill loads
ptxas info    : Used 220 registers, 584 bytes cmem[0]
ptxas info    : Compiling entry function '_Z16flash_fwd_kernelI23Flash_fwd_kernel_traitsILi192ELi128ELi64ELi8ELb0ELb0EN7cutlass6half_tE19Flash_kernel_traitsILi192ELi128ELi64ELi8ES2_EELb0ELb1ELb0ELb1ELb0EEv16Flash_fwd_params' for 'sm_80'
ptxas info    : Function properties for _Z16flash_fwd_kernelI23Flash_fwd_kernel_traitsILi192ELi128ELi64ELi8ELb0ELb0EN7cutlass6half_tE19Flash_kernel_traitsILi192ELi128ELi64ELi8ES2_EELb0ELb1ELb0ELb1ELb0EEv16Flash_fwd_params
    0 bytes stack frame, 0 bytes spill stores, 0 bytes spill loads
ptxas info    : Used 221 registers, 584 bytes cmem[0]
ptxas info    : Compiling entry function '_Z16flash_fwd_kernelI23Flash_fwd_kernel_traitsILi192ELi128ELi64ELi8ELb0ELb0EN7cutlass6half_tE19Flash_kernel_traitsILi192ELi128ELi64ELi8ES2_EELb0ELb1ELb1ELb0ELb0EEv16Flash_fwd_params' for 'sm_80'
ptxas info    : Function properties for _Z16flash_fwd_kernelI23Flash_fwd_kernel_traitsILi192ELi128ELi64ELi8ELb0ELb0EN7cutlass6half_tE19Flash_kernel_traitsILi192ELi128ELi64ELi8ES2_EELb0ELb1ELb1ELb0ELb0EEv16Flash_fwd_params
    0 bytes stack frame, 0 bytes spill stores, 0 bytes spill loads
ptxas info    : Used 225 registers, 584 bytes cmem[0]
ptxas info    : Compiling entry function '_Z16flash_fwd_kernelI23Flash_fwd_kernel_traitsILi192ELi128ELi64ELi8ELb0ELb0EN7cutlass6half_tE19Flash_kernel_traitsILi192ELi128ELi64ELi8ES2_EELb0ELb1ELb1ELb1ELb0EEv16Flash_fwd_params' for 'sm_80'
ptxas info    : Function properties for _Z16flash_fwd_kernelI23Flash_fwd_kernel_traitsILi192ELi128ELi64ELi8ELb0ELb0EN7cutlass6half_tE19Flash_kernel_traitsILi192ELi128ELi64ELi8ES2_EELb0ELb1ELb1ELb1ELb0EEv16Flash_fwd_params
    0 bytes stack frame, 0 bytes spill stores, 0 bytes spill loads
ptxas info    : Used 230 registers, 584 bytes cmem[0]
ptxas info    : Compiling entry function '_Z16flash_fwd_kernelI23Flash_fwd_kernel_traitsILi192ELi64ELi64ELi4ELb0ELb0EN7cutlass6half_tE19Flash_kernel_traitsILi192ELi64ELi64ELi4ES2_EELb1ELb0ELb0ELb0ELb0EEv16Flash_fwd_params' for 'sm_80'
ptxas info    : Function properties for _Z16flash_fwd_kernelI23Flash_fwd_kernel_traitsILi192ELi64ELi64ELi4ELb0ELb0EN7cutlass6half_tE19Flash_kernel_traitsILi192ELi64ELi64ELi4ES2_EELb1ELb0ELb0ELb0ELb0EEv16Flash_fwd_params
    0 bytes stack frame, 0 bytes spill stores, 0 bytes spill loads
ptxas info    : Used 222 registers, 584 bytes cmem[0]
ptxas info    : Compiling entry function '_Z16flash_fwd_kernelI23Flash_fwd_kernel_traitsILi192ELi64ELi64ELi4ELb0ELb0EN7cutlass6half_tE19Flash_kernel_traitsILi192ELi64ELi64ELi4ES2_EELb1ELb0ELb0ELb0ELb1EEv16Flash_fwd_params' for 'sm_80'
ptxas info    : Function properties for _Z16flash_fwd_kernelI23Flash_fwd_kernel_traitsILi192ELi64ELi64ELi4ELb0ELb0EN7cutlass6half_tE19Flash_kernel_traitsILi192ELi64ELi64ELi4ES2_EELb1ELb0ELb0ELb0ELb1EEv16Flash_fwd_params
    40 bytes stack frame, 40 bytes spill stores, 40 bytes spill loads
ptxas info    : Used 255 registers, 584 bytes cmem[0]
ptxas info    : Compiling entry function '_Z16flash_fwd_kernelI23Flash_fwd_kernel_traitsILi192ELi64ELi64ELi4ELb0ELb0EN7cutlass6half_tE19Flash_kernel_traitsILi192ELi64ELi64ELi4ES2_EELb1ELb0ELb0ELb1ELb0EEv16Flash_fwd_params' for 'sm_80'
ptxas info    : Function properties for _Z16flash_fwd_kernelI23Flash_fwd_kernel_traitsILi192ELi64ELi64ELi4ELb0ELb0EN7cutlass6half_tE19Flash_kernel_traitsILi192ELi64ELi64ELi4ES2_EELb1ELb0ELb0ELb1ELb0EEv16Flash_fwd_params
    0 bytes stack frame, 0 bytes spill stores, 0 bytes spill loads
ptxas info    : Used 221 registers, 584 bytes cmem[0]
ptxas info    : Compiling entry function '_Z16flash_fwd_kernelI23Flash_fwd_kernel_traitsILi192ELi64ELi64ELi4ELb0ELb0EN7cutlass6half_tE19Flash_kernel_traitsILi192ELi64ELi64ELi4ES2_EELb1ELb0ELb0ELb1ELb1EEv16Flash_fwd_params' for 'sm_80'
ptxas info    : Function properties for _Z16flash_fwd_kernelI23Flash_fwd_kernel_traitsILi192ELi64ELi64ELi4ELb0ELb0EN7cutlass6half_tE19Flash_kernel_traitsILi192ELi64ELi64ELi4ES2_EELb1ELb0ELb0ELb1ELb1EEv16Flash_fwd_params
    24 bytes stack frame, 24 bytes spill stores, 24 bytes spill loads
ptxas info    : Used 255 registers, 584 bytes cmem[0]
ptxas info    : Compiling entry function '_Z16flash_fwd_kernelI23Flash_fwd_kernel_traitsILi192ELi64ELi64ELi4ELb0ELb0EN7cutlass6half_tE19Flash_kernel_traitsILi192ELi64ELi64ELi4ES2_EELb1ELb0ELb1ELb0ELb0EEv16Flash_fwd_params' for 'sm_80'
ptxas info    : Function properties for _Z16flash_fwd_kernelI23Flash_fwd_kernel_traitsILi192ELi64ELi64ELi4ELb0ELb0EN7cutlass6half_tE19Flash_kernel_traitsILi192ELi64ELi64ELi4ES2_EELb1ELb0ELb1ELb0ELb0EEv16Flash_fwd_params
    0 bytes stack frame, 0 bytes spill stores, 0 bytes spill loads
ptxas info    : Used 223 registers, 584 bytes cmem[0]
ptxas info    : Compiling entry function '_Z16flash_fwd_kernelI23Flash_fwd_kernel_traitsILi192ELi64ELi64ELi4ELb0ELb0EN7cutlass6half_tE19Flash_kernel_traitsILi192ELi64ELi64ELi4ES2_EELb1ELb0ELb1ELb0ELb1EEv16Flash_fwd_params' for 'sm_80'
ptxas info    : Function properties for _Z16flash_fwd_kernelI23Flash_fwd_kernel_traitsILi192ELi64ELi64ELi4ELb0ELb0EN7cutlass6half_tE19Flash_kernel_traitsILi192ELi64ELi64ELi4ES2_EELb1ELb0ELb1ELb0ELb1EEv16Flash_fwd_params
    32 bytes stack frame, 32 bytes spill stores, 32 bytes spill loads
ptxas info    : Used 255 registers, 584 bytes cmem[0]
ptxas info    : Compiling entry function '_Z16flash_fwd_kernelI23Flash_fwd_kernel_traitsILi192ELi64ELi64ELi4ELb0ELb0EN7cutlass6half_tE19Flash_kernel_traitsILi192ELi64ELi64ELi4ES2_EELb1ELb0ELb1ELb1ELb0EEv16Flash_fwd_params' for 'sm_80'
ptxas info    : Function properties for _Z16flash_fwd_kernelI23Flash_fwd_kernel_traitsILi192ELi64ELi64ELi4ELb0ELb0EN7cutlass6half_tE19Flash_kernel_traitsILi192ELi64ELi64ELi4ES2_EELb1ELb0ELb1ELb1ELb0EEv16Flash_fwd_params
    0 bytes stack frame, 0 bytes spill stores, 0 bytes spill loads
ptxas info    : Used 227 registers, 584 bytes cmem[0]
ptxas info    : Compiling entry function '_Z16flash_fwd_kernelI23Flash_fwd_kernel_traitsILi192ELi64ELi64ELi4ELb0ELb0EN7cutlass6half_tE19Flash_kernel_traitsILi192ELi64ELi64ELi4ES2_EELb1ELb0ELb1ELb1ELb1EEv16Flash_fwd_params' for 'sm_80'
ptxas info    : Function properties for _Z16flash_fwd_kernelI23Flash_fwd_kernel_traitsILi192ELi64ELi64ELi4ELb0ELb0EN7cutlass6half_tE19Flash_kernel_traitsILi192ELi64ELi64ELi4ES2_EELb1ELb0ELb1ELb1ELb1EEv16Flash_fwd_params
    24 bytes stack frame, 28 bytes spill stores, 20 bytes spill loads
ptxas info    : Used 255 registers, 584 bytes cmem[0]
ptxas info    : Compiling entry function '_Z16flash_fwd_kernelI23Flash_fwd_kernel_traitsILi192ELi64ELi64ELi4ELb0ELb0EN7cutlass6half_tE19Flash_kernel_traitsILi192ELi64ELi64ELi4ES2_EELb1ELb1ELb0ELb0ELb0EEv16Flash_fwd_params' for 'sm_80'
ptxas info    : Function properties for _Z16flash_fwd_kernelI23Flash_fwd_kernel_traitsILi192ELi64ELi64ELi4ELb0ELb0EN7cutlass6half_tE19Flash_kernel_traitsILi192ELi64ELi64ELi4ES2_EELb1ELb1ELb0ELb0ELb0EEv16Flash_fwd_params
    0 bytes stack frame, 0 bytes spill stores, 0 bytes spill loads
ptxas info    : Used 222 registers, 584 bytes cmem[0]
ptxas info    : Compiling entry function '_Z16flash_fwd_kernelI23Flash_fwd_kernel_traitsILi192ELi64ELi64ELi4ELb0ELb0EN7cutlass6half_tE19Flash_kernel_traitsILi192ELi64ELi64ELi4ES2_EELb1ELb1ELb0ELb0ELb1EEv16Flash_fwd_params' for 'sm_80'
ptxas info    : Function properties for _Z16flash_fwd_kernelI23Flash_fwd_kernel_traitsILi192ELi64ELi64ELi4ELb0ELb0EN7cutlass6half_tE19Flash_kernel_traitsILi192ELi64ELi64ELi4ES2_EELb1ELb1ELb0ELb0ELb1EEv16Flash_fwd_params
    40 bytes stack frame, 48 bytes spill stores, 48 bytes spill loads
ptxas info    : Used 255 registers, 584 bytes cmem[0]
ptxas info    : Compiling entry function '_Z16flash_fwd_kernelI23Flash_fwd_kernel_traitsILi192ELi64ELi64ELi4ELb0ELb0EN7cutlass6half_tE19Flash_kernel_traitsILi192ELi64ELi64ELi4ES2_EELb1ELb1ELb0ELb1ELb0EEv16Flash_fwd_params' for 'sm_80'
ptxas info    : Function properties for _Z16flash_fwd_kernelI23Flash_fwd_kernel_traitsILi192ELi64ELi64ELi4ELb0ELb0EN7cutlass6half_tE19Flash_kernel_traitsILi192ELi64ELi64ELi4ES2_EELb1ELb1ELb0ELb1ELb0EEv16Flash_fwd_params
    0 bytes stack frame, 0 bytes spill stores, 0 bytes spill loads
ptxas info    : Used 223 registers, 584 bytes cmem[0]
ptxas info    : Compiling entry function '_Z16flash_fwd_kernelI23Flash_fwd_kernel_traitsILi192ELi64ELi64ELi4ELb0ELb0EN7cutlass6half_tE19Flash_kernel_traitsILi192ELi64ELi64ELi4ES2_EELb1ELb1ELb0ELb1ELb1EEv16Flash_fwd_params' for 'sm_80'
ptxas info    : Function properties for _Z16flash_fwd_kernelI23Flash_fwd_kernel_traitsILi192ELi64ELi64ELi4ELb0ELb0EN7cutlass6half_tE19Flash_kernel_traitsILi192ELi64ELi64ELi4ES2_EELb1ELb1ELb0ELb1ELb1EEv16Flash_fwd_params
    24 bytes stack frame, 24 bytes spill stores, 24 bytes spill loads
ptxas info    : Used 255 registers, 584 bytes cmem[0]
ptxas info    : Compiling entry function '_Z16flash_fwd_kernelI23Flash_fwd_kernel_traitsILi192ELi64ELi64ELi4ELb0ELb0EN7cutlass6half_tE19Flash_kernel_traitsILi192ELi64ELi64ELi4ES2_EELb1ELb1ELb1ELb0ELb0EEv16Flash_fwd_params' for 'sm_80'
ptxas info    : Function properties for _Z16flash_fwd_kernelI23Flash_fwd_kernel_traitsILi192ELi64ELi64ELi4ELb0ELb0EN7cutlass6half_tE19Flash_kernel_traitsILi192ELi64ELi64ELi4ES2_EELb1ELb1ELb1ELb0ELb0EEv16Flash_fwd_params
    0 bytes stack frame, 0 bytes spill stores, 0 bytes spill loads
ptxas info    : Used 231 registers, 584 bytes cmem[0]
ptxas info    : Compiling entry function '_Z16flash_fwd_kernelI23Flash_fwd_kernel_traitsILi192ELi64ELi64ELi4ELb0ELb0EN7cutlass6half_tE19Flash_kernel_traitsILi192ELi64ELi64ELi4ES2_EELb1ELb1ELb1ELb0ELb1EEv16Flash_fwd_params' for 'sm_80'
ptxas info    : Function properties for _Z16flash_fwd_kernelI23Flash_fwd_kernel_traitsILi192ELi64ELi64ELi4ELb0ELb0EN7cutlass6half_tE19Flash_kernel_traitsILi192ELi64ELi64ELi4ES2_EELb1ELb1ELb1ELb0ELb1EEv16Flash_fwd_params
    40 bytes stack frame, 38 bytes spill stores, 38 bytes spill loads
ptxas info    : Used 255 registers, 584 bytes cmem[0]
ptxas info    : Compiling entry function '_Z16flash_fwd_kernelI23Flash_fwd_kernel_traitsILi192ELi64ELi64ELi4ELb0ELb0EN7cutlass6half_tE19Flash_kernel_traitsILi192ELi64ELi64ELi4ES2_EELb1ELb1ELb1ELb1ELb0EEv16Flash_fwd_params' for 'sm_80'
ptxas info    : Function properties for _Z16flash_fwd_kernelI23Flash_fwd_kernel_traitsILi192ELi64ELi64ELi4ELb0ELb0EN7cutlass6half_tE19Flash_kernel_traitsILi192ELi64ELi64ELi4ES2_EELb1ELb1ELb1ELb1ELb0EEv16Flash_fwd_params
    0 bytes stack frame, 0 bytes spill stores, 0 bytes spill loads
ptxas info    : Used 226 registers, 584 bytes cmem[0]
ptxas info    : Compiling entry function '_Z16flash_fwd_kernelI23Flash_fwd_kernel_traitsILi192ELi64ELi64ELi4ELb0ELb0EN7cutlass6half_tE19Flash_kernel_traitsILi192ELi64ELi64ELi4ES2_EELb1ELb1ELb1ELb1ELb1EEv16Flash_fwd_params' for 'sm_80'
ptxas info    : Function properties for _Z16flash_fwd_kernelI23Flash_fwd_kernel_traitsILi192ELi64ELi64ELi4ELb0ELb0EN7cutlass6half_tE19Flash_kernel_traitsILi192ELi64ELi64ELi4ES2_EELb1ELb1ELb1ELb1ELb1EEv16Flash_fwd_params
    32 bytes stack frame, 36 bytes spill stores, 28 bytes spill loads
ptxas info    : Used 255 registers, 584 bytes cmem[0]
/anaconda/envs/flashattention/lib/python3.9/site-packages/torch/include/c10/core/SymInt.h(84): warning: integer conversion resulted in a change of sign

[5/37] /usr/local/cuda-11.4/bin/nvcc  -I/home/v-feiychen/flash-attention/csrc/flash_attn -I/home/v-feiychen/flash-attention/csrc/flash_attn/src -I/home/v-feiychen/flash-attention/csrc/cutlass/include -I/anaconda/envs/flashattention/lib/python3.9/site-packages/torch/include -I/anaconda/envs/flashattention/lib/python3.9/site-packages/torch/include/torch/csrc/api/include -I/anaconda/envs/flashattention/lib/python3.9/site-packages/torch/include/TH -I/anaconda/envs/flashattention/lib/python3.9/site-packages/torch/include/THC -I/usr/local/cuda-11.4/include -I/anaconda/envs/flashattention/include/python3.9 -c -c /home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_hdim192_bf16_sm80.cu -o /home/v-feiychen/flash-attention/build/temp.linux-x86_64-cpython-39/csrc/flash_attn/src/flash_fwd_hdim192_bf16_sm80.o -D__CUDA_NO_HALF_OPERATORS__ -D__CUDA_NO_HALF_CONVERSIONS__ -D__CUDA_NO_BFLOAT16_CONVERSIONS__ -D__CUDA_NO_HALF2_OPERATORS__ --expt-relaxed-constexpr --compiler-options ''"'"'-fPIC'"'"'' -O3 -std=c++17 -U__CUDA_NO_HALF_OPERATORS__ -U__CUDA_NO_HALF_CONVERSIONS__ -U__CUDA_NO_HALF2_OPERATORS__ -U__CUDA_NO_BFLOAT16_CONVERSIONS__ --expt-relaxed-constexpr --expt-extended-lambda --use_fast_math --ptxas-options=-v -lineinfo -gencode arch=compute_80,code=sm_80 --threads 4 -DTORCH_API_INCLUDE_EXTENSION_H '-DPYBIND11_COMPILER_TYPE="_gcc"' '-DPYBIND11_STDLIB="_libstdcpp"' '-DPYBIND11_BUILD_ABI="_cxxabi1011"' -DTORCH_EXTENSION_NAME=flash_attn_2_cuda -D_GLIBCXX_USE_CXX11_ABI=0
/anaconda/envs/flashattention/lib/python3.9/site-packages/torch/include/c10/core/SymInt.h(84): warning: integer conversion resulted in a change of sign

/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_kernel.h(140): warning: variable "MMA_M" was declared but never referenced
          detected during:
            instantiation of "void flash::compute_attn_1rowblock<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params,Is_Q_split_K>(const Params &, int, int, int) [with Kernel_traits=Flash_fwd_kernel_traits<192, 64, 64, 4, false, false, cutlass::bfloat16_t, Flash_kernel_traits<192, 64, 64, 4, cutlass::bfloat16_t>>, Is_dropout=true, Is_causal=true, Is_even_N=true, Is_even_K=true, Return_softmax=true, Params=Flash_fwd_params, Is_Q_split_K=false]" 
(589): here
            instantiation of "void flash::compute_attn<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &) [with Kernel_traits=Flash_fwd_kernel_traits<192, 64, 64, 4, false, false, cutlass::bfloat16_t, Flash_kernel_traits<192, 64, 64, 4, cutlass::bfloat16_t>>, Is_dropout=true, Is_causal=true, Is_even_N=true, Is_even_K=true, Return_softmax=true, Params=Flash_fwd_params]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(15): here
            instantiation of "void flash_fwd_kernel<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax>(Flash_fwd_params) [with Kernel_traits=Flash_fwd_kernel_traits<192, 64, 64, 4, false, false, cutlass::bfloat16_t, Flash_kernel_traits<192, 64, 64, 4, cutlass::bfloat16_t>>, Is_dropout=true, Is_causal=true, Is_even_N=true, Is_even_K=true, Return_softmax=true]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(34): here
            instantiation of "void run_flash_fwd<Kernel_traits,Is_dropout,Is_causal>(Flash_fwd_params &, cudaStream_t) [with Kernel_traits=Flash_fwd_kernel_traits<192, 64, 64, 4, false, false, cutlass::bfloat16_t, Flash_kernel_traits<192, 64, 64, 4, cutlass::bfloat16_t>>, Is_dropout=true, Is_causal=true]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(183): here
            instantiation of "void run_mha_fwd_hdim192<T>(Flash_fwd_params &, cudaStream_t) [with T=cutlass::bfloat16_t]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_hdim192_bf16_sm80.cu(15): here

/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_kernel.h(276): warning: variable "tQrQ" was declared but never referenced
          detected during:
            instantiation of "void flash::compute_attn_1rowblock<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params,Is_Q_split_K>(const Params &, int, int, int) [with Kernel_traits=Flash_fwd_kernel_traits<192, 64, 64, 4, false, false, cutlass::bfloat16_t, Flash_kernel_traits<192, 64, 64, 4, cutlass::bfloat16_t>>, Is_dropout=true, Is_causal=true, Is_even_N=true, Is_even_K=true, Return_softmax=true, Params=Flash_fwd_params, Is_Q_split_K=false]" 
(589): here
            instantiation of "void flash::compute_attn<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &) [with Kernel_traits=Flash_fwd_kernel_traits<192, 64, 64, 4, false, false, cutlass::bfloat16_t, Flash_kernel_traits<192, 64, 64, 4, cutlass::bfloat16_t>>, Is_dropout=true, Is_causal=true, Is_even_N=true, Is_even_K=true, Return_softmax=true, Params=Flash_fwd_params]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(15): here
            instantiation of "void flash_fwd_kernel<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax>(Flash_fwd_params) [with Kernel_traits=Flash_fwd_kernel_traits<192, 64, 64, 4, false, false, cutlass::bfloat16_t, Flash_kernel_traits<192, 64, 64, 4, cutlass::bfloat16_t>>, Is_dropout=true, Is_causal=true, Is_even_N=true, Is_even_K=true, Return_softmax=true]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(34): here
            instantiation of "void run_flash_fwd<Kernel_traits,Is_dropout,Is_causal>(Flash_fwd_params &, cudaStream_t) [with Kernel_traits=Flash_fwd_kernel_traits<192, 64, 64, 4, false, false, cutlass::bfloat16_t, Flash_kernel_traits<192, 64, 64, 4, cutlass::bfloat16_t>>, Is_dropout=true, Is_causal=true]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(183): here
            instantiation of "void run_mha_fwd_hdim192<T>(Flash_fwd_params &, cudaStream_t) [with T=cutlass::bfloat16_t]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_hdim192_bf16_sm80.cu(15): here

/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_kernel.h(140): warning: variable "MMA_M" was declared but never referenced
          detected during:
            instantiation of "void flash::compute_attn_1rowblock<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params,Is_Q_split_K>(const Params &, int, int, int) [with Kernel_traits=Flash_fwd_kernel_traits<192, 64, 64, 4, false, false, cutlass::bfloat16_t, Flash_kernel_traits<192, 64, 64, 4, cutlass::bfloat16_t>>, Is_dropout=true, Is_causal=true, Is_even_N=true, Is_even_K=true, Return_softmax=false, Params=Flash_fwd_params, Is_Q_split_K=false]" 
(589): here
            instantiation of "void flash::compute_attn<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &) [with Kernel_traits=Flash_fwd_kernel_traits<192, 64, 64, 4, false, false, cutlass::bfloat16_t, Flash_kernel_traits<192, 64, 64, 4, cutlass::bfloat16_t>>, Is_dropout=true, Is_causal=true, Is_even_N=true, Is_even_K=true, Return_softmax=false, Params=Flash_fwd_params]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(15): here
            instantiation of "void flash_fwd_kernel<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax>(Flash_fwd_params) [with Kernel_traits=Flash_fwd_kernel_traits<192, 64, 64, 4, false, false, cutlass::bfloat16_t, Flash_kernel_traits<192, 64, 64, 4, cutlass::bfloat16_t>>, Is_dropout=true, Is_causal=true, Is_even_N=true, Is_even_K=true, Return_softmax=false]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(34): here
            instantiation of "void run_flash_fwd<Kernel_traits,Is_dropout,Is_causal>(Flash_fwd_params &, cudaStream_t) [with Kernel_traits=Flash_fwd_kernel_traits<192, 64, 64, 4, false, false, cutlass::bfloat16_t, Flash_kernel_traits<192, 64, 64, 4, cutlass::bfloat16_t>>, Is_dropout=true, Is_causal=true]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(183): here
            instantiation of "void run_mha_fwd_hdim192<T>(Flash_fwd_params &, cudaStream_t) [with T=cutlass::bfloat16_t]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_hdim192_bf16_sm80.cu(15): here

/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_kernel.h(276): warning: variable "tQrQ" was declared but never referenced
          detected during:
            instantiation of "void flash::compute_attn_1rowblock<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params,Is_Q_split_K>(const Params &, int, int, int) [with Kernel_traits=Flash_fwd_kernel_traits<192, 64, 64, 4, false, false, cutlass::bfloat16_t, Flash_kernel_traits<192, 64, 64, 4, cutlass::bfloat16_t>>, Is_dropout=true, Is_causal=true, Is_even_N=true, Is_even_K=true, Return_softmax=false, Params=Flash_fwd_params, Is_Q_split_K=false]" 
(589): here
            instantiation of "void flash::compute_attn<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &) [with Kernel_traits=Flash_fwd_kernel_traits<192, 64, 64, 4, false, false, cutlass::bfloat16_t, Flash_kernel_traits<192, 64, 64, 4, cutlass::bfloat16_t>>, Is_dropout=true, Is_causal=true, Is_even_N=true, Is_even_K=true, Return_softmax=false, Params=Flash_fwd_params]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(15): here
            instantiation of "void flash_fwd_kernel<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax>(Flash_fwd_params) [with Kernel_traits=Flash_fwd_kernel_traits<192, 64, 64, 4, false, false, cutlass::bfloat16_t, Flash_kernel_traits<192, 64, 64, 4, cutlass::bfloat16_t>>, Is_dropout=true, Is_causal=true, Is_even_N=true, Is_even_K=true, Return_softmax=false]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(34): here
            instantiation of "void run_flash_fwd<Kernel_traits,Is_dropout,Is_causal>(Flash_fwd_params &, cudaStream_t) [with Kernel_traits=Flash_fwd_kernel_traits<192, 64, 64, 4, false, false, cutlass::bfloat16_t, Flash_kernel_traits<192, 64, 64, 4, cutlass::bfloat16_t>>, Is_dropout=true, Is_causal=true]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(183): here
            instantiation of "void run_mha_fwd_hdim192<T>(Flash_fwd_params &, cudaStream_t) [with T=cutlass::bfloat16_t]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_hdim192_bf16_sm80.cu(15): here

/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_kernel.h(140): warning: variable "MMA_M" was declared but never referenced
          detected during:
            instantiation of "void flash::compute_attn_1rowblock<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params,Is_Q_split_K>(const Params &, int, int, int) [with Kernel_traits=Flash_fwd_kernel_traits<192, 64, 64, 4, false, false, cutlass::bfloat16_t, Flash_kernel_traits<192, 64, 64, 4, cutlass::bfloat16_t>>, Is_dropout=true, Is_causal=true, Is_even_N=true, Is_even_K=false, Return_softmax=true, Params=Flash_fwd_params, Is_Q_split_K=false]" 
(589): here
            instantiation of "void flash::compute_attn<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &) [with Kernel_traits=Flash_fwd_kernel_traits<192, 64, 64, 4, false, false, cutlass::bfloat16_t, Flash_kernel_traits<192, 64, 64, 4, cutlass::bfloat16_t>>, Is_dropout=true, Is_causal=true, Is_even_N=true, Is_even_K=false, Return_softmax=true, Params=Flash_fwd_params]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(15): here
            instantiation of "void flash_fwd_kernel<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax>(Flash_fwd_params) [with Kernel_traits=Flash_fwd_kernel_traits<192, 64, 64, 4, false, false, cutlass::bfloat16_t, Flash_kernel_traits<192, 64, 64, 4, cutlass::bfloat16_t>>, Is_dropout=true, Is_causal=true, Is_even_N=true, Is_even_K=false, Return_softmax=true]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(34): here
            instantiation of "void run_flash_fwd<Kernel_traits,Is_dropout,Is_causal>(Flash_fwd_params &, cudaStream_t) [with Kernel_traits=Flash_fwd_kernel_traits<192, 64, 64, 4, false, false, cutlass::bfloat16_t, Flash_kernel_traits<192, 64, 64, 4, cutlass::bfloat16_t>>, Is_dropout=true, Is_causal=true]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(183): here
            instantiation of "void run_mha_fwd_hdim192<T>(Flash_fwd_params &, cudaStream_t) [with T=cutlass::bfloat16_t]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_hdim192_bf16_sm80.cu(15): here

/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_kernel.h(276): warning: variable "tQrQ" was declared but never referenced
          detected during:
            instantiation of "void flash::compute_attn_1rowblock<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params,Is_Q_split_K>(const Params &, int, int, int) [with Kernel_traits=Flash_fwd_kernel_traits<192, 64, 64, 4, false, false, cutlass::bfloat16_t, Flash_kernel_traits<192, 64, 64, 4, cutlass::bfloat16_t>>, Is_dropout=true, Is_causal=true, Is_even_N=true, Is_even_K=false, Return_softmax=true, Params=Flash_fwd_params, Is_Q_split_K=false]" 
(589): here
            instantiation of "void flash::compute_attn<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &) [with Kernel_traits=Flash_fwd_kernel_traits<192, 64, 64, 4, false, false, cutlass::bfloat16_t, Flash_kernel_traits<192, 64, 64, 4, cutlass::bfloat16_t>>, Is_dropout=true, Is_causal=true, Is_even_N=true, Is_even_K=false, Return_softmax=true, Params=Flash_fwd_params]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(15): here
            instantiation of "void flash_fwd_kernel<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax>(Flash_fwd_params) [with Kernel_traits=Flash_fwd_kernel_traits<192, 64, 64, 4, false, false, cutlass::bfloat16_t, Flash_kernel_traits<192, 64, 64, 4, cutlass::bfloat16_t>>, Is_dropout=true, Is_causal=true, Is_even_N=true, Is_even_K=false, Return_softmax=true]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(34): here
            instantiation of "void run_flash_fwd<Kernel_traits,Is_dropout,Is_causal>(Flash_fwd_params &, cudaStream_t) [with Kernel_traits=Flash_fwd_kernel_traits<192, 64, 64, 4, false, false, cutlass::bfloat16_t, Flash_kernel_traits<192, 64, 64, 4, cutlass::bfloat16_t>>, Is_dropout=true, Is_causal=true]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(183): here
            instantiation of "void run_mha_fwd_hdim192<T>(Flash_fwd_params &, cudaStream_t) [with T=cutlass::bfloat16_t]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_hdim192_bf16_sm80.cu(15): here

/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_kernel.h(140): warning: variable "MMA_M" was declared but never referenced
          detected during:
            instantiation of "void flash::compute_attn_1rowblock<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params,Is_Q_split_K>(const Params &, int, int, int) [with Kernel_traits=Flash_fwd_kernel_traits<192, 64, 64, 4, false, false, cutlass::bfloat16_t, Flash_kernel_traits<192, 64, 64, 4, cutlass::bfloat16_t>>, Is_dropout=true, Is_causal=true, Is_even_N=true, Is_even_K=false, Return_softmax=false, Params=Flash_fwd_params, Is_Q_split_K=false]" 
(589): here
            instantiation of "void flash::compute_attn<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &) [with Kernel_traits=Flash_fwd_kernel_traits<192, 64, 64, 4, false, false, cutlass::bfloat16_t, Flash_kernel_traits<192, 64, 64, 4, cutlass::bfloat16_t>>, Is_dropout=true, Is_causal=true, Is_even_N=true, Is_even_K=false, Return_softmax=false, Params=Flash_fwd_params]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(15): here
            instantiation of "void flash_fwd_kernel<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax>(Flash_fwd_params) [with Kernel_traits=Flash_fwd_kernel_traits<192, 64, 64, 4, false, false, cutlass::bfloat16_t, Flash_kernel_traits<192, 64, 64, 4, cutlass::bfloat16_t>>, Is_dropout=true, Is_causal=true, Is_even_N=true, Is_even_K=false, Return_softmax=false]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(34): here
            instantiation of "void run_flash_fwd<Kernel_traits,Is_dropout,Is_causal>(Flash_fwd_params &, cudaStream_t) [with Kernel_traits=Flash_fwd_kernel_traits<192, 64, 64, 4, false, false, cutlass::bfloat16_t, Flash_kernel_traits<192, 64, 64, 4, cutlass::bfloat16_t>>, Is_dropout=true, Is_causal=true]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(183): here
            instantiation of "void run_mha_fwd_hdim192<T>(Flash_fwd_params &, cudaStream_t) [with T=cutlass::bfloat16_t]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_hdim192_bf16_sm80.cu(15): here

/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_kernel.h(276): warning: variable "tQrQ" was declared but never referenced
          detected during:
            instantiation of "void flash::compute_attn_1rowblock<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params,Is_Q_split_K>(const Params &, int, int, int) [with Kernel_traits=Flash_fwd_kernel_traits<192, 64, 64, 4, false, false, cutlass::bfloat16_t, Flash_kernel_traits<192, 64, 64, 4, cutlass::bfloat16_t>>, Is_dropout=true, Is_causal=true, Is_even_N=true, Is_even_K=false, Return_softmax=false, Params=Flash_fwd_params, Is_Q_split_K=false]" 
(589): here
            instantiation of "void flash::compute_attn<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &) [with Kernel_traits=Flash_fwd_kernel_traits<192, 64, 64, 4, false, false, cutlass::bfloat16_t, Flash_kernel_traits<192, 64, 64, 4, cutlass::bfloat16_t>>, Is_dropout=true, Is_causal=true, Is_even_N=true, Is_even_K=false, Return_softmax=false, Params=Flash_fwd_params]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(15): here
            instantiation of "void flash_fwd_kernel<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax>(Flash_fwd_params) [with Kernel_traits=Flash_fwd_kernel_traits<192, 64, 64, 4, false, false, cutlass::bfloat16_t, Flash_kernel_traits<192, 64, 64, 4, cutlass::bfloat16_t>>, Is_dropout=true, Is_causal=true, Is_even_N=true, Is_even_K=false, Return_softmax=false]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(34): here
            instantiation of "void run_flash_fwd<Kernel_traits,Is_dropout,Is_causal>(Flash_fwd_params &, cudaStream_t) [with Kernel_traits=Flash_fwd_kernel_traits<192, 64, 64, 4, false, false, cutlass::bfloat16_t, Flash_kernel_traits<192, 64, 64, 4, cutlass::bfloat16_t>>, Is_dropout=true, Is_causal=true]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(183): here
            instantiation of "void run_mha_fwd_hdim192<T>(Flash_fwd_params &, cudaStream_t) [with T=cutlass::bfloat16_t]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_hdim192_bf16_sm80.cu(15): here

/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_kernel.h(140): warning: variable "MMA_M" was declared but never referenced
          detected during:
            instantiation of "void flash::compute_attn_1rowblock<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params,Is_Q_split_K>(const Params &, int, int, int) [with Kernel_traits=Flash_fwd_kernel_traits<192, 64, 64, 4, false, false, cutlass::bfloat16_t, Flash_kernel_traits<192, 64, 64, 4, cutlass::bfloat16_t>>, Is_dropout=true, Is_causal=true, Is_even_N=false, Is_even_K=true, Return_softmax=true, Params=Flash_fwd_params, Is_Q_split_K=false]" 
(589): here
            instantiation of "void flash::compute_attn<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &) [with Kernel_traits=Flash_fwd_kernel_traits<192, 64, 64, 4, false, false, cutlass::bfloat16_t, Flash_kernel_traits<192, 64, 64, 4, cutlass::bfloat16_t>>, Is_dropout=true, Is_causal=true, Is_even_N=false, Is_even_K=true, Return_softmax=true, Params=Flash_fwd_params]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(15): here
            instantiation of "void flash_fwd_kernel<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax>(Flash_fwd_params) [with Kernel_traits=Flash_fwd_kernel_traits<192, 64, 64, 4, false, false, cutlass::bfloat16_t, Flash_kernel_traits<192, 64, 64, 4, cutlass::bfloat16_t>>, Is_dropout=true, Is_causal=true, Is_even_N=false, Is_even_K=true, Return_softmax=true]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(34): here
            instantiation of "void run_flash_fwd<Kernel_traits,Is_dropout,Is_causal>(Flash_fwd_params &, cudaStream_t) [with Kernel_traits=Flash_fwd_kernel_traits<192, 64, 64, 4, false, false, cutlass::bfloat16_t, Flash_kernel_traits<192, 64, 64, 4, cutlass::bfloat16_t>>, Is_dropout=true, Is_causal=true]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(183): here
            instantiation of "void run_mha_fwd_hdim192<T>(Flash_fwd_params &, cudaStream_t) [with T=cutlass::bfloat16_t]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_hdim192_bf16_sm80.cu(15): here

/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_kernel.h(276): warning: variable "tQrQ" was declared but never referenced
          detected during:
            instantiation of "void flash::compute_attn_1rowblock<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params,Is_Q_split_K>(const Params &, int, int, int) [with Kernel_traits=Flash_fwd_kernel_traits<192, 64, 64, 4, false, false, cutlass::bfloat16_t, Flash_kernel_traits<192, 64, 64, 4, cutlass::bfloat16_t>>, Is_dropout=true, Is_causal=true, Is_even_N=false, Is_even_K=true, Return_softmax=true, Params=Flash_fwd_params, Is_Q_split_K=false]" 
(589): here
            instantiation of "void flash::compute_attn<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &) [with Kernel_traits=Flash_fwd_kernel_traits<192, 64, 64, 4, false, false, cutlass::bfloat16_t, Flash_kernel_traits<192, 64, 64, 4, cutlass::bfloat16_t>>, Is_dropout=true, Is_causal=true, Is_even_N=false, Is_even_K=true, Return_softmax=true, Params=Flash_fwd_params]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(15): here
            instantiation of "void flash_fwd_kernel<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax>(Flash_fwd_params) [with Kernel_traits=Flash_fwd_kernel_traits<192, 64, 64, 4, false, false, cutlass::bfloat16_t, Flash_kernel_traits<192, 64, 64, 4, cutlass::bfloat16_t>>, Is_dropout=true, Is_causal=true, Is_even_N=false, Is_even_K=true, Return_softmax=true]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(34): here
            instantiation of "void run_flash_fwd<Kernel_traits,Is_dropout,Is_causal>(Flash_fwd_params &, cudaStream_t) [with Kernel_traits=Flash_fwd_kernel_traits<192, 64, 64, 4, false, false, cutlass::bfloat16_t, Flash_kernel_traits<192, 64, 64, 4, cutlass::bfloat16_t>>, Is_dropout=true, Is_causal=true]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(183): here
            instantiation of "void run_mha_fwd_hdim192<T>(Flash_fwd_params &, cudaStream_t) [with T=cutlass::bfloat16_t]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_hdim192_bf16_sm80.cu(15): here

/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_kernel.h(140): warning: variable "MMA_M" was declared but never referenced
          detected during:
            instantiation of "void flash::compute_attn_1rowblock<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params,Is_Q_split_K>(const Params &, int, int, int) [with Kernel_traits=Flash_fwd_kernel_traits<192, 64, 64, 4, false, false, cutlass::bfloat16_t, Flash_kernel_traits<192, 64, 64, 4, cutlass::bfloat16_t>>, Is_dropout=true, Is_causal=true, Is_even_N=false, Is_even_K=true, Return_softmax=false, Params=Flash_fwd_params, Is_Q_split_K=false]" 
(589): here
            instantiation of "void flash::compute_attn<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &) [with Kernel_traits=Flash_fwd_kernel_traits<192, 64, 64, 4, false, false, cutlass::bfloat16_t, Flash_kernel_traits<192, 64, 64, 4, cutlass::bfloat16_t>>, Is_dropout=true, Is_causal=true, Is_even_N=false, Is_even_K=true, Return_softmax=false, Params=Flash_fwd_params]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(15): here
            instantiation of "void flash_fwd_kernel<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax>(Flash_fwd_params) [with Kernel_traits=Flash_fwd_kernel_traits<192, 64, 64, 4, false, false, cutlass::bfloat16_t, Flash_kernel_traits<192, 64, 64, 4, cutlass::bfloat16_t>>, Is_dropout=true, Is_causal=true, Is_even_N=false, Is_even_K=true, Return_softmax=false]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(34): here
            instantiation of "void run_flash_fwd<Kernel_traits,Is_dropout,Is_causal>(Flash_fwd_params &, cudaStream_t) [with Kernel_traits=Flash_fwd_kernel_traits<192, 64, 64, 4, false, false, cutlass::bfloat16_t, Flash_kernel_traits<192, 64, 64, 4, cutlass::bfloat16_t>>, Is_dropout=true, Is_causal=true]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(183): here
            instantiation of "void run_mha_fwd_hdim192<T>(Flash_fwd_params &, cudaStream_t) [with T=cutlass::bfloat16_t]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_hdim192_bf16_sm80.cu(15): here

/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_kernel.h(276): warning: variable "tQrQ" was declared but never referenced
          detected during:
            instantiation of "void flash::compute_attn_1rowblock<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params,Is_Q_split_K>(const Params &, int, int, int) [with Kernel_traits=Flash_fwd_kernel_traits<192, 64, 64, 4, false, false, cutlass::bfloat16_t, Flash_kernel_traits<192, 64, 64, 4, cutlass::bfloat16_t>>, Is_dropout=true, Is_causal=true, Is_even_N=false, Is_even_K=true, Return_softmax=false, Params=Flash_fwd_params, Is_Q_split_K=false]" 
(589): here
            instantiation of "void flash::compute_attn<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &) [with Kernel_traits=Flash_fwd_kernel_traits<192, 64, 64, 4, false, false, cutlass::bfloat16_t, Flash_kernel_traits<192, 64, 64, 4, cutlass::bfloat16_t>>, Is_dropout=true, Is_causal=true, Is_even_N=false, Is_even_K=true, Return_softmax=false, Params=Flash_fwd_params]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(15): here
            instantiation of "void flash_fwd_kernel<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax>(Flash_fwd_params) [with Kernel_traits=Flash_fwd_kernel_traits<192, 64, 64, 4, false, false, cutlass::bfloat16_t, Flash_kernel_traits<192, 64, 64, 4, cutlass::bfloat16_t>>, Is_dropout=true, Is_causal=true, Is_even_N=false, Is_even_K=true, Return_softmax=false]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(34): here
            instantiation of "void run_flash_fwd<Kernel_traits,Is_dropout,Is_causal>(Flash_fwd_params &, cudaStream_t) [with Kernel_traits=Flash_fwd_kernel_traits<192, 64, 64, 4, false, false, cutlass::bfloat16_t, Flash_kernel_traits<192, 64, 64, 4, cutlass::bfloat16_t>>, Is_dropout=true, Is_causal=true]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(183): here
            instantiation of "void run_mha_fwd_hdim192<T>(Flash_fwd_params &, cudaStream_t) [with T=cutlass::bfloat16_t]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_hdim192_bf16_sm80.cu(15): here

/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_kernel.h(140): warning: variable "MMA_M" was declared but never referenced
          detected during:
            instantiation of "void flash::compute_attn_1rowblock<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params,Is_Q_split_K>(const Params &, int, int, int) [with Kernel_traits=Flash_fwd_kernel_traits<192, 64, 64, 4, false, false, cutlass::bfloat16_t, Flash_kernel_traits<192, 64, 64, 4, cutlass::bfloat16_t>>, Is_dropout=true, Is_causal=true, Is_even_N=false, Is_even_K=false, Return_softmax=true, Params=Flash_fwd_params, Is_Q_split_K=false]" 
(589): here
            instantiation of "void flash::compute_attn<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &) [with Kernel_traits=Flash_fwd_kernel_traits<192, 64, 64, 4, false, false, cutlass::bfloat16_t, Flash_kernel_traits<192, 64, 64, 4, cutlass::bfloat16_t>>, Is_dropout=true, Is_causal=true, Is_even_N=false, Is_even_K=false, Return_softmax=true, Params=Flash_fwd_params]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(15): here
            instantiation of "void flash_fwd_kernel<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax>(Flash_fwd_params) [with Kernel_traits=Flash_fwd_kernel_traits<192, 64, 64, 4, false, false, cutlass::bfloat16_t, Flash_kernel_traits<192, 64, 64, 4, cutlass::bfloat16_t>>, Is_dropout=true, Is_causal=true, Is_even_N=false, Is_even_K=false, Return_softmax=true]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(34): here
            instantiation of "void run_flash_fwd<Kernel_traits,Is_dropout,Is_causal>(Flash_fwd_params &, cudaStream_t) [with Kernel_traits=Flash_fwd_kernel_traits<192, 64, 64, 4, false, false, cutlass::bfloat16_t, Flash_kernel_traits<192, 64, 64, 4, cutlass::bfloat16_t>>, Is_dropout=true, Is_causal=true]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(183): here
            instantiation of "void run_mha_fwd_hdim192<T>(Flash_fwd_params &, cudaStream_t) [with T=cutlass::bfloat16_t]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_hdim192_bf16_sm80.cu(15): here

/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_kernel.h(276): warning: variable "tQrQ" was declared but never referenced
          detected during:
            instantiation of "void flash::compute_attn_1rowblock<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params,Is_Q_split_K>(const Params &, int, int, int) [with Kernel_traits=Flash_fwd_kernel_traits<192, 64, 64, 4, false, false, cutlass::bfloat16_t, Flash_kernel_traits<192, 64, 64, 4, cutlass::bfloat16_t>>, Is_dropout=true, Is_causal=true, Is_even_N=false, Is_even_K=false, Return_softmax=true, Params=Flash_fwd_params, Is_Q_split_K=false]" 
(589): here
            instantiation of "void flash::compute_attn<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &) [with Kernel_traits=Flash_fwd_kernel_traits<192, 64, 64, 4, false, false, cutlass::bfloat16_t, Flash_kernel_traits<192, 64, 64, 4, cutlass::bfloat16_t>>, Is_dropout=true, Is_causal=true, Is_even_N=false, Is_even_K=false, Return_softmax=true, Params=Flash_fwd_params]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(15): here
            instantiation of "void flash_fwd_kernel<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax>(Flash_fwd_params) [with Kernel_traits=Flash_fwd_kernel_traits<192, 64, 64, 4, false, false, cutlass::bfloat16_t, Flash_kernel_traits<192, 64, 64, 4, cutlass::bfloat16_t>>, Is_dropout=true, Is_causal=true, Is_even_N=false, Is_even_K=false, Return_softmax=true]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(34): here
            instantiation of "void run_flash_fwd<Kernel_traits,Is_dropout,Is_causal>(Flash_fwd_params &, cudaStream_t) [with Kernel_traits=Flash_fwd_kernel_traits<192, 64, 64, 4, false, false, cutlass::bfloat16_t, Flash_kernel_traits<192, 64, 64, 4, cutlass::bfloat16_t>>, Is_dropout=true, Is_causal=true]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(183): here
            instantiation of "void run_mha_fwd_hdim192<T>(Flash_fwd_params &, cudaStream_t) [with T=cutlass::bfloat16_t]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_hdim192_bf16_sm80.cu(15): here

/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_kernel.h(140): warning: variable "MMA_M" was declared but never referenced
          detected during:
            instantiation of "void flash::compute_attn_1rowblock<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params,Is_Q_split_K>(const Params &, int, int, int) [with Kernel_traits=Flash_fwd_kernel_traits<192, 64, 64, 4, false, false, cutlass::bfloat16_t, Flash_kernel_traits<192, 64, 64, 4, cutlass::bfloat16_t>>, Is_dropout=true, Is_causal=true, Is_even_N=false, Is_even_K=false, Return_softmax=false, Params=Flash_fwd_params, Is_Q_split_K=false]" 
(589): here
            instantiation of "void flash::compute_attn<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &) [with Kernel_traits=Flash_fwd_kernel_traits<192, 64, 64, 4, false, false, cutlass::bfloat16_t, Flash_kernel_traits<192, 64, 64, 4, cutlass::bfloat16_t>>, Is_dropout=true, Is_causal=true, Is_even_N=false, Is_even_K=false, Return_softmax=false, Params=Flash_fwd_params]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(15): here
            instantiation of "void flash_fwd_kernel<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax>(Flash_fwd_params) [with Kernel_traits=Flash_fwd_kernel_traits<192, 64, 64, 4, false, false, cutlass::bfloat16_t, Flash_kernel_traits<192, 64, 64, 4, cutlass::bfloat16_t>>, Is_dropout=true, Is_causal=true, Is_even_N=false, Is_even_K=false, Return_softmax=false]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(34): here
            instantiation of "void run_flash_fwd<Kernel_traits,Is_dropout,Is_causal>(Flash_fwd_params &, cudaStream_t) [with Kernel_traits=Flash_fwd_kernel_traits<192, 64, 64, 4, false, false, cutlass::bfloat16_t, Flash_kernel_traits<192, 64, 64, 4, cutlass::bfloat16_t>>, Is_dropout=true, Is_causal=true]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(183): here
            instantiation of "void run_mha_fwd_hdim192<T>(Flash_fwd_params &, cudaStream_t) [with T=cutlass::bfloat16_t]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_hdim192_bf16_sm80.cu(15): here

/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_kernel.h(276): warning: variable "tQrQ" was declared but never referenced
          detected during:
            instantiation of "void flash::compute_attn_1rowblock<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params,Is_Q_split_K>(const Params &, int, int, int) [with Kernel_traits=Flash_fwd_kernel_traits<192, 64, 64, 4, false, false, cutlass::bfloat16_t, Flash_kernel_traits<192, 64, 64, 4, cutlass::bfloat16_t>>, Is_dropout=true, Is_causal=true, Is_even_N=false, Is_even_K=false, Return_softmax=false, Params=Flash_fwd_params, Is_Q_split_K=false]" 
(589): here
            instantiation of "void flash::compute_attn<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &) [with Kernel_traits=Flash_fwd_kernel_traits<192, 64, 64, 4, false, false, cutlass::bfloat16_t, Flash_kernel_traits<192, 64, 64, 4, cutlass::bfloat16_t>>, Is_dropout=true, Is_causal=true, Is_even_N=false, Is_even_K=false, Return_softmax=false, Params=Flash_fwd_params]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(15): here
            instantiation of "void flash_fwd_kernel<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax>(Flash_fwd_params) [with Kernel_traits=Flash_fwd_kernel_traits<192, 64, 64, 4, false, false, cutlass::bfloat16_t, Flash_kernel_traits<192, 64, 64, 4, cutlass::bfloat16_t>>, Is_dropout=true, Is_causal=true, Is_even_N=false, Is_even_K=false, Return_softmax=false]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(34): here
            instantiation of "void run_flash_fwd<Kernel_traits,Is_dropout,Is_causal>(Flash_fwd_params &, cudaStream_t) [with Kernel_traits=Flash_fwd_kernel_traits<192, 64, 64, 4, false, false, cutlass::bfloat16_t, Flash_kernel_traits<192, 64, 64, 4, cutlass::bfloat16_t>>, Is_dropout=true, Is_causal=true]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(183): here
            instantiation of "void run_mha_fwd_hdim192<T>(Flash_fwd_params &, cudaStream_t) [with T=cutlass::bfloat16_t]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_hdim192_bf16_sm80.cu(15): here

/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_kernel.h(140): warning: variable "MMA_M" was declared but never referenced
          detected during:
            instantiation of "void flash::compute_attn_1rowblock<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params,Is_Q_split_K>(const Params &, int, int, int) [with Kernel_traits=Flash_fwd_kernel_traits<192, 64, 64, 4, false, false, cutlass::bfloat16_t, Flash_kernel_traits<192, 64, 64, 4, cutlass::bfloat16_t>>, Is_dropout=true, Is_causal=false, Is_even_N=true, Is_even_K=true, Return_softmax=true, Params=Flash_fwd_params, Is_Q_split_K=false]" 
(589): here
            instantiation of "void flash::compute_attn<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &) [with Kernel_traits=Flash_fwd_kernel_traits<192, 64, 64, 4, false, false, cutlass::bfloat16_t, Flash_kernel_traits<192, 64, 64, 4, cutlass::bfloat16_t>>, Is_dropout=true, Is_causal=false, Is_even_N=true, Is_even_K=true, Return_softmax=true, Params=Flash_fwd_params]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(15): here
            instantiation of "void flash_fwd_kernel<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax>(Flash_fwd_params) [with Kernel_traits=Flash_fwd_kernel_traits<192, 64, 64, 4, false, false, cutlass::bfloat16_t, Flash_kernel_traits<192, 64, 64, 4, cutlass::bfloat16_t>>, Is_dropout=true, Is_causal=false, Is_even_N=true, Is_even_K=true, Return_softmax=true]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(34): here
            instantiation of "void run_flash_fwd<Kernel_traits,Is_dropout,Is_causal>(Flash_fwd_params &, cudaStream_t) [with Kernel_traits=Flash_fwd_kernel_traits<192, 64, 64, 4, false, false, cutlass::bfloat16_t, Flash_kernel_traits<192, 64, 64, 4, cutlass::bfloat16_t>>, Is_dropout=true, Is_causal=false]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(183): here
            instantiation of "void run_mha_fwd_hdim192<T>(Flash_fwd_params &, cudaStream_t) [with T=cutlass::bfloat16_t]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_hdim192_bf16_sm80.cu(15): here

/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_kernel.h(276): warning: variable "tQrQ" was declared but never referenced
          detected during:
            instantiation of "void flash::compute_attn_1rowblock<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params,Is_Q_split_K>(const Params &, int, int, int) [with Kernel_traits=Flash_fwd_kernel_traits<192, 64, 64, 4, false, false, cutlass::bfloat16_t, Flash_kernel_traits<192, 64, 64, 4, cutlass::bfloat16_t>>, Is_dropout=true, Is_causal=false, Is_even_N=true, Is_even_K=true, Return_softmax=true, Params=Flash_fwd_params, Is_Q_split_K=false]" 
(589): here
            instantiation of "void flash::compute_attn<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &) [with Kernel_traits=Flash_fwd_kernel_traits<192, 64, 64, 4, false, false, cutlass::bfloat16_t, Flash_kernel_traits<192, 64, 64, 4, cutlass::bfloat16_t>>, Is_dropout=true, Is_causal=false, Is_even_N=true, Is_even_K=true, Return_softmax=true, Params=Flash_fwd_params]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(15): here
            instantiation of "void flash_fwd_kernel<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax>(Flash_fwd_params) [with Kernel_traits=Flash_fwd_kernel_traits<192, 64, 64, 4, false, false, cutlass::bfloat16_t, Flash_kernel_traits<192, 64, 64, 4, cutlass::bfloat16_t>>, Is_dropout=true, Is_causal=false, Is_even_N=true, Is_even_K=true, Return_softmax=true]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(34): here
            instantiation of "void run_flash_fwd<Kernel_traits,Is_dropout,Is_causal>(Flash_fwd_params &, cudaStream_t) [with Kernel_traits=Flash_fwd_kernel_traits<192, 64, 64, 4, false, false, cutlass::bfloat16_t, Flash_kernel_traits<192, 64, 64, 4, cutlass::bfloat16_t>>, Is_dropout=true, Is_causal=false]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(183): here
            instantiation of "void run_mha_fwd_hdim192<T>(Flash_fwd_params &, cudaStream_t) [with T=cutlass::bfloat16_t]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_hdim192_bf16_sm80.cu(15): here

/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_kernel.h(140): warning: variable "MMA_M" was declared but never referenced
          detected during:
            instantiation of "void flash::compute_attn_1rowblock<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params,Is_Q_split_K>(const Params &, int, int, int) [with Kernel_traits=Flash_fwd_kernel_traits<192, 64, 64, 4, false, false, cutlass::bfloat16_t, Flash_kernel_traits<192, 64, 64, 4, cutlass::bfloat16_t>>, Is_dropout=true, Is_causal=false, Is_even_N=true, Is_even_K=true, Return_softmax=false, Params=Flash_fwd_params, Is_Q_split_K=false]" 
(589): here
            instantiation of "void flash::compute_attn<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &) [with Kernel_traits=Flash_fwd_kernel_traits<192, 64, 64, 4, false, false, cutlass::bfloat16_t, Flash_kernel_traits<192, 64, 64, 4, cutlass::bfloat16_t>>, Is_dropout=true, Is_causal=false, Is_even_N=true, Is_even_K=true, Return_softmax=false, Params=Flash_fwd_params]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(15): here
            instantiation of "void flash_fwd_kernel<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax>(Flash_fwd_params) [with Kernel_traits=Flash_fwd_kernel_traits<192, 64, 64, 4, false, false, cutlass::bfloat16_t, Flash_kernel_traits<192, 64, 64, 4, cutlass::bfloat16_t>>, Is_dropout=true, Is_causal=false, Is_even_N=true, Is_even_K=true, Return_softmax=false]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(34): here
            instantiation of "void run_flash_fwd<Kernel_traits,Is_dropout,Is_causal>(Flash_fwd_params &, cudaStream_t) [with Kernel_traits=Flash_fwd_kernel_traits<192, 64, 64, 4, false, false, cutlass::bfloat16_t, Flash_kernel_traits<192, 64, 64, 4, cutlass::bfloat16_t>>, Is_dropout=true, Is_causal=false]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(183): here
            instantiation of "void run_mha_fwd_hdim192<T>(Flash_fwd_params &, cudaStream_t) [with T=cutlass::bfloat16_t]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_hdim192_bf16_sm80.cu(15): here

/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_kernel.h(276): warning: variable "tQrQ" was declared but never referenced
          detected during:
            instantiation of "void flash::compute_attn_1rowblock<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params,Is_Q_split_K>(const Params &, int, int, int) [with Kernel_traits=Flash_fwd_kernel_traits<192, 64, 64, 4, false, false, cutlass::bfloat16_t, Flash_kernel_traits<192, 64, 64, 4, cutlass::bfloat16_t>>, Is_dropout=true, Is_causal=false, Is_even_N=true, Is_even_K=true, Return_softmax=false, Params=Flash_fwd_params, Is_Q_split_K=false]" 
(589): here
            instantiation of "void flash::compute_attn<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &) [with Kernel_traits=Flash_fwd_kernel_traits<192, 64, 64, 4, false, false, cutlass::bfloat16_t, Flash_kernel_traits<192, 64, 64, 4, cutlass::bfloat16_t>>, Is_dropout=true, Is_causal=false, Is_even_N=true, Is_even_K=true, Return_softmax=false, Params=Flash_fwd_params]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(15): here
            instantiation of "void flash_fwd_kernel<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax>(Flash_fwd_params) [with Kernel_traits=Flash_fwd_kernel_traits<192, 64, 64, 4, false, false, cutlass::bfloat16_t, Flash_kernel_traits<192, 64, 64, 4, cutlass::bfloat16_t>>, Is_dropout=true, Is_causal=false, Is_even_N=true, Is_even_K=true, Return_softmax=false]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(34): here
            instantiation of "void run_flash_fwd<Kernel_traits,Is_dropout,Is_causal>(Flash_fwd_params &, cudaStream_t) [with Kernel_traits=Flash_fwd_kernel_traits<192, 64, 64, 4, false, false, cutlass::bfloat16_t, Flash_kernel_traits<192, 64, 64, 4, cutlass::bfloat16_t>>, Is_dropout=true, Is_causal=false]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(183): here
            instantiation of "void run_mha_fwd_hdim192<T>(Flash_fwd_params &, cudaStream_t) [with T=cutlass::bfloat16_t]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_hdim192_bf16_sm80.cu(15): here

/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_kernel.h(140): warning: variable "MMA_M" was declared but never referenced
          detected during:
            instantiation of "void flash::compute_attn_1rowblock<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params,Is_Q_split_K>(const Params &, int, int, int) [with Kernel_traits=Flash_fwd_kernel_traits<192, 64, 64, 4, false, false, cutlass::bfloat16_t, Flash_kernel_traits<192, 64, 64, 4, cutlass::bfloat16_t>>, Is_dropout=true, Is_causal=false, Is_even_N=true, Is_even_K=false, Return_softmax=true, Params=Flash_fwd_params, Is_Q_split_K=false]" 
(589): here
            instantiation of "void flash::compute_attn<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &) [with Kernel_traits=Flash_fwd_kernel_traits<192, 64, 64, 4, false, false, cutlass::bfloat16_t, Flash_kernel_traits<192, 64, 64, 4, cutlass::bfloat16_t>>, Is_dropout=true, Is_causal=false, Is_even_N=true, Is_even_K=false, Return_softmax=true, Params=Flash_fwd_params]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(15): here
            instantiation of "void flash_fwd_kernel<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax>(Flash_fwd_params) [with Kernel_traits=Flash_fwd_kernel_traits<192, 64, 64, 4, false, false, cutlass::bfloat16_t, Flash_kernel_traits<192, 64, 64, 4, cutlass::bfloat16_t>>, Is_dropout=true, Is_causal=false, Is_even_N=true, Is_even_K=false, Return_softmax=true]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(34): here
            instantiation of "void run_flash_fwd<Kernel_traits,Is_dropout,Is_causal>(Flash_fwd_params &, cudaStream_t) [with Kernel_traits=Flash_fwd_kernel_traits<192, 64, 64, 4, false, false, cutlass::bfloat16_t, Flash_kernel_traits<192, 64, 64, 4, cutlass::bfloat16_t>>, Is_dropout=true, Is_causal=false]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(183): here
            instantiation of "void run_mha_fwd_hdim192<T>(Flash_fwd_params &, cudaStream_t) [with T=cutlass::bfloat16_t]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_hdim192_bf16_sm80.cu(15): here

/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_kernel.h(276): warning: variable "tQrQ" was declared but never referenced
          detected during:
            instantiation of "void flash::compute_attn_1rowblock<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params,Is_Q_split_K>(const Params &, int, int, int) [with Kernel_traits=Flash_fwd_kernel_traits<192, 64, 64, 4, false, false, cutlass::bfloat16_t, Flash_kernel_traits<192, 64, 64, 4, cutlass::bfloat16_t>>, Is_dropout=true, Is_causal=false, Is_even_N=true, Is_even_K=false, Return_softmax=true, Params=Flash_fwd_params, Is_Q_split_K=false]" 
(589): here
            instantiation of "void flash::compute_attn<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &) [with Kernel_traits=Flash_fwd_kernel_traits<192, 64, 64, 4, false, false, cutlass::bfloat16_t, Flash_kernel_traits<192, 64, 64, 4, cutlass::bfloat16_t>>, Is_dropout=true, Is_causal=false, Is_even_N=true, Is_even_K=false, Return_softmax=true, Params=Flash_fwd_params]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(15): here
            instantiation of "void flash_fwd_kernel<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax>(Flash_fwd_params) [with Kernel_traits=Flash_fwd_kernel_traits<192, 64, 64, 4, false, false, cutlass::bfloat16_t, Flash_kernel_traits<192, 64, 64, 4, cutlass::bfloat16_t>>, Is_dropout=true, Is_causal=false, Is_even_N=true, Is_even_K=false, Return_softmax=true]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(34): here
            instantiation of "void run_flash_fwd<Kernel_traits,Is_dropout,Is_causal>(Flash_fwd_params &, cudaStream_t) [with Kernel_traits=Flash_fwd_kernel_traits<192, 64, 64, 4, false, false, cutlass::bfloat16_t, Flash_kernel_traits<192, 64, 64, 4, cutlass::bfloat16_t>>, Is_dropout=true, Is_causal=false]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(183): here
            instantiation of "void run_mha_fwd_hdim192<T>(Flash_fwd_params &, cudaStream_t) [with T=cutlass::bfloat16_t]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_hdim192_bf16_sm80.cu(15): here

/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_kernel.h(140): warning: variable "MMA_M" was declared but never referenced
          detected during:
            instantiation of "void flash::compute_attn_1rowblock<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params,Is_Q_split_K>(const Params &, int, int, int) [with Kernel_traits=Flash_fwd_kernel_traits<192, 64, 64, 4, false, false, cutlass::bfloat16_t, Flash_kernel_traits<192, 64, 64, 4, cutlass::bfloat16_t>>, Is_dropout=true, Is_causal=false, Is_even_N=true, Is_even_K=false, Return_softmax=false, Params=Flash_fwd_params, Is_Q_split_K=false]" 
(589): here
            instantiation of "void flash::compute_attn<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &) [with Kernel_traits=Flash_fwd_kernel_traits<192, 64, 64, 4, false, false, cutlass::bfloat16_t, Flash_kernel_traits<192, 64, 64, 4, cutlass::bfloat16_t>>, Is_dropout=true, Is_causal=false, Is_even_N=true, Is_even_K=false, Return_softmax=false, Params=Flash_fwd_params]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(15): here
            instantiation of "void flash_fwd_kernel<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax>(Flash_fwd_params) [with Kernel_traits=Flash_fwd_kernel_traits<192, 64, 64, 4, false, false, cutlass::bfloat16_t, Flash_kernel_traits<192, 64, 64, 4, cutlass::bfloat16_t>>, Is_dropout=true, Is_causal=false, Is_even_N=true, Is_even_K=false, Return_softmax=false]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(34): here
            instantiation of "void run_flash_fwd<Kernel_traits,Is_dropout,Is_causal>(Flash_fwd_params &, cudaStream_t) [with Kernel_traits=Flash_fwd_kernel_traits<192, 64, 64, 4, false, false, cutlass::bfloat16_t, Flash_kernel_traits<192, 64, 64, 4, cutlass::bfloat16_t>>, Is_dropout=true, Is_causal=false]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(183): here
            instantiation of "void run_mha_fwd_hdim192<T>(Flash_fwd_params &, cudaStream_t) [with T=cutlass::bfloat16_t]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_hdim192_bf16_sm80.cu(15): here

/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_kernel.h(276): warning: variable "tQrQ" was declared but never referenced
          detected during:
            instantiation of "void flash::compute_attn_1rowblock<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params,Is_Q_split_K>(const Params &, int, int, int) [with Kernel_traits=Flash_fwd_kernel_traits<192, 64, 64, 4, false, false, cutlass::bfloat16_t, Flash_kernel_traits<192, 64, 64, 4, cutlass::bfloat16_t>>, Is_dropout=true, Is_causal=false, Is_even_N=true, Is_even_K=false, Return_softmax=false, Params=Flash_fwd_params, Is_Q_split_K=false]" 
(589): here
            instantiation of "void flash::compute_attn<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &) [with Kernel_traits=Flash_fwd_kernel_traits<192, 64, 64, 4, false, false, cutlass::bfloat16_t, Flash_kernel_traits<192, 64, 64, 4, cutlass::bfloat16_t>>, Is_dropout=true, Is_causal=false, Is_even_N=true, Is_even_K=false, Return_softmax=false, Params=Flash_fwd_params]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(15): here
            instantiation of "void flash_fwd_kernel<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax>(Flash_fwd_params) [with Kernel_traits=Flash_fwd_kernel_traits<192, 64, 64, 4, false, false, cutlass::bfloat16_t, Flash_kernel_traits<192, 64, 64, 4, cutlass::bfloat16_t>>, Is_dropout=true, Is_causal=false, Is_even_N=true, Is_even_K=false, Return_softmax=false]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(34): here
            instantiation of "void run_flash_fwd<Kernel_traits,Is_dropout,Is_causal>(Flash_fwd_params &, cudaStream_t) [with Kernel_traits=Flash_fwd_kernel_traits<192, 64, 64, 4, false, false, cutlass::bfloat16_t, Flash_kernel_traits<192, 64, 64, 4, cutlass::bfloat16_t>>, Is_dropout=true, Is_causal=false]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(183): here
            instantiation of "void run_mha_fwd_hdim192<T>(Flash_fwd_params &, cudaStream_t) [with T=cutlass::bfloat16_t]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_hdim192_bf16_sm80.cu(15): here

/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_kernel.h(140): warning: variable "MMA_M" was declared but never referenced
          detected during:
            instantiation of "void flash::compute_attn_1rowblock<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params,Is_Q_split_K>(const Params &, int, int, int) [with Kernel_traits=Flash_fwd_kernel_traits<192, 64, 64, 4, false, false, cutlass::bfloat16_t, Flash_kernel_traits<192, 64, 64, 4, cutlass::bfloat16_t>>, Is_dropout=true, Is_causal=false, Is_even_N=false, Is_even_K=true, Return_softmax=true, Params=Flash_fwd_params, Is_Q_split_K=false]" 
(589): here
            instantiation of "void flash::compute_attn<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &) [with Kernel_traits=Flash_fwd_kernel_traits<192, 64, 64, 4, false, false, cutlass::bfloat16_t, Flash_kernel_traits<192, 64, 64, 4, cutlass::bfloat16_t>>, Is_dropout=true, Is_causal=false, Is_even_N=false, Is_even_K=true, Return_softmax=true, Params=Flash_fwd_params]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(15): here
            instantiation of "void flash_fwd_kernel<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax>(Flash_fwd_params) [with Kernel_traits=Flash_fwd_kernel_traits<192, 64, 64, 4, false, false, cutlass::bfloat16_t, Flash_kernel_traits<192, 64, 64, 4, cutlass::bfloat16_t>>, Is_dropout=true, Is_causal=false, Is_even_N=false, Is_even_K=true, Return_softmax=true]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(34): here
            instantiation of "void run_flash_fwd<Kernel_traits,Is_dropout,Is_causal>(Flash_fwd_params &, cudaStream_t) [with Kernel_traits=Flash_fwd_kernel_traits<192, 64, 64, 4, false, false, cutlass::bfloat16_t, Flash_kernel_traits<192, 64, 64, 4, cutlass::bfloat16_t>>, Is_dropout=true, Is_causal=false]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(183): here
            instantiation of "void run_mha_fwd_hdim192<T>(Flash_fwd_params &, cudaStream_t) [with T=cutlass::bfloat16_t]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_hdim192_bf16_sm80.cu(15): here

/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_kernel.h(276): warning: variable "tQrQ" was declared but never referenced
          detected during:
            instantiation of "void flash::compute_attn_1rowblock<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params,Is_Q_split_K>(const Params &, int, int, int) [with Kernel_traits=Flash_fwd_kernel_traits<192, 64, 64, 4, false, false, cutlass::bfloat16_t, Flash_kernel_traits<192, 64, 64, 4, cutlass::bfloat16_t>>, Is_dropout=true, Is_causal=false, Is_even_N=false, Is_even_K=true, Return_softmax=true, Params=Flash_fwd_params, Is_Q_split_K=false]" 
(589): here
            instantiation of "void flash::compute_attn<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &) [with Kernel_traits=Flash_fwd_kernel_traits<192, 64, 64, 4, false, false, cutlass::bfloat16_t, Flash_kernel_traits<192, 64, 64, 4, cutlass::bfloat16_t>>, Is_dropout=true, Is_causal=false, Is_even_N=false, Is_even_K=true, Return_softmax=true, Params=Flash_fwd_params]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(15): here
            instantiation of "void flash_fwd_kernel<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax>(Flash_fwd_params) [with Kernel_traits=Flash_fwd_kernel_traits<192, 64, 64, 4, false, false, cutlass::bfloat16_t, Flash_kernel_traits<192, 64, 64, 4, cutlass::bfloat16_t>>, Is_dropout=true, Is_causal=false, Is_even_N=false, Is_even_K=true, Return_softmax=true]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(34): here
            instantiation of "void run_flash_fwd<Kernel_traits,Is_dropout,Is_causal>(Flash_fwd_params &, cudaStream_t) [with Kernel_traits=Flash_fwd_kernel_traits<192, 64, 64, 4, false, false, cutlass::bfloat16_t, Flash_kernel_traits<192, 64, 64, 4, cutlass::bfloat16_t>>, Is_dropout=true, Is_causal=false]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(183): here
            instantiation of "void run_mha_fwd_hdim192<T>(Flash_fwd_params &, cudaStream_t) [with T=cutlass::bfloat16_t]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_hdim192_bf16_sm80.cu(15): here

/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_kernel.h(140): warning: variable "MMA_M" was declared but never referenced
          detected during:
            instantiation of "void flash::compute_attn_1rowblock<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params,Is_Q_split_K>(const Params &, int, int, int) [with Kernel_traits=Flash_fwd_kernel_traits<192, 64, 64, 4, false, false, cutlass::bfloat16_t, Flash_kernel_traits<192, 64, 64, 4, cutlass::bfloat16_t>>, Is_dropout=true, Is_causal=false, Is_even_N=false, Is_even_K=true, Return_softmax=false, Params=Flash_fwd_params, Is_Q_split_K=false]" 
(589): here
            instantiation of "void flash::compute_attn<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &) [with Kernel_traits=Flash_fwd_kernel_traits<192, 64, 64, 4, false, false, cutlass::bfloat16_t, Flash_kernel_traits<192, 64, 64, 4, cutlass::bfloat16_t>>, Is_dropout=true, Is_causal=false, Is_even_N=false, Is_even_K=true, Return_softmax=false, Params=Flash_fwd_params]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(15): here
            instantiation of "void flash_fwd_kernel<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax>(Flash_fwd_params) [with Kernel_traits=Flash_fwd_kernel_traits<192, 64, 64, 4, false, false, cutlass::bfloat16_t, Flash_kernel_traits<192, 64, 64, 4, cutlass::bfloat16_t>>, Is_dropout=true, Is_causal=false, Is_even_N=false, Is_even_K=true, Return_softmax=false]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(34): here
            instantiation of "void run_flash_fwd<Kernel_traits,Is_dropout,Is_causal>(Flash_fwd_params &, cudaStream_t) [with Kernel_traits=Flash_fwd_kernel_traits<192, 64, 64, 4, false, false, cutlass::bfloat16_t, Flash_kernel_traits<192, 64, 64, 4, cutlass::bfloat16_t>>, Is_dropout=true, Is_causal=false]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(183): here
            instantiation of "void run_mha_fwd_hdim192<T>(Flash_fwd_params &, cudaStream_t) [with T=cutlass::bfloat16_t]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_hdim192_bf16_sm80.cu(15): here

/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_kernel.h(276): warning: variable "tQrQ" was declared but never referenced
          detected during:
            instantiation of "void flash::compute_attn_1rowblock<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params,Is_Q_split_K>(const Params &, int, int, int) [with Kernel_traits=Flash_fwd_kernel_traits<192, 64, 64, 4, false, false, cutlass::bfloat16_t, Flash_kernel_traits<192, 64, 64, 4, cutlass::bfloat16_t>>, Is_dropout=true, Is_causal=false, Is_even_N=false, Is_even_K=true, Return_softmax=false, Params=Flash_fwd_params, Is_Q_split_K=false]" 
(589): here
            instantiation of "void flash::compute_attn<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &) [with Kernel_traits=Flash_fwd_kernel_traits<192, 64, 64, 4, false, false, cutlass::bfloat16_t, Flash_kernel_traits<192, 64, 64, 4, cutlass::bfloat16_t>>, Is_dropout=true, Is_causal=false, Is_even_N=false, Is_even_K=true, Return_softmax=false, Params=Flash_fwd_params]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(15): here
            instantiation of "void flash_fwd_kernel<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax>(Flash_fwd_params) [with Kernel_traits=Flash_fwd_kernel_traits<192, 64, 64, 4, false, false, cutlass::bfloat16_t, Flash_kernel_traits<192, 64, 64, 4, cutlass::bfloat16_t>>, Is_dropout=true, Is_causal=false, Is_even_N=false, Is_even_K=true, Return_softmax=false]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(34): here
            instantiation of "void run_flash_fwd<Kernel_traits,Is_dropout,Is_causal>(Flash_fwd_params &, cudaStream_t) [with Kernel_traits=Flash_fwd_kernel_traits<192, 64, 64, 4, false, false, cutlass::bfloat16_t, Flash_kernel_traits<192, 64, 64, 4, cutlass::bfloat16_t>>, Is_dropout=true, Is_causal=false]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(183): here
            instantiation of "void run_mha_fwd_hdim192<T>(Flash_fwd_params &, cudaStream_t) [with T=cutlass::bfloat16_t]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_hdim192_bf16_sm80.cu(15): here

/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_kernel.h(140): warning: variable "MMA_M" was declared but never referenced
          detected during:
            instantiation of "void flash::compute_attn_1rowblock<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params,Is_Q_split_K>(const Params &, int, int, int) [with Kernel_traits=Flash_fwd_kernel_traits<192, 64, 64, 4, false, false, cutlass::bfloat16_t, Flash_kernel_traits<192, 64, 64, 4, cutlass::bfloat16_t>>, Is_dropout=true, Is_causal=false, Is_even_N=false, Is_even_K=false, Return_softmax=true, Params=Flash_fwd_params, Is_Q_split_K=false]" 
(589): here
            instantiation of "void flash::compute_attn<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &) [with Kernel_traits=Flash_fwd_kernel_traits<192, 64, 64, 4, false, false, cutlass::bfloat16_t, Flash_kernel_traits<192, 64, 64, 4, cutlass::bfloat16_t>>, Is_dropout=true, Is_causal=false, Is_even_N=false, Is_even_K=false, Return_softmax=true, Params=Flash_fwd_params]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(15): here
            instantiation of "void flash_fwd_kernel<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax>(Flash_fwd_params) [with Kernel_traits=Flash_fwd_kernel_traits<192, 64, 64, 4, false, false, cutlass::bfloat16_t, Flash_kernel_traits<192, 64, 64, 4, cutlass::bfloat16_t>>, Is_dropout=true, Is_causal=false, Is_even_N=false, Is_even_K=false, Return_softmax=true]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(34): here
            instantiation of "void run_flash_fwd<Kernel_traits,Is_dropout,Is_causal>(Flash_fwd_params &, cudaStream_t) [with Kernel_traits=Flash_fwd_kernel_traits<192, 64, 64, 4, false, false, cutlass::bfloat16_t, Flash_kernel_traits<192, 64, 64, 4, cutlass::bfloat16_t>>, Is_dropout=true, Is_causal=false]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(183): here
            instantiation of "void run_mha_fwd_hdim192<T>(Flash_fwd_params &, cudaStream_t) [with T=cutlass::bfloat16_t]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_hdim192_bf16_sm80.cu(15): here

/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_kernel.h(276): warning: variable "tQrQ" was declared but never referenced
          detected during:
            instantiation of "void flash::compute_attn_1rowblock<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params,Is_Q_split_K>(const Params &, int, int, int) [with Kernel_traits=Flash_fwd_kernel_traits<192, 64, 64, 4, false, false, cutlass::bfloat16_t, Flash_kernel_traits<192, 64, 64, 4, cutlass::bfloat16_t>>, Is_dropout=true, Is_causal=false, Is_even_N=false, Is_even_K=false, Return_softmax=true, Params=Flash_fwd_params, Is_Q_split_K=false]" 
(589): here
            instantiation of "void flash::compute_attn<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &) [with Kernel_traits=Flash_fwd_kernel_traits<192, 64, 64, 4, false, false, cutlass::bfloat16_t, Flash_kernel_traits<192, 64, 64, 4, cutlass::bfloat16_t>>, Is_dropout=true, Is_causal=false, Is_even_N=false, Is_even_K=false, Return_softmax=true, Params=Flash_fwd_params]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(15): here
            instantiation of "void flash_fwd_kernel<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax>(Flash_fwd_params) [with Kernel_traits=Flash_fwd_kernel_traits<192, 64, 64, 4, false, false, cutlass::bfloat16_t, Flash_kernel_traits<192, 64, 64, 4, cutlass::bfloat16_t>>, Is_dropout=true, Is_causal=false, Is_even_N=false, Is_even_K=false, Return_softmax=true]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(34): here
            instantiation of "void run_flash_fwd<Kernel_traits,Is_dropout,Is_causal>(Flash_fwd_params &, cudaStream_t) [with Kernel_traits=Flash_fwd_kernel_traits<192, 64, 64, 4, false, false, cutlass::bfloat16_t, Flash_kernel_traits<192, 64, 64, 4, cutlass::bfloat16_t>>, Is_dropout=true, Is_causal=false]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(183): here
            instantiation of "void run_mha_fwd_hdim192<T>(Flash_fwd_params &, cudaStream_t) [with T=cutlass::bfloat16_t]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_hdim192_bf16_sm80.cu(15): here

/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_kernel.h(140): warning: variable "MMA_M" was declared but never referenced
          detected during:
            instantiation of "void flash::compute_attn_1rowblock<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params,Is_Q_split_K>(const Params &, int, int, int) [with Kernel_traits=Flash_fwd_kernel_traits<192, 64, 64, 4, false, false, cutlass::bfloat16_t, Flash_kernel_traits<192, 64, 64, 4, cutlass::bfloat16_t>>, Is_dropout=true, Is_causal=false, Is_even_N=false, Is_even_K=false, Return_softmax=false, Params=Flash_fwd_params, Is_Q_split_K=false]" 
(589): here
            instantiation of "void flash::compute_attn<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &) [with Kernel_traits=Flash_fwd_kernel_traits<192, 64, 64, 4, false, false, cutlass::bfloat16_t, Flash_kernel_traits<192, 64, 64, 4, cutlass::bfloat16_t>>, Is_dropout=true, Is_causal=false, Is_even_N=false, Is_even_K=false, Return_softmax=false, Params=Flash_fwd_params]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(15): here
            instantiation of "void flash_fwd_kernel<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax>(Flash_fwd_params) [with Kernel_traits=Flash_fwd_kernel_traits<192, 64, 64, 4, false, false, cutlass::bfloat16_t, Flash_kernel_traits<192, 64, 64, 4, cutlass::bfloat16_t>>, Is_dropout=true, Is_causal=false, Is_even_N=false, Is_even_K=false, Return_softmax=false]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(34): here
            instantiation of "void run_flash_fwd<Kernel_traits,Is_dropout,Is_causal>(Flash_fwd_params &, cudaStream_t) [with Kernel_traits=Flash_fwd_kernel_traits<192, 64, 64, 4, false, false, cutlass::bfloat16_t, Flash_kernel_traits<192, 64, 64, 4, cutlass::bfloat16_t>>, Is_dropout=true, Is_causal=false]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(183): here
            instantiation of "void run_mha_fwd_hdim192<T>(Flash_fwd_params &, cudaStream_t) [with T=cutlass::bfloat16_t]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_hdim192_bf16_sm80.cu(15): here

/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_kernel.h(276): warning: variable "tQrQ" was declared but never referenced
          detected during:
            instantiation of "void flash::compute_attn_1rowblock<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params,Is_Q_split_K>(const Params &, int, int, int) [with Kernel_traits=Flash_fwd_kernel_traits<192, 64, 64, 4, false, false, cutlass::bfloat16_t, Flash_kernel_traits<192, 64, 64, 4, cutlass::bfloat16_t>>, Is_dropout=true, Is_causal=false, Is_even_N=false, Is_even_K=false, Return_softmax=false, Params=Flash_fwd_params, Is_Q_split_K=false]" 
(589): here
            instantiation of "void flash::compute_attn<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &) [with Kernel_traits=Flash_fwd_kernel_traits<192, 64, 64, 4, false, false, cutlass::bfloat16_t, Flash_kernel_traits<192, 64, 64, 4, cutlass::bfloat16_t>>, Is_dropout=true, Is_causal=false, Is_even_N=false, Is_even_K=false, Return_softmax=false, Params=Flash_fwd_params]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(15): here
            instantiation of "void flash_fwd_kernel<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax>(Flash_fwd_params) [with Kernel_traits=Flash_fwd_kernel_traits<192, 64, 64, 4, false, false, cutlass::bfloat16_t, Flash_kernel_traits<192, 64, 64, 4, cutlass::bfloat16_t>>, Is_dropout=true, Is_causal=false, Is_even_N=false, Is_even_K=false, Return_softmax=false]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(34): here
            instantiation of "void run_flash_fwd<Kernel_traits,Is_dropout,Is_causal>(Flash_fwd_params &, cudaStream_t) [with Kernel_traits=Flash_fwd_kernel_traits<192, 64, 64, 4, false, false, cutlass::bfloat16_t, Flash_kernel_traits<192, 64, 64, 4, cutlass::bfloat16_t>>, Is_dropout=true, Is_causal=false]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(183): here
            instantiation of "void run_mha_fwd_hdim192<T>(Flash_fwd_params &, cudaStream_t) [with T=cutlass::bfloat16_t]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_hdim192_bf16_sm80.cu(15): here

/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_kernel.h(140): warning: variable "MMA_M" was declared but never referenced
          detected during:
            instantiation of "void flash::compute_attn_1rowblock<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params,Is_Q_split_K>(const Params &, int, int, int) [with Kernel_traits=Flash_fwd_kernel_traits<192, 128, 64, 8, false, false, cutlass::bfloat16_t, Flash_kernel_traits<192, 128, 64, 8, cutlass::bfloat16_t>>, Is_dropout=false, Is_causal=true, Is_even_N=true, Is_even_K=true, Return_softmax=false, Params=Flash_fwd_params, Is_Q_split_K=false]" 
(589): here
            instantiation of "void flash::compute_attn<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &) [with Kernel_traits=Flash_fwd_kernel_traits<192, 128, 64, 8, false, false, cutlass::bfloat16_t, Flash_kernel_traits<192, 128, 64, 8, cutlass::bfloat16_t>>, Is_dropout=false, Is_causal=true, Is_even_N=true, Is_even_K=true, Return_softmax=false, Params=Flash_fwd_params]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(15): here
            instantiation of "void flash_fwd_kernel<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax>(Flash_fwd_params) [with Kernel_traits=Flash_fwd_kernel_traits<192, 128, 64, 8, false, false, cutlass::bfloat16_t, Flash_kernel_traits<192, 128, 64, 8, cutlass::bfloat16_t>>, Is_dropout=false, Is_causal=true, Is_even_N=true, Is_even_K=true, Return_softmax=false]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(34): here
            instantiation of "void run_flash_fwd<Kernel_traits,Is_dropout,Is_causal>(Flash_fwd_params &, cudaStream_t) [with Kernel_traits=Flash_fwd_kernel_traits<192, 128, 64, 8, false, false, cutlass::bfloat16_t, Flash_kernel_traits<192, 128, 64, 8, cutlass::bfloat16_t>>, Is_dropout=false, Is_causal=true]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(183): here
            instantiation of "void run_mha_fwd_hdim192<T>(Flash_fwd_params &, cudaStream_t) [with T=cutlass::bfloat16_t]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_hdim192_bf16_sm80.cu(15): here

/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_kernel.h(276): warning: variable "tQrQ" was declared but never referenced
          detected during:
            instantiation of "void flash::compute_attn_1rowblock<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params,Is_Q_split_K>(const Params &, int, int, int) [with Kernel_traits=Flash_fwd_kernel_traits<192, 128, 64, 8, false, false, cutlass::bfloat16_t, Flash_kernel_traits<192, 128, 64, 8, cutlass::bfloat16_t>>, Is_dropout=false, Is_causal=true, Is_even_N=true, Is_even_K=true, Return_softmax=false, Params=Flash_fwd_params, Is_Q_split_K=false]" 
(589): here
            instantiation of "void flash::compute_attn<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &) [with Kernel_traits=Flash_fwd_kernel_traits<192, 128, 64, 8, false, false, cutlass::bfloat16_t, Flash_kernel_traits<192, 128, 64, 8, cutlass::bfloat16_t>>, Is_dropout=false, Is_causal=true, Is_even_N=true, Is_even_K=true, Return_softmax=false, Params=Flash_fwd_params]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(15): here
            instantiation of "void flash_fwd_kernel<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax>(Flash_fwd_params) [with Kernel_traits=Flash_fwd_kernel_traits<192, 128, 64, 8, false, false, cutlass::bfloat16_t, Flash_kernel_traits<192, 128, 64, 8, cutlass::bfloat16_t>>, Is_dropout=false, Is_causal=true, Is_even_N=true, Is_even_K=true, Return_softmax=false]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(34): here
            instantiation of "void run_flash_fwd<Kernel_traits,Is_dropout,Is_causal>(Flash_fwd_params &, cudaStream_t) [with Kernel_traits=Flash_fwd_kernel_traits<192, 128, 64, 8, false, false, cutlass::bfloat16_t, Flash_kernel_traits<192, 128, 64, 8, cutlass::bfloat16_t>>, Is_dropout=false, Is_causal=true]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(183): here
            instantiation of "void run_mha_fwd_hdim192<T>(Flash_fwd_params &, cudaStream_t) [with T=cutlass::bfloat16_t]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_hdim192_bf16_sm80.cu(15): here

/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_kernel.h(140): warning: variable "MMA_M" was declared but never referenced
          detected during:
            instantiation of "void flash::compute_attn_1rowblock<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params,Is_Q_split_K>(const Params &, int, int, int) [with Kernel_traits=Flash_fwd_kernel_traits<192, 128, 64, 8, false, false, cutlass::bfloat16_t, Flash_kernel_traits<192, 128, 64, 8, cutlass::bfloat16_t>>, Is_dropout=false, Is_causal=true, Is_even_N=true, Is_even_K=false, Return_softmax=false, Params=Flash_fwd_params, Is_Q_split_K=false]" 
(589): here
            instantiation of "void flash::compute_attn<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &) [with Kernel_traits=Flash_fwd_kernel_traits<192, 128, 64, 8, false, false, cutlass::bfloat16_t, Flash_kernel_traits<192, 128, 64, 8, cutlass::bfloat16_t>>, Is_dropout=false, Is_causal=true, Is_even_N=true, Is_even_K=false, Return_softmax=false, Params=Flash_fwd_params]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(15): here
            instantiation of "void flash_fwd_kernel<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax>(Flash_fwd_params) [with Kernel_traits=Flash_fwd_kernel_traits<192, 128, 64, 8, false, false, cutlass::bfloat16_t, Flash_kernel_traits<192, 128, 64, 8, cutlass::bfloat16_t>>, Is_dropout=false, Is_causal=true, Is_even_N=true, Is_even_K=false, Return_softmax=false]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(34): here
            instantiation of "void run_flash_fwd<Kernel_traits,Is_dropout,Is_causal>(Flash_fwd_params &, cudaStream_t) [with Kernel_traits=Flash_fwd_kernel_traits<192, 128, 64, 8, false, false, cutlass::bfloat16_t, Flash_kernel_traits<192, 128, 64, 8, cutlass::bfloat16_t>>, Is_dropout=false, Is_causal=true]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(183): here
            instantiation of "void run_mha_fwd_hdim192<T>(Flash_fwd_params &, cudaStream_t) [with T=cutlass::bfloat16_t]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_hdim192_bf16_sm80.cu(15): here

/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_kernel.h(276): warning: variable "tQrQ" was declared but never referenced
          detected during:
            instantiation of "void flash::compute_attn_1rowblock<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params,Is_Q_split_K>(const Params &, int, int, int) [with Kernel_traits=Flash_fwd_kernel_traits<192, 128, 64, 8, false, false, cutlass::bfloat16_t, Flash_kernel_traits<192, 128, 64, 8, cutlass::bfloat16_t>>, Is_dropout=false, Is_causal=true, Is_even_N=true, Is_even_K=false, Return_softmax=false, Params=Flash_fwd_params, Is_Q_split_K=false]" 
(589): here
            instantiation of "void flash::compute_attn<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &) [with Kernel_traits=Flash_fwd_kernel_traits<192, 128, 64, 8, false, false, cutlass::bfloat16_t, Flash_kernel_traits<192, 128, 64, 8, cutlass::bfloat16_t>>, Is_dropout=false, Is_causal=true, Is_even_N=true, Is_even_K=false, Return_softmax=false, Params=Flash_fwd_params]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(15): here
            instantiation of "void flash_fwd_kernel<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax>(Flash_fwd_params) [with Kernel_traits=Flash_fwd_kernel_traits<192, 128, 64, 8, false, false, cutlass::bfloat16_t, Flash_kernel_traits<192, 128, 64, 8, cutlass::bfloat16_t>>, Is_dropout=false, Is_causal=true, Is_even_N=true, Is_even_K=false, Return_softmax=false]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(34): here
            instantiation of "void run_flash_fwd<Kernel_traits,Is_dropout,Is_causal>(Flash_fwd_params &, cudaStream_t) [with Kernel_traits=Flash_fwd_kernel_traits<192, 128, 64, 8, false, false, cutlass::bfloat16_t, Flash_kernel_traits<192, 128, 64, 8, cutlass::bfloat16_t>>, Is_dropout=false, Is_causal=true]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(183): here
            instantiation of "void run_mha_fwd_hdim192<T>(Flash_fwd_params &, cudaStream_t) [with T=cutlass::bfloat16_t]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_hdim192_bf16_sm80.cu(15): here

/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_kernel.h(140): warning: variable "MMA_M" was declared but never referenced
          detected during:
            instantiation of "void flash::compute_attn_1rowblock<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params,Is_Q_split_K>(const Params &, int, int, int) [with Kernel_traits=Flash_fwd_kernel_traits<192, 128, 64, 8, false, false, cutlass::bfloat16_t, Flash_kernel_traits<192, 128, 64, 8, cutlass::bfloat16_t>>, Is_dropout=false, Is_causal=true, Is_even_N=false, Is_even_K=true, Return_softmax=false, Params=Flash_fwd_params, Is_Q_split_K=false]" 
(589): here
            instantiation of "void flash::compute_attn<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &) [with Kernel_traits=Flash_fwd_kernel_traits<192, 128, 64, 8, false, false, cutlass::bfloat16_t, Flash_kernel_traits<192, 128, 64, 8, cutlass::bfloat16_t>>, Is_dropout=false, Is_causal=true, Is_even_N=false, Is_even_K=true, Return_softmax=false, Params=Flash_fwd_params]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(15): here
            instantiation of "void flash_fwd_kernel<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax>(Flash_fwd_params) [with Kernel_traits=Flash_fwd_kernel_traits<192, 128, 64, 8, false, false, cutlass::bfloat16_t, Flash_kernel_traits<192, 128, 64, 8, cutlass::bfloat16_t>>, Is_dropout=false, Is_causal=true, Is_even_N=false, Is_even_K=true, Return_softmax=false]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(34): here
            instantiation of "void run_flash_fwd<Kernel_traits,Is_dropout,Is_causal>(Flash_fwd_params &, cudaStream_t) [with Kernel_traits=Flash_fwd_kernel_traits<192, 128, 64, 8, false, false, cutlass::bfloat16_t, Flash_kernel_traits<192, 128, 64, 8, cutlass::bfloat16_t>>, Is_dropout=false, Is_causal=true]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(183): here
            instantiation of "void run_mha_fwd_hdim192<T>(Flash_fwd_params &, cudaStream_t) [with T=cutlass::bfloat16_t]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_hdim192_bf16_sm80.cu(15): here

/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_kernel.h(276): warning: variable "tQrQ" was declared but never referenced
          detected during:
            instantiation of "void flash::compute_attn_1rowblock<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params,Is_Q_split_K>(const Params &, int, int, int) [with Kernel_traits=Flash_fwd_kernel_traits<192, 128, 64, 8, false, false, cutlass::bfloat16_t, Flash_kernel_traits<192, 128, 64, 8, cutlass::bfloat16_t>>, Is_dropout=false, Is_causal=true, Is_even_N=false, Is_even_K=true, Return_softmax=false, Params=Flash_fwd_params, Is_Q_split_K=false]" 
(589): here
            instantiation of "void flash::compute_attn<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &) [with Kernel_traits=Flash_fwd_kernel_traits<192, 128, 64, 8, false, false, cutlass::bfloat16_t, Flash_kernel_traits<192, 128, 64, 8, cutlass::bfloat16_t>>, Is_dropout=false, Is_causal=true, Is_even_N=false, Is_even_K=true, Return_softmax=false, Params=Flash_fwd_params]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(15): here
            instantiation of "void flash_fwd_kernel<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax>(Flash_fwd_params) [with Kernel_traits=Flash_fwd_kernel_traits<192, 128, 64, 8, false, false, cutlass::bfloat16_t, Flash_kernel_traits<192, 128, 64, 8, cutlass::bfloat16_t>>, Is_dropout=false, Is_causal=true, Is_even_N=false, Is_even_K=true, Return_softmax=false]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(34): here
            instantiation of "void run_flash_fwd<Kernel_traits,Is_dropout,Is_causal>(Flash_fwd_params &, cudaStream_t) [with Kernel_traits=Flash_fwd_kernel_traits<192, 128, 64, 8, false, false, cutlass::bfloat16_t, Flash_kernel_traits<192, 128, 64, 8, cutlass::bfloat16_t>>, Is_dropout=false, Is_causal=true]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(183): here
            instantiation of "void run_mha_fwd_hdim192<T>(Flash_fwd_params &, cudaStream_t) [with T=cutlass::bfloat16_t]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_hdim192_bf16_sm80.cu(15): here

/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_kernel.h(140): warning: variable "MMA_M" was declared but never referenced
          detected during:
            instantiation of "void flash::compute_attn_1rowblock<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params,Is_Q_split_K>(const Params &, int, int, int) [with Kernel_traits=Flash_fwd_kernel_traits<192, 128, 64, 8, false, false, cutlass::bfloat16_t, Flash_kernel_traits<192, 128, 64, 8, cutlass::bfloat16_t>>, Is_dropout=false, Is_causal=true, Is_even_N=false, Is_even_K=false, Return_softmax=false, Params=Flash_fwd_params, Is_Q_split_K=false]" 
(589): here
            instantiation of "void flash::compute_attn<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &) [with Kernel_traits=Flash_fwd_kernel_traits<192, 128, 64, 8, false, false, cutlass::bfloat16_t, Flash_kernel_traits<192, 128, 64, 8, cutlass::bfloat16_t>>, Is_dropout=false, Is_causal=true, Is_even_N=false, Is_even_K=false, Return_softmax=false, Params=Flash_fwd_params]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(15): here
            instantiation of "void flash_fwd_kernel<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax>(Flash_fwd_params) [with Kernel_traits=Flash_fwd_kernel_traits<192, 128, 64, 8, false, false, cutlass::bfloat16_t, Flash_kernel_traits<192, 128, 64, 8, cutlass::bfloat16_t>>, Is_dropout=false, Is_causal=true, Is_even_N=false, Is_even_K=false, Return_softmax=false]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(34): here
            instantiation of "void run_flash_fwd<Kernel_traits,Is_dropout,Is_causal>(Flash_fwd_params &, cudaStream_t) [with Kernel_traits=Flash_fwd_kernel_traits<192, 128, 64, 8, false, false, cutlass::bfloat16_t, Flash_kernel_traits<192, 128, 64, 8, cutlass::bfloat16_t>>, Is_dropout=false, Is_causal=true]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(183): here
            instantiation of "void run_mha_fwd_hdim192<T>(Flash_fwd_params &, cudaStream_t) [with T=cutlass::bfloat16_t]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_hdim192_bf16_sm80.cu(15): here

/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_kernel.h(276): warning: variable "tQrQ" was declared but never referenced
          detected during:
            instantiation of "void flash::compute_attn_1rowblock<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params,Is_Q_split_K>(const Params &, int, int, int) [with Kernel_traits=Flash_fwd_kernel_traits<192, 128, 64, 8, false, false, cutlass::bfloat16_t, Flash_kernel_traits<192, 128, 64, 8, cutlass::bfloat16_t>>, Is_dropout=false, Is_causal=true, Is_even_N=false, Is_even_K=false, Return_softmax=false, Params=Flash_fwd_params, Is_Q_split_K=false]" 
(589): here
            instantiation of "void flash::compute_attn<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &) [with Kernel_traits=Flash_fwd_kernel_traits<192, 128, 64, 8, false, false, cutlass::bfloat16_t, Flash_kernel_traits<192, 128, 64, 8, cutlass::bfloat16_t>>, Is_dropout=false, Is_causal=true, Is_even_N=false, Is_even_K=false, Return_softmax=false, Params=Flash_fwd_params]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(15): here
            instantiation of "void flash_fwd_kernel<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax>(Flash_fwd_params) [with Kernel_traits=Flash_fwd_kernel_traits<192, 128, 64, 8, false, false, cutlass::bfloat16_t, Flash_kernel_traits<192, 128, 64, 8, cutlass::bfloat16_t>>, Is_dropout=false, Is_causal=true, Is_even_N=false, Is_even_K=false, Return_softmax=false]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(34): here
            instantiation of "void run_flash_fwd<Kernel_traits,Is_dropout,Is_causal>(Flash_fwd_params &, cudaStream_t) [with Kernel_traits=Flash_fwd_kernel_traits<192, 128, 64, 8, false, false, cutlass::bfloat16_t, Flash_kernel_traits<192, 128, 64, 8, cutlass::bfloat16_t>>, Is_dropout=false, Is_causal=true]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(183): here
            instantiation of "void run_mha_fwd_hdim192<T>(Flash_fwd_params &, cudaStream_t) [with T=cutlass::bfloat16_t]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_hdim192_bf16_sm80.cu(15): here

/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_kernel.h(140): warning: variable "MMA_M" was declared but never referenced
          detected during:
            instantiation of "void flash::compute_attn_1rowblock<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params,Is_Q_split_K>(const Params &, int, int, int) [with Kernel_traits=Flash_fwd_kernel_traits<192, 128, 64, 8, false, false, cutlass::bfloat16_t, Flash_kernel_traits<192, 128, 64, 8, cutlass::bfloat16_t>>, Is_dropout=false, Is_causal=false, Is_even_N=true, Is_even_K=true, Return_softmax=false, Params=Flash_fwd_params, Is_Q_split_K=false]" 
(589): here
            instantiation of "void flash::compute_attn<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &) [with Kernel_traits=Flash_fwd_kernel_traits<192, 128, 64, 8, false, false, cutlass::bfloat16_t, Flash_kernel_traits<192, 128, 64, 8, cutlass::bfloat16_t>>, Is_dropout=false, Is_causal=false, Is_even_N=true, Is_even_K=true, Return_softmax=false, Params=Flash_fwd_params]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(15): here
            instantiation of "void flash_fwd_kernel<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax>(Flash_fwd_params) [with Kernel_traits=Flash_fwd_kernel_traits<192, 128, 64, 8, false, false, cutlass::bfloat16_t, Flash_kernel_traits<192, 128, 64, 8, cutlass::bfloat16_t>>, Is_dropout=false, Is_causal=false, Is_even_N=true, Is_even_K=true, Return_softmax=false]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(34): here
            instantiation of "void run_flash_fwd<Kernel_traits,Is_dropout,Is_causal>(Flash_fwd_params &, cudaStream_t) [with Kernel_traits=Flash_fwd_kernel_traits<192, 128, 64, 8, false, false, cutlass::bfloat16_t, Flash_kernel_traits<192, 128, 64, 8, cutlass::bfloat16_t>>, Is_dropout=false, Is_causal=false]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(183): here
            instantiation of "void run_mha_fwd_hdim192<T>(Flash_fwd_params &, cudaStream_t) [with T=cutlass::bfloat16_t]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_hdim192_bf16_sm80.cu(15): here

/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_kernel.h(276): warning: variable "tQrQ" was declared but never referenced
          detected during:
            instantiation of "void flash::compute_attn_1rowblock<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params,Is_Q_split_K>(const Params &, int, int, int) [with Kernel_traits=Flash_fwd_kernel_traits<192, 128, 64, 8, false, false, cutlass::bfloat16_t, Flash_kernel_traits<192, 128, 64, 8, cutlass::bfloat16_t>>, Is_dropout=false, Is_causal=false, Is_even_N=true, Is_even_K=true, Return_softmax=false, Params=Flash_fwd_params, Is_Q_split_K=false]" 
(589): here
            instantiation of "void flash::compute_attn<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &) [with Kernel_traits=Flash_fwd_kernel_traits<192, 128, 64, 8, false, false, cutlass::bfloat16_t, Flash_kernel_traits<192, 128, 64, 8, cutlass::bfloat16_t>>, Is_dropout=false, Is_causal=false, Is_even_N=true, Is_even_K=true, Return_softmax=false, Params=Flash_fwd_params]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(15): here
            instantiation of "void flash_fwd_kernel<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax>(Flash_fwd_params) [with Kernel_traits=Flash_fwd_kernel_traits<192, 128, 64, 8, false, false, cutlass::bfloat16_t, Flash_kernel_traits<192, 128, 64, 8, cutlass::bfloat16_t>>, Is_dropout=false, Is_causal=false, Is_even_N=true, Is_even_K=true, Return_softmax=false]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(34): here
            instantiation of "void run_flash_fwd<Kernel_traits,Is_dropout,Is_causal>(Flash_fwd_params &, cudaStream_t) [with Kernel_traits=Flash_fwd_kernel_traits<192, 128, 64, 8, false, false, cutlass::bfloat16_t, Flash_kernel_traits<192, 128, 64, 8, cutlass::bfloat16_t>>, Is_dropout=false, Is_causal=false]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(183): here
            instantiation of "void run_mha_fwd_hdim192<T>(Flash_fwd_params &, cudaStream_t) [with T=cutlass::bfloat16_t]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_hdim192_bf16_sm80.cu(15): here

/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_kernel.h(140): warning: variable "MMA_M" was declared but never referenced
          detected during:
            instantiation of "void flash::compute_attn_1rowblock<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params,Is_Q_split_K>(const Params &, int, int, int) [with Kernel_traits=Flash_fwd_kernel_traits<192, 128, 64, 8, false, false, cutlass::bfloat16_t, Flash_kernel_traits<192, 128, 64, 8, cutlass::bfloat16_t>>, Is_dropout=false, Is_causal=false, Is_even_N=true, Is_even_K=false, Return_softmax=false, Params=Flash_fwd_params, Is_Q_split_K=false]" 
(589): here
            instantiation of "void flash::compute_attn<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &) [with Kernel_traits=Flash_fwd_kernel_traits<192, 128, 64, 8, false, false, cutlass::bfloat16_t, Flash_kernel_traits<192, 128, 64, 8, cutlass::bfloat16_t>>, Is_dropout=false, Is_causal=false, Is_even_N=true, Is_even_K=false, Return_softmax=false, Params=Flash_fwd_params]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(15): here
            instantiation of "void flash_fwd_kernel<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax>(Flash_fwd_params) [with Kernel_traits=Flash_fwd_kernel_traits<192, 128, 64, 8, false, false, cutlass::bfloat16_t, Flash_kernel_traits<192, 128, 64, 8, cutlass::bfloat16_t>>, Is_dropout=false, Is_causal=false, Is_even_N=true, Is_even_K=false, Return_softmax=false]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(34): here
            instantiation of "void run_flash_fwd<Kernel_traits,Is_dropout,Is_causal>(Flash_fwd_params &, cudaStream_t) [with Kernel_traits=Flash_fwd_kernel_traits<192, 128, 64, 8, false, false, cutlass::bfloat16_t, Flash_kernel_traits<192, 128, 64, 8, cutlass::bfloat16_t>>, Is_dropout=false, Is_causal=false]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(183): here
            instantiation of "void run_mha_fwd_hdim192<T>(Flash_fwd_params &, cudaStream_t) [with T=cutlass::bfloat16_t]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_hdim192_bf16_sm80.cu(15): here

/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_kernel.h(276): warning: variable "tQrQ" was declared but never referenced
          detected during:
            instantiation of "void flash::compute_attn_1rowblock<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params,Is_Q_split_K>(const Params &, int, int, int) [with Kernel_traits=Flash_fwd_kernel_traits<192, 128, 64, 8, false, false, cutlass::bfloat16_t, Flash_kernel_traits<192, 128, 64, 8, cutlass::bfloat16_t>>, Is_dropout=false, Is_causal=false, Is_even_N=true, Is_even_K=false, Return_softmax=false, Params=Flash_fwd_params, Is_Q_split_K=false]" 
(589): here
            instantiation of "void flash::compute_attn<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &) [with Kernel_traits=Flash_fwd_kernel_traits<192, 128, 64, 8, false, false, cutlass::bfloat16_t, Flash_kernel_traits<192, 128, 64, 8, cutlass::bfloat16_t>>, Is_dropout=false, Is_causal=false, Is_even_N=true, Is_even_K=false, Return_softmax=false, Params=Flash_fwd_params]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(15): here
            instantiation of "void flash_fwd_kernel<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax>(Flash_fwd_params) [with Kernel_traits=Flash_fwd_kernel_traits<192, 128, 64, 8, false, false, cutlass::bfloat16_t, Flash_kernel_traits<192, 128, 64, 8, cutlass::bfloat16_t>>, Is_dropout=false, Is_causal=false, Is_even_N=true, Is_even_K=false, Return_softmax=false]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(34): here
            instantiation of "void run_flash_fwd<Kernel_traits,Is_dropout,Is_causal>(Flash_fwd_params &, cudaStream_t) [with Kernel_traits=Flash_fwd_kernel_traits<192, 128, 64, 8, false, false, cutlass::bfloat16_t, Flash_kernel_traits<192, 128, 64, 8, cutlass::bfloat16_t>>, Is_dropout=false, Is_causal=false]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(183): here
            instantiation of "void run_mha_fwd_hdim192<T>(Flash_fwd_params &, cudaStream_t) [with T=cutlass::bfloat16_t]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_hdim192_bf16_sm80.cu(15): here

/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_kernel.h(140): warning: variable "MMA_M" was declared but never referenced
          detected during:
            instantiation of "void flash::compute_attn_1rowblock<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params,Is_Q_split_K>(const Params &, int, int, int) [with Kernel_traits=Flash_fwd_kernel_traits<192, 128, 64, 8, false, false, cutlass::bfloat16_t, Flash_kernel_traits<192, 128, 64, 8, cutlass::bfloat16_t>>, Is_dropout=false, Is_causal=false, Is_even_N=false, Is_even_K=true, Return_softmax=false, Params=Flash_fwd_params, Is_Q_split_K=false]" 
(589): here
            instantiation of "void flash::compute_attn<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &) [with Kernel_traits=Flash_fwd_kernel_traits<192, 128, 64, 8, false, false, cutlass::bfloat16_t, Flash_kernel_traits<192, 128, 64, 8, cutlass::bfloat16_t>>, Is_dropout=false, Is_causal=false, Is_even_N=false, Is_even_K=true, Return_softmax=false, Params=Flash_fwd_params]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(15): here
            instantiation of "void flash_fwd_kernel<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax>(Flash_fwd_params) [with Kernel_traits=Flash_fwd_kernel_traits<192, 128, 64, 8, false, false, cutlass::bfloat16_t, Flash_kernel_traits<192, 128, 64, 8, cutlass::bfloat16_t>>, Is_dropout=false, Is_causal=false, Is_even_N=false, Is_even_K=true, Return_softmax=false]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(34): here
            instantiation of "void run_flash_fwd<Kernel_traits,Is_dropout,Is_causal>(Flash_fwd_params &, cudaStream_t) [with Kernel_traits=Flash_fwd_kernel_traits<192, 128, 64, 8, false, false, cutlass::bfloat16_t, Flash_kernel_traits<192, 128, 64, 8, cutlass::bfloat16_t>>, Is_dropout=false, Is_causal=false]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(183): here
            instantiation of "void run_mha_fwd_hdim192<T>(Flash_fwd_params &, cudaStream_t) [with T=cutlass::bfloat16_t]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_hdim192_bf16_sm80.cu(15): here

/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_kernel.h(276): warning: variable "tQrQ" was declared but never referenced
          detected during:
            instantiation of "void flash::compute_attn_1rowblock<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params,Is_Q_split_K>(const Params &, int, int, int) [with Kernel_traits=Flash_fwd_kernel_traits<192, 128, 64, 8, false, false, cutlass::bfloat16_t, Flash_kernel_traits<192, 128, 64, 8, cutlass::bfloat16_t>>, Is_dropout=false, Is_causal=false, Is_even_N=false, Is_even_K=true, Return_softmax=false, Params=Flash_fwd_params, Is_Q_split_K=false]" 
(589): here
            instantiation of "void flash::compute_attn<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &) [with Kernel_traits=Flash_fwd_kernel_traits<192, 128, 64, 8, false, false, cutlass::bfloat16_t, Flash_kernel_traits<192, 128, 64, 8, cutlass::bfloat16_t>>, Is_dropout=false, Is_causal=false, Is_even_N=false, Is_even_K=true, Return_softmax=false, Params=Flash_fwd_params]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(15): here
            instantiation of "void flash_fwd_kernel<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax>(Flash_fwd_params) [with Kernel_traits=Flash_fwd_kernel_traits<192, 128, 64, 8, false, false, cutlass::bfloat16_t, Flash_kernel_traits<192, 128, 64, 8, cutlass::bfloat16_t>>, Is_dropout=false, Is_causal=false, Is_even_N=false, Is_even_K=true, Return_softmax=false]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(34): here
            instantiation of "void run_flash_fwd<Kernel_traits,Is_dropout,Is_causal>(Flash_fwd_params &, cudaStream_t) [with Kernel_traits=Flash_fwd_kernel_traits<192, 128, 64, 8, false, false, cutlass::bfloat16_t, Flash_kernel_traits<192, 128, 64, 8, cutlass::bfloat16_t>>, Is_dropout=false, Is_causal=false]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(183): here
            instantiation of "void run_mha_fwd_hdim192<T>(Flash_fwd_params &, cudaStream_t) [with T=cutlass::bfloat16_t]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_hdim192_bf16_sm80.cu(15): here

/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_kernel.h(140): warning: variable "MMA_M" was declared but never referenced
          detected during:
            instantiation of "void flash::compute_attn_1rowblock<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params,Is_Q_split_K>(const Params &, int, int, int) [with Kernel_traits=Flash_fwd_kernel_traits<192, 128, 64, 8, false, false, cutlass::bfloat16_t, Flash_kernel_traits<192, 128, 64, 8, cutlass::bfloat16_t>>, Is_dropout=false, Is_causal=false, Is_even_N=false, Is_even_K=false, Return_softmax=false, Params=Flash_fwd_params, Is_Q_split_K=false]" 
(589): here
            instantiation of "void flash::compute_attn<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &) [with Kernel_traits=Flash_fwd_kernel_traits<192, 128, 64, 8, false, false, cutlass::bfloat16_t, Flash_kernel_traits<192, 128, 64, 8, cutlass::bfloat16_t>>, Is_dropout=false, Is_causal=false, Is_even_N=false, Is_even_K=false, Return_softmax=false, Params=Flash_fwd_params]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(15): here
            instantiation of "void flash_fwd_kernel<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax>(Flash_fwd_params) [with Kernel_traits=Flash_fwd_kernel_traits<192, 128, 64, 8, false, false, cutlass::bfloat16_t, Flash_kernel_traits<192, 128, 64, 8, cutlass::bfloat16_t>>, Is_dropout=false, Is_causal=false, Is_even_N=false, Is_even_K=false, Return_softmax=false]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(34): here
            instantiation of "void run_flash_fwd<Kernel_traits,Is_dropout,Is_causal>(Flash_fwd_params &, cudaStream_t) [with Kernel_traits=Flash_fwd_kernel_traits<192, 128, 64, 8, false, false, cutlass::bfloat16_t, Flash_kernel_traits<192, 128, 64, 8, cutlass::bfloat16_t>>, Is_dropout=false, Is_causal=false]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(183): here
            instantiation of "void run_mha_fwd_hdim192<T>(Flash_fwd_params &, cudaStream_t) [with T=cutlass::bfloat16_t]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_hdim192_bf16_sm80.cu(15): here

/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_kernel.h(276): warning: variable "tQrQ" was declared but never referenced
          detected during:
            instantiation of "void flash::compute_attn_1rowblock<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params,Is_Q_split_K>(const Params &, int, int, int) [with Kernel_traits=Flash_fwd_kernel_traits<192, 128, 64, 8, false, false, cutlass::bfloat16_t, Flash_kernel_traits<192, 128, 64, 8, cutlass::bfloat16_t>>, Is_dropout=false, Is_causal=false, Is_even_N=false, Is_even_K=false, Return_softmax=false, Params=Flash_fwd_params, Is_Q_split_K=false]" 
(589): here
            instantiation of "void flash::compute_attn<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &) [with Kernel_traits=Flash_fwd_kernel_traits<192, 128, 64, 8, false, false, cutlass::bfloat16_t, Flash_kernel_traits<192, 128, 64, 8, cutlass::bfloat16_t>>, Is_dropout=false, Is_causal=false, Is_even_N=false, Is_even_K=false, Return_softmax=false, Params=Flash_fwd_params]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(15): here
            instantiation of "void flash_fwd_kernel<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax>(Flash_fwd_params) [with Kernel_traits=Flash_fwd_kernel_traits<192, 128, 64, 8, false, false, cutlass::bfloat16_t, Flash_kernel_traits<192, 128, 64, 8, cutlass::bfloat16_t>>, Is_dropout=false, Is_causal=false, Is_even_N=false, Is_even_K=false, Return_softmax=false]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(34): here
            instantiation of "void run_flash_fwd<Kernel_traits,Is_dropout,Is_causal>(Flash_fwd_params &, cudaStream_t) [with Kernel_traits=Flash_fwd_kernel_traits<192, 128, 64, 8, false, false, cutlass::bfloat16_t, Flash_kernel_traits<192, 128, 64, 8, cutlass::bfloat16_t>>, Is_dropout=false, Is_causal=false]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(183): here
            instantiation of "void run_mha_fwd_hdim192<T>(Flash_fwd_params &, cudaStream_t) [with T=cutlass::bfloat16_t]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_hdim192_bf16_sm80.cu(15): here

ptxas info    : 2 bytes gmem
ptxas info    : Compiling entry function '_Z16flash_fwd_kernelI23Flash_fwd_kernel_traitsILi192ELi128ELi64ELi8ELb0ELb0EN7cutlass10bfloat16_tE19Flash_kernel_traitsILi192ELi128ELi64ELi8ES2_EELb0ELb0ELb0ELb0ELb0EEv16Flash_fwd_params' for 'sm_80'
ptxas info    : Function properties for _Z16flash_fwd_kernelI23Flash_fwd_kernel_traitsILi192ELi128ELi64ELi8ELb0ELb0EN7cutlass10bfloat16_tE19Flash_kernel_traitsILi192ELi128ELi64ELi8ES2_EELb0ELb0ELb0ELb0ELb0EEv16Flash_fwd_params
    0 bytes stack frame, 0 bytes spill stores, 0 bytes spill loads
ptxas info    : Used 209 registers, 584 bytes cmem[0]
ptxas info    : Compiling entry function '_Z16flash_fwd_kernelI23Flash_fwd_kernel_traitsILi192ELi128ELi64ELi8ELb0ELb0EN7cutlass10bfloat16_tE19Flash_kernel_traitsILi192ELi128ELi64ELi8ES2_EELb0ELb0ELb0ELb1ELb0EEv16Flash_fwd_params' for 'sm_80'
ptxas info    : Function properties for _Z16flash_fwd_kernelI23Flash_fwd_kernel_traitsILi192ELi128ELi64ELi8ELb0ELb0EN7cutlass10bfloat16_tE19Flash_kernel_traitsILi192ELi128ELi64ELi8ES2_EELb0ELb0ELb0ELb1ELb0EEv16Flash_fwd_params
    0 bytes stack frame, 0 bytes spill stores, 0 bytes spill loads
ptxas info    : Used 179 registers, 584 bytes cmem[0]
ptxas info    : Compiling entry function '_Z16flash_fwd_kernelI23Flash_fwd_kernel_traitsILi192ELi128ELi64ELi8ELb0ELb0EN7cutlass10bfloat16_tE19Flash_kernel_traitsILi192ELi128ELi64ELi8ES2_EELb0ELb0ELb1ELb0ELb0EEv16Flash_fwd_params' for 'sm_80'
ptxas info    : Function properties for _Z16flash_fwd_kernelI23Flash_fwd_kernel_traitsILi192ELi128ELi64ELi8ELb0ELb0EN7cutlass10bfloat16_tE19Flash_kernel_traitsILi192ELi128ELi64ELi8ES2_EELb0ELb0ELb1ELb0ELb0EEv16Flash_fwd_params
    0 bytes stack frame, 0 bytes spill stores, 0 bytes spill loads
ptxas info    : Used 173 registers, 584 bytes cmem[0]
ptxas info    : Compiling entry function '_Z16flash_fwd_kernelI23Flash_fwd_kernel_traitsILi192ELi128ELi64ELi8ELb0ELb0EN7cutlass10bfloat16_tE19Flash_kernel_traitsILi192ELi128ELi64ELi8ES2_EELb0ELb0ELb1ELb1ELb0EEv16Flash_fwd_params' for 'sm_80'
ptxas info    : Function properties for _Z16flash_fwd_kernelI23Flash_fwd_kernel_traitsILi192ELi128ELi64ELi8ELb0ELb0EN7cutlass10bfloat16_tE19Flash_kernel_traitsILi192ELi128ELi64ELi8ES2_EELb0ELb0ELb1ELb1ELb0EEv16Flash_fwd_params
    0 bytes stack frame, 0 bytes spill stores, 0 bytes spill loads
ptxas info    : Used 173 registers, 584 bytes cmem[0]
ptxas info    : Compiling entry function '_Z16flash_fwd_kernelI23Flash_fwd_kernel_traitsILi192ELi128ELi64ELi8ELb0ELb0EN7cutlass10bfloat16_tE19Flash_kernel_traitsILi192ELi128ELi64ELi8ES2_EELb0ELb1ELb0ELb0ELb0EEv16Flash_fwd_params' for 'sm_80'
ptxas info    : Function properties for _Z16flash_fwd_kernelI23Flash_fwd_kernel_traitsILi192ELi128ELi64ELi8ELb0ELb0EN7cutlass10bfloat16_tE19Flash_kernel_traitsILi192ELi128ELi64ELi8ES2_EELb0ELb1ELb0ELb0ELb0EEv16Flash_fwd_params
    0 bytes stack frame, 0 bytes spill stores, 0 bytes spill loads
ptxas info    : Used 220 registers, 584 bytes cmem[0]
ptxas info    : Compiling entry function '_Z16flash_fwd_kernelI23Flash_fwd_kernel_traitsILi192ELi128ELi64ELi8ELb0ELb0EN7cutlass10bfloat16_tE19Flash_kernel_traitsILi192ELi128ELi64ELi8ES2_EELb0ELb1ELb0ELb1ELb0EEv16Flash_fwd_params' for 'sm_80'
ptxas info    : Function properties for _Z16flash_fwd_kernelI23Flash_fwd_kernel_traitsILi192ELi128ELi64ELi8ELb0ELb0EN7cutlass10bfloat16_tE19Flash_kernel_traitsILi192ELi128ELi64ELi8ES2_EELb0ELb1ELb0ELb1ELb0EEv16Flash_fwd_params
    0 bytes stack frame, 0 bytes spill stores, 0 bytes spill loads
ptxas info    : Used 221 registers, 584 bytes cmem[0]
ptxas info    : Compiling entry function '_Z16flash_fwd_kernelI23Flash_fwd_kernel_traitsILi192ELi128ELi64ELi8ELb0ELb0EN7cutlass10bfloat16_tE19Flash_kernel_traitsILi192ELi128ELi64ELi8ES2_EELb0ELb1ELb1ELb0ELb0EEv16Flash_fwd_params' for 'sm_80'
ptxas info    : Function properties for _Z16flash_fwd_kernelI23Flash_fwd_kernel_traitsILi192ELi128ELi64ELi8ELb0ELb0EN7cutlass10bfloat16_tE19Flash_kernel_traitsILi192ELi128ELi64ELi8ES2_EELb0ELb1ELb1ELb0ELb0EEv16Flash_fwd_params
    0 bytes stack frame, 0 bytes spill stores, 0 bytes spill loads
ptxas info    : Used 225 registers, 584 bytes cmem[0]
ptxas info    : Compiling entry function '_Z16flash_fwd_kernelI23Flash_fwd_kernel_traitsILi192ELi128ELi64ELi8ELb0ELb0EN7cutlass10bfloat16_tE19Flash_kernel_traitsILi192ELi128ELi64ELi8ES2_EELb0ELb1ELb1ELb1ELb0EEv16Flash_fwd_params' for 'sm_80'
ptxas info    : Function properties for _Z16flash_fwd_kernelI23Flash_fwd_kernel_traitsILi192ELi128ELi64ELi8ELb0ELb0EN7cutlass10bfloat16_tE19Flash_kernel_traitsILi192ELi128ELi64ELi8ES2_EELb0ELb1ELb1ELb1ELb0EEv16Flash_fwd_params
    0 bytes stack frame, 0 bytes spill stores, 0 bytes spill loads
ptxas info    : Used 230 registers, 584 bytes cmem[0]
ptxas info    : Compiling entry function '_Z16flash_fwd_kernelI23Flash_fwd_kernel_traitsILi192ELi64ELi64ELi4ELb0ELb0EN7cutlass10bfloat16_tE19Flash_kernel_traitsILi192ELi64ELi64ELi4ES2_EELb1ELb0ELb0ELb0ELb0EEv16Flash_fwd_params' for 'sm_80'
ptxas info    : Function properties for _Z16flash_fwd_kernelI23Flash_fwd_kernel_traitsILi192ELi64ELi64ELi4ELb0ELb0EN7cutlass10bfloat16_tE19Flash_kernel_traitsILi192ELi64ELi64ELi4ES2_EELb1ELb0ELb0ELb0ELb0EEv16Flash_fwd_params
    0 bytes stack frame, 0 bytes spill stores, 0 bytes spill loads
ptxas info    : Used 222 registers, 584 bytes cmem[0]
ptxas info    : Compiling entry function '_Z16flash_fwd_kernelI23Flash_fwd_kernel_traitsILi192ELi64ELi64ELi4ELb0ELb0EN7cutlass10bfloat16_tE19Flash_kernel_traitsILi192ELi64ELi64ELi4ES2_EELb1ELb0ELb0ELb0ELb1EEv16Flash_fwd_params' for 'sm_80'
ptxas info    : Function properties for _Z16flash_fwd_kernelI23Flash_fwd_kernel_traitsILi192ELi64ELi64ELi4ELb0ELb0EN7cutlass10bfloat16_tE19Flash_kernel_traitsILi192ELi64ELi64ELi4ES2_EELb1ELb0ELb0ELb0ELb1EEv16Flash_fwd_params
    0 bytes stack frame, 0 bytes spill stores, 0 bytes spill loads
ptxas info    : Used 242 registers, 584 bytes cmem[0]
ptxas info    : Compiling entry function '_Z16flash_fwd_kernelI23Flash_fwd_kernel_traitsILi192ELi64ELi64ELi4ELb0ELb0EN7cutlass10bfloat16_tE19Flash_kernel_traitsILi192ELi64ELi64ELi4ES2_EELb1ELb0ELb0ELb1ELb0EEv16Flash_fwd_params' for 'sm_80'
ptxas info    : Function properties for _Z16flash_fwd_kernelI23Flash_fwd_kernel_traitsILi192ELi64ELi64ELi4ELb0ELb0EN7cutlass10bfloat16_tE19Flash_kernel_traitsILi192ELi64ELi64ELi4ES2_EELb1ELb0ELb0ELb1ELb0EEv16Flash_fwd_params
    0 bytes stack frame, 0 bytes spill stores, 0 bytes spill loads
ptxas info    : Used 221 registers, 584 bytes cmem[0]
ptxas info    : Compiling entry function '_Z16flash_fwd_kernelI23Flash_fwd_kernel_traitsILi192ELi64ELi64ELi4ELb0ELb0EN7cutlass10bfloat16_tE19Flash_kernel_traitsILi192ELi64ELi64ELi4ES2_EELb1ELb0ELb0ELb1ELb1EEv16Flash_fwd_params' for 'sm_80'
ptxas info    : Function properties for _Z16flash_fwd_kernelI23Flash_fwd_kernel_traitsILi192ELi64ELi64ELi4ELb0ELb0EN7cutlass10bfloat16_tE19Flash_kernel_traitsILi192ELi64ELi64ELi4ES2_EELb1ELb0ELb0ELb1ELb1EEv16Flash_fwd_params
    0 bytes stack frame, 0 bytes spill stores, 0 bytes spill loads
ptxas info    : Used 250 registers, 584 bytes cmem[0]
ptxas info    : Compiling entry function '_Z16flash_fwd_kernelI23Flash_fwd_kernel_traitsILi192ELi64ELi64ELi4ELb0ELb0EN7cutlass10bfloat16_tE19Flash_kernel_traitsILi192ELi64ELi64ELi4ES2_EELb1ELb0ELb1ELb0ELb0EEv16Flash_fwd_params' for 'sm_80'
ptxas info    : Function properties for _Z16flash_fwd_kernelI23Flash_fwd_kernel_traitsILi192ELi64ELi64ELi4ELb0ELb0EN7cutlass10bfloat16_tE19Flash_kernel_traitsILi192ELi64ELi64ELi4ES2_EELb1ELb0ELb1ELb0ELb0EEv16Flash_fwd_params
    0 bytes stack frame, 0 bytes spill stores, 0 bytes spill loads
ptxas info    : Used 223 registers, 584 bytes cmem[0]
ptxas info    : Compiling entry function '_Z16flash_fwd_kernelI23Flash_fwd_kernel_traitsILi192ELi64ELi64ELi4ELb0ELb0EN7cutlass10bfloat16_tE19Flash_kernel_traitsILi192ELi64ELi64ELi4ES2_EELb1ELb0ELb1ELb0ELb1EEv16Flash_fwd_params' for 'sm_80'
ptxas info    : Function properties for _Z16flash_fwd_kernelI23Flash_fwd_kernel_traitsILi192ELi64ELi64ELi4ELb0ELb0EN7cutlass10bfloat16_tE19Flash_kernel_traitsILi192ELi64ELi64ELi4ES2_EELb1ELb0ELb1ELb0ELb1EEv16Flash_fwd_params
    0 bytes stack frame, 0 bytes spill stores, 0 bytes spill loads
ptxas info    : Used 240 registers, 584 bytes cmem[0]
ptxas info    : Compiling entry function '_Z16flash_fwd_kernelI23Flash_fwd_kernel_traitsILi192ELi64ELi64ELi4ELb0ELb0EN7cutlass10bfloat16_tE19Flash_kernel_traitsILi192ELi64ELi64ELi4ES2_EELb1ELb0ELb1ELb1ELb0EEv16Flash_fwd_params' for 'sm_80'
ptxas info    : Function properties for _Z16flash_fwd_kernelI23Flash_fwd_kernel_traitsILi192ELi64ELi64ELi4ELb0ELb0EN7cutlass10bfloat16_tE19Flash_kernel_traitsILi192ELi64ELi64ELi4ES2_EELb1ELb0ELb1ELb1ELb0EEv16Flash_fwd_params
    0 bytes stack frame, 0 bytes spill stores, 0 bytes spill loads
ptxas info    : Used 227 registers, 584 bytes cmem[0]
ptxas info    : Compiling entry function '_Z16flash_fwd_kernelI23Flash_fwd_kernel_traitsILi192ELi64ELi64ELi4ELb0ELb0EN7cutlass10bfloat16_tE19Flash_kernel_traitsILi192ELi64ELi64ELi4ES2_EELb1ELb0ELb1ELb1ELb1EEv16Flash_fwd_params' for 'sm_80'
ptxas info    : Function properties for _Z16flash_fwd_kernelI23Flash_fwd_kernel_traitsILi192ELi64ELi64ELi4ELb0ELb0EN7cutlass10bfloat16_tE19Flash_kernel_traitsILi192ELi64ELi64ELi4ES2_EELb1ELb0ELb1ELb1ELb1EEv16Flash_fwd_params
    0 bytes stack frame, 0 bytes spill stores, 0 bytes spill loads
ptxas info    : Used 246 registers, 584 bytes cmem[0]
ptxas info    : Compiling entry function '_Z16flash_fwd_kernelI23Flash_fwd_kernel_traitsILi192ELi64ELi64ELi4ELb0ELb0EN7cutlass10bfloat16_tE19Flash_kernel_traitsILi192ELi64ELi64ELi4ES2_EELb1ELb1ELb0ELb0ELb0EEv16Flash_fwd_params' for 'sm_80'
ptxas info    : Function properties for _Z16flash_fwd_kernelI23Flash_fwd_kernel_traitsILi192ELi64ELi64ELi4ELb0ELb0EN7cutlass10bfloat16_tE19Flash_kernel_traitsILi192ELi64ELi64ELi4ES2_EELb1ELb1ELb0ELb0ELb0EEv16Flash_fwd_params
    0 bytes stack frame, 0 bytes spill stores, 0 bytes spill loads
ptxas info    : Used 222 registers, 584 bytes cmem[0]
ptxas info    : Compiling entry function '_Z16flash_fwd_kernelI23Flash_fwd_kernel_traitsILi192ELi64ELi64ELi4ELb0ELb0EN7cutlass10bfloat16_tE19Flash_kernel_traitsILi192ELi64ELi64ELi4ES2_EELb1ELb1ELb0ELb0ELb1EEv16Flash_fwd_params' for 'sm_80'
ptxas info    : Function properties for _Z16flash_fwd_kernelI23Flash_fwd_kernel_traitsILi192ELi64ELi64ELi4ELb0ELb0EN7cutlass10bfloat16_tE19Flash_kernel_traitsILi192ELi64ELi64ELi4ES2_EELb1ELb1ELb0ELb0ELb1EEv16Flash_fwd_params
    0 bytes stack frame, 0 bytes spill stores, 0 bytes spill loads
ptxas info    : Used 250 registers, 584 bytes cmem[0]
ptxas info    : Compiling entry function '_Z16flash_fwd_kernelI23Flash_fwd_kernel_traitsILi192ELi64ELi64ELi4ELb0ELb0EN7cutlass10bfloat16_tE19Flash_kernel_traitsILi192ELi64ELi64ELi4ES2_EELb1ELb1ELb0ELb1ELb0EEv16Flash_fwd_params' for 'sm_80'
ptxas info    : Function properties for _Z16flash_fwd_kernelI23Flash_fwd_kernel_traitsILi192ELi64ELi64ELi4ELb0ELb0EN7cutlass10bfloat16_tE19Flash_kernel_traitsILi192ELi64ELi64ELi4ES2_EELb1ELb1ELb0ELb1ELb0EEv16Flash_fwd_params
    0 bytes stack frame, 0 bytes spill stores, 0 bytes spill loads
ptxas info    : Used 223 registers, 584 bytes cmem[0]
ptxas info    : Compiling entry function '_Z16flash_fwd_kernelI23Flash_fwd_kernel_traitsILi192ELi64ELi64ELi4ELb0ELb0EN7cutlass10bfloat16_tE19Flash_kernel_traitsILi192ELi64ELi64ELi4ES2_EELb1ELb1ELb0ELb1ELb1EEv16Flash_fwd_params' for 'sm_80'
ptxas info    : Function properties for _Z16flash_fwd_kernelI23Flash_fwd_kernel_traitsILi192ELi64ELi64ELi4ELb0ELb0EN7cutlass10bfloat16_tE19Flash_kernel_traitsILi192ELi64ELi64ELi4ES2_EELb1ELb1ELb0ELb1ELb1EEv16Flash_fwd_params
    0 bytes stack frame, 0 bytes spill stores, 0 bytes spill loads
ptxas info    : Used 250 registers, 584 bytes cmem[0]
ptxas info    : Compiling entry function '_Z16flash_fwd_kernelI23Flash_fwd_kernel_traitsILi192ELi64ELi64ELi4ELb0ELb0EN7cutlass10bfloat16_tE19Flash_kernel_traitsILi192ELi64ELi64ELi4ES2_EELb1ELb1ELb1ELb0ELb0EEv16Flash_fwd_params' for 'sm_80'
ptxas info    : Function properties for _Z16flash_fwd_kernelI23Flash_fwd_kernel_traitsILi192ELi64ELi64ELi4ELb0ELb0EN7cutlass10bfloat16_tE19Flash_kernel_traitsILi192ELi64ELi64ELi4ES2_EELb1ELb1ELb1ELb0ELb0EEv16Flash_fwd_params
    0 bytes stack frame, 0 bytes spill stores, 0 bytes spill loads
ptxas info    : Used 231 registers, 584 bytes cmem[0]
ptxas info    : Compiling entry function '_Z16flash_fwd_kernelI23Flash_fwd_kernel_traitsILi192ELi64ELi64ELi4ELb0ELb0EN7cutlass10bfloat16_tE19Flash_kernel_traitsILi192ELi64ELi64ELi4ES2_EELb1ELb1ELb1ELb0ELb1EEv16Flash_fwd_params' for 'sm_80'
ptxas info    : Function properties for _Z16flash_fwd_kernelI23Flash_fwd_kernel_traitsILi192ELi64ELi64ELi4ELb0ELb0EN7cutlass10bfloat16_tE19Flash_kernel_traitsILi192ELi64ELi64ELi4ES2_EELb1ELb1ELb1ELb0ELb1EEv16Flash_fwd_params
    0 bytes stack frame, 0 bytes spill stores, 0 bytes spill loads
ptxas info    : Used 252 registers, 584 bytes cmem[0]
ptxas info    : Compiling entry function '_Z16flash_fwd_kernelI23Flash_fwd_kernel_traitsILi192ELi64ELi64ELi4ELb0ELb0EN7cutlass10bfloat16_tE19Flash_kernel_traitsILi192ELi64ELi64ELi4ES2_EELb1ELb1ELb1ELb1ELb0EEv16Flash_fwd_params' for 'sm_80'
ptxas info    : Function properties for _Z16flash_fwd_kernelI23Flash_fwd_kernel_traitsILi192ELi64ELi64ELi4ELb0ELb0EN7cutlass10bfloat16_tE19Flash_kernel_traitsILi192ELi64ELi64ELi4ES2_EELb1ELb1ELb1ELb1ELb0EEv16Flash_fwd_params
    0 bytes stack frame, 0 bytes spill stores, 0 bytes spill loads
ptxas info    : Used 226 registers, 584 bytes cmem[0]
ptxas info    : Compiling entry function '_Z16flash_fwd_kernelI23Flash_fwd_kernel_traitsILi192ELi64ELi64ELi4ELb0ELb0EN7cutlass10bfloat16_tE19Flash_kernel_traitsILi192ELi64ELi64ELi4ES2_EELb1ELb1ELb1ELb1ELb1EEv16Flash_fwd_params' for 'sm_80'
ptxas info    : Function properties for _Z16flash_fwd_kernelI23Flash_fwd_kernel_traitsILi192ELi64ELi64ELi4ELb0ELb0EN7cutlass10bfloat16_tE19Flash_kernel_traitsILi192ELi64ELi64ELi4ES2_EELb1ELb1ELb1ELb1ELb1EEv16Flash_fwd_params
    0 bytes stack frame, 0 bytes spill stores, 0 bytes spill loads
ptxas info    : Used 254 registers, 584 bytes cmem[0]
/anaconda/envs/flashattention/lib/python3.9/site-packages/torch/include/c10/core/SymInt.h(84): warning: integer conversion resulted in a change of sign

[6/37] /usr/local/cuda-11.4/bin/nvcc  -I/home/v-feiychen/flash-attention/csrc/flash_attn -I/home/v-feiychen/flash-attention/csrc/flash_attn/src -I/home/v-feiychen/flash-attention/csrc/cutlass/include -I/anaconda/envs/flashattention/lib/python3.9/site-packages/torch/include -I/anaconda/envs/flashattention/lib/python3.9/site-packages/torch/include/torch/csrc/api/include -I/anaconda/envs/flashattention/lib/python3.9/site-packages/torch/include/TH -I/anaconda/envs/flashattention/lib/python3.9/site-packages/torch/include/THC -I/usr/local/cuda-11.4/include -I/anaconda/envs/flashattention/include/python3.9 -c -c /home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_bwd_hdim32_fp16_sm80.cu -o /home/v-feiychen/flash-attention/build/temp.linux-x86_64-cpython-39/csrc/flash_attn/src/flash_bwd_hdim32_fp16_sm80.o -D__CUDA_NO_HALF_OPERATORS__ -D__CUDA_NO_HALF_CONVERSIONS__ -D__CUDA_NO_BFLOAT16_CONVERSIONS__ -D__CUDA_NO_HALF2_OPERATORS__ --expt-relaxed-constexpr --compiler-options ''"'"'-fPIC'"'"'' -O3 -std=c++17 -U__CUDA_NO_HALF_OPERATORS__ -U__CUDA_NO_HALF_CONVERSIONS__ -U__CUDA_NO_HALF2_OPERATORS__ -U__CUDA_NO_BFLOAT16_CONVERSIONS__ --expt-relaxed-constexpr --expt-extended-lambda --use_fast_math --ptxas-options=-v -lineinfo -gencode arch=compute_80,code=sm_80 --threads 4 -DTORCH_API_INCLUDE_EXTENSION_H '-DPYBIND11_COMPILER_TYPE="_gcc"' '-DPYBIND11_STDLIB="_libstdcpp"' '-DPYBIND11_BUILD_ABI="_cxxabi1011"' -DTORCH_EXTENSION_NAME=flash_attn_2_cuda -D_GLIBCXX_USE_CXX11_ABI=0
/anaconda/envs/flashattention/lib/python3.9/site-packages/torch/include/c10/core/SymInt.h(84): warning: integer conversion resulted in a change of sign

ptxas info    : 2 bytes gmem
ptxas info    : Compiling entry function '_Z44flash_bwd_dq_dk_dv_loop_seqk_parallel_kernelI23Flash_bwd_kernel_traitsILi32ELi128ELi128ELi8ELi4ELi4ELi4ELb1ELb0EN7cutlass6half_tE19Flash_kernel_traitsILi32ELi128ELi128ELi8ES2_EELb0ELb0ELb0ELb0EEv16Flash_bwd_params' for 'sm_80'
ptxas info    : Function properties for _Z44flash_bwd_dq_dk_dv_loop_seqk_parallel_kernelI23Flash_bwd_kernel_traitsILi32ELi128ELi128ELi8ELi4ELi4ELi4ELb1ELb0EN7cutlass6half_tE19Flash_kernel_traitsILi32ELi128ELi128ELi8ES2_EELb0ELb0ELb0ELb0EEv16Flash_bwd_params
    0 bytes stack frame, 0 bytes spill stores, 0 bytes spill loads
ptxas info    : Used 254 registers, 696 bytes cmem[0]
ptxas info    : Compiling entry function '_Z44flash_bwd_dq_dk_dv_loop_seqk_parallel_kernelI23Flash_bwd_kernel_traitsILi32ELi128ELi128ELi8ELi4ELi4ELi4ELb1ELb0EN7cutlass6half_tE19Flash_kernel_traitsILi32ELi128ELi128ELi8ES2_EELb0ELb0ELb0ELb1EEv16Flash_bwd_params' for 'sm_80'
ptxas info    : Function properties for _Z44flash_bwd_dq_dk_dv_loop_seqk_parallel_kernelI23Flash_bwd_kernel_traitsILi32ELi128ELi128ELi8ELi4ELi4ELi4ELb1ELb0EN7cutlass6half_tE19Flash_kernel_traitsILi32ELi128ELi128ELi8ES2_EELb0ELb0ELb0ELb1EEv16Flash_bwd_params
    0 bytes stack frame, 0 bytes spill stores, 0 bytes spill loads
ptxas info    : Used 254 registers, 696 bytes cmem[0]
ptxas info    : Compiling entry function '_Z44flash_bwd_dq_dk_dv_loop_seqk_parallel_kernelI23Flash_bwd_kernel_traitsILi32ELi128ELi128ELi8ELi4ELi4ELi4ELb1ELb0EN7cutlass6half_tE19Flash_kernel_traitsILi32ELi128ELi128ELi8ES2_EELb0ELb0ELb1ELb0EEv16Flash_bwd_params' for 'sm_80'
ptxas info    : Function properties for _Z44flash_bwd_dq_dk_dv_loop_seqk_parallel_kernelI23Flash_bwd_kernel_traitsILi32ELi128ELi128ELi8ELi4ELi4ELi4ELb1ELb0EN7cutlass6half_tE19Flash_kernel_traitsILi32ELi128ELi128ELi8ES2_EELb0ELb0ELb1ELb0EEv16Flash_bwd_params
    0 bytes stack frame, 0 bytes spill stores, 0 bytes spill loads
ptxas info    : Used 251 registers, 696 bytes cmem[0]
ptxas info    : Compiling entry function '_Z44flash_bwd_dq_dk_dv_loop_seqk_parallel_kernelI23Flash_bwd_kernel_traitsILi32ELi128ELi128ELi8ELi4ELi4ELi4ELb1ELb0EN7cutlass6half_tE19Flash_kernel_traitsILi32ELi128ELi128ELi8ES2_EELb0ELb0ELb1ELb1EEv16Flash_bwd_params' for 'sm_80'
ptxas info    : Function properties for _Z44flash_bwd_dq_dk_dv_loop_seqk_parallel_kernelI23Flash_bwd_kernel_traitsILi32ELi128ELi128ELi8ELi4ELi4ELi4ELb1ELb0EN7cutlass6half_tE19Flash_kernel_traitsILi32ELi128ELi128ELi8ES2_EELb0ELb0ELb1ELb1EEv16Flash_bwd_params
    0 bytes stack frame, 0 bytes spill stores, 0 bytes spill loads
ptxas info    : Used 251 registers, 696 bytes cmem[0]
ptxas info    : Compiling entry function '_Z44flash_bwd_dq_dk_dv_loop_seqk_parallel_kernelI23Flash_bwd_kernel_traitsILi32ELi128ELi128ELi8ELi4ELi4ELi4ELb1ELb0EN7cutlass6half_tE19Flash_kernel_traitsILi32ELi128ELi128ELi8ES2_EELb0ELb1ELb0ELb0EEv16Flash_bwd_params' for 'sm_80'
ptxas info    : Function properties for _Z44flash_bwd_dq_dk_dv_loop_seqk_parallel_kernelI23Flash_bwd_kernel_traitsILi32ELi128ELi128ELi8ELi4ELi4ELi4ELb1ELb0EN7cutlass6half_tE19Flash_kernel_traitsILi32ELi128ELi128ELi8ES2_EELb0ELb1ELb0ELb0EEv16Flash_bwd_params
    0 bytes stack frame, 0 bytes spill stores, 0 bytes spill loads
ptxas info    : Used 253 registers, 696 bytes cmem[0]
ptxas info    : Compiling entry function '_Z44flash_bwd_dq_dk_dv_loop_seqk_parallel_kernelI23Flash_bwd_kernel_traitsILi32ELi128ELi128ELi8ELi4ELi4ELi4ELb1ELb0EN7cutlass6half_tE19Flash_kernel_traitsILi32ELi128ELi128ELi8ES2_EELb0ELb1ELb0ELb1EEv16Flash_bwd_params' for 'sm_80'
ptxas info    : Function properties for _Z44flash_bwd_dq_dk_dv_loop_seqk_parallel_kernelI23Flash_bwd_kernel_traitsILi32ELi128ELi128ELi8ELi4ELi4ELi4ELb1ELb0EN7cutlass6half_tE19Flash_kernel_traitsILi32ELi128ELi128ELi8ES2_EELb0ELb1ELb0ELb1EEv16Flash_bwd_params
    0 bytes stack frame, 0 bytes spill stores, 0 bytes spill loads
ptxas info    : Used 252 registers, 696 bytes cmem[0]
ptxas info    : Compiling entry function '_Z44flash_bwd_dq_dk_dv_loop_seqk_parallel_kernelI23Flash_bwd_kernel_traitsILi32ELi128ELi128ELi8ELi4ELi4ELi4ELb1ELb0EN7cutlass6half_tE19Flash_kernel_traitsILi32ELi128ELi128ELi8ES2_EELb0ELb1ELb1ELb0EEv16Flash_bwd_params' for 'sm_80'
ptxas info    : Function properties for _Z44flash_bwd_dq_dk_dv_loop_seqk_parallel_kernelI23Flash_bwd_kernel_traitsILi32ELi128ELi128ELi8ELi4ELi4ELi4ELb1ELb0EN7cutlass6half_tE19Flash_kernel_traitsILi32ELi128ELi128ELi8ES2_EELb0ELb1ELb1ELb0EEv16Flash_bwd_params
    0 bytes stack frame, 0 bytes spill stores, 0 bytes spill loads
ptxas info    : Used 246 registers, 696 bytes cmem[0]
ptxas info    : Compiling entry function '_Z44flash_bwd_dq_dk_dv_loop_seqk_parallel_kernelI23Flash_bwd_kernel_traitsILi32ELi128ELi128ELi8ELi4ELi4ELi4ELb1ELb0EN7cutlass6half_tE19Flash_kernel_traitsILi32ELi128ELi128ELi8ES2_EELb0ELb1ELb1ELb1EEv16Flash_bwd_params' for 'sm_80'
ptxas info    : Function properties for _Z44flash_bwd_dq_dk_dv_loop_seqk_parallel_kernelI23Flash_bwd_kernel_traitsILi32ELi128ELi128ELi8ELi4ELi4ELi4ELb1ELb0EN7cutlass6half_tE19Flash_kernel_traitsILi32ELi128ELi128ELi8ES2_EELb0ELb1ELb1ELb1EEv16Flash_bwd_params
    0 bytes stack frame, 0 bytes spill stores, 0 bytes spill loads
ptxas info    : Used 249 registers, 696 bytes cmem[0]
ptxas info    : Compiling entry function '_Z27flash_bwd_convert_dq_kernelI23Flash_bwd_kernel_traitsILi32ELi128ELi128ELi8ELi4ELi4ELi4ELb1ELb0EN7cutlass6half_tE19Flash_kernel_traitsILi32ELi128ELi128ELi8ES2_EEEv16Flash_bwd_params' for 'sm_80'
ptxas info    : Function properties for _Z27flash_bwd_convert_dq_kernelI23Flash_bwd_kernel_traitsILi32ELi128ELi128ELi8ELi4ELi4ELi4ELb1ELb0EN7cutlass6half_tE19Flash_kernel_traitsILi32ELi128ELi128ELi8ES2_EEEv16Flash_bwd_params
    0 bytes stack frame, 0 bytes spill stores, 0 bytes spill loads
ptxas info    : Used 32 registers, 696 bytes cmem[0]
ptxas info    : Compiling entry function '_Z44flash_bwd_dq_dk_dv_loop_seqk_parallel_kernelI23Flash_bwd_kernel_traitsILi32ELi128ELi128ELi8ELi4ELi4ELi4ELb1ELb0EN7cutlass6half_tE19Flash_kernel_traitsILi32ELi128ELi128ELi8ES2_EELb1ELb0ELb0ELb0EEv16Flash_bwd_params' for 'sm_80'
ptxas info    : Function properties for _Z44flash_bwd_dq_dk_dv_loop_seqk_parallel_kernelI23Flash_bwd_kernel_traitsILi32ELi128ELi128ELi8ELi4ELi4ELi4ELb1ELb0EN7cutlass6half_tE19Flash_kernel_traitsILi32ELi128ELi128ELi8ES2_EELb1ELb0ELb0ELb0EEv16Flash_bwd_params
    0 bytes stack frame, 0 bytes spill stores, 0 bytes spill loads
ptxas info    : Used 253 registers, 696 bytes cmem[0]
ptxas info    : Compiling entry function '_Z44flash_bwd_dq_dk_dv_loop_seqk_parallel_kernelI23Flash_bwd_kernel_traitsILi32ELi128ELi128ELi8ELi4ELi4ELi4ELb1ELb0EN7cutlass6half_tE19Flash_kernel_traitsILi32ELi128ELi128ELi8ES2_EELb1ELb0ELb0ELb1EEv16Flash_bwd_params' for 'sm_80'
ptxas info    : Function properties for _Z44flash_bwd_dq_dk_dv_loop_seqk_parallel_kernelI23Flash_bwd_kernel_traitsILi32ELi128ELi128ELi8ELi4ELi4ELi4ELb1ELb0EN7cutlass6half_tE19Flash_kernel_traitsILi32ELi128ELi128ELi8ES2_EELb1ELb0ELb0ELb1EEv16Flash_bwd_params
    0 bytes stack frame, 0 bytes spill stores, 0 bytes spill loads
ptxas info    : Used 246 registers, 696 bytes cmem[0]
ptxas info    : Compiling entry function '_Z44flash_bwd_dq_dk_dv_loop_seqk_parallel_kernelI23Flash_bwd_kernel_traitsILi32ELi128ELi128ELi8ELi4ELi4ELi4ELb1ELb0EN7cutlass6half_tE19Flash_kernel_traitsILi32ELi128ELi128ELi8ES2_EELb1ELb0ELb1ELb0EEv16Flash_bwd_params' for 'sm_80'
ptxas info    : Function properties for _Z44flash_bwd_dq_dk_dv_loop_seqk_parallel_kernelI23Flash_bwd_kernel_traitsILi32ELi128ELi128ELi8ELi4ELi4ELi4ELb1ELb0EN7cutlass6half_tE19Flash_kernel_traitsILi32ELi128ELi128ELi8ES2_EELb1ELb0ELb1ELb0EEv16Flash_bwd_params
    0 bytes stack frame, 0 bytes spill stores, 0 bytes spill loads
ptxas info    : Used 245 registers, 696 bytes cmem[0]
ptxas info    : Compiling entry function '_Z44flash_bwd_dq_dk_dv_loop_seqk_parallel_kernelI23Flash_bwd_kernel_traitsILi32ELi128ELi128ELi8ELi4ELi4ELi4ELb1ELb0EN7cutlass6half_tE19Flash_kernel_traitsILi32ELi128ELi128ELi8ES2_EELb1ELb0ELb1ELb1EEv16Flash_bwd_params' for 'sm_80'
ptxas info    : Function properties for _Z44flash_bwd_dq_dk_dv_loop_seqk_parallel_kernelI23Flash_bwd_kernel_traitsILi32ELi128ELi128ELi8ELi4ELi4ELi4ELb1ELb0EN7cutlass6half_tE19Flash_kernel_traitsILi32ELi128ELi128ELi8ES2_EELb1ELb0ELb1ELb1EEv16Flash_bwd_params
    0 bytes stack frame, 0 bytes spill stores, 0 bytes spill loads
ptxas info    : Used 248 registers, 696 bytes cmem[0]
ptxas info    : Compiling entry function '_Z44flash_bwd_dq_dk_dv_loop_seqk_parallel_kernelI23Flash_bwd_kernel_traitsILi32ELi128ELi128ELi8ELi4ELi4ELi4ELb1ELb0EN7cutlass6half_tE19Flash_kernel_traitsILi32ELi128ELi128ELi8ES2_EELb1ELb1ELb0ELb0EEv16Flash_bwd_params' for 'sm_80'
ptxas info    : Function properties for _Z44flash_bwd_dq_dk_dv_loop_seqk_parallel_kernelI23Flash_bwd_kernel_traitsILi32ELi128ELi128ELi8ELi4ELi4ELi4ELb1ELb0EN7cutlass6half_tE19Flash_kernel_traitsILi32ELi128ELi128ELi8ES2_EELb1ELb1ELb0ELb0EEv16Flash_bwd_params
    0 bytes stack frame, 0 bytes spill stores, 0 bytes spill loads
ptxas info    : Used 251 registers, 696 bytes cmem[0]
ptxas info    : Compiling entry function '_Z44flash_bwd_dq_dk_dv_loop_seqk_parallel_kernelI23Flash_bwd_kernel_traitsILi32ELi128ELi128ELi8ELi4ELi4ELi4ELb1ELb0EN7cutlass6half_tE19Flash_kernel_traitsILi32ELi128ELi128ELi8ES2_EELb1ELb1ELb0ELb1EEv16Flash_bwd_params' for 'sm_80'
ptxas info    : Function properties for _Z44flash_bwd_dq_dk_dv_loop_seqk_parallel_kernelI23Flash_bwd_kernel_traitsILi32ELi128ELi128ELi8ELi4ELi4ELi4ELb1ELb0EN7cutlass6half_tE19Flash_kernel_traitsILi32ELi128ELi128ELi8ES2_EELb1ELb1ELb0ELb1EEv16Flash_bwd_params
    0 bytes stack frame, 0 bytes spill stores, 0 bytes spill loads
ptxas info    : Used 251 registers, 696 bytes cmem[0]
ptxas info    : Compiling entry function '_Z44flash_bwd_dq_dk_dv_loop_seqk_parallel_kernelI23Flash_bwd_kernel_traitsILi32ELi128ELi128ELi8ELi4ELi4ELi4ELb1ELb0EN7cutlass6half_tE19Flash_kernel_traitsILi32ELi128ELi128ELi8ES2_EELb1ELb1ELb1ELb0EEv16Flash_bwd_params' for 'sm_80'
ptxas info    : Function properties for _Z44flash_bwd_dq_dk_dv_loop_seqk_parallel_kernelI23Flash_bwd_kernel_traitsILi32ELi128ELi128ELi8ELi4ELi4ELi4ELb1ELb0EN7cutlass6half_tE19Flash_kernel_traitsILi32ELi128ELi128ELi8ES2_EELb1ELb1ELb1ELb0EEv16Flash_bwd_params
    0 bytes stack frame, 0 bytes spill stores, 0 bytes spill loads
ptxas info    : Used 255 registers, 696 bytes cmem[0]
ptxas info    : Compiling entry function '_Z44flash_bwd_dq_dk_dv_loop_seqk_parallel_kernelI23Flash_bwd_kernel_traitsILi32ELi128ELi128ELi8ELi4ELi4ELi4ELb1ELb0EN7cutlass6half_tE19Flash_kernel_traitsILi32ELi128ELi128ELi8ES2_EELb1ELb1ELb1ELb1EEv16Flash_bwd_params' for 'sm_80'
ptxas info    : Function properties for _Z44flash_bwd_dq_dk_dv_loop_seqk_parallel_kernelI23Flash_bwd_kernel_traitsILi32ELi128ELi128ELi8ELi4ELi4ELi4ELb1ELb0EN7cutlass6half_tE19Flash_kernel_traitsILi32ELi128ELi128ELi8ES2_EELb1ELb1ELb1ELb1EEv16Flash_bwd_params
    0 bytes stack frame, 0 bytes spill stores, 0 bytes spill loads
ptxas info    : Used 254 registers, 696 bytes cmem[0]
ptxas info    : Compiling entry function '_Z25flash_bwd_dot_do_o_kernelILb1E23Flash_bwd_kernel_traitsILi32ELi128ELi128ELi8ELi4ELi4ELi4ELb1ELb0EN7cutlass6half_tE19Flash_kernel_traitsILi32ELi128ELi128ELi8ES2_EEEv16Flash_bwd_params' for 'sm_80'
ptxas info    : Function properties for _Z25flash_bwd_dot_do_o_kernelILb1E23Flash_bwd_kernel_traitsILi32ELi128ELi128ELi8ELi4ELi4ELi4ELb1ELb0EN7cutlass6half_tE19Flash_kernel_traitsILi32ELi128ELi128ELi8ES2_EEEv16Flash_bwd_params
    0 bytes stack frame, 0 bytes spill stores, 0 bytes spill loads
ptxas info    : Used 31 registers, 696 bytes cmem[0]
ptxas info    : Compiling entry function '_Z27flash_bwd_convert_dq_kernelI23Flash_bwd_kernel_traitsILi32ELi128ELi128ELi8ELi4ELi4ELi4ELb0ELb0EN7cutlass6half_tE19Flash_kernel_traitsILi32ELi128ELi128ELi8ES2_EEEv16Flash_bwd_params' for 'sm_80'
ptxas info    : Function properties for _Z27flash_bwd_convert_dq_kernelI23Flash_bwd_kernel_traitsILi32ELi128ELi128ELi8ELi4ELi4ELi4ELb0ELb0EN7cutlass6half_tE19Flash_kernel_traitsILi32ELi128ELi128ELi8ES2_EEEv16Flash_bwd_params
    0 bytes stack frame, 0 bytes spill stores, 0 bytes spill loads
ptxas info    : Used 32 registers, 696 bytes cmem[0]
ptxas info    : Compiling entry function '_Z44flash_bwd_dq_dk_dv_loop_seqk_parallel_kernelI23Flash_bwd_kernel_traitsILi32ELi128ELi128ELi8ELi4ELi4ELi4ELb0ELb0EN7cutlass6half_tE19Flash_kernel_traitsILi32ELi128ELi128ELi8ES2_EELb1ELb0ELb0ELb0EEv16Flash_bwd_params' for 'sm_80'
ptxas info    : Function properties for _Z44flash_bwd_dq_dk_dv_loop_seqk_parallel_kernelI23Flash_bwd_kernel_traitsILi32ELi128ELi128ELi8ELi4ELi4ELi4ELb0ELb0EN7cutlass6half_tE19Flash_kernel_traitsILi32ELi128ELi128ELi8ES2_EELb1ELb0ELb0ELb0EEv16Flash_bwd_params
    0 bytes stack frame, 0 bytes spill stores, 0 bytes spill loads
ptxas info    : Used 240 registers, 696 bytes cmem[0]
ptxas info    : Compiling entry function '_Z44flash_bwd_dq_dk_dv_loop_seqk_parallel_kernelI23Flash_bwd_kernel_traitsILi32ELi128ELi128ELi8ELi4ELi4ELi4ELb0ELb0EN7cutlass6half_tE19Flash_kernel_traitsILi32ELi128ELi128ELi8ES2_EELb1ELb0ELb0ELb1EEv16Flash_bwd_params' for 'sm_80'
ptxas info    : Function properties for _Z44flash_bwd_dq_dk_dv_loop_seqk_parallel_kernelI23Flash_bwd_kernel_traitsILi32ELi128ELi128ELi8ELi4ELi4ELi4ELb0ELb0EN7cutlass6half_tE19Flash_kernel_traitsILi32ELi128ELi128ELi8ES2_EELb1ELb0ELb0ELb1EEv16Flash_bwd_params
    0 bytes stack frame, 0 bytes spill stores, 0 bytes spill loads
ptxas info    : Used 243 registers, 696 bytes cmem[0]
ptxas info    : Compiling entry function '_Z44flash_bwd_dq_dk_dv_loop_seqk_parallel_kernelI23Flash_bwd_kernel_traitsILi32ELi128ELi128ELi8ELi4ELi4ELi4ELb0ELb0EN7cutlass6half_tE19Flash_kernel_traitsILi32ELi128ELi128ELi8ES2_EELb1ELb0ELb1ELb0EEv16Flash_bwd_params' for 'sm_80'
ptxas info    : Function properties for _Z44flash_bwd_dq_dk_dv_loop_seqk_parallel_kernelI23Flash_bwd_kernel_traitsILi32ELi128ELi128ELi8ELi4ELi4ELi4ELb0ELb0EN7cutlass6half_tE19Flash_kernel_traitsILi32ELi128ELi128ELi8ES2_EELb1ELb0ELb1ELb0EEv16Flash_bwd_params
    0 bytes stack frame, 0 bytes spill stores, 0 bytes spill loads
ptxas info    : Used 242 registers, 696 bytes cmem[0]
ptxas info    : Compiling entry function '_Z44flash_bwd_dq_dk_dv_loop_seqk_parallel_kernelI23Flash_bwd_kernel_traitsILi32ELi128ELi128ELi8ELi4ELi4ELi4ELb0ELb0EN7cutlass6half_tE19Flash_kernel_traitsILi32ELi128ELi128ELi8ES2_EELb1ELb0ELb1ELb1EEv16Flash_bwd_params' for 'sm_80'
ptxas info    : Function properties for _Z44flash_bwd_dq_dk_dv_loop_seqk_parallel_kernelI23Flash_bwd_kernel_traitsILi32ELi128ELi128ELi8ELi4ELi4ELi4ELb0ELb0EN7cutlass6half_tE19Flash_kernel_traitsILi32ELi128ELi128ELi8ES2_EELb1ELb0ELb1ELb1EEv16Flash_bwd_params
    0 bytes stack frame, 0 bytes spill stores, 0 bytes spill loads
ptxas info    : Used 244 registers, 696 bytes cmem[0]
ptxas info    : Compiling entry function '_Z44flash_bwd_dq_dk_dv_loop_seqk_parallel_kernelI23Flash_bwd_kernel_traitsILi32ELi128ELi128ELi8ELi4ELi4ELi4ELb0ELb0EN7cutlass6half_tE19Flash_kernel_traitsILi32ELi128ELi128ELi8ES2_EELb1ELb1ELb0ELb0EEv16Flash_bwd_params' for 'sm_80'
ptxas info    : Function properties for _Z44flash_bwd_dq_dk_dv_loop_seqk_parallel_kernelI23Flash_bwd_kernel_traitsILi32ELi128ELi128ELi8ELi4ELi4ELi4ELb0ELb0EN7cutlass6half_tE19Flash_kernel_traitsILi32ELi128ELi128ELi8ES2_EELb1ELb1ELb0ELb0EEv16Flash_bwd_params
    0 bytes stack frame, 0 bytes spill stores, 0 bytes spill loads
ptxas info    : Used 251 registers, 696 bytes cmem[0]
ptxas info    : Compiling entry function '_Z44flash_bwd_dq_dk_dv_loop_seqk_parallel_kernelI23Flash_bwd_kernel_traitsILi32ELi128ELi128ELi8ELi4ELi4ELi4ELb0ELb0EN7cutlass6half_tE19Flash_kernel_traitsILi32ELi128ELi128ELi8ES2_EELb1ELb1ELb0ELb1EEv16Flash_bwd_params' for 'sm_80'
ptxas info    : Function properties for _Z44flash_bwd_dq_dk_dv_loop_seqk_parallel_kernelI23Flash_bwd_kernel_traitsILi32ELi128ELi128ELi8ELi4ELi4ELi4ELb0ELb0EN7cutlass6half_tE19Flash_kernel_traitsILi32ELi128ELi128ELi8ES2_EELb1ELb1ELb0ELb1EEv16Flash_bwd_params
    0 bytes stack frame, 0 bytes spill stores, 0 bytes spill loads
ptxas info    : Used 250 registers, 696 bytes cmem[0]
ptxas info    : Compiling entry function '_Z44flash_bwd_dq_dk_dv_loop_seqk_parallel_kernelI23Flash_bwd_kernel_traitsILi32ELi128ELi128ELi8ELi4ELi4ELi4ELb0ELb0EN7cutlass6half_tE19Flash_kernel_traitsILi32ELi128ELi128ELi8ES2_EELb1ELb1ELb1ELb0EEv16Flash_bwd_params' for 'sm_80'
ptxas info    : Function properties for _Z44flash_bwd_dq_dk_dv_loop_seqk_parallel_kernelI23Flash_bwd_kernel_traitsILi32ELi128ELi128ELi8ELi4ELi4ELi4ELb0ELb0EN7cutlass6half_tE19Flash_kernel_traitsILi32ELi128ELi128ELi8ES2_EELb1ELb1ELb1ELb0EEv16Flash_bwd_params
    0 bytes stack frame, 0 bytes spill stores, 0 bytes spill loads
ptxas info    : Used 244 registers, 696 bytes cmem[0]
ptxas info    : Compiling entry function '_Z44flash_bwd_dq_dk_dv_loop_seqk_parallel_kernelI23Flash_bwd_kernel_traitsILi32ELi128ELi128ELi8ELi4ELi4ELi4ELb0ELb0EN7cutlass6half_tE19Flash_kernel_traitsILi32ELi128ELi128ELi8ES2_EELb1ELb1ELb1ELb1EEv16Flash_bwd_params' for 'sm_80'
ptxas info    : Function properties for _Z44flash_bwd_dq_dk_dv_loop_seqk_parallel_kernelI23Flash_bwd_kernel_traitsILi32ELi128ELi128ELi8ELi4ELi4ELi4ELb0ELb0EN7cutlass6half_tE19Flash_kernel_traitsILi32ELi128ELi128ELi8ES2_EELb1ELb1ELb1ELb1EEv16Flash_bwd_params
    0 bytes stack frame, 0 bytes spill stores, 0 bytes spill loads
ptxas info    : Used 247 registers, 696 bytes cmem[0]
ptxas info    : Compiling entry function '_Z25flash_bwd_dot_do_o_kernelILb1E23Flash_bwd_kernel_traitsILi32ELi128ELi128ELi8ELi4ELi4ELi4ELb0ELb0EN7cutlass6half_tE19Flash_kernel_traitsILi32ELi128ELi128ELi8ES2_EEEv16Flash_bwd_params' for 'sm_80'
ptxas info    : Function properties for _Z25flash_bwd_dot_do_o_kernelILb1E23Flash_bwd_kernel_traitsILi32ELi128ELi128ELi8ELi4ELi4ELi4ELb0ELb0EN7cutlass6half_tE19Flash_kernel_traitsILi32ELi128ELi128ELi8ES2_EEEv16Flash_bwd_params
    0 bytes stack frame, 0 bytes spill stores, 0 bytes spill loads
ptxas info    : Used 31 registers, 696 bytes cmem[0]
/anaconda/envs/flashattention/lib/python3.9/site-packages/torch/include/c10/core/SymInt.h(84): warning: integer conversion resulted in a change of sign

[7/37] /usr/local/cuda-11.4/bin/nvcc  -I/home/v-feiychen/flash-attention/csrc/flash_attn -I/home/v-feiychen/flash-attention/csrc/flash_attn/src -I/home/v-feiychen/flash-attention/csrc/cutlass/include -I/anaconda/envs/flashattention/lib/python3.9/site-packages/torch/include -I/anaconda/envs/flashattention/lib/python3.9/site-packages/torch/include/torch/csrc/api/include -I/anaconda/envs/flashattention/lib/python3.9/site-packages/torch/include/TH -I/anaconda/envs/flashattention/lib/python3.9/site-packages/torch/include/THC -I/usr/local/cuda-11.4/include -I/anaconda/envs/flashattention/include/python3.9 -c -c /home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_bwd_hdim32_bf16_sm80.cu -o /home/v-feiychen/flash-attention/build/temp.linux-x86_64-cpython-39/csrc/flash_attn/src/flash_bwd_hdim32_bf16_sm80.o -D__CUDA_NO_HALF_OPERATORS__ -D__CUDA_NO_HALF_CONVERSIONS__ -D__CUDA_NO_BFLOAT16_CONVERSIONS__ -D__CUDA_NO_HALF2_OPERATORS__ --expt-relaxed-constexpr --compiler-options ''"'"'-fPIC'"'"'' -O3 -std=c++17 -U__CUDA_NO_HALF_OPERATORS__ -U__CUDA_NO_HALF_CONVERSIONS__ -U__CUDA_NO_HALF2_OPERATORS__ -U__CUDA_NO_BFLOAT16_CONVERSIONS__ --expt-relaxed-constexpr --expt-extended-lambda --use_fast_math --ptxas-options=-v -lineinfo -gencode arch=compute_80,code=sm_80 --threads 4 -DTORCH_API_INCLUDE_EXTENSION_H '-DPYBIND11_COMPILER_TYPE="_gcc"' '-DPYBIND11_STDLIB="_libstdcpp"' '-DPYBIND11_BUILD_ABI="_cxxabi1011"' -DTORCH_EXTENSION_NAME=flash_attn_2_cuda -D_GLIBCXX_USE_CXX11_ABI=0
/anaconda/envs/flashattention/lib/python3.9/site-packages/torch/include/c10/core/SymInt.h(84): warning: integer conversion resulted in a change of sign

ptxas info    : 2 bytes gmem
ptxas info    : Compiling entry function '_Z44flash_bwd_dq_dk_dv_loop_seqk_parallel_kernelI23Flash_bwd_kernel_traitsILi32ELi128ELi128ELi8ELi4ELi4ELi4ELb1ELb0EN7cutlass10bfloat16_tE19Flash_kernel_traitsILi32ELi128ELi128ELi8ES2_EELb0ELb0ELb0ELb0EEv16Flash_bwd_params' for 'sm_80'
ptxas info    : Function properties for _Z44flash_bwd_dq_dk_dv_loop_seqk_parallel_kernelI23Flash_bwd_kernel_traitsILi32ELi128ELi128ELi8ELi4ELi4ELi4ELb1ELb0EN7cutlass10bfloat16_tE19Flash_kernel_traitsILi32ELi128ELi128ELi8ES2_EELb0ELb0ELb0ELb0EEv16Flash_bwd_params
    0 bytes stack frame, 0 bytes spill stores, 0 bytes spill loads
ptxas info    : Used 254 registers, 696 bytes cmem[0]
ptxas info    : Compiling entry function '_Z44flash_bwd_dq_dk_dv_loop_seqk_parallel_kernelI23Flash_bwd_kernel_traitsILi32ELi128ELi128ELi8ELi4ELi4ELi4ELb1ELb0EN7cutlass10bfloat16_tE19Flash_kernel_traitsILi32ELi128ELi128ELi8ES2_EELb0ELb0ELb0ELb1EEv16Flash_bwd_params' for 'sm_80'
ptxas info    : Function properties for _Z44flash_bwd_dq_dk_dv_loop_seqk_parallel_kernelI23Flash_bwd_kernel_traitsILi32ELi128ELi128ELi8ELi4ELi4ELi4ELb1ELb0EN7cutlass10bfloat16_tE19Flash_kernel_traitsILi32ELi128ELi128ELi8ES2_EELb0ELb0ELb0ELb1EEv16Flash_bwd_params
    0 bytes stack frame, 0 bytes spill stores, 0 bytes spill loads
ptxas info    : Used 254 registers, 696 bytes cmem[0]
ptxas info    : Compiling entry function '_Z44flash_bwd_dq_dk_dv_loop_seqk_parallel_kernelI23Flash_bwd_kernel_traitsILi32ELi128ELi128ELi8ELi4ELi4ELi4ELb1ELb0EN7cutlass10bfloat16_tE19Flash_kernel_traitsILi32ELi128ELi128ELi8ES2_EELb0ELb0ELb1ELb0EEv16Flash_bwd_params' for 'sm_80'
ptxas info    : Function properties for _Z44flash_bwd_dq_dk_dv_loop_seqk_parallel_kernelI23Flash_bwd_kernel_traitsILi32ELi128ELi128ELi8ELi4ELi4ELi4ELb1ELb0EN7cutlass10bfloat16_tE19Flash_kernel_traitsILi32ELi128ELi128ELi8ES2_EELb0ELb0ELb1ELb0EEv16Flash_bwd_params
    0 bytes stack frame, 0 bytes spill stores, 0 bytes spill loads
ptxas info    : Used 251 registers, 696 bytes cmem[0]
ptxas info    : Compiling entry function '_Z44flash_bwd_dq_dk_dv_loop_seqk_parallel_kernelI23Flash_bwd_kernel_traitsILi32ELi128ELi128ELi8ELi4ELi4ELi4ELb1ELb0EN7cutlass10bfloat16_tE19Flash_kernel_traitsILi32ELi128ELi128ELi8ES2_EELb0ELb0ELb1ELb1EEv16Flash_bwd_params' for 'sm_80'
ptxas info    : Function properties for _Z44flash_bwd_dq_dk_dv_loop_seqk_parallel_kernelI23Flash_bwd_kernel_traitsILi32ELi128ELi128ELi8ELi4ELi4ELi4ELb1ELb0EN7cutlass10bfloat16_tE19Flash_kernel_traitsILi32ELi128ELi128ELi8ES2_EELb0ELb0ELb1ELb1EEv16Flash_bwd_params
    0 bytes stack frame, 0 bytes spill stores, 0 bytes spill loads
ptxas info    : Used 251 registers, 696 bytes cmem[0]
ptxas info    : Compiling entry function '_Z44flash_bwd_dq_dk_dv_loop_seqk_parallel_kernelI23Flash_bwd_kernel_traitsILi32ELi128ELi128ELi8ELi4ELi4ELi4ELb1ELb0EN7cutlass10bfloat16_tE19Flash_kernel_traitsILi32ELi128ELi128ELi8ES2_EELb0ELb1ELb0ELb0EEv16Flash_bwd_params' for 'sm_80'
ptxas info    : Function properties for _Z44flash_bwd_dq_dk_dv_loop_seqk_parallel_kernelI23Flash_bwd_kernel_traitsILi32ELi128ELi128ELi8ELi4ELi4ELi4ELb1ELb0EN7cutlass10bfloat16_tE19Flash_kernel_traitsILi32ELi128ELi128ELi8ES2_EELb0ELb1ELb0ELb0EEv16Flash_bwd_params
    0 bytes stack frame, 0 bytes spill stores, 0 bytes spill loads
ptxas info    : Used 253 registers, 696 bytes cmem[0]
ptxas info    : Compiling entry function '_Z44flash_bwd_dq_dk_dv_loop_seqk_parallel_kernelI23Flash_bwd_kernel_traitsILi32ELi128ELi128ELi8ELi4ELi4ELi4ELb1ELb0EN7cutlass10bfloat16_tE19Flash_kernel_traitsILi32ELi128ELi128ELi8ES2_EELb0ELb1ELb0ELb1EEv16Flash_bwd_params' for 'sm_80'
ptxas info    : Function properties for _Z44flash_bwd_dq_dk_dv_loop_seqk_parallel_kernelI23Flash_bwd_kernel_traitsILi32ELi128ELi128ELi8ELi4ELi4ELi4ELb1ELb0EN7cutlass10bfloat16_tE19Flash_kernel_traitsILi32ELi128ELi128ELi8ES2_EELb0ELb1ELb0ELb1EEv16Flash_bwd_params
    0 bytes stack frame, 0 bytes spill stores, 0 bytes spill loads
ptxas info    : Used 252 registers, 696 bytes cmem[0]
ptxas info    : Compiling entry function '_Z44flash_bwd_dq_dk_dv_loop_seqk_parallel_kernelI23Flash_bwd_kernel_traitsILi32ELi128ELi128ELi8ELi4ELi4ELi4ELb1ELb0EN7cutlass10bfloat16_tE19Flash_kernel_traitsILi32ELi128ELi128ELi8ES2_EELb0ELb1ELb1ELb0EEv16Flash_bwd_params' for 'sm_80'
ptxas info    : Function properties for _Z44flash_bwd_dq_dk_dv_loop_seqk_parallel_kernelI23Flash_bwd_kernel_traitsILi32ELi128ELi128ELi8ELi4ELi4ELi4ELb1ELb0EN7cutlass10bfloat16_tE19Flash_kernel_traitsILi32ELi128ELi128ELi8ES2_EELb0ELb1ELb1ELb0EEv16Flash_bwd_params
    0 bytes stack frame, 0 bytes spill stores, 0 bytes spill loads
ptxas info    : Used 246 registers, 696 bytes cmem[0]
ptxas info    : Compiling entry function '_Z44flash_bwd_dq_dk_dv_loop_seqk_parallel_kernelI23Flash_bwd_kernel_traitsILi32ELi128ELi128ELi8ELi4ELi4ELi4ELb1ELb0EN7cutlass10bfloat16_tE19Flash_kernel_traitsILi32ELi128ELi128ELi8ES2_EELb0ELb1ELb1ELb1EEv16Flash_bwd_params' for 'sm_80'
ptxas info    : Function properties for _Z44flash_bwd_dq_dk_dv_loop_seqk_parallel_kernelI23Flash_bwd_kernel_traitsILi32ELi128ELi128ELi8ELi4ELi4ELi4ELb1ELb0EN7cutlass10bfloat16_tE19Flash_kernel_traitsILi32ELi128ELi128ELi8ES2_EELb0ELb1ELb1ELb1EEv16Flash_bwd_params
    0 bytes stack frame, 0 bytes spill stores, 0 bytes spill loads
ptxas info    : Used 249 registers, 696 bytes cmem[0]
ptxas info    : Compiling entry function '_Z27flash_bwd_convert_dq_kernelI23Flash_bwd_kernel_traitsILi32ELi128ELi128ELi8ELi4ELi4ELi4ELb1ELb0EN7cutlass10bfloat16_tE19Flash_kernel_traitsILi32ELi128ELi128ELi8ES2_EEEv16Flash_bwd_params' for 'sm_80'
ptxas info    : Function properties for _Z27flash_bwd_convert_dq_kernelI23Flash_bwd_kernel_traitsILi32ELi128ELi128ELi8ELi4ELi4ELi4ELb1ELb0EN7cutlass10bfloat16_tE19Flash_kernel_traitsILi32ELi128ELi128ELi8ES2_EEEv16Flash_bwd_params
    0 bytes stack frame, 0 bytes spill stores, 0 bytes spill loads
ptxas info    : Used 32 registers, 696 bytes cmem[0]
ptxas info    : Compiling entry function '_Z44flash_bwd_dq_dk_dv_loop_seqk_parallel_kernelI23Flash_bwd_kernel_traitsILi32ELi128ELi128ELi8ELi4ELi4ELi4ELb1ELb0EN7cutlass10bfloat16_tE19Flash_kernel_traitsILi32ELi128ELi128ELi8ES2_EELb1ELb0ELb0ELb0EEv16Flash_bwd_params' for 'sm_80'
ptxas info    : Function properties for _Z44flash_bwd_dq_dk_dv_loop_seqk_parallel_kernelI23Flash_bwd_kernel_traitsILi32ELi128ELi128ELi8ELi4ELi4ELi4ELb1ELb0EN7cutlass10bfloat16_tE19Flash_kernel_traitsILi32ELi128ELi128ELi8ES2_EELb1ELb0ELb0ELb0EEv16Flash_bwd_params
    0 bytes stack frame, 0 bytes spill stores, 0 bytes spill loads
ptxas info    : Used 253 registers, 696 bytes cmem[0]
ptxas info    : Compiling entry function '_Z44flash_bwd_dq_dk_dv_loop_seqk_parallel_kernelI23Flash_bwd_kernel_traitsILi32ELi128ELi128ELi8ELi4ELi4ELi4ELb1ELb0EN7cutlass10bfloat16_tE19Flash_kernel_traitsILi32ELi128ELi128ELi8ES2_EELb1ELb0ELb0ELb1EEv16Flash_bwd_params' for 'sm_80'
ptxas info    : Function properties for _Z44flash_bwd_dq_dk_dv_loop_seqk_parallel_kernelI23Flash_bwd_kernel_traitsILi32ELi128ELi128ELi8ELi4ELi4ELi4ELb1ELb0EN7cutlass10bfloat16_tE19Flash_kernel_traitsILi32ELi128ELi128ELi8ES2_EELb1ELb0ELb0ELb1EEv16Flash_bwd_params
    0 bytes stack frame, 0 bytes spill stores, 0 bytes spill loads
ptxas info    : Used 246 registers, 696 bytes cmem[0]
ptxas info    : Compiling entry function '_Z44flash_bwd_dq_dk_dv_loop_seqk_parallel_kernelI23Flash_bwd_kernel_traitsILi32ELi128ELi128ELi8ELi4ELi4ELi4ELb1ELb0EN7cutlass10bfloat16_tE19Flash_kernel_traitsILi32ELi128ELi128ELi8ES2_EELb1ELb0ELb1ELb0EEv16Flash_bwd_params' for 'sm_80'
ptxas info    : Function properties for _Z44flash_bwd_dq_dk_dv_loop_seqk_parallel_kernelI23Flash_bwd_kernel_traitsILi32ELi128ELi128ELi8ELi4ELi4ELi4ELb1ELb0EN7cutlass10bfloat16_tE19Flash_kernel_traitsILi32ELi128ELi128ELi8ES2_EELb1ELb0ELb1ELb0EEv16Flash_bwd_params
    0 bytes stack frame, 0 bytes spill stores, 0 bytes spill loads
ptxas info    : Used 245 registers, 696 bytes cmem[0]
ptxas info    : Compiling entry function '_Z44flash_bwd_dq_dk_dv_loop_seqk_parallel_kernelI23Flash_bwd_kernel_traitsILi32ELi128ELi128ELi8ELi4ELi4ELi4ELb1ELb0EN7cutlass10bfloat16_tE19Flash_kernel_traitsILi32ELi128ELi128ELi8ES2_EELb1ELb0ELb1ELb1EEv16Flash_bwd_params' for 'sm_80'
ptxas info    : Function properties for _Z44flash_bwd_dq_dk_dv_loop_seqk_parallel_kernelI23Flash_bwd_kernel_traitsILi32ELi128ELi128ELi8ELi4ELi4ELi4ELb1ELb0EN7cutlass10bfloat16_tE19Flash_kernel_traitsILi32ELi128ELi128ELi8ES2_EELb1ELb0ELb1ELb1EEv16Flash_bwd_params
    0 bytes stack frame, 0 bytes spill stores, 0 bytes spill loads
ptxas info    : Used 248 registers, 696 bytes cmem[0]
ptxas info    : Compiling entry function '_Z44flash_bwd_dq_dk_dv_loop_seqk_parallel_kernelI23Flash_bwd_kernel_traitsILi32ELi128ELi128ELi8ELi4ELi4ELi4ELb1ELb0EN7cutlass10bfloat16_tE19Flash_kernel_traitsILi32ELi128ELi128ELi8ES2_EELb1ELb1ELb0ELb0EEv16Flash_bwd_params' for 'sm_80'
ptxas info    : Function properties for _Z44flash_bwd_dq_dk_dv_loop_seqk_parallel_kernelI23Flash_bwd_kernel_traitsILi32ELi128ELi128ELi8ELi4ELi4ELi4ELb1ELb0EN7cutlass10bfloat16_tE19Flash_kernel_traitsILi32ELi128ELi128ELi8ES2_EELb1ELb1ELb0ELb0EEv16Flash_bwd_params
    0 bytes stack frame, 0 bytes spill stores, 0 bytes spill loads
ptxas info    : Used 251 registers, 696 bytes cmem[0]
ptxas info    : Compiling entry function '_Z44flash_bwd_dq_dk_dv_loop_seqk_parallel_kernelI23Flash_bwd_kernel_traitsILi32ELi128ELi128ELi8ELi4ELi4ELi4ELb1ELb0EN7cutlass10bfloat16_tE19Flash_kernel_traitsILi32ELi128ELi128ELi8ES2_EELb1ELb1ELb0ELb1EEv16Flash_bwd_params' for 'sm_80'
ptxas info    : Function properties for _Z44flash_bwd_dq_dk_dv_loop_seqk_parallel_kernelI23Flash_bwd_kernel_traitsILi32ELi128ELi128ELi8ELi4ELi4ELi4ELb1ELb0EN7cutlass10bfloat16_tE19Flash_kernel_traitsILi32ELi128ELi128ELi8ES2_EELb1ELb1ELb0ELb1EEv16Flash_bwd_params
    0 bytes stack frame, 0 bytes spill stores, 0 bytes spill loads
ptxas info    : Used 251 registers, 696 bytes cmem[0]
ptxas info    : Compiling entry function '_Z44flash_bwd_dq_dk_dv_loop_seqk_parallel_kernelI23Flash_bwd_kernel_traitsILi32ELi128ELi128ELi8ELi4ELi4ELi4ELb1ELb0EN7cutlass10bfloat16_tE19Flash_kernel_traitsILi32ELi128ELi128ELi8ES2_EELb1ELb1ELb1ELb0EEv16Flash_bwd_params' for 'sm_80'
ptxas info    : Function properties for _Z44flash_bwd_dq_dk_dv_loop_seqk_parallel_kernelI23Flash_bwd_kernel_traitsILi32ELi128ELi128ELi8ELi4ELi4ELi4ELb1ELb0EN7cutlass10bfloat16_tE19Flash_kernel_traitsILi32ELi128ELi128ELi8ES2_EELb1ELb1ELb1ELb0EEv16Flash_bwd_params
    0 bytes stack frame, 0 bytes spill stores, 0 bytes spill loads
ptxas info    : Used 255 registers, 696 bytes cmem[0]
ptxas info    : Compiling entry function '_Z44flash_bwd_dq_dk_dv_loop_seqk_parallel_kernelI23Flash_bwd_kernel_traitsILi32ELi128ELi128ELi8ELi4ELi4ELi4ELb1ELb0EN7cutlass10bfloat16_tE19Flash_kernel_traitsILi32ELi128ELi128ELi8ES2_EELb1ELb1ELb1ELb1EEv16Flash_bwd_params' for 'sm_80'
ptxas info    : Function properties for _Z44flash_bwd_dq_dk_dv_loop_seqk_parallel_kernelI23Flash_bwd_kernel_traitsILi32ELi128ELi128ELi8ELi4ELi4ELi4ELb1ELb0EN7cutlass10bfloat16_tE19Flash_kernel_traitsILi32ELi128ELi128ELi8ES2_EELb1ELb1ELb1ELb1EEv16Flash_bwd_params
    0 bytes stack frame, 0 bytes spill stores, 0 bytes spill loads
ptxas info    : Used 254 registers, 696 bytes cmem[0]
ptxas info    : Compiling entry function '_Z25flash_bwd_dot_do_o_kernelILb1E23Flash_bwd_kernel_traitsILi32ELi128ELi128ELi8ELi4ELi4ELi4ELb1ELb0EN7cutlass10bfloat16_tE19Flash_kernel_traitsILi32ELi128ELi128ELi8ES2_EEEv16Flash_bwd_params' for 'sm_80'
ptxas info    : Function properties for _Z25flash_bwd_dot_do_o_kernelILb1E23Flash_bwd_kernel_traitsILi32ELi128ELi128ELi8ELi4ELi4ELi4ELb1ELb0EN7cutlass10bfloat16_tE19Flash_kernel_traitsILi32ELi128ELi128ELi8ES2_EEEv16Flash_bwd_params
    0 bytes stack frame, 0 bytes spill stores, 0 bytes spill loads
ptxas info    : Used 30 registers, 696 bytes cmem[0]
ptxas info    : Compiling entry function '_Z27flash_bwd_convert_dq_kernelI23Flash_bwd_kernel_traitsILi32ELi128ELi128ELi8ELi4ELi4ELi4ELb0ELb0EN7cutlass10bfloat16_tE19Flash_kernel_traitsILi32ELi128ELi128ELi8ES2_EEEv16Flash_bwd_params' for 'sm_80'
ptxas info    : Function properties for _Z27flash_bwd_convert_dq_kernelI23Flash_bwd_kernel_traitsILi32ELi128ELi128ELi8ELi4ELi4ELi4ELb0ELb0EN7cutlass10bfloat16_tE19Flash_kernel_traitsILi32ELi128ELi128ELi8ES2_EEEv16Flash_bwd_params
    0 bytes stack frame, 0 bytes spill stores, 0 bytes spill loads
ptxas info    : Used 32 registers, 696 bytes cmem[0]
ptxas info    : Compiling entry function '_Z44flash_bwd_dq_dk_dv_loop_seqk_parallel_kernelI23Flash_bwd_kernel_traitsILi32ELi128ELi128ELi8ELi4ELi4ELi4ELb0ELb0EN7cutlass10bfloat16_tE19Flash_kernel_traitsILi32ELi128ELi128ELi8ES2_EELb1ELb0ELb0ELb0EEv16Flash_bwd_params' for 'sm_80'
ptxas info    : Function properties for _Z44flash_bwd_dq_dk_dv_loop_seqk_parallel_kernelI23Flash_bwd_kernel_traitsILi32ELi128ELi128ELi8ELi4ELi4ELi4ELb0ELb0EN7cutlass10bfloat16_tE19Flash_kernel_traitsILi32ELi128ELi128ELi8ES2_EELb1ELb0ELb0ELb0EEv16Flash_bwd_params
    0 bytes stack frame, 0 bytes spill stores, 0 bytes spill loads
ptxas info    : Used 240 registers, 696 bytes cmem[0]
ptxas info    : Compiling entry function '_Z44flash_bwd_dq_dk_dv_loop_seqk_parallel_kernelI23Flash_bwd_kernel_traitsILi32ELi128ELi128ELi8ELi4ELi4ELi4ELb0ELb0EN7cutlass10bfloat16_tE19Flash_kernel_traitsILi32ELi128ELi128ELi8ES2_EELb1ELb0ELb0ELb1EEv16Flash_bwd_params' for 'sm_80'
ptxas info    : Function properties for _Z44flash_bwd_dq_dk_dv_loop_seqk_parallel_kernelI23Flash_bwd_kernel_traitsILi32ELi128ELi128ELi8ELi4ELi4ELi4ELb0ELb0EN7cutlass10bfloat16_tE19Flash_kernel_traitsILi32ELi128ELi128ELi8ES2_EELb1ELb0ELb0ELb1EEv16Flash_bwd_params
    0 bytes stack frame, 0 bytes spill stores, 0 bytes spill loads
ptxas info    : Used 243 registers, 696 bytes cmem[0]
ptxas info    : Compiling entry function '_Z44flash_bwd_dq_dk_dv_loop_seqk_parallel_kernelI23Flash_bwd_kernel_traitsILi32ELi128ELi128ELi8ELi4ELi4ELi4ELb0ELb0EN7cutlass10bfloat16_tE19Flash_kernel_traitsILi32ELi128ELi128ELi8ES2_EELb1ELb0ELb1ELb0EEv16Flash_bwd_params' for 'sm_80'
ptxas info    : Function properties for _Z44flash_bwd_dq_dk_dv_loop_seqk_parallel_kernelI23Flash_bwd_kernel_traitsILi32ELi128ELi128ELi8ELi4ELi4ELi4ELb0ELb0EN7cutlass10bfloat16_tE19Flash_kernel_traitsILi32ELi128ELi128ELi8ES2_EELb1ELb0ELb1ELb0EEv16Flash_bwd_params
    0 bytes stack frame, 0 bytes spill stores, 0 bytes spill loads
ptxas info    : Used 242 registers, 696 bytes cmem[0]
ptxas info    : Compiling entry function '_Z44flash_bwd_dq_dk_dv_loop_seqk_parallel_kernelI23Flash_bwd_kernel_traitsILi32ELi128ELi128ELi8ELi4ELi4ELi4ELb0ELb0EN7cutlass10bfloat16_tE19Flash_kernel_traitsILi32ELi128ELi128ELi8ES2_EELb1ELb0ELb1ELb1EEv16Flash_bwd_params' for 'sm_80'
ptxas info    : Function properties for _Z44flash_bwd_dq_dk_dv_loop_seqk_parallel_kernelI23Flash_bwd_kernel_traitsILi32ELi128ELi128ELi8ELi4ELi4ELi4ELb0ELb0EN7cutlass10bfloat16_tE19Flash_kernel_traitsILi32ELi128ELi128ELi8ES2_EELb1ELb0ELb1ELb1EEv16Flash_bwd_params
    0 bytes stack frame, 0 bytes spill stores, 0 bytes spill loads
ptxas info    : Used 244 registers, 696 bytes cmem[0]
ptxas info    : Compiling entry function '_Z44flash_bwd_dq_dk_dv_loop_seqk_parallel_kernelI23Flash_bwd_kernel_traitsILi32ELi128ELi128ELi8ELi4ELi4ELi4ELb0ELb0EN7cutlass10bfloat16_tE19Flash_kernel_traitsILi32ELi128ELi128ELi8ES2_EELb1ELb1ELb0ELb0EEv16Flash_bwd_params' for 'sm_80'
ptxas info    : Function properties for _Z44flash_bwd_dq_dk_dv_loop_seqk_parallel_kernelI23Flash_bwd_kernel_traitsILi32ELi128ELi128ELi8ELi4ELi4ELi4ELb0ELb0EN7cutlass10bfloat16_tE19Flash_kernel_traitsILi32ELi128ELi128ELi8ES2_EELb1ELb1ELb0ELb0EEv16Flash_bwd_params
    0 bytes stack frame, 0 bytes spill stores, 0 bytes spill loads
ptxas info    : Used 251 registers, 696 bytes cmem[0]
ptxas info    : Compiling entry function '_Z44flash_bwd_dq_dk_dv_loop_seqk_parallel_kernelI23Flash_bwd_kernel_traitsILi32ELi128ELi128ELi8ELi4ELi4ELi4ELb0ELb0EN7cutlass10bfloat16_tE19Flash_kernel_traitsILi32ELi128ELi128ELi8ES2_EELb1ELb1ELb0ELb1EEv16Flash_bwd_params' for 'sm_80'
ptxas info    : Function properties for _Z44flash_bwd_dq_dk_dv_loop_seqk_parallel_kernelI23Flash_bwd_kernel_traitsILi32ELi128ELi128ELi8ELi4ELi4ELi4ELb0ELb0EN7cutlass10bfloat16_tE19Flash_kernel_traitsILi32ELi128ELi128ELi8ES2_EELb1ELb1ELb0ELb1EEv16Flash_bwd_params
    0 bytes stack frame, 0 bytes spill stores, 0 bytes spill loads
ptxas info    : Used 250 registers, 696 bytes cmem[0]
ptxas info    : Compiling entry function '_Z44flash_bwd_dq_dk_dv_loop_seqk_parallel_kernelI23Flash_bwd_kernel_traitsILi32ELi128ELi128ELi8ELi4ELi4ELi4ELb0ELb0EN7cutlass10bfloat16_tE19Flash_kernel_traitsILi32ELi128ELi128ELi8ES2_EELb1ELb1ELb1ELb0EEv16Flash_bwd_params' for 'sm_80'
ptxas info    : Function properties for _Z44flash_bwd_dq_dk_dv_loop_seqk_parallel_kernelI23Flash_bwd_kernel_traitsILi32ELi128ELi128ELi8ELi4ELi4ELi4ELb0ELb0EN7cutlass10bfloat16_tE19Flash_kernel_traitsILi32ELi128ELi128ELi8ES2_EELb1ELb1ELb1ELb0EEv16Flash_bwd_params
    0 bytes stack frame, 0 bytes spill stores, 0 bytes spill loads
ptxas info    : Used 244 registers, 696 bytes cmem[0]
ptxas info    : Compiling entry function '_Z44flash_bwd_dq_dk_dv_loop_seqk_parallel_kernelI23Flash_bwd_kernel_traitsILi32ELi128ELi128ELi8ELi4ELi4ELi4ELb0ELb0EN7cutlass10bfloat16_tE19Flash_kernel_traitsILi32ELi128ELi128ELi8ES2_EELb1ELb1ELb1ELb1EEv16Flash_bwd_params' for 'sm_80'
ptxas info    : Function properties for _Z44flash_bwd_dq_dk_dv_loop_seqk_parallel_kernelI23Flash_bwd_kernel_traitsILi32ELi128ELi128ELi8ELi4ELi4ELi4ELb0ELb0EN7cutlass10bfloat16_tE19Flash_kernel_traitsILi32ELi128ELi128ELi8ES2_EELb1ELb1ELb1ELb1EEv16Flash_bwd_params
    0 bytes stack frame, 0 bytes spill stores, 0 bytes spill loads
ptxas info    : Used 247 registers, 696 bytes cmem[0]
ptxas info    : Compiling entry function '_Z25flash_bwd_dot_do_o_kernelILb1E23Flash_bwd_kernel_traitsILi32ELi128ELi128ELi8ELi4ELi4ELi4ELb0ELb0EN7cutlass10bfloat16_tE19Flash_kernel_traitsILi32ELi128ELi128ELi8ES2_EEEv16Flash_bwd_params' for 'sm_80'
ptxas info    : Function properties for _Z25flash_bwd_dot_do_o_kernelILb1E23Flash_bwd_kernel_traitsILi32ELi128ELi128ELi8ELi4ELi4ELi4ELb0ELb0EN7cutlass10bfloat16_tE19Flash_kernel_traitsILi32ELi128ELi128ELi8ES2_EEEv16Flash_bwd_params
    0 bytes stack frame, 0 bytes spill stores, 0 bytes spill loads
ptxas info    : Used 30 registers, 696 bytes cmem[0]
/anaconda/envs/flashattention/lib/python3.9/site-packages/torch/include/c10/core/SymInt.h(84): warning: integer conversion resulted in a change of sign

[8/37] /usr/local/cuda-11.4/bin/nvcc  -I/home/v-feiychen/flash-attention/csrc/flash_attn -I/home/v-feiychen/flash-attention/csrc/flash_attn/src -I/home/v-feiychen/flash-attention/csrc/cutlass/include -I/anaconda/envs/flashattention/lib/python3.9/site-packages/torch/include -I/anaconda/envs/flashattention/lib/python3.9/site-packages/torch/include/torch/csrc/api/include -I/anaconda/envs/flashattention/lib/python3.9/site-packages/torch/include/TH -I/anaconda/envs/flashattention/lib/python3.9/site-packages/torch/include/THC -I/usr/local/cuda-11.4/include -I/anaconda/envs/flashattention/include/python3.9 -c -c /home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_bwd_hdim160_fp16_sm80.cu -o /home/v-feiychen/flash-attention/build/temp.linux-x86_64-cpython-39/csrc/flash_attn/src/flash_bwd_hdim160_fp16_sm80.o -D__CUDA_NO_HALF_OPERATORS__ -D__CUDA_NO_HALF_CONVERSIONS__ -D__CUDA_NO_BFLOAT16_CONVERSIONS__ -D__CUDA_NO_HALF2_OPERATORS__ --expt-relaxed-constexpr --compiler-options ''"'"'-fPIC'"'"'' -O3 -std=c++17 -U__CUDA_NO_HALF_OPERATORS__ -U__CUDA_NO_HALF_CONVERSIONS__ -U__CUDA_NO_HALF2_OPERATORS__ -U__CUDA_NO_BFLOAT16_CONVERSIONS__ --expt-relaxed-constexpr --expt-extended-lambda --use_fast_math --ptxas-options=-v -lineinfo -gencode arch=compute_80,code=sm_80 --threads 4 -DTORCH_API_INCLUDE_EXTENSION_H '-DPYBIND11_COMPILER_TYPE="_gcc"' '-DPYBIND11_STDLIB="_libstdcpp"' '-DPYBIND11_BUILD_ABI="_cxxabi1011"' -DTORCH_EXTENSION_NAME=flash_attn_2_cuda -D_GLIBCXX_USE_CXX11_ABI=0
/anaconda/envs/flashattention/lib/python3.9/site-packages/torch/include/c10/core/SymInt.h(84): warning: integer conversion resulted in a change of sign

ptxas info    : 2 bytes gmem
ptxas info    : Compiling entry function '_Z44flash_bwd_dq_dk_dv_loop_seqk_parallel_kernelI23Flash_bwd_kernel_traitsILi160ELi64ELi64ELi8ELi4ELi4ELi4ELb0ELb1EN7cutlass6half_tE19Flash_kernel_traitsILi160ELi64ELi64ELi8ES2_EELb0ELb0ELb0ELb0EEv16Flash_bwd_params' for 'sm_80'
ptxas info    : Function properties for _Z44flash_bwd_dq_dk_dv_loop_seqk_parallel_kernelI23Flash_bwd_kernel_traitsILi160ELi64ELi64ELi8ELi4ELi4ELi4ELb0ELb1EN7cutlass6half_tE19Flash_kernel_traitsILi160ELi64ELi64ELi8ES2_EELb0ELb0ELb0ELb0EEv16Flash_bwd_params
    0 bytes stack frame, 0 bytes spill stores, 0 bytes spill loads
ptxas info    : Used 204 registers, 696 bytes cmem[0]
ptxas info    : Compiling entry function '_Z44flash_bwd_dq_dk_dv_loop_seqk_parallel_kernelI23Flash_bwd_kernel_traitsILi160ELi64ELi64ELi8ELi4ELi4ELi4ELb0ELb1EN7cutlass6half_tE19Flash_kernel_traitsILi160ELi64ELi64ELi8ES2_EELb0ELb0ELb0ELb1EEv16Flash_bwd_params' for 'sm_80'
ptxas info    : Function properties for _Z44flash_bwd_dq_dk_dv_loop_seqk_parallel_kernelI23Flash_bwd_kernel_traitsILi160ELi64ELi64ELi8ELi4ELi4ELi4ELb0ELb1EN7cutlass6half_tE19Flash_kernel_traitsILi160ELi64ELi64ELi8ES2_EELb0ELb0ELb0ELb1EEv16Flash_bwd_params
    0 bytes stack frame, 0 bytes spill stores, 0 bytes spill loads
ptxas info    : Used 204 registers, 696 bytes cmem[0]
ptxas info    : Compiling entry function '_Z44flash_bwd_dq_dk_dv_loop_seqk_parallel_kernelI23Flash_bwd_kernel_traitsILi160ELi64ELi64ELi8ELi4ELi4ELi4ELb0ELb1EN7cutlass6half_tE19Flash_kernel_traitsILi160ELi64ELi64ELi8ES2_EELb0ELb0ELb1ELb0EEv16Flash_bwd_params' for 'sm_80'
ptxas info    : Function properties for _Z44flash_bwd_dq_dk_dv_loop_seqk_parallel_kernelI23Flash_bwd_kernel_traitsILi160ELi64ELi64ELi8ELi4ELi4ELi4ELb0ELb1EN7cutlass6half_tE19Flash_kernel_traitsILi160ELi64ELi64ELi8ES2_EELb0ELb0ELb1ELb0EEv16Flash_bwd_params
    0 bytes stack frame, 0 bytes spill stores, 0 bytes spill loads
ptxas info    : Used 190 registers, 696 bytes cmem[0]
ptxas info    : Compiling entry function '_Z44flash_bwd_dq_dk_dv_loop_seqk_parallel_kernelI23Flash_bwd_kernel_traitsILi160ELi64ELi64ELi8ELi4ELi4ELi4ELb0ELb1EN7cutlass6half_tE19Flash_kernel_traitsILi160ELi64ELi64ELi8ES2_EELb0ELb0ELb1ELb1EEv16Flash_bwd_params' for 'sm_80'
ptxas info    : Function properties for _Z44flash_bwd_dq_dk_dv_loop_seqk_parallel_kernelI23Flash_bwd_kernel_traitsILi160ELi64ELi64ELi8ELi4ELi4ELi4ELb0ELb1EN7cutlass6half_tE19Flash_kernel_traitsILi160ELi64ELi64ELi8ES2_EELb0ELb0ELb1ELb1EEv16Flash_bwd_params
    0 bytes stack frame, 0 bytes spill stores, 0 bytes spill loads
ptxas info    : Used 196 registers, 696 bytes cmem[0]
ptxas info    : Compiling entry function '_Z44flash_bwd_dq_dk_dv_loop_seqk_parallel_kernelI23Flash_bwd_kernel_traitsILi160ELi64ELi64ELi8ELi4ELi4ELi4ELb0ELb1EN7cutlass6half_tE19Flash_kernel_traitsILi160ELi64ELi64ELi8ES2_EELb0ELb1ELb0ELb0EEv16Flash_bwd_params' for 'sm_80'
ptxas info    : Function properties for _Z44flash_bwd_dq_dk_dv_loop_seqk_parallel_kernelI23Flash_bwd_kernel_traitsILi160ELi64ELi64ELi8ELi4ELi4ELi4ELb0ELb1EN7cutlass6half_tE19Flash_kernel_traitsILi160ELi64ELi64ELi8ES2_EELb0ELb1ELb0ELb0EEv16Flash_bwd_params
    0 bytes stack frame, 0 bytes spill stores, 0 bytes spill loads
ptxas info    : Used 210 registers, 696 bytes cmem[0]
ptxas info    : Compiling entry function '_Z44flash_bwd_dq_dk_dv_loop_seqk_parallel_kernelI23Flash_bwd_kernel_traitsILi160ELi64ELi64ELi8ELi4ELi4ELi4ELb0ELb1EN7cutlass6half_tE19Flash_kernel_traitsILi160ELi64ELi64ELi8ES2_EELb0ELb1ELb0ELb1EEv16Flash_bwd_params' for 'sm_80'
ptxas info    : Function properties for _Z44flash_bwd_dq_dk_dv_loop_seqk_parallel_kernelI23Flash_bwd_kernel_traitsILi160ELi64ELi64ELi8ELi4ELi4ELi4ELb0ELb1EN7cutlass6half_tE19Flash_kernel_traitsILi160ELi64ELi64ELi8ES2_EELb0ELb1ELb0ELb1EEv16Flash_bwd_params
    0 bytes stack frame, 0 bytes spill stores, 0 bytes spill loads
ptxas info    : Used 206 registers, 696 bytes cmem[0]
ptxas info    : Compiling entry function '_Z44flash_bwd_dq_dk_dv_loop_seqk_parallel_kernelI23Flash_bwd_kernel_traitsILi160ELi64ELi64ELi8ELi4ELi4ELi4ELb0ELb1EN7cutlass6half_tE19Flash_kernel_traitsILi160ELi64ELi64ELi8ES2_EELb0ELb1ELb1ELb0EEv16Flash_bwd_params' for 'sm_80'
ptxas info    : Function properties for _Z44flash_bwd_dq_dk_dv_loop_seqk_parallel_kernelI23Flash_bwd_kernel_traitsILi160ELi64ELi64ELi8ELi4ELi4ELi4ELb0ELb1EN7cutlass6half_tE19Flash_kernel_traitsILi160ELi64ELi64ELi8ES2_EELb0ELb1ELb1ELb0EEv16Flash_bwd_params
    0 bytes stack frame, 0 bytes spill stores, 0 bytes spill loads
ptxas info    : Used 202 registers, 696 bytes cmem[0]
ptxas info    : Compiling entry function '_Z44flash_bwd_dq_dk_dv_loop_seqk_parallel_kernelI23Flash_bwd_kernel_traitsILi160ELi64ELi64ELi8ELi4ELi4ELi4ELb0ELb1EN7cutlass6half_tE19Flash_kernel_traitsILi160ELi64ELi64ELi8ES2_EELb0ELb1ELb1ELb1EEv16Flash_bwd_params' for 'sm_80'
ptxas info    : Function properties for _Z44flash_bwd_dq_dk_dv_loop_seqk_parallel_kernelI23Flash_bwd_kernel_traitsILi160ELi64ELi64ELi8ELi4ELi4ELi4ELb0ELb1EN7cutlass6half_tE19Flash_kernel_traitsILi160ELi64ELi64ELi8ES2_EELb0ELb1ELb1ELb1EEv16Flash_bwd_params
    0 bytes stack frame, 0 bytes spill stores, 0 bytes spill loads
ptxas info    : Used 200 registers, 696 bytes cmem[0]
ptxas info    : Compiling entry function '_Z44flash_bwd_dq_dk_dv_loop_seqk_parallel_kernelI23Flash_bwd_kernel_traitsILi160ELi64ELi64ELi8ELi4ELi4ELi4ELb0ELb0EN7cutlass6half_tE19Flash_kernel_traitsILi160ELi64ELi64ELi8ES2_EELb0ELb0ELb0ELb0EEv16Flash_bwd_params' for 'sm_80'
ptxas info    : Function properties for _Z44flash_bwd_dq_dk_dv_loop_seqk_parallel_kernelI23Flash_bwd_kernel_traitsILi160ELi64ELi64ELi8ELi4ELi4ELi4ELb0ELb0EN7cutlass6half_tE19Flash_kernel_traitsILi160ELi64ELi64ELi8ES2_EELb0ELb0ELb0ELb0EEv16Flash_bwd_params
    0 bytes stack frame, 0 bytes spill stores, 0 bytes spill loads
ptxas info    : Used 210 registers, 696 bytes cmem[0]
ptxas info    : Compiling entry function '_Z44flash_bwd_dq_dk_dv_loop_seqk_parallel_kernelI23Flash_bwd_kernel_traitsILi160ELi64ELi64ELi8ELi4ELi4ELi4ELb0ELb0EN7cutlass6half_tE19Flash_kernel_traitsILi160ELi64ELi64ELi8ES2_EELb0ELb0ELb0ELb1EEv16Flash_bwd_params' for 'sm_80'
ptxas info    : Function properties for _Z44flash_bwd_dq_dk_dv_loop_seqk_parallel_kernelI23Flash_bwd_kernel_traitsILi160ELi64ELi64ELi8ELi4ELi4ELi4ELb0ELb0EN7cutlass6half_tE19Flash_kernel_traitsILi160ELi64ELi64ELi8ES2_EELb0ELb0ELb0ELb1EEv16Flash_bwd_params
    0 bytes stack frame, 0 bytes spill stores, 0 bytes spill loads
ptxas info    : Used 212 registers, 696 bytes cmem[0]
ptxas info    : Compiling entry function '_Z44flash_bwd_dq_dk_dv_loop_seqk_parallel_kernelI23Flash_bwd_kernel_traitsILi160ELi64ELi64ELi8ELi4ELi4ELi4ELb0ELb0EN7cutlass6half_tE19Flash_kernel_traitsILi160ELi64ELi64ELi8ES2_EELb0ELb0ELb1ELb0EEv16Flash_bwd_params' for 'sm_80'
ptxas info    : Function properties for _Z44flash_bwd_dq_dk_dv_loop_seqk_parallel_kernelI23Flash_bwd_kernel_traitsILi160ELi64ELi64ELi8ELi4ELi4ELi4ELb0ELb0EN7cutlass6half_tE19Flash_kernel_traitsILi160ELi64ELi64ELi8ES2_EELb0ELb0ELb1ELb0EEv16Flash_bwd_params
    0 bytes stack frame, 0 bytes spill stores, 0 bytes spill loads
ptxas info    : Used 204 registers, 696 bytes cmem[0]
ptxas info    : Compiling entry function '_Z44flash_bwd_dq_dk_dv_loop_seqk_parallel_kernelI23Flash_bwd_kernel_traitsILi160ELi64ELi64ELi8ELi4ELi4ELi4ELb0ELb0EN7cutlass6half_tE19Flash_kernel_traitsILi160ELi64ELi64ELi8ES2_EELb0ELb0ELb1ELb1EEv16Flash_bwd_params' for 'sm_80'
ptxas info    : Function properties for _Z44flash_bwd_dq_dk_dv_loop_seqk_parallel_kernelI23Flash_bwd_kernel_traitsILi160ELi64ELi64ELi8ELi4ELi4ELi4ELb0ELb0EN7cutlass6half_tE19Flash_kernel_traitsILi160ELi64ELi64ELi8ES2_EELb0ELb0ELb1ELb1EEv16Flash_bwd_params
    0 bytes stack frame, 0 bytes spill stores, 0 bytes spill loads
ptxas info    : Used 204 registers, 696 bytes cmem[0]
ptxas info    : Compiling entry function '_Z44flash_bwd_dq_dk_dv_loop_seqk_parallel_kernelI23Flash_bwd_kernel_traitsILi160ELi64ELi64ELi8ELi4ELi4ELi4ELb0ELb0EN7cutlass6half_tE19Flash_kernel_traitsILi160ELi64ELi64ELi8ES2_EELb0ELb1ELb0ELb0EEv16Flash_bwd_params' for 'sm_80'
ptxas info    : Function properties for _Z44flash_bwd_dq_dk_dv_loop_seqk_parallel_kernelI23Flash_bwd_kernel_traitsILi160ELi64ELi64ELi8ELi4ELi4ELi4ELb0ELb0EN7cutlass6half_tE19Flash_kernel_traitsILi160ELi64ELi64ELi8ES2_EELb0ELb1ELb0ELb0EEv16Flash_bwd_params
    0 bytes stack frame, 0 bytes spill stores, 0 bytes spill loads
ptxas info    : Used 216 registers, 696 bytes cmem[0]
ptxas info    : Compiling entry function '_Z44flash_bwd_dq_dk_dv_loop_seqk_parallel_kernelI23Flash_bwd_kernel_traitsILi160ELi64ELi64ELi8ELi4ELi4ELi4ELb0ELb0EN7cutlass6half_tE19Flash_kernel_traitsILi160ELi64ELi64ELi8ES2_EELb0ELb1ELb0ELb1EEv16Flash_bwd_params' for 'sm_80'
ptxas info    : Function properties for _Z44flash_bwd_dq_dk_dv_loop_seqk_parallel_kernelI23Flash_bwd_kernel_traitsILi160ELi64ELi64ELi8ELi4ELi4ELi4ELb0ELb0EN7cutlass6half_tE19Flash_kernel_traitsILi160ELi64ELi64ELi8ES2_EELb0ELb1ELb0ELb1EEv16Flash_bwd_params
    0 bytes stack frame, 0 bytes spill stores, 0 bytes spill loads
ptxas info    : Used 214 registers, 696 bytes cmem[0]
ptxas info    : Compiling entry function '_Z44flash_bwd_dq_dk_dv_loop_seqk_parallel_kernelI23Flash_bwd_kernel_traitsILi160ELi64ELi64ELi8ELi4ELi4ELi4ELb0ELb0EN7cutlass6half_tE19Flash_kernel_traitsILi160ELi64ELi64ELi8ES2_EELb0ELb1ELb1ELb0EEv16Flash_bwd_params' for 'sm_80'
ptxas info    : Function properties for _Z44flash_bwd_dq_dk_dv_loop_seqk_parallel_kernelI23Flash_bwd_kernel_traitsILi160ELi64ELi64ELi8ELi4ELi4ELi4ELb0ELb0EN7cutlass6half_tE19Flash_kernel_traitsILi160ELi64ELi64ELi8ES2_EELb0ELb1ELb1ELb0EEv16Flash_bwd_params
    0 bytes stack frame, 0 bytes spill stores, 0 bytes spill loads
ptxas info    : Used 208 registers, 696 bytes cmem[0]
ptxas info    : Compiling entry function '_Z44flash_bwd_dq_dk_dv_loop_seqk_parallel_kernelI23Flash_bwd_kernel_traitsILi160ELi64ELi64ELi8ELi4ELi4ELi4ELb0ELb0EN7cutlass6half_tE19Flash_kernel_traitsILi160ELi64ELi64ELi8ES2_EELb0ELb1ELb1ELb1EEv16Flash_bwd_params' for 'sm_80'
ptxas info    : Function properties for _Z44flash_bwd_dq_dk_dv_loop_seqk_parallel_kernelI23Flash_bwd_kernel_traitsILi160ELi64ELi64ELi8ELi4ELi4ELi4ELb0ELb0EN7cutlass6half_tE19Flash_kernel_traitsILi160ELi64ELi64ELi8ES2_EELb0ELb1ELb1ELb1EEv16Flash_bwd_params
    0 bytes stack frame, 0 bytes spill stores, 0 bytes spill loads
ptxas info    : Used 206 registers, 696 bytes cmem[0]
ptxas info    : Compiling entry function '_Z27flash_bwd_convert_dq_kernelI23Flash_bwd_kernel_traitsILi160ELi64ELi64ELi8ELi4ELi4ELi4ELb0ELb1EN7cutlass6half_tE19Flash_kernel_traitsILi160ELi64ELi64ELi8ES2_EEEv16Flash_bwd_params' for 'sm_80'
ptxas info    : Function properties for _Z27flash_bwd_convert_dq_kernelI23Flash_bwd_kernel_traitsILi160ELi64ELi64ELi8ELi4ELi4ELi4ELb0ELb1EN7cutlass6half_tE19Flash_kernel_traitsILi160ELi64ELi64ELi8ES2_EEEv16Flash_bwd_params
    0 bytes stack frame, 0 bytes spill stores, 0 bytes spill loads
ptxas info    : Used 32 registers, 696 bytes cmem[0]
ptxas info    : Compiling entry function '_Z44flash_bwd_dq_dk_dv_loop_seqk_parallel_kernelI23Flash_bwd_kernel_traitsILi160ELi64ELi64ELi8ELi4ELi4ELi4ELb0ELb1EN7cutlass6half_tE19Flash_kernel_traitsILi160ELi64ELi64ELi8ES2_EELb1ELb0ELb0ELb0EEv16Flash_bwd_params' for 'sm_80'
ptxas info    : Function properties for _Z44flash_bwd_dq_dk_dv_loop_seqk_parallel_kernelI23Flash_bwd_kernel_traitsILi160ELi64ELi64ELi8ELi4ELi4ELi4ELb0ELb1EN7cutlass6half_tE19Flash_kernel_traitsILi160ELi64ELi64ELi8ES2_EELb1ELb0ELb0ELb0EEv16Flash_bwd_params
    0 bytes stack frame, 0 bytes spill stores, 0 bytes spill loads
ptxas info    : Used 222 registers, 696 bytes cmem[0]
ptxas info    : Compiling entry function '_Z44flash_bwd_dq_dk_dv_loop_seqk_parallel_kernelI23Flash_bwd_kernel_traitsILi160ELi64ELi64ELi8ELi4ELi4ELi4ELb0ELb1EN7cutlass6half_tE19Flash_kernel_traitsILi160ELi64ELi64ELi8ES2_EELb1ELb0ELb0ELb1EEv16Flash_bwd_params' for 'sm_80'
ptxas info    : Function properties for _Z44flash_bwd_dq_dk_dv_loop_seqk_parallel_kernelI23Flash_bwd_kernel_traitsILi160ELi64ELi64ELi8ELi4ELi4ELi4ELb0ELb1EN7cutlass6half_tE19Flash_kernel_traitsILi160ELi64ELi64ELi8ES2_EELb1ELb0ELb0ELb1EEv16Flash_bwd_params
    0 bytes stack frame, 0 bytes spill stores, 0 bytes spill loads
ptxas info    : Used 222 registers, 696 bytes cmem[0]
ptxas info    : Compiling entry function '_Z44flash_bwd_dq_dk_dv_loop_seqk_parallel_kernelI23Flash_bwd_kernel_traitsILi160ELi64ELi64ELi8ELi4ELi4ELi4ELb0ELb1EN7cutlass6half_tE19Flash_kernel_traitsILi160ELi64ELi64ELi8ES2_EELb1ELb0ELb1ELb0EEv16Flash_bwd_params' for 'sm_80'
ptxas info    : Function properties for _Z44flash_bwd_dq_dk_dv_loop_seqk_parallel_kernelI23Flash_bwd_kernel_traitsILi160ELi64ELi64ELi8ELi4ELi4ELi4ELb0ELb1EN7cutlass6half_tE19Flash_kernel_traitsILi160ELi64ELi64ELi8ES2_EELb1ELb0ELb1ELb0EEv16Flash_bwd_params
    0 bytes stack frame, 0 bytes spill stores, 0 bytes spill loads
ptxas info    : Used 208 registers, 696 bytes cmem[0]
ptxas info    : Compiling entry function '_Z44flash_bwd_dq_dk_dv_loop_seqk_parallel_kernelI23Flash_bwd_kernel_traitsILi160ELi64ELi64ELi8ELi4ELi4ELi4ELb0ELb1EN7cutlass6half_tE19Flash_kernel_traitsILi160ELi64ELi64ELi8ES2_EELb1ELb0ELb1ELb1EEv16Flash_bwd_params' for 'sm_80'
ptxas info    : Function properties for _Z44flash_bwd_dq_dk_dv_loop_seqk_parallel_kernelI23Flash_bwd_kernel_traitsILi160ELi64ELi64ELi8ELi4ELi4ELi4ELb0ELb1EN7cutlass6half_tE19Flash_kernel_traitsILi160ELi64ELi64ELi8ES2_EELb1ELb0ELb1ELb1EEv16Flash_bwd_params
    0 bytes stack frame, 0 bytes spill stores, 0 bytes spill loads
ptxas info    : Used 214 registers, 696 bytes cmem[0]
ptxas info    : Compiling entry function '_Z44flash_bwd_dq_dk_dv_loop_seqk_parallel_kernelI23Flash_bwd_kernel_traitsILi160ELi64ELi64ELi8ELi4ELi4ELi4ELb0ELb1EN7cutlass6half_tE19Flash_kernel_traitsILi160ELi64ELi64ELi8ES2_EELb1ELb1ELb0ELb0EEv16Flash_bwd_params' for 'sm_80'
ptxas info    : Function properties for _Z44flash_bwd_dq_dk_dv_loop_seqk_parallel_kernelI23Flash_bwd_kernel_traitsILi160ELi64ELi64ELi8ELi4ELi4ELi4ELb0ELb1EN7cutlass6half_tE19Flash_kernel_traitsILi160ELi64ELi64ELi8ES2_EELb1ELb1ELb0ELb0EEv16Flash_bwd_params
    0 bytes stack frame, 0 bytes spill stores, 0 bytes spill loads
ptxas info    : Used 216 registers, 696 bytes cmem[0]
ptxas info    : Compiling entry function '_Z44flash_bwd_dq_dk_dv_loop_seqk_parallel_kernelI23Flash_bwd_kernel_traitsILi160ELi64ELi64ELi8ELi4ELi4ELi4ELb0ELb1EN7cutlass6half_tE19Flash_kernel_traitsILi160ELi64ELi64ELi8ES2_EELb1ELb1ELb0ELb1EEv16Flash_bwd_params' for 'sm_80'
ptxas info    : Function properties for _Z44flash_bwd_dq_dk_dv_loop_seqk_parallel_kernelI23Flash_bwd_kernel_traitsILi160ELi64ELi64ELi8ELi4ELi4ELi4ELb0ELb1EN7cutlass6half_tE19Flash_kernel_traitsILi160ELi64ELi64ELi8ES2_EELb1ELb1ELb0ELb1EEv16Flash_bwd_params
    0 bytes stack frame, 0 bytes spill stores, 0 bytes spill loads
ptxas info    : Used 212 registers, 696 bytes cmem[0]
ptxas info    : Compiling entry function '_Z44flash_bwd_dq_dk_dv_loop_seqk_parallel_kernelI23Flash_bwd_kernel_traitsILi160ELi64ELi64ELi8ELi4ELi4ELi4ELb0ELb1EN7cutlass6half_tE19Flash_kernel_traitsILi160ELi64ELi64ELi8ES2_EELb1ELb1ELb1ELb0EEv16Flash_bwd_params' for 'sm_80'
ptxas info    : Function properties for _Z44flash_bwd_dq_dk_dv_loop_seqk_parallel_kernelI23Flash_bwd_kernel_traitsILi160ELi64ELi64ELi8ELi4ELi4ELi4ELb0ELb1EN7cutlass6half_tE19Flash_kernel_traitsILi160ELi64ELi64ELi8ES2_EELb1ELb1ELb1ELb0EEv16Flash_bwd_params
    0 bytes stack frame, 0 bytes spill stores, 0 bytes spill loads
ptxas info    : Used 208 registers, 696 bytes cmem[0]
ptxas info    : Compiling entry function '_Z44flash_bwd_dq_dk_dv_loop_seqk_parallel_kernelI23Flash_bwd_kernel_traitsILi160ELi64ELi64ELi8ELi4ELi4ELi4ELb0ELb1EN7cutlass6half_tE19Flash_kernel_traitsILi160ELi64ELi64ELi8ES2_EELb1ELb1ELb1ELb1EEv16Flash_bwd_params' for 'sm_80'
ptxas info    : Function properties for _Z44flash_bwd_dq_dk_dv_loop_seqk_parallel_kernelI23Flash_bwd_kernel_traitsILi160ELi64ELi64ELi8ELi4ELi4ELi4ELb0ELb1EN7cutlass6half_tE19Flash_kernel_traitsILi160ELi64ELi64ELi8ES2_EELb1ELb1ELb1ELb1EEv16Flash_bwd_params
    0 bytes stack frame, 0 bytes spill stores, 0 bytes spill loads
ptxas info    : Used 206 registers, 696 bytes cmem[0]
ptxas info    : Compiling entry function '_Z25flash_bwd_dot_do_o_kernelILb1E23Flash_bwd_kernel_traitsILi160ELi64ELi64ELi8ELi4ELi4ELi4ELb0ELb1EN7cutlass6half_tE19Flash_kernel_traitsILi160ELi64ELi64ELi8ES2_EEEv16Flash_bwd_params' for 'sm_80'
ptxas info    : Function properties for _Z25flash_bwd_dot_do_o_kernelILb1E23Flash_bwd_kernel_traitsILi160ELi64ELi64ELi8ELi4ELi4ELi4ELb0ELb1EN7cutlass6half_tE19Flash_kernel_traitsILi160ELi64ELi64ELi8ES2_EEEv16Flash_bwd_params
    0 bytes stack frame, 0 bytes spill stores, 0 bytes spill loads
ptxas info    : Used 64 registers, 696 bytes cmem[0]
ptxas info    : Compiling entry function '_Z27flash_bwd_convert_dq_kernelI23Flash_bwd_kernel_traitsILi160ELi64ELi64ELi8ELi4ELi4ELi4ELb0ELb0EN7cutlass6half_tE19Flash_kernel_traitsILi160ELi64ELi64ELi8ES2_EEEv16Flash_bwd_params' for 'sm_80'
ptxas info    : Function properties for _Z27flash_bwd_convert_dq_kernelI23Flash_bwd_kernel_traitsILi160ELi64ELi64ELi8ELi4ELi4ELi4ELb0ELb0EN7cutlass6half_tE19Flash_kernel_traitsILi160ELi64ELi64ELi8ES2_EEEv16Flash_bwd_params
    0 bytes stack frame, 0 bytes spill stores, 0 bytes spill loads
ptxas info    : Used 32 registers, 696 bytes cmem[0]
ptxas info    : Compiling entry function '_Z44flash_bwd_dq_dk_dv_loop_seqk_parallel_kernelI23Flash_bwd_kernel_traitsILi160ELi64ELi64ELi8ELi4ELi4ELi4ELb0ELb0EN7cutlass6half_tE19Flash_kernel_traitsILi160ELi64ELi64ELi8ES2_EELb1ELb0ELb0ELb0EEv16Flash_bwd_params' for 'sm_80'
ptxas info    : Function properties for _Z44flash_bwd_dq_dk_dv_loop_seqk_parallel_kernelI23Flash_bwd_kernel_traitsILi160ELi64ELi64ELi8ELi4ELi4ELi4ELb0ELb0EN7cutlass6half_tE19Flash_kernel_traitsILi160ELi64ELi64ELi8ES2_EELb1ELb0ELb0ELb0EEv16Flash_bwd_params
    0 bytes stack frame, 0 bytes spill stores, 0 bytes spill loads
ptxas info    : Used 226 registers, 696 bytes cmem[0]
ptxas info    : Compiling entry function '_Z44flash_bwd_dq_dk_dv_loop_seqk_parallel_kernelI23Flash_bwd_kernel_traitsILi160ELi64ELi64ELi8ELi4ELi4ELi4ELb0ELb0EN7cutlass6half_tE19Flash_kernel_traitsILi160ELi64ELi64ELi8ES2_EELb1ELb0ELb0ELb1EEv16Flash_bwd_params' for 'sm_80'
ptxas info    : Function properties for _Z44flash_bwd_dq_dk_dv_loop_seqk_parallel_kernelI23Flash_bwd_kernel_traitsILi160ELi64ELi64ELi8ELi4ELi4ELi4ELb0ELb0EN7cutlass6half_tE19Flash_kernel_traitsILi160ELi64ELi64ELi8ES2_EELb1ELb0ELb0ELb1EEv16Flash_bwd_params
    0 bytes stack frame, 0 bytes spill stores, 0 bytes spill loads
ptxas info    : Used 216 registers, 696 bytes cmem[0]
ptxas info    : Compiling entry function '_Z44flash_bwd_dq_dk_dv_loop_seqk_parallel_kernelI23Flash_bwd_kernel_traitsILi160ELi64ELi64ELi8ELi4ELi4ELi4ELb0ELb0EN7cutlass6half_tE19Flash_kernel_traitsILi160ELi64ELi64ELi8ES2_EELb1ELb0ELb1ELb0EEv16Flash_bwd_params' for 'sm_80'
ptxas info    : Function properties for _Z44flash_bwd_dq_dk_dv_loop_seqk_parallel_kernelI23Flash_bwd_kernel_traitsILi160ELi64ELi64ELi8ELi4ELi4ELi4ELb0ELb0EN7cutlass6half_tE19Flash_kernel_traitsILi160ELi64ELi64ELi8ES2_EELb1ELb0ELb1ELb0EEv16Flash_bwd_params
    0 bytes stack frame, 0 bytes spill stores, 0 bytes spill loads
ptxas info    : Used 220 registers, 696 bytes cmem[0]
ptxas info    : Compiling entry function '_Z44flash_bwd_dq_dk_dv_loop_seqk_parallel_kernelI23Flash_bwd_kernel_traitsILi160ELi64ELi64ELi8ELi4ELi4ELi4ELb0ELb0EN7cutlass6half_tE19Flash_kernel_traitsILi160ELi64ELi64ELi8ES2_EELb1ELb0ELb1ELb1EEv16Flash_bwd_params' for 'sm_80'
ptxas info    : Function properties for _Z44flash_bwd_dq_dk_dv_loop_seqk_parallel_kernelI23Flash_bwd_kernel_traitsILi160ELi64ELi64ELi8ELi4ELi4ELi4ELb0ELb0EN7cutlass6half_tE19Flash_kernel_traitsILi160ELi64ELi64ELi8ES2_EELb1ELb0ELb1ELb1EEv16Flash_bwd_params
    0 bytes stack frame, 0 bytes spill stores, 0 bytes spill loads
ptxas info    : Used 222 registers, 696 bytes cmem[0]
ptxas info    : Compiling entry function '_Z44flash_bwd_dq_dk_dv_loop_seqk_parallel_kernelI23Flash_bwd_kernel_traitsILi160ELi64ELi64ELi8ELi4ELi4ELi4ELb0ELb0EN7cutlass6half_tE19Flash_kernel_traitsILi160ELi64ELi64ELi8ES2_EELb1ELb1ELb0ELb0EEv16Flash_bwd_params' for 'sm_80'
ptxas info    : Function properties for _Z44flash_bwd_dq_dk_dv_loop_seqk_parallel_kernelI23Flash_bwd_kernel_traitsILi160ELi64ELi64ELi8ELi4ELi4ELi4ELb0ELb0EN7cutlass6half_tE19Flash_kernel_traitsILi160ELi64ELi64ELi8ES2_EELb1ELb1ELb0ELb0EEv16Flash_bwd_params
    0 bytes stack frame, 0 bytes spill stores, 0 bytes spill loads
ptxas info    : Used 222 registers, 696 bytes cmem[0]
ptxas info    : Compiling entry function '_Z44flash_bwd_dq_dk_dv_loop_seqk_parallel_kernelI23Flash_bwd_kernel_traitsILi160ELi64ELi64ELi8ELi4ELi4ELi4ELb0ELb0EN7cutlass6half_tE19Flash_kernel_traitsILi160ELi64ELi64ELi8ES2_EELb1ELb1ELb0ELb1EEv16Flash_bwd_params' for 'sm_80'
ptxas info    : Function properties for _Z44flash_bwd_dq_dk_dv_loop_seqk_parallel_kernelI23Flash_bwd_kernel_traitsILi160ELi64ELi64ELi8ELi4ELi4ELi4ELb0ELb0EN7cutlass6half_tE19Flash_kernel_traitsILi160ELi64ELi64ELi8ES2_EELb1ELb1ELb0ELb1EEv16Flash_bwd_params
    0 bytes stack frame, 0 bytes spill stores, 0 bytes spill loads
ptxas info    : Used 220 registers, 696 bytes cmem[0]
ptxas info    : Compiling entry function '_Z44flash_bwd_dq_dk_dv_loop_seqk_parallel_kernelI23Flash_bwd_kernel_traitsILi160ELi64ELi64ELi8ELi4ELi4ELi4ELb0ELb0EN7cutlass6half_tE19Flash_kernel_traitsILi160ELi64ELi64ELi8ES2_EELb1ELb1ELb1ELb0EEv16Flash_bwd_params' for 'sm_80'
ptxas info    : Function properties for _Z44flash_bwd_dq_dk_dv_loop_seqk_parallel_kernelI23Flash_bwd_kernel_traitsILi160ELi64ELi64ELi8ELi4ELi4ELi4ELb0ELb0EN7cutlass6half_tE19Flash_kernel_traitsILi160ELi64ELi64ELi8ES2_EELb1ELb1ELb1ELb0EEv16Flash_bwd_params
    0 bytes stack frame, 0 bytes spill stores, 0 bytes spill loads
ptxas info    : Used 212 registers, 696 bytes cmem[0]
ptxas info    : Compiling entry function '_Z44flash_bwd_dq_dk_dv_loop_seqk_parallel_kernelI23Flash_bwd_kernel_traitsILi160ELi64ELi64ELi8ELi4ELi4ELi4ELb0ELb0EN7cutlass6half_tE19Flash_kernel_traitsILi160ELi64ELi64ELi8ES2_EELb1ELb1ELb1ELb1EEv16Flash_bwd_params' for 'sm_80'
ptxas info    : Function properties for _Z44flash_bwd_dq_dk_dv_loop_seqk_parallel_kernelI23Flash_bwd_kernel_traitsILi160ELi64ELi64ELi8ELi4ELi4ELi4ELb0ELb0EN7cutlass6half_tE19Flash_kernel_traitsILi160ELi64ELi64ELi8ES2_EELb1ELb1ELb1ELb1EEv16Flash_bwd_params
    0 bytes stack frame, 0 bytes spill stores, 0 bytes spill loads
ptxas info    : Used 212 registers, 696 bytes cmem[0]
ptxas info    : Compiling entry function '_Z25flash_bwd_dot_do_o_kernelILb1E23Flash_bwd_kernel_traitsILi160ELi64ELi64ELi8ELi4ELi4ELi4ELb0ELb0EN7cutlass6half_tE19Flash_kernel_traitsILi160ELi64ELi64ELi8ES2_EEEv16Flash_bwd_params' for 'sm_80'
ptxas info    : Function properties for _Z25flash_bwd_dot_do_o_kernelILb1E23Flash_bwd_kernel_traitsILi160ELi64ELi64ELi8ELi4ELi4ELi4ELb0ELb0EN7cutlass6half_tE19Flash_kernel_traitsILi160ELi64ELi64ELi8ES2_EEEv16Flash_bwd_params
    0 bytes stack frame, 0 bytes spill stores, 0 bytes spill loads
ptxas info    : Used 64 registers, 696 bytes cmem[0]
/anaconda/envs/flashattention/lib/python3.9/site-packages/torch/include/c10/core/SymInt.h(84): warning: integer conversion resulted in a change of sign

[9/37] /usr/local/cuda-11.4/bin/nvcc  -I/home/v-feiychen/flash-attention/csrc/flash_attn -I/home/v-feiychen/flash-attention/csrc/flash_attn/src -I/home/v-feiychen/flash-attention/csrc/cutlass/include -I/anaconda/envs/flashattention/lib/python3.9/site-packages/torch/include -I/anaconda/envs/flashattention/lib/python3.9/site-packages/torch/include/torch/csrc/api/include -I/anaconda/envs/flashattention/lib/python3.9/site-packages/torch/include/TH -I/anaconda/envs/flashattention/lib/python3.9/site-packages/torch/include/THC -I/usr/local/cuda-11.4/include -I/anaconda/envs/flashattention/include/python3.9 -c -c /home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_bwd_hdim160_bf16_sm80.cu -o /home/v-feiychen/flash-attention/build/temp.linux-x86_64-cpython-39/csrc/flash_attn/src/flash_bwd_hdim160_bf16_sm80.o -D__CUDA_NO_HALF_OPERATORS__ -D__CUDA_NO_HALF_CONVERSIONS__ -D__CUDA_NO_BFLOAT16_CONVERSIONS__ -D__CUDA_NO_HALF2_OPERATORS__ --expt-relaxed-constexpr --compiler-options ''"'"'-fPIC'"'"'' -O3 -std=c++17 -U__CUDA_NO_HALF_OPERATORS__ -U__CUDA_NO_HALF_CONVERSIONS__ -U__CUDA_NO_HALF2_OPERATORS__ -U__CUDA_NO_BFLOAT16_CONVERSIONS__ --expt-relaxed-constexpr --expt-extended-lambda --use_fast_math --ptxas-options=-v -lineinfo -gencode arch=compute_80,code=sm_80 --threads 4 -DTORCH_API_INCLUDE_EXTENSION_H '-DPYBIND11_COMPILER_TYPE="_gcc"' '-DPYBIND11_STDLIB="_libstdcpp"' '-DPYBIND11_BUILD_ABI="_cxxabi1011"' -DTORCH_EXTENSION_NAME=flash_attn_2_cuda -D_GLIBCXX_USE_CXX11_ABI=0
/anaconda/envs/flashattention/lib/python3.9/site-packages/torch/include/c10/core/SymInt.h(84): warning: integer conversion resulted in a change of sign

ptxas info    : 2 bytes gmem
ptxas info    : Compiling entry function '_Z44flash_bwd_dq_dk_dv_loop_seqk_parallel_kernelI23Flash_bwd_kernel_traitsILi160ELi64ELi64ELi8ELi4ELi4ELi4ELb0ELb1EN7cutlass10bfloat16_tE19Flash_kernel_traitsILi160ELi64ELi64ELi8ES2_EELb0ELb0ELb0ELb0EEv16Flash_bwd_params' for 'sm_80'
ptxas info    : Function properties for _Z44flash_bwd_dq_dk_dv_loop_seqk_parallel_kernelI23Flash_bwd_kernel_traitsILi160ELi64ELi64ELi8ELi4ELi4ELi4ELb0ELb1EN7cutlass10bfloat16_tE19Flash_kernel_traitsILi160ELi64ELi64ELi8ES2_EELb0ELb0ELb0ELb0EEv16Flash_bwd_params
    0 bytes stack frame, 0 bytes spill stores, 0 bytes spill loads
ptxas info    : Used 204 registers, 696 bytes cmem[0]
ptxas info    : Compiling entry function '_Z44flash_bwd_dq_dk_dv_loop_seqk_parallel_kernelI23Flash_bwd_kernel_traitsILi160ELi64ELi64ELi8ELi4ELi4ELi4ELb0ELb1EN7cutlass10bfloat16_tE19Flash_kernel_traitsILi160ELi64ELi64ELi8ES2_EELb0ELb0ELb0ELb1EEv16Flash_bwd_params' for 'sm_80'
ptxas info    : Function properties for _Z44flash_bwd_dq_dk_dv_loop_seqk_parallel_kernelI23Flash_bwd_kernel_traitsILi160ELi64ELi64ELi8ELi4ELi4ELi4ELb0ELb1EN7cutlass10bfloat16_tE19Flash_kernel_traitsILi160ELi64ELi64ELi8ES2_EELb0ELb0ELb0ELb1EEv16Flash_bwd_params
    0 bytes stack frame, 0 bytes spill stores, 0 bytes spill loads
ptxas info    : Used 204 registers, 696 bytes cmem[0]
ptxas info    : Compiling entry function '_Z44flash_bwd_dq_dk_dv_loop_seqk_parallel_kernelI23Flash_bwd_kernel_traitsILi160ELi64ELi64ELi8ELi4ELi4ELi4ELb0ELb1EN7cutlass10bfloat16_tE19Flash_kernel_traitsILi160ELi64ELi64ELi8ES2_EELb0ELb0ELb1ELb0EEv16Flash_bwd_params' for 'sm_80'
ptxas info    : Function properties for _Z44flash_bwd_dq_dk_dv_loop_seqk_parallel_kernelI23Flash_bwd_kernel_traitsILi160ELi64ELi64ELi8ELi4ELi4ELi4ELb0ELb1EN7cutlass10bfloat16_tE19Flash_kernel_traitsILi160ELi64ELi64ELi8ES2_EELb0ELb0ELb1ELb0EEv16Flash_bwd_params
    0 bytes stack frame, 0 bytes spill stores, 0 bytes spill loads
ptxas info    : Used 190 registers, 696 bytes cmem[0]
ptxas info    : Compiling entry function '_Z44flash_bwd_dq_dk_dv_loop_seqk_parallel_kernelI23Flash_bwd_kernel_traitsILi160ELi64ELi64ELi8ELi4ELi4ELi4ELb0ELb1EN7cutlass10bfloat16_tE19Flash_kernel_traitsILi160ELi64ELi64ELi8ES2_EELb0ELb0ELb1ELb1EEv16Flash_bwd_params' for 'sm_80'
ptxas info    : Function properties for _Z44flash_bwd_dq_dk_dv_loop_seqk_parallel_kernelI23Flash_bwd_kernel_traitsILi160ELi64ELi64ELi8ELi4ELi4ELi4ELb0ELb1EN7cutlass10bfloat16_tE19Flash_kernel_traitsILi160ELi64ELi64ELi8ES2_EELb0ELb0ELb1ELb1EEv16Flash_bwd_params
    0 bytes stack frame, 0 bytes spill stores, 0 bytes spill loads
ptxas info    : Used 196 registers, 696 bytes cmem[0]
ptxas info    : Compiling entry function '_Z44flash_bwd_dq_dk_dv_loop_seqk_parallel_kernelI23Flash_bwd_kernel_traitsILi160ELi64ELi64ELi8ELi4ELi4ELi4ELb0ELb1EN7cutlass10bfloat16_tE19Flash_kernel_traitsILi160ELi64ELi64ELi8ES2_EELb0ELb1ELb0ELb0EEv16Flash_bwd_params' for 'sm_80'
ptxas info    : Function properties for _Z44flash_bwd_dq_dk_dv_loop_seqk_parallel_kernelI23Flash_bwd_kernel_traitsILi160ELi64ELi64ELi8ELi4ELi4ELi4ELb0ELb1EN7cutlass10bfloat16_tE19Flash_kernel_traitsILi160ELi64ELi64ELi8ES2_EELb0ELb1ELb0ELb0EEv16Flash_bwd_params
    0 bytes stack frame, 0 bytes spill stores, 0 bytes spill loads
ptxas info    : Used 210 registers, 696 bytes cmem[0]
ptxas info    : Compiling entry function '_Z44flash_bwd_dq_dk_dv_loop_seqk_parallel_kernelI23Flash_bwd_kernel_traitsILi160ELi64ELi64ELi8ELi4ELi4ELi4ELb0ELb1EN7cutlass10bfloat16_tE19Flash_kernel_traitsILi160ELi64ELi64ELi8ES2_EELb0ELb1ELb0ELb1EEv16Flash_bwd_params' for 'sm_80'
ptxas info    : Function properties for _Z44flash_bwd_dq_dk_dv_loop_seqk_parallel_kernelI23Flash_bwd_kernel_traitsILi160ELi64ELi64ELi8ELi4ELi4ELi4ELb0ELb1EN7cutlass10bfloat16_tE19Flash_kernel_traitsILi160ELi64ELi64ELi8ES2_EELb0ELb1ELb0ELb1EEv16Flash_bwd_params
    0 bytes stack frame, 0 bytes spill stores, 0 bytes spill loads
ptxas info    : Used 206 registers, 696 bytes cmem[0]
ptxas info    : Compiling entry function '_Z44flash_bwd_dq_dk_dv_loop_seqk_parallel_kernelI23Flash_bwd_kernel_traitsILi160ELi64ELi64ELi8ELi4ELi4ELi4ELb0ELb1EN7cutlass10bfloat16_tE19Flash_kernel_traitsILi160ELi64ELi64ELi8ES2_EELb0ELb1ELb1ELb0EEv16Flash_bwd_params' for 'sm_80'
ptxas info    : Function properties for _Z44flash_bwd_dq_dk_dv_loop_seqk_parallel_kernelI23Flash_bwd_kernel_traitsILi160ELi64ELi64ELi8ELi4ELi4ELi4ELb0ELb1EN7cutlass10bfloat16_tE19Flash_kernel_traitsILi160ELi64ELi64ELi8ES2_EELb0ELb1ELb1ELb0EEv16Flash_bwd_params
    0 bytes stack frame, 0 bytes spill stores, 0 bytes spill loads
ptxas info    : Used 202 registers, 696 bytes cmem[0]
ptxas info    : Compiling entry function '_Z44flash_bwd_dq_dk_dv_loop_seqk_parallel_kernelI23Flash_bwd_kernel_traitsILi160ELi64ELi64ELi8ELi4ELi4ELi4ELb0ELb1EN7cutlass10bfloat16_tE19Flash_kernel_traitsILi160ELi64ELi64ELi8ES2_EELb0ELb1ELb1ELb1EEv16Flash_bwd_params' for 'sm_80'
ptxas info    : Function properties for _Z44flash_bwd_dq_dk_dv_loop_seqk_parallel_kernelI23Flash_bwd_kernel_traitsILi160ELi64ELi64ELi8ELi4ELi4ELi4ELb0ELb1EN7cutlass10bfloat16_tE19Flash_kernel_traitsILi160ELi64ELi64ELi8ES2_EELb0ELb1ELb1ELb1EEv16Flash_bwd_params
    0 bytes stack frame, 0 bytes spill stores, 0 bytes spill loads
ptxas info    : Used 200 registers, 696 bytes cmem[0]
ptxas info    : Compiling entry function '_Z44flash_bwd_dq_dk_dv_loop_seqk_parallel_kernelI23Flash_bwd_kernel_traitsILi160ELi64ELi64ELi8ELi4ELi4ELi4ELb0ELb0EN7cutlass10bfloat16_tE19Flash_kernel_traitsILi160ELi64ELi64ELi8ES2_EELb0ELb0ELb0ELb0EEv16Flash_bwd_params' for 'sm_80'
ptxas info    : Function properties for _Z44flash_bwd_dq_dk_dv_loop_seqk_parallel_kernelI23Flash_bwd_kernel_traitsILi160ELi64ELi64ELi8ELi4ELi4ELi4ELb0ELb0EN7cutlass10bfloat16_tE19Flash_kernel_traitsILi160ELi64ELi64ELi8ES2_EELb0ELb0ELb0ELb0EEv16Flash_bwd_params
    0 bytes stack frame, 0 bytes spill stores, 0 bytes spill loads
ptxas info    : Used 210 registers, 696 bytes cmem[0]
ptxas info    : Compiling entry function '_Z44flash_bwd_dq_dk_dv_loop_seqk_parallel_kernelI23Flash_bwd_kernel_traitsILi160ELi64ELi64ELi8ELi4ELi4ELi4ELb0ELb0EN7cutlass10bfloat16_tE19Flash_kernel_traitsILi160ELi64ELi64ELi8ES2_EELb0ELb0ELb0ELb1EEv16Flash_bwd_params' for 'sm_80'
ptxas info    : Function properties for _Z44flash_bwd_dq_dk_dv_loop_seqk_parallel_kernelI23Flash_bwd_kernel_traitsILi160ELi64ELi64ELi8ELi4ELi4ELi4ELb0ELb0EN7cutlass10bfloat16_tE19Flash_kernel_traitsILi160ELi64ELi64ELi8ES2_EELb0ELb0ELb0ELb1EEv16Flash_bwd_params
    0 bytes stack frame, 0 bytes spill stores, 0 bytes spill loads
ptxas info    : Used 212 registers, 696 bytes cmem[0]
ptxas info    : Compiling entry function '_Z44flash_bwd_dq_dk_dv_loop_seqk_parallel_kernelI23Flash_bwd_kernel_traitsILi160ELi64ELi64ELi8ELi4ELi4ELi4ELb0ELb0EN7cutlass10bfloat16_tE19Flash_kernel_traitsILi160ELi64ELi64ELi8ES2_EELb0ELb0ELb1ELb0EEv16Flash_bwd_params' for 'sm_80'
ptxas info    : Function properties for _Z44flash_bwd_dq_dk_dv_loop_seqk_parallel_kernelI23Flash_bwd_kernel_traitsILi160ELi64ELi64ELi8ELi4ELi4ELi4ELb0ELb0EN7cutlass10bfloat16_tE19Flash_kernel_traitsILi160ELi64ELi64ELi8ES2_EELb0ELb0ELb1ELb0EEv16Flash_bwd_params
    0 bytes stack frame, 0 bytes spill stores, 0 bytes spill loads
ptxas info    : Used 204 registers, 696 bytes cmem[0]
ptxas info    : Compiling entry function '_Z44flash_bwd_dq_dk_dv_loop_seqk_parallel_kernelI23Flash_bwd_kernel_traitsILi160ELi64ELi64ELi8ELi4ELi4ELi4ELb0ELb0EN7cutlass10bfloat16_tE19Flash_kernel_traitsILi160ELi64ELi64ELi8ES2_EELb0ELb0ELb1ELb1EEv16Flash_bwd_params' for 'sm_80'
ptxas info    : Function properties for _Z44flash_bwd_dq_dk_dv_loop_seqk_parallel_kernelI23Flash_bwd_kernel_traitsILi160ELi64ELi64ELi8ELi4ELi4ELi4ELb0ELb0EN7cutlass10bfloat16_tE19Flash_kernel_traitsILi160ELi64ELi64ELi8ES2_EELb0ELb0ELb1ELb1EEv16Flash_bwd_params
    0 bytes stack frame, 0 bytes spill stores, 0 bytes spill loads
ptxas info    : Used 204 registers, 696 bytes cmem[0]
ptxas info    : Compiling entry function '_Z44flash_bwd_dq_dk_dv_loop_seqk_parallel_kernelI23Flash_bwd_kernel_traitsILi160ELi64ELi64ELi8ELi4ELi4ELi4ELb0ELb0EN7cutlass10bfloat16_tE19Flash_kernel_traitsILi160ELi64ELi64ELi8ES2_EELb0ELb1ELb0ELb0EEv16Flash_bwd_params' for 'sm_80'
ptxas info    : Function properties for _Z44flash_bwd_dq_dk_dv_loop_seqk_parallel_kernelI23Flash_bwd_kernel_traitsILi160ELi64ELi64ELi8ELi4ELi4ELi4ELb0ELb0EN7cutlass10bfloat16_tE19Flash_kernel_traitsILi160ELi64ELi64ELi8ES2_EELb0ELb1ELb0ELb0EEv16Flash_bwd_params
    0 bytes stack frame, 0 bytes spill stores, 0 bytes spill loads
ptxas info    : Used 216 registers, 696 bytes cmem[0]
ptxas info    : Compiling entry function '_Z44flash_bwd_dq_dk_dv_loop_seqk_parallel_kernelI23Flash_bwd_kernel_traitsILi160ELi64ELi64ELi8ELi4ELi4ELi4ELb0ELb0EN7cutlass10bfloat16_tE19Flash_kernel_traitsILi160ELi64ELi64ELi8ES2_EELb0ELb1ELb0ELb1EEv16Flash_bwd_params' for 'sm_80'
ptxas info    : Function properties for _Z44flash_bwd_dq_dk_dv_loop_seqk_parallel_kernelI23Flash_bwd_kernel_traitsILi160ELi64ELi64ELi8ELi4ELi4ELi4ELb0ELb0EN7cutlass10bfloat16_tE19Flash_kernel_traitsILi160ELi64ELi64ELi8ES2_EELb0ELb1ELb0ELb1EEv16Flash_bwd_params
    0 bytes stack frame, 0 bytes spill stores, 0 bytes spill loads
ptxas info    : Used 214 registers, 696 bytes cmem[0]
ptxas info    : Compiling entry function '_Z44flash_bwd_dq_dk_dv_loop_seqk_parallel_kernelI23Flash_bwd_kernel_traitsILi160ELi64ELi64ELi8ELi4ELi4ELi4ELb0ELb0EN7cutlass10bfloat16_tE19Flash_kernel_traitsILi160ELi64ELi64ELi8ES2_EELb0ELb1ELb1ELb0EEv16Flash_bwd_params' for 'sm_80'
ptxas info    : Function properties for _Z44flash_bwd_dq_dk_dv_loop_seqk_parallel_kernelI23Flash_bwd_kernel_traitsILi160ELi64ELi64ELi8ELi4ELi4ELi4ELb0ELb0EN7cutlass10bfloat16_tE19Flash_kernel_traitsILi160ELi64ELi64ELi8ES2_EELb0ELb1ELb1ELb0EEv16Flash_bwd_params
    0 bytes stack frame, 0 bytes spill stores, 0 bytes spill loads
ptxas info    : Used 208 registers, 696 bytes cmem[0]
ptxas info    : Compiling entry function '_Z44flash_bwd_dq_dk_dv_loop_seqk_parallel_kernelI23Flash_bwd_kernel_traitsILi160ELi64ELi64ELi8ELi4ELi4ELi4ELb0ELb0EN7cutlass10bfloat16_tE19Flash_kernel_traitsILi160ELi64ELi64ELi8ES2_EELb0ELb1ELb1ELb1EEv16Flash_bwd_params' for 'sm_80'
ptxas info    : Function properties for _Z44flash_bwd_dq_dk_dv_loop_seqk_parallel_kernelI23Flash_bwd_kernel_traitsILi160ELi64ELi64ELi8ELi4ELi4ELi4ELb0ELb0EN7cutlass10bfloat16_tE19Flash_kernel_traitsILi160ELi64ELi64ELi8ES2_EELb0ELb1ELb1ELb1EEv16Flash_bwd_params
    0 bytes stack frame, 0 bytes spill stores, 0 bytes spill loads
ptxas info    : Used 206 registers, 696 bytes cmem[0]
ptxas info    : Compiling entry function '_Z27flash_bwd_convert_dq_kernelI23Flash_bwd_kernel_traitsILi160ELi64ELi64ELi8ELi4ELi4ELi4ELb0ELb1EN7cutlass10bfloat16_tE19Flash_kernel_traitsILi160ELi64ELi64ELi8ES2_EEEv16Flash_bwd_params' for 'sm_80'
ptxas info    : Function properties for _Z27flash_bwd_convert_dq_kernelI23Flash_bwd_kernel_traitsILi160ELi64ELi64ELi8ELi4ELi4ELi4ELb0ELb1EN7cutlass10bfloat16_tE19Flash_kernel_traitsILi160ELi64ELi64ELi8ES2_EEEv16Flash_bwd_params
    0 bytes stack frame, 0 bytes spill stores, 0 bytes spill loads
ptxas info    : Used 32 registers, 696 bytes cmem[0]
ptxas info    : Compiling entry function '_Z44flash_bwd_dq_dk_dv_loop_seqk_parallel_kernelI23Flash_bwd_kernel_traitsILi160ELi64ELi64ELi8ELi4ELi4ELi4ELb0ELb1EN7cutlass10bfloat16_tE19Flash_kernel_traitsILi160ELi64ELi64ELi8ES2_EELb1ELb0ELb0ELb0EEv16Flash_bwd_params' for 'sm_80'
ptxas info    : Function properties for _Z44flash_bwd_dq_dk_dv_loop_seqk_parallel_kernelI23Flash_bwd_kernel_traitsILi160ELi64ELi64ELi8ELi4ELi4ELi4ELb0ELb1EN7cutlass10bfloat16_tE19Flash_kernel_traitsILi160ELi64ELi64ELi8ES2_EELb1ELb0ELb0ELb0EEv16Flash_bwd_params
    0 bytes stack frame, 0 bytes spill stores, 0 bytes spill loads
ptxas info    : Used 222 registers, 696 bytes cmem[0]
ptxas info    : Compiling entry function '_Z44flash_bwd_dq_dk_dv_loop_seqk_parallel_kernelI23Flash_bwd_kernel_traitsILi160ELi64ELi64ELi8ELi4ELi4ELi4ELb0ELb1EN7cutlass10bfloat16_tE19Flash_kernel_traitsILi160ELi64ELi64ELi8ES2_EELb1ELb0ELb0ELb1EEv16Flash_bwd_params' for 'sm_80'
ptxas info    : Function properties for _Z44flash_bwd_dq_dk_dv_loop_seqk_parallel_kernelI23Flash_bwd_kernel_traitsILi160ELi64ELi64ELi8ELi4ELi4ELi4ELb0ELb1EN7cutlass10bfloat16_tE19Flash_kernel_traitsILi160ELi64ELi64ELi8ES2_EELb1ELb0ELb0ELb1EEv16Flash_bwd_params
    0 bytes stack frame, 0 bytes spill stores, 0 bytes spill loads
ptxas info    : Used 222 registers, 696 bytes cmem[0]
ptxas info    : Compiling entry function '_Z44flash_bwd_dq_dk_dv_loop_seqk_parallel_kernelI23Flash_bwd_kernel_traitsILi160ELi64ELi64ELi8ELi4ELi4ELi4ELb0ELb1EN7cutlass10bfloat16_tE19Flash_kernel_traitsILi160ELi64ELi64ELi8ES2_EELb1ELb0ELb1ELb0EEv16Flash_bwd_params' for 'sm_80'
ptxas info    : Function properties for _Z44flash_bwd_dq_dk_dv_loop_seqk_parallel_kernelI23Flash_bwd_kernel_traitsILi160ELi64ELi64ELi8ELi4ELi4ELi4ELb0ELb1EN7cutlass10bfloat16_tE19Flash_kernel_traitsILi160ELi64ELi64ELi8ES2_EELb1ELb0ELb1ELb0EEv16Flash_bwd_params
    0 bytes stack frame, 0 bytes spill stores, 0 bytes spill loads
ptxas info    : Used 208 registers, 696 bytes cmem[0]
ptxas info    : Compiling entry function '_Z44flash_bwd_dq_dk_dv_loop_seqk_parallel_kernelI23Flash_bwd_kernel_traitsILi160ELi64ELi64ELi8ELi4ELi4ELi4ELb0ELb1EN7cutlass10bfloat16_tE19Flash_kernel_traitsILi160ELi64ELi64ELi8ES2_EELb1ELb0ELb1ELb1EEv16Flash_bwd_params' for 'sm_80'
ptxas info    : Function properties for _Z44flash_bwd_dq_dk_dv_loop_seqk_parallel_kernelI23Flash_bwd_kernel_traitsILi160ELi64ELi64ELi8ELi4ELi4ELi4ELb0ELb1EN7cutlass10bfloat16_tE19Flash_kernel_traitsILi160ELi64ELi64ELi8ES2_EELb1ELb0ELb1ELb1EEv16Flash_bwd_params
    0 bytes stack frame, 0 bytes spill stores, 0 bytes spill loads
ptxas info    : Used 214 registers, 696 bytes cmem[0]
ptxas info    : Compiling entry function '_Z44flash_bwd_dq_dk_dv_loop_seqk_parallel_kernelI23Flash_bwd_kernel_traitsILi160ELi64ELi64ELi8ELi4ELi4ELi4ELb0ELb1EN7cutlass10bfloat16_tE19Flash_kernel_traitsILi160ELi64ELi64ELi8ES2_EELb1ELb1ELb0ELb0EEv16Flash_bwd_params' for 'sm_80'
ptxas info    : Function properties for _Z44flash_bwd_dq_dk_dv_loop_seqk_parallel_kernelI23Flash_bwd_kernel_traitsILi160ELi64ELi64ELi8ELi4ELi4ELi4ELb0ELb1EN7cutlass10bfloat16_tE19Flash_kernel_traitsILi160ELi64ELi64ELi8ES2_EELb1ELb1ELb0ELb0EEv16Flash_bwd_params
    0 bytes stack frame, 0 bytes spill stores, 0 bytes spill loads
ptxas info    : Used 216 registers, 696 bytes cmem[0]
ptxas info    : Compiling entry function '_Z44flash_bwd_dq_dk_dv_loop_seqk_parallel_kernelI23Flash_bwd_kernel_traitsILi160ELi64ELi64ELi8ELi4ELi4ELi4ELb0ELb1EN7cutlass10bfloat16_tE19Flash_kernel_traitsILi160ELi64ELi64ELi8ES2_EELb1ELb1ELb0ELb1EEv16Flash_bwd_params' for 'sm_80'
ptxas info    : Function properties for _Z44flash_bwd_dq_dk_dv_loop_seqk_parallel_kernelI23Flash_bwd_kernel_traitsILi160ELi64ELi64ELi8ELi4ELi4ELi4ELb0ELb1EN7cutlass10bfloat16_tE19Flash_kernel_traitsILi160ELi64ELi64ELi8ES2_EELb1ELb1ELb0ELb1EEv16Flash_bwd_params
    0 bytes stack frame, 0 bytes spill stores, 0 bytes spill loads
ptxas info    : Used 212 registers, 696 bytes cmem[0]
ptxas info    : Compiling entry function '_Z44flash_bwd_dq_dk_dv_loop_seqk_parallel_kernelI23Flash_bwd_kernel_traitsILi160ELi64ELi64ELi8ELi4ELi4ELi4ELb0ELb1EN7cutlass10bfloat16_tE19Flash_kernel_traitsILi160ELi64ELi64ELi8ES2_EELb1ELb1ELb1ELb0EEv16Flash_bwd_params' for 'sm_80'
ptxas info    : Function properties for _Z44flash_bwd_dq_dk_dv_loop_seqk_parallel_kernelI23Flash_bwd_kernel_traitsILi160ELi64ELi64ELi8ELi4ELi4ELi4ELb0ELb1EN7cutlass10bfloat16_tE19Flash_kernel_traitsILi160ELi64ELi64ELi8ES2_EELb1ELb1ELb1ELb0EEv16Flash_bwd_params
    0 bytes stack frame, 0 bytes spill stores, 0 bytes spill loads
ptxas info    : Used 208 registers, 696 bytes cmem[0]
ptxas info    : Compiling entry function '_Z44flash_bwd_dq_dk_dv_loop_seqk_parallel_kernelI23Flash_bwd_kernel_traitsILi160ELi64ELi64ELi8ELi4ELi4ELi4ELb0ELb1EN7cutlass10bfloat16_tE19Flash_kernel_traitsILi160ELi64ELi64ELi8ES2_EELb1ELb1ELb1ELb1EEv16Flash_bwd_params' for 'sm_80'
ptxas info    : Function properties for _Z44flash_bwd_dq_dk_dv_loop_seqk_parallel_kernelI23Flash_bwd_kernel_traitsILi160ELi64ELi64ELi8ELi4ELi4ELi4ELb0ELb1EN7cutlass10bfloat16_tE19Flash_kernel_traitsILi160ELi64ELi64ELi8ES2_EELb1ELb1ELb1ELb1EEv16Flash_bwd_params
    0 bytes stack frame, 0 bytes spill stores, 0 bytes spill loads
ptxas info    : Used 206 registers, 696 bytes cmem[0]
ptxas info    : Compiling entry function '_Z25flash_bwd_dot_do_o_kernelILb1E23Flash_bwd_kernel_traitsILi160ELi64ELi64ELi8ELi4ELi4ELi4ELb0ELb1EN7cutlass10bfloat16_tE19Flash_kernel_traitsILi160ELi64ELi64ELi8ES2_EEEv16Flash_bwd_params' for 'sm_80'
ptxas info    : Function properties for _Z25flash_bwd_dot_do_o_kernelILb1E23Flash_bwd_kernel_traitsILi160ELi64ELi64ELi8ELi4ELi4ELi4ELb0ELb1EN7cutlass10bfloat16_tE19Flash_kernel_traitsILi160ELi64ELi64ELi8ES2_EEEv16Flash_bwd_params
    0 bytes stack frame, 0 bytes spill stores, 0 bytes spill loads
ptxas info    : Used 64 registers, 696 bytes cmem[0]
ptxas info    : Compiling entry function '_Z27flash_bwd_convert_dq_kernelI23Flash_bwd_kernel_traitsILi160ELi64ELi64ELi8ELi4ELi4ELi4ELb0ELb0EN7cutlass10bfloat16_tE19Flash_kernel_traitsILi160ELi64ELi64ELi8ES2_EEEv16Flash_bwd_params' for 'sm_80'
ptxas info    : Function properties for _Z27flash_bwd_convert_dq_kernelI23Flash_bwd_kernel_traitsILi160ELi64ELi64ELi8ELi4ELi4ELi4ELb0ELb0EN7cutlass10bfloat16_tE19Flash_kernel_traitsILi160ELi64ELi64ELi8ES2_EEEv16Flash_bwd_params
    0 bytes stack frame, 0 bytes spill stores, 0 bytes spill loads
ptxas info    : Used 32 registers, 696 bytes cmem[0]
ptxas info    : Compiling entry function '_Z44flash_bwd_dq_dk_dv_loop_seqk_parallel_kernelI23Flash_bwd_kernel_traitsILi160ELi64ELi64ELi8ELi4ELi4ELi4ELb0ELb0EN7cutlass10bfloat16_tE19Flash_kernel_traitsILi160ELi64ELi64ELi8ES2_EELb1ELb0ELb0ELb0EEv16Flash_bwd_params' for 'sm_80'
ptxas info    : Function properties for _Z44flash_bwd_dq_dk_dv_loop_seqk_parallel_kernelI23Flash_bwd_kernel_traitsILi160ELi64ELi64ELi8ELi4ELi4ELi4ELb0ELb0EN7cutlass10bfloat16_tE19Flash_kernel_traitsILi160ELi64ELi64ELi8ES2_EELb1ELb0ELb0ELb0EEv16Flash_bwd_params
    0 bytes stack frame, 0 bytes spill stores, 0 bytes spill loads
ptxas info    : Used 226 registers, 696 bytes cmem[0]
ptxas info    : Compiling entry function '_Z44flash_bwd_dq_dk_dv_loop_seqk_parallel_kernelI23Flash_bwd_kernel_traitsILi160ELi64ELi64ELi8ELi4ELi4ELi4ELb0ELb0EN7cutlass10bfloat16_tE19Flash_kernel_traitsILi160ELi64ELi64ELi8ES2_EELb1ELb0ELb0ELb1EEv16Flash_bwd_params' for 'sm_80'
ptxas info    : Function properties for _Z44flash_bwd_dq_dk_dv_loop_seqk_parallel_kernelI23Flash_bwd_kernel_traitsILi160ELi64ELi64ELi8ELi4ELi4ELi4ELb0ELb0EN7cutlass10bfloat16_tE19Flash_kernel_traitsILi160ELi64ELi64ELi8ES2_EELb1ELb0ELb0ELb1EEv16Flash_bwd_params
    0 bytes stack frame, 0 bytes spill stores, 0 bytes spill loads
ptxas info    : Used 216 registers, 696 bytes cmem[0]
ptxas info    : Compiling entry function '_Z44flash_bwd_dq_dk_dv_loop_seqk_parallel_kernelI23Flash_bwd_kernel_traitsILi160ELi64ELi64ELi8ELi4ELi4ELi4ELb0ELb0EN7cutlass10bfloat16_tE19Flash_kernel_traitsILi160ELi64ELi64ELi8ES2_EELb1ELb0ELb1ELb0EEv16Flash_bwd_params' for 'sm_80'
ptxas info    : Function properties for _Z44flash_bwd_dq_dk_dv_loop_seqk_parallel_kernelI23Flash_bwd_kernel_traitsILi160ELi64ELi64ELi8ELi4ELi4ELi4ELb0ELb0EN7cutlass10bfloat16_tE19Flash_kernel_traitsILi160ELi64ELi64ELi8ES2_EELb1ELb0ELb1ELb0EEv16Flash_bwd_params
    0 bytes stack frame, 0 bytes spill stores, 0 bytes spill loads
ptxas info    : Used 220 registers, 696 bytes cmem[0]
ptxas info    : Compiling entry function '_Z44flash_bwd_dq_dk_dv_loop_seqk_parallel_kernelI23Flash_bwd_kernel_traitsILi160ELi64ELi64ELi8ELi4ELi4ELi4ELb0ELb0EN7cutlass10bfloat16_tE19Flash_kernel_traitsILi160ELi64ELi64ELi8ES2_EELb1ELb0ELb1ELb1EEv16Flash_bwd_params' for 'sm_80'
ptxas info    : Function properties for _Z44flash_bwd_dq_dk_dv_loop_seqk_parallel_kernelI23Flash_bwd_kernel_traitsILi160ELi64ELi64ELi8ELi4ELi4ELi4ELb0ELb0EN7cutlass10bfloat16_tE19Flash_kernel_traitsILi160ELi64ELi64ELi8ES2_EELb1ELb0ELb1ELb1EEv16Flash_bwd_params
    0 bytes stack frame, 0 bytes spill stores, 0 bytes spill loads
ptxas info    : Used 222 registers, 696 bytes cmem[0]
ptxas info    : Compiling entry function '_Z44flash_bwd_dq_dk_dv_loop_seqk_parallel_kernelI23Flash_bwd_kernel_traitsILi160ELi64ELi64ELi8ELi4ELi4ELi4ELb0ELb0EN7cutlass10bfloat16_tE19Flash_kernel_traitsILi160ELi64ELi64ELi8ES2_EELb1ELb1ELb0ELb0EEv16Flash_bwd_params' for 'sm_80'
ptxas info    : Function properties for _Z44flash_bwd_dq_dk_dv_loop_seqk_parallel_kernelI23Flash_bwd_kernel_traitsILi160ELi64ELi64ELi8ELi4ELi4ELi4ELb0ELb0EN7cutlass10bfloat16_tE19Flash_kernel_traitsILi160ELi64ELi64ELi8ES2_EELb1ELb1ELb0ELb0EEv16Flash_bwd_params
    0 bytes stack frame, 0 bytes spill stores, 0 bytes spill loads
ptxas info    : Used 222 registers, 696 bytes cmem[0]
ptxas info    : Compiling entry function '_Z44flash_bwd_dq_dk_dv_loop_seqk_parallel_kernelI23Flash_bwd_kernel_traitsILi160ELi64ELi64ELi8ELi4ELi4ELi4ELb0ELb0EN7cutlass10bfloat16_tE19Flash_kernel_traitsILi160ELi64ELi64ELi8ES2_EELb1ELb1ELb0ELb1EEv16Flash_bwd_params' for 'sm_80'
ptxas info    : Function properties for _Z44flash_bwd_dq_dk_dv_loop_seqk_parallel_kernelI23Flash_bwd_kernel_traitsILi160ELi64ELi64ELi8ELi4ELi4ELi4ELb0ELb0EN7cutlass10bfloat16_tE19Flash_kernel_traitsILi160ELi64ELi64ELi8ES2_EELb1ELb1ELb0ELb1EEv16Flash_bwd_params
    0 bytes stack frame, 0 bytes spill stores, 0 bytes spill loads
ptxas info    : Used 220 registers, 696 bytes cmem[0]
ptxas info    : Compiling entry function '_Z44flash_bwd_dq_dk_dv_loop_seqk_parallel_kernelI23Flash_bwd_kernel_traitsILi160ELi64ELi64ELi8ELi4ELi4ELi4ELb0ELb0EN7cutlass10bfloat16_tE19Flash_kernel_traitsILi160ELi64ELi64ELi8ES2_EELb1ELb1ELb1ELb0EEv16Flash_bwd_params' for 'sm_80'
ptxas info    : Function properties for _Z44flash_bwd_dq_dk_dv_loop_seqk_parallel_kernelI23Flash_bwd_kernel_traitsILi160ELi64ELi64ELi8ELi4ELi4ELi4ELb0ELb0EN7cutlass10bfloat16_tE19Flash_kernel_traitsILi160ELi64ELi64ELi8ES2_EELb1ELb1ELb1ELb0EEv16Flash_bwd_params
    0 bytes stack frame, 0 bytes spill stores, 0 bytes spill loads
ptxas info    : Used 212 registers, 696 bytes cmem[0]
ptxas info    : Compiling entry function '_Z44flash_bwd_dq_dk_dv_loop_seqk_parallel_kernelI23Flash_bwd_kernel_traitsILi160ELi64ELi64ELi8ELi4ELi4ELi4ELb0ELb0EN7cutlass10bfloat16_tE19Flash_kernel_traitsILi160ELi64ELi64ELi8ES2_EELb1ELb1ELb1ELb1EEv16Flash_bwd_params' for 'sm_80'
ptxas info    : Function properties for _Z44flash_bwd_dq_dk_dv_loop_seqk_parallel_kernelI23Flash_bwd_kernel_traitsILi160ELi64ELi64ELi8ELi4ELi4ELi4ELb0ELb0EN7cutlass10bfloat16_tE19Flash_kernel_traitsILi160ELi64ELi64ELi8ES2_EELb1ELb1ELb1ELb1EEv16Flash_bwd_params
    0 bytes stack frame, 0 bytes spill stores, 0 bytes spill loads
ptxas info    : Used 212 registers, 696 bytes cmem[0]
ptxas info    : Compiling entry function '_Z25flash_bwd_dot_do_o_kernelILb1E23Flash_bwd_kernel_traitsILi160ELi64ELi64ELi8ELi4ELi4ELi4ELb0ELb0EN7cutlass10bfloat16_tE19Flash_kernel_traitsILi160ELi64ELi64ELi8ES2_EEEv16Flash_bwd_params' for 'sm_80'
ptxas info    : Function properties for _Z25flash_bwd_dot_do_o_kernelILb1E23Flash_bwd_kernel_traitsILi160ELi64ELi64ELi8ELi4ELi4ELi4ELb0ELb0EN7cutlass10bfloat16_tE19Flash_kernel_traitsILi160ELi64ELi64ELi8ES2_EEEv16Flash_bwd_params
    0 bytes stack frame, 0 bytes spill stores, 0 bytes spill loads
ptxas info    : Used 64 registers, 696 bytes cmem[0]
/anaconda/envs/flashattention/lib/python3.9/site-packages/torch/include/c10/core/SymInt.h(84): warning: integer conversion resulted in a change of sign

[10/37] /usr/local/cuda-11.4/bin/nvcc  -I/home/v-feiychen/flash-attention/csrc/flash_attn -I/home/v-feiychen/flash-attention/csrc/flash_attn/src -I/home/v-feiychen/flash-attention/csrc/cutlass/include -I/anaconda/envs/flashattention/lib/python3.9/site-packages/torch/include -I/anaconda/envs/flashattention/lib/python3.9/site-packages/torch/include/torch/csrc/api/include -I/anaconda/envs/flashattention/lib/python3.9/site-packages/torch/include/TH -I/anaconda/envs/flashattention/lib/python3.9/site-packages/torch/include/THC -I/usr/local/cuda-11.4/include -I/anaconda/envs/flashattention/include/python3.9 -c -c /home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_hdim128_bf16_sm80.cu -o /home/v-feiychen/flash-attention/build/temp.linux-x86_64-cpython-39/csrc/flash_attn/src/flash_fwd_hdim128_bf16_sm80.o -D__CUDA_NO_HALF_OPERATORS__ -D__CUDA_NO_HALF_CONVERSIONS__ -D__CUDA_NO_BFLOAT16_CONVERSIONS__ -D__CUDA_NO_HALF2_OPERATORS__ --expt-relaxed-constexpr --compiler-options ''"'"'-fPIC'"'"'' -O3 -std=c++17 -U__CUDA_NO_HALF_OPERATORS__ -U__CUDA_NO_HALF_CONVERSIONS__ -U__CUDA_NO_HALF2_OPERATORS__ -U__CUDA_NO_BFLOAT16_CONVERSIONS__ --expt-relaxed-constexpr --expt-extended-lambda --use_fast_math --ptxas-options=-v -lineinfo -gencode arch=compute_80,code=sm_80 --threads 4 -DTORCH_API_INCLUDE_EXTENSION_H '-DPYBIND11_COMPILER_TYPE="_gcc"' '-DPYBIND11_STDLIB="_libstdcpp"' '-DPYBIND11_BUILD_ABI="_cxxabi1011"' -DTORCH_EXTENSION_NAME=flash_attn_2_cuda -D_GLIBCXX_USE_CXX11_ABI=0
/anaconda/envs/flashattention/lib/python3.9/site-packages/torch/include/c10/core/SymInt.h(84): warning: integer conversion resulted in a change of sign

/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_kernel.h(140): warning: variable "MMA_M" was declared but never referenced
          detected during:
            instantiation of "void flash::compute_attn_1rowblock<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params,Is_Q_split_K>(const Params &, int, int, int) [with Kernel_traits=Flash_fwd_kernel_traits<128, 128, 32, 4, false, false, cutlass::bfloat16_t, Flash_kernel_traits<128, 128, 32, 4, cutlass::bfloat16_t>>, Is_dropout=true, Is_causal=true, Is_even_N=true, Is_even_K=true, Return_softmax=true, Params=Flash_fwd_params, Is_Q_split_K=false]" 
(589): here
            instantiation of "void flash::compute_attn<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &) [with Kernel_traits=Flash_fwd_kernel_traits<128, 128, 32, 4, false, false, cutlass::bfloat16_t, Flash_kernel_traits<128, 128, 32, 4, cutlass::bfloat16_t>>, Is_dropout=true, Is_causal=true, Is_even_N=true, Is_even_K=true, Return_softmax=true, Params=Flash_fwd_params]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(15): here
            instantiation of "void flash_fwd_kernel<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax>(Flash_fwd_params) [with Kernel_traits=Flash_fwd_kernel_traits<128, 128, 32, 4, false, false, cutlass::bfloat16_t, Flash_kernel_traits<128, 128, 32, 4, cutlass::bfloat16_t>>, Is_dropout=true, Is_causal=true, Is_even_N=true, Is_even_K=true, Return_softmax=true]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(34): here
            instantiation of "void run_flash_fwd<Kernel_traits,Is_dropout,Is_causal>(Flash_fwd_params &, cudaStream_t) [with Kernel_traits=Flash_fwd_kernel_traits<128, 128, 32, 4, false, false, cutlass::bfloat16_t, Flash_kernel_traits<128, 128, 32, 4, cutlass::bfloat16_t>>, Is_dropout=true, Is_causal=true]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(118): here
            instantiation of "void run_mha_fwd_hdim128<T>(Flash_fwd_params &, cudaStream_t) [with T=cutlass::bfloat16_t]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_hdim128_bf16_sm80.cu(18): here

/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_kernel.h(276): warning: variable "tQrQ" was declared but never referenced
          detected during:
            instantiation of "void flash::compute_attn_1rowblock<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params,Is_Q_split_K>(const Params &, int, int, int) [with Kernel_traits=Flash_fwd_kernel_traits<128, 128, 32, 4, false, false, cutlass::bfloat16_t, Flash_kernel_traits<128, 128, 32, 4, cutlass::bfloat16_t>>, Is_dropout=true, Is_causal=true, Is_even_N=true, Is_even_K=true, Return_softmax=true, Params=Flash_fwd_params, Is_Q_split_K=false]" 
(589): here
            instantiation of "void flash::compute_attn<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &) [with Kernel_traits=Flash_fwd_kernel_traits<128, 128, 32, 4, false, false, cutlass::bfloat16_t, Flash_kernel_traits<128, 128, 32, 4, cutlass::bfloat16_t>>, Is_dropout=true, Is_causal=true, Is_even_N=true, Is_even_K=true, Return_softmax=true, Params=Flash_fwd_params]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(15): here
            instantiation of "void flash_fwd_kernel<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax>(Flash_fwd_params) [with Kernel_traits=Flash_fwd_kernel_traits<128, 128, 32, 4, false, false, cutlass::bfloat16_t, Flash_kernel_traits<128, 128, 32, 4, cutlass::bfloat16_t>>, Is_dropout=true, Is_causal=true, Is_even_N=true, Is_even_K=true, Return_softmax=true]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(34): here
            instantiation of "void run_flash_fwd<Kernel_traits,Is_dropout,Is_causal>(Flash_fwd_params &, cudaStream_t) [with Kernel_traits=Flash_fwd_kernel_traits<128, 128, 32, 4, false, false, cutlass::bfloat16_t, Flash_kernel_traits<128, 128, 32, 4, cutlass::bfloat16_t>>, Is_dropout=true, Is_causal=true]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(118): here
            instantiation of "void run_mha_fwd_hdim128<T>(Flash_fwd_params &, cudaStream_t) [with T=cutlass::bfloat16_t]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_hdim128_bf16_sm80.cu(18): here

/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_kernel.h(140): warning: variable "MMA_M" was declared but never referenced
          detected during:
            instantiation of "void flash::compute_attn_1rowblock<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params,Is_Q_split_K>(const Params &, int, int, int) [with Kernel_traits=Flash_fwd_kernel_traits<128, 128, 32, 4, false, false, cutlass::bfloat16_t, Flash_kernel_traits<128, 128, 32, 4, cutlass::bfloat16_t>>, Is_dropout=true, Is_causal=true, Is_even_N=true, Is_even_K=true, Return_softmax=false, Params=Flash_fwd_params, Is_Q_split_K=false]" 
(589): here
            instantiation of "void flash::compute_attn<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &) [with Kernel_traits=Flash_fwd_kernel_traits<128, 128, 32, 4, false, false, cutlass::bfloat16_t, Flash_kernel_traits<128, 128, 32, 4, cutlass::bfloat16_t>>, Is_dropout=true, Is_causal=true, Is_even_N=true, Is_even_K=true, Return_softmax=false, Params=Flash_fwd_params]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(15): here
            instantiation of "void flash_fwd_kernel<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax>(Flash_fwd_params) [with Kernel_traits=Flash_fwd_kernel_traits<128, 128, 32, 4, false, false, cutlass::bfloat16_t, Flash_kernel_traits<128, 128, 32, 4, cutlass::bfloat16_t>>, Is_dropout=true, Is_causal=true, Is_even_N=true, Is_even_K=true, Return_softmax=false]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(34): here
            instantiation of "void run_flash_fwd<Kernel_traits,Is_dropout,Is_causal>(Flash_fwd_params &, cudaStream_t) [with Kernel_traits=Flash_fwd_kernel_traits<128, 128, 32, 4, false, false, cutlass::bfloat16_t, Flash_kernel_traits<128, 128, 32, 4, cutlass::bfloat16_t>>, Is_dropout=true, Is_causal=true]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(118): here
            instantiation of "void run_mha_fwd_hdim128<T>(Flash_fwd_params &, cudaStream_t) [with T=cutlass::bfloat16_t]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_hdim128_bf16_sm80.cu(18): here

/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_kernel.h(276): warning: variable "tQrQ" was declared but never referenced
          detected during:
            instantiation of "void flash::compute_attn_1rowblock<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params,Is_Q_split_K>(const Params &, int, int, int) [with Kernel_traits=Flash_fwd_kernel_traits<128, 128, 32, 4, false, false, cutlass::bfloat16_t, Flash_kernel_traits<128, 128, 32, 4, cutlass::bfloat16_t>>, Is_dropout=true, Is_causal=true, Is_even_N=true, Is_even_K=true, Return_softmax=false, Params=Flash_fwd_params, Is_Q_split_K=false]" 
(589): here
            instantiation of "void flash::compute_attn<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &) [with Kernel_traits=Flash_fwd_kernel_traits<128, 128, 32, 4, false, false, cutlass::bfloat16_t, Flash_kernel_traits<128, 128, 32, 4, cutlass::bfloat16_t>>, Is_dropout=true, Is_causal=true, Is_even_N=true, Is_even_K=true, Return_softmax=false, Params=Flash_fwd_params]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(15): here
            instantiation of "void flash_fwd_kernel<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax>(Flash_fwd_params) [with Kernel_traits=Flash_fwd_kernel_traits<128, 128, 32, 4, false, false, cutlass::bfloat16_t, Flash_kernel_traits<128, 128, 32, 4, cutlass::bfloat16_t>>, Is_dropout=true, Is_causal=true, Is_even_N=true, Is_even_K=true, Return_softmax=false]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(34): here
            instantiation of "void run_flash_fwd<Kernel_traits,Is_dropout,Is_causal>(Flash_fwd_params &, cudaStream_t) [with Kernel_traits=Flash_fwd_kernel_traits<128, 128, 32, 4, false, false, cutlass::bfloat16_t, Flash_kernel_traits<128, 128, 32, 4, cutlass::bfloat16_t>>, Is_dropout=true, Is_causal=true]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(118): here
            instantiation of "void run_mha_fwd_hdim128<T>(Flash_fwd_params &, cudaStream_t) [with T=cutlass::bfloat16_t]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_hdim128_bf16_sm80.cu(18): here

/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_kernel.h(140): warning: variable "MMA_M" was declared but never referenced
          detected during:
            instantiation of "void flash::compute_attn_1rowblock<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params,Is_Q_split_K>(const Params &, int, int, int) [with Kernel_traits=Flash_fwd_kernel_traits<128, 128, 32, 4, false, false, cutlass::bfloat16_t, Flash_kernel_traits<128, 128, 32, 4, cutlass::bfloat16_t>>, Is_dropout=true, Is_causal=true, Is_even_N=true, Is_even_K=false, Return_softmax=true, Params=Flash_fwd_params, Is_Q_split_K=false]" 
(589): here
            instantiation of "void flash::compute_attn<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &) [with Kernel_traits=Flash_fwd_kernel_traits<128, 128, 32, 4, false, false, cutlass::bfloat16_t, Flash_kernel_traits<128, 128, 32, 4, cutlass::bfloat16_t>>, Is_dropout=true, Is_causal=true, Is_even_N=true, Is_even_K=false, Return_softmax=true, Params=Flash_fwd_params]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(15): here
            instantiation of "void flash_fwd_kernel<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax>(Flash_fwd_params) [with Kernel_traits=Flash_fwd_kernel_traits<128, 128, 32, 4, false, false, cutlass::bfloat16_t, Flash_kernel_traits<128, 128, 32, 4, cutlass::bfloat16_t>>, Is_dropout=true, Is_causal=true, Is_even_N=true, Is_even_K=false, Return_softmax=true]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(34): here
            instantiation of "void run_flash_fwd<Kernel_traits,Is_dropout,Is_causal>(Flash_fwd_params &, cudaStream_t) [with Kernel_traits=Flash_fwd_kernel_traits<128, 128, 32, 4, false, false, cutlass::bfloat16_t, Flash_kernel_traits<128, 128, 32, 4, cutlass::bfloat16_t>>, Is_dropout=true, Is_causal=true]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(118): here
            instantiation of "void run_mha_fwd_hdim128<T>(Flash_fwd_params &, cudaStream_t) [with T=cutlass::bfloat16_t]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_hdim128_bf16_sm80.cu(18): here

/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_kernel.h(276): warning: variable "tQrQ" was declared but never referenced
          detected during:
            instantiation of "void flash::compute_attn_1rowblock<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params,Is_Q_split_K>(const Params &, int, int, int) [with Kernel_traits=Flash_fwd_kernel_traits<128, 128, 32, 4, false, false, cutlass::bfloat16_t, Flash_kernel_traits<128, 128, 32, 4, cutlass::bfloat16_t>>, Is_dropout=true, Is_causal=true, Is_even_N=true, Is_even_K=false, Return_softmax=true, Params=Flash_fwd_params, Is_Q_split_K=false]" 
(589): here
            instantiation of "void flash::compute_attn<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &) [with Kernel_traits=Flash_fwd_kernel_traits<128, 128, 32, 4, false, false, cutlass::bfloat16_t, Flash_kernel_traits<128, 128, 32, 4, cutlass::bfloat16_t>>, Is_dropout=true, Is_causal=true, Is_even_N=true, Is_even_K=false, Return_softmax=true, Params=Flash_fwd_params]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(15): here
            instantiation of "void flash_fwd_kernel<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax>(Flash_fwd_params) [with Kernel_traits=Flash_fwd_kernel_traits<128, 128, 32, 4, false, false, cutlass::bfloat16_t, Flash_kernel_traits<128, 128, 32, 4, cutlass::bfloat16_t>>, Is_dropout=true, Is_causal=true, Is_even_N=true, Is_even_K=false, Return_softmax=true]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(34): here
            instantiation of "void run_flash_fwd<Kernel_traits,Is_dropout,Is_causal>(Flash_fwd_params &, cudaStream_t) [with Kernel_traits=Flash_fwd_kernel_traits<128, 128, 32, 4, false, false, cutlass::bfloat16_t, Flash_kernel_traits<128, 128, 32, 4, cutlass::bfloat16_t>>, Is_dropout=true, Is_causal=true]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(118): here
            instantiation of "void run_mha_fwd_hdim128<T>(Flash_fwd_params &, cudaStream_t) [with T=cutlass::bfloat16_t]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_hdim128_bf16_sm80.cu(18): here

/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_kernel.h(140): warning: variable "MMA_M" was declared but never referenced
          detected during:
            instantiation of "void flash::compute_attn_1rowblock<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params,Is_Q_split_K>(const Params &, int, int, int) [with Kernel_traits=Flash_fwd_kernel_traits<128, 128, 32, 4, false, false, cutlass::bfloat16_t, Flash_kernel_traits<128, 128, 32, 4, cutlass::bfloat16_t>>, Is_dropout=true, Is_causal=true, Is_even_N=true, Is_even_K=false, Return_softmax=false, Params=Flash_fwd_params, Is_Q_split_K=false]" 
(589): here
            instantiation of "void flash::compute_attn<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &) [with Kernel_traits=Flash_fwd_kernel_traits<128, 128, 32, 4, false, false, cutlass::bfloat16_t, Flash_kernel_traits<128, 128, 32, 4, cutlass::bfloat16_t>>, Is_dropout=true, Is_causal=true, Is_even_N=true, Is_even_K=false, Return_softmax=false, Params=Flash_fwd_params]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(15): here
            instantiation of "void flash_fwd_kernel<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax>(Flash_fwd_params) [with Kernel_traits=Flash_fwd_kernel_traits<128, 128, 32, 4, false, false, cutlass::bfloat16_t, Flash_kernel_traits<128, 128, 32, 4, cutlass::bfloat16_t>>, Is_dropout=true, Is_causal=true, Is_even_N=true, Is_even_K=false, Return_softmax=false]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(34): here
            instantiation of "void run_flash_fwd<Kernel_traits,Is_dropout,Is_causal>(Flash_fwd_params &, cudaStream_t) [with Kernel_traits=Flash_fwd_kernel_traits<128, 128, 32, 4, false, false, cutlass::bfloat16_t, Flash_kernel_traits<128, 128, 32, 4, cutlass::bfloat16_t>>, Is_dropout=true, Is_causal=true]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(118): here
            instantiation of "void run_mha_fwd_hdim128<T>(Flash_fwd_params &, cudaStream_t) [with T=cutlass::bfloat16_t]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_hdim128_bf16_sm80.cu(18): here

/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_kernel.h(276): warning: variable "tQrQ" was declared but never referenced
          detected during:
            instantiation of "void flash::compute_attn_1rowblock<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params,Is_Q_split_K>(const Params &, int, int, int) [with Kernel_traits=Flash_fwd_kernel_traits<128, 128, 32, 4, false, false, cutlass::bfloat16_t, Flash_kernel_traits<128, 128, 32, 4, cutlass::bfloat16_t>>, Is_dropout=true, Is_causal=true, Is_even_N=true, Is_even_K=false, Return_softmax=false, Params=Flash_fwd_params, Is_Q_split_K=false]" 
(589): here
            instantiation of "void flash::compute_attn<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &) [with Kernel_traits=Flash_fwd_kernel_traits<128, 128, 32, 4, false, false, cutlass::bfloat16_t, Flash_kernel_traits<128, 128, 32, 4, cutlass::bfloat16_t>>, Is_dropout=true, Is_causal=true, Is_even_N=true, Is_even_K=false, Return_softmax=false, Params=Flash_fwd_params]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(15): here
            instantiation of "void flash_fwd_kernel<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax>(Flash_fwd_params) [with Kernel_traits=Flash_fwd_kernel_traits<128, 128, 32, 4, false, false, cutlass::bfloat16_t, Flash_kernel_traits<128, 128, 32, 4, cutlass::bfloat16_t>>, Is_dropout=true, Is_causal=true, Is_even_N=true, Is_even_K=false, Return_softmax=false]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(34): here
            instantiation of "void run_flash_fwd<Kernel_traits,Is_dropout,Is_causal>(Flash_fwd_params &, cudaStream_t) [with Kernel_traits=Flash_fwd_kernel_traits<128, 128, 32, 4, false, false, cutlass::bfloat16_t, Flash_kernel_traits<128, 128, 32, 4, cutlass::bfloat16_t>>, Is_dropout=true, Is_causal=true]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(118): here
            instantiation of "void run_mha_fwd_hdim128<T>(Flash_fwd_params &, cudaStream_t) [with T=cutlass::bfloat16_t]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_hdim128_bf16_sm80.cu(18): here

/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_kernel.h(140): warning: variable "MMA_M" was declared but never referenced
          detected during:
            instantiation of "void flash::compute_attn_1rowblock<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params,Is_Q_split_K>(const Params &, int, int, int) [with Kernel_traits=Flash_fwd_kernel_traits<128, 128, 32, 4, false, false, cutlass::bfloat16_t, Flash_kernel_traits<128, 128, 32, 4, cutlass::bfloat16_t>>, Is_dropout=true, Is_causal=true, Is_even_N=false, Is_even_K=true, Return_softmax=true, Params=Flash_fwd_params, Is_Q_split_K=false]" 
(589): here
            instantiation of "void flash::compute_attn<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &) [with Kernel_traits=Flash_fwd_kernel_traits<128, 128, 32, 4, false, false, cutlass::bfloat16_t, Flash_kernel_traits<128, 128, 32, 4, cutlass::bfloat16_t>>, Is_dropout=true, Is_causal=true, Is_even_N=false, Is_even_K=true, Return_softmax=true, Params=Flash_fwd_params]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(15): here
            instantiation of "void flash_fwd_kernel<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax>(Flash_fwd_params) [with Kernel_traits=Flash_fwd_kernel_traits<128, 128, 32, 4, false, false, cutlass::bfloat16_t, Flash_kernel_traits<128, 128, 32, 4, cutlass::bfloat16_t>>, Is_dropout=true, Is_causal=true, Is_even_N=false, Is_even_K=true, Return_softmax=true]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(34): here
            instantiation of "void run_flash_fwd<Kernel_traits,Is_dropout,Is_causal>(Flash_fwd_params &, cudaStream_t) [with Kernel_traits=Flash_fwd_kernel_traits<128, 128, 32, 4, false, false, cutlass::bfloat16_t, Flash_kernel_traits<128, 128, 32, 4, cutlass::bfloat16_t>>, Is_dropout=true, Is_causal=true]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(118): here
            instantiation of "void run_mha_fwd_hdim128<T>(Flash_fwd_params &, cudaStream_t) [with T=cutlass::bfloat16_t]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_hdim128_bf16_sm80.cu(18): here

/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_kernel.h(276): warning: variable "tQrQ" was declared but never referenced
          detected during:
            instantiation of "void flash::compute_attn_1rowblock<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params,Is_Q_split_K>(const Params &, int, int, int) [with Kernel_traits=Flash_fwd_kernel_traits<128, 128, 32, 4, false, false, cutlass::bfloat16_t, Flash_kernel_traits<128, 128, 32, 4, cutlass::bfloat16_t>>, Is_dropout=true, Is_causal=true, Is_even_N=false, Is_even_K=true, Return_softmax=true, Params=Flash_fwd_params, Is_Q_split_K=false]" 
(589): here
            instantiation of "void flash::compute_attn<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &) [with Kernel_traits=Flash_fwd_kernel_traits<128, 128, 32, 4, false, false, cutlass::bfloat16_t, Flash_kernel_traits<128, 128, 32, 4, cutlass::bfloat16_t>>, Is_dropout=true, Is_causal=true, Is_even_N=false, Is_even_K=true, Return_softmax=true, Params=Flash_fwd_params]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(15): here
            instantiation of "void flash_fwd_kernel<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax>(Flash_fwd_params) [with Kernel_traits=Flash_fwd_kernel_traits<128, 128, 32, 4, false, false, cutlass::bfloat16_t, Flash_kernel_traits<128, 128, 32, 4, cutlass::bfloat16_t>>, Is_dropout=true, Is_causal=true, Is_even_N=false, Is_even_K=true, Return_softmax=true]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(34): here
            instantiation of "void run_flash_fwd<Kernel_traits,Is_dropout,Is_causal>(Flash_fwd_params &, cudaStream_t) [with Kernel_traits=Flash_fwd_kernel_traits<128, 128, 32, 4, false, false, cutlass::bfloat16_t, Flash_kernel_traits<128, 128, 32, 4, cutlass::bfloat16_t>>, Is_dropout=true, Is_causal=true]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(118): here
            instantiation of "void run_mha_fwd_hdim128<T>(Flash_fwd_params &, cudaStream_t) [with T=cutlass::bfloat16_t]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_hdim128_bf16_sm80.cu(18): here

/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_kernel.h(140): warning: variable "MMA_M" was declared but never referenced
          detected during:
            instantiation of "void flash::compute_attn_1rowblock<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params,Is_Q_split_K>(const Params &, int, int, int) [with Kernel_traits=Flash_fwd_kernel_traits<128, 128, 32, 4, false, false, cutlass::bfloat16_t, Flash_kernel_traits<128, 128, 32, 4, cutlass::bfloat16_t>>, Is_dropout=true, Is_causal=true, Is_even_N=false, Is_even_K=true, Return_softmax=false, Params=Flash_fwd_params, Is_Q_split_K=false]" 
(589): here
            instantiation of "void flash::compute_attn<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &) [with Kernel_traits=Flash_fwd_kernel_traits<128, 128, 32, 4, false, false, cutlass::bfloat16_t, Flash_kernel_traits<128, 128, 32, 4, cutlass::bfloat16_t>>, Is_dropout=true, Is_causal=true, Is_even_N=false, Is_even_K=true, Return_softmax=false, Params=Flash_fwd_params]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(15): here
            instantiation of "void flash_fwd_kernel<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax>(Flash_fwd_params) [with Kernel_traits=Flash_fwd_kernel_traits<128, 128, 32, 4, false, false, cutlass::bfloat16_t, Flash_kernel_traits<128, 128, 32, 4, cutlass::bfloat16_t>>, Is_dropout=true, Is_causal=true, Is_even_N=false, Is_even_K=true, Return_softmax=false]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(34): here
            instantiation of "void run_flash_fwd<Kernel_traits,Is_dropout,Is_causal>(Flash_fwd_params &, cudaStream_t) [with Kernel_traits=Flash_fwd_kernel_traits<128, 128, 32, 4, false, false, cutlass::bfloat16_t, Flash_kernel_traits<128, 128, 32, 4, cutlass::bfloat16_t>>, Is_dropout=true, Is_causal=true]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(118): here
            instantiation of "void run_mha_fwd_hdim128<T>(Flash_fwd_params &, cudaStream_t) [with T=cutlass::bfloat16_t]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_hdim128_bf16_sm80.cu(18): here

/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_kernel.h(276): warning: variable "tQrQ" was declared but never referenced
          detected during:
            instantiation of "void flash::compute_attn_1rowblock<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params,Is_Q_split_K>(const Params &, int, int, int) [with Kernel_traits=Flash_fwd_kernel_traits<128, 128, 32, 4, false, false, cutlass::bfloat16_t, Flash_kernel_traits<128, 128, 32, 4, cutlass::bfloat16_t>>, Is_dropout=true, Is_causal=true, Is_even_N=false, Is_even_K=true, Return_softmax=false, Params=Flash_fwd_params, Is_Q_split_K=false]" 
(589): here
            instantiation of "void flash::compute_attn<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &) [with Kernel_traits=Flash_fwd_kernel_traits<128, 128, 32, 4, false, false, cutlass::bfloat16_t, Flash_kernel_traits<128, 128, 32, 4, cutlass::bfloat16_t>>, Is_dropout=true, Is_causal=true, Is_even_N=false, Is_even_K=true, Return_softmax=false, Params=Flash_fwd_params]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(15): here
            instantiation of "void flash_fwd_kernel<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax>(Flash_fwd_params) [with Kernel_traits=Flash_fwd_kernel_traits<128, 128, 32, 4, false, false, cutlass::bfloat16_t, Flash_kernel_traits<128, 128, 32, 4, cutlass::bfloat16_t>>, Is_dropout=true, Is_causal=true, Is_even_N=false, Is_even_K=true, Return_softmax=false]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(34): here
            instantiation of "void run_flash_fwd<Kernel_traits,Is_dropout,Is_causal>(Flash_fwd_params &, cudaStream_t) [with Kernel_traits=Flash_fwd_kernel_traits<128, 128, 32, 4, false, false, cutlass::bfloat16_t, Flash_kernel_traits<128, 128, 32, 4, cutlass::bfloat16_t>>, Is_dropout=true, Is_causal=true]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(118): here
            instantiation of "void run_mha_fwd_hdim128<T>(Flash_fwd_params &, cudaStream_t) [with T=cutlass::bfloat16_t]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_hdim128_bf16_sm80.cu(18): here

/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_kernel.h(140): warning: variable "MMA_M" was declared but never referenced
          detected during:
            instantiation of "void flash::compute_attn_1rowblock<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params,Is_Q_split_K>(const Params &, int, int, int) [with Kernel_traits=Flash_fwd_kernel_traits<128, 128, 32, 4, false, false, cutlass::bfloat16_t, Flash_kernel_traits<128, 128, 32, 4, cutlass::bfloat16_t>>, Is_dropout=true, Is_causal=true, Is_even_N=false, Is_even_K=false, Return_softmax=true, Params=Flash_fwd_params, Is_Q_split_K=false]" 
(589): here
            instantiation of "void flash::compute_attn<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &) [with Kernel_traits=Flash_fwd_kernel_traits<128, 128, 32, 4, false, false, cutlass::bfloat16_t, Flash_kernel_traits<128, 128, 32, 4, cutlass::bfloat16_t>>, Is_dropout=true, Is_causal=true, Is_even_N=false, Is_even_K=false, Return_softmax=true, Params=Flash_fwd_params]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(15): here
            instantiation of "void flash_fwd_kernel<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax>(Flash_fwd_params) [with Kernel_traits=Flash_fwd_kernel_traits<128, 128, 32, 4, false, false, cutlass::bfloat16_t, Flash_kernel_traits<128, 128, 32, 4, cutlass::bfloat16_t>>, Is_dropout=true, Is_causal=true, Is_even_N=false, Is_even_K=false, Return_softmax=true]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(34): here
            instantiation of "void run_flash_fwd<Kernel_traits,Is_dropout,Is_causal>(Flash_fwd_params &, cudaStream_t) [with Kernel_traits=Flash_fwd_kernel_traits<128, 128, 32, 4, false, false, cutlass::bfloat16_t, Flash_kernel_traits<128, 128, 32, 4, cutlass::bfloat16_t>>, Is_dropout=true, Is_causal=true]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(118): here
            instantiation of "void run_mha_fwd_hdim128<T>(Flash_fwd_params &, cudaStream_t) [with T=cutlass::bfloat16_t]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_hdim128_bf16_sm80.cu(18): here

/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_kernel.h(276): warning: variable "tQrQ" was declared but never referenced
          detected during:
            instantiation of "void flash::compute_attn_1rowblock<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params,Is_Q_split_K>(const Params &, int, int, int) [with Kernel_traits=Flash_fwd_kernel_traits<128, 128, 32, 4, false, false, cutlass::bfloat16_t, Flash_kernel_traits<128, 128, 32, 4, cutlass::bfloat16_t>>, Is_dropout=true, Is_causal=true, Is_even_N=false, Is_even_K=false, Return_softmax=true, Params=Flash_fwd_params, Is_Q_split_K=false]" 
(589): here
            instantiation of "void flash::compute_attn<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &) [with Kernel_traits=Flash_fwd_kernel_traits<128, 128, 32, 4, false, false, cutlass::bfloat16_t, Flash_kernel_traits<128, 128, 32, 4, cutlass::bfloat16_t>>, Is_dropout=true, Is_causal=true, Is_even_N=false, Is_even_K=false, Return_softmax=true, Params=Flash_fwd_params]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(15): here
            instantiation of "void flash_fwd_kernel<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax>(Flash_fwd_params) [with Kernel_traits=Flash_fwd_kernel_traits<128, 128, 32, 4, false, false, cutlass::bfloat16_t, Flash_kernel_traits<128, 128, 32, 4, cutlass::bfloat16_t>>, Is_dropout=true, Is_causal=true, Is_even_N=false, Is_even_K=false, Return_softmax=true]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(34): here
            instantiation of "void run_flash_fwd<Kernel_traits,Is_dropout,Is_causal>(Flash_fwd_params &, cudaStream_t) [with Kernel_traits=Flash_fwd_kernel_traits<128, 128, 32, 4, false, false, cutlass::bfloat16_t, Flash_kernel_traits<128, 128, 32, 4, cutlass::bfloat16_t>>, Is_dropout=true, Is_causal=true]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(118): here
            instantiation of "void run_mha_fwd_hdim128<T>(Flash_fwd_params &, cudaStream_t) [with T=cutlass::bfloat16_t]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_hdim128_bf16_sm80.cu(18): here

/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_kernel.h(140): warning: variable "MMA_M" was declared but never referenced
          detected during:
            instantiation of "void flash::compute_attn_1rowblock<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params,Is_Q_split_K>(const Params &, int, int, int) [with Kernel_traits=Flash_fwd_kernel_traits<128, 128, 32, 4, false, false, cutlass::bfloat16_t, Flash_kernel_traits<128, 128, 32, 4, cutlass::bfloat16_t>>, Is_dropout=true, Is_causal=true, Is_even_N=false, Is_even_K=false, Return_softmax=false, Params=Flash_fwd_params, Is_Q_split_K=false]" 
(589): here
            instantiation of "void flash::compute_attn<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &) [with Kernel_traits=Flash_fwd_kernel_traits<128, 128, 32, 4, false, false, cutlass::bfloat16_t, Flash_kernel_traits<128, 128, 32, 4, cutlass::bfloat16_t>>, Is_dropout=true, Is_causal=true, Is_even_N=false, Is_even_K=false, Return_softmax=false, Params=Flash_fwd_params]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(15): here
            instantiation of "void flash_fwd_kernel<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax>(Flash_fwd_params) [with Kernel_traits=Flash_fwd_kernel_traits<128, 128, 32, 4, false, false, cutlass::bfloat16_t, Flash_kernel_traits<128, 128, 32, 4, cutlass::bfloat16_t>>, Is_dropout=true, Is_causal=true, Is_even_N=false, Is_even_K=false, Return_softmax=false]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(34): here
            instantiation of "void run_flash_fwd<Kernel_traits,Is_dropout,Is_causal>(Flash_fwd_params &, cudaStream_t) [with Kernel_traits=Flash_fwd_kernel_traits<128, 128, 32, 4, false, false, cutlass::bfloat16_t, Flash_kernel_traits<128, 128, 32, 4, cutlass::bfloat16_t>>, Is_dropout=true, Is_causal=true]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(118): here
            instantiation of "void run_mha_fwd_hdim128<T>(Flash_fwd_params &, cudaStream_t) [with T=cutlass::bfloat16_t]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_hdim128_bf16_sm80.cu(18): here

/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_kernel.h(276): warning: variable "tQrQ" was declared but never referenced
          detected during:
            instantiation of "void flash::compute_attn_1rowblock<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params,Is_Q_split_K>(const Params &, int, int, int) [with Kernel_traits=Flash_fwd_kernel_traits<128, 128, 32, 4, false, false, cutlass::bfloat16_t, Flash_kernel_traits<128, 128, 32, 4, cutlass::bfloat16_t>>, Is_dropout=true, Is_causal=true, Is_even_N=false, Is_even_K=false, Return_softmax=false, Params=Flash_fwd_params, Is_Q_split_K=false]" 
(589): here
            instantiation of "void flash::compute_attn<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &) [with Kernel_traits=Flash_fwd_kernel_traits<128, 128, 32, 4, false, false, cutlass::bfloat16_t, Flash_kernel_traits<128, 128, 32, 4, cutlass::bfloat16_t>>, Is_dropout=true, Is_causal=true, Is_even_N=false, Is_even_K=false, Return_softmax=false, Params=Flash_fwd_params]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(15): here
            instantiation of "void flash_fwd_kernel<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax>(Flash_fwd_params) [with Kernel_traits=Flash_fwd_kernel_traits<128, 128, 32, 4, false, false, cutlass::bfloat16_t, Flash_kernel_traits<128, 128, 32, 4, cutlass::bfloat16_t>>, Is_dropout=true, Is_causal=true, Is_even_N=false, Is_even_K=false, Return_softmax=false]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(34): here
            instantiation of "void run_flash_fwd<Kernel_traits,Is_dropout,Is_causal>(Flash_fwd_params &, cudaStream_t) [with Kernel_traits=Flash_fwd_kernel_traits<128, 128, 32, 4, false, false, cutlass::bfloat16_t, Flash_kernel_traits<128, 128, 32, 4, cutlass::bfloat16_t>>, Is_dropout=true, Is_causal=true]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(118): here
            instantiation of "void run_mha_fwd_hdim128<T>(Flash_fwd_params &, cudaStream_t) [with T=cutlass::bfloat16_t]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_hdim128_bf16_sm80.cu(18): here

/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_kernel.h(140): warning: variable "MMA_M" was declared but never referenced
          detected during:
            instantiation of "void flash::compute_attn_1rowblock<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params,Is_Q_split_K>(const Params &, int, int, int) [with Kernel_traits=Flash_fwd_kernel_traits<128, 128, 32, 4, false, false, cutlass::bfloat16_t, Flash_kernel_traits<128, 128, 32, 4, cutlass::bfloat16_t>>, Is_dropout=true, Is_causal=false, Is_even_N=true, Is_even_K=true, Return_softmax=true, Params=Flash_fwd_params, Is_Q_split_K=false]" 
(589): here
            instantiation of "void flash::compute_attn<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &) [with Kernel_traits=Flash_fwd_kernel_traits<128, 128, 32, 4, false, false, cutlass::bfloat16_t, Flash_kernel_traits<128, 128, 32, 4, cutlass::bfloat16_t>>, Is_dropout=true, Is_causal=false, Is_even_N=true, Is_even_K=true, Return_softmax=true, Params=Flash_fwd_params]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(15): here
            instantiation of "void flash_fwd_kernel<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax>(Flash_fwd_params) [with Kernel_traits=Flash_fwd_kernel_traits<128, 128, 32, 4, false, false, cutlass::bfloat16_t, Flash_kernel_traits<128, 128, 32, 4, cutlass::bfloat16_t>>, Is_dropout=true, Is_causal=false, Is_even_N=true, Is_even_K=true, Return_softmax=true]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(34): here
            instantiation of "void run_flash_fwd<Kernel_traits,Is_dropout,Is_causal>(Flash_fwd_params &, cudaStream_t) [with Kernel_traits=Flash_fwd_kernel_traits<128, 128, 32, 4, false, false, cutlass::bfloat16_t, Flash_kernel_traits<128, 128, 32, 4, cutlass::bfloat16_t>>, Is_dropout=true, Is_causal=false]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(118): here
            instantiation of "void run_mha_fwd_hdim128<T>(Flash_fwd_params &, cudaStream_t) [with T=cutlass::bfloat16_t]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_hdim128_bf16_sm80.cu(18): here

/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_kernel.h(276): warning: variable "tQrQ" was declared but never referenced
          detected during:
            instantiation of "void flash::compute_attn_1rowblock<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params,Is_Q_split_K>(const Params &, int, int, int) [with Kernel_traits=Flash_fwd_kernel_traits<128, 128, 32, 4, false, false, cutlass::bfloat16_t, Flash_kernel_traits<128, 128, 32, 4, cutlass::bfloat16_t>>, Is_dropout=true, Is_causal=false, Is_even_N=true, Is_even_K=true, Return_softmax=true, Params=Flash_fwd_params, Is_Q_split_K=false]" 
(589): here
            instantiation of "void flash::compute_attn<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &) [with Kernel_traits=Flash_fwd_kernel_traits<128, 128, 32, 4, false, false, cutlass::bfloat16_t, Flash_kernel_traits<128, 128, 32, 4, cutlass::bfloat16_t>>, Is_dropout=true, Is_causal=false, Is_even_N=true, Is_even_K=true, Return_softmax=true, Params=Flash_fwd_params]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(15): here
            instantiation of "void flash_fwd_kernel<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax>(Flash_fwd_params) [with Kernel_traits=Flash_fwd_kernel_traits<128, 128, 32, 4, false, false, cutlass::bfloat16_t, Flash_kernel_traits<128, 128, 32, 4, cutlass::bfloat16_t>>, Is_dropout=true, Is_causal=false, Is_even_N=true, Is_even_K=true, Return_softmax=true]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(34): here
            instantiation of "void run_flash_fwd<Kernel_traits,Is_dropout,Is_causal>(Flash_fwd_params &, cudaStream_t) [with Kernel_traits=Flash_fwd_kernel_traits<128, 128, 32, 4, false, false, cutlass::bfloat16_t, Flash_kernel_traits<128, 128, 32, 4, cutlass::bfloat16_t>>, Is_dropout=true, Is_causal=false]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(118): here
            instantiation of "void run_mha_fwd_hdim128<T>(Flash_fwd_params &, cudaStream_t) [with T=cutlass::bfloat16_t]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_hdim128_bf16_sm80.cu(18): here

/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_kernel.h(140): warning: variable "MMA_M" was declared but never referenced
          detected during:
            instantiation of "void flash::compute_attn_1rowblock<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params,Is_Q_split_K>(const Params &, int, int, int) [with Kernel_traits=Flash_fwd_kernel_traits<128, 128, 32, 4, false, false, cutlass::bfloat16_t, Flash_kernel_traits<128, 128, 32, 4, cutlass::bfloat16_t>>, Is_dropout=true, Is_causal=false, Is_even_N=true, Is_even_K=true, Return_softmax=false, Params=Flash_fwd_params, Is_Q_split_K=false]" 
(589): here
            instantiation of "void flash::compute_attn<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &) [with Kernel_traits=Flash_fwd_kernel_traits<128, 128, 32, 4, false, false, cutlass::bfloat16_t, Flash_kernel_traits<128, 128, 32, 4, cutlass::bfloat16_t>>, Is_dropout=true, Is_causal=false, Is_even_N=true, Is_even_K=true, Return_softmax=false, Params=Flash_fwd_params]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(15): here
            instantiation of "void flash_fwd_kernel<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax>(Flash_fwd_params) [with Kernel_traits=Flash_fwd_kernel_traits<128, 128, 32, 4, false, false, cutlass::bfloat16_t, Flash_kernel_traits<128, 128, 32, 4, cutlass::bfloat16_t>>, Is_dropout=true, Is_causal=false, Is_even_N=true, Is_even_K=true, Return_softmax=false]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(34): here
            instantiation of "void run_flash_fwd<Kernel_traits,Is_dropout,Is_causal>(Flash_fwd_params &, cudaStream_t) [with Kernel_traits=Flash_fwd_kernel_traits<128, 128, 32, 4, false, false, cutlass::bfloat16_t, Flash_kernel_traits<128, 128, 32, 4, cutlass::bfloat16_t>>, Is_dropout=true, Is_causal=false]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(118): here
            instantiation of "void run_mha_fwd_hdim128<T>(Flash_fwd_params &, cudaStream_t) [with T=cutlass::bfloat16_t]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_hdim128_bf16_sm80.cu(18): here

/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_kernel.h(276): warning: variable "tQrQ" was declared but never referenced
          detected during:
            instantiation of "void flash::compute_attn_1rowblock<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params,Is_Q_split_K>(const Params &, int, int, int) [with Kernel_traits=Flash_fwd_kernel_traits<128, 128, 32, 4, false, false, cutlass::bfloat16_t, Flash_kernel_traits<128, 128, 32, 4, cutlass::bfloat16_t>>, Is_dropout=true, Is_causal=false, Is_even_N=true, Is_even_K=true, Return_softmax=false, Params=Flash_fwd_params, Is_Q_split_K=false]" 
(589): here
            instantiation of "void flash::compute_attn<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &) [with Kernel_traits=Flash_fwd_kernel_traits<128, 128, 32, 4, false, false, cutlass::bfloat16_t, Flash_kernel_traits<128, 128, 32, 4, cutlass::bfloat16_t>>, Is_dropout=true, Is_causal=false, Is_even_N=true, Is_even_K=true, Return_softmax=false, Params=Flash_fwd_params]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(15): here
            instantiation of "void flash_fwd_kernel<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax>(Flash_fwd_params) [with Kernel_traits=Flash_fwd_kernel_traits<128, 128, 32, 4, false, false, cutlass::bfloat16_t, Flash_kernel_traits<128, 128, 32, 4, cutlass::bfloat16_t>>, Is_dropout=true, Is_causal=false, Is_even_N=true, Is_even_K=true, Return_softmax=false]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(34): here
            instantiation of "void run_flash_fwd<Kernel_traits,Is_dropout,Is_causal>(Flash_fwd_params &, cudaStream_t) [with Kernel_traits=Flash_fwd_kernel_traits<128, 128, 32, 4, false, false, cutlass::bfloat16_t, Flash_kernel_traits<128, 128, 32, 4, cutlass::bfloat16_t>>, Is_dropout=true, Is_causal=false]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(118): here
            instantiation of "void run_mha_fwd_hdim128<T>(Flash_fwd_params &, cudaStream_t) [with T=cutlass::bfloat16_t]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_hdim128_bf16_sm80.cu(18): here

/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_kernel.h(140): warning: variable "MMA_M" was declared but never referenced
          detected during:
            instantiation of "void flash::compute_attn_1rowblock<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params,Is_Q_split_K>(const Params &, int, int, int) [with Kernel_traits=Flash_fwd_kernel_traits<128, 128, 32, 4, false, false, cutlass::bfloat16_t, Flash_kernel_traits<128, 128, 32, 4, cutlass::bfloat16_t>>, Is_dropout=true, Is_causal=false, Is_even_N=true, Is_even_K=false, Return_softmax=true, Params=Flash_fwd_params, Is_Q_split_K=false]" 
(589): here
            instantiation of "void flash::compute_attn<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &) [with Kernel_traits=Flash_fwd_kernel_traits<128, 128, 32, 4, false, false, cutlass::bfloat16_t, Flash_kernel_traits<128, 128, 32, 4, cutlass::bfloat16_t>>, Is_dropout=true, Is_causal=false, Is_even_N=true, Is_even_K=false, Return_softmax=true, Params=Flash_fwd_params]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(15): here
            instantiation of "void flash_fwd_kernel<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax>(Flash_fwd_params) [with Kernel_traits=Flash_fwd_kernel_traits<128, 128, 32, 4, false, false, cutlass::bfloat16_t, Flash_kernel_traits<128, 128, 32, 4, cutlass::bfloat16_t>>, Is_dropout=true, Is_causal=false, Is_even_N=true, Is_even_K=false, Return_softmax=true]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(34): here
            instantiation of "void run_flash_fwd<Kernel_traits,Is_dropout,Is_causal>(Flash_fwd_params &, cudaStream_t) [with Kernel_traits=Flash_fwd_kernel_traits<128, 128, 32, 4, false, false, cutlass::bfloat16_t, Flash_kernel_traits<128, 128, 32, 4, cutlass::bfloat16_t>>, Is_dropout=true, Is_causal=false]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(118): here
            instantiation of "void run_mha_fwd_hdim128<T>(Flash_fwd_params &, cudaStream_t) [with T=cutlass::bfloat16_t]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_hdim128_bf16_sm80.cu(18): here

/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_kernel.h(276): warning: variable "tQrQ" was declared but never referenced
          detected during:
            instantiation of "void flash::compute_attn_1rowblock<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params,Is_Q_split_K>(const Params &, int, int, int) [with Kernel_traits=Flash_fwd_kernel_traits<128, 128, 32, 4, false, false, cutlass::bfloat16_t, Flash_kernel_traits<128, 128, 32, 4, cutlass::bfloat16_t>>, Is_dropout=true, Is_causal=false, Is_even_N=true, Is_even_K=false, Return_softmax=true, Params=Flash_fwd_params, Is_Q_split_K=false]" 
(589): here
            instantiation of "void flash::compute_attn<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &) [with Kernel_traits=Flash_fwd_kernel_traits<128, 128, 32, 4, false, false, cutlass::bfloat16_t, Flash_kernel_traits<128, 128, 32, 4, cutlass::bfloat16_t>>, Is_dropout=true, Is_causal=false, Is_even_N=true, Is_even_K=false, Return_softmax=true, Params=Flash_fwd_params]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(15): here
            instantiation of "void flash_fwd_kernel<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax>(Flash_fwd_params) [with Kernel_traits=Flash_fwd_kernel_traits<128, 128, 32, 4, false, false, cutlass::bfloat16_t, Flash_kernel_traits<128, 128, 32, 4, cutlass::bfloat16_t>>, Is_dropout=true, Is_causal=false, Is_even_N=true, Is_even_K=false, Return_softmax=true]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(34): here
            instantiation of "void run_flash_fwd<Kernel_traits,Is_dropout,Is_causal>(Flash_fwd_params &, cudaStream_t) [with Kernel_traits=Flash_fwd_kernel_traits<128, 128, 32, 4, false, false, cutlass::bfloat16_t, Flash_kernel_traits<128, 128, 32, 4, cutlass::bfloat16_t>>, Is_dropout=true, Is_causal=false]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(118): here
            instantiation of "void run_mha_fwd_hdim128<T>(Flash_fwd_params &, cudaStream_t) [with T=cutlass::bfloat16_t]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_hdim128_bf16_sm80.cu(18): here

/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_kernel.h(140): warning: variable "MMA_M" was declared but never referenced
          detected during:
            instantiation of "void flash::compute_attn_1rowblock<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params,Is_Q_split_K>(const Params &, int, int, int) [with Kernel_traits=Flash_fwd_kernel_traits<128, 128, 32, 4, false, false, cutlass::bfloat16_t, Flash_kernel_traits<128, 128, 32, 4, cutlass::bfloat16_t>>, Is_dropout=true, Is_causal=false, Is_even_N=true, Is_even_K=false, Return_softmax=false, Params=Flash_fwd_params, Is_Q_split_K=false]" 
(589): here
            instantiation of "void flash::compute_attn<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &) [with Kernel_traits=Flash_fwd_kernel_traits<128, 128, 32, 4, false, false, cutlass::bfloat16_t, Flash_kernel_traits<128, 128, 32, 4, cutlass::bfloat16_t>>, Is_dropout=true, Is_causal=false, Is_even_N=true, Is_even_K=false, Return_softmax=false, Params=Flash_fwd_params]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(15): here
            instantiation of "void flash_fwd_kernel<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax>(Flash_fwd_params) [with Kernel_traits=Flash_fwd_kernel_traits<128, 128, 32, 4, false, false, cutlass::bfloat16_t, Flash_kernel_traits<128, 128, 32, 4, cutlass::bfloat16_t>>, Is_dropout=true, Is_causal=false, Is_even_N=true, Is_even_K=false, Return_softmax=false]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(34): here
            instantiation of "void run_flash_fwd<Kernel_traits,Is_dropout,Is_causal>(Flash_fwd_params &, cudaStream_t) [with Kernel_traits=Flash_fwd_kernel_traits<128, 128, 32, 4, false, false, cutlass::bfloat16_t, Flash_kernel_traits<128, 128, 32, 4, cutlass::bfloat16_t>>, Is_dropout=true, Is_causal=false]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(118): here
            instantiation of "void run_mha_fwd_hdim128<T>(Flash_fwd_params &, cudaStream_t) [with T=cutlass::bfloat16_t]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_hdim128_bf16_sm80.cu(18): here

/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_kernel.h(276): warning: variable "tQrQ" was declared but never referenced
          detected during:
            instantiation of "void flash::compute_attn_1rowblock<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params,Is_Q_split_K>(const Params &, int, int, int) [with Kernel_traits=Flash_fwd_kernel_traits<128, 128, 32, 4, false, false, cutlass::bfloat16_t, Flash_kernel_traits<128, 128, 32, 4, cutlass::bfloat16_t>>, Is_dropout=true, Is_causal=false, Is_even_N=true, Is_even_K=false, Return_softmax=false, Params=Flash_fwd_params, Is_Q_split_K=false]" 
(589): here
            instantiation of "void flash::compute_attn<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &) [with Kernel_traits=Flash_fwd_kernel_traits<128, 128, 32, 4, false, false, cutlass::bfloat16_t, Flash_kernel_traits<128, 128, 32, 4, cutlass::bfloat16_t>>, Is_dropout=true, Is_causal=false, Is_even_N=true, Is_even_K=false, Return_softmax=false, Params=Flash_fwd_params]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(15): here
            instantiation of "void flash_fwd_kernel<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax>(Flash_fwd_params) [with Kernel_traits=Flash_fwd_kernel_traits<128, 128, 32, 4, false, false, cutlass::bfloat16_t, Flash_kernel_traits<128, 128, 32, 4, cutlass::bfloat16_t>>, Is_dropout=true, Is_causal=false, Is_even_N=true, Is_even_K=false, Return_softmax=false]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(34): here
            instantiation of "void run_flash_fwd<Kernel_traits,Is_dropout,Is_causal>(Flash_fwd_params &, cudaStream_t) [with Kernel_traits=Flash_fwd_kernel_traits<128, 128, 32, 4, false, false, cutlass::bfloat16_t, Flash_kernel_traits<128, 128, 32, 4, cutlass::bfloat16_t>>, Is_dropout=true, Is_causal=false]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(118): here
            instantiation of "void run_mha_fwd_hdim128<T>(Flash_fwd_params &, cudaStream_t) [with T=cutlass::bfloat16_t]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_hdim128_bf16_sm80.cu(18): here

/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_kernel.h(140): warning: variable "MMA_M" was declared but never referenced
          detected during:
            instantiation of "void flash::compute_attn_1rowblock<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params,Is_Q_split_K>(const Params &, int, int, int) [with Kernel_traits=Flash_fwd_kernel_traits<128, 128, 32, 4, false, false, cutlass::bfloat16_t, Flash_kernel_traits<128, 128, 32, 4, cutlass::bfloat16_t>>, Is_dropout=true, Is_causal=false, Is_even_N=false, Is_even_K=true, Return_softmax=true, Params=Flash_fwd_params, Is_Q_split_K=false]" 
(589): here
            instantiation of "void flash::compute_attn<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &) [with Kernel_traits=Flash_fwd_kernel_traits<128, 128, 32, 4, false, false, cutlass::bfloat16_t, Flash_kernel_traits<128, 128, 32, 4, cutlass::bfloat16_t>>, Is_dropout=true, Is_causal=false, Is_even_N=false, Is_even_K=true, Return_softmax=true, Params=Flash_fwd_params]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(15): here
            instantiation of "void flash_fwd_kernel<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax>(Flash_fwd_params) [with Kernel_traits=Flash_fwd_kernel_traits<128, 128, 32, 4, false, false, cutlass::bfloat16_t, Flash_kernel_traits<128, 128, 32, 4, cutlass::bfloat16_t>>, Is_dropout=true, Is_causal=false, Is_even_N=false, Is_even_K=true, Return_softmax=true]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(34): here
            instantiation of "void run_flash_fwd<Kernel_traits,Is_dropout,Is_causal>(Flash_fwd_params &, cudaStream_t) [with Kernel_traits=Flash_fwd_kernel_traits<128, 128, 32, 4, false, false, cutlass::bfloat16_t, Flash_kernel_traits<128, 128, 32, 4, cutlass::bfloat16_t>>, Is_dropout=true, Is_causal=false]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(118): here
            instantiation of "void run_mha_fwd_hdim128<T>(Flash_fwd_params &, cudaStream_t) [with T=cutlass::bfloat16_t]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_hdim128_bf16_sm80.cu(18): here

/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_kernel.h(276): warning: variable "tQrQ" was declared but never referenced
          detected during:
            instantiation of "void flash::compute_attn_1rowblock<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params,Is_Q_split_K>(const Params &, int, int, int) [with Kernel_traits=Flash_fwd_kernel_traits<128, 128, 32, 4, false, false, cutlass::bfloat16_t, Flash_kernel_traits<128, 128, 32, 4, cutlass::bfloat16_t>>, Is_dropout=true, Is_causal=false, Is_even_N=false, Is_even_K=true, Return_softmax=true, Params=Flash_fwd_params, Is_Q_split_K=false]" 
(589): here
            instantiation of "void flash::compute_attn<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &) [with Kernel_traits=Flash_fwd_kernel_traits<128, 128, 32, 4, false, false, cutlass::bfloat16_t, Flash_kernel_traits<128, 128, 32, 4, cutlass::bfloat16_t>>, Is_dropout=true, Is_causal=false, Is_even_N=false, Is_even_K=true, Return_softmax=true, Params=Flash_fwd_params]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(15): here
            instantiation of "void flash_fwd_kernel<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax>(Flash_fwd_params) [with Kernel_traits=Flash_fwd_kernel_traits<128, 128, 32, 4, false, false, cutlass::bfloat16_t, Flash_kernel_traits<128, 128, 32, 4, cutlass::bfloat16_t>>, Is_dropout=true, Is_causal=false, Is_even_N=false, Is_even_K=true, Return_softmax=true]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(34): here
            instantiation of "void run_flash_fwd<Kernel_traits,Is_dropout,Is_causal>(Flash_fwd_params &, cudaStream_t) [with Kernel_traits=Flash_fwd_kernel_traits<128, 128, 32, 4, false, false, cutlass::bfloat16_t, Flash_kernel_traits<128, 128, 32, 4, cutlass::bfloat16_t>>, Is_dropout=true, Is_causal=false]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(118): here
            instantiation of "void run_mha_fwd_hdim128<T>(Flash_fwd_params &, cudaStream_t) [with T=cutlass::bfloat16_t]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_hdim128_bf16_sm80.cu(18): here

/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_kernel.h(140): warning: variable "MMA_M" was declared but never referenced
          detected during:
            instantiation of "void flash::compute_attn_1rowblock<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params,Is_Q_split_K>(const Params &, int, int, int) [with Kernel_traits=Flash_fwd_kernel_traits<128, 128, 32, 4, false, false, cutlass::bfloat16_t, Flash_kernel_traits<128, 128, 32, 4, cutlass::bfloat16_t>>, Is_dropout=true, Is_causal=false, Is_even_N=false, Is_even_K=true, Return_softmax=false, Params=Flash_fwd_params, Is_Q_split_K=false]" 
(589): here
            instantiation of "void flash::compute_attn<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &) [with Kernel_traits=Flash_fwd_kernel_traits<128, 128, 32, 4, false, false, cutlass::bfloat16_t, Flash_kernel_traits<128, 128, 32, 4, cutlass::bfloat16_t>>, Is_dropout=true, Is_causal=false, Is_even_N=false, Is_even_K=true, Return_softmax=false, Params=Flash_fwd_params]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(15): here
            instantiation of "void flash_fwd_kernel<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax>(Flash_fwd_params) [with Kernel_traits=Flash_fwd_kernel_traits<128, 128, 32, 4, false, false, cutlass::bfloat16_t, Flash_kernel_traits<128, 128, 32, 4, cutlass::bfloat16_t>>, Is_dropout=true, Is_causal=false, Is_even_N=false, Is_even_K=true, Return_softmax=false]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(34): here
            instantiation of "void run_flash_fwd<Kernel_traits,Is_dropout,Is_causal>(Flash_fwd_params &, cudaStream_t) [with Kernel_traits=Flash_fwd_kernel_traits<128, 128, 32, 4, false, false, cutlass::bfloat16_t, Flash_kernel_traits<128, 128, 32, 4, cutlass::bfloat16_t>>, Is_dropout=true, Is_causal=false]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(118): here
            instantiation of "void run_mha_fwd_hdim128<T>(Flash_fwd_params &, cudaStream_t) [with T=cutlass::bfloat16_t]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_hdim128_bf16_sm80.cu(18): here

/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_kernel.h(276): warning: variable "tQrQ" was declared but never referenced
          detected during:
            instantiation of "void flash::compute_attn_1rowblock<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params,Is_Q_split_K>(const Params &, int, int, int) [with Kernel_traits=Flash_fwd_kernel_traits<128, 128, 32, 4, false, false, cutlass::bfloat16_t, Flash_kernel_traits<128, 128, 32, 4, cutlass::bfloat16_t>>, Is_dropout=true, Is_causal=false, Is_even_N=false, Is_even_K=true, Return_softmax=false, Params=Flash_fwd_params, Is_Q_split_K=false]" 
(589): here
            instantiation of "void flash::compute_attn<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &) [with Kernel_traits=Flash_fwd_kernel_traits<128, 128, 32, 4, false, false, cutlass::bfloat16_t, Flash_kernel_traits<128, 128, 32, 4, cutlass::bfloat16_t>>, Is_dropout=true, Is_causal=false, Is_even_N=false, Is_even_K=true, Return_softmax=false, Params=Flash_fwd_params]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(15): here
            instantiation of "void flash_fwd_kernel<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax>(Flash_fwd_params) [with Kernel_traits=Flash_fwd_kernel_traits<128, 128, 32, 4, false, false, cutlass::bfloat16_t, Flash_kernel_traits<128, 128, 32, 4, cutlass::bfloat16_t>>, Is_dropout=true, Is_causal=false, Is_even_N=false, Is_even_K=true, Return_softmax=false]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(34): here
            instantiation of "void run_flash_fwd<Kernel_traits,Is_dropout,Is_causal>(Flash_fwd_params &, cudaStream_t) [with Kernel_traits=Flash_fwd_kernel_traits<128, 128, 32, 4, false, false, cutlass::bfloat16_t, Flash_kernel_traits<128, 128, 32, 4, cutlass::bfloat16_t>>, Is_dropout=true, Is_causal=false]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(118): here
            instantiation of "void run_mha_fwd_hdim128<T>(Flash_fwd_params &, cudaStream_t) [with T=cutlass::bfloat16_t]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_hdim128_bf16_sm80.cu(18): here

/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_kernel.h(140): warning: variable "MMA_M" was declared but never referenced
          detected during:
            instantiation of "void flash::compute_attn_1rowblock<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params,Is_Q_split_K>(const Params &, int, int, int) [with Kernel_traits=Flash_fwd_kernel_traits<128, 128, 32, 4, false, false, cutlass::bfloat16_t, Flash_kernel_traits<128, 128, 32, 4, cutlass::bfloat16_t>>, Is_dropout=true, Is_causal=false, Is_even_N=false, Is_even_K=false, Return_softmax=true, Params=Flash_fwd_params, Is_Q_split_K=false]" 
(589): here
            instantiation of "void flash::compute_attn<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &) [with Kernel_traits=Flash_fwd_kernel_traits<128, 128, 32, 4, false, false, cutlass::bfloat16_t, Flash_kernel_traits<128, 128, 32, 4, cutlass::bfloat16_t>>, Is_dropout=true, Is_causal=false, Is_even_N=false, Is_even_K=false, Return_softmax=true, Params=Flash_fwd_params]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(15): here
            instantiation of "void flash_fwd_kernel<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax>(Flash_fwd_params) [with Kernel_traits=Flash_fwd_kernel_traits<128, 128, 32, 4, false, false, cutlass::bfloat16_t, Flash_kernel_traits<128, 128, 32, 4, cutlass::bfloat16_t>>, Is_dropout=true, Is_causal=false, Is_even_N=false, Is_even_K=false, Return_softmax=true]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(34): here
            instantiation of "void run_flash_fwd<Kernel_traits,Is_dropout,Is_causal>(Flash_fwd_params &, cudaStream_t) [with Kernel_traits=Flash_fwd_kernel_traits<128, 128, 32, 4, false, false, cutlass::bfloat16_t, Flash_kernel_traits<128, 128, 32, 4, cutlass::bfloat16_t>>, Is_dropout=true, Is_causal=false]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(118): here
            instantiation of "void run_mha_fwd_hdim128<T>(Flash_fwd_params &, cudaStream_t) [with T=cutlass::bfloat16_t]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_hdim128_bf16_sm80.cu(18): here

/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_kernel.h(276): warning: variable "tQrQ" was declared but never referenced
          detected during:
            instantiation of "void flash::compute_attn_1rowblock<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params,Is_Q_split_K>(const Params &, int, int, int) [with Kernel_traits=Flash_fwd_kernel_traits<128, 128, 32, 4, false, false, cutlass::bfloat16_t, Flash_kernel_traits<128, 128, 32, 4, cutlass::bfloat16_t>>, Is_dropout=true, Is_causal=false, Is_even_N=false, Is_even_K=false, Return_softmax=true, Params=Flash_fwd_params, Is_Q_split_K=false]" 
(589): here
            instantiation of "void flash::compute_attn<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &) [with Kernel_traits=Flash_fwd_kernel_traits<128, 128, 32, 4, false, false, cutlass::bfloat16_t, Flash_kernel_traits<128, 128, 32, 4, cutlass::bfloat16_t>>, Is_dropout=true, Is_causal=false, Is_even_N=false, Is_even_K=false, Return_softmax=true, Params=Flash_fwd_params]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(15): here
            instantiation of "void flash_fwd_kernel<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax>(Flash_fwd_params) [with Kernel_traits=Flash_fwd_kernel_traits<128, 128, 32, 4, false, false, cutlass::bfloat16_t, Flash_kernel_traits<128, 128, 32, 4, cutlass::bfloat16_t>>, Is_dropout=true, Is_causal=false, Is_even_N=false, Is_even_K=false, Return_softmax=true]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(34): here
            instantiation of "void run_flash_fwd<Kernel_traits,Is_dropout,Is_causal>(Flash_fwd_params &, cudaStream_t) [with Kernel_traits=Flash_fwd_kernel_traits<128, 128, 32, 4, false, false, cutlass::bfloat16_t, Flash_kernel_traits<128, 128, 32, 4, cutlass::bfloat16_t>>, Is_dropout=true, Is_causal=false]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(118): here
            instantiation of "void run_mha_fwd_hdim128<T>(Flash_fwd_params &, cudaStream_t) [with T=cutlass::bfloat16_t]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_hdim128_bf16_sm80.cu(18): here

/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_kernel.h(140): warning: variable "MMA_M" was declared but never referenced
          detected during:
            instantiation of "void flash::compute_attn_1rowblock<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params,Is_Q_split_K>(const Params &, int, int, int) [with Kernel_traits=Flash_fwd_kernel_traits<128, 128, 32, 4, false, false, cutlass::bfloat16_t, Flash_kernel_traits<128, 128, 32, 4, cutlass::bfloat16_t>>, Is_dropout=true, Is_causal=false, Is_even_N=false, Is_even_K=false, Return_softmax=false, Params=Flash_fwd_params, Is_Q_split_K=false]" 
(589): here
            instantiation of "void flash::compute_attn<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &) [with Kernel_traits=Flash_fwd_kernel_traits<128, 128, 32, 4, false, false, cutlass::bfloat16_t, Flash_kernel_traits<128, 128, 32, 4, cutlass::bfloat16_t>>, Is_dropout=true, Is_causal=false, Is_even_N=false, Is_even_K=false, Return_softmax=false, Params=Flash_fwd_params]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(15): here
            instantiation of "void flash_fwd_kernel<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax>(Flash_fwd_params) [with Kernel_traits=Flash_fwd_kernel_traits<128, 128, 32, 4, false, false, cutlass::bfloat16_t, Flash_kernel_traits<128, 128, 32, 4, cutlass::bfloat16_t>>, Is_dropout=true, Is_causal=false, Is_even_N=false, Is_even_K=false, Return_softmax=false]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(34): here
            instantiation of "void run_flash_fwd<Kernel_traits,Is_dropout,Is_causal>(Flash_fwd_params &, cudaStream_t) [with Kernel_traits=Flash_fwd_kernel_traits<128, 128, 32, 4, false, false, cutlass::bfloat16_t, Flash_kernel_traits<128, 128, 32, 4, cutlass::bfloat16_t>>, Is_dropout=true, Is_causal=false]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(118): here
            instantiation of "void run_mha_fwd_hdim128<T>(Flash_fwd_params &, cudaStream_t) [with T=cutlass::bfloat16_t]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_hdim128_bf16_sm80.cu(18): here

/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_kernel.h(276): warning: variable "tQrQ" was declared but never referenced
          detected during:
            instantiation of "void flash::compute_attn_1rowblock<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params,Is_Q_split_K>(const Params &, int, int, int) [with Kernel_traits=Flash_fwd_kernel_traits<128, 128, 32, 4, false, false, cutlass::bfloat16_t, Flash_kernel_traits<128, 128, 32, 4, cutlass::bfloat16_t>>, Is_dropout=true, Is_causal=false, Is_even_N=false, Is_even_K=false, Return_softmax=false, Params=Flash_fwd_params, Is_Q_split_K=false]" 
(589): here
            instantiation of "void flash::compute_attn<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &) [with Kernel_traits=Flash_fwd_kernel_traits<128, 128, 32, 4, false, false, cutlass::bfloat16_t, Flash_kernel_traits<128, 128, 32, 4, cutlass::bfloat16_t>>, Is_dropout=true, Is_causal=false, Is_even_N=false, Is_even_K=false, Return_softmax=false, Params=Flash_fwd_params]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(15): here
            instantiation of "void flash_fwd_kernel<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax>(Flash_fwd_params) [with Kernel_traits=Flash_fwd_kernel_traits<128, 128, 32, 4, false, false, cutlass::bfloat16_t, Flash_kernel_traits<128, 128, 32, 4, cutlass::bfloat16_t>>, Is_dropout=true, Is_causal=false, Is_even_N=false, Is_even_K=false, Return_softmax=false]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(34): here
            instantiation of "void run_flash_fwd<Kernel_traits,Is_dropout,Is_causal>(Flash_fwd_params &, cudaStream_t) [with Kernel_traits=Flash_fwd_kernel_traits<128, 128, 32, 4, false, false, cutlass::bfloat16_t, Flash_kernel_traits<128, 128, 32, 4, cutlass::bfloat16_t>>, Is_dropout=true, Is_causal=false]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(118): here
            instantiation of "void run_mha_fwd_hdim128<T>(Flash_fwd_params &, cudaStream_t) [with T=cutlass::bfloat16_t]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_hdim128_bf16_sm80.cu(18): here

/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_kernel.h(140): warning: variable "MMA_M" was declared but never referenced
          detected during:
            instantiation of "void flash::compute_attn_1rowblock<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params,Is_Q_split_K>(const Params &, int, int, int) [with Kernel_traits=Flash_fwd_kernel_traits<128, 64, 64, 4, false, false, cutlass::bfloat16_t, Flash_kernel_traits<128, 64, 64, 4, cutlass::bfloat16_t>>, Is_dropout=false, Is_causal=true, Is_even_N=true, Is_even_K=true, Return_softmax=false, Params=Flash_fwd_params, Is_Q_split_K=false]" 
(589): here
            instantiation of "void flash::compute_attn<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &) [with Kernel_traits=Flash_fwd_kernel_traits<128, 64, 64, 4, false, false, cutlass::bfloat16_t, Flash_kernel_traits<128, 64, 64, 4, cutlass::bfloat16_t>>, Is_dropout=false, Is_causal=true, Is_even_N=true, Is_even_K=true, Return_softmax=false, Params=Flash_fwd_params]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(15): here
            instantiation of "void flash_fwd_kernel<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax>(Flash_fwd_params) [with Kernel_traits=Flash_fwd_kernel_traits<128, 64, 64, 4, false, false, cutlass::bfloat16_t, Flash_kernel_traits<128, 64, 64, 4, cutlass::bfloat16_t>>, Is_dropout=false, Is_causal=true, Is_even_N=true, Is_even_K=true, Return_softmax=false]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(34): here
            instantiation of "void run_flash_fwd<Kernel_traits,Is_dropout,Is_causal>(Flash_fwd_params &, cudaStream_t) [with Kernel_traits=Flash_fwd_kernel_traits<128, 64, 64, 4, false, false, cutlass::bfloat16_t, Flash_kernel_traits<128, 64, 64, 4, cutlass::bfloat16_t>>, Is_dropout=false, Is_causal=true]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(118): here
            instantiation of "void run_mha_fwd_hdim128<T>(Flash_fwd_params &, cudaStream_t) [with T=cutlass::bfloat16_t]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_hdim128_bf16_sm80.cu(18): here

/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_kernel.h(276): warning: variable "tQrQ" was declared but never referenced
          detected during:
            instantiation of "void flash::compute_attn_1rowblock<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params,Is_Q_split_K>(const Params &, int, int, int) [with Kernel_traits=Flash_fwd_kernel_traits<128, 64, 64, 4, false, false, cutlass::bfloat16_t, Flash_kernel_traits<128, 64, 64, 4, cutlass::bfloat16_t>>, Is_dropout=false, Is_causal=true, Is_even_N=true, Is_even_K=true, Return_softmax=false, Params=Flash_fwd_params, Is_Q_split_K=false]" 
(589): here
            instantiation of "void flash::compute_attn<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &) [with Kernel_traits=Flash_fwd_kernel_traits<128, 64, 64, 4, false, false, cutlass::bfloat16_t, Flash_kernel_traits<128, 64, 64, 4, cutlass::bfloat16_t>>, Is_dropout=false, Is_causal=true, Is_even_N=true, Is_even_K=true, Return_softmax=false, Params=Flash_fwd_params]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(15): here
            instantiation of "void flash_fwd_kernel<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax>(Flash_fwd_params) [with Kernel_traits=Flash_fwd_kernel_traits<128, 64, 64, 4, false, false, cutlass::bfloat16_t, Flash_kernel_traits<128, 64, 64, 4, cutlass::bfloat16_t>>, Is_dropout=false, Is_causal=true, Is_even_N=true, Is_even_K=true, Return_softmax=false]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(34): here
            instantiation of "void run_flash_fwd<Kernel_traits,Is_dropout,Is_causal>(Flash_fwd_params &, cudaStream_t) [with Kernel_traits=Flash_fwd_kernel_traits<128, 64, 64, 4, false, false, cutlass::bfloat16_t, Flash_kernel_traits<128, 64, 64, 4, cutlass::bfloat16_t>>, Is_dropout=false, Is_causal=true]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(118): here
            instantiation of "void run_mha_fwd_hdim128<T>(Flash_fwd_params &, cudaStream_t) [with T=cutlass::bfloat16_t]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_hdim128_bf16_sm80.cu(18): here

/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_kernel.h(140): warning: variable "MMA_M" was declared but never referenced
          detected during:
            instantiation of "void flash::compute_attn_1rowblock<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params,Is_Q_split_K>(const Params &, int, int, int) [with Kernel_traits=Flash_fwd_kernel_traits<128, 64, 64, 4, false, false, cutlass::bfloat16_t, Flash_kernel_traits<128, 64, 64, 4, cutlass::bfloat16_t>>, Is_dropout=false, Is_causal=true, Is_even_N=true, Is_even_K=false, Return_softmax=false, Params=Flash_fwd_params, Is_Q_split_K=false]" 
(589): here
            instantiation of "void flash::compute_attn<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &) [with Kernel_traits=Flash_fwd_kernel_traits<128, 64, 64, 4, false, false, cutlass::bfloat16_t, Flash_kernel_traits<128, 64, 64, 4, cutlass::bfloat16_t>>, Is_dropout=false, Is_causal=true, Is_even_N=true, Is_even_K=false, Return_softmax=false, Params=Flash_fwd_params]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(15): here
            instantiation of "void flash_fwd_kernel<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax>(Flash_fwd_params) [with Kernel_traits=Flash_fwd_kernel_traits<128, 64, 64, 4, false, false, cutlass::bfloat16_t, Flash_kernel_traits<128, 64, 64, 4, cutlass::bfloat16_t>>, Is_dropout=false, Is_causal=true, Is_even_N=true, Is_even_K=false, Return_softmax=false]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(34): here
            instantiation of "void run_flash_fwd<Kernel_traits,Is_dropout,Is_causal>(Flash_fwd_params &, cudaStream_t) [with Kernel_traits=Flash_fwd_kernel_traits<128, 64, 64, 4, false, false, cutlass::bfloat16_t, Flash_kernel_traits<128, 64, 64, 4, cutlass::bfloat16_t>>, Is_dropout=false, Is_causal=true]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(118): here
            instantiation of "void run_mha_fwd_hdim128<T>(Flash_fwd_params &, cudaStream_t) [with T=cutlass::bfloat16_t]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_hdim128_bf16_sm80.cu(18): here

/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_kernel.h(276): warning: variable "tQrQ" was declared but never referenced
          detected during:
            instantiation of "void flash::compute_attn_1rowblock<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params,Is_Q_split_K>(const Params &, int, int, int) [with Kernel_traits=Flash_fwd_kernel_traits<128, 64, 64, 4, false, false, cutlass::bfloat16_t, Flash_kernel_traits<128, 64, 64, 4, cutlass::bfloat16_t>>, Is_dropout=false, Is_causal=true, Is_even_N=true, Is_even_K=false, Return_softmax=false, Params=Flash_fwd_params, Is_Q_split_K=false]" 
(589): here
            instantiation of "void flash::compute_attn<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &) [with Kernel_traits=Flash_fwd_kernel_traits<128, 64, 64, 4, false, false, cutlass::bfloat16_t, Flash_kernel_traits<128, 64, 64, 4, cutlass::bfloat16_t>>, Is_dropout=false, Is_causal=true, Is_even_N=true, Is_even_K=false, Return_softmax=false, Params=Flash_fwd_params]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(15): here
            instantiation of "void flash_fwd_kernel<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax>(Flash_fwd_params) [with Kernel_traits=Flash_fwd_kernel_traits<128, 64, 64, 4, false, false, cutlass::bfloat16_t, Flash_kernel_traits<128, 64, 64, 4, cutlass::bfloat16_t>>, Is_dropout=false, Is_causal=true, Is_even_N=true, Is_even_K=false, Return_softmax=false]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(34): here
            instantiation of "void run_flash_fwd<Kernel_traits,Is_dropout,Is_causal>(Flash_fwd_params &, cudaStream_t) [with Kernel_traits=Flash_fwd_kernel_traits<128, 64, 64, 4, false, false, cutlass::bfloat16_t, Flash_kernel_traits<128, 64, 64, 4, cutlass::bfloat16_t>>, Is_dropout=false, Is_causal=true]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(118): here
            instantiation of "void run_mha_fwd_hdim128<T>(Flash_fwd_params &, cudaStream_t) [with T=cutlass::bfloat16_t]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_hdim128_bf16_sm80.cu(18): here

/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_kernel.h(140): warning: variable "MMA_M" was declared but never referenced
          detected during:
            instantiation of "void flash::compute_attn_1rowblock<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params,Is_Q_split_K>(const Params &, int, int, int) [with Kernel_traits=Flash_fwd_kernel_traits<128, 64, 64, 4, false, false, cutlass::bfloat16_t, Flash_kernel_traits<128, 64, 64, 4, cutlass::bfloat16_t>>, Is_dropout=false, Is_causal=true, Is_even_N=false, Is_even_K=true, Return_softmax=false, Params=Flash_fwd_params, Is_Q_split_K=false]" 
(589): here
            instantiation of "void flash::compute_attn<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &) [with Kernel_traits=Flash_fwd_kernel_traits<128, 64, 64, 4, false, false, cutlass::bfloat16_t, Flash_kernel_traits<128, 64, 64, 4, cutlass::bfloat16_t>>, Is_dropout=false, Is_causal=true, Is_even_N=false, Is_even_K=true, Return_softmax=false, Params=Flash_fwd_params]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(15): here
            instantiation of "void flash_fwd_kernel<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax>(Flash_fwd_params) [with Kernel_traits=Flash_fwd_kernel_traits<128, 64, 64, 4, false, false, cutlass::bfloat16_t, Flash_kernel_traits<128, 64, 64, 4, cutlass::bfloat16_t>>, Is_dropout=false, Is_causal=true, Is_even_N=false, Is_even_K=true, Return_softmax=false]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(34): here
            instantiation of "void run_flash_fwd<Kernel_traits,Is_dropout,Is_causal>(Flash_fwd_params &, cudaStream_t) [with Kernel_traits=Flash_fwd_kernel_traits<128, 64, 64, 4, false, false, cutlass::bfloat16_t, Flash_kernel_traits<128, 64, 64, 4, cutlass::bfloat16_t>>, Is_dropout=false, Is_causal=true]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(118): here
            instantiation of "void run_mha_fwd_hdim128<T>(Flash_fwd_params &, cudaStream_t) [with T=cutlass::bfloat16_t]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_hdim128_bf16_sm80.cu(18): here

/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_kernel.h(276): warning: variable "tQrQ" was declared but never referenced
          detected during:
            instantiation of "void flash::compute_attn_1rowblock<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params,Is_Q_split_K>(const Params &, int, int, int) [with Kernel_traits=Flash_fwd_kernel_traits<128, 64, 64, 4, false, false, cutlass::bfloat16_t, Flash_kernel_traits<128, 64, 64, 4, cutlass::bfloat16_t>>, Is_dropout=false, Is_causal=true, Is_even_N=false, Is_even_K=true, Return_softmax=false, Params=Flash_fwd_params, Is_Q_split_K=false]" 
(589): here
            instantiation of "void flash::compute_attn<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &) [with Kernel_traits=Flash_fwd_kernel_traits<128, 64, 64, 4, false, false, cutlass::bfloat16_t, Flash_kernel_traits<128, 64, 64, 4, cutlass::bfloat16_t>>, Is_dropout=false, Is_causal=true, Is_even_N=false, Is_even_K=true, Return_softmax=false, Params=Flash_fwd_params]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(15): here
            instantiation of "void flash_fwd_kernel<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax>(Flash_fwd_params) [with Kernel_traits=Flash_fwd_kernel_traits<128, 64, 64, 4, false, false, cutlass::bfloat16_t, Flash_kernel_traits<128, 64, 64, 4, cutlass::bfloat16_t>>, Is_dropout=false, Is_causal=true, Is_even_N=false, Is_even_K=true, Return_softmax=false]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(34): here
            instantiation of "void run_flash_fwd<Kernel_traits,Is_dropout,Is_causal>(Flash_fwd_params &, cudaStream_t) [with Kernel_traits=Flash_fwd_kernel_traits<128, 64, 64, 4, false, false, cutlass::bfloat16_t, Flash_kernel_traits<128, 64, 64, 4, cutlass::bfloat16_t>>, Is_dropout=false, Is_causal=true]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(118): here
            instantiation of "void run_mha_fwd_hdim128<T>(Flash_fwd_params &, cudaStream_t) [with T=cutlass::bfloat16_t]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_hdim128_bf16_sm80.cu(18): here

/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_kernel.h(140): warning: variable "MMA_M" was declared but never referenced
          detected during:
            instantiation of "void flash::compute_attn_1rowblock<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params,Is_Q_split_K>(const Params &, int, int, int) [with Kernel_traits=Flash_fwd_kernel_traits<128, 64, 64, 4, false, false, cutlass::bfloat16_t, Flash_kernel_traits<128, 64, 64, 4, cutlass::bfloat16_t>>, Is_dropout=false, Is_causal=true, Is_even_N=false, Is_even_K=false, Return_softmax=false, Params=Flash_fwd_params, Is_Q_split_K=false]" 
(589): here
            instantiation of "void flash::compute_attn<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &) [with Kernel_traits=Flash_fwd_kernel_traits<128, 64, 64, 4, false, false, cutlass::bfloat16_t, Flash_kernel_traits<128, 64, 64, 4, cutlass::bfloat16_t>>, Is_dropout=false, Is_causal=true, Is_even_N=false, Is_even_K=false, Return_softmax=false, Params=Flash_fwd_params]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(15): here
            instantiation of "void flash_fwd_kernel<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax>(Flash_fwd_params) [with Kernel_traits=Flash_fwd_kernel_traits<128, 64, 64, 4, false, false, cutlass::bfloat16_t, Flash_kernel_traits<128, 64, 64, 4, cutlass::bfloat16_t>>, Is_dropout=false, Is_causal=true, Is_even_N=false, Is_even_K=false, Return_softmax=false]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(34): here
            instantiation of "void run_flash_fwd<Kernel_traits,Is_dropout,Is_causal>(Flash_fwd_params &, cudaStream_t) [with Kernel_traits=Flash_fwd_kernel_traits<128, 64, 64, 4, false, false, cutlass::bfloat16_t, Flash_kernel_traits<128, 64, 64, 4, cutlass::bfloat16_t>>, Is_dropout=false, Is_causal=true]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(118): here
            instantiation of "void run_mha_fwd_hdim128<T>(Flash_fwd_params &, cudaStream_t) [with T=cutlass::bfloat16_t]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_hdim128_bf16_sm80.cu(18): here

/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_kernel.h(276): warning: variable "tQrQ" was declared but never referenced
          detected during:
            instantiation of "void flash::compute_attn_1rowblock<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params,Is_Q_split_K>(const Params &, int, int, int) [with Kernel_traits=Flash_fwd_kernel_traits<128, 64, 64, 4, false, false, cutlass::bfloat16_t, Flash_kernel_traits<128, 64, 64, 4, cutlass::bfloat16_t>>, Is_dropout=false, Is_causal=true, Is_even_N=false, Is_even_K=false, Return_softmax=false, Params=Flash_fwd_params, Is_Q_split_K=false]" 
(589): here
            instantiation of "void flash::compute_attn<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &) [with Kernel_traits=Flash_fwd_kernel_traits<128, 64, 64, 4, false, false, cutlass::bfloat16_t, Flash_kernel_traits<128, 64, 64, 4, cutlass::bfloat16_t>>, Is_dropout=false, Is_causal=true, Is_even_N=false, Is_even_K=false, Return_softmax=false, Params=Flash_fwd_params]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(15): here
            instantiation of "void flash_fwd_kernel<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax>(Flash_fwd_params) [with Kernel_traits=Flash_fwd_kernel_traits<128, 64, 64, 4, false, false, cutlass::bfloat16_t, Flash_kernel_traits<128, 64, 64, 4, cutlass::bfloat16_t>>, Is_dropout=false, Is_causal=true, Is_even_N=false, Is_even_K=false, Return_softmax=false]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(34): here
            instantiation of "void run_flash_fwd<Kernel_traits,Is_dropout,Is_causal>(Flash_fwd_params &, cudaStream_t) [with Kernel_traits=Flash_fwd_kernel_traits<128, 64, 64, 4, false, false, cutlass::bfloat16_t, Flash_kernel_traits<128, 64, 64, 4, cutlass::bfloat16_t>>, Is_dropout=false, Is_causal=true]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(118): here
            instantiation of "void run_mha_fwd_hdim128<T>(Flash_fwd_params &, cudaStream_t) [with T=cutlass::bfloat16_t]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_hdim128_bf16_sm80.cu(18): here

/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_kernel.h(140): warning: variable "MMA_M" was declared but never referenced
          detected during:
            instantiation of "void flash::compute_attn_1rowblock<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params,Is_Q_split_K>(const Params &, int, int, int) [with Kernel_traits=Flash_fwd_kernel_traits<128, 128, 64, 4, false, false, cutlass::bfloat16_t, Flash_kernel_traits<128, 128, 64, 4, cutlass::bfloat16_t>>, Is_dropout=false, Is_causal=true, Is_even_N=true, Is_even_K=true, Return_softmax=false, Params=Flash_fwd_params, Is_Q_split_K=false]" 
(589): here
            instantiation of "void flash::compute_attn<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &) [with Kernel_traits=Flash_fwd_kernel_traits<128, 128, 64, 4, false, false, cutlass::bfloat16_t, Flash_kernel_traits<128, 128, 64, 4, cutlass::bfloat16_t>>, Is_dropout=false, Is_causal=true, Is_even_N=true, Is_even_K=true, Return_softmax=false, Params=Flash_fwd_params]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(15): here
            instantiation of "void flash_fwd_kernel<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax>(Flash_fwd_params) [with Kernel_traits=Flash_fwd_kernel_traits<128, 128, 64, 4, false, false, cutlass::bfloat16_t, Flash_kernel_traits<128, 128, 64, 4, cutlass::bfloat16_t>>, Is_dropout=false, Is_causal=true, Is_even_N=true, Is_even_K=true, Return_softmax=false]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(34): here
            instantiation of "void run_flash_fwd<Kernel_traits,Is_dropout,Is_causal>(Flash_fwd_params &, cudaStream_t) [with Kernel_traits=Flash_fwd_kernel_traits<128, 128, 64, 4, false, false, cutlass::bfloat16_t, Flash_kernel_traits<128, 128, 64, 4, cutlass::bfloat16_t>>, Is_dropout=false, Is_causal=true]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(118): here
            instantiation of "void run_mha_fwd_hdim128<T>(Flash_fwd_params &, cudaStream_t) [with T=cutlass::bfloat16_t]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_hdim128_bf16_sm80.cu(18): here

/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_kernel.h(276): warning: variable "tQrQ" was declared but never referenced
          detected during:
            instantiation of "void flash::compute_attn_1rowblock<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params,Is_Q_split_K>(const Params &, int, int, int) [with Kernel_traits=Flash_fwd_kernel_traits<128, 128, 64, 4, false, false, cutlass::bfloat16_t, Flash_kernel_traits<128, 128, 64, 4, cutlass::bfloat16_t>>, Is_dropout=false, Is_causal=true, Is_even_N=true, Is_even_K=true, Return_softmax=false, Params=Flash_fwd_params, Is_Q_split_K=false]" 
(589): here
            instantiation of "void flash::compute_attn<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &) [with Kernel_traits=Flash_fwd_kernel_traits<128, 128, 64, 4, false, false, cutlass::bfloat16_t, Flash_kernel_traits<128, 128, 64, 4, cutlass::bfloat16_t>>, Is_dropout=false, Is_causal=true, Is_even_N=true, Is_even_K=true, Return_softmax=false, Params=Flash_fwd_params]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(15): here
            instantiation of "void flash_fwd_kernel<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax>(Flash_fwd_params) [with Kernel_traits=Flash_fwd_kernel_traits<128, 128, 64, 4, false, false, cutlass::bfloat16_t, Flash_kernel_traits<128, 128, 64, 4, cutlass::bfloat16_t>>, Is_dropout=false, Is_causal=true, Is_even_N=true, Is_even_K=true, Return_softmax=false]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(34): here
            instantiation of "void run_flash_fwd<Kernel_traits,Is_dropout,Is_causal>(Flash_fwd_params &, cudaStream_t) [with Kernel_traits=Flash_fwd_kernel_traits<128, 128, 64, 4, false, false, cutlass::bfloat16_t, Flash_kernel_traits<128, 128, 64, 4, cutlass::bfloat16_t>>, Is_dropout=false, Is_causal=true]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(118): here
            instantiation of "void run_mha_fwd_hdim128<T>(Flash_fwd_params &, cudaStream_t) [with T=cutlass::bfloat16_t]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_hdim128_bf16_sm80.cu(18): here

/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_kernel.h(140): warning: variable "MMA_M" was declared but never referenced
          detected during:
            instantiation of "void flash::compute_attn_1rowblock<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params,Is_Q_split_K>(const Params &, int, int, int) [with Kernel_traits=Flash_fwd_kernel_traits<128, 128, 64, 4, false, false, cutlass::bfloat16_t, Flash_kernel_traits<128, 128, 64, 4, cutlass::bfloat16_t>>, Is_dropout=false, Is_causal=true, Is_even_N=true, Is_even_K=false, Return_softmax=false, Params=Flash_fwd_params, Is_Q_split_K=false]" 
(589): here
            instantiation of "void flash::compute_attn<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &) [with Kernel_traits=Flash_fwd_kernel_traits<128, 128, 64, 4, false, false, cutlass::bfloat16_t, Flash_kernel_traits<128, 128, 64, 4, cutlass::bfloat16_t>>, Is_dropout=false, Is_causal=true, Is_even_N=true, Is_even_K=false, Return_softmax=false, Params=Flash_fwd_params]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(15): here
            instantiation of "void flash_fwd_kernel<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax>(Flash_fwd_params) [with Kernel_traits=Flash_fwd_kernel_traits<128, 128, 64, 4, false, false, cutlass::bfloat16_t, Flash_kernel_traits<128, 128, 64, 4, cutlass::bfloat16_t>>, Is_dropout=false, Is_causal=true, Is_even_N=true, Is_even_K=false, Return_softmax=false]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(34): here
            instantiation of "void run_flash_fwd<Kernel_traits,Is_dropout,Is_causal>(Flash_fwd_params &, cudaStream_t) [with Kernel_traits=Flash_fwd_kernel_traits<128, 128, 64, 4, false, false, cutlass::bfloat16_t, Flash_kernel_traits<128, 128, 64, 4, cutlass::bfloat16_t>>, Is_dropout=false, Is_causal=true]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(118): here
            instantiation of "void run_mha_fwd_hdim128<T>(Flash_fwd_params &, cudaStream_t) [with T=cutlass::bfloat16_t]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_hdim128_bf16_sm80.cu(18): here

/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_kernel.h(276): warning: variable "tQrQ" was declared but never referenced
          detected during:
            instantiation of "void flash::compute_attn_1rowblock<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params,Is_Q_split_K>(const Params &, int, int, int) [with Kernel_traits=Flash_fwd_kernel_traits<128, 128, 64, 4, false, false, cutlass::bfloat16_t, Flash_kernel_traits<128, 128, 64, 4, cutlass::bfloat16_t>>, Is_dropout=false, Is_causal=true, Is_even_N=true, Is_even_K=false, Return_softmax=false, Params=Flash_fwd_params, Is_Q_split_K=false]" 
(589): here
            instantiation of "void flash::compute_attn<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &) [with Kernel_traits=Flash_fwd_kernel_traits<128, 128, 64, 4, false, false, cutlass::bfloat16_t, Flash_kernel_traits<128, 128, 64, 4, cutlass::bfloat16_t>>, Is_dropout=false, Is_causal=true, Is_even_N=true, Is_even_K=false, Return_softmax=false, Params=Flash_fwd_params]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(15): here
            instantiation of "void flash_fwd_kernel<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax>(Flash_fwd_params) [with Kernel_traits=Flash_fwd_kernel_traits<128, 128, 64, 4, false, false, cutlass::bfloat16_t, Flash_kernel_traits<128, 128, 64, 4, cutlass::bfloat16_t>>, Is_dropout=false, Is_causal=true, Is_even_N=true, Is_even_K=false, Return_softmax=false]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(34): here
            instantiation of "void run_flash_fwd<Kernel_traits,Is_dropout,Is_causal>(Flash_fwd_params &, cudaStream_t) [with Kernel_traits=Flash_fwd_kernel_traits<128, 128, 64, 4, false, false, cutlass::bfloat16_t, Flash_kernel_traits<128, 128, 64, 4, cutlass::bfloat16_t>>, Is_dropout=false, Is_causal=true]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(118): here
            instantiation of "void run_mha_fwd_hdim128<T>(Flash_fwd_params &, cudaStream_t) [with T=cutlass::bfloat16_t]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_hdim128_bf16_sm80.cu(18): here

/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_kernel.h(140): warning: variable "MMA_M" was declared but never referenced
          detected during:
            instantiation of "void flash::compute_attn_1rowblock<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params,Is_Q_split_K>(const Params &, int, int, int) [with Kernel_traits=Flash_fwd_kernel_traits<128, 128, 64, 4, false, false, cutlass::bfloat16_t, Flash_kernel_traits<128, 128, 64, 4, cutlass::bfloat16_t>>, Is_dropout=false, Is_causal=true, Is_even_N=false, Is_even_K=true, Return_softmax=false, Params=Flash_fwd_params, Is_Q_split_K=false]" 
(589): here
            instantiation of "void flash::compute_attn<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &) [with Kernel_traits=Flash_fwd_kernel_traits<128, 128, 64, 4, false, false, cutlass::bfloat16_t, Flash_kernel_traits<128, 128, 64, 4, cutlass::bfloat16_t>>, Is_dropout=false, Is_causal=true, Is_even_N=false, Is_even_K=true, Return_softmax=false, Params=Flash_fwd_params]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(15): here
            instantiation of "void flash_fwd_kernel<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax>(Flash_fwd_params) [with Kernel_traits=Flash_fwd_kernel_traits<128, 128, 64, 4, false, false, cutlass::bfloat16_t, Flash_kernel_traits<128, 128, 64, 4, cutlass::bfloat16_t>>, Is_dropout=false, Is_causal=true, Is_even_N=false, Is_even_K=true, Return_softmax=false]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(34): here
            instantiation of "void run_flash_fwd<Kernel_traits,Is_dropout,Is_causal>(Flash_fwd_params &, cudaStream_t) [with Kernel_traits=Flash_fwd_kernel_traits<128, 128, 64, 4, false, false, cutlass::bfloat16_t, Flash_kernel_traits<128, 128, 64, 4, cutlass::bfloat16_t>>, Is_dropout=false, Is_causal=true]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(118): here
            instantiation of "void run_mha_fwd_hdim128<T>(Flash_fwd_params &, cudaStream_t) [with T=cutlass::bfloat16_t]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_hdim128_bf16_sm80.cu(18): here

/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_kernel.h(276): warning: variable "tQrQ" was declared but never referenced
          detected during:
            instantiation of "void flash::compute_attn_1rowblock<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params,Is_Q_split_K>(const Params &, int, int, int) [with Kernel_traits=Flash_fwd_kernel_traits<128, 128, 64, 4, false, false, cutlass::bfloat16_t, Flash_kernel_traits<128, 128, 64, 4, cutlass::bfloat16_t>>, Is_dropout=false, Is_causal=true, Is_even_N=false, Is_even_K=true, Return_softmax=false, Params=Flash_fwd_params, Is_Q_split_K=false]" 
(589): here
            instantiation of "void flash::compute_attn<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &) [with Kernel_traits=Flash_fwd_kernel_traits<128, 128, 64, 4, false, false, cutlass::bfloat16_t, Flash_kernel_traits<128, 128, 64, 4, cutlass::bfloat16_t>>, Is_dropout=false, Is_causal=true, Is_even_N=false, Is_even_K=true, Return_softmax=false, Params=Flash_fwd_params]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(15): here
            instantiation of "void flash_fwd_kernel<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax>(Flash_fwd_params) [with Kernel_traits=Flash_fwd_kernel_traits<128, 128, 64, 4, false, false, cutlass::bfloat16_t, Flash_kernel_traits<128, 128, 64, 4, cutlass::bfloat16_t>>, Is_dropout=false, Is_causal=true, Is_even_N=false, Is_even_K=true, Return_softmax=false]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(34): here
            instantiation of "void run_flash_fwd<Kernel_traits,Is_dropout,Is_causal>(Flash_fwd_params &, cudaStream_t) [with Kernel_traits=Flash_fwd_kernel_traits<128, 128, 64, 4, false, false, cutlass::bfloat16_t, Flash_kernel_traits<128, 128, 64, 4, cutlass::bfloat16_t>>, Is_dropout=false, Is_causal=true]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(118): here
            instantiation of "void run_mha_fwd_hdim128<T>(Flash_fwd_params &, cudaStream_t) [with T=cutlass::bfloat16_t]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_hdim128_bf16_sm80.cu(18): here

/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_kernel.h(140): warning: variable "MMA_M" was declared but never referenced
          detected during:
            instantiation of "void flash::compute_attn_1rowblock<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params,Is_Q_split_K>(const Params &, int, int, int) [with Kernel_traits=Flash_fwd_kernel_traits<128, 128, 64, 4, false, false, cutlass::bfloat16_t, Flash_kernel_traits<128, 128, 64, 4, cutlass::bfloat16_t>>, Is_dropout=false, Is_causal=true, Is_even_N=false, Is_even_K=false, Return_softmax=false, Params=Flash_fwd_params, Is_Q_split_K=false]" 
(589): here
            instantiation of "void flash::compute_attn<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &) [with Kernel_traits=Flash_fwd_kernel_traits<128, 128, 64, 4, false, false, cutlass::bfloat16_t, Flash_kernel_traits<128, 128, 64, 4, cutlass::bfloat16_t>>, Is_dropout=false, Is_causal=true, Is_even_N=false, Is_even_K=false, Return_softmax=false, Params=Flash_fwd_params]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(15): here
            instantiation of "void flash_fwd_kernel<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax>(Flash_fwd_params) [with Kernel_traits=Flash_fwd_kernel_traits<128, 128, 64, 4, false, false, cutlass::bfloat16_t, Flash_kernel_traits<128, 128, 64, 4, cutlass::bfloat16_t>>, Is_dropout=false, Is_causal=true, Is_even_N=false, Is_even_K=false, Return_softmax=false]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(34): here
            instantiation of "void run_flash_fwd<Kernel_traits,Is_dropout,Is_causal>(Flash_fwd_params &, cudaStream_t) [with Kernel_traits=Flash_fwd_kernel_traits<128, 128, 64, 4, false, false, cutlass::bfloat16_t, Flash_kernel_traits<128, 128, 64, 4, cutlass::bfloat16_t>>, Is_dropout=false, Is_causal=true]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(118): here
            instantiation of "void run_mha_fwd_hdim128<T>(Flash_fwd_params &, cudaStream_t) [with T=cutlass::bfloat16_t]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_hdim128_bf16_sm80.cu(18): here

/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_kernel.h(276): warning: variable "tQrQ" was declared but never referenced
          detected during:
            instantiation of "void flash::compute_attn_1rowblock<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params,Is_Q_split_K>(const Params &, int, int, int) [with Kernel_traits=Flash_fwd_kernel_traits<128, 128, 64, 4, false, false, cutlass::bfloat16_t, Flash_kernel_traits<128, 128, 64, 4, cutlass::bfloat16_t>>, Is_dropout=false, Is_causal=true, Is_even_N=false, Is_even_K=false, Return_softmax=false, Params=Flash_fwd_params, Is_Q_split_K=false]" 
(589): here
            instantiation of "void flash::compute_attn<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &) [with Kernel_traits=Flash_fwd_kernel_traits<128, 128, 64, 4, false, false, cutlass::bfloat16_t, Flash_kernel_traits<128, 128, 64, 4, cutlass::bfloat16_t>>, Is_dropout=false, Is_causal=true, Is_even_N=false, Is_even_K=false, Return_softmax=false, Params=Flash_fwd_params]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(15): here
            instantiation of "void flash_fwd_kernel<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax>(Flash_fwd_params) [with Kernel_traits=Flash_fwd_kernel_traits<128, 128, 64, 4, false, false, cutlass::bfloat16_t, Flash_kernel_traits<128, 128, 64, 4, cutlass::bfloat16_t>>, Is_dropout=false, Is_causal=true, Is_even_N=false, Is_even_K=false, Return_softmax=false]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(34): here
            instantiation of "void run_flash_fwd<Kernel_traits,Is_dropout,Is_causal>(Flash_fwd_params &, cudaStream_t) [with Kernel_traits=Flash_fwd_kernel_traits<128, 128, 64, 4, false, false, cutlass::bfloat16_t, Flash_kernel_traits<128, 128, 64, 4, cutlass::bfloat16_t>>, Is_dropout=false, Is_causal=true]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(118): here
            instantiation of "void run_mha_fwd_hdim128<T>(Flash_fwd_params &, cudaStream_t) [with T=cutlass::bfloat16_t]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_hdim128_bf16_sm80.cu(18): here

/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_kernel.h(140): warning: variable "MMA_M" was declared but never referenced
          detected during:
            instantiation of "void flash::compute_attn_1rowblock<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params,Is_Q_split_K>(const Params &, int, int, int) [with Kernel_traits=Flash_fwd_kernel_traits<128, 128, 32, 4, false, false, cutlass::bfloat16_t, Flash_kernel_traits<128, 128, 32, 4, cutlass::bfloat16_t>>, Is_dropout=false, Is_causal=false, Is_even_N=true, Is_even_K=true, Return_softmax=false, Params=Flash_fwd_params, Is_Q_split_K=false]" 
(589): here
            instantiation of "void flash::compute_attn<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &) [with Kernel_traits=Flash_fwd_kernel_traits<128, 128, 32, 4, false, false, cutlass::bfloat16_t, Flash_kernel_traits<128, 128, 32, 4, cutlass::bfloat16_t>>, Is_dropout=false, Is_causal=false, Is_even_N=true, Is_even_K=true, Return_softmax=false, Params=Flash_fwd_params]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(15): here
            instantiation of "void flash_fwd_kernel<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax>(Flash_fwd_params) [with Kernel_traits=Flash_fwd_kernel_traits<128, 128, 32, 4, false, false, cutlass::bfloat16_t, Flash_kernel_traits<128, 128, 32, 4, cutlass::bfloat16_t>>, Is_dropout=false, Is_causal=false, Is_even_N=true, Is_even_K=true, Return_softmax=false]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(34): here
            instantiation of "void run_flash_fwd<Kernel_traits,Is_dropout,Is_causal>(Flash_fwd_params &, cudaStream_t) [with Kernel_traits=Flash_fwd_kernel_traits<128, 128, 32, 4, false, false, cutlass::bfloat16_t, Flash_kernel_traits<128, 128, 32, 4, cutlass::bfloat16_t>>, Is_dropout=false, Is_causal=false]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(118): here
            instantiation of "void run_mha_fwd_hdim128<T>(Flash_fwd_params &, cudaStream_t) [with T=cutlass::bfloat16_t]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_hdim128_bf16_sm80.cu(18): here

/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_kernel.h(276): warning: variable "tQrQ" was declared but never referenced
          detected during:
            instantiation of "void flash::compute_attn_1rowblock<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params,Is_Q_split_K>(const Params &, int, int, int) [with Kernel_traits=Flash_fwd_kernel_traits<128, 128, 32, 4, false, false, cutlass::bfloat16_t, Flash_kernel_traits<128, 128, 32, 4, cutlass::bfloat16_t>>, Is_dropout=false, Is_causal=false, Is_even_N=true, Is_even_K=true, Return_softmax=false, Params=Flash_fwd_params, Is_Q_split_K=false]" 
(589): here
            instantiation of "void flash::compute_attn<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &) [with Kernel_traits=Flash_fwd_kernel_traits<128, 128, 32, 4, false, false, cutlass::bfloat16_t, Flash_kernel_traits<128, 128, 32, 4, cutlass::bfloat16_t>>, Is_dropout=false, Is_causal=false, Is_even_N=true, Is_even_K=true, Return_softmax=false, Params=Flash_fwd_params]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(15): here
            instantiation of "void flash_fwd_kernel<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax>(Flash_fwd_params) [with Kernel_traits=Flash_fwd_kernel_traits<128, 128, 32, 4, false, false, cutlass::bfloat16_t, Flash_kernel_traits<128, 128, 32, 4, cutlass::bfloat16_t>>, Is_dropout=false, Is_causal=false, Is_even_N=true, Is_even_K=true, Return_softmax=false]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(34): here
            instantiation of "void run_flash_fwd<Kernel_traits,Is_dropout,Is_causal>(Flash_fwd_params &, cudaStream_t) [with Kernel_traits=Flash_fwd_kernel_traits<128, 128, 32, 4, false, false, cutlass::bfloat16_t, Flash_kernel_traits<128, 128, 32, 4, cutlass::bfloat16_t>>, Is_dropout=false, Is_causal=false]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(118): here
            instantiation of "void run_mha_fwd_hdim128<T>(Flash_fwd_params &, cudaStream_t) [with T=cutlass::bfloat16_t]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_hdim128_bf16_sm80.cu(18): here

/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_kernel.h(140): warning: variable "MMA_M" was declared but never referenced
          detected during:
            instantiation of "void flash::compute_attn_1rowblock<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params,Is_Q_split_K>(const Params &, int, int, int) [with Kernel_traits=Flash_fwd_kernel_traits<128, 128, 32, 4, false, false, cutlass::bfloat16_t, Flash_kernel_traits<128, 128, 32, 4, cutlass::bfloat16_t>>, Is_dropout=false, Is_causal=false, Is_even_N=true, Is_even_K=false, Return_softmax=false, Params=Flash_fwd_params, Is_Q_split_K=false]" 
(589): here
            instantiation of "void flash::compute_attn<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &) [with Kernel_traits=Flash_fwd_kernel_traits<128, 128, 32, 4, false, false, cutlass::bfloat16_t, Flash_kernel_traits<128, 128, 32, 4, cutlass::bfloat16_t>>, Is_dropout=false, Is_causal=false, Is_even_N=true, Is_even_K=false, Return_softmax=false, Params=Flash_fwd_params]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(15): here
            instantiation of "void flash_fwd_kernel<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax>(Flash_fwd_params) [with Kernel_traits=Flash_fwd_kernel_traits<128, 128, 32, 4, false, false, cutlass::bfloat16_t, Flash_kernel_traits<128, 128, 32, 4, cutlass::bfloat16_t>>, Is_dropout=false, Is_causal=false, Is_even_N=true, Is_even_K=false, Return_softmax=false]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(34): here
            instantiation of "void run_flash_fwd<Kernel_traits,Is_dropout,Is_causal>(Flash_fwd_params &, cudaStream_t) [with Kernel_traits=Flash_fwd_kernel_traits<128, 128, 32, 4, false, false, cutlass::bfloat16_t, Flash_kernel_traits<128, 128, 32, 4, cutlass::bfloat16_t>>, Is_dropout=false, Is_causal=false]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(118): here
            instantiation of "void run_mha_fwd_hdim128<T>(Flash_fwd_params &, cudaStream_t) [with T=cutlass::bfloat16_t]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_hdim128_bf16_sm80.cu(18): here

/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_kernel.h(276): warning: variable "tQrQ" was declared but never referenced
          detected during:
            instantiation of "void flash::compute_attn_1rowblock<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params,Is_Q_split_K>(const Params &, int, int, int) [with Kernel_traits=Flash_fwd_kernel_traits<128, 128, 32, 4, false, false, cutlass::bfloat16_t, Flash_kernel_traits<128, 128, 32, 4, cutlass::bfloat16_t>>, Is_dropout=false, Is_causal=false, Is_even_N=true, Is_even_K=false, Return_softmax=false, Params=Flash_fwd_params, Is_Q_split_K=false]" 
(589): here
            instantiation of "void flash::compute_attn<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &) [with Kernel_traits=Flash_fwd_kernel_traits<128, 128, 32, 4, false, false, cutlass::bfloat16_t, Flash_kernel_traits<128, 128, 32, 4, cutlass::bfloat16_t>>, Is_dropout=false, Is_causal=false, Is_even_N=true, Is_even_K=false, Return_softmax=false, Params=Flash_fwd_params]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(15): here
            instantiation of "void flash_fwd_kernel<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax>(Flash_fwd_params) [with Kernel_traits=Flash_fwd_kernel_traits<128, 128, 32, 4, false, false, cutlass::bfloat16_t, Flash_kernel_traits<128, 128, 32, 4, cutlass::bfloat16_t>>, Is_dropout=false, Is_causal=false, Is_even_N=true, Is_even_K=false, Return_softmax=false]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(34): here
            instantiation of "void run_flash_fwd<Kernel_traits,Is_dropout,Is_causal>(Flash_fwd_params &, cudaStream_t) [with Kernel_traits=Flash_fwd_kernel_traits<128, 128, 32, 4, false, false, cutlass::bfloat16_t, Flash_kernel_traits<128, 128, 32, 4, cutlass::bfloat16_t>>, Is_dropout=false, Is_causal=false]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(118): here
            instantiation of "void run_mha_fwd_hdim128<T>(Flash_fwd_params &, cudaStream_t) [with T=cutlass::bfloat16_t]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_hdim128_bf16_sm80.cu(18): here

/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_kernel.h(140): warning: variable "MMA_M" was declared but never referenced
          detected during:
            instantiation of "void flash::compute_attn_1rowblock<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params,Is_Q_split_K>(const Params &, int, int, int) [with Kernel_traits=Flash_fwd_kernel_traits<128, 128, 32, 4, false, false, cutlass::bfloat16_t, Flash_kernel_traits<128, 128, 32, 4, cutlass::bfloat16_t>>, Is_dropout=false, Is_causal=false, Is_even_N=false, Is_even_K=true, Return_softmax=false, Params=Flash_fwd_params, Is_Q_split_K=false]" 
(589): here
            instantiation of "void flash::compute_attn<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &) [with Kernel_traits=Flash_fwd_kernel_traits<128, 128, 32, 4, false, false, cutlass::bfloat16_t, Flash_kernel_traits<128, 128, 32, 4, cutlass::bfloat16_t>>, Is_dropout=false, Is_causal=false, Is_even_N=false, Is_even_K=true, Return_softmax=false, Params=Flash_fwd_params]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(15): here
            instantiation of "void flash_fwd_kernel<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax>(Flash_fwd_params) [with Kernel_traits=Flash_fwd_kernel_traits<128, 128, 32, 4, false, false, cutlass::bfloat16_t, Flash_kernel_traits<128, 128, 32, 4, cutlass::bfloat16_t>>, Is_dropout=false, Is_causal=false, Is_even_N=false, Is_even_K=true, Return_softmax=false]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(34): here
            instantiation of "void run_flash_fwd<Kernel_traits,Is_dropout,Is_causal>(Flash_fwd_params &, cudaStream_t) [with Kernel_traits=Flash_fwd_kernel_traits<128, 128, 32, 4, false, false, cutlass::bfloat16_t, Flash_kernel_traits<128, 128, 32, 4, cutlass::bfloat16_t>>, Is_dropout=false, Is_causal=false]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(118): here
            instantiation of "void run_mha_fwd_hdim128<T>(Flash_fwd_params &, cudaStream_t) [with T=cutlass::bfloat16_t]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_hdim128_bf16_sm80.cu(18): here

/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_kernel.h(276): warning: variable "tQrQ" was declared but never referenced
          detected during:
            instantiation of "void flash::compute_attn_1rowblock<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params,Is_Q_split_K>(const Params &, int, int, int) [with Kernel_traits=Flash_fwd_kernel_traits<128, 128, 32, 4, false, false, cutlass::bfloat16_t, Flash_kernel_traits<128, 128, 32, 4, cutlass::bfloat16_t>>, Is_dropout=false, Is_causal=false, Is_even_N=false, Is_even_K=true, Return_softmax=false, Params=Flash_fwd_params, Is_Q_split_K=false]" 
(589): here
            instantiation of "void flash::compute_attn<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &) [with Kernel_traits=Flash_fwd_kernel_traits<128, 128, 32, 4, false, false, cutlass::bfloat16_t, Flash_kernel_traits<128, 128, 32, 4, cutlass::bfloat16_t>>, Is_dropout=false, Is_causal=false, Is_even_N=false, Is_even_K=true, Return_softmax=false, Params=Flash_fwd_params]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(15): here
            instantiation of "void flash_fwd_kernel<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax>(Flash_fwd_params) [with Kernel_traits=Flash_fwd_kernel_traits<128, 128, 32, 4, false, false, cutlass::bfloat16_t, Flash_kernel_traits<128, 128, 32, 4, cutlass::bfloat16_t>>, Is_dropout=false, Is_causal=false, Is_even_N=false, Is_even_K=true, Return_softmax=false]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(34): here
            instantiation of "void run_flash_fwd<Kernel_traits,Is_dropout,Is_causal>(Flash_fwd_params &, cudaStream_t) [with Kernel_traits=Flash_fwd_kernel_traits<128, 128, 32, 4, false, false, cutlass::bfloat16_t, Flash_kernel_traits<128, 128, 32, 4, cutlass::bfloat16_t>>, Is_dropout=false, Is_causal=false]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(118): here
            instantiation of "void run_mha_fwd_hdim128<T>(Flash_fwd_params &, cudaStream_t) [with T=cutlass::bfloat16_t]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_hdim128_bf16_sm80.cu(18): here

/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_kernel.h(140): warning: variable "MMA_M" was declared but never referenced
          detected during:
            instantiation of "void flash::compute_attn_1rowblock<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params,Is_Q_split_K>(const Params &, int, int, int) [with Kernel_traits=Flash_fwd_kernel_traits<128, 128, 32, 4, false, false, cutlass::bfloat16_t, Flash_kernel_traits<128, 128, 32, 4, cutlass::bfloat16_t>>, Is_dropout=false, Is_causal=false, Is_even_N=false, Is_even_K=false, Return_softmax=false, Params=Flash_fwd_params, Is_Q_split_K=false]" 
(589): here
            instantiation of "void flash::compute_attn<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &) [with Kernel_traits=Flash_fwd_kernel_traits<128, 128, 32, 4, false, false, cutlass::bfloat16_t, Flash_kernel_traits<128, 128, 32, 4, cutlass::bfloat16_t>>, Is_dropout=false, Is_causal=false, Is_even_N=false, Is_even_K=false, Return_softmax=false, Params=Flash_fwd_params]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(15): here
            instantiation of "void flash_fwd_kernel<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax>(Flash_fwd_params) [with Kernel_traits=Flash_fwd_kernel_traits<128, 128, 32, 4, false, false, cutlass::bfloat16_t, Flash_kernel_traits<128, 128, 32, 4, cutlass::bfloat16_t>>, Is_dropout=false, Is_causal=false, Is_even_N=false, Is_even_K=false, Return_softmax=false]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(34): here
            instantiation of "void run_flash_fwd<Kernel_traits,Is_dropout,Is_causal>(Flash_fwd_params &, cudaStream_t) [with Kernel_traits=Flash_fwd_kernel_traits<128, 128, 32, 4, false, false, cutlass::bfloat16_t, Flash_kernel_traits<128, 128, 32, 4, cutlass::bfloat16_t>>, Is_dropout=false, Is_causal=false]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(118): here
            instantiation of "void run_mha_fwd_hdim128<T>(Flash_fwd_params &, cudaStream_t) [with T=cutlass::bfloat16_t]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_hdim128_bf16_sm80.cu(18): here

/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_kernel.h(276): warning: variable "tQrQ" was declared but never referenced
          detected during:
            instantiation of "void flash::compute_attn_1rowblock<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params,Is_Q_split_K>(const Params &, int, int, int) [with Kernel_traits=Flash_fwd_kernel_traits<128, 128, 32, 4, false, false, cutlass::bfloat16_t, Flash_kernel_traits<128, 128, 32, 4, cutlass::bfloat16_t>>, Is_dropout=false, Is_causal=false, Is_even_N=false, Is_even_K=false, Return_softmax=false, Params=Flash_fwd_params, Is_Q_split_K=false]" 
(589): here
            instantiation of "void flash::compute_attn<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &) [with Kernel_traits=Flash_fwd_kernel_traits<128, 128, 32, 4, false, false, cutlass::bfloat16_t, Flash_kernel_traits<128, 128, 32, 4, cutlass::bfloat16_t>>, Is_dropout=false, Is_causal=false, Is_even_N=false, Is_even_K=false, Return_softmax=false, Params=Flash_fwd_params]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(15): here
            instantiation of "void flash_fwd_kernel<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax>(Flash_fwd_params) [with Kernel_traits=Flash_fwd_kernel_traits<128, 128, 32, 4, false, false, cutlass::bfloat16_t, Flash_kernel_traits<128, 128, 32, 4, cutlass::bfloat16_t>>, Is_dropout=false, Is_causal=false, Is_even_N=false, Is_even_K=false, Return_softmax=false]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(34): here
            instantiation of "void run_flash_fwd<Kernel_traits,Is_dropout,Is_causal>(Flash_fwd_params &, cudaStream_t) [with Kernel_traits=Flash_fwd_kernel_traits<128, 128, 32, 4, false, false, cutlass::bfloat16_t, Flash_kernel_traits<128, 128, 32, 4, cutlass::bfloat16_t>>, Is_dropout=false, Is_causal=false]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(118): here
            instantiation of "void run_mha_fwd_hdim128<T>(Flash_fwd_params &, cudaStream_t) [with T=cutlass::bfloat16_t]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_hdim128_bf16_sm80.cu(18): here

/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_kernel.h(140): warning: variable "MMA_M" was declared but never referenced
          detected during:
            instantiation of "void flash::compute_attn_1rowblock<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params,Is_Q_split_K>(const Params &, int, int, int) [with Kernel_traits=Flash_fwd_kernel_traits<128, 128, 64, 4, false, false, cutlass::bfloat16_t, Flash_kernel_traits<128, 128, 64, 4, cutlass::bfloat16_t>>, Is_dropout=false, Is_causal=false, Is_even_N=true, Is_even_K=true, Return_softmax=false, Params=Flash_fwd_params, Is_Q_split_K=false]" 
(589): here
            instantiation of "void flash::compute_attn<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &) [with Kernel_traits=Flash_fwd_kernel_traits<128, 128, 64, 4, false, false, cutlass::bfloat16_t, Flash_kernel_traits<128, 128, 64, 4, cutlass::bfloat16_t>>, Is_dropout=false, Is_causal=false, Is_even_N=true, Is_even_K=true, Return_softmax=false, Params=Flash_fwd_params]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(15): here
            instantiation of "void flash_fwd_kernel<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax>(Flash_fwd_params) [with Kernel_traits=Flash_fwd_kernel_traits<128, 128, 64, 4, false, false, cutlass::bfloat16_t, Flash_kernel_traits<128, 128, 64, 4, cutlass::bfloat16_t>>, Is_dropout=false, Is_causal=false, Is_even_N=true, Is_even_K=true, Return_softmax=false]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(34): here
            instantiation of "void run_flash_fwd<Kernel_traits,Is_dropout,Is_causal>(Flash_fwd_params &, cudaStream_t) [with Kernel_traits=Flash_fwd_kernel_traits<128, 128, 64, 4, false, false, cutlass::bfloat16_t, Flash_kernel_traits<128, 128, 64, 4, cutlass::bfloat16_t>>, Is_dropout=false, Is_causal=false]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(118): here
            instantiation of "void run_mha_fwd_hdim128<T>(Flash_fwd_params &, cudaStream_t) [with T=cutlass::bfloat16_t]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_hdim128_bf16_sm80.cu(18): here

/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_kernel.h(276): warning: variable "tQrQ" was declared but never referenced
          detected during:
            instantiation of "void flash::compute_attn_1rowblock<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params,Is_Q_split_K>(const Params &, int, int, int) [with Kernel_traits=Flash_fwd_kernel_traits<128, 128, 64, 4, false, false, cutlass::bfloat16_t, Flash_kernel_traits<128, 128, 64, 4, cutlass::bfloat16_t>>, Is_dropout=false, Is_causal=false, Is_even_N=true, Is_even_K=true, Return_softmax=false, Params=Flash_fwd_params, Is_Q_split_K=false]" 
(589): here
            instantiation of "void flash::compute_attn<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &) [with Kernel_traits=Flash_fwd_kernel_traits<128, 128, 64, 4, false, false, cutlass::bfloat16_t, Flash_kernel_traits<128, 128, 64, 4, cutlass::bfloat16_t>>, Is_dropout=false, Is_causal=false, Is_even_N=true, Is_even_K=true, Return_softmax=false, Params=Flash_fwd_params]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(15): here
            instantiation of "void flash_fwd_kernel<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax>(Flash_fwd_params) [with Kernel_traits=Flash_fwd_kernel_traits<128, 128, 64, 4, false, false, cutlass::bfloat16_t, Flash_kernel_traits<128, 128, 64, 4, cutlass::bfloat16_t>>, Is_dropout=false, Is_causal=false, Is_even_N=true, Is_even_K=true, Return_softmax=false]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(34): here
            instantiation of "void run_flash_fwd<Kernel_traits,Is_dropout,Is_causal>(Flash_fwd_params &, cudaStream_t) [with Kernel_traits=Flash_fwd_kernel_traits<128, 128, 64, 4, false, false, cutlass::bfloat16_t, Flash_kernel_traits<128, 128, 64, 4, cutlass::bfloat16_t>>, Is_dropout=false, Is_causal=false]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(118): here
            instantiation of "void run_mha_fwd_hdim128<T>(Flash_fwd_params &, cudaStream_t) [with T=cutlass::bfloat16_t]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_hdim128_bf16_sm80.cu(18): here

/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_kernel.h(140): warning: variable "MMA_M" was declared but never referenced
          detected during:
            instantiation of "void flash::compute_attn_1rowblock<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params,Is_Q_split_K>(const Params &, int, int, int) [with Kernel_traits=Flash_fwd_kernel_traits<128, 128, 64, 4, false, false, cutlass::bfloat16_t, Flash_kernel_traits<128, 128, 64, 4, cutlass::bfloat16_t>>, Is_dropout=false, Is_causal=false, Is_even_N=true, Is_even_K=false, Return_softmax=false, Params=Flash_fwd_params, Is_Q_split_K=false]" 
(589): here
            instantiation of "void flash::compute_attn<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &) [with Kernel_traits=Flash_fwd_kernel_traits<128, 128, 64, 4, false, false, cutlass::bfloat16_t, Flash_kernel_traits<128, 128, 64, 4, cutlass::bfloat16_t>>, Is_dropout=false, Is_causal=false, Is_even_N=true, Is_even_K=false, Return_softmax=false, Params=Flash_fwd_params]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(15): here
            instantiation of "void flash_fwd_kernel<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax>(Flash_fwd_params) [with Kernel_traits=Flash_fwd_kernel_traits<128, 128, 64, 4, false, false, cutlass::bfloat16_t, Flash_kernel_traits<128, 128, 64, 4, cutlass::bfloat16_t>>, Is_dropout=false, Is_causal=false, Is_even_N=true, Is_even_K=false, Return_softmax=false]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(34): here
            instantiation of "void run_flash_fwd<Kernel_traits,Is_dropout,Is_causal>(Flash_fwd_params &, cudaStream_t) [with Kernel_traits=Flash_fwd_kernel_traits<128, 128, 64, 4, false, false, cutlass::bfloat16_t, Flash_kernel_traits<128, 128, 64, 4, cutlass::bfloat16_t>>, Is_dropout=false, Is_causal=false]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(118): here
            instantiation of "void run_mha_fwd_hdim128<T>(Flash_fwd_params &, cudaStream_t) [with T=cutlass::bfloat16_t]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_hdim128_bf16_sm80.cu(18): here

/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_kernel.h(276): warning: variable "tQrQ" was declared but never referenced
          detected during:
            instantiation of "void flash::compute_attn_1rowblock<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params,Is_Q_split_K>(const Params &, int, int, int) [with Kernel_traits=Flash_fwd_kernel_traits<128, 128, 64, 4, false, false, cutlass::bfloat16_t, Flash_kernel_traits<128, 128, 64, 4, cutlass::bfloat16_t>>, Is_dropout=false, Is_causal=false, Is_even_N=true, Is_even_K=false, Return_softmax=false, Params=Flash_fwd_params, Is_Q_split_K=false]" 
(589): here
            instantiation of "void flash::compute_attn<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &) [with Kernel_traits=Flash_fwd_kernel_traits<128, 128, 64, 4, false, false, cutlass::bfloat16_t, Flash_kernel_traits<128, 128, 64, 4, cutlass::bfloat16_t>>, Is_dropout=false, Is_causal=false, Is_even_N=true, Is_even_K=false, Return_softmax=false, Params=Flash_fwd_params]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(15): here
            instantiation of "void flash_fwd_kernel<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax>(Flash_fwd_params) [with Kernel_traits=Flash_fwd_kernel_traits<128, 128, 64, 4, false, false, cutlass::bfloat16_t, Flash_kernel_traits<128, 128, 64, 4, cutlass::bfloat16_t>>, Is_dropout=false, Is_causal=false, Is_even_N=true, Is_even_K=false, Return_softmax=false]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(34): here
            instantiation of "void run_flash_fwd<Kernel_traits,Is_dropout,Is_causal>(Flash_fwd_params &, cudaStream_t) [with Kernel_traits=Flash_fwd_kernel_traits<128, 128, 64, 4, false, false, cutlass::bfloat16_t, Flash_kernel_traits<128, 128, 64, 4, cutlass::bfloat16_t>>, Is_dropout=false, Is_causal=false]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(118): here
            instantiation of "void run_mha_fwd_hdim128<T>(Flash_fwd_params &, cudaStream_t) [with T=cutlass::bfloat16_t]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_hdim128_bf16_sm80.cu(18): here

/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_kernel.h(140): warning: variable "MMA_M" was declared but never referenced
          detected during:
            instantiation of "void flash::compute_attn_1rowblock<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params,Is_Q_split_K>(const Params &, int, int, int) [with Kernel_traits=Flash_fwd_kernel_traits<128, 128, 64, 4, false, false, cutlass::bfloat16_t, Flash_kernel_traits<128, 128, 64, 4, cutlass::bfloat16_t>>, Is_dropout=false, Is_causal=false, Is_even_N=false, Is_even_K=true, Return_softmax=false, Params=Flash_fwd_params, Is_Q_split_K=false]" 
(589): here
            instantiation of "void flash::compute_attn<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &) [with Kernel_traits=Flash_fwd_kernel_traits<128, 128, 64, 4, false, false, cutlass::bfloat16_t, Flash_kernel_traits<128, 128, 64, 4, cutlass::bfloat16_t>>, Is_dropout=false, Is_causal=false, Is_even_N=false, Is_even_K=true, Return_softmax=false, Params=Flash_fwd_params]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(15): here
            instantiation of "void flash_fwd_kernel<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax>(Flash_fwd_params) [with Kernel_traits=Flash_fwd_kernel_traits<128, 128, 64, 4, false, false, cutlass::bfloat16_t, Flash_kernel_traits<128, 128, 64, 4, cutlass::bfloat16_t>>, Is_dropout=false, Is_causal=false, Is_even_N=false, Is_even_K=true, Return_softmax=false]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(34): here
            instantiation of "void run_flash_fwd<Kernel_traits,Is_dropout,Is_causal>(Flash_fwd_params &, cudaStream_t) [with Kernel_traits=Flash_fwd_kernel_traits<128, 128, 64, 4, false, false, cutlass::bfloat16_t, Flash_kernel_traits<128, 128, 64, 4, cutlass::bfloat16_t>>, Is_dropout=false, Is_causal=false]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(118): here
            instantiation of "void run_mha_fwd_hdim128<T>(Flash_fwd_params &, cudaStream_t) [with T=cutlass::bfloat16_t]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_hdim128_bf16_sm80.cu(18): here

/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_kernel.h(276): warning: variable "tQrQ" was declared but never referenced
          detected during:
            instantiation of "void flash::compute_attn_1rowblock<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params,Is_Q_split_K>(const Params &, int, int, int) [with Kernel_traits=Flash_fwd_kernel_traits<128, 128, 64, 4, false, false, cutlass::bfloat16_t, Flash_kernel_traits<128, 128, 64, 4, cutlass::bfloat16_t>>, Is_dropout=false, Is_causal=false, Is_even_N=false, Is_even_K=true, Return_softmax=false, Params=Flash_fwd_params, Is_Q_split_K=false]" 
(589): here
            instantiation of "void flash::compute_attn<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &) [with Kernel_traits=Flash_fwd_kernel_traits<128, 128, 64, 4, false, false, cutlass::bfloat16_t, Flash_kernel_traits<128, 128, 64, 4, cutlass::bfloat16_t>>, Is_dropout=false, Is_causal=false, Is_even_N=false, Is_even_K=true, Return_softmax=false, Params=Flash_fwd_params]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(15): here
            instantiation of "void flash_fwd_kernel<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax>(Flash_fwd_params) [with Kernel_traits=Flash_fwd_kernel_traits<128, 128, 64, 4, false, false, cutlass::bfloat16_t, Flash_kernel_traits<128, 128, 64, 4, cutlass::bfloat16_t>>, Is_dropout=false, Is_causal=false, Is_even_N=false, Is_even_K=true, Return_softmax=false]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(34): here
            instantiation of "void run_flash_fwd<Kernel_traits,Is_dropout,Is_causal>(Flash_fwd_params &, cudaStream_t) [with Kernel_traits=Flash_fwd_kernel_traits<128, 128, 64, 4, false, false, cutlass::bfloat16_t, Flash_kernel_traits<128, 128, 64, 4, cutlass::bfloat16_t>>, Is_dropout=false, Is_causal=false]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(118): here
            instantiation of "void run_mha_fwd_hdim128<T>(Flash_fwd_params &, cudaStream_t) [with T=cutlass::bfloat16_t]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_hdim128_bf16_sm80.cu(18): here

/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_kernel.h(140): warning: variable "MMA_M" was declared but never referenced
          detected during:
            instantiation of "void flash::compute_attn_1rowblock<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params,Is_Q_split_K>(const Params &, int, int, int) [with Kernel_traits=Flash_fwd_kernel_traits<128, 128, 64, 4, false, false, cutlass::bfloat16_t, Flash_kernel_traits<128, 128, 64, 4, cutlass::bfloat16_t>>, Is_dropout=false, Is_causal=false, Is_even_N=false, Is_even_K=false, Return_softmax=false, Params=Flash_fwd_params, Is_Q_split_K=false]" 
(589): here
            instantiation of "void flash::compute_attn<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &) [with Kernel_traits=Flash_fwd_kernel_traits<128, 128, 64, 4, false, false, cutlass::bfloat16_t, Flash_kernel_traits<128, 128, 64, 4, cutlass::bfloat16_t>>, Is_dropout=false, Is_causal=false, Is_even_N=false, Is_even_K=false, Return_softmax=false, Params=Flash_fwd_params]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(15): here
            instantiation of "void flash_fwd_kernel<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax>(Flash_fwd_params) [with Kernel_traits=Flash_fwd_kernel_traits<128, 128, 64, 4, false, false, cutlass::bfloat16_t, Flash_kernel_traits<128, 128, 64, 4, cutlass::bfloat16_t>>, Is_dropout=false, Is_causal=false, Is_even_N=false, Is_even_K=false, Return_softmax=false]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(34): here
            instantiation of "void run_flash_fwd<Kernel_traits,Is_dropout,Is_causal>(Flash_fwd_params &, cudaStream_t) [with Kernel_traits=Flash_fwd_kernel_traits<128, 128, 64, 4, false, false, cutlass::bfloat16_t, Flash_kernel_traits<128, 128, 64, 4, cutlass::bfloat16_t>>, Is_dropout=false, Is_causal=false]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(118): here
            instantiation of "void run_mha_fwd_hdim128<T>(Flash_fwd_params &, cudaStream_t) [with T=cutlass::bfloat16_t]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_hdim128_bf16_sm80.cu(18): here

/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_kernel.h(276): warning: variable "tQrQ" was declared but never referenced
          detected during:
            instantiation of "void flash::compute_attn_1rowblock<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params,Is_Q_split_K>(const Params &, int, int, int) [with Kernel_traits=Flash_fwd_kernel_traits<128, 128, 64, 4, false, false, cutlass::bfloat16_t, Flash_kernel_traits<128, 128, 64, 4, cutlass::bfloat16_t>>, Is_dropout=false, Is_causal=false, Is_even_N=false, Is_even_K=false, Return_softmax=false, Params=Flash_fwd_params, Is_Q_split_K=false]" 
(589): here
            instantiation of "void flash::compute_attn<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &) [with Kernel_traits=Flash_fwd_kernel_traits<128, 128, 64, 4, false, false, cutlass::bfloat16_t, Flash_kernel_traits<128, 128, 64, 4, cutlass::bfloat16_t>>, Is_dropout=false, Is_causal=false, Is_even_N=false, Is_even_K=false, Return_softmax=false, Params=Flash_fwd_params]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(15): here
            instantiation of "void flash_fwd_kernel<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax>(Flash_fwd_params) [with Kernel_traits=Flash_fwd_kernel_traits<128, 128, 64, 4, false, false, cutlass::bfloat16_t, Flash_kernel_traits<128, 128, 64, 4, cutlass::bfloat16_t>>, Is_dropout=false, Is_causal=false, Is_even_N=false, Is_even_K=false, Return_softmax=false]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(34): here
            instantiation of "void run_flash_fwd<Kernel_traits,Is_dropout,Is_causal>(Flash_fwd_params &, cudaStream_t) [with Kernel_traits=Flash_fwd_kernel_traits<128, 128, 64, 4, false, false, cutlass::bfloat16_t, Flash_kernel_traits<128, 128, 64, 4, cutlass::bfloat16_t>>, Is_dropout=false, Is_causal=false]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(118): here
            instantiation of "void run_mha_fwd_hdim128<T>(Flash_fwd_params &, cudaStream_t) [with T=cutlass::bfloat16_t]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_hdim128_bf16_sm80.cu(18): here

ptxas info    : 2 bytes gmem
ptxas info    : Compiling entry function '_Z16flash_fwd_kernelI23Flash_fwd_kernel_traitsILi128ELi128ELi64ELi4ELb0ELb0EN7cutlass10bfloat16_tE19Flash_kernel_traitsILi128ELi128ELi64ELi4ES2_EELb0ELb0ELb0ELb0ELb0EEv16Flash_fwd_params' for 'sm_80'
ptxas info    : Function properties for _Z16flash_fwd_kernelI23Flash_fwd_kernel_traitsILi128ELi128ELi64ELi4ELb0ELb0EN7cutlass10bfloat16_tE19Flash_kernel_traitsILi128ELi128ELi64ELi4ES2_EELb0ELb0ELb0ELb0ELb0EEv16Flash_fwd_params
    24 bytes stack frame, 32 bytes spill stores, 32 bytes spill loads
ptxas info    : Used 255 registers, 584 bytes cmem[0]
ptxas info    : Compiling entry function '_Z16flash_fwd_kernelI23Flash_fwd_kernel_traitsILi128ELi128ELi64ELi4ELb0ELb0EN7cutlass10bfloat16_tE19Flash_kernel_traitsILi128ELi128ELi64ELi4ES2_EELb0ELb0ELb0ELb1ELb0EEv16Flash_fwd_params' for 'sm_80'
ptxas info    : Function properties for _Z16flash_fwd_kernelI23Flash_fwd_kernel_traitsILi128ELi128ELi64ELi4ELb0ELb0EN7cutlass10bfloat16_tE19Flash_kernel_traitsILi128ELi128ELi64ELi4ES2_EELb0ELb0ELb0ELb1ELb0EEv16Flash_fwd_params
    16 bytes stack frame, 32 bytes spill stores, 16 bytes spill loads
ptxas info    : Used 255 registers, 584 bytes cmem[0]
ptxas info    : Compiling entry function '_Z16flash_fwd_kernelI23Flash_fwd_kernel_traitsILi128ELi128ELi64ELi4ELb0ELb0EN7cutlass10bfloat16_tE19Flash_kernel_traitsILi128ELi128ELi64ELi4ES2_EELb0ELb0ELb1ELb0ELb0EEv16Flash_fwd_params' for 'sm_80'
ptxas info    : Function properties for _Z16flash_fwd_kernelI23Flash_fwd_kernel_traitsILi128ELi128ELi64ELi4ELb0ELb0EN7cutlass10bfloat16_tE19Flash_kernel_traitsILi128ELi128ELi64ELi4ES2_EELb0ELb0ELb1ELb0ELb0EEv16Flash_fwd_params
    16 bytes stack frame, 24 bytes spill stores, 24 bytes spill loads
ptxas info    : Used 255 registers, 584 bytes cmem[0]
ptxas info    : Compiling entry function '_Z16flash_fwd_kernelI23Flash_fwd_kernel_traitsILi128ELi128ELi64ELi4ELb0ELb0EN7cutlass10bfloat16_tE19Flash_kernel_traitsILi128ELi128ELi64ELi4ES2_EELb0ELb0ELb1ELb1ELb0EEv16Flash_fwd_params' for 'sm_80'
ptxas info    : Function properties for _Z16flash_fwd_kernelI23Flash_fwd_kernel_traitsILi128ELi128ELi64ELi4ELb0ELb0EN7cutlass10bfloat16_tE19Flash_kernel_traitsILi128ELi128ELi64ELi4ES2_EELb0ELb0ELb1ELb1ELb0EEv16Flash_fwd_params
    24 bytes stack frame, 36 bytes spill stores, 36 bytes spill loads
ptxas info    : Used 255 registers, 584 bytes cmem[0]
ptxas info    : Compiling entry function '_Z16flash_fwd_kernelI23Flash_fwd_kernel_traitsILi128ELi128ELi32ELi4ELb0ELb0EN7cutlass10bfloat16_tE19Flash_kernel_traitsILi128ELi128ELi32ELi4ES2_EELb0ELb0ELb0ELb0ELb0EEv16Flash_fwd_params' for 'sm_80'
ptxas info    : Function properties for _Z16flash_fwd_kernelI23Flash_fwd_kernel_traitsILi128ELi128ELi32ELi4ELb0ELb0EN7cutlass10bfloat16_tE19Flash_kernel_traitsILi128ELi128ELi32ELi4ES2_EELb0ELb0ELb0ELb0ELb0EEv16Flash_fwd_params
    0 bytes stack frame, 0 bytes spill stores, 0 bytes spill loads
ptxas info    : Used 243 registers, 584 bytes cmem[0]
ptxas info    : Compiling entry function '_Z16flash_fwd_kernelI23Flash_fwd_kernel_traitsILi128ELi128ELi32ELi4ELb0ELb0EN7cutlass10bfloat16_tE19Flash_kernel_traitsILi128ELi128ELi32ELi4ES2_EELb0ELb0ELb0ELb1ELb0EEv16Flash_fwd_params' for 'sm_80'
ptxas info    : Function properties for _Z16flash_fwd_kernelI23Flash_fwd_kernel_traitsILi128ELi128ELi32ELi4ELb0ELb0EN7cutlass10bfloat16_tE19Flash_kernel_traitsILi128ELi128ELi32ELi4ES2_EELb0ELb0ELb0ELb1ELb0EEv16Flash_fwd_params
    0 bytes stack frame, 0 bytes spill stores, 0 bytes spill loads
ptxas info    : Used 242 registers, 584 bytes cmem[0]
ptxas info    : Compiling entry function '_Z16flash_fwd_kernelI23Flash_fwd_kernel_traitsILi128ELi128ELi32ELi4ELb0ELb0EN7cutlass10bfloat16_tE19Flash_kernel_traitsILi128ELi128ELi32ELi4ES2_EELb0ELb0ELb1ELb0ELb0EEv16Flash_fwd_params' for 'sm_80'
ptxas info    : Function properties for _Z16flash_fwd_kernelI23Flash_fwd_kernel_traitsILi128ELi128ELi32ELi4ELb0ELb0EN7cutlass10bfloat16_tE19Flash_kernel_traitsILi128ELi128ELi32ELi4ES2_EELb0ELb0ELb1ELb0ELb0EEv16Flash_fwd_params
    0 bytes stack frame, 0 bytes spill stores, 0 bytes spill loads
ptxas info    : Used 242 registers, 584 bytes cmem[0]
ptxas info    : Compiling entry function '_Z16flash_fwd_kernelI23Flash_fwd_kernel_traitsILi128ELi128ELi32ELi4ELb0ELb0EN7cutlass10bfloat16_tE19Flash_kernel_traitsILi128ELi128ELi32ELi4ES2_EELb0ELb0ELb1ELb1ELb0EEv16Flash_fwd_params' for 'sm_80'
ptxas info    : Function properties for _Z16flash_fwd_kernelI23Flash_fwd_kernel_traitsILi128ELi128ELi32ELi4ELb0ELb0EN7cutlass10bfloat16_tE19Flash_kernel_traitsILi128ELi128ELi32ELi4ES2_EELb0ELb0ELb1ELb1ELb0EEv16Flash_fwd_params
    0 bytes stack frame, 0 bytes spill stores, 0 bytes spill loads
ptxas info    : Used 242 registers, 584 bytes cmem[0]
ptxas info    : Compiling entry function '_Z16flash_fwd_kernelI23Flash_fwd_kernel_traitsILi128ELi128ELi64ELi4ELb0ELb0EN7cutlass10bfloat16_tE19Flash_kernel_traitsILi128ELi128ELi64ELi4ES2_EELb0ELb1ELb0ELb0ELb0EEv16Flash_fwd_params' for 'sm_80'
ptxas info    : Function properties for _Z16flash_fwd_kernelI23Flash_fwd_kernel_traitsILi128ELi128ELi64ELi4ELb0ELb0EN7cutlass10bfloat16_tE19Flash_kernel_traitsILi128ELi128ELi64ELi4ES2_EELb0ELb1ELb0ELb0ELb0EEv16Flash_fwd_params
    256 bytes stack frame, 320 bytes spill stores, 356 bytes spill loads
ptxas info    : Used 255 registers, 584 bytes cmem[0]
ptxas info    : Compiling entry function '_Z16flash_fwd_kernelI23Flash_fwd_kernel_traitsILi128ELi128ELi64ELi4ELb0ELb0EN7cutlass10bfloat16_tE19Flash_kernel_traitsILi128ELi128ELi64ELi4ES2_EELb0ELb1ELb0ELb1ELb0EEv16Flash_fwd_params' for 'sm_80'
ptxas info    : Function properties for _Z16flash_fwd_kernelI23Flash_fwd_kernel_traitsILi128ELi128ELi64ELi4ELb0ELb0EN7cutlass10bfloat16_tE19Flash_kernel_traitsILi128ELi128ELi64ELi4ES2_EELb0ELb1ELb0ELb1ELb0EEv16Flash_fwd_params
    192 bytes stack frame, 292 bytes spill stores, 316 bytes spill loads
ptxas info    : Used 255 registers, 584 bytes cmem[0]
ptxas info    : Compiling entry function '_Z16flash_fwd_kernelI23Flash_fwd_kernel_traitsILi128ELi128ELi64ELi4ELb0ELb0EN7cutlass10bfloat16_tE19Flash_kernel_traitsILi128ELi128ELi64ELi4ES2_EELb0ELb1ELb1ELb0ELb0EEv16Flash_fwd_params' for 'sm_80'
ptxas info    : Function properties for _Z16flash_fwd_kernelI23Flash_fwd_kernel_traitsILi128ELi128ELi64ELi4ELb0ELb0EN7cutlass10bfloat16_tE19Flash_kernel_traitsILi128ELi128ELi64ELi4ES2_EELb0ELb1ELb1ELb0ELb0EEv16Flash_fwd_params
    128 bytes stack frame, 192 bytes spill stores, 220 bytes spill loads
ptxas info    : Used 255 registers, 584 bytes cmem[0]
ptxas info    : Compiling entry function '_Z16flash_fwd_kernelI23Flash_fwd_kernel_traitsILi128ELi128ELi64ELi4ELb0ELb0EN7cutlass10bfloat16_tE19Flash_kernel_traitsILi128ELi128ELi64ELi4ES2_EELb0ELb1ELb1ELb1ELb0EEv16Flash_fwd_params' for 'sm_80'
ptxas info    : Function properties for _Z16flash_fwd_kernelI23Flash_fwd_kernel_traitsILi128ELi128ELi64ELi4ELb0ELb0EN7cutlass10bfloat16_tE19Flash_kernel_traitsILi128ELi128ELi64ELi4ES2_EELb0ELb1ELb1ELb1ELb0EEv16Flash_fwd_params
    160 bytes stack frame, 220 bytes spill stores, 180 bytes spill loads
ptxas info    : Used 255 registers, 584 bytes cmem[0]
ptxas info    : Compiling entry function '_Z16flash_fwd_kernelI23Flash_fwd_kernel_traitsILi128ELi64ELi64ELi4ELb0ELb0EN7cutlass10bfloat16_tE19Flash_kernel_traitsILi128ELi64ELi64ELi4ES2_EELb0ELb1ELb0ELb0ELb0EEv16Flash_fwd_params' for 'sm_80'
ptxas info    : Function properties for _Z16flash_fwd_kernelI23Flash_fwd_kernel_traitsILi128ELi64ELi64ELi4ELb0ELb0EN7cutlass10bfloat16_tE19Flash_kernel_traitsILi128ELi64ELi64ELi4ES2_EELb0ELb1ELb0ELb0ELb0EEv16Flash_fwd_params
    0 bytes stack frame, 0 bytes spill stores, 0 bytes spill loads
ptxas info    : Used 163 registers, 584 bytes cmem[0]
ptxas info    : Compiling entry function '_Z16flash_fwd_kernelI23Flash_fwd_kernel_traitsILi128ELi64ELi64ELi4ELb0ELb0EN7cutlass10bfloat16_tE19Flash_kernel_traitsILi128ELi64ELi64ELi4ES2_EELb0ELb1ELb0ELb1ELb0EEv16Flash_fwd_params' for 'sm_80'
ptxas info    : Function properties for _Z16flash_fwd_kernelI23Flash_fwd_kernel_traitsILi128ELi64ELi64ELi4ELb0ELb0EN7cutlass10bfloat16_tE19Flash_kernel_traitsILi128ELi64ELi64ELi4ES2_EELb0ELb1ELb0ELb1ELb0EEv16Flash_fwd_params
    0 bytes stack frame, 0 bytes spill stores, 0 bytes spill loads
ptxas info    : Used 160 registers, 584 bytes cmem[0]
ptxas info    : Compiling entry function '_Z16flash_fwd_kernelI23Flash_fwd_kernel_traitsILi128ELi64ELi64ELi4ELb0ELb0EN7cutlass10bfloat16_tE19Flash_kernel_traitsILi128ELi64ELi64ELi4ES2_EELb0ELb1ELb1ELb0ELb0EEv16Flash_fwd_params' for 'sm_80'
ptxas info    : Function properties for _Z16flash_fwd_kernelI23Flash_fwd_kernel_traitsILi128ELi64ELi64ELi4ELb0ELb0EN7cutlass10bfloat16_tE19Flash_kernel_traitsILi128ELi64ELi64ELi4ES2_EELb0ELb1ELb1ELb0ELb0EEv16Flash_fwd_params
    0 bytes stack frame, 0 bytes spill stores, 0 bytes spill loads
ptxas info    : Used 167 registers, 584 bytes cmem[0]
ptxas info    : Compiling entry function '_Z16flash_fwd_kernelI23Flash_fwd_kernel_traitsILi128ELi64ELi64ELi4ELb0ELb0EN7cutlass10bfloat16_tE19Flash_kernel_traitsILi128ELi64ELi64ELi4ES2_EELb0ELb1ELb1ELb1ELb0EEv16Flash_fwd_params' for 'sm_80'
ptxas info    : Function properties for _Z16flash_fwd_kernelI23Flash_fwd_kernel_traitsILi128ELi64ELi64ELi4ELb0ELb0EN7cutlass10bfloat16_tE19Flash_kernel_traitsILi128ELi64ELi64ELi4ES2_EELb0ELb1ELb1ELb1ELb0EEv16Flash_fwd_params
    0 bytes stack frame, 0 bytes spill stores, 0 bytes spill loads
ptxas info    : Used 166 registers, 584 bytes cmem[0]
ptxas info    : Compiling entry function '_Z16flash_fwd_kernelI23Flash_fwd_kernel_traitsILi128ELi128ELi32ELi4ELb0ELb0EN7cutlass10bfloat16_tE19Flash_kernel_traitsILi128ELi128ELi32ELi4ES2_EELb1ELb0ELb0ELb0ELb0EEv16Flash_fwd_params' for 'sm_80'
ptxas info    : Function properties for _Z16flash_fwd_kernelI23Flash_fwd_kernel_traitsILi128ELi128ELi32ELi4ELb0ELb0EN7cutlass10bfloat16_tE19Flash_kernel_traitsILi128ELi128ELi32ELi4ES2_EELb1ELb0ELb0ELb0ELb0EEv16Flash_fwd_params
    0 bytes stack frame, 0 bytes spill stores, 0 bytes spill loads
ptxas info    : Used 252 registers, 584 bytes cmem[0]
ptxas info    : Compiling entry function '_Z16flash_fwd_kernelI23Flash_fwd_kernel_traitsILi128ELi128ELi32ELi4ELb0ELb0EN7cutlass10bfloat16_tE19Flash_kernel_traitsILi128ELi128ELi32ELi4ES2_EELb1ELb0ELb0ELb0ELb1EEv16Flash_fwd_params' for 'sm_80'
ptxas info    : Function properties for _Z16flash_fwd_kernelI23Flash_fwd_kernel_traitsILi128ELi128ELi32ELi4ELb0ELb0EN7cutlass10bfloat16_tE19Flash_kernel_traitsILi128ELi128ELi32ELi4ES2_EELb1ELb0ELb0ELb0ELb1EEv16Flash_fwd_params
    8 bytes stack frame, 4 bytes spill stores, 4 bytes spill loads
ptxas info    : Used 255 registers, 584 bytes cmem[0]
ptxas info    : Compiling entry function '_Z16flash_fwd_kernelI23Flash_fwd_kernel_traitsILi128ELi128ELi32ELi4ELb0ELb0EN7cutlass10bfloat16_tE19Flash_kernel_traitsILi128ELi128ELi32ELi4ES2_EELb1ELb0ELb0ELb1ELb0EEv16Flash_fwd_params' for 'sm_80'
ptxas info    : Function properties for _Z16flash_fwd_kernelI23Flash_fwd_kernel_traitsILi128ELi128ELi32ELi4ELb0ELb0EN7cutlass10bfloat16_tE19Flash_kernel_traitsILi128ELi128ELi32ELi4ES2_EELb1ELb0ELb0ELb1ELb0EEv16Flash_fwd_params
    0 bytes stack frame, 0 bytes spill stores, 0 bytes spill loads
ptxas info    : Used 252 registers, 584 bytes cmem[0]
ptxas info    : Compiling entry function '_Z16flash_fwd_kernelI23Flash_fwd_kernel_traitsILi128ELi128ELi32ELi4ELb0ELb0EN7cutlass10bfloat16_tE19Flash_kernel_traitsILi128ELi128ELi32ELi4ES2_EELb1ELb0ELb0ELb1ELb1EEv16Flash_fwd_params' for 'sm_80'
ptxas info    : Function properties for _Z16flash_fwd_kernelI23Flash_fwd_kernel_traitsILi128ELi128ELi32ELi4ELb0ELb0EN7cutlass10bfloat16_tE19Flash_kernel_traitsILi128ELi128ELi32ELi4ES2_EELb1ELb0ELb0ELb1ELb1EEv16Flash_fwd_params
    0 bytes stack frame, 0 bytes spill stores, 0 bytes spill loads
ptxas info    : Used 255 registers, 584 bytes cmem[0]
ptxas info    : Compiling entry function '_Z16flash_fwd_kernelI23Flash_fwd_kernel_traitsILi128ELi128ELi32ELi4ELb0ELb0EN7cutlass10bfloat16_tE19Flash_kernel_traitsILi128ELi128ELi32ELi4ES2_EELb1ELb0ELb1ELb0ELb0EEv16Flash_fwd_params' for 'sm_80'
ptxas info    : Function properties for _Z16flash_fwd_kernelI23Flash_fwd_kernel_traitsILi128ELi128ELi32ELi4ELb0ELb0EN7cutlass10bfloat16_tE19Flash_kernel_traitsILi128ELi128ELi32ELi4ES2_EELb1ELb0ELb1ELb0ELb0EEv16Flash_fwd_params
    0 bytes stack frame, 0 bytes spill stores, 0 bytes spill loads
ptxas info    : Used 252 registers, 584 bytes cmem[0]
ptxas info    : Compiling entry function '_Z16flash_fwd_kernelI23Flash_fwd_kernel_traitsILi128ELi128ELi32ELi4ELb0ELb0EN7cutlass10bfloat16_tE19Flash_kernel_traitsILi128ELi128ELi32ELi4ES2_EELb1ELb0ELb1ELb0ELb1EEv16Flash_fwd_params' for 'sm_80'
ptxas info    : Function properties for _Z16flash_fwd_kernelI23Flash_fwd_kernel_traitsILi128ELi128ELi32ELi4ELb0ELb0EN7cutlass10bfloat16_tE19Flash_kernel_traitsILi128ELi128ELi32ELi4ES2_EELb1ELb0ELb1ELb0ELb1EEv16Flash_fwd_params
    0 bytes stack frame, 0 bytes spill stores, 0 bytes spill loads
ptxas info    : Used 255 registers, 584 bytes cmem[0]
ptxas info    : Compiling entry function '_Z16flash_fwd_kernelI23Flash_fwd_kernel_traitsILi128ELi128ELi32ELi4ELb0ELb0EN7cutlass10bfloat16_tE19Flash_kernel_traitsILi128ELi128ELi32ELi4ES2_EELb1ELb0ELb1ELb1ELb0EEv16Flash_fwd_params' for 'sm_80'
ptxas info    : Function properties for _Z16flash_fwd_kernelI23Flash_fwd_kernel_traitsILi128ELi128ELi32ELi4ELb0ELb0EN7cutlass10bfloat16_tE19Flash_kernel_traitsILi128ELi128ELi32ELi4ES2_EELb1ELb0ELb1ELb1ELb0EEv16Flash_fwd_params
    0 bytes stack frame, 0 bytes spill stores, 0 bytes spill loads
ptxas info    : Used 252 registers, 584 bytes cmem[0]
ptxas info    : Compiling entry function '_Z16flash_fwd_kernelI23Flash_fwd_kernel_traitsILi128ELi128ELi32ELi4ELb0ELb0EN7cutlass10bfloat16_tE19Flash_kernel_traitsILi128ELi128ELi32ELi4ES2_EELb1ELb0ELb1ELb1ELb1EEv16Flash_fwd_params' for 'sm_80'
ptxas info    : Function properties for _Z16flash_fwd_kernelI23Flash_fwd_kernel_traitsILi128ELi128ELi32ELi4ELb0ELb0EN7cutlass10bfloat16_tE19Flash_kernel_traitsILi128ELi128ELi32ELi4ES2_EELb1ELb0ELb1ELb1ELb1EEv16Flash_fwd_params
    0 bytes stack frame, 0 bytes spill stores, 0 bytes spill loads
ptxas info    : Used 255 registers, 584 bytes cmem[0]
ptxas info    : Compiling entry function '_Z16flash_fwd_kernelI23Flash_fwd_kernel_traitsILi128ELi128ELi32ELi4ELb0ELb0EN7cutlass10bfloat16_tE19Flash_kernel_traitsILi128ELi128ELi32ELi4ES2_EELb1ELb1ELb0ELb0ELb0EEv16Flash_fwd_params' for 'sm_80'
ptxas info    : Function properties for _Z16flash_fwd_kernelI23Flash_fwd_kernel_traitsILi128ELi128ELi32ELi4ELb0ELb0EN7cutlass10bfloat16_tE19Flash_kernel_traitsILi128ELi128ELi32ELi4ES2_EELb1ELb1ELb0ELb0ELb0EEv16Flash_fwd_params
    16 bytes stack frame, 16 bytes spill stores, 24 bytes spill loads
ptxas info    : Used 255 registers, 584 bytes cmem[0]
ptxas info    : Compiling entry function '_Z16flash_fwd_kernelI23Flash_fwd_kernel_traitsILi128ELi128ELi32ELi4ELb0ELb0EN7cutlass10bfloat16_tE19Flash_kernel_traitsILi128ELi128ELi32ELi4ES2_EELb1ELb1ELb0ELb0ELb1EEv16Flash_fwd_params' for 'sm_80'
ptxas info    : Function properties for _Z16flash_fwd_kernelI23Flash_fwd_kernel_traitsILi128ELi128ELi32ELi4ELb0ELb0EN7cutlass10bfloat16_tE19Flash_kernel_traitsILi128ELi128ELi32ELi4ES2_EELb1ELb1ELb0ELb0ELb1EEv16Flash_fwd_params
    48 bytes stack frame, 44 bytes spill stores, 64 bytes spill loads
ptxas info    : Used 255 registers, 584 bytes cmem[0]
ptxas info    : Compiling entry function '_Z16flash_fwd_kernelI23Flash_fwd_kernel_traitsILi128ELi128ELi32ELi4ELb0ELb0EN7cutlass10bfloat16_tE19Flash_kernel_traitsILi128ELi128ELi32ELi4ES2_EELb1ELb1ELb0ELb1ELb0EEv16Flash_fwd_params' for 'sm_80'
ptxas info    : Function properties for _Z16flash_fwd_kernelI23Flash_fwd_kernel_traitsILi128ELi128ELi32ELi4ELb0ELb0EN7cutlass10bfloat16_tE19Flash_kernel_traitsILi128ELi128ELi32ELi4ES2_EELb1ELb1ELb0ELb1ELb0EEv16Flash_fwd_params
    16 bytes stack frame, 12 bytes spill stores, 28 bytes spill loads
ptxas info    : Used 255 registers, 584 bytes cmem[0]
ptxas info    : Compiling entry function '_Z16flash_fwd_kernelI23Flash_fwd_kernel_traitsILi128ELi128ELi32ELi4ELb0ELb0EN7cutlass10bfloat16_tE19Flash_kernel_traitsILi128ELi128ELi32ELi4ES2_EELb1ELb1ELb0ELb1ELb1EEv16Flash_fwd_params' for 'sm_80'
ptxas info    : Function properties for _Z16flash_fwd_kernelI23Flash_fwd_kernel_traitsILi128ELi128ELi32ELi4ELb0ELb0EN7cutlass10bfloat16_tE19Flash_kernel_traitsILi128ELi128ELi32ELi4ES2_EELb1ELb1ELb0ELb1ELb1EEv16Flash_fwd_params
    112 bytes stack frame, 114 bytes spill stores, 138 bytes spill loads
ptxas info    : Used 255 registers, 584 bytes cmem[0]
ptxas info    : Compiling entry function '_Z16flash_fwd_kernelI23Flash_fwd_kernel_traitsILi128ELi128ELi32ELi4ELb0ELb0EN7cutlass10bfloat16_tE19Flash_kernel_traitsILi128ELi128ELi32ELi4ES2_EELb1ELb1ELb1ELb0ELb0EEv16Flash_fwd_params' for 'sm_80'
ptxas info    : Function properties for _Z16flash_fwd_kernelI23Flash_fwd_kernel_traitsILi128ELi128ELi32ELi4ELb0ELb0EN7cutlass10bfloat16_tE19Flash_kernel_traitsILi128ELi128ELi32ELi4ES2_EELb1ELb1ELb1ELb0ELb0EEv16Flash_fwd_params
    16 bytes stack frame, 16 bytes spill stores, 24 bytes spill loads
ptxas info    : Used 255 registers, 584 bytes cmem[0]
ptxas info    : Compiling entry function '_Z16flash_fwd_kernelI23Flash_fwd_kernel_traitsILi128ELi128ELi32ELi4ELb0ELb0EN7cutlass10bfloat16_tE19Flash_kernel_traitsILi128ELi128ELi32ELi4ES2_EELb1ELb1ELb1ELb0ELb1EEv16Flash_fwd_params' for 'sm_80'
ptxas info    : Function properties for _Z16flash_fwd_kernelI23Flash_fwd_kernel_traitsILi128ELi128ELi32ELi4ELb0ELb0EN7cutlass10bfloat16_tE19Flash_kernel_traitsILi128ELi128ELi32ELi4ES2_EELb1ELb1ELb1ELb0ELb1EEv16Flash_fwd_params
    8 bytes stack frame, 8 bytes spill stores, 12 bytes spill loads
ptxas info    : Used 255 registers, 584 bytes cmem[0]
ptxas info    : Compiling entry function '_Z16flash_fwd_kernelI23Flash_fwd_kernel_traitsILi128ELi128ELi32ELi4ELb0ELb0EN7cutlass10bfloat16_tE19Flash_kernel_traitsILi128ELi128ELi32ELi4ES2_EELb1ELb1ELb1ELb1ELb0EEv16Flash_fwd_params' for 'sm_80'
ptxas info    : Function properties for _Z16flash_fwd_kernelI23Flash_fwd_kernel_traitsILi128ELi128ELi32ELi4ELb0ELb0EN7cutlass10bfloat16_tE19Flash_kernel_traitsILi128ELi128ELi32ELi4ES2_EELb1ELb1ELb1ELb1ELb0EEv16Flash_fwd_params
    8 bytes stack frame, 4 bytes spill stores, 4 bytes spill loads
ptxas info    : Used 255 registers, 584 bytes cmem[0]
ptxas info    : Compiling entry function '_Z16flash_fwd_kernelI23Flash_fwd_kernel_traitsILi128ELi128ELi32ELi4ELb0ELb0EN7cutlass10bfloat16_tE19Flash_kernel_traitsILi128ELi128ELi32ELi4ES2_EELb1ELb1ELb1ELb1ELb1EEv16Flash_fwd_params' for 'sm_80'
ptxas info    : Function properties for _Z16flash_fwd_kernelI23Flash_fwd_kernel_traitsILi128ELi128ELi32ELi4ELb0ELb0EN7cutlass10bfloat16_tE19Flash_kernel_traitsILi128ELi128ELi32ELi4ES2_EELb1ELb1ELb1ELb1ELb1EEv16Flash_fwd_params
    56 bytes stack frame, 56 bytes spill stores, 68 bytes spill loads
ptxas info    : Used 255 registers, 584 bytes cmem[0]
/anaconda/envs/flashattention/lib/python3.9/site-packages/torch/include/c10/core/SymInt.h(84): warning: integer conversion resulted in a change of sign

[11/37] /usr/local/cuda-11.4/bin/nvcc  -I/home/v-feiychen/flash-attention/csrc/flash_attn -I/home/v-feiychen/flash-attention/csrc/flash_attn/src -I/home/v-feiychen/flash-attention/csrc/cutlass/include -I/anaconda/envs/flashattention/lib/python3.9/site-packages/torch/include -I/anaconda/envs/flashattention/lib/python3.9/site-packages/torch/include/torch/csrc/api/include -I/anaconda/envs/flashattention/lib/python3.9/site-packages/torch/include/TH -I/anaconda/envs/flashattention/lib/python3.9/site-packages/torch/include/THC -I/usr/local/cuda-11.4/include -I/anaconda/envs/flashattention/include/python3.9 -c -c /home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_bwd_hdim96_bf16_sm80.cu -o /home/v-feiychen/flash-attention/build/temp.linux-x86_64-cpython-39/csrc/flash_attn/src/flash_bwd_hdim96_bf16_sm80.o -D__CUDA_NO_HALF_OPERATORS__ -D__CUDA_NO_HALF_CONVERSIONS__ -D__CUDA_NO_BFLOAT16_CONVERSIONS__ -D__CUDA_NO_HALF2_OPERATORS__ --expt-relaxed-constexpr --compiler-options ''"'"'-fPIC'"'"'' -O3 -std=c++17 -U__CUDA_NO_HALF_OPERATORS__ -U__CUDA_NO_HALF_CONVERSIONS__ -U__CUDA_NO_HALF2_OPERATORS__ -U__CUDA_NO_BFLOAT16_CONVERSIONS__ --expt-relaxed-constexpr --expt-extended-lambda --use_fast_math --ptxas-options=-v -lineinfo -gencode arch=compute_80,code=sm_80 --threads 4 -DTORCH_API_INCLUDE_EXTENSION_H '-DPYBIND11_COMPILER_TYPE="_gcc"' '-DPYBIND11_STDLIB="_libstdcpp"' '-DPYBIND11_BUILD_ABI="_cxxabi1011"' -DTORCH_EXTENSION_NAME=flash_attn_2_cuda -D_GLIBCXX_USE_CXX11_ABI=0
/anaconda/envs/flashattention/lib/python3.9/site-packages/torch/include/c10/core/SymInt.h(84): warning: integer conversion resulted in a change of sign

ptxas info    : 2 bytes gmem
ptxas info    : Compiling entry function '_Z44flash_bwd_dq_dk_dv_loop_seqk_parallel_kernelI23Flash_bwd_kernel_traitsILi96ELi64ELi128ELi8ELi2ELi4ELi4ELb1ELb0EN7cutlass10bfloat16_tE19Flash_kernel_traitsILi96ELi64ELi128ELi8ES2_EELb0ELb0ELb0ELb0EEv16Flash_bwd_params' for 'sm_80'
ptxas info    : Function properties for _Z44flash_bwd_dq_dk_dv_loop_seqk_parallel_kernelI23Flash_bwd_kernel_traitsILi96ELi64ELi128ELi8ELi2ELi4ELi4ELb1ELb0EN7cutlass10bfloat16_tE19Flash_kernel_traitsILi96ELi64ELi128ELi8ES2_EELb0ELb0ELb0ELb0EEv16Flash_bwd_params
    16 bytes stack frame, 12 bytes spill stores, 12 bytes spill loads
ptxas info    : Used 255 registers, 696 bytes cmem[0]
ptxas info    : Compiling entry function '_Z44flash_bwd_dq_dk_dv_loop_seqk_parallel_kernelI23Flash_bwd_kernel_traitsILi96ELi64ELi128ELi8ELi2ELi4ELi4ELb1ELb0EN7cutlass10bfloat16_tE19Flash_kernel_traitsILi96ELi64ELi128ELi8ES2_EELb0ELb0ELb0ELb1EEv16Flash_bwd_params' for 'sm_80'
ptxas info    : Function properties for _Z44flash_bwd_dq_dk_dv_loop_seqk_parallel_kernelI23Flash_bwd_kernel_traitsILi96ELi64ELi128ELi8ELi2ELi4ELi4ELb1ELb0EN7cutlass10bfloat16_tE19Flash_kernel_traitsILi96ELi64ELi128ELi8ES2_EELb0ELb0ELb0ELb1EEv16Flash_bwd_params
    24 bytes stack frame, 20 bytes spill stores, 20 bytes spill loads
ptxas info    : Used 255 registers, 696 bytes cmem[0]
ptxas info    : Compiling entry function '_Z44flash_bwd_dq_dk_dv_loop_seqk_parallel_kernelI23Flash_bwd_kernel_traitsILi96ELi64ELi128ELi8ELi2ELi4ELi4ELb1ELb0EN7cutlass10bfloat16_tE19Flash_kernel_traitsILi96ELi64ELi128ELi8ES2_EELb0ELb0ELb1ELb0EEv16Flash_bwd_params' for 'sm_80'
ptxas info    : Function properties for _Z44flash_bwd_dq_dk_dv_loop_seqk_parallel_kernelI23Flash_bwd_kernel_traitsILi96ELi64ELi128ELi8ELi2ELi4ELi4ELb1ELb0EN7cutlass10bfloat16_tE19Flash_kernel_traitsILi96ELi64ELi128ELi8ES2_EELb0ELb0ELb1ELb0EEv16Flash_bwd_params
    0 bytes stack frame, 0 bytes spill stores, 0 bytes spill loads
ptxas info    : Used 254 registers, 696 bytes cmem[0]
ptxas info    : Compiling entry function '_Z44flash_bwd_dq_dk_dv_loop_seqk_parallel_kernelI23Flash_bwd_kernel_traitsILi96ELi64ELi128ELi8ELi2ELi4ELi4ELb1ELb0EN7cutlass10bfloat16_tE19Flash_kernel_traitsILi96ELi64ELi128ELi8ES2_EELb0ELb0ELb1ELb1EEv16Flash_bwd_params' for 'sm_80'
ptxas info    : Function properties for _Z44flash_bwd_dq_dk_dv_loop_seqk_parallel_kernelI23Flash_bwd_kernel_traitsILi96ELi64ELi128ELi8ELi2ELi4ELi4ELb1ELb0EN7cutlass10bfloat16_tE19Flash_kernel_traitsILi96ELi64ELi128ELi8ES2_EELb0ELb0ELb1ELb1EEv16Flash_bwd_params
    0 bytes stack frame, 0 bytes spill stores, 0 bytes spill loads
ptxas info    : Used 254 registers, 696 bytes cmem[0]
ptxas info    : Compiling entry function '_Z44flash_bwd_dq_dk_dv_loop_seqk_parallel_kernelI23Flash_bwd_kernel_traitsILi96ELi64ELi128ELi8ELi2ELi4ELi4ELb1ELb0EN7cutlass10bfloat16_tE19Flash_kernel_traitsILi96ELi64ELi128ELi8ES2_EELb0ELb1ELb0ELb0EEv16Flash_bwd_params' for 'sm_80'
ptxas info    : Function properties for _Z44flash_bwd_dq_dk_dv_loop_seqk_parallel_kernelI23Flash_bwd_kernel_traitsILi96ELi64ELi128ELi8ELi2ELi4ELi4ELb1ELb0EN7cutlass10bfloat16_tE19Flash_kernel_traitsILi96ELi64ELi128ELi8ES2_EELb0ELb1ELb0ELb0EEv16Flash_bwd_params
    0 bytes stack frame, 0 bytes spill stores, 0 bytes spill loads
ptxas info    : Used 255 registers, 696 bytes cmem[0]
ptxas info    : Compiling entry function '_Z44flash_bwd_dq_dk_dv_loop_seqk_parallel_kernelI23Flash_bwd_kernel_traitsILi96ELi64ELi128ELi8ELi2ELi4ELi4ELb1ELb0EN7cutlass10bfloat16_tE19Flash_kernel_traitsILi96ELi64ELi128ELi8ES2_EELb0ELb1ELb0ELb1EEv16Flash_bwd_params' for 'sm_80'
ptxas info    : Function properties for _Z44flash_bwd_dq_dk_dv_loop_seqk_parallel_kernelI23Flash_bwd_kernel_traitsILi96ELi64ELi128ELi8ELi2ELi4ELi4ELb1ELb0EN7cutlass10bfloat16_tE19Flash_kernel_traitsILi96ELi64ELi128ELi8ES2_EELb0ELb1ELb0ELb1EEv16Flash_bwd_params
    0 bytes stack frame, 0 bytes spill stores, 0 bytes spill loads
ptxas info    : Used 254 registers, 696 bytes cmem[0]
ptxas info    : Compiling entry function '_Z44flash_bwd_dq_dk_dv_loop_seqk_parallel_kernelI23Flash_bwd_kernel_traitsILi96ELi64ELi128ELi8ELi2ELi4ELi4ELb1ELb0EN7cutlass10bfloat16_tE19Flash_kernel_traitsILi96ELi64ELi128ELi8ES2_EELb0ELb1ELb1ELb0EEv16Flash_bwd_params' for 'sm_80'
ptxas info    : Function properties for _Z44flash_bwd_dq_dk_dv_loop_seqk_parallel_kernelI23Flash_bwd_kernel_traitsILi96ELi64ELi128ELi8ELi2ELi4ELi4ELb1ELb0EN7cutlass10bfloat16_tE19Flash_kernel_traitsILi96ELi64ELi128ELi8ES2_EELb0ELb1ELb1ELb0EEv16Flash_bwd_params
    0 bytes stack frame, 0 bytes spill stores, 0 bytes spill loads
ptxas info    : Used 254 registers, 696 bytes cmem[0]
ptxas info    : Compiling entry function '_Z44flash_bwd_dq_dk_dv_loop_seqk_parallel_kernelI23Flash_bwd_kernel_traitsILi96ELi64ELi128ELi8ELi2ELi4ELi4ELb1ELb0EN7cutlass10bfloat16_tE19Flash_kernel_traitsILi96ELi64ELi128ELi8ES2_EELb0ELb1ELb1ELb1EEv16Flash_bwd_params' for 'sm_80'
ptxas info    : Function properties for _Z44flash_bwd_dq_dk_dv_loop_seqk_parallel_kernelI23Flash_bwd_kernel_traitsILi96ELi64ELi128ELi8ELi2ELi4ELi4ELb1ELb0EN7cutlass10bfloat16_tE19Flash_kernel_traitsILi96ELi64ELi128ELi8ES2_EELb0ELb1ELb1ELb1EEv16Flash_bwd_params
    0 bytes stack frame, 0 bytes spill stores, 0 bytes spill loads
ptxas info    : Used 254 registers, 696 bytes cmem[0]
ptxas info    : Compiling entry function '_Z27flash_bwd_convert_dq_kernelI23Flash_bwd_kernel_traitsILi96ELi64ELi128ELi8ELi2ELi4ELi4ELb1ELb0EN7cutlass10bfloat16_tE19Flash_kernel_traitsILi96ELi64ELi128ELi8ES2_EEEv16Flash_bwd_params' for 'sm_80'
ptxas info    : Function properties for _Z27flash_bwd_convert_dq_kernelI23Flash_bwd_kernel_traitsILi96ELi64ELi128ELi8ELi2ELi4ELi4ELb1ELb0EN7cutlass10bfloat16_tE19Flash_kernel_traitsILi96ELi64ELi128ELi8ES2_EEEv16Flash_bwd_params
    0 bytes stack frame, 0 bytes spill stores, 0 bytes spill loads
ptxas info    : Used 32 registers, 696 bytes cmem[0]
ptxas info    : Compiling entry function '_Z44flash_bwd_dq_dk_dv_loop_seqk_parallel_kernelI23Flash_bwd_kernel_traitsILi96ELi64ELi128ELi8ELi2ELi4ELi4ELb1ELb0EN7cutlass10bfloat16_tE19Flash_kernel_traitsILi96ELi64ELi128ELi8ES2_EELb1ELb0ELb0ELb0EEv16Flash_bwd_params' for 'sm_80'
ptxas info    : Function properties for _Z44flash_bwd_dq_dk_dv_loop_seqk_parallel_kernelI23Flash_bwd_kernel_traitsILi96ELi64ELi128ELi8ELi2ELi4ELi4ELb1ELb0EN7cutlass10bfloat16_tE19Flash_kernel_traitsILi96ELi64ELi128ELi8ES2_EELb1ELb0ELb0ELb0EEv16Flash_bwd_params
    16 bytes stack frame, 12 bytes spill stores, 12 bytes spill loads
ptxas info    : Used 255 registers, 696 bytes cmem[0]
ptxas info    : Compiling entry function '_Z44flash_bwd_dq_dk_dv_loop_seqk_parallel_kernelI23Flash_bwd_kernel_traitsILi96ELi64ELi128ELi8ELi2ELi4ELi4ELb1ELb0EN7cutlass10bfloat16_tE19Flash_kernel_traitsILi96ELi64ELi128ELi8ES2_EELb1ELb0ELb0ELb1EEv16Flash_bwd_params' for 'sm_80'
ptxas info    : Function properties for _Z44flash_bwd_dq_dk_dv_loop_seqk_parallel_kernelI23Flash_bwd_kernel_traitsILi96ELi64ELi128ELi8ELi2ELi4ELi4ELb1ELb0EN7cutlass10bfloat16_tE19Flash_kernel_traitsILi96ELi64ELi128ELi8ES2_EELb1ELb0ELb0ELb1EEv16Flash_bwd_params
    0 bytes stack frame, 0 bytes spill stores, 0 bytes spill loads
ptxas info    : Used 254 registers, 696 bytes cmem[0]
ptxas info    : Compiling entry function '_Z44flash_bwd_dq_dk_dv_loop_seqk_parallel_kernelI23Flash_bwd_kernel_traitsILi96ELi64ELi128ELi8ELi2ELi4ELi4ELb1ELb0EN7cutlass10bfloat16_tE19Flash_kernel_traitsILi96ELi64ELi128ELi8ES2_EELb1ELb0ELb1ELb0EEv16Flash_bwd_params' for 'sm_80'
ptxas info    : Function properties for _Z44flash_bwd_dq_dk_dv_loop_seqk_parallel_kernelI23Flash_bwd_kernel_traitsILi96ELi64ELi128ELi8ELi2ELi4ELi4ELb1ELb0EN7cutlass10bfloat16_tE19Flash_kernel_traitsILi96ELi64ELi128ELi8ES2_EELb1ELb0ELb1ELb0EEv16Flash_bwd_params
    8 bytes stack frame, 4 bytes spill stores, 4 bytes spill loads
ptxas info    : Used 255 registers, 696 bytes cmem[0]
ptxas info    : Compiling entry function '_Z44flash_bwd_dq_dk_dv_loop_seqk_parallel_kernelI23Flash_bwd_kernel_traitsILi96ELi64ELi128ELi8ELi2ELi4ELi4ELb1ELb0EN7cutlass10bfloat16_tE19Flash_kernel_traitsILi96ELi64ELi128ELi8ES2_EELb1ELb0ELb1ELb1EEv16Flash_bwd_params' for 'sm_80'
ptxas info    : Function properties for _Z44flash_bwd_dq_dk_dv_loop_seqk_parallel_kernelI23Flash_bwd_kernel_traitsILi96ELi64ELi128ELi8ELi2ELi4ELi4ELb1ELb0EN7cutlass10bfloat16_tE19Flash_kernel_traitsILi96ELi64ELi128ELi8ES2_EELb1ELb0ELb1ELb1EEv16Flash_bwd_params
    0 bytes stack frame, 0 bytes spill stores, 0 bytes spill loads
ptxas info    : Used 255 registers, 696 bytes cmem[0]
ptxas info    : Compiling entry function '_Z44flash_bwd_dq_dk_dv_loop_seqk_parallel_kernelI23Flash_bwd_kernel_traitsILi96ELi64ELi128ELi8ELi2ELi4ELi4ELb1ELb0EN7cutlass10bfloat16_tE19Flash_kernel_traitsILi96ELi64ELi128ELi8ES2_EELb1ELb1ELb0ELb0EEv16Flash_bwd_params' for 'sm_80'
ptxas info    : Function properties for _Z44flash_bwd_dq_dk_dv_loop_seqk_parallel_kernelI23Flash_bwd_kernel_traitsILi96ELi64ELi128ELi8ELi2ELi4ELi4ELb1ELb0EN7cutlass10bfloat16_tE19Flash_kernel_traitsILi96ELi64ELi128ELi8ES2_EELb1ELb1ELb0ELb0EEv16Flash_bwd_params
    0 bytes stack frame, 0 bytes spill stores, 0 bytes spill loads
ptxas info    : Used 255 registers, 696 bytes cmem[0]
ptxas info    : Compiling entry function '_Z44flash_bwd_dq_dk_dv_loop_seqk_parallel_kernelI23Flash_bwd_kernel_traitsILi96ELi64ELi128ELi8ELi2ELi4ELi4ELb1ELb0EN7cutlass10bfloat16_tE19Flash_kernel_traitsILi96ELi64ELi128ELi8ES2_EELb1ELb1ELb0ELb1EEv16Flash_bwd_params' for 'sm_80'
ptxas info    : Function properties for _Z44flash_bwd_dq_dk_dv_loop_seqk_parallel_kernelI23Flash_bwd_kernel_traitsILi96ELi64ELi128ELi8ELi2ELi4ELi4ELb1ELb0EN7cutlass10bfloat16_tE19Flash_kernel_traitsILi96ELi64ELi128ELi8ES2_EELb1ELb1ELb0ELb1EEv16Flash_bwd_params
    0 bytes stack frame, 0 bytes spill stores, 0 bytes spill loads
ptxas info    : Used 255 registers, 696 bytes cmem[0]
ptxas info    : Compiling entry function '_Z44flash_bwd_dq_dk_dv_loop_seqk_parallel_kernelI23Flash_bwd_kernel_traitsILi96ELi64ELi128ELi8ELi2ELi4ELi4ELb1ELb0EN7cutlass10bfloat16_tE19Flash_kernel_traitsILi96ELi64ELi128ELi8ES2_EELb1ELb1ELb1ELb0EEv16Flash_bwd_params' for 'sm_80'
ptxas info    : Function properties for _Z44flash_bwd_dq_dk_dv_loop_seqk_parallel_kernelI23Flash_bwd_kernel_traitsILi96ELi64ELi128ELi8ELi2ELi4ELi4ELb1ELb0EN7cutlass10bfloat16_tE19Flash_kernel_traitsILi96ELi64ELi128ELi8ES2_EELb1ELb1ELb1ELb0EEv16Flash_bwd_params
    0 bytes stack frame, 0 bytes spill stores, 0 bytes spill loads
ptxas info    : Used 255 registers, 696 bytes cmem[0]
ptxas info    : Compiling entry function '_Z44flash_bwd_dq_dk_dv_loop_seqk_parallel_kernelI23Flash_bwd_kernel_traitsILi96ELi64ELi128ELi8ELi2ELi4ELi4ELb1ELb0EN7cutlass10bfloat16_tE19Flash_kernel_traitsILi96ELi64ELi128ELi8ES2_EELb1ELb1ELb1ELb1EEv16Flash_bwd_params' for 'sm_80'
ptxas info    : Function properties for _Z44flash_bwd_dq_dk_dv_loop_seqk_parallel_kernelI23Flash_bwd_kernel_traitsILi96ELi64ELi128ELi8ELi2ELi4ELi4ELb1ELb0EN7cutlass10bfloat16_tE19Flash_kernel_traitsILi96ELi64ELi128ELi8ES2_EELb1ELb1ELb1ELb1EEv16Flash_bwd_params
    0 bytes stack frame, 0 bytes spill stores, 0 bytes spill loads
ptxas info    : Used 255 registers, 696 bytes cmem[0]
ptxas info    : Compiling entry function '_Z25flash_bwd_dot_do_o_kernelILb1E23Flash_bwd_kernel_traitsILi96ELi64ELi128ELi8ELi2ELi4ELi4ELb1ELb0EN7cutlass10bfloat16_tE19Flash_kernel_traitsILi96ELi64ELi128ELi8ES2_EEEv16Flash_bwd_params' for 'sm_80'
ptxas info    : Function properties for _Z25flash_bwd_dot_do_o_kernelILb1E23Flash_bwd_kernel_traitsILi96ELi64ELi128ELi8ELi2ELi4ELi4ELb1ELb0EN7cutlass10bfloat16_tE19Flash_kernel_traitsILi96ELi64ELi128ELi8ES2_EEEv16Flash_bwd_params
    0 bytes stack frame, 0 bytes spill stores, 0 bytes spill loads
ptxas info    : Used 44 registers, 696 bytes cmem[0]
ptxas info    : Compiling entry function '_Z27flash_bwd_convert_dq_kernelI23Flash_bwd_kernel_traitsILi96ELi64ELi128ELi8ELi2ELi4ELi4ELb0ELb0EN7cutlass10bfloat16_tE19Flash_kernel_traitsILi96ELi64ELi128ELi8ES2_EEEv16Flash_bwd_params' for 'sm_80'
ptxas info    : Function properties for _Z27flash_bwd_convert_dq_kernelI23Flash_bwd_kernel_traitsILi96ELi64ELi128ELi8ELi2ELi4ELi4ELb0ELb0EN7cutlass10bfloat16_tE19Flash_kernel_traitsILi96ELi64ELi128ELi8ES2_EEEv16Flash_bwd_params
    0 bytes stack frame, 0 bytes spill stores, 0 bytes spill loads
ptxas info    : Used 32 registers, 696 bytes cmem[0]
ptxas info    : Compiling entry function '_Z44flash_bwd_dq_dk_dv_loop_seqk_parallel_kernelI23Flash_bwd_kernel_traitsILi96ELi64ELi128ELi8ELi2ELi4ELi4ELb0ELb0EN7cutlass10bfloat16_tE19Flash_kernel_traitsILi96ELi64ELi128ELi8ES2_EELb1ELb0ELb0ELb0EEv16Flash_bwd_params' for 'sm_80'
ptxas info    : Function properties for _Z44flash_bwd_dq_dk_dv_loop_seqk_parallel_kernelI23Flash_bwd_kernel_traitsILi96ELi64ELi128ELi8ELi2ELi4ELi4ELb0ELb0EN7cutlass10bfloat16_tE19Flash_kernel_traitsILi96ELi64ELi128ELi8ES2_EELb1ELb0ELb0ELb0EEv16Flash_bwd_params
    0 bytes stack frame, 0 bytes spill stores, 0 bytes spill loads
ptxas info    : Used 238 registers, 696 bytes cmem[0]
ptxas info    : Compiling entry function '_Z44flash_bwd_dq_dk_dv_loop_seqk_parallel_kernelI23Flash_bwd_kernel_traitsILi96ELi64ELi128ELi8ELi2ELi4ELi4ELb0ELb0EN7cutlass10bfloat16_tE19Flash_kernel_traitsILi96ELi64ELi128ELi8ES2_EELb1ELb0ELb0ELb1EEv16Flash_bwd_params' for 'sm_80'
ptxas info    : Function properties for _Z44flash_bwd_dq_dk_dv_loop_seqk_parallel_kernelI23Flash_bwd_kernel_traitsILi96ELi64ELi128ELi8ELi2ELi4ELi4ELb0ELb0EN7cutlass10bfloat16_tE19Flash_kernel_traitsILi96ELi64ELi128ELi8ES2_EELb1ELb0ELb0ELb1EEv16Flash_bwd_params
    0 bytes stack frame, 0 bytes spill stores, 0 bytes spill loads
ptxas info    : Used 238 registers, 696 bytes cmem[0]
ptxas info    : Compiling entry function '_Z44flash_bwd_dq_dk_dv_loop_seqk_parallel_kernelI23Flash_bwd_kernel_traitsILi96ELi64ELi128ELi8ELi2ELi4ELi4ELb0ELb0EN7cutlass10bfloat16_tE19Flash_kernel_traitsILi96ELi64ELi128ELi8ES2_EELb1ELb0ELb1ELb0EEv16Flash_bwd_params' for 'sm_80'
ptxas info    : Function properties for _Z44flash_bwd_dq_dk_dv_loop_seqk_parallel_kernelI23Flash_bwd_kernel_traitsILi96ELi64ELi128ELi8ELi2ELi4ELi4ELb0ELb0EN7cutlass10bfloat16_tE19Flash_kernel_traitsILi96ELi64ELi128ELi8ES2_EELb1ELb0ELb1ELb0EEv16Flash_bwd_params
    0 bytes stack frame, 0 bytes spill stores, 0 bytes spill loads
ptxas info    : Used 238 registers, 696 bytes cmem[0]
ptxas info    : Compiling entry function '_Z44flash_bwd_dq_dk_dv_loop_seqk_parallel_kernelI23Flash_bwd_kernel_traitsILi96ELi64ELi128ELi8ELi2ELi4ELi4ELb0ELb0EN7cutlass10bfloat16_tE19Flash_kernel_traitsILi96ELi64ELi128ELi8ES2_EELb1ELb0ELb1ELb1EEv16Flash_bwd_params' for 'sm_80'
ptxas info    : Function properties for _Z44flash_bwd_dq_dk_dv_loop_seqk_parallel_kernelI23Flash_bwd_kernel_traitsILi96ELi64ELi128ELi8ELi2ELi4ELi4ELb0ELb0EN7cutlass10bfloat16_tE19Flash_kernel_traitsILi96ELi64ELi128ELi8ES2_EELb1ELb0ELb1ELb1EEv16Flash_bwd_params
    0 bytes stack frame, 0 bytes spill stores, 0 bytes spill loads
ptxas info    : Used 243 registers, 696 bytes cmem[0]
ptxas info    : Compiling entry function '_Z44flash_bwd_dq_dk_dv_loop_seqk_parallel_kernelI23Flash_bwd_kernel_traitsILi96ELi64ELi128ELi8ELi2ELi4ELi4ELb0ELb0EN7cutlass10bfloat16_tE19Flash_kernel_traitsILi96ELi64ELi128ELi8ES2_EELb1ELb1ELb0ELb0EEv16Flash_bwd_params' for 'sm_80'
ptxas info    : Function properties for _Z44flash_bwd_dq_dk_dv_loop_seqk_parallel_kernelI23Flash_bwd_kernel_traitsILi96ELi64ELi128ELi8ELi2ELi4ELi4ELb0ELb0EN7cutlass10bfloat16_tE19Flash_kernel_traitsILi96ELi64ELi128ELi8ES2_EELb1ELb1ELb0ELb0EEv16Flash_bwd_params
    0 bytes stack frame, 0 bytes spill stores, 0 bytes spill loads
ptxas info    : Used 255 registers, 696 bytes cmem[0]
ptxas info    : Compiling entry function '_Z44flash_bwd_dq_dk_dv_loop_seqk_parallel_kernelI23Flash_bwd_kernel_traitsILi96ELi64ELi128ELi8ELi2ELi4ELi4ELb0ELb0EN7cutlass10bfloat16_tE19Flash_kernel_traitsILi96ELi64ELi128ELi8ES2_EELb1ELb1ELb0ELb1EEv16Flash_bwd_params' for 'sm_80'
ptxas info    : Function properties for _Z44flash_bwd_dq_dk_dv_loop_seqk_parallel_kernelI23Flash_bwd_kernel_traitsILi96ELi64ELi128ELi8ELi2ELi4ELi4ELb0ELb0EN7cutlass10bfloat16_tE19Flash_kernel_traitsILi96ELi64ELi128ELi8ES2_EELb1ELb1ELb0ELb1EEv16Flash_bwd_params
    0 bytes stack frame, 0 bytes spill stores, 0 bytes spill loads
ptxas info    : Used 253 registers, 696 bytes cmem[0]
ptxas info    : Compiling entry function '_Z44flash_bwd_dq_dk_dv_loop_seqk_parallel_kernelI23Flash_bwd_kernel_traitsILi96ELi64ELi128ELi8ELi2ELi4ELi4ELb0ELb0EN7cutlass10bfloat16_tE19Flash_kernel_traitsILi96ELi64ELi128ELi8ES2_EELb1ELb1ELb1ELb0EEv16Flash_bwd_params' for 'sm_80'
ptxas info    : Function properties for _Z44flash_bwd_dq_dk_dv_loop_seqk_parallel_kernelI23Flash_bwd_kernel_traitsILi96ELi64ELi128ELi8ELi2ELi4ELi4ELb0ELb0EN7cutlass10bfloat16_tE19Flash_kernel_traitsILi96ELi64ELi128ELi8ES2_EELb1ELb1ELb1ELb0EEv16Flash_bwd_params
    0 bytes stack frame, 0 bytes spill stores, 0 bytes spill loads
ptxas info    : Used 250 registers, 696 bytes cmem[0]
ptxas info    : Compiling entry function '_Z44flash_bwd_dq_dk_dv_loop_seqk_parallel_kernelI23Flash_bwd_kernel_traitsILi96ELi64ELi128ELi8ELi2ELi4ELi4ELb0ELb0EN7cutlass10bfloat16_tE19Flash_kernel_traitsILi96ELi64ELi128ELi8ES2_EELb1ELb1ELb1ELb1EEv16Flash_bwd_params' for 'sm_80'
ptxas info    : Function properties for _Z44flash_bwd_dq_dk_dv_loop_seqk_parallel_kernelI23Flash_bwd_kernel_traitsILi96ELi64ELi128ELi8ELi2ELi4ELi4ELb0ELb0EN7cutlass10bfloat16_tE19Flash_kernel_traitsILi96ELi64ELi128ELi8ES2_EELb1ELb1ELb1ELb1EEv16Flash_bwd_params
    0 bytes stack frame, 0 bytes spill stores, 0 bytes spill loads
ptxas info    : Used 245 registers, 696 bytes cmem[0]
ptxas info    : Compiling entry function '_Z25flash_bwd_dot_do_o_kernelILb1E23Flash_bwd_kernel_traitsILi96ELi64ELi128ELi8ELi2ELi4ELi4ELb0ELb0EN7cutlass10bfloat16_tE19Flash_kernel_traitsILi96ELi64ELi128ELi8ES2_EEEv16Flash_bwd_params' for 'sm_80'
ptxas info    : Function properties for _Z25flash_bwd_dot_do_o_kernelILb1E23Flash_bwd_kernel_traitsILi96ELi64ELi128ELi8ELi2ELi4ELi4ELb0ELb0EN7cutlass10bfloat16_tE19Flash_kernel_traitsILi96ELi64ELi128ELi8ES2_EEEv16Flash_bwd_params
    0 bytes stack frame, 0 bytes spill stores, 0 bytes spill loads
ptxas info    : Used 44 registers, 696 bytes cmem[0]
/anaconda/envs/flashattention/lib/python3.9/site-packages/torch/include/c10/core/SymInt.h(84): warning: integer conversion resulted in a change of sign

[12/37] /usr/local/cuda-11.4/bin/nvcc  -I/home/v-feiychen/flash-attention/csrc/flash_attn -I/home/v-feiychen/flash-attention/csrc/flash_attn/src -I/home/v-feiychen/flash-attention/csrc/cutlass/include -I/anaconda/envs/flashattention/lib/python3.9/site-packages/torch/include -I/anaconda/envs/flashattention/lib/python3.9/site-packages/torch/include/torch/csrc/api/include -I/anaconda/envs/flashattention/lib/python3.9/site-packages/torch/include/TH -I/anaconda/envs/flashattention/lib/python3.9/site-packages/torch/include/THC -I/usr/local/cuda-11.4/include -I/anaconda/envs/flashattention/include/python3.9 -c -c /home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_hdim128_fp16_sm80.cu -o /home/v-feiychen/flash-attention/build/temp.linux-x86_64-cpython-39/csrc/flash_attn/src/flash_fwd_hdim128_fp16_sm80.o -D__CUDA_NO_HALF_OPERATORS__ -D__CUDA_NO_HALF_CONVERSIONS__ -D__CUDA_NO_BFLOAT16_CONVERSIONS__ -D__CUDA_NO_HALF2_OPERATORS__ --expt-relaxed-constexpr --compiler-options ''"'"'-fPIC'"'"'' -O3 -std=c++17 -U__CUDA_NO_HALF_OPERATORS__ -U__CUDA_NO_HALF_CONVERSIONS__ -U__CUDA_NO_HALF2_OPERATORS__ -U__CUDA_NO_BFLOAT16_CONVERSIONS__ --expt-relaxed-constexpr --expt-extended-lambda --use_fast_math --ptxas-options=-v -lineinfo -gencode arch=compute_80,code=sm_80 --threads 4 -DTORCH_API_INCLUDE_EXTENSION_H '-DPYBIND11_COMPILER_TYPE="_gcc"' '-DPYBIND11_STDLIB="_libstdcpp"' '-DPYBIND11_BUILD_ABI="_cxxabi1011"' -DTORCH_EXTENSION_NAME=flash_attn_2_cuda -D_GLIBCXX_USE_CXX11_ABI=0
/anaconda/envs/flashattention/lib/python3.9/site-packages/torch/include/c10/core/SymInt.h(84): warning: integer conversion resulted in a change of sign

/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_kernel.h(140): warning: variable "MMA_M" was declared but never referenced
          detected during:
            instantiation of "void flash::compute_attn_1rowblock<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params,Is_Q_split_K>(const Params &, int, int, int) [with Kernel_traits=Flash_fwd_kernel_traits<128, 128, 32, 4, false, false, cutlass::half_t, Flash_kernel_traits<128, 128, 32, 4, cutlass::half_t>>, Is_dropout=true, Is_causal=true, Is_even_N=true, Is_even_K=true, Return_softmax=true, Params=Flash_fwd_params, Is_Q_split_K=false]" 
(589): here
            instantiation of "void flash::compute_attn<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &) [with Kernel_traits=Flash_fwd_kernel_traits<128, 128, 32, 4, false, false, cutlass::half_t, Flash_kernel_traits<128, 128, 32, 4, cutlass::half_t>>, Is_dropout=true, Is_causal=true, Is_even_N=true, Is_even_K=true, Return_softmax=true, Params=Flash_fwd_params]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(15): here
            instantiation of "void flash_fwd_kernel<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax>(Flash_fwd_params) [with Kernel_traits=Flash_fwd_kernel_traits<128, 128, 32, 4, false, false, cutlass::half_t, Flash_kernel_traits<128, 128, 32, 4, cutlass::half_t>>, Is_dropout=true, Is_causal=true, Is_even_N=true, Is_even_K=true, Return_softmax=true]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(34): here
            instantiation of "void run_flash_fwd<Kernel_traits,Is_dropout,Is_causal>(Flash_fwd_params &, cudaStream_t) [with Kernel_traits=Flash_fwd_kernel_traits<128, 128, 32, 4, false, false, cutlass::half_t, Flash_kernel_traits<128, 128, 32, 4, cutlass::half_t>>, Is_dropout=true, Is_causal=true]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(118): here
            instantiation of "void run_mha_fwd_hdim128<T>(Flash_fwd_params &, cudaStream_t) [with T=cutlass::half_t]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_hdim128_fp16_sm80.cu(31): here

/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_kernel.h(276): warning: variable "tQrQ" was declared but never referenced
          detected during:
            instantiation of "void flash::compute_attn_1rowblock<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params,Is_Q_split_K>(const Params &, int, int, int) [with Kernel_traits=Flash_fwd_kernel_traits<128, 128, 32, 4, false, false, cutlass::half_t, Flash_kernel_traits<128, 128, 32, 4, cutlass::half_t>>, Is_dropout=true, Is_causal=true, Is_even_N=true, Is_even_K=true, Return_softmax=true, Params=Flash_fwd_params, Is_Q_split_K=false]" 
(589): here
            instantiation of "void flash::compute_attn<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &) [with Kernel_traits=Flash_fwd_kernel_traits<128, 128, 32, 4, false, false, cutlass::half_t, Flash_kernel_traits<128, 128, 32, 4, cutlass::half_t>>, Is_dropout=true, Is_causal=true, Is_even_N=true, Is_even_K=true, Return_softmax=true, Params=Flash_fwd_params]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(15): here
            instantiation of "void flash_fwd_kernel<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax>(Flash_fwd_params) [with Kernel_traits=Flash_fwd_kernel_traits<128, 128, 32, 4, false, false, cutlass::half_t, Flash_kernel_traits<128, 128, 32, 4, cutlass::half_t>>, Is_dropout=true, Is_causal=true, Is_even_N=true, Is_even_K=true, Return_softmax=true]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(34): here
            instantiation of "void run_flash_fwd<Kernel_traits,Is_dropout,Is_causal>(Flash_fwd_params &, cudaStream_t) [with Kernel_traits=Flash_fwd_kernel_traits<128, 128, 32, 4, false, false, cutlass::half_t, Flash_kernel_traits<128, 128, 32, 4, cutlass::half_t>>, Is_dropout=true, Is_causal=true]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(118): here
            instantiation of "void run_mha_fwd_hdim128<T>(Flash_fwd_params &, cudaStream_t) [with T=cutlass::half_t]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_hdim128_fp16_sm80.cu(31): here

/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_kernel.h(140): warning: variable "MMA_M" was declared but never referenced
          detected during:
            instantiation of "void flash::compute_attn_1rowblock<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params,Is_Q_split_K>(const Params &, int, int, int) [with Kernel_traits=Flash_fwd_kernel_traits<128, 128, 32, 4, false, false, cutlass::half_t, Flash_kernel_traits<128, 128, 32, 4, cutlass::half_t>>, Is_dropout=true, Is_causal=true, Is_even_N=true, Is_even_K=true, Return_softmax=false, Params=Flash_fwd_params, Is_Q_split_K=false]" 
(589): here
            instantiation of "void flash::compute_attn<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &) [with Kernel_traits=Flash_fwd_kernel_traits<128, 128, 32, 4, false, false, cutlass::half_t, Flash_kernel_traits<128, 128, 32, 4, cutlass::half_t>>, Is_dropout=true, Is_causal=true, Is_even_N=true, Is_even_K=true, Return_softmax=false, Params=Flash_fwd_params]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(15): here
            instantiation of "void flash_fwd_kernel<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax>(Flash_fwd_params) [with Kernel_traits=Flash_fwd_kernel_traits<128, 128, 32, 4, false, false, cutlass::half_t, Flash_kernel_traits<128, 128, 32, 4, cutlass::half_t>>, Is_dropout=true, Is_causal=true, Is_even_N=true, Is_even_K=true, Return_softmax=false]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(34): here
            instantiation of "void run_flash_fwd<Kernel_traits,Is_dropout,Is_causal>(Flash_fwd_params &, cudaStream_t) [with Kernel_traits=Flash_fwd_kernel_traits<128, 128, 32, 4, false, false, cutlass::half_t, Flash_kernel_traits<128, 128, 32, 4, cutlass::half_t>>, Is_dropout=true, Is_causal=true]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(118): here
            instantiation of "void run_mha_fwd_hdim128<T>(Flash_fwd_params &, cudaStream_t) [with T=cutlass::half_t]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_hdim128_fp16_sm80.cu(31): here

/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_kernel.h(276): warning: variable "tQrQ" was declared but never referenced
          detected during:
            instantiation of "void flash::compute_attn_1rowblock<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params,Is_Q_split_K>(const Params &, int, int, int) [with Kernel_traits=Flash_fwd_kernel_traits<128, 128, 32, 4, false, false, cutlass::half_t, Flash_kernel_traits<128, 128, 32, 4, cutlass::half_t>>, Is_dropout=true, Is_causal=true, Is_even_N=true, Is_even_K=true, Return_softmax=false, Params=Flash_fwd_params, Is_Q_split_K=false]" 
(589): here
            instantiation of "void flash::compute_attn<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &) [with Kernel_traits=Flash_fwd_kernel_traits<128, 128, 32, 4, false, false, cutlass::half_t, Flash_kernel_traits<128, 128, 32, 4, cutlass::half_t>>, Is_dropout=true, Is_causal=true, Is_even_N=true, Is_even_K=true, Return_softmax=false, Params=Flash_fwd_params]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(15): here
            instantiation of "void flash_fwd_kernel<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax>(Flash_fwd_params) [with Kernel_traits=Flash_fwd_kernel_traits<128, 128, 32, 4, false, false, cutlass::half_t, Flash_kernel_traits<128, 128, 32, 4, cutlass::half_t>>, Is_dropout=true, Is_causal=true, Is_even_N=true, Is_even_K=true, Return_softmax=false]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(34): here
            instantiation of "void run_flash_fwd<Kernel_traits,Is_dropout,Is_causal>(Flash_fwd_params &, cudaStream_t) [with Kernel_traits=Flash_fwd_kernel_traits<128, 128, 32, 4, false, false, cutlass::half_t, Flash_kernel_traits<128, 128, 32, 4, cutlass::half_t>>, Is_dropout=true, Is_causal=true]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(118): here
            instantiation of "void run_mha_fwd_hdim128<T>(Flash_fwd_params &, cudaStream_t) [with T=cutlass::half_t]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_hdim128_fp16_sm80.cu(31): here

/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_kernel.h(140): warning: variable "MMA_M" was declared but never referenced
          detected during:
            instantiation of "void flash::compute_attn_1rowblock<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params,Is_Q_split_K>(const Params &, int, int, int) [with Kernel_traits=Flash_fwd_kernel_traits<128, 128, 32, 4, false, false, cutlass::half_t, Flash_kernel_traits<128, 128, 32, 4, cutlass::half_t>>, Is_dropout=true, Is_causal=true, Is_even_N=true, Is_even_K=false, Return_softmax=true, Params=Flash_fwd_params, Is_Q_split_K=false]" 
(589): here
            instantiation of "void flash::compute_attn<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &) [with Kernel_traits=Flash_fwd_kernel_traits<128, 128, 32, 4, false, false, cutlass::half_t, Flash_kernel_traits<128, 128, 32, 4, cutlass::half_t>>, Is_dropout=true, Is_causal=true, Is_even_N=true, Is_even_K=false, Return_softmax=true, Params=Flash_fwd_params]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(15): here
            instantiation of "void flash_fwd_kernel<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax>(Flash_fwd_params) [with Kernel_traits=Flash_fwd_kernel_traits<128, 128, 32, 4, false, false, cutlass::half_t, Flash_kernel_traits<128, 128, 32, 4, cutlass::half_t>>, Is_dropout=true, Is_causal=true, Is_even_N=true, Is_even_K=false, Return_softmax=true]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(34): here
            instantiation of "void run_flash_fwd<Kernel_traits,Is_dropout,Is_causal>(Flash_fwd_params &, cudaStream_t) [with Kernel_traits=Flash_fwd_kernel_traits<128, 128, 32, 4, false, false, cutlass::half_t, Flash_kernel_traits<128, 128, 32, 4, cutlass::half_t>>, Is_dropout=true, Is_causal=true]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(118): here
            instantiation of "void run_mha_fwd_hdim128<T>(Flash_fwd_params &, cudaStream_t) [with T=cutlass::half_t]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_hdim128_fp16_sm80.cu(31): here

/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_kernel.h(276): warning: variable "tQrQ" was declared but never referenced
          detected during:
            instantiation of "void flash::compute_attn_1rowblock<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params,Is_Q_split_K>(const Params &, int, int, int) [with Kernel_traits=Flash_fwd_kernel_traits<128, 128, 32, 4, false, false, cutlass::half_t, Flash_kernel_traits<128, 128, 32, 4, cutlass::half_t>>, Is_dropout=true, Is_causal=true, Is_even_N=true, Is_even_K=false, Return_softmax=true, Params=Flash_fwd_params, Is_Q_split_K=false]" 
(589): here
            instantiation of "void flash::compute_attn<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &) [with Kernel_traits=Flash_fwd_kernel_traits<128, 128, 32, 4, false, false, cutlass::half_t, Flash_kernel_traits<128, 128, 32, 4, cutlass::half_t>>, Is_dropout=true, Is_causal=true, Is_even_N=true, Is_even_K=false, Return_softmax=true, Params=Flash_fwd_params]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(15): here
            instantiation of "void flash_fwd_kernel<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax>(Flash_fwd_params) [with Kernel_traits=Flash_fwd_kernel_traits<128, 128, 32, 4, false, false, cutlass::half_t, Flash_kernel_traits<128, 128, 32, 4, cutlass::half_t>>, Is_dropout=true, Is_causal=true, Is_even_N=true, Is_even_K=false, Return_softmax=true]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(34): here
            instantiation of "void run_flash_fwd<Kernel_traits,Is_dropout,Is_causal>(Flash_fwd_params &, cudaStream_t) [with Kernel_traits=Flash_fwd_kernel_traits<128, 128, 32, 4, false, false, cutlass::half_t, Flash_kernel_traits<128, 128, 32, 4, cutlass::half_t>>, Is_dropout=true, Is_causal=true]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(118): here
            instantiation of "void run_mha_fwd_hdim128<T>(Flash_fwd_params &, cudaStream_t) [with T=cutlass::half_t]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_hdim128_fp16_sm80.cu(31): here

/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_kernel.h(140): warning: variable "MMA_M" was declared but never referenced
          detected during:
            instantiation of "void flash::compute_attn_1rowblock<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params,Is_Q_split_K>(const Params &, int, int, int) [with Kernel_traits=Flash_fwd_kernel_traits<128, 128, 32, 4, false, false, cutlass::half_t, Flash_kernel_traits<128, 128, 32, 4, cutlass::half_t>>, Is_dropout=true, Is_causal=true, Is_even_N=true, Is_even_K=false, Return_softmax=false, Params=Flash_fwd_params, Is_Q_split_K=false]" 
(589): here
            instantiation of "void flash::compute_attn<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &) [with Kernel_traits=Flash_fwd_kernel_traits<128, 128, 32, 4, false, false, cutlass::half_t, Flash_kernel_traits<128, 128, 32, 4, cutlass::half_t>>, Is_dropout=true, Is_causal=true, Is_even_N=true, Is_even_K=false, Return_softmax=false, Params=Flash_fwd_params]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(15): here
            instantiation of "void flash_fwd_kernel<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax>(Flash_fwd_params) [with Kernel_traits=Flash_fwd_kernel_traits<128, 128, 32, 4, false, false, cutlass::half_t, Flash_kernel_traits<128, 128, 32, 4, cutlass::half_t>>, Is_dropout=true, Is_causal=true, Is_even_N=true, Is_even_K=false, Return_softmax=false]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(34): here
            instantiation of "void run_flash_fwd<Kernel_traits,Is_dropout,Is_causal>(Flash_fwd_params &, cudaStream_t) [with Kernel_traits=Flash_fwd_kernel_traits<128, 128, 32, 4, false, false, cutlass::half_t, Flash_kernel_traits<128, 128, 32, 4, cutlass::half_t>>, Is_dropout=true, Is_causal=true]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(118): here
            instantiation of "void run_mha_fwd_hdim128<T>(Flash_fwd_params &, cudaStream_t) [with T=cutlass::half_t]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_hdim128_fp16_sm80.cu(31): here

/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_kernel.h(276): warning: variable "tQrQ" was declared but never referenced
          detected during:
            instantiation of "void flash::compute_attn_1rowblock<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params,Is_Q_split_K>(const Params &, int, int, int) [with Kernel_traits=Flash_fwd_kernel_traits<128, 128, 32, 4, false, false, cutlass::half_t, Flash_kernel_traits<128, 128, 32, 4, cutlass::half_t>>, Is_dropout=true, Is_causal=true, Is_even_N=true, Is_even_K=false, Return_softmax=false, Params=Flash_fwd_params, Is_Q_split_K=false]" 
(589): here
            instantiation of "void flash::compute_attn<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &) [with Kernel_traits=Flash_fwd_kernel_traits<128, 128, 32, 4, false, false, cutlass::half_t, Flash_kernel_traits<128, 128, 32, 4, cutlass::half_t>>, Is_dropout=true, Is_causal=true, Is_even_N=true, Is_even_K=false, Return_softmax=false, Params=Flash_fwd_params]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(15): here
            instantiation of "void flash_fwd_kernel<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax>(Flash_fwd_params) [with Kernel_traits=Flash_fwd_kernel_traits<128, 128, 32, 4, false, false, cutlass::half_t, Flash_kernel_traits<128, 128, 32, 4, cutlass::half_t>>, Is_dropout=true, Is_causal=true, Is_even_N=true, Is_even_K=false, Return_softmax=false]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(34): here
            instantiation of "void run_flash_fwd<Kernel_traits,Is_dropout,Is_causal>(Flash_fwd_params &, cudaStream_t) [with Kernel_traits=Flash_fwd_kernel_traits<128, 128, 32, 4, false, false, cutlass::half_t, Flash_kernel_traits<128, 128, 32, 4, cutlass::half_t>>, Is_dropout=true, Is_causal=true]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(118): here
            instantiation of "void run_mha_fwd_hdim128<T>(Flash_fwd_params &, cudaStream_t) [with T=cutlass::half_t]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_hdim128_fp16_sm80.cu(31): here

/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_kernel.h(140): warning: variable "MMA_M" was declared but never referenced
          detected during:
            instantiation of "void flash::compute_attn_1rowblock<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params,Is_Q_split_K>(const Params &, int, int, int) [with Kernel_traits=Flash_fwd_kernel_traits<128, 128, 32, 4, false, false, cutlass::half_t, Flash_kernel_traits<128, 128, 32, 4, cutlass::half_t>>, Is_dropout=true, Is_causal=true, Is_even_N=false, Is_even_K=true, Return_softmax=true, Params=Flash_fwd_params, Is_Q_split_K=false]" 
(589): here
            instantiation of "void flash::compute_attn<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &) [with Kernel_traits=Flash_fwd_kernel_traits<128, 128, 32, 4, false, false, cutlass::half_t, Flash_kernel_traits<128, 128, 32, 4, cutlass::half_t>>, Is_dropout=true, Is_causal=true, Is_even_N=false, Is_even_K=true, Return_softmax=true, Params=Flash_fwd_params]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(15): here
            instantiation of "void flash_fwd_kernel<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax>(Flash_fwd_params) [with Kernel_traits=Flash_fwd_kernel_traits<128, 128, 32, 4, false, false, cutlass::half_t, Flash_kernel_traits<128, 128, 32, 4, cutlass::half_t>>, Is_dropout=true, Is_causal=true, Is_even_N=false, Is_even_K=true, Return_softmax=true]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(34): here
            instantiation of "void run_flash_fwd<Kernel_traits,Is_dropout,Is_causal>(Flash_fwd_params &, cudaStream_t) [with Kernel_traits=Flash_fwd_kernel_traits<128, 128, 32, 4, false, false, cutlass::half_t, Flash_kernel_traits<128, 128, 32, 4, cutlass::half_t>>, Is_dropout=true, Is_causal=true]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(118): here
            instantiation of "void run_mha_fwd_hdim128<T>(Flash_fwd_params &, cudaStream_t) [with T=cutlass::half_t]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_hdim128_fp16_sm80.cu(31): here

/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_kernel.h(276): warning: variable "tQrQ" was declared but never referenced
          detected during:
            instantiation of "void flash::compute_attn_1rowblock<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params,Is_Q_split_K>(const Params &, int, int, int) [with Kernel_traits=Flash_fwd_kernel_traits<128, 128, 32, 4, false, false, cutlass::half_t, Flash_kernel_traits<128, 128, 32, 4, cutlass::half_t>>, Is_dropout=true, Is_causal=true, Is_even_N=false, Is_even_K=true, Return_softmax=true, Params=Flash_fwd_params, Is_Q_split_K=false]" 
(589): here
            instantiation of "void flash::compute_attn<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &) [with Kernel_traits=Flash_fwd_kernel_traits<128, 128, 32, 4, false, false, cutlass::half_t, Flash_kernel_traits<128, 128, 32, 4, cutlass::half_t>>, Is_dropout=true, Is_causal=true, Is_even_N=false, Is_even_K=true, Return_softmax=true, Params=Flash_fwd_params]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(15): here
            instantiation of "void flash_fwd_kernel<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax>(Flash_fwd_params) [with Kernel_traits=Flash_fwd_kernel_traits<128, 128, 32, 4, false, false, cutlass::half_t, Flash_kernel_traits<128, 128, 32, 4, cutlass::half_t>>, Is_dropout=true, Is_causal=true, Is_even_N=false, Is_even_K=true, Return_softmax=true]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(34): here
            instantiation of "void run_flash_fwd<Kernel_traits,Is_dropout,Is_causal>(Flash_fwd_params &, cudaStream_t) [with Kernel_traits=Flash_fwd_kernel_traits<128, 128, 32, 4, false, false, cutlass::half_t, Flash_kernel_traits<128, 128, 32, 4, cutlass::half_t>>, Is_dropout=true, Is_causal=true]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(118): here
            instantiation of "void run_mha_fwd_hdim128<T>(Flash_fwd_params &, cudaStream_t) [with T=cutlass::half_t]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_hdim128_fp16_sm80.cu(31): here

/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_kernel.h(140): warning: variable "MMA_M" was declared but never referenced
          detected during:
            instantiation of "void flash::compute_attn_1rowblock<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params,Is_Q_split_K>(const Params &, int, int, int) [with Kernel_traits=Flash_fwd_kernel_traits<128, 128, 32, 4, false, false, cutlass::half_t, Flash_kernel_traits<128, 128, 32, 4, cutlass::half_t>>, Is_dropout=true, Is_causal=true, Is_even_N=false, Is_even_K=true, Return_softmax=false, Params=Flash_fwd_params, Is_Q_split_K=false]" 
(589): here
            instantiation of "void flash::compute_attn<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &) [with Kernel_traits=Flash_fwd_kernel_traits<128, 128, 32, 4, false, false, cutlass::half_t, Flash_kernel_traits<128, 128, 32, 4, cutlass::half_t>>, Is_dropout=true, Is_causal=true, Is_even_N=false, Is_even_K=true, Return_softmax=false, Params=Flash_fwd_params]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(15): here
            instantiation of "void flash_fwd_kernel<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax>(Flash_fwd_params) [with Kernel_traits=Flash_fwd_kernel_traits<128, 128, 32, 4, false, false, cutlass::half_t, Flash_kernel_traits<128, 128, 32, 4, cutlass::half_t>>, Is_dropout=true, Is_causal=true, Is_even_N=false, Is_even_K=true, Return_softmax=false]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(34): here
            instantiation of "void run_flash_fwd<Kernel_traits,Is_dropout,Is_causal>(Flash_fwd_params &, cudaStream_t) [with Kernel_traits=Flash_fwd_kernel_traits<128, 128, 32, 4, false, false, cutlass::half_t, Flash_kernel_traits<128, 128, 32, 4, cutlass::half_t>>, Is_dropout=true, Is_causal=true]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(118): here
            instantiation of "void run_mha_fwd_hdim128<T>(Flash_fwd_params &, cudaStream_t) [with T=cutlass::half_t]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_hdim128_fp16_sm80.cu(31): here

/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_kernel.h(276): warning: variable "tQrQ" was declared but never referenced
          detected during:
            instantiation of "void flash::compute_attn_1rowblock<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params,Is_Q_split_K>(const Params &, int, int, int) [with Kernel_traits=Flash_fwd_kernel_traits<128, 128, 32, 4, false, false, cutlass::half_t, Flash_kernel_traits<128, 128, 32, 4, cutlass::half_t>>, Is_dropout=true, Is_causal=true, Is_even_N=false, Is_even_K=true, Return_softmax=false, Params=Flash_fwd_params, Is_Q_split_K=false]" 
(589): here
            instantiation of "void flash::compute_attn<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &) [with Kernel_traits=Flash_fwd_kernel_traits<128, 128, 32, 4, false, false, cutlass::half_t, Flash_kernel_traits<128, 128, 32, 4, cutlass::half_t>>, Is_dropout=true, Is_causal=true, Is_even_N=false, Is_even_K=true, Return_softmax=false, Params=Flash_fwd_params]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(15): here
            instantiation of "void flash_fwd_kernel<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax>(Flash_fwd_params) [with Kernel_traits=Flash_fwd_kernel_traits<128, 128, 32, 4, false, false, cutlass::half_t, Flash_kernel_traits<128, 128, 32, 4, cutlass::half_t>>, Is_dropout=true, Is_causal=true, Is_even_N=false, Is_even_K=true, Return_softmax=false]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(34): here
            instantiation of "void run_flash_fwd<Kernel_traits,Is_dropout,Is_causal>(Flash_fwd_params &, cudaStream_t) [with Kernel_traits=Flash_fwd_kernel_traits<128, 128, 32, 4, false, false, cutlass::half_t, Flash_kernel_traits<128, 128, 32, 4, cutlass::half_t>>, Is_dropout=true, Is_causal=true]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(118): here
            instantiation of "void run_mha_fwd_hdim128<T>(Flash_fwd_params &, cudaStream_t) [with T=cutlass::half_t]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_hdim128_fp16_sm80.cu(31): here

/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_kernel.h(140): warning: variable "MMA_M" was declared but never referenced
          detected during:
            instantiation of "void flash::compute_attn_1rowblock<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params,Is_Q_split_K>(const Params &, int, int, int) [with Kernel_traits=Flash_fwd_kernel_traits<128, 128, 32, 4, false, false, cutlass::half_t, Flash_kernel_traits<128, 128, 32, 4, cutlass::half_t>>, Is_dropout=true, Is_causal=true, Is_even_N=false, Is_even_K=false, Return_softmax=true, Params=Flash_fwd_params, Is_Q_split_K=false]" 
(589): here
            instantiation of "void flash::compute_attn<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &) [with Kernel_traits=Flash_fwd_kernel_traits<128, 128, 32, 4, false, false, cutlass::half_t, Flash_kernel_traits<128, 128, 32, 4, cutlass::half_t>>, Is_dropout=true, Is_causal=true, Is_even_N=false, Is_even_K=false, Return_softmax=true, Params=Flash_fwd_params]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(15): here
            instantiation of "void flash_fwd_kernel<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax>(Flash_fwd_params) [with Kernel_traits=Flash_fwd_kernel_traits<128, 128, 32, 4, false, false, cutlass::half_t, Flash_kernel_traits<128, 128, 32, 4, cutlass::half_t>>, Is_dropout=true, Is_causal=true, Is_even_N=false, Is_even_K=false, Return_softmax=true]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(34): here
            instantiation of "void run_flash_fwd<Kernel_traits,Is_dropout,Is_causal>(Flash_fwd_params &, cudaStream_t) [with Kernel_traits=Flash_fwd_kernel_traits<128, 128, 32, 4, false, false, cutlass::half_t, Flash_kernel_traits<128, 128, 32, 4, cutlass::half_t>>, Is_dropout=true, Is_causal=true]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(118): here
            instantiation of "void run_mha_fwd_hdim128<T>(Flash_fwd_params &, cudaStream_t) [with T=cutlass::half_t]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_hdim128_fp16_sm80.cu(31): here

/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_kernel.h(276): warning: variable "tQrQ" was declared but never referenced
          detected during:
            instantiation of "void flash::compute_attn_1rowblock<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params,Is_Q_split_K>(const Params &, int, int, int) [with Kernel_traits=Flash_fwd_kernel_traits<128, 128, 32, 4, false, false, cutlass::half_t, Flash_kernel_traits<128, 128, 32, 4, cutlass::half_t>>, Is_dropout=true, Is_causal=true, Is_even_N=false, Is_even_K=false, Return_softmax=true, Params=Flash_fwd_params, Is_Q_split_K=false]" 
(589): here
            instantiation of "void flash::compute_attn<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &) [with Kernel_traits=Flash_fwd_kernel_traits<128, 128, 32, 4, false, false, cutlass::half_t, Flash_kernel_traits<128, 128, 32, 4, cutlass::half_t>>, Is_dropout=true, Is_causal=true, Is_even_N=false, Is_even_K=false, Return_softmax=true, Params=Flash_fwd_params]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(15): here
            instantiation of "void flash_fwd_kernel<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax>(Flash_fwd_params) [with Kernel_traits=Flash_fwd_kernel_traits<128, 128, 32, 4, false, false, cutlass::half_t, Flash_kernel_traits<128, 128, 32, 4, cutlass::half_t>>, Is_dropout=true, Is_causal=true, Is_even_N=false, Is_even_K=false, Return_softmax=true]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(34): here
            instantiation of "void run_flash_fwd<Kernel_traits,Is_dropout,Is_causal>(Flash_fwd_params &, cudaStream_t) [with Kernel_traits=Flash_fwd_kernel_traits<128, 128, 32, 4, false, false, cutlass::half_t, Flash_kernel_traits<128, 128, 32, 4, cutlass::half_t>>, Is_dropout=true, Is_causal=true]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(118): here
            instantiation of "void run_mha_fwd_hdim128<T>(Flash_fwd_params &, cudaStream_t) [with T=cutlass::half_t]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_hdim128_fp16_sm80.cu(31): here

/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_kernel.h(140): warning: variable "MMA_M" was declared but never referenced
          detected during:
            instantiation of "void flash::compute_attn_1rowblock<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params,Is_Q_split_K>(const Params &, int, int, int) [with Kernel_traits=Flash_fwd_kernel_traits<128, 128, 32, 4, false, false, cutlass::half_t, Flash_kernel_traits<128, 128, 32, 4, cutlass::half_t>>, Is_dropout=true, Is_causal=true, Is_even_N=false, Is_even_K=false, Return_softmax=false, Params=Flash_fwd_params, Is_Q_split_K=false]" 
(589): here
            instantiation of "void flash::compute_attn<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &) [with Kernel_traits=Flash_fwd_kernel_traits<128, 128, 32, 4, false, false, cutlass::half_t, Flash_kernel_traits<128, 128, 32, 4, cutlass::half_t>>, Is_dropout=true, Is_causal=true, Is_even_N=false, Is_even_K=false, Return_softmax=false, Params=Flash_fwd_params]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(15): here
            instantiation of "void flash_fwd_kernel<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax>(Flash_fwd_params) [with Kernel_traits=Flash_fwd_kernel_traits<128, 128, 32, 4, false, false, cutlass::half_t, Flash_kernel_traits<128, 128, 32, 4, cutlass::half_t>>, Is_dropout=true, Is_causal=true, Is_even_N=false, Is_even_K=false, Return_softmax=false]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(34): here
            instantiation of "void run_flash_fwd<Kernel_traits,Is_dropout,Is_causal>(Flash_fwd_params &, cudaStream_t) [with Kernel_traits=Flash_fwd_kernel_traits<128, 128, 32, 4, false, false, cutlass::half_t, Flash_kernel_traits<128, 128, 32, 4, cutlass::half_t>>, Is_dropout=true, Is_causal=true]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(118): here
            instantiation of "void run_mha_fwd_hdim128<T>(Flash_fwd_params &, cudaStream_t) [with T=cutlass::half_t]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_hdim128_fp16_sm80.cu(31): here

/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_kernel.h(276): warning: variable "tQrQ" was declared but never referenced
          detected during:
            instantiation of "void flash::compute_attn_1rowblock<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params,Is_Q_split_K>(const Params &, int, int, int) [with Kernel_traits=Flash_fwd_kernel_traits<128, 128, 32, 4, false, false, cutlass::half_t, Flash_kernel_traits<128, 128, 32, 4, cutlass::half_t>>, Is_dropout=true, Is_causal=true, Is_even_N=false, Is_even_K=false, Return_softmax=false, Params=Flash_fwd_params, Is_Q_split_K=false]" 
(589): here
            instantiation of "void flash::compute_attn<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &) [with Kernel_traits=Flash_fwd_kernel_traits<128, 128, 32, 4, false, false, cutlass::half_t, Flash_kernel_traits<128, 128, 32, 4, cutlass::half_t>>, Is_dropout=true, Is_causal=true, Is_even_N=false, Is_even_K=false, Return_softmax=false, Params=Flash_fwd_params]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(15): here
            instantiation of "void flash_fwd_kernel<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax>(Flash_fwd_params) [with Kernel_traits=Flash_fwd_kernel_traits<128, 128, 32, 4, false, false, cutlass::half_t, Flash_kernel_traits<128, 128, 32, 4, cutlass::half_t>>, Is_dropout=true, Is_causal=true, Is_even_N=false, Is_even_K=false, Return_softmax=false]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(34): here
            instantiation of "void run_flash_fwd<Kernel_traits,Is_dropout,Is_causal>(Flash_fwd_params &, cudaStream_t) [with Kernel_traits=Flash_fwd_kernel_traits<128, 128, 32, 4, false, false, cutlass::half_t, Flash_kernel_traits<128, 128, 32, 4, cutlass::half_t>>, Is_dropout=true, Is_causal=true]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(118): here
            instantiation of "void run_mha_fwd_hdim128<T>(Flash_fwd_params &, cudaStream_t) [with T=cutlass::half_t]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_hdim128_fp16_sm80.cu(31): here

/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_kernel.h(140): warning: variable "MMA_M" was declared but never referenced
          detected during:
            instantiation of "void flash::compute_attn_1rowblock<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params,Is_Q_split_K>(const Params &, int, int, int) [with Kernel_traits=Flash_fwd_kernel_traits<128, 128, 32, 4, false, false, cutlass::half_t, Flash_kernel_traits<128, 128, 32, 4, cutlass::half_t>>, Is_dropout=true, Is_causal=false, Is_even_N=true, Is_even_K=true, Return_softmax=true, Params=Flash_fwd_params, Is_Q_split_K=false]" 
(589): here
            instantiation of "void flash::compute_attn<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &) [with Kernel_traits=Flash_fwd_kernel_traits<128, 128, 32, 4, false, false, cutlass::half_t, Flash_kernel_traits<128, 128, 32, 4, cutlass::half_t>>, Is_dropout=true, Is_causal=false, Is_even_N=true, Is_even_K=true, Return_softmax=true, Params=Flash_fwd_params]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(15): here
            instantiation of "void flash_fwd_kernel<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax>(Flash_fwd_params) [with Kernel_traits=Flash_fwd_kernel_traits<128, 128, 32, 4, false, false, cutlass::half_t, Flash_kernel_traits<128, 128, 32, 4, cutlass::half_t>>, Is_dropout=true, Is_causal=false, Is_even_N=true, Is_even_K=true, Return_softmax=true]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(34): here
            instantiation of "void run_flash_fwd<Kernel_traits,Is_dropout,Is_causal>(Flash_fwd_params &, cudaStream_t) [with Kernel_traits=Flash_fwd_kernel_traits<128, 128, 32, 4, false, false, cutlass::half_t, Flash_kernel_traits<128, 128, 32, 4, cutlass::half_t>>, Is_dropout=true, Is_causal=false]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(118): here
            instantiation of "void run_mha_fwd_hdim128<T>(Flash_fwd_params &, cudaStream_t) [with T=cutlass::half_t]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_hdim128_fp16_sm80.cu(31): here

/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_kernel.h(276): warning: variable "tQrQ" was declared but never referenced
          detected during:
            instantiation of "void flash::compute_attn_1rowblock<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params,Is_Q_split_K>(const Params &, int, int, int) [with Kernel_traits=Flash_fwd_kernel_traits<128, 128, 32, 4, false, false, cutlass::half_t, Flash_kernel_traits<128, 128, 32, 4, cutlass::half_t>>, Is_dropout=true, Is_causal=false, Is_even_N=true, Is_even_K=true, Return_softmax=true, Params=Flash_fwd_params, Is_Q_split_K=false]" 
(589): here
            instantiation of "void flash::compute_attn<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &) [with Kernel_traits=Flash_fwd_kernel_traits<128, 128, 32, 4, false, false, cutlass::half_t, Flash_kernel_traits<128, 128, 32, 4, cutlass::half_t>>, Is_dropout=true, Is_causal=false, Is_even_N=true, Is_even_K=true, Return_softmax=true, Params=Flash_fwd_params]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(15): here
            instantiation of "void flash_fwd_kernel<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax>(Flash_fwd_params) [with Kernel_traits=Flash_fwd_kernel_traits<128, 128, 32, 4, false, false, cutlass::half_t, Flash_kernel_traits<128, 128, 32, 4, cutlass::half_t>>, Is_dropout=true, Is_causal=false, Is_even_N=true, Is_even_K=true, Return_softmax=true]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(34): here
            instantiation of "void run_flash_fwd<Kernel_traits,Is_dropout,Is_causal>(Flash_fwd_params &, cudaStream_t) [with Kernel_traits=Flash_fwd_kernel_traits<128, 128, 32, 4, false, false, cutlass::half_t, Flash_kernel_traits<128, 128, 32, 4, cutlass::half_t>>, Is_dropout=true, Is_causal=false]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(118): here
            instantiation of "void run_mha_fwd_hdim128<T>(Flash_fwd_params &, cudaStream_t) [with T=cutlass::half_t]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_hdim128_fp16_sm80.cu(31): here

/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_kernel.h(140): warning: variable "MMA_M" was declared but never referenced
          detected during:
            instantiation of "void flash::compute_attn_1rowblock<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params,Is_Q_split_K>(const Params &, int, int, int) [with Kernel_traits=Flash_fwd_kernel_traits<128, 128, 32, 4, false, false, cutlass::half_t, Flash_kernel_traits<128, 128, 32, 4, cutlass::half_t>>, Is_dropout=true, Is_causal=false, Is_even_N=true, Is_even_K=true, Return_softmax=false, Params=Flash_fwd_params, Is_Q_split_K=false]" 
(589): here
            instantiation of "void flash::compute_attn<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &) [with Kernel_traits=Flash_fwd_kernel_traits<128, 128, 32, 4, false, false, cutlass::half_t, Flash_kernel_traits<128, 128, 32, 4, cutlass::half_t>>, Is_dropout=true, Is_causal=false, Is_even_N=true, Is_even_K=true, Return_softmax=false, Params=Flash_fwd_params]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(15): here
            instantiation of "void flash_fwd_kernel<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax>(Flash_fwd_params) [with Kernel_traits=Flash_fwd_kernel_traits<128, 128, 32, 4, false, false, cutlass::half_t, Flash_kernel_traits<128, 128, 32, 4, cutlass::half_t>>, Is_dropout=true, Is_causal=false, Is_even_N=true, Is_even_K=true, Return_softmax=false]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(34): here
            instantiation of "void run_flash_fwd<Kernel_traits,Is_dropout,Is_causal>(Flash_fwd_params &, cudaStream_t) [with Kernel_traits=Flash_fwd_kernel_traits<128, 128, 32, 4, false, false, cutlass::half_t, Flash_kernel_traits<128, 128, 32, 4, cutlass::half_t>>, Is_dropout=true, Is_causal=false]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(118): here
            instantiation of "void run_mha_fwd_hdim128<T>(Flash_fwd_params &, cudaStream_t) [with T=cutlass::half_t]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_hdim128_fp16_sm80.cu(31): here

/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_kernel.h(276): warning: variable "tQrQ" was declared but never referenced
          detected during:
            instantiation of "void flash::compute_attn_1rowblock<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params,Is_Q_split_K>(const Params &, int, int, int) [with Kernel_traits=Flash_fwd_kernel_traits<128, 128, 32, 4, false, false, cutlass::half_t, Flash_kernel_traits<128, 128, 32, 4, cutlass::half_t>>, Is_dropout=true, Is_causal=false, Is_even_N=true, Is_even_K=true, Return_softmax=false, Params=Flash_fwd_params, Is_Q_split_K=false]" 
(589): here
            instantiation of "void flash::compute_attn<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &) [with Kernel_traits=Flash_fwd_kernel_traits<128, 128, 32, 4, false, false, cutlass::half_t, Flash_kernel_traits<128, 128, 32, 4, cutlass::half_t>>, Is_dropout=true, Is_causal=false, Is_even_N=true, Is_even_K=true, Return_softmax=false, Params=Flash_fwd_params]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(15): here
            instantiation of "void flash_fwd_kernel<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax>(Flash_fwd_params) [with Kernel_traits=Flash_fwd_kernel_traits<128, 128, 32, 4, false, false, cutlass::half_t, Flash_kernel_traits<128, 128, 32, 4, cutlass::half_t>>, Is_dropout=true, Is_causal=false, Is_even_N=true, Is_even_K=true, Return_softmax=false]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(34): here
            instantiation of "void run_flash_fwd<Kernel_traits,Is_dropout,Is_causal>(Flash_fwd_params &, cudaStream_t) [with Kernel_traits=Flash_fwd_kernel_traits<128, 128, 32, 4, false, false, cutlass::half_t, Flash_kernel_traits<128, 128, 32, 4, cutlass::half_t>>, Is_dropout=true, Is_causal=false]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(118): here
            instantiation of "void run_mha_fwd_hdim128<T>(Flash_fwd_params &, cudaStream_t) [with T=cutlass::half_t]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_hdim128_fp16_sm80.cu(31): here

/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_kernel.h(140): warning: variable "MMA_M" was declared but never referenced
          detected during:
            instantiation of "void flash::compute_attn_1rowblock<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params,Is_Q_split_K>(const Params &, int, int, int) [with Kernel_traits=Flash_fwd_kernel_traits<128, 128, 32, 4, false, false, cutlass::half_t, Flash_kernel_traits<128, 128, 32, 4, cutlass::half_t>>, Is_dropout=true, Is_causal=false, Is_even_N=true, Is_even_K=false, Return_softmax=true, Params=Flash_fwd_params, Is_Q_split_K=false]" 
(589): here
            instantiation of "void flash::compute_attn<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &) [with Kernel_traits=Flash_fwd_kernel_traits<128, 128, 32, 4, false, false, cutlass::half_t, Flash_kernel_traits<128, 128, 32, 4, cutlass::half_t>>, Is_dropout=true, Is_causal=false, Is_even_N=true, Is_even_K=false, Return_softmax=true, Params=Flash_fwd_params]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(15): here
            instantiation of "void flash_fwd_kernel<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax>(Flash_fwd_params) [with Kernel_traits=Flash_fwd_kernel_traits<128, 128, 32, 4, false, false, cutlass::half_t, Flash_kernel_traits<128, 128, 32, 4, cutlass::half_t>>, Is_dropout=true, Is_causal=false, Is_even_N=true, Is_even_K=false, Return_softmax=true]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(34): here
            instantiation of "void run_flash_fwd<Kernel_traits,Is_dropout,Is_causal>(Flash_fwd_params &, cudaStream_t) [with Kernel_traits=Flash_fwd_kernel_traits<128, 128, 32, 4, false, false, cutlass::half_t, Flash_kernel_traits<128, 128, 32, 4, cutlass::half_t>>, Is_dropout=true, Is_causal=false]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(118): here
            instantiation of "void run_mha_fwd_hdim128<T>(Flash_fwd_params &, cudaStream_t) [with T=cutlass::half_t]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_hdim128_fp16_sm80.cu(31): here

/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_kernel.h(276): warning: variable "tQrQ" was declared but never referenced
          detected during:
            instantiation of "void flash::compute_attn_1rowblock<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params,Is_Q_split_K>(const Params &, int, int, int) [with Kernel_traits=Flash_fwd_kernel_traits<128, 128, 32, 4, false, false, cutlass::half_t, Flash_kernel_traits<128, 128, 32, 4, cutlass::half_t>>, Is_dropout=true, Is_causal=false, Is_even_N=true, Is_even_K=false, Return_softmax=true, Params=Flash_fwd_params, Is_Q_split_K=false]" 
(589): here
            instantiation of "void flash::compute_attn<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &) [with Kernel_traits=Flash_fwd_kernel_traits<128, 128, 32, 4, false, false, cutlass::half_t, Flash_kernel_traits<128, 128, 32, 4, cutlass::half_t>>, Is_dropout=true, Is_causal=false, Is_even_N=true, Is_even_K=false, Return_softmax=true, Params=Flash_fwd_params]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(15): here
            instantiation of "void flash_fwd_kernel<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax>(Flash_fwd_params) [with Kernel_traits=Flash_fwd_kernel_traits<128, 128, 32, 4, false, false, cutlass::half_t, Flash_kernel_traits<128, 128, 32, 4, cutlass::half_t>>, Is_dropout=true, Is_causal=false, Is_even_N=true, Is_even_K=false, Return_softmax=true]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(34): here
            instantiation of "void run_flash_fwd<Kernel_traits,Is_dropout,Is_causal>(Flash_fwd_params &, cudaStream_t) [with Kernel_traits=Flash_fwd_kernel_traits<128, 128, 32, 4, false, false, cutlass::half_t, Flash_kernel_traits<128, 128, 32, 4, cutlass::half_t>>, Is_dropout=true, Is_causal=false]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(118): here
            instantiation of "void run_mha_fwd_hdim128<T>(Flash_fwd_params &, cudaStream_t) [with T=cutlass::half_t]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_hdim128_fp16_sm80.cu(31): here

/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_kernel.h(140): warning: variable "MMA_M" was declared but never referenced
          detected during:
            instantiation of "void flash::compute_attn_1rowblock<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params,Is_Q_split_K>(const Params &, int, int, int) [with Kernel_traits=Flash_fwd_kernel_traits<128, 128, 32, 4, false, false, cutlass::half_t, Flash_kernel_traits<128, 128, 32, 4, cutlass::half_t>>, Is_dropout=true, Is_causal=false, Is_even_N=true, Is_even_K=false, Return_softmax=false, Params=Flash_fwd_params, Is_Q_split_K=false]" 
(589): here
            instantiation of "void flash::compute_attn<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &) [with Kernel_traits=Flash_fwd_kernel_traits<128, 128, 32, 4, false, false, cutlass::half_t, Flash_kernel_traits<128, 128, 32, 4, cutlass::half_t>>, Is_dropout=true, Is_causal=false, Is_even_N=true, Is_even_K=false, Return_softmax=false, Params=Flash_fwd_params]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(15): here
            instantiation of "void flash_fwd_kernel<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax>(Flash_fwd_params) [with Kernel_traits=Flash_fwd_kernel_traits<128, 128, 32, 4, false, false, cutlass::half_t, Flash_kernel_traits<128, 128, 32, 4, cutlass::half_t>>, Is_dropout=true, Is_causal=false, Is_even_N=true, Is_even_K=false, Return_softmax=false]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(34): here
            instantiation of "void run_flash_fwd<Kernel_traits,Is_dropout,Is_causal>(Flash_fwd_params &, cudaStream_t) [with Kernel_traits=Flash_fwd_kernel_traits<128, 128, 32, 4, false, false, cutlass::half_t, Flash_kernel_traits<128, 128, 32, 4, cutlass::half_t>>, Is_dropout=true, Is_causal=false]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(118): here
            instantiation of "void run_mha_fwd_hdim128<T>(Flash_fwd_params &, cudaStream_t) [with T=cutlass::half_t]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_hdim128_fp16_sm80.cu(31): here

/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_kernel.h(276): warning: variable "tQrQ" was declared but never referenced
          detected during:
            instantiation of "void flash::compute_attn_1rowblock<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params,Is_Q_split_K>(const Params &, int, int, int) [with Kernel_traits=Flash_fwd_kernel_traits<128, 128, 32, 4, false, false, cutlass::half_t, Flash_kernel_traits<128, 128, 32, 4, cutlass::half_t>>, Is_dropout=true, Is_causal=false, Is_even_N=true, Is_even_K=false, Return_softmax=false, Params=Flash_fwd_params, Is_Q_split_K=false]" 
(589): here
            instantiation of "void flash::compute_attn<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &) [with Kernel_traits=Flash_fwd_kernel_traits<128, 128, 32, 4, false, false, cutlass::half_t, Flash_kernel_traits<128, 128, 32, 4, cutlass::half_t>>, Is_dropout=true, Is_causal=false, Is_even_N=true, Is_even_K=false, Return_softmax=false, Params=Flash_fwd_params]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(15): here
            instantiation of "void flash_fwd_kernel<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax>(Flash_fwd_params) [with Kernel_traits=Flash_fwd_kernel_traits<128, 128, 32, 4, false, false, cutlass::half_t, Flash_kernel_traits<128, 128, 32, 4, cutlass::half_t>>, Is_dropout=true, Is_causal=false, Is_even_N=true, Is_even_K=false, Return_softmax=false]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(34): here
            instantiation of "void run_flash_fwd<Kernel_traits,Is_dropout,Is_causal>(Flash_fwd_params &, cudaStream_t) [with Kernel_traits=Flash_fwd_kernel_traits<128, 128, 32, 4, false, false, cutlass::half_t, Flash_kernel_traits<128, 128, 32, 4, cutlass::half_t>>, Is_dropout=true, Is_causal=false]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(118): here
            instantiation of "void run_mha_fwd_hdim128<T>(Flash_fwd_params &, cudaStream_t) [with T=cutlass::half_t]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_hdim128_fp16_sm80.cu(31): here

/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_kernel.h(140): warning: variable "MMA_M" was declared but never referenced
          detected during:
            instantiation of "void flash::compute_attn_1rowblock<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params,Is_Q_split_K>(const Params &, int, int, int) [with Kernel_traits=Flash_fwd_kernel_traits<128, 128, 32, 4, false, false, cutlass::half_t, Flash_kernel_traits<128, 128, 32, 4, cutlass::half_t>>, Is_dropout=true, Is_causal=false, Is_even_N=false, Is_even_K=true, Return_softmax=true, Params=Flash_fwd_params, Is_Q_split_K=false]" 
(589): here
            instantiation of "void flash::compute_attn<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &) [with Kernel_traits=Flash_fwd_kernel_traits<128, 128, 32, 4, false, false, cutlass::half_t, Flash_kernel_traits<128, 128, 32, 4, cutlass::half_t>>, Is_dropout=true, Is_causal=false, Is_even_N=false, Is_even_K=true, Return_softmax=true, Params=Flash_fwd_params]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(15): here
            instantiation of "void flash_fwd_kernel<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax>(Flash_fwd_params) [with Kernel_traits=Flash_fwd_kernel_traits<128, 128, 32, 4, false, false, cutlass::half_t, Flash_kernel_traits<128, 128, 32, 4, cutlass::half_t>>, Is_dropout=true, Is_causal=false, Is_even_N=false, Is_even_K=true, Return_softmax=true]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(34): here
            instantiation of "void run_flash_fwd<Kernel_traits,Is_dropout,Is_causal>(Flash_fwd_params &, cudaStream_t) [with Kernel_traits=Flash_fwd_kernel_traits<128, 128, 32, 4, false, false, cutlass::half_t, Flash_kernel_traits<128, 128, 32, 4, cutlass::half_t>>, Is_dropout=true, Is_causal=false]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(118): here
            instantiation of "void run_mha_fwd_hdim128<T>(Flash_fwd_params &, cudaStream_t) [with T=cutlass::half_t]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_hdim128_fp16_sm80.cu(31): here

/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_kernel.h(276): warning: variable "tQrQ" was declared but never referenced
          detected during:
            instantiation of "void flash::compute_attn_1rowblock<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params,Is_Q_split_K>(const Params &, int, int, int) [with Kernel_traits=Flash_fwd_kernel_traits<128, 128, 32, 4, false, false, cutlass::half_t, Flash_kernel_traits<128, 128, 32, 4, cutlass::half_t>>, Is_dropout=true, Is_causal=false, Is_even_N=false, Is_even_K=true, Return_softmax=true, Params=Flash_fwd_params, Is_Q_split_K=false]" 
(589): here
            instantiation of "void flash::compute_attn<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &) [with Kernel_traits=Flash_fwd_kernel_traits<128, 128, 32, 4, false, false, cutlass::half_t, Flash_kernel_traits<128, 128, 32, 4, cutlass::half_t>>, Is_dropout=true, Is_causal=false, Is_even_N=false, Is_even_K=true, Return_softmax=true, Params=Flash_fwd_params]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(15): here
            instantiation of "void flash_fwd_kernel<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax>(Flash_fwd_params) [with Kernel_traits=Flash_fwd_kernel_traits<128, 128, 32, 4, false, false, cutlass::half_t, Flash_kernel_traits<128, 128, 32, 4, cutlass::half_t>>, Is_dropout=true, Is_causal=false, Is_even_N=false, Is_even_K=true, Return_softmax=true]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(34): here
            instantiation of "void run_flash_fwd<Kernel_traits,Is_dropout,Is_causal>(Flash_fwd_params &, cudaStream_t) [with Kernel_traits=Flash_fwd_kernel_traits<128, 128, 32, 4, false, false, cutlass::half_t, Flash_kernel_traits<128, 128, 32, 4, cutlass::half_t>>, Is_dropout=true, Is_causal=false]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(118): here
            instantiation of "void run_mha_fwd_hdim128<T>(Flash_fwd_params &, cudaStream_t) [with T=cutlass::half_t]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_hdim128_fp16_sm80.cu(31): here

/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_kernel.h(140): warning: variable "MMA_M" was declared but never referenced
          detected during:
            instantiation of "void flash::compute_attn_1rowblock<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params,Is_Q_split_K>(const Params &, int, int, int) [with Kernel_traits=Flash_fwd_kernel_traits<128, 128, 32, 4, false, false, cutlass::half_t, Flash_kernel_traits<128, 128, 32, 4, cutlass::half_t>>, Is_dropout=true, Is_causal=false, Is_even_N=false, Is_even_K=true, Return_softmax=false, Params=Flash_fwd_params, Is_Q_split_K=false]" 
(589): here
            instantiation of "void flash::compute_attn<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &) [with Kernel_traits=Flash_fwd_kernel_traits<128, 128, 32, 4, false, false, cutlass::half_t, Flash_kernel_traits<128, 128, 32, 4, cutlass::half_t>>, Is_dropout=true, Is_causal=false, Is_even_N=false, Is_even_K=true, Return_softmax=false, Params=Flash_fwd_params]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(15): here
            instantiation of "void flash_fwd_kernel<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax>(Flash_fwd_params) [with Kernel_traits=Flash_fwd_kernel_traits<128, 128, 32, 4, false, false, cutlass::half_t, Flash_kernel_traits<128, 128, 32, 4, cutlass::half_t>>, Is_dropout=true, Is_causal=false, Is_even_N=false, Is_even_K=true, Return_softmax=false]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(34): here
            instantiation of "void run_flash_fwd<Kernel_traits,Is_dropout,Is_causal>(Flash_fwd_params &, cudaStream_t) [with Kernel_traits=Flash_fwd_kernel_traits<128, 128, 32, 4, false, false, cutlass::half_t, Flash_kernel_traits<128, 128, 32, 4, cutlass::half_t>>, Is_dropout=true, Is_causal=false]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(118): here
            instantiation of "void run_mha_fwd_hdim128<T>(Flash_fwd_params &, cudaStream_t) [with T=cutlass::half_t]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_hdim128_fp16_sm80.cu(31): here

/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_kernel.h(276): warning: variable "tQrQ" was declared but never referenced
          detected during:
            instantiation of "void flash::compute_attn_1rowblock<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params,Is_Q_split_K>(const Params &, int, int, int) [with Kernel_traits=Flash_fwd_kernel_traits<128, 128, 32, 4, false, false, cutlass::half_t, Flash_kernel_traits<128, 128, 32, 4, cutlass::half_t>>, Is_dropout=true, Is_causal=false, Is_even_N=false, Is_even_K=true, Return_softmax=false, Params=Flash_fwd_params, Is_Q_split_K=false]" 
(589): here
            instantiation of "void flash::compute_attn<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &) [with Kernel_traits=Flash_fwd_kernel_traits<128, 128, 32, 4, false, false, cutlass::half_t, Flash_kernel_traits<128, 128, 32, 4, cutlass::half_t>>, Is_dropout=true, Is_causal=false, Is_even_N=false, Is_even_K=true, Return_softmax=false, Params=Flash_fwd_params]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(15): here
            instantiation of "void flash_fwd_kernel<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax>(Flash_fwd_params) [with Kernel_traits=Flash_fwd_kernel_traits<128, 128, 32, 4, false, false, cutlass::half_t, Flash_kernel_traits<128, 128, 32, 4, cutlass::half_t>>, Is_dropout=true, Is_causal=false, Is_even_N=false, Is_even_K=true, Return_softmax=false]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(34): here
            instantiation of "void run_flash_fwd<Kernel_traits,Is_dropout,Is_causal>(Flash_fwd_params &, cudaStream_t) [with Kernel_traits=Flash_fwd_kernel_traits<128, 128, 32, 4, false, false, cutlass::half_t, Flash_kernel_traits<128, 128, 32, 4, cutlass::half_t>>, Is_dropout=true, Is_causal=false]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(118): here
            instantiation of "void run_mha_fwd_hdim128<T>(Flash_fwd_params &, cudaStream_t) [with T=cutlass::half_t]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_hdim128_fp16_sm80.cu(31): here

/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_kernel.h(140): warning: variable "MMA_M" was declared but never referenced
          detected during:
            instantiation of "void flash::compute_attn_1rowblock<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params,Is_Q_split_K>(const Params &, int, int, int) [with Kernel_traits=Flash_fwd_kernel_traits<128, 128, 32, 4, false, false, cutlass::half_t, Flash_kernel_traits<128, 128, 32, 4, cutlass::half_t>>, Is_dropout=true, Is_causal=false, Is_even_N=false, Is_even_K=false, Return_softmax=true, Params=Flash_fwd_params, Is_Q_split_K=false]" 
(589): here
            instantiation of "void flash::compute_attn<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &) [with Kernel_traits=Flash_fwd_kernel_traits<128, 128, 32, 4, false, false, cutlass::half_t, Flash_kernel_traits<128, 128, 32, 4, cutlass::half_t>>, Is_dropout=true, Is_causal=false, Is_even_N=false, Is_even_K=false, Return_softmax=true, Params=Flash_fwd_params]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(15): here
            instantiation of "void flash_fwd_kernel<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax>(Flash_fwd_params) [with Kernel_traits=Flash_fwd_kernel_traits<128, 128, 32, 4, false, false, cutlass::half_t, Flash_kernel_traits<128, 128, 32, 4, cutlass::half_t>>, Is_dropout=true, Is_causal=false, Is_even_N=false, Is_even_K=false, Return_softmax=true]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(34): here
            instantiation of "void run_flash_fwd<Kernel_traits,Is_dropout,Is_causal>(Flash_fwd_params &, cudaStream_t) [with Kernel_traits=Flash_fwd_kernel_traits<128, 128, 32, 4, false, false, cutlass::half_t, Flash_kernel_traits<128, 128, 32, 4, cutlass::half_t>>, Is_dropout=true, Is_causal=false]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(118): here
            instantiation of "void run_mha_fwd_hdim128<T>(Flash_fwd_params &, cudaStream_t) [with T=cutlass::half_t]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_hdim128_fp16_sm80.cu(31): here

/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_kernel.h(276): warning: variable "tQrQ" was declared but never referenced
          detected during:
            instantiation of "void flash::compute_attn_1rowblock<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params,Is_Q_split_K>(const Params &, int, int, int) [with Kernel_traits=Flash_fwd_kernel_traits<128, 128, 32, 4, false, false, cutlass::half_t, Flash_kernel_traits<128, 128, 32, 4, cutlass::half_t>>, Is_dropout=true, Is_causal=false, Is_even_N=false, Is_even_K=false, Return_softmax=true, Params=Flash_fwd_params, Is_Q_split_K=false]" 
(589): here
            instantiation of "void flash::compute_attn<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &) [with Kernel_traits=Flash_fwd_kernel_traits<128, 128, 32, 4, false, false, cutlass::half_t, Flash_kernel_traits<128, 128, 32, 4, cutlass::half_t>>, Is_dropout=true, Is_causal=false, Is_even_N=false, Is_even_K=false, Return_softmax=true, Params=Flash_fwd_params]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(15): here
            instantiation of "void flash_fwd_kernel<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax>(Flash_fwd_params) [with Kernel_traits=Flash_fwd_kernel_traits<128, 128, 32, 4, false, false, cutlass::half_t, Flash_kernel_traits<128, 128, 32, 4, cutlass::half_t>>, Is_dropout=true, Is_causal=false, Is_even_N=false, Is_even_K=false, Return_softmax=true]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(34): here
            instantiation of "void run_flash_fwd<Kernel_traits,Is_dropout,Is_causal>(Flash_fwd_params &, cudaStream_t) [with Kernel_traits=Flash_fwd_kernel_traits<128, 128, 32, 4, false, false, cutlass::half_t, Flash_kernel_traits<128, 128, 32, 4, cutlass::half_t>>, Is_dropout=true, Is_causal=false]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(118): here
            instantiation of "void run_mha_fwd_hdim128<T>(Flash_fwd_params &, cudaStream_t) [with T=cutlass::half_t]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_hdim128_fp16_sm80.cu(31): here

/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_kernel.h(140): warning: variable "MMA_M" was declared but never referenced
          detected during:
            instantiation of "void flash::compute_attn_1rowblock<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params,Is_Q_split_K>(const Params &, int, int, int) [with Kernel_traits=Flash_fwd_kernel_traits<128, 128, 32, 4, false, false, cutlass::half_t, Flash_kernel_traits<128, 128, 32, 4, cutlass::half_t>>, Is_dropout=true, Is_causal=false, Is_even_N=false, Is_even_K=false, Return_softmax=false, Params=Flash_fwd_params, Is_Q_split_K=false]" 
(589): here
            instantiation of "void flash::compute_attn<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &) [with Kernel_traits=Flash_fwd_kernel_traits<128, 128, 32, 4, false, false, cutlass::half_t, Flash_kernel_traits<128, 128, 32, 4, cutlass::half_t>>, Is_dropout=true, Is_causal=false, Is_even_N=false, Is_even_K=false, Return_softmax=false, Params=Flash_fwd_params]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(15): here
            instantiation of "void flash_fwd_kernel<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax>(Flash_fwd_params) [with Kernel_traits=Flash_fwd_kernel_traits<128, 128, 32, 4, false, false, cutlass::half_t, Flash_kernel_traits<128, 128, 32, 4, cutlass::half_t>>, Is_dropout=true, Is_causal=false, Is_even_N=false, Is_even_K=false, Return_softmax=false]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(34): here
            instantiation of "void run_flash_fwd<Kernel_traits,Is_dropout,Is_causal>(Flash_fwd_params &, cudaStream_t) [with Kernel_traits=Flash_fwd_kernel_traits<128, 128, 32, 4, false, false, cutlass::half_t, Flash_kernel_traits<128, 128, 32, 4, cutlass::half_t>>, Is_dropout=true, Is_causal=false]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(118): here
            instantiation of "void run_mha_fwd_hdim128<T>(Flash_fwd_params &, cudaStream_t) [with T=cutlass::half_t]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_hdim128_fp16_sm80.cu(31): here

/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_kernel.h(276): warning: variable "tQrQ" was declared but never referenced
          detected during:
            instantiation of "void flash::compute_attn_1rowblock<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params,Is_Q_split_K>(const Params &, int, int, int) [with Kernel_traits=Flash_fwd_kernel_traits<128, 128, 32, 4, false, false, cutlass::half_t, Flash_kernel_traits<128, 128, 32, 4, cutlass::half_t>>, Is_dropout=true, Is_causal=false, Is_even_N=false, Is_even_K=false, Return_softmax=false, Params=Flash_fwd_params, Is_Q_split_K=false]" 
(589): here
            instantiation of "void flash::compute_attn<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &) [with Kernel_traits=Flash_fwd_kernel_traits<128, 128, 32, 4, false, false, cutlass::half_t, Flash_kernel_traits<128, 128, 32, 4, cutlass::half_t>>, Is_dropout=true, Is_causal=false, Is_even_N=false, Is_even_K=false, Return_softmax=false, Params=Flash_fwd_params]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(15): here
            instantiation of "void flash_fwd_kernel<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax>(Flash_fwd_params) [with Kernel_traits=Flash_fwd_kernel_traits<128, 128, 32, 4, false, false, cutlass::half_t, Flash_kernel_traits<128, 128, 32, 4, cutlass::half_t>>, Is_dropout=true, Is_causal=false, Is_even_N=false, Is_even_K=false, Return_softmax=false]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(34): here
            instantiation of "void run_flash_fwd<Kernel_traits,Is_dropout,Is_causal>(Flash_fwd_params &, cudaStream_t) [with Kernel_traits=Flash_fwd_kernel_traits<128, 128, 32, 4, false, false, cutlass::half_t, Flash_kernel_traits<128, 128, 32, 4, cutlass::half_t>>, Is_dropout=true, Is_causal=false]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(118): here
            instantiation of "void run_mha_fwd_hdim128<T>(Flash_fwd_params &, cudaStream_t) [with T=cutlass::half_t]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_hdim128_fp16_sm80.cu(31): here

/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_kernel.h(140): warning: variable "MMA_M" was declared but never referenced
          detected during:
            instantiation of "void flash::compute_attn_1rowblock<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params,Is_Q_split_K>(const Params &, int, int, int) [with Kernel_traits=Flash_fwd_kernel_traits<128, 64, 64, 4, false, false, cutlass::half_t, Flash_kernel_traits<128, 64, 64, 4, cutlass::half_t>>, Is_dropout=false, Is_causal=true, Is_even_N=true, Is_even_K=true, Return_softmax=false, Params=Flash_fwd_params, Is_Q_split_K=false]" 
(589): here
            instantiation of "void flash::compute_attn<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &) [with Kernel_traits=Flash_fwd_kernel_traits<128, 64, 64, 4, false, false, cutlass::half_t, Flash_kernel_traits<128, 64, 64, 4, cutlass::half_t>>, Is_dropout=false, Is_causal=true, Is_even_N=true, Is_even_K=true, Return_softmax=false, Params=Flash_fwd_params]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(15): here
            instantiation of "void flash_fwd_kernel<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax>(Flash_fwd_params) [with Kernel_traits=Flash_fwd_kernel_traits<128, 64, 64, 4, false, false, cutlass::half_t, Flash_kernel_traits<128, 64, 64, 4, cutlass::half_t>>, Is_dropout=false, Is_causal=true, Is_even_N=true, Is_even_K=true, Return_softmax=false]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(34): here
            instantiation of "void run_flash_fwd<Kernel_traits,Is_dropout,Is_causal>(Flash_fwd_params &, cudaStream_t) [with Kernel_traits=Flash_fwd_kernel_traits<128, 64, 64, 4, false, false, cutlass::half_t, Flash_kernel_traits<128, 64, 64, 4, cutlass::half_t>>, Is_dropout=false, Is_causal=true]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(118): here
            instantiation of "void run_mha_fwd_hdim128<T>(Flash_fwd_params &, cudaStream_t) [with T=cutlass::half_t]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_hdim128_fp16_sm80.cu(31): here

/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_kernel.h(276): warning: variable "tQrQ" was declared but never referenced
          detected during:
            instantiation of "void flash::compute_attn_1rowblock<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params,Is_Q_split_K>(const Params &, int, int, int) [with Kernel_traits=Flash_fwd_kernel_traits<128, 64, 64, 4, false, false, cutlass::half_t, Flash_kernel_traits<128, 64, 64, 4, cutlass::half_t>>, Is_dropout=false, Is_causal=true, Is_even_N=true, Is_even_K=true, Return_softmax=false, Params=Flash_fwd_params, Is_Q_split_K=false]" 
(589): here
            instantiation of "void flash::compute_attn<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &) [with Kernel_traits=Flash_fwd_kernel_traits<128, 64, 64, 4, false, false, cutlass::half_t, Flash_kernel_traits<128, 64, 64, 4, cutlass::half_t>>, Is_dropout=false, Is_causal=true, Is_even_N=true, Is_even_K=true, Return_softmax=false, Params=Flash_fwd_params]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(15): here
            instantiation of "void flash_fwd_kernel<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax>(Flash_fwd_params) [with Kernel_traits=Flash_fwd_kernel_traits<128, 64, 64, 4, false, false, cutlass::half_t, Flash_kernel_traits<128, 64, 64, 4, cutlass::half_t>>, Is_dropout=false, Is_causal=true, Is_even_N=true, Is_even_K=true, Return_softmax=false]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(34): here
            instantiation of "void run_flash_fwd<Kernel_traits,Is_dropout,Is_causal>(Flash_fwd_params &, cudaStream_t) [with Kernel_traits=Flash_fwd_kernel_traits<128, 64, 64, 4, false, false, cutlass::half_t, Flash_kernel_traits<128, 64, 64, 4, cutlass::half_t>>, Is_dropout=false, Is_causal=true]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(118): here
            instantiation of "void run_mha_fwd_hdim128<T>(Flash_fwd_params &, cudaStream_t) [with T=cutlass::half_t]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_hdim128_fp16_sm80.cu(31): here

/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_kernel.h(140): warning: variable "MMA_M" was declared but never referenced
          detected during:
            instantiation of "void flash::compute_attn_1rowblock<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params,Is_Q_split_K>(const Params &, int, int, int) [with Kernel_traits=Flash_fwd_kernel_traits<128, 64, 64, 4, false, false, cutlass::half_t, Flash_kernel_traits<128, 64, 64, 4, cutlass::half_t>>, Is_dropout=false, Is_causal=true, Is_even_N=true, Is_even_K=false, Return_softmax=false, Params=Flash_fwd_params, Is_Q_split_K=false]" 
(589): here
            instantiation of "void flash::compute_attn<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &) [with Kernel_traits=Flash_fwd_kernel_traits<128, 64, 64, 4, false, false, cutlass::half_t, Flash_kernel_traits<128, 64, 64, 4, cutlass::half_t>>, Is_dropout=false, Is_causal=true, Is_even_N=true, Is_even_K=false, Return_softmax=false, Params=Flash_fwd_params]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(15): here
            instantiation of "void flash_fwd_kernel<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax>(Flash_fwd_params) [with Kernel_traits=Flash_fwd_kernel_traits<128, 64, 64, 4, false, false, cutlass::half_t, Flash_kernel_traits<128, 64, 64, 4, cutlass::half_t>>, Is_dropout=false, Is_causal=true, Is_even_N=true, Is_even_K=false, Return_softmax=false]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(34): here
            instantiation of "void run_flash_fwd<Kernel_traits,Is_dropout,Is_causal>(Flash_fwd_params &, cudaStream_t) [with Kernel_traits=Flash_fwd_kernel_traits<128, 64, 64, 4, false, false, cutlass::half_t, Flash_kernel_traits<128, 64, 64, 4, cutlass::half_t>>, Is_dropout=false, Is_causal=true]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(118): here
            instantiation of "void run_mha_fwd_hdim128<T>(Flash_fwd_params &, cudaStream_t) [with T=cutlass::half_t]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_hdim128_fp16_sm80.cu(31): here

/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_kernel.h(276): warning: variable "tQrQ" was declared but never referenced
          detected during:
            instantiation of "void flash::compute_attn_1rowblock<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params,Is_Q_split_K>(const Params &, int, int, int) [with Kernel_traits=Flash_fwd_kernel_traits<128, 64, 64, 4, false, false, cutlass::half_t, Flash_kernel_traits<128, 64, 64, 4, cutlass::half_t>>, Is_dropout=false, Is_causal=true, Is_even_N=true, Is_even_K=false, Return_softmax=false, Params=Flash_fwd_params, Is_Q_split_K=false]" 
(589): here
            instantiation of "void flash::compute_attn<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &) [with Kernel_traits=Flash_fwd_kernel_traits<128, 64, 64, 4, false, false, cutlass::half_t, Flash_kernel_traits<128, 64, 64, 4, cutlass::half_t>>, Is_dropout=false, Is_causal=true, Is_even_N=true, Is_even_K=false, Return_softmax=false, Params=Flash_fwd_params]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(15): here
            instantiation of "void flash_fwd_kernel<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax>(Flash_fwd_params) [with Kernel_traits=Flash_fwd_kernel_traits<128, 64, 64, 4, false, false, cutlass::half_t, Flash_kernel_traits<128, 64, 64, 4, cutlass::half_t>>, Is_dropout=false, Is_causal=true, Is_even_N=true, Is_even_K=false, Return_softmax=false]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(34): here
            instantiation of "void run_flash_fwd<Kernel_traits,Is_dropout,Is_causal>(Flash_fwd_params &, cudaStream_t) [with Kernel_traits=Flash_fwd_kernel_traits<128, 64, 64, 4, false, false, cutlass::half_t, Flash_kernel_traits<128, 64, 64, 4, cutlass::half_t>>, Is_dropout=false, Is_causal=true]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(118): here
            instantiation of "void run_mha_fwd_hdim128<T>(Flash_fwd_params &, cudaStream_t) [with T=cutlass::half_t]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_hdim128_fp16_sm80.cu(31): here

/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_kernel.h(140): warning: variable "MMA_M" was declared but never referenced
          detected during:
            instantiation of "void flash::compute_attn_1rowblock<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params,Is_Q_split_K>(const Params &, int, int, int) [with Kernel_traits=Flash_fwd_kernel_traits<128, 64, 64, 4, false, false, cutlass::half_t, Flash_kernel_traits<128, 64, 64, 4, cutlass::half_t>>, Is_dropout=false, Is_causal=true, Is_even_N=false, Is_even_K=true, Return_softmax=false, Params=Flash_fwd_params, Is_Q_split_K=false]" 
(589): here
            instantiation of "void flash::compute_attn<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &) [with Kernel_traits=Flash_fwd_kernel_traits<128, 64, 64, 4, false, false, cutlass::half_t, Flash_kernel_traits<128, 64, 64, 4, cutlass::half_t>>, Is_dropout=false, Is_causal=true, Is_even_N=false, Is_even_K=true, Return_softmax=false, Params=Flash_fwd_params]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(15): here
            instantiation of "void flash_fwd_kernel<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax>(Flash_fwd_params) [with Kernel_traits=Flash_fwd_kernel_traits<128, 64, 64, 4, false, false, cutlass::half_t, Flash_kernel_traits<128, 64, 64, 4, cutlass::half_t>>, Is_dropout=false, Is_causal=true, Is_even_N=false, Is_even_K=true, Return_softmax=false]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(34): here
            instantiation of "void run_flash_fwd<Kernel_traits,Is_dropout,Is_causal>(Flash_fwd_params &, cudaStream_t) [with Kernel_traits=Flash_fwd_kernel_traits<128, 64, 64, 4, false, false, cutlass::half_t, Flash_kernel_traits<128, 64, 64, 4, cutlass::half_t>>, Is_dropout=false, Is_causal=true]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(118): here
            instantiation of "void run_mha_fwd_hdim128<T>(Flash_fwd_params &, cudaStream_t) [with T=cutlass::half_t]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_hdim128_fp16_sm80.cu(31): here

/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_kernel.h(276): warning: variable "tQrQ" was declared but never referenced
          detected during:
            instantiation of "void flash::compute_attn_1rowblock<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params,Is_Q_split_K>(const Params &, int, int, int) [with Kernel_traits=Flash_fwd_kernel_traits<128, 64, 64, 4, false, false, cutlass::half_t, Flash_kernel_traits<128, 64, 64, 4, cutlass::half_t>>, Is_dropout=false, Is_causal=true, Is_even_N=false, Is_even_K=true, Return_softmax=false, Params=Flash_fwd_params, Is_Q_split_K=false]" 
(589): here
            instantiation of "void flash::compute_attn<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &) [with Kernel_traits=Flash_fwd_kernel_traits<128, 64, 64, 4, false, false, cutlass::half_t, Flash_kernel_traits<128, 64, 64, 4, cutlass::half_t>>, Is_dropout=false, Is_causal=true, Is_even_N=false, Is_even_K=true, Return_softmax=false, Params=Flash_fwd_params]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(15): here
            instantiation of "void flash_fwd_kernel<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax>(Flash_fwd_params) [with Kernel_traits=Flash_fwd_kernel_traits<128, 64, 64, 4, false, false, cutlass::half_t, Flash_kernel_traits<128, 64, 64, 4, cutlass::half_t>>, Is_dropout=false, Is_causal=true, Is_even_N=false, Is_even_K=true, Return_softmax=false]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(34): here
            instantiation of "void run_flash_fwd<Kernel_traits,Is_dropout,Is_causal>(Flash_fwd_params &, cudaStream_t) [with Kernel_traits=Flash_fwd_kernel_traits<128, 64, 64, 4, false, false, cutlass::half_t, Flash_kernel_traits<128, 64, 64, 4, cutlass::half_t>>, Is_dropout=false, Is_causal=true]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(118): here
            instantiation of "void run_mha_fwd_hdim128<T>(Flash_fwd_params &, cudaStream_t) [with T=cutlass::half_t]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_hdim128_fp16_sm80.cu(31): here

/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_kernel.h(140): warning: variable "MMA_M" was declared but never referenced
          detected during:
            instantiation of "void flash::compute_attn_1rowblock<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params,Is_Q_split_K>(const Params &, int, int, int) [with Kernel_traits=Flash_fwd_kernel_traits<128, 64, 64, 4, false, false, cutlass::half_t, Flash_kernel_traits<128, 64, 64, 4, cutlass::half_t>>, Is_dropout=false, Is_causal=true, Is_even_N=false, Is_even_K=false, Return_softmax=false, Params=Flash_fwd_params, Is_Q_split_K=false]" 
(589): here
            instantiation of "void flash::compute_attn<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &) [with Kernel_traits=Flash_fwd_kernel_traits<128, 64, 64, 4, false, false, cutlass::half_t, Flash_kernel_traits<128, 64, 64, 4, cutlass::half_t>>, Is_dropout=false, Is_causal=true, Is_even_N=false, Is_even_K=false, Return_softmax=false, Params=Flash_fwd_params]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(15): here
            instantiation of "void flash_fwd_kernel<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax>(Flash_fwd_params) [with Kernel_traits=Flash_fwd_kernel_traits<128, 64, 64, 4, false, false, cutlass::half_t, Flash_kernel_traits<128, 64, 64, 4, cutlass::half_t>>, Is_dropout=false, Is_causal=true, Is_even_N=false, Is_even_K=false, Return_softmax=false]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(34): here
            instantiation of "void run_flash_fwd<Kernel_traits,Is_dropout,Is_causal>(Flash_fwd_params &, cudaStream_t) [with Kernel_traits=Flash_fwd_kernel_traits<128, 64, 64, 4, false, false, cutlass::half_t, Flash_kernel_traits<128, 64, 64, 4, cutlass::half_t>>, Is_dropout=false, Is_causal=true]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(118): here
            instantiation of "void run_mha_fwd_hdim128<T>(Flash_fwd_params &, cudaStream_t) [with T=cutlass::half_t]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_hdim128_fp16_sm80.cu(31): here

/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_kernel.h(276): warning: variable "tQrQ" was declared but never referenced
          detected during:
            instantiation of "void flash::compute_attn_1rowblock<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params,Is_Q_split_K>(const Params &, int, int, int) [with Kernel_traits=Flash_fwd_kernel_traits<128, 64, 64, 4, false, false, cutlass::half_t, Flash_kernel_traits<128, 64, 64, 4, cutlass::half_t>>, Is_dropout=false, Is_causal=true, Is_even_N=false, Is_even_K=false, Return_softmax=false, Params=Flash_fwd_params, Is_Q_split_K=false]" 
(589): here
            instantiation of "void flash::compute_attn<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &) [with Kernel_traits=Flash_fwd_kernel_traits<128, 64, 64, 4, false, false, cutlass::half_t, Flash_kernel_traits<128, 64, 64, 4, cutlass::half_t>>, Is_dropout=false, Is_causal=true, Is_even_N=false, Is_even_K=false, Return_softmax=false, Params=Flash_fwd_params]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(15): here
            instantiation of "void flash_fwd_kernel<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax>(Flash_fwd_params) [with Kernel_traits=Flash_fwd_kernel_traits<128, 64, 64, 4, false, false, cutlass::half_t, Flash_kernel_traits<128, 64, 64, 4, cutlass::half_t>>, Is_dropout=false, Is_causal=true, Is_even_N=false, Is_even_K=false, Return_softmax=false]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(34): here
            instantiation of "void run_flash_fwd<Kernel_traits,Is_dropout,Is_causal>(Flash_fwd_params &, cudaStream_t) [with Kernel_traits=Flash_fwd_kernel_traits<128, 64, 64, 4, false, false, cutlass::half_t, Flash_kernel_traits<128, 64, 64, 4, cutlass::half_t>>, Is_dropout=false, Is_causal=true]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(118): here
            instantiation of "void run_mha_fwd_hdim128<T>(Flash_fwd_params &, cudaStream_t) [with T=cutlass::half_t]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_hdim128_fp16_sm80.cu(31): here

/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_kernel.h(140): warning: variable "MMA_M" was declared but never referenced
          detected during:
            instantiation of "void flash::compute_attn_1rowblock<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params,Is_Q_split_K>(const Params &, int, int, int) [with Kernel_traits=Flash_fwd_kernel_traits<128, 128, 64, 4, false, false, cutlass::half_t, Flash_kernel_traits<128, 128, 64, 4, cutlass::half_t>>, Is_dropout=false, Is_causal=true, Is_even_N=true, Is_even_K=true, Return_softmax=false, Params=Flash_fwd_params, Is_Q_split_K=false]" 
(589): here
            instantiation of "void flash::compute_attn<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &) [with Kernel_traits=Flash_fwd_kernel_traits<128, 128, 64, 4, false, false, cutlass::half_t, Flash_kernel_traits<128, 128, 64, 4, cutlass::half_t>>, Is_dropout=false, Is_causal=true, Is_even_N=true, Is_even_K=true, Return_softmax=false, Params=Flash_fwd_params]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(15): here
            instantiation of "void flash_fwd_kernel<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax>(Flash_fwd_params) [with Kernel_traits=Flash_fwd_kernel_traits<128, 128, 64, 4, false, false, cutlass::half_t, Flash_kernel_traits<128, 128, 64, 4, cutlass::half_t>>, Is_dropout=false, Is_causal=true, Is_even_N=true, Is_even_K=true, Return_softmax=false]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(34): here
            instantiation of "void run_flash_fwd<Kernel_traits,Is_dropout,Is_causal>(Flash_fwd_params &, cudaStream_t) [with Kernel_traits=Flash_fwd_kernel_traits<128, 128, 64, 4, false, false, cutlass::half_t, Flash_kernel_traits<128, 128, 64, 4, cutlass::half_t>>, Is_dropout=false, Is_causal=true]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(118): here
            instantiation of "void run_mha_fwd_hdim128<T>(Flash_fwd_params &, cudaStream_t) [with T=cutlass::half_t]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_hdim128_fp16_sm80.cu(31): here

/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_kernel.h(276): warning: variable "tQrQ" was declared but never referenced
          detected during:
            instantiation of "void flash::compute_attn_1rowblock<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params,Is_Q_split_K>(const Params &, int, int, int) [with Kernel_traits=Flash_fwd_kernel_traits<128, 128, 64, 4, false, false, cutlass::half_t, Flash_kernel_traits<128, 128, 64, 4, cutlass::half_t>>, Is_dropout=false, Is_causal=true, Is_even_N=true, Is_even_K=true, Return_softmax=false, Params=Flash_fwd_params, Is_Q_split_K=false]" 
(589): here
            instantiation of "void flash::compute_attn<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &) [with Kernel_traits=Flash_fwd_kernel_traits<128, 128, 64, 4, false, false, cutlass::half_t, Flash_kernel_traits<128, 128, 64, 4, cutlass::half_t>>, Is_dropout=false, Is_causal=true, Is_even_N=true, Is_even_K=true, Return_softmax=false, Params=Flash_fwd_params]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(15): here
            instantiation of "void flash_fwd_kernel<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax>(Flash_fwd_params) [with Kernel_traits=Flash_fwd_kernel_traits<128, 128, 64, 4, false, false, cutlass::half_t, Flash_kernel_traits<128, 128, 64, 4, cutlass::half_t>>, Is_dropout=false, Is_causal=true, Is_even_N=true, Is_even_K=true, Return_softmax=false]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(34): here
            instantiation of "void run_flash_fwd<Kernel_traits,Is_dropout,Is_causal>(Flash_fwd_params &, cudaStream_t) [with Kernel_traits=Flash_fwd_kernel_traits<128, 128, 64, 4, false, false, cutlass::half_t, Flash_kernel_traits<128, 128, 64, 4, cutlass::half_t>>, Is_dropout=false, Is_causal=true]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(118): here
            instantiation of "void run_mha_fwd_hdim128<T>(Flash_fwd_params &, cudaStream_t) [with T=cutlass::half_t]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_hdim128_fp16_sm80.cu(31): here

/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_kernel.h(140): warning: variable "MMA_M" was declared but never referenced
          detected during:
            instantiation of "void flash::compute_attn_1rowblock<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params,Is_Q_split_K>(const Params &, int, int, int) [with Kernel_traits=Flash_fwd_kernel_traits<128, 128, 64, 4, false, false, cutlass::half_t, Flash_kernel_traits<128, 128, 64, 4, cutlass::half_t>>, Is_dropout=false, Is_causal=true, Is_even_N=true, Is_even_K=false, Return_softmax=false, Params=Flash_fwd_params, Is_Q_split_K=false]" 
(589): here
            instantiation of "void flash::compute_attn<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &) [with Kernel_traits=Flash_fwd_kernel_traits<128, 128, 64, 4, false, false, cutlass::half_t, Flash_kernel_traits<128, 128, 64, 4, cutlass::half_t>>, Is_dropout=false, Is_causal=true, Is_even_N=true, Is_even_K=false, Return_softmax=false, Params=Flash_fwd_params]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(15): here
            instantiation of "void flash_fwd_kernel<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax>(Flash_fwd_params) [with Kernel_traits=Flash_fwd_kernel_traits<128, 128, 64, 4, false, false, cutlass::half_t, Flash_kernel_traits<128, 128, 64, 4, cutlass::half_t>>, Is_dropout=false, Is_causal=true, Is_even_N=true, Is_even_K=false, Return_softmax=false]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(34): here
            instantiation of "void run_flash_fwd<Kernel_traits,Is_dropout,Is_causal>(Flash_fwd_params &, cudaStream_t) [with Kernel_traits=Flash_fwd_kernel_traits<128, 128, 64, 4, false, false, cutlass::half_t, Flash_kernel_traits<128, 128, 64, 4, cutlass::half_t>>, Is_dropout=false, Is_causal=true]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(118): here
            instantiation of "void run_mha_fwd_hdim128<T>(Flash_fwd_params &, cudaStream_t) [with T=cutlass::half_t]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_hdim128_fp16_sm80.cu(31): here

/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_kernel.h(276): warning: variable "tQrQ" was declared but never referenced
          detected during:
            instantiation of "void flash::compute_attn_1rowblock<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params,Is_Q_split_K>(const Params &, int, int, int) [with Kernel_traits=Flash_fwd_kernel_traits<128, 128, 64, 4, false, false, cutlass::half_t, Flash_kernel_traits<128, 128, 64, 4, cutlass::half_t>>, Is_dropout=false, Is_causal=true, Is_even_N=true, Is_even_K=false, Return_softmax=false, Params=Flash_fwd_params, Is_Q_split_K=false]" 
(589): here
            instantiation of "void flash::compute_attn<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &) [with Kernel_traits=Flash_fwd_kernel_traits<128, 128, 64, 4, false, false, cutlass::half_t, Flash_kernel_traits<128, 128, 64, 4, cutlass::half_t>>, Is_dropout=false, Is_causal=true, Is_even_N=true, Is_even_K=false, Return_softmax=false, Params=Flash_fwd_params]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(15): here
            instantiation of "void flash_fwd_kernel<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax>(Flash_fwd_params) [with Kernel_traits=Flash_fwd_kernel_traits<128, 128, 64, 4, false, false, cutlass::half_t, Flash_kernel_traits<128, 128, 64, 4, cutlass::half_t>>, Is_dropout=false, Is_causal=true, Is_even_N=true, Is_even_K=false, Return_softmax=false]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(34): here
            instantiation of "void run_flash_fwd<Kernel_traits,Is_dropout,Is_causal>(Flash_fwd_params &, cudaStream_t) [with Kernel_traits=Flash_fwd_kernel_traits<128, 128, 64, 4, false, false, cutlass::half_t, Flash_kernel_traits<128, 128, 64, 4, cutlass::half_t>>, Is_dropout=false, Is_causal=true]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(118): here
            instantiation of "void run_mha_fwd_hdim128<T>(Flash_fwd_params &, cudaStream_t) [with T=cutlass::half_t]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_hdim128_fp16_sm80.cu(31): here

/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_kernel.h(140): warning: variable "MMA_M" was declared but never referenced
          detected during:
            instantiation of "void flash::compute_attn_1rowblock<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params,Is_Q_split_K>(const Params &, int, int, int) [with Kernel_traits=Flash_fwd_kernel_traits<128, 128, 64, 4, false, false, cutlass::half_t, Flash_kernel_traits<128, 128, 64, 4, cutlass::half_t>>, Is_dropout=false, Is_causal=true, Is_even_N=false, Is_even_K=true, Return_softmax=false, Params=Flash_fwd_params, Is_Q_split_K=false]" 
(589): here
            instantiation of "void flash::compute_attn<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &) [with Kernel_traits=Flash_fwd_kernel_traits<128, 128, 64, 4, false, false, cutlass::half_t, Flash_kernel_traits<128, 128, 64, 4, cutlass::half_t>>, Is_dropout=false, Is_causal=true, Is_even_N=false, Is_even_K=true, Return_softmax=false, Params=Flash_fwd_params]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(15): here
            instantiation of "void flash_fwd_kernel<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax>(Flash_fwd_params) [with Kernel_traits=Flash_fwd_kernel_traits<128, 128, 64, 4, false, false, cutlass::half_t, Flash_kernel_traits<128, 128, 64, 4, cutlass::half_t>>, Is_dropout=false, Is_causal=true, Is_even_N=false, Is_even_K=true, Return_softmax=false]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(34): here
            instantiation of "void run_flash_fwd<Kernel_traits,Is_dropout,Is_causal>(Flash_fwd_params &, cudaStream_t) [with Kernel_traits=Flash_fwd_kernel_traits<128, 128, 64, 4, false, false, cutlass::half_t, Flash_kernel_traits<128, 128, 64, 4, cutlass::half_t>>, Is_dropout=false, Is_causal=true]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(118): here
            instantiation of "void run_mha_fwd_hdim128<T>(Flash_fwd_params &, cudaStream_t) [with T=cutlass::half_t]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_hdim128_fp16_sm80.cu(31): here

/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_kernel.h(276): warning: variable "tQrQ" was declared but never referenced
          detected during:
            instantiation of "void flash::compute_attn_1rowblock<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params,Is_Q_split_K>(const Params &, int, int, int) [with Kernel_traits=Flash_fwd_kernel_traits<128, 128, 64, 4, false, false, cutlass::half_t, Flash_kernel_traits<128, 128, 64, 4, cutlass::half_t>>, Is_dropout=false, Is_causal=true, Is_even_N=false, Is_even_K=true, Return_softmax=false, Params=Flash_fwd_params, Is_Q_split_K=false]" 
(589): here
            instantiation of "void flash::compute_attn<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &) [with Kernel_traits=Flash_fwd_kernel_traits<128, 128, 64, 4, false, false, cutlass::half_t, Flash_kernel_traits<128, 128, 64, 4, cutlass::half_t>>, Is_dropout=false, Is_causal=true, Is_even_N=false, Is_even_K=true, Return_softmax=false, Params=Flash_fwd_params]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(15): here
            instantiation of "void flash_fwd_kernel<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax>(Flash_fwd_params) [with Kernel_traits=Flash_fwd_kernel_traits<128, 128, 64, 4, false, false, cutlass::half_t, Flash_kernel_traits<128, 128, 64, 4, cutlass::half_t>>, Is_dropout=false, Is_causal=true, Is_even_N=false, Is_even_K=true, Return_softmax=false]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(34): here
            instantiation of "void run_flash_fwd<Kernel_traits,Is_dropout,Is_causal>(Flash_fwd_params &, cudaStream_t) [with Kernel_traits=Flash_fwd_kernel_traits<128, 128, 64, 4, false, false, cutlass::half_t, Flash_kernel_traits<128, 128, 64, 4, cutlass::half_t>>, Is_dropout=false, Is_causal=true]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(118): here
            instantiation of "void run_mha_fwd_hdim128<T>(Flash_fwd_params &, cudaStream_t) [with T=cutlass::half_t]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_hdim128_fp16_sm80.cu(31): here

/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_kernel.h(140): warning: variable "MMA_M" was declared but never referenced
          detected during:
            instantiation of "void flash::compute_attn_1rowblock<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params,Is_Q_split_K>(const Params &, int, int, int) [with Kernel_traits=Flash_fwd_kernel_traits<128, 128, 64, 4, false, false, cutlass::half_t, Flash_kernel_traits<128, 128, 64, 4, cutlass::half_t>>, Is_dropout=false, Is_causal=true, Is_even_N=false, Is_even_K=false, Return_softmax=false, Params=Flash_fwd_params, Is_Q_split_K=false]" 
(589): here
            instantiation of "void flash::compute_attn<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &) [with Kernel_traits=Flash_fwd_kernel_traits<128, 128, 64, 4, false, false, cutlass::half_t, Flash_kernel_traits<128, 128, 64, 4, cutlass::half_t>>, Is_dropout=false, Is_causal=true, Is_even_N=false, Is_even_K=false, Return_softmax=false, Params=Flash_fwd_params]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(15): here
            instantiation of "void flash_fwd_kernel<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax>(Flash_fwd_params) [with Kernel_traits=Flash_fwd_kernel_traits<128, 128, 64, 4, false, false, cutlass::half_t, Flash_kernel_traits<128, 128, 64, 4, cutlass::half_t>>, Is_dropout=false, Is_causal=true, Is_even_N=false, Is_even_K=false, Return_softmax=false]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(34): here
            instantiation of "void run_flash_fwd<Kernel_traits,Is_dropout,Is_causal>(Flash_fwd_params &, cudaStream_t) [with Kernel_traits=Flash_fwd_kernel_traits<128, 128, 64, 4, false, false, cutlass::half_t, Flash_kernel_traits<128, 128, 64, 4, cutlass::half_t>>, Is_dropout=false, Is_causal=true]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(118): here
            instantiation of "void run_mha_fwd_hdim128<T>(Flash_fwd_params &, cudaStream_t) [with T=cutlass::half_t]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_hdim128_fp16_sm80.cu(31): here

/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_kernel.h(276): warning: variable "tQrQ" was declared but never referenced
          detected during:
            instantiation of "void flash::compute_attn_1rowblock<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params,Is_Q_split_K>(const Params &, int, int, int) [with Kernel_traits=Flash_fwd_kernel_traits<128, 128, 64, 4, false, false, cutlass::half_t, Flash_kernel_traits<128, 128, 64, 4, cutlass::half_t>>, Is_dropout=false, Is_causal=true, Is_even_N=false, Is_even_K=false, Return_softmax=false, Params=Flash_fwd_params, Is_Q_split_K=false]" 
(589): here
            instantiation of "void flash::compute_attn<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &) [with Kernel_traits=Flash_fwd_kernel_traits<128, 128, 64, 4, false, false, cutlass::half_t, Flash_kernel_traits<128, 128, 64, 4, cutlass::half_t>>, Is_dropout=false, Is_causal=true, Is_even_N=false, Is_even_K=false, Return_softmax=false, Params=Flash_fwd_params]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(15): here
            instantiation of "void flash_fwd_kernel<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax>(Flash_fwd_params) [with Kernel_traits=Flash_fwd_kernel_traits<128, 128, 64, 4, false, false, cutlass::half_t, Flash_kernel_traits<128, 128, 64, 4, cutlass::half_t>>, Is_dropout=false, Is_causal=true, Is_even_N=false, Is_even_K=false, Return_softmax=false]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(34): here
            instantiation of "void run_flash_fwd<Kernel_traits,Is_dropout,Is_causal>(Flash_fwd_params &, cudaStream_t) [with Kernel_traits=Flash_fwd_kernel_traits<128, 128, 64, 4, false, false, cutlass::half_t, Flash_kernel_traits<128, 128, 64, 4, cutlass::half_t>>, Is_dropout=false, Is_causal=true]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(118): here
            instantiation of "void run_mha_fwd_hdim128<T>(Flash_fwd_params &, cudaStream_t) [with T=cutlass::half_t]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_hdim128_fp16_sm80.cu(31): here

/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_kernel.h(140): warning: variable "MMA_M" was declared but never referenced
          detected during:
            instantiation of "void flash::compute_attn_1rowblock<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params,Is_Q_split_K>(const Params &, int, int, int) [with Kernel_traits=Flash_fwd_kernel_traits<128, 128, 32, 4, false, false, cutlass::half_t, Flash_kernel_traits<128, 128, 32, 4, cutlass::half_t>>, Is_dropout=false, Is_causal=false, Is_even_N=true, Is_even_K=true, Return_softmax=false, Params=Flash_fwd_params, Is_Q_split_K=false]" 
(589): here
            instantiation of "void flash::compute_attn<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &) [with Kernel_traits=Flash_fwd_kernel_traits<128, 128, 32, 4, false, false, cutlass::half_t, Flash_kernel_traits<128, 128, 32, 4, cutlass::half_t>>, Is_dropout=false, Is_causal=false, Is_even_N=true, Is_even_K=true, Return_softmax=false, Params=Flash_fwd_params]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(15): here
            instantiation of "void flash_fwd_kernel<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax>(Flash_fwd_params) [with Kernel_traits=Flash_fwd_kernel_traits<128, 128, 32, 4, false, false, cutlass::half_t, Flash_kernel_traits<128, 128, 32, 4, cutlass::half_t>>, Is_dropout=false, Is_causal=false, Is_even_N=true, Is_even_K=true, Return_softmax=false]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(34): here
            instantiation of "void run_flash_fwd<Kernel_traits,Is_dropout,Is_causal>(Flash_fwd_params &, cudaStream_t) [with Kernel_traits=Flash_fwd_kernel_traits<128, 128, 32, 4, false, false, cutlass::half_t, Flash_kernel_traits<128, 128, 32, 4, cutlass::half_t>>, Is_dropout=false, Is_causal=false]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(118): here
            instantiation of "void run_mha_fwd_hdim128<T>(Flash_fwd_params &, cudaStream_t) [with T=cutlass::half_t]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_hdim128_fp16_sm80.cu(31): here

/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_kernel.h(276): warning: variable "tQrQ" was declared but never referenced
          detected during:
            instantiation of "void flash::compute_attn_1rowblock<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params,Is_Q_split_K>(const Params &, int, int, int) [with Kernel_traits=Flash_fwd_kernel_traits<128, 128, 32, 4, false, false, cutlass::half_t, Flash_kernel_traits<128, 128, 32, 4, cutlass::half_t>>, Is_dropout=false, Is_causal=false, Is_even_N=true, Is_even_K=true, Return_softmax=false, Params=Flash_fwd_params, Is_Q_split_K=false]" 
(589): here
            instantiation of "void flash::compute_attn<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &) [with Kernel_traits=Flash_fwd_kernel_traits<128, 128, 32, 4, false, false, cutlass::half_t, Flash_kernel_traits<128, 128, 32, 4, cutlass::half_t>>, Is_dropout=false, Is_causal=false, Is_even_N=true, Is_even_K=true, Return_softmax=false, Params=Flash_fwd_params]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(15): here
            instantiation of "void flash_fwd_kernel<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax>(Flash_fwd_params) [with Kernel_traits=Flash_fwd_kernel_traits<128, 128, 32, 4, false, false, cutlass::half_t, Flash_kernel_traits<128, 128, 32, 4, cutlass::half_t>>, Is_dropout=false, Is_causal=false, Is_even_N=true, Is_even_K=true, Return_softmax=false]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(34): here
            instantiation of "void run_flash_fwd<Kernel_traits,Is_dropout,Is_causal>(Flash_fwd_params &, cudaStream_t) [with Kernel_traits=Flash_fwd_kernel_traits<128, 128, 32, 4, false, false, cutlass::half_t, Flash_kernel_traits<128, 128, 32, 4, cutlass::half_t>>, Is_dropout=false, Is_causal=false]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(118): here
            instantiation of "void run_mha_fwd_hdim128<T>(Flash_fwd_params &, cudaStream_t) [with T=cutlass::half_t]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_hdim128_fp16_sm80.cu(31): here

/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_kernel.h(140): warning: variable "MMA_M" was declared but never referenced
          detected during:
            instantiation of "void flash::compute_attn_1rowblock<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params,Is_Q_split_K>(const Params &, int, int, int) [with Kernel_traits=Flash_fwd_kernel_traits<128, 128, 32, 4, false, false, cutlass::half_t, Flash_kernel_traits<128, 128, 32, 4, cutlass::half_t>>, Is_dropout=false, Is_causal=false, Is_even_N=true, Is_even_K=false, Return_softmax=false, Params=Flash_fwd_params, Is_Q_split_K=false]" 
(589): here
            instantiation of "void flash::compute_attn<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &) [with Kernel_traits=Flash_fwd_kernel_traits<128, 128, 32, 4, false, false, cutlass::half_t, Flash_kernel_traits<128, 128, 32, 4, cutlass::half_t>>, Is_dropout=false, Is_causal=false, Is_even_N=true, Is_even_K=false, Return_softmax=false, Params=Flash_fwd_params]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(15): here
            instantiation of "void flash_fwd_kernel<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax>(Flash_fwd_params) [with Kernel_traits=Flash_fwd_kernel_traits<128, 128, 32, 4, false, false, cutlass::half_t, Flash_kernel_traits<128, 128, 32, 4, cutlass::half_t>>, Is_dropout=false, Is_causal=false, Is_even_N=true, Is_even_K=false, Return_softmax=false]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(34): here
            instantiation of "void run_flash_fwd<Kernel_traits,Is_dropout,Is_causal>(Flash_fwd_params &, cudaStream_t) [with Kernel_traits=Flash_fwd_kernel_traits<128, 128, 32, 4, false, false, cutlass::half_t, Flash_kernel_traits<128, 128, 32, 4, cutlass::half_t>>, Is_dropout=false, Is_causal=false]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(118): here
            instantiation of "void run_mha_fwd_hdim128<T>(Flash_fwd_params &, cudaStream_t) [with T=cutlass::half_t]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_hdim128_fp16_sm80.cu(31): here

/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_kernel.h(276): warning: variable "tQrQ" was declared but never referenced
          detected during:
            instantiation of "void flash::compute_attn_1rowblock<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params,Is_Q_split_K>(const Params &, int, int, int) [with Kernel_traits=Flash_fwd_kernel_traits<128, 128, 32, 4, false, false, cutlass::half_t, Flash_kernel_traits<128, 128, 32, 4, cutlass::half_t>>, Is_dropout=false, Is_causal=false, Is_even_N=true, Is_even_K=false, Return_softmax=false, Params=Flash_fwd_params, Is_Q_split_K=false]" 
(589): here
            instantiation of "void flash::compute_attn<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &) [with Kernel_traits=Flash_fwd_kernel_traits<128, 128, 32, 4, false, false, cutlass::half_t, Flash_kernel_traits<128, 128, 32, 4, cutlass::half_t>>, Is_dropout=false, Is_causal=false, Is_even_N=true, Is_even_K=false, Return_softmax=false, Params=Flash_fwd_params]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(15): here
            instantiation of "void flash_fwd_kernel<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax>(Flash_fwd_params) [with Kernel_traits=Flash_fwd_kernel_traits<128, 128, 32, 4, false, false, cutlass::half_t, Flash_kernel_traits<128, 128, 32, 4, cutlass::half_t>>, Is_dropout=false, Is_causal=false, Is_even_N=true, Is_even_K=false, Return_softmax=false]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(34): here
            instantiation of "void run_flash_fwd<Kernel_traits,Is_dropout,Is_causal>(Flash_fwd_params &, cudaStream_t) [with Kernel_traits=Flash_fwd_kernel_traits<128, 128, 32, 4, false, false, cutlass::half_t, Flash_kernel_traits<128, 128, 32, 4, cutlass::half_t>>, Is_dropout=false, Is_causal=false]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(118): here
            instantiation of "void run_mha_fwd_hdim128<T>(Flash_fwd_params &, cudaStream_t) [with T=cutlass::half_t]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_hdim128_fp16_sm80.cu(31): here

/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_kernel.h(140): warning: variable "MMA_M" was declared but never referenced
          detected during:
            instantiation of "void flash::compute_attn_1rowblock<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params,Is_Q_split_K>(const Params &, int, int, int) [with Kernel_traits=Flash_fwd_kernel_traits<128, 128, 32, 4, false, false, cutlass::half_t, Flash_kernel_traits<128, 128, 32, 4, cutlass::half_t>>, Is_dropout=false, Is_causal=false, Is_even_N=false, Is_even_K=true, Return_softmax=false, Params=Flash_fwd_params, Is_Q_split_K=false]" 
(589): here
            instantiation of "void flash::compute_attn<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &) [with Kernel_traits=Flash_fwd_kernel_traits<128, 128, 32, 4, false, false, cutlass::half_t, Flash_kernel_traits<128, 128, 32, 4, cutlass::half_t>>, Is_dropout=false, Is_causal=false, Is_even_N=false, Is_even_K=true, Return_softmax=false, Params=Flash_fwd_params]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(15): here
            instantiation of "void flash_fwd_kernel<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax>(Flash_fwd_params) [with Kernel_traits=Flash_fwd_kernel_traits<128, 128, 32, 4, false, false, cutlass::half_t, Flash_kernel_traits<128, 128, 32, 4, cutlass::half_t>>, Is_dropout=false, Is_causal=false, Is_even_N=false, Is_even_K=true, Return_softmax=false]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(34): here
            instantiation of "void run_flash_fwd<Kernel_traits,Is_dropout,Is_causal>(Flash_fwd_params &, cudaStream_t) [with Kernel_traits=Flash_fwd_kernel_traits<128, 128, 32, 4, false, false, cutlass::half_t, Flash_kernel_traits<128, 128, 32, 4, cutlass::half_t>>, Is_dropout=false, Is_causal=false]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(118): here
            instantiation of "void run_mha_fwd_hdim128<T>(Flash_fwd_params &, cudaStream_t) [with T=cutlass::half_t]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_hdim128_fp16_sm80.cu(31): here

/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_kernel.h(276): warning: variable "tQrQ" was declared but never referenced
          detected during:
            instantiation of "void flash::compute_attn_1rowblock<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params,Is_Q_split_K>(const Params &, int, int, int) [with Kernel_traits=Flash_fwd_kernel_traits<128, 128, 32, 4, false, false, cutlass::half_t, Flash_kernel_traits<128, 128, 32, 4, cutlass::half_t>>, Is_dropout=false, Is_causal=false, Is_even_N=false, Is_even_K=true, Return_softmax=false, Params=Flash_fwd_params, Is_Q_split_K=false]" 
(589): here
            instantiation of "void flash::compute_attn<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &) [with Kernel_traits=Flash_fwd_kernel_traits<128, 128, 32, 4, false, false, cutlass::half_t, Flash_kernel_traits<128, 128, 32, 4, cutlass::half_t>>, Is_dropout=false, Is_causal=false, Is_even_N=false, Is_even_K=true, Return_softmax=false, Params=Flash_fwd_params]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(15): here
            instantiation of "void flash_fwd_kernel<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax>(Flash_fwd_params) [with Kernel_traits=Flash_fwd_kernel_traits<128, 128, 32, 4, false, false, cutlass::half_t, Flash_kernel_traits<128, 128, 32, 4, cutlass::half_t>>, Is_dropout=false, Is_causal=false, Is_even_N=false, Is_even_K=true, Return_softmax=false]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(34): here
            instantiation of "void run_flash_fwd<Kernel_traits,Is_dropout,Is_causal>(Flash_fwd_params &, cudaStream_t) [with Kernel_traits=Flash_fwd_kernel_traits<128, 128, 32, 4, false, false, cutlass::half_t, Flash_kernel_traits<128, 128, 32, 4, cutlass::half_t>>, Is_dropout=false, Is_causal=false]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(118): here
            instantiation of "void run_mha_fwd_hdim128<T>(Flash_fwd_params &, cudaStream_t) [with T=cutlass::half_t]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_hdim128_fp16_sm80.cu(31): here

/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_kernel.h(140): warning: variable "MMA_M" was declared but never referenced
          detected during:
            instantiation of "void flash::compute_attn_1rowblock<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params,Is_Q_split_K>(const Params &, int, int, int) [with Kernel_traits=Flash_fwd_kernel_traits<128, 128, 32, 4, false, false, cutlass::half_t, Flash_kernel_traits<128, 128, 32, 4, cutlass::half_t>>, Is_dropout=false, Is_causal=false, Is_even_N=false, Is_even_K=false, Return_softmax=false, Params=Flash_fwd_params, Is_Q_split_K=false]" 
(589): here
            instantiation of "void flash::compute_attn<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &) [with Kernel_traits=Flash_fwd_kernel_traits<128, 128, 32, 4, false, false, cutlass::half_t, Flash_kernel_traits<128, 128, 32, 4, cutlass::half_t>>, Is_dropout=false, Is_causal=false, Is_even_N=false, Is_even_K=false, Return_softmax=false, Params=Flash_fwd_params]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(15): here
            instantiation of "void flash_fwd_kernel<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax>(Flash_fwd_params) [with Kernel_traits=Flash_fwd_kernel_traits<128, 128, 32, 4, false, false, cutlass::half_t, Flash_kernel_traits<128, 128, 32, 4, cutlass::half_t>>, Is_dropout=false, Is_causal=false, Is_even_N=false, Is_even_K=false, Return_softmax=false]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(34): here
            instantiation of "void run_flash_fwd<Kernel_traits,Is_dropout,Is_causal>(Flash_fwd_params &, cudaStream_t) [with Kernel_traits=Flash_fwd_kernel_traits<128, 128, 32, 4, false, false, cutlass::half_t, Flash_kernel_traits<128, 128, 32, 4, cutlass::half_t>>, Is_dropout=false, Is_causal=false]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(118): here
            instantiation of "void run_mha_fwd_hdim128<T>(Flash_fwd_params &, cudaStream_t) [with T=cutlass::half_t]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_hdim128_fp16_sm80.cu(31): here

/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_kernel.h(276): warning: variable "tQrQ" was declared but never referenced
          detected during:
            instantiation of "void flash::compute_attn_1rowblock<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params,Is_Q_split_K>(const Params &, int, int, int) [with Kernel_traits=Flash_fwd_kernel_traits<128, 128, 32, 4, false, false, cutlass::half_t, Flash_kernel_traits<128, 128, 32, 4, cutlass::half_t>>, Is_dropout=false, Is_causal=false, Is_even_N=false, Is_even_K=false, Return_softmax=false, Params=Flash_fwd_params, Is_Q_split_K=false]" 
(589): here
            instantiation of "void flash::compute_attn<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &) [with Kernel_traits=Flash_fwd_kernel_traits<128, 128, 32, 4, false, false, cutlass::half_t, Flash_kernel_traits<128, 128, 32, 4, cutlass::half_t>>, Is_dropout=false, Is_causal=false, Is_even_N=false, Is_even_K=false, Return_softmax=false, Params=Flash_fwd_params]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(15): here
            instantiation of "void flash_fwd_kernel<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax>(Flash_fwd_params) [with Kernel_traits=Flash_fwd_kernel_traits<128, 128, 32, 4, false, false, cutlass::half_t, Flash_kernel_traits<128, 128, 32, 4, cutlass::half_t>>, Is_dropout=false, Is_causal=false, Is_even_N=false, Is_even_K=false, Return_softmax=false]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(34): here
            instantiation of "void run_flash_fwd<Kernel_traits,Is_dropout,Is_causal>(Flash_fwd_params &, cudaStream_t) [with Kernel_traits=Flash_fwd_kernel_traits<128, 128, 32, 4, false, false, cutlass::half_t, Flash_kernel_traits<128, 128, 32, 4, cutlass::half_t>>, Is_dropout=false, Is_causal=false]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(118): here
            instantiation of "void run_mha_fwd_hdim128<T>(Flash_fwd_params &, cudaStream_t) [with T=cutlass::half_t]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_hdim128_fp16_sm80.cu(31): here

/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_kernel.h(140): warning: variable "MMA_M" was declared but never referenced
          detected during:
            instantiation of "void flash::compute_attn_1rowblock<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params,Is_Q_split_K>(const Params &, int, int, int) [with Kernel_traits=Flash_fwd_kernel_traits<128, 128, 64, 4, false, false, cutlass::half_t, Flash_kernel_traits<128, 128, 64, 4, cutlass::half_t>>, Is_dropout=false, Is_causal=false, Is_even_N=true, Is_even_K=true, Return_softmax=false, Params=Flash_fwd_params, Is_Q_split_K=false]" 
(589): here
            instantiation of "void flash::compute_attn<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &) [with Kernel_traits=Flash_fwd_kernel_traits<128, 128, 64, 4, false, false, cutlass::half_t, Flash_kernel_traits<128, 128, 64, 4, cutlass::half_t>>, Is_dropout=false, Is_causal=false, Is_even_N=true, Is_even_K=true, Return_softmax=false, Params=Flash_fwd_params]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(15): here
            instantiation of "void flash_fwd_kernel<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax>(Flash_fwd_params) [with Kernel_traits=Flash_fwd_kernel_traits<128, 128, 64, 4, false, false, cutlass::half_t, Flash_kernel_traits<128, 128, 64, 4, cutlass::half_t>>, Is_dropout=false, Is_causal=false, Is_even_N=true, Is_even_K=true, Return_softmax=false]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(34): here
            instantiation of "void run_flash_fwd<Kernel_traits,Is_dropout,Is_causal>(Flash_fwd_params &, cudaStream_t) [with Kernel_traits=Flash_fwd_kernel_traits<128, 128, 64, 4, false, false, cutlass::half_t, Flash_kernel_traits<128, 128, 64, 4, cutlass::half_t>>, Is_dropout=false, Is_causal=false]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(118): here
            instantiation of "void run_mha_fwd_hdim128<T>(Flash_fwd_params &, cudaStream_t) [with T=cutlass::half_t]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_hdim128_fp16_sm80.cu(31): here

/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_kernel.h(276): warning: variable "tQrQ" was declared but never referenced
          detected during:
            instantiation of "void flash::compute_attn_1rowblock<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params,Is_Q_split_K>(const Params &, int, int, int) [with Kernel_traits=Flash_fwd_kernel_traits<128, 128, 64, 4, false, false, cutlass::half_t, Flash_kernel_traits<128, 128, 64, 4, cutlass::half_t>>, Is_dropout=false, Is_causal=false, Is_even_N=true, Is_even_K=true, Return_softmax=false, Params=Flash_fwd_params, Is_Q_split_K=false]" 
(589): here
            instantiation of "void flash::compute_attn<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &) [with Kernel_traits=Flash_fwd_kernel_traits<128, 128, 64, 4, false, false, cutlass::half_t, Flash_kernel_traits<128, 128, 64, 4, cutlass::half_t>>, Is_dropout=false, Is_causal=false, Is_even_N=true, Is_even_K=true, Return_softmax=false, Params=Flash_fwd_params]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(15): here
            instantiation of "void flash_fwd_kernel<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax>(Flash_fwd_params) [with Kernel_traits=Flash_fwd_kernel_traits<128, 128, 64, 4, false, false, cutlass::half_t, Flash_kernel_traits<128, 128, 64, 4, cutlass::half_t>>, Is_dropout=false, Is_causal=false, Is_even_N=true, Is_even_K=true, Return_softmax=false]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(34): here
            instantiation of "void run_flash_fwd<Kernel_traits,Is_dropout,Is_causal>(Flash_fwd_params &, cudaStream_t) [with Kernel_traits=Flash_fwd_kernel_traits<128, 128, 64, 4, false, false, cutlass::half_t, Flash_kernel_traits<128, 128, 64, 4, cutlass::half_t>>, Is_dropout=false, Is_causal=false]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(118): here
            instantiation of "void run_mha_fwd_hdim128<T>(Flash_fwd_params &, cudaStream_t) [with T=cutlass::half_t]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_hdim128_fp16_sm80.cu(31): here

/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_kernel.h(140): warning: variable "MMA_M" was declared but never referenced
          detected during:
            instantiation of "void flash::compute_attn_1rowblock<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params,Is_Q_split_K>(const Params &, int, int, int) [with Kernel_traits=Flash_fwd_kernel_traits<128, 128, 64, 4, false, false, cutlass::half_t, Flash_kernel_traits<128, 128, 64, 4, cutlass::half_t>>, Is_dropout=false, Is_causal=false, Is_even_N=true, Is_even_K=false, Return_softmax=false, Params=Flash_fwd_params, Is_Q_split_K=false]" 
(589): here
            instantiation of "void flash::compute_attn<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &) [with Kernel_traits=Flash_fwd_kernel_traits<128, 128, 64, 4, false, false, cutlass::half_t, Flash_kernel_traits<128, 128, 64, 4, cutlass::half_t>>, Is_dropout=false, Is_causal=false, Is_even_N=true, Is_even_K=false, Return_softmax=false, Params=Flash_fwd_params]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(15): here
            instantiation of "void flash_fwd_kernel<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax>(Flash_fwd_params) [with Kernel_traits=Flash_fwd_kernel_traits<128, 128, 64, 4, false, false, cutlass::half_t, Flash_kernel_traits<128, 128, 64, 4, cutlass::half_t>>, Is_dropout=false, Is_causal=false, Is_even_N=true, Is_even_K=false, Return_softmax=false]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(34): here
            instantiation of "void run_flash_fwd<Kernel_traits,Is_dropout,Is_causal>(Flash_fwd_params &, cudaStream_t) [with Kernel_traits=Flash_fwd_kernel_traits<128, 128, 64, 4, false, false, cutlass::half_t, Flash_kernel_traits<128, 128, 64, 4, cutlass::half_t>>, Is_dropout=false, Is_causal=false]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(118): here
            instantiation of "void run_mha_fwd_hdim128<T>(Flash_fwd_params &, cudaStream_t) [with T=cutlass::half_t]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_hdim128_fp16_sm80.cu(31): here

/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_kernel.h(276): warning: variable "tQrQ" was declared but never referenced
          detected during:
            instantiation of "void flash::compute_attn_1rowblock<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params,Is_Q_split_K>(const Params &, int, int, int) [with Kernel_traits=Flash_fwd_kernel_traits<128, 128, 64, 4, false, false, cutlass::half_t, Flash_kernel_traits<128, 128, 64, 4, cutlass::half_t>>, Is_dropout=false, Is_causal=false, Is_even_N=true, Is_even_K=false, Return_softmax=false, Params=Flash_fwd_params, Is_Q_split_K=false]" 
(589): here
            instantiation of "void flash::compute_attn<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &) [with Kernel_traits=Flash_fwd_kernel_traits<128, 128, 64, 4, false, false, cutlass::half_t, Flash_kernel_traits<128, 128, 64, 4, cutlass::half_t>>, Is_dropout=false, Is_causal=false, Is_even_N=true, Is_even_K=false, Return_softmax=false, Params=Flash_fwd_params]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(15): here
            instantiation of "void flash_fwd_kernel<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax>(Flash_fwd_params) [with Kernel_traits=Flash_fwd_kernel_traits<128, 128, 64, 4, false, false, cutlass::half_t, Flash_kernel_traits<128, 128, 64, 4, cutlass::half_t>>, Is_dropout=false, Is_causal=false, Is_even_N=true, Is_even_K=false, Return_softmax=false]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(34): here
            instantiation of "void run_flash_fwd<Kernel_traits,Is_dropout,Is_causal>(Flash_fwd_params &, cudaStream_t) [with Kernel_traits=Flash_fwd_kernel_traits<128, 128, 64, 4, false, false, cutlass::half_t, Flash_kernel_traits<128, 128, 64, 4, cutlass::half_t>>, Is_dropout=false, Is_causal=false]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(118): here
            instantiation of "void run_mha_fwd_hdim128<T>(Flash_fwd_params &, cudaStream_t) [with T=cutlass::half_t]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_hdim128_fp16_sm80.cu(31): here

/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_kernel.h(140): warning: variable "MMA_M" was declared but never referenced
          detected during:
            instantiation of "void flash::compute_attn_1rowblock<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params,Is_Q_split_K>(const Params &, int, int, int) [with Kernel_traits=Flash_fwd_kernel_traits<128, 128, 64, 4, false, false, cutlass::half_t, Flash_kernel_traits<128, 128, 64, 4, cutlass::half_t>>, Is_dropout=false, Is_causal=false, Is_even_N=false, Is_even_K=true, Return_softmax=false, Params=Flash_fwd_params, Is_Q_split_K=false]" 
(589): here
            instantiation of "void flash::compute_attn<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &) [with Kernel_traits=Flash_fwd_kernel_traits<128, 128, 64, 4, false, false, cutlass::half_t, Flash_kernel_traits<128, 128, 64, 4, cutlass::half_t>>, Is_dropout=false, Is_causal=false, Is_even_N=false, Is_even_K=true, Return_softmax=false, Params=Flash_fwd_params]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(15): here
            instantiation of "void flash_fwd_kernel<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax>(Flash_fwd_params) [with Kernel_traits=Flash_fwd_kernel_traits<128, 128, 64, 4, false, false, cutlass::half_t, Flash_kernel_traits<128, 128, 64, 4, cutlass::half_t>>, Is_dropout=false, Is_causal=false, Is_even_N=false, Is_even_K=true, Return_softmax=false]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(34): here
            instantiation of "void run_flash_fwd<Kernel_traits,Is_dropout,Is_causal>(Flash_fwd_params &, cudaStream_t) [with Kernel_traits=Flash_fwd_kernel_traits<128, 128, 64, 4, false, false, cutlass::half_t, Flash_kernel_traits<128, 128, 64, 4, cutlass::half_t>>, Is_dropout=false, Is_causal=false]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(118): here
            instantiation of "void run_mha_fwd_hdim128<T>(Flash_fwd_params &, cudaStream_t) [with T=cutlass::half_t]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_hdim128_fp16_sm80.cu(31): here

/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_kernel.h(276): warning: variable "tQrQ" was declared but never referenced
          detected during:
            instantiation of "void flash::compute_attn_1rowblock<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params,Is_Q_split_K>(const Params &, int, int, int) [with Kernel_traits=Flash_fwd_kernel_traits<128, 128, 64, 4, false, false, cutlass::half_t, Flash_kernel_traits<128, 128, 64, 4, cutlass::half_t>>, Is_dropout=false, Is_causal=false, Is_even_N=false, Is_even_K=true, Return_softmax=false, Params=Flash_fwd_params, Is_Q_split_K=false]" 
(589): here
            instantiation of "void flash::compute_attn<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &) [with Kernel_traits=Flash_fwd_kernel_traits<128, 128, 64, 4, false, false, cutlass::half_t, Flash_kernel_traits<128, 128, 64, 4, cutlass::half_t>>, Is_dropout=false, Is_causal=false, Is_even_N=false, Is_even_K=true, Return_softmax=false, Params=Flash_fwd_params]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(15): here
            instantiation of "void flash_fwd_kernel<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax>(Flash_fwd_params) [with Kernel_traits=Flash_fwd_kernel_traits<128, 128, 64, 4, false, false, cutlass::half_t, Flash_kernel_traits<128, 128, 64, 4, cutlass::half_t>>, Is_dropout=false, Is_causal=false, Is_even_N=false, Is_even_K=true, Return_softmax=false]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(34): here
            instantiation of "void run_flash_fwd<Kernel_traits,Is_dropout,Is_causal>(Flash_fwd_params &, cudaStream_t) [with Kernel_traits=Flash_fwd_kernel_traits<128, 128, 64, 4, false, false, cutlass::half_t, Flash_kernel_traits<128, 128, 64, 4, cutlass::half_t>>, Is_dropout=false, Is_causal=false]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(118): here
            instantiation of "void run_mha_fwd_hdim128<T>(Flash_fwd_params &, cudaStream_t) [with T=cutlass::half_t]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_hdim128_fp16_sm80.cu(31): here

/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_kernel.h(140): warning: variable "MMA_M" was declared but never referenced
          detected during:
            instantiation of "void flash::compute_attn_1rowblock<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params,Is_Q_split_K>(const Params &, int, int, int) [with Kernel_traits=Flash_fwd_kernel_traits<128, 128, 64, 4, false, false, cutlass::half_t, Flash_kernel_traits<128, 128, 64, 4, cutlass::half_t>>, Is_dropout=false, Is_causal=false, Is_even_N=false, Is_even_K=false, Return_softmax=false, Params=Flash_fwd_params, Is_Q_split_K=false]" 
(589): here
            instantiation of "void flash::compute_attn<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &) [with Kernel_traits=Flash_fwd_kernel_traits<128, 128, 64, 4, false, false, cutlass::half_t, Flash_kernel_traits<128, 128, 64, 4, cutlass::half_t>>, Is_dropout=false, Is_causal=false, Is_even_N=false, Is_even_K=false, Return_softmax=false, Params=Flash_fwd_params]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(15): here
            instantiation of "void flash_fwd_kernel<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax>(Flash_fwd_params) [with Kernel_traits=Flash_fwd_kernel_traits<128, 128, 64, 4, false, false, cutlass::half_t, Flash_kernel_traits<128, 128, 64, 4, cutlass::half_t>>, Is_dropout=false, Is_causal=false, Is_even_N=false, Is_even_K=false, Return_softmax=false]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(34): here
            instantiation of "void run_flash_fwd<Kernel_traits,Is_dropout,Is_causal>(Flash_fwd_params &, cudaStream_t) [with Kernel_traits=Flash_fwd_kernel_traits<128, 128, 64, 4, false, false, cutlass::half_t, Flash_kernel_traits<128, 128, 64, 4, cutlass::half_t>>, Is_dropout=false, Is_causal=false]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(118): here
            instantiation of "void run_mha_fwd_hdim128<T>(Flash_fwd_params &, cudaStream_t) [with T=cutlass::half_t]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_hdim128_fp16_sm80.cu(31): here

/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_kernel.h(276): warning: variable "tQrQ" was declared but never referenced
          detected during:
            instantiation of "void flash::compute_attn_1rowblock<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params,Is_Q_split_K>(const Params &, int, int, int) [with Kernel_traits=Flash_fwd_kernel_traits<128, 128, 64, 4, false, false, cutlass::half_t, Flash_kernel_traits<128, 128, 64, 4, cutlass::half_t>>, Is_dropout=false, Is_causal=false, Is_even_N=false, Is_even_K=false, Return_softmax=false, Params=Flash_fwd_params, Is_Q_split_K=false]" 
(589): here
            instantiation of "void flash::compute_attn<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &) [with Kernel_traits=Flash_fwd_kernel_traits<128, 128, 64, 4, false, false, cutlass::half_t, Flash_kernel_traits<128, 128, 64, 4, cutlass::half_t>>, Is_dropout=false, Is_causal=false, Is_even_N=false, Is_even_K=false, Return_softmax=false, Params=Flash_fwd_params]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(15): here
            instantiation of "void flash_fwd_kernel<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax>(Flash_fwd_params) [with Kernel_traits=Flash_fwd_kernel_traits<128, 128, 64, 4, false, false, cutlass::half_t, Flash_kernel_traits<128, 128, 64, 4, cutlass::half_t>>, Is_dropout=false, Is_causal=false, Is_even_N=false, Is_even_K=false, Return_softmax=false]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(34): here
            instantiation of "void run_flash_fwd<Kernel_traits,Is_dropout,Is_causal>(Flash_fwd_params &, cudaStream_t) [with Kernel_traits=Flash_fwd_kernel_traits<128, 128, 64, 4, false, false, cutlass::half_t, Flash_kernel_traits<128, 128, 64, 4, cutlass::half_t>>, Is_dropout=false, Is_causal=false]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(118): here
            instantiation of "void run_mha_fwd_hdim128<T>(Flash_fwd_params &, cudaStream_t) [with T=cutlass::half_t]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_hdim128_fp16_sm80.cu(31): here

ptxas info    : 2 bytes gmem
ptxas info    : Compiling entry function '_Z16flash_fwd_kernelI23Flash_fwd_kernel_traitsILi128ELi128ELi64ELi4ELb0ELb0EN7cutlass6half_tE19Flash_kernel_traitsILi128ELi128ELi64ELi4ES2_EELb0ELb0ELb0ELb0ELb0EEv16Flash_fwd_params' for 'sm_80'
ptxas info    : Function properties for _Z16flash_fwd_kernelI23Flash_fwd_kernel_traitsILi128ELi128ELi64ELi4ELb0ELb0EN7cutlass6half_tE19Flash_kernel_traitsILi128ELi128ELi64ELi4ES2_EELb0ELb0ELb0ELb0ELb0EEv16Flash_fwd_params
    24 bytes stack frame, 32 bytes spill stores, 32 bytes spill loads
ptxas info    : Used 255 registers, 584 bytes cmem[0]
ptxas info    : Compiling entry function '_Z16flash_fwd_kernelI23Flash_fwd_kernel_traitsILi128ELi128ELi64ELi4ELb0ELb0EN7cutlass6half_tE19Flash_kernel_traitsILi128ELi128ELi64ELi4ES2_EELb0ELb0ELb0ELb1ELb0EEv16Flash_fwd_params' for 'sm_80'
ptxas info    : Function properties for _Z16flash_fwd_kernelI23Flash_fwd_kernel_traitsILi128ELi128ELi64ELi4ELb0ELb0EN7cutlass6half_tE19Flash_kernel_traitsILi128ELi128ELi64ELi4ES2_EELb0ELb0ELb0ELb1ELb0EEv16Flash_fwd_params
    16 bytes stack frame, 32 bytes spill stores, 16 bytes spill loads
ptxas info    : Used 255 registers, 584 bytes cmem[0]
ptxas info    : Compiling entry function '_Z16flash_fwd_kernelI23Flash_fwd_kernel_traitsILi128ELi128ELi64ELi4ELb0ELb0EN7cutlass6half_tE19Flash_kernel_traitsILi128ELi128ELi64ELi4ES2_EELb0ELb0ELb1ELb0ELb0EEv16Flash_fwd_params' for 'sm_80'
ptxas info    : Function properties for _Z16flash_fwd_kernelI23Flash_fwd_kernel_traitsILi128ELi128ELi64ELi4ELb0ELb0EN7cutlass6half_tE19Flash_kernel_traitsILi128ELi128ELi64ELi4ES2_EELb0ELb0ELb1ELb0ELb0EEv16Flash_fwd_params
    16 bytes stack frame, 24 bytes spill stores, 24 bytes spill loads
ptxas info    : Used 255 registers, 584 bytes cmem[0]
ptxas info    : Compiling entry function '_Z16flash_fwd_kernelI23Flash_fwd_kernel_traitsILi128ELi128ELi64ELi4ELb0ELb0EN7cutlass6half_tE19Flash_kernel_traitsILi128ELi128ELi64ELi4ES2_EELb0ELb0ELb1ELb1ELb0EEv16Flash_fwd_params' for 'sm_80'
ptxas info    : Function properties for _Z16flash_fwd_kernelI23Flash_fwd_kernel_traitsILi128ELi128ELi64ELi4ELb0ELb0EN7cutlass6half_tE19Flash_kernel_traitsILi128ELi128ELi64ELi4ES2_EELb0ELb0ELb1ELb1ELb0EEv16Flash_fwd_params
    24 bytes stack frame, 36 bytes spill stores, 36 bytes spill loads
ptxas info    : Used 255 registers, 584 bytes cmem[0]
ptxas info    : Compiling entry function '_Z16flash_fwd_kernelI23Flash_fwd_kernel_traitsILi128ELi128ELi32ELi4ELb0ELb0EN7cutlass6half_tE19Flash_kernel_traitsILi128ELi128ELi32ELi4ES2_EELb0ELb0ELb0ELb0ELb0EEv16Flash_fwd_params' for 'sm_80'
ptxas info    : Function properties for _Z16flash_fwd_kernelI23Flash_fwd_kernel_traitsILi128ELi128ELi32ELi4ELb0ELb0EN7cutlass6half_tE19Flash_kernel_traitsILi128ELi128ELi32ELi4ES2_EELb0ELb0ELb0ELb0ELb0EEv16Flash_fwd_params
    0 bytes stack frame, 0 bytes spill stores, 0 bytes spill loads
ptxas info    : Used 243 registers, 584 bytes cmem[0]
ptxas info    : Compiling entry function '_Z16flash_fwd_kernelI23Flash_fwd_kernel_traitsILi128ELi128ELi32ELi4ELb0ELb0EN7cutlass6half_tE19Flash_kernel_traitsILi128ELi128ELi32ELi4ES2_EELb0ELb0ELb0ELb1ELb0EEv16Flash_fwd_params' for 'sm_80'
ptxas info    : Function properties for _Z16flash_fwd_kernelI23Flash_fwd_kernel_traitsILi128ELi128ELi32ELi4ELb0ELb0EN7cutlass6half_tE19Flash_kernel_traitsILi128ELi128ELi32ELi4ES2_EELb0ELb0ELb0ELb1ELb0EEv16Flash_fwd_params
    0 bytes stack frame, 0 bytes spill stores, 0 bytes spill loads
ptxas info    : Used 242 registers, 584 bytes cmem[0]
ptxas info    : Compiling entry function '_Z16flash_fwd_kernelI23Flash_fwd_kernel_traitsILi128ELi128ELi32ELi4ELb0ELb0EN7cutlass6half_tE19Flash_kernel_traitsILi128ELi128ELi32ELi4ES2_EELb0ELb0ELb1ELb0ELb0EEv16Flash_fwd_params' for 'sm_80'
ptxas info    : Function properties for _Z16flash_fwd_kernelI23Flash_fwd_kernel_traitsILi128ELi128ELi32ELi4ELb0ELb0EN7cutlass6half_tE19Flash_kernel_traitsILi128ELi128ELi32ELi4ES2_EELb0ELb0ELb1ELb0ELb0EEv16Flash_fwd_params
    0 bytes stack frame, 0 bytes spill stores, 0 bytes spill loads
ptxas info    : Used 242 registers, 584 bytes cmem[0]
ptxas info    : Compiling entry function '_Z16flash_fwd_kernelI23Flash_fwd_kernel_traitsILi128ELi128ELi32ELi4ELb0ELb0EN7cutlass6half_tE19Flash_kernel_traitsILi128ELi128ELi32ELi4ES2_EELb0ELb0ELb1ELb1ELb0EEv16Flash_fwd_params' for 'sm_80'
ptxas info    : Function properties for _Z16flash_fwd_kernelI23Flash_fwd_kernel_traitsILi128ELi128ELi32ELi4ELb0ELb0EN7cutlass6half_tE19Flash_kernel_traitsILi128ELi128ELi32ELi4ES2_EELb0ELb0ELb1ELb1ELb0EEv16Flash_fwd_params
    0 bytes stack frame, 0 bytes spill stores, 0 bytes spill loads
ptxas info    : Used 242 registers, 584 bytes cmem[0]
ptxas info    : Compiling entry function '_Z16flash_fwd_kernelI23Flash_fwd_kernel_traitsILi128ELi128ELi64ELi4ELb0ELb0EN7cutlass6half_tE19Flash_kernel_traitsILi128ELi128ELi64ELi4ES2_EELb0ELb1ELb0ELb0ELb0EEv16Flash_fwd_params' for 'sm_80'
ptxas info    : Function properties for _Z16flash_fwd_kernelI23Flash_fwd_kernel_traitsILi128ELi128ELi64ELi4ELb0ELb0EN7cutlass6half_tE19Flash_kernel_traitsILi128ELi128ELi64ELi4ES2_EELb0ELb1ELb0ELb0ELb0EEv16Flash_fwd_params
    256 bytes stack frame, 320 bytes spill stores, 356 bytes spill loads
ptxas info    : Used 255 registers, 584 bytes cmem[0]
ptxas info    : Compiling entry function '_Z16flash_fwd_kernelI23Flash_fwd_kernel_traitsILi128ELi128ELi64ELi4ELb0ELb0EN7cutlass6half_tE19Flash_kernel_traitsILi128ELi128ELi64ELi4ES2_EELb0ELb1ELb0ELb1ELb0EEv16Flash_fwd_params' for 'sm_80'
ptxas info    : Function properties for _Z16flash_fwd_kernelI23Flash_fwd_kernel_traitsILi128ELi128ELi64ELi4ELb0ELb0EN7cutlass6half_tE19Flash_kernel_traitsILi128ELi128ELi64ELi4ES2_EELb0ELb1ELb0ELb1ELb0EEv16Flash_fwd_params
    192 bytes stack frame, 292 bytes spill stores, 316 bytes spill loads
ptxas info    : Used 255 registers, 584 bytes cmem[0]
ptxas info    : Compiling entry function '_Z16flash_fwd_kernelI23Flash_fwd_kernel_traitsILi128ELi128ELi64ELi4ELb0ELb0EN7cutlass6half_tE19Flash_kernel_traitsILi128ELi128ELi64ELi4ES2_EELb0ELb1ELb1ELb0ELb0EEv16Flash_fwd_params' for 'sm_80'
ptxas info    : Function properties for _Z16flash_fwd_kernelI23Flash_fwd_kernel_traitsILi128ELi128ELi64ELi4ELb0ELb0EN7cutlass6half_tE19Flash_kernel_traitsILi128ELi128ELi64ELi4ES2_EELb0ELb1ELb1ELb0ELb0EEv16Flash_fwd_params
    128 bytes stack frame, 192 bytes spill stores, 220 bytes spill loads
ptxas info    : Used 255 registers, 584 bytes cmem[0]
ptxas info    : Compiling entry function '_Z16flash_fwd_kernelI23Flash_fwd_kernel_traitsILi128ELi128ELi64ELi4ELb0ELb0EN7cutlass6half_tE19Flash_kernel_traitsILi128ELi128ELi64ELi4ES2_EELb0ELb1ELb1ELb1ELb0EEv16Flash_fwd_params' for 'sm_80'
ptxas info    : Function properties for _Z16flash_fwd_kernelI23Flash_fwd_kernel_traitsILi128ELi128ELi64ELi4ELb0ELb0EN7cutlass6half_tE19Flash_kernel_traitsILi128ELi128ELi64ELi4ES2_EELb0ELb1ELb1ELb1ELb0EEv16Flash_fwd_params
    160 bytes stack frame, 220 bytes spill stores, 180 bytes spill loads
ptxas info    : Used 255 registers, 584 bytes cmem[0]
ptxas info    : Compiling entry function '_Z16flash_fwd_kernelI23Flash_fwd_kernel_traitsILi128ELi64ELi64ELi4ELb0ELb0EN7cutlass6half_tE19Flash_kernel_traitsILi128ELi64ELi64ELi4ES2_EELb0ELb1ELb0ELb0ELb0EEv16Flash_fwd_params' for 'sm_80'
ptxas info    : Function properties for _Z16flash_fwd_kernelI23Flash_fwd_kernel_traitsILi128ELi64ELi64ELi4ELb0ELb0EN7cutlass6half_tE19Flash_kernel_traitsILi128ELi64ELi64ELi4ES2_EELb0ELb1ELb0ELb0ELb0EEv16Flash_fwd_params
    0 bytes stack frame, 0 bytes spill stores, 0 bytes spill loads
ptxas info    : Used 163 registers, 584 bytes cmem[0]
ptxas info    : Compiling entry function '_Z16flash_fwd_kernelI23Flash_fwd_kernel_traitsILi128ELi64ELi64ELi4ELb0ELb0EN7cutlass6half_tE19Flash_kernel_traitsILi128ELi64ELi64ELi4ES2_EELb0ELb1ELb0ELb1ELb0EEv16Flash_fwd_params' for 'sm_80'
ptxas info    : Function properties for _Z16flash_fwd_kernelI23Flash_fwd_kernel_traitsILi128ELi64ELi64ELi4ELb0ELb0EN7cutlass6half_tE19Flash_kernel_traitsILi128ELi64ELi64ELi4ES2_EELb0ELb1ELb0ELb1ELb0EEv16Flash_fwd_params
    0 bytes stack frame, 0 bytes spill stores, 0 bytes spill loads
ptxas info    : Used 160 registers, 584 bytes cmem[0]
ptxas info    : Compiling entry function '_Z16flash_fwd_kernelI23Flash_fwd_kernel_traitsILi128ELi64ELi64ELi4ELb0ELb0EN7cutlass6half_tE19Flash_kernel_traitsILi128ELi64ELi64ELi4ES2_EELb0ELb1ELb1ELb0ELb0EEv16Flash_fwd_params' for 'sm_80'
ptxas info    : Function properties for _Z16flash_fwd_kernelI23Flash_fwd_kernel_traitsILi128ELi64ELi64ELi4ELb0ELb0EN7cutlass6half_tE19Flash_kernel_traitsILi128ELi64ELi64ELi4ES2_EELb0ELb1ELb1ELb0ELb0EEv16Flash_fwd_params
    0 bytes stack frame, 0 bytes spill stores, 0 bytes spill loads
ptxas info    : Used 167 registers, 584 bytes cmem[0]
ptxas info    : Compiling entry function '_Z16flash_fwd_kernelI23Flash_fwd_kernel_traitsILi128ELi64ELi64ELi4ELb0ELb0EN7cutlass6half_tE19Flash_kernel_traitsILi128ELi64ELi64ELi4ES2_EELb0ELb1ELb1ELb1ELb0EEv16Flash_fwd_params' for 'sm_80'
ptxas info    : Function properties for _Z16flash_fwd_kernelI23Flash_fwd_kernel_traitsILi128ELi64ELi64ELi4ELb0ELb0EN7cutlass6half_tE19Flash_kernel_traitsILi128ELi64ELi64ELi4ES2_EELb0ELb1ELb1ELb1ELb0EEv16Flash_fwd_params
    0 bytes stack frame, 0 bytes spill stores, 0 bytes spill loads
ptxas info    : Used 166 registers, 584 bytes cmem[0]
ptxas info    : Compiling entry function '_Z16flash_fwd_kernelI23Flash_fwd_kernel_traitsILi128ELi128ELi32ELi4ELb0ELb0EN7cutlass6half_tE19Flash_kernel_traitsILi128ELi128ELi32ELi4ES2_EELb1ELb0ELb0ELb0ELb0EEv16Flash_fwd_params' for 'sm_80'
ptxas info    : Function properties for _Z16flash_fwd_kernelI23Flash_fwd_kernel_traitsILi128ELi128ELi32ELi4ELb0ELb0EN7cutlass6half_tE19Flash_kernel_traitsILi128ELi128ELi32ELi4ES2_EELb1ELb0ELb0ELb0ELb0EEv16Flash_fwd_params
    0 bytes stack frame, 0 bytes spill stores, 0 bytes spill loads
ptxas info    : Used 252 registers, 584 bytes cmem[0]
ptxas info    : Compiling entry function '_Z16flash_fwd_kernelI23Flash_fwd_kernel_traitsILi128ELi128ELi32ELi4ELb0ELb0EN7cutlass6half_tE19Flash_kernel_traitsILi128ELi128ELi32ELi4ES2_EELb1ELb0ELb0ELb0ELb1EEv16Flash_fwd_params' for 'sm_80'
ptxas info    : Function properties for _Z16flash_fwd_kernelI23Flash_fwd_kernel_traitsILi128ELi128ELi32ELi4ELb0ELb0EN7cutlass6half_tE19Flash_kernel_traitsILi128ELi128ELi32ELi4ES2_EELb1ELb0ELb0ELb0ELb1EEv16Flash_fwd_params
    120 bytes stack frame, 106 bytes spill stores, 106 bytes spill loads
ptxas info    : Used 255 registers, 584 bytes cmem[0]
ptxas info    : Compiling entry function '_Z16flash_fwd_kernelI23Flash_fwd_kernel_traitsILi128ELi128ELi32ELi4ELb0ELb0EN7cutlass6half_tE19Flash_kernel_traitsILi128ELi128ELi32ELi4ES2_EELb1ELb0ELb0ELb1ELb0EEv16Flash_fwd_params' for 'sm_80'
ptxas info    : Function properties for _Z16flash_fwd_kernelI23Flash_fwd_kernel_traitsILi128ELi128ELi32ELi4ELb0ELb0EN7cutlass6half_tE19Flash_kernel_traitsILi128ELi128ELi32ELi4ES2_EELb1ELb0ELb0ELb1ELb0EEv16Flash_fwd_params
    0 bytes stack frame, 0 bytes spill stores, 0 bytes spill loads
ptxas info    : Used 252 registers, 584 bytes cmem[0]
ptxas info    : Compiling entry function '_Z16flash_fwd_kernelI23Flash_fwd_kernel_traitsILi128ELi128ELi32ELi4ELb0ELb0EN7cutlass6half_tE19Flash_kernel_traitsILi128ELi128ELi32ELi4ES2_EELb1ELb0ELb0ELb1ELb1EEv16Flash_fwd_params' for 'sm_80'
ptxas info    : Function properties for _Z16flash_fwd_kernelI23Flash_fwd_kernel_traitsILi128ELi128ELi32ELi4ELb0ELb0EN7cutlass6half_tE19Flash_kernel_traitsILi128ELi128ELi32ELi4ES2_EELb1ELb0ELb0ELb1ELb1EEv16Flash_fwd_params
    152 bytes stack frame, 92 bytes spill stores, 122 bytes spill loads
ptxas info    : Used 255 registers, 584 bytes cmem[0]
ptxas info    : Compiling entry function '_Z16flash_fwd_kernelI23Flash_fwd_kernel_traitsILi128ELi128ELi32ELi4ELb0ELb0EN7cutlass6half_tE19Flash_kernel_traitsILi128ELi128ELi32ELi4ES2_EELb1ELb0ELb1ELb0ELb0EEv16Flash_fwd_params' for 'sm_80'
ptxas info    : Function properties for _Z16flash_fwd_kernelI23Flash_fwd_kernel_traitsILi128ELi128ELi32ELi4ELb0ELb0EN7cutlass6half_tE19Flash_kernel_traitsILi128ELi128ELi32ELi4ES2_EELb1ELb0ELb1ELb0ELb0EEv16Flash_fwd_params
    0 bytes stack frame, 0 bytes spill stores, 0 bytes spill loads
ptxas info    : Used 252 registers, 584 bytes cmem[0]
ptxas info    : Compiling entry function '_Z16flash_fwd_kernelI23Flash_fwd_kernel_traitsILi128ELi128ELi32ELi4ELb0ELb0EN7cutlass6half_tE19Flash_kernel_traitsILi128ELi128ELi32ELi4ES2_EELb1ELb0ELb1ELb0ELb1EEv16Flash_fwd_params' for 'sm_80'
ptxas info    : Function properties for _Z16flash_fwd_kernelI23Flash_fwd_kernel_traitsILi128ELi128ELi32ELi4ELb0ELb0EN7cutlass6half_tE19Flash_kernel_traitsILi128ELi128ELi32ELi4ES2_EELb1ELb0ELb1ELb0ELb1EEv16Flash_fwd_params
    72 bytes stack frame, 74 bytes spill stores, 74 bytes spill loads
ptxas info    : Used 255 registers, 584 bytes cmem[0]
ptxas info    : Compiling entry function '_Z16flash_fwd_kernelI23Flash_fwd_kernel_traitsILi128ELi128ELi32ELi4ELb0ELb0EN7cutlass6half_tE19Flash_kernel_traitsILi128ELi128ELi32ELi4ES2_EELb1ELb0ELb1ELb1ELb0EEv16Flash_fwd_params' for 'sm_80'
ptxas info    : Function properties for _Z16flash_fwd_kernelI23Flash_fwd_kernel_traitsILi128ELi128ELi32ELi4ELb0ELb0EN7cutlass6half_tE19Flash_kernel_traitsILi128ELi128ELi32ELi4ES2_EELb1ELb0ELb1ELb1ELb0EEv16Flash_fwd_params
    0 bytes stack frame, 0 bytes spill stores, 0 bytes spill loads
ptxas info    : Used 252 registers, 584 bytes cmem[0]
ptxas info    : Compiling entry function '_Z16flash_fwd_kernelI23Flash_fwd_kernel_traitsILi128ELi128ELi32ELi4ELb0ELb0EN7cutlass6half_tE19Flash_kernel_traitsILi128ELi128ELi32ELi4ES2_EELb1ELb0ELb1ELb1ELb1EEv16Flash_fwd_params' for 'sm_80'
ptxas info    : Function properties for _Z16flash_fwd_kernelI23Flash_fwd_kernel_traitsILi128ELi128ELi32ELi4ELb0ELb0EN7cutlass6half_tE19Flash_kernel_traitsILi128ELi128ELi32ELi4ES2_EELb1ELb0ELb1ELb1ELb1EEv16Flash_fwd_params
    80 bytes stack frame, 84 bytes spill stores, 102 bytes spill loads
ptxas info    : Used 255 registers, 584 bytes cmem[0]
ptxas info    : Compiling entry function '_Z16flash_fwd_kernelI23Flash_fwd_kernel_traitsILi128ELi128ELi32ELi4ELb0ELb0EN7cutlass6half_tE19Flash_kernel_traitsILi128ELi128ELi32ELi4ES2_EELb1ELb1ELb0ELb0ELb0EEv16Flash_fwd_params' for 'sm_80'
ptxas info    : Function properties for _Z16flash_fwd_kernelI23Flash_fwd_kernel_traitsILi128ELi128ELi32ELi4ELb0ELb0EN7cutlass6half_tE19Flash_kernel_traitsILi128ELi128ELi32ELi4ES2_EELb1ELb1ELb0ELb0ELb0EEv16Flash_fwd_params
    16 bytes stack frame, 16 bytes spill stores, 24 bytes spill loads
ptxas info    : Used 255 registers, 584 bytes cmem[0]
ptxas info    : Compiling entry function '_Z16flash_fwd_kernelI23Flash_fwd_kernel_traitsILi128ELi128ELi32ELi4ELb0ELb0EN7cutlass6half_tE19Flash_kernel_traitsILi128ELi128ELi32ELi4ES2_EELb1ELb1ELb0ELb0ELb1EEv16Flash_fwd_params' for 'sm_80'
ptxas info    : Function properties for _Z16flash_fwd_kernelI23Flash_fwd_kernel_traitsILi128ELi128ELi32ELi4ELb0ELb0EN7cutlass6half_tE19Flash_kernel_traitsILi128ELi128ELi32ELi4ES2_EELb1ELb1ELb0ELb0ELb1EEv16Flash_fwd_params
    152 bytes stack frame, 190 bytes spill stores, 246 bytes spill loads
ptxas info    : Used 255 registers, 584 bytes cmem[0]
ptxas info    : Compiling entry function '_Z16flash_fwd_kernelI23Flash_fwd_kernel_traitsILi128ELi128ELi32ELi4ELb0ELb0EN7cutlass6half_tE19Flash_kernel_traitsILi128ELi128ELi32ELi4ES2_EELb1ELb1ELb0ELb1ELb0EEv16Flash_fwd_params' for 'sm_80'
ptxas info    : Function properties for _Z16flash_fwd_kernelI23Flash_fwd_kernel_traitsILi128ELi128ELi32ELi4ELb0ELb0EN7cutlass6half_tE19Flash_kernel_traitsILi128ELi128ELi32ELi4ES2_EELb1ELb1ELb0ELb1ELb0EEv16Flash_fwd_params
    16 bytes stack frame, 12 bytes spill stores, 28 bytes spill loads
ptxas info    : Used 255 registers, 584 bytes cmem[0]
ptxas info    : Compiling entry function '_Z16flash_fwd_kernelI23Flash_fwd_kernel_traitsILi128ELi128ELi32ELi4ELb0ELb0EN7cutlass6half_tE19Flash_kernel_traitsILi128ELi128ELi32ELi4ES2_EELb1ELb1ELb0ELb1ELb1EEv16Flash_fwd_params' for 'sm_80'
ptxas info    : Function properties for _Z16flash_fwd_kernelI23Flash_fwd_kernel_traitsILi128ELi128ELi32ELi4ELb0ELb0EN7cutlass6half_tE19Flash_kernel_traitsILi128ELi128ELi32ELi4ES2_EELb1ELb1ELb0ELb1ELb1EEv16Flash_fwd_params
    160 bytes stack frame, 180 bytes spill stores, 266 bytes spill loads
ptxas info    : Used 255 registers, 584 bytes cmem[0]
ptxas info    : Compiling entry function '_Z16flash_fwd_kernelI23Flash_fwd_kernel_traitsILi128ELi128ELi32ELi4ELb0ELb0EN7cutlass6half_tE19Flash_kernel_traitsILi128ELi128ELi32ELi4ES2_EELb1ELb1ELb1ELb0ELb0EEv16Flash_fwd_params' for 'sm_80'
ptxas info    : Function properties for _Z16flash_fwd_kernelI23Flash_fwd_kernel_traitsILi128ELi128ELi32ELi4ELb0ELb0EN7cutlass6half_tE19Flash_kernel_traitsILi128ELi128ELi32ELi4ES2_EELb1ELb1ELb1ELb0ELb0EEv16Flash_fwd_params
    16 bytes stack frame, 16 bytes spill stores, 24 bytes spill loads
ptxas info    : Used 255 registers, 584 bytes cmem[0]
ptxas info    : Compiling entry function '_Z16flash_fwd_kernelI23Flash_fwd_kernel_traitsILi128ELi128ELi32ELi4ELb0ELb0EN7cutlass6half_tE19Flash_kernel_traitsILi128ELi128ELi32ELi4ES2_EELb1ELb1ELb1ELb0ELb1EEv16Flash_fwd_params' for 'sm_80'
ptxas info    : Function properties for _Z16flash_fwd_kernelI23Flash_fwd_kernel_traitsILi128ELi128ELi32ELi4ELb0ELb0EN7cutlass6half_tE19Flash_kernel_traitsILi128ELi128ELi32ELi4ES2_EELb1ELb1ELb1ELb0ELb1EEv16Flash_fwd_params
    112 bytes stack frame, 150 bytes spill stores, 174 bytes spill loads
ptxas info    : Used 255 registers, 584 bytes cmem[0]
ptxas info    : Compiling entry function '_Z16flash_fwd_kernelI23Flash_fwd_kernel_traitsILi128ELi128ELi32ELi4ELb0ELb0EN7cutlass6half_tE19Flash_kernel_traitsILi128ELi128ELi32ELi4ES2_EELb1ELb1ELb1ELb1ELb0EEv16Flash_fwd_params' for 'sm_80'
ptxas info    : Function properties for _Z16flash_fwd_kernelI23Flash_fwd_kernel_traitsILi128ELi128ELi32ELi4ELb0ELb0EN7cutlass6half_tE19Flash_kernel_traitsILi128ELi128ELi32ELi4ES2_EELb1ELb1ELb1ELb1ELb0EEv16Flash_fwd_params
    8 bytes stack frame, 4 bytes spill stores, 4 bytes spill loads
ptxas info    : Used 255 registers, 584 bytes cmem[0]
ptxas info    : Compiling entry function '_Z16flash_fwd_kernelI23Flash_fwd_kernel_traitsILi128ELi128ELi32ELi4ELb0ELb0EN7cutlass6half_tE19Flash_kernel_traitsILi128ELi128ELi32ELi4ES2_EELb1ELb1ELb1ELb1ELb1EEv16Flash_fwd_params' for 'sm_80'
ptxas info    : Function properties for _Z16flash_fwd_kernelI23Flash_fwd_kernel_traitsILi128ELi128ELi32ELi4ELb0ELb0EN7cutlass6half_tE19Flash_kernel_traitsILi128ELi128ELi32ELi4ES2_EELb1ELb1ELb1ELb1ELb1EEv16Flash_fwd_params
    144 bytes stack frame, 180 bytes spill stores, 248 bytes spill loads
ptxas info    : Used 255 registers, 584 bytes cmem[0]
/anaconda/envs/flashattention/lib/python3.9/site-packages/torch/include/c10/core/SymInt.h(84): warning: integer conversion resulted in a change of sign

[13/37] /usr/local/cuda-11.4/bin/nvcc  -I/home/v-feiychen/flash-attention/csrc/flash_attn -I/home/v-feiychen/flash-attention/csrc/flash_attn/src -I/home/v-feiychen/flash-attention/csrc/cutlass/include -I/anaconda/envs/flashattention/lib/python3.9/site-packages/torch/include -I/anaconda/envs/flashattention/lib/python3.9/site-packages/torch/include/torch/csrc/api/include -I/anaconda/envs/flashattention/lib/python3.9/site-packages/torch/include/TH -I/anaconda/envs/flashattention/lib/python3.9/site-packages/torch/include/THC -I/usr/local/cuda-11.4/include -I/anaconda/envs/flashattention/include/python3.9 -c -c /home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_bwd_hdim96_fp16_sm80.cu -o /home/v-feiychen/flash-attention/build/temp.linux-x86_64-cpython-39/csrc/flash_attn/src/flash_bwd_hdim96_fp16_sm80.o -D__CUDA_NO_HALF_OPERATORS__ -D__CUDA_NO_HALF_CONVERSIONS__ -D__CUDA_NO_BFLOAT16_CONVERSIONS__ -D__CUDA_NO_HALF2_OPERATORS__ --expt-relaxed-constexpr --compiler-options ''"'"'-fPIC'"'"'' -O3 -std=c++17 -U__CUDA_NO_HALF_OPERATORS__ -U__CUDA_NO_HALF_CONVERSIONS__ -U__CUDA_NO_HALF2_OPERATORS__ -U__CUDA_NO_BFLOAT16_CONVERSIONS__ --expt-relaxed-constexpr --expt-extended-lambda --use_fast_math --ptxas-options=-v -lineinfo -gencode arch=compute_80,code=sm_80 --threads 4 -DTORCH_API_INCLUDE_EXTENSION_H '-DPYBIND11_COMPILER_TYPE="_gcc"' '-DPYBIND11_STDLIB="_libstdcpp"' '-DPYBIND11_BUILD_ABI="_cxxabi1011"' -DTORCH_EXTENSION_NAME=flash_attn_2_cuda -D_GLIBCXX_USE_CXX11_ABI=0
/anaconda/envs/flashattention/lib/python3.9/site-packages/torch/include/c10/core/SymInt.h(84): warning: integer conversion resulted in a change of sign

ptxas info    : 2 bytes gmem
ptxas info    : Compiling entry function '_Z44flash_bwd_dq_dk_dv_loop_seqk_parallel_kernelI23Flash_bwd_kernel_traitsILi96ELi64ELi128ELi8ELi2ELi4ELi4ELb1ELb0EN7cutlass6half_tE19Flash_kernel_traitsILi96ELi64ELi128ELi8ES2_EELb0ELb0ELb0ELb0EEv16Flash_bwd_params' for 'sm_80'
ptxas info    : Function properties for _Z44flash_bwd_dq_dk_dv_loop_seqk_parallel_kernelI23Flash_bwd_kernel_traitsILi96ELi64ELi128ELi8ELi2ELi4ELi4ELb1ELb0EN7cutlass6half_tE19Flash_kernel_traitsILi96ELi64ELi128ELi8ES2_EELb0ELb0ELb0ELb0EEv16Flash_bwd_params
    16 bytes stack frame, 12 bytes spill stores, 12 bytes spill loads
ptxas info    : Used 255 registers, 696 bytes cmem[0]
ptxas info    : Compiling entry function '_Z44flash_bwd_dq_dk_dv_loop_seqk_parallel_kernelI23Flash_bwd_kernel_traitsILi96ELi64ELi128ELi8ELi2ELi4ELi4ELb1ELb0EN7cutlass6half_tE19Flash_kernel_traitsILi96ELi64ELi128ELi8ES2_EELb0ELb0ELb0ELb1EEv16Flash_bwd_params' for 'sm_80'
ptxas info    : Function properties for _Z44flash_bwd_dq_dk_dv_loop_seqk_parallel_kernelI23Flash_bwd_kernel_traitsILi96ELi64ELi128ELi8ELi2ELi4ELi4ELb1ELb0EN7cutlass6half_tE19Flash_kernel_traitsILi96ELi64ELi128ELi8ES2_EELb0ELb0ELb0ELb1EEv16Flash_bwd_params
    24 bytes stack frame, 20 bytes spill stores, 20 bytes spill loads
ptxas info    : Used 255 registers, 696 bytes cmem[0]
ptxas info    : Compiling entry function '_Z44flash_bwd_dq_dk_dv_loop_seqk_parallel_kernelI23Flash_bwd_kernel_traitsILi96ELi64ELi128ELi8ELi2ELi4ELi4ELb1ELb0EN7cutlass6half_tE19Flash_kernel_traitsILi96ELi64ELi128ELi8ES2_EELb0ELb0ELb1ELb0EEv16Flash_bwd_params' for 'sm_80'
ptxas info    : Function properties for _Z44flash_bwd_dq_dk_dv_loop_seqk_parallel_kernelI23Flash_bwd_kernel_traitsILi96ELi64ELi128ELi8ELi2ELi4ELi4ELb1ELb0EN7cutlass6half_tE19Flash_kernel_traitsILi96ELi64ELi128ELi8ES2_EELb0ELb0ELb1ELb0EEv16Flash_bwd_params
    0 bytes stack frame, 0 bytes spill stores, 0 bytes spill loads
ptxas info    : Used 254 registers, 696 bytes cmem[0]
ptxas info    : Compiling entry function '_Z44flash_bwd_dq_dk_dv_loop_seqk_parallel_kernelI23Flash_bwd_kernel_traitsILi96ELi64ELi128ELi8ELi2ELi4ELi4ELb1ELb0EN7cutlass6half_tE19Flash_kernel_traitsILi96ELi64ELi128ELi8ES2_EELb0ELb0ELb1ELb1EEv16Flash_bwd_params' for 'sm_80'
ptxas info    : Function properties for _Z44flash_bwd_dq_dk_dv_loop_seqk_parallel_kernelI23Flash_bwd_kernel_traitsILi96ELi64ELi128ELi8ELi2ELi4ELi4ELb1ELb0EN7cutlass6half_tE19Flash_kernel_traitsILi96ELi64ELi128ELi8ES2_EELb0ELb0ELb1ELb1EEv16Flash_bwd_params
    0 bytes stack frame, 0 bytes spill stores, 0 bytes spill loads
ptxas info    : Used 254 registers, 696 bytes cmem[0]
ptxas info    : Compiling entry function '_Z44flash_bwd_dq_dk_dv_loop_seqk_parallel_kernelI23Flash_bwd_kernel_traitsILi96ELi64ELi128ELi8ELi2ELi4ELi4ELb1ELb0EN7cutlass6half_tE19Flash_kernel_traitsILi96ELi64ELi128ELi8ES2_EELb0ELb1ELb0ELb0EEv16Flash_bwd_params' for 'sm_80'
ptxas info    : Function properties for _Z44flash_bwd_dq_dk_dv_loop_seqk_parallel_kernelI23Flash_bwd_kernel_traitsILi96ELi64ELi128ELi8ELi2ELi4ELi4ELb1ELb0EN7cutlass6half_tE19Flash_kernel_traitsILi96ELi64ELi128ELi8ES2_EELb0ELb1ELb0ELb0EEv16Flash_bwd_params
    0 bytes stack frame, 0 bytes spill stores, 0 bytes spill loads
ptxas info    : Used 255 registers, 696 bytes cmem[0]
ptxas info    : Compiling entry function '_Z44flash_bwd_dq_dk_dv_loop_seqk_parallel_kernelI23Flash_bwd_kernel_traitsILi96ELi64ELi128ELi8ELi2ELi4ELi4ELb1ELb0EN7cutlass6half_tE19Flash_kernel_traitsILi96ELi64ELi128ELi8ES2_EELb0ELb1ELb0ELb1EEv16Flash_bwd_params' for 'sm_80'
ptxas info    : Function properties for _Z44flash_bwd_dq_dk_dv_loop_seqk_parallel_kernelI23Flash_bwd_kernel_traitsILi96ELi64ELi128ELi8ELi2ELi4ELi4ELb1ELb0EN7cutlass6half_tE19Flash_kernel_traitsILi96ELi64ELi128ELi8ES2_EELb0ELb1ELb0ELb1EEv16Flash_bwd_params
    0 bytes stack frame, 0 bytes spill stores, 0 bytes spill loads
ptxas info    : Used 254 registers, 696 bytes cmem[0]
ptxas info    : Compiling entry function '_Z44flash_bwd_dq_dk_dv_loop_seqk_parallel_kernelI23Flash_bwd_kernel_traitsILi96ELi64ELi128ELi8ELi2ELi4ELi4ELb1ELb0EN7cutlass6half_tE19Flash_kernel_traitsILi96ELi64ELi128ELi8ES2_EELb0ELb1ELb1ELb0EEv16Flash_bwd_params' for 'sm_80'
ptxas info    : Function properties for _Z44flash_bwd_dq_dk_dv_loop_seqk_parallel_kernelI23Flash_bwd_kernel_traitsILi96ELi64ELi128ELi8ELi2ELi4ELi4ELb1ELb0EN7cutlass6half_tE19Flash_kernel_traitsILi96ELi64ELi128ELi8ES2_EELb0ELb1ELb1ELb0EEv16Flash_bwd_params
    0 bytes stack frame, 0 bytes spill stores, 0 bytes spill loads
ptxas info    : Used 254 registers, 696 bytes cmem[0]
ptxas info    : Compiling entry function '_Z44flash_bwd_dq_dk_dv_loop_seqk_parallel_kernelI23Flash_bwd_kernel_traitsILi96ELi64ELi128ELi8ELi2ELi4ELi4ELb1ELb0EN7cutlass6half_tE19Flash_kernel_traitsILi96ELi64ELi128ELi8ES2_EELb0ELb1ELb1ELb1EEv16Flash_bwd_params' for 'sm_80'
ptxas info    : Function properties for _Z44flash_bwd_dq_dk_dv_loop_seqk_parallel_kernelI23Flash_bwd_kernel_traitsILi96ELi64ELi128ELi8ELi2ELi4ELi4ELb1ELb0EN7cutlass6half_tE19Flash_kernel_traitsILi96ELi64ELi128ELi8ES2_EELb0ELb1ELb1ELb1EEv16Flash_bwd_params
    0 bytes stack frame, 0 bytes spill stores, 0 bytes spill loads
ptxas info    : Used 254 registers, 696 bytes cmem[0]
ptxas info    : Compiling entry function '_Z27flash_bwd_convert_dq_kernelI23Flash_bwd_kernel_traitsILi96ELi64ELi128ELi8ELi2ELi4ELi4ELb1ELb0EN7cutlass6half_tE19Flash_kernel_traitsILi96ELi64ELi128ELi8ES2_EEEv16Flash_bwd_params' for 'sm_80'
ptxas info    : Function properties for _Z27flash_bwd_convert_dq_kernelI23Flash_bwd_kernel_traitsILi96ELi64ELi128ELi8ELi2ELi4ELi4ELb1ELb0EN7cutlass6half_tE19Flash_kernel_traitsILi96ELi64ELi128ELi8ES2_EEEv16Flash_bwd_params
    0 bytes stack frame, 0 bytes spill stores, 0 bytes spill loads
ptxas info    : Used 32 registers, 696 bytes cmem[0]
ptxas info    : Compiling entry function '_Z44flash_bwd_dq_dk_dv_loop_seqk_parallel_kernelI23Flash_bwd_kernel_traitsILi96ELi64ELi128ELi8ELi2ELi4ELi4ELb1ELb0EN7cutlass6half_tE19Flash_kernel_traitsILi96ELi64ELi128ELi8ES2_EELb1ELb0ELb0ELb0EEv16Flash_bwd_params' for 'sm_80'
ptxas info    : Function properties for _Z44flash_bwd_dq_dk_dv_loop_seqk_parallel_kernelI23Flash_bwd_kernel_traitsILi96ELi64ELi128ELi8ELi2ELi4ELi4ELb1ELb0EN7cutlass6half_tE19Flash_kernel_traitsILi96ELi64ELi128ELi8ES2_EELb1ELb0ELb0ELb0EEv16Flash_bwd_params
    16 bytes stack frame, 12 bytes spill stores, 12 bytes spill loads
ptxas info    : Used 255 registers, 696 bytes cmem[0]
ptxas info    : Compiling entry function '_Z44flash_bwd_dq_dk_dv_loop_seqk_parallel_kernelI23Flash_bwd_kernel_traitsILi96ELi64ELi128ELi8ELi2ELi4ELi4ELb1ELb0EN7cutlass6half_tE19Flash_kernel_traitsILi96ELi64ELi128ELi8ES2_EELb1ELb0ELb0ELb1EEv16Flash_bwd_params' for 'sm_80'
ptxas info    : Function properties for _Z44flash_bwd_dq_dk_dv_loop_seqk_parallel_kernelI23Flash_bwd_kernel_traitsILi96ELi64ELi128ELi8ELi2ELi4ELi4ELb1ELb0EN7cutlass6half_tE19Flash_kernel_traitsILi96ELi64ELi128ELi8ES2_EELb1ELb0ELb0ELb1EEv16Flash_bwd_params
    0 bytes stack frame, 0 bytes spill stores, 0 bytes spill loads
ptxas info    : Used 254 registers, 696 bytes cmem[0]
ptxas info    : Compiling entry function '_Z44flash_bwd_dq_dk_dv_loop_seqk_parallel_kernelI23Flash_bwd_kernel_traitsILi96ELi64ELi128ELi8ELi2ELi4ELi4ELb1ELb0EN7cutlass6half_tE19Flash_kernel_traitsILi96ELi64ELi128ELi8ES2_EELb1ELb0ELb1ELb0EEv16Flash_bwd_params' for 'sm_80'
ptxas info    : Function properties for _Z44flash_bwd_dq_dk_dv_loop_seqk_parallel_kernelI23Flash_bwd_kernel_traitsILi96ELi64ELi128ELi8ELi2ELi4ELi4ELb1ELb0EN7cutlass6half_tE19Flash_kernel_traitsILi96ELi64ELi128ELi8ES2_EELb1ELb0ELb1ELb0EEv16Flash_bwd_params
    8 bytes stack frame, 4 bytes spill stores, 4 bytes spill loads
ptxas info    : Used 255 registers, 696 bytes cmem[0]
ptxas info    : Compiling entry function '_Z44flash_bwd_dq_dk_dv_loop_seqk_parallel_kernelI23Flash_bwd_kernel_traitsILi96ELi64ELi128ELi8ELi2ELi4ELi4ELb1ELb0EN7cutlass6half_tE19Flash_kernel_traitsILi96ELi64ELi128ELi8ES2_EELb1ELb0ELb1ELb1EEv16Flash_bwd_params' for 'sm_80'
ptxas info    : Function properties for _Z44flash_bwd_dq_dk_dv_loop_seqk_parallel_kernelI23Flash_bwd_kernel_traitsILi96ELi64ELi128ELi8ELi2ELi4ELi4ELb1ELb0EN7cutlass6half_tE19Flash_kernel_traitsILi96ELi64ELi128ELi8ES2_EELb1ELb0ELb1ELb1EEv16Flash_bwd_params
    0 bytes stack frame, 0 bytes spill stores, 0 bytes spill loads
ptxas info    : Used 255 registers, 696 bytes cmem[0]
ptxas info    : Compiling entry function '_Z44flash_bwd_dq_dk_dv_loop_seqk_parallel_kernelI23Flash_bwd_kernel_traitsILi96ELi64ELi128ELi8ELi2ELi4ELi4ELb1ELb0EN7cutlass6half_tE19Flash_kernel_traitsILi96ELi64ELi128ELi8ES2_EELb1ELb1ELb0ELb0EEv16Flash_bwd_params' for 'sm_80'
ptxas info    : Function properties for _Z44flash_bwd_dq_dk_dv_loop_seqk_parallel_kernelI23Flash_bwd_kernel_traitsILi96ELi64ELi128ELi8ELi2ELi4ELi4ELb1ELb0EN7cutlass6half_tE19Flash_kernel_traitsILi96ELi64ELi128ELi8ES2_EELb1ELb1ELb0ELb0EEv16Flash_bwd_params
    0 bytes stack frame, 0 bytes spill stores, 0 bytes spill loads
ptxas info    : Used 255 registers, 696 bytes cmem[0]
ptxas info    : Compiling entry function '_Z44flash_bwd_dq_dk_dv_loop_seqk_parallel_kernelI23Flash_bwd_kernel_traitsILi96ELi64ELi128ELi8ELi2ELi4ELi4ELb1ELb0EN7cutlass6half_tE19Flash_kernel_traitsILi96ELi64ELi128ELi8ES2_EELb1ELb1ELb0ELb1EEv16Flash_bwd_params' for 'sm_80'
ptxas info    : Function properties for _Z44flash_bwd_dq_dk_dv_loop_seqk_parallel_kernelI23Flash_bwd_kernel_traitsILi96ELi64ELi128ELi8ELi2ELi4ELi4ELb1ELb0EN7cutlass6half_tE19Flash_kernel_traitsILi96ELi64ELi128ELi8ES2_EELb1ELb1ELb0ELb1EEv16Flash_bwd_params
    0 bytes stack frame, 0 bytes spill stores, 0 bytes spill loads
ptxas info    : Used 255 registers, 696 bytes cmem[0]
ptxas info    : Compiling entry function '_Z44flash_bwd_dq_dk_dv_loop_seqk_parallel_kernelI23Flash_bwd_kernel_traitsILi96ELi64ELi128ELi8ELi2ELi4ELi4ELb1ELb0EN7cutlass6half_tE19Flash_kernel_traitsILi96ELi64ELi128ELi8ES2_EELb1ELb1ELb1ELb0EEv16Flash_bwd_params' for 'sm_80'
ptxas info    : Function properties for _Z44flash_bwd_dq_dk_dv_loop_seqk_parallel_kernelI23Flash_bwd_kernel_traitsILi96ELi64ELi128ELi8ELi2ELi4ELi4ELb1ELb0EN7cutlass6half_tE19Flash_kernel_traitsILi96ELi64ELi128ELi8ES2_EELb1ELb1ELb1ELb0EEv16Flash_bwd_params
    0 bytes stack frame, 0 bytes spill stores, 0 bytes spill loads
ptxas info    : Used 255 registers, 696 bytes cmem[0]
ptxas info    : Compiling entry function '_Z44flash_bwd_dq_dk_dv_loop_seqk_parallel_kernelI23Flash_bwd_kernel_traitsILi96ELi64ELi128ELi8ELi2ELi4ELi4ELb1ELb0EN7cutlass6half_tE19Flash_kernel_traitsILi96ELi64ELi128ELi8ES2_EELb1ELb1ELb1ELb1EEv16Flash_bwd_params' for 'sm_80'
ptxas info    : Function properties for _Z44flash_bwd_dq_dk_dv_loop_seqk_parallel_kernelI23Flash_bwd_kernel_traitsILi96ELi64ELi128ELi8ELi2ELi4ELi4ELb1ELb0EN7cutlass6half_tE19Flash_kernel_traitsILi96ELi64ELi128ELi8ES2_EELb1ELb1ELb1ELb1EEv16Flash_bwd_params
    0 bytes stack frame, 0 bytes spill stores, 0 bytes spill loads
ptxas info    : Used 255 registers, 696 bytes cmem[0]
ptxas info    : Compiling entry function '_Z25flash_bwd_dot_do_o_kernelILb1E23Flash_bwd_kernel_traitsILi96ELi64ELi128ELi8ELi2ELi4ELi4ELb1ELb0EN7cutlass6half_tE19Flash_kernel_traitsILi96ELi64ELi128ELi8ES2_EEEv16Flash_bwd_params' for 'sm_80'
ptxas info    : Function properties for _Z25flash_bwd_dot_do_o_kernelILb1E23Flash_bwd_kernel_traitsILi96ELi64ELi128ELi8ELi2ELi4ELi4ELb1ELb0EN7cutlass6half_tE19Flash_kernel_traitsILi96ELi64ELi128ELi8ES2_EEEv16Flash_bwd_params
    0 bytes stack frame, 0 bytes spill stores, 0 bytes spill loads
ptxas info    : Used 42 registers, 696 bytes cmem[0]
ptxas info    : Compiling entry function '_Z27flash_bwd_convert_dq_kernelI23Flash_bwd_kernel_traitsILi96ELi64ELi128ELi8ELi2ELi4ELi4ELb0ELb0EN7cutlass6half_tE19Flash_kernel_traitsILi96ELi64ELi128ELi8ES2_EEEv16Flash_bwd_params' for 'sm_80'
ptxas info    : Function properties for _Z27flash_bwd_convert_dq_kernelI23Flash_bwd_kernel_traitsILi96ELi64ELi128ELi8ELi2ELi4ELi4ELb0ELb0EN7cutlass6half_tE19Flash_kernel_traitsILi96ELi64ELi128ELi8ES2_EEEv16Flash_bwd_params
    0 bytes stack frame, 0 bytes spill stores, 0 bytes spill loads
ptxas info    : Used 32 registers, 696 bytes cmem[0]
ptxas info    : Compiling entry function '_Z44flash_bwd_dq_dk_dv_loop_seqk_parallel_kernelI23Flash_bwd_kernel_traitsILi96ELi64ELi128ELi8ELi2ELi4ELi4ELb0ELb0EN7cutlass6half_tE19Flash_kernel_traitsILi96ELi64ELi128ELi8ES2_EELb1ELb0ELb0ELb0EEv16Flash_bwd_params' for 'sm_80'
ptxas info    : Function properties for _Z44flash_bwd_dq_dk_dv_loop_seqk_parallel_kernelI23Flash_bwd_kernel_traitsILi96ELi64ELi128ELi8ELi2ELi4ELi4ELb0ELb0EN7cutlass6half_tE19Flash_kernel_traitsILi96ELi64ELi128ELi8ES2_EELb1ELb0ELb0ELb0EEv16Flash_bwd_params
    0 bytes stack frame, 0 bytes spill stores, 0 bytes spill loads
ptxas info    : Used 238 registers, 696 bytes cmem[0]
ptxas info    : Compiling entry function '_Z44flash_bwd_dq_dk_dv_loop_seqk_parallel_kernelI23Flash_bwd_kernel_traitsILi96ELi64ELi128ELi8ELi2ELi4ELi4ELb0ELb0EN7cutlass6half_tE19Flash_kernel_traitsILi96ELi64ELi128ELi8ES2_EELb1ELb0ELb0ELb1EEv16Flash_bwd_params' for 'sm_80'
ptxas info    : Function properties for _Z44flash_bwd_dq_dk_dv_loop_seqk_parallel_kernelI23Flash_bwd_kernel_traitsILi96ELi64ELi128ELi8ELi2ELi4ELi4ELb0ELb0EN7cutlass6half_tE19Flash_kernel_traitsILi96ELi64ELi128ELi8ES2_EELb1ELb0ELb0ELb1EEv16Flash_bwd_params
    0 bytes stack frame, 0 bytes spill stores, 0 bytes spill loads
ptxas info    : Used 238 registers, 696 bytes cmem[0]
ptxas info    : Compiling entry function '_Z44flash_bwd_dq_dk_dv_loop_seqk_parallel_kernelI23Flash_bwd_kernel_traitsILi96ELi64ELi128ELi8ELi2ELi4ELi4ELb0ELb0EN7cutlass6half_tE19Flash_kernel_traitsILi96ELi64ELi128ELi8ES2_EELb1ELb0ELb1ELb0EEv16Flash_bwd_params' for 'sm_80'
ptxas info    : Function properties for _Z44flash_bwd_dq_dk_dv_loop_seqk_parallel_kernelI23Flash_bwd_kernel_traitsILi96ELi64ELi128ELi8ELi2ELi4ELi4ELb0ELb0EN7cutlass6half_tE19Flash_kernel_traitsILi96ELi64ELi128ELi8ES2_EELb1ELb0ELb1ELb0EEv16Flash_bwd_params
    0 bytes stack frame, 0 bytes spill stores, 0 bytes spill loads
ptxas info    : Used 238 registers, 696 bytes cmem[0]
ptxas info    : Compiling entry function '_Z44flash_bwd_dq_dk_dv_loop_seqk_parallel_kernelI23Flash_bwd_kernel_traitsILi96ELi64ELi128ELi8ELi2ELi4ELi4ELb0ELb0EN7cutlass6half_tE19Flash_kernel_traitsILi96ELi64ELi128ELi8ES2_EELb1ELb0ELb1ELb1EEv16Flash_bwd_params' for 'sm_80'
ptxas info    : Function properties for _Z44flash_bwd_dq_dk_dv_loop_seqk_parallel_kernelI23Flash_bwd_kernel_traitsILi96ELi64ELi128ELi8ELi2ELi4ELi4ELb0ELb0EN7cutlass6half_tE19Flash_kernel_traitsILi96ELi64ELi128ELi8ES2_EELb1ELb0ELb1ELb1EEv16Flash_bwd_params
    0 bytes stack frame, 0 bytes spill stores, 0 bytes spill loads
ptxas info    : Used 243 registers, 696 bytes cmem[0]
ptxas info    : Compiling entry function '_Z44flash_bwd_dq_dk_dv_loop_seqk_parallel_kernelI23Flash_bwd_kernel_traitsILi96ELi64ELi128ELi8ELi2ELi4ELi4ELb0ELb0EN7cutlass6half_tE19Flash_kernel_traitsILi96ELi64ELi128ELi8ES2_EELb1ELb1ELb0ELb0EEv16Flash_bwd_params' for 'sm_80'
ptxas info    : Function properties for _Z44flash_bwd_dq_dk_dv_loop_seqk_parallel_kernelI23Flash_bwd_kernel_traitsILi96ELi64ELi128ELi8ELi2ELi4ELi4ELb0ELb0EN7cutlass6half_tE19Flash_kernel_traitsILi96ELi64ELi128ELi8ES2_EELb1ELb1ELb0ELb0EEv16Flash_bwd_params
    0 bytes stack frame, 0 bytes spill stores, 0 bytes spill loads
ptxas info    : Used 255 registers, 696 bytes cmem[0]
ptxas info    : Compiling entry function '_Z44flash_bwd_dq_dk_dv_loop_seqk_parallel_kernelI23Flash_bwd_kernel_traitsILi96ELi64ELi128ELi8ELi2ELi4ELi4ELb0ELb0EN7cutlass6half_tE19Flash_kernel_traitsILi96ELi64ELi128ELi8ES2_EELb1ELb1ELb0ELb1EEv16Flash_bwd_params' for 'sm_80'
ptxas info    : Function properties for _Z44flash_bwd_dq_dk_dv_loop_seqk_parallel_kernelI23Flash_bwd_kernel_traitsILi96ELi64ELi128ELi8ELi2ELi4ELi4ELb0ELb0EN7cutlass6half_tE19Flash_kernel_traitsILi96ELi64ELi128ELi8ES2_EELb1ELb1ELb0ELb1EEv16Flash_bwd_params
    0 bytes stack frame, 0 bytes spill stores, 0 bytes spill loads
ptxas info    : Used 253 registers, 696 bytes cmem[0]
ptxas info    : Compiling entry function '_Z44flash_bwd_dq_dk_dv_loop_seqk_parallel_kernelI23Flash_bwd_kernel_traitsILi96ELi64ELi128ELi8ELi2ELi4ELi4ELb0ELb0EN7cutlass6half_tE19Flash_kernel_traitsILi96ELi64ELi128ELi8ES2_EELb1ELb1ELb1ELb0EEv16Flash_bwd_params' for 'sm_80'
ptxas info    : Function properties for _Z44flash_bwd_dq_dk_dv_loop_seqk_parallel_kernelI23Flash_bwd_kernel_traitsILi96ELi64ELi128ELi8ELi2ELi4ELi4ELb0ELb0EN7cutlass6half_tE19Flash_kernel_traitsILi96ELi64ELi128ELi8ES2_EELb1ELb1ELb1ELb0EEv16Flash_bwd_params
    0 bytes stack frame, 0 bytes spill stores, 0 bytes spill loads
ptxas info    : Used 250 registers, 696 bytes cmem[0]
ptxas info    : Compiling entry function '_Z44flash_bwd_dq_dk_dv_loop_seqk_parallel_kernelI23Flash_bwd_kernel_traitsILi96ELi64ELi128ELi8ELi2ELi4ELi4ELb0ELb0EN7cutlass6half_tE19Flash_kernel_traitsILi96ELi64ELi128ELi8ES2_EELb1ELb1ELb1ELb1EEv16Flash_bwd_params' for 'sm_80'
ptxas info    : Function properties for _Z44flash_bwd_dq_dk_dv_loop_seqk_parallel_kernelI23Flash_bwd_kernel_traitsILi96ELi64ELi128ELi8ELi2ELi4ELi4ELb0ELb0EN7cutlass6half_tE19Flash_kernel_traitsILi96ELi64ELi128ELi8ES2_EELb1ELb1ELb1ELb1EEv16Flash_bwd_params
    0 bytes stack frame, 0 bytes spill stores, 0 bytes spill loads
ptxas info    : Used 245 registers, 696 bytes cmem[0]
ptxas info    : Compiling entry function '_Z25flash_bwd_dot_do_o_kernelILb1E23Flash_bwd_kernel_traitsILi96ELi64ELi128ELi8ELi2ELi4ELi4ELb0ELb0EN7cutlass6half_tE19Flash_kernel_traitsILi96ELi64ELi128ELi8ES2_EEEv16Flash_bwd_params' for 'sm_80'
ptxas info    : Function properties for _Z25flash_bwd_dot_do_o_kernelILb1E23Flash_bwd_kernel_traitsILi96ELi64ELi128ELi8ELi2ELi4ELi4ELb0ELb0EN7cutlass6half_tE19Flash_kernel_traitsILi96ELi64ELi128ELi8ES2_EEEv16Flash_bwd_params
    0 bytes stack frame, 0 bytes spill stores, 0 bytes spill loads
ptxas info    : Used 42 registers, 696 bytes cmem[0]
/anaconda/envs/flashattention/lib/python3.9/site-packages/torch/include/c10/core/SymInt.h(84): warning: integer conversion resulted in a change of sign

[14/37] /usr/local/cuda-11.4/bin/nvcc  -I/home/v-feiychen/flash-attention/csrc/flash_attn -I/home/v-feiychen/flash-attention/csrc/flash_attn/src -I/home/v-feiychen/flash-attention/csrc/cutlass/include -I/anaconda/envs/flashattention/lib/python3.9/site-packages/torch/include -I/anaconda/envs/flashattention/lib/python3.9/site-packages/torch/include/torch/csrc/api/include -I/anaconda/envs/flashattention/lib/python3.9/site-packages/torch/include/TH -I/anaconda/envs/flashattention/lib/python3.9/site-packages/torch/include/THC -I/usr/local/cuda-11.4/include -I/anaconda/envs/flashattention/include/python3.9 -c -c /home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_bwd_hdim192_fp16_sm80.cu -o /home/v-feiychen/flash-attention/build/temp.linux-x86_64-cpython-39/csrc/flash_attn/src/flash_bwd_hdim192_fp16_sm80.o -D__CUDA_NO_HALF_OPERATORS__ -D__CUDA_NO_HALF_CONVERSIONS__ -D__CUDA_NO_BFLOAT16_CONVERSIONS__ -D__CUDA_NO_HALF2_OPERATORS__ --expt-relaxed-constexpr --compiler-options ''"'"'-fPIC'"'"'' -O3 -std=c++17 -U__CUDA_NO_HALF_OPERATORS__ -U__CUDA_NO_HALF_CONVERSIONS__ -U__CUDA_NO_HALF2_OPERATORS__ -U__CUDA_NO_BFLOAT16_CONVERSIONS__ --expt-relaxed-constexpr --expt-extended-lambda --use_fast_math --ptxas-options=-v -lineinfo -gencode arch=compute_80,code=sm_80 --threads 4 -DTORCH_API_INCLUDE_EXTENSION_H '-DPYBIND11_COMPILER_TYPE="_gcc"' '-DPYBIND11_STDLIB="_libstdcpp"' '-DPYBIND11_BUILD_ABI="_cxxabi1011"' -DTORCH_EXTENSION_NAME=flash_attn_2_cuda -D_GLIBCXX_USE_CXX11_ABI=0
/anaconda/envs/flashattention/lib/python3.9/site-packages/torch/include/c10/core/SymInt.h(84): warning: integer conversion resulted in a change of sign

ptxas info    : 2 bytes gmem
ptxas info    : Compiling entry function '_Z44flash_bwd_dq_dk_dv_loop_seqk_parallel_kernelI23Flash_bwd_kernel_traitsILi192ELi64ELi64ELi8ELi4ELi2ELi2ELb1ELb1EN7cutlass6half_tE19Flash_kernel_traitsILi192ELi64ELi64ELi8ES2_EELb0ELb0ELb0ELb0EEv16Flash_bwd_params' for 'sm_80'
ptxas info    : Function properties for _Z44flash_bwd_dq_dk_dv_loop_seqk_parallel_kernelI23Flash_bwd_kernel_traitsILi192ELi64ELi64ELi8ELi4ELi2ELi2ELb1ELb1EN7cutlass6half_tE19Flash_kernel_traitsILi192ELi64ELi64ELi8ES2_EELb0ELb0ELb0ELb0EEv16Flash_bwd_params
    176 bytes stack frame, 192 bytes spill stores, 192 bytes spill loads
ptxas info    : Used 255 registers, 696 bytes cmem[0]
ptxas info    : Compiling entry function '_Z44flash_bwd_dq_dk_dv_loop_seqk_parallel_kernelI23Flash_bwd_kernel_traitsILi192ELi64ELi64ELi8ELi4ELi2ELi2ELb1ELb1EN7cutlass6half_tE19Flash_kernel_traitsILi192ELi64ELi64ELi8ES2_EELb0ELb0ELb0ELb1EEv16Flash_bwd_params' for 'sm_80'
ptxas info    : Function properties for _Z44flash_bwd_dq_dk_dv_loop_seqk_parallel_kernelI23Flash_bwd_kernel_traitsILi192ELi64ELi64ELi8ELi4ELi2ELi2ELb1ELb1EN7cutlass6half_tE19Flash_kernel_traitsILi192ELi64ELi64ELi8ES2_EELb0ELb0ELb0ELb1EEv16Flash_bwd_params
    176 bytes stack frame, 188 bytes spill stores, 192 bytes spill loads
ptxas info    : Used 255 registers, 696 bytes cmem[0]
ptxas info    : Compiling entry function '_Z44flash_bwd_dq_dk_dv_loop_seqk_parallel_kernelI23Flash_bwd_kernel_traitsILi192ELi64ELi64ELi8ELi4ELi2ELi2ELb1ELb1EN7cutlass6half_tE19Flash_kernel_traitsILi192ELi64ELi64ELi8ES2_EELb0ELb0ELb1ELb0EEv16Flash_bwd_params' for 'sm_80'
ptxas info    : Function properties for _Z44flash_bwd_dq_dk_dv_loop_seqk_parallel_kernelI23Flash_bwd_kernel_traitsILi192ELi64ELi64ELi8ELi4ELi2ELi2ELb1ELb1EN7cutlass6half_tE19Flash_kernel_traitsILi192ELi64ELi64ELi8ES2_EELb0ELb0ELb1ELb0EEv16Flash_bwd_params
    168 bytes stack frame, 176 bytes spill stores, 180 bytes spill loads
ptxas info    : Used 255 registers, 696 bytes cmem[0]
ptxas info    : Compiling entry function '_Z44flash_bwd_dq_dk_dv_loop_seqk_parallel_kernelI23Flash_bwd_kernel_traitsILi192ELi64ELi64ELi8ELi4ELi2ELi2ELb1ELb1EN7cutlass6half_tE19Flash_kernel_traitsILi192ELi64ELi64ELi8ES2_EELb0ELb0ELb1ELb1EEv16Flash_bwd_params' for 'sm_80'
ptxas info    : Function properties for _Z44flash_bwd_dq_dk_dv_loop_seqk_parallel_kernelI23Flash_bwd_kernel_traitsILi192ELi64ELi64ELi8ELi4ELi2ELi2ELb1ELb1EN7cutlass6half_tE19Flash_kernel_traitsILi192ELi64ELi64ELi8ES2_EELb0ELb0ELb1ELb1EEv16Flash_bwd_params
    168 bytes stack frame, 176 bytes spill stores, 180 bytes spill loads
ptxas info    : Used 255 registers, 696 bytes cmem[0]
ptxas info    : Compiling entry function '_Z44flash_bwd_dq_dk_dv_loop_seqk_parallel_kernelI23Flash_bwd_kernel_traitsILi192ELi64ELi64ELi8ELi4ELi2ELi2ELb1ELb1EN7cutlass6half_tE19Flash_kernel_traitsILi192ELi64ELi64ELi8ES2_EELb0ELb1ELb0ELb0EEv16Flash_bwd_params' for 'sm_80'
ptxas info    : Function properties for _Z44flash_bwd_dq_dk_dv_loop_seqk_parallel_kernelI23Flash_bwd_kernel_traitsILi192ELi64ELi64ELi8ELi4ELi2ELi2ELb1ELb1EN7cutlass6half_tE19Flash_kernel_traitsILi192ELi64ELi64ELi8ES2_EELb0ELb1ELb0ELb0EEv16Flash_bwd_params
    192 bytes stack frame, 196 bytes spill stores, 208 bytes spill loads
ptxas info    : Used 255 registers, 696 bytes cmem[0]
ptxas info    : Compiling entry function '_Z44flash_bwd_dq_dk_dv_loop_seqk_parallel_kernelI23Flash_bwd_kernel_traitsILi192ELi64ELi64ELi8ELi4ELi2ELi2ELb1ELb1EN7cutlass6half_tE19Flash_kernel_traitsILi192ELi64ELi64ELi8ES2_EELb0ELb1ELb0ELb1EEv16Flash_bwd_params' for 'sm_80'
ptxas info    : Function properties for _Z44flash_bwd_dq_dk_dv_loop_seqk_parallel_kernelI23Flash_bwd_kernel_traitsILi192ELi64ELi64ELi8ELi4ELi2ELi2ELb1ELb1EN7cutlass6half_tE19Flash_kernel_traitsILi192ELi64ELi64ELi8ES2_EELb0ELb1ELb0ELb1EEv16Flash_bwd_params
    176 bytes stack frame, 192 bytes spill stores, 196 bytes spill loads
ptxas info    : Used 255 registers, 696 bytes cmem[0]
ptxas info    : Compiling entry function '_Z44flash_bwd_dq_dk_dv_loop_seqk_parallel_kernelI23Flash_bwd_kernel_traitsILi192ELi64ELi64ELi8ELi4ELi2ELi2ELb1ELb1EN7cutlass6half_tE19Flash_kernel_traitsILi192ELi64ELi64ELi8ES2_EELb0ELb1ELb1ELb0EEv16Flash_bwd_params' for 'sm_80'
ptxas info    : Function properties for _Z44flash_bwd_dq_dk_dv_loop_seqk_parallel_kernelI23Flash_bwd_kernel_traitsILi192ELi64ELi64ELi8ELi4ELi2ELi2ELb1ELb1EN7cutlass6half_tE19Flash_kernel_traitsILi192ELi64ELi64ELi8ES2_EELb0ELb1ELb1ELb0EEv16Flash_bwd_params
    176 bytes stack frame, 180 bytes spill stores, 192 bytes spill loads
ptxas info    : Used 255 registers, 696 bytes cmem[0]
ptxas info    : Compiling entry function '_Z44flash_bwd_dq_dk_dv_loop_seqk_parallel_kernelI23Flash_bwd_kernel_traitsILi192ELi64ELi64ELi8ELi4ELi2ELi2ELb1ELb1EN7cutlass6half_tE19Flash_kernel_traitsILi192ELi64ELi64ELi8ES2_EELb0ELb1ELb1ELb1EEv16Flash_bwd_params' for 'sm_80'
ptxas info    : Function properties for _Z44flash_bwd_dq_dk_dv_loop_seqk_parallel_kernelI23Flash_bwd_kernel_traitsILi192ELi64ELi64ELi8ELi4ELi2ELi2ELb1ELb1EN7cutlass6half_tE19Flash_kernel_traitsILi192ELi64ELi64ELi8ES2_EELb0ELb1ELb1ELb1EEv16Flash_bwd_params
    176 bytes stack frame, 180 bytes spill stores, 192 bytes spill loads
ptxas info    : Used 255 registers, 696 bytes cmem[0]
ptxas info    : Compiling entry function '_Z44flash_bwd_dq_dk_dv_loop_seqk_parallel_kernelI23Flash_bwd_kernel_traitsILi192ELi64ELi64ELi8ELi4ELi2ELi2ELb0ELb0EN7cutlass6half_tE19Flash_kernel_traitsILi192ELi64ELi64ELi8ES2_EELb0ELb0ELb0ELb0EEv16Flash_bwd_params' for 'sm_80'
ptxas info    : Function properties for _Z44flash_bwd_dq_dk_dv_loop_seqk_parallel_kernelI23Flash_bwd_kernel_traitsILi192ELi64ELi64ELi8ELi4ELi2ELi2ELb0ELb0EN7cutlass6half_tE19Flash_kernel_traitsILi192ELi64ELi64ELi8ES2_EELb0ELb0ELb0ELb0EEv16Flash_bwd_params
    0 bytes stack frame, 0 bytes spill stores, 0 bytes spill loads
ptxas info    : Used 236 registers, 696 bytes cmem[0]
ptxas info    : Compiling entry function '_Z44flash_bwd_dq_dk_dv_loop_seqk_parallel_kernelI23Flash_bwd_kernel_traitsILi192ELi64ELi64ELi8ELi4ELi2ELi2ELb0ELb0EN7cutlass6half_tE19Flash_kernel_traitsILi192ELi64ELi64ELi8ES2_EELb0ELb0ELb0ELb1EEv16Flash_bwd_params' for 'sm_80'
ptxas info    : Function properties for _Z44flash_bwd_dq_dk_dv_loop_seqk_parallel_kernelI23Flash_bwd_kernel_traitsILi192ELi64ELi64ELi8ELi4ELi2ELi2ELb0ELb0EN7cutlass6half_tE19Flash_kernel_traitsILi192ELi64ELi64ELi8ES2_EELb0ELb0ELb0ELb1EEv16Flash_bwd_params
    0 bytes stack frame, 0 bytes spill stores, 0 bytes spill loads
ptxas info    : Used 236 registers, 696 bytes cmem[0]
ptxas info    : Compiling entry function '_Z44flash_bwd_dq_dk_dv_loop_seqk_parallel_kernelI23Flash_bwd_kernel_traitsILi192ELi64ELi64ELi8ELi4ELi2ELi2ELb0ELb0EN7cutlass6half_tE19Flash_kernel_traitsILi192ELi64ELi64ELi8ES2_EELb0ELb0ELb1ELb0EEv16Flash_bwd_params' for 'sm_80'
ptxas info    : Function properties for _Z44flash_bwd_dq_dk_dv_loop_seqk_parallel_kernelI23Flash_bwd_kernel_traitsILi192ELi64ELi64ELi8ELi4ELi2ELi2ELb0ELb0EN7cutlass6half_tE19Flash_kernel_traitsILi192ELi64ELi64ELi8ES2_EELb0ELb0ELb1ELb0EEv16Flash_bwd_params
    0 bytes stack frame, 0 bytes spill stores, 0 bytes spill loads
ptxas info    : Used 231 registers, 696 bytes cmem[0]
ptxas info    : Compiling entry function '_Z44flash_bwd_dq_dk_dv_loop_seqk_parallel_kernelI23Flash_bwd_kernel_traitsILi192ELi64ELi64ELi8ELi4ELi2ELi2ELb0ELb0EN7cutlass6half_tE19Flash_kernel_traitsILi192ELi64ELi64ELi8ES2_EELb0ELb0ELb1ELb1EEv16Flash_bwd_params' for 'sm_80'
ptxas info    : Function properties for _Z44flash_bwd_dq_dk_dv_loop_seqk_parallel_kernelI23Flash_bwd_kernel_traitsILi192ELi64ELi64ELi8ELi4ELi2ELi2ELb0ELb0EN7cutlass6half_tE19Flash_kernel_traitsILi192ELi64ELi64ELi8ES2_EELb0ELb0ELb1ELb1EEv16Flash_bwd_params
    0 bytes stack frame, 0 bytes spill stores, 0 bytes spill loads
ptxas info    : Used 235 registers, 696 bytes cmem[0]
ptxas info    : Compiling entry function '_Z44flash_bwd_dq_dk_dv_loop_seqk_parallel_kernelI23Flash_bwd_kernel_traitsILi192ELi64ELi64ELi8ELi4ELi2ELi2ELb0ELb0EN7cutlass6half_tE19Flash_kernel_traitsILi192ELi64ELi64ELi8ES2_EELb0ELb1ELb0ELb0EEv16Flash_bwd_params' for 'sm_80'
ptxas info    : Function properties for _Z44flash_bwd_dq_dk_dv_loop_seqk_parallel_kernelI23Flash_bwd_kernel_traitsILi192ELi64ELi64ELi8ELi4ELi2ELi2ELb0ELb0EN7cutlass6half_tE19Flash_kernel_traitsILi192ELi64ELi64ELi8ES2_EELb0ELb1ELb0ELb0EEv16Flash_bwd_params
    0 bytes stack frame, 0 bytes spill stores, 0 bytes spill loads
ptxas info    : Used 236 registers, 696 bytes cmem[0]
ptxas info    : Compiling entry function '_Z44flash_bwd_dq_dk_dv_loop_seqk_parallel_kernelI23Flash_bwd_kernel_traitsILi192ELi64ELi64ELi8ELi4ELi2ELi2ELb0ELb0EN7cutlass6half_tE19Flash_kernel_traitsILi192ELi64ELi64ELi8ES2_EELb0ELb1ELb0ELb1EEv16Flash_bwd_params' for 'sm_80'
ptxas info    : Function properties for _Z44flash_bwd_dq_dk_dv_loop_seqk_parallel_kernelI23Flash_bwd_kernel_traitsILi192ELi64ELi64ELi8ELi4ELi2ELi2ELb0ELb0EN7cutlass6half_tE19Flash_kernel_traitsILi192ELi64ELi64ELi8ES2_EELb0ELb1ELb0ELb1EEv16Flash_bwd_params
    0 bytes stack frame, 0 bytes spill stores, 0 bytes spill loads
ptxas info    : Used 236 registers, 696 bytes cmem[0]
ptxas info    : Compiling entry function '_Z44flash_bwd_dq_dk_dv_loop_seqk_parallel_kernelI23Flash_bwd_kernel_traitsILi192ELi64ELi64ELi8ELi4ELi2ELi2ELb0ELb0EN7cutlass6half_tE19Flash_kernel_traitsILi192ELi64ELi64ELi8ES2_EELb0ELb1ELb1ELb0EEv16Flash_bwd_params' for 'sm_80'
ptxas info    : Function properties for _Z44flash_bwd_dq_dk_dv_loop_seqk_parallel_kernelI23Flash_bwd_kernel_traitsILi192ELi64ELi64ELi8ELi4ELi2ELi2ELb0ELb0EN7cutlass6half_tE19Flash_kernel_traitsILi192ELi64ELi64ELi8ES2_EELb0ELb1ELb1ELb0EEv16Flash_bwd_params
    0 bytes stack frame, 0 bytes spill stores, 0 bytes spill loads
ptxas info    : Used 234 registers, 696 bytes cmem[0]
ptxas info    : Compiling entry function '_Z44flash_bwd_dq_dk_dv_loop_seqk_parallel_kernelI23Flash_bwd_kernel_traitsILi192ELi64ELi64ELi8ELi4ELi2ELi2ELb0ELb0EN7cutlass6half_tE19Flash_kernel_traitsILi192ELi64ELi64ELi8ES2_EELb0ELb1ELb1ELb1EEv16Flash_bwd_params' for 'sm_80'
ptxas info    : Function properties for _Z44flash_bwd_dq_dk_dv_loop_seqk_parallel_kernelI23Flash_bwd_kernel_traitsILi192ELi64ELi64ELi8ELi4ELi2ELi2ELb0ELb0EN7cutlass6half_tE19Flash_kernel_traitsILi192ELi64ELi64ELi8ES2_EELb0ELb1ELb1ELb1EEv16Flash_bwd_params
    0 bytes stack frame, 0 bytes spill stores, 0 bytes spill loads
ptxas info    : Used 238 registers, 696 bytes cmem[0]
ptxas info    : Compiling entry function '_Z27flash_bwd_convert_dq_kernelI23Flash_bwd_kernel_traitsILi192ELi64ELi64ELi8ELi4ELi2ELi2ELb1ELb1EN7cutlass6half_tE19Flash_kernel_traitsILi192ELi64ELi64ELi8ES2_EEEv16Flash_bwd_params' for 'sm_80'
ptxas info    : Function properties for _Z27flash_bwd_convert_dq_kernelI23Flash_bwd_kernel_traitsILi192ELi64ELi64ELi8ELi4ELi2ELi2ELb1ELb1EN7cutlass6half_tE19Flash_kernel_traitsILi192ELi64ELi64ELi8ES2_EEEv16Flash_bwd_params
    0 bytes stack frame, 0 bytes spill stores, 0 bytes spill loads
ptxas info    : Used 32 registers, 696 bytes cmem[0]
ptxas info    : Compiling entry function '_Z44flash_bwd_dq_dk_dv_loop_seqk_parallel_kernelI23Flash_bwd_kernel_traitsILi192ELi64ELi64ELi8ELi4ELi2ELi2ELb1ELb1EN7cutlass6half_tE19Flash_kernel_traitsILi192ELi64ELi64ELi8ES2_EELb1ELb0ELb0ELb0EEv16Flash_bwd_params' for 'sm_80'
ptxas info    : Function properties for _Z44flash_bwd_dq_dk_dv_loop_seqk_parallel_kernelI23Flash_bwd_kernel_traitsILi192ELi64ELi64ELi8ELi4ELi2ELi2ELb1ELb1EN7cutlass6half_tE19Flash_kernel_traitsILi192ELi64ELi64ELi8ES2_EELb1ELb0ELb0ELb0EEv16Flash_bwd_params
    192 bytes stack frame, 200 bytes spill stores, 208 bytes spill loads
ptxas info    : Used 255 registers, 696 bytes cmem[0]
ptxas info    : Compiling entry function '_Z44flash_bwd_dq_dk_dv_loop_seqk_parallel_kernelI23Flash_bwd_kernel_traitsILi192ELi64ELi64ELi8ELi4ELi2ELi2ELb1ELb1EN7cutlass6half_tE19Flash_kernel_traitsILi192ELi64ELi64ELi8ES2_EELb1ELb0ELb0ELb1EEv16Flash_bwd_params' for 'sm_80'
ptxas info    : Function properties for _Z44flash_bwd_dq_dk_dv_loop_seqk_parallel_kernelI23Flash_bwd_kernel_traitsILi192ELi64ELi64ELi8ELi4ELi2ELi2ELb1ELb1EN7cutlass6half_tE19Flash_kernel_traitsILi192ELi64ELi64ELi8ES2_EELb1ELb0ELb0ELb1EEv16Flash_bwd_params
    192 bytes stack frame, 196 bytes spill stores, 204 bytes spill loads
ptxas info    : Used 255 registers, 696 bytes cmem[0]
ptxas info    : Compiling entry function '_Z44flash_bwd_dq_dk_dv_loop_seqk_parallel_kernelI23Flash_bwd_kernel_traitsILi192ELi64ELi64ELi8ELi4ELi2ELi2ELb1ELb1EN7cutlass6half_tE19Flash_kernel_traitsILi192ELi64ELi64ELi8ES2_EELb1ELb0ELb1ELb0EEv16Flash_bwd_params' for 'sm_80'
ptxas info    : Function properties for _Z44flash_bwd_dq_dk_dv_loop_seqk_parallel_kernelI23Flash_bwd_kernel_traitsILi192ELi64ELi64ELi8ELi4ELi2ELi2ELb1ELb1EN7cutlass6half_tE19Flash_kernel_traitsILi192ELi64ELi64ELi8ES2_EELb1ELb0ELb1ELb0EEv16Flash_bwd_params
    192 bytes stack frame, 196 bytes spill stores, 204 bytes spill loads
ptxas info    : Used 255 registers, 696 bytes cmem[0]
ptxas info    : Compiling entry function '_Z44flash_bwd_dq_dk_dv_loop_seqk_parallel_kernelI23Flash_bwd_kernel_traitsILi192ELi64ELi64ELi8ELi4ELi2ELi2ELb1ELb1EN7cutlass6half_tE19Flash_kernel_traitsILi192ELi64ELi64ELi8ES2_EELb1ELb0ELb1ELb1EEv16Flash_bwd_params' for 'sm_80'
ptxas info    : Function properties for _Z44flash_bwd_dq_dk_dv_loop_seqk_parallel_kernelI23Flash_bwd_kernel_traitsILi192ELi64ELi64ELi8ELi4ELi2ELi2ELb1ELb1EN7cutlass6half_tE19Flash_kernel_traitsILi192ELi64ELi64ELi8ES2_EELb1ELb0ELb1ELb1EEv16Flash_bwd_params
    192 bytes stack frame, 196 bytes spill stores, 196 bytes spill loads
ptxas info    : Used 255 registers, 696 bytes cmem[0]
ptxas info    : Compiling entry function '_Z44flash_bwd_dq_dk_dv_loop_seqk_parallel_kernelI23Flash_bwd_kernel_traitsILi192ELi64ELi64ELi8ELi4ELi2ELi2ELb1ELb1EN7cutlass6half_tE19Flash_kernel_traitsILi192ELi64ELi64ELi8ES2_EELb1ELb1ELb0ELb0EEv16Flash_bwd_params' for 'sm_80'
ptxas info    : Function properties for _Z44flash_bwd_dq_dk_dv_loop_seqk_parallel_kernelI23Flash_bwd_kernel_traitsILi192ELi64ELi64ELi8ELi4ELi2ELi2ELb1ELb1EN7cutlass6half_tE19Flash_kernel_traitsILi192ELi64ELi64ELi8ES2_EELb1ELb1ELb0ELb0EEv16Flash_bwd_params
    208 bytes stack frame, 212 bytes spill stores, 220 bytes spill loads
ptxas info    : Used 255 registers, 696 bytes cmem[0]
ptxas info    : Compiling entry function '_Z44flash_bwd_dq_dk_dv_loop_seqk_parallel_kernelI23Flash_bwd_kernel_traitsILi192ELi64ELi64ELi8ELi4ELi2ELi2ELb1ELb1EN7cutlass6half_tE19Flash_kernel_traitsILi192ELi64ELi64ELi8ES2_EELb1ELb1ELb0ELb1EEv16Flash_bwd_params' for 'sm_80'
ptxas info    : Function properties for _Z44flash_bwd_dq_dk_dv_loop_seqk_parallel_kernelI23Flash_bwd_kernel_traitsILi192ELi64ELi64ELi8ELi4ELi2ELi2ELb1ELb1EN7cutlass6half_tE19Flash_kernel_traitsILi192ELi64ELi64ELi8ES2_EELb1ELb1ELb0ELb1EEv16Flash_bwd_params
    192 bytes stack frame, 200 bytes spill stores, 208 bytes spill loads
ptxas info    : Used 255 registers, 696 bytes cmem[0]
ptxas info    : Compiling entry function '_Z44flash_bwd_dq_dk_dv_loop_seqk_parallel_kernelI23Flash_bwd_kernel_traitsILi192ELi64ELi64ELi8ELi4ELi2ELi2ELb1ELb1EN7cutlass6half_tE19Flash_kernel_traitsILi192ELi64ELi64ELi8ES2_EELb1ELb1ELb1ELb0EEv16Flash_bwd_params' for 'sm_80'
ptxas info    : Function properties for _Z44flash_bwd_dq_dk_dv_loop_seqk_parallel_kernelI23Flash_bwd_kernel_traitsILi192ELi64ELi64ELi8ELi4ELi2ELi2ELb1ELb1EN7cutlass6half_tE19Flash_kernel_traitsILi192ELi64ELi64ELi8ES2_EELb1ELb1ELb1ELb0EEv16Flash_bwd_params
    208 bytes stack frame, 212 bytes spill stores, 220 bytes spill loads
ptxas info    : Used 255 registers, 696 bytes cmem[0]
ptxas info    : Compiling entry function '_Z44flash_bwd_dq_dk_dv_loop_seqk_parallel_kernelI23Flash_bwd_kernel_traitsILi192ELi64ELi64ELi8ELi4ELi2ELi2ELb1ELb1EN7cutlass6half_tE19Flash_kernel_traitsILi192ELi64ELi64ELi8ES2_EELb1ELb1ELb1ELb1EEv16Flash_bwd_params' for 'sm_80'
ptxas info    : Function properties for _Z44flash_bwd_dq_dk_dv_loop_seqk_parallel_kernelI23Flash_bwd_kernel_traitsILi192ELi64ELi64ELi8ELi4ELi2ELi2ELb1ELb1EN7cutlass6half_tE19Flash_kernel_traitsILi192ELi64ELi64ELi8ES2_EELb1ELb1ELb1ELb1EEv16Flash_bwd_params
    192 bytes stack frame, 196 bytes spill stores, 204 bytes spill loads
ptxas info    : Used 255 registers, 696 bytes cmem[0]
ptxas info    : Compiling entry function '_Z25flash_bwd_dot_do_o_kernelILb1E23Flash_bwd_kernel_traitsILi192ELi64ELi64ELi8ELi4ELi2ELi2ELb1ELb1EN7cutlass6half_tE19Flash_kernel_traitsILi192ELi64ELi64ELi8ES2_EEEv16Flash_bwd_params' for 'sm_80'
ptxas info    : Function properties for _Z25flash_bwd_dot_do_o_kernelILb1E23Flash_bwd_kernel_traitsILi192ELi64ELi64ELi8ELi4ELi2ELi2ELb1ELb1EN7cutlass6half_tE19Flash_kernel_traitsILi192ELi64ELi64ELi8ES2_EEEv16Flash_bwd_params
    0 bytes stack frame, 0 bytes spill stores, 0 bytes spill loads
ptxas info    : Used 69 registers, 696 bytes cmem[0]
ptxas info    : Compiling entry function '_Z27flash_bwd_convert_dq_kernelI23Flash_bwd_kernel_traitsILi192ELi64ELi64ELi8ELi4ELi2ELi2ELb0ELb0EN7cutlass6half_tE19Flash_kernel_traitsILi192ELi64ELi64ELi8ES2_EEEv16Flash_bwd_params' for 'sm_80'
ptxas info    : Function properties for _Z27flash_bwd_convert_dq_kernelI23Flash_bwd_kernel_traitsILi192ELi64ELi64ELi8ELi4ELi2ELi2ELb0ELb0EN7cutlass6half_tE19Flash_kernel_traitsILi192ELi64ELi64ELi8ES2_EEEv16Flash_bwd_params
    0 bytes stack frame, 0 bytes spill stores, 0 bytes spill loads
ptxas info    : Used 32 registers, 696 bytes cmem[0]
ptxas info    : Compiling entry function '_Z44flash_bwd_dq_dk_dv_loop_seqk_parallel_kernelI23Flash_bwd_kernel_traitsILi192ELi64ELi64ELi8ELi4ELi2ELi2ELb0ELb0EN7cutlass6half_tE19Flash_kernel_traitsILi192ELi64ELi64ELi8ES2_EELb1ELb0ELb0ELb0EEv16Flash_bwd_params' for 'sm_80'
ptxas info    : Function properties for _Z44flash_bwd_dq_dk_dv_loop_seqk_parallel_kernelI23Flash_bwd_kernel_traitsILi192ELi64ELi64ELi8ELi4ELi2ELi2ELb0ELb0EN7cutlass6half_tE19Flash_kernel_traitsILi192ELi64ELi64ELi8ES2_EELb1ELb0ELb0ELb0EEv16Flash_bwd_params
    0 bytes stack frame, 0 bytes spill stores, 0 bytes spill loads
ptxas info    : Used 237 registers, 696 bytes cmem[0]
ptxas info    : Compiling entry function '_Z44flash_bwd_dq_dk_dv_loop_seqk_parallel_kernelI23Flash_bwd_kernel_traitsILi192ELi64ELi64ELi8ELi4ELi2ELi2ELb0ELb0EN7cutlass6half_tE19Flash_kernel_traitsILi192ELi64ELi64ELi8ES2_EELb1ELb0ELb0ELb1EEv16Flash_bwd_params' for 'sm_80'
ptxas info    : Function properties for _Z44flash_bwd_dq_dk_dv_loop_seqk_parallel_kernelI23Flash_bwd_kernel_traitsILi192ELi64ELi64ELi8ELi4ELi2ELi2ELb0ELb0EN7cutlass6half_tE19Flash_kernel_traitsILi192ELi64ELi64ELi8ES2_EELb1ELb0ELb0ELb1EEv16Flash_bwd_params
    0 bytes stack frame, 0 bytes spill stores, 0 bytes spill loads
ptxas info    : Used 238 registers, 696 bytes cmem[0]
ptxas info    : Compiling entry function '_Z44flash_bwd_dq_dk_dv_loop_seqk_parallel_kernelI23Flash_bwd_kernel_traitsILi192ELi64ELi64ELi8ELi4ELi2ELi2ELb0ELb0EN7cutlass6half_tE19Flash_kernel_traitsILi192ELi64ELi64ELi8ES2_EELb1ELb0ELb1ELb0EEv16Flash_bwd_params' for 'sm_80'
ptxas info    : Function properties for _Z44flash_bwd_dq_dk_dv_loop_seqk_parallel_kernelI23Flash_bwd_kernel_traitsILi192ELi64ELi64ELi8ELi4ELi2ELi2ELb0ELb0EN7cutlass6half_tE19Flash_kernel_traitsILi192ELi64ELi64ELi8ES2_EELb1ELb0ELb1ELb0EEv16Flash_bwd_params
    0 bytes stack frame, 0 bytes spill stores, 0 bytes spill loads
ptxas info    : Used 232 registers, 696 bytes cmem[0]
ptxas info    : Compiling entry function '_Z44flash_bwd_dq_dk_dv_loop_seqk_parallel_kernelI23Flash_bwd_kernel_traitsILi192ELi64ELi64ELi8ELi4ELi2ELi2ELb0ELb0EN7cutlass6half_tE19Flash_kernel_traitsILi192ELi64ELi64ELi8ES2_EELb1ELb0ELb1ELb1EEv16Flash_bwd_params' for 'sm_80'
ptxas info    : Function properties for _Z44flash_bwd_dq_dk_dv_loop_seqk_parallel_kernelI23Flash_bwd_kernel_traitsILi192ELi64ELi64ELi8ELi4ELi2ELi2ELb0ELb0EN7cutlass6half_tE19Flash_kernel_traitsILi192ELi64ELi64ELi8ES2_EELb1ELb0ELb1ELb1EEv16Flash_bwd_params
    0 bytes stack frame, 0 bytes spill stores, 0 bytes spill loads
ptxas info    : Used 233 registers, 696 bytes cmem[0]
ptxas info    : Compiling entry function '_Z44flash_bwd_dq_dk_dv_loop_seqk_parallel_kernelI23Flash_bwd_kernel_traitsILi192ELi64ELi64ELi8ELi4ELi2ELi2ELb0ELb0EN7cutlass6half_tE19Flash_kernel_traitsILi192ELi64ELi64ELi8ES2_EELb1ELb1ELb0ELb0EEv16Flash_bwd_params' for 'sm_80'
ptxas info    : Function properties for _Z44flash_bwd_dq_dk_dv_loop_seqk_parallel_kernelI23Flash_bwd_kernel_traitsILi192ELi64ELi64ELi8ELi4ELi2ELi2ELb0ELb0EN7cutlass6half_tE19Flash_kernel_traitsILi192ELi64ELi64ELi8ES2_EELb1ELb1ELb0ELb0EEv16Flash_bwd_params
    0 bytes stack frame, 0 bytes spill stores, 0 bytes spill loads
ptxas info    : Used 240 registers, 696 bytes cmem[0]
ptxas info    : Compiling entry function '_Z44flash_bwd_dq_dk_dv_loop_seqk_parallel_kernelI23Flash_bwd_kernel_traitsILi192ELi64ELi64ELi8ELi4ELi2ELi2ELb0ELb0EN7cutlass6half_tE19Flash_kernel_traitsILi192ELi64ELi64ELi8ES2_EELb1ELb1ELb0ELb1EEv16Flash_bwd_params' for 'sm_80'
ptxas info    : Function properties for _Z44flash_bwd_dq_dk_dv_loop_seqk_parallel_kernelI23Flash_bwd_kernel_traitsILi192ELi64ELi64ELi8ELi4ELi2ELi2ELb0ELb0EN7cutlass6half_tE19Flash_kernel_traitsILi192ELi64ELi64ELi8ES2_EELb1ELb1ELb0ELb1EEv16Flash_bwd_params
    0 bytes stack frame, 0 bytes spill stores, 0 bytes spill loads
ptxas info    : Used 239 registers, 696 bytes cmem[0]
ptxas info    : Compiling entry function '_Z44flash_bwd_dq_dk_dv_loop_seqk_parallel_kernelI23Flash_bwd_kernel_traitsILi192ELi64ELi64ELi8ELi4ELi2ELi2ELb0ELb0EN7cutlass6half_tE19Flash_kernel_traitsILi192ELi64ELi64ELi8ES2_EELb1ELb1ELb1ELb0EEv16Flash_bwd_params' for 'sm_80'
ptxas info    : Function properties for _Z44flash_bwd_dq_dk_dv_loop_seqk_parallel_kernelI23Flash_bwd_kernel_traitsILi192ELi64ELi64ELi8ELi4ELi2ELi2ELb0ELb0EN7cutlass6half_tE19Flash_kernel_traitsILi192ELi64ELi64ELi8ES2_EELb1ELb1ELb1ELb0EEv16Flash_bwd_params
    0 bytes stack frame, 0 bytes spill stores, 0 bytes spill loads
ptxas info    : Used 236 registers, 696 bytes cmem[0]
ptxas info    : Compiling entry function '_Z44flash_bwd_dq_dk_dv_loop_seqk_parallel_kernelI23Flash_bwd_kernel_traitsILi192ELi64ELi64ELi8ELi4ELi2ELi2ELb0ELb0EN7cutlass6half_tE19Flash_kernel_traitsILi192ELi64ELi64ELi8ES2_EELb1ELb1ELb1ELb1EEv16Flash_bwd_params' for 'sm_80'
ptxas info    : Function properties for _Z44flash_bwd_dq_dk_dv_loop_seqk_parallel_kernelI23Flash_bwd_kernel_traitsILi192ELi64ELi64ELi8ELi4ELi2ELi2ELb0ELb0EN7cutlass6half_tE19Flash_kernel_traitsILi192ELi64ELi64ELi8ES2_EELb1ELb1ELb1ELb1EEv16Flash_bwd_params
    0 bytes stack frame, 0 bytes spill stores, 0 bytes spill loads
ptxas info    : Used 240 registers, 696 bytes cmem[0]
ptxas info    : Compiling entry function '_Z25flash_bwd_dot_do_o_kernelILb1E23Flash_bwd_kernel_traitsILi192ELi64ELi64ELi8ELi4ELi2ELi2ELb0ELb0EN7cutlass6half_tE19Flash_kernel_traitsILi192ELi64ELi64ELi8ES2_EEEv16Flash_bwd_params' for 'sm_80'
ptxas info    : Function properties for _Z25flash_bwd_dot_do_o_kernelILb1E23Flash_bwd_kernel_traitsILi192ELi64ELi64ELi8ELi4ELi2ELi2ELb0ELb0EN7cutlass6half_tE19Flash_kernel_traitsILi192ELi64ELi64ELi8ES2_EEEv16Flash_bwd_params
    0 bytes stack frame, 0 bytes spill stores, 0 bytes spill loads
ptxas info    : Used 69 registers, 696 bytes cmem[0]
/anaconda/envs/flashattention/lib/python3.9/site-packages/torch/include/c10/core/SymInt.h(84): warning: integer conversion resulted in a change of sign

[15/37] /usr/local/cuda-11.4/bin/nvcc  -I/home/v-feiychen/flash-attention/csrc/flash_attn -I/home/v-feiychen/flash-attention/csrc/flash_attn/src -I/home/v-feiychen/flash-attention/csrc/cutlass/include -I/anaconda/envs/flashattention/lib/python3.9/site-packages/torch/include -I/anaconda/envs/flashattention/lib/python3.9/site-packages/torch/include/torch/csrc/api/include -I/anaconda/envs/flashattention/lib/python3.9/site-packages/torch/include/TH -I/anaconda/envs/flashattention/lib/python3.9/site-packages/torch/include/THC -I/usr/local/cuda-11.4/include -I/anaconda/envs/flashattention/include/python3.9 -c -c /home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_bwd_hdim192_bf16_sm80.cu -o /home/v-feiychen/flash-attention/build/temp.linux-x86_64-cpython-39/csrc/flash_attn/src/flash_bwd_hdim192_bf16_sm80.o -D__CUDA_NO_HALF_OPERATORS__ -D__CUDA_NO_HALF_CONVERSIONS__ -D__CUDA_NO_BFLOAT16_CONVERSIONS__ -D__CUDA_NO_HALF2_OPERATORS__ --expt-relaxed-constexpr --compiler-options ''"'"'-fPIC'"'"'' -O3 -std=c++17 -U__CUDA_NO_HALF_OPERATORS__ -U__CUDA_NO_HALF_CONVERSIONS__ -U__CUDA_NO_HALF2_OPERATORS__ -U__CUDA_NO_BFLOAT16_CONVERSIONS__ --expt-relaxed-constexpr --expt-extended-lambda --use_fast_math --ptxas-options=-v -lineinfo -gencode arch=compute_80,code=sm_80 --threads 4 -DTORCH_API_INCLUDE_EXTENSION_H '-DPYBIND11_COMPILER_TYPE="_gcc"' '-DPYBIND11_STDLIB="_libstdcpp"' '-DPYBIND11_BUILD_ABI="_cxxabi1011"' -DTORCH_EXTENSION_NAME=flash_attn_2_cuda -D_GLIBCXX_USE_CXX11_ABI=0
/anaconda/envs/flashattention/lib/python3.9/site-packages/torch/include/c10/core/SymInt.h(84): warning: integer conversion resulted in a change of sign

ptxas info    : 2 bytes gmem
ptxas info    : Compiling entry function '_Z44flash_bwd_dq_dk_dv_loop_seqk_parallel_kernelI23Flash_bwd_kernel_traitsILi192ELi64ELi64ELi8ELi4ELi2ELi2ELb1ELb1EN7cutlass10bfloat16_tE19Flash_kernel_traitsILi192ELi64ELi64ELi8ES2_EELb0ELb0ELb0ELb0EEv16Flash_bwd_params' for 'sm_80'
ptxas info    : Function properties for _Z44flash_bwd_dq_dk_dv_loop_seqk_parallel_kernelI23Flash_bwd_kernel_traitsILi192ELi64ELi64ELi8ELi4ELi2ELi2ELb1ELb1EN7cutlass10bfloat16_tE19Flash_kernel_traitsILi192ELi64ELi64ELi8ES2_EELb0ELb0ELb0ELb0EEv16Flash_bwd_params
    176 bytes stack frame, 192 bytes spill stores, 192 bytes spill loads
ptxas info    : Used 255 registers, 696 bytes cmem[0]
ptxas info    : Compiling entry function '_Z44flash_bwd_dq_dk_dv_loop_seqk_parallel_kernelI23Flash_bwd_kernel_traitsILi192ELi64ELi64ELi8ELi4ELi2ELi2ELb1ELb1EN7cutlass10bfloat16_tE19Flash_kernel_traitsILi192ELi64ELi64ELi8ES2_EELb0ELb0ELb0ELb1EEv16Flash_bwd_params' for 'sm_80'
ptxas info    : Function properties for _Z44flash_bwd_dq_dk_dv_loop_seqk_parallel_kernelI23Flash_bwd_kernel_traitsILi192ELi64ELi64ELi8ELi4ELi2ELi2ELb1ELb1EN7cutlass10bfloat16_tE19Flash_kernel_traitsILi192ELi64ELi64ELi8ES2_EELb0ELb0ELb0ELb1EEv16Flash_bwd_params
    176 bytes stack frame, 188 bytes spill stores, 192 bytes spill loads
ptxas info    : Used 255 registers, 696 bytes cmem[0]
ptxas info    : Compiling entry function '_Z44flash_bwd_dq_dk_dv_loop_seqk_parallel_kernelI23Flash_bwd_kernel_traitsILi192ELi64ELi64ELi8ELi4ELi2ELi2ELb1ELb1EN7cutlass10bfloat16_tE19Flash_kernel_traitsILi192ELi64ELi64ELi8ES2_EELb0ELb0ELb1ELb0EEv16Flash_bwd_params' for 'sm_80'
ptxas info    : Function properties for _Z44flash_bwd_dq_dk_dv_loop_seqk_parallel_kernelI23Flash_bwd_kernel_traitsILi192ELi64ELi64ELi8ELi4ELi2ELi2ELb1ELb1EN7cutlass10bfloat16_tE19Flash_kernel_traitsILi192ELi64ELi64ELi8ES2_EELb0ELb0ELb1ELb0EEv16Flash_bwd_params
    168 bytes stack frame, 176 bytes spill stores, 180 bytes spill loads
ptxas info    : Used 255 registers, 696 bytes cmem[0]
ptxas info    : Compiling entry function '_Z44flash_bwd_dq_dk_dv_loop_seqk_parallel_kernelI23Flash_bwd_kernel_traitsILi192ELi64ELi64ELi8ELi4ELi2ELi2ELb1ELb1EN7cutlass10bfloat16_tE19Flash_kernel_traitsILi192ELi64ELi64ELi8ES2_EELb0ELb0ELb1ELb1EEv16Flash_bwd_params' for 'sm_80'
ptxas info    : Function properties for _Z44flash_bwd_dq_dk_dv_loop_seqk_parallel_kernelI23Flash_bwd_kernel_traitsILi192ELi64ELi64ELi8ELi4ELi2ELi2ELb1ELb1EN7cutlass10bfloat16_tE19Flash_kernel_traitsILi192ELi64ELi64ELi8ES2_EELb0ELb0ELb1ELb1EEv16Flash_bwd_params
    168 bytes stack frame, 176 bytes spill stores, 180 bytes spill loads
ptxas info    : Used 255 registers, 696 bytes cmem[0]
ptxas info    : Compiling entry function '_Z44flash_bwd_dq_dk_dv_loop_seqk_parallel_kernelI23Flash_bwd_kernel_traitsILi192ELi64ELi64ELi8ELi4ELi2ELi2ELb1ELb1EN7cutlass10bfloat16_tE19Flash_kernel_traitsILi192ELi64ELi64ELi8ES2_EELb0ELb1ELb0ELb0EEv16Flash_bwd_params' for 'sm_80'
ptxas info    : Function properties for _Z44flash_bwd_dq_dk_dv_loop_seqk_parallel_kernelI23Flash_bwd_kernel_traitsILi192ELi64ELi64ELi8ELi4ELi2ELi2ELb1ELb1EN7cutlass10bfloat16_tE19Flash_kernel_traitsILi192ELi64ELi64ELi8ES2_EELb0ELb1ELb0ELb0EEv16Flash_bwd_params
    192 bytes stack frame, 196 bytes spill stores, 208 bytes spill loads
ptxas info    : Used 255 registers, 696 bytes cmem[0]
ptxas info    : Compiling entry function '_Z44flash_bwd_dq_dk_dv_loop_seqk_parallel_kernelI23Flash_bwd_kernel_traitsILi192ELi64ELi64ELi8ELi4ELi2ELi2ELb1ELb1EN7cutlass10bfloat16_tE19Flash_kernel_traitsILi192ELi64ELi64ELi8ES2_EELb0ELb1ELb0ELb1EEv16Flash_bwd_params' for 'sm_80'
ptxas info    : Function properties for _Z44flash_bwd_dq_dk_dv_loop_seqk_parallel_kernelI23Flash_bwd_kernel_traitsILi192ELi64ELi64ELi8ELi4ELi2ELi2ELb1ELb1EN7cutlass10bfloat16_tE19Flash_kernel_traitsILi192ELi64ELi64ELi8ES2_EELb0ELb1ELb0ELb1EEv16Flash_bwd_params
    176 bytes stack frame, 192 bytes spill stores, 196 bytes spill loads
ptxas info    : Used 255 registers, 696 bytes cmem[0]
ptxas info    : Compiling entry function '_Z44flash_bwd_dq_dk_dv_loop_seqk_parallel_kernelI23Flash_bwd_kernel_traitsILi192ELi64ELi64ELi8ELi4ELi2ELi2ELb1ELb1EN7cutlass10bfloat16_tE19Flash_kernel_traitsILi192ELi64ELi64ELi8ES2_EELb0ELb1ELb1ELb0EEv16Flash_bwd_params' for 'sm_80'
ptxas info    : Function properties for _Z44flash_bwd_dq_dk_dv_loop_seqk_parallel_kernelI23Flash_bwd_kernel_traitsILi192ELi64ELi64ELi8ELi4ELi2ELi2ELb1ELb1EN7cutlass10bfloat16_tE19Flash_kernel_traitsILi192ELi64ELi64ELi8ES2_EELb0ELb1ELb1ELb0EEv16Flash_bwd_params
    176 bytes stack frame, 180 bytes spill stores, 192 bytes spill loads
ptxas info    : Used 255 registers, 696 bytes cmem[0]
ptxas info    : Compiling entry function '_Z44flash_bwd_dq_dk_dv_loop_seqk_parallel_kernelI23Flash_bwd_kernel_traitsILi192ELi64ELi64ELi8ELi4ELi2ELi2ELb1ELb1EN7cutlass10bfloat16_tE19Flash_kernel_traitsILi192ELi64ELi64ELi8ES2_EELb0ELb1ELb1ELb1EEv16Flash_bwd_params' for 'sm_80'
ptxas info    : Function properties for _Z44flash_bwd_dq_dk_dv_loop_seqk_parallel_kernelI23Flash_bwd_kernel_traitsILi192ELi64ELi64ELi8ELi4ELi2ELi2ELb1ELb1EN7cutlass10bfloat16_tE19Flash_kernel_traitsILi192ELi64ELi64ELi8ES2_EELb0ELb1ELb1ELb1EEv16Flash_bwd_params
    176 bytes stack frame, 180 bytes spill stores, 192 bytes spill loads
ptxas info    : Used 255 registers, 696 bytes cmem[0]
ptxas info    : Compiling entry function '_Z44flash_bwd_dq_dk_dv_loop_seqk_parallel_kernelI23Flash_bwd_kernel_traitsILi192ELi64ELi64ELi8ELi4ELi2ELi2ELb0ELb0EN7cutlass10bfloat16_tE19Flash_kernel_traitsILi192ELi64ELi64ELi8ES2_EELb0ELb0ELb0ELb0EEv16Flash_bwd_params' for 'sm_80'
ptxas info    : Function properties for _Z44flash_bwd_dq_dk_dv_loop_seqk_parallel_kernelI23Flash_bwd_kernel_traitsILi192ELi64ELi64ELi8ELi4ELi2ELi2ELb0ELb0EN7cutlass10bfloat16_tE19Flash_kernel_traitsILi192ELi64ELi64ELi8ES2_EELb0ELb0ELb0ELb0EEv16Flash_bwd_params
    0 bytes stack frame, 0 bytes spill stores, 0 bytes spill loads
ptxas info    : Used 236 registers, 696 bytes cmem[0]
ptxas info    : Compiling entry function '_Z44flash_bwd_dq_dk_dv_loop_seqk_parallel_kernelI23Flash_bwd_kernel_traitsILi192ELi64ELi64ELi8ELi4ELi2ELi2ELb0ELb0EN7cutlass10bfloat16_tE19Flash_kernel_traitsILi192ELi64ELi64ELi8ES2_EELb0ELb0ELb0ELb1EEv16Flash_bwd_params' for 'sm_80'
ptxas info    : Function properties for _Z44flash_bwd_dq_dk_dv_loop_seqk_parallel_kernelI23Flash_bwd_kernel_traitsILi192ELi64ELi64ELi8ELi4ELi2ELi2ELb0ELb0EN7cutlass10bfloat16_tE19Flash_kernel_traitsILi192ELi64ELi64ELi8ES2_EELb0ELb0ELb0ELb1EEv16Flash_bwd_params
    0 bytes stack frame, 0 bytes spill stores, 0 bytes spill loads
ptxas info    : Used 236 registers, 696 bytes cmem[0]
ptxas info    : Compiling entry function '_Z44flash_bwd_dq_dk_dv_loop_seqk_parallel_kernelI23Flash_bwd_kernel_traitsILi192ELi64ELi64ELi8ELi4ELi2ELi2ELb0ELb0EN7cutlass10bfloat16_tE19Flash_kernel_traitsILi192ELi64ELi64ELi8ES2_EELb0ELb0ELb1ELb0EEv16Flash_bwd_params' for 'sm_80'
ptxas info    : Function properties for _Z44flash_bwd_dq_dk_dv_loop_seqk_parallel_kernelI23Flash_bwd_kernel_traitsILi192ELi64ELi64ELi8ELi4ELi2ELi2ELb0ELb0EN7cutlass10bfloat16_tE19Flash_kernel_traitsILi192ELi64ELi64ELi8ES2_EELb0ELb0ELb1ELb0EEv16Flash_bwd_params
    0 bytes stack frame, 0 bytes spill stores, 0 bytes spill loads
ptxas info    : Used 231 registers, 696 bytes cmem[0]
ptxas info    : Compiling entry function '_Z44flash_bwd_dq_dk_dv_loop_seqk_parallel_kernelI23Flash_bwd_kernel_traitsILi192ELi64ELi64ELi8ELi4ELi2ELi2ELb0ELb0EN7cutlass10bfloat16_tE19Flash_kernel_traitsILi192ELi64ELi64ELi8ES2_EELb0ELb0ELb1ELb1EEv16Flash_bwd_params' for 'sm_80'
ptxas info    : Function properties for _Z44flash_bwd_dq_dk_dv_loop_seqk_parallel_kernelI23Flash_bwd_kernel_traitsILi192ELi64ELi64ELi8ELi4ELi2ELi2ELb0ELb0EN7cutlass10bfloat16_tE19Flash_kernel_traitsILi192ELi64ELi64ELi8ES2_EELb0ELb0ELb1ELb1EEv16Flash_bwd_params
    0 bytes stack frame, 0 bytes spill stores, 0 bytes spill loads
ptxas info    : Used 235 registers, 696 bytes cmem[0]
ptxas info    : Compiling entry function '_Z44flash_bwd_dq_dk_dv_loop_seqk_parallel_kernelI23Flash_bwd_kernel_traitsILi192ELi64ELi64ELi8ELi4ELi2ELi2ELb0ELb0EN7cutlass10bfloat16_tE19Flash_kernel_traitsILi192ELi64ELi64ELi8ES2_EELb0ELb1ELb0ELb0EEv16Flash_bwd_params' for 'sm_80'
ptxas info    : Function properties for _Z44flash_bwd_dq_dk_dv_loop_seqk_parallel_kernelI23Flash_bwd_kernel_traitsILi192ELi64ELi64ELi8ELi4ELi2ELi2ELb0ELb0EN7cutlass10bfloat16_tE19Flash_kernel_traitsILi192ELi64ELi64ELi8ES2_EELb0ELb1ELb0ELb0EEv16Flash_bwd_params
    0 bytes stack frame, 0 bytes spill stores, 0 bytes spill loads
ptxas info    : Used 236 registers, 696 bytes cmem[0]
ptxas info    : Compiling entry function '_Z44flash_bwd_dq_dk_dv_loop_seqk_parallel_kernelI23Flash_bwd_kernel_traitsILi192ELi64ELi64ELi8ELi4ELi2ELi2ELb0ELb0EN7cutlass10bfloat16_tE19Flash_kernel_traitsILi192ELi64ELi64ELi8ES2_EELb0ELb1ELb0ELb1EEv16Flash_bwd_params' for 'sm_80'
ptxas info    : Function properties for _Z44flash_bwd_dq_dk_dv_loop_seqk_parallel_kernelI23Flash_bwd_kernel_traitsILi192ELi64ELi64ELi8ELi4ELi2ELi2ELb0ELb0EN7cutlass10bfloat16_tE19Flash_kernel_traitsILi192ELi64ELi64ELi8ES2_EELb0ELb1ELb0ELb1EEv16Flash_bwd_params
    0 bytes stack frame, 0 bytes spill stores, 0 bytes spill loads
ptxas info    : Used 236 registers, 696 bytes cmem[0]
ptxas info    : Compiling entry function '_Z44flash_bwd_dq_dk_dv_loop_seqk_parallel_kernelI23Flash_bwd_kernel_traitsILi192ELi64ELi64ELi8ELi4ELi2ELi2ELb0ELb0EN7cutlass10bfloat16_tE19Flash_kernel_traitsILi192ELi64ELi64ELi8ES2_EELb0ELb1ELb1ELb0EEv16Flash_bwd_params' for 'sm_80'
ptxas info    : Function properties for _Z44flash_bwd_dq_dk_dv_loop_seqk_parallel_kernelI23Flash_bwd_kernel_traitsILi192ELi64ELi64ELi8ELi4ELi2ELi2ELb0ELb0EN7cutlass10bfloat16_tE19Flash_kernel_traitsILi192ELi64ELi64ELi8ES2_EELb0ELb1ELb1ELb0EEv16Flash_bwd_params
    0 bytes stack frame, 0 bytes spill stores, 0 bytes spill loads
ptxas info    : Used 234 registers, 696 bytes cmem[0]
ptxas info    : Compiling entry function '_Z44flash_bwd_dq_dk_dv_loop_seqk_parallel_kernelI23Flash_bwd_kernel_traitsILi192ELi64ELi64ELi8ELi4ELi2ELi2ELb0ELb0EN7cutlass10bfloat16_tE19Flash_kernel_traitsILi192ELi64ELi64ELi8ES2_EELb0ELb1ELb1ELb1EEv16Flash_bwd_params' for 'sm_80'
ptxas info    : Function properties for _Z44flash_bwd_dq_dk_dv_loop_seqk_parallel_kernelI23Flash_bwd_kernel_traitsILi192ELi64ELi64ELi8ELi4ELi2ELi2ELb0ELb0EN7cutlass10bfloat16_tE19Flash_kernel_traitsILi192ELi64ELi64ELi8ES2_EELb0ELb1ELb1ELb1EEv16Flash_bwd_params
    0 bytes stack frame, 0 bytes spill stores, 0 bytes spill loads
ptxas info    : Used 238 registers, 696 bytes cmem[0]
ptxas info    : Compiling entry function '_Z27flash_bwd_convert_dq_kernelI23Flash_bwd_kernel_traitsILi192ELi64ELi64ELi8ELi4ELi2ELi2ELb1ELb1EN7cutlass10bfloat16_tE19Flash_kernel_traitsILi192ELi64ELi64ELi8ES2_EEEv16Flash_bwd_params' for 'sm_80'
ptxas info    : Function properties for _Z27flash_bwd_convert_dq_kernelI23Flash_bwd_kernel_traitsILi192ELi64ELi64ELi8ELi4ELi2ELi2ELb1ELb1EN7cutlass10bfloat16_tE19Flash_kernel_traitsILi192ELi64ELi64ELi8ES2_EEEv16Flash_bwd_params
    0 bytes stack frame, 0 bytes spill stores, 0 bytes spill loads
ptxas info    : Used 32 registers, 696 bytes cmem[0]
ptxas info    : Compiling entry function '_Z44flash_bwd_dq_dk_dv_loop_seqk_parallel_kernelI23Flash_bwd_kernel_traitsILi192ELi64ELi64ELi8ELi4ELi2ELi2ELb1ELb1EN7cutlass10bfloat16_tE19Flash_kernel_traitsILi192ELi64ELi64ELi8ES2_EELb1ELb0ELb0ELb0EEv16Flash_bwd_params' for 'sm_80'
ptxas info    : Function properties for _Z44flash_bwd_dq_dk_dv_loop_seqk_parallel_kernelI23Flash_bwd_kernel_traitsILi192ELi64ELi64ELi8ELi4ELi2ELi2ELb1ELb1EN7cutlass10bfloat16_tE19Flash_kernel_traitsILi192ELi64ELi64ELi8ES2_EELb1ELb0ELb0ELb0EEv16Flash_bwd_params
    192 bytes stack frame, 200 bytes spill stores, 208 bytes spill loads
ptxas info    : Used 255 registers, 696 bytes cmem[0]
ptxas info    : Compiling entry function '_Z44flash_bwd_dq_dk_dv_loop_seqk_parallel_kernelI23Flash_bwd_kernel_traitsILi192ELi64ELi64ELi8ELi4ELi2ELi2ELb1ELb1EN7cutlass10bfloat16_tE19Flash_kernel_traitsILi192ELi64ELi64ELi8ES2_EELb1ELb0ELb0ELb1EEv16Flash_bwd_params' for 'sm_80'
ptxas info    : Function properties for _Z44flash_bwd_dq_dk_dv_loop_seqk_parallel_kernelI23Flash_bwd_kernel_traitsILi192ELi64ELi64ELi8ELi4ELi2ELi2ELb1ELb1EN7cutlass10bfloat16_tE19Flash_kernel_traitsILi192ELi64ELi64ELi8ES2_EELb1ELb0ELb0ELb1EEv16Flash_bwd_params
    192 bytes stack frame, 196 bytes spill stores, 204 bytes spill loads
ptxas info    : Used 255 registers, 696 bytes cmem[0]
ptxas info    : Compiling entry function '_Z44flash_bwd_dq_dk_dv_loop_seqk_parallel_kernelI23Flash_bwd_kernel_traitsILi192ELi64ELi64ELi8ELi4ELi2ELi2ELb1ELb1EN7cutlass10bfloat16_tE19Flash_kernel_traitsILi192ELi64ELi64ELi8ES2_EELb1ELb0ELb1ELb0EEv16Flash_bwd_params' for 'sm_80'
ptxas info    : Function properties for _Z44flash_bwd_dq_dk_dv_loop_seqk_parallel_kernelI23Flash_bwd_kernel_traitsILi192ELi64ELi64ELi8ELi4ELi2ELi2ELb1ELb1EN7cutlass10bfloat16_tE19Flash_kernel_traitsILi192ELi64ELi64ELi8ES2_EELb1ELb0ELb1ELb0EEv16Flash_bwd_params
    192 bytes stack frame, 196 bytes spill stores, 204 bytes spill loads
ptxas info    : Used 255 registers, 696 bytes cmem[0]
ptxas info    : Compiling entry function '_Z44flash_bwd_dq_dk_dv_loop_seqk_parallel_kernelI23Flash_bwd_kernel_traitsILi192ELi64ELi64ELi8ELi4ELi2ELi2ELb1ELb1EN7cutlass10bfloat16_tE19Flash_kernel_traitsILi192ELi64ELi64ELi8ES2_EELb1ELb0ELb1ELb1EEv16Flash_bwd_params' for 'sm_80'
ptxas info    : Function properties for _Z44flash_bwd_dq_dk_dv_loop_seqk_parallel_kernelI23Flash_bwd_kernel_traitsILi192ELi64ELi64ELi8ELi4ELi2ELi2ELb1ELb1EN7cutlass10bfloat16_tE19Flash_kernel_traitsILi192ELi64ELi64ELi8ES2_EELb1ELb0ELb1ELb1EEv16Flash_bwd_params
    192 bytes stack frame, 196 bytes spill stores, 196 bytes spill loads
ptxas info    : Used 255 registers, 696 bytes cmem[0]
ptxas info    : Compiling entry function '_Z44flash_bwd_dq_dk_dv_loop_seqk_parallel_kernelI23Flash_bwd_kernel_traitsILi192ELi64ELi64ELi8ELi4ELi2ELi2ELb1ELb1EN7cutlass10bfloat16_tE19Flash_kernel_traitsILi192ELi64ELi64ELi8ES2_EELb1ELb1ELb0ELb0EEv16Flash_bwd_params' for 'sm_80'
ptxas info    : Function properties for _Z44flash_bwd_dq_dk_dv_loop_seqk_parallel_kernelI23Flash_bwd_kernel_traitsILi192ELi64ELi64ELi8ELi4ELi2ELi2ELb1ELb1EN7cutlass10bfloat16_tE19Flash_kernel_traitsILi192ELi64ELi64ELi8ES2_EELb1ELb1ELb0ELb0EEv16Flash_bwd_params
    208 bytes stack frame, 212 bytes spill stores, 220 bytes spill loads
ptxas info    : Used 255 registers, 696 bytes cmem[0]
ptxas info    : Compiling entry function '_Z44flash_bwd_dq_dk_dv_loop_seqk_parallel_kernelI23Flash_bwd_kernel_traitsILi192ELi64ELi64ELi8ELi4ELi2ELi2ELb1ELb1EN7cutlass10bfloat16_tE19Flash_kernel_traitsILi192ELi64ELi64ELi8ES2_EELb1ELb1ELb0ELb1EEv16Flash_bwd_params' for 'sm_80'
ptxas info    : Function properties for _Z44flash_bwd_dq_dk_dv_loop_seqk_parallel_kernelI23Flash_bwd_kernel_traitsILi192ELi64ELi64ELi8ELi4ELi2ELi2ELb1ELb1EN7cutlass10bfloat16_tE19Flash_kernel_traitsILi192ELi64ELi64ELi8ES2_EELb1ELb1ELb0ELb1EEv16Flash_bwd_params
    192 bytes stack frame, 200 bytes spill stores, 208 bytes spill loads
ptxas info    : Used 255 registers, 696 bytes cmem[0]
ptxas info    : Compiling entry function '_Z44flash_bwd_dq_dk_dv_loop_seqk_parallel_kernelI23Flash_bwd_kernel_traitsILi192ELi64ELi64ELi8ELi4ELi2ELi2ELb1ELb1EN7cutlass10bfloat16_tE19Flash_kernel_traitsILi192ELi64ELi64ELi8ES2_EELb1ELb1ELb1ELb0EEv16Flash_bwd_params' for 'sm_80'
ptxas info    : Function properties for _Z44flash_bwd_dq_dk_dv_loop_seqk_parallel_kernelI23Flash_bwd_kernel_traitsILi192ELi64ELi64ELi8ELi4ELi2ELi2ELb1ELb1EN7cutlass10bfloat16_tE19Flash_kernel_traitsILi192ELi64ELi64ELi8ES2_EELb1ELb1ELb1ELb0EEv16Flash_bwd_params
    208 bytes stack frame, 212 bytes spill stores, 220 bytes spill loads
ptxas info    : Used 255 registers, 696 bytes cmem[0]
ptxas info    : Compiling entry function '_Z44flash_bwd_dq_dk_dv_loop_seqk_parallel_kernelI23Flash_bwd_kernel_traitsILi192ELi64ELi64ELi8ELi4ELi2ELi2ELb1ELb1EN7cutlass10bfloat16_tE19Flash_kernel_traitsILi192ELi64ELi64ELi8ES2_EELb1ELb1ELb1ELb1EEv16Flash_bwd_params' for 'sm_80'
ptxas info    : Function properties for _Z44flash_bwd_dq_dk_dv_loop_seqk_parallel_kernelI23Flash_bwd_kernel_traitsILi192ELi64ELi64ELi8ELi4ELi2ELi2ELb1ELb1EN7cutlass10bfloat16_tE19Flash_kernel_traitsILi192ELi64ELi64ELi8ES2_EELb1ELb1ELb1ELb1EEv16Flash_bwd_params
    192 bytes stack frame, 196 bytes spill stores, 204 bytes spill loads
ptxas info    : Used 255 registers, 696 bytes cmem[0]
ptxas info    : Compiling entry function '_Z25flash_bwd_dot_do_o_kernelILb1E23Flash_bwd_kernel_traitsILi192ELi64ELi64ELi8ELi4ELi2ELi2ELb1ELb1EN7cutlass10bfloat16_tE19Flash_kernel_traitsILi192ELi64ELi64ELi8ES2_EEEv16Flash_bwd_params' for 'sm_80'
ptxas info    : Function properties for _Z25flash_bwd_dot_do_o_kernelILb1E23Flash_bwd_kernel_traitsILi192ELi64ELi64ELi8ELi4ELi2ELi2ELb1ELb1EN7cutlass10bfloat16_tE19Flash_kernel_traitsILi192ELi64ELi64ELi8ES2_EEEv16Flash_bwd_params
    0 bytes stack frame, 0 bytes spill stores, 0 bytes spill loads
ptxas info    : Used 69 registers, 696 bytes cmem[0]
ptxas info    : Compiling entry function '_Z27flash_bwd_convert_dq_kernelI23Flash_bwd_kernel_traitsILi192ELi64ELi64ELi8ELi4ELi2ELi2ELb0ELb0EN7cutlass10bfloat16_tE19Flash_kernel_traitsILi192ELi64ELi64ELi8ES2_EEEv16Flash_bwd_params' for 'sm_80'
ptxas info    : Function properties for _Z27flash_bwd_convert_dq_kernelI23Flash_bwd_kernel_traitsILi192ELi64ELi64ELi8ELi4ELi2ELi2ELb0ELb0EN7cutlass10bfloat16_tE19Flash_kernel_traitsILi192ELi64ELi64ELi8ES2_EEEv16Flash_bwd_params
    0 bytes stack frame, 0 bytes spill stores, 0 bytes spill loads
ptxas info    : Used 32 registers, 696 bytes cmem[0]
ptxas info    : Compiling entry function '_Z44flash_bwd_dq_dk_dv_loop_seqk_parallel_kernelI23Flash_bwd_kernel_traitsILi192ELi64ELi64ELi8ELi4ELi2ELi2ELb0ELb0EN7cutlass10bfloat16_tE19Flash_kernel_traitsILi192ELi64ELi64ELi8ES2_EELb1ELb0ELb0ELb0EEv16Flash_bwd_params' for 'sm_80'
ptxas info    : Function properties for _Z44flash_bwd_dq_dk_dv_loop_seqk_parallel_kernelI23Flash_bwd_kernel_traitsILi192ELi64ELi64ELi8ELi4ELi2ELi2ELb0ELb0EN7cutlass10bfloat16_tE19Flash_kernel_traitsILi192ELi64ELi64ELi8ES2_EELb1ELb0ELb0ELb0EEv16Flash_bwd_params
    0 bytes stack frame, 0 bytes spill stores, 0 bytes spill loads
ptxas info    : Used 237 registers, 696 bytes cmem[0]
ptxas info    : Compiling entry function '_Z44flash_bwd_dq_dk_dv_loop_seqk_parallel_kernelI23Flash_bwd_kernel_traitsILi192ELi64ELi64ELi8ELi4ELi2ELi2ELb0ELb0EN7cutlass10bfloat16_tE19Flash_kernel_traitsILi192ELi64ELi64ELi8ES2_EELb1ELb0ELb0ELb1EEv16Flash_bwd_params' for 'sm_80'
ptxas info    : Function properties for _Z44flash_bwd_dq_dk_dv_loop_seqk_parallel_kernelI23Flash_bwd_kernel_traitsILi192ELi64ELi64ELi8ELi4ELi2ELi2ELb0ELb0EN7cutlass10bfloat16_tE19Flash_kernel_traitsILi192ELi64ELi64ELi8ES2_EELb1ELb0ELb0ELb1EEv16Flash_bwd_params
    0 bytes stack frame, 0 bytes spill stores, 0 bytes spill loads
ptxas info    : Used 238 registers, 696 bytes cmem[0]
ptxas info    : Compiling entry function '_Z44flash_bwd_dq_dk_dv_loop_seqk_parallel_kernelI23Flash_bwd_kernel_traitsILi192ELi64ELi64ELi8ELi4ELi2ELi2ELb0ELb0EN7cutlass10bfloat16_tE19Flash_kernel_traitsILi192ELi64ELi64ELi8ES2_EELb1ELb0ELb1ELb0EEv16Flash_bwd_params' for 'sm_80'
ptxas info    : Function properties for _Z44flash_bwd_dq_dk_dv_loop_seqk_parallel_kernelI23Flash_bwd_kernel_traitsILi192ELi64ELi64ELi8ELi4ELi2ELi2ELb0ELb0EN7cutlass10bfloat16_tE19Flash_kernel_traitsILi192ELi64ELi64ELi8ES2_EELb1ELb0ELb1ELb0EEv16Flash_bwd_params
    0 bytes stack frame, 0 bytes spill stores, 0 bytes spill loads
ptxas info    : Used 232 registers, 696 bytes cmem[0]
ptxas info    : Compiling entry function '_Z44flash_bwd_dq_dk_dv_loop_seqk_parallel_kernelI23Flash_bwd_kernel_traitsILi192ELi64ELi64ELi8ELi4ELi2ELi2ELb0ELb0EN7cutlass10bfloat16_tE19Flash_kernel_traitsILi192ELi64ELi64ELi8ES2_EELb1ELb0ELb1ELb1EEv16Flash_bwd_params' for 'sm_80'
ptxas info    : Function properties for _Z44flash_bwd_dq_dk_dv_loop_seqk_parallel_kernelI23Flash_bwd_kernel_traitsILi192ELi64ELi64ELi8ELi4ELi2ELi2ELb0ELb0EN7cutlass10bfloat16_tE19Flash_kernel_traitsILi192ELi64ELi64ELi8ES2_EELb1ELb0ELb1ELb1EEv16Flash_bwd_params
    0 bytes stack frame, 0 bytes spill stores, 0 bytes spill loads
ptxas info    : Used 233 registers, 696 bytes cmem[0]
ptxas info    : Compiling entry function '_Z44flash_bwd_dq_dk_dv_loop_seqk_parallel_kernelI23Flash_bwd_kernel_traitsILi192ELi64ELi64ELi8ELi4ELi2ELi2ELb0ELb0EN7cutlass10bfloat16_tE19Flash_kernel_traitsILi192ELi64ELi64ELi8ES2_EELb1ELb1ELb0ELb0EEv16Flash_bwd_params' for 'sm_80'
ptxas info    : Function properties for _Z44flash_bwd_dq_dk_dv_loop_seqk_parallel_kernelI23Flash_bwd_kernel_traitsILi192ELi64ELi64ELi8ELi4ELi2ELi2ELb0ELb0EN7cutlass10bfloat16_tE19Flash_kernel_traitsILi192ELi64ELi64ELi8ES2_EELb1ELb1ELb0ELb0EEv16Flash_bwd_params
    0 bytes stack frame, 0 bytes spill stores, 0 bytes spill loads
ptxas info    : Used 240 registers, 696 bytes cmem[0]
ptxas info    : Compiling entry function '_Z44flash_bwd_dq_dk_dv_loop_seqk_parallel_kernelI23Flash_bwd_kernel_traitsILi192ELi64ELi64ELi8ELi4ELi2ELi2ELb0ELb0EN7cutlass10bfloat16_tE19Flash_kernel_traitsILi192ELi64ELi64ELi8ES2_EELb1ELb1ELb0ELb1EEv16Flash_bwd_params' for 'sm_80'
ptxas info    : Function properties for _Z44flash_bwd_dq_dk_dv_loop_seqk_parallel_kernelI23Flash_bwd_kernel_traitsILi192ELi64ELi64ELi8ELi4ELi2ELi2ELb0ELb0EN7cutlass10bfloat16_tE19Flash_kernel_traitsILi192ELi64ELi64ELi8ES2_EELb1ELb1ELb0ELb1EEv16Flash_bwd_params
    0 bytes stack frame, 0 bytes spill stores, 0 bytes spill loads
ptxas info    : Used 239 registers, 696 bytes cmem[0]
ptxas info    : Compiling entry function '_Z44flash_bwd_dq_dk_dv_loop_seqk_parallel_kernelI23Flash_bwd_kernel_traitsILi192ELi64ELi64ELi8ELi4ELi2ELi2ELb0ELb0EN7cutlass10bfloat16_tE19Flash_kernel_traitsILi192ELi64ELi64ELi8ES2_EELb1ELb1ELb1ELb0EEv16Flash_bwd_params' for 'sm_80'
ptxas info    : Function properties for _Z44flash_bwd_dq_dk_dv_loop_seqk_parallel_kernelI23Flash_bwd_kernel_traitsILi192ELi64ELi64ELi8ELi4ELi2ELi2ELb0ELb0EN7cutlass10bfloat16_tE19Flash_kernel_traitsILi192ELi64ELi64ELi8ES2_EELb1ELb1ELb1ELb0EEv16Flash_bwd_params
    0 bytes stack frame, 0 bytes spill stores, 0 bytes spill loads
ptxas info    : Used 236 registers, 696 bytes cmem[0]
ptxas info    : Compiling entry function '_Z44flash_bwd_dq_dk_dv_loop_seqk_parallel_kernelI23Flash_bwd_kernel_traitsILi192ELi64ELi64ELi8ELi4ELi2ELi2ELb0ELb0EN7cutlass10bfloat16_tE19Flash_kernel_traitsILi192ELi64ELi64ELi8ES2_EELb1ELb1ELb1ELb1EEv16Flash_bwd_params' for 'sm_80'
ptxas info    : Function properties for _Z44flash_bwd_dq_dk_dv_loop_seqk_parallel_kernelI23Flash_bwd_kernel_traitsILi192ELi64ELi64ELi8ELi4ELi2ELi2ELb0ELb0EN7cutlass10bfloat16_tE19Flash_kernel_traitsILi192ELi64ELi64ELi8ES2_EELb1ELb1ELb1ELb1EEv16Flash_bwd_params
    0 bytes stack frame, 0 bytes spill stores, 0 bytes spill loads
ptxas info    : Used 240 registers, 696 bytes cmem[0]
ptxas info    : Compiling entry function '_Z25flash_bwd_dot_do_o_kernelILb1E23Flash_bwd_kernel_traitsILi192ELi64ELi64ELi8ELi4ELi2ELi2ELb0ELb0EN7cutlass10bfloat16_tE19Flash_kernel_traitsILi192ELi64ELi64ELi8ES2_EEEv16Flash_bwd_params' for 'sm_80'
ptxas info    : Function properties for _Z25flash_bwd_dot_do_o_kernelILb1E23Flash_bwd_kernel_traitsILi192ELi64ELi64ELi8ELi4ELi2ELi2ELb0ELb0EN7cutlass10bfloat16_tE19Flash_kernel_traitsILi192ELi64ELi64ELi8ES2_EEEv16Flash_bwd_params
    0 bytes stack frame, 0 bytes spill stores, 0 bytes spill loads
ptxas info    : Used 69 registers, 696 bytes cmem[0]
/anaconda/envs/flashattention/lib/python3.9/site-packages/torch/include/c10/core/SymInt.h(84): warning: integer conversion resulted in a change of sign

[16/37] /usr/local/cuda-11.4/bin/nvcc  -I/home/v-feiychen/flash-attention/csrc/flash_attn -I/home/v-feiychen/flash-attention/csrc/flash_attn/src -I/home/v-feiychen/flash-attention/csrc/cutlass/include -I/anaconda/envs/flashattention/lib/python3.9/site-packages/torch/include -I/anaconda/envs/flashattention/lib/python3.9/site-packages/torch/include/torch/csrc/api/include -I/anaconda/envs/flashattention/lib/python3.9/site-packages/torch/include/TH -I/anaconda/envs/flashattention/lib/python3.9/site-packages/torch/include/THC -I/usr/local/cuda-11.4/include -I/anaconda/envs/flashattention/include/python3.9 -c -c /home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_bwd_hdim256_fp16_sm80.cu -o /home/v-feiychen/flash-attention/build/temp.linux-x86_64-cpython-39/csrc/flash_attn/src/flash_bwd_hdim256_fp16_sm80.o -D__CUDA_NO_HALF_OPERATORS__ -D__CUDA_NO_HALF_CONVERSIONS__ -D__CUDA_NO_BFLOAT16_CONVERSIONS__ -D__CUDA_NO_HALF2_OPERATORS__ --expt-relaxed-constexpr --compiler-options ''"'"'-fPIC'"'"'' -O3 -std=c++17 -U__CUDA_NO_HALF_OPERATORS__ -U__CUDA_NO_HALF_CONVERSIONS__ -U__CUDA_NO_HALF2_OPERATORS__ -U__CUDA_NO_BFLOAT16_CONVERSIONS__ --expt-relaxed-constexpr --expt-extended-lambda --use_fast_math --ptxas-options=-v -lineinfo -gencode arch=compute_80,code=sm_80 --threads 4 -DTORCH_API_INCLUDE_EXTENSION_H '-DPYBIND11_COMPILER_TYPE="_gcc"' '-DPYBIND11_STDLIB="_libstdcpp"' '-DPYBIND11_BUILD_ABI="_cxxabi1011"' -DTORCH_EXTENSION_NAME=flash_attn_2_cuda -D_GLIBCXX_USE_CXX11_ABI=0
/anaconda/envs/flashattention/lib/python3.9/site-packages/torch/include/c10/core/SymInt.h(84): warning: integer conversion resulted in a change of sign

ptxas info    : 2 bytes gmem
ptxas info    : Compiling entry function '_Z44flash_bwd_dq_dk_dv_loop_seqk_parallel_kernelI23Flash_bwd_kernel_traitsILi256ELi64ELi64ELi8ELi4ELi2ELi2ELb0ELb1EN7cutlass6half_tE19Flash_kernel_traitsILi256ELi64ELi64ELi8ES2_EELb0ELb0ELb0ELb0EEv16Flash_bwd_params' for 'sm_80'
ptxas info    : Function properties for _Z44flash_bwd_dq_dk_dv_loop_seqk_parallel_kernelI23Flash_bwd_kernel_traitsILi256ELi64ELi64ELi8ELi4ELi2ELi2ELb0ELb1EN7cutlass6half_tE19Flash_kernel_traitsILi256ELi64ELi64ELi8ES2_EELb0ELb0ELb0ELb0EEv16Flash_bwd_params
    16 bytes stack frame, 12 bytes spill stores, 12 bytes spill loads
ptxas info    : Used 255 registers, 696 bytes cmem[0]
ptxas info    : Compiling entry function '_Z44flash_bwd_dq_dk_dv_loop_seqk_parallel_kernelI23Flash_bwd_kernel_traitsILi256ELi64ELi64ELi8ELi4ELi2ELi2ELb0ELb1EN7cutlass6half_tE19Flash_kernel_traitsILi256ELi64ELi64ELi8ES2_EELb0ELb0ELb0ELb1EEv16Flash_bwd_params' for 'sm_80'
ptxas info    : Function properties for _Z44flash_bwd_dq_dk_dv_loop_seqk_parallel_kernelI23Flash_bwd_kernel_traitsILi256ELi64ELi64ELi8ELi4ELi2ELi2ELb0ELb1EN7cutlass6half_tE19Flash_kernel_traitsILi256ELi64ELi64ELi8ES2_EELb0ELb0ELb0ELb1EEv16Flash_bwd_params
    0 bytes stack frame, 0 bytes spill stores, 0 bytes spill loads
ptxas info    : Used 252 registers, 696 bytes cmem[0]
ptxas info    : Compiling entry function '_Z44flash_bwd_dq_dk_dv_loop_seqk_parallel_kernelI23Flash_bwd_kernel_traitsILi256ELi64ELi64ELi8ELi4ELi2ELi2ELb0ELb1EN7cutlass6half_tE19Flash_kernel_traitsILi256ELi64ELi64ELi8ES2_EELb0ELb0ELb1ELb0EEv16Flash_bwd_params' for 'sm_80'
ptxas info    : Function properties for _Z44flash_bwd_dq_dk_dv_loop_seqk_parallel_kernelI23Flash_bwd_kernel_traitsILi256ELi64ELi64ELi8ELi4ELi2ELi2ELb0ELb1EN7cutlass6half_tE19Flash_kernel_traitsILi256ELi64ELi64ELi8ES2_EELb0ELb0ELb1ELb0EEv16Flash_bwd_params
    0 bytes stack frame, 0 bytes spill stores, 0 bytes spill loads
ptxas info    : Used 254 registers, 696 bytes cmem[0]
ptxas info    : Compiling entry function '_Z44flash_bwd_dq_dk_dv_loop_seqk_parallel_kernelI23Flash_bwd_kernel_traitsILi256ELi64ELi64ELi8ELi4ELi2ELi2ELb0ELb1EN7cutlass6half_tE19Flash_kernel_traitsILi256ELi64ELi64ELi8ES2_EELb0ELb0ELb1ELb1EEv16Flash_bwd_params' for 'sm_80'
ptxas info    : Function properties for _Z44flash_bwd_dq_dk_dv_loop_seqk_parallel_kernelI23Flash_bwd_kernel_traitsILi256ELi64ELi64ELi8ELi4ELi2ELi2ELb0ELb1EN7cutlass6half_tE19Flash_kernel_traitsILi256ELi64ELi64ELi8ES2_EELb0ELb0ELb1ELb1EEv16Flash_bwd_params
    8 bytes stack frame, 4 bytes spill stores, 4 bytes spill loads
ptxas info    : Used 255 registers, 696 bytes cmem[0]
ptxas info    : Compiling entry function '_Z44flash_bwd_dq_dk_dv_loop_seqk_parallel_kernelI23Flash_bwd_kernel_traitsILi256ELi64ELi64ELi8ELi4ELi2ELi2ELb0ELb1EN7cutlass6half_tE19Flash_kernel_traitsILi256ELi64ELi64ELi8ES2_EELb0ELb1ELb0ELb0EEv16Flash_bwd_params' for 'sm_80'
ptxas info    : Function properties for _Z44flash_bwd_dq_dk_dv_loop_seqk_parallel_kernelI23Flash_bwd_kernel_traitsILi256ELi64ELi64ELi8ELi4ELi2ELi2ELb0ELb1EN7cutlass6half_tE19Flash_kernel_traitsILi256ELi64ELi64ELi8ES2_EELb0ELb1ELb0ELb0EEv16Flash_bwd_params
    16 bytes stack frame, 12 bytes spill stores, 12 bytes spill loads
ptxas info    : Used 255 registers, 696 bytes cmem[0]
ptxas info    : Compiling entry function '_Z44flash_bwd_dq_dk_dv_loop_seqk_parallel_kernelI23Flash_bwd_kernel_traitsILi256ELi64ELi64ELi8ELi4ELi2ELi2ELb0ELb1EN7cutlass6half_tE19Flash_kernel_traitsILi256ELi64ELi64ELi8ES2_EELb0ELb1ELb0ELb1EEv16Flash_bwd_params' for 'sm_80'
ptxas info    : Function properties for _Z44flash_bwd_dq_dk_dv_loop_seqk_parallel_kernelI23Flash_bwd_kernel_traitsILi256ELi64ELi64ELi8ELi4ELi2ELi2ELb0ELb1EN7cutlass6half_tE19Flash_kernel_traitsILi256ELi64ELi64ELi8ES2_EELb0ELb1ELb0ELb1EEv16Flash_bwd_params
    8 bytes stack frame, 8 bytes spill stores, 8 bytes spill loads
ptxas info    : Used 255 registers, 696 bytes cmem[0]
ptxas info    : Compiling entry function '_Z44flash_bwd_dq_dk_dv_loop_seqk_parallel_kernelI23Flash_bwd_kernel_traitsILi256ELi64ELi64ELi8ELi4ELi2ELi2ELb0ELb1EN7cutlass6half_tE19Flash_kernel_traitsILi256ELi64ELi64ELi8ES2_EELb0ELb1ELb1ELb0EEv16Flash_bwd_params' for 'sm_80'
ptxas info    : Function properties for _Z44flash_bwd_dq_dk_dv_loop_seqk_parallel_kernelI23Flash_bwd_kernel_traitsILi256ELi64ELi64ELi8ELi4ELi2ELi2ELb0ELb1EN7cutlass6half_tE19Flash_kernel_traitsILi256ELi64ELi64ELi8ES2_EELb0ELb1ELb1ELb0EEv16Flash_bwd_params
    0 bytes stack frame, 0 bytes spill stores, 0 bytes spill loads
ptxas info    : Used 252 registers, 696 bytes cmem[0]
ptxas info    : Compiling entry function '_Z44flash_bwd_dq_dk_dv_loop_seqk_parallel_kernelI23Flash_bwd_kernel_traitsILi256ELi64ELi64ELi8ELi4ELi2ELi2ELb0ELb1EN7cutlass6half_tE19Flash_kernel_traitsILi256ELi64ELi64ELi8ES2_EELb0ELb1ELb1ELb1EEv16Flash_bwd_params' for 'sm_80'
ptxas info    : Function properties for _Z44flash_bwd_dq_dk_dv_loop_seqk_parallel_kernelI23Flash_bwd_kernel_traitsILi256ELi64ELi64ELi8ELi4ELi2ELi2ELb0ELb1EN7cutlass6half_tE19Flash_kernel_traitsILi256ELi64ELi64ELi8ES2_EELb0ELb1ELb1ELb1EEv16Flash_bwd_params
    0 bytes stack frame, 0 bytes spill stores, 0 bytes spill loads
ptxas info    : Used 255 registers, 696 bytes cmem[0]
ptxas info    : Compiling entry function '_Z44flash_bwd_dq_dk_dv_loop_seqk_parallel_kernelI23Flash_bwd_kernel_traitsILi256ELi64ELi64ELi8ELi4ELi2ELi2ELb0ELb0EN7cutlass6half_tE19Flash_kernel_traitsILi256ELi64ELi64ELi8ES2_EELb0ELb0ELb0ELb0EEv16Flash_bwd_params' for 'sm_80'
ptxas info    : Function properties for _Z44flash_bwd_dq_dk_dv_loop_seqk_parallel_kernelI23Flash_bwd_kernel_traitsILi256ELi64ELi64ELi8ELi4ELi2ELi2ELb0ELb0EN7cutlass6half_tE19Flash_kernel_traitsILi256ELi64ELi64ELi8ES2_EELb0ELb0ELb0ELb0EEv16Flash_bwd_params
    8 bytes stack frame, 8 bytes spill stores, 8 bytes spill loads
ptxas info    : Used 255 registers, 696 bytes cmem[0]
ptxas info    : Compiling entry function '_Z44flash_bwd_dq_dk_dv_loop_seqk_parallel_kernelI23Flash_bwd_kernel_traitsILi256ELi64ELi64ELi8ELi4ELi2ELi2ELb0ELb0EN7cutlass6half_tE19Flash_kernel_traitsILi256ELi64ELi64ELi8ES2_EELb0ELb0ELb0ELb1EEv16Flash_bwd_params' for 'sm_80'
ptxas info    : Function properties for _Z44flash_bwd_dq_dk_dv_loop_seqk_parallel_kernelI23Flash_bwd_kernel_traitsILi256ELi64ELi64ELi8ELi4ELi2ELi2ELb0ELb0EN7cutlass6half_tE19Flash_kernel_traitsILi256ELi64ELi64ELi8ES2_EELb0ELb0ELb0ELb1EEv16Flash_bwd_params
    16 bytes stack frame, 12 bytes spill stores, 12 bytes spill loads
ptxas info    : Used 255 registers, 696 bytes cmem[0]
ptxas info    : Compiling entry function '_Z44flash_bwd_dq_dk_dv_loop_seqk_parallel_kernelI23Flash_bwd_kernel_traitsILi256ELi64ELi64ELi8ELi4ELi2ELi2ELb0ELb0EN7cutlass6half_tE19Flash_kernel_traitsILi256ELi64ELi64ELi8ES2_EELb0ELb0ELb1ELb0EEv16Flash_bwd_params' for 'sm_80'
ptxas info    : Function properties for _Z44flash_bwd_dq_dk_dv_loop_seqk_parallel_kernelI23Flash_bwd_kernel_traitsILi256ELi64ELi64ELi8ELi4ELi2ELi2ELb0ELb0EN7cutlass6half_tE19Flash_kernel_traitsILi256ELi64ELi64ELi8ES2_EELb0ELb0ELb1ELb0EEv16Flash_bwd_params
    0 bytes stack frame, 0 bytes spill stores, 0 bytes spill loads
ptxas info    : Used 254 registers, 696 bytes cmem[0]
ptxas info    : Compiling entry function '_Z44flash_bwd_dq_dk_dv_loop_seqk_parallel_kernelI23Flash_bwd_kernel_traitsILi256ELi64ELi64ELi8ELi4ELi2ELi2ELb0ELb0EN7cutlass6half_tE19Flash_kernel_traitsILi256ELi64ELi64ELi8ES2_EELb0ELb0ELb1ELb1EEv16Flash_bwd_params' for 'sm_80'
ptxas info    : Function properties for _Z44flash_bwd_dq_dk_dv_loop_seqk_parallel_kernelI23Flash_bwd_kernel_traitsILi256ELi64ELi64ELi8ELi4ELi2ELi2ELb0ELb0EN7cutlass6half_tE19Flash_kernel_traitsILi256ELi64ELi64ELi8ES2_EELb0ELb0ELb1ELb1EEv16Flash_bwd_params
    0 bytes stack frame, 0 bytes spill stores, 0 bytes spill loads
ptxas info    : Used 252 registers, 696 bytes cmem[0]
ptxas info    : Compiling entry function '_Z44flash_bwd_dq_dk_dv_loop_seqk_parallel_kernelI23Flash_bwd_kernel_traitsILi256ELi64ELi64ELi8ELi4ELi2ELi2ELb0ELb0EN7cutlass6half_tE19Flash_kernel_traitsILi256ELi64ELi64ELi8ES2_EELb0ELb1ELb0ELb0EEv16Flash_bwd_params' for 'sm_80'
ptxas info    : Function properties for _Z44flash_bwd_dq_dk_dv_loop_seqk_parallel_kernelI23Flash_bwd_kernel_traitsILi256ELi64ELi64ELi8ELi4ELi2ELi2ELb0ELb0EN7cutlass6half_tE19Flash_kernel_traitsILi256ELi64ELi64ELi8ES2_EELb0ELb1ELb0ELb0EEv16Flash_bwd_params
    8 bytes stack frame, 8 bytes spill stores, 8 bytes spill loads
ptxas info    : Used 255 registers, 696 bytes cmem[0]
ptxas info    : Compiling entry function '_Z44flash_bwd_dq_dk_dv_loop_seqk_parallel_kernelI23Flash_bwd_kernel_traitsILi256ELi64ELi64ELi8ELi4ELi2ELi2ELb0ELb0EN7cutlass6half_tE19Flash_kernel_traitsILi256ELi64ELi64ELi8ES2_EELb0ELb1ELb0ELb1EEv16Flash_bwd_params' for 'sm_80'
ptxas info    : Function properties for _Z44flash_bwd_dq_dk_dv_loop_seqk_parallel_kernelI23Flash_bwd_kernel_traitsILi256ELi64ELi64ELi8ELi4ELi2ELi2ELb0ELb0EN7cutlass6half_tE19Flash_kernel_traitsILi256ELi64ELi64ELi8ES2_EELb0ELb1ELb0ELb1EEv16Flash_bwd_params
    16 bytes stack frame, 12 bytes spill stores, 12 bytes spill loads
ptxas info    : Used 255 registers, 696 bytes cmem[0]
ptxas info    : Compiling entry function '_Z44flash_bwd_dq_dk_dv_loop_seqk_parallel_kernelI23Flash_bwd_kernel_traitsILi256ELi64ELi64ELi8ELi4ELi2ELi2ELb0ELb0EN7cutlass6half_tE19Flash_kernel_traitsILi256ELi64ELi64ELi8ES2_EELb0ELb1ELb1ELb0EEv16Flash_bwd_params' for 'sm_80'
ptxas info    : Function properties for _Z44flash_bwd_dq_dk_dv_loop_seqk_parallel_kernelI23Flash_bwd_kernel_traitsILi256ELi64ELi64ELi8ELi4ELi2ELi2ELb0ELb0EN7cutlass6half_tE19Flash_kernel_traitsILi256ELi64ELi64ELi8ES2_EELb0ELb1ELb1ELb0EEv16Flash_bwd_params
    0 bytes stack frame, 0 bytes spill stores, 0 bytes spill loads
ptxas info    : Used 254 registers, 696 bytes cmem[0]
ptxas info    : Compiling entry function '_Z44flash_bwd_dq_dk_dv_loop_seqk_parallel_kernelI23Flash_bwd_kernel_traitsILi256ELi64ELi64ELi8ELi4ELi2ELi2ELb0ELb0EN7cutlass6half_tE19Flash_kernel_traitsILi256ELi64ELi64ELi8ES2_EELb0ELb1ELb1ELb1EEv16Flash_bwd_params' for 'sm_80'
ptxas info    : Function properties for _Z44flash_bwd_dq_dk_dv_loop_seqk_parallel_kernelI23Flash_bwd_kernel_traitsILi256ELi64ELi64ELi8ELi4ELi2ELi2ELb0ELb0EN7cutlass6half_tE19Flash_kernel_traitsILi256ELi64ELi64ELi8ES2_EELb0ELb1ELb1ELb1EEv16Flash_bwd_params
    0 bytes stack frame, 0 bytes spill stores, 0 bytes spill loads
ptxas info    : Used 255 registers, 696 bytes cmem[0]
ptxas info    : Compiling entry function '_Z27flash_bwd_convert_dq_kernelI23Flash_bwd_kernel_traitsILi256ELi64ELi64ELi8ELi4ELi2ELi2ELb0ELb1EN7cutlass6half_tE19Flash_kernel_traitsILi256ELi64ELi64ELi8ES2_EEEv16Flash_bwd_params' for 'sm_80'
ptxas info    : Function properties for _Z27flash_bwd_convert_dq_kernelI23Flash_bwd_kernel_traitsILi256ELi64ELi64ELi8ELi4ELi2ELi2ELb0ELb1EN7cutlass6half_tE19Flash_kernel_traitsILi256ELi64ELi64ELi8ES2_EEEv16Flash_bwd_params
    0 bytes stack frame, 0 bytes spill stores, 0 bytes spill loads
ptxas info    : Used 40 registers, 696 bytes cmem[0]
ptxas info    : Compiling entry function '_Z44flash_bwd_dq_dk_dv_loop_seqk_parallel_kernelI23Flash_bwd_kernel_traitsILi256ELi64ELi64ELi8ELi4ELi2ELi2ELb0ELb1EN7cutlass6half_tE19Flash_kernel_traitsILi256ELi64ELi64ELi8ES2_EELb1ELb0ELb0ELb0EEv16Flash_bwd_params' for 'sm_80'
ptxas info    : Function properties for _Z44flash_bwd_dq_dk_dv_loop_seqk_parallel_kernelI23Flash_bwd_kernel_traitsILi256ELi64ELi64ELi8ELi4ELi2ELi2ELb0ELb1EN7cutlass6half_tE19Flash_kernel_traitsILi256ELi64ELi64ELi8ES2_EELb1ELb0ELb0ELb0EEv16Flash_bwd_params
    16 bytes stack frame, 12 bytes spill stores, 12 bytes spill loads
ptxas info    : Used 255 registers, 696 bytes cmem[0]
ptxas info    : Compiling entry function '_Z44flash_bwd_dq_dk_dv_loop_seqk_parallel_kernelI23Flash_bwd_kernel_traitsILi256ELi64ELi64ELi8ELi4ELi2ELi2ELb0ELb1EN7cutlass6half_tE19Flash_kernel_traitsILi256ELi64ELi64ELi8ES2_EELb1ELb0ELb0ELb1EEv16Flash_bwd_params' for 'sm_80'
ptxas info    : Function properties for _Z44flash_bwd_dq_dk_dv_loop_seqk_parallel_kernelI23Flash_bwd_kernel_traitsILi256ELi64ELi64ELi8ELi4ELi2ELi2ELb0ELb1EN7cutlass6half_tE19Flash_kernel_traitsILi256ELi64ELi64ELi8ES2_EELb1ELb0ELb0ELb1EEv16Flash_bwd_params
    8 bytes stack frame, 4 bytes spill stores, 4 bytes spill loads
ptxas info    : Used 255 registers, 696 bytes cmem[0]
ptxas info    : Compiling entry function '_Z44flash_bwd_dq_dk_dv_loop_seqk_parallel_kernelI23Flash_bwd_kernel_traitsILi256ELi64ELi64ELi8ELi4ELi2ELi2ELb0ELb1EN7cutlass6half_tE19Flash_kernel_traitsILi256ELi64ELi64ELi8ES2_EELb1ELb0ELb1ELb0EEv16Flash_bwd_params' for 'sm_80'
ptxas info    : Function properties for _Z44flash_bwd_dq_dk_dv_loop_seqk_parallel_kernelI23Flash_bwd_kernel_traitsILi256ELi64ELi64ELi8ELi4ELi2ELi2ELb0ELb1EN7cutlass6half_tE19Flash_kernel_traitsILi256ELi64ELi64ELi8ES2_EELb1ELb0ELb1ELb0EEv16Flash_bwd_params
    24 bytes stack frame, 24 bytes spill stores, 24 bytes spill loads
ptxas info    : Used 255 registers, 696 bytes cmem[0]
ptxas info    : Compiling entry function '_Z44flash_bwd_dq_dk_dv_loop_seqk_parallel_kernelI23Flash_bwd_kernel_traitsILi256ELi64ELi64ELi8ELi4ELi2ELi2ELb0ELb1EN7cutlass6half_tE19Flash_kernel_traitsILi256ELi64ELi64ELi8ES2_EELb1ELb0ELb1ELb1EEv16Flash_bwd_params' for 'sm_80'
ptxas info    : Function properties for _Z44flash_bwd_dq_dk_dv_loop_seqk_parallel_kernelI23Flash_bwd_kernel_traitsILi256ELi64ELi64ELi8ELi4ELi2ELi2ELb0ELb1EN7cutlass6half_tE19Flash_kernel_traitsILi256ELi64ELi64ELi8ES2_EELb1ELb0ELb1ELb1EEv16Flash_bwd_params
    24 bytes stack frame, 20 bytes spill stores, 20 bytes spill loads
ptxas info    : Used 255 registers, 696 bytes cmem[0]
ptxas info    : Compiling entry function '_Z44flash_bwd_dq_dk_dv_loop_seqk_parallel_kernelI23Flash_bwd_kernel_traitsILi256ELi64ELi64ELi8ELi4ELi2ELi2ELb0ELb1EN7cutlass6half_tE19Flash_kernel_traitsILi256ELi64ELi64ELi8ES2_EELb1ELb1ELb0ELb0EEv16Flash_bwd_params' for 'sm_80'
ptxas info    : Function properties for _Z44flash_bwd_dq_dk_dv_loop_seqk_parallel_kernelI23Flash_bwd_kernel_traitsILi256ELi64ELi64ELi8ELi4ELi2ELi2ELb0ELb1EN7cutlass6half_tE19Flash_kernel_traitsILi256ELi64ELi64ELi8ES2_EELb1ELb1ELb0ELb0EEv16Flash_bwd_params
    48 bytes stack frame, 44 bytes spill stores, 44 bytes spill loads
ptxas info    : Used 255 registers, 696 bytes cmem[0]
ptxas info    : Compiling entry function '_Z44flash_bwd_dq_dk_dv_loop_seqk_parallel_kernelI23Flash_bwd_kernel_traitsILi256ELi64ELi64ELi8ELi4ELi2ELi2ELb0ELb1EN7cutlass6half_tE19Flash_kernel_traitsILi256ELi64ELi64ELi8ES2_EELb1ELb1ELb0ELb1EEv16Flash_bwd_params' for 'sm_80'
ptxas info    : Function properties for _Z44flash_bwd_dq_dk_dv_loop_seqk_parallel_kernelI23Flash_bwd_kernel_traitsILi256ELi64ELi64ELi8ELi4ELi2ELi2ELb0ELb1EN7cutlass6half_tE19Flash_kernel_traitsILi256ELi64ELi64ELi8ES2_EELb1ELb1ELb0ELb1EEv16Flash_bwd_params
    8 bytes stack frame, 8 bytes spill stores, 8 bytes spill loads
ptxas info    : Used 255 registers, 696 bytes cmem[0]
ptxas info    : Compiling entry function '_Z44flash_bwd_dq_dk_dv_loop_seqk_parallel_kernelI23Flash_bwd_kernel_traitsILi256ELi64ELi64ELi8ELi4ELi2ELi2ELb0ELb1EN7cutlass6half_tE19Flash_kernel_traitsILi256ELi64ELi64ELi8ES2_EELb1ELb1ELb1ELb0EEv16Flash_bwd_params' for 'sm_80'
ptxas info    : Function properties for _Z44flash_bwd_dq_dk_dv_loop_seqk_parallel_kernelI23Flash_bwd_kernel_traitsILi256ELi64ELi64ELi8ELi4ELi2ELi2ELb0ELb1EN7cutlass6half_tE19Flash_kernel_traitsILi256ELi64ELi64ELi8ES2_EELb1ELb1ELb1ELb0EEv16Flash_bwd_params
    8 bytes stack frame, 4 bytes spill stores, 4 bytes spill loads
ptxas info    : Used 255 registers, 696 bytes cmem[0]
ptxas info    : Compiling entry function '_Z44flash_bwd_dq_dk_dv_loop_seqk_parallel_kernelI23Flash_bwd_kernel_traitsILi256ELi64ELi64ELi8ELi4ELi2ELi2ELb0ELb1EN7cutlass6half_tE19Flash_kernel_traitsILi256ELi64ELi64ELi8ES2_EELb1ELb1ELb1ELb1EEv16Flash_bwd_params' for 'sm_80'
ptxas info    : Function properties for _Z44flash_bwd_dq_dk_dv_loop_seqk_parallel_kernelI23Flash_bwd_kernel_traitsILi256ELi64ELi64ELi8ELi4ELi2ELi2ELb0ELb1EN7cutlass6half_tE19Flash_kernel_traitsILi256ELi64ELi64ELi8ES2_EELb1ELb1ELb1ELb1EEv16Flash_bwd_params
    8 bytes stack frame, 4 bytes spill stores, 4 bytes spill loads
ptxas info    : Used 255 registers, 696 bytes cmem[0]
ptxas info    : Compiling entry function '_Z25flash_bwd_dot_do_o_kernelILb1E23Flash_bwd_kernel_traitsILi256ELi64ELi64ELi8ELi4ELi2ELi2ELb0ELb1EN7cutlass6half_tE19Flash_kernel_traitsILi256ELi64ELi64ELi8ES2_EEEv16Flash_bwd_params' for 'sm_80'
ptxas info    : Function properties for _Z25flash_bwd_dot_do_o_kernelILb1E23Flash_bwd_kernel_traitsILi256ELi64ELi64ELi8ELi4ELi2ELi2ELb0ELb1EN7cutlass6half_tE19Flash_kernel_traitsILi256ELi64ELi64ELi8ES2_EEEv16Flash_bwd_params
    0 bytes stack frame, 0 bytes spill stores, 0 bytes spill loads
ptxas info    : Used 92 registers, 696 bytes cmem[0]
ptxas info    : Compiling entry function '_Z27flash_bwd_convert_dq_kernelI23Flash_bwd_kernel_traitsILi256ELi64ELi64ELi8ELi4ELi2ELi2ELb0ELb0EN7cutlass6half_tE19Flash_kernel_traitsILi256ELi64ELi64ELi8ES2_EEEv16Flash_bwd_params' for 'sm_80'
ptxas info    : Function properties for _Z27flash_bwd_convert_dq_kernelI23Flash_bwd_kernel_traitsILi256ELi64ELi64ELi8ELi4ELi2ELi2ELb0ELb0EN7cutlass6half_tE19Flash_kernel_traitsILi256ELi64ELi64ELi8ES2_EEEv16Flash_bwd_params
    0 bytes stack frame, 0 bytes spill stores, 0 bytes spill loads
ptxas info    : Used 40 registers, 696 bytes cmem[0]
ptxas info    : Compiling entry function '_Z44flash_bwd_dq_dk_dv_loop_seqk_parallel_kernelI23Flash_bwd_kernel_traitsILi256ELi64ELi64ELi8ELi4ELi2ELi2ELb0ELb0EN7cutlass6half_tE19Flash_kernel_traitsILi256ELi64ELi64ELi8ES2_EELb1ELb0ELb0ELb0EEv16Flash_bwd_params' for 'sm_80'
ptxas info    : Function properties for _Z44flash_bwd_dq_dk_dv_loop_seqk_parallel_kernelI23Flash_bwd_kernel_traitsILi256ELi64ELi64ELi8ELi4ELi2ELi2ELb0ELb0EN7cutlass6half_tE19Flash_kernel_traitsILi256ELi64ELi64ELi8ES2_EELb1ELb0ELb0ELb0EEv16Flash_bwd_params
    24 bytes stack frame, 20 bytes spill stores, 20 bytes spill loads
ptxas info    : Used 255 registers, 696 bytes cmem[0]
ptxas info    : Compiling entry function '_Z44flash_bwd_dq_dk_dv_loop_seqk_parallel_kernelI23Flash_bwd_kernel_traitsILi256ELi64ELi64ELi8ELi4ELi2ELi2ELb0ELb0EN7cutlass6half_tE19Flash_kernel_traitsILi256ELi64ELi64ELi8ES2_EELb1ELb0ELb0ELb1EEv16Flash_bwd_params' for 'sm_80'
ptxas info    : Function properties for _Z44flash_bwd_dq_dk_dv_loop_seqk_parallel_kernelI23Flash_bwd_kernel_traitsILi256ELi64ELi64ELi8ELi4ELi2ELi2ELb0ELb0EN7cutlass6half_tE19Flash_kernel_traitsILi256ELi64ELi64ELi8ES2_EELb1ELb0ELb0ELb1EEv16Flash_bwd_params
    8 bytes stack frame, 8 bytes spill stores, 8 bytes spill loads
ptxas info    : Used 255 registers, 696 bytes cmem[0]
ptxas info    : Compiling entry function '_Z44flash_bwd_dq_dk_dv_loop_seqk_parallel_kernelI23Flash_bwd_kernel_traitsILi256ELi64ELi64ELi8ELi4ELi2ELi2ELb0ELb0EN7cutlass6half_tE19Flash_kernel_traitsILi256ELi64ELi64ELi8ES2_EELb1ELb0ELb1ELb0EEv16Flash_bwd_params' for 'sm_80'
ptxas info    : Function properties for _Z44flash_bwd_dq_dk_dv_loop_seqk_parallel_kernelI23Flash_bwd_kernel_traitsILi256ELi64ELi64ELi8ELi4ELi2ELi2ELb0ELb0EN7cutlass6half_tE19Flash_kernel_traitsILi256ELi64ELi64ELi8ES2_EELb1ELb0ELb1ELb0EEv16Flash_bwd_params
    24 bytes stack frame, 20 bytes spill stores, 20 bytes spill loads
ptxas info    : Used 255 registers, 696 bytes cmem[0]
ptxas info    : Compiling entry function '_Z44flash_bwd_dq_dk_dv_loop_seqk_parallel_kernelI23Flash_bwd_kernel_traitsILi256ELi64ELi64ELi8ELi4ELi2ELi2ELb0ELb0EN7cutlass6half_tE19Flash_kernel_traitsILi256ELi64ELi64ELi8ES2_EELb1ELb0ELb1ELb1EEv16Flash_bwd_params' for 'sm_80'
ptxas info    : Function properties for _Z44flash_bwd_dq_dk_dv_loop_seqk_parallel_kernelI23Flash_bwd_kernel_traitsILi256ELi64ELi64ELi8ELi4ELi2ELi2ELb0ELb0EN7cutlass6half_tE19Flash_kernel_traitsILi256ELi64ELi64ELi8ES2_EELb1ELb0ELb1ELb1EEv16Flash_bwd_params
    0 bytes stack frame, 0 bytes spill stores, 0 bytes spill loads
ptxas info    : Used 255 registers, 696 bytes cmem[0]
ptxas info    : Compiling entry function '_Z44flash_bwd_dq_dk_dv_loop_seqk_parallel_kernelI23Flash_bwd_kernel_traitsILi256ELi64ELi64ELi8ELi4ELi2ELi2ELb0ELb0EN7cutlass6half_tE19Flash_kernel_traitsILi256ELi64ELi64ELi8ES2_EELb1ELb1ELb0ELb0EEv16Flash_bwd_params' for 'sm_80'
ptxas info    : Function properties for _Z44flash_bwd_dq_dk_dv_loop_seqk_parallel_kernelI23Flash_bwd_kernel_traitsILi256ELi64ELi64ELi8ELi4ELi2ELi2ELb0ELb0EN7cutlass6half_tE19Flash_kernel_traitsILi256ELi64ELi64ELi8ES2_EELb1ELb1ELb0ELb0EEv16Flash_bwd_params
    32 bytes stack frame, 28 bytes spill stores, 28 bytes spill loads
ptxas info    : Used 255 registers, 696 bytes cmem[0]
ptxas info    : Compiling entry function '_Z44flash_bwd_dq_dk_dv_loop_seqk_parallel_kernelI23Flash_bwd_kernel_traitsILi256ELi64ELi64ELi8ELi4ELi2ELi2ELb0ELb0EN7cutlass6half_tE19Flash_kernel_traitsILi256ELi64ELi64ELi8ES2_EELb1ELb1ELb0ELb1EEv16Flash_bwd_params' for 'sm_80'
ptxas info    : Function properties for _Z44flash_bwd_dq_dk_dv_loop_seqk_parallel_kernelI23Flash_bwd_kernel_traitsILi256ELi64ELi64ELi8ELi4ELi2ELi2ELb0ELb0EN7cutlass6half_tE19Flash_kernel_traitsILi256ELi64ELi64ELi8ES2_EELb1ELb1ELb0ELb1EEv16Flash_bwd_params
    8 bytes stack frame, 8 bytes spill stores, 8 bytes spill loads
ptxas info    : Used 255 registers, 696 bytes cmem[0]
ptxas info    : Compiling entry function '_Z44flash_bwd_dq_dk_dv_loop_seqk_parallel_kernelI23Flash_bwd_kernel_traitsILi256ELi64ELi64ELi8ELi4ELi2ELi2ELb0ELb0EN7cutlass6half_tE19Flash_kernel_traitsILi256ELi64ELi64ELi8ES2_EELb1ELb1ELb1ELb0EEv16Flash_bwd_params' for 'sm_80'
ptxas info    : Function properties for _Z44flash_bwd_dq_dk_dv_loop_seqk_parallel_kernelI23Flash_bwd_kernel_traitsILi256ELi64ELi64ELi8ELi4ELi2ELi2ELb0ELb0EN7cutlass6half_tE19Flash_kernel_traitsILi256ELi64ELi64ELi8ES2_EELb1ELb1ELb1ELb0EEv16Flash_bwd_params
    8 bytes stack frame, 8 bytes spill stores, 8 bytes spill loads
ptxas info    : Used 255 registers, 696 bytes cmem[0]
ptxas info    : Compiling entry function '_Z44flash_bwd_dq_dk_dv_loop_seqk_parallel_kernelI23Flash_bwd_kernel_traitsILi256ELi64ELi64ELi8ELi4ELi2ELi2ELb0ELb0EN7cutlass6half_tE19Flash_kernel_traitsILi256ELi64ELi64ELi8ES2_EELb1ELb1ELb1ELb1EEv16Flash_bwd_params' for 'sm_80'
ptxas info    : Function properties for _Z44flash_bwd_dq_dk_dv_loop_seqk_parallel_kernelI23Flash_bwd_kernel_traitsILi256ELi64ELi64ELi8ELi4ELi2ELi2ELb0ELb0EN7cutlass6half_tE19Flash_kernel_traitsILi256ELi64ELi64ELi8ES2_EELb1ELb1ELb1ELb1EEv16Flash_bwd_params
    8 bytes stack frame, 8 bytes spill stores, 8 bytes spill loads
ptxas info    : Used 255 registers, 696 bytes cmem[0]
ptxas info    : Compiling entry function '_Z25flash_bwd_dot_do_o_kernelILb1E23Flash_bwd_kernel_traitsILi256ELi64ELi64ELi8ELi4ELi2ELi2ELb0ELb0EN7cutlass6half_tE19Flash_kernel_traitsILi256ELi64ELi64ELi8ES2_EEEv16Flash_bwd_params' for 'sm_80'
ptxas info    : Function properties for _Z25flash_bwd_dot_do_o_kernelILb1E23Flash_bwd_kernel_traitsILi256ELi64ELi64ELi8ELi4ELi2ELi2ELb0ELb0EN7cutlass6half_tE19Flash_kernel_traitsILi256ELi64ELi64ELi8ES2_EEEv16Flash_bwd_params
    0 bytes stack frame, 0 bytes spill stores, 0 bytes spill loads
ptxas info    : Used 92 registers, 696 bytes cmem[0]
/anaconda/envs/flashattention/lib/python3.9/site-packages/torch/include/c10/core/SymInt.h(84): warning: integer conversion resulted in a change of sign

[17/37] /usr/local/cuda-11.4/bin/nvcc  -I/home/v-feiychen/flash-attention/csrc/flash_attn -I/home/v-feiychen/flash-attention/csrc/flash_attn/src -I/home/v-feiychen/flash-attention/csrc/cutlass/include -I/anaconda/envs/flashattention/lib/python3.9/site-packages/torch/include -I/anaconda/envs/flashattention/lib/python3.9/site-packages/torch/include/torch/csrc/api/include -I/anaconda/envs/flashattention/lib/python3.9/site-packages/torch/include/TH -I/anaconda/envs/flashattention/lib/python3.9/site-packages/torch/include/THC -I/usr/local/cuda-11.4/include -I/anaconda/envs/flashattention/include/python3.9 -c -c /home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_bwd_hdim256_bf16_sm80.cu -o /home/v-feiychen/flash-attention/build/temp.linux-x86_64-cpython-39/csrc/flash_attn/src/flash_bwd_hdim256_bf16_sm80.o -D__CUDA_NO_HALF_OPERATORS__ -D__CUDA_NO_HALF_CONVERSIONS__ -D__CUDA_NO_BFLOAT16_CONVERSIONS__ -D__CUDA_NO_HALF2_OPERATORS__ --expt-relaxed-constexpr --compiler-options ''"'"'-fPIC'"'"'' -O3 -std=c++17 -U__CUDA_NO_HALF_OPERATORS__ -U__CUDA_NO_HALF_CONVERSIONS__ -U__CUDA_NO_HALF2_OPERATORS__ -U__CUDA_NO_BFLOAT16_CONVERSIONS__ --expt-relaxed-constexpr --expt-extended-lambda --use_fast_math --ptxas-options=-v -lineinfo -gencode arch=compute_80,code=sm_80 --threads 4 -DTORCH_API_INCLUDE_EXTENSION_H '-DPYBIND11_COMPILER_TYPE="_gcc"' '-DPYBIND11_STDLIB="_libstdcpp"' '-DPYBIND11_BUILD_ABI="_cxxabi1011"' -DTORCH_EXTENSION_NAME=flash_attn_2_cuda -D_GLIBCXX_USE_CXX11_ABI=0
/anaconda/envs/flashattention/lib/python3.9/site-packages/torch/include/c10/core/SymInt.h(84): warning: integer conversion resulted in a change of sign

ptxas info    : 2 bytes gmem
ptxas info    : Compiling entry function '_Z44flash_bwd_dq_dk_dv_loop_seqk_parallel_kernelI23Flash_bwd_kernel_traitsILi256ELi64ELi64ELi8ELi4ELi2ELi2ELb0ELb1EN7cutlass10bfloat16_tE19Flash_kernel_traitsILi256ELi64ELi64ELi8ES2_EELb0ELb0ELb0ELb0EEv16Flash_bwd_params' for 'sm_80'
ptxas info    : Function properties for _Z44flash_bwd_dq_dk_dv_loop_seqk_parallel_kernelI23Flash_bwd_kernel_traitsILi256ELi64ELi64ELi8ELi4ELi2ELi2ELb0ELb1EN7cutlass10bfloat16_tE19Flash_kernel_traitsILi256ELi64ELi64ELi8ES2_EELb0ELb0ELb0ELb0EEv16Flash_bwd_params
    16 bytes stack frame, 12 bytes spill stores, 12 bytes spill loads
ptxas info    : Used 255 registers, 696 bytes cmem[0]
ptxas info    : Compiling entry function '_Z44flash_bwd_dq_dk_dv_loop_seqk_parallel_kernelI23Flash_bwd_kernel_traitsILi256ELi64ELi64ELi8ELi4ELi2ELi2ELb0ELb1EN7cutlass10bfloat16_tE19Flash_kernel_traitsILi256ELi64ELi64ELi8ES2_EELb0ELb0ELb0ELb1EEv16Flash_bwd_params' for 'sm_80'
ptxas info    : Function properties for _Z44flash_bwd_dq_dk_dv_loop_seqk_parallel_kernelI23Flash_bwd_kernel_traitsILi256ELi64ELi64ELi8ELi4ELi2ELi2ELb0ELb1EN7cutlass10bfloat16_tE19Flash_kernel_traitsILi256ELi64ELi64ELi8ES2_EELb0ELb0ELb0ELb1EEv16Flash_bwd_params
    0 bytes stack frame, 0 bytes spill stores, 0 bytes spill loads
ptxas info    : Used 252 registers, 696 bytes cmem[0]
ptxas info    : Compiling entry function '_Z44flash_bwd_dq_dk_dv_loop_seqk_parallel_kernelI23Flash_bwd_kernel_traitsILi256ELi64ELi64ELi8ELi4ELi2ELi2ELb0ELb1EN7cutlass10bfloat16_tE19Flash_kernel_traitsILi256ELi64ELi64ELi8ES2_EELb0ELb0ELb1ELb0EEv16Flash_bwd_params' for 'sm_80'
ptxas info    : Function properties for _Z44flash_bwd_dq_dk_dv_loop_seqk_parallel_kernelI23Flash_bwd_kernel_traitsILi256ELi64ELi64ELi8ELi4ELi2ELi2ELb0ELb1EN7cutlass10bfloat16_tE19Flash_kernel_traitsILi256ELi64ELi64ELi8ES2_EELb0ELb0ELb1ELb0EEv16Flash_bwd_params
    0 bytes stack frame, 0 bytes spill stores, 0 bytes spill loads
ptxas info    : Used 254 registers, 696 bytes cmem[0]
ptxas info    : Compiling entry function '_Z44flash_bwd_dq_dk_dv_loop_seqk_parallel_kernelI23Flash_bwd_kernel_traitsILi256ELi64ELi64ELi8ELi4ELi2ELi2ELb0ELb1EN7cutlass10bfloat16_tE19Flash_kernel_traitsILi256ELi64ELi64ELi8ES2_EELb0ELb0ELb1ELb1EEv16Flash_bwd_params' for 'sm_80'
ptxas info    : Function properties for _Z44flash_bwd_dq_dk_dv_loop_seqk_parallel_kernelI23Flash_bwd_kernel_traitsILi256ELi64ELi64ELi8ELi4ELi2ELi2ELb0ELb1EN7cutlass10bfloat16_tE19Flash_kernel_traitsILi256ELi64ELi64ELi8ES2_EELb0ELb0ELb1ELb1EEv16Flash_bwd_params
    8 bytes stack frame, 4 bytes spill stores, 4 bytes spill loads
ptxas info    : Used 255 registers, 696 bytes cmem[0]
ptxas info    : Compiling entry function '_Z44flash_bwd_dq_dk_dv_loop_seqk_parallel_kernelI23Flash_bwd_kernel_traitsILi256ELi64ELi64ELi8ELi4ELi2ELi2ELb0ELb1EN7cutlass10bfloat16_tE19Flash_kernel_traitsILi256ELi64ELi64ELi8ES2_EELb0ELb1ELb0ELb0EEv16Flash_bwd_params' for 'sm_80'
ptxas info    : Function properties for _Z44flash_bwd_dq_dk_dv_loop_seqk_parallel_kernelI23Flash_bwd_kernel_traitsILi256ELi64ELi64ELi8ELi4ELi2ELi2ELb0ELb1EN7cutlass10bfloat16_tE19Flash_kernel_traitsILi256ELi64ELi64ELi8ES2_EELb0ELb1ELb0ELb0EEv16Flash_bwd_params
    16 bytes stack frame, 12 bytes spill stores, 12 bytes spill loads
ptxas info    : Used 255 registers, 696 bytes cmem[0]
ptxas info    : Compiling entry function '_Z44flash_bwd_dq_dk_dv_loop_seqk_parallel_kernelI23Flash_bwd_kernel_traitsILi256ELi64ELi64ELi8ELi4ELi2ELi2ELb0ELb1EN7cutlass10bfloat16_tE19Flash_kernel_traitsILi256ELi64ELi64ELi8ES2_EELb0ELb1ELb0ELb1EEv16Flash_bwd_params' for 'sm_80'
ptxas info    : Function properties for _Z44flash_bwd_dq_dk_dv_loop_seqk_parallel_kernelI23Flash_bwd_kernel_traitsILi256ELi64ELi64ELi8ELi4ELi2ELi2ELb0ELb1EN7cutlass10bfloat16_tE19Flash_kernel_traitsILi256ELi64ELi64ELi8ES2_EELb0ELb1ELb0ELb1EEv16Flash_bwd_params
    8 bytes stack frame, 8 bytes spill stores, 8 bytes spill loads
ptxas info    : Used 255 registers, 696 bytes cmem[0]
ptxas info    : Compiling entry function '_Z44flash_bwd_dq_dk_dv_loop_seqk_parallel_kernelI23Flash_bwd_kernel_traitsILi256ELi64ELi64ELi8ELi4ELi2ELi2ELb0ELb1EN7cutlass10bfloat16_tE19Flash_kernel_traitsILi256ELi64ELi64ELi8ES2_EELb0ELb1ELb1ELb0EEv16Flash_bwd_params' for 'sm_80'
ptxas info    : Function properties for _Z44flash_bwd_dq_dk_dv_loop_seqk_parallel_kernelI23Flash_bwd_kernel_traitsILi256ELi64ELi64ELi8ELi4ELi2ELi2ELb0ELb1EN7cutlass10bfloat16_tE19Flash_kernel_traitsILi256ELi64ELi64ELi8ES2_EELb0ELb1ELb1ELb0EEv16Flash_bwd_params
    0 bytes stack frame, 0 bytes spill stores, 0 bytes spill loads
ptxas info    : Used 252 registers, 696 bytes cmem[0]
ptxas info    : Compiling entry function '_Z44flash_bwd_dq_dk_dv_loop_seqk_parallel_kernelI23Flash_bwd_kernel_traitsILi256ELi64ELi64ELi8ELi4ELi2ELi2ELb0ELb1EN7cutlass10bfloat16_tE19Flash_kernel_traitsILi256ELi64ELi64ELi8ES2_EELb0ELb1ELb1ELb1EEv16Flash_bwd_params' for 'sm_80'
ptxas info    : Function properties for _Z44flash_bwd_dq_dk_dv_loop_seqk_parallel_kernelI23Flash_bwd_kernel_traitsILi256ELi64ELi64ELi8ELi4ELi2ELi2ELb0ELb1EN7cutlass10bfloat16_tE19Flash_kernel_traitsILi256ELi64ELi64ELi8ES2_EELb0ELb1ELb1ELb1EEv16Flash_bwd_params
    0 bytes stack frame, 0 bytes spill stores, 0 bytes spill loads
ptxas info    : Used 255 registers, 696 bytes cmem[0]
ptxas info    : Compiling entry function '_Z44flash_bwd_dq_dk_dv_loop_seqk_parallel_kernelI23Flash_bwd_kernel_traitsILi256ELi64ELi64ELi8ELi4ELi2ELi2ELb0ELb0EN7cutlass10bfloat16_tE19Flash_kernel_traitsILi256ELi64ELi64ELi8ES2_EELb0ELb0ELb0ELb0EEv16Flash_bwd_params' for 'sm_80'
ptxas info    : Function properties for _Z44flash_bwd_dq_dk_dv_loop_seqk_parallel_kernelI23Flash_bwd_kernel_traitsILi256ELi64ELi64ELi8ELi4ELi2ELi2ELb0ELb0EN7cutlass10bfloat16_tE19Flash_kernel_traitsILi256ELi64ELi64ELi8ES2_EELb0ELb0ELb0ELb0EEv16Flash_bwd_params
    8 bytes stack frame, 8 bytes spill stores, 8 bytes spill loads
ptxas info    : Used 255 registers, 696 bytes cmem[0]
ptxas info    : Compiling entry function '_Z44flash_bwd_dq_dk_dv_loop_seqk_parallel_kernelI23Flash_bwd_kernel_traitsILi256ELi64ELi64ELi8ELi4ELi2ELi2ELb0ELb0EN7cutlass10bfloat16_tE19Flash_kernel_traitsILi256ELi64ELi64ELi8ES2_EELb0ELb0ELb0ELb1EEv16Flash_bwd_params' for 'sm_80'
ptxas info    : Function properties for _Z44flash_bwd_dq_dk_dv_loop_seqk_parallel_kernelI23Flash_bwd_kernel_traitsILi256ELi64ELi64ELi8ELi4ELi2ELi2ELb0ELb0EN7cutlass10bfloat16_tE19Flash_kernel_traitsILi256ELi64ELi64ELi8ES2_EELb0ELb0ELb0ELb1EEv16Flash_bwd_params
    16 bytes stack frame, 12 bytes spill stores, 12 bytes spill loads
ptxas info    : Used 255 registers, 696 bytes cmem[0]
ptxas info    : Compiling entry function '_Z44flash_bwd_dq_dk_dv_loop_seqk_parallel_kernelI23Flash_bwd_kernel_traitsILi256ELi64ELi64ELi8ELi4ELi2ELi2ELb0ELb0EN7cutlass10bfloat16_tE19Flash_kernel_traitsILi256ELi64ELi64ELi8ES2_EELb0ELb0ELb1ELb0EEv16Flash_bwd_params' for 'sm_80'
ptxas info    : Function properties for _Z44flash_bwd_dq_dk_dv_loop_seqk_parallel_kernelI23Flash_bwd_kernel_traitsILi256ELi64ELi64ELi8ELi4ELi2ELi2ELb0ELb0EN7cutlass10bfloat16_tE19Flash_kernel_traitsILi256ELi64ELi64ELi8ES2_EELb0ELb0ELb1ELb0EEv16Flash_bwd_params
    0 bytes stack frame, 0 bytes spill stores, 0 bytes spill loads
ptxas info    : Used 254 registers, 696 bytes cmem[0]
ptxas info    : Compiling entry function '_Z44flash_bwd_dq_dk_dv_loop_seqk_parallel_kernelI23Flash_bwd_kernel_traitsILi256ELi64ELi64ELi8ELi4ELi2ELi2ELb0ELb0EN7cutlass10bfloat16_tE19Flash_kernel_traitsILi256ELi64ELi64ELi8ES2_EELb0ELb0ELb1ELb1EEv16Flash_bwd_params' for 'sm_80'
ptxas info    : Function properties for _Z44flash_bwd_dq_dk_dv_loop_seqk_parallel_kernelI23Flash_bwd_kernel_traitsILi256ELi64ELi64ELi8ELi4ELi2ELi2ELb0ELb0EN7cutlass10bfloat16_tE19Flash_kernel_traitsILi256ELi64ELi64ELi8ES2_EELb0ELb0ELb1ELb1EEv16Flash_bwd_params
    0 bytes stack frame, 0 bytes spill stores, 0 bytes spill loads
ptxas info    : Used 252 registers, 696 bytes cmem[0]
ptxas info    : Compiling entry function '_Z44flash_bwd_dq_dk_dv_loop_seqk_parallel_kernelI23Flash_bwd_kernel_traitsILi256ELi64ELi64ELi8ELi4ELi2ELi2ELb0ELb0EN7cutlass10bfloat16_tE19Flash_kernel_traitsILi256ELi64ELi64ELi8ES2_EELb0ELb1ELb0ELb0EEv16Flash_bwd_params' for 'sm_80'
ptxas info    : Function properties for _Z44flash_bwd_dq_dk_dv_loop_seqk_parallel_kernelI23Flash_bwd_kernel_traitsILi256ELi64ELi64ELi8ELi4ELi2ELi2ELb0ELb0EN7cutlass10bfloat16_tE19Flash_kernel_traitsILi256ELi64ELi64ELi8ES2_EELb0ELb1ELb0ELb0EEv16Flash_bwd_params
    8 bytes stack frame, 8 bytes spill stores, 8 bytes spill loads
ptxas info    : Used 255 registers, 696 bytes cmem[0]
ptxas info    : Compiling entry function '_Z44flash_bwd_dq_dk_dv_loop_seqk_parallel_kernelI23Flash_bwd_kernel_traitsILi256ELi64ELi64ELi8ELi4ELi2ELi2ELb0ELb0EN7cutlass10bfloat16_tE19Flash_kernel_traitsILi256ELi64ELi64ELi8ES2_EELb0ELb1ELb0ELb1EEv16Flash_bwd_params' for 'sm_80'
ptxas info    : Function properties for _Z44flash_bwd_dq_dk_dv_loop_seqk_parallel_kernelI23Flash_bwd_kernel_traitsILi256ELi64ELi64ELi8ELi4ELi2ELi2ELb0ELb0EN7cutlass10bfloat16_tE19Flash_kernel_traitsILi256ELi64ELi64ELi8ES2_EELb0ELb1ELb0ELb1EEv16Flash_bwd_params
    16 bytes stack frame, 12 bytes spill stores, 12 bytes spill loads
ptxas info    : Used 255 registers, 696 bytes cmem[0]
ptxas info    : Compiling entry function '_Z44flash_bwd_dq_dk_dv_loop_seqk_parallel_kernelI23Flash_bwd_kernel_traitsILi256ELi64ELi64ELi8ELi4ELi2ELi2ELb0ELb0EN7cutlass10bfloat16_tE19Flash_kernel_traitsILi256ELi64ELi64ELi8ES2_EELb0ELb1ELb1ELb0EEv16Flash_bwd_params' for 'sm_80'
ptxas info    : Function properties for _Z44flash_bwd_dq_dk_dv_loop_seqk_parallel_kernelI23Flash_bwd_kernel_traitsILi256ELi64ELi64ELi8ELi4ELi2ELi2ELb0ELb0EN7cutlass10bfloat16_tE19Flash_kernel_traitsILi256ELi64ELi64ELi8ES2_EELb0ELb1ELb1ELb0EEv16Flash_bwd_params
    0 bytes stack frame, 0 bytes spill stores, 0 bytes spill loads
ptxas info    : Used 254 registers, 696 bytes cmem[0]
ptxas info    : Compiling entry function '_Z44flash_bwd_dq_dk_dv_loop_seqk_parallel_kernelI23Flash_bwd_kernel_traitsILi256ELi64ELi64ELi8ELi4ELi2ELi2ELb0ELb0EN7cutlass10bfloat16_tE19Flash_kernel_traitsILi256ELi64ELi64ELi8ES2_EELb0ELb1ELb1ELb1EEv16Flash_bwd_params' for 'sm_80'
ptxas info    : Function properties for _Z44flash_bwd_dq_dk_dv_loop_seqk_parallel_kernelI23Flash_bwd_kernel_traitsILi256ELi64ELi64ELi8ELi4ELi2ELi2ELb0ELb0EN7cutlass10bfloat16_tE19Flash_kernel_traitsILi256ELi64ELi64ELi8ES2_EELb0ELb1ELb1ELb1EEv16Flash_bwd_params
    0 bytes stack frame, 0 bytes spill stores, 0 bytes spill loads
ptxas info    : Used 255 registers, 696 bytes cmem[0]
ptxas info    : Compiling entry function '_Z27flash_bwd_convert_dq_kernelI23Flash_bwd_kernel_traitsILi256ELi64ELi64ELi8ELi4ELi2ELi2ELb0ELb1EN7cutlass10bfloat16_tE19Flash_kernel_traitsILi256ELi64ELi64ELi8ES2_EEEv16Flash_bwd_params' for 'sm_80'
ptxas info    : Function properties for _Z27flash_bwd_convert_dq_kernelI23Flash_bwd_kernel_traitsILi256ELi64ELi64ELi8ELi4ELi2ELi2ELb0ELb1EN7cutlass10bfloat16_tE19Flash_kernel_traitsILi256ELi64ELi64ELi8ES2_EEEv16Flash_bwd_params
    0 bytes stack frame, 0 bytes spill stores, 0 bytes spill loads
ptxas info    : Used 40 registers, 696 bytes cmem[0]
ptxas info    : Compiling entry function '_Z44flash_bwd_dq_dk_dv_loop_seqk_parallel_kernelI23Flash_bwd_kernel_traitsILi256ELi64ELi64ELi8ELi4ELi2ELi2ELb0ELb1EN7cutlass10bfloat16_tE19Flash_kernel_traitsILi256ELi64ELi64ELi8ES2_EELb1ELb0ELb0ELb0EEv16Flash_bwd_params' for 'sm_80'
ptxas info    : Function properties for _Z44flash_bwd_dq_dk_dv_loop_seqk_parallel_kernelI23Flash_bwd_kernel_traitsILi256ELi64ELi64ELi8ELi4ELi2ELi2ELb0ELb1EN7cutlass10bfloat16_tE19Flash_kernel_traitsILi256ELi64ELi64ELi8ES2_EELb1ELb0ELb0ELb0EEv16Flash_bwd_params
    16 bytes stack frame, 12 bytes spill stores, 12 bytes spill loads
ptxas info    : Used 255 registers, 696 bytes cmem[0]
ptxas info    : Compiling entry function '_Z44flash_bwd_dq_dk_dv_loop_seqk_parallel_kernelI23Flash_bwd_kernel_traitsILi256ELi64ELi64ELi8ELi4ELi2ELi2ELb0ELb1EN7cutlass10bfloat16_tE19Flash_kernel_traitsILi256ELi64ELi64ELi8ES2_EELb1ELb0ELb0ELb1EEv16Flash_bwd_params' for 'sm_80'
ptxas info    : Function properties for _Z44flash_bwd_dq_dk_dv_loop_seqk_parallel_kernelI23Flash_bwd_kernel_traitsILi256ELi64ELi64ELi8ELi4ELi2ELi2ELb0ELb1EN7cutlass10bfloat16_tE19Flash_kernel_traitsILi256ELi64ELi64ELi8ES2_EELb1ELb0ELb0ELb1EEv16Flash_bwd_params
    8 bytes stack frame, 4 bytes spill stores, 4 bytes spill loads
ptxas info    : Used 255 registers, 696 bytes cmem[0]
ptxas info    : Compiling entry function '_Z44flash_bwd_dq_dk_dv_loop_seqk_parallel_kernelI23Flash_bwd_kernel_traitsILi256ELi64ELi64ELi8ELi4ELi2ELi2ELb0ELb1EN7cutlass10bfloat16_tE19Flash_kernel_traitsILi256ELi64ELi64ELi8ES2_EELb1ELb0ELb1ELb0EEv16Flash_bwd_params' for 'sm_80'
ptxas info    : Function properties for _Z44flash_bwd_dq_dk_dv_loop_seqk_parallel_kernelI23Flash_bwd_kernel_traitsILi256ELi64ELi64ELi8ELi4ELi2ELi2ELb0ELb1EN7cutlass10bfloat16_tE19Flash_kernel_traitsILi256ELi64ELi64ELi8ES2_EELb1ELb0ELb1ELb0EEv16Flash_bwd_params
    24 bytes stack frame, 24 bytes spill stores, 24 bytes spill loads
ptxas info    : Used 255 registers, 696 bytes cmem[0]
ptxas info    : Compiling entry function '_Z44flash_bwd_dq_dk_dv_loop_seqk_parallel_kernelI23Flash_bwd_kernel_traitsILi256ELi64ELi64ELi8ELi4ELi2ELi2ELb0ELb1EN7cutlass10bfloat16_tE19Flash_kernel_traitsILi256ELi64ELi64ELi8ES2_EELb1ELb0ELb1ELb1EEv16Flash_bwd_params' for 'sm_80'
ptxas info    : Function properties for _Z44flash_bwd_dq_dk_dv_loop_seqk_parallel_kernelI23Flash_bwd_kernel_traitsILi256ELi64ELi64ELi8ELi4ELi2ELi2ELb0ELb1EN7cutlass10bfloat16_tE19Flash_kernel_traitsILi256ELi64ELi64ELi8ES2_EELb1ELb0ELb1ELb1EEv16Flash_bwd_params
    24 bytes stack frame, 20 bytes spill stores, 20 bytes spill loads
ptxas info    : Used 255 registers, 696 bytes cmem[0]
ptxas info    : Compiling entry function '_Z44flash_bwd_dq_dk_dv_loop_seqk_parallel_kernelI23Flash_bwd_kernel_traitsILi256ELi64ELi64ELi8ELi4ELi2ELi2ELb0ELb1EN7cutlass10bfloat16_tE19Flash_kernel_traitsILi256ELi64ELi64ELi8ES2_EELb1ELb1ELb0ELb0EEv16Flash_bwd_params' for 'sm_80'
ptxas info    : Function properties for _Z44flash_bwd_dq_dk_dv_loop_seqk_parallel_kernelI23Flash_bwd_kernel_traitsILi256ELi64ELi64ELi8ELi4ELi2ELi2ELb0ELb1EN7cutlass10bfloat16_tE19Flash_kernel_traitsILi256ELi64ELi64ELi8ES2_EELb1ELb1ELb0ELb0EEv16Flash_bwd_params
    48 bytes stack frame, 44 bytes spill stores, 44 bytes spill loads
ptxas info    : Used 255 registers, 696 bytes cmem[0]
ptxas info    : Compiling entry function '_Z44flash_bwd_dq_dk_dv_loop_seqk_parallel_kernelI23Flash_bwd_kernel_traitsILi256ELi64ELi64ELi8ELi4ELi2ELi2ELb0ELb1EN7cutlass10bfloat16_tE19Flash_kernel_traitsILi256ELi64ELi64ELi8ES2_EELb1ELb1ELb0ELb1EEv16Flash_bwd_params' for 'sm_80'
ptxas info    : Function properties for _Z44flash_bwd_dq_dk_dv_loop_seqk_parallel_kernelI23Flash_bwd_kernel_traitsILi256ELi64ELi64ELi8ELi4ELi2ELi2ELb0ELb1EN7cutlass10bfloat16_tE19Flash_kernel_traitsILi256ELi64ELi64ELi8ES2_EELb1ELb1ELb0ELb1EEv16Flash_bwd_params
    8 bytes stack frame, 8 bytes spill stores, 8 bytes spill loads
ptxas info    : Used 255 registers, 696 bytes cmem[0]
ptxas info    : Compiling entry function '_Z44flash_bwd_dq_dk_dv_loop_seqk_parallel_kernelI23Flash_bwd_kernel_traitsILi256ELi64ELi64ELi8ELi4ELi2ELi2ELb0ELb1EN7cutlass10bfloat16_tE19Flash_kernel_traitsILi256ELi64ELi64ELi8ES2_EELb1ELb1ELb1ELb0EEv16Flash_bwd_params' for 'sm_80'
ptxas info    : Function properties for _Z44flash_bwd_dq_dk_dv_loop_seqk_parallel_kernelI23Flash_bwd_kernel_traitsILi256ELi64ELi64ELi8ELi4ELi2ELi2ELb0ELb1EN7cutlass10bfloat16_tE19Flash_kernel_traitsILi256ELi64ELi64ELi8ES2_EELb1ELb1ELb1ELb0EEv16Flash_bwd_params
    8 bytes stack frame, 4 bytes spill stores, 4 bytes spill loads
ptxas info    : Used 255 registers, 696 bytes cmem[0]
ptxas info    : Compiling entry function '_Z44flash_bwd_dq_dk_dv_loop_seqk_parallel_kernelI23Flash_bwd_kernel_traitsILi256ELi64ELi64ELi8ELi4ELi2ELi2ELb0ELb1EN7cutlass10bfloat16_tE19Flash_kernel_traitsILi256ELi64ELi64ELi8ES2_EELb1ELb1ELb1ELb1EEv16Flash_bwd_params' for 'sm_80'
ptxas info    : Function properties for _Z44flash_bwd_dq_dk_dv_loop_seqk_parallel_kernelI23Flash_bwd_kernel_traitsILi256ELi64ELi64ELi8ELi4ELi2ELi2ELb0ELb1EN7cutlass10bfloat16_tE19Flash_kernel_traitsILi256ELi64ELi64ELi8ES2_EELb1ELb1ELb1ELb1EEv16Flash_bwd_params
    8 bytes stack frame, 4 bytes spill stores, 4 bytes spill loads
ptxas info    : Used 255 registers, 696 bytes cmem[0]
ptxas info    : Compiling entry function '_Z25flash_bwd_dot_do_o_kernelILb1E23Flash_bwd_kernel_traitsILi256ELi64ELi64ELi8ELi4ELi2ELi2ELb0ELb1EN7cutlass10bfloat16_tE19Flash_kernel_traitsILi256ELi64ELi64ELi8ES2_EEEv16Flash_bwd_params' for 'sm_80'
ptxas info    : Function properties for _Z25flash_bwd_dot_do_o_kernelILb1E23Flash_bwd_kernel_traitsILi256ELi64ELi64ELi8ELi4ELi2ELi2ELb0ELb1EN7cutlass10bfloat16_tE19Flash_kernel_traitsILi256ELi64ELi64ELi8ES2_EEEv16Flash_bwd_params
    0 bytes stack frame, 0 bytes spill stores, 0 bytes spill loads
ptxas info    : Used 92 registers, 696 bytes cmem[0]
ptxas info    : Compiling entry function '_Z27flash_bwd_convert_dq_kernelI23Flash_bwd_kernel_traitsILi256ELi64ELi64ELi8ELi4ELi2ELi2ELb0ELb0EN7cutlass10bfloat16_tE19Flash_kernel_traitsILi256ELi64ELi64ELi8ES2_EEEv16Flash_bwd_params' for 'sm_80'
ptxas info    : Function properties for _Z27flash_bwd_convert_dq_kernelI23Flash_bwd_kernel_traitsILi256ELi64ELi64ELi8ELi4ELi2ELi2ELb0ELb0EN7cutlass10bfloat16_tE19Flash_kernel_traitsILi256ELi64ELi64ELi8ES2_EEEv16Flash_bwd_params
    0 bytes stack frame, 0 bytes spill stores, 0 bytes spill loads
ptxas info    : Used 40 registers, 696 bytes cmem[0]
ptxas info    : Compiling entry function '_Z44flash_bwd_dq_dk_dv_loop_seqk_parallel_kernelI23Flash_bwd_kernel_traitsILi256ELi64ELi64ELi8ELi4ELi2ELi2ELb0ELb0EN7cutlass10bfloat16_tE19Flash_kernel_traitsILi256ELi64ELi64ELi8ES2_EELb1ELb0ELb0ELb0EEv16Flash_bwd_params' for 'sm_80'
ptxas info    : Function properties for _Z44flash_bwd_dq_dk_dv_loop_seqk_parallel_kernelI23Flash_bwd_kernel_traitsILi256ELi64ELi64ELi8ELi4ELi2ELi2ELb0ELb0EN7cutlass10bfloat16_tE19Flash_kernel_traitsILi256ELi64ELi64ELi8ES2_EELb1ELb0ELb0ELb0EEv16Flash_bwd_params
    24 bytes stack frame, 20 bytes spill stores, 20 bytes spill loads
ptxas info    : Used 255 registers, 696 bytes cmem[0]
ptxas info    : Compiling entry function '_Z44flash_bwd_dq_dk_dv_loop_seqk_parallel_kernelI23Flash_bwd_kernel_traitsILi256ELi64ELi64ELi8ELi4ELi2ELi2ELb0ELb0EN7cutlass10bfloat16_tE19Flash_kernel_traitsILi256ELi64ELi64ELi8ES2_EELb1ELb0ELb0ELb1EEv16Flash_bwd_params' for 'sm_80'
ptxas info    : Function properties for _Z44flash_bwd_dq_dk_dv_loop_seqk_parallel_kernelI23Flash_bwd_kernel_traitsILi256ELi64ELi64ELi8ELi4ELi2ELi2ELb0ELb0EN7cutlass10bfloat16_tE19Flash_kernel_traitsILi256ELi64ELi64ELi8ES2_EELb1ELb0ELb0ELb1EEv16Flash_bwd_params
    8 bytes stack frame, 8 bytes spill stores, 8 bytes spill loads
ptxas info    : Used 255 registers, 696 bytes cmem[0]
ptxas info    : Compiling entry function '_Z44flash_bwd_dq_dk_dv_loop_seqk_parallel_kernelI23Flash_bwd_kernel_traitsILi256ELi64ELi64ELi8ELi4ELi2ELi2ELb0ELb0EN7cutlass10bfloat16_tE19Flash_kernel_traitsILi256ELi64ELi64ELi8ES2_EELb1ELb0ELb1ELb0EEv16Flash_bwd_params' for 'sm_80'
ptxas info    : Function properties for _Z44flash_bwd_dq_dk_dv_loop_seqk_parallel_kernelI23Flash_bwd_kernel_traitsILi256ELi64ELi64ELi8ELi4ELi2ELi2ELb0ELb0EN7cutlass10bfloat16_tE19Flash_kernel_traitsILi256ELi64ELi64ELi8ES2_EELb1ELb0ELb1ELb0EEv16Flash_bwd_params
    24 bytes stack frame, 20 bytes spill stores, 20 bytes spill loads
ptxas info    : Used 255 registers, 696 bytes cmem[0]
ptxas info    : Compiling entry function '_Z44flash_bwd_dq_dk_dv_loop_seqk_parallel_kernelI23Flash_bwd_kernel_traitsILi256ELi64ELi64ELi8ELi4ELi2ELi2ELb0ELb0EN7cutlass10bfloat16_tE19Flash_kernel_traitsILi256ELi64ELi64ELi8ES2_EELb1ELb0ELb1ELb1EEv16Flash_bwd_params' for 'sm_80'
ptxas info    : Function properties for _Z44flash_bwd_dq_dk_dv_loop_seqk_parallel_kernelI23Flash_bwd_kernel_traitsILi256ELi64ELi64ELi8ELi4ELi2ELi2ELb0ELb0EN7cutlass10bfloat16_tE19Flash_kernel_traitsILi256ELi64ELi64ELi8ES2_EELb1ELb0ELb1ELb1EEv16Flash_bwd_params
    0 bytes stack frame, 0 bytes spill stores, 0 bytes spill loads
ptxas info    : Used 255 registers, 696 bytes cmem[0]
ptxas info    : Compiling entry function '_Z44flash_bwd_dq_dk_dv_loop_seqk_parallel_kernelI23Flash_bwd_kernel_traitsILi256ELi64ELi64ELi8ELi4ELi2ELi2ELb0ELb0EN7cutlass10bfloat16_tE19Flash_kernel_traitsILi256ELi64ELi64ELi8ES2_EELb1ELb1ELb0ELb0EEv16Flash_bwd_params' for 'sm_80'
ptxas info    : Function properties for _Z44flash_bwd_dq_dk_dv_loop_seqk_parallel_kernelI23Flash_bwd_kernel_traitsILi256ELi64ELi64ELi8ELi4ELi2ELi2ELb0ELb0EN7cutlass10bfloat16_tE19Flash_kernel_traitsILi256ELi64ELi64ELi8ES2_EELb1ELb1ELb0ELb0EEv16Flash_bwd_params
    32 bytes stack frame, 28 bytes spill stores, 28 bytes spill loads
ptxas info    : Used 255 registers, 696 bytes cmem[0]
ptxas info    : Compiling entry function '_Z44flash_bwd_dq_dk_dv_loop_seqk_parallel_kernelI23Flash_bwd_kernel_traitsILi256ELi64ELi64ELi8ELi4ELi2ELi2ELb0ELb0EN7cutlass10bfloat16_tE19Flash_kernel_traitsILi256ELi64ELi64ELi8ES2_EELb1ELb1ELb0ELb1EEv16Flash_bwd_params' for 'sm_80'
ptxas info    : Function properties for _Z44flash_bwd_dq_dk_dv_loop_seqk_parallel_kernelI23Flash_bwd_kernel_traitsILi256ELi64ELi64ELi8ELi4ELi2ELi2ELb0ELb0EN7cutlass10bfloat16_tE19Flash_kernel_traitsILi256ELi64ELi64ELi8ES2_EELb1ELb1ELb0ELb1EEv16Flash_bwd_params
    8 bytes stack frame, 8 bytes spill stores, 8 bytes spill loads
ptxas info    : Used 255 registers, 696 bytes cmem[0]
ptxas info    : Compiling entry function '_Z44flash_bwd_dq_dk_dv_loop_seqk_parallel_kernelI23Flash_bwd_kernel_traitsILi256ELi64ELi64ELi8ELi4ELi2ELi2ELb0ELb0EN7cutlass10bfloat16_tE19Flash_kernel_traitsILi256ELi64ELi64ELi8ES2_EELb1ELb1ELb1ELb0EEv16Flash_bwd_params' for 'sm_80'
ptxas info    : Function properties for _Z44flash_bwd_dq_dk_dv_loop_seqk_parallel_kernelI23Flash_bwd_kernel_traitsILi256ELi64ELi64ELi8ELi4ELi2ELi2ELb0ELb0EN7cutlass10bfloat16_tE19Flash_kernel_traitsILi256ELi64ELi64ELi8ES2_EELb1ELb1ELb1ELb0EEv16Flash_bwd_params
    8 bytes stack frame, 8 bytes spill stores, 8 bytes spill loads
ptxas info    : Used 255 registers, 696 bytes cmem[0]
ptxas info    : Compiling entry function '_Z44flash_bwd_dq_dk_dv_loop_seqk_parallel_kernelI23Flash_bwd_kernel_traitsILi256ELi64ELi64ELi8ELi4ELi2ELi2ELb0ELb0EN7cutlass10bfloat16_tE19Flash_kernel_traitsILi256ELi64ELi64ELi8ES2_EELb1ELb1ELb1ELb1EEv16Flash_bwd_params' for 'sm_80'
ptxas info    : Function properties for _Z44flash_bwd_dq_dk_dv_loop_seqk_parallel_kernelI23Flash_bwd_kernel_traitsILi256ELi64ELi64ELi8ELi4ELi2ELi2ELb0ELb0EN7cutlass10bfloat16_tE19Flash_kernel_traitsILi256ELi64ELi64ELi8ES2_EELb1ELb1ELb1ELb1EEv16Flash_bwd_params
    8 bytes stack frame, 8 bytes spill stores, 8 bytes spill loads
ptxas info    : Used 255 registers, 696 bytes cmem[0]
ptxas info    : Compiling entry function '_Z25flash_bwd_dot_do_o_kernelILb1E23Flash_bwd_kernel_traitsILi256ELi64ELi64ELi8ELi4ELi2ELi2ELb0ELb0EN7cutlass10bfloat16_tE19Flash_kernel_traitsILi256ELi64ELi64ELi8ES2_EEEv16Flash_bwd_params' for 'sm_80'
ptxas info    : Function properties for _Z25flash_bwd_dot_do_o_kernelILb1E23Flash_bwd_kernel_traitsILi256ELi64ELi64ELi8ELi4ELi2ELi2ELb0ELb0EN7cutlass10bfloat16_tE19Flash_kernel_traitsILi256ELi64ELi64ELi8ES2_EEEv16Flash_bwd_params
    0 bytes stack frame, 0 bytes spill stores, 0 bytes spill loads
ptxas info    : Used 92 registers, 696 bytes cmem[0]
/anaconda/envs/flashattention/lib/python3.9/site-packages/torch/include/c10/core/SymInt.h(84): warning: integer conversion resulted in a change of sign

[18/37] /usr/local/cuda-11.4/bin/nvcc  -I/home/v-feiychen/flash-attention/csrc/flash_attn -I/home/v-feiychen/flash-attention/csrc/flash_attn/src -I/home/v-feiychen/flash-attention/csrc/cutlass/include -I/anaconda/envs/flashattention/lib/python3.9/site-packages/torch/include -I/anaconda/envs/flashattention/lib/python3.9/site-packages/torch/include/torch/csrc/api/include -I/anaconda/envs/flashattention/lib/python3.9/site-packages/torch/include/TH -I/anaconda/envs/flashattention/lib/python3.9/site-packages/torch/include/THC -I/usr/local/cuda-11.4/include -I/anaconda/envs/flashattention/include/python3.9 -c -c /home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_hdim224_bf16_sm80.cu -o /home/v-feiychen/flash-attention/build/temp.linux-x86_64-cpython-39/csrc/flash_attn/src/flash_fwd_hdim224_bf16_sm80.o -D__CUDA_NO_HALF_OPERATORS__ -D__CUDA_NO_HALF_CONVERSIONS__ -D__CUDA_NO_BFLOAT16_CONVERSIONS__ -D__CUDA_NO_HALF2_OPERATORS__ --expt-relaxed-constexpr --compiler-options ''"'"'-fPIC'"'"'' -O3 -std=c++17 -U__CUDA_NO_HALF_OPERATORS__ -U__CUDA_NO_HALF_CONVERSIONS__ -U__CUDA_NO_HALF2_OPERATORS__ -U__CUDA_NO_BFLOAT16_CONVERSIONS__ --expt-relaxed-constexpr --expt-extended-lambda --use_fast_math --ptxas-options=-v -lineinfo -gencode arch=compute_80,code=sm_80 --threads 4 -DTORCH_API_INCLUDE_EXTENSION_H '-DPYBIND11_COMPILER_TYPE="_gcc"' '-DPYBIND11_STDLIB="_libstdcpp"' '-DPYBIND11_BUILD_ABI="_cxxabi1011"' -DTORCH_EXTENSION_NAME=flash_attn_2_cuda -D_GLIBCXX_USE_CXX11_ABI=0
/anaconda/envs/flashattention/lib/python3.9/site-packages/torch/include/c10/core/SymInt.h(84): warning: integer conversion resulted in a change of sign

/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_kernel.h(140): warning: variable "MMA_M" was declared but never referenced
          detected during:
            instantiation of "void flash::compute_attn_1rowblock<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params,Is_Q_split_K>(const Params &, int, int, int) [with Kernel_traits=Flash_fwd_kernel_traits<224, 128, 64, 8, false, false, cutlass::bfloat16_t, Flash_kernel_traits<224, 128, 64, 8, cutlass::bfloat16_t>>, Is_dropout=true, Is_causal=true, Is_even_N=true, Is_even_K=true, Return_softmax=true, Params=Flash_fwd_params, Is_Q_split_K=false]" 
(589): here
            instantiation of "void flash::compute_attn<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &) [with Kernel_traits=Flash_fwd_kernel_traits<224, 128, 64, 8, false, false, cutlass::bfloat16_t, Flash_kernel_traits<224, 128, 64, 8, cutlass::bfloat16_t>>, Is_dropout=true, Is_causal=true, Is_even_N=true, Is_even_K=true, Return_softmax=true, Params=Flash_fwd_params]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(15): here
            instantiation of "void flash_fwd_kernel<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax>(Flash_fwd_params) [with Kernel_traits=Flash_fwd_kernel_traits<224, 128, 64, 8, false, false, cutlass::bfloat16_t, Flash_kernel_traits<224, 128, 64, 8, cutlass::bfloat16_t>>, Is_dropout=true, Is_causal=true, Is_even_N=true, Is_even_K=true, Return_softmax=true]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(34): here
            instantiation of "void run_flash_fwd<Kernel_traits,Is_dropout,Is_causal>(Flash_fwd_params &, cudaStream_t) [with Kernel_traits=Flash_fwd_kernel_traits<224, 128, 64, 8, false, false, cutlass::bfloat16_t, Flash_kernel_traits<224, 128, 64, 8, cutlass::bfloat16_t>>, Is_dropout=true, Is_causal=true]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(208): here
            instantiation of "void run_mha_fwd_hdim224<T>(Flash_fwd_params &, cudaStream_t) [with T=cutlass::bfloat16_t]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_hdim224_bf16_sm80.cu(8): here

/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_kernel.h(276): warning: variable "tQrQ" was declared but never referenced
          detected during:
            instantiation of "void flash::compute_attn_1rowblock<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params,Is_Q_split_K>(const Params &, int, int, int) [with Kernel_traits=Flash_fwd_kernel_traits<224, 128, 64, 8, false, false, cutlass::bfloat16_t, Flash_kernel_traits<224, 128, 64, 8, cutlass::bfloat16_t>>, Is_dropout=true, Is_causal=true, Is_even_N=true, Is_even_K=true, Return_softmax=true, Params=Flash_fwd_params, Is_Q_split_K=false]" 
(589): here
            instantiation of "void flash::compute_attn<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &) [with Kernel_traits=Flash_fwd_kernel_traits<224, 128, 64, 8, false, false, cutlass::bfloat16_t, Flash_kernel_traits<224, 128, 64, 8, cutlass::bfloat16_t>>, Is_dropout=true, Is_causal=true, Is_even_N=true, Is_even_K=true, Return_softmax=true, Params=Flash_fwd_params]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(15): here
            instantiation of "void flash_fwd_kernel<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax>(Flash_fwd_params) [with Kernel_traits=Flash_fwd_kernel_traits<224, 128, 64, 8, false, false, cutlass::bfloat16_t, Flash_kernel_traits<224, 128, 64, 8, cutlass::bfloat16_t>>, Is_dropout=true, Is_causal=true, Is_even_N=true, Is_even_K=true, Return_softmax=true]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(34): here
            instantiation of "void run_flash_fwd<Kernel_traits,Is_dropout,Is_causal>(Flash_fwd_params &, cudaStream_t) [with Kernel_traits=Flash_fwd_kernel_traits<224, 128, 64, 8, false, false, cutlass::bfloat16_t, Flash_kernel_traits<224, 128, 64, 8, cutlass::bfloat16_t>>, Is_dropout=true, Is_causal=true]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(208): here
            instantiation of "void run_mha_fwd_hdim224<T>(Flash_fwd_params &, cudaStream_t) [with T=cutlass::bfloat16_t]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_hdim224_bf16_sm80.cu(8): here

/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_kernel.h(140): warning: variable "MMA_M" was declared but never referenced
          detected during:
            instantiation of "void flash::compute_attn_1rowblock<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params,Is_Q_split_K>(const Params &, int, int, int) [with Kernel_traits=Flash_fwd_kernel_traits<224, 128, 64, 8, false, false, cutlass::bfloat16_t, Flash_kernel_traits<224, 128, 64, 8, cutlass::bfloat16_t>>, Is_dropout=true, Is_causal=true, Is_even_N=true, Is_even_K=true, Return_softmax=false, Params=Flash_fwd_params, Is_Q_split_K=false]" 
(589): here
            instantiation of "void flash::compute_attn<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &) [with Kernel_traits=Flash_fwd_kernel_traits<224, 128, 64, 8, false, false, cutlass::bfloat16_t, Flash_kernel_traits<224, 128, 64, 8, cutlass::bfloat16_t>>, Is_dropout=true, Is_causal=true, Is_even_N=true, Is_even_K=true, Return_softmax=false, Params=Flash_fwd_params]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(15): here
            instantiation of "void flash_fwd_kernel<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax>(Flash_fwd_params) [with Kernel_traits=Flash_fwd_kernel_traits<224, 128, 64, 8, false, false, cutlass::bfloat16_t, Flash_kernel_traits<224, 128, 64, 8, cutlass::bfloat16_t>>, Is_dropout=true, Is_causal=true, Is_even_N=true, Is_even_K=true, Return_softmax=false]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(34): here
            instantiation of "void run_flash_fwd<Kernel_traits,Is_dropout,Is_causal>(Flash_fwd_params &, cudaStream_t) [with Kernel_traits=Flash_fwd_kernel_traits<224, 128, 64, 8, false, false, cutlass::bfloat16_t, Flash_kernel_traits<224, 128, 64, 8, cutlass::bfloat16_t>>, Is_dropout=true, Is_causal=true]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(208): here
            instantiation of "void run_mha_fwd_hdim224<T>(Flash_fwd_params &, cudaStream_t) [with T=cutlass::bfloat16_t]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_hdim224_bf16_sm80.cu(8): here

/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_kernel.h(276): warning: variable "tQrQ" was declared but never referenced
          detected during:
            instantiation of "void flash::compute_attn_1rowblock<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params,Is_Q_split_K>(const Params &, int, int, int) [with Kernel_traits=Flash_fwd_kernel_traits<224, 128, 64, 8, false, false, cutlass::bfloat16_t, Flash_kernel_traits<224, 128, 64, 8, cutlass::bfloat16_t>>, Is_dropout=true, Is_causal=true, Is_even_N=true, Is_even_K=true, Return_softmax=false, Params=Flash_fwd_params, Is_Q_split_K=false]" 
(589): here
            instantiation of "void flash::compute_attn<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &) [with Kernel_traits=Flash_fwd_kernel_traits<224, 128, 64, 8, false, false, cutlass::bfloat16_t, Flash_kernel_traits<224, 128, 64, 8, cutlass::bfloat16_t>>, Is_dropout=true, Is_causal=true, Is_even_N=true, Is_even_K=true, Return_softmax=false, Params=Flash_fwd_params]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(15): here
            instantiation of "void flash_fwd_kernel<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax>(Flash_fwd_params) [with Kernel_traits=Flash_fwd_kernel_traits<224, 128, 64, 8, false, false, cutlass::bfloat16_t, Flash_kernel_traits<224, 128, 64, 8, cutlass::bfloat16_t>>, Is_dropout=true, Is_causal=true, Is_even_N=true, Is_even_K=true, Return_softmax=false]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(34): here
            instantiation of "void run_flash_fwd<Kernel_traits,Is_dropout,Is_causal>(Flash_fwd_params &, cudaStream_t) [with Kernel_traits=Flash_fwd_kernel_traits<224, 128, 64, 8, false, false, cutlass::bfloat16_t, Flash_kernel_traits<224, 128, 64, 8, cutlass::bfloat16_t>>, Is_dropout=true, Is_causal=true]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(208): here
            instantiation of "void run_mha_fwd_hdim224<T>(Flash_fwd_params &, cudaStream_t) [with T=cutlass::bfloat16_t]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_hdim224_bf16_sm80.cu(8): here

/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_kernel.h(140): warning: variable "MMA_M" was declared but never referenced
          detected during:
            instantiation of "void flash::compute_attn_1rowblock<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params,Is_Q_split_K>(const Params &, int, int, int) [with Kernel_traits=Flash_fwd_kernel_traits<224, 128, 64, 8, false, false, cutlass::bfloat16_t, Flash_kernel_traits<224, 128, 64, 8, cutlass::bfloat16_t>>, Is_dropout=true, Is_causal=true, Is_even_N=true, Is_even_K=false, Return_softmax=true, Params=Flash_fwd_params, Is_Q_split_K=false]" 
(589): here
            instantiation of "void flash::compute_attn<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &) [with Kernel_traits=Flash_fwd_kernel_traits<224, 128, 64, 8, false, false, cutlass::bfloat16_t, Flash_kernel_traits<224, 128, 64, 8, cutlass::bfloat16_t>>, Is_dropout=true, Is_causal=true, Is_even_N=true, Is_even_K=false, Return_softmax=true, Params=Flash_fwd_params]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(15): here
            instantiation of "void flash_fwd_kernel<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax>(Flash_fwd_params) [with Kernel_traits=Flash_fwd_kernel_traits<224, 128, 64, 8, false, false, cutlass::bfloat16_t, Flash_kernel_traits<224, 128, 64, 8, cutlass::bfloat16_t>>, Is_dropout=true, Is_causal=true, Is_even_N=true, Is_even_K=false, Return_softmax=true]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(34): here
            instantiation of "void run_flash_fwd<Kernel_traits,Is_dropout,Is_causal>(Flash_fwd_params &, cudaStream_t) [with Kernel_traits=Flash_fwd_kernel_traits<224, 128, 64, 8, false, false, cutlass::bfloat16_t, Flash_kernel_traits<224, 128, 64, 8, cutlass::bfloat16_t>>, Is_dropout=true, Is_causal=true]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(208): here
            instantiation of "void run_mha_fwd_hdim224<T>(Flash_fwd_params &, cudaStream_t) [with T=cutlass::bfloat16_t]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_hdim224_bf16_sm80.cu(8): here

/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_kernel.h(276): warning: variable "tQrQ" was declared but never referenced
          detected during:
            instantiation of "void flash::compute_attn_1rowblock<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params,Is_Q_split_K>(const Params &, int, int, int) [with Kernel_traits=Flash_fwd_kernel_traits<224, 128, 64, 8, false, false, cutlass::bfloat16_t, Flash_kernel_traits<224, 128, 64, 8, cutlass::bfloat16_t>>, Is_dropout=true, Is_causal=true, Is_even_N=true, Is_even_K=false, Return_softmax=true, Params=Flash_fwd_params, Is_Q_split_K=false]" 
(589): here
            instantiation of "void flash::compute_attn<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &) [with Kernel_traits=Flash_fwd_kernel_traits<224, 128, 64, 8, false, false, cutlass::bfloat16_t, Flash_kernel_traits<224, 128, 64, 8, cutlass::bfloat16_t>>, Is_dropout=true, Is_causal=true, Is_even_N=true, Is_even_K=false, Return_softmax=true, Params=Flash_fwd_params]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(15): here
            instantiation of "void flash_fwd_kernel<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax>(Flash_fwd_params) [with Kernel_traits=Flash_fwd_kernel_traits<224, 128, 64, 8, false, false, cutlass::bfloat16_t, Flash_kernel_traits<224, 128, 64, 8, cutlass::bfloat16_t>>, Is_dropout=true, Is_causal=true, Is_even_N=true, Is_even_K=false, Return_softmax=true]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(34): here
            instantiation of "void run_flash_fwd<Kernel_traits,Is_dropout,Is_causal>(Flash_fwd_params &, cudaStream_t) [with Kernel_traits=Flash_fwd_kernel_traits<224, 128, 64, 8, false, false, cutlass::bfloat16_t, Flash_kernel_traits<224, 128, 64, 8, cutlass::bfloat16_t>>, Is_dropout=true, Is_causal=true]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(208): here
            instantiation of "void run_mha_fwd_hdim224<T>(Flash_fwd_params &, cudaStream_t) [with T=cutlass::bfloat16_t]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_hdim224_bf16_sm80.cu(8): here

/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_kernel.h(140): warning: variable "MMA_M" was declared but never referenced
          detected during:
            instantiation of "void flash::compute_attn_1rowblock<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params,Is_Q_split_K>(const Params &, int, int, int) [with Kernel_traits=Flash_fwd_kernel_traits<224, 128, 64, 8, false, false, cutlass::bfloat16_t, Flash_kernel_traits<224, 128, 64, 8, cutlass::bfloat16_t>>, Is_dropout=true, Is_causal=true, Is_even_N=true, Is_even_K=false, Return_softmax=false, Params=Flash_fwd_params, Is_Q_split_K=false]" 
(589): here
            instantiation of "void flash::compute_attn<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &) [with Kernel_traits=Flash_fwd_kernel_traits<224, 128, 64, 8, false, false, cutlass::bfloat16_t, Flash_kernel_traits<224, 128, 64, 8, cutlass::bfloat16_t>>, Is_dropout=true, Is_causal=true, Is_even_N=true, Is_even_K=false, Return_softmax=false, Params=Flash_fwd_params]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(15): here
            instantiation of "void flash_fwd_kernel<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax>(Flash_fwd_params) [with Kernel_traits=Flash_fwd_kernel_traits<224, 128, 64, 8, false, false, cutlass::bfloat16_t, Flash_kernel_traits<224, 128, 64, 8, cutlass::bfloat16_t>>, Is_dropout=true, Is_causal=true, Is_even_N=true, Is_even_K=false, Return_softmax=false]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(34): here
            instantiation of "void run_flash_fwd<Kernel_traits,Is_dropout,Is_causal>(Flash_fwd_params &, cudaStream_t) [with Kernel_traits=Flash_fwd_kernel_traits<224, 128, 64, 8, false, false, cutlass::bfloat16_t, Flash_kernel_traits<224, 128, 64, 8, cutlass::bfloat16_t>>, Is_dropout=true, Is_causal=true]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(208): here
            instantiation of "void run_mha_fwd_hdim224<T>(Flash_fwd_params &, cudaStream_t) [with T=cutlass::bfloat16_t]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_hdim224_bf16_sm80.cu(8): here

/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_kernel.h(276): warning: variable "tQrQ" was declared but never referenced
          detected during:
            instantiation of "void flash::compute_attn_1rowblock<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params,Is_Q_split_K>(const Params &, int, int, int) [with Kernel_traits=Flash_fwd_kernel_traits<224, 128, 64, 8, false, false, cutlass::bfloat16_t, Flash_kernel_traits<224, 128, 64, 8, cutlass::bfloat16_t>>, Is_dropout=true, Is_causal=true, Is_even_N=true, Is_even_K=false, Return_softmax=false, Params=Flash_fwd_params, Is_Q_split_K=false]" 
(589): here
            instantiation of "void flash::compute_attn<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &) [with Kernel_traits=Flash_fwd_kernel_traits<224, 128, 64, 8, false, false, cutlass::bfloat16_t, Flash_kernel_traits<224, 128, 64, 8, cutlass::bfloat16_t>>, Is_dropout=true, Is_causal=true, Is_even_N=true, Is_even_K=false, Return_softmax=false, Params=Flash_fwd_params]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(15): here
            instantiation of "void flash_fwd_kernel<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax>(Flash_fwd_params) [with Kernel_traits=Flash_fwd_kernel_traits<224, 128, 64, 8, false, false, cutlass::bfloat16_t, Flash_kernel_traits<224, 128, 64, 8, cutlass::bfloat16_t>>, Is_dropout=true, Is_causal=true, Is_even_N=true, Is_even_K=false, Return_softmax=false]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(34): here
            instantiation of "void run_flash_fwd<Kernel_traits,Is_dropout,Is_causal>(Flash_fwd_params &, cudaStream_t) [with Kernel_traits=Flash_fwd_kernel_traits<224, 128, 64, 8, false, false, cutlass::bfloat16_t, Flash_kernel_traits<224, 128, 64, 8, cutlass::bfloat16_t>>, Is_dropout=true, Is_causal=true]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(208): here
            instantiation of "void run_mha_fwd_hdim224<T>(Flash_fwd_params &, cudaStream_t) [with T=cutlass::bfloat16_t]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_hdim224_bf16_sm80.cu(8): here

/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_kernel.h(140): warning: variable "MMA_M" was declared but never referenced
          detected during:
            instantiation of "void flash::compute_attn_1rowblock<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params,Is_Q_split_K>(const Params &, int, int, int) [with Kernel_traits=Flash_fwd_kernel_traits<224, 128, 64, 8, false, false, cutlass::bfloat16_t, Flash_kernel_traits<224, 128, 64, 8, cutlass::bfloat16_t>>, Is_dropout=true, Is_causal=true, Is_even_N=false, Is_even_K=true, Return_softmax=true, Params=Flash_fwd_params, Is_Q_split_K=false]" 
(589): here
            instantiation of "void flash::compute_attn<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &) [with Kernel_traits=Flash_fwd_kernel_traits<224, 128, 64, 8, false, false, cutlass::bfloat16_t, Flash_kernel_traits<224, 128, 64, 8, cutlass::bfloat16_t>>, Is_dropout=true, Is_causal=true, Is_even_N=false, Is_even_K=true, Return_softmax=true, Params=Flash_fwd_params]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(15): here
            instantiation of "void flash_fwd_kernel<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax>(Flash_fwd_params) [with Kernel_traits=Flash_fwd_kernel_traits<224, 128, 64, 8, false, false, cutlass::bfloat16_t, Flash_kernel_traits<224, 128, 64, 8, cutlass::bfloat16_t>>, Is_dropout=true, Is_causal=true, Is_even_N=false, Is_even_K=true, Return_softmax=true]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(34): here
            instantiation of "void run_flash_fwd<Kernel_traits,Is_dropout,Is_causal>(Flash_fwd_params &, cudaStream_t) [with Kernel_traits=Flash_fwd_kernel_traits<224, 128, 64, 8, false, false, cutlass::bfloat16_t, Flash_kernel_traits<224, 128, 64, 8, cutlass::bfloat16_t>>, Is_dropout=true, Is_causal=true]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(208): here
            instantiation of "void run_mha_fwd_hdim224<T>(Flash_fwd_params &, cudaStream_t) [with T=cutlass::bfloat16_t]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_hdim224_bf16_sm80.cu(8): here

/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_kernel.h(276): warning: variable "tQrQ" was declared but never referenced
          detected during:
            instantiation of "void flash::compute_attn_1rowblock<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params,Is_Q_split_K>(const Params &, int, int, int) [with Kernel_traits=Flash_fwd_kernel_traits<224, 128, 64, 8, false, false, cutlass::bfloat16_t, Flash_kernel_traits<224, 128, 64, 8, cutlass::bfloat16_t>>, Is_dropout=true, Is_causal=true, Is_even_N=false, Is_even_K=true, Return_softmax=true, Params=Flash_fwd_params, Is_Q_split_K=false]" 
(589): here
            instantiation of "void flash::compute_attn<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &) [with Kernel_traits=Flash_fwd_kernel_traits<224, 128, 64, 8, false, false, cutlass::bfloat16_t, Flash_kernel_traits<224, 128, 64, 8, cutlass::bfloat16_t>>, Is_dropout=true, Is_causal=true, Is_even_N=false, Is_even_K=true, Return_softmax=true, Params=Flash_fwd_params]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(15): here
            instantiation of "void flash_fwd_kernel<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax>(Flash_fwd_params) [with Kernel_traits=Flash_fwd_kernel_traits<224, 128, 64, 8, false, false, cutlass::bfloat16_t, Flash_kernel_traits<224, 128, 64, 8, cutlass::bfloat16_t>>, Is_dropout=true, Is_causal=true, Is_even_N=false, Is_even_K=true, Return_softmax=true]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(34): here
            instantiation of "void run_flash_fwd<Kernel_traits,Is_dropout,Is_causal>(Flash_fwd_params &, cudaStream_t) [with Kernel_traits=Flash_fwd_kernel_traits<224, 128, 64, 8, false, false, cutlass::bfloat16_t, Flash_kernel_traits<224, 128, 64, 8, cutlass::bfloat16_t>>, Is_dropout=true, Is_causal=true]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(208): here
            instantiation of "void run_mha_fwd_hdim224<T>(Flash_fwd_params &, cudaStream_t) [with T=cutlass::bfloat16_t]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_hdim224_bf16_sm80.cu(8): here

/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_kernel.h(140): warning: variable "MMA_M" was declared but never referenced
          detected during:
            instantiation of "void flash::compute_attn_1rowblock<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params,Is_Q_split_K>(const Params &, int, int, int) [with Kernel_traits=Flash_fwd_kernel_traits<224, 128, 64, 8, false, false, cutlass::bfloat16_t, Flash_kernel_traits<224, 128, 64, 8, cutlass::bfloat16_t>>, Is_dropout=true, Is_causal=true, Is_even_N=false, Is_even_K=true, Return_softmax=false, Params=Flash_fwd_params, Is_Q_split_K=false]" 
(589): here
            instantiation of "void flash::compute_attn<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &) [with Kernel_traits=Flash_fwd_kernel_traits<224, 128, 64, 8, false, false, cutlass::bfloat16_t, Flash_kernel_traits<224, 128, 64, 8, cutlass::bfloat16_t>>, Is_dropout=true, Is_causal=true, Is_even_N=false, Is_even_K=true, Return_softmax=false, Params=Flash_fwd_params]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(15): here
            instantiation of "void flash_fwd_kernel<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax>(Flash_fwd_params) [with Kernel_traits=Flash_fwd_kernel_traits<224, 128, 64, 8, false, false, cutlass::bfloat16_t, Flash_kernel_traits<224, 128, 64, 8, cutlass::bfloat16_t>>, Is_dropout=true, Is_causal=true, Is_even_N=false, Is_even_K=true, Return_softmax=false]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(34): here
            instantiation of "void run_flash_fwd<Kernel_traits,Is_dropout,Is_causal>(Flash_fwd_params &, cudaStream_t) [with Kernel_traits=Flash_fwd_kernel_traits<224, 128, 64, 8, false, false, cutlass::bfloat16_t, Flash_kernel_traits<224, 128, 64, 8, cutlass::bfloat16_t>>, Is_dropout=true, Is_causal=true]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(208): here
            instantiation of "void run_mha_fwd_hdim224<T>(Flash_fwd_params &, cudaStream_t) [with T=cutlass::bfloat16_t]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_hdim224_bf16_sm80.cu(8): here

/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_kernel.h(276): warning: variable "tQrQ" was declared but never referenced
          detected during:
            instantiation of "void flash::compute_attn_1rowblock<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params,Is_Q_split_K>(const Params &, int, int, int) [with Kernel_traits=Flash_fwd_kernel_traits<224, 128, 64, 8, false, false, cutlass::bfloat16_t, Flash_kernel_traits<224, 128, 64, 8, cutlass::bfloat16_t>>, Is_dropout=true, Is_causal=true, Is_even_N=false, Is_even_K=true, Return_softmax=false, Params=Flash_fwd_params, Is_Q_split_K=false]" 
(589): here
            instantiation of "void flash::compute_attn<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &) [with Kernel_traits=Flash_fwd_kernel_traits<224, 128, 64, 8, false, false, cutlass::bfloat16_t, Flash_kernel_traits<224, 128, 64, 8, cutlass::bfloat16_t>>, Is_dropout=true, Is_causal=true, Is_even_N=false, Is_even_K=true, Return_softmax=false, Params=Flash_fwd_params]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(15): here
            instantiation of "void flash_fwd_kernel<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax>(Flash_fwd_params) [with Kernel_traits=Flash_fwd_kernel_traits<224, 128, 64, 8, false, false, cutlass::bfloat16_t, Flash_kernel_traits<224, 128, 64, 8, cutlass::bfloat16_t>>, Is_dropout=true, Is_causal=true, Is_even_N=false, Is_even_K=true, Return_softmax=false]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(34): here
            instantiation of "void run_flash_fwd<Kernel_traits,Is_dropout,Is_causal>(Flash_fwd_params &, cudaStream_t) [with Kernel_traits=Flash_fwd_kernel_traits<224, 128, 64, 8, false, false, cutlass::bfloat16_t, Flash_kernel_traits<224, 128, 64, 8, cutlass::bfloat16_t>>, Is_dropout=true, Is_causal=true]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(208): here
            instantiation of "void run_mha_fwd_hdim224<T>(Flash_fwd_params &, cudaStream_t) [with T=cutlass::bfloat16_t]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_hdim224_bf16_sm80.cu(8): here

/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_kernel.h(140): warning: variable "MMA_M" was declared but never referenced
          detected during:
            instantiation of "void flash::compute_attn_1rowblock<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params,Is_Q_split_K>(const Params &, int, int, int) [with Kernel_traits=Flash_fwd_kernel_traits<224, 128, 64, 8, false, false, cutlass::bfloat16_t, Flash_kernel_traits<224, 128, 64, 8, cutlass::bfloat16_t>>, Is_dropout=true, Is_causal=true, Is_even_N=false, Is_even_K=false, Return_softmax=true, Params=Flash_fwd_params, Is_Q_split_K=false]" 
(589): here
            instantiation of "void flash::compute_attn<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &) [with Kernel_traits=Flash_fwd_kernel_traits<224, 128, 64, 8, false, false, cutlass::bfloat16_t, Flash_kernel_traits<224, 128, 64, 8, cutlass::bfloat16_t>>, Is_dropout=true, Is_causal=true, Is_even_N=false, Is_even_K=false, Return_softmax=true, Params=Flash_fwd_params]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(15): here
            instantiation of "void flash_fwd_kernel<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax>(Flash_fwd_params) [with Kernel_traits=Flash_fwd_kernel_traits<224, 128, 64, 8, false, false, cutlass::bfloat16_t, Flash_kernel_traits<224, 128, 64, 8, cutlass::bfloat16_t>>, Is_dropout=true, Is_causal=true, Is_even_N=false, Is_even_K=false, Return_softmax=true]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(34): here
            instantiation of "void run_flash_fwd<Kernel_traits,Is_dropout,Is_causal>(Flash_fwd_params &, cudaStream_t) [with Kernel_traits=Flash_fwd_kernel_traits<224, 128, 64, 8, false, false, cutlass::bfloat16_t, Flash_kernel_traits<224, 128, 64, 8, cutlass::bfloat16_t>>, Is_dropout=true, Is_causal=true]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(208): here
            instantiation of "void run_mha_fwd_hdim224<T>(Flash_fwd_params &, cudaStream_t) [with T=cutlass::bfloat16_t]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_hdim224_bf16_sm80.cu(8): here

/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_kernel.h(276): warning: variable "tQrQ" was declared but never referenced
          detected during:
            instantiation of "void flash::compute_attn_1rowblock<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params,Is_Q_split_K>(const Params &, int, int, int) [with Kernel_traits=Flash_fwd_kernel_traits<224, 128, 64, 8, false, false, cutlass::bfloat16_t, Flash_kernel_traits<224, 128, 64, 8, cutlass::bfloat16_t>>, Is_dropout=true, Is_causal=true, Is_even_N=false, Is_even_K=false, Return_softmax=true, Params=Flash_fwd_params, Is_Q_split_K=false]" 
(589): here
            instantiation of "void flash::compute_attn<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &) [with Kernel_traits=Flash_fwd_kernel_traits<224, 128, 64, 8, false, false, cutlass::bfloat16_t, Flash_kernel_traits<224, 128, 64, 8, cutlass::bfloat16_t>>, Is_dropout=true, Is_causal=true, Is_even_N=false, Is_even_K=false, Return_softmax=true, Params=Flash_fwd_params]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(15): here
            instantiation of "void flash_fwd_kernel<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax>(Flash_fwd_params) [with Kernel_traits=Flash_fwd_kernel_traits<224, 128, 64, 8, false, false, cutlass::bfloat16_t, Flash_kernel_traits<224, 128, 64, 8, cutlass::bfloat16_t>>, Is_dropout=true, Is_causal=true, Is_even_N=false, Is_even_K=false, Return_softmax=true]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(34): here
            instantiation of "void run_flash_fwd<Kernel_traits,Is_dropout,Is_causal>(Flash_fwd_params &, cudaStream_t) [with Kernel_traits=Flash_fwd_kernel_traits<224, 128, 64, 8, false, false, cutlass::bfloat16_t, Flash_kernel_traits<224, 128, 64, 8, cutlass::bfloat16_t>>, Is_dropout=true, Is_causal=true]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(208): here
            instantiation of "void run_mha_fwd_hdim224<T>(Flash_fwd_params &, cudaStream_t) [with T=cutlass::bfloat16_t]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_hdim224_bf16_sm80.cu(8): here

/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_kernel.h(140): warning: variable "MMA_M" was declared but never referenced
          detected during:
            instantiation of "void flash::compute_attn_1rowblock<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params,Is_Q_split_K>(const Params &, int, int, int) [with Kernel_traits=Flash_fwd_kernel_traits<224, 128, 64, 8, false, false, cutlass::bfloat16_t, Flash_kernel_traits<224, 128, 64, 8, cutlass::bfloat16_t>>, Is_dropout=true, Is_causal=true, Is_even_N=false, Is_even_K=false, Return_softmax=false, Params=Flash_fwd_params, Is_Q_split_K=false]" 
(589): here
            instantiation of "void flash::compute_attn<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &) [with Kernel_traits=Flash_fwd_kernel_traits<224, 128, 64, 8, false, false, cutlass::bfloat16_t, Flash_kernel_traits<224, 128, 64, 8, cutlass::bfloat16_t>>, Is_dropout=true, Is_causal=true, Is_even_N=false, Is_even_K=false, Return_softmax=false, Params=Flash_fwd_params]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(15): here
            instantiation of "void flash_fwd_kernel<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax>(Flash_fwd_params) [with Kernel_traits=Flash_fwd_kernel_traits<224, 128, 64, 8, false, false, cutlass::bfloat16_t, Flash_kernel_traits<224, 128, 64, 8, cutlass::bfloat16_t>>, Is_dropout=true, Is_causal=true, Is_even_N=false, Is_even_K=false, Return_softmax=false]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(34): here
            instantiation of "void run_flash_fwd<Kernel_traits,Is_dropout,Is_causal>(Flash_fwd_params &, cudaStream_t) [with Kernel_traits=Flash_fwd_kernel_traits<224, 128, 64, 8, false, false, cutlass::bfloat16_t, Flash_kernel_traits<224, 128, 64, 8, cutlass::bfloat16_t>>, Is_dropout=true, Is_causal=true]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(208): here
            instantiation of "void run_mha_fwd_hdim224<T>(Flash_fwd_params &, cudaStream_t) [with T=cutlass::bfloat16_t]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_hdim224_bf16_sm80.cu(8): here

/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_kernel.h(276): warning: variable "tQrQ" was declared but never referenced
          detected during:
            instantiation of "void flash::compute_attn_1rowblock<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params,Is_Q_split_K>(const Params &, int, int, int) [with Kernel_traits=Flash_fwd_kernel_traits<224, 128, 64, 8, false, false, cutlass::bfloat16_t, Flash_kernel_traits<224, 128, 64, 8, cutlass::bfloat16_t>>, Is_dropout=true, Is_causal=true, Is_even_N=false, Is_even_K=false, Return_softmax=false, Params=Flash_fwd_params, Is_Q_split_K=false]" 
(589): here
            instantiation of "void flash::compute_attn<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &) [with Kernel_traits=Flash_fwd_kernel_traits<224, 128, 64, 8, false, false, cutlass::bfloat16_t, Flash_kernel_traits<224, 128, 64, 8, cutlass::bfloat16_t>>, Is_dropout=true, Is_causal=true, Is_even_N=false, Is_even_K=false, Return_softmax=false, Params=Flash_fwd_params]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(15): here
            instantiation of "void flash_fwd_kernel<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax>(Flash_fwd_params) [with Kernel_traits=Flash_fwd_kernel_traits<224, 128, 64, 8, false, false, cutlass::bfloat16_t, Flash_kernel_traits<224, 128, 64, 8, cutlass::bfloat16_t>>, Is_dropout=true, Is_causal=true, Is_even_N=false, Is_even_K=false, Return_softmax=false]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(34): here
            instantiation of "void run_flash_fwd<Kernel_traits,Is_dropout,Is_causal>(Flash_fwd_params &, cudaStream_t) [with Kernel_traits=Flash_fwd_kernel_traits<224, 128, 64, 8, false, false, cutlass::bfloat16_t, Flash_kernel_traits<224, 128, 64, 8, cutlass::bfloat16_t>>, Is_dropout=true, Is_causal=true]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(208): here
            instantiation of "void run_mha_fwd_hdim224<T>(Flash_fwd_params &, cudaStream_t) [with T=cutlass::bfloat16_t]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_hdim224_bf16_sm80.cu(8): here

/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_kernel.h(140): warning: variable "MMA_M" was declared but never referenced
          detected during:
            instantiation of "void flash::compute_attn_1rowblock<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params,Is_Q_split_K>(const Params &, int, int, int) [with Kernel_traits=Flash_fwd_kernel_traits<224, 64, 64, 4, false, false, cutlass::bfloat16_t, Flash_kernel_traits<224, 64, 64, 4, cutlass::bfloat16_t>>, Is_dropout=true, Is_causal=true, Is_even_N=true, Is_even_K=true, Return_softmax=true, Params=Flash_fwd_params, Is_Q_split_K=false]" 
(589): here
            instantiation of "void flash::compute_attn<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &) [with Kernel_traits=Flash_fwd_kernel_traits<224, 64, 64, 4, false, false, cutlass::bfloat16_t, Flash_kernel_traits<224, 64, 64, 4, cutlass::bfloat16_t>>, Is_dropout=true, Is_causal=true, Is_even_N=true, Is_even_K=true, Return_softmax=true, Params=Flash_fwd_params]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(15): here
            instantiation of "void flash_fwd_kernel<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax>(Flash_fwd_params) [with Kernel_traits=Flash_fwd_kernel_traits<224, 64, 64, 4, false, false, cutlass::bfloat16_t, Flash_kernel_traits<224, 64, 64, 4, cutlass::bfloat16_t>>, Is_dropout=true, Is_causal=true, Is_even_N=true, Is_even_K=true, Return_softmax=true]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(34): here
            instantiation of "void run_flash_fwd<Kernel_traits,Is_dropout,Is_causal>(Flash_fwd_params &, cudaStream_t) [with Kernel_traits=Flash_fwd_kernel_traits<224, 64, 64, 4, false, false, cutlass::bfloat16_t, Flash_kernel_traits<224, 64, 64, 4, cutlass::bfloat16_t>>, Is_dropout=true, Is_causal=true]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(208): here
            instantiation of "void run_mha_fwd_hdim224<T>(Flash_fwd_params &, cudaStream_t) [with T=cutlass::bfloat16_t]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_hdim224_bf16_sm80.cu(8): here

/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_kernel.h(276): warning: variable "tQrQ" was declared but never referenced
          detected during:
            instantiation of "void flash::compute_attn_1rowblock<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params,Is_Q_split_K>(const Params &, int, int, int) [with Kernel_traits=Flash_fwd_kernel_traits<224, 64, 64, 4, false, false, cutlass::bfloat16_t, Flash_kernel_traits<224, 64, 64, 4, cutlass::bfloat16_t>>, Is_dropout=true, Is_causal=true, Is_even_N=true, Is_even_K=true, Return_softmax=true, Params=Flash_fwd_params, Is_Q_split_K=false]" 
(589): here
            instantiation of "void flash::compute_attn<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &) [with Kernel_traits=Flash_fwd_kernel_traits<224, 64, 64, 4, false, false, cutlass::bfloat16_t, Flash_kernel_traits<224, 64, 64, 4, cutlass::bfloat16_t>>, Is_dropout=true, Is_causal=true, Is_even_N=true, Is_even_K=true, Return_softmax=true, Params=Flash_fwd_params]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(15): here
            instantiation of "void flash_fwd_kernel<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax>(Flash_fwd_params) [with Kernel_traits=Flash_fwd_kernel_traits<224, 64, 64, 4, false, false, cutlass::bfloat16_t, Flash_kernel_traits<224, 64, 64, 4, cutlass::bfloat16_t>>, Is_dropout=true, Is_causal=true, Is_even_N=true, Is_even_K=true, Return_softmax=true]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(34): here
            instantiation of "void run_flash_fwd<Kernel_traits,Is_dropout,Is_causal>(Flash_fwd_params &, cudaStream_t) [with Kernel_traits=Flash_fwd_kernel_traits<224, 64, 64, 4, false, false, cutlass::bfloat16_t, Flash_kernel_traits<224, 64, 64, 4, cutlass::bfloat16_t>>, Is_dropout=true, Is_causal=true]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(208): here
            instantiation of "void run_mha_fwd_hdim224<T>(Flash_fwd_params &, cudaStream_t) [with T=cutlass::bfloat16_t]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_hdim224_bf16_sm80.cu(8): here

/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_kernel.h(140): warning: variable "MMA_M" was declared but never referenced
          detected during:
            instantiation of "void flash::compute_attn_1rowblock<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params,Is_Q_split_K>(const Params &, int, int, int) [with Kernel_traits=Flash_fwd_kernel_traits<224, 64, 64, 4, false, false, cutlass::bfloat16_t, Flash_kernel_traits<224, 64, 64, 4, cutlass::bfloat16_t>>, Is_dropout=true, Is_causal=true, Is_even_N=true, Is_even_K=true, Return_softmax=false, Params=Flash_fwd_params, Is_Q_split_K=false]" 
(589): here
            instantiation of "void flash::compute_attn<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &) [with Kernel_traits=Flash_fwd_kernel_traits<224, 64, 64, 4, false, false, cutlass::bfloat16_t, Flash_kernel_traits<224, 64, 64, 4, cutlass::bfloat16_t>>, Is_dropout=true, Is_causal=true, Is_even_N=true, Is_even_K=true, Return_softmax=false, Params=Flash_fwd_params]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(15): here
            instantiation of "void flash_fwd_kernel<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax>(Flash_fwd_params) [with Kernel_traits=Flash_fwd_kernel_traits<224, 64, 64, 4, false, false, cutlass::bfloat16_t, Flash_kernel_traits<224, 64, 64, 4, cutlass::bfloat16_t>>, Is_dropout=true, Is_causal=true, Is_even_N=true, Is_even_K=true, Return_softmax=false]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(34): here
            instantiation of "void run_flash_fwd<Kernel_traits,Is_dropout,Is_causal>(Flash_fwd_params &, cudaStream_t) [with Kernel_traits=Flash_fwd_kernel_traits<224, 64, 64, 4, false, false, cutlass::bfloat16_t, Flash_kernel_traits<224, 64, 64, 4, cutlass::bfloat16_t>>, Is_dropout=true, Is_causal=true]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(208): here
            instantiation of "void run_mha_fwd_hdim224<T>(Flash_fwd_params &, cudaStream_t) [with T=cutlass::bfloat16_t]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_hdim224_bf16_sm80.cu(8): here

/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_kernel.h(276): warning: variable "tQrQ" was declared but never referenced
          detected during:
            instantiation of "void flash::compute_attn_1rowblock<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params,Is_Q_split_K>(const Params &, int, int, int) [with Kernel_traits=Flash_fwd_kernel_traits<224, 64, 64, 4, false, false, cutlass::bfloat16_t, Flash_kernel_traits<224, 64, 64, 4, cutlass::bfloat16_t>>, Is_dropout=true, Is_causal=true, Is_even_N=true, Is_even_K=true, Return_softmax=false, Params=Flash_fwd_params, Is_Q_split_K=false]" 
(589): here
            instantiation of "void flash::compute_attn<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &) [with Kernel_traits=Flash_fwd_kernel_traits<224, 64, 64, 4, false, false, cutlass::bfloat16_t, Flash_kernel_traits<224, 64, 64, 4, cutlass::bfloat16_t>>, Is_dropout=true, Is_causal=true, Is_even_N=true, Is_even_K=true, Return_softmax=false, Params=Flash_fwd_params]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(15): here
            instantiation of "void flash_fwd_kernel<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax>(Flash_fwd_params) [with Kernel_traits=Flash_fwd_kernel_traits<224, 64, 64, 4, false, false, cutlass::bfloat16_t, Flash_kernel_traits<224, 64, 64, 4, cutlass::bfloat16_t>>, Is_dropout=true, Is_causal=true, Is_even_N=true, Is_even_K=true, Return_softmax=false]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(34): here
            instantiation of "void run_flash_fwd<Kernel_traits,Is_dropout,Is_causal>(Flash_fwd_params &, cudaStream_t) [with Kernel_traits=Flash_fwd_kernel_traits<224, 64, 64, 4, false, false, cutlass::bfloat16_t, Flash_kernel_traits<224, 64, 64, 4, cutlass::bfloat16_t>>, Is_dropout=true, Is_causal=true]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(208): here
            instantiation of "void run_mha_fwd_hdim224<T>(Flash_fwd_params &, cudaStream_t) [with T=cutlass::bfloat16_t]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_hdim224_bf16_sm80.cu(8): here

/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_kernel.h(140): warning: variable "MMA_M" was declared but never referenced
          detected during:
            instantiation of "void flash::compute_attn_1rowblock<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params,Is_Q_split_K>(const Params &, int, int, int) [with Kernel_traits=Flash_fwd_kernel_traits<224, 64, 64, 4, false, false, cutlass::bfloat16_t, Flash_kernel_traits<224, 64, 64, 4, cutlass::bfloat16_t>>, Is_dropout=true, Is_causal=true, Is_even_N=true, Is_even_K=false, Return_softmax=true, Params=Flash_fwd_params, Is_Q_split_K=false]" 
(589): here
            instantiation of "void flash::compute_attn<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &) [with Kernel_traits=Flash_fwd_kernel_traits<224, 64, 64, 4, false, false, cutlass::bfloat16_t, Flash_kernel_traits<224, 64, 64, 4, cutlass::bfloat16_t>>, Is_dropout=true, Is_causal=true, Is_even_N=true, Is_even_K=false, Return_softmax=true, Params=Flash_fwd_params]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(15): here
            instantiation of "void flash_fwd_kernel<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax>(Flash_fwd_params) [with Kernel_traits=Flash_fwd_kernel_traits<224, 64, 64, 4, false, false, cutlass::bfloat16_t, Flash_kernel_traits<224, 64, 64, 4, cutlass::bfloat16_t>>, Is_dropout=true, Is_causal=true, Is_even_N=true, Is_even_K=false, Return_softmax=true]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(34): here
            instantiation of "void run_flash_fwd<Kernel_traits,Is_dropout,Is_causal>(Flash_fwd_params &, cudaStream_t) [with Kernel_traits=Flash_fwd_kernel_traits<224, 64, 64, 4, false, false, cutlass::bfloat16_t, Flash_kernel_traits<224, 64, 64, 4, cutlass::bfloat16_t>>, Is_dropout=true, Is_causal=true]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(208): here
            instantiation of "void run_mha_fwd_hdim224<T>(Flash_fwd_params &, cudaStream_t) [with T=cutlass::bfloat16_t]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_hdim224_bf16_sm80.cu(8): here

/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_kernel.h(276): warning: variable "tQrQ" was declared but never referenced
          detected during:
            instantiation of "void flash::compute_attn_1rowblock<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params,Is_Q_split_K>(const Params &, int, int, int) [with Kernel_traits=Flash_fwd_kernel_traits<224, 64, 64, 4, false, false, cutlass::bfloat16_t, Flash_kernel_traits<224, 64, 64, 4, cutlass::bfloat16_t>>, Is_dropout=true, Is_causal=true, Is_even_N=true, Is_even_K=false, Return_softmax=true, Params=Flash_fwd_params, Is_Q_split_K=false]" 
(589): here
            instantiation of "void flash::compute_attn<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &) [with Kernel_traits=Flash_fwd_kernel_traits<224, 64, 64, 4, false, false, cutlass::bfloat16_t, Flash_kernel_traits<224, 64, 64, 4, cutlass::bfloat16_t>>, Is_dropout=true, Is_causal=true, Is_even_N=true, Is_even_K=false, Return_softmax=true, Params=Flash_fwd_params]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(15): here
            instantiation of "void flash_fwd_kernel<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax>(Flash_fwd_params) [with Kernel_traits=Flash_fwd_kernel_traits<224, 64, 64, 4, false, false, cutlass::bfloat16_t, Flash_kernel_traits<224, 64, 64, 4, cutlass::bfloat16_t>>, Is_dropout=true, Is_causal=true, Is_even_N=true, Is_even_K=false, Return_softmax=true]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(34): here
            instantiation of "void run_flash_fwd<Kernel_traits,Is_dropout,Is_causal>(Flash_fwd_params &, cudaStream_t) [with Kernel_traits=Flash_fwd_kernel_traits<224, 64, 64, 4, false, false, cutlass::bfloat16_t, Flash_kernel_traits<224, 64, 64, 4, cutlass::bfloat16_t>>, Is_dropout=true, Is_causal=true]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(208): here
            instantiation of "void run_mha_fwd_hdim224<T>(Flash_fwd_params &, cudaStream_t) [with T=cutlass::bfloat16_t]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_hdim224_bf16_sm80.cu(8): here

/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_kernel.h(140): warning: variable "MMA_M" was declared but never referenced
          detected during:
            instantiation of "void flash::compute_attn_1rowblock<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params,Is_Q_split_K>(const Params &, int, int, int) [with Kernel_traits=Flash_fwd_kernel_traits<224, 64, 64, 4, false, false, cutlass::bfloat16_t, Flash_kernel_traits<224, 64, 64, 4, cutlass::bfloat16_t>>, Is_dropout=true, Is_causal=true, Is_even_N=true, Is_even_K=false, Return_softmax=false, Params=Flash_fwd_params, Is_Q_split_K=false]" 
(589): here
            instantiation of "void flash::compute_attn<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &) [with Kernel_traits=Flash_fwd_kernel_traits<224, 64, 64, 4, false, false, cutlass::bfloat16_t, Flash_kernel_traits<224, 64, 64, 4, cutlass::bfloat16_t>>, Is_dropout=true, Is_causal=true, Is_even_N=true, Is_even_K=false, Return_softmax=false, Params=Flash_fwd_params]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(15): here
            instantiation of "void flash_fwd_kernel<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax>(Flash_fwd_params) [with Kernel_traits=Flash_fwd_kernel_traits<224, 64, 64, 4, false, false, cutlass::bfloat16_t, Flash_kernel_traits<224, 64, 64, 4, cutlass::bfloat16_t>>, Is_dropout=true, Is_causal=true, Is_even_N=true, Is_even_K=false, Return_softmax=false]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(34): here
            instantiation of "void run_flash_fwd<Kernel_traits,Is_dropout,Is_causal>(Flash_fwd_params &, cudaStream_t) [with Kernel_traits=Flash_fwd_kernel_traits<224, 64, 64, 4, false, false, cutlass::bfloat16_t, Flash_kernel_traits<224, 64, 64, 4, cutlass::bfloat16_t>>, Is_dropout=true, Is_causal=true]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(208): here
            instantiation of "void run_mha_fwd_hdim224<T>(Flash_fwd_params &, cudaStream_t) [with T=cutlass::bfloat16_t]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_hdim224_bf16_sm80.cu(8): here

/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_kernel.h(276): warning: variable "tQrQ" was declared but never referenced
          detected during:
            instantiation of "void flash::compute_attn_1rowblock<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params,Is_Q_split_K>(const Params &, int, int, int) [with Kernel_traits=Flash_fwd_kernel_traits<224, 64, 64, 4, false, false, cutlass::bfloat16_t, Flash_kernel_traits<224, 64, 64, 4, cutlass::bfloat16_t>>, Is_dropout=true, Is_causal=true, Is_even_N=true, Is_even_K=false, Return_softmax=false, Params=Flash_fwd_params, Is_Q_split_K=false]" 
(589): here
            instantiation of "void flash::compute_attn<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &) [with Kernel_traits=Flash_fwd_kernel_traits<224, 64, 64, 4, false, false, cutlass::bfloat16_t, Flash_kernel_traits<224, 64, 64, 4, cutlass::bfloat16_t>>, Is_dropout=true, Is_causal=true, Is_even_N=true, Is_even_K=false, Return_softmax=false, Params=Flash_fwd_params]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(15): here
            instantiation of "void flash_fwd_kernel<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax>(Flash_fwd_params) [with Kernel_traits=Flash_fwd_kernel_traits<224, 64, 64, 4, false, false, cutlass::bfloat16_t, Flash_kernel_traits<224, 64, 64, 4, cutlass::bfloat16_t>>, Is_dropout=true, Is_causal=true, Is_even_N=true, Is_even_K=false, Return_softmax=false]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(34): here
            instantiation of "void run_flash_fwd<Kernel_traits,Is_dropout,Is_causal>(Flash_fwd_params &, cudaStream_t) [with Kernel_traits=Flash_fwd_kernel_traits<224, 64, 64, 4, false, false, cutlass::bfloat16_t, Flash_kernel_traits<224, 64, 64, 4, cutlass::bfloat16_t>>, Is_dropout=true, Is_causal=true]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(208): here
            instantiation of "void run_mha_fwd_hdim224<T>(Flash_fwd_params &, cudaStream_t) [with T=cutlass::bfloat16_t]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_hdim224_bf16_sm80.cu(8): here

/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_kernel.h(140): warning: variable "MMA_M" was declared but never referenced
          detected during:
            instantiation of "void flash::compute_attn_1rowblock<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params,Is_Q_split_K>(const Params &, int, int, int) [with Kernel_traits=Flash_fwd_kernel_traits<224, 64, 64, 4, false, false, cutlass::bfloat16_t, Flash_kernel_traits<224, 64, 64, 4, cutlass::bfloat16_t>>, Is_dropout=true, Is_causal=true, Is_even_N=false, Is_even_K=true, Return_softmax=true, Params=Flash_fwd_params, Is_Q_split_K=false]" 
(589): here
            instantiation of "void flash::compute_attn<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &) [with Kernel_traits=Flash_fwd_kernel_traits<224, 64, 64, 4, false, false, cutlass::bfloat16_t, Flash_kernel_traits<224, 64, 64, 4, cutlass::bfloat16_t>>, Is_dropout=true, Is_causal=true, Is_even_N=false, Is_even_K=true, Return_softmax=true, Params=Flash_fwd_params]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(15): here
            instantiation of "void flash_fwd_kernel<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax>(Flash_fwd_params) [with Kernel_traits=Flash_fwd_kernel_traits<224, 64, 64, 4, false, false, cutlass::bfloat16_t, Flash_kernel_traits<224, 64, 64, 4, cutlass::bfloat16_t>>, Is_dropout=true, Is_causal=true, Is_even_N=false, Is_even_K=true, Return_softmax=true]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(34): here
            instantiation of "void run_flash_fwd<Kernel_traits,Is_dropout,Is_causal>(Flash_fwd_params &, cudaStream_t) [with Kernel_traits=Flash_fwd_kernel_traits<224, 64, 64, 4, false, false, cutlass::bfloat16_t, Flash_kernel_traits<224, 64, 64, 4, cutlass::bfloat16_t>>, Is_dropout=true, Is_causal=true]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(208): here
            instantiation of "void run_mha_fwd_hdim224<T>(Flash_fwd_params &, cudaStream_t) [with T=cutlass::bfloat16_t]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_hdim224_bf16_sm80.cu(8): here

/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_kernel.h(276): warning: variable "tQrQ" was declared but never referenced
          detected during:
            instantiation of "void flash::compute_attn_1rowblock<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params,Is_Q_split_K>(const Params &, int, int, int) [with Kernel_traits=Flash_fwd_kernel_traits<224, 64, 64, 4, false, false, cutlass::bfloat16_t, Flash_kernel_traits<224, 64, 64, 4, cutlass::bfloat16_t>>, Is_dropout=true, Is_causal=true, Is_even_N=false, Is_even_K=true, Return_softmax=true, Params=Flash_fwd_params, Is_Q_split_K=false]" 
(589): here
            instantiation of "void flash::compute_attn<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &) [with Kernel_traits=Flash_fwd_kernel_traits<224, 64, 64, 4, false, false, cutlass::bfloat16_t, Flash_kernel_traits<224, 64, 64, 4, cutlass::bfloat16_t>>, Is_dropout=true, Is_causal=true, Is_even_N=false, Is_even_K=true, Return_softmax=true, Params=Flash_fwd_params]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(15): here
            instantiation of "void flash_fwd_kernel<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax>(Flash_fwd_params) [with Kernel_traits=Flash_fwd_kernel_traits<224, 64, 64, 4, false, false, cutlass::bfloat16_t, Flash_kernel_traits<224, 64, 64, 4, cutlass::bfloat16_t>>, Is_dropout=true, Is_causal=true, Is_even_N=false, Is_even_K=true, Return_softmax=true]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(34): here
            instantiation of "void run_flash_fwd<Kernel_traits,Is_dropout,Is_causal>(Flash_fwd_params &, cudaStream_t) [with Kernel_traits=Flash_fwd_kernel_traits<224, 64, 64, 4, false, false, cutlass::bfloat16_t, Flash_kernel_traits<224, 64, 64, 4, cutlass::bfloat16_t>>, Is_dropout=true, Is_causal=true]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(208): here
            instantiation of "void run_mha_fwd_hdim224<T>(Flash_fwd_params &, cudaStream_t) [with T=cutlass::bfloat16_t]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_hdim224_bf16_sm80.cu(8): here

/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_kernel.h(140): warning: variable "MMA_M" was declared but never referenced
          detected during:
            instantiation of "void flash::compute_attn_1rowblock<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params,Is_Q_split_K>(const Params &, int, int, int) [with Kernel_traits=Flash_fwd_kernel_traits<224, 64, 64, 4, false, false, cutlass::bfloat16_t, Flash_kernel_traits<224, 64, 64, 4, cutlass::bfloat16_t>>, Is_dropout=true, Is_causal=true, Is_even_N=false, Is_even_K=true, Return_softmax=false, Params=Flash_fwd_params, Is_Q_split_K=false]" 
(589): here
            instantiation of "void flash::compute_attn<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &) [with Kernel_traits=Flash_fwd_kernel_traits<224, 64, 64, 4, false, false, cutlass::bfloat16_t, Flash_kernel_traits<224, 64, 64, 4, cutlass::bfloat16_t>>, Is_dropout=true, Is_causal=true, Is_even_N=false, Is_even_K=true, Return_softmax=false, Params=Flash_fwd_params]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(15): here
            instantiation of "void flash_fwd_kernel<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax>(Flash_fwd_params) [with Kernel_traits=Flash_fwd_kernel_traits<224, 64, 64, 4, false, false, cutlass::bfloat16_t, Flash_kernel_traits<224, 64, 64, 4, cutlass::bfloat16_t>>, Is_dropout=true, Is_causal=true, Is_even_N=false, Is_even_K=true, Return_softmax=false]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(34): here
            instantiation of "void run_flash_fwd<Kernel_traits,Is_dropout,Is_causal>(Flash_fwd_params &, cudaStream_t) [with Kernel_traits=Flash_fwd_kernel_traits<224, 64, 64, 4, false, false, cutlass::bfloat16_t, Flash_kernel_traits<224, 64, 64, 4, cutlass::bfloat16_t>>, Is_dropout=true, Is_causal=true]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(208): here
            instantiation of "void run_mha_fwd_hdim224<T>(Flash_fwd_params &, cudaStream_t) [with T=cutlass::bfloat16_t]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_hdim224_bf16_sm80.cu(8): here

/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_kernel.h(276): warning: variable "tQrQ" was declared but never referenced
          detected during:
            instantiation of "void flash::compute_attn_1rowblock<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params,Is_Q_split_K>(const Params &, int, int, int) [with Kernel_traits=Flash_fwd_kernel_traits<224, 64, 64, 4, false, false, cutlass::bfloat16_t, Flash_kernel_traits<224, 64, 64, 4, cutlass::bfloat16_t>>, Is_dropout=true, Is_causal=true, Is_even_N=false, Is_even_K=true, Return_softmax=false, Params=Flash_fwd_params, Is_Q_split_K=false]" 
(589): here
            instantiation of "void flash::compute_attn<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &) [with Kernel_traits=Flash_fwd_kernel_traits<224, 64, 64, 4, false, false, cutlass::bfloat16_t, Flash_kernel_traits<224, 64, 64, 4, cutlass::bfloat16_t>>, Is_dropout=true, Is_causal=true, Is_even_N=false, Is_even_K=true, Return_softmax=false, Params=Flash_fwd_params]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(15): here
            instantiation of "void flash_fwd_kernel<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax>(Flash_fwd_params) [with Kernel_traits=Flash_fwd_kernel_traits<224, 64, 64, 4, false, false, cutlass::bfloat16_t, Flash_kernel_traits<224, 64, 64, 4, cutlass::bfloat16_t>>, Is_dropout=true, Is_causal=true, Is_even_N=false, Is_even_K=true, Return_softmax=false]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(34): here
            instantiation of "void run_flash_fwd<Kernel_traits,Is_dropout,Is_causal>(Flash_fwd_params &, cudaStream_t) [with Kernel_traits=Flash_fwd_kernel_traits<224, 64, 64, 4, false, false, cutlass::bfloat16_t, Flash_kernel_traits<224, 64, 64, 4, cutlass::bfloat16_t>>, Is_dropout=true, Is_causal=true]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(208): here
            instantiation of "void run_mha_fwd_hdim224<T>(Flash_fwd_params &, cudaStream_t) [with T=cutlass::bfloat16_t]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_hdim224_bf16_sm80.cu(8): here

/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_kernel.h(140): warning: variable "MMA_M" was declared but never referenced
          detected during:
            instantiation of "void flash::compute_attn_1rowblock<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params,Is_Q_split_K>(const Params &, int, int, int) [with Kernel_traits=Flash_fwd_kernel_traits<224, 64, 64, 4, false, false, cutlass::bfloat16_t, Flash_kernel_traits<224, 64, 64, 4, cutlass::bfloat16_t>>, Is_dropout=true, Is_causal=true, Is_even_N=false, Is_even_K=false, Return_softmax=true, Params=Flash_fwd_params, Is_Q_split_K=false]" 
(589): here
            instantiation of "void flash::compute_attn<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &) [with Kernel_traits=Flash_fwd_kernel_traits<224, 64, 64, 4, false, false, cutlass::bfloat16_t, Flash_kernel_traits<224, 64, 64, 4, cutlass::bfloat16_t>>, Is_dropout=true, Is_causal=true, Is_even_N=false, Is_even_K=false, Return_softmax=true, Params=Flash_fwd_params]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(15): here
            instantiation of "void flash_fwd_kernel<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax>(Flash_fwd_params) [with Kernel_traits=Flash_fwd_kernel_traits<224, 64, 64, 4, false, false, cutlass::bfloat16_t, Flash_kernel_traits<224, 64, 64, 4, cutlass::bfloat16_t>>, Is_dropout=true, Is_causal=true, Is_even_N=false, Is_even_K=false, Return_softmax=true]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(34): here
            instantiation of "void run_flash_fwd<Kernel_traits,Is_dropout,Is_causal>(Flash_fwd_params &, cudaStream_t) [with Kernel_traits=Flash_fwd_kernel_traits<224, 64, 64, 4, false, false, cutlass::bfloat16_t, Flash_kernel_traits<224, 64, 64, 4, cutlass::bfloat16_t>>, Is_dropout=true, Is_causal=true]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(208): here
            instantiation of "void run_mha_fwd_hdim224<T>(Flash_fwd_params &, cudaStream_t) [with T=cutlass::bfloat16_t]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_hdim224_bf16_sm80.cu(8): here

/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_kernel.h(276): warning: variable "tQrQ" was declared but never referenced
          detected during:
            instantiation of "void flash::compute_attn_1rowblock<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params,Is_Q_split_K>(const Params &, int, int, int) [with Kernel_traits=Flash_fwd_kernel_traits<224, 64, 64, 4, false, false, cutlass::bfloat16_t, Flash_kernel_traits<224, 64, 64, 4, cutlass::bfloat16_t>>, Is_dropout=true, Is_causal=true, Is_even_N=false, Is_even_K=false, Return_softmax=true, Params=Flash_fwd_params, Is_Q_split_K=false]" 
(589): here
            instantiation of "void flash::compute_attn<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &) [with Kernel_traits=Flash_fwd_kernel_traits<224, 64, 64, 4, false, false, cutlass::bfloat16_t, Flash_kernel_traits<224, 64, 64, 4, cutlass::bfloat16_t>>, Is_dropout=true, Is_causal=true, Is_even_N=false, Is_even_K=false, Return_softmax=true, Params=Flash_fwd_params]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(15): here
            instantiation of "void flash_fwd_kernel<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax>(Flash_fwd_params) [with Kernel_traits=Flash_fwd_kernel_traits<224, 64, 64, 4, false, false, cutlass::bfloat16_t, Flash_kernel_traits<224, 64, 64, 4, cutlass::bfloat16_t>>, Is_dropout=true, Is_causal=true, Is_even_N=false, Is_even_K=false, Return_softmax=true]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(34): here
            instantiation of "void run_flash_fwd<Kernel_traits,Is_dropout,Is_causal>(Flash_fwd_params &, cudaStream_t) [with Kernel_traits=Flash_fwd_kernel_traits<224, 64, 64, 4, false, false, cutlass::bfloat16_t, Flash_kernel_traits<224, 64, 64, 4, cutlass::bfloat16_t>>, Is_dropout=true, Is_causal=true]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(208): here
            instantiation of "void run_mha_fwd_hdim224<T>(Flash_fwd_params &, cudaStream_t) [with T=cutlass::bfloat16_t]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_hdim224_bf16_sm80.cu(8): here

/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_kernel.h(140): warning: variable "MMA_M" was declared but never referenced
          detected during:
            instantiation of "void flash::compute_attn_1rowblock<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params,Is_Q_split_K>(const Params &, int, int, int) [with Kernel_traits=Flash_fwd_kernel_traits<224, 64, 64, 4, false, false, cutlass::bfloat16_t, Flash_kernel_traits<224, 64, 64, 4, cutlass::bfloat16_t>>, Is_dropout=true, Is_causal=true, Is_even_N=false, Is_even_K=false, Return_softmax=false, Params=Flash_fwd_params, Is_Q_split_K=false]" 
(589): here
            instantiation of "void flash::compute_attn<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &) [with Kernel_traits=Flash_fwd_kernel_traits<224, 64, 64, 4, false, false, cutlass::bfloat16_t, Flash_kernel_traits<224, 64, 64, 4, cutlass::bfloat16_t>>, Is_dropout=true, Is_causal=true, Is_even_N=false, Is_even_K=false, Return_softmax=false, Params=Flash_fwd_params]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(15): here
            instantiation of "void flash_fwd_kernel<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax>(Flash_fwd_params) [with Kernel_traits=Flash_fwd_kernel_traits<224, 64, 64, 4, false, false, cutlass::bfloat16_t, Flash_kernel_traits<224, 64, 64, 4, cutlass::bfloat16_t>>, Is_dropout=true, Is_causal=true, Is_even_N=false, Is_even_K=false, Return_softmax=false]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(34): here
            instantiation of "void run_flash_fwd<Kernel_traits,Is_dropout,Is_causal>(Flash_fwd_params &, cudaStream_t) [with Kernel_traits=Flash_fwd_kernel_traits<224, 64, 64, 4, false, false, cutlass::bfloat16_t, Flash_kernel_traits<224, 64, 64, 4, cutlass::bfloat16_t>>, Is_dropout=true, Is_causal=true]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(208): here
            instantiation of "void run_mha_fwd_hdim224<T>(Flash_fwd_params &, cudaStream_t) [with T=cutlass::bfloat16_t]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_hdim224_bf16_sm80.cu(8): here

/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_kernel.h(276): warning: variable "tQrQ" was declared but never referenced
          detected during:
            instantiation of "void flash::compute_attn_1rowblock<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params,Is_Q_split_K>(const Params &, int, int, int) [with Kernel_traits=Flash_fwd_kernel_traits<224, 64, 64, 4, false, false, cutlass::bfloat16_t, Flash_kernel_traits<224, 64, 64, 4, cutlass::bfloat16_t>>, Is_dropout=true, Is_causal=true, Is_even_N=false, Is_even_K=false, Return_softmax=false, Params=Flash_fwd_params, Is_Q_split_K=false]" 
(589): here
            instantiation of "void flash::compute_attn<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &) [with Kernel_traits=Flash_fwd_kernel_traits<224, 64, 64, 4, false, false, cutlass::bfloat16_t, Flash_kernel_traits<224, 64, 64, 4, cutlass::bfloat16_t>>, Is_dropout=true, Is_causal=true, Is_even_N=false, Is_even_K=false, Return_softmax=false, Params=Flash_fwd_params]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(15): here
            instantiation of "void flash_fwd_kernel<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax>(Flash_fwd_params) [with Kernel_traits=Flash_fwd_kernel_traits<224, 64, 64, 4, false, false, cutlass::bfloat16_t, Flash_kernel_traits<224, 64, 64, 4, cutlass::bfloat16_t>>, Is_dropout=true, Is_causal=true, Is_even_N=false, Is_even_K=false, Return_softmax=false]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(34): here
            instantiation of "void run_flash_fwd<Kernel_traits,Is_dropout,Is_causal>(Flash_fwd_params &, cudaStream_t) [with Kernel_traits=Flash_fwd_kernel_traits<224, 64, 64, 4, false, false, cutlass::bfloat16_t, Flash_kernel_traits<224, 64, 64, 4, cutlass::bfloat16_t>>, Is_dropout=true, Is_causal=true]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(208): here
            instantiation of "void run_mha_fwd_hdim224<T>(Flash_fwd_params &, cudaStream_t) [with T=cutlass::bfloat16_t]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_hdim224_bf16_sm80.cu(8): here

/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_kernel.h(140): warning: variable "MMA_M" was declared but never referenced
          detected during:
            instantiation of "void flash::compute_attn_1rowblock<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params,Is_Q_split_K>(const Params &, int, int, int) [with Kernel_traits=Flash_fwd_kernel_traits<224, 128, 64, 8, false, false, cutlass::bfloat16_t, Flash_kernel_traits<224, 128, 64, 8, cutlass::bfloat16_t>>, Is_dropout=true, Is_causal=false, Is_even_N=true, Is_even_K=true, Return_softmax=true, Params=Flash_fwd_params, Is_Q_split_K=false]" 
(589): here
            instantiation of "void flash::compute_attn<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &) [with Kernel_traits=Flash_fwd_kernel_traits<224, 128, 64, 8, false, false, cutlass::bfloat16_t, Flash_kernel_traits<224, 128, 64, 8, cutlass::bfloat16_t>>, Is_dropout=true, Is_causal=false, Is_even_N=true, Is_even_K=true, Return_softmax=true, Params=Flash_fwd_params]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(15): here
            instantiation of "void flash_fwd_kernel<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax>(Flash_fwd_params) [with Kernel_traits=Flash_fwd_kernel_traits<224, 128, 64, 8, false, false, cutlass::bfloat16_t, Flash_kernel_traits<224, 128, 64, 8, cutlass::bfloat16_t>>, Is_dropout=true, Is_causal=false, Is_even_N=true, Is_even_K=true, Return_softmax=true]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(34): here
            instantiation of "void run_flash_fwd<Kernel_traits,Is_dropout,Is_causal>(Flash_fwd_params &, cudaStream_t) [with Kernel_traits=Flash_fwd_kernel_traits<224, 128, 64, 8, false, false, cutlass::bfloat16_t, Flash_kernel_traits<224, 128, 64, 8, cutlass::bfloat16_t>>, Is_dropout=true, Is_causal=false]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(208): here
            instantiation of "void run_mha_fwd_hdim224<T>(Flash_fwd_params &, cudaStream_t) [with T=cutlass::bfloat16_t]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_hdim224_bf16_sm80.cu(8): here

/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_kernel.h(276): warning: variable "tQrQ" was declared but never referenced
          detected during:
            instantiation of "void flash::compute_attn_1rowblock<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params,Is_Q_split_K>(const Params &, int, int, int) [with Kernel_traits=Flash_fwd_kernel_traits<224, 128, 64, 8, false, false, cutlass::bfloat16_t, Flash_kernel_traits<224, 128, 64, 8, cutlass::bfloat16_t>>, Is_dropout=true, Is_causal=false, Is_even_N=true, Is_even_K=true, Return_softmax=true, Params=Flash_fwd_params, Is_Q_split_K=false]" 
(589): here
            instantiation of "void flash::compute_attn<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &) [with Kernel_traits=Flash_fwd_kernel_traits<224, 128, 64, 8, false, false, cutlass::bfloat16_t, Flash_kernel_traits<224, 128, 64, 8, cutlass::bfloat16_t>>, Is_dropout=true, Is_causal=false, Is_even_N=true, Is_even_K=true, Return_softmax=true, Params=Flash_fwd_params]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(15): here
            instantiation of "void flash_fwd_kernel<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax>(Flash_fwd_params) [with Kernel_traits=Flash_fwd_kernel_traits<224, 128, 64, 8, false, false, cutlass::bfloat16_t, Flash_kernel_traits<224, 128, 64, 8, cutlass::bfloat16_t>>, Is_dropout=true, Is_causal=false, Is_even_N=true, Is_even_K=true, Return_softmax=true]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(34): here
            instantiation of "void run_flash_fwd<Kernel_traits,Is_dropout,Is_causal>(Flash_fwd_params &, cudaStream_t) [with Kernel_traits=Flash_fwd_kernel_traits<224, 128, 64, 8, false, false, cutlass::bfloat16_t, Flash_kernel_traits<224, 128, 64, 8, cutlass::bfloat16_t>>, Is_dropout=true, Is_causal=false]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(208): here
            instantiation of "void run_mha_fwd_hdim224<T>(Flash_fwd_params &, cudaStream_t) [with T=cutlass::bfloat16_t]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_hdim224_bf16_sm80.cu(8): here

/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_kernel.h(140): warning: variable "MMA_M" was declared but never referenced
          detected during:
            instantiation of "void flash::compute_attn_1rowblock<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params,Is_Q_split_K>(const Params &, int, int, int) [with Kernel_traits=Flash_fwd_kernel_traits<224, 128, 64, 8, false, false, cutlass::bfloat16_t, Flash_kernel_traits<224, 128, 64, 8, cutlass::bfloat16_t>>, Is_dropout=true, Is_causal=false, Is_even_N=true, Is_even_K=true, Return_softmax=false, Params=Flash_fwd_params, Is_Q_split_K=false]" 
(589): here
            instantiation of "void flash::compute_attn<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &) [with Kernel_traits=Flash_fwd_kernel_traits<224, 128, 64, 8, false, false, cutlass::bfloat16_t, Flash_kernel_traits<224, 128, 64, 8, cutlass::bfloat16_t>>, Is_dropout=true, Is_causal=false, Is_even_N=true, Is_even_K=true, Return_softmax=false, Params=Flash_fwd_params]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(15): here
            instantiation of "void flash_fwd_kernel<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax>(Flash_fwd_params) [with Kernel_traits=Flash_fwd_kernel_traits<224, 128, 64, 8, false, false, cutlass::bfloat16_t, Flash_kernel_traits<224, 128, 64, 8, cutlass::bfloat16_t>>, Is_dropout=true, Is_causal=false, Is_even_N=true, Is_even_K=true, Return_softmax=false]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(34): here
            instantiation of "void run_flash_fwd<Kernel_traits,Is_dropout,Is_causal>(Flash_fwd_params &, cudaStream_t) [with Kernel_traits=Flash_fwd_kernel_traits<224, 128, 64, 8, false, false, cutlass::bfloat16_t, Flash_kernel_traits<224, 128, 64, 8, cutlass::bfloat16_t>>, Is_dropout=true, Is_causal=false]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(208): here
            instantiation of "void run_mha_fwd_hdim224<T>(Flash_fwd_params &, cudaStream_t) [with T=cutlass::bfloat16_t]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_hdim224_bf16_sm80.cu(8): here

/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_kernel.h(276): warning: variable "tQrQ" was declared but never referenced
          detected during:
            instantiation of "void flash::compute_attn_1rowblock<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params,Is_Q_split_K>(const Params &, int, int, int) [with Kernel_traits=Flash_fwd_kernel_traits<224, 128, 64, 8, false, false, cutlass::bfloat16_t, Flash_kernel_traits<224, 128, 64, 8, cutlass::bfloat16_t>>, Is_dropout=true, Is_causal=false, Is_even_N=true, Is_even_K=true, Return_softmax=false, Params=Flash_fwd_params, Is_Q_split_K=false]" 
(589): here
            instantiation of "void flash::compute_attn<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &) [with Kernel_traits=Flash_fwd_kernel_traits<224, 128, 64, 8, false, false, cutlass::bfloat16_t, Flash_kernel_traits<224, 128, 64, 8, cutlass::bfloat16_t>>, Is_dropout=true, Is_causal=false, Is_even_N=true, Is_even_K=true, Return_softmax=false, Params=Flash_fwd_params]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(15): here
            instantiation of "void flash_fwd_kernel<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax>(Flash_fwd_params) [with Kernel_traits=Flash_fwd_kernel_traits<224, 128, 64, 8, false, false, cutlass::bfloat16_t, Flash_kernel_traits<224, 128, 64, 8, cutlass::bfloat16_t>>, Is_dropout=true, Is_causal=false, Is_even_N=true, Is_even_K=true, Return_softmax=false]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(34): here
            instantiation of "void run_flash_fwd<Kernel_traits,Is_dropout,Is_causal>(Flash_fwd_params &, cudaStream_t) [with Kernel_traits=Flash_fwd_kernel_traits<224, 128, 64, 8, false, false, cutlass::bfloat16_t, Flash_kernel_traits<224, 128, 64, 8, cutlass::bfloat16_t>>, Is_dropout=true, Is_causal=false]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(208): here
            instantiation of "void run_mha_fwd_hdim224<T>(Flash_fwd_params &, cudaStream_t) [with T=cutlass::bfloat16_t]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_hdim224_bf16_sm80.cu(8): here

/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_kernel.h(140): warning: variable "MMA_M" was declared but never referenced
          detected during:
            instantiation of "void flash::compute_attn_1rowblock<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params,Is_Q_split_K>(const Params &, int, int, int) [with Kernel_traits=Flash_fwd_kernel_traits<224, 128, 64, 8, false, false, cutlass::bfloat16_t, Flash_kernel_traits<224, 128, 64, 8, cutlass::bfloat16_t>>, Is_dropout=true, Is_causal=false, Is_even_N=true, Is_even_K=false, Return_softmax=true, Params=Flash_fwd_params, Is_Q_split_K=false]" 
(589): here
            instantiation of "void flash::compute_attn<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &) [with Kernel_traits=Flash_fwd_kernel_traits<224, 128, 64, 8, false, false, cutlass::bfloat16_t, Flash_kernel_traits<224, 128, 64, 8, cutlass::bfloat16_t>>, Is_dropout=true, Is_causal=false, Is_even_N=true, Is_even_K=false, Return_softmax=true, Params=Flash_fwd_params]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(15): here
            instantiation of "void flash_fwd_kernel<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax>(Flash_fwd_params) [with Kernel_traits=Flash_fwd_kernel_traits<224, 128, 64, 8, false, false, cutlass::bfloat16_t, Flash_kernel_traits<224, 128, 64, 8, cutlass::bfloat16_t>>, Is_dropout=true, Is_causal=false, Is_even_N=true, Is_even_K=false, Return_softmax=true]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(34): here
            instantiation of "void run_flash_fwd<Kernel_traits,Is_dropout,Is_causal>(Flash_fwd_params &, cudaStream_t) [with Kernel_traits=Flash_fwd_kernel_traits<224, 128, 64, 8, false, false, cutlass::bfloat16_t, Flash_kernel_traits<224, 128, 64, 8, cutlass::bfloat16_t>>, Is_dropout=true, Is_causal=false]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(208): here
            instantiation of "void run_mha_fwd_hdim224<T>(Flash_fwd_params &, cudaStream_t) [with T=cutlass::bfloat16_t]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_hdim224_bf16_sm80.cu(8): here

/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_kernel.h(276): warning: variable "tQrQ" was declared but never referenced
          detected during:
            instantiation of "void flash::compute_attn_1rowblock<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params,Is_Q_split_K>(const Params &, int, int, int) [with Kernel_traits=Flash_fwd_kernel_traits<224, 128, 64, 8, false, false, cutlass::bfloat16_t, Flash_kernel_traits<224, 128, 64, 8, cutlass::bfloat16_t>>, Is_dropout=true, Is_causal=false, Is_even_N=true, Is_even_K=false, Return_softmax=true, Params=Flash_fwd_params, Is_Q_split_K=false]" 
(589): here
            instantiation of "void flash::compute_attn<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &) [with Kernel_traits=Flash_fwd_kernel_traits<224, 128, 64, 8, false, false, cutlass::bfloat16_t, Flash_kernel_traits<224, 128, 64, 8, cutlass::bfloat16_t>>, Is_dropout=true, Is_causal=false, Is_even_N=true, Is_even_K=false, Return_softmax=true, Params=Flash_fwd_params]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(15): here
            instantiation of "void flash_fwd_kernel<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax>(Flash_fwd_params) [with Kernel_traits=Flash_fwd_kernel_traits<224, 128, 64, 8, false, false, cutlass::bfloat16_t, Flash_kernel_traits<224, 128, 64, 8, cutlass::bfloat16_t>>, Is_dropout=true, Is_causal=false, Is_even_N=true, Is_even_K=false, Return_softmax=true]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(34): here
            instantiation of "void run_flash_fwd<Kernel_traits,Is_dropout,Is_causal>(Flash_fwd_params &, cudaStream_t) [with Kernel_traits=Flash_fwd_kernel_traits<224, 128, 64, 8, false, false, cutlass::bfloat16_t, Flash_kernel_traits<224, 128, 64, 8, cutlass::bfloat16_t>>, Is_dropout=true, Is_causal=false]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(208): here
            instantiation of "void run_mha_fwd_hdim224<T>(Flash_fwd_params &, cudaStream_t) [with T=cutlass::bfloat16_t]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_hdim224_bf16_sm80.cu(8): here

/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_kernel.h(140): warning: variable "MMA_M" was declared but never referenced
          detected during:
            instantiation of "void flash::compute_attn_1rowblock<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params,Is_Q_split_K>(const Params &, int, int, int) [with Kernel_traits=Flash_fwd_kernel_traits<224, 128, 64, 8, false, false, cutlass::bfloat16_t, Flash_kernel_traits<224, 128, 64, 8, cutlass::bfloat16_t>>, Is_dropout=true, Is_causal=false, Is_even_N=true, Is_even_K=false, Return_softmax=false, Params=Flash_fwd_params, Is_Q_split_K=false]" 
(589): here
            instantiation of "void flash::compute_attn<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &) [with Kernel_traits=Flash_fwd_kernel_traits<224, 128, 64, 8, false, false, cutlass::bfloat16_t, Flash_kernel_traits<224, 128, 64, 8, cutlass::bfloat16_t>>, Is_dropout=true, Is_causal=false, Is_even_N=true, Is_even_K=false, Return_softmax=false, Params=Flash_fwd_params]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(15): here
            instantiation of "void flash_fwd_kernel<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax>(Flash_fwd_params) [with Kernel_traits=Flash_fwd_kernel_traits<224, 128, 64, 8, false, false, cutlass::bfloat16_t, Flash_kernel_traits<224, 128, 64, 8, cutlass::bfloat16_t>>, Is_dropout=true, Is_causal=false, Is_even_N=true, Is_even_K=false, Return_softmax=false]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(34): here
            instantiation of "void run_flash_fwd<Kernel_traits,Is_dropout,Is_causal>(Flash_fwd_params &, cudaStream_t) [with Kernel_traits=Flash_fwd_kernel_traits<224, 128, 64, 8, false, false, cutlass::bfloat16_t, Flash_kernel_traits<224, 128, 64, 8, cutlass::bfloat16_t>>, Is_dropout=true, Is_causal=false]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(208): here
            instantiation of "void run_mha_fwd_hdim224<T>(Flash_fwd_params &, cudaStream_t) [with T=cutlass::bfloat16_t]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_hdim224_bf16_sm80.cu(8): here

/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_kernel.h(276): warning: variable "tQrQ" was declared but never referenced
          detected during:
            instantiation of "void flash::compute_attn_1rowblock<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params,Is_Q_split_K>(const Params &, int, int, int) [with Kernel_traits=Flash_fwd_kernel_traits<224, 128, 64, 8, false, false, cutlass::bfloat16_t, Flash_kernel_traits<224, 128, 64, 8, cutlass::bfloat16_t>>, Is_dropout=true, Is_causal=false, Is_even_N=true, Is_even_K=false, Return_softmax=false, Params=Flash_fwd_params, Is_Q_split_K=false]" 
(589): here
            instantiation of "void flash::compute_attn<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &) [with Kernel_traits=Flash_fwd_kernel_traits<224, 128, 64, 8, false, false, cutlass::bfloat16_t, Flash_kernel_traits<224, 128, 64, 8, cutlass::bfloat16_t>>, Is_dropout=true, Is_causal=false, Is_even_N=true, Is_even_K=false, Return_softmax=false, Params=Flash_fwd_params]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(15): here
            instantiation of "void flash_fwd_kernel<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax>(Flash_fwd_params) [with Kernel_traits=Flash_fwd_kernel_traits<224, 128, 64, 8, false, false, cutlass::bfloat16_t, Flash_kernel_traits<224, 128, 64, 8, cutlass::bfloat16_t>>, Is_dropout=true, Is_causal=false, Is_even_N=true, Is_even_K=false, Return_softmax=false]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(34): here
            instantiation of "void run_flash_fwd<Kernel_traits,Is_dropout,Is_causal>(Flash_fwd_params &, cudaStream_t) [with Kernel_traits=Flash_fwd_kernel_traits<224, 128, 64, 8, false, false, cutlass::bfloat16_t, Flash_kernel_traits<224, 128, 64, 8, cutlass::bfloat16_t>>, Is_dropout=true, Is_causal=false]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(208): here
            instantiation of "void run_mha_fwd_hdim224<T>(Flash_fwd_params &, cudaStream_t) [with T=cutlass::bfloat16_t]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_hdim224_bf16_sm80.cu(8): here

/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_kernel.h(140): warning: variable "MMA_M" was declared but never referenced
          detected during:
            instantiation of "void flash::compute_attn_1rowblock<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params,Is_Q_split_K>(const Params &, int, int, int) [with Kernel_traits=Flash_fwd_kernel_traits<224, 128, 64, 8, false, false, cutlass::bfloat16_t, Flash_kernel_traits<224, 128, 64, 8, cutlass::bfloat16_t>>, Is_dropout=true, Is_causal=false, Is_even_N=false, Is_even_K=true, Return_softmax=true, Params=Flash_fwd_params, Is_Q_split_K=false]" 
(589): here
            instantiation of "void flash::compute_attn<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &) [with Kernel_traits=Flash_fwd_kernel_traits<224, 128, 64, 8, false, false, cutlass::bfloat16_t, Flash_kernel_traits<224, 128, 64, 8, cutlass::bfloat16_t>>, Is_dropout=true, Is_causal=false, Is_even_N=false, Is_even_K=true, Return_softmax=true, Params=Flash_fwd_params]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(15): here
            instantiation of "void flash_fwd_kernel<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax>(Flash_fwd_params) [with Kernel_traits=Flash_fwd_kernel_traits<224, 128, 64, 8, false, false, cutlass::bfloat16_t, Flash_kernel_traits<224, 128, 64, 8, cutlass::bfloat16_t>>, Is_dropout=true, Is_causal=false, Is_even_N=false, Is_even_K=true, Return_softmax=true]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(34): here
            instantiation of "void run_flash_fwd<Kernel_traits,Is_dropout,Is_causal>(Flash_fwd_params &, cudaStream_t) [with Kernel_traits=Flash_fwd_kernel_traits<224, 128, 64, 8, false, false, cutlass::bfloat16_t, Flash_kernel_traits<224, 128, 64, 8, cutlass::bfloat16_t>>, Is_dropout=true, Is_causal=false]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(208): here
            instantiation of "void run_mha_fwd_hdim224<T>(Flash_fwd_params &, cudaStream_t) [with T=cutlass::bfloat16_t]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_hdim224_bf16_sm80.cu(8): here

/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_kernel.h(276): warning: variable "tQrQ" was declared but never referenced
          detected during:
            instantiation of "void flash::compute_attn_1rowblock<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params,Is_Q_split_K>(const Params &, int, int, int) [with Kernel_traits=Flash_fwd_kernel_traits<224, 128, 64, 8, false, false, cutlass::bfloat16_t, Flash_kernel_traits<224, 128, 64, 8, cutlass::bfloat16_t>>, Is_dropout=true, Is_causal=false, Is_even_N=false, Is_even_K=true, Return_softmax=true, Params=Flash_fwd_params, Is_Q_split_K=false]" 
(589): here
            instantiation of "void flash::compute_attn<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &) [with Kernel_traits=Flash_fwd_kernel_traits<224, 128, 64, 8, false, false, cutlass::bfloat16_t, Flash_kernel_traits<224, 128, 64, 8, cutlass::bfloat16_t>>, Is_dropout=true, Is_causal=false, Is_even_N=false, Is_even_K=true, Return_softmax=true, Params=Flash_fwd_params]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(15): here
            instantiation of "void flash_fwd_kernel<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax>(Flash_fwd_params) [with Kernel_traits=Flash_fwd_kernel_traits<224, 128, 64, 8, false, false, cutlass::bfloat16_t, Flash_kernel_traits<224, 128, 64, 8, cutlass::bfloat16_t>>, Is_dropout=true, Is_causal=false, Is_even_N=false, Is_even_K=true, Return_softmax=true]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(34): here
            instantiation of "void run_flash_fwd<Kernel_traits,Is_dropout,Is_causal>(Flash_fwd_params &, cudaStream_t) [with Kernel_traits=Flash_fwd_kernel_traits<224, 128, 64, 8, false, false, cutlass::bfloat16_t, Flash_kernel_traits<224, 128, 64, 8, cutlass::bfloat16_t>>, Is_dropout=true, Is_causal=false]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(208): here
            instantiation of "void run_mha_fwd_hdim224<T>(Flash_fwd_params &, cudaStream_t) [with T=cutlass::bfloat16_t]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_hdim224_bf16_sm80.cu(8): here

/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_kernel.h(140): warning: variable "MMA_M" was declared but never referenced
          detected during:
            instantiation of "void flash::compute_attn_1rowblock<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params,Is_Q_split_K>(const Params &, int, int, int) [with Kernel_traits=Flash_fwd_kernel_traits<224, 128, 64, 8, false, false, cutlass::bfloat16_t, Flash_kernel_traits<224, 128, 64, 8, cutlass::bfloat16_t>>, Is_dropout=true, Is_causal=false, Is_even_N=false, Is_even_K=true, Return_softmax=false, Params=Flash_fwd_params, Is_Q_split_K=false]" 
(589): here
            instantiation of "void flash::compute_attn<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &) [with Kernel_traits=Flash_fwd_kernel_traits<224, 128, 64, 8, false, false, cutlass::bfloat16_t, Flash_kernel_traits<224, 128, 64, 8, cutlass::bfloat16_t>>, Is_dropout=true, Is_causal=false, Is_even_N=false, Is_even_K=true, Return_softmax=false, Params=Flash_fwd_params]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(15): here
            instantiation of "void flash_fwd_kernel<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax>(Flash_fwd_params) [with Kernel_traits=Flash_fwd_kernel_traits<224, 128, 64, 8, false, false, cutlass::bfloat16_t, Flash_kernel_traits<224, 128, 64, 8, cutlass::bfloat16_t>>, Is_dropout=true, Is_causal=false, Is_even_N=false, Is_even_K=true, Return_softmax=false]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(34): here
            instantiation of "void run_flash_fwd<Kernel_traits,Is_dropout,Is_causal>(Flash_fwd_params &, cudaStream_t) [with Kernel_traits=Flash_fwd_kernel_traits<224, 128, 64, 8, false, false, cutlass::bfloat16_t, Flash_kernel_traits<224, 128, 64, 8, cutlass::bfloat16_t>>, Is_dropout=true, Is_causal=false]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(208): here
            instantiation of "void run_mha_fwd_hdim224<T>(Flash_fwd_params &, cudaStream_t) [with T=cutlass::bfloat16_t]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_hdim224_bf16_sm80.cu(8): here

/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_kernel.h(276): warning: variable "tQrQ" was declared but never referenced
          detected during:
            instantiation of "void flash::compute_attn_1rowblock<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params,Is_Q_split_K>(const Params &, int, int, int) [with Kernel_traits=Flash_fwd_kernel_traits<224, 128, 64, 8, false, false, cutlass::bfloat16_t, Flash_kernel_traits<224, 128, 64, 8, cutlass::bfloat16_t>>, Is_dropout=true, Is_causal=false, Is_even_N=false, Is_even_K=true, Return_softmax=false, Params=Flash_fwd_params, Is_Q_split_K=false]" 
(589): here
            instantiation of "void flash::compute_attn<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &) [with Kernel_traits=Flash_fwd_kernel_traits<224, 128, 64, 8, false, false, cutlass::bfloat16_t, Flash_kernel_traits<224, 128, 64, 8, cutlass::bfloat16_t>>, Is_dropout=true, Is_causal=false, Is_even_N=false, Is_even_K=true, Return_softmax=false, Params=Flash_fwd_params]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(15): here
            instantiation of "void flash_fwd_kernel<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax>(Flash_fwd_params) [with Kernel_traits=Flash_fwd_kernel_traits<224, 128, 64, 8, false, false, cutlass::bfloat16_t, Flash_kernel_traits<224, 128, 64, 8, cutlass::bfloat16_t>>, Is_dropout=true, Is_causal=false, Is_even_N=false, Is_even_K=true, Return_softmax=false]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(34): here
            instantiation of "void run_flash_fwd<Kernel_traits,Is_dropout,Is_causal>(Flash_fwd_params &, cudaStream_t) [with Kernel_traits=Flash_fwd_kernel_traits<224, 128, 64, 8, false, false, cutlass::bfloat16_t, Flash_kernel_traits<224, 128, 64, 8, cutlass::bfloat16_t>>, Is_dropout=true, Is_causal=false]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(208): here
            instantiation of "void run_mha_fwd_hdim224<T>(Flash_fwd_params &, cudaStream_t) [with T=cutlass::bfloat16_t]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_hdim224_bf16_sm80.cu(8): here

/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_kernel.h(140): warning: variable "MMA_M" was declared but never referenced
          detected during:
            instantiation of "void flash::compute_attn_1rowblock<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params,Is_Q_split_K>(const Params &, int, int, int) [with Kernel_traits=Flash_fwd_kernel_traits<224, 128, 64, 8, false, false, cutlass::bfloat16_t, Flash_kernel_traits<224, 128, 64, 8, cutlass::bfloat16_t>>, Is_dropout=true, Is_causal=false, Is_even_N=false, Is_even_K=false, Return_softmax=true, Params=Flash_fwd_params, Is_Q_split_K=false]" 
(589): here
            instantiation of "void flash::compute_attn<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &) [with Kernel_traits=Flash_fwd_kernel_traits<224, 128, 64, 8, false, false, cutlass::bfloat16_t, Flash_kernel_traits<224, 128, 64, 8, cutlass::bfloat16_t>>, Is_dropout=true, Is_causal=false, Is_even_N=false, Is_even_K=false, Return_softmax=true, Params=Flash_fwd_params]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(15): here
            instantiation of "void flash_fwd_kernel<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax>(Flash_fwd_params) [with Kernel_traits=Flash_fwd_kernel_traits<224, 128, 64, 8, false, false, cutlass::bfloat16_t, Flash_kernel_traits<224, 128, 64, 8, cutlass::bfloat16_t>>, Is_dropout=true, Is_causal=false, Is_even_N=false, Is_even_K=false, Return_softmax=true]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(34): here
            instantiation of "void run_flash_fwd<Kernel_traits,Is_dropout,Is_causal>(Flash_fwd_params &, cudaStream_t) [with Kernel_traits=Flash_fwd_kernel_traits<224, 128, 64, 8, false, false, cutlass::bfloat16_t, Flash_kernel_traits<224, 128, 64, 8, cutlass::bfloat16_t>>, Is_dropout=true, Is_causal=false]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(208): here
            instantiation of "void run_mha_fwd_hdim224<T>(Flash_fwd_params &, cudaStream_t) [with T=cutlass::bfloat16_t]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_hdim224_bf16_sm80.cu(8): here

/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_kernel.h(276): warning: variable "tQrQ" was declared but never referenced
          detected during:
            instantiation of "void flash::compute_attn_1rowblock<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params,Is_Q_split_K>(const Params &, int, int, int) [with Kernel_traits=Flash_fwd_kernel_traits<224, 128, 64, 8, false, false, cutlass::bfloat16_t, Flash_kernel_traits<224, 128, 64, 8, cutlass::bfloat16_t>>, Is_dropout=true, Is_causal=false, Is_even_N=false, Is_even_K=false, Return_softmax=true, Params=Flash_fwd_params, Is_Q_split_K=false]" 
(589): here
            instantiation of "void flash::compute_attn<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &) [with Kernel_traits=Flash_fwd_kernel_traits<224, 128, 64, 8, false, false, cutlass::bfloat16_t, Flash_kernel_traits<224, 128, 64, 8, cutlass::bfloat16_t>>, Is_dropout=true, Is_causal=false, Is_even_N=false, Is_even_K=false, Return_softmax=true, Params=Flash_fwd_params]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(15): here
            instantiation of "void flash_fwd_kernel<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax>(Flash_fwd_params) [with Kernel_traits=Flash_fwd_kernel_traits<224, 128, 64, 8, false, false, cutlass::bfloat16_t, Flash_kernel_traits<224, 128, 64, 8, cutlass::bfloat16_t>>, Is_dropout=true, Is_causal=false, Is_even_N=false, Is_even_K=false, Return_softmax=true]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(34): here
            instantiation of "void run_flash_fwd<Kernel_traits,Is_dropout,Is_causal>(Flash_fwd_params &, cudaStream_t) [with Kernel_traits=Flash_fwd_kernel_traits<224, 128, 64, 8, false, false, cutlass::bfloat16_t, Flash_kernel_traits<224, 128, 64, 8, cutlass::bfloat16_t>>, Is_dropout=true, Is_causal=false]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(208): here
            instantiation of "void run_mha_fwd_hdim224<T>(Flash_fwd_params &, cudaStream_t) [with T=cutlass::bfloat16_t]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_hdim224_bf16_sm80.cu(8): here

/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_kernel.h(140): warning: variable "MMA_M" was declared but never referenced
          detected during:
            instantiation of "void flash::compute_attn_1rowblock<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params,Is_Q_split_K>(const Params &, int, int, int) [with Kernel_traits=Flash_fwd_kernel_traits<224, 128, 64, 8, false, false, cutlass::bfloat16_t, Flash_kernel_traits<224, 128, 64, 8, cutlass::bfloat16_t>>, Is_dropout=true, Is_causal=false, Is_even_N=false, Is_even_K=false, Return_softmax=false, Params=Flash_fwd_params, Is_Q_split_K=false]" 
(589): here
            instantiation of "void flash::compute_attn<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &) [with Kernel_traits=Flash_fwd_kernel_traits<224, 128, 64, 8, false, false, cutlass::bfloat16_t, Flash_kernel_traits<224, 128, 64, 8, cutlass::bfloat16_t>>, Is_dropout=true, Is_causal=false, Is_even_N=false, Is_even_K=false, Return_softmax=false, Params=Flash_fwd_params]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(15): here
            instantiation of "void flash_fwd_kernel<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax>(Flash_fwd_params) [with Kernel_traits=Flash_fwd_kernel_traits<224, 128, 64, 8, false, false, cutlass::bfloat16_t, Flash_kernel_traits<224, 128, 64, 8, cutlass::bfloat16_t>>, Is_dropout=true, Is_causal=false, Is_even_N=false, Is_even_K=false, Return_softmax=false]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(34): here
            instantiation of "void run_flash_fwd<Kernel_traits,Is_dropout,Is_causal>(Flash_fwd_params &, cudaStream_t) [with Kernel_traits=Flash_fwd_kernel_traits<224, 128, 64, 8, false, false, cutlass::bfloat16_t, Flash_kernel_traits<224, 128, 64, 8, cutlass::bfloat16_t>>, Is_dropout=true, Is_causal=false]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(208): here
            instantiation of "void run_mha_fwd_hdim224<T>(Flash_fwd_params &, cudaStream_t) [with T=cutlass::bfloat16_t]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_hdim224_bf16_sm80.cu(8): here

/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_kernel.h(276): warning: variable "tQrQ" was declared but never referenced
          detected during:
            instantiation of "void flash::compute_attn_1rowblock<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params,Is_Q_split_K>(const Params &, int, int, int) [with Kernel_traits=Flash_fwd_kernel_traits<224, 128, 64, 8, false, false, cutlass::bfloat16_t, Flash_kernel_traits<224, 128, 64, 8, cutlass::bfloat16_t>>, Is_dropout=true, Is_causal=false, Is_even_N=false, Is_even_K=false, Return_softmax=false, Params=Flash_fwd_params, Is_Q_split_K=false]" 
(589): here
            instantiation of "void flash::compute_attn<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &) [with Kernel_traits=Flash_fwd_kernel_traits<224, 128, 64, 8, false, false, cutlass::bfloat16_t, Flash_kernel_traits<224, 128, 64, 8, cutlass::bfloat16_t>>, Is_dropout=true, Is_causal=false, Is_even_N=false, Is_even_K=false, Return_softmax=false, Params=Flash_fwd_params]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(15): here
            instantiation of "void flash_fwd_kernel<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax>(Flash_fwd_params) [with Kernel_traits=Flash_fwd_kernel_traits<224, 128, 64, 8, false, false, cutlass::bfloat16_t, Flash_kernel_traits<224, 128, 64, 8, cutlass::bfloat16_t>>, Is_dropout=true, Is_causal=false, Is_even_N=false, Is_even_K=false, Return_softmax=false]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(34): here
            instantiation of "void run_flash_fwd<Kernel_traits,Is_dropout,Is_causal>(Flash_fwd_params &, cudaStream_t) [with Kernel_traits=Flash_fwd_kernel_traits<224, 128, 64, 8, false, false, cutlass::bfloat16_t, Flash_kernel_traits<224, 128, 64, 8, cutlass::bfloat16_t>>, Is_dropout=true, Is_causal=false]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(208): here
            instantiation of "void run_mha_fwd_hdim224<T>(Flash_fwd_params &, cudaStream_t) [with T=cutlass::bfloat16_t]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_hdim224_bf16_sm80.cu(8): here

/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_kernel.h(140): warning: variable "MMA_M" was declared but never referenced
          detected during:
            instantiation of "void flash::compute_attn_1rowblock<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params,Is_Q_split_K>(const Params &, int, int, int) [with Kernel_traits=Flash_fwd_kernel_traits<224, 64, 64, 4, false, false, cutlass::bfloat16_t, Flash_kernel_traits<224, 64, 64, 4, cutlass::bfloat16_t>>, Is_dropout=true, Is_causal=false, Is_even_N=true, Is_even_K=true, Return_softmax=true, Params=Flash_fwd_params, Is_Q_split_K=false]" 
(589): here
            instantiation of "void flash::compute_attn<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &) [with Kernel_traits=Flash_fwd_kernel_traits<224, 64, 64, 4, false, false, cutlass::bfloat16_t, Flash_kernel_traits<224, 64, 64, 4, cutlass::bfloat16_t>>, Is_dropout=true, Is_causal=false, Is_even_N=true, Is_even_K=true, Return_softmax=true, Params=Flash_fwd_params]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(15): here
            instantiation of "void flash_fwd_kernel<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax>(Flash_fwd_params) [with Kernel_traits=Flash_fwd_kernel_traits<224, 64, 64, 4, false, false, cutlass::bfloat16_t, Flash_kernel_traits<224, 64, 64, 4, cutlass::bfloat16_t>>, Is_dropout=true, Is_causal=false, Is_even_N=true, Is_even_K=true, Return_softmax=true]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(34): here
            instantiation of "void run_flash_fwd<Kernel_traits,Is_dropout,Is_causal>(Flash_fwd_params &, cudaStream_t) [with Kernel_traits=Flash_fwd_kernel_traits<224, 64, 64, 4, false, false, cutlass::bfloat16_t, Flash_kernel_traits<224, 64, 64, 4, cutlass::bfloat16_t>>, Is_dropout=true, Is_causal=false]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(208): here
            instantiation of "void run_mha_fwd_hdim224<T>(Flash_fwd_params &, cudaStream_t) [with T=cutlass::bfloat16_t]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_hdim224_bf16_sm80.cu(8): here

/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_kernel.h(276): warning: variable "tQrQ" was declared but never referenced
          detected during:
            instantiation of "void flash::compute_attn_1rowblock<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params,Is_Q_split_K>(const Params &, int, int, int) [with Kernel_traits=Flash_fwd_kernel_traits<224, 64, 64, 4, false, false, cutlass::bfloat16_t, Flash_kernel_traits<224, 64, 64, 4, cutlass::bfloat16_t>>, Is_dropout=true, Is_causal=false, Is_even_N=true, Is_even_K=true, Return_softmax=true, Params=Flash_fwd_params, Is_Q_split_K=false]" 
(589): here
            instantiation of "void flash::compute_attn<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &) [with Kernel_traits=Flash_fwd_kernel_traits<224, 64, 64, 4, false, false, cutlass::bfloat16_t, Flash_kernel_traits<224, 64, 64, 4, cutlass::bfloat16_t>>, Is_dropout=true, Is_causal=false, Is_even_N=true, Is_even_K=true, Return_softmax=true, Params=Flash_fwd_params]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(15): here
            instantiation of "void flash_fwd_kernel<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax>(Flash_fwd_params) [with Kernel_traits=Flash_fwd_kernel_traits<224, 64, 64, 4, false, false, cutlass::bfloat16_t, Flash_kernel_traits<224, 64, 64, 4, cutlass::bfloat16_t>>, Is_dropout=true, Is_causal=false, Is_even_N=true, Is_even_K=true, Return_softmax=true]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(34): here
            instantiation of "void run_flash_fwd<Kernel_traits,Is_dropout,Is_causal>(Flash_fwd_params &, cudaStream_t) [with Kernel_traits=Flash_fwd_kernel_traits<224, 64, 64, 4, false, false, cutlass::bfloat16_t, Flash_kernel_traits<224, 64, 64, 4, cutlass::bfloat16_t>>, Is_dropout=true, Is_causal=false]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(208): here
            instantiation of "void run_mha_fwd_hdim224<T>(Flash_fwd_params &, cudaStream_t) [with T=cutlass::bfloat16_t]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_hdim224_bf16_sm80.cu(8): here

/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_kernel.h(140): warning: variable "MMA_M" was declared but never referenced
          detected during:
            instantiation of "void flash::compute_attn_1rowblock<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params,Is_Q_split_K>(const Params &, int, int, int) [with Kernel_traits=Flash_fwd_kernel_traits<224, 64, 64, 4, false, false, cutlass::bfloat16_t, Flash_kernel_traits<224, 64, 64, 4, cutlass::bfloat16_t>>, Is_dropout=true, Is_causal=false, Is_even_N=true, Is_even_K=true, Return_softmax=false, Params=Flash_fwd_params, Is_Q_split_K=false]" 
(589): here
            instantiation of "void flash::compute_attn<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &) [with Kernel_traits=Flash_fwd_kernel_traits<224, 64, 64, 4, false, false, cutlass::bfloat16_t, Flash_kernel_traits<224, 64, 64, 4, cutlass::bfloat16_t>>, Is_dropout=true, Is_causal=false, Is_even_N=true, Is_even_K=true, Return_softmax=false, Params=Flash_fwd_params]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(15): here
            instantiation of "void flash_fwd_kernel<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax>(Flash_fwd_params) [with Kernel_traits=Flash_fwd_kernel_traits<224, 64, 64, 4, false, false, cutlass::bfloat16_t, Flash_kernel_traits<224, 64, 64, 4, cutlass::bfloat16_t>>, Is_dropout=true, Is_causal=false, Is_even_N=true, Is_even_K=true, Return_softmax=false]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(34): here
            instantiation of "void run_flash_fwd<Kernel_traits,Is_dropout,Is_causal>(Flash_fwd_params &, cudaStream_t) [with Kernel_traits=Flash_fwd_kernel_traits<224, 64, 64, 4, false, false, cutlass::bfloat16_t, Flash_kernel_traits<224, 64, 64, 4, cutlass::bfloat16_t>>, Is_dropout=true, Is_causal=false]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(208): here
            instantiation of "void run_mha_fwd_hdim224<T>(Flash_fwd_params &, cudaStream_t) [with T=cutlass::bfloat16_t]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_hdim224_bf16_sm80.cu(8): here

/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_kernel.h(276): warning: variable "tQrQ" was declared but never referenced
          detected during:
            instantiation of "void flash::compute_attn_1rowblock<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params,Is_Q_split_K>(const Params &, int, int, int) [with Kernel_traits=Flash_fwd_kernel_traits<224, 64, 64, 4, false, false, cutlass::bfloat16_t, Flash_kernel_traits<224, 64, 64, 4, cutlass::bfloat16_t>>, Is_dropout=true, Is_causal=false, Is_even_N=true, Is_even_K=true, Return_softmax=false, Params=Flash_fwd_params, Is_Q_split_K=false]" 
(589): here
            instantiation of "void flash::compute_attn<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &) [with Kernel_traits=Flash_fwd_kernel_traits<224, 64, 64, 4, false, false, cutlass::bfloat16_t, Flash_kernel_traits<224, 64, 64, 4, cutlass::bfloat16_t>>, Is_dropout=true, Is_causal=false, Is_even_N=true, Is_even_K=true, Return_softmax=false, Params=Flash_fwd_params]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(15): here
            instantiation of "void flash_fwd_kernel<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax>(Flash_fwd_params) [with Kernel_traits=Flash_fwd_kernel_traits<224, 64, 64, 4, false, false, cutlass::bfloat16_t, Flash_kernel_traits<224, 64, 64, 4, cutlass::bfloat16_t>>, Is_dropout=true, Is_causal=false, Is_even_N=true, Is_even_K=true, Return_softmax=false]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(34): here
            instantiation of "void run_flash_fwd<Kernel_traits,Is_dropout,Is_causal>(Flash_fwd_params &, cudaStream_t) [with Kernel_traits=Flash_fwd_kernel_traits<224, 64, 64, 4, false, false, cutlass::bfloat16_t, Flash_kernel_traits<224, 64, 64, 4, cutlass::bfloat16_t>>, Is_dropout=true, Is_causal=false]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(208): here
            instantiation of "void run_mha_fwd_hdim224<T>(Flash_fwd_params &, cudaStream_t) [with T=cutlass::bfloat16_t]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_hdim224_bf16_sm80.cu(8): here

/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_kernel.h(140): warning: variable "MMA_M" was declared but never referenced
          detected during:
            instantiation of "void flash::compute_attn_1rowblock<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params,Is_Q_split_K>(const Params &, int, int, int) [with Kernel_traits=Flash_fwd_kernel_traits<224, 64, 64, 4, false, false, cutlass::bfloat16_t, Flash_kernel_traits<224, 64, 64, 4, cutlass::bfloat16_t>>, Is_dropout=true, Is_causal=false, Is_even_N=true, Is_even_K=false, Return_softmax=true, Params=Flash_fwd_params, Is_Q_split_K=false]" 
(589): here
            instantiation of "void flash::compute_attn<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &) [with Kernel_traits=Flash_fwd_kernel_traits<224, 64, 64, 4, false, false, cutlass::bfloat16_t, Flash_kernel_traits<224, 64, 64, 4, cutlass::bfloat16_t>>, Is_dropout=true, Is_causal=false, Is_even_N=true, Is_even_K=false, Return_softmax=true, Params=Flash_fwd_params]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(15): here
            instantiation of "void flash_fwd_kernel<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax>(Flash_fwd_params) [with Kernel_traits=Flash_fwd_kernel_traits<224, 64, 64, 4, false, false, cutlass::bfloat16_t, Flash_kernel_traits<224, 64, 64, 4, cutlass::bfloat16_t>>, Is_dropout=true, Is_causal=false, Is_even_N=true, Is_even_K=false, Return_softmax=true]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(34): here
            instantiation of "void run_flash_fwd<Kernel_traits,Is_dropout,Is_causal>(Flash_fwd_params &, cudaStream_t) [with Kernel_traits=Flash_fwd_kernel_traits<224, 64, 64, 4, false, false, cutlass::bfloat16_t, Flash_kernel_traits<224, 64, 64, 4, cutlass::bfloat16_t>>, Is_dropout=true, Is_causal=false]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(208): here
            instantiation of "void run_mha_fwd_hdim224<T>(Flash_fwd_params &, cudaStream_t) [with T=cutlass::bfloat16_t]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_hdim224_bf16_sm80.cu(8): here

/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_kernel.h(276): warning: variable "tQrQ" was declared but never referenced
          detected during:
            instantiation of "void flash::compute_attn_1rowblock<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params,Is_Q_split_K>(const Params &, int, int, int) [with Kernel_traits=Flash_fwd_kernel_traits<224, 64, 64, 4, false, false, cutlass::bfloat16_t, Flash_kernel_traits<224, 64, 64, 4, cutlass::bfloat16_t>>, Is_dropout=true, Is_causal=false, Is_even_N=true, Is_even_K=false, Return_softmax=true, Params=Flash_fwd_params, Is_Q_split_K=false]" 
(589): here
            instantiation of "void flash::compute_attn<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &) [with Kernel_traits=Flash_fwd_kernel_traits<224, 64, 64, 4, false, false, cutlass::bfloat16_t, Flash_kernel_traits<224, 64, 64, 4, cutlass::bfloat16_t>>, Is_dropout=true, Is_causal=false, Is_even_N=true, Is_even_K=false, Return_softmax=true, Params=Flash_fwd_params]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(15): here
            instantiation of "void flash_fwd_kernel<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax>(Flash_fwd_params) [with Kernel_traits=Flash_fwd_kernel_traits<224, 64, 64, 4, false, false, cutlass::bfloat16_t, Flash_kernel_traits<224, 64, 64, 4, cutlass::bfloat16_t>>, Is_dropout=true, Is_causal=false, Is_even_N=true, Is_even_K=false, Return_softmax=true]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(34): here
            instantiation of "void run_flash_fwd<Kernel_traits,Is_dropout,Is_causal>(Flash_fwd_params &, cudaStream_t) [with Kernel_traits=Flash_fwd_kernel_traits<224, 64, 64, 4, false, false, cutlass::bfloat16_t, Flash_kernel_traits<224, 64, 64, 4, cutlass::bfloat16_t>>, Is_dropout=true, Is_causal=false]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(208): here
            instantiation of "void run_mha_fwd_hdim224<T>(Flash_fwd_params &, cudaStream_t) [with T=cutlass::bfloat16_t]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_hdim224_bf16_sm80.cu(8): here

/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_kernel.h(140): warning: variable "MMA_M" was declared but never referenced
          detected during:
            instantiation of "void flash::compute_attn_1rowblock<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params,Is_Q_split_K>(const Params &, int, int, int) [with Kernel_traits=Flash_fwd_kernel_traits<224, 64, 64, 4, false, false, cutlass::bfloat16_t, Flash_kernel_traits<224, 64, 64, 4, cutlass::bfloat16_t>>, Is_dropout=true, Is_causal=false, Is_even_N=true, Is_even_K=false, Return_softmax=false, Params=Flash_fwd_params, Is_Q_split_K=false]" 
(589): here
            instantiation of "void flash::compute_attn<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &) [with Kernel_traits=Flash_fwd_kernel_traits<224, 64, 64, 4, false, false, cutlass::bfloat16_t, Flash_kernel_traits<224, 64, 64, 4, cutlass::bfloat16_t>>, Is_dropout=true, Is_causal=false, Is_even_N=true, Is_even_K=false, Return_softmax=false, Params=Flash_fwd_params]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(15): here
            instantiation of "void flash_fwd_kernel<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax>(Flash_fwd_params) [with Kernel_traits=Flash_fwd_kernel_traits<224, 64, 64, 4, false, false, cutlass::bfloat16_t, Flash_kernel_traits<224, 64, 64, 4, cutlass::bfloat16_t>>, Is_dropout=true, Is_causal=false, Is_even_N=true, Is_even_K=false, Return_softmax=false]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(34): here
            instantiation of "void run_flash_fwd<Kernel_traits,Is_dropout,Is_causal>(Flash_fwd_params &, cudaStream_t) [with Kernel_traits=Flash_fwd_kernel_traits<224, 64, 64, 4, false, false, cutlass::bfloat16_t, Flash_kernel_traits<224, 64, 64, 4, cutlass::bfloat16_t>>, Is_dropout=true, Is_causal=false]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(208): here
            instantiation of "void run_mha_fwd_hdim224<T>(Flash_fwd_params &, cudaStream_t) [with T=cutlass::bfloat16_t]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_hdim224_bf16_sm80.cu(8): here

/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_kernel.h(276): warning: variable "tQrQ" was declared but never referenced
          detected during:
            instantiation of "void flash::compute_attn_1rowblock<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params,Is_Q_split_K>(const Params &, int, int, int) [with Kernel_traits=Flash_fwd_kernel_traits<224, 64, 64, 4, false, false, cutlass::bfloat16_t, Flash_kernel_traits<224, 64, 64, 4, cutlass::bfloat16_t>>, Is_dropout=true, Is_causal=false, Is_even_N=true, Is_even_K=false, Return_softmax=false, Params=Flash_fwd_params, Is_Q_split_K=false]" 
(589): here
            instantiation of "void flash::compute_attn<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &) [with Kernel_traits=Flash_fwd_kernel_traits<224, 64, 64, 4, false, false, cutlass::bfloat16_t, Flash_kernel_traits<224, 64, 64, 4, cutlass::bfloat16_t>>, Is_dropout=true, Is_causal=false, Is_even_N=true, Is_even_K=false, Return_softmax=false, Params=Flash_fwd_params]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(15): here
            instantiation of "void flash_fwd_kernel<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax>(Flash_fwd_params) [with Kernel_traits=Flash_fwd_kernel_traits<224, 64, 64, 4, false, false, cutlass::bfloat16_t, Flash_kernel_traits<224, 64, 64, 4, cutlass::bfloat16_t>>, Is_dropout=true, Is_causal=false, Is_even_N=true, Is_even_K=false, Return_softmax=false]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(34): here
            instantiation of "void run_flash_fwd<Kernel_traits,Is_dropout,Is_causal>(Flash_fwd_params &, cudaStream_t) [with Kernel_traits=Flash_fwd_kernel_traits<224, 64, 64, 4, false, false, cutlass::bfloat16_t, Flash_kernel_traits<224, 64, 64, 4, cutlass::bfloat16_t>>, Is_dropout=true, Is_causal=false]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(208): here
            instantiation of "void run_mha_fwd_hdim224<T>(Flash_fwd_params &, cudaStream_t) [with T=cutlass::bfloat16_t]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_hdim224_bf16_sm80.cu(8): here

/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_kernel.h(140): warning: variable "MMA_M" was declared but never referenced
          detected during:
            instantiation of "void flash::compute_attn_1rowblock<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params,Is_Q_split_K>(const Params &, int, int, int) [with Kernel_traits=Flash_fwd_kernel_traits<224, 64, 64, 4, false, false, cutlass::bfloat16_t, Flash_kernel_traits<224, 64, 64, 4, cutlass::bfloat16_t>>, Is_dropout=true, Is_causal=false, Is_even_N=false, Is_even_K=true, Return_softmax=true, Params=Flash_fwd_params, Is_Q_split_K=false]" 
(589): here
            instantiation of "void flash::compute_attn<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &) [with Kernel_traits=Flash_fwd_kernel_traits<224, 64, 64, 4, false, false, cutlass::bfloat16_t, Flash_kernel_traits<224, 64, 64, 4, cutlass::bfloat16_t>>, Is_dropout=true, Is_causal=false, Is_even_N=false, Is_even_K=true, Return_softmax=true, Params=Flash_fwd_params]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(15): here
            instantiation of "void flash_fwd_kernel<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax>(Flash_fwd_params) [with Kernel_traits=Flash_fwd_kernel_traits<224, 64, 64, 4, false, false, cutlass::bfloat16_t, Flash_kernel_traits<224, 64, 64, 4, cutlass::bfloat16_t>>, Is_dropout=true, Is_causal=false, Is_even_N=false, Is_even_K=true, Return_softmax=true]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(34): here
            instantiation of "void run_flash_fwd<Kernel_traits,Is_dropout,Is_causal>(Flash_fwd_params &, cudaStream_t) [with Kernel_traits=Flash_fwd_kernel_traits<224, 64, 64, 4, false, false, cutlass::bfloat16_t, Flash_kernel_traits<224, 64, 64, 4, cutlass::bfloat16_t>>, Is_dropout=true, Is_causal=false]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(208): here
            instantiation of "void run_mha_fwd_hdim224<T>(Flash_fwd_params &, cudaStream_t) [with T=cutlass::bfloat16_t]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_hdim224_bf16_sm80.cu(8): here

/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_kernel.h(276): warning: variable "tQrQ" was declared but never referenced
          detected during:
            instantiation of "void flash::compute_attn_1rowblock<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params,Is_Q_split_K>(const Params &, int, int, int) [with Kernel_traits=Flash_fwd_kernel_traits<224, 64, 64, 4, false, false, cutlass::bfloat16_t, Flash_kernel_traits<224, 64, 64, 4, cutlass::bfloat16_t>>, Is_dropout=true, Is_causal=false, Is_even_N=false, Is_even_K=true, Return_softmax=true, Params=Flash_fwd_params, Is_Q_split_K=false]" 
(589): here
            instantiation of "void flash::compute_attn<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &) [with Kernel_traits=Flash_fwd_kernel_traits<224, 64, 64, 4, false, false, cutlass::bfloat16_t, Flash_kernel_traits<224, 64, 64, 4, cutlass::bfloat16_t>>, Is_dropout=true, Is_causal=false, Is_even_N=false, Is_even_K=true, Return_softmax=true, Params=Flash_fwd_params]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(15): here
            instantiation of "void flash_fwd_kernel<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax>(Flash_fwd_params) [with Kernel_traits=Flash_fwd_kernel_traits<224, 64, 64, 4, false, false, cutlass::bfloat16_t, Flash_kernel_traits<224, 64, 64, 4, cutlass::bfloat16_t>>, Is_dropout=true, Is_causal=false, Is_even_N=false, Is_even_K=true, Return_softmax=true]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(34): here
            instantiation of "void run_flash_fwd<Kernel_traits,Is_dropout,Is_causal>(Flash_fwd_params &, cudaStream_t) [with Kernel_traits=Flash_fwd_kernel_traits<224, 64, 64, 4, false, false, cutlass::bfloat16_t, Flash_kernel_traits<224, 64, 64, 4, cutlass::bfloat16_t>>, Is_dropout=true, Is_causal=false]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(208): here
            instantiation of "void run_mha_fwd_hdim224<T>(Flash_fwd_params &, cudaStream_t) [with T=cutlass::bfloat16_t]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_hdim224_bf16_sm80.cu(8): here

/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_kernel.h(140): warning: variable "MMA_M" was declared but never referenced
          detected during:
            instantiation of "void flash::compute_attn_1rowblock<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params,Is_Q_split_K>(const Params &, int, int, int) [with Kernel_traits=Flash_fwd_kernel_traits<224, 64, 64, 4, false, false, cutlass::bfloat16_t, Flash_kernel_traits<224, 64, 64, 4, cutlass::bfloat16_t>>, Is_dropout=true, Is_causal=false, Is_even_N=false, Is_even_K=true, Return_softmax=false, Params=Flash_fwd_params, Is_Q_split_K=false]" 
(589): here
            instantiation of "void flash::compute_attn<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &) [with Kernel_traits=Flash_fwd_kernel_traits<224, 64, 64, 4, false, false, cutlass::bfloat16_t, Flash_kernel_traits<224, 64, 64, 4, cutlass::bfloat16_t>>, Is_dropout=true, Is_causal=false, Is_even_N=false, Is_even_K=true, Return_softmax=false, Params=Flash_fwd_params]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(15): here
            instantiation of "void flash_fwd_kernel<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax>(Flash_fwd_params) [with Kernel_traits=Flash_fwd_kernel_traits<224, 64, 64, 4, false, false, cutlass::bfloat16_t, Flash_kernel_traits<224, 64, 64, 4, cutlass::bfloat16_t>>, Is_dropout=true, Is_causal=false, Is_even_N=false, Is_even_K=true, Return_softmax=false]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(34): here
            instantiation of "void run_flash_fwd<Kernel_traits,Is_dropout,Is_causal>(Flash_fwd_params &, cudaStream_t) [with Kernel_traits=Flash_fwd_kernel_traits<224, 64, 64, 4, false, false, cutlass::bfloat16_t, Flash_kernel_traits<224, 64, 64, 4, cutlass::bfloat16_t>>, Is_dropout=true, Is_causal=false]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(208): here
            instantiation of "void run_mha_fwd_hdim224<T>(Flash_fwd_params &, cudaStream_t) [with T=cutlass::bfloat16_t]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_hdim224_bf16_sm80.cu(8): here

/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_kernel.h(276): warning: variable "tQrQ" was declared but never referenced
          detected during:
            instantiation of "void flash::compute_attn_1rowblock<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params,Is_Q_split_K>(const Params &, int, int, int) [with Kernel_traits=Flash_fwd_kernel_traits<224, 64, 64, 4, false, false, cutlass::bfloat16_t, Flash_kernel_traits<224, 64, 64, 4, cutlass::bfloat16_t>>, Is_dropout=true, Is_causal=false, Is_even_N=false, Is_even_K=true, Return_softmax=false, Params=Flash_fwd_params, Is_Q_split_K=false]" 
(589): here
            instantiation of "void flash::compute_attn<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &) [with Kernel_traits=Flash_fwd_kernel_traits<224, 64, 64, 4, false, false, cutlass::bfloat16_t, Flash_kernel_traits<224, 64, 64, 4, cutlass::bfloat16_t>>, Is_dropout=true, Is_causal=false, Is_even_N=false, Is_even_K=true, Return_softmax=false, Params=Flash_fwd_params]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(15): here
            instantiation of "void flash_fwd_kernel<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax>(Flash_fwd_params) [with Kernel_traits=Flash_fwd_kernel_traits<224, 64, 64, 4, false, false, cutlass::bfloat16_t, Flash_kernel_traits<224, 64, 64, 4, cutlass::bfloat16_t>>, Is_dropout=true, Is_causal=false, Is_even_N=false, Is_even_K=true, Return_softmax=false]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(34): here
            instantiation of "void run_flash_fwd<Kernel_traits,Is_dropout,Is_causal>(Flash_fwd_params &, cudaStream_t) [with Kernel_traits=Flash_fwd_kernel_traits<224, 64, 64, 4, false, false, cutlass::bfloat16_t, Flash_kernel_traits<224, 64, 64, 4, cutlass::bfloat16_t>>, Is_dropout=true, Is_causal=false]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(208): here
            instantiation of "void run_mha_fwd_hdim224<T>(Flash_fwd_params &, cudaStream_t) [with T=cutlass::bfloat16_t]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_hdim224_bf16_sm80.cu(8): here

/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_kernel.h(140): warning: variable "MMA_M" was declared but never referenced
          detected during:
            instantiation of "void flash::compute_attn_1rowblock<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params,Is_Q_split_K>(const Params &, int, int, int) [with Kernel_traits=Flash_fwd_kernel_traits<224, 64, 64, 4, false, false, cutlass::bfloat16_t, Flash_kernel_traits<224, 64, 64, 4, cutlass::bfloat16_t>>, Is_dropout=true, Is_causal=false, Is_even_N=false, Is_even_K=false, Return_softmax=true, Params=Flash_fwd_params, Is_Q_split_K=false]" 
(589): here
            instantiation of "void flash::compute_attn<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &) [with Kernel_traits=Flash_fwd_kernel_traits<224, 64, 64, 4, false, false, cutlass::bfloat16_t, Flash_kernel_traits<224, 64, 64, 4, cutlass::bfloat16_t>>, Is_dropout=true, Is_causal=false, Is_even_N=false, Is_even_K=false, Return_softmax=true, Params=Flash_fwd_params]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(15): here
            instantiation of "void flash_fwd_kernel<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax>(Flash_fwd_params) [with Kernel_traits=Flash_fwd_kernel_traits<224, 64, 64, 4, false, false, cutlass::bfloat16_t, Flash_kernel_traits<224, 64, 64, 4, cutlass::bfloat16_t>>, Is_dropout=true, Is_causal=false, Is_even_N=false, Is_even_K=false, Return_softmax=true]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(34): here
            instantiation of "void run_flash_fwd<Kernel_traits,Is_dropout,Is_causal>(Flash_fwd_params &, cudaStream_t) [with Kernel_traits=Flash_fwd_kernel_traits<224, 64, 64, 4, false, false, cutlass::bfloat16_t, Flash_kernel_traits<224, 64, 64, 4, cutlass::bfloat16_t>>, Is_dropout=true, Is_causal=false]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(208): here
            instantiation of "void run_mha_fwd_hdim224<T>(Flash_fwd_params &, cudaStream_t) [with T=cutlass::bfloat16_t]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_hdim224_bf16_sm80.cu(8): here

/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_kernel.h(276): warning: variable "tQrQ" was declared but never referenced
          detected during:
            instantiation of "void flash::compute_attn_1rowblock<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params,Is_Q_split_K>(const Params &, int, int, int) [with Kernel_traits=Flash_fwd_kernel_traits<224, 64, 64, 4, false, false, cutlass::bfloat16_t, Flash_kernel_traits<224, 64, 64, 4, cutlass::bfloat16_t>>, Is_dropout=true, Is_causal=false, Is_even_N=false, Is_even_K=false, Return_softmax=true, Params=Flash_fwd_params, Is_Q_split_K=false]" 
(589): here
            instantiation of "void flash::compute_attn<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &) [with Kernel_traits=Flash_fwd_kernel_traits<224, 64, 64, 4, false, false, cutlass::bfloat16_t, Flash_kernel_traits<224, 64, 64, 4, cutlass::bfloat16_t>>, Is_dropout=true, Is_causal=false, Is_even_N=false, Is_even_K=false, Return_softmax=true, Params=Flash_fwd_params]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(15): here
            instantiation of "void flash_fwd_kernel<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax>(Flash_fwd_params) [with Kernel_traits=Flash_fwd_kernel_traits<224, 64, 64, 4, false, false, cutlass::bfloat16_t, Flash_kernel_traits<224, 64, 64, 4, cutlass::bfloat16_t>>, Is_dropout=true, Is_causal=false, Is_even_N=false, Is_even_K=false, Return_softmax=true]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(34): here
            instantiation of "void run_flash_fwd<Kernel_traits,Is_dropout,Is_causal>(Flash_fwd_params &, cudaStream_t) [with Kernel_traits=Flash_fwd_kernel_traits<224, 64, 64, 4, false, false, cutlass::bfloat16_t, Flash_kernel_traits<224, 64, 64, 4, cutlass::bfloat16_t>>, Is_dropout=true, Is_causal=false]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(208): here
            instantiation of "void run_mha_fwd_hdim224<T>(Flash_fwd_params &, cudaStream_t) [with T=cutlass::bfloat16_t]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_hdim224_bf16_sm80.cu(8): here

/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_kernel.h(140): warning: variable "MMA_M" was declared but never referenced
          detected during:
            instantiation of "void flash::compute_attn_1rowblock<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params,Is_Q_split_K>(const Params &, int, int, int) [with Kernel_traits=Flash_fwd_kernel_traits<224, 64, 64, 4, false, false, cutlass::bfloat16_t, Flash_kernel_traits<224, 64, 64, 4, cutlass::bfloat16_t>>, Is_dropout=true, Is_causal=false, Is_even_N=false, Is_even_K=false, Return_softmax=false, Params=Flash_fwd_params, Is_Q_split_K=false]" 
(589): here
            instantiation of "void flash::compute_attn<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &) [with Kernel_traits=Flash_fwd_kernel_traits<224, 64, 64, 4, false, false, cutlass::bfloat16_t, Flash_kernel_traits<224, 64, 64, 4, cutlass::bfloat16_t>>, Is_dropout=true, Is_causal=false, Is_even_N=false, Is_even_K=false, Return_softmax=false, Params=Flash_fwd_params]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(15): here
            instantiation of "void flash_fwd_kernel<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax>(Flash_fwd_params) [with Kernel_traits=Flash_fwd_kernel_traits<224, 64, 64, 4, false, false, cutlass::bfloat16_t, Flash_kernel_traits<224, 64, 64, 4, cutlass::bfloat16_t>>, Is_dropout=true, Is_causal=false, Is_even_N=false, Is_even_K=false, Return_softmax=false]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(34): here
            instantiation of "void run_flash_fwd<Kernel_traits,Is_dropout,Is_causal>(Flash_fwd_params &, cudaStream_t) [with Kernel_traits=Flash_fwd_kernel_traits<224, 64, 64, 4, false, false, cutlass::bfloat16_t, Flash_kernel_traits<224, 64, 64, 4, cutlass::bfloat16_t>>, Is_dropout=true, Is_causal=false]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(208): here
            instantiation of "void run_mha_fwd_hdim224<T>(Flash_fwd_params &, cudaStream_t) [with T=cutlass::bfloat16_t]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_hdim224_bf16_sm80.cu(8): here

/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_kernel.h(276): warning: variable "tQrQ" was declared but never referenced
          detected during:
            instantiation of "void flash::compute_attn_1rowblock<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params,Is_Q_split_K>(const Params &, int, int, int) [with Kernel_traits=Flash_fwd_kernel_traits<224, 64, 64, 4, false, false, cutlass::bfloat16_t, Flash_kernel_traits<224, 64, 64, 4, cutlass::bfloat16_t>>, Is_dropout=true, Is_causal=false, Is_even_N=false, Is_even_K=false, Return_softmax=false, Params=Flash_fwd_params, Is_Q_split_K=false]" 
(589): here
            instantiation of "void flash::compute_attn<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &) [with Kernel_traits=Flash_fwd_kernel_traits<224, 64, 64, 4, false, false, cutlass::bfloat16_t, Flash_kernel_traits<224, 64, 64, 4, cutlass::bfloat16_t>>, Is_dropout=true, Is_causal=false, Is_even_N=false, Is_even_K=false, Return_softmax=false, Params=Flash_fwd_params]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(15): here
            instantiation of "void flash_fwd_kernel<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax>(Flash_fwd_params) [with Kernel_traits=Flash_fwd_kernel_traits<224, 64, 64, 4, false, false, cutlass::bfloat16_t, Flash_kernel_traits<224, 64, 64, 4, cutlass::bfloat16_t>>, Is_dropout=true, Is_causal=false, Is_even_N=false, Is_even_K=false, Return_softmax=false]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(34): here
            instantiation of "void run_flash_fwd<Kernel_traits,Is_dropout,Is_causal>(Flash_fwd_params &, cudaStream_t) [with Kernel_traits=Flash_fwd_kernel_traits<224, 64, 64, 4, false, false, cutlass::bfloat16_t, Flash_kernel_traits<224, 64, 64, 4, cutlass::bfloat16_t>>, Is_dropout=true, Is_causal=false]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(208): here
            instantiation of "void run_mha_fwd_hdim224<T>(Flash_fwd_params &, cudaStream_t) [with T=cutlass::bfloat16_t]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_hdim224_bf16_sm80.cu(8): here

/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_kernel.h(140): warning: variable "MMA_M" was declared but never referenced
          detected during:
            instantiation of "void flash::compute_attn_1rowblock<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params,Is_Q_split_K>(const Params &, int, int, int) [with Kernel_traits=Flash_fwd_kernel_traits<224, 128, 64, 8, false, false, cutlass::bfloat16_t, Flash_kernel_traits<224, 128, 64, 8, cutlass::bfloat16_t>>, Is_dropout=false, Is_causal=true, Is_even_N=true, Is_even_K=true, Return_softmax=false, Params=Flash_fwd_params, Is_Q_split_K=false]" 
(589): here
            instantiation of "void flash::compute_attn<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &) [with Kernel_traits=Flash_fwd_kernel_traits<224, 128, 64, 8, false, false, cutlass::bfloat16_t, Flash_kernel_traits<224, 128, 64, 8, cutlass::bfloat16_t>>, Is_dropout=false, Is_causal=true, Is_even_N=true, Is_even_K=true, Return_softmax=false, Params=Flash_fwd_params]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(15): here
            instantiation of "void flash_fwd_kernel<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax>(Flash_fwd_params) [with Kernel_traits=Flash_fwd_kernel_traits<224, 128, 64, 8, false, false, cutlass::bfloat16_t, Flash_kernel_traits<224, 128, 64, 8, cutlass::bfloat16_t>>, Is_dropout=false, Is_causal=true, Is_even_N=true, Is_even_K=true, Return_softmax=false]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(34): here
            instantiation of "void run_flash_fwd<Kernel_traits,Is_dropout,Is_causal>(Flash_fwd_params &, cudaStream_t) [with Kernel_traits=Flash_fwd_kernel_traits<224, 128, 64, 8, false, false, cutlass::bfloat16_t, Flash_kernel_traits<224, 128, 64, 8, cutlass::bfloat16_t>>, Is_dropout=false, Is_causal=true]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(208): here
            instantiation of "void run_mha_fwd_hdim224<T>(Flash_fwd_params &, cudaStream_t) [with T=cutlass::bfloat16_t]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_hdim224_bf16_sm80.cu(8): here

/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_kernel.h(276): warning: variable "tQrQ" was declared but never referenced
          detected during:
            instantiation of "void flash::compute_attn_1rowblock<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params,Is_Q_split_K>(const Params &, int, int, int) [with Kernel_traits=Flash_fwd_kernel_traits<224, 128, 64, 8, false, false, cutlass::bfloat16_t, Flash_kernel_traits<224, 128, 64, 8, cutlass::bfloat16_t>>, Is_dropout=false, Is_causal=true, Is_even_N=true, Is_even_K=true, Return_softmax=false, Params=Flash_fwd_params, Is_Q_split_K=false]" 
(589): here
            instantiation of "void flash::compute_attn<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &) [with Kernel_traits=Flash_fwd_kernel_traits<224, 128, 64, 8, false, false, cutlass::bfloat16_t, Flash_kernel_traits<224, 128, 64, 8, cutlass::bfloat16_t>>, Is_dropout=false, Is_causal=true, Is_even_N=true, Is_even_K=true, Return_softmax=false, Params=Flash_fwd_params]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(15): here
            instantiation of "void flash_fwd_kernel<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax>(Flash_fwd_params) [with Kernel_traits=Flash_fwd_kernel_traits<224, 128, 64, 8, false, false, cutlass::bfloat16_t, Flash_kernel_traits<224, 128, 64, 8, cutlass::bfloat16_t>>, Is_dropout=false, Is_causal=true, Is_even_N=true, Is_even_K=true, Return_softmax=false]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(34): here
            instantiation of "void run_flash_fwd<Kernel_traits,Is_dropout,Is_causal>(Flash_fwd_params &, cudaStream_t) [with Kernel_traits=Flash_fwd_kernel_traits<224, 128, 64, 8, false, false, cutlass::bfloat16_t, Flash_kernel_traits<224, 128, 64, 8, cutlass::bfloat16_t>>, Is_dropout=false, Is_causal=true]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(208): here
            instantiation of "void run_mha_fwd_hdim224<T>(Flash_fwd_params &, cudaStream_t) [with T=cutlass::bfloat16_t]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_hdim224_bf16_sm80.cu(8): here

/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_kernel.h(140): warning: variable "MMA_M" was declared but never referenced
          detected during:
            instantiation of "void flash::compute_attn_1rowblock<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params,Is_Q_split_K>(const Params &, int, int, int) [with Kernel_traits=Flash_fwd_kernel_traits<224, 128, 64, 8, false, false, cutlass::bfloat16_t, Flash_kernel_traits<224, 128, 64, 8, cutlass::bfloat16_t>>, Is_dropout=false, Is_causal=true, Is_even_N=true, Is_even_K=false, Return_softmax=false, Params=Flash_fwd_params, Is_Q_split_K=false]" 
(589): here
            instantiation of "void flash::compute_attn<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &) [with Kernel_traits=Flash_fwd_kernel_traits<224, 128, 64, 8, false, false, cutlass::bfloat16_t, Flash_kernel_traits<224, 128, 64, 8, cutlass::bfloat16_t>>, Is_dropout=false, Is_causal=true, Is_even_N=true, Is_even_K=false, Return_softmax=false, Params=Flash_fwd_params]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(15): here
            instantiation of "void flash_fwd_kernel<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax>(Flash_fwd_params) [with Kernel_traits=Flash_fwd_kernel_traits<224, 128, 64, 8, false, false, cutlass::bfloat16_t, Flash_kernel_traits<224, 128, 64, 8, cutlass::bfloat16_t>>, Is_dropout=false, Is_causal=true, Is_even_N=true, Is_even_K=false, Return_softmax=false]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(34): here
            instantiation of "void run_flash_fwd<Kernel_traits,Is_dropout,Is_causal>(Flash_fwd_params &, cudaStream_t) [with Kernel_traits=Flash_fwd_kernel_traits<224, 128, 64, 8, false, false, cutlass::bfloat16_t, Flash_kernel_traits<224, 128, 64, 8, cutlass::bfloat16_t>>, Is_dropout=false, Is_causal=true]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(208): here
            instantiation of "void run_mha_fwd_hdim224<T>(Flash_fwd_params &, cudaStream_t) [with T=cutlass::bfloat16_t]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_hdim224_bf16_sm80.cu(8): here

/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_kernel.h(276): warning: variable "tQrQ" was declared but never referenced
          detected during:
            instantiation of "void flash::compute_attn_1rowblock<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params,Is_Q_split_K>(const Params &, int, int, int) [with Kernel_traits=Flash_fwd_kernel_traits<224, 128, 64, 8, false, false, cutlass::bfloat16_t, Flash_kernel_traits<224, 128, 64, 8, cutlass::bfloat16_t>>, Is_dropout=false, Is_causal=true, Is_even_N=true, Is_even_K=false, Return_softmax=false, Params=Flash_fwd_params, Is_Q_split_K=false]" 
(589): here
            instantiation of "void flash::compute_attn<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &) [with Kernel_traits=Flash_fwd_kernel_traits<224, 128, 64, 8, false, false, cutlass::bfloat16_t, Flash_kernel_traits<224, 128, 64, 8, cutlass::bfloat16_t>>, Is_dropout=false, Is_causal=true, Is_even_N=true, Is_even_K=false, Return_softmax=false, Params=Flash_fwd_params]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(15): here
            instantiation of "void flash_fwd_kernel<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax>(Flash_fwd_params) [with Kernel_traits=Flash_fwd_kernel_traits<224, 128, 64, 8, false, false, cutlass::bfloat16_t, Flash_kernel_traits<224, 128, 64, 8, cutlass::bfloat16_t>>, Is_dropout=false, Is_causal=true, Is_even_N=true, Is_even_K=false, Return_softmax=false]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(34): here
            instantiation of "void run_flash_fwd<Kernel_traits,Is_dropout,Is_causal>(Flash_fwd_params &, cudaStream_t) [with Kernel_traits=Flash_fwd_kernel_traits<224, 128, 64, 8, false, false, cutlass::bfloat16_t, Flash_kernel_traits<224, 128, 64, 8, cutlass::bfloat16_t>>, Is_dropout=false, Is_causal=true]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(208): here
            instantiation of "void run_mha_fwd_hdim224<T>(Flash_fwd_params &, cudaStream_t) [with T=cutlass::bfloat16_t]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_hdim224_bf16_sm80.cu(8): here

/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_kernel.h(140): warning: variable "MMA_M" was declared but never referenced
          detected during:
            instantiation of "void flash::compute_attn_1rowblock<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params,Is_Q_split_K>(const Params &, int, int, int) [with Kernel_traits=Flash_fwd_kernel_traits<224, 128, 64, 8, false, false, cutlass::bfloat16_t, Flash_kernel_traits<224, 128, 64, 8, cutlass::bfloat16_t>>, Is_dropout=false, Is_causal=true, Is_even_N=false, Is_even_K=true, Return_softmax=false, Params=Flash_fwd_params, Is_Q_split_K=false]" 
(589): here
            instantiation of "void flash::compute_attn<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &) [with Kernel_traits=Flash_fwd_kernel_traits<224, 128, 64, 8, false, false, cutlass::bfloat16_t, Flash_kernel_traits<224, 128, 64, 8, cutlass::bfloat16_t>>, Is_dropout=false, Is_causal=true, Is_even_N=false, Is_even_K=true, Return_softmax=false, Params=Flash_fwd_params]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(15): here
            instantiation of "void flash_fwd_kernel<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax>(Flash_fwd_params) [with Kernel_traits=Flash_fwd_kernel_traits<224, 128, 64, 8, false, false, cutlass::bfloat16_t, Flash_kernel_traits<224, 128, 64, 8, cutlass::bfloat16_t>>, Is_dropout=false, Is_causal=true, Is_even_N=false, Is_even_K=true, Return_softmax=false]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(34): here
            instantiation of "void run_flash_fwd<Kernel_traits,Is_dropout,Is_causal>(Flash_fwd_params &, cudaStream_t) [with Kernel_traits=Flash_fwd_kernel_traits<224, 128, 64, 8, false, false, cutlass::bfloat16_t, Flash_kernel_traits<224, 128, 64, 8, cutlass::bfloat16_t>>, Is_dropout=false, Is_causal=true]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(208): here
            instantiation of "void run_mha_fwd_hdim224<T>(Flash_fwd_params &, cudaStream_t) [with T=cutlass::bfloat16_t]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_hdim224_bf16_sm80.cu(8): here

/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_kernel.h(276): warning: variable "tQrQ" was declared but never referenced
          detected during:
            instantiation of "void flash::compute_attn_1rowblock<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params,Is_Q_split_K>(const Params &, int, int, int) [with Kernel_traits=Flash_fwd_kernel_traits<224, 128, 64, 8, false, false, cutlass::bfloat16_t, Flash_kernel_traits<224, 128, 64, 8, cutlass::bfloat16_t>>, Is_dropout=false, Is_causal=true, Is_even_N=false, Is_even_K=true, Return_softmax=false, Params=Flash_fwd_params, Is_Q_split_K=false]" 
(589): here
            instantiation of "void flash::compute_attn<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &) [with Kernel_traits=Flash_fwd_kernel_traits<224, 128, 64, 8, false, false, cutlass::bfloat16_t, Flash_kernel_traits<224, 128, 64, 8, cutlass::bfloat16_t>>, Is_dropout=false, Is_causal=true, Is_even_N=false, Is_even_K=true, Return_softmax=false, Params=Flash_fwd_params]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(15): here
            instantiation of "void flash_fwd_kernel<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax>(Flash_fwd_params) [with Kernel_traits=Flash_fwd_kernel_traits<224, 128, 64, 8, false, false, cutlass::bfloat16_t, Flash_kernel_traits<224, 128, 64, 8, cutlass::bfloat16_t>>, Is_dropout=false, Is_causal=true, Is_even_N=false, Is_even_K=true, Return_softmax=false]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(34): here
            instantiation of "void run_flash_fwd<Kernel_traits,Is_dropout,Is_causal>(Flash_fwd_params &, cudaStream_t) [with Kernel_traits=Flash_fwd_kernel_traits<224, 128, 64, 8, false, false, cutlass::bfloat16_t, Flash_kernel_traits<224, 128, 64, 8, cutlass::bfloat16_t>>, Is_dropout=false, Is_causal=true]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(208): here
            instantiation of "void run_mha_fwd_hdim224<T>(Flash_fwd_params &, cudaStream_t) [with T=cutlass::bfloat16_t]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_hdim224_bf16_sm80.cu(8): here

/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_kernel.h(140): warning: variable "MMA_M" was declared but never referenced
          detected during:
            instantiation of "void flash::compute_attn_1rowblock<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params,Is_Q_split_K>(const Params &, int, int, int) [with Kernel_traits=Flash_fwd_kernel_traits<224, 128, 64, 8, false, false, cutlass::bfloat16_t, Flash_kernel_traits<224, 128, 64, 8, cutlass::bfloat16_t>>, Is_dropout=false, Is_causal=true, Is_even_N=false, Is_even_K=false, Return_softmax=false, Params=Flash_fwd_params, Is_Q_split_K=false]" 
(589): here
            instantiation of "void flash::compute_attn<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &) [with Kernel_traits=Flash_fwd_kernel_traits<224, 128, 64, 8, false, false, cutlass::bfloat16_t, Flash_kernel_traits<224, 128, 64, 8, cutlass::bfloat16_t>>, Is_dropout=false, Is_causal=true, Is_even_N=false, Is_even_K=false, Return_softmax=false, Params=Flash_fwd_params]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(15): here
            instantiation of "void flash_fwd_kernel<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax>(Flash_fwd_params) [with Kernel_traits=Flash_fwd_kernel_traits<224, 128, 64, 8, false, false, cutlass::bfloat16_t, Flash_kernel_traits<224, 128, 64, 8, cutlass::bfloat16_t>>, Is_dropout=false, Is_causal=true, Is_even_N=false, Is_even_K=false, Return_softmax=false]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(34): here
            instantiation of "void run_flash_fwd<Kernel_traits,Is_dropout,Is_causal>(Flash_fwd_params &, cudaStream_t) [with Kernel_traits=Flash_fwd_kernel_traits<224, 128, 64, 8, false, false, cutlass::bfloat16_t, Flash_kernel_traits<224, 128, 64, 8, cutlass::bfloat16_t>>, Is_dropout=false, Is_causal=true]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(208): here
            instantiation of "void run_mha_fwd_hdim224<T>(Flash_fwd_params &, cudaStream_t) [with T=cutlass::bfloat16_t]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_hdim224_bf16_sm80.cu(8): here

/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_kernel.h(276): warning: variable "tQrQ" was declared but never referenced
          detected during:
            instantiation of "void flash::compute_attn_1rowblock<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params,Is_Q_split_K>(const Params &, int, int, int) [with Kernel_traits=Flash_fwd_kernel_traits<224, 128, 64, 8, false, false, cutlass::bfloat16_t, Flash_kernel_traits<224, 128, 64, 8, cutlass::bfloat16_t>>, Is_dropout=false, Is_causal=true, Is_even_N=false, Is_even_K=false, Return_softmax=false, Params=Flash_fwd_params, Is_Q_split_K=false]" 
(589): here
            instantiation of "void flash::compute_attn<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &) [with Kernel_traits=Flash_fwd_kernel_traits<224, 128, 64, 8, false, false, cutlass::bfloat16_t, Flash_kernel_traits<224, 128, 64, 8, cutlass::bfloat16_t>>, Is_dropout=false, Is_causal=true, Is_even_N=false, Is_even_K=false, Return_softmax=false, Params=Flash_fwd_params]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(15): here
            instantiation of "void flash_fwd_kernel<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax>(Flash_fwd_params) [with Kernel_traits=Flash_fwd_kernel_traits<224, 128, 64, 8, false, false, cutlass::bfloat16_t, Flash_kernel_traits<224, 128, 64, 8, cutlass::bfloat16_t>>, Is_dropout=false, Is_causal=true, Is_even_N=false, Is_even_K=false, Return_softmax=false]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(34): here
            instantiation of "void run_flash_fwd<Kernel_traits,Is_dropout,Is_causal>(Flash_fwd_params &, cudaStream_t) [with Kernel_traits=Flash_fwd_kernel_traits<224, 128, 64, 8, false, false, cutlass::bfloat16_t, Flash_kernel_traits<224, 128, 64, 8, cutlass::bfloat16_t>>, Is_dropout=false, Is_causal=true]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(208): here
            instantiation of "void run_mha_fwd_hdim224<T>(Flash_fwd_params &, cudaStream_t) [with T=cutlass::bfloat16_t]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_hdim224_bf16_sm80.cu(8): here

/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_kernel.h(140): warning: variable "MMA_M" was declared but never referenced
          detected during:
            instantiation of "void flash::compute_attn_1rowblock<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params,Is_Q_split_K>(const Params &, int, int, int) [with Kernel_traits=Flash_fwd_kernel_traits<224, 64, 64, 4, false, false, cutlass::bfloat16_t, Flash_kernel_traits<224, 64, 64, 4, cutlass::bfloat16_t>>, Is_dropout=false, Is_causal=true, Is_even_N=true, Is_even_K=true, Return_softmax=false, Params=Flash_fwd_params, Is_Q_split_K=false]" 
(589): here
            instantiation of "void flash::compute_attn<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &) [with Kernel_traits=Flash_fwd_kernel_traits<224, 64, 64, 4, false, false, cutlass::bfloat16_t, Flash_kernel_traits<224, 64, 64, 4, cutlass::bfloat16_t>>, Is_dropout=false, Is_causal=true, Is_even_N=true, Is_even_K=true, Return_softmax=false, Params=Flash_fwd_params]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(15): here
            instantiation of "void flash_fwd_kernel<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax>(Flash_fwd_params) [with Kernel_traits=Flash_fwd_kernel_traits<224, 64, 64, 4, false, false, cutlass::bfloat16_t, Flash_kernel_traits<224, 64, 64, 4, cutlass::bfloat16_t>>, Is_dropout=false, Is_causal=true, Is_even_N=true, Is_even_K=true, Return_softmax=false]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(34): here
            instantiation of "void run_flash_fwd<Kernel_traits,Is_dropout,Is_causal>(Flash_fwd_params &, cudaStream_t) [with Kernel_traits=Flash_fwd_kernel_traits<224, 64, 64, 4, false, false, cutlass::bfloat16_t, Flash_kernel_traits<224, 64, 64, 4, cutlass::bfloat16_t>>, Is_dropout=false, Is_causal=true]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(208): here
            instantiation of "void run_mha_fwd_hdim224<T>(Flash_fwd_params &, cudaStream_t) [with T=cutlass::bfloat16_t]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_hdim224_bf16_sm80.cu(8): here

/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_kernel.h(276): warning: variable "tQrQ" was declared but never referenced
          detected during:
            instantiation of "void flash::compute_attn_1rowblock<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params,Is_Q_split_K>(const Params &, int, int, int) [with Kernel_traits=Flash_fwd_kernel_traits<224, 64, 64, 4, false, false, cutlass::bfloat16_t, Flash_kernel_traits<224, 64, 64, 4, cutlass::bfloat16_t>>, Is_dropout=false, Is_causal=true, Is_even_N=true, Is_even_K=true, Return_softmax=false, Params=Flash_fwd_params, Is_Q_split_K=false]" 
(589): here
            instantiation of "void flash::compute_attn<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &) [with Kernel_traits=Flash_fwd_kernel_traits<224, 64, 64, 4, false, false, cutlass::bfloat16_t, Flash_kernel_traits<224, 64, 64, 4, cutlass::bfloat16_t>>, Is_dropout=false, Is_causal=true, Is_even_N=true, Is_even_K=true, Return_softmax=false, Params=Flash_fwd_params]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(15): here
            instantiation of "void flash_fwd_kernel<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax>(Flash_fwd_params) [with Kernel_traits=Flash_fwd_kernel_traits<224, 64, 64, 4, false, false, cutlass::bfloat16_t, Flash_kernel_traits<224, 64, 64, 4, cutlass::bfloat16_t>>, Is_dropout=false, Is_causal=true, Is_even_N=true, Is_even_K=true, Return_softmax=false]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(34): here
            instantiation of "void run_flash_fwd<Kernel_traits,Is_dropout,Is_causal>(Flash_fwd_params &, cudaStream_t) [with Kernel_traits=Flash_fwd_kernel_traits<224, 64, 64, 4, false, false, cutlass::bfloat16_t, Flash_kernel_traits<224, 64, 64, 4, cutlass::bfloat16_t>>, Is_dropout=false, Is_causal=true]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(208): here
            instantiation of "void run_mha_fwd_hdim224<T>(Flash_fwd_params &, cudaStream_t) [with T=cutlass::bfloat16_t]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_hdim224_bf16_sm80.cu(8): here

/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_kernel.h(140): warning: variable "MMA_M" was declared but never referenced
          detected during:
            instantiation of "void flash::compute_attn_1rowblock<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params,Is_Q_split_K>(const Params &, int, int, int) [with Kernel_traits=Flash_fwd_kernel_traits<224, 64, 64, 4, false, false, cutlass::bfloat16_t, Flash_kernel_traits<224, 64, 64, 4, cutlass::bfloat16_t>>, Is_dropout=false, Is_causal=true, Is_even_N=true, Is_even_K=false, Return_softmax=false, Params=Flash_fwd_params, Is_Q_split_K=false]" 
(589): here
            instantiation of "void flash::compute_attn<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &) [with Kernel_traits=Flash_fwd_kernel_traits<224, 64, 64, 4, false, false, cutlass::bfloat16_t, Flash_kernel_traits<224, 64, 64, 4, cutlass::bfloat16_t>>, Is_dropout=false, Is_causal=true, Is_even_N=true, Is_even_K=false, Return_softmax=false, Params=Flash_fwd_params]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(15): here
            instantiation of "void flash_fwd_kernel<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax>(Flash_fwd_params) [with Kernel_traits=Flash_fwd_kernel_traits<224, 64, 64, 4, false, false, cutlass::bfloat16_t, Flash_kernel_traits<224, 64, 64, 4, cutlass::bfloat16_t>>, Is_dropout=false, Is_causal=true, Is_even_N=true, Is_even_K=false, Return_softmax=false]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(34): here
            instantiation of "void run_flash_fwd<Kernel_traits,Is_dropout,Is_causal>(Flash_fwd_params &, cudaStream_t) [with Kernel_traits=Flash_fwd_kernel_traits<224, 64, 64, 4, false, false, cutlass::bfloat16_t, Flash_kernel_traits<224, 64, 64, 4, cutlass::bfloat16_t>>, Is_dropout=false, Is_causal=true]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(208): here
            instantiation of "void run_mha_fwd_hdim224<T>(Flash_fwd_params &, cudaStream_t) [with T=cutlass::bfloat16_t]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_hdim224_bf16_sm80.cu(8): here

/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_kernel.h(276): warning: variable "tQrQ" was declared but never referenced
          detected during:
            instantiation of "void flash::compute_attn_1rowblock<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params,Is_Q_split_K>(const Params &, int, int, int) [with Kernel_traits=Flash_fwd_kernel_traits<224, 64, 64, 4, false, false, cutlass::bfloat16_t, Flash_kernel_traits<224, 64, 64, 4, cutlass::bfloat16_t>>, Is_dropout=false, Is_causal=true, Is_even_N=true, Is_even_K=false, Return_softmax=false, Params=Flash_fwd_params, Is_Q_split_K=false]" 
(589): here
            instantiation of "void flash::compute_attn<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &) [with Kernel_traits=Flash_fwd_kernel_traits<224, 64, 64, 4, false, false, cutlass::bfloat16_t, Flash_kernel_traits<224, 64, 64, 4, cutlass::bfloat16_t>>, Is_dropout=false, Is_causal=true, Is_even_N=true, Is_even_K=false, Return_softmax=false, Params=Flash_fwd_params]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(15): here
            instantiation of "void flash_fwd_kernel<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax>(Flash_fwd_params) [with Kernel_traits=Flash_fwd_kernel_traits<224, 64, 64, 4, false, false, cutlass::bfloat16_t, Flash_kernel_traits<224, 64, 64, 4, cutlass::bfloat16_t>>, Is_dropout=false, Is_causal=true, Is_even_N=true, Is_even_K=false, Return_softmax=false]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(34): here
            instantiation of "void run_flash_fwd<Kernel_traits,Is_dropout,Is_causal>(Flash_fwd_params &, cudaStream_t) [with Kernel_traits=Flash_fwd_kernel_traits<224, 64, 64, 4, false, false, cutlass::bfloat16_t, Flash_kernel_traits<224, 64, 64, 4, cutlass::bfloat16_t>>, Is_dropout=false, Is_causal=true]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(208): here
            instantiation of "void run_mha_fwd_hdim224<T>(Flash_fwd_params &, cudaStream_t) [with T=cutlass::bfloat16_t]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_hdim224_bf16_sm80.cu(8): here

/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_kernel.h(140): warning: variable "MMA_M" was declared but never referenced
          detected during:
            instantiation of "void flash::compute_attn_1rowblock<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params,Is_Q_split_K>(const Params &, int, int, int) [with Kernel_traits=Flash_fwd_kernel_traits<224, 64, 64, 4, false, false, cutlass::bfloat16_t, Flash_kernel_traits<224, 64, 64, 4, cutlass::bfloat16_t>>, Is_dropout=false, Is_causal=true, Is_even_N=false, Is_even_K=true, Return_softmax=false, Params=Flash_fwd_params, Is_Q_split_K=false]" 
(589): here
            instantiation of "void flash::compute_attn<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &) [with Kernel_traits=Flash_fwd_kernel_traits<224, 64, 64, 4, false, false, cutlass::bfloat16_t, Flash_kernel_traits<224, 64, 64, 4, cutlass::bfloat16_t>>, Is_dropout=false, Is_causal=true, Is_even_N=false, Is_even_K=true, Return_softmax=false, Params=Flash_fwd_params]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(15): here
            instantiation of "void flash_fwd_kernel<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax>(Flash_fwd_params) [with Kernel_traits=Flash_fwd_kernel_traits<224, 64, 64, 4, false, false, cutlass::bfloat16_t, Flash_kernel_traits<224, 64, 64, 4, cutlass::bfloat16_t>>, Is_dropout=false, Is_causal=true, Is_even_N=false, Is_even_K=true, Return_softmax=false]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(34): here
            instantiation of "void run_flash_fwd<Kernel_traits,Is_dropout,Is_causal>(Flash_fwd_params &, cudaStream_t) [with Kernel_traits=Flash_fwd_kernel_traits<224, 64, 64, 4, false, false, cutlass::bfloat16_t, Flash_kernel_traits<224, 64, 64, 4, cutlass::bfloat16_t>>, Is_dropout=false, Is_causal=true]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(208): here
            instantiation of "void run_mha_fwd_hdim224<T>(Flash_fwd_params &, cudaStream_t) [with T=cutlass::bfloat16_t]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_hdim224_bf16_sm80.cu(8): here

/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_kernel.h(276): warning: variable "tQrQ" was declared but never referenced
          detected during:
            instantiation of "void flash::compute_attn_1rowblock<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params,Is_Q_split_K>(const Params &, int, int, int) [with Kernel_traits=Flash_fwd_kernel_traits<224, 64, 64, 4, false, false, cutlass::bfloat16_t, Flash_kernel_traits<224, 64, 64, 4, cutlass::bfloat16_t>>, Is_dropout=false, Is_causal=true, Is_even_N=false, Is_even_K=true, Return_softmax=false, Params=Flash_fwd_params, Is_Q_split_K=false]" 
(589): here
            instantiation of "void flash::compute_attn<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &) [with Kernel_traits=Flash_fwd_kernel_traits<224, 64, 64, 4, false, false, cutlass::bfloat16_t, Flash_kernel_traits<224, 64, 64, 4, cutlass::bfloat16_t>>, Is_dropout=false, Is_causal=true, Is_even_N=false, Is_even_K=true, Return_softmax=false, Params=Flash_fwd_params]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(15): here
            instantiation of "void flash_fwd_kernel<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax>(Flash_fwd_params) [with Kernel_traits=Flash_fwd_kernel_traits<224, 64, 64, 4, false, false, cutlass::bfloat16_t, Flash_kernel_traits<224, 64, 64, 4, cutlass::bfloat16_t>>, Is_dropout=false, Is_causal=true, Is_even_N=false, Is_even_K=true, Return_softmax=false]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(34): here
            instantiation of "void run_flash_fwd<Kernel_traits,Is_dropout,Is_causal>(Flash_fwd_params &, cudaStream_t) [with Kernel_traits=Flash_fwd_kernel_traits<224, 64, 64, 4, false, false, cutlass::bfloat16_t, Flash_kernel_traits<224, 64, 64, 4, cutlass::bfloat16_t>>, Is_dropout=false, Is_causal=true]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(208): here
            instantiation of "void run_mha_fwd_hdim224<T>(Flash_fwd_params &, cudaStream_t) [with T=cutlass::bfloat16_t]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_hdim224_bf16_sm80.cu(8): here

/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_kernel.h(140): warning: variable "MMA_M" was declared but never referenced
          detected during:
            instantiation of "void flash::compute_attn_1rowblock<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params,Is_Q_split_K>(const Params &, int, int, int) [with Kernel_traits=Flash_fwd_kernel_traits<224, 64, 64, 4, false, false, cutlass::bfloat16_t, Flash_kernel_traits<224, 64, 64, 4, cutlass::bfloat16_t>>, Is_dropout=false, Is_causal=true, Is_even_N=false, Is_even_K=false, Return_softmax=false, Params=Flash_fwd_params, Is_Q_split_K=false]" 
(589): here
            instantiation of "void flash::compute_attn<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &) [with Kernel_traits=Flash_fwd_kernel_traits<224, 64, 64, 4, false, false, cutlass::bfloat16_t, Flash_kernel_traits<224, 64, 64, 4, cutlass::bfloat16_t>>, Is_dropout=false, Is_causal=true, Is_even_N=false, Is_even_K=false, Return_softmax=false, Params=Flash_fwd_params]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(15): here
            instantiation of "void flash_fwd_kernel<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax>(Flash_fwd_params) [with Kernel_traits=Flash_fwd_kernel_traits<224, 64, 64, 4, false, false, cutlass::bfloat16_t, Flash_kernel_traits<224, 64, 64, 4, cutlass::bfloat16_t>>, Is_dropout=false, Is_causal=true, Is_even_N=false, Is_even_K=false, Return_softmax=false]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(34): here
            instantiation of "void run_flash_fwd<Kernel_traits,Is_dropout,Is_causal>(Flash_fwd_params &, cudaStream_t) [with Kernel_traits=Flash_fwd_kernel_traits<224, 64, 64, 4, false, false, cutlass::bfloat16_t, Flash_kernel_traits<224, 64, 64, 4, cutlass::bfloat16_t>>, Is_dropout=false, Is_causal=true]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(208): here
            instantiation of "void run_mha_fwd_hdim224<T>(Flash_fwd_params &, cudaStream_t) [with T=cutlass::bfloat16_t]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_hdim224_bf16_sm80.cu(8): here

/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_kernel.h(276): warning: variable "tQrQ" was declared but never referenced
          detected during:
            instantiation of "void flash::compute_attn_1rowblock<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params,Is_Q_split_K>(const Params &, int, int, int) [with Kernel_traits=Flash_fwd_kernel_traits<224, 64, 64, 4, false, false, cutlass::bfloat16_t, Flash_kernel_traits<224, 64, 64, 4, cutlass::bfloat16_t>>, Is_dropout=false, Is_causal=true, Is_even_N=false, Is_even_K=false, Return_softmax=false, Params=Flash_fwd_params, Is_Q_split_K=false]" 
(589): here
            instantiation of "void flash::compute_attn<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &) [with Kernel_traits=Flash_fwd_kernel_traits<224, 64, 64, 4, false, false, cutlass::bfloat16_t, Flash_kernel_traits<224, 64, 64, 4, cutlass::bfloat16_t>>, Is_dropout=false, Is_causal=true, Is_even_N=false, Is_even_K=false, Return_softmax=false, Params=Flash_fwd_params]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(15): here
            instantiation of "void flash_fwd_kernel<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax>(Flash_fwd_params) [with Kernel_traits=Flash_fwd_kernel_traits<224, 64, 64, 4, false, false, cutlass::bfloat16_t, Flash_kernel_traits<224, 64, 64, 4, cutlass::bfloat16_t>>, Is_dropout=false, Is_causal=true, Is_even_N=false, Is_even_K=false, Return_softmax=false]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(34): here
            instantiation of "void run_flash_fwd<Kernel_traits,Is_dropout,Is_causal>(Flash_fwd_params &, cudaStream_t) [with Kernel_traits=Flash_fwd_kernel_traits<224, 64, 64, 4, false, false, cutlass::bfloat16_t, Flash_kernel_traits<224, 64, 64, 4, cutlass::bfloat16_t>>, Is_dropout=false, Is_causal=true]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(208): here
            instantiation of "void run_mha_fwd_hdim224<T>(Flash_fwd_params &, cudaStream_t) [with T=cutlass::bfloat16_t]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_hdim224_bf16_sm80.cu(8): here

/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_kernel.h(140): warning: variable "MMA_M" was declared but never referenced
          detected during:
            instantiation of "void flash::compute_attn_1rowblock<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params,Is_Q_split_K>(const Params &, int, int, int) [with Kernel_traits=Flash_fwd_kernel_traits<224, 128, 64, 8, false, false, cutlass::bfloat16_t, Flash_kernel_traits<224, 128, 64, 8, cutlass::bfloat16_t>>, Is_dropout=false, Is_causal=false, Is_even_N=true, Is_even_K=true, Return_softmax=false, Params=Flash_fwd_params, Is_Q_split_K=false]" 
(589): here
            instantiation of "void flash::compute_attn<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &) [with Kernel_traits=Flash_fwd_kernel_traits<224, 128, 64, 8, false, false, cutlass::bfloat16_t, Flash_kernel_traits<224, 128, 64, 8, cutlass::bfloat16_t>>, Is_dropout=false, Is_causal=false, Is_even_N=true, Is_even_K=true, Return_softmax=false, Params=Flash_fwd_params]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(15): here
            instantiation of "void flash_fwd_kernel<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax>(Flash_fwd_params) [with Kernel_traits=Flash_fwd_kernel_traits<224, 128, 64, 8, false, false, cutlass::bfloat16_t, Flash_kernel_traits<224, 128, 64, 8, cutlass::bfloat16_t>>, Is_dropout=false, Is_causal=false, Is_even_N=true, Is_even_K=true, Return_softmax=false]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(34): here
            instantiation of "void run_flash_fwd<Kernel_traits,Is_dropout,Is_causal>(Flash_fwd_params &, cudaStream_t) [with Kernel_traits=Flash_fwd_kernel_traits<224, 128, 64, 8, false, false, cutlass::bfloat16_t, Flash_kernel_traits<224, 128, 64, 8, cutlass::bfloat16_t>>, Is_dropout=false, Is_causal=false]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(208): here
            instantiation of "void run_mha_fwd_hdim224<T>(Flash_fwd_params &, cudaStream_t) [with T=cutlass::bfloat16_t]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_hdim224_bf16_sm80.cu(8): here

/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_kernel.h(276): warning: variable "tQrQ" was declared but never referenced
          detected during:
            instantiation of "void flash::compute_attn_1rowblock<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params,Is_Q_split_K>(const Params &, int, int, int) [with Kernel_traits=Flash_fwd_kernel_traits<224, 128, 64, 8, false, false, cutlass::bfloat16_t, Flash_kernel_traits<224, 128, 64, 8, cutlass::bfloat16_t>>, Is_dropout=false, Is_causal=false, Is_even_N=true, Is_even_K=true, Return_softmax=false, Params=Flash_fwd_params, Is_Q_split_K=false]" 
(589): here
            instantiation of "void flash::compute_attn<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &) [with Kernel_traits=Flash_fwd_kernel_traits<224, 128, 64, 8, false, false, cutlass::bfloat16_t, Flash_kernel_traits<224, 128, 64, 8, cutlass::bfloat16_t>>, Is_dropout=false, Is_causal=false, Is_even_N=true, Is_even_K=true, Return_softmax=false, Params=Flash_fwd_params]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(15): here
            instantiation of "void flash_fwd_kernel<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax>(Flash_fwd_params) [with Kernel_traits=Flash_fwd_kernel_traits<224, 128, 64, 8, false, false, cutlass::bfloat16_t, Flash_kernel_traits<224, 128, 64, 8, cutlass::bfloat16_t>>, Is_dropout=false, Is_causal=false, Is_even_N=true, Is_even_K=true, Return_softmax=false]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(34): here
            instantiation of "void run_flash_fwd<Kernel_traits,Is_dropout,Is_causal>(Flash_fwd_params &, cudaStream_t) [with Kernel_traits=Flash_fwd_kernel_traits<224, 128, 64, 8, false, false, cutlass::bfloat16_t, Flash_kernel_traits<224, 128, 64, 8, cutlass::bfloat16_t>>, Is_dropout=false, Is_causal=false]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(208): here
            instantiation of "void run_mha_fwd_hdim224<T>(Flash_fwd_params &, cudaStream_t) [with T=cutlass::bfloat16_t]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_hdim224_bf16_sm80.cu(8): here

/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_kernel.h(140): warning: variable "MMA_M" was declared but never referenced
          detected during:
            instantiation of "void flash::compute_attn_1rowblock<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params,Is_Q_split_K>(const Params &, int, int, int) [with Kernel_traits=Flash_fwd_kernel_traits<224, 128, 64, 8, false, false, cutlass::bfloat16_t, Flash_kernel_traits<224, 128, 64, 8, cutlass::bfloat16_t>>, Is_dropout=false, Is_causal=false, Is_even_N=true, Is_even_K=false, Return_softmax=false, Params=Flash_fwd_params, Is_Q_split_K=false]" 
(589): here
            instantiation of "void flash::compute_attn<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &) [with Kernel_traits=Flash_fwd_kernel_traits<224, 128, 64, 8, false, false, cutlass::bfloat16_t, Flash_kernel_traits<224, 128, 64, 8, cutlass::bfloat16_t>>, Is_dropout=false, Is_causal=false, Is_even_N=true, Is_even_K=false, Return_softmax=false, Params=Flash_fwd_params]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(15): here
            instantiation of "void flash_fwd_kernel<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax>(Flash_fwd_params) [with Kernel_traits=Flash_fwd_kernel_traits<224, 128, 64, 8, false, false, cutlass::bfloat16_t, Flash_kernel_traits<224, 128, 64, 8, cutlass::bfloat16_t>>, Is_dropout=false, Is_causal=false, Is_even_N=true, Is_even_K=false, Return_softmax=false]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(34): here
            instantiation of "void run_flash_fwd<Kernel_traits,Is_dropout,Is_causal>(Flash_fwd_params &, cudaStream_t) [with Kernel_traits=Flash_fwd_kernel_traits<224, 128, 64, 8, false, false, cutlass::bfloat16_t, Flash_kernel_traits<224, 128, 64, 8, cutlass::bfloat16_t>>, Is_dropout=false, Is_causal=false]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(208): here
            instantiation of "void run_mha_fwd_hdim224<T>(Flash_fwd_params &, cudaStream_t) [with T=cutlass::bfloat16_t]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_hdim224_bf16_sm80.cu(8): here

/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_kernel.h(276): warning: variable "tQrQ" was declared but never referenced
          detected during:
            instantiation of "void flash::compute_attn_1rowblock<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params,Is_Q_split_K>(const Params &, int, int, int) [with Kernel_traits=Flash_fwd_kernel_traits<224, 128, 64, 8, false, false, cutlass::bfloat16_t, Flash_kernel_traits<224, 128, 64, 8, cutlass::bfloat16_t>>, Is_dropout=false, Is_causal=false, Is_even_N=true, Is_even_K=false, Return_softmax=false, Params=Flash_fwd_params, Is_Q_split_K=false]" 
(589): here
            instantiation of "void flash::compute_attn<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &) [with Kernel_traits=Flash_fwd_kernel_traits<224, 128, 64, 8, false, false, cutlass::bfloat16_t, Flash_kernel_traits<224, 128, 64, 8, cutlass::bfloat16_t>>, Is_dropout=false, Is_causal=false, Is_even_N=true, Is_even_K=false, Return_softmax=false, Params=Flash_fwd_params]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(15): here
            instantiation of "void flash_fwd_kernel<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax>(Flash_fwd_params) [with Kernel_traits=Flash_fwd_kernel_traits<224, 128, 64, 8, false, false, cutlass::bfloat16_t, Flash_kernel_traits<224, 128, 64, 8, cutlass::bfloat16_t>>, Is_dropout=false, Is_causal=false, Is_even_N=true, Is_even_K=false, Return_softmax=false]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(34): here
            instantiation of "void run_flash_fwd<Kernel_traits,Is_dropout,Is_causal>(Flash_fwd_params &, cudaStream_t) [with Kernel_traits=Flash_fwd_kernel_traits<224, 128, 64, 8, false, false, cutlass::bfloat16_t, Flash_kernel_traits<224, 128, 64, 8, cutlass::bfloat16_t>>, Is_dropout=false, Is_causal=false]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(208): here
            instantiation of "void run_mha_fwd_hdim224<T>(Flash_fwd_params &, cudaStream_t) [with T=cutlass::bfloat16_t]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_hdim224_bf16_sm80.cu(8): here

/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_kernel.h(140): warning: variable "MMA_M" was declared but never referenced
          detected during:
            instantiation of "void flash::compute_attn_1rowblock<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params,Is_Q_split_K>(const Params &, int, int, int) [with Kernel_traits=Flash_fwd_kernel_traits<224, 128, 64, 8, false, false, cutlass::bfloat16_t, Flash_kernel_traits<224, 128, 64, 8, cutlass::bfloat16_t>>, Is_dropout=false, Is_causal=false, Is_even_N=false, Is_even_K=true, Return_softmax=false, Params=Flash_fwd_params, Is_Q_split_K=false]" 
(589): here
            instantiation of "void flash::compute_attn<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &) [with Kernel_traits=Flash_fwd_kernel_traits<224, 128, 64, 8, false, false, cutlass::bfloat16_t, Flash_kernel_traits<224, 128, 64, 8, cutlass::bfloat16_t>>, Is_dropout=false, Is_causal=false, Is_even_N=false, Is_even_K=true, Return_softmax=false, Params=Flash_fwd_params]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(15): here
            instantiation of "void flash_fwd_kernel<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax>(Flash_fwd_params) [with Kernel_traits=Flash_fwd_kernel_traits<224, 128, 64, 8, false, false, cutlass::bfloat16_t, Flash_kernel_traits<224, 128, 64, 8, cutlass::bfloat16_t>>, Is_dropout=false, Is_causal=false, Is_even_N=false, Is_even_K=true, Return_softmax=false]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(34): here
            instantiation of "void run_flash_fwd<Kernel_traits,Is_dropout,Is_causal>(Flash_fwd_params &, cudaStream_t) [with Kernel_traits=Flash_fwd_kernel_traits<224, 128, 64, 8, false, false, cutlass::bfloat16_t, Flash_kernel_traits<224, 128, 64, 8, cutlass::bfloat16_t>>, Is_dropout=false, Is_causal=false]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(208): here
            instantiation of "void run_mha_fwd_hdim224<T>(Flash_fwd_params &, cudaStream_t) [with T=cutlass::bfloat16_t]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_hdim224_bf16_sm80.cu(8): here

/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_kernel.h(276): warning: variable "tQrQ" was declared but never referenced
          detected during:
            instantiation of "void flash::compute_attn_1rowblock<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params,Is_Q_split_K>(const Params &, int, int, int) [with Kernel_traits=Flash_fwd_kernel_traits<224, 128, 64, 8, false, false, cutlass::bfloat16_t, Flash_kernel_traits<224, 128, 64, 8, cutlass::bfloat16_t>>, Is_dropout=false, Is_causal=false, Is_even_N=false, Is_even_K=true, Return_softmax=false, Params=Flash_fwd_params, Is_Q_split_K=false]" 
(589): here
            instantiation of "void flash::compute_attn<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &) [with Kernel_traits=Flash_fwd_kernel_traits<224, 128, 64, 8, false, false, cutlass::bfloat16_t, Flash_kernel_traits<224, 128, 64, 8, cutlass::bfloat16_t>>, Is_dropout=false, Is_causal=false, Is_even_N=false, Is_even_K=true, Return_softmax=false, Params=Flash_fwd_params]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(15): here
            instantiation of "void flash_fwd_kernel<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax>(Flash_fwd_params) [with Kernel_traits=Flash_fwd_kernel_traits<224, 128, 64, 8, false, false, cutlass::bfloat16_t, Flash_kernel_traits<224, 128, 64, 8, cutlass::bfloat16_t>>, Is_dropout=false, Is_causal=false, Is_even_N=false, Is_even_K=true, Return_softmax=false]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(34): here
            instantiation of "void run_flash_fwd<Kernel_traits,Is_dropout,Is_causal>(Flash_fwd_params &, cudaStream_t) [with Kernel_traits=Flash_fwd_kernel_traits<224, 128, 64, 8, false, false, cutlass::bfloat16_t, Flash_kernel_traits<224, 128, 64, 8, cutlass::bfloat16_t>>, Is_dropout=false, Is_causal=false]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(208): here
            instantiation of "void run_mha_fwd_hdim224<T>(Flash_fwd_params &, cudaStream_t) [with T=cutlass::bfloat16_t]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_hdim224_bf16_sm80.cu(8): here

/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_kernel.h(140): warning: variable "MMA_M" was declared but never referenced
          detected during:
            instantiation of "void flash::compute_attn_1rowblock<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params,Is_Q_split_K>(const Params &, int, int, int) [with Kernel_traits=Flash_fwd_kernel_traits<224, 128, 64, 8, false, false, cutlass::bfloat16_t, Flash_kernel_traits<224, 128, 64, 8, cutlass::bfloat16_t>>, Is_dropout=false, Is_causal=false, Is_even_N=false, Is_even_K=false, Return_softmax=false, Params=Flash_fwd_params, Is_Q_split_K=false]" 
(589): here
            instantiation of "void flash::compute_attn<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &) [with Kernel_traits=Flash_fwd_kernel_traits<224, 128, 64, 8, false, false, cutlass::bfloat16_t, Flash_kernel_traits<224, 128, 64, 8, cutlass::bfloat16_t>>, Is_dropout=false, Is_causal=false, Is_even_N=false, Is_even_K=false, Return_softmax=false, Params=Flash_fwd_params]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(15): here
            instantiation of "void flash_fwd_kernel<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax>(Flash_fwd_params) [with Kernel_traits=Flash_fwd_kernel_traits<224, 128, 64, 8, false, false, cutlass::bfloat16_t, Flash_kernel_traits<224, 128, 64, 8, cutlass::bfloat16_t>>, Is_dropout=false, Is_causal=false, Is_even_N=false, Is_even_K=false, Return_softmax=false]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(34): here
            instantiation of "void run_flash_fwd<Kernel_traits,Is_dropout,Is_causal>(Flash_fwd_params &, cudaStream_t) [with Kernel_traits=Flash_fwd_kernel_traits<224, 128, 64, 8, false, false, cutlass::bfloat16_t, Flash_kernel_traits<224, 128, 64, 8, cutlass::bfloat16_t>>, Is_dropout=false, Is_causal=false]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(208): here
            instantiation of "void run_mha_fwd_hdim224<T>(Flash_fwd_params &, cudaStream_t) [with T=cutlass::bfloat16_t]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_hdim224_bf16_sm80.cu(8): here

/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_kernel.h(276): warning: variable "tQrQ" was declared but never referenced
          detected during:
            instantiation of "void flash::compute_attn_1rowblock<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params,Is_Q_split_K>(const Params &, int, int, int) [with Kernel_traits=Flash_fwd_kernel_traits<224, 128, 64, 8, false, false, cutlass::bfloat16_t, Flash_kernel_traits<224, 128, 64, 8, cutlass::bfloat16_t>>, Is_dropout=false, Is_causal=false, Is_even_N=false, Is_even_K=false, Return_softmax=false, Params=Flash_fwd_params, Is_Q_split_K=false]" 
(589): here
            instantiation of "void flash::compute_attn<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &) [with Kernel_traits=Flash_fwd_kernel_traits<224, 128, 64, 8, false, false, cutlass::bfloat16_t, Flash_kernel_traits<224, 128, 64, 8, cutlass::bfloat16_t>>, Is_dropout=false, Is_causal=false, Is_even_N=false, Is_even_K=false, Return_softmax=false, Params=Flash_fwd_params]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(15): here
            instantiation of "void flash_fwd_kernel<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax>(Flash_fwd_params) [with Kernel_traits=Flash_fwd_kernel_traits<224, 128, 64, 8, false, false, cutlass::bfloat16_t, Flash_kernel_traits<224, 128, 64, 8, cutlass::bfloat16_t>>, Is_dropout=false, Is_causal=false, Is_even_N=false, Is_even_K=false, Return_softmax=false]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(34): here
            instantiation of "void run_flash_fwd<Kernel_traits,Is_dropout,Is_causal>(Flash_fwd_params &, cudaStream_t) [with Kernel_traits=Flash_fwd_kernel_traits<224, 128, 64, 8, false, false, cutlass::bfloat16_t, Flash_kernel_traits<224, 128, 64, 8, cutlass::bfloat16_t>>, Is_dropout=false, Is_causal=false]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(208): here
            instantiation of "void run_mha_fwd_hdim224<T>(Flash_fwd_params &, cudaStream_t) [with T=cutlass::bfloat16_t]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_hdim224_bf16_sm80.cu(8): here

/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_kernel.h(140): warning: variable "MMA_M" was declared but never referenced
          detected during:
            instantiation of "void flash::compute_attn_1rowblock<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params,Is_Q_split_K>(const Params &, int, int, int) [with Kernel_traits=Flash_fwd_kernel_traits<224, 64, 64, 4, false, false, cutlass::bfloat16_t, Flash_kernel_traits<224, 64, 64, 4, cutlass::bfloat16_t>>, Is_dropout=false, Is_causal=false, Is_even_N=true, Is_even_K=true, Return_softmax=false, Params=Flash_fwd_params, Is_Q_split_K=false]" 
(589): here
            instantiation of "void flash::compute_attn<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &) [with Kernel_traits=Flash_fwd_kernel_traits<224, 64, 64, 4, false, false, cutlass::bfloat16_t, Flash_kernel_traits<224, 64, 64, 4, cutlass::bfloat16_t>>, Is_dropout=false, Is_causal=false, Is_even_N=true, Is_even_K=true, Return_softmax=false, Params=Flash_fwd_params]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(15): here
            instantiation of "void flash_fwd_kernel<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax>(Flash_fwd_params) [with Kernel_traits=Flash_fwd_kernel_traits<224, 64, 64, 4, false, false, cutlass::bfloat16_t, Flash_kernel_traits<224, 64, 64, 4, cutlass::bfloat16_t>>, Is_dropout=false, Is_causal=false, Is_even_N=true, Is_even_K=true, Return_softmax=false]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(34): here
            instantiation of "void run_flash_fwd<Kernel_traits,Is_dropout,Is_causal>(Flash_fwd_params &, cudaStream_t) [with Kernel_traits=Flash_fwd_kernel_traits<224, 64, 64, 4, false, false, cutlass::bfloat16_t, Flash_kernel_traits<224, 64, 64, 4, cutlass::bfloat16_t>>, Is_dropout=false, Is_causal=false]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(208): here
            instantiation of "void run_mha_fwd_hdim224<T>(Flash_fwd_params &, cudaStream_t) [with T=cutlass::bfloat16_t]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_hdim224_bf16_sm80.cu(8): here

/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_kernel.h(276): warning: variable "tQrQ" was declared but never referenced
          detected during:
            instantiation of "void flash::compute_attn_1rowblock<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params,Is_Q_split_K>(const Params &, int, int, int) [with Kernel_traits=Flash_fwd_kernel_traits<224, 64, 64, 4, false, false, cutlass::bfloat16_t, Flash_kernel_traits<224, 64, 64, 4, cutlass::bfloat16_t>>, Is_dropout=false, Is_causal=false, Is_even_N=true, Is_even_K=true, Return_softmax=false, Params=Flash_fwd_params, Is_Q_split_K=false]" 
(589): here
            instantiation of "void flash::compute_attn<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &) [with Kernel_traits=Flash_fwd_kernel_traits<224, 64, 64, 4, false, false, cutlass::bfloat16_t, Flash_kernel_traits<224, 64, 64, 4, cutlass::bfloat16_t>>, Is_dropout=false, Is_causal=false, Is_even_N=true, Is_even_K=true, Return_softmax=false, Params=Flash_fwd_params]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(15): here
            instantiation of "void flash_fwd_kernel<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax>(Flash_fwd_params) [with Kernel_traits=Flash_fwd_kernel_traits<224, 64, 64, 4, false, false, cutlass::bfloat16_t, Flash_kernel_traits<224, 64, 64, 4, cutlass::bfloat16_t>>, Is_dropout=false, Is_causal=false, Is_even_N=true, Is_even_K=true, Return_softmax=false]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(34): here
            instantiation of "void run_flash_fwd<Kernel_traits,Is_dropout,Is_causal>(Flash_fwd_params &, cudaStream_t) [with Kernel_traits=Flash_fwd_kernel_traits<224, 64, 64, 4, false, false, cutlass::bfloat16_t, Flash_kernel_traits<224, 64, 64, 4, cutlass::bfloat16_t>>, Is_dropout=false, Is_causal=false]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(208): here
            instantiation of "void run_mha_fwd_hdim224<T>(Flash_fwd_params &, cudaStream_t) [with T=cutlass::bfloat16_t]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_hdim224_bf16_sm80.cu(8): here

/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_kernel.h(140): warning: variable "MMA_M" was declared but never referenced
          detected during:
            instantiation of "void flash::compute_attn_1rowblock<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params,Is_Q_split_K>(const Params &, int, int, int) [with Kernel_traits=Flash_fwd_kernel_traits<224, 64, 64, 4, false, false, cutlass::bfloat16_t, Flash_kernel_traits<224, 64, 64, 4, cutlass::bfloat16_t>>, Is_dropout=false, Is_causal=false, Is_even_N=true, Is_even_K=false, Return_softmax=false, Params=Flash_fwd_params, Is_Q_split_K=false]" 
(589): here
            instantiation of "void flash::compute_attn<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &) [with Kernel_traits=Flash_fwd_kernel_traits<224, 64, 64, 4, false, false, cutlass::bfloat16_t, Flash_kernel_traits<224, 64, 64, 4, cutlass::bfloat16_t>>, Is_dropout=false, Is_causal=false, Is_even_N=true, Is_even_K=false, Return_softmax=false, Params=Flash_fwd_params]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(15): here
            instantiation of "void flash_fwd_kernel<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax>(Flash_fwd_params) [with Kernel_traits=Flash_fwd_kernel_traits<224, 64, 64, 4, false, false, cutlass::bfloat16_t, Flash_kernel_traits<224, 64, 64, 4, cutlass::bfloat16_t>>, Is_dropout=false, Is_causal=false, Is_even_N=true, Is_even_K=false, Return_softmax=false]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(34): here
            instantiation of "void run_flash_fwd<Kernel_traits,Is_dropout,Is_causal>(Flash_fwd_params &, cudaStream_t) [with Kernel_traits=Flash_fwd_kernel_traits<224, 64, 64, 4, false, false, cutlass::bfloat16_t, Flash_kernel_traits<224, 64, 64, 4, cutlass::bfloat16_t>>, Is_dropout=false, Is_causal=false]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(208): here
            instantiation of "void run_mha_fwd_hdim224<T>(Flash_fwd_params &, cudaStream_t) [with T=cutlass::bfloat16_t]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_hdim224_bf16_sm80.cu(8): here

/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_kernel.h(276): warning: variable "tQrQ" was declared but never referenced
          detected during:
            instantiation of "void flash::compute_attn_1rowblock<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params,Is_Q_split_K>(const Params &, int, int, int) [with Kernel_traits=Flash_fwd_kernel_traits<224, 64, 64, 4, false, false, cutlass::bfloat16_t, Flash_kernel_traits<224, 64, 64, 4, cutlass::bfloat16_t>>, Is_dropout=false, Is_causal=false, Is_even_N=true, Is_even_K=false, Return_softmax=false, Params=Flash_fwd_params, Is_Q_split_K=false]" 
(589): here
            instantiation of "void flash::compute_attn<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &) [with Kernel_traits=Flash_fwd_kernel_traits<224, 64, 64, 4, false, false, cutlass::bfloat16_t, Flash_kernel_traits<224, 64, 64, 4, cutlass::bfloat16_t>>, Is_dropout=false, Is_causal=false, Is_even_N=true, Is_even_K=false, Return_softmax=false, Params=Flash_fwd_params]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(15): here
            instantiation of "void flash_fwd_kernel<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax>(Flash_fwd_params) [with Kernel_traits=Flash_fwd_kernel_traits<224, 64, 64, 4, false, false, cutlass::bfloat16_t, Flash_kernel_traits<224, 64, 64, 4, cutlass::bfloat16_t>>, Is_dropout=false, Is_causal=false, Is_even_N=true, Is_even_K=false, Return_softmax=false]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(34): here
            instantiation of "void run_flash_fwd<Kernel_traits,Is_dropout,Is_causal>(Flash_fwd_params &, cudaStream_t) [with Kernel_traits=Flash_fwd_kernel_traits<224, 64, 64, 4, false, false, cutlass::bfloat16_t, Flash_kernel_traits<224, 64, 64, 4, cutlass::bfloat16_t>>, Is_dropout=false, Is_causal=false]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(208): here
            instantiation of "void run_mha_fwd_hdim224<T>(Flash_fwd_params &, cudaStream_t) [with T=cutlass::bfloat16_t]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_hdim224_bf16_sm80.cu(8): here

/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_kernel.h(140): warning: variable "MMA_M" was declared but never referenced
          detected during:
            instantiation of "void flash::compute_attn_1rowblock<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params,Is_Q_split_K>(const Params &, int, int, int) [with Kernel_traits=Flash_fwd_kernel_traits<224, 64, 64, 4, false, false, cutlass::bfloat16_t, Flash_kernel_traits<224, 64, 64, 4, cutlass::bfloat16_t>>, Is_dropout=false, Is_causal=false, Is_even_N=false, Is_even_K=true, Return_softmax=false, Params=Flash_fwd_params, Is_Q_split_K=false]" 
(589): here
            instantiation of "void flash::compute_attn<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &) [with Kernel_traits=Flash_fwd_kernel_traits<224, 64, 64, 4, false, false, cutlass::bfloat16_t, Flash_kernel_traits<224, 64, 64, 4, cutlass::bfloat16_t>>, Is_dropout=false, Is_causal=false, Is_even_N=false, Is_even_K=true, Return_softmax=false, Params=Flash_fwd_params]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(15): here
            instantiation of "void flash_fwd_kernel<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax>(Flash_fwd_params) [with Kernel_traits=Flash_fwd_kernel_traits<224, 64, 64, 4, false, false, cutlass::bfloat16_t, Flash_kernel_traits<224, 64, 64, 4, cutlass::bfloat16_t>>, Is_dropout=false, Is_causal=false, Is_even_N=false, Is_even_K=true, Return_softmax=false]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(34): here
            instantiation of "void run_flash_fwd<Kernel_traits,Is_dropout,Is_causal>(Flash_fwd_params &, cudaStream_t) [with Kernel_traits=Flash_fwd_kernel_traits<224, 64, 64, 4, false, false, cutlass::bfloat16_t, Flash_kernel_traits<224, 64, 64, 4, cutlass::bfloat16_t>>, Is_dropout=false, Is_causal=false]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(208): here
            instantiation of "void run_mha_fwd_hdim224<T>(Flash_fwd_params &, cudaStream_t) [with T=cutlass::bfloat16_t]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_hdim224_bf16_sm80.cu(8): here

/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_kernel.h(276): warning: variable "tQrQ" was declared but never referenced
          detected during:
            instantiation of "void flash::compute_attn_1rowblock<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params,Is_Q_split_K>(const Params &, int, int, int) [with Kernel_traits=Flash_fwd_kernel_traits<224, 64, 64, 4, false, false, cutlass::bfloat16_t, Flash_kernel_traits<224, 64, 64, 4, cutlass::bfloat16_t>>, Is_dropout=false, Is_causal=false, Is_even_N=false, Is_even_K=true, Return_softmax=false, Params=Flash_fwd_params, Is_Q_split_K=false]" 
(589): here
            instantiation of "void flash::compute_attn<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &) [with Kernel_traits=Flash_fwd_kernel_traits<224, 64, 64, 4, false, false, cutlass::bfloat16_t, Flash_kernel_traits<224, 64, 64, 4, cutlass::bfloat16_t>>, Is_dropout=false, Is_causal=false, Is_even_N=false, Is_even_K=true, Return_softmax=false, Params=Flash_fwd_params]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(15): here
            instantiation of "void flash_fwd_kernel<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax>(Flash_fwd_params) [with Kernel_traits=Flash_fwd_kernel_traits<224, 64, 64, 4, false, false, cutlass::bfloat16_t, Flash_kernel_traits<224, 64, 64, 4, cutlass::bfloat16_t>>, Is_dropout=false, Is_causal=false, Is_even_N=false, Is_even_K=true, Return_softmax=false]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(34): here
            instantiation of "void run_flash_fwd<Kernel_traits,Is_dropout,Is_causal>(Flash_fwd_params &, cudaStream_t) [with Kernel_traits=Flash_fwd_kernel_traits<224, 64, 64, 4, false, false, cutlass::bfloat16_t, Flash_kernel_traits<224, 64, 64, 4, cutlass::bfloat16_t>>, Is_dropout=false, Is_causal=false]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(208): here
            instantiation of "void run_mha_fwd_hdim224<T>(Flash_fwd_params &, cudaStream_t) [with T=cutlass::bfloat16_t]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_hdim224_bf16_sm80.cu(8): here

/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_kernel.h(140): warning: variable "MMA_M" was declared but never referenced
          detected during:
            instantiation of "void flash::compute_attn_1rowblock<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params,Is_Q_split_K>(const Params &, int, int, int) [with Kernel_traits=Flash_fwd_kernel_traits<224, 64, 64, 4, false, false, cutlass::bfloat16_t, Flash_kernel_traits<224, 64, 64, 4, cutlass::bfloat16_t>>, Is_dropout=false, Is_causal=false, Is_even_N=false, Is_even_K=false, Return_softmax=false, Params=Flash_fwd_params, Is_Q_split_K=false]" 
(589): here
            instantiation of "void flash::compute_attn<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &) [with Kernel_traits=Flash_fwd_kernel_traits<224, 64, 64, 4, false, false, cutlass::bfloat16_t, Flash_kernel_traits<224, 64, 64, 4, cutlass::bfloat16_t>>, Is_dropout=false, Is_causal=false, Is_even_N=false, Is_even_K=false, Return_softmax=false, Params=Flash_fwd_params]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(15): here
            instantiation of "void flash_fwd_kernel<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax>(Flash_fwd_params) [with Kernel_traits=Flash_fwd_kernel_traits<224, 64, 64, 4, false, false, cutlass::bfloat16_t, Flash_kernel_traits<224, 64, 64, 4, cutlass::bfloat16_t>>, Is_dropout=false, Is_causal=false, Is_even_N=false, Is_even_K=false, Return_softmax=false]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(34): here
            instantiation of "void run_flash_fwd<Kernel_traits,Is_dropout,Is_causal>(Flash_fwd_params &, cudaStream_t) [with Kernel_traits=Flash_fwd_kernel_traits<224, 64, 64, 4, false, false, cutlass::bfloat16_t, Flash_kernel_traits<224, 64, 64, 4, cutlass::bfloat16_t>>, Is_dropout=false, Is_causal=false]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(208): here
            instantiation of "void run_mha_fwd_hdim224<T>(Flash_fwd_params &, cudaStream_t) [with T=cutlass::bfloat16_t]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_hdim224_bf16_sm80.cu(8): here

/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_kernel.h(276): warning: variable "tQrQ" was declared but never referenced
          detected during:
            instantiation of "void flash::compute_attn_1rowblock<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params,Is_Q_split_K>(const Params &, int, int, int) [with Kernel_traits=Flash_fwd_kernel_traits<224, 64, 64, 4, false, false, cutlass::bfloat16_t, Flash_kernel_traits<224, 64, 64, 4, cutlass::bfloat16_t>>, Is_dropout=false, Is_causal=false, Is_even_N=false, Is_even_K=false, Return_softmax=false, Params=Flash_fwd_params, Is_Q_split_K=false]" 
(589): here
            instantiation of "void flash::compute_attn<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &) [with Kernel_traits=Flash_fwd_kernel_traits<224, 64, 64, 4, false, false, cutlass::bfloat16_t, Flash_kernel_traits<224, 64, 64, 4, cutlass::bfloat16_t>>, Is_dropout=false, Is_causal=false, Is_even_N=false, Is_even_K=false, Return_softmax=false, Params=Flash_fwd_params]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(15): here
            instantiation of "void flash_fwd_kernel<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax>(Flash_fwd_params) [with Kernel_traits=Flash_fwd_kernel_traits<224, 64, 64, 4, false, false, cutlass::bfloat16_t, Flash_kernel_traits<224, 64, 64, 4, cutlass::bfloat16_t>>, Is_dropout=false, Is_causal=false, Is_even_N=false, Is_even_K=false, Return_softmax=false]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(34): here
            instantiation of "void run_flash_fwd<Kernel_traits,Is_dropout,Is_causal>(Flash_fwd_params &, cudaStream_t) [with Kernel_traits=Flash_fwd_kernel_traits<224, 64, 64, 4, false, false, cutlass::bfloat16_t, Flash_kernel_traits<224, 64, 64, 4, cutlass::bfloat16_t>>, Is_dropout=false, Is_causal=false]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(208): here
            instantiation of "void run_mha_fwd_hdim224<T>(Flash_fwd_params &, cudaStream_t) [with T=cutlass::bfloat16_t]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_hdim224_bf16_sm80.cu(8): here

ptxas info    : 2 bytes gmem
ptxas info    : Compiling entry function '_Z16flash_fwd_kernelI23Flash_fwd_kernel_traitsILi224ELi64ELi64ELi4ELb0ELb0EN7cutlass10bfloat16_tE19Flash_kernel_traitsILi224ELi64ELi64ELi4ES2_EELb0ELb0ELb0ELb0ELb0EEv16Flash_fwd_params' for 'sm_80'
ptxas info    : Function properties for _Z16flash_fwd_kernelI23Flash_fwd_kernel_traitsILi224ELi64ELi64ELi4ELb0ELb0EN7cutlass10bfloat16_tE19Flash_kernel_traitsILi224ELi64ELi64ELi4ES2_EELb0ELb0ELb0ELb0ELb0EEv16Flash_fwd_params
    0 bytes stack frame, 0 bytes spill stores, 0 bytes spill loads
ptxas info    : Used 218 registers, 584 bytes cmem[0]
ptxas info    : Compiling entry function '_Z16flash_fwd_kernelI23Flash_fwd_kernel_traitsILi224ELi64ELi64ELi4ELb0ELb0EN7cutlass10bfloat16_tE19Flash_kernel_traitsILi224ELi64ELi64ELi4ES2_EELb0ELb0ELb0ELb1ELb0EEv16Flash_fwd_params' for 'sm_80'
ptxas info    : Function properties for _Z16flash_fwd_kernelI23Flash_fwd_kernel_traitsILi224ELi64ELi64ELi4ELb0ELb0EN7cutlass10bfloat16_tE19Flash_kernel_traitsILi224ELi64ELi64ELi4ES2_EELb0ELb0ELb0ELb1ELb0EEv16Flash_fwd_params
    0 bytes stack frame, 0 bytes spill stores, 0 bytes spill loads
ptxas info    : Used 211 registers, 584 bytes cmem[0]
ptxas info    : Compiling entry function '_Z16flash_fwd_kernelI23Flash_fwd_kernel_traitsILi224ELi64ELi64ELi4ELb0ELb0EN7cutlass10bfloat16_tE19Flash_kernel_traitsILi224ELi64ELi64ELi4ES2_EELb0ELb0ELb1ELb0ELb0EEv16Flash_fwd_params' for 'sm_80'
ptxas info    : Function properties for _Z16flash_fwd_kernelI23Flash_fwd_kernel_traitsILi224ELi64ELi64ELi4ELb0ELb0EN7cutlass10bfloat16_tE19Flash_kernel_traitsILi224ELi64ELi64ELi4ES2_EELb0ELb0ELb1ELb0ELb0EEv16Flash_fwd_params
    0 bytes stack frame, 0 bytes spill stores, 0 bytes spill loads
ptxas info    : Used 218 registers, 584 bytes cmem[0]
ptxas info    : Compiling entry function '_Z16flash_fwd_kernelI23Flash_fwd_kernel_traitsILi224ELi64ELi64ELi4ELb0ELb0EN7cutlass10bfloat16_tE19Flash_kernel_traitsILi224ELi64ELi64ELi4ES2_EELb0ELb0ELb1ELb1ELb0EEv16Flash_fwd_params' for 'sm_80'
ptxas info    : Function properties for _Z16flash_fwd_kernelI23Flash_fwd_kernel_traitsILi224ELi64ELi64ELi4ELb0ELb0EN7cutlass10bfloat16_tE19Flash_kernel_traitsILi224ELi64ELi64ELi4ES2_EELb0ELb0ELb1ELb1ELb0EEv16Flash_fwd_params
    0 bytes stack frame, 0 bytes spill stores, 0 bytes spill loads
ptxas info    : Used 211 registers, 584 bytes cmem[0]
ptxas info    : Compiling entry function '_Z16flash_fwd_kernelI23Flash_fwd_kernel_traitsILi224ELi128ELi64ELi8ELb0ELb0EN7cutlass10bfloat16_tE19Flash_kernel_traitsILi224ELi128ELi64ELi8ES2_EELb0ELb0ELb0ELb0ELb0EEv16Flash_fwd_params' for 'sm_80'
ptxas info    : Function properties for _Z16flash_fwd_kernelI23Flash_fwd_kernel_traitsILi224ELi128ELi64ELi8ELb0ELb0EN7cutlass10bfloat16_tE19Flash_kernel_traitsILi224ELi128ELi64ELi8ES2_EELb0ELb0ELb0ELb0ELb0EEv16Flash_fwd_params
    0 bytes stack frame, 0 bytes spill stores, 0 bytes spill loads
ptxas info    : Used 221 registers, 584 bytes cmem[0]
ptxas info    : Compiling entry function '_Z16flash_fwd_kernelI23Flash_fwd_kernel_traitsILi224ELi128ELi64ELi8ELb0ELb0EN7cutlass10bfloat16_tE19Flash_kernel_traitsILi224ELi128ELi64ELi8ES2_EELb0ELb0ELb0ELb1ELb0EEv16Flash_fwd_params' for 'sm_80'
ptxas info    : Function properties for _Z16flash_fwd_kernelI23Flash_fwd_kernel_traitsILi224ELi128ELi64ELi8ELb0ELb0EN7cutlass10bfloat16_tE19Flash_kernel_traitsILi224ELi128ELi64ELi8ES2_EELb0ELb0ELb0ELb1ELb0EEv16Flash_fwd_params
    0 bytes stack frame, 0 bytes spill stores, 0 bytes spill loads
ptxas info    : Used 214 registers, 584 bytes cmem[0]
ptxas info    : Compiling entry function '_Z16flash_fwd_kernelI23Flash_fwd_kernel_traitsILi224ELi128ELi64ELi8ELb0ELb0EN7cutlass10bfloat16_tE19Flash_kernel_traitsILi224ELi128ELi64ELi8ES2_EELb0ELb0ELb1ELb0ELb0EEv16Flash_fwd_params' for 'sm_80'
ptxas info    : Function properties for _Z16flash_fwd_kernelI23Flash_fwd_kernel_traitsILi224ELi128ELi64ELi8ELb0ELb0EN7cutlass10bfloat16_tE19Flash_kernel_traitsILi224ELi128ELi64ELi8ES2_EELb0ELb0ELb1ELb0ELb0EEv16Flash_fwd_params
    0 bytes stack frame, 0 bytes spill stores, 0 bytes spill loads
ptxas info    : Used 218 registers, 584 bytes cmem[0]
ptxas info    : Compiling entry function '_Z16flash_fwd_kernelI23Flash_fwd_kernel_traitsILi224ELi128ELi64ELi8ELb0ELb0EN7cutlass10bfloat16_tE19Flash_kernel_traitsILi224ELi128ELi64ELi8ES2_EELb0ELb0ELb1ELb1ELb0EEv16Flash_fwd_params' for 'sm_80'
ptxas info    : Function properties for _Z16flash_fwd_kernelI23Flash_fwd_kernel_traitsILi224ELi128ELi64ELi8ELb0ELb0EN7cutlass10bfloat16_tE19Flash_kernel_traitsILi224ELi128ELi64ELi8ES2_EELb0ELb0ELb1ELb1ELb0EEv16Flash_fwd_params
    0 bytes stack frame, 0 bytes spill stores, 0 bytes spill loads
ptxas info    : Used 212 registers, 584 bytes cmem[0]
ptxas info    : Compiling entry function '_Z16flash_fwd_kernelI23Flash_fwd_kernel_traitsILi224ELi64ELi64ELi4ELb0ELb0EN7cutlass10bfloat16_tE19Flash_kernel_traitsILi224ELi64ELi64ELi4ES2_EELb0ELb1ELb0ELb0ELb0EEv16Flash_fwd_params' for 'sm_80'
ptxas info    : Function properties for _Z16flash_fwd_kernelI23Flash_fwd_kernel_traitsILi224ELi64ELi64ELi4ELb0ELb0EN7cutlass10bfloat16_tE19Flash_kernel_traitsILi224ELi64ELi64ELi4ES2_EELb0ELb1ELb0ELb0ELb0EEv16Flash_fwd_params
    0 bytes stack frame, 0 bytes spill stores, 0 bytes spill loads
ptxas info    : Used 219 registers, 584 bytes cmem[0]
ptxas info    : Compiling entry function '_Z16flash_fwd_kernelI23Flash_fwd_kernel_traitsILi224ELi64ELi64ELi4ELb0ELb0EN7cutlass10bfloat16_tE19Flash_kernel_traitsILi224ELi64ELi64ELi4ES2_EELb0ELb1ELb0ELb1ELb0EEv16Flash_fwd_params' for 'sm_80'
ptxas info    : Function properties for _Z16flash_fwd_kernelI23Flash_fwd_kernel_traitsILi224ELi64ELi64ELi4ELb0ELb0EN7cutlass10bfloat16_tE19Flash_kernel_traitsILi224ELi64ELi64ELi4ES2_EELb0ELb1ELb0ELb1ELb0EEv16Flash_fwd_params
    0 bytes stack frame, 0 bytes spill stores, 0 bytes spill loads
ptxas info    : Used 212 registers, 584 bytes cmem[0]
ptxas info    : Compiling entry function '_Z16flash_fwd_kernelI23Flash_fwd_kernel_traitsILi224ELi64ELi64ELi4ELb0ELb0EN7cutlass10bfloat16_tE19Flash_kernel_traitsILi224ELi64ELi64ELi4ES2_EELb0ELb1ELb1ELb0ELb0EEv16Flash_fwd_params' for 'sm_80'
ptxas info    : Function properties for _Z16flash_fwd_kernelI23Flash_fwd_kernel_traitsILi224ELi64ELi64ELi4ELb0ELb0EN7cutlass10bfloat16_tE19Flash_kernel_traitsILi224ELi64ELi64ELi4ES2_EELb0ELb1ELb1ELb0ELb0EEv16Flash_fwd_params
    0 bytes stack frame, 0 bytes spill stores, 0 bytes spill loads
ptxas info    : Used 230 registers, 584 bytes cmem[0]
ptxas info    : Compiling entry function '_Z16flash_fwd_kernelI23Flash_fwd_kernel_traitsILi224ELi64ELi64ELi4ELb0ELb0EN7cutlass10bfloat16_tE19Flash_kernel_traitsILi224ELi64ELi64ELi4ES2_EELb0ELb1ELb1ELb1ELb0EEv16Flash_fwd_params' for 'sm_80'
ptxas info    : Function properties for _Z16flash_fwd_kernelI23Flash_fwd_kernel_traitsILi224ELi64ELi64ELi4ELb0ELb0EN7cutlass10bfloat16_tE19Flash_kernel_traitsILi224ELi64ELi64ELi4ES2_EELb0ELb1ELb1ELb1ELb0EEv16Flash_fwd_params
    0 bytes stack frame, 0 bytes spill stores, 0 bytes spill loads
ptxas info    : Used 222 registers, 584 bytes cmem[0]
ptxas info    : Compiling entry function '_Z16flash_fwd_kernelI23Flash_fwd_kernel_traitsILi224ELi128ELi64ELi8ELb0ELb0EN7cutlass10bfloat16_tE19Flash_kernel_traitsILi224ELi128ELi64ELi8ES2_EELb0ELb1ELb0ELb0ELb0EEv16Flash_fwd_params' for 'sm_80'
ptxas info    : Function properties for _Z16flash_fwd_kernelI23Flash_fwd_kernel_traitsILi224ELi128ELi64ELi8ELb0ELb0EN7cutlass10bfloat16_tE19Flash_kernel_traitsILi224ELi128ELi64ELi8ES2_EELb0ELb1ELb0ELb0ELb0EEv16Flash_fwd_params
    0 bytes stack frame, 0 bytes spill stores, 0 bytes spill loads
ptxas info    : Used 234 registers, 584 bytes cmem[0]
ptxas info    : Compiling entry function '_Z16flash_fwd_kernelI23Flash_fwd_kernel_traitsILi224ELi128ELi64ELi8ELb0ELb0EN7cutlass10bfloat16_tE19Flash_kernel_traitsILi224ELi128ELi64ELi8ES2_EELb0ELb1ELb0ELb1ELb0EEv16Flash_fwd_params' for 'sm_80'
ptxas info    : Function properties for _Z16flash_fwd_kernelI23Flash_fwd_kernel_traitsILi224ELi128ELi64ELi8ELb0ELb0EN7cutlass10bfloat16_tE19Flash_kernel_traitsILi224ELi128ELi64ELi8ES2_EELb0ELb1ELb0ELb1ELb0EEv16Flash_fwd_params
    0 bytes stack frame, 0 bytes spill stores, 0 bytes spill loads
ptxas info    : Used 226 registers, 584 bytes cmem[0]
ptxas info    : Compiling entry function '_Z16flash_fwd_kernelI23Flash_fwd_kernel_traitsILi224ELi128ELi64ELi8ELb0ELb0EN7cutlass10bfloat16_tE19Flash_kernel_traitsILi224ELi128ELi64ELi8ES2_EELb0ELb1ELb1ELb0ELb0EEv16Flash_fwd_params' for 'sm_80'
ptxas info    : Function properties for _Z16flash_fwd_kernelI23Flash_fwd_kernel_traitsILi224ELi128ELi64ELi8ELb0ELb0EN7cutlass10bfloat16_tE19Flash_kernel_traitsILi224ELi128ELi64ELi8ES2_EELb0ELb1ELb1ELb0ELb0EEv16Flash_fwd_params
    48 bytes stack frame, 80 bytes spill stores, 72 bytes spill loads
ptxas info    : Used 255 registers, 584 bytes cmem[0]
ptxas info    : Compiling entry function '_Z16flash_fwd_kernelI23Flash_fwd_kernel_traitsILi224ELi128ELi64ELi8ELb0ELb0EN7cutlass10bfloat16_tE19Flash_kernel_traitsILi224ELi128ELi64ELi8ES2_EELb0ELb1ELb1ELb1ELb0EEv16Flash_fwd_params' for 'sm_80'
ptxas info    : Function properties for _Z16flash_fwd_kernelI23Flash_fwd_kernel_traitsILi224ELi128ELi64ELi8ELb0ELb0EN7cutlass10bfloat16_tE19Flash_kernel_traitsILi224ELi128ELi64ELi8ES2_EELb0ELb1ELb1ELb1ELb0EEv16Flash_fwd_params
    0 bytes stack frame, 0 bytes spill stores, 0 bytes spill loads
ptxas info    : Used 228 registers, 584 bytes cmem[0]
ptxas info    : Compiling entry function '_Z16flash_fwd_kernelI23Flash_fwd_kernel_traitsILi224ELi64ELi64ELi4ELb0ELb0EN7cutlass10bfloat16_tE19Flash_kernel_traitsILi224ELi64ELi64ELi4ES2_EELb1ELb0ELb0ELb0ELb0EEv16Flash_fwd_params' for 'sm_80'
ptxas info    : Function properties for _Z16flash_fwd_kernelI23Flash_fwd_kernel_traitsILi224ELi64ELi64ELi4ELb0ELb0EN7cutlass10bfloat16_tE19Flash_kernel_traitsILi224ELi64ELi64ELi4ES2_EELb1ELb0ELb0ELb0ELb0EEv16Flash_fwd_params
    0 bytes stack frame, 0 bytes spill stores, 0 bytes spill loads
ptxas info    : Used 230 registers, 584 bytes cmem[0]
ptxas info    : Compiling entry function '_Z16flash_fwd_kernelI23Flash_fwd_kernel_traitsILi224ELi64ELi64ELi4ELb0ELb0EN7cutlass10bfloat16_tE19Flash_kernel_traitsILi224ELi64ELi64ELi4ES2_EELb1ELb0ELb0ELb0ELb1EEv16Flash_fwd_params' for 'sm_80'
ptxas info    : Function properties for _Z16flash_fwd_kernelI23Flash_fwd_kernel_traitsILi224ELi64ELi64ELi4ELb0ELb0EN7cutlass10bfloat16_tE19Flash_kernel_traitsILi224ELi64ELi64ELi4ES2_EELb1ELb0ELb0ELb0ELb1EEv16Flash_fwd_params
    8 bytes stack frame, 8 bytes spill stores, 8 bytes spill loads
ptxas info    : Used 255 registers, 584 bytes cmem[0]
ptxas info    : Compiling entry function '_Z16flash_fwd_kernelI23Flash_fwd_kernel_traitsILi224ELi64ELi64ELi4ELb0ELb0EN7cutlass10bfloat16_tE19Flash_kernel_traitsILi224ELi64ELi64ELi4ES2_EELb1ELb0ELb0ELb1ELb0EEv16Flash_fwd_params' for 'sm_80'
ptxas info    : Function properties for _Z16flash_fwd_kernelI23Flash_fwd_kernel_traitsILi224ELi64ELi64ELi4ELb0ELb0EN7cutlass10bfloat16_tE19Flash_kernel_traitsILi224ELi64ELi64ELi4ES2_EELb1ELb0ELb0ELb1ELb0EEv16Flash_fwd_params
    0 bytes stack frame, 0 bytes spill stores, 0 bytes spill loads
ptxas info    : Used 222 registers, 584 bytes cmem[0]
ptxas info    : Compiling entry function '_Z16flash_fwd_kernelI23Flash_fwd_kernel_traitsILi224ELi64ELi64ELi4ELb0ELb0EN7cutlass10bfloat16_tE19Flash_kernel_traitsILi224ELi64ELi64ELi4ES2_EELb1ELb0ELb0ELb1ELb1EEv16Flash_fwd_params' for 'sm_80'
ptxas info    : Function properties for _Z16flash_fwd_kernelI23Flash_fwd_kernel_traitsILi224ELi64ELi64ELi4ELb0ELb0EN7cutlass10bfloat16_tE19Flash_kernel_traitsILi224ELi64ELi64ELi4ES2_EELb1ELb0ELb0ELb1ELb1EEv16Flash_fwd_params
    0 bytes stack frame, 0 bytes spill stores, 0 bytes spill loads
ptxas info    : Used 248 registers, 584 bytes cmem[0]
ptxas info    : Compiling entry function '_Z16flash_fwd_kernelI23Flash_fwd_kernel_traitsILi224ELi64ELi64ELi4ELb0ELb0EN7cutlass10bfloat16_tE19Flash_kernel_traitsILi224ELi64ELi64ELi4ES2_EELb1ELb0ELb1ELb0ELb0EEv16Flash_fwd_params' for 'sm_80'
ptxas info    : Function properties for _Z16flash_fwd_kernelI23Flash_fwd_kernel_traitsILi224ELi64ELi64ELi4ELb0ELb0EN7cutlass10bfloat16_tE19Flash_kernel_traitsILi224ELi64ELi64ELi4ES2_EELb1ELb0ELb1ELb0ELb0EEv16Flash_fwd_params
    0 bytes stack frame, 0 bytes spill stores, 0 bytes spill loads
ptxas info    : Used 228 registers, 584 bytes cmem[0]
ptxas info    : Compiling entry function '_Z16flash_fwd_kernelI23Flash_fwd_kernel_traitsILi224ELi64ELi64ELi4ELb0ELb0EN7cutlass10bfloat16_tE19Flash_kernel_traitsILi224ELi64ELi64ELi4ES2_EELb1ELb0ELb1ELb0ELb1EEv16Flash_fwd_params' for 'sm_80'
ptxas info    : Function properties for _Z16flash_fwd_kernelI23Flash_fwd_kernel_traitsILi224ELi64ELi64ELi4ELb0ELb0EN7cutlass10bfloat16_tE19Flash_kernel_traitsILi224ELi64ELi64ELi4ES2_EELb1ELb0ELb1ELb0ELb1EEv16Flash_fwd_params
    24 bytes stack frame, 24 bytes spill stores, 24 bytes spill loads
ptxas info    : Used 255 registers, 584 bytes cmem[0]
ptxas info    : Compiling entry function '_Z16flash_fwd_kernelI23Flash_fwd_kernel_traitsILi224ELi64ELi64ELi4ELb0ELb0EN7cutlass10bfloat16_tE19Flash_kernel_traitsILi224ELi64ELi64ELi4ES2_EELb1ELb0ELb1ELb1ELb0EEv16Flash_fwd_params' for 'sm_80'
ptxas info    : Function properties for _Z16flash_fwd_kernelI23Flash_fwd_kernel_traitsILi224ELi64ELi64ELi4ELb0ELb0EN7cutlass10bfloat16_tE19Flash_kernel_traitsILi224ELi64ELi64ELi4ES2_EELb1ELb0ELb1ELb1ELb0EEv16Flash_fwd_params
    0 bytes stack frame, 0 bytes spill stores, 0 bytes spill loads
ptxas info    : Used 222 registers, 584 bytes cmem[0]
ptxas info    : Compiling entry function '_Z16flash_fwd_kernelI23Flash_fwd_kernel_traitsILi224ELi64ELi64ELi4ELb0ELb0EN7cutlass10bfloat16_tE19Flash_kernel_traitsILi224ELi64ELi64ELi4ES2_EELb1ELb0ELb1ELb1ELb1EEv16Flash_fwd_params' for 'sm_80'
ptxas info    : Function properties for _Z16flash_fwd_kernelI23Flash_fwd_kernel_traitsILi224ELi64ELi64ELi4ELb0ELb0EN7cutlass10bfloat16_tE19Flash_kernel_traitsILi224ELi64ELi64ELi4ES2_EELb1ELb0ELb1ELb1ELb1EEv16Flash_fwd_params
    0 bytes stack frame, 0 bytes spill stores, 0 bytes spill loads
ptxas info    : Used 242 registers, 584 bytes cmem[0]
ptxas info    : Compiling entry function '_Z16flash_fwd_kernelI23Flash_fwd_kernel_traitsILi224ELi128ELi64ELi8ELb0ELb0EN7cutlass10bfloat16_tE19Flash_kernel_traitsILi224ELi128ELi64ELi8ES2_EELb1ELb0ELb0ELb0ELb0EEv16Flash_fwd_params' for 'sm_80'
ptxas info    : Function properties for _Z16flash_fwd_kernelI23Flash_fwd_kernel_traitsILi224ELi128ELi64ELi8ELb0ELb0EN7cutlass10bfloat16_tE19Flash_kernel_traitsILi224ELi128ELi64ELi8ES2_EELb1ELb0ELb0ELb0ELb0EEv16Flash_fwd_params
    0 bytes stack frame, 0 bytes spill stores, 0 bytes spill loads
ptxas info    : Used 234 registers, 584 bytes cmem[0]
ptxas info    : Compiling entry function '_Z16flash_fwd_kernelI23Flash_fwd_kernel_traitsILi224ELi128ELi64ELi8ELb0ELb0EN7cutlass10bfloat16_tE19Flash_kernel_traitsILi224ELi128ELi64ELi8ES2_EELb1ELb0ELb0ELb0ELb1EEv16Flash_fwd_params' for 'sm_80'
ptxas info    : Function properties for _Z16flash_fwd_kernelI23Flash_fwd_kernel_traitsILi224ELi128ELi64ELi8ELb0ELb0EN7cutlass10bfloat16_tE19Flash_kernel_traitsILi224ELi128ELi64ELi8ES2_EELb1ELb0ELb0ELb0ELb1EEv16Flash_fwd_params
    8 bytes stack frame, 8 bytes spill stores, 8 bytes spill loads
ptxas info    : Used 255 registers, 584 bytes cmem[0]
ptxas info    : Compiling entry function '_Z16flash_fwd_kernelI23Flash_fwd_kernel_traitsILi224ELi128ELi64ELi8ELb0ELb0EN7cutlass10bfloat16_tE19Flash_kernel_traitsILi224ELi128ELi64ELi8ES2_EELb1ELb0ELb0ELb1ELb0EEv16Flash_fwd_params' for 'sm_80'
ptxas info    : Function properties for _Z16flash_fwd_kernelI23Flash_fwd_kernel_traitsILi224ELi128ELi64ELi8ELb0ELb0EN7cutlass10bfloat16_tE19Flash_kernel_traitsILi224ELi128ELi64ELi8ES2_EELb1ELb0ELb0ELb1ELb0EEv16Flash_fwd_params
    0 bytes stack frame, 0 bytes spill stores, 0 bytes spill loads
ptxas info    : Used 226 registers, 584 bytes cmem[0]
ptxas info    : Compiling entry function '_Z16flash_fwd_kernelI23Flash_fwd_kernel_traitsILi224ELi128ELi64ELi8ELb0ELb0EN7cutlass10bfloat16_tE19Flash_kernel_traitsILi224ELi128ELi64ELi8ES2_EELb1ELb0ELb0ELb1ELb1EEv16Flash_fwd_params' for 'sm_80'
ptxas info    : Function properties for _Z16flash_fwd_kernelI23Flash_fwd_kernel_traitsILi224ELi128ELi64ELi8ELb0ELb0EN7cutlass10bfloat16_tE19Flash_kernel_traitsILi224ELi128ELi64ELi8ES2_EELb1ELb0ELb0ELb1ELb1EEv16Flash_fwd_params
    0 bytes stack frame, 0 bytes spill stores, 0 bytes spill loads
ptxas info    : Used 248 registers, 584 bytes cmem[0]
ptxas info    : Compiling entry function '_Z16flash_fwd_kernelI23Flash_fwd_kernel_traitsILi224ELi128ELi64ELi8ELb0ELb0EN7cutlass10bfloat16_tE19Flash_kernel_traitsILi224ELi128ELi64ELi8ES2_EELb1ELb0ELb1ELb0ELb0EEv16Flash_fwd_params' for 'sm_80'
ptxas info    : Function properties for _Z16flash_fwd_kernelI23Flash_fwd_kernel_traitsILi224ELi128ELi64ELi8ELb0ELb0EN7cutlass10bfloat16_tE19Flash_kernel_traitsILi224ELi128ELi64ELi8ES2_EELb1ELb0ELb1ELb0ELb0EEv16Flash_fwd_params
    0 bytes stack frame, 0 bytes spill stores, 0 bytes spill loads
ptxas info    : Used 234 registers, 584 bytes cmem[0]
ptxas info    : Compiling entry function '_Z16flash_fwd_kernelI23Flash_fwd_kernel_traitsILi224ELi128ELi64ELi8ELb0ELb0EN7cutlass10bfloat16_tE19Flash_kernel_traitsILi224ELi128ELi64ELi8ES2_EELb1ELb0ELb1ELb0ELb1EEv16Flash_fwd_params' for 'sm_80'
ptxas info    : Function properties for _Z16flash_fwd_kernelI23Flash_fwd_kernel_traitsILi224ELi128ELi64ELi8ELb0ELb0EN7cutlass10bfloat16_tE19Flash_kernel_traitsILi224ELi128ELi64ELi8ES2_EELb1ELb0ELb1ELb0ELb1EEv16Flash_fwd_params
    24 bytes stack frame, 24 bytes spill stores, 20 bytes spill loads
ptxas info    : Used 255 registers, 584 bytes cmem[0]
ptxas info    : Compiling entry function '_Z16flash_fwd_kernelI23Flash_fwd_kernel_traitsILi224ELi128ELi64ELi8ELb0ELb0EN7cutlass10bfloat16_tE19Flash_kernel_traitsILi224ELi128ELi64ELi8ES2_EELb1ELb0ELb1ELb1ELb0EEv16Flash_fwd_params' for 'sm_80'
ptxas info    : Function properties for _Z16flash_fwd_kernelI23Flash_fwd_kernel_traitsILi224ELi128ELi64ELi8ELb0ELb0EN7cutlass10bfloat16_tE19Flash_kernel_traitsILi224ELi128ELi64ELi8ES2_EELb1ELb0ELb1ELb1ELb0EEv16Flash_fwd_params
    0 bytes stack frame, 0 bytes spill stores, 0 bytes spill loads
ptxas info    : Used 222 registers, 584 bytes cmem[0]
ptxas info    : Compiling entry function '_Z16flash_fwd_kernelI23Flash_fwd_kernel_traitsILi224ELi128ELi64ELi8ELb0ELb0EN7cutlass10bfloat16_tE19Flash_kernel_traitsILi224ELi128ELi64ELi8ES2_EELb1ELb0ELb1ELb1ELb1EEv16Flash_fwd_params' for 'sm_80'
ptxas info    : Function properties for _Z16flash_fwd_kernelI23Flash_fwd_kernel_traitsILi224ELi128ELi64ELi8ELb0ELb0EN7cutlass10bfloat16_tE19Flash_kernel_traitsILi224ELi128ELi64ELi8ES2_EELb1ELb0ELb1ELb1ELb1EEv16Flash_fwd_params
    0 bytes stack frame, 0 bytes spill stores, 0 bytes spill loads
ptxas info    : Used 242 registers, 584 bytes cmem[0]
ptxas info    : Compiling entry function '_Z16flash_fwd_kernelI23Flash_fwd_kernel_traitsILi224ELi64ELi64ELi4ELb0ELb0EN7cutlass10bfloat16_tE19Flash_kernel_traitsILi224ELi64ELi64ELi4ES2_EELb1ELb1ELb0ELb0ELb0EEv16Flash_fwd_params' for 'sm_80'
ptxas info    : Function properties for _Z16flash_fwd_kernelI23Flash_fwd_kernel_traitsILi224ELi64ELi64ELi4ELb0ELb0EN7cutlass10bfloat16_tE19Flash_kernel_traitsILi224ELi64ELi64ELi4ES2_EELb1ELb1ELb0ELb0ELb0EEv16Flash_fwd_params
    0 bytes stack frame, 0 bytes spill stores, 0 bytes spill loads
ptxas info    : Used 232 registers, 584 bytes cmem[0]
ptxas info    : Compiling entry function '_Z16flash_fwd_kernelI23Flash_fwd_kernel_traitsILi224ELi64ELi64ELi4ELb0ELb0EN7cutlass10bfloat16_tE19Flash_kernel_traitsILi224ELi64ELi64ELi4ES2_EELb1ELb1ELb0ELb0ELb1EEv16Flash_fwd_params' for 'sm_80'
ptxas info    : Function properties for _Z16flash_fwd_kernelI23Flash_fwd_kernel_traitsILi224ELi64ELi64ELi4ELb0ELb0EN7cutlass10bfloat16_tE19Flash_kernel_traitsILi224ELi64ELi64ELi4ES2_EELb1ELb1ELb0ELb0ELb1EEv16Flash_fwd_params
    8 bytes stack frame, 8 bytes spill stores, 8 bytes spill loads
ptxas info    : Used 255 registers, 584 bytes cmem[0]
ptxas info    : Compiling entry function '_Z16flash_fwd_kernelI23Flash_fwd_kernel_traitsILi224ELi64ELi64ELi4ELb0ELb0EN7cutlass10bfloat16_tE19Flash_kernel_traitsILi224ELi64ELi64ELi4ES2_EELb1ELb1ELb0ELb1ELb0EEv16Flash_fwd_params' for 'sm_80'
ptxas info    : Function properties for _Z16flash_fwd_kernelI23Flash_fwd_kernel_traitsILi224ELi64ELi64ELi4ELb0ELb0EN7cutlass10bfloat16_tE19Flash_kernel_traitsILi224ELi64ELi64ELi4ES2_EELb1ELb1ELb0ELb1ELb0EEv16Flash_fwd_params
    0 bytes stack frame, 0 bytes spill stores, 0 bytes spill loads
ptxas info    : Used 222 registers, 584 bytes cmem[0]
ptxas info    : Compiling entry function '_Z16flash_fwd_kernelI23Flash_fwd_kernel_traitsILi224ELi64ELi64ELi4ELb0ELb0EN7cutlass10bfloat16_tE19Flash_kernel_traitsILi224ELi64ELi64ELi4ES2_EELb1ELb1ELb0ELb1ELb1EEv16Flash_fwd_params' for 'sm_80'
ptxas info    : Function properties for _Z16flash_fwd_kernelI23Flash_fwd_kernel_traitsILi224ELi64ELi64ELi4ELb0ELb0EN7cutlass10bfloat16_tE19Flash_kernel_traitsILi224ELi64ELi64ELi4ES2_EELb1ELb1ELb0ELb1ELb1EEv16Flash_fwd_params
    0 bytes stack frame, 0 bytes spill stores, 0 bytes spill loads
ptxas info    : Used 248 registers, 584 bytes cmem[0]
ptxas info    : Compiling entry function '_Z16flash_fwd_kernelI23Flash_fwd_kernel_traitsILi224ELi64ELi64ELi4ELb0ELb0EN7cutlass10bfloat16_tE19Flash_kernel_traitsILi224ELi64ELi64ELi4ES2_EELb1ELb1ELb1ELb0ELb0EEv16Flash_fwd_params' for 'sm_80'
ptxas info    : Function properties for _Z16flash_fwd_kernelI23Flash_fwd_kernel_traitsILi224ELi64ELi64ELi4ELb0ELb0EN7cutlass10bfloat16_tE19Flash_kernel_traitsILi224ELi64ELi64ELi4ES2_EELb1ELb1ELb1ELb0ELb0EEv16Flash_fwd_params
    0 bytes stack frame, 0 bytes spill stores, 0 bytes spill loads
ptxas info    : Used 238 registers, 584 bytes cmem[0]
ptxas info    : Compiling entry function '_Z16flash_fwd_kernelI23Flash_fwd_kernel_traitsILi224ELi64ELi64ELi4ELb0ELb0EN7cutlass10bfloat16_tE19Flash_kernel_traitsILi224ELi64ELi64ELi4ES2_EELb1ELb1ELb1ELb0ELb1EEv16Flash_fwd_params' for 'sm_80'
ptxas info    : Function properties for _Z16flash_fwd_kernelI23Flash_fwd_kernel_traitsILi224ELi64ELi64ELi4ELb0ELb0EN7cutlass10bfloat16_tE19Flash_kernel_traitsILi224ELi64ELi64ELi4ES2_EELb1ELb1ELb1ELb0ELb1EEv16Flash_fwd_params
    24 bytes stack frame, 20 bytes spill stores, 20 bytes spill loads
ptxas info    : Used 255 registers, 584 bytes cmem[0]
ptxas info    : Compiling entry function '_Z16flash_fwd_kernelI23Flash_fwd_kernel_traitsILi224ELi64ELi64ELi4ELb0ELb0EN7cutlass10bfloat16_tE19Flash_kernel_traitsILi224ELi64ELi64ELi4ES2_EELb1ELb1ELb1ELb1ELb0EEv16Flash_fwd_params' for 'sm_80'
ptxas info    : Function properties for _Z16flash_fwd_kernelI23Flash_fwd_kernel_traitsILi224ELi64ELi64ELi4ELb0ELb0EN7cutlass10bfloat16_tE19Flash_kernel_traitsILi224ELi64ELi64ELi4ES2_EELb1ELb1ELb1ELb1ELb0EEv16Flash_fwd_params
    0 bytes stack frame, 0 bytes spill stores, 0 bytes spill loads
ptxas info    : Used 232 registers, 584 bytes cmem[0]
ptxas info    : Compiling entry function '_Z16flash_fwd_kernelI23Flash_fwd_kernel_traitsILi224ELi64ELi64ELi4ELb0ELb0EN7cutlass10bfloat16_tE19Flash_kernel_traitsILi224ELi64ELi64ELi4ES2_EELb1ELb1ELb1ELb1ELb1EEv16Flash_fwd_params' for 'sm_80'
ptxas info    : Function properties for _Z16flash_fwd_kernelI23Flash_fwd_kernel_traitsILi224ELi64ELi64ELi4ELb0ELb0EN7cutlass10bfloat16_tE19Flash_kernel_traitsILi224ELi64ELi64ELi4ES2_EELb1ELb1ELb1ELb1ELb1EEv16Flash_fwd_params
    0 bytes stack frame, 0 bytes spill stores, 0 bytes spill loads
ptxas info    : Used 254 registers, 584 bytes cmem[0]
ptxas info    : Compiling entry function '_Z16flash_fwd_kernelI23Flash_fwd_kernel_traitsILi224ELi128ELi64ELi8ELb0ELb0EN7cutlass10bfloat16_tE19Flash_kernel_traitsILi224ELi128ELi64ELi8ES2_EELb1ELb1ELb0ELb0ELb0EEv16Flash_fwd_params' for 'sm_80'
ptxas info    : Function properties for _Z16flash_fwd_kernelI23Flash_fwd_kernel_traitsILi224ELi128ELi64ELi8ELb0ELb0EN7cutlass10bfloat16_tE19Flash_kernel_traitsILi224ELi128ELi64ELi8ES2_EELb1ELb1ELb0ELb0ELb0EEv16Flash_fwd_params
    72 bytes stack frame, 72 bytes spill stores, 72 bytes spill loads
ptxas info    : Used 255 registers, 584 bytes cmem[0]
ptxas info    : Compiling entry function '_Z16flash_fwd_kernelI23Flash_fwd_kernel_traitsILi224ELi128ELi64ELi8ELb0ELb0EN7cutlass10bfloat16_tE19Flash_kernel_traitsILi224ELi128ELi64ELi8ES2_EELb1ELb1ELb0ELb0ELb1EEv16Flash_fwd_params' for 'sm_80'
ptxas info    : Function properties for _Z16flash_fwd_kernelI23Flash_fwd_kernel_traitsILi224ELi128ELi64ELi8ELb0ELb0EN7cutlass10bfloat16_tE19Flash_kernel_traitsILi224ELi128ELi64ELi8ES2_EELb1ELb1ELb0ELb0ELb1EEv16Flash_fwd_params
    0 bytes stack frame, 0 bytes spill stores, 0 bytes spill loads
ptxas info    : Used 255 registers, 584 bytes cmem[0]
ptxas info    : Compiling entry function '_Z16flash_fwd_kernelI23Flash_fwd_kernel_traitsILi224ELi128ELi64ELi8ELb0ELb0EN7cutlass10bfloat16_tE19Flash_kernel_traitsILi224ELi128ELi64ELi8ES2_EELb1ELb1ELb0ELb1ELb0EEv16Flash_fwd_params' for 'sm_80'
ptxas info    : Function properties for _Z16flash_fwd_kernelI23Flash_fwd_kernel_traitsILi224ELi128ELi64ELi8ELb0ELb0EN7cutlass10bfloat16_tE19Flash_kernel_traitsILi224ELi128ELi64ELi8ES2_EELb1ELb1ELb0ELb1ELb0EEv16Flash_fwd_params
    0 bytes stack frame, 0 bytes spill stores, 0 bytes spill loads
ptxas info    : Used 251 registers, 584 bytes cmem[0]
ptxas info    : Compiling entry function '_Z16flash_fwd_kernelI23Flash_fwd_kernel_traitsILi224ELi128ELi64ELi8ELb0ELb0EN7cutlass10bfloat16_tE19Flash_kernel_traitsILi224ELi128ELi64ELi8ES2_EELb1ELb1ELb0ELb1ELb1EEv16Flash_fwd_params' for 'sm_80'
ptxas info    : Function properties for _Z16flash_fwd_kernelI23Flash_fwd_kernel_traitsILi224ELi128ELi64ELi8ELb0ELb0EN7cutlass10bfloat16_tE19Flash_kernel_traitsILi224ELi128ELi64ELi8ES2_EELb1ELb1ELb0ELb1ELb1EEv16Flash_fwd_params
    0 bytes stack frame, 0 bytes spill stores, 0 bytes spill loads
ptxas info    : Used 252 registers, 584 bytes cmem[0]
ptxas info    : Compiling entry function '_Z16flash_fwd_kernelI23Flash_fwd_kernel_traitsILi224ELi128ELi64ELi8ELb0ELb0EN7cutlass10bfloat16_tE19Flash_kernel_traitsILi224ELi128ELi64ELi8ES2_EELb1ELb1ELb1ELb0ELb0EEv16Flash_fwd_params' for 'sm_80'
ptxas info    : Function properties for _Z16flash_fwd_kernelI23Flash_fwd_kernel_traitsILi224ELi128ELi64ELi8ELb0ELb0EN7cutlass10bfloat16_tE19Flash_kernel_traitsILi224ELi128ELi64ELi8ES2_EELb1ELb1ELb1ELb0ELb0EEv16Flash_fwd_params
    72 bytes stack frame, 84 bytes spill stores, 84 bytes spill loads
ptxas info    : Used 255 registers, 584 bytes cmem[0]
ptxas info    : Compiling entry function '_Z16flash_fwd_kernelI23Flash_fwd_kernel_traitsILi224ELi128ELi64ELi8ELb0ELb0EN7cutlass10bfloat16_tE19Flash_kernel_traitsILi224ELi128ELi64ELi8ES2_EELb1ELb1ELb1ELb0ELb1EEv16Flash_fwd_params' for 'sm_80'
ptxas info    : Function properties for _Z16flash_fwd_kernelI23Flash_fwd_kernel_traitsILi224ELi128ELi64ELi8ELb0ELb0EN7cutlass10bfloat16_tE19Flash_kernel_traitsILi224ELi128ELi64ELi8ES2_EELb1ELb1ELb1ELb0ELb1EEv16Flash_fwd_params
    72 bytes stack frame, 88 bytes spill stores, 88 bytes spill loads
ptxas info    : Used 255 registers, 584 bytes cmem[0]
ptxas info    : Compiling entry function '_Z16flash_fwd_kernelI23Flash_fwd_kernel_traitsILi224ELi128ELi64ELi8ELb0ELb0EN7cutlass10bfloat16_tE19Flash_kernel_traitsILi224ELi128ELi64ELi8ES2_EELb1ELb1ELb1ELb1ELb0EEv16Flash_fwd_params' for 'sm_80'
ptxas info    : Function properties for _Z16flash_fwd_kernelI23Flash_fwd_kernel_traitsILi224ELi128ELi64ELi8ELb0ELb0EN7cutlass10bfloat16_tE19Flash_kernel_traitsILi224ELi128ELi64ELi8ES2_EELb1ELb1ELb1ELb1ELb0EEv16Flash_fwd_params
    0 bytes stack frame, 0 bytes spill stores, 0 bytes spill loads
ptxas info    : Used 235 registers, 584 bytes cmem[0]
ptxas info    : Compiling entry function '_Z16flash_fwd_kernelI23Flash_fwd_kernel_traitsILi224ELi128ELi64ELi8ELb0ELb0EN7cutlass10bfloat16_tE19Flash_kernel_traitsILi224ELi128ELi64ELi8ES2_EELb1ELb1ELb1ELb1ELb1EEv16Flash_fwd_params' for 'sm_80'
ptxas info    : Function properties for _Z16flash_fwd_kernelI23Flash_fwd_kernel_traitsILi224ELi128ELi64ELi8ELb0ELb0EN7cutlass10bfloat16_tE19Flash_kernel_traitsILi224ELi128ELi64ELi8ES2_EELb1ELb1ELb1ELb1ELb1EEv16Flash_fwd_params
    0 bytes stack frame, 0 bytes spill stores, 0 bytes spill loads
ptxas info    : Used 250 registers, 584 bytes cmem[0]
/anaconda/envs/flashattention/lib/python3.9/site-packages/torch/include/c10/core/SymInt.h(84): warning: integer conversion resulted in a change of sign

[19/37] /usr/local/cuda-11.4/bin/nvcc  -I/home/v-feiychen/flash-attention/csrc/flash_attn -I/home/v-feiychen/flash-attention/csrc/flash_attn/src -I/home/v-feiychen/flash-attention/csrc/cutlass/include -I/anaconda/envs/flashattention/lib/python3.9/site-packages/torch/include -I/anaconda/envs/flashattention/lib/python3.9/site-packages/torch/include/torch/csrc/api/include -I/anaconda/envs/flashattention/lib/python3.9/site-packages/torch/include/TH -I/anaconda/envs/flashattention/lib/python3.9/site-packages/torch/include/THC -I/usr/local/cuda-11.4/include -I/anaconda/envs/flashattention/include/python3.9 -c -c /home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_bwd_hdim64_fp16_sm80.cu -o /home/v-feiychen/flash-attention/build/temp.linux-x86_64-cpython-39/csrc/flash_attn/src/flash_bwd_hdim64_fp16_sm80.o -D__CUDA_NO_HALF_OPERATORS__ -D__CUDA_NO_HALF_CONVERSIONS__ -D__CUDA_NO_BFLOAT16_CONVERSIONS__ -D__CUDA_NO_HALF2_OPERATORS__ --expt-relaxed-constexpr --compiler-options ''"'"'-fPIC'"'"'' -O3 -std=c++17 -U__CUDA_NO_HALF_OPERATORS__ -U__CUDA_NO_HALF_CONVERSIONS__ -U__CUDA_NO_HALF2_OPERATORS__ -U__CUDA_NO_BFLOAT16_CONVERSIONS__ --expt-relaxed-constexpr --expt-extended-lambda --use_fast_math --ptxas-options=-v -lineinfo -gencode arch=compute_80,code=sm_80 --threads 4 -DTORCH_API_INCLUDE_EXTENSION_H '-DPYBIND11_COMPILER_TYPE="_gcc"' '-DPYBIND11_STDLIB="_libstdcpp"' '-DPYBIND11_BUILD_ABI="_cxxabi1011"' -DTORCH_EXTENSION_NAME=flash_attn_2_cuda -D_GLIBCXX_USE_CXX11_ABI=0
/anaconda/envs/flashattention/lib/python3.9/site-packages/torch/include/c10/core/SymInt.h(84): warning: integer conversion resulted in a change of sign

ptxas info    : 2 bytes gmem
ptxas info    : Compiling entry function '_Z44flash_bwd_dq_dk_dv_loop_seqk_parallel_kernelI23Flash_bwd_kernel_traitsILi64ELi64ELi128ELi8ELi2ELi4ELi4ELb1ELb0EN7cutlass6half_tE19Flash_kernel_traitsILi64ELi64ELi128ELi8ES2_EELb0ELb0ELb0ELb0EEv16Flash_bwd_params' for 'sm_80'
ptxas info    : Function properties for _Z44flash_bwd_dq_dk_dv_loop_seqk_parallel_kernelI23Flash_bwd_kernel_traitsILi64ELi64ELi128ELi8ELi2ELi4ELi4ELb1ELb0EN7cutlass6half_tE19Flash_kernel_traitsILi64ELi64ELi128ELi8ES2_EELb0ELb0ELb0ELb0EEv16Flash_bwd_params
    0 bytes stack frame, 0 bytes spill stores, 0 bytes spill loads
ptxas info    : Used 230 registers, 696 bytes cmem[0]
ptxas info    : Compiling entry function '_Z44flash_bwd_dq_dk_dv_loop_seqk_parallel_kernelI23Flash_bwd_kernel_traitsILi64ELi64ELi128ELi8ELi2ELi4ELi4ELb1ELb0EN7cutlass6half_tE19Flash_kernel_traitsILi64ELi64ELi128ELi8ES2_EELb0ELb0ELb0ELb1EEv16Flash_bwd_params' for 'sm_80'
ptxas info    : Function properties for _Z44flash_bwd_dq_dk_dv_loop_seqk_parallel_kernelI23Flash_bwd_kernel_traitsILi64ELi64ELi128ELi8ELi2ELi4ELi4ELb1ELb0EN7cutlass6half_tE19Flash_kernel_traitsILi64ELi64ELi128ELi8ES2_EELb0ELb0ELb0ELb1EEv16Flash_bwd_params
    0 bytes stack frame, 0 bytes spill stores, 0 bytes spill loads
ptxas info    : Used 232 registers, 696 bytes cmem[0]
ptxas info    : Compiling entry function '_Z44flash_bwd_dq_dk_dv_loop_seqk_parallel_kernelI23Flash_bwd_kernel_traitsILi64ELi64ELi128ELi8ELi2ELi4ELi4ELb1ELb0EN7cutlass6half_tE19Flash_kernel_traitsILi64ELi64ELi128ELi8ES2_EELb0ELb0ELb1ELb0EEv16Flash_bwd_params' for 'sm_80'
ptxas info    : Function properties for _Z44flash_bwd_dq_dk_dv_loop_seqk_parallel_kernelI23Flash_bwd_kernel_traitsILi64ELi64ELi128ELi8ELi2ELi4ELi4ELb1ELb0EN7cutlass6half_tE19Flash_kernel_traitsILi64ELi64ELi128ELi8ES2_EELb0ELb0ELb1ELb0EEv16Flash_bwd_params
    0 bytes stack frame, 0 bytes spill stores, 0 bytes spill loads
ptxas info    : Used 220 registers, 696 bytes cmem[0]
ptxas info    : Compiling entry function '_Z44flash_bwd_dq_dk_dv_loop_seqk_parallel_kernelI23Flash_bwd_kernel_traitsILi64ELi64ELi128ELi8ELi2ELi4ELi4ELb1ELb0EN7cutlass6half_tE19Flash_kernel_traitsILi64ELi64ELi128ELi8ES2_EELb0ELb0ELb1ELb1EEv16Flash_bwd_params' for 'sm_80'
ptxas info    : Function properties for _Z44flash_bwd_dq_dk_dv_loop_seqk_parallel_kernelI23Flash_bwd_kernel_traitsILi64ELi64ELi128ELi8ELi2ELi4ELi4ELb1ELb0EN7cutlass6half_tE19Flash_kernel_traitsILi64ELi64ELi128ELi8ES2_EELb0ELb0ELb1ELb1EEv16Flash_bwd_params
    0 bytes stack frame, 0 bytes spill stores, 0 bytes spill loads
ptxas info    : Used 221 registers, 696 bytes cmem[0]
ptxas info    : Compiling entry function '_Z44flash_bwd_dq_dk_dv_loop_seqk_parallel_kernelI23Flash_bwd_kernel_traitsILi64ELi64ELi128ELi8ELi2ELi4ELi4ELb1ELb0EN7cutlass6half_tE19Flash_kernel_traitsILi64ELi64ELi128ELi8ES2_EELb0ELb1ELb0ELb0EEv16Flash_bwd_params' for 'sm_80'
ptxas info    : Function properties for _Z44flash_bwd_dq_dk_dv_loop_seqk_parallel_kernelI23Flash_bwd_kernel_traitsILi64ELi64ELi128ELi8ELi2ELi4ELi4ELb1ELb0EN7cutlass6half_tE19Flash_kernel_traitsILi64ELi64ELi128ELi8ES2_EELb0ELb1ELb0ELb0EEv16Flash_bwd_params
    0 bytes stack frame, 0 bytes spill stores, 0 bytes spill loads
ptxas info    : Used 231 registers, 696 bytes cmem[0]
ptxas info    : Compiling entry function '_Z44flash_bwd_dq_dk_dv_loop_seqk_parallel_kernelI23Flash_bwd_kernel_traitsILi64ELi64ELi128ELi8ELi2ELi4ELi4ELb1ELb0EN7cutlass6half_tE19Flash_kernel_traitsILi64ELi64ELi128ELi8ES2_EELb0ELb1ELb0ELb1EEv16Flash_bwd_params' for 'sm_80'
ptxas info    : Function properties for _Z44flash_bwd_dq_dk_dv_loop_seqk_parallel_kernelI23Flash_bwd_kernel_traitsILi64ELi64ELi128ELi8ELi2ELi4ELi4ELb1ELb0EN7cutlass6half_tE19Flash_kernel_traitsILi64ELi64ELi128ELi8ES2_EELb0ELb1ELb0ELb1EEv16Flash_bwd_params
    0 bytes stack frame, 0 bytes spill stores, 0 bytes spill loads
ptxas info    : Used 229 registers, 696 bytes cmem[0]
ptxas info    : Compiling entry function '_Z44flash_bwd_dq_dk_dv_loop_seqk_parallel_kernelI23Flash_bwd_kernel_traitsILi64ELi64ELi128ELi8ELi2ELi4ELi4ELb1ELb0EN7cutlass6half_tE19Flash_kernel_traitsILi64ELi64ELi128ELi8ES2_EELb0ELb1ELb1ELb0EEv16Flash_bwd_params' for 'sm_80'
ptxas info    : Function properties for _Z44flash_bwd_dq_dk_dv_loop_seqk_parallel_kernelI23Flash_bwd_kernel_traitsILi64ELi64ELi128ELi8ELi2ELi4ELi4ELb1ELb0EN7cutlass6half_tE19Flash_kernel_traitsILi64ELi64ELi128ELi8ES2_EELb0ELb1ELb1ELb0EEv16Flash_bwd_params
    0 bytes stack frame, 0 bytes spill stores, 0 bytes spill loads
ptxas info    : Used 225 registers, 696 bytes cmem[0]
ptxas info    : Compiling entry function '_Z44flash_bwd_dq_dk_dv_loop_seqk_parallel_kernelI23Flash_bwd_kernel_traitsILi64ELi64ELi128ELi8ELi2ELi4ELi4ELb1ELb0EN7cutlass6half_tE19Flash_kernel_traitsILi64ELi64ELi128ELi8ES2_EELb0ELb1ELb1ELb1EEv16Flash_bwd_params' for 'sm_80'
ptxas info    : Function properties for _Z44flash_bwd_dq_dk_dv_loop_seqk_parallel_kernelI23Flash_bwd_kernel_traitsILi64ELi64ELi128ELi8ELi2ELi4ELi4ELb1ELb0EN7cutlass6half_tE19Flash_kernel_traitsILi64ELi64ELi128ELi8ES2_EELb0ELb1ELb1ELb1EEv16Flash_bwd_params
    0 bytes stack frame, 0 bytes spill stores, 0 bytes spill loads
ptxas info    : Used 230 registers, 696 bytes cmem[0]
ptxas info    : Compiling entry function '_Z44flash_bwd_dq_dk_dv_loop_seqk_parallel_kernelI23Flash_bwd_kernel_traitsILi64ELi128ELi128ELi8ELi4ELi4ELi4ELb0ELb0EN7cutlass6half_tE19Flash_kernel_traitsILi64ELi128ELi128ELi8ES2_EELb0ELb0ELb0ELb0EEv16Flash_bwd_params' for 'sm_80'
ptxas info    : Function properties for _Z44flash_bwd_dq_dk_dv_loop_seqk_parallel_kernelI23Flash_bwd_kernel_traitsILi64ELi128ELi128ELi8ELi4ELi4ELi4ELb0ELb0EN7cutlass6half_tE19Flash_kernel_traitsILi64ELi128ELi128ELi8ES2_EELb0ELb0ELb0ELb0EEv16Flash_bwd_params
    32 bytes stack frame, 44 bytes spill stores, 32 bytes spill loads
ptxas info    : Used 255 registers, 696 bytes cmem[0]
ptxas info    : Compiling entry function '_Z44flash_bwd_dq_dk_dv_loop_seqk_parallel_kernelI23Flash_bwd_kernel_traitsILi64ELi128ELi128ELi8ELi4ELi4ELi4ELb0ELb0EN7cutlass6half_tE19Flash_kernel_traitsILi64ELi128ELi128ELi8ES2_EELb0ELb0ELb0ELb1EEv16Flash_bwd_params' for 'sm_80'
ptxas info    : Function properties for _Z44flash_bwd_dq_dk_dv_loop_seqk_parallel_kernelI23Flash_bwd_kernel_traitsILi64ELi128ELi128ELi8ELi4ELi4ELi4ELb0ELb0EN7cutlass6half_tE19Flash_kernel_traitsILi64ELi128ELi128ELi8ES2_EELb0ELb0ELb0ELb1EEv16Flash_bwd_params
    32 bytes stack frame, 36 bytes spill stores, 28 bytes spill loads
ptxas info    : Used 255 registers, 696 bytes cmem[0]
ptxas info    : Compiling entry function '_Z44flash_bwd_dq_dk_dv_loop_seqk_parallel_kernelI23Flash_bwd_kernel_traitsILi64ELi128ELi128ELi8ELi4ELi4ELi4ELb0ELb0EN7cutlass6half_tE19Flash_kernel_traitsILi64ELi128ELi128ELi8ES2_EELb0ELb0ELb1ELb0EEv16Flash_bwd_params' for 'sm_80'
ptxas info    : Function properties for _Z44flash_bwd_dq_dk_dv_loop_seqk_parallel_kernelI23Flash_bwd_kernel_traitsILi64ELi128ELi128ELi8ELi4ELi4ELi4ELb0ELb0EN7cutlass6half_tE19Flash_kernel_traitsILi64ELi128ELi128ELi8ES2_EELb0ELb0ELb1ELb0EEv16Flash_bwd_params
    40 bytes stack frame, 64 bytes spill stores, 60 bytes spill loads
ptxas info    : Used 255 registers, 696 bytes cmem[0]
ptxas info    : Compiling entry function '_Z44flash_bwd_dq_dk_dv_loop_seqk_parallel_kernelI23Flash_bwd_kernel_traitsILi64ELi128ELi128ELi8ELi4ELi4ELi4ELb0ELb0EN7cutlass6half_tE19Flash_kernel_traitsILi64ELi128ELi128ELi8ES2_EELb0ELb0ELb1ELb1EEv16Flash_bwd_params' for 'sm_80'
ptxas info    : Function properties for _Z44flash_bwd_dq_dk_dv_loop_seqk_parallel_kernelI23Flash_bwd_kernel_traitsILi64ELi128ELi128ELi8ELi4ELi4ELi4ELb0ELb0EN7cutlass6half_tE19Flash_kernel_traitsILi64ELi128ELi128ELi8ES2_EELb0ELb0ELb1ELb1EEv16Flash_bwd_params
    40 bytes stack frame, 64 bytes spill stores, 64 bytes spill loads
ptxas info    : Used 255 registers, 696 bytes cmem[0]
ptxas info    : Compiling entry function '_Z44flash_bwd_dq_dk_dv_loop_seqk_parallel_kernelI23Flash_bwd_kernel_traitsILi64ELi128ELi128ELi8ELi4ELi4ELi4ELb0ELb0EN7cutlass6half_tE19Flash_kernel_traitsILi64ELi128ELi128ELi8ES2_EELb0ELb1ELb0ELb0EEv16Flash_bwd_params' for 'sm_80'
ptxas info    : Function properties for _Z44flash_bwd_dq_dk_dv_loop_seqk_parallel_kernelI23Flash_bwd_kernel_traitsILi64ELi128ELi128ELi8ELi4ELi4ELi4ELb0ELb0EN7cutlass6half_tE19Flash_kernel_traitsILi64ELi128ELi128ELi8ES2_EELb0ELb1ELb0ELb0EEv16Flash_bwd_params
    40 bytes stack frame, 40 bytes spill stores, 40 bytes spill loads
ptxas info    : Used 255 registers, 696 bytes cmem[0]
ptxas info    : Compiling entry function '_Z44flash_bwd_dq_dk_dv_loop_seqk_parallel_kernelI23Flash_bwd_kernel_traitsILi64ELi128ELi128ELi8ELi4ELi4ELi4ELb0ELb0EN7cutlass6half_tE19Flash_kernel_traitsILi64ELi128ELi128ELi8ES2_EELb0ELb1ELb0ELb1EEv16Flash_bwd_params' for 'sm_80'
ptxas info    : Function properties for _Z44flash_bwd_dq_dk_dv_loop_seqk_parallel_kernelI23Flash_bwd_kernel_traitsILi64ELi128ELi128ELi8ELi4ELi4ELi4ELb0ELb0EN7cutlass6half_tE19Flash_kernel_traitsILi64ELi128ELi128ELi8ES2_EELb0ELb1ELb0ELb1EEv16Flash_bwd_params
    48 bytes stack frame, 44 bytes spill stores, 44 bytes spill loads
ptxas info    : Used 255 registers, 696 bytes cmem[0]
ptxas info    : Compiling entry function '_Z44flash_bwd_dq_dk_dv_loop_seqk_parallel_kernelI23Flash_bwd_kernel_traitsILi64ELi128ELi128ELi8ELi4ELi4ELi4ELb0ELb0EN7cutlass6half_tE19Flash_kernel_traitsILi64ELi128ELi128ELi8ES2_EELb0ELb1ELb1ELb0EEv16Flash_bwd_params' for 'sm_80'
ptxas info    : Function properties for _Z44flash_bwd_dq_dk_dv_loop_seqk_parallel_kernelI23Flash_bwd_kernel_traitsILi64ELi128ELi128ELi8ELi4ELi4ELi4ELb0ELb0EN7cutlass6half_tE19Flash_kernel_traitsILi64ELi128ELi128ELi8ES2_EELb0ELb1ELb1ELb0EEv16Flash_bwd_params
    32 bytes stack frame, 36 bytes spill stores, 28 bytes spill loads
ptxas info    : Used 255 registers, 696 bytes cmem[0]
ptxas info    : Compiling entry function '_Z44flash_bwd_dq_dk_dv_loop_seqk_parallel_kernelI23Flash_bwd_kernel_traitsILi64ELi128ELi128ELi8ELi4ELi4ELi4ELb0ELb0EN7cutlass6half_tE19Flash_kernel_traitsILi64ELi128ELi128ELi8ES2_EELb0ELb1ELb1ELb1EEv16Flash_bwd_params' for 'sm_80'
ptxas info    : Function properties for _Z44flash_bwd_dq_dk_dv_loop_seqk_parallel_kernelI23Flash_bwd_kernel_traitsILi64ELi128ELi128ELi8ELi4ELi4ELi4ELb0ELb0EN7cutlass6half_tE19Flash_kernel_traitsILi64ELi128ELi128ELi8ES2_EELb0ELb1ELb1ELb1EEv16Flash_bwd_params
    0 bytes stack frame, 0 bytes spill stores, 0 bytes spill loads
ptxas info    : Used 255 registers, 696 bytes cmem[0]
ptxas info    : Compiling entry function '_Z27flash_bwd_convert_dq_kernelI23Flash_bwd_kernel_traitsILi64ELi64ELi128ELi8ELi2ELi4ELi4ELb1ELb0EN7cutlass6half_tE19Flash_kernel_traitsILi64ELi64ELi128ELi8ES2_EEEv16Flash_bwd_params' for 'sm_80'
ptxas info    : Function properties for _Z27flash_bwd_convert_dq_kernelI23Flash_bwd_kernel_traitsILi64ELi64ELi128ELi8ELi2ELi4ELi4ELb1ELb0EN7cutlass6half_tE19Flash_kernel_traitsILi64ELi64ELi128ELi8ES2_EEEv16Flash_bwd_params
    0 bytes stack frame, 0 bytes spill stores, 0 bytes spill loads
ptxas info    : Used 32 registers, 696 bytes cmem[0]
ptxas info    : Compiling entry function '_Z44flash_bwd_dq_dk_dv_loop_seqk_parallel_kernelI23Flash_bwd_kernel_traitsILi64ELi64ELi128ELi8ELi2ELi4ELi4ELb1ELb0EN7cutlass6half_tE19Flash_kernel_traitsILi64ELi64ELi128ELi8ES2_EELb1ELb0ELb0ELb0EEv16Flash_bwd_params' for 'sm_80'
ptxas info    : Function properties for _Z44flash_bwd_dq_dk_dv_loop_seqk_parallel_kernelI23Flash_bwd_kernel_traitsILi64ELi64ELi128ELi8ELi2ELi4ELi4ELb1ELb0EN7cutlass6half_tE19Flash_kernel_traitsILi64ELi64ELi128ELi8ES2_EELb1ELb0ELb0ELb0EEv16Flash_bwd_params
    0 bytes stack frame, 0 bytes spill stores, 0 bytes spill loads
ptxas info    : Used 237 registers, 696 bytes cmem[0]
ptxas info    : Compiling entry function '_Z44flash_bwd_dq_dk_dv_loop_seqk_parallel_kernelI23Flash_bwd_kernel_traitsILi64ELi64ELi128ELi8ELi2ELi4ELi4ELb1ELb0EN7cutlass6half_tE19Flash_kernel_traitsILi64ELi64ELi128ELi8ES2_EELb1ELb0ELb0ELb1EEv16Flash_bwd_params' for 'sm_80'
ptxas info    : Function properties for _Z44flash_bwd_dq_dk_dv_loop_seqk_parallel_kernelI23Flash_bwd_kernel_traitsILi64ELi64ELi128ELi8ELi2ELi4ELi4ELb1ELb0EN7cutlass6half_tE19Flash_kernel_traitsILi64ELi64ELi128ELi8ES2_EELb1ELb0ELb0ELb1EEv16Flash_bwd_params
    0 bytes stack frame, 0 bytes spill stores, 0 bytes spill loads
ptxas info    : Used 237 registers, 696 bytes cmem[0]
ptxas info    : Compiling entry function '_Z44flash_bwd_dq_dk_dv_loop_seqk_parallel_kernelI23Flash_bwd_kernel_traitsILi64ELi64ELi128ELi8ELi2ELi4ELi4ELb1ELb0EN7cutlass6half_tE19Flash_kernel_traitsILi64ELi64ELi128ELi8ES2_EELb1ELb0ELb1ELb0EEv16Flash_bwd_params' for 'sm_80'
ptxas info    : Function properties for _Z44flash_bwd_dq_dk_dv_loop_seqk_parallel_kernelI23Flash_bwd_kernel_traitsILi64ELi64ELi128ELi8ELi2ELi4ELi4ELb1ELb0EN7cutlass6half_tE19Flash_kernel_traitsILi64ELi64ELi128ELi8ES2_EELb1ELb0ELb1ELb0EEv16Flash_bwd_params
    0 bytes stack frame, 0 bytes spill stores, 0 bytes spill loads
ptxas info    : Used 222 registers, 696 bytes cmem[0]
ptxas info    : Compiling entry function '_Z44flash_bwd_dq_dk_dv_loop_seqk_parallel_kernelI23Flash_bwd_kernel_traitsILi64ELi64ELi128ELi8ELi2ELi4ELi4ELb1ELb0EN7cutlass6half_tE19Flash_kernel_traitsILi64ELi64ELi128ELi8ES2_EELb1ELb0ELb1ELb1EEv16Flash_bwd_params' for 'sm_80'
ptxas info    : Function properties for _Z44flash_bwd_dq_dk_dv_loop_seqk_parallel_kernelI23Flash_bwd_kernel_traitsILi64ELi64ELi128ELi8ELi2ELi4ELi4ELb1ELb0EN7cutlass6half_tE19Flash_kernel_traitsILi64ELi64ELi128ELi8ES2_EELb1ELb0ELb1ELb1EEv16Flash_bwd_params
    0 bytes stack frame, 0 bytes spill stores, 0 bytes spill loads
ptxas info    : Used 226 registers, 696 bytes cmem[0]
ptxas info    : Compiling entry function '_Z44flash_bwd_dq_dk_dv_loop_seqk_parallel_kernelI23Flash_bwd_kernel_traitsILi64ELi64ELi128ELi8ELi2ELi4ELi4ELb1ELb0EN7cutlass6half_tE19Flash_kernel_traitsILi64ELi64ELi128ELi8ES2_EELb1ELb1ELb0ELb0EEv16Flash_bwd_params' for 'sm_80'
ptxas info    : Function properties for _Z44flash_bwd_dq_dk_dv_loop_seqk_parallel_kernelI23Flash_bwd_kernel_traitsILi64ELi64ELi128ELi8ELi2ELi4ELi4ELb1ELb0EN7cutlass6half_tE19Flash_kernel_traitsILi64ELi64ELi128ELi8ES2_EELb1ELb1ELb0ELb0EEv16Flash_bwd_params
    0 bytes stack frame, 0 bytes spill stores, 0 bytes spill loads
ptxas info    : Used 236 registers, 696 bytes cmem[0]
ptxas info    : Compiling entry function '_Z44flash_bwd_dq_dk_dv_loop_seqk_parallel_kernelI23Flash_bwd_kernel_traitsILi64ELi64ELi128ELi8ELi2ELi4ELi4ELb1ELb0EN7cutlass6half_tE19Flash_kernel_traitsILi64ELi64ELi128ELi8ES2_EELb1ELb1ELb0ELb1EEv16Flash_bwd_params' for 'sm_80'
ptxas info    : Function properties for _Z44flash_bwd_dq_dk_dv_loop_seqk_parallel_kernelI23Flash_bwd_kernel_traitsILi64ELi64ELi128ELi8ELi2ELi4ELi4ELb1ELb0EN7cutlass6half_tE19Flash_kernel_traitsILi64ELi64ELi128ELi8ES2_EELb1ELb1ELb0ELb1EEv16Flash_bwd_params
    0 bytes stack frame, 0 bytes spill stores, 0 bytes spill loads
ptxas info    : Used 238 registers, 696 bytes cmem[0]
ptxas info    : Compiling entry function '_Z44flash_bwd_dq_dk_dv_loop_seqk_parallel_kernelI23Flash_bwd_kernel_traitsILi64ELi64ELi128ELi8ELi2ELi4ELi4ELb1ELb0EN7cutlass6half_tE19Flash_kernel_traitsILi64ELi64ELi128ELi8ES2_EELb1ELb1ELb1ELb0EEv16Flash_bwd_params' for 'sm_80'
ptxas info    : Function properties for _Z44flash_bwd_dq_dk_dv_loop_seqk_parallel_kernelI23Flash_bwd_kernel_traitsILi64ELi64ELi128ELi8ELi2ELi4ELi4ELb1ELb0EN7cutlass6half_tE19Flash_kernel_traitsILi64ELi64ELi128ELi8ES2_EELb1ELb1ELb1ELb0EEv16Flash_bwd_params
    0 bytes stack frame, 0 bytes spill stores, 0 bytes spill loads
ptxas info    : Used 234 registers, 696 bytes cmem[0]
ptxas info    : Compiling entry function '_Z44flash_bwd_dq_dk_dv_loop_seqk_parallel_kernelI23Flash_bwd_kernel_traitsILi64ELi64ELi128ELi8ELi2ELi4ELi4ELb1ELb0EN7cutlass6half_tE19Flash_kernel_traitsILi64ELi64ELi128ELi8ES2_EELb1ELb1ELb1ELb1EEv16Flash_bwd_params' for 'sm_80'
ptxas info    : Function properties for _Z44flash_bwd_dq_dk_dv_loop_seqk_parallel_kernelI23Flash_bwd_kernel_traitsILi64ELi64ELi128ELi8ELi2ELi4ELi4ELb1ELb0EN7cutlass6half_tE19Flash_kernel_traitsILi64ELi64ELi128ELi8ES2_EELb1ELb1ELb1ELb1EEv16Flash_bwd_params
    0 bytes stack frame, 0 bytes spill stores, 0 bytes spill loads
ptxas info    : Used 239 registers, 696 bytes cmem[0]
ptxas info    : Compiling entry function '_Z25flash_bwd_dot_do_o_kernelILb1E23Flash_bwd_kernel_traitsILi64ELi64ELi128ELi8ELi2ELi4ELi4ELb1ELb0EN7cutlass6half_tE19Flash_kernel_traitsILi64ELi64ELi128ELi8ES2_EEEv16Flash_bwd_params' for 'sm_80'
ptxas info    : Function properties for _Z25flash_bwd_dot_do_o_kernelILb1E23Flash_bwd_kernel_traitsILi64ELi64ELi128ELi8ELi2ELi4ELi4ELb1ELb0EN7cutlass6half_tE19Flash_kernel_traitsILi64ELi64ELi128ELi8ES2_EEEv16Flash_bwd_params
    0 bytes stack frame, 0 bytes spill stores, 0 bytes spill loads
ptxas info    : Used 31 registers, 696 bytes cmem[0]
ptxas info    : Compiling entry function '_Z27flash_bwd_convert_dq_kernelI23Flash_bwd_kernel_traitsILi64ELi128ELi128ELi8ELi4ELi4ELi4ELb0ELb0EN7cutlass6half_tE19Flash_kernel_traitsILi64ELi128ELi128ELi8ES2_EEEv16Flash_bwd_params' for 'sm_80'
ptxas info    : Function properties for _Z27flash_bwd_convert_dq_kernelI23Flash_bwd_kernel_traitsILi64ELi128ELi128ELi8ELi4ELi4ELi4ELb0ELb0EN7cutlass6half_tE19Flash_kernel_traitsILi64ELi128ELi128ELi8ES2_EEEv16Flash_bwd_params
    0 bytes stack frame, 0 bytes spill stores, 0 bytes spill loads
ptxas info    : Used 32 registers, 696 bytes cmem[0]
ptxas info    : Compiling entry function '_Z44flash_bwd_dq_dk_dv_loop_seqk_parallel_kernelI23Flash_bwd_kernel_traitsILi64ELi128ELi128ELi8ELi4ELi4ELi4ELb0ELb0EN7cutlass6half_tE19Flash_kernel_traitsILi64ELi128ELi128ELi8ES2_EELb1ELb0ELb0ELb0EEv16Flash_bwd_params' for 'sm_80'
ptxas info    : Function properties for _Z44flash_bwd_dq_dk_dv_loop_seqk_parallel_kernelI23Flash_bwd_kernel_traitsILi64ELi128ELi128ELi8ELi4ELi4ELi4ELb0ELb0EN7cutlass6half_tE19Flash_kernel_traitsILi64ELi128ELi128ELi8ES2_EELb1ELb0ELb0ELb0EEv16Flash_bwd_params
    48 bytes stack frame, 48 bytes spill stores, 44 bytes spill loads
ptxas info    : Used 255 registers, 696 bytes cmem[0]
ptxas info    : Compiling entry function '_Z44flash_bwd_dq_dk_dv_loop_seqk_parallel_kernelI23Flash_bwd_kernel_traitsILi64ELi128ELi128ELi8ELi4ELi4ELi4ELb0ELb0EN7cutlass6half_tE19Flash_kernel_traitsILi64ELi128ELi128ELi8ES2_EELb1ELb0ELb0ELb1EEv16Flash_bwd_params' for 'sm_80'
ptxas info    : Function properties for _Z44flash_bwd_dq_dk_dv_loop_seqk_parallel_kernelI23Flash_bwd_kernel_traitsILi64ELi128ELi128ELi8ELi4ELi4ELi4ELb0ELb0EN7cutlass6half_tE19Flash_kernel_traitsILi64ELi128ELi128ELi8ES2_EELb1ELb0ELb0ELb1EEv16Flash_bwd_params
    56 bytes stack frame, 72 bytes spill stores, 72 bytes spill loads
ptxas info    : Used 255 registers, 696 bytes cmem[0]
ptxas info    : Compiling entry function '_Z44flash_bwd_dq_dk_dv_loop_seqk_parallel_kernelI23Flash_bwd_kernel_traitsILi64ELi128ELi128ELi8ELi4ELi4ELi4ELb0ELb0EN7cutlass6half_tE19Flash_kernel_traitsILi64ELi128ELi128ELi8ES2_EELb1ELb0ELb1ELb0EEv16Flash_bwd_params' for 'sm_80'
ptxas info    : Function properties for _Z44flash_bwd_dq_dk_dv_loop_seqk_parallel_kernelI23Flash_bwd_kernel_traitsILi64ELi128ELi128ELi8ELi4ELi4ELi4ELb0ELb0EN7cutlass6half_tE19Flash_kernel_traitsILi64ELi128ELi128ELi8ES2_EELb1ELb0ELb1ELb0EEv16Flash_bwd_params
    56 bytes stack frame, 68 bytes spill stores, 72 bytes spill loads
ptxas info    : Used 255 registers, 696 bytes cmem[0]
ptxas info    : Compiling entry function '_Z44flash_bwd_dq_dk_dv_loop_seqk_parallel_kernelI23Flash_bwd_kernel_traitsILi64ELi128ELi128ELi8ELi4ELi4ELi4ELb0ELb0EN7cutlass6half_tE19Flash_kernel_traitsILi64ELi128ELi128ELi8ES2_EELb1ELb0ELb1ELb1EEv16Flash_bwd_params' for 'sm_80'
ptxas info    : Function properties for _Z44flash_bwd_dq_dk_dv_loop_seqk_parallel_kernelI23Flash_bwd_kernel_traitsILi64ELi128ELi128ELi8ELi4ELi4ELi4ELb0ELb0EN7cutlass6half_tE19Flash_kernel_traitsILi64ELi128ELi128ELi8ES2_EELb1ELb0ELb1ELb1EEv16Flash_bwd_params
    56 bytes stack frame, 72 bytes spill stores, 80 bytes spill loads
ptxas info    : Used 255 registers, 696 bytes cmem[0]
ptxas info    : Compiling entry function '_Z44flash_bwd_dq_dk_dv_loop_seqk_parallel_kernelI23Flash_bwd_kernel_traitsILi64ELi128ELi128ELi8ELi4ELi4ELi4ELb0ELb0EN7cutlass6half_tE19Flash_kernel_traitsILi64ELi128ELi128ELi8ES2_EELb1ELb1ELb0ELb0EEv16Flash_bwd_params' for 'sm_80'
ptxas info    : Function properties for _Z44flash_bwd_dq_dk_dv_loop_seqk_parallel_kernelI23Flash_bwd_kernel_traitsILi64ELi128ELi128ELi8ELi4ELi4ELi4ELb0ELb0EN7cutlass6half_tE19Flash_kernel_traitsILi64ELi128ELi128ELi8ES2_EELb1ELb1ELb0ELb0EEv16Flash_bwd_params
    24 bytes stack frame, 20 bytes spill stores, 20 bytes spill loads
ptxas info    : Used 255 registers, 696 bytes cmem[0]
ptxas info    : Compiling entry function '_Z44flash_bwd_dq_dk_dv_loop_seqk_parallel_kernelI23Flash_bwd_kernel_traitsILi64ELi128ELi128ELi8ELi4ELi4ELi4ELb0ELb0EN7cutlass6half_tE19Flash_kernel_traitsILi64ELi128ELi128ELi8ES2_EELb1ELb1ELb0ELb1EEv16Flash_bwd_params' for 'sm_80'
ptxas info    : Function properties for _Z44flash_bwd_dq_dk_dv_loop_seqk_parallel_kernelI23Flash_bwd_kernel_traitsILi64ELi128ELi128ELi8ELi4ELi4ELi4ELb0ELb0EN7cutlass6half_tE19Flash_kernel_traitsILi64ELi128ELi128ELi8ES2_EELb1ELb1ELb0ELb1EEv16Flash_bwd_params
    32 bytes stack frame, 28 bytes spill stores, 28 bytes spill loads
ptxas info    : Used 255 registers, 696 bytes cmem[0]
ptxas info    : Compiling entry function '_Z44flash_bwd_dq_dk_dv_loop_seqk_parallel_kernelI23Flash_bwd_kernel_traitsILi64ELi128ELi128ELi8ELi4ELi4ELi4ELb0ELb0EN7cutlass6half_tE19Flash_kernel_traitsILi64ELi128ELi128ELi8ES2_EELb1ELb1ELb1ELb0EEv16Flash_bwd_params' for 'sm_80'
ptxas info    : Function properties for _Z44flash_bwd_dq_dk_dv_loop_seqk_parallel_kernelI23Flash_bwd_kernel_traitsILi64ELi128ELi128ELi8ELi4ELi4ELi4ELb0ELb0EN7cutlass6half_tE19Flash_kernel_traitsILi64ELi128ELi128ELi8ES2_EELb1ELb1ELb1ELb0EEv16Flash_bwd_params
    48 bytes stack frame, 60 bytes spill stores, 56 bytes spill loads
ptxas info    : Used 255 registers, 696 bytes cmem[0]
ptxas info    : Compiling entry function '_Z44flash_bwd_dq_dk_dv_loop_seqk_parallel_kernelI23Flash_bwd_kernel_traitsILi64ELi128ELi128ELi8ELi4ELi4ELi4ELb0ELb0EN7cutlass6half_tE19Flash_kernel_traitsILi64ELi128ELi128ELi8ES2_EELb1ELb1ELb1ELb1EEv16Flash_bwd_params' for 'sm_80'
ptxas info    : Function properties for _Z44flash_bwd_dq_dk_dv_loop_seqk_parallel_kernelI23Flash_bwd_kernel_traitsILi64ELi128ELi128ELi8ELi4ELi4ELi4ELb0ELb0EN7cutlass6half_tE19Flash_kernel_traitsILi64ELi128ELi128ELi8ES2_EELb1ELb1ELb1ELb1EEv16Flash_bwd_params
    16 bytes stack frame, 16 bytes spill stores, 16 bytes spill loads
ptxas info    : Used 255 registers, 696 bytes cmem[0]
ptxas info    : Compiling entry function '_Z25flash_bwd_dot_do_o_kernelILb1E23Flash_bwd_kernel_traitsILi64ELi128ELi128ELi8ELi4ELi4ELi4ELb0ELb0EN7cutlass6half_tE19Flash_kernel_traitsILi64ELi128ELi128ELi8ES2_EEEv16Flash_bwd_params' for 'sm_80'
ptxas info    : Function properties for _Z25flash_bwd_dot_do_o_kernelILb1E23Flash_bwd_kernel_traitsILi64ELi128ELi128ELi8ELi4ELi4ELi4ELb0ELb0EN7cutlass6half_tE19Flash_kernel_traitsILi64ELi128ELi128ELi8ES2_EEEv16Flash_bwd_params
    0 bytes stack frame, 0 bytes spill stores, 0 bytes spill loads
ptxas info    : Used 46 registers, 696 bytes cmem[0]
/anaconda/envs/flashattention/lib/python3.9/site-packages/torch/include/c10/core/SymInt.h(84): warning: integer conversion resulted in a change of sign

[20/37] /usr/local/cuda-11.4/bin/nvcc  -I/home/v-feiychen/flash-attention/csrc/flash_attn -I/home/v-feiychen/flash-attention/csrc/flash_attn/src -I/home/v-feiychen/flash-attention/csrc/cutlass/include -I/anaconda/envs/flashattention/lib/python3.9/site-packages/torch/include -I/anaconda/envs/flashattention/lib/python3.9/site-packages/torch/include/torch/csrc/api/include -I/anaconda/envs/flashattention/lib/python3.9/site-packages/torch/include/TH -I/anaconda/envs/flashattention/lib/python3.9/site-packages/torch/include/THC -I/usr/local/cuda-11.4/include -I/anaconda/envs/flashattention/include/python3.9 -c -c /home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_bwd_hdim128_bf16_sm80.cu -o /home/v-feiychen/flash-attention/build/temp.linux-x86_64-cpython-39/csrc/flash_attn/src/flash_bwd_hdim128_bf16_sm80.o -D__CUDA_NO_HALF_OPERATORS__ -D__CUDA_NO_HALF_CONVERSIONS__ -D__CUDA_NO_BFLOAT16_CONVERSIONS__ -D__CUDA_NO_HALF2_OPERATORS__ --expt-relaxed-constexpr --compiler-options ''"'"'-fPIC'"'"'' -O3 -std=c++17 -U__CUDA_NO_HALF_OPERATORS__ -U__CUDA_NO_HALF_CONVERSIONS__ -U__CUDA_NO_HALF2_OPERATORS__ -U__CUDA_NO_BFLOAT16_CONVERSIONS__ --expt-relaxed-constexpr --expt-extended-lambda --use_fast_math --ptxas-options=-v -lineinfo -gencode arch=compute_80,code=sm_80 --threads 4 -DTORCH_API_INCLUDE_EXTENSION_H '-DPYBIND11_COMPILER_TYPE="_gcc"' '-DPYBIND11_STDLIB="_libstdcpp"' '-DPYBIND11_BUILD_ABI="_cxxabi1011"' -DTORCH_EXTENSION_NAME=flash_attn_2_cuda -D_GLIBCXX_USE_CXX11_ABI=0
/anaconda/envs/flashattention/lib/python3.9/site-packages/torch/include/c10/core/SymInt.h(84): warning: integer conversion resulted in a change of sign

ptxas info    : 2 bytes gmem
ptxas info    : Compiling entry function '_Z44flash_bwd_dq_dk_dv_loop_seqk_parallel_kernelI23Flash_bwd_kernel_traitsILi128ELi64ELi64ELi8ELi4ELi2ELi2ELb1ELb0EN7cutlass10bfloat16_tE19Flash_kernel_traitsILi128ELi64ELi64ELi8ES2_EELb0ELb0ELb0ELb0EEv16Flash_bwd_params' for 'sm_80'
ptxas info    : Function properties for _Z44flash_bwd_dq_dk_dv_loop_seqk_parallel_kernelI23Flash_bwd_kernel_traitsILi128ELi64ELi64ELi8ELi4ELi2ELi2ELb1ELb0EN7cutlass10bfloat16_tE19Flash_kernel_traitsILi128ELi64ELi64ELi8ES2_EELb0ELb0ELb0ELb0EEv16Flash_bwd_params
    0 bytes stack frame, 0 bytes spill stores, 0 bytes spill loads
ptxas info    : Used 242 registers, 696 bytes cmem[0]
ptxas info    : Compiling entry function '_Z44flash_bwd_dq_dk_dv_loop_seqk_parallel_kernelI23Flash_bwd_kernel_traitsILi128ELi64ELi64ELi8ELi4ELi2ELi2ELb1ELb0EN7cutlass10bfloat16_tE19Flash_kernel_traitsILi128ELi64ELi64ELi8ES2_EELb0ELb0ELb0ELb1EEv16Flash_bwd_params' for 'sm_80'
ptxas info    : Function properties for _Z44flash_bwd_dq_dk_dv_loop_seqk_parallel_kernelI23Flash_bwd_kernel_traitsILi128ELi64ELi64ELi8ELi4ELi2ELi2ELb1ELb0EN7cutlass10bfloat16_tE19Flash_kernel_traitsILi128ELi64ELi64ELi8ES2_EELb0ELb0ELb0ELb1EEv16Flash_bwd_params
    0 bytes stack frame, 0 bytes spill stores, 0 bytes spill loads
ptxas info    : Used 241 registers, 696 bytes cmem[0]
ptxas info    : Compiling entry function '_Z44flash_bwd_dq_dk_dv_loop_seqk_parallel_kernelI23Flash_bwd_kernel_traitsILi128ELi64ELi64ELi8ELi4ELi2ELi2ELb1ELb0EN7cutlass10bfloat16_tE19Flash_kernel_traitsILi128ELi64ELi64ELi8ES2_EELb0ELb0ELb1ELb0EEv16Flash_bwd_params' for 'sm_80'
ptxas info    : Function properties for _Z44flash_bwd_dq_dk_dv_loop_seqk_parallel_kernelI23Flash_bwd_kernel_traitsILi128ELi64ELi64ELi8ELi4ELi2ELi2ELb1ELb0EN7cutlass10bfloat16_tE19Flash_kernel_traitsILi128ELi64ELi64ELi8ES2_EELb0ELb0ELb1ELb0EEv16Flash_bwd_params
    0 bytes stack frame, 0 bytes spill stores, 0 bytes spill loads
ptxas info    : Used 239 registers, 696 bytes cmem[0]
ptxas info    : Compiling entry function '_Z44flash_bwd_dq_dk_dv_loop_seqk_parallel_kernelI23Flash_bwd_kernel_traitsILi128ELi64ELi64ELi8ELi4ELi2ELi2ELb1ELb0EN7cutlass10bfloat16_tE19Flash_kernel_traitsILi128ELi64ELi64ELi8ES2_EELb0ELb0ELb1ELb1EEv16Flash_bwd_params' for 'sm_80'
ptxas info    : Function properties for _Z44flash_bwd_dq_dk_dv_loop_seqk_parallel_kernelI23Flash_bwd_kernel_traitsILi128ELi64ELi64ELi8ELi4ELi2ELi2ELb1ELb0EN7cutlass10bfloat16_tE19Flash_kernel_traitsILi128ELi64ELi64ELi8ES2_EELb0ELb0ELb1ELb1EEv16Flash_bwd_params
    0 bytes stack frame, 0 bytes spill stores, 0 bytes spill loads
ptxas info    : Used 236 registers, 696 bytes cmem[0]
ptxas info    : Compiling entry function '_Z44flash_bwd_dq_dk_dv_loop_seqk_parallel_kernelI23Flash_bwd_kernel_traitsILi128ELi64ELi64ELi8ELi4ELi2ELi2ELb1ELb0EN7cutlass10bfloat16_tE19Flash_kernel_traitsILi128ELi64ELi64ELi8ES2_EELb0ELb1ELb0ELb0EEv16Flash_bwd_params' for 'sm_80'
ptxas info    : Function properties for _Z44flash_bwd_dq_dk_dv_loop_seqk_parallel_kernelI23Flash_bwd_kernel_traitsILi128ELi64ELi64ELi8ELi4ELi2ELi2ELb1ELb0EN7cutlass10bfloat16_tE19Flash_kernel_traitsILi128ELi64ELi64ELi8ES2_EELb0ELb1ELb0ELb0EEv16Flash_bwd_params
    0 bytes stack frame, 0 bytes spill stores, 0 bytes spill loads
ptxas info    : Used 240 registers, 696 bytes cmem[0]
ptxas info    : Compiling entry function '_Z44flash_bwd_dq_dk_dv_loop_seqk_parallel_kernelI23Flash_bwd_kernel_traitsILi128ELi64ELi64ELi8ELi4ELi2ELi2ELb1ELb0EN7cutlass10bfloat16_tE19Flash_kernel_traitsILi128ELi64ELi64ELi8ES2_EELb0ELb1ELb0ELb1EEv16Flash_bwd_params' for 'sm_80'
ptxas info    : Function properties for _Z44flash_bwd_dq_dk_dv_loop_seqk_parallel_kernelI23Flash_bwd_kernel_traitsILi128ELi64ELi64ELi8ELi4ELi2ELi2ELb1ELb0EN7cutlass10bfloat16_tE19Flash_kernel_traitsILi128ELi64ELi64ELi8ES2_EELb0ELb1ELb0ELb1EEv16Flash_bwd_params
    0 bytes stack frame, 0 bytes spill stores, 0 bytes spill loads
ptxas info    : Used 240 registers, 696 bytes cmem[0]
ptxas info    : Compiling entry function '_Z44flash_bwd_dq_dk_dv_loop_seqk_parallel_kernelI23Flash_bwd_kernel_traitsILi128ELi64ELi64ELi8ELi4ELi2ELi2ELb1ELb0EN7cutlass10bfloat16_tE19Flash_kernel_traitsILi128ELi64ELi64ELi8ES2_EELb0ELb1ELb1ELb0EEv16Flash_bwd_params' for 'sm_80'
ptxas info    : Function properties for _Z44flash_bwd_dq_dk_dv_loop_seqk_parallel_kernelI23Flash_bwd_kernel_traitsILi128ELi64ELi64ELi8ELi4ELi2ELi2ELb1ELb0EN7cutlass10bfloat16_tE19Flash_kernel_traitsILi128ELi64ELi64ELi8ES2_EELb0ELb1ELb1ELb0EEv16Flash_bwd_params
    0 bytes stack frame, 0 bytes spill stores, 0 bytes spill loads
ptxas info    : Used 238 registers, 696 bytes cmem[0]
ptxas info    : Compiling entry function '_Z44flash_bwd_dq_dk_dv_loop_seqk_parallel_kernelI23Flash_bwd_kernel_traitsILi128ELi64ELi64ELi8ELi4ELi2ELi2ELb1ELb0EN7cutlass10bfloat16_tE19Flash_kernel_traitsILi128ELi64ELi64ELi8ES2_EELb0ELb1ELb1ELb1EEv16Flash_bwd_params' for 'sm_80'
ptxas info    : Function properties for _Z44flash_bwd_dq_dk_dv_loop_seqk_parallel_kernelI23Flash_bwd_kernel_traitsILi128ELi64ELi64ELi8ELi4ELi2ELi2ELb1ELb0EN7cutlass10bfloat16_tE19Flash_kernel_traitsILi128ELi64ELi64ELi8ES2_EELb0ELb1ELb1ELb1EEv16Flash_bwd_params
    0 bytes stack frame, 0 bytes spill stores, 0 bytes spill loads
ptxas info    : Used 240 registers, 696 bytes cmem[0]
ptxas info    : Compiling entry function '_Z44flash_bwd_dq_dk_dv_loop_seqk_parallel_kernelI23Flash_bwd_kernel_traitsILi128ELi64ELi128ELi8ELi2ELi4ELi2ELb0ELb0EN7cutlass10bfloat16_tE19Flash_kernel_traitsILi128ELi64ELi128ELi8ES2_EELb0ELb0ELb0ELb0EEv16Flash_bwd_params' for 'sm_80'
ptxas info    : Function properties for _Z44flash_bwd_dq_dk_dv_loop_seqk_parallel_kernelI23Flash_bwd_kernel_traitsILi128ELi64ELi128ELi8ELi2ELi4ELi2ELb0ELb0EN7cutlass10bfloat16_tE19Flash_kernel_traitsILi128ELi64ELi128ELi8ES2_EELb0ELb0ELb0ELb0EEv16Flash_bwd_params
    136 bytes stack frame, 164 bytes spill stores, 152 bytes spill loads
ptxas info    : Used 255 registers, 696 bytes cmem[0]
ptxas info    : Compiling entry function '_Z44flash_bwd_dq_dk_dv_loop_seqk_parallel_kernelI23Flash_bwd_kernel_traitsILi128ELi64ELi128ELi8ELi2ELi4ELi2ELb0ELb0EN7cutlass10bfloat16_tE19Flash_kernel_traitsILi128ELi64ELi128ELi8ES2_EELb0ELb0ELb0ELb1EEv16Flash_bwd_params' for 'sm_80'
ptxas info    : Function properties for _Z44flash_bwd_dq_dk_dv_loop_seqk_parallel_kernelI23Flash_bwd_kernel_traitsILi128ELi64ELi128ELi8ELi2ELi4ELi2ELb0ELb0EN7cutlass10bfloat16_tE19Flash_kernel_traitsILi128ELi64ELi128ELi8ES2_EELb0ELb0ELb0ELb1EEv16Flash_bwd_params
    136 bytes stack frame, 168 bytes spill stores, 180 bytes spill loads
ptxas info    : Used 255 registers, 696 bytes cmem[0]
ptxas info    : Compiling entry function '_Z44flash_bwd_dq_dk_dv_loop_seqk_parallel_kernelI23Flash_bwd_kernel_traitsILi128ELi64ELi128ELi8ELi2ELi4ELi2ELb0ELb0EN7cutlass10bfloat16_tE19Flash_kernel_traitsILi128ELi64ELi128ELi8ES2_EELb0ELb0ELb1ELb0EEv16Flash_bwd_params' for 'sm_80'
ptxas info    : Function properties for _Z44flash_bwd_dq_dk_dv_loop_seqk_parallel_kernelI23Flash_bwd_kernel_traitsILi128ELi64ELi128ELi8ELi2ELi4ELi2ELb0ELb0EN7cutlass10bfloat16_tE19Flash_kernel_traitsILi128ELi64ELi128ELi8ES2_EELb0ELb0ELb1ELb0EEv16Flash_bwd_params
    120 bytes stack frame, 156 bytes spill stores, 168 bytes spill loads
ptxas info    : Used 255 registers, 696 bytes cmem[0]
ptxas info    : Compiling entry function '_Z44flash_bwd_dq_dk_dv_loop_seqk_parallel_kernelI23Flash_bwd_kernel_traitsILi128ELi64ELi128ELi8ELi2ELi4ELi2ELb0ELb0EN7cutlass10bfloat16_tE19Flash_kernel_traitsILi128ELi64ELi128ELi8ES2_EELb0ELb0ELb1ELb1EEv16Flash_bwd_params' for 'sm_80'
ptxas info    : Function properties for _Z44flash_bwd_dq_dk_dv_loop_seqk_parallel_kernelI23Flash_bwd_kernel_traitsILi128ELi64ELi128ELi8ELi2ELi4ELi2ELb0ELb0EN7cutlass10bfloat16_tE19Flash_kernel_traitsILi128ELi64ELi128ELi8ES2_EELb0ELb0ELb1ELb1EEv16Flash_bwd_params
    136 bytes stack frame, 160 bytes spill stores, 176 bytes spill loads
ptxas info    : Used 255 registers, 696 bytes cmem[0]
ptxas info    : Compiling entry function '_Z44flash_bwd_dq_dk_dv_loop_seqk_parallel_kernelI23Flash_bwd_kernel_traitsILi128ELi64ELi128ELi8ELi2ELi4ELi2ELb0ELb0EN7cutlass10bfloat16_tE19Flash_kernel_traitsILi128ELi64ELi128ELi8ES2_EELb0ELb1ELb0ELb0EEv16Flash_bwd_params' for 'sm_80'
ptxas info    : Function properties for _Z44flash_bwd_dq_dk_dv_loop_seqk_parallel_kernelI23Flash_bwd_kernel_traitsILi128ELi64ELi128ELi8ELi2ELi4ELi2ELb0ELb0EN7cutlass10bfloat16_tE19Flash_kernel_traitsILi128ELi64ELi128ELi8ES2_EELb0ELb1ELb0ELb0EEv16Flash_bwd_params
    120 bytes stack frame, 144 bytes spill stores, 132 bytes spill loads
ptxas info    : Used 255 registers, 696 bytes cmem[0]
ptxas info    : Compiling entry function '_Z44flash_bwd_dq_dk_dv_loop_seqk_parallel_kernelI23Flash_bwd_kernel_traitsILi128ELi64ELi128ELi8ELi2ELi4ELi2ELb0ELb0EN7cutlass10bfloat16_tE19Flash_kernel_traitsILi128ELi64ELi128ELi8ES2_EELb0ELb1ELb0ELb1EEv16Flash_bwd_params' for 'sm_80'
ptxas info    : Function properties for _Z44flash_bwd_dq_dk_dv_loop_seqk_parallel_kernelI23Flash_bwd_kernel_traitsILi128ELi64ELi128ELi8ELi2ELi4ELi2ELb0ELb0EN7cutlass10bfloat16_tE19Flash_kernel_traitsILi128ELi64ELi128ELi8ES2_EELb0ELb1ELb0ELb1EEv16Flash_bwd_params
    144 bytes stack frame, 168 bytes spill stores, 180 bytes spill loads
ptxas info    : Used 255 registers, 696 bytes cmem[0]
ptxas info    : Compiling entry function '_Z44flash_bwd_dq_dk_dv_loop_seqk_parallel_kernelI23Flash_bwd_kernel_traitsILi128ELi64ELi128ELi8ELi2ELi4ELi2ELb0ELb0EN7cutlass10bfloat16_tE19Flash_kernel_traitsILi128ELi64ELi128ELi8ES2_EELb0ELb1ELb1ELb0EEv16Flash_bwd_params' for 'sm_80'
ptxas info    : Function properties for _Z44flash_bwd_dq_dk_dv_loop_seqk_parallel_kernelI23Flash_bwd_kernel_traitsILi128ELi64ELi128ELi8ELi2ELi4ELi2ELb0ELb0EN7cutlass10bfloat16_tE19Flash_kernel_traitsILi128ELi64ELi128ELi8ES2_EELb0ELb1ELb1ELb0EEv16Flash_bwd_params
    136 bytes stack frame, 156 bytes spill stores, 152 bytes spill loads
ptxas info    : Used 255 registers, 696 bytes cmem[0]
ptxas info    : Compiling entry function '_Z44flash_bwd_dq_dk_dv_loop_seqk_parallel_kernelI23Flash_bwd_kernel_traitsILi128ELi64ELi128ELi8ELi2ELi4ELi2ELb0ELb0EN7cutlass10bfloat16_tE19Flash_kernel_traitsILi128ELi64ELi128ELi8ES2_EELb0ELb1ELb1ELb1EEv16Flash_bwd_params' for 'sm_80'
ptxas info    : Function properties for _Z44flash_bwd_dq_dk_dv_loop_seqk_parallel_kernelI23Flash_bwd_kernel_traitsILi128ELi64ELi128ELi8ELi2ELi4ELi2ELb0ELb0EN7cutlass10bfloat16_tE19Flash_kernel_traitsILi128ELi64ELi128ELi8ES2_EELb0ELb1ELb1ELb1EEv16Flash_bwd_params
    144 bytes stack frame, 164 bytes spill stores, 156 bytes spill loads
ptxas info    : Used 255 registers, 696 bytes cmem[0]
ptxas info    : Compiling entry function '_Z27flash_bwd_convert_dq_kernelI23Flash_bwd_kernel_traitsILi128ELi64ELi64ELi8ELi4ELi2ELi2ELb1ELb0EN7cutlass10bfloat16_tE19Flash_kernel_traitsILi128ELi64ELi64ELi8ES2_EEEv16Flash_bwd_params' for 'sm_80'
ptxas info    : Function properties for _Z27flash_bwd_convert_dq_kernelI23Flash_bwd_kernel_traitsILi128ELi64ELi64ELi8ELi4ELi2ELi2ELb1ELb0EN7cutlass10bfloat16_tE19Flash_kernel_traitsILi128ELi64ELi64ELi8ES2_EEEv16Flash_bwd_params
    0 bytes stack frame, 0 bytes spill stores, 0 bytes spill loads
ptxas info    : Used 32 registers, 696 bytes cmem[0]
ptxas info    : Compiling entry function '_Z44flash_bwd_dq_dk_dv_loop_seqk_parallel_kernelI23Flash_bwd_kernel_traitsILi128ELi64ELi64ELi8ELi4ELi2ELi2ELb1ELb0EN7cutlass10bfloat16_tE19Flash_kernel_traitsILi128ELi64ELi64ELi8ES2_EELb1ELb0ELb0ELb0EEv16Flash_bwd_params' for 'sm_80'
ptxas info    : Function properties for _Z44flash_bwd_dq_dk_dv_loop_seqk_parallel_kernelI23Flash_bwd_kernel_traitsILi128ELi64ELi64ELi8ELi4ELi2ELi2ELb1ELb0EN7cutlass10bfloat16_tE19Flash_kernel_traitsILi128ELi64ELi64ELi8ES2_EELb1ELb0ELb0ELb0EEv16Flash_bwd_params
    0 bytes stack frame, 0 bytes spill stores, 0 bytes spill loads
ptxas info    : Used 239 registers, 696 bytes cmem[0]
ptxas info    : Compiling entry function '_Z44flash_bwd_dq_dk_dv_loop_seqk_parallel_kernelI23Flash_bwd_kernel_traitsILi128ELi64ELi64ELi8ELi4ELi2ELi2ELb1ELb0EN7cutlass10bfloat16_tE19Flash_kernel_traitsILi128ELi64ELi64ELi8ES2_EELb1ELb0ELb0ELb1EEv16Flash_bwd_params' for 'sm_80'
ptxas info    : Function properties for _Z44flash_bwd_dq_dk_dv_loop_seqk_parallel_kernelI23Flash_bwd_kernel_traitsILi128ELi64ELi64ELi8ELi4ELi2ELi2ELb1ELb0EN7cutlass10bfloat16_tE19Flash_kernel_traitsILi128ELi64ELi64ELi8ES2_EELb1ELb0ELb0ELb1EEv16Flash_bwd_params
    0 bytes stack frame, 0 bytes spill stores, 0 bytes spill loads
ptxas info    : Used 242 registers, 696 bytes cmem[0]
ptxas info    : Compiling entry function '_Z44flash_bwd_dq_dk_dv_loop_seqk_parallel_kernelI23Flash_bwd_kernel_traitsILi128ELi64ELi64ELi8ELi4ELi2ELi2ELb1ELb0EN7cutlass10bfloat16_tE19Flash_kernel_traitsILi128ELi64ELi64ELi8ES2_EELb1ELb0ELb1ELb0EEv16Flash_bwd_params' for 'sm_80'
ptxas info    : Function properties for _Z44flash_bwd_dq_dk_dv_loop_seqk_parallel_kernelI23Flash_bwd_kernel_traitsILi128ELi64ELi64ELi8ELi4ELi2ELi2ELb1ELb0EN7cutlass10bfloat16_tE19Flash_kernel_traitsILi128ELi64ELi64ELi8ES2_EELb1ELb0ELb1ELb0EEv16Flash_bwd_params
    0 bytes stack frame, 0 bytes spill stores, 0 bytes spill loads
ptxas info    : Used 240 registers, 696 bytes cmem[0]
ptxas info    : Compiling entry function '_Z44flash_bwd_dq_dk_dv_loop_seqk_parallel_kernelI23Flash_bwd_kernel_traitsILi128ELi64ELi64ELi8ELi4ELi2ELi2ELb1ELb0EN7cutlass10bfloat16_tE19Flash_kernel_traitsILi128ELi64ELi64ELi8ES2_EELb1ELb0ELb1ELb1EEv16Flash_bwd_params' for 'sm_80'
ptxas info    : Function properties for _Z44flash_bwd_dq_dk_dv_loop_seqk_parallel_kernelI23Flash_bwd_kernel_traitsILi128ELi64ELi64ELi8ELi4ELi2ELi2ELb1ELb0EN7cutlass10bfloat16_tE19Flash_kernel_traitsILi128ELi64ELi64ELi8ES2_EELb1ELb0ELb1ELb1EEv16Flash_bwd_params
    0 bytes stack frame, 0 bytes spill stores, 0 bytes spill loads
ptxas info    : Used 240 registers, 696 bytes cmem[0]
ptxas info    : Compiling entry function '_Z44flash_bwd_dq_dk_dv_loop_seqk_parallel_kernelI23Flash_bwd_kernel_traitsILi128ELi64ELi64ELi8ELi4ELi2ELi2ELb1ELb0EN7cutlass10bfloat16_tE19Flash_kernel_traitsILi128ELi64ELi64ELi8ES2_EELb1ELb1ELb0ELb0EEv16Flash_bwd_params' for 'sm_80'
ptxas info    : Function properties for _Z44flash_bwd_dq_dk_dv_loop_seqk_parallel_kernelI23Flash_bwd_kernel_traitsILi128ELi64ELi64ELi8ELi4ELi2ELi2ELb1ELb0EN7cutlass10bfloat16_tE19Flash_kernel_traitsILi128ELi64ELi64ELi8ES2_EELb1ELb1ELb0ELb0EEv16Flash_bwd_params
    0 bytes stack frame, 0 bytes spill stores, 0 bytes spill loads
ptxas info    : Used 244 registers, 696 bytes cmem[0]
ptxas info    : Compiling entry function '_Z44flash_bwd_dq_dk_dv_loop_seqk_parallel_kernelI23Flash_bwd_kernel_traitsILi128ELi64ELi64ELi8ELi4ELi2ELi2ELb1ELb0EN7cutlass10bfloat16_tE19Flash_kernel_traitsILi128ELi64ELi64ELi8ES2_EELb1ELb1ELb0ELb1EEv16Flash_bwd_params' for 'sm_80'
ptxas info    : Function properties for _Z44flash_bwd_dq_dk_dv_loop_seqk_parallel_kernelI23Flash_bwd_kernel_traitsILi128ELi64ELi64ELi8ELi4ELi2ELi2ELb1ELb0EN7cutlass10bfloat16_tE19Flash_kernel_traitsILi128ELi64ELi64ELi8ES2_EELb1ELb1ELb0ELb1EEv16Flash_bwd_params
    0 bytes stack frame, 0 bytes spill stores, 0 bytes spill loads
ptxas info    : Used 241 registers, 696 bytes cmem[0]
ptxas info    : Compiling entry function '_Z44flash_bwd_dq_dk_dv_loop_seqk_parallel_kernelI23Flash_bwd_kernel_traitsILi128ELi64ELi64ELi8ELi4ELi2ELi2ELb1ELb0EN7cutlass10bfloat16_tE19Flash_kernel_traitsILi128ELi64ELi64ELi8ES2_EELb1ELb1ELb1ELb0EEv16Flash_bwd_params' for 'sm_80'
ptxas info    : Function properties for _Z44flash_bwd_dq_dk_dv_loop_seqk_parallel_kernelI23Flash_bwd_kernel_traitsILi128ELi64ELi64ELi8ELi4ELi2ELi2ELb1ELb0EN7cutlass10bfloat16_tE19Flash_kernel_traitsILi128ELi64ELi64ELi8ES2_EELb1ELb1ELb1ELb0EEv16Flash_bwd_params
    0 bytes stack frame, 0 bytes spill stores, 0 bytes spill loads
ptxas info    : Used 239 registers, 696 bytes cmem[0]
ptxas info    : Compiling entry function '_Z44flash_bwd_dq_dk_dv_loop_seqk_parallel_kernelI23Flash_bwd_kernel_traitsILi128ELi64ELi64ELi8ELi4ELi2ELi2ELb1ELb0EN7cutlass10bfloat16_tE19Flash_kernel_traitsILi128ELi64ELi64ELi8ES2_EELb1ELb1ELb1ELb1EEv16Flash_bwd_params' for 'sm_80'
ptxas info    : Function properties for _Z44flash_bwd_dq_dk_dv_loop_seqk_parallel_kernelI23Flash_bwd_kernel_traitsILi128ELi64ELi64ELi8ELi4ELi2ELi2ELb1ELb0EN7cutlass10bfloat16_tE19Flash_kernel_traitsILi128ELi64ELi64ELi8ES2_EELb1ELb1ELb1ELb1EEv16Flash_bwd_params
    0 bytes stack frame, 0 bytes spill stores, 0 bytes spill loads
ptxas info    : Used 240 registers, 696 bytes cmem[0]
ptxas info    : Compiling entry function '_Z25flash_bwd_dot_do_o_kernelILb1E23Flash_bwd_kernel_traitsILi128ELi64ELi64ELi8ELi4ELi2ELi2ELb1ELb0EN7cutlass10bfloat16_tE19Flash_kernel_traitsILi128ELi64ELi64ELi8ES2_EEEv16Flash_bwd_params' for 'sm_80'
ptxas info    : Function properties for _Z25flash_bwd_dot_do_o_kernelILb1E23Flash_bwd_kernel_traitsILi128ELi64ELi64ELi8ELi4ELi2ELi2ELb1ELb0EN7cutlass10bfloat16_tE19Flash_kernel_traitsILi128ELi64ELi64ELi8ES2_EEEv16Flash_bwd_params
    0 bytes stack frame, 0 bytes spill stores, 0 bytes spill loads
ptxas info    : Used 47 registers, 696 bytes cmem[0]
ptxas info    : Compiling entry function '_Z27flash_bwd_convert_dq_kernelI23Flash_bwd_kernel_traitsILi128ELi64ELi128ELi8ELi2ELi4ELi2ELb0ELb0EN7cutlass10bfloat16_tE19Flash_kernel_traitsILi128ELi64ELi128ELi8ES2_EEEv16Flash_bwd_params' for 'sm_80'
ptxas info    : Function properties for _Z27flash_bwd_convert_dq_kernelI23Flash_bwd_kernel_traitsILi128ELi64ELi128ELi8ELi2ELi4ELi2ELb0ELb0EN7cutlass10bfloat16_tE19Flash_kernel_traitsILi128ELi64ELi128ELi8ES2_EEEv16Flash_bwd_params
    0 bytes stack frame, 0 bytes spill stores, 0 bytes spill loads
ptxas info    : Used 32 registers, 696 bytes cmem[0]
ptxas info    : Compiling entry function '_Z44flash_bwd_dq_dk_dv_loop_seqk_parallel_kernelI23Flash_bwd_kernel_traitsILi128ELi64ELi128ELi8ELi2ELi4ELi2ELb0ELb0EN7cutlass10bfloat16_tE19Flash_kernel_traitsILi128ELi64ELi128ELi8ES2_EELb1ELb0ELb0ELb0EEv16Flash_bwd_params' for 'sm_80'
ptxas info    : Function properties for _Z44flash_bwd_dq_dk_dv_loop_seqk_parallel_kernelI23Flash_bwd_kernel_traitsILi128ELi64ELi128ELi8ELi2ELi4ELi2ELb0ELb0EN7cutlass10bfloat16_tE19Flash_kernel_traitsILi128ELi64ELi128ELi8ES2_EELb1ELb0ELb0ELb0EEv16Flash_bwd_params
    136 bytes stack frame, 168 bytes spill stores, 184 bytes spill loads
ptxas info    : Used 255 registers, 696 bytes cmem[0]
ptxas info    : Compiling entry function '_Z44flash_bwd_dq_dk_dv_loop_seqk_parallel_kernelI23Flash_bwd_kernel_traitsILi128ELi64ELi128ELi8ELi2ELi4ELi2ELb0ELb0EN7cutlass10bfloat16_tE19Flash_kernel_traitsILi128ELi64ELi128ELi8ES2_EELb1ELb0ELb0ELb1EEv16Flash_bwd_params' for 'sm_80'
ptxas info    : Function properties for _Z44flash_bwd_dq_dk_dv_loop_seqk_parallel_kernelI23Flash_bwd_kernel_traitsILi128ELi64ELi128ELi8ELi2ELi4ELi2ELb0ELb0EN7cutlass10bfloat16_tE19Flash_kernel_traitsILi128ELi64ELi128ELi8ES2_EELb1ELb0ELb0ELb1EEv16Flash_bwd_params
    128 bytes stack frame, 160 bytes spill stores, 176 bytes spill loads
ptxas info    : Used 255 registers, 696 bytes cmem[0]
ptxas info    : Compiling entry function '_Z44flash_bwd_dq_dk_dv_loop_seqk_parallel_kernelI23Flash_bwd_kernel_traitsILi128ELi64ELi128ELi8ELi2ELi4ELi2ELb0ELb0EN7cutlass10bfloat16_tE19Flash_kernel_traitsILi128ELi64ELi128ELi8ES2_EELb1ELb0ELb1ELb0EEv16Flash_bwd_params' for 'sm_80'
ptxas info    : Function properties for _Z44flash_bwd_dq_dk_dv_loop_seqk_parallel_kernelI23Flash_bwd_kernel_traitsILi128ELi64ELi128ELi8ELi2ELi4ELi2ELb0ELb0EN7cutlass10bfloat16_tE19Flash_kernel_traitsILi128ELi64ELi128ELi8ES2_EELb1ELb0ELb1ELb0EEv16Flash_bwd_params
    112 bytes stack frame, 140 bytes spill stores, 164 bytes spill loads
ptxas info    : Used 255 registers, 696 bytes cmem[0]
ptxas info    : Compiling entry function '_Z44flash_bwd_dq_dk_dv_loop_seqk_parallel_kernelI23Flash_bwd_kernel_traitsILi128ELi64ELi128ELi8ELi2ELi4ELi2ELb0ELb0EN7cutlass10bfloat16_tE19Flash_kernel_traitsILi128ELi64ELi128ELi8ES2_EELb1ELb0ELb1ELb1EEv16Flash_bwd_params' for 'sm_80'
ptxas info    : Function properties for _Z44flash_bwd_dq_dk_dv_loop_seqk_parallel_kernelI23Flash_bwd_kernel_traitsILi128ELi64ELi128ELi8ELi2ELi4ELi2ELb0ELb0EN7cutlass10bfloat16_tE19Flash_kernel_traitsILi128ELi64ELi128ELi8ES2_EELb1ELb0ELb1ELb1EEv16Flash_bwd_params
    0 bytes stack frame, 0 bytes spill stores, 0 bytes spill loads
ptxas info    : Used 255 registers, 696 bytes cmem[0]
ptxas info    : Compiling entry function '_Z44flash_bwd_dq_dk_dv_loop_seqk_parallel_kernelI23Flash_bwd_kernel_traitsILi128ELi64ELi128ELi8ELi2ELi4ELi2ELb0ELb0EN7cutlass10bfloat16_tE19Flash_kernel_traitsILi128ELi64ELi128ELi8ES2_EELb1ELb1ELb0ELb0EEv16Flash_bwd_params' for 'sm_80'
ptxas info    : Function properties for _Z44flash_bwd_dq_dk_dv_loop_seqk_parallel_kernelI23Flash_bwd_kernel_traitsILi128ELi64ELi128ELi8ELi2ELi4ELi2ELb0ELb0EN7cutlass10bfloat16_tE19Flash_kernel_traitsILi128ELi64ELi128ELi8ES2_EELb1ELb1ELb0ELb0EEv16Flash_bwd_params
    24 bytes stack frame, 24 bytes spill stores, 24 bytes spill loads
ptxas info    : Used 255 registers, 696 bytes cmem[0]
ptxas info    : Compiling entry function '_Z44flash_bwd_dq_dk_dv_loop_seqk_parallel_kernelI23Flash_bwd_kernel_traitsILi128ELi64ELi128ELi8ELi2ELi4ELi2ELb0ELb0EN7cutlass10bfloat16_tE19Flash_kernel_traitsILi128ELi64ELi128ELi8ES2_EELb1ELb1ELb0ELb1EEv16Flash_bwd_params' for 'sm_80'
ptxas info    : Function properties for _Z44flash_bwd_dq_dk_dv_loop_seqk_parallel_kernelI23Flash_bwd_kernel_traitsILi128ELi64ELi128ELi8ELi2ELi4ELi2ELb0ELb0EN7cutlass10bfloat16_tE19Flash_kernel_traitsILi128ELi64ELi128ELi8ES2_EELb1ELb1ELb0ELb1EEv16Flash_bwd_params
    16 bytes stack frame, 16 bytes spill stores, 16 bytes spill loads
ptxas info    : Used 255 registers, 696 bytes cmem[0]
ptxas info    : Compiling entry function '_Z44flash_bwd_dq_dk_dv_loop_seqk_parallel_kernelI23Flash_bwd_kernel_traitsILi128ELi64ELi128ELi8ELi2ELi4ELi2ELb0ELb0EN7cutlass10bfloat16_tE19Flash_kernel_traitsILi128ELi64ELi128ELi8ES2_EELb1ELb1ELb1ELb0EEv16Flash_bwd_params' for 'sm_80'
ptxas info    : Function properties for _Z44flash_bwd_dq_dk_dv_loop_seqk_parallel_kernelI23Flash_bwd_kernel_traitsILi128ELi64ELi128ELi8ELi2ELi4ELi2ELb0ELb0EN7cutlass10bfloat16_tE19Flash_kernel_traitsILi128ELi64ELi128ELi8ES2_EELb1ELb1ELb1ELb0EEv16Flash_bwd_params
    40 bytes stack frame, 48 bytes spill stores, 40 bytes spill loads
ptxas info    : Used 255 registers, 696 bytes cmem[0]
ptxas info    : Compiling entry function '_Z44flash_bwd_dq_dk_dv_loop_seqk_parallel_kernelI23Flash_bwd_kernel_traitsILi128ELi64ELi128ELi8ELi2ELi4ELi2ELb0ELb0EN7cutlass10bfloat16_tE19Flash_kernel_traitsILi128ELi64ELi128ELi8ES2_EELb1ELb1ELb1ELb1EEv16Flash_bwd_params' for 'sm_80'
ptxas info    : Function properties for _Z44flash_bwd_dq_dk_dv_loop_seqk_parallel_kernelI23Flash_bwd_kernel_traitsILi128ELi64ELi128ELi8ELi2ELi4ELi2ELb0ELb0EN7cutlass10bfloat16_tE19Flash_kernel_traitsILi128ELi64ELi128ELi8ES2_EELb1ELb1ELb1ELb1EEv16Flash_bwd_params
    16 bytes stack frame, 12 bytes spill stores, 12 bytes spill loads
ptxas info    : Used 255 registers, 696 bytes cmem[0]
ptxas info    : Compiling entry function '_Z25flash_bwd_dot_do_o_kernelILb1E23Flash_bwd_kernel_traitsILi128ELi64ELi128ELi8ELi2ELi4ELi2ELb0ELb0EN7cutlass10bfloat16_tE19Flash_kernel_traitsILi128ELi64ELi128ELi8ES2_EEEv16Flash_bwd_params' for 'sm_80'
ptxas info    : Function properties for _Z25flash_bwd_dot_do_o_kernelILb1E23Flash_bwd_kernel_traitsILi128ELi64ELi128ELi8ELi2ELi4ELi2ELb0ELb0EN7cutlass10bfloat16_tE19Flash_kernel_traitsILi128ELi64ELi128ELi8ES2_EEEv16Flash_bwd_params
    0 bytes stack frame, 0 bytes spill stores, 0 bytes spill loads
ptxas info    : Used 47 registers, 696 bytes cmem[0]
/anaconda/envs/flashattention/lib/python3.9/site-packages/torch/include/c10/core/SymInt.h(84): warning: integer conversion resulted in a change of sign

[21/37] /usr/local/cuda-11.4/bin/nvcc  -I/home/v-feiychen/flash-attention/csrc/flash_attn -I/home/v-feiychen/flash-attention/csrc/flash_attn/src -I/home/v-feiychen/flash-attention/csrc/cutlass/include -I/anaconda/envs/flashattention/lib/python3.9/site-packages/torch/include -I/anaconda/envs/flashattention/lib/python3.9/site-packages/torch/include/torch/csrc/api/include -I/anaconda/envs/flashattention/lib/python3.9/site-packages/torch/include/TH -I/anaconda/envs/flashattention/lib/python3.9/site-packages/torch/include/THC -I/usr/local/cuda-11.4/include -I/anaconda/envs/flashattention/include/python3.9 -c -c /home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_bwd_hdim64_bf16_sm80.cu -o /home/v-feiychen/flash-attention/build/temp.linux-x86_64-cpython-39/csrc/flash_attn/src/flash_bwd_hdim64_bf16_sm80.o -D__CUDA_NO_HALF_OPERATORS__ -D__CUDA_NO_HALF_CONVERSIONS__ -D__CUDA_NO_BFLOAT16_CONVERSIONS__ -D__CUDA_NO_HALF2_OPERATORS__ --expt-relaxed-constexpr --compiler-options ''"'"'-fPIC'"'"'' -O3 -std=c++17 -U__CUDA_NO_HALF_OPERATORS__ -U__CUDA_NO_HALF_CONVERSIONS__ -U__CUDA_NO_HALF2_OPERATORS__ -U__CUDA_NO_BFLOAT16_CONVERSIONS__ --expt-relaxed-constexpr --expt-extended-lambda --use_fast_math --ptxas-options=-v -lineinfo -gencode arch=compute_80,code=sm_80 --threads 4 -DTORCH_API_INCLUDE_EXTENSION_H '-DPYBIND11_COMPILER_TYPE="_gcc"' '-DPYBIND11_STDLIB="_libstdcpp"' '-DPYBIND11_BUILD_ABI="_cxxabi1011"' -DTORCH_EXTENSION_NAME=flash_attn_2_cuda -D_GLIBCXX_USE_CXX11_ABI=0
/anaconda/envs/flashattention/lib/python3.9/site-packages/torch/include/c10/core/SymInt.h(84): warning: integer conversion resulted in a change of sign

ptxas info    : 2 bytes gmem
ptxas info    : Compiling entry function '_Z44flash_bwd_dq_dk_dv_loop_seqk_parallel_kernelI23Flash_bwd_kernel_traitsILi64ELi64ELi128ELi8ELi2ELi4ELi4ELb1ELb0EN7cutlass10bfloat16_tE19Flash_kernel_traitsILi64ELi64ELi128ELi8ES2_EELb0ELb0ELb0ELb0EEv16Flash_bwd_params' for 'sm_80'
ptxas info    : Function properties for _Z44flash_bwd_dq_dk_dv_loop_seqk_parallel_kernelI23Flash_bwd_kernel_traitsILi64ELi64ELi128ELi8ELi2ELi4ELi4ELb1ELb0EN7cutlass10bfloat16_tE19Flash_kernel_traitsILi64ELi64ELi128ELi8ES2_EELb0ELb0ELb0ELb0EEv16Flash_bwd_params
    0 bytes stack frame, 0 bytes spill stores, 0 bytes spill loads
ptxas info    : Used 230 registers, 696 bytes cmem[0]
ptxas info    : Compiling entry function '_Z44flash_bwd_dq_dk_dv_loop_seqk_parallel_kernelI23Flash_bwd_kernel_traitsILi64ELi64ELi128ELi8ELi2ELi4ELi4ELb1ELb0EN7cutlass10bfloat16_tE19Flash_kernel_traitsILi64ELi64ELi128ELi8ES2_EELb0ELb0ELb0ELb1EEv16Flash_bwd_params' for 'sm_80'
ptxas info    : Function properties for _Z44flash_bwd_dq_dk_dv_loop_seqk_parallel_kernelI23Flash_bwd_kernel_traitsILi64ELi64ELi128ELi8ELi2ELi4ELi4ELb1ELb0EN7cutlass10bfloat16_tE19Flash_kernel_traitsILi64ELi64ELi128ELi8ES2_EELb0ELb0ELb0ELb1EEv16Flash_bwd_params
    0 bytes stack frame, 0 bytes spill stores, 0 bytes spill loads
ptxas info    : Used 232 registers, 696 bytes cmem[0]
ptxas info    : Compiling entry function '_Z44flash_bwd_dq_dk_dv_loop_seqk_parallel_kernelI23Flash_bwd_kernel_traitsILi64ELi64ELi128ELi8ELi2ELi4ELi4ELb1ELb0EN7cutlass10bfloat16_tE19Flash_kernel_traitsILi64ELi64ELi128ELi8ES2_EELb0ELb0ELb1ELb0EEv16Flash_bwd_params' for 'sm_80'
ptxas info    : Function properties for _Z44flash_bwd_dq_dk_dv_loop_seqk_parallel_kernelI23Flash_bwd_kernel_traitsILi64ELi64ELi128ELi8ELi2ELi4ELi4ELb1ELb0EN7cutlass10bfloat16_tE19Flash_kernel_traitsILi64ELi64ELi128ELi8ES2_EELb0ELb0ELb1ELb0EEv16Flash_bwd_params
    0 bytes stack frame, 0 bytes spill stores, 0 bytes spill loads
ptxas info    : Used 220 registers, 696 bytes cmem[0]
ptxas info    : Compiling entry function '_Z44flash_bwd_dq_dk_dv_loop_seqk_parallel_kernelI23Flash_bwd_kernel_traitsILi64ELi64ELi128ELi8ELi2ELi4ELi4ELb1ELb0EN7cutlass10bfloat16_tE19Flash_kernel_traitsILi64ELi64ELi128ELi8ES2_EELb0ELb0ELb1ELb1EEv16Flash_bwd_params' for 'sm_80'
ptxas info    : Function properties for _Z44flash_bwd_dq_dk_dv_loop_seqk_parallel_kernelI23Flash_bwd_kernel_traitsILi64ELi64ELi128ELi8ELi2ELi4ELi4ELb1ELb0EN7cutlass10bfloat16_tE19Flash_kernel_traitsILi64ELi64ELi128ELi8ES2_EELb0ELb0ELb1ELb1EEv16Flash_bwd_params
    0 bytes stack frame, 0 bytes spill stores, 0 bytes spill loads
ptxas info    : Used 221 registers, 696 bytes cmem[0]
ptxas info    : Compiling entry function '_Z44flash_bwd_dq_dk_dv_loop_seqk_parallel_kernelI23Flash_bwd_kernel_traitsILi64ELi64ELi128ELi8ELi2ELi4ELi4ELb1ELb0EN7cutlass10bfloat16_tE19Flash_kernel_traitsILi64ELi64ELi128ELi8ES2_EELb0ELb1ELb0ELb0EEv16Flash_bwd_params' for 'sm_80'
ptxas info    : Function properties for _Z44flash_bwd_dq_dk_dv_loop_seqk_parallel_kernelI23Flash_bwd_kernel_traitsILi64ELi64ELi128ELi8ELi2ELi4ELi4ELb1ELb0EN7cutlass10bfloat16_tE19Flash_kernel_traitsILi64ELi64ELi128ELi8ES2_EELb0ELb1ELb0ELb0EEv16Flash_bwd_params
    0 bytes stack frame, 0 bytes spill stores, 0 bytes spill loads
ptxas info    : Used 231 registers, 696 bytes cmem[0]
ptxas info    : Compiling entry function '_Z44flash_bwd_dq_dk_dv_loop_seqk_parallel_kernelI23Flash_bwd_kernel_traitsILi64ELi64ELi128ELi8ELi2ELi4ELi4ELb1ELb0EN7cutlass10bfloat16_tE19Flash_kernel_traitsILi64ELi64ELi128ELi8ES2_EELb0ELb1ELb0ELb1EEv16Flash_bwd_params' for 'sm_80'
ptxas info    : Function properties for _Z44flash_bwd_dq_dk_dv_loop_seqk_parallel_kernelI23Flash_bwd_kernel_traitsILi64ELi64ELi128ELi8ELi2ELi4ELi4ELb1ELb0EN7cutlass10bfloat16_tE19Flash_kernel_traitsILi64ELi64ELi128ELi8ES2_EELb0ELb1ELb0ELb1EEv16Flash_bwd_params
    0 bytes stack frame, 0 bytes spill stores, 0 bytes spill loads
ptxas info    : Used 229 registers, 696 bytes cmem[0]
ptxas info    : Compiling entry function '_Z44flash_bwd_dq_dk_dv_loop_seqk_parallel_kernelI23Flash_bwd_kernel_traitsILi64ELi64ELi128ELi8ELi2ELi4ELi4ELb1ELb0EN7cutlass10bfloat16_tE19Flash_kernel_traitsILi64ELi64ELi128ELi8ES2_EELb0ELb1ELb1ELb0EEv16Flash_bwd_params' for 'sm_80'
ptxas info    : Function properties for _Z44flash_bwd_dq_dk_dv_loop_seqk_parallel_kernelI23Flash_bwd_kernel_traitsILi64ELi64ELi128ELi8ELi2ELi4ELi4ELb1ELb0EN7cutlass10bfloat16_tE19Flash_kernel_traitsILi64ELi64ELi128ELi8ES2_EELb0ELb1ELb1ELb0EEv16Flash_bwd_params
    0 bytes stack frame, 0 bytes spill stores, 0 bytes spill loads
ptxas info    : Used 225 registers, 696 bytes cmem[0]
ptxas info    : Compiling entry function '_Z44flash_bwd_dq_dk_dv_loop_seqk_parallel_kernelI23Flash_bwd_kernel_traitsILi64ELi64ELi128ELi8ELi2ELi4ELi4ELb1ELb0EN7cutlass10bfloat16_tE19Flash_kernel_traitsILi64ELi64ELi128ELi8ES2_EELb0ELb1ELb1ELb1EEv16Flash_bwd_params' for 'sm_80'
ptxas info    : Function properties for _Z44flash_bwd_dq_dk_dv_loop_seqk_parallel_kernelI23Flash_bwd_kernel_traitsILi64ELi64ELi128ELi8ELi2ELi4ELi4ELb1ELb0EN7cutlass10bfloat16_tE19Flash_kernel_traitsILi64ELi64ELi128ELi8ES2_EELb0ELb1ELb1ELb1EEv16Flash_bwd_params
    0 bytes stack frame, 0 bytes spill stores, 0 bytes spill loads
ptxas info    : Used 230 registers, 696 bytes cmem[0]
ptxas info    : Compiling entry function '_Z44flash_bwd_dq_dk_dv_loop_seqk_parallel_kernelI23Flash_bwd_kernel_traitsILi64ELi128ELi128ELi8ELi4ELi4ELi4ELb0ELb0EN7cutlass10bfloat16_tE19Flash_kernel_traitsILi64ELi128ELi128ELi8ES2_EELb0ELb0ELb0ELb0EEv16Flash_bwd_params' for 'sm_80'
ptxas info    : Function properties for _Z44flash_bwd_dq_dk_dv_loop_seqk_parallel_kernelI23Flash_bwd_kernel_traitsILi64ELi128ELi128ELi8ELi4ELi4ELi4ELb0ELb0EN7cutlass10bfloat16_tE19Flash_kernel_traitsILi64ELi128ELi128ELi8ES2_EELb0ELb0ELb0ELb0EEv16Flash_bwd_params
    32 bytes stack frame, 44 bytes spill stores, 32 bytes spill loads
ptxas info    : Used 255 registers, 696 bytes cmem[0]
ptxas info    : Compiling entry function '_Z44flash_bwd_dq_dk_dv_loop_seqk_parallel_kernelI23Flash_bwd_kernel_traitsILi64ELi128ELi128ELi8ELi4ELi4ELi4ELb0ELb0EN7cutlass10bfloat16_tE19Flash_kernel_traitsILi64ELi128ELi128ELi8ES2_EELb0ELb0ELb0ELb1EEv16Flash_bwd_params' for 'sm_80'
ptxas info    : Function properties for _Z44flash_bwd_dq_dk_dv_loop_seqk_parallel_kernelI23Flash_bwd_kernel_traitsILi64ELi128ELi128ELi8ELi4ELi4ELi4ELb0ELb0EN7cutlass10bfloat16_tE19Flash_kernel_traitsILi64ELi128ELi128ELi8ES2_EELb0ELb0ELb0ELb1EEv16Flash_bwd_params
    32 bytes stack frame, 36 bytes spill stores, 28 bytes spill loads
ptxas info    : Used 255 registers, 696 bytes cmem[0]
ptxas info    : Compiling entry function '_Z44flash_bwd_dq_dk_dv_loop_seqk_parallel_kernelI23Flash_bwd_kernel_traitsILi64ELi128ELi128ELi8ELi4ELi4ELi4ELb0ELb0EN7cutlass10bfloat16_tE19Flash_kernel_traitsILi64ELi128ELi128ELi8ES2_EELb0ELb0ELb1ELb0EEv16Flash_bwd_params' for 'sm_80'
ptxas info    : Function properties for _Z44flash_bwd_dq_dk_dv_loop_seqk_parallel_kernelI23Flash_bwd_kernel_traitsILi64ELi128ELi128ELi8ELi4ELi4ELi4ELb0ELb0EN7cutlass10bfloat16_tE19Flash_kernel_traitsILi64ELi128ELi128ELi8ES2_EELb0ELb0ELb1ELb0EEv16Flash_bwd_params
    40 bytes stack frame, 64 bytes spill stores, 60 bytes spill loads
ptxas info    : Used 255 registers, 696 bytes cmem[0]
ptxas info    : Compiling entry function '_Z44flash_bwd_dq_dk_dv_loop_seqk_parallel_kernelI23Flash_bwd_kernel_traitsILi64ELi128ELi128ELi8ELi4ELi4ELi4ELb0ELb0EN7cutlass10bfloat16_tE19Flash_kernel_traitsILi64ELi128ELi128ELi8ES2_EELb0ELb0ELb1ELb1EEv16Flash_bwd_params' for 'sm_80'
ptxas info    : Function properties for _Z44flash_bwd_dq_dk_dv_loop_seqk_parallel_kernelI23Flash_bwd_kernel_traitsILi64ELi128ELi128ELi8ELi4ELi4ELi4ELb0ELb0EN7cutlass10bfloat16_tE19Flash_kernel_traitsILi64ELi128ELi128ELi8ES2_EELb0ELb0ELb1ELb1EEv16Flash_bwd_params
    40 bytes stack frame, 64 bytes spill stores, 64 bytes spill loads
ptxas info    : Used 255 registers, 696 bytes cmem[0]
ptxas info    : Compiling entry function '_Z44flash_bwd_dq_dk_dv_loop_seqk_parallel_kernelI23Flash_bwd_kernel_traitsILi64ELi128ELi128ELi8ELi4ELi4ELi4ELb0ELb0EN7cutlass10bfloat16_tE19Flash_kernel_traitsILi64ELi128ELi128ELi8ES2_EELb0ELb1ELb0ELb0EEv16Flash_bwd_params' for 'sm_80'
ptxas info    : Function properties for _Z44flash_bwd_dq_dk_dv_loop_seqk_parallel_kernelI23Flash_bwd_kernel_traitsILi64ELi128ELi128ELi8ELi4ELi4ELi4ELb0ELb0EN7cutlass10bfloat16_tE19Flash_kernel_traitsILi64ELi128ELi128ELi8ES2_EELb0ELb1ELb0ELb0EEv16Flash_bwd_params
    40 bytes stack frame, 40 bytes spill stores, 40 bytes spill loads
ptxas info    : Used 255 registers, 696 bytes cmem[0]
ptxas info    : Compiling entry function '_Z44flash_bwd_dq_dk_dv_loop_seqk_parallel_kernelI23Flash_bwd_kernel_traitsILi64ELi128ELi128ELi8ELi4ELi4ELi4ELb0ELb0EN7cutlass10bfloat16_tE19Flash_kernel_traitsILi64ELi128ELi128ELi8ES2_EELb0ELb1ELb0ELb1EEv16Flash_bwd_params' for 'sm_80'
ptxas info    : Function properties for _Z44flash_bwd_dq_dk_dv_loop_seqk_parallel_kernelI23Flash_bwd_kernel_traitsILi64ELi128ELi128ELi8ELi4ELi4ELi4ELb0ELb0EN7cutlass10bfloat16_tE19Flash_kernel_traitsILi64ELi128ELi128ELi8ES2_EELb0ELb1ELb0ELb1EEv16Flash_bwd_params
    48 bytes stack frame, 44 bytes spill stores, 44 bytes spill loads
ptxas info    : Used 255 registers, 696 bytes cmem[0]
ptxas info    : Compiling entry function '_Z44flash_bwd_dq_dk_dv_loop_seqk_parallel_kernelI23Flash_bwd_kernel_traitsILi64ELi128ELi128ELi8ELi4ELi4ELi4ELb0ELb0EN7cutlass10bfloat16_tE19Flash_kernel_traitsILi64ELi128ELi128ELi8ES2_EELb0ELb1ELb1ELb0EEv16Flash_bwd_params' for 'sm_80'
ptxas info    : Function properties for _Z44flash_bwd_dq_dk_dv_loop_seqk_parallel_kernelI23Flash_bwd_kernel_traitsILi64ELi128ELi128ELi8ELi4ELi4ELi4ELb0ELb0EN7cutlass10bfloat16_tE19Flash_kernel_traitsILi64ELi128ELi128ELi8ES2_EELb0ELb1ELb1ELb0EEv16Flash_bwd_params
    32 bytes stack frame, 36 bytes spill stores, 28 bytes spill loads
ptxas info    : Used 255 registers, 696 bytes cmem[0]
ptxas info    : Compiling entry function '_Z44flash_bwd_dq_dk_dv_loop_seqk_parallel_kernelI23Flash_bwd_kernel_traitsILi64ELi128ELi128ELi8ELi4ELi4ELi4ELb0ELb0EN7cutlass10bfloat16_tE19Flash_kernel_traitsILi64ELi128ELi128ELi8ES2_EELb0ELb1ELb1ELb1EEv16Flash_bwd_params' for 'sm_80'
ptxas info    : Function properties for _Z44flash_bwd_dq_dk_dv_loop_seqk_parallel_kernelI23Flash_bwd_kernel_traitsILi64ELi128ELi128ELi8ELi4ELi4ELi4ELb0ELb0EN7cutlass10bfloat16_tE19Flash_kernel_traitsILi64ELi128ELi128ELi8ES2_EELb0ELb1ELb1ELb1EEv16Flash_bwd_params
    0 bytes stack frame, 0 bytes spill stores, 0 bytes spill loads
ptxas info    : Used 255 registers, 696 bytes cmem[0]
ptxas info    : Compiling entry function '_Z27flash_bwd_convert_dq_kernelI23Flash_bwd_kernel_traitsILi64ELi64ELi128ELi8ELi2ELi4ELi4ELb1ELb0EN7cutlass10bfloat16_tE19Flash_kernel_traitsILi64ELi64ELi128ELi8ES2_EEEv16Flash_bwd_params' for 'sm_80'
ptxas info    : Function properties for _Z27flash_bwd_convert_dq_kernelI23Flash_bwd_kernel_traitsILi64ELi64ELi128ELi8ELi2ELi4ELi4ELb1ELb0EN7cutlass10bfloat16_tE19Flash_kernel_traitsILi64ELi64ELi128ELi8ES2_EEEv16Flash_bwd_params
    0 bytes stack frame, 0 bytes spill stores, 0 bytes spill loads
ptxas info    : Used 32 registers, 696 bytes cmem[0]
ptxas info    : Compiling entry function '_Z44flash_bwd_dq_dk_dv_loop_seqk_parallel_kernelI23Flash_bwd_kernel_traitsILi64ELi64ELi128ELi8ELi2ELi4ELi4ELb1ELb0EN7cutlass10bfloat16_tE19Flash_kernel_traitsILi64ELi64ELi128ELi8ES2_EELb1ELb0ELb0ELb0EEv16Flash_bwd_params' for 'sm_80'
ptxas info    : Function properties for _Z44flash_bwd_dq_dk_dv_loop_seqk_parallel_kernelI23Flash_bwd_kernel_traitsILi64ELi64ELi128ELi8ELi2ELi4ELi4ELb1ELb0EN7cutlass10bfloat16_tE19Flash_kernel_traitsILi64ELi64ELi128ELi8ES2_EELb1ELb0ELb0ELb0EEv16Flash_bwd_params
    0 bytes stack frame, 0 bytes spill stores, 0 bytes spill loads
ptxas info    : Used 237 registers, 696 bytes cmem[0]
ptxas info    : Compiling entry function '_Z44flash_bwd_dq_dk_dv_loop_seqk_parallel_kernelI23Flash_bwd_kernel_traitsILi64ELi64ELi128ELi8ELi2ELi4ELi4ELb1ELb0EN7cutlass10bfloat16_tE19Flash_kernel_traitsILi64ELi64ELi128ELi8ES2_EELb1ELb0ELb0ELb1EEv16Flash_bwd_params' for 'sm_80'
ptxas info    : Function properties for _Z44flash_bwd_dq_dk_dv_loop_seqk_parallel_kernelI23Flash_bwd_kernel_traitsILi64ELi64ELi128ELi8ELi2ELi4ELi4ELb1ELb0EN7cutlass10bfloat16_tE19Flash_kernel_traitsILi64ELi64ELi128ELi8ES2_EELb1ELb0ELb0ELb1EEv16Flash_bwd_params
    0 bytes stack frame, 0 bytes spill stores, 0 bytes spill loads
ptxas info    : Used 237 registers, 696 bytes cmem[0]
ptxas info    : Compiling entry function '_Z44flash_bwd_dq_dk_dv_loop_seqk_parallel_kernelI23Flash_bwd_kernel_traitsILi64ELi64ELi128ELi8ELi2ELi4ELi4ELb1ELb0EN7cutlass10bfloat16_tE19Flash_kernel_traitsILi64ELi64ELi128ELi8ES2_EELb1ELb0ELb1ELb0EEv16Flash_bwd_params' for 'sm_80'
ptxas info    : Function properties for _Z44flash_bwd_dq_dk_dv_loop_seqk_parallel_kernelI23Flash_bwd_kernel_traitsILi64ELi64ELi128ELi8ELi2ELi4ELi4ELb1ELb0EN7cutlass10bfloat16_tE19Flash_kernel_traitsILi64ELi64ELi128ELi8ES2_EELb1ELb0ELb1ELb0EEv16Flash_bwd_params
    0 bytes stack frame, 0 bytes spill stores, 0 bytes spill loads
ptxas info    : Used 222 registers, 696 bytes cmem[0]
ptxas info    : Compiling entry function '_Z44flash_bwd_dq_dk_dv_loop_seqk_parallel_kernelI23Flash_bwd_kernel_traitsILi64ELi64ELi128ELi8ELi2ELi4ELi4ELb1ELb0EN7cutlass10bfloat16_tE19Flash_kernel_traitsILi64ELi64ELi128ELi8ES2_EELb1ELb0ELb1ELb1EEv16Flash_bwd_params' for 'sm_80'
ptxas info    : Function properties for _Z44flash_bwd_dq_dk_dv_loop_seqk_parallel_kernelI23Flash_bwd_kernel_traitsILi64ELi64ELi128ELi8ELi2ELi4ELi4ELb1ELb0EN7cutlass10bfloat16_tE19Flash_kernel_traitsILi64ELi64ELi128ELi8ES2_EELb1ELb0ELb1ELb1EEv16Flash_bwd_params
    0 bytes stack frame, 0 bytes spill stores, 0 bytes spill loads
ptxas info    : Used 226 registers, 696 bytes cmem[0]
ptxas info    : Compiling entry function '_Z44flash_bwd_dq_dk_dv_loop_seqk_parallel_kernelI23Flash_bwd_kernel_traitsILi64ELi64ELi128ELi8ELi2ELi4ELi4ELb1ELb0EN7cutlass10bfloat16_tE19Flash_kernel_traitsILi64ELi64ELi128ELi8ES2_EELb1ELb1ELb0ELb0EEv16Flash_bwd_params' for 'sm_80'
ptxas info    : Function properties for _Z44flash_bwd_dq_dk_dv_loop_seqk_parallel_kernelI23Flash_bwd_kernel_traitsILi64ELi64ELi128ELi8ELi2ELi4ELi4ELb1ELb0EN7cutlass10bfloat16_tE19Flash_kernel_traitsILi64ELi64ELi128ELi8ES2_EELb1ELb1ELb0ELb0EEv16Flash_bwd_params
    0 bytes stack frame, 0 bytes spill stores, 0 bytes spill loads
ptxas info    : Used 236 registers, 696 bytes cmem[0]
ptxas info    : Compiling entry function '_Z44flash_bwd_dq_dk_dv_loop_seqk_parallel_kernelI23Flash_bwd_kernel_traitsILi64ELi64ELi128ELi8ELi2ELi4ELi4ELb1ELb0EN7cutlass10bfloat16_tE19Flash_kernel_traitsILi64ELi64ELi128ELi8ES2_EELb1ELb1ELb0ELb1EEv16Flash_bwd_params' for 'sm_80'
ptxas info    : Function properties for _Z44flash_bwd_dq_dk_dv_loop_seqk_parallel_kernelI23Flash_bwd_kernel_traitsILi64ELi64ELi128ELi8ELi2ELi4ELi4ELb1ELb0EN7cutlass10bfloat16_tE19Flash_kernel_traitsILi64ELi64ELi128ELi8ES2_EELb1ELb1ELb0ELb1EEv16Flash_bwd_params
    0 bytes stack frame, 0 bytes spill stores, 0 bytes spill loads
ptxas info    : Used 238 registers, 696 bytes cmem[0]
ptxas info    : Compiling entry function '_Z44flash_bwd_dq_dk_dv_loop_seqk_parallel_kernelI23Flash_bwd_kernel_traitsILi64ELi64ELi128ELi8ELi2ELi4ELi4ELb1ELb0EN7cutlass10bfloat16_tE19Flash_kernel_traitsILi64ELi64ELi128ELi8ES2_EELb1ELb1ELb1ELb0EEv16Flash_bwd_params' for 'sm_80'
ptxas info    : Function properties for _Z44flash_bwd_dq_dk_dv_loop_seqk_parallel_kernelI23Flash_bwd_kernel_traitsILi64ELi64ELi128ELi8ELi2ELi4ELi4ELb1ELb0EN7cutlass10bfloat16_tE19Flash_kernel_traitsILi64ELi64ELi128ELi8ES2_EELb1ELb1ELb1ELb0EEv16Flash_bwd_params
    0 bytes stack frame, 0 bytes spill stores, 0 bytes spill loads
ptxas info    : Used 234 registers, 696 bytes cmem[0]
ptxas info    : Compiling entry function '_Z44flash_bwd_dq_dk_dv_loop_seqk_parallel_kernelI23Flash_bwd_kernel_traitsILi64ELi64ELi128ELi8ELi2ELi4ELi4ELb1ELb0EN7cutlass10bfloat16_tE19Flash_kernel_traitsILi64ELi64ELi128ELi8ES2_EELb1ELb1ELb1ELb1EEv16Flash_bwd_params' for 'sm_80'
ptxas info    : Function properties for _Z44flash_bwd_dq_dk_dv_loop_seqk_parallel_kernelI23Flash_bwd_kernel_traitsILi64ELi64ELi128ELi8ELi2ELi4ELi4ELb1ELb0EN7cutlass10bfloat16_tE19Flash_kernel_traitsILi64ELi64ELi128ELi8ES2_EELb1ELb1ELb1ELb1EEv16Flash_bwd_params
    0 bytes stack frame, 0 bytes spill stores, 0 bytes spill loads
ptxas info    : Used 239 registers, 696 bytes cmem[0]
ptxas info    : Compiling entry function '_Z25flash_bwd_dot_do_o_kernelILb1E23Flash_bwd_kernel_traitsILi64ELi64ELi128ELi8ELi2ELi4ELi4ELb1ELb0EN7cutlass10bfloat16_tE19Flash_kernel_traitsILi64ELi64ELi128ELi8ES2_EEEv16Flash_bwd_params' for 'sm_80'
ptxas info    : Function properties for _Z25flash_bwd_dot_do_o_kernelILb1E23Flash_bwd_kernel_traitsILi64ELi64ELi128ELi8ELi2ELi4ELi4ELb1ELb0EN7cutlass10bfloat16_tE19Flash_kernel_traitsILi64ELi64ELi128ELi8ES2_EEEv16Flash_bwd_params
    0 bytes stack frame, 0 bytes spill stores, 0 bytes spill loads
ptxas info    : Used 30 registers, 696 bytes cmem[0]
ptxas info    : Compiling entry function '_Z27flash_bwd_convert_dq_kernelI23Flash_bwd_kernel_traitsILi64ELi128ELi128ELi8ELi4ELi4ELi4ELb0ELb0EN7cutlass10bfloat16_tE19Flash_kernel_traitsILi64ELi128ELi128ELi8ES2_EEEv16Flash_bwd_params' for 'sm_80'
ptxas info    : Function properties for _Z27flash_bwd_convert_dq_kernelI23Flash_bwd_kernel_traitsILi64ELi128ELi128ELi8ELi4ELi4ELi4ELb0ELb0EN7cutlass10bfloat16_tE19Flash_kernel_traitsILi64ELi128ELi128ELi8ES2_EEEv16Flash_bwd_params
    0 bytes stack frame, 0 bytes spill stores, 0 bytes spill loads
ptxas info    : Used 32 registers, 696 bytes cmem[0]
ptxas info    : Compiling entry function '_Z44flash_bwd_dq_dk_dv_loop_seqk_parallel_kernelI23Flash_bwd_kernel_traitsILi64ELi128ELi128ELi8ELi4ELi4ELi4ELb0ELb0EN7cutlass10bfloat16_tE19Flash_kernel_traitsILi64ELi128ELi128ELi8ES2_EELb1ELb0ELb0ELb0EEv16Flash_bwd_params' for 'sm_80'
ptxas info    : Function properties for _Z44flash_bwd_dq_dk_dv_loop_seqk_parallel_kernelI23Flash_bwd_kernel_traitsILi64ELi128ELi128ELi8ELi4ELi4ELi4ELb0ELb0EN7cutlass10bfloat16_tE19Flash_kernel_traitsILi64ELi128ELi128ELi8ES2_EELb1ELb0ELb0ELb0EEv16Flash_bwd_params
    48 bytes stack frame, 48 bytes spill stores, 44 bytes spill loads
ptxas info    : Used 255 registers, 696 bytes cmem[0]
ptxas info    : Compiling entry function '_Z44flash_bwd_dq_dk_dv_loop_seqk_parallel_kernelI23Flash_bwd_kernel_traitsILi64ELi128ELi128ELi8ELi4ELi4ELi4ELb0ELb0EN7cutlass10bfloat16_tE19Flash_kernel_traitsILi64ELi128ELi128ELi8ES2_EELb1ELb0ELb0ELb1EEv16Flash_bwd_params' for 'sm_80'
ptxas info    : Function properties for _Z44flash_bwd_dq_dk_dv_loop_seqk_parallel_kernelI23Flash_bwd_kernel_traitsILi64ELi128ELi128ELi8ELi4ELi4ELi4ELb0ELb0EN7cutlass10bfloat16_tE19Flash_kernel_traitsILi64ELi128ELi128ELi8ES2_EELb1ELb0ELb0ELb1EEv16Flash_bwd_params
    56 bytes stack frame, 72 bytes spill stores, 72 bytes spill loads
ptxas info    : Used 255 registers, 696 bytes cmem[0]
ptxas info    : Compiling entry function '_Z44flash_bwd_dq_dk_dv_loop_seqk_parallel_kernelI23Flash_bwd_kernel_traitsILi64ELi128ELi128ELi8ELi4ELi4ELi4ELb0ELb0EN7cutlass10bfloat16_tE19Flash_kernel_traitsILi64ELi128ELi128ELi8ES2_EELb1ELb0ELb1ELb0EEv16Flash_bwd_params' for 'sm_80'
ptxas info    : Function properties for _Z44flash_bwd_dq_dk_dv_loop_seqk_parallel_kernelI23Flash_bwd_kernel_traitsILi64ELi128ELi128ELi8ELi4ELi4ELi4ELb0ELb0EN7cutlass10bfloat16_tE19Flash_kernel_traitsILi64ELi128ELi128ELi8ES2_EELb1ELb0ELb1ELb0EEv16Flash_bwd_params
    56 bytes stack frame, 68 bytes spill stores, 72 bytes spill loads
ptxas info    : Used 255 registers, 696 bytes cmem[0]
ptxas info    : Compiling entry function '_Z44flash_bwd_dq_dk_dv_loop_seqk_parallel_kernelI23Flash_bwd_kernel_traitsILi64ELi128ELi128ELi8ELi4ELi4ELi4ELb0ELb0EN7cutlass10bfloat16_tE19Flash_kernel_traitsILi64ELi128ELi128ELi8ES2_EELb1ELb0ELb1ELb1EEv16Flash_bwd_params' for 'sm_80'
ptxas info    : Function properties for _Z44flash_bwd_dq_dk_dv_loop_seqk_parallel_kernelI23Flash_bwd_kernel_traitsILi64ELi128ELi128ELi8ELi4ELi4ELi4ELb0ELb0EN7cutlass10bfloat16_tE19Flash_kernel_traitsILi64ELi128ELi128ELi8ES2_EELb1ELb0ELb1ELb1EEv16Flash_bwd_params
    56 bytes stack frame, 72 bytes spill stores, 80 bytes spill loads
ptxas info    : Used 255 registers, 696 bytes cmem[0]
ptxas info    : Compiling entry function '_Z44flash_bwd_dq_dk_dv_loop_seqk_parallel_kernelI23Flash_bwd_kernel_traitsILi64ELi128ELi128ELi8ELi4ELi4ELi4ELb0ELb0EN7cutlass10bfloat16_tE19Flash_kernel_traitsILi64ELi128ELi128ELi8ES2_EELb1ELb1ELb0ELb0EEv16Flash_bwd_params' for 'sm_80'
ptxas info    : Function properties for _Z44flash_bwd_dq_dk_dv_loop_seqk_parallel_kernelI23Flash_bwd_kernel_traitsILi64ELi128ELi128ELi8ELi4ELi4ELi4ELb0ELb0EN7cutlass10bfloat16_tE19Flash_kernel_traitsILi64ELi128ELi128ELi8ES2_EELb1ELb1ELb0ELb0EEv16Flash_bwd_params
    24 bytes stack frame, 20 bytes spill stores, 20 bytes spill loads
ptxas info    : Used 255 registers, 696 bytes cmem[0]
ptxas info    : Compiling entry function '_Z44flash_bwd_dq_dk_dv_loop_seqk_parallel_kernelI23Flash_bwd_kernel_traitsILi64ELi128ELi128ELi8ELi4ELi4ELi4ELb0ELb0EN7cutlass10bfloat16_tE19Flash_kernel_traitsILi64ELi128ELi128ELi8ES2_EELb1ELb1ELb0ELb1EEv16Flash_bwd_params' for 'sm_80'
ptxas info    : Function properties for _Z44flash_bwd_dq_dk_dv_loop_seqk_parallel_kernelI23Flash_bwd_kernel_traitsILi64ELi128ELi128ELi8ELi4ELi4ELi4ELb0ELb0EN7cutlass10bfloat16_tE19Flash_kernel_traitsILi64ELi128ELi128ELi8ES2_EELb1ELb1ELb0ELb1EEv16Flash_bwd_params
    32 bytes stack frame, 28 bytes spill stores, 28 bytes spill loads
ptxas info    : Used 255 registers, 696 bytes cmem[0]
ptxas info    : Compiling entry function '_Z44flash_bwd_dq_dk_dv_loop_seqk_parallel_kernelI23Flash_bwd_kernel_traitsILi64ELi128ELi128ELi8ELi4ELi4ELi4ELb0ELb0EN7cutlass10bfloat16_tE19Flash_kernel_traitsILi64ELi128ELi128ELi8ES2_EELb1ELb1ELb1ELb0EEv16Flash_bwd_params' for 'sm_80'
ptxas info    : Function properties for _Z44flash_bwd_dq_dk_dv_loop_seqk_parallel_kernelI23Flash_bwd_kernel_traitsILi64ELi128ELi128ELi8ELi4ELi4ELi4ELb0ELb0EN7cutlass10bfloat16_tE19Flash_kernel_traitsILi64ELi128ELi128ELi8ES2_EELb1ELb1ELb1ELb0EEv16Flash_bwd_params
    48 bytes stack frame, 60 bytes spill stores, 56 bytes spill loads
ptxas info    : Used 255 registers, 696 bytes cmem[0]
ptxas info    : Compiling entry function '_Z44flash_bwd_dq_dk_dv_loop_seqk_parallel_kernelI23Flash_bwd_kernel_traitsILi64ELi128ELi128ELi8ELi4ELi4ELi4ELb0ELb0EN7cutlass10bfloat16_tE19Flash_kernel_traitsILi64ELi128ELi128ELi8ES2_EELb1ELb1ELb1ELb1EEv16Flash_bwd_params' for 'sm_80'
ptxas info    : Function properties for _Z44flash_bwd_dq_dk_dv_loop_seqk_parallel_kernelI23Flash_bwd_kernel_traitsILi64ELi128ELi128ELi8ELi4ELi4ELi4ELb0ELb0EN7cutlass10bfloat16_tE19Flash_kernel_traitsILi64ELi128ELi128ELi8ES2_EELb1ELb1ELb1ELb1EEv16Flash_bwd_params
    16 bytes stack frame, 16 bytes spill stores, 16 bytes spill loads
ptxas info    : Used 255 registers, 696 bytes cmem[0]
ptxas info    : Compiling entry function '_Z25flash_bwd_dot_do_o_kernelILb1E23Flash_bwd_kernel_traitsILi64ELi128ELi128ELi8ELi4ELi4ELi4ELb0ELb0EN7cutlass10bfloat16_tE19Flash_kernel_traitsILi64ELi128ELi128ELi8ES2_EEEv16Flash_bwd_params' for 'sm_80'
ptxas info    : Function properties for _Z25flash_bwd_dot_do_o_kernelILb1E23Flash_bwd_kernel_traitsILi64ELi128ELi128ELi8ELi4ELi4ELi4ELb0ELb0EN7cutlass10bfloat16_tE19Flash_kernel_traitsILi64ELi128ELi128ELi8ES2_EEEv16Flash_bwd_params
    0 bytes stack frame, 0 bytes spill stores, 0 bytes spill loads
ptxas info    : Used 45 registers, 696 bytes cmem[0]
/anaconda/envs/flashattention/lib/python3.9/site-packages/torch/include/c10/core/SymInt.h(84): warning: integer conversion resulted in a change of sign

[22/37] /usr/local/cuda-11.4/bin/nvcc  -I/home/v-feiychen/flash-attention/csrc/flash_attn -I/home/v-feiychen/flash-attention/csrc/flash_attn/src -I/home/v-feiychen/flash-attention/csrc/cutlass/include -I/anaconda/envs/flashattention/lib/python3.9/site-packages/torch/include -I/anaconda/envs/flashattention/lib/python3.9/site-packages/torch/include/torch/csrc/api/include -I/anaconda/envs/flashattention/lib/python3.9/site-packages/torch/include/TH -I/anaconda/envs/flashattention/lib/python3.9/site-packages/torch/include/THC -I/usr/local/cuda-11.4/include -I/anaconda/envs/flashattention/include/python3.9 -c -c /home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_hdim224_fp16_sm80.cu -o /home/v-feiychen/flash-attention/build/temp.linux-x86_64-cpython-39/csrc/flash_attn/src/flash_fwd_hdim224_fp16_sm80.o -D__CUDA_NO_HALF_OPERATORS__ -D__CUDA_NO_HALF_CONVERSIONS__ -D__CUDA_NO_BFLOAT16_CONVERSIONS__ -D__CUDA_NO_HALF2_OPERATORS__ --expt-relaxed-constexpr --compiler-options ''"'"'-fPIC'"'"'' -O3 -std=c++17 -U__CUDA_NO_HALF_OPERATORS__ -U__CUDA_NO_HALF_CONVERSIONS__ -U__CUDA_NO_HALF2_OPERATORS__ -U__CUDA_NO_BFLOAT16_CONVERSIONS__ --expt-relaxed-constexpr --expt-extended-lambda --use_fast_math --ptxas-options=-v -lineinfo -gencode arch=compute_80,code=sm_80 --threads 4 -DTORCH_API_INCLUDE_EXTENSION_H '-DPYBIND11_COMPILER_TYPE="_gcc"' '-DPYBIND11_STDLIB="_libstdcpp"' '-DPYBIND11_BUILD_ABI="_cxxabi1011"' -DTORCH_EXTENSION_NAME=flash_attn_2_cuda -D_GLIBCXX_USE_CXX11_ABI=0
/anaconda/envs/flashattention/lib/python3.9/site-packages/torch/include/c10/core/SymInt.h(84): warning: integer conversion resulted in a change of sign

/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_kernel.h(140): warning: variable "MMA_M" was declared but never referenced
          detected during:
            instantiation of "void flash::compute_attn_1rowblock<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params,Is_Q_split_K>(const Params &, int, int, int) [with Kernel_traits=Flash_fwd_kernel_traits<224, 128, 64, 8, false, false, cutlass::half_t, Flash_kernel_traits<224, 128, 64, 8, cutlass::half_t>>, Is_dropout=true, Is_causal=true, Is_even_N=true, Is_even_K=true, Return_softmax=true, Params=Flash_fwd_params, Is_Q_split_K=false]" 
(589): here
            instantiation of "void flash::compute_attn<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &) [with Kernel_traits=Flash_fwd_kernel_traits<224, 128, 64, 8, false, false, cutlass::half_t, Flash_kernel_traits<224, 128, 64, 8, cutlass::half_t>>, Is_dropout=true, Is_causal=true, Is_even_N=true, Is_even_K=true, Return_softmax=true, Params=Flash_fwd_params]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(15): here
            instantiation of "void flash_fwd_kernel<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax>(Flash_fwd_params) [with Kernel_traits=Flash_fwd_kernel_traits<224, 128, 64, 8, false, false, cutlass::half_t, Flash_kernel_traits<224, 128, 64, 8, cutlass::half_t>>, Is_dropout=true, Is_causal=true, Is_even_N=true, Is_even_K=true, Return_softmax=true]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(34): here
            instantiation of "void run_flash_fwd<Kernel_traits,Is_dropout,Is_causal>(Flash_fwd_params &, cudaStream_t) [with Kernel_traits=Flash_fwd_kernel_traits<224, 128, 64, 8, false, false, cutlass::half_t, Flash_kernel_traits<224, 128, 64, 8, cutlass::half_t>>, Is_dropout=true, Is_causal=true]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(208): here
            instantiation of "void run_mha_fwd_hdim224<T>(Flash_fwd_params &, cudaStream_t) [with T=cutlass::half_t]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_hdim224_fp16_sm80.cu(8): here

/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_kernel.h(276): warning: variable "tQrQ" was declared but never referenced
          detected during:
            instantiation of "void flash::compute_attn_1rowblock<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params,Is_Q_split_K>(const Params &, int, int, int) [with Kernel_traits=Flash_fwd_kernel_traits<224, 128, 64, 8, false, false, cutlass::half_t, Flash_kernel_traits<224, 128, 64, 8, cutlass::half_t>>, Is_dropout=true, Is_causal=true, Is_even_N=true, Is_even_K=true, Return_softmax=true, Params=Flash_fwd_params, Is_Q_split_K=false]" 
(589): here
            instantiation of "void flash::compute_attn<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &) [with Kernel_traits=Flash_fwd_kernel_traits<224, 128, 64, 8, false, false, cutlass::half_t, Flash_kernel_traits<224, 128, 64, 8, cutlass::half_t>>, Is_dropout=true, Is_causal=true, Is_even_N=true, Is_even_K=true, Return_softmax=true, Params=Flash_fwd_params]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(15): here
            instantiation of "void flash_fwd_kernel<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax>(Flash_fwd_params) [with Kernel_traits=Flash_fwd_kernel_traits<224, 128, 64, 8, false, false, cutlass::half_t, Flash_kernel_traits<224, 128, 64, 8, cutlass::half_t>>, Is_dropout=true, Is_causal=true, Is_even_N=true, Is_even_K=true, Return_softmax=true]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(34): here
            instantiation of "void run_flash_fwd<Kernel_traits,Is_dropout,Is_causal>(Flash_fwd_params &, cudaStream_t) [with Kernel_traits=Flash_fwd_kernel_traits<224, 128, 64, 8, false, false, cutlass::half_t, Flash_kernel_traits<224, 128, 64, 8, cutlass::half_t>>, Is_dropout=true, Is_causal=true]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(208): here
            instantiation of "void run_mha_fwd_hdim224<T>(Flash_fwd_params &, cudaStream_t) [with T=cutlass::half_t]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_hdim224_fp16_sm80.cu(8): here

/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_kernel.h(140): warning: variable "MMA_M" was declared but never referenced
          detected during:
            instantiation of "void flash::compute_attn_1rowblock<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params,Is_Q_split_K>(const Params &, int, int, int) [with Kernel_traits=Flash_fwd_kernel_traits<224, 128, 64, 8, false, false, cutlass::half_t, Flash_kernel_traits<224, 128, 64, 8, cutlass::half_t>>, Is_dropout=true, Is_causal=true, Is_even_N=true, Is_even_K=true, Return_softmax=false, Params=Flash_fwd_params, Is_Q_split_K=false]" 
(589): here
            instantiation of "void flash::compute_attn<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &) [with Kernel_traits=Flash_fwd_kernel_traits<224, 128, 64, 8, false, false, cutlass::half_t, Flash_kernel_traits<224, 128, 64, 8, cutlass::half_t>>, Is_dropout=true, Is_causal=true, Is_even_N=true, Is_even_K=true, Return_softmax=false, Params=Flash_fwd_params]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(15): here
            instantiation of "void flash_fwd_kernel<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax>(Flash_fwd_params) [with Kernel_traits=Flash_fwd_kernel_traits<224, 128, 64, 8, false, false, cutlass::half_t, Flash_kernel_traits<224, 128, 64, 8, cutlass::half_t>>, Is_dropout=true, Is_causal=true, Is_even_N=true, Is_even_K=true, Return_softmax=false]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(34): here
            instantiation of "void run_flash_fwd<Kernel_traits,Is_dropout,Is_causal>(Flash_fwd_params &, cudaStream_t) [with Kernel_traits=Flash_fwd_kernel_traits<224, 128, 64, 8, false, false, cutlass::half_t, Flash_kernel_traits<224, 128, 64, 8, cutlass::half_t>>, Is_dropout=true, Is_causal=true]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(208): here
            instantiation of "void run_mha_fwd_hdim224<T>(Flash_fwd_params &, cudaStream_t) [with T=cutlass::half_t]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_hdim224_fp16_sm80.cu(8): here

/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_kernel.h(276): warning: variable "tQrQ" was declared but never referenced
          detected during:
            instantiation of "void flash::compute_attn_1rowblock<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params,Is_Q_split_K>(const Params &, int, int, int) [with Kernel_traits=Flash_fwd_kernel_traits<224, 128, 64, 8, false, false, cutlass::half_t, Flash_kernel_traits<224, 128, 64, 8, cutlass::half_t>>, Is_dropout=true, Is_causal=true, Is_even_N=true, Is_even_K=true, Return_softmax=false, Params=Flash_fwd_params, Is_Q_split_K=false]" 
(589): here
            instantiation of "void flash::compute_attn<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &) [with Kernel_traits=Flash_fwd_kernel_traits<224, 128, 64, 8, false, false, cutlass::half_t, Flash_kernel_traits<224, 128, 64, 8, cutlass::half_t>>, Is_dropout=true, Is_causal=true, Is_even_N=true, Is_even_K=true, Return_softmax=false, Params=Flash_fwd_params]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(15): here
            instantiation of "void flash_fwd_kernel<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax>(Flash_fwd_params) [with Kernel_traits=Flash_fwd_kernel_traits<224, 128, 64, 8, false, false, cutlass::half_t, Flash_kernel_traits<224, 128, 64, 8, cutlass::half_t>>, Is_dropout=true, Is_causal=true, Is_even_N=true, Is_even_K=true, Return_softmax=false]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(34): here
            instantiation of "void run_flash_fwd<Kernel_traits,Is_dropout,Is_causal>(Flash_fwd_params &, cudaStream_t) [with Kernel_traits=Flash_fwd_kernel_traits<224, 128, 64, 8, false, false, cutlass::half_t, Flash_kernel_traits<224, 128, 64, 8, cutlass::half_t>>, Is_dropout=true, Is_causal=true]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(208): here
            instantiation of "void run_mha_fwd_hdim224<T>(Flash_fwd_params &, cudaStream_t) [with T=cutlass::half_t]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_hdim224_fp16_sm80.cu(8): here

/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_kernel.h(140): warning: variable "MMA_M" was declared but never referenced
          detected during:
            instantiation of "void flash::compute_attn_1rowblock<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params,Is_Q_split_K>(const Params &, int, int, int) [with Kernel_traits=Flash_fwd_kernel_traits<224, 128, 64, 8, false, false, cutlass::half_t, Flash_kernel_traits<224, 128, 64, 8, cutlass::half_t>>, Is_dropout=true, Is_causal=true, Is_even_N=true, Is_even_K=false, Return_softmax=true, Params=Flash_fwd_params, Is_Q_split_K=false]" 
(589): here
            instantiation of "void flash::compute_attn<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &) [with Kernel_traits=Flash_fwd_kernel_traits<224, 128, 64, 8, false, false, cutlass::half_t, Flash_kernel_traits<224, 128, 64, 8, cutlass::half_t>>, Is_dropout=true, Is_causal=true, Is_even_N=true, Is_even_K=false, Return_softmax=true, Params=Flash_fwd_params]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(15): here
            instantiation of "void flash_fwd_kernel<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax>(Flash_fwd_params) [with Kernel_traits=Flash_fwd_kernel_traits<224, 128, 64, 8, false, false, cutlass::half_t, Flash_kernel_traits<224, 128, 64, 8, cutlass::half_t>>, Is_dropout=true, Is_causal=true, Is_even_N=true, Is_even_K=false, Return_softmax=true]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(34): here
            instantiation of "void run_flash_fwd<Kernel_traits,Is_dropout,Is_causal>(Flash_fwd_params &, cudaStream_t) [with Kernel_traits=Flash_fwd_kernel_traits<224, 128, 64, 8, false, false, cutlass::half_t, Flash_kernel_traits<224, 128, 64, 8, cutlass::half_t>>, Is_dropout=true, Is_causal=true]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(208): here
            instantiation of "void run_mha_fwd_hdim224<T>(Flash_fwd_params &, cudaStream_t) [with T=cutlass::half_t]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_hdim224_fp16_sm80.cu(8): here

/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_kernel.h(276): warning: variable "tQrQ" was declared but never referenced
          detected during:
            instantiation of "void flash::compute_attn_1rowblock<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params,Is_Q_split_K>(const Params &, int, int, int) [with Kernel_traits=Flash_fwd_kernel_traits<224, 128, 64, 8, false, false, cutlass::half_t, Flash_kernel_traits<224, 128, 64, 8, cutlass::half_t>>, Is_dropout=true, Is_causal=true, Is_even_N=true, Is_even_K=false, Return_softmax=true, Params=Flash_fwd_params, Is_Q_split_K=false]" 
(589): here
            instantiation of "void flash::compute_attn<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &) [with Kernel_traits=Flash_fwd_kernel_traits<224, 128, 64, 8, false, false, cutlass::half_t, Flash_kernel_traits<224, 128, 64, 8, cutlass::half_t>>, Is_dropout=true, Is_causal=true, Is_even_N=true, Is_even_K=false, Return_softmax=true, Params=Flash_fwd_params]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(15): here
            instantiation of "void flash_fwd_kernel<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax>(Flash_fwd_params) [with Kernel_traits=Flash_fwd_kernel_traits<224, 128, 64, 8, false, false, cutlass::half_t, Flash_kernel_traits<224, 128, 64, 8, cutlass::half_t>>, Is_dropout=true, Is_causal=true, Is_even_N=true, Is_even_K=false, Return_softmax=true]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(34): here
            instantiation of "void run_flash_fwd<Kernel_traits,Is_dropout,Is_causal>(Flash_fwd_params &, cudaStream_t) [with Kernel_traits=Flash_fwd_kernel_traits<224, 128, 64, 8, false, false, cutlass::half_t, Flash_kernel_traits<224, 128, 64, 8, cutlass::half_t>>, Is_dropout=true, Is_causal=true]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(208): here
            instantiation of "void run_mha_fwd_hdim224<T>(Flash_fwd_params &, cudaStream_t) [with T=cutlass::half_t]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_hdim224_fp16_sm80.cu(8): here

/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_kernel.h(140): warning: variable "MMA_M" was declared but never referenced
          detected during:
            instantiation of "void flash::compute_attn_1rowblock<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params,Is_Q_split_K>(const Params &, int, int, int) [with Kernel_traits=Flash_fwd_kernel_traits<224, 128, 64, 8, false, false, cutlass::half_t, Flash_kernel_traits<224, 128, 64, 8, cutlass::half_t>>, Is_dropout=true, Is_causal=true, Is_even_N=true, Is_even_K=false, Return_softmax=false, Params=Flash_fwd_params, Is_Q_split_K=false]" 
(589): here
            instantiation of "void flash::compute_attn<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &) [with Kernel_traits=Flash_fwd_kernel_traits<224, 128, 64, 8, false, false, cutlass::half_t, Flash_kernel_traits<224, 128, 64, 8, cutlass::half_t>>, Is_dropout=true, Is_causal=true, Is_even_N=true, Is_even_K=false, Return_softmax=false, Params=Flash_fwd_params]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(15): here
            instantiation of "void flash_fwd_kernel<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax>(Flash_fwd_params) [with Kernel_traits=Flash_fwd_kernel_traits<224, 128, 64, 8, false, false, cutlass::half_t, Flash_kernel_traits<224, 128, 64, 8, cutlass::half_t>>, Is_dropout=true, Is_causal=true, Is_even_N=true, Is_even_K=false, Return_softmax=false]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(34): here
            instantiation of "void run_flash_fwd<Kernel_traits,Is_dropout,Is_causal>(Flash_fwd_params &, cudaStream_t) [with Kernel_traits=Flash_fwd_kernel_traits<224, 128, 64, 8, false, false, cutlass::half_t, Flash_kernel_traits<224, 128, 64, 8, cutlass::half_t>>, Is_dropout=true, Is_causal=true]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(208): here
            instantiation of "void run_mha_fwd_hdim224<T>(Flash_fwd_params &, cudaStream_t) [with T=cutlass::half_t]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_hdim224_fp16_sm80.cu(8): here

/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_kernel.h(276): warning: variable "tQrQ" was declared but never referenced
          detected during:
            instantiation of "void flash::compute_attn_1rowblock<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params,Is_Q_split_K>(const Params &, int, int, int) [with Kernel_traits=Flash_fwd_kernel_traits<224, 128, 64, 8, false, false, cutlass::half_t, Flash_kernel_traits<224, 128, 64, 8, cutlass::half_t>>, Is_dropout=true, Is_causal=true, Is_even_N=true, Is_even_K=false, Return_softmax=false, Params=Flash_fwd_params, Is_Q_split_K=false]" 
(589): here
            instantiation of "void flash::compute_attn<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &) [with Kernel_traits=Flash_fwd_kernel_traits<224, 128, 64, 8, false, false, cutlass::half_t, Flash_kernel_traits<224, 128, 64, 8, cutlass::half_t>>, Is_dropout=true, Is_causal=true, Is_even_N=true, Is_even_K=false, Return_softmax=false, Params=Flash_fwd_params]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(15): here
            instantiation of "void flash_fwd_kernel<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax>(Flash_fwd_params) [with Kernel_traits=Flash_fwd_kernel_traits<224, 128, 64, 8, false, false, cutlass::half_t, Flash_kernel_traits<224, 128, 64, 8, cutlass::half_t>>, Is_dropout=true, Is_causal=true, Is_even_N=true, Is_even_K=false, Return_softmax=false]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(34): here
            instantiation of "void run_flash_fwd<Kernel_traits,Is_dropout,Is_causal>(Flash_fwd_params &, cudaStream_t) [with Kernel_traits=Flash_fwd_kernel_traits<224, 128, 64, 8, false, false, cutlass::half_t, Flash_kernel_traits<224, 128, 64, 8, cutlass::half_t>>, Is_dropout=true, Is_causal=true]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(208): here
            instantiation of "void run_mha_fwd_hdim224<T>(Flash_fwd_params &, cudaStream_t) [with T=cutlass::half_t]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_hdim224_fp16_sm80.cu(8): here

/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_kernel.h(140): warning: variable "MMA_M" was declared but never referenced
          detected during:
            instantiation of "void flash::compute_attn_1rowblock<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params,Is_Q_split_K>(const Params &, int, int, int) [with Kernel_traits=Flash_fwd_kernel_traits<224, 128, 64, 8, false, false, cutlass::half_t, Flash_kernel_traits<224, 128, 64, 8, cutlass::half_t>>, Is_dropout=true, Is_causal=true, Is_even_N=false, Is_even_K=true, Return_softmax=true, Params=Flash_fwd_params, Is_Q_split_K=false]" 
(589): here
            instantiation of "void flash::compute_attn<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &) [with Kernel_traits=Flash_fwd_kernel_traits<224, 128, 64, 8, false, false, cutlass::half_t, Flash_kernel_traits<224, 128, 64, 8, cutlass::half_t>>, Is_dropout=true, Is_causal=true, Is_even_N=false, Is_even_K=true, Return_softmax=true, Params=Flash_fwd_params]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(15): here
            instantiation of "void flash_fwd_kernel<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax>(Flash_fwd_params) [with Kernel_traits=Flash_fwd_kernel_traits<224, 128, 64, 8, false, false, cutlass::half_t, Flash_kernel_traits<224, 128, 64, 8, cutlass::half_t>>, Is_dropout=true, Is_causal=true, Is_even_N=false, Is_even_K=true, Return_softmax=true]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(34): here
            instantiation of "void run_flash_fwd<Kernel_traits,Is_dropout,Is_causal>(Flash_fwd_params &, cudaStream_t) [with Kernel_traits=Flash_fwd_kernel_traits<224, 128, 64, 8, false, false, cutlass::half_t, Flash_kernel_traits<224, 128, 64, 8, cutlass::half_t>>, Is_dropout=true, Is_causal=true]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(208): here
            instantiation of "void run_mha_fwd_hdim224<T>(Flash_fwd_params &, cudaStream_t) [with T=cutlass::half_t]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_hdim224_fp16_sm80.cu(8): here

/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_kernel.h(276): warning: variable "tQrQ" was declared but never referenced
          detected during:
            instantiation of "void flash::compute_attn_1rowblock<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params,Is_Q_split_K>(const Params &, int, int, int) [with Kernel_traits=Flash_fwd_kernel_traits<224, 128, 64, 8, false, false, cutlass::half_t, Flash_kernel_traits<224, 128, 64, 8, cutlass::half_t>>, Is_dropout=true, Is_causal=true, Is_even_N=false, Is_even_K=true, Return_softmax=true, Params=Flash_fwd_params, Is_Q_split_K=false]" 
(589): here
            instantiation of "void flash::compute_attn<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &) [with Kernel_traits=Flash_fwd_kernel_traits<224, 128, 64, 8, false, false, cutlass::half_t, Flash_kernel_traits<224, 128, 64, 8, cutlass::half_t>>, Is_dropout=true, Is_causal=true, Is_even_N=false, Is_even_K=true, Return_softmax=true, Params=Flash_fwd_params]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(15): here
            instantiation of "void flash_fwd_kernel<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax>(Flash_fwd_params) [with Kernel_traits=Flash_fwd_kernel_traits<224, 128, 64, 8, false, false, cutlass::half_t, Flash_kernel_traits<224, 128, 64, 8, cutlass::half_t>>, Is_dropout=true, Is_causal=true, Is_even_N=false, Is_even_K=true, Return_softmax=true]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(34): here
            instantiation of "void run_flash_fwd<Kernel_traits,Is_dropout,Is_causal>(Flash_fwd_params &, cudaStream_t) [with Kernel_traits=Flash_fwd_kernel_traits<224, 128, 64, 8, false, false, cutlass::half_t, Flash_kernel_traits<224, 128, 64, 8, cutlass::half_t>>, Is_dropout=true, Is_causal=true]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(208): here
            instantiation of "void run_mha_fwd_hdim224<T>(Flash_fwd_params &, cudaStream_t) [with T=cutlass::half_t]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_hdim224_fp16_sm80.cu(8): here

/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_kernel.h(140): warning: variable "MMA_M" was declared but never referenced
          detected during:
            instantiation of "void flash::compute_attn_1rowblock<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params,Is_Q_split_K>(const Params &, int, int, int) [with Kernel_traits=Flash_fwd_kernel_traits<224, 128, 64, 8, false, false, cutlass::half_t, Flash_kernel_traits<224, 128, 64, 8, cutlass::half_t>>, Is_dropout=true, Is_causal=true, Is_even_N=false, Is_even_K=true, Return_softmax=false, Params=Flash_fwd_params, Is_Q_split_K=false]" 
(589): here
            instantiation of "void flash::compute_attn<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &) [with Kernel_traits=Flash_fwd_kernel_traits<224, 128, 64, 8, false, false, cutlass::half_t, Flash_kernel_traits<224, 128, 64, 8, cutlass::half_t>>, Is_dropout=true, Is_causal=true, Is_even_N=false, Is_even_K=true, Return_softmax=false, Params=Flash_fwd_params]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(15): here
            instantiation of "void flash_fwd_kernel<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax>(Flash_fwd_params) [with Kernel_traits=Flash_fwd_kernel_traits<224, 128, 64, 8, false, false, cutlass::half_t, Flash_kernel_traits<224, 128, 64, 8, cutlass::half_t>>, Is_dropout=true, Is_causal=true, Is_even_N=false, Is_even_K=true, Return_softmax=false]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(34): here
            instantiation of "void run_flash_fwd<Kernel_traits,Is_dropout,Is_causal>(Flash_fwd_params &, cudaStream_t) [with Kernel_traits=Flash_fwd_kernel_traits<224, 128, 64, 8, false, false, cutlass::half_t, Flash_kernel_traits<224, 128, 64, 8, cutlass::half_t>>, Is_dropout=true, Is_causal=true]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(208): here
            instantiation of "void run_mha_fwd_hdim224<T>(Flash_fwd_params &, cudaStream_t) [with T=cutlass::half_t]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_hdim224_fp16_sm80.cu(8): here

/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_kernel.h(276): warning: variable "tQrQ" was declared but never referenced
          detected during:
            instantiation of "void flash::compute_attn_1rowblock<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params,Is_Q_split_K>(const Params &, int, int, int) [with Kernel_traits=Flash_fwd_kernel_traits<224, 128, 64, 8, false, false, cutlass::half_t, Flash_kernel_traits<224, 128, 64, 8, cutlass::half_t>>, Is_dropout=true, Is_causal=true, Is_even_N=false, Is_even_K=true, Return_softmax=false, Params=Flash_fwd_params, Is_Q_split_K=false]" 
(589): here
            instantiation of "void flash::compute_attn<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &) [with Kernel_traits=Flash_fwd_kernel_traits<224, 128, 64, 8, false, false, cutlass::half_t, Flash_kernel_traits<224, 128, 64, 8, cutlass::half_t>>, Is_dropout=true, Is_causal=true, Is_even_N=false, Is_even_K=true, Return_softmax=false, Params=Flash_fwd_params]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(15): here
            instantiation of "void flash_fwd_kernel<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax>(Flash_fwd_params) [with Kernel_traits=Flash_fwd_kernel_traits<224, 128, 64, 8, false, false, cutlass::half_t, Flash_kernel_traits<224, 128, 64, 8, cutlass::half_t>>, Is_dropout=true, Is_causal=true, Is_even_N=false, Is_even_K=true, Return_softmax=false]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(34): here
            instantiation of "void run_flash_fwd<Kernel_traits,Is_dropout,Is_causal>(Flash_fwd_params &, cudaStream_t) [with Kernel_traits=Flash_fwd_kernel_traits<224, 128, 64, 8, false, false, cutlass::half_t, Flash_kernel_traits<224, 128, 64, 8, cutlass::half_t>>, Is_dropout=true, Is_causal=true]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(208): here
            instantiation of "void run_mha_fwd_hdim224<T>(Flash_fwd_params &, cudaStream_t) [with T=cutlass::half_t]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_hdim224_fp16_sm80.cu(8): here

/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_kernel.h(140): warning: variable "MMA_M" was declared but never referenced
          detected during:
            instantiation of "void flash::compute_attn_1rowblock<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params,Is_Q_split_K>(const Params &, int, int, int) [with Kernel_traits=Flash_fwd_kernel_traits<224, 128, 64, 8, false, false, cutlass::half_t, Flash_kernel_traits<224, 128, 64, 8, cutlass::half_t>>, Is_dropout=true, Is_causal=true, Is_even_N=false, Is_even_K=false, Return_softmax=true, Params=Flash_fwd_params, Is_Q_split_K=false]" 
(589): here
            instantiation of "void flash::compute_attn<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &) [with Kernel_traits=Flash_fwd_kernel_traits<224, 128, 64, 8, false, false, cutlass::half_t, Flash_kernel_traits<224, 128, 64, 8, cutlass::half_t>>, Is_dropout=true, Is_causal=true, Is_even_N=false, Is_even_K=false, Return_softmax=true, Params=Flash_fwd_params]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(15): here
            instantiation of "void flash_fwd_kernel<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax>(Flash_fwd_params) [with Kernel_traits=Flash_fwd_kernel_traits<224, 128, 64, 8, false, false, cutlass::half_t, Flash_kernel_traits<224, 128, 64, 8, cutlass::half_t>>, Is_dropout=true, Is_causal=true, Is_even_N=false, Is_even_K=false, Return_softmax=true]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(34): here
            instantiation of "void run_flash_fwd<Kernel_traits,Is_dropout,Is_causal>(Flash_fwd_params &, cudaStream_t) [with Kernel_traits=Flash_fwd_kernel_traits<224, 128, 64, 8, false, false, cutlass::half_t, Flash_kernel_traits<224, 128, 64, 8, cutlass::half_t>>, Is_dropout=true, Is_causal=true]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(208): here
            instantiation of "void run_mha_fwd_hdim224<T>(Flash_fwd_params &, cudaStream_t) [with T=cutlass::half_t]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_hdim224_fp16_sm80.cu(8): here

/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_kernel.h(276): warning: variable "tQrQ" was declared but never referenced
          detected during:
            instantiation of "void flash::compute_attn_1rowblock<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params,Is_Q_split_K>(const Params &, int, int, int) [with Kernel_traits=Flash_fwd_kernel_traits<224, 128, 64, 8, false, false, cutlass::half_t, Flash_kernel_traits<224, 128, 64, 8, cutlass::half_t>>, Is_dropout=true, Is_causal=true, Is_even_N=false, Is_even_K=false, Return_softmax=true, Params=Flash_fwd_params, Is_Q_split_K=false]" 
(589): here
            instantiation of "void flash::compute_attn<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &) [with Kernel_traits=Flash_fwd_kernel_traits<224, 128, 64, 8, false, false, cutlass::half_t, Flash_kernel_traits<224, 128, 64, 8, cutlass::half_t>>, Is_dropout=true, Is_causal=true, Is_even_N=false, Is_even_K=false, Return_softmax=true, Params=Flash_fwd_params]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(15): here
            instantiation of "void flash_fwd_kernel<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax>(Flash_fwd_params) [with Kernel_traits=Flash_fwd_kernel_traits<224, 128, 64, 8, false, false, cutlass::half_t, Flash_kernel_traits<224, 128, 64, 8, cutlass::half_t>>, Is_dropout=true, Is_causal=true, Is_even_N=false, Is_even_K=false, Return_softmax=true]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(34): here
            instantiation of "void run_flash_fwd<Kernel_traits,Is_dropout,Is_causal>(Flash_fwd_params &, cudaStream_t) [with Kernel_traits=Flash_fwd_kernel_traits<224, 128, 64, 8, false, false, cutlass::half_t, Flash_kernel_traits<224, 128, 64, 8, cutlass::half_t>>, Is_dropout=true, Is_causal=true]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(208): here
            instantiation of "void run_mha_fwd_hdim224<T>(Flash_fwd_params &, cudaStream_t) [with T=cutlass::half_t]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_hdim224_fp16_sm80.cu(8): here

/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_kernel.h(140): warning: variable "MMA_M" was declared but never referenced
          detected during:
            instantiation of "void flash::compute_attn_1rowblock<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params,Is_Q_split_K>(const Params &, int, int, int) [with Kernel_traits=Flash_fwd_kernel_traits<224, 128, 64, 8, false, false, cutlass::half_t, Flash_kernel_traits<224, 128, 64, 8, cutlass::half_t>>, Is_dropout=true, Is_causal=true, Is_even_N=false, Is_even_K=false, Return_softmax=false, Params=Flash_fwd_params, Is_Q_split_K=false]" 
(589): here
            instantiation of "void flash::compute_attn<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &) [with Kernel_traits=Flash_fwd_kernel_traits<224, 128, 64, 8, false, false, cutlass::half_t, Flash_kernel_traits<224, 128, 64, 8, cutlass::half_t>>, Is_dropout=true, Is_causal=true, Is_even_N=false, Is_even_K=false, Return_softmax=false, Params=Flash_fwd_params]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(15): here
            instantiation of "void flash_fwd_kernel<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax>(Flash_fwd_params) [with Kernel_traits=Flash_fwd_kernel_traits<224, 128, 64, 8, false, false, cutlass::half_t, Flash_kernel_traits<224, 128, 64, 8, cutlass::half_t>>, Is_dropout=true, Is_causal=true, Is_even_N=false, Is_even_K=false, Return_softmax=false]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(34): here
            instantiation of "void run_flash_fwd<Kernel_traits,Is_dropout,Is_causal>(Flash_fwd_params &, cudaStream_t) [with Kernel_traits=Flash_fwd_kernel_traits<224, 128, 64, 8, false, false, cutlass::half_t, Flash_kernel_traits<224, 128, 64, 8, cutlass::half_t>>, Is_dropout=true, Is_causal=true]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(208): here
            instantiation of "void run_mha_fwd_hdim224<T>(Flash_fwd_params &, cudaStream_t) [with T=cutlass::half_t]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_hdim224_fp16_sm80.cu(8): here

/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_kernel.h(276): warning: variable "tQrQ" was declared but never referenced
          detected during:
            instantiation of "void flash::compute_attn_1rowblock<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params,Is_Q_split_K>(const Params &, int, int, int) [with Kernel_traits=Flash_fwd_kernel_traits<224, 128, 64, 8, false, false, cutlass::half_t, Flash_kernel_traits<224, 128, 64, 8, cutlass::half_t>>, Is_dropout=true, Is_causal=true, Is_even_N=false, Is_even_K=false, Return_softmax=false, Params=Flash_fwd_params, Is_Q_split_K=false]" 
(589): here
            instantiation of "void flash::compute_attn<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &) [with Kernel_traits=Flash_fwd_kernel_traits<224, 128, 64, 8, false, false, cutlass::half_t, Flash_kernel_traits<224, 128, 64, 8, cutlass::half_t>>, Is_dropout=true, Is_causal=true, Is_even_N=false, Is_even_K=false, Return_softmax=false, Params=Flash_fwd_params]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(15): here
            instantiation of "void flash_fwd_kernel<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax>(Flash_fwd_params) [with Kernel_traits=Flash_fwd_kernel_traits<224, 128, 64, 8, false, false, cutlass::half_t, Flash_kernel_traits<224, 128, 64, 8, cutlass::half_t>>, Is_dropout=true, Is_causal=true, Is_even_N=false, Is_even_K=false, Return_softmax=false]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(34): here
            instantiation of "void run_flash_fwd<Kernel_traits,Is_dropout,Is_causal>(Flash_fwd_params &, cudaStream_t) [with Kernel_traits=Flash_fwd_kernel_traits<224, 128, 64, 8, false, false, cutlass::half_t, Flash_kernel_traits<224, 128, 64, 8, cutlass::half_t>>, Is_dropout=true, Is_causal=true]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(208): here
            instantiation of "void run_mha_fwd_hdim224<T>(Flash_fwd_params &, cudaStream_t) [with T=cutlass::half_t]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_hdim224_fp16_sm80.cu(8): here

/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_kernel.h(140): warning: variable "MMA_M" was declared but never referenced
          detected during:
            instantiation of "void flash::compute_attn_1rowblock<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params,Is_Q_split_K>(const Params &, int, int, int) [with Kernel_traits=Flash_fwd_kernel_traits<224, 64, 64, 4, false, false, cutlass::half_t, Flash_kernel_traits<224, 64, 64, 4, cutlass::half_t>>, Is_dropout=true, Is_causal=true, Is_even_N=true, Is_even_K=true, Return_softmax=true, Params=Flash_fwd_params, Is_Q_split_K=false]" 
(589): here
            instantiation of "void flash::compute_attn<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &) [with Kernel_traits=Flash_fwd_kernel_traits<224, 64, 64, 4, false, false, cutlass::half_t, Flash_kernel_traits<224, 64, 64, 4, cutlass::half_t>>, Is_dropout=true, Is_causal=true, Is_even_N=true, Is_even_K=true, Return_softmax=true, Params=Flash_fwd_params]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(15): here
            instantiation of "void flash_fwd_kernel<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax>(Flash_fwd_params) [with Kernel_traits=Flash_fwd_kernel_traits<224, 64, 64, 4, false, false, cutlass::half_t, Flash_kernel_traits<224, 64, 64, 4, cutlass::half_t>>, Is_dropout=true, Is_causal=true, Is_even_N=true, Is_even_K=true, Return_softmax=true]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(34): here
            instantiation of "void run_flash_fwd<Kernel_traits,Is_dropout,Is_causal>(Flash_fwd_params &, cudaStream_t) [with Kernel_traits=Flash_fwd_kernel_traits<224, 64, 64, 4, false, false, cutlass::half_t, Flash_kernel_traits<224, 64, 64, 4, cutlass::half_t>>, Is_dropout=true, Is_causal=true]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(208): here
            instantiation of "void run_mha_fwd_hdim224<T>(Flash_fwd_params &, cudaStream_t) [with T=cutlass::half_t]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_hdim224_fp16_sm80.cu(8): here

/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_kernel.h(276): warning: variable "tQrQ" was declared but never referenced
          detected during:
            instantiation of "void flash::compute_attn_1rowblock<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params,Is_Q_split_K>(const Params &, int, int, int) [with Kernel_traits=Flash_fwd_kernel_traits<224, 64, 64, 4, false, false, cutlass::half_t, Flash_kernel_traits<224, 64, 64, 4, cutlass::half_t>>, Is_dropout=true, Is_causal=true, Is_even_N=true, Is_even_K=true, Return_softmax=true, Params=Flash_fwd_params, Is_Q_split_K=false]" 
(589): here
            instantiation of "void flash::compute_attn<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &) [with Kernel_traits=Flash_fwd_kernel_traits<224, 64, 64, 4, false, false, cutlass::half_t, Flash_kernel_traits<224, 64, 64, 4, cutlass::half_t>>, Is_dropout=true, Is_causal=true, Is_even_N=true, Is_even_K=true, Return_softmax=true, Params=Flash_fwd_params]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(15): here
            instantiation of "void flash_fwd_kernel<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax>(Flash_fwd_params) [with Kernel_traits=Flash_fwd_kernel_traits<224, 64, 64, 4, false, false, cutlass::half_t, Flash_kernel_traits<224, 64, 64, 4, cutlass::half_t>>, Is_dropout=true, Is_causal=true, Is_even_N=true, Is_even_K=true, Return_softmax=true]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(34): here
            instantiation of "void run_flash_fwd<Kernel_traits,Is_dropout,Is_causal>(Flash_fwd_params &, cudaStream_t) [with Kernel_traits=Flash_fwd_kernel_traits<224, 64, 64, 4, false, false, cutlass::half_t, Flash_kernel_traits<224, 64, 64, 4, cutlass::half_t>>, Is_dropout=true, Is_causal=true]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(208): here
            instantiation of "void run_mha_fwd_hdim224<T>(Flash_fwd_params &, cudaStream_t) [with T=cutlass::half_t]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_hdim224_fp16_sm80.cu(8): here

/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_kernel.h(140): warning: variable "MMA_M" was declared but never referenced
          detected during:
            instantiation of "void flash::compute_attn_1rowblock<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params,Is_Q_split_K>(const Params &, int, int, int) [with Kernel_traits=Flash_fwd_kernel_traits<224, 64, 64, 4, false, false, cutlass::half_t, Flash_kernel_traits<224, 64, 64, 4, cutlass::half_t>>, Is_dropout=true, Is_causal=true, Is_even_N=true, Is_even_K=true, Return_softmax=false, Params=Flash_fwd_params, Is_Q_split_K=false]" 
(589): here
            instantiation of "void flash::compute_attn<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &) [with Kernel_traits=Flash_fwd_kernel_traits<224, 64, 64, 4, false, false, cutlass::half_t, Flash_kernel_traits<224, 64, 64, 4, cutlass::half_t>>, Is_dropout=true, Is_causal=true, Is_even_N=true, Is_even_K=true, Return_softmax=false, Params=Flash_fwd_params]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(15): here
            instantiation of "void flash_fwd_kernel<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax>(Flash_fwd_params) [with Kernel_traits=Flash_fwd_kernel_traits<224, 64, 64, 4, false, false, cutlass::half_t, Flash_kernel_traits<224, 64, 64, 4, cutlass::half_t>>, Is_dropout=true, Is_causal=true, Is_even_N=true, Is_even_K=true, Return_softmax=false]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(34): here
            instantiation of "void run_flash_fwd<Kernel_traits,Is_dropout,Is_causal>(Flash_fwd_params &, cudaStream_t) [with Kernel_traits=Flash_fwd_kernel_traits<224, 64, 64, 4, false, false, cutlass::half_t, Flash_kernel_traits<224, 64, 64, 4, cutlass::half_t>>, Is_dropout=true, Is_causal=true]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(208): here
            instantiation of "void run_mha_fwd_hdim224<T>(Flash_fwd_params &, cudaStream_t) [with T=cutlass::half_t]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_hdim224_fp16_sm80.cu(8): here

/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_kernel.h(276): warning: variable "tQrQ" was declared but never referenced
          detected during:
            instantiation of "void flash::compute_attn_1rowblock<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params,Is_Q_split_K>(const Params &, int, int, int) [with Kernel_traits=Flash_fwd_kernel_traits<224, 64, 64, 4, false, false, cutlass::half_t, Flash_kernel_traits<224, 64, 64, 4, cutlass::half_t>>, Is_dropout=true, Is_causal=true, Is_even_N=true, Is_even_K=true, Return_softmax=false, Params=Flash_fwd_params, Is_Q_split_K=false]" 
(589): here
            instantiation of "void flash::compute_attn<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &) [with Kernel_traits=Flash_fwd_kernel_traits<224, 64, 64, 4, false, false, cutlass::half_t, Flash_kernel_traits<224, 64, 64, 4, cutlass::half_t>>, Is_dropout=true, Is_causal=true, Is_even_N=true, Is_even_K=true, Return_softmax=false, Params=Flash_fwd_params]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(15): here
            instantiation of "void flash_fwd_kernel<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax>(Flash_fwd_params) [with Kernel_traits=Flash_fwd_kernel_traits<224, 64, 64, 4, false, false, cutlass::half_t, Flash_kernel_traits<224, 64, 64, 4, cutlass::half_t>>, Is_dropout=true, Is_causal=true, Is_even_N=true, Is_even_K=true, Return_softmax=false]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(34): here
            instantiation of "void run_flash_fwd<Kernel_traits,Is_dropout,Is_causal>(Flash_fwd_params &, cudaStream_t) [with Kernel_traits=Flash_fwd_kernel_traits<224, 64, 64, 4, false, false, cutlass::half_t, Flash_kernel_traits<224, 64, 64, 4, cutlass::half_t>>, Is_dropout=true, Is_causal=true]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(208): here
            instantiation of "void run_mha_fwd_hdim224<T>(Flash_fwd_params &, cudaStream_t) [with T=cutlass::half_t]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_hdim224_fp16_sm80.cu(8): here

/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_kernel.h(140): warning: variable "MMA_M" was declared but never referenced
          detected during:
            instantiation of "void flash::compute_attn_1rowblock<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params,Is_Q_split_K>(const Params &, int, int, int) [with Kernel_traits=Flash_fwd_kernel_traits<224, 64, 64, 4, false, false, cutlass::half_t, Flash_kernel_traits<224, 64, 64, 4, cutlass::half_t>>, Is_dropout=true, Is_causal=true, Is_even_N=true, Is_even_K=false, Return_softmax=true, Params=Flash_fwd_params, Is_Q_split_K=false]" 
(589): here
            instantiation of "void flash::compute_attn<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &) [with Kernel_traits=Flash_fwd_kernel_traits<224, 64, 64, 4, false, false, cutlass::half_t, Flash_kernel_traits<224, 64, 64, 4, cutlass::half_t>>, Is_dropout=true, Is_causal=true, Is_even_N=true, Is_even_K=false, Return_softmax=true, Params=Flash_fwd_params]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(15): here
            instantiation of "void flash_fwd_kernel<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax>(Flash_fwd_params) [with Kernel_traits=Flash_fwd_kernel_traits<224, 64, 64, 4, false, false, cutlass::half_t, Flash_kernel_traits<224, 64, 64, 4, cutlass::half_t>>, Is_dropout=true, Is_causal=true, Is_even_N=true, Is_even_K=false, Return_softmax=true]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(34): here
            instantiation of "void run_flash_fwd<Kernel_traits,Is_dropout,Is_causal>(Flash_fwd_params &, cudaStream_t) [with Kernel_traits=Flash_fwd_kernel_traits<224, 64, 64, 4, false, false, cutlass::half_t, Flash_kernel_traits<224, 64, 64, 4, cutlass::half_t>>, Is_dropout=true, Is_causal=true]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(208): here
            instantiation of "void run_mha_fwd_hdim224<T>(Flash_fwd_params &, cudaStream_t) [with T=cutlass::half_t]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_hdim224_fp16_sm80.cu(8): here

/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_kernel.h(276): warning: variable "tQrQ" was declared but never referenced
          detected during:
            instantiation of "void flash::compute_attn_1rowblock<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params,Is_Q_split_K>(const Params &, int, int, int) [with Kernel_traits=Flash_fwd_kernel_traits<224, 64, 64, 4, false, false, cutlass::half_t, Flash_kernel_traits<224, 64, 64, 4, cutlass::half_t>>, Is_dropout=true, Is_causal=true, Is_even_N=true, Is_even_K=false, Return_softmax=true, Params=Flash_fwd_params, Is_Q_split_K=false]" 
(589): here
            instantiation of "void flash::compute_attn<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &) [with Kernel_traits=Flash_fwd_kernel_traits<224, 64, 64, 4, false, false, cutlass::half_t, Flash_kernel_traits<224, 64, 64, 4, cutlass::half_t>>, Is_dropout=true, Is_causal=true, Is_even_N=true, Is_even_K=false, Return_softmax=true, Params=Flash_fwd_params]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(15): here
            instantiation of "void flash_fwd_kernel<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax>(Flash_fwd_params) [with Kernel_traits=Flash_fwd_kernel_traits<224, 64, 64, 4, false, false, cutlass::half_t, Flash_kernel_traits<224, 64, 64, 4, cutlass::half_t>>, Is_dropout=true, Is_causal=true, Is_even_N=true, Is_even_K=false, Return_softmax=true]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(34): here
            instantiation of "void run_flash_fwd<Kernel_traits,Is_dropout,Is_causal>(Flash_fwd_params &, cudaStream_t) [with Kernel_traits=Flash_fwd_kernel_traits<224, 64, 64, 4, false, false, cutlass::half_t, Flash_kernel_traits<224, 64, 64, 4, cutlass::half_t>>, Is_dropout=true, Is_causal=true]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(208): here
            instantiation of "void run_mha_fwd_hdim224<T>(Flash_fwd_params &, cudaStream_t) [with T=cutlass::half_t]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_hdim224_fp16_sm80.cu(8): here

/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_kernel.h(140): warning: variable "MMA_M" was declared but never referenced
          detected during:
            instantiation of "void flash::compute_attn_1rowblock<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params,Is_Q_split_K>(const Params &, int, int, int) [with Kernel_traits=Flash_fwd_kernel_traits<224, 64, 64, 4, false, false, cutlass::half_t, Flash_kernel_traits<224, 64, 64, 4, cutlass::half_t>>, Is_dropout=true, Is_causal=true, Is_even_N=true, Is_even_K=false, Return_softmax=false, Params=Flash_fwd_params, Is_Q_split_K=false]" 
(589): here
            instantiation of "void flash::compute_attn<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &) [with Kernel_traits=Flash_fwd_kernel_traits<224, 64, 64, 4, false, false, cutlass::half_t, Flash_kernel_traits<224, 64, 64, 4, cutlass::half_t>>, Is_dropout=true, Is_causal=true, Is_even_N=true, Is_even_K=false, Return_softmax=false, Params=Flash_fwd_params]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(15): here
            instantiation of "void flash_fwd_kernel<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax>(Flash_fwd_params) [with Kernel_traits=Flash_fwd_kernel_traits<224, 64, 64, 4, false, false, cutlass::half_t, Flash_kernel_traits<224, 64, 64, 4, cutlass::half_t>>, Is_dropout=true, Is_causal=true, Is_even_N=true, Is_even_K=false, Return_softmax=false]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(34): here
            instantiation of "void run_flash_fwd<Kernel_traits,Is_dropout,Is_causal>(Flash_fwd_params &, cudaStream_t) [with Kernel_traits=Flash_fwd_kernel_traits<224, 64, 64, 4, false, false, cutlass::half_t, Flash_kernel_traits<224, 64, 64, 4, cutlass::half_t>>, Is_dropout=true, Is_causal=true]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(208): here
            instantiation of "void run_mha_fwd_hdim224<T>(Flash_fwd_params &, cudaStream_t) [with T=cutlass::half_t]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_hdim224_fp16_sm80.cu(8): here

/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_kernel.h(276): warning: variable "tQrQ" was declared but never referenced
          detected during:
            instantiation of "void flash::compute_attn_1rowblock<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params,Is_Q_split_K>(const Params &, int, int, int) [with Kernel_traits=Flash_fwd_kernel_traits<224, 64, 64, 4, false, false, cutlass::half_t, Flash_kernel_traits<224, 64, 64, 4, cutlass::half_t>>, Is_dropout=true, Is_causal=true, Is_even_N=true, Is_even_K=false, Return_softmax=false, Params=Flash_fwd_params, Is_Q_split_K=false]" 
(589): here
            instantiation of "void flash::compute_attn<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &) [with Kernel_traits=Flash_fwd_kernel_traits<224, 64, 64, 4, false, false, cutlass::half_t, Flash_kernel_traits<224, 64, 64, 4, cutlass::half_t>>, Is_dropout=true, Is_causal=true, Is_even_N=true, Is_even_K=false, Return_softmax=false, Params=Flash_fwd_params]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(15): here
            instantiation of "void flash_fwd_kernel<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax>(Flash_fwd_params) [with Kernel_traits=Flash_fwd_kernel_traits<224, 64, 64, 4, false, false, cutlass::half_t, Flash_kernel_traits<224, 64, 64, 4, cutlass::half_t>>, Is_dropout=true, Is_causal=true, Is_even_N=true, Is_even_K=false, Return_softmax=false]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(34): here
            instantiation of "void run_flash_fwd<Kernel_traits,Is_dropout,Is_causal>(Flash_fwd_params &, cudaStream_t) [with Kernel_traits=Flash_fwd_kernel_traits<224, 64, 64, 4, false, false, cutlass::half_t, Flash_kernel_traits<224, 64, 64, 4, cutlass::half_t>>, Is_dropout=true, Is_causal=true]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(208): here
            instantiation of "void run_mha_fwd_hdim224<T>(Flash_fwd_params &, cudaStream_t) [with T=cutlass::half_t]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_hdim224_fp16_sm80.cu(8): here

/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_kernel.h(140): warning: variable "MMA_M" was declared but never referenced
          detected during:
            instantiation of "void flash::compute_attn_1rowblock<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params,Is_Q_split_K>(const Params &, int, int, int) [with Kernel_traits=Flash_fwd_kernel_traits<224, 64, 64, 4, false, false, cutlass::half_t, Flash_kernel_traits<224, 64, 64, 4, cutlass::half_t>>, Is_dropout=true, Is_causal=true, Is_even_N=false, Is_even_K=true, Return_softmax=true, Params=Flash_fwd_params, Is_Q_split_K=false]" 
(589): here
            instantiation of "void flash::compute_attn<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &) [with Kernel_traits=Flash_fwd_kernel_traits<224, 64, 64, 4, false, false, cutlass::half_t, Flash_kernel_traits<224, 64, 64, 4, cutlass::half_t>>, Is_dropout=true, Is_causal=true, Is_even_N=false, Is_even_K=true, Return_softmax=true, Params=Flash_fwd_params]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(15): here
            instantiation of "void flash_fwd_kernel<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax>(Flash_fwd_params) [with Kernel_traits=Flash_fwd_kernel_traits<224, 64, 64, 4, false, false, cutlass::half_t, Flash_kernel_traits<224, 64, 64, 4, cutlass::half_t>>, Is_dropout=true, Is_causal=true, Is_even_N=false, Is_even_K=true, Return_softmax=true]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(34): here
            instantiation of "void run_flash_fwd<Kernel_traits,Is_dropout,Is_causal>(Flash_fwd_params &, cudaStream_t) [with Kernel_traits=Flash_fwd_kernel_traits<224, 64, 64, 4, false, false, cutlass::half_t, Flash_kernel_traits<224, 64, 64, 4, cutlass::half_t>>, Is_dropout=true, Is_causal=true]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(208): here
            instantiation of "void run_mha_fwd_hdim224<T>(Flash_fwd_params &, cudaStream_t) [with T=cutlass::half_t]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_hdim224_fp16_sm80.cu(8): here

/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_kernel.h(276): warning: variable "tQrQ" was declared but never referenced
          detected during:
            instantiation of "void flash::compute_attn_1rowblock<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params,Is_Q_split_K>(const Params &, int, int, int) [with Kernel_traits=Flash_fwd_kernel_traits<224, 64, 64, 4, false, false, cutlass::half_t, Flash_kernel_traits<224, 64, 64, 4, cutlass::half_t>>, Is_dropout=true, Is_causal=true, Is_even_N=false, Is_even_K=true, Return_softmax=true, Params=Flash_fwd_params, Is_Q_split_K=false]" 
(589): here
            instantiation of "void flash::compute_attn<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &) [with Kernel_traits=Flash_fwd_kernel_traits<224, 64, 64, 4, false, false, cutlass::half_t, Flash_kernel_traits<224, 64, 64, 4, cutlass::half_t>>, Is_dropout=true, Is_causal=true, Is_even_N=false, Is_even_K=true, Return_softmax=true, Params=Flash_fwd_params]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(15): here
            instantiation of "void flash_fwd_kernel<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax>(Flash_fwd_params) [with Kernel_traits=Flash_fwd_kernel_traits<224, 64, 64, 4, false, false, cutlass::half_t, Flash_kernel_traits<224, 64, 64, 4, cutlass::half_t>>, Is_dropout=true, Is_causal=true, Is_even_N=false, Is_even_K=true, Return_softmax=true]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(34): here
            instantiation of "void run_flash_fwd<Kernel_traits,Is_dropout,Is_causal>(Flash_fwd_params &, cudaStream_t) [with Kernel_traits=Flash_fwd_kernel_traits<224, 64, 64, 4, false, false, cutlass::half_t, Flash_kernel_traits<224, 64, 64, 4, cutlass::half_t>>, Is_dropout=true, Is_causal=true]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(208): here
            instantiation of "void run_mha_fwd_hdim224<T>(Flash_fwd_params &, cudaStream_t) [with T=cutlass::half_t]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_hdim224_fp16_sm80.cu(8): here

/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_kernel.h(140): warning: variable "MMA_M" was declared but never referenced
          detected during:
            instantiation of "void flash::compute_attn_1rowblock<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params,Is_Q_split_K>(const Params &, int, int, int) [with Kernel_traits=Flash_fwd_kernel_traits<224, 64, 64, 4, false, false, cutlass::half_t, Flash_kernel_traits<224, 64, 64, 4, cutlass::half_t>>, Is_dropout=true, Is_causal=true, Is_even_N=false, Is_even_K=true, Return_softmax=false, Params=Flash_fwd_params, Is_Q_split_K=false]" 
(589): here
            instantiation of "void flash::compute_attn<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &) [with Kernel_traits=Flash_fwd_kernel_traits<224, 64, 64, 4, false, false, cutlass::half_t, Flash_kernel_traits<224, 64, 64, 4, cutlass::half_t>>, Is_dropout=true, Is_causal=true, Is_even_N=false, Is_even_K=true, Return_softmax=false, Params=Flash_fwd_params]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(15): here
            instantiation of "void flash_fwd_kernel<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax>(Flash_fwd_params) [with Kernel_traits=Flash_fwd_kernel_traits<224, 64, 64, 4, false, false, cutlass::half_t, Flash_kernel_traits<224, 64, 64, 4, cutlass::half_t>>, Is_dropout=true, Is_causal=true, Is_even_N=false, Is_even_K=true, Return_softmax=false]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(34): here
            instantiation of "void run_flash_fwd<Kernel_traits,Is_dropout,Is_causal>(Flash_fwd_params &, cudaStream_t) [with Kernel_traits=Flash_fwd_kernel_traits<224, 64, 64, 4, false, false, cutlass::half_t, Flash_kernel_traits<224, 64, 64, 4, cutlass::half_t>>, Is_dropout=true, Is_causal=true]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(208): here
            instantiation of "void run_mha_fwd_hdim224<T>(Flash_fwd_params &, cudaStream_t) [with T=cutlass::half_t]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_hdim224_fp16_sm80.cu(8): here

/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_kernel.h(276): warning: variable "tQrQ" was declared but never referenced
          detected during:
            instantiation of "void flash::compute_attn_1rowblock<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params,Is_Q_split_K>(const Params &, int, int, int) [with Kernel_traits=Flash_fwd_kernel_traits<224, 64, 64, 4, false, false, cutlass::half_t, Flash_kernel_traits<224, 64, 64, 4, cutlass::half_t>>, Is_dropout=true, Is_causal=true, Is_even_N=false, Is_even_K=true, Return_softmax=false, Params=Flash_fwd_params, Is_Q_split_K=false]" 
(589): here
            instantiation of "void flash::compute_attn<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &) [with Kernel_traits=Flash_fwd_kernel_traits<224, 64, 64, 4, false, false, cutlass::half_t, Flash_kernel_traits<224, 64, 64, 4, cutlass::half_t>>, Is_dropout=true, Is_causal=true, Is_even_N=false, Is_even_K=true, Return_softmax=false, Params=Flash_fwd_params]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(15): here
            instantiation of "void flash_fwd_kernel<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax>(Flash_fwd_params) [with Kernel_traits=Flash_fwd_kernel_traits<224, 64, 64, 4, false, false, cutlass::half_t, Flash_kernel_traits<224, 64, 64, 4, cutlass::half_t>>, Is_dropout=true, Is_causal=true, Is_even_N=false, Is_even_K=true, Return_softmax=false]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(34): here
            instantiation of "void run_flash_fwd<Kernel_traits,Is_dropout,Is_causal>(Flash_fwd_params &, cudaStream_t) [with Kernel_traits=Flash_fwd_kernel_traits<224, 64, 64, 4, false, false, cutlass::half_t, Flash_kernel_traits<224, 64, 64, 4, cutlass::half_t>>, Is_dropout=true, Is_causal=true]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(208): here
            instantiation of "void run_mha_fwd_hdim224<T>(Flash_fwd_params &, cudaStream_t) [with T=cutlass::half_t]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_hdim224_fp16_sm80.cu(8): here

/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_kernel.h(140): warning: variable "MMA_M" was declared but never referenced
          detected during:
            instantiation of "void flash::compute_attn_1rowblock<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params,Is_Q_split_K>(const Params &, int, int, int) [with Kernel_traits=Flash_fwd_kernel_traits<224, 64, 64, 4, false, false, cutlass::half_t, Flash_kernel_traits<224, 64, 64, 4, cutlass::half_t>>, Is_dropout=true, Is_causal=true, Is_even_N=false, Is_even_K=false, Return_softmax=true, Params=Flash_fwd_params, Is_Q_split_K=false]" 
(589): here
            instantiation of "void flash::compute_attn<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &) [with Kernel_traits=Flash_fwd_kernel_traits<224, 64, 64, 4, false, false, cutlass::half_t, Flash_kernel_traits<224, 64, 64, 4, cutlass::half_t>>, Is_dropout=true, Is_causal=true, Is_even_N=false, Is_even_K=false, Return_softmax=true, Params=Flash_fwd_params]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(15): here
            instantiation of "void flash_fwd_kernel<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax>(Flash_fwd_params) [with Kernel_traits=Flash_fwd_kernel_traits<224, 64, 64, 4, false, false, cutlass::half_t, Flash_kernel_traits<224, 64, 64, 4, cutlass::half_t>>, Is_dropout=true, Is_causal=true, Is_even_N=false, Is_even_K=false, Return_softmax=true]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(34): here
            instantiation of "void run_flash_fwd<Kernel_traits,Is_dropout,Is_causal>(Flash_fwd_params &, cudaStream_t) [with Kernel_traits=Flash_fwd_kernel_traits<224, 64, 64, 4, false, false, cutlass::half_t, Flash_kernel_traits<224, 64, 64, 4, cutlass::half_t>>, Is_dropout=true, Is_causal=true]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(208): here
            instantiation of "void run_mha_fwd_hdim224<T>(Flash_fwd_params &, cudaStream_t) [with T=cutlass::half_t]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_hdim224_fp16_sm80.cu(8): here

/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_kernel.h(276): warning: variable "tQrQ" was declared but never referenced
          detected during:
            instantiation of "void flash::compute_attn_1rowblock<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params,Is_Q_split_K>(const Params &, int, int, int) [with Kernel_traits=Flash_fwd_kernel_traits<224, 64, 64, 4, false, false, cutlass::half_t, Flash_kernel_traits<224, 64, 64, 4, cutlass::half_t>>, Is_dropout=true, Is_causal=true, Is_even_N=false, Is_even_K=false, Return_softmax=true, Params=Flash_fwd_params, Is_Q_split_K=false]" 
(589): here
            instantiation of "void flash::compute_attn<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &) [with Kernel_traits=Flash_fwd_kernel_traits<224, 64, 64, 4, false, false, cutlass::half_t, Flash_kernel_traits<224, 64, 64, 4, cutlass::half_t>>, Is_dropout=true, Is_causal=true, Is_even_N=false, Is_even_K=false, Return_softmax=true, Params=Flash_fwd_params]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(15): here
            instantiation of "void flash_fwd_kernel<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax>(Flash_fwd_params) [with Kernel_traits=Flash_fwd_kernel_traits<224, 64, 64, 4, false, false, cutlass::half_t, Flash_kernel_traits<224, 64, 64, 4, cutlass::half_t>>, Is_dropout=true, Is_causal=true, Is_even_N=false, Is_even_K=false, Return_softmax=true]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(34): here
            instantiation of "void run_flash_fwd<Kernel_traits,Is_dropout,Is_causal>(Flash_fwd_params &, cudaStream_t) [with Kernel_traits=Flash_fwd_kernel_traits<224, 64, 64, 4, false, false, cutlass::half_t, Flash_kernel_traits<224, 64, 64, 4, cutlass::half_t>>, Is_dropout=true, Is_causal=true]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(208): here
            instantiation of "void run_mha_fwd_hdim224<T>(Flash_fwd_params &, cudaStream_t) [with T=cutlass::half_t]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_hdim224_fp16_sm80.cu(8): here

/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_kernel.h(140): warning: variable "MMA_M" was declared but never referenced
          detected during:
            instantiation of "void flash::compute_attn_1rowblock<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params,Is_Q_split_K>(const Params &, int, int, int) [with Kernel_traits=Flash_fwd_kernel_traits<224, 64, 64, 4, false, false, cutlass::half_t, Flash_kernel_traits<224, 64, 64, 4, cutlass::half_t>>, Is_dropout=true, Is_causal=true, Is_even_N=false, Is_even_K=false, Return_softmax=false, Params=Flash_fwd_params, Is_Q_split_K=false]" 
(589): here
            instantiation of "void flash::compute_attn<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &) [with Kernel_traits=Flash_fwd_kernel_traits<224, 64, 64, 4, false, false, cutlass::half_t, Flash_kernel_traits<224, 64, 64, 4, cutlass::half_t>>, Is_dropout=true, Is_causal=true, Is_even_N=false, Is_even_K=false, Return_softmax=false, Params=Flash_fwd_params]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(15): here
            instantiation of "void flash_fwd_kernel<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax>(Flash_fwd_params) [with Kernel_traits=Flash_fwd_kernel_traits<224, 64, 64, 4, false, false, cutlass::half_t, Flash_kernel_traits<224, 64, 64, 4, cutlass::half_t>>, Is_dropout=true, Is_causal=true, Is_even_N=false, Is_even_K=false, Return_softmax=false]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(34): here
            instantiation of "void run_flash_fwd<Kernel_traits,Is_dropout,Is_causal>(Flash_fwd_params &, cudaStream_t) [with Kernel_traits=Flash_fwd_kernel_traits<224, 64, 64, 4, false, false, cutlass::half_t, Flash_kernel_traits<224, 64, 64, 4, cutlass::half_t>>, Is_dropout=true, Is_causal=true]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(208): here
            instantiation of "void run_mha_fwd_hdim224<T>(Flash_fwd_params &, cudaStream_t) [with T=cutlass::half_t]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_hdim224_fp16_sm80.cu(8): here

/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_kernel.h(276): warning: variable "tQrQ" was declared but never referenced
          detected during:
            instantiation of "void flash::compute_attn_1rowblock<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params,Is_Q_split_K>(const Params &, int, int, int) [with Kernel_traits=Flash_fwd_kernel_traits<224, 64, 64, 4, false, false, cutlass::half_t, Flash_kernel_traits<224, 64, 64, 4, cutlass::half_t>>, Is_dropout=true, Is_causal=true, Is_even_N=false, Is_even_K=false, Return_softmax=false, Params=Flash_fwd_params, Is_Q_split_K=false]" 
(589): here
            instantiation of "void flash::compute_attn<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &) [with Kernel_traits=Flash_fwd_kernel_traits<224, 64, 64, 4, false, false, cutlass::half_t, Flash_kernel_traits<224, 64, 64, 4, cutlass::half_t>>, Is_dropout=true, Is_causal=true, Is_even_N=false, Is_even_K=false, Return_softmax=false, Params=Flash_fwd_params]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(15): here
            instantiation of "void flash_fwd_kernel<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax>(Flash_fwd_params) [with Kernel_traits=Flash_fwd_kernel_traits<224, 64, 64, 4, false, false, cutlass::half_t, Flash_kernel_traits<224, 64, 64, 4, cutlass::half_t>>, Is_dropout=true, Is_causal=true, Is_even_N=false, Is_even_K=false, Return_softmax=false]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(34): here
            instantiation of "void run_flash_fwd<Kernel_traits,Is_dropout,Is_causal>(Flash_fwd_params &, cudaStream_t) [with Kernel_traits=Flash_fwd_kernel_traits<224, 64, 64, 4, false, false, cutlass::half_t, Flash_kernel_traits<224, 64, 64, 4, cutlass::half_t>>, Is_dropout=true, Is_causal=true]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(208): here
            instantiation of "void run_mha_fwd_hdim224<T>(Flash_fwd_params &, cudaStream_t) [with T=cutlass::half_t]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_hdim224_fp16_sm80.cu(8): here

/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_kernel.h(140): warning: variable "MMA_M" was declared but never referenced
          detected during:
            instantiation of "void flash::compute_attn_1rowblock<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params,Is_Q_split_K>(const Params &, int, int, int) [with Kernel_traits=Flash_fwd_kernel_traits<224, 128, 64, 8, false, false, cutlass::half_t, Flash_kernel_traits<224, 128, 64, 8, cutlass::half_t>>, Is_dropout=true, Is_causal=false, Is_even_N=true, Is_even_K=true, Return_softmax=true, Params=Flash_fwd_params, Is_Q_split_K=false]" 
(589): here
            instantiation of "void flash::compute_attn<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &) [with Kernel_traits=Flash_fwd_kernel_traits<224, 128, 64, 8, false, false, cutlass::half_t, Flash_kernel_traits<224, 128, 64, 8, cutlass::half_t>>, Is_dropout=true, Is_causal=false, Is_even_N=true, Is_even_K=true, Return_softmax=true, Params=Flash_fwd_params]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(15): here
            instantiation of "void flash_fwd_kernel<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax>(Flash_fwd_params) [with Kernel_traits=Flash_fwd_kernel_traits<224, 128, 64, 8, false, false, cutlass::half_t, Flash_kernel_traits<224, 128, 64, 8, cutlass::half_t>>, Is_dropout=true, Is_causal=false, Is_even_N=true, Is_even_K=true, Return_softmax=true]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(34): here
            instantiation of "void run_flash_fwd<Kernel_traits,Is_dropout,Is_causal>(Flash_fwd_params &, cudaStream_t) [with Kernel_traits=Flash_fwd_kernel_traits<224, 128, 64, 8, false, false, cutlass::half_t, Flash_kernel_traits<224, 128, 64, 8, cutlass::half_t>>, Is_dropout=true, Is_causal=false]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(208): here
            instantiation of "void run_mha_fwd_hdim224<T>(Flash_fwd_params &, cudaStream_t) [with T=cutlass::half_t]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_hdim224_fp16_sm80.cu(8): here

/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_kernel.h(276): warning: variable "tQrQ" was declared but never referenced
          detected during:
            instantiation of "void flash::compute_attn_1rowblock<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params,Is_Q_split_K>(const Params &, int, int, int) [with Kernel_traits=Flash_fwd_kernel_traits<224, 128, 64, 8, false, false, cutlass::half_t, Flash_kernel_traits<224, 128, 64, 8, cutlass::half_t>>, Is_dropout=true, Is_causal=false, Is_even_N=true, Is_even_K=true, Return_softmax=true, Params=Flash_fwd_params, Is_Q_split_K=false]" 
(589): here
            instantiation of "void flash::compute_attn<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &) [with Kernel_traits=Flash_fwd_kernel_traits<224, 128, 64, 8, false, false, cutlass::half_t, Flash_kernel_traits<224, 128, 64, 8, cutlass::half_t>>, Is_dropout=true, Is_causal=false, Is_even_N=true, Is_even_K=true, Return_softmax=true, Params=Flash_fwd_params]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(15): here
            instantiation of "void flash_fwd_kernel<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax>(Flash_fwd_params) [with Kernel_traits=Flash_fwd_kernel_traits<224, 128, 64, 8, false, false, cutlass::half_t, Flash_kernel_traits<224, 128, 64, 8, cutlass::half_t>>, Is_dropout=true, Is_causal=false, Is_even_N=true, Is_even_K=true, Return_softmax=true]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(34): here
            instantiation of "void run_flash_fwd<Kernel_traits,Is_dropout,Is_causal>(Flash_fwd_params &, cudaStream_t) [with Kernel_traits=Flash_fwd_kernel_traits<224, 128, 64, 8, false, false, cutlass::half_t, Flash_kernel_traits<224, 128, 64, 8, cutlass::half_t>>, Is_dropout=true, Is_causal=false]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(208): here
            instantiation of "void run_mha_fwd_hdim224<T>(Flash_fwd_params &, cudaStream_t) [with T=cutlass::half_t]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_hdim224_fp16_sm80.cu(8): here

/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_kernel.h(140): warning: variable "MMA_M" was declared but never referenced
          detected during:
            instantiation of "void flash::compute_attn_1rowblock<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params,Is_Q_split_K>(const Params &, int, int, int) [with Kernel_traits=Flash_fwd_kernel_traits<224, 128, 64, 8, false, false, cutlass::half_t, Flash_kernel_traits<224, 128, 64, 8, cutlass::half_t>>, Is_dropout=true, Is_causal=false, Is_even_N=true, Is_even_K=true, Return_softmax=false, Params=Flash_fwd_params, Is_Q_split_K=false]" 
(589): here
            instantiation of "void flash::compute_attn<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &) [with Kernel_traits=Flash_fwd_kernel_traits<224, 128, 64, 8, false, false, cutlass::half_t, Flash_kernel_traits<224, 128, 64, 8, cutlass::half_t>>, Is_dropout=true, Is_causal=false, Is_even_N=true, Is_even_K=true, Return_softmax=false, Params=Flash_fwd_params]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(15): here
            instantiation of "void flash_fwd_kernel<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax>(Flash_fwd_params) [with Kernel_traits=Flash_fwd_kernel_traits<224, 128, 64, 8, false, false, cutlass::half_t, Flash_kernel_traits<224, 128, 64, 8, cutlass::half_t>>, Is_dropout=true, Is_causal=false, Is_even_N=true, Is_even_K=true, Return_softmax=false]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(34): here
            instantiation of "void run_flash_fwd<Kernel_traits,Is_dropout,Is_causal>(Flash_fwd_params &, cudaStream_t) [with Kernel_traits=Flash_fwd_kernel_traits<224, 128, 64, 8, false, false, cutlass::half_t, Flash_kernel_traits<224, 128, 64, 8, cutlass::half_t>>, Is_dropout=true, Is_causal=false]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(208): here
            instantiation of "void run_mha_fwd_hdim224<T>(Flash_fwd_params &, cudaStream_t) [with T=cutlass::half_t]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_hdim224_fp16_sm80.cu(8): here

/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_kernel.h(276): warning: variable "tQrQ" was declared but never referenced
          detected during:
            instantiation of "void flash::compute_attn_1rowblock<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params,Is_Q_split_K>(const Params &, int, int, int) [with Kernel_traits=Flash_fwd_kernel_traits<224, 128, 64, 8, false, false, cutlass::half_t, Flash_kernel_traits<224, 128, 64, 8, cutlass::half_t>>, Is_dropout=true, Is_causal=false, Is_even_N=true, Is_even_K=true, Return_softmax=false, Params=Flash_fwd_params, Is_Q_split_K=false]" 
(589): here
            instantiation of "void flash::compute_attn<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &) [with Kernel_traits=Flash_fwd_kernel_traits<224, 128, 64, 8, false, false, cutlass::half_t, Flash_kernel_traits<224, 128, 64, 8, cutlass::half_t>>, Is_dropout=true, Is_causal=false, Is_even_N=true, Is_even_K=true, Return_softmax=false, Params=Flash_fwd_params]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(15): here
            instantiation of "void flash_fwd_kernel<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax>(Flash_fwd_params) [with Kernel_traits=Flash_fwd_kernel_traits<224, 128, 64, 8, false, false, cutlass::half_t, Flash_kernel_traits<224, 128, 64, 8, cutlass::half_t>>, Is_dropout=true, Is_causal=false, Is_even_N=true, Is_even_K=true, Return_softmax=false]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(34): here
            instantiation of "void run_flash_fwd<Kernel_traits,Is_dropout,Is_causal>(Flash_fwd_params &, cudaStream_t) [with Kernel_traits=Flash_fwd_kernel_traits<224, 128, 64, 8, false, false, cutlass::half_t, Flash_kernel_traits<224, 128, 64, 8, cutlass::half_t>>, Is_dropout=true, Is_causal=false]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(208): here
            instantiation of "void run_mha_fwd_hdim224<T>(Flash_fwd_params &, cudaStream_t) [with T=cutlass::half_t]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_hdim224_fp16_sm80.cu(8): here

/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_kernel.h(140): warning: variable "MMA_M" was declared but never referenced
          detected during:
            instantiation of "void flash::compute_attn_1rowblock<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params,Is_Q_split_K>(const Params &, int, int, int) [with Kernel_traits=Flash_fwd_kernel_traits<224, 128, 64, 8, false, false, cutlass::half_t, Flash_kernel_traits<224, 128, 64, 8, cutlass::half_t>>, Is_dropout=true, Is_causal=false, Is_even_N=true, Is_even_K=false, Return_softmax=true, Params=Flash_fwd_params, Is_Q_split_K=false]" 
(589): here
            instantiation of "void flash::compute_attn<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &) [with Kernel_traits=Flash_fwd_kernel_traits<224, 128, 64, 8, false, false, cutlass::half_t, Flash_kernel_traits<224, 128, 64, 8, cutlass::half_t>>, Is_dropout=true, Is_causal=false, Is_even_N=true, Is_even_K=false, Return_softmax=true, Params=Flash_fwd_params]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(15): here
            instantiation of "void flash_fwd_kernel<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax>(Flash_fwd_params) [with Kernel_traits=Flash_fwd_kernel_traits<224, 128, 64, 8, false, false, cutlass::half_t, Flash_kernel_traits<224, 128, 64, 8, cutlass::half_t>>, Is_dropout=true, Is_causal=false, Is_even_N=true, Is_even_K=false, Return_softmax=true]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(34): here
            instantiation of "void run_flash_fwd<Kernel_traits,Is_dropout,Is_causal>(Flash_fwd_params &, cudaStream_t) [with Kernel_traits=Flash_fwd_kernel_traits<224, 128, 64, 8, false, false, cutlass::half_t, Flash_kernel_traits<224, 128, 64, 8, cutlass::half_t>>, Is_dropout=true, Is_causal=false]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(208): here
            instantiation of "void run_mha_fwd_hdim224<T>(Flash_fwd_params &, cudaStream_t) [with T=cutlass::half_t]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_hdim224_fp16_sm80.cu(8): here

/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_kernel.h(276): warning: variable "tQrQ" was declared but never referenced
          detected during:
            instantiation of "void flash::compute_attn_1rowblock<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params,Is_Q_split_K>(const Params &, int, int, int) [with Kernel_traits=Flash_fwd_kernel_traits<224, 128, 64, 8, false, false, cutlass::half_t, Flash_kernel_traits<224, 128, 64, 8, cutlass::half_t>>, Is_dropout=true, Is_causal=false, Is_even_N=true, Is_even_K=false, Return_softmax=true, Params=Flash_fwd_params, Is_Q_split_K=false]" 
(589): here
            instantiation of "void flash::compute_attn<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &) [with Kernel_traits=Flash_fwd_kernel_traits<224, 128, 64, 8, false, false, cutlass::half_t, Flash_kernel_traits<224, 128, 64, 8, cutlass::half_t>>, Is_dropout=true, Is_causal=false, Is_even_N=true, Is_even_K=false, Return_softmax=true, Params=Flash_fwd_params]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(15): here
            instantiation of "void flash_fwd_kernel<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax>(Flash_fwd_params) [with Kernel_traits=Flash_fwd_kernel_traits<224, 128, 64, 8, false, false, cutlass::half_t, Flash_kernel_traits<224, 128, 64, 8, cutlass::half_t>>, Is_dropout=true, Is_causal=false, Is_even_N=true, Is_even_K=false, Return_softmax=true]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(34): here
            instantiation of "void run_flash_fwd<Kernel_traits,Is_dropout,Is_causal>(Flash_fwd_params &, cudaStream_t) [with Kernel_traits=Flash_fwd_kernel_traits<224, 128, 64, 8, false, false, cutlass::half_t, Flash_kernel_traits<224, 128, 64, 8, cutlass::half_t>>, Is_dropout=true, Is_causal=false]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(208): here
            instantiation of "void run_mha_fwd_hdim224<T>(Flash_fwd_params &, cudaStream_t) [with T=cutlass::half_t]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_hdim224_fp16_sm80.cu(8): here

/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_kernel.h(140): warning: variable "MMA_M" was declared but never referenced
          detected during:
            instantiation of "void flash::compute_attn_1rowblock<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params,Is_Q_split_K>(const Params &, int, int, int) [with Kernel_traits=Flash_fwd_kernel_traits<224, 128, 64, 8, false, false, cutlass::half_t, Flash_kernel_traits<224, 128, 64, 8, cutlass::half_t>>, Is_dropout=true, Is_causal=false, Is_even_N=true, Is_even_K=false, Return_softmax=false, Params=Flash_fwd_params, Is_Q_split_K=false]" 
(589): here
            instantiation of "void flash::compute_attn<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &) [with Kernel_traits=Flash_fwd_kernel_traits<224, 128, 64, 8, false, false, cutlass::half_t, Flash_kernel_traits<224, 128, 64, 8, cutlass::half_t>>, Is_dropout=true, Is_causal=false, Is_even_N=true, Is_even_K=false, Return_softmax=false, Params=Flash_fwd_params]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(15): here
            instantiation of "void flash_fwd_kernel<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax>(Flash_fwd_params) [with Kernel_traits=Flash_fwd_kernel_traits<224, 128, 64, 8, false, false, cutlass::half_t, Flash_kernel_traits<224, 128, 64, 8, cutlass::half_t>>, Is_dropout=true, Is_causal=false, Is_even_N=true, Is_even_K=false, Return_softmax=false]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(34): here
            instantiation of "void run_flash_fwd<Kernel_traits,Is_dropout,Is_causal>(Flash_fwd_params &, cudaStream_t) [with Kernel_traits=Flash_fwd_kernel_traits<224, 128, 64, 8, false, false, cutlass::half_t, Flash_kernel_traits<224, 128, 64, 8, cutlass::half_t>>, Is_dropout=true, Is_causal=false]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(208): here
            instantiation of "void run_mha_fwd_hdim224<T>(Flash_fwd_params &, cudaStream_t) [with T=cutlass::half_t]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_hdim224_fp16_sm80.cu(8): here

/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_kernel.h(276): warning: variable "tQrQ" was declared but never referenced
          detected during:
            instantiation of "void flash::compute_attn_1rowblock<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params,Is_Q_split_K>(const Params &, int, int, int) [with Kernel_traits=Flash_fwd_kernel_traits<224, 128, 64, 8, false, false, cutlass::half_t, Flash_kernel_traits<224, 128, 64, 8, cutlass::half_t>>, Is_dropout=true, Is_causal=false, Is_even_N=true, Is_even_K=false, Return_softmax=false, Params=Flash_fwd_params, Is_Q_split_K=false]" 
(589): here
            instantiation of "void flash::compute_attn<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &) [with Kernel_traits=Flash_fwd_kernel_traits<224, 128, 64, 8, false, false, cutlass::half_t, Flash_kernel_traits<224, 128, 64, 8, cutlass::half_t>>, Is_dropout=true, Is_causal=false, Is_even_N=true, Is_even_K=false, Return_softmax=false, Params=Flash_fwd_params]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(15): here
            instantiation of "void flash_fwd_kernel<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax>(Flash_fwd_params) [with Kernel_traits=Flash_fwd_kernel_traits<224, 128, 64, 8, false, false, cutlass::half_t, Flash_kernel_traits<224, 128, 64, 8, cutlass::half_t>>, Is_dropout=true, Is_causal=false, Is_even_N=true, Is_even_K=false, Return_softmax=false]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(34): here
            instantiation of "void run_flash_fwd<Kernel_traits,Is_dropout,Is_causal>(Flash_fwd_params &, cudaStream_t) [with Kernel_traits=Flash_fwd_kernel_traits<224, 128, 64, 8, false, false, cutlass::half_t, Flash_kernel_traits<224, 128, 64, 8, cutlass::half_t>>, Is_dropout=true, Is_causal=false]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(208): here
            instantiation of "void run_mha_fwd_hdim224<T>(Flash_fwd_params &, cudaStream_t) [with T=cutlass::half_t]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_hdim224_fp16_sm80.cu(8): here

/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_kernel.h(140): warning: variable "MMA_M" was declared but never referenced
          detected during:
            instantiation of "void flash::compute_attn_1rowblock<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params,Is_Q_split_K>(const Params &, int, int, int) [with Kernel_traits=Flash_fwd_kernel_traits<224, 128, 64, 8, false, false, cutlass::half_t, Flash_kernel_traits<224, 128, 64, 8, cutlass::half_t>>, Is_dropout=true, Is_causal=false, Is_even_N=false, Is_even_K=true, Return_softmax=true, Params=Flash_fwd_params, Is_Q_split_K=false]" 
(589): here
            instantiation of "void flash::compute_attn<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &) [with Kernel_traits=Flash_fwd_kernel_traits<224, 128, 64, 8, false, false, cutlass::half_t, Flash_kernel_traits<224, 128, 64, 8, cutlass::half_t>>, Is_dropout=true, Is_causal=false, Is_even_N=false, Is_even_K=true, Return_softmax=true, Params=Flash_fwd_params]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(15): here
            instantiation of "void flash_fwd_kernel<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax>(Flash_fwd_params) [with Kernel_traits=Flash_fwd_kernel_traits<224, 128, 64, 8, false, false, cutlass::half_t, Flash_kernel_traits<224, 128, 64, 8, cutlass::half_t>>, Is_dropout=true, Is_causal=false, Is_even_N=false, Is_even_K=true, Return_softmax=true]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(34): here
            instantiation of "void run_flash_fwd<Kernel_traits,Is_dropout,Is_causal>(Flash_fwd_params &, cudaStream_t) [with Kernel_traits=Flash_fwd_kernel_traits<224, 128, 64, 8, false, false, cutlass::half_t, Flash_kernel_traits<224, 128, 64, 8, cutlass::half_t>>, Is_dropout=true, Is_causal=false]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(208): here
            instantiation of "void run_mha_fwd_hdim224<T>(Flash_fwd_params &, cudaStream_t) [with T=cutlass::half_t]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_hdim224_fp16_sm80.cu(8): here

/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_kernel.h(276): warning: variable "tQrQ" was declared but never referenced
          detected during:
            instantiation of "void flash::compute_attn_1rowblock<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params,Is_Q_split_K>(const Params &, int, int, int) [with Kernel_traits=Flash_fwd_kernel_traits<224, 128, 64, 8, false, false, cutlass::half_t, Flash_kernel_traits<224, 128, 64, 8, cutlass::half_t>>, Is_dropout=true, Is_causal=false, Is_even_N=false, Is_even_K=true, Return_softmax=true, Params=Flash_fwd_params, Is_Q_split_K=false]" 
(589): here
            instantiation of "void flash::compute_attn<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &) [with Kernel_traits=Flash_fwd_kernel_traits<224, 128, 64, 8, false, false, cutlass::half_t, Flash_kernel_traits<224, 128, 64, 8, cutlass::half_t>>, Is_dropout=true, Is_causal=false, Is_even_N=false, Is_even_K=true, Return_softmax=true, Params=Flash_fwd_params]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(15): here
            instantiation of "void flash_fwd_kernel<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax>(Flash_fwd_params) [with Kernel_traits=Flash_fwd_kernel_traits<224, 128, 64, 8, false, false, cutlass::half_t, Flash_kernel_traits<224, 128, 64, 8, cutlass::half_t>>, Is_dropout=true, Is_causal=false, Is_even_N=false, Is_even_K=true, Return_softmax=true]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(34): here
            instantiation of "void run_flash_fwd<Kernel_traits,Is_dropout,Is_causal>(Flash_fwd_params &, cudaStream_t) [with Kernel_traits=Flash_fwd_kernel_traits<224, 128, 64, 8, false, false, cutlass::half_t, Flash_kernel_traits<224, 128, 64, 8, cutlass::half_t>>, Is_dropout=true, Is_causal=false]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(208): here
            instantiation of "void run_mha_fwd_hdim224<T>(Flash_fwd_params &, cudaStream_t) [with T=cutlass::half_t]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_hdim224_fp16_sm80.cu(8): here

/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_kernel.h(140): warning: variable "MMA_M" was declared but never referenced
          detected during:
            instantiation of "void flash::compute_attn_1rowblock<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params,Is_Q_split_K>(const Params &, int, int, int) [with Kernel_traits=Flash_fwd_kernel_traits<224, 128, 64, 8, false, false, cutlass::half_t, Flash_kernel_traits<224, 128, 64, 8, cutlass::half_t>>, Is_dropout=true, Is_causal=false, Is_even_N=false, Is_even_K=true, Return_softmax=false, Params=Flash_fwd_params, Is_Q_split_K=false]" 
(589): here
            instantiation of "void flash::compute_attn<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &) [with Kernel_traits=Flash_fwd_kernel_traits<224, 128, 64, 8, false, false, cutlass::half_t, Flash_kernel_traits<224, 128, 64, 8, cutlass::half_t>>, Is_dropout=true, Is_causal=false, Is_even_N=false, Is_even_K=true, Return_softmax=false, Params=Flash_fwd_params]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(15): here
            instantiation of "void flash_fwd_kernel<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax>(Flash_fwd_params) [with Kernel_traits=Flash_fwd_kernel_traits<224, 128, 64, 8, false, false, cutlass::half_t, Flash_kernel_traits<224, 128, 64, 8, cutlass::half_t>>, Is_dropout=true, Is_causal=false, Is_even_N=false, Is_even_K=true, Return_softmax=false]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(34): here
            instantiation of "void run_flash_fwd<Kernel_traits,Is_dropout,Is_causal>(Flash_fwd_params &, cudaStream_t) [with Kernel_traits=Flash_fwd_kernel_traits<224, 128, 64, 8, false, false, cutlass::half_t, Flash_kernel_traits<224, 128, 64, 8, cutlass::half_t>>, Is_dropout=true, Is_causal=false]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(208): here
            instantiation of "void run_mha_fwd_hdim224<T>(Flash_fwd_params &, cudaStream_t) [with T=cutlass::half_t]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_hdim224_fp16_sm80.cu(8): here

/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_kernel.h(276): warning: variable "tQrQ" was declared but never referenced
          detected during:
            instantiation of "void flash::compute_attn_1rowblock<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params,Is_Q_split_K>(const Params &, int, int, int) [with Kernel_traits=Flash_fwd_kernel_traits<224, 128, 64, 8, false, false, cutlass::half_t, Flash_kernel_traits<224, 128, 64, 8, cutlass::half_t>>, Is_dropout=true, Is_causal=false, Is_even_N=false, Is_even_K=true, Return_softmax=false, Params=Flash_fwd_params, Is_Q_split_K=false]" 
(589): here
            instantiation of "void flash::compute_attn<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &) [with Kernel_traits=Flash_fwd_kernel_traits<224, 128, 64, 8, false, false, cutlass::half_t, Flash_kernel_traits<224, 128, 64, 8, cutlass::half_t>>, Is_dropout=true, Is_causal=false, Is_even_N=false, Is_even_K=true, Return_softmax=false, Params=Flash_fwd_params]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(15): here
            instantiation of "void flash_fwd_kernel<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax>(Flash_fwd_params) [with Kernel_traits=Flash_fwd_kernel_traits<224, 128, 64, 8, false, false, cutlass::half_t, Flash_kernel_traits<224, 128, 64, 8, cutlass::half_t>>, Is_dropout=true, Is_causal=false, Is_even_N=false, Is_even_K=true, Return_softmax=false]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(34): here
            instantiation of "void run_flash_fwd<Kernel_traits,Is_dropout,Is_causal>(Flash_fwd_params &, cudaStream_t) [with Kernel_traits=Flash_fwd_kernel_traits<224, 128, 64, 8, false, false, cutlass::half_t, Flash_kernel_traits<224, 128, 64, 8, cutlass::half_t>>, Is_dropout=true, Is_causal=false]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(208): here
            instantiation of "void run_mha_fwd_hdim224<T>(Flash_fwd_params &, cudaStream_t) [with T=cutlass::half_t]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_hdim224_fp16_sm80.cu(8): here

/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_kernel.h(140): warning: variable "MMA_M" was declared but never referenced
          detected during:
            instantiation of "void flash::compute_attn_1rowblock<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params,Is_Q_split_K>(const Params &, int, int, int) [with Kernel_traits=Flash_fwd_kernel_traits<224, 128, 64, 8, false, false, cutlass::half_t, Flash_kernel_traits<224, 128, 64, 8, cutlass::half_t>>, Is_dropout=true, Is_causal=false, Is_even_N=false, Is_even_K=false, Return_softmax=true, Params=Flash_fwd_params, Is_Q_split_K=false]" 
(589): here
            instantiation of "void flash::compute_attn<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &) [with Kernel_traits=Flash_fwd_kernel_traits<224, 128, 64, 8, false, false, cutlass::half_t, Flash_kernel_traits<224, 128, 64, 8, cutlass::half_t>>, Is_dropout=true, Is_causal=false, Is_even_N=false, Is_even_K=false, Return_softmax=true, Params=Flash_fwd_params]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(15): here
            instantiation of "void flash_fwd_kernel<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax>(Flash_fwd_params) [with Kernel_traits=Flash_fwd_kernel_traits<224, 128, 64, 8, false, false, cutlass::half_t, Flash_kernel_traits<224, 128, 64, 8, cutlass::half_t>>, Is_dropout=true, Is_causal=false, Is_even_N=false, Is_even_K=false, Return_softmax=true]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(34): here
            instantiation of "void run_flash_fwd<Kernel_traits,Is_dropout,Is_causal>(Flash_fwd_params &, cudaStream_t) [with Kernel_traits=Flash_fwd_kernel_traits<224, 128, 64, 8, false, false, cutlass::half_t, Flash_kernel_traits<224, 128, 64, 8, cutlass::half_t>>, Is_dropout=true, Is_causal=false]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(208): here
            instantiation of "void run_mha_fwd_hdim224<T>(Flash_fwd_params &, cudaStream_t) [with T=cutlass::half_t]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_hdim224_fp16_sm80.cu(8): here

/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_kernel.h(276): warning: variable "tQrQ" was declared but never referenced
          detected during:
            instantiation of "void flash::compute_attn_1rowblock<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params,Is_Q_split_K>(const Params &, int, int, int) [with Kernel_traits=Flash_fwd_kernel_traits<224, 128, 64, 8, false, false, cutlass::half_t, Flash_kernel_traits<224, 128, 64, 8, cutlass::half_t>>, Is_dropout=true, Is_causal=false, Is_even_N=false, Is_even_K=false, Return_softmax=true, Params=Flash_fwd_params, Is_Q_split_K=false]" 
(589): here
            instantiation of "void flash::compute_attn<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &) [with Kernel_traits=Flash_fwd_kernel_traits<224, 128, 64, 8, false, false, cutlass::half_t, Flash_kernel_traits<224, 128, 64, 8, cutlass::half_t>>, Is_dropout=true, Is_causal=false, Is_even_N=false, Is_even_K=false, Return_softmax=true, Params=Flash_fwd_params]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(15): here
            instantiation of "void flash_fwd_kernel<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax>(Flash_fwd_params) [with Kernel_traits=Flash_fwd_kernel_traits<224, 128, 64, 8, false, false, cutlass::half_t, Flash_kernel_traits<224, 128, 64, 8, cutlass::half_t>>, Is_dropout=true, Is_causal=false, Is_even_N=false, Is_even_K=false, Return_softmax=true]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(34): here
            instantiation of "void run_flash_fwd<Kernel_traits,Is_dropout,Is_causal>(Flash_fwd_params &, cudaStream_t) [with Kernel_traits=Flash_fwd_kernel_traits<224, 128, 64, 8, false, false, cutlass::half_t, Flash_kernel_traits<224, 128, 64, 8, cutlass::half_t>>, Is_dropout=true, Is_causal=false]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(208): here
            instantiation of "void run_mha_fwd_hdim224<T>(Flash_fwd_params &, cudaStream_t) [with T=cutlass::half_t]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_hdim224_fp16_sm80.cu(8): here

/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_kernel.h(140): warning: variable "MMA_M" was declared but never referenced
          detected during:
            instantiation of "void flash::compute_attn_1rowblock<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params,Is_Q_split_K>(const Params &, int, int, int) [with Kernel_traits=Flash_fwd_kernel_traits<224, 128, 64, 8, false, false, cutlass::half_t, Flash_kernel_traits<224, 128, 64, 8, cutlass::half_t>>, Is_dropout=true, Is_causal=false, Is_even_N=false, Is_even_K=false, Return_softmax=false, Params=Flash_fwd_params, Is_Q_split_K=false]" 
(589): here
            instantiation of "void flash::compute_attn<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &) [with Kernel_traits=Flash_fwd_kernel_traits<224, 128, 64, 8, false, false, cutlass::half_t, Flash_kernel_traits<224, 128, 64, 8, cutlass::half_t>>, Is_dropout=true, Is_causal=false, Is_even_N=false, Is_even_K=false, Return_softmax=false, Params=Flash_fwd_params]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(15): here
            instantiation of "void flash_fwd_kernel<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax>(Flash_fwd_params) [with Kernel_traits=Flash_fwd_kernel_traits<224, 128, 64, 8, false, false, cutlass::half_t, Flash_kernel_traits<224, 128, 64, 8, cutlass::half_t>>, Is_dropout=true, Is_causal=false, Is_even_N=false, Is_even_K=false, Return_softmax=false]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(34): here
            instantiation of "void run_flash_fwd<Kernel_traits,Is_dropout,Is_causal>(Flash_fwd_params &, cudaStream_t) [with Kernel_traits=Flash_fwd_kernel_traits<224, 128, 64, 8, false, false, cutlass::half_t, Flash_kernel_traits<224, 128, 64, 8, cutlass::half_t>>, Is_dropout=true, Is_causal=false]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(208): here
            instantiation of "void run_mha_fwd_hdim224<T>(Flash_fwd_params &, cudaStream_t) [with T=cutlass::half_t]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_hdim224_fp16_sm80.cu(8): here

/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_kernel.h(276): warning: variable "tQrQ" was declared but never referenced
          detected during:
            instantiation of "void flash::compute_attn_1rowblock<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params,Is_Q_split_K>(const Params &, int, int, int) [with Kernel_traits=Flash_fwd_kernel_traits<224, 128, 64, 8, false, false, cutlass::half_t, Flash_kernel_traits<224, 128, 64, 8, cutlass::half_t>>, Is_dropout=true, Is_causal=false, Is_even_N=false, Is_even_K=false, Return_softmax=false, Params=Flash_fwd_params, Is_Q_split_K=false]" 
(589): here
            instantiation of "void flash::compute_attn<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &) [with Kernel_traits=Flash_fwd_kernel_traits<224, 128, 64, 8, false, false, cutlass::half_t, Flash_kernel_traits<224, 128, 64, 8, cutlass::half_t>>, Is_dropout=true, Is_causal=false, Is_even_N=false, Is_even_K=false, Return_softmax=false, Params=Flash_fwd_params]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(15): here
            instantiation of "void flash_fwd_kernel<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax>(Flash_fwd_params) [with Kernel_traits=Flash_fwd_kernel_traits<224, 128, 64, 8, false, false, cutlass::half_t, Flash_kernel_traits<224, 128, 64, 8, cutlass::half_t>>, Is_dropout=true, Is_causal=false, Is_even_N=false, Is_even_K=false, Return_softmax=false]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(34): here
            instantiation of "void run_flash_fwd<Kernel_traits,Is_dropout,Is_causal>(Flash_fwd_params &, cudaStream_t) [with Kernel_traits=Flash_fwd_kernel_traits<224, 128, 64, 8, false, false, cutlass::half_t, Flash_kernel_traits<224, 128, 64, 8, cutlass::half_t>>, Is_dropout=true, Is_causal=false]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(208): here
            instantiation of "void run_mha_fwd_hdim224<T>(Flash_fwd_params &, cudaStream_t) [with T=cutlass::half_t]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_hdim224_fp16_sm80.cu(8): here

/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_kernel.h(140): warning: variable "MMA_M" was declared but never referenced
          detected during:
            instantiation of "void flash::compute_attn_1rowblock<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params,Is_Q_split_K>(const Params &, int, int, int) [with Kernel_traits=Flash_fwd_kernel_traits<224, 64, 64, 4, false, false, cutlass::half_t, Flash_kernel_traits<224, 64, 64, 4, cutlass::half_t>>, Is_dropout=true, Is_causal=false, Is_even_N=true, Is_even_K=true, Return_softmax=true, Params=Flash_fwd_params, Is_Q_split_K=false]" 
(589): here
            instantiation of "void flash::compute_attn<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &) [with Kernel_traits=Flash_fwd_kernel_traits<224, 64, 64, 4, false, false, cutlass::half_t, Flash_kernel_traits<224, 64, 64, 4, cutlass::half_t>>, Is_dropout=true, Is_causal=false, Is_even_N=true, Is_even_K=true, Return_softmax=true, Params=Flash_fwd_params]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(15): here
            instantiation of "void flash_fwd_kernel<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax>(Flash_fwd_params) [with Kernel_traits=Flash_fwd_kernel_traits<224, 64, 64, 4, false, false, cutlass::half_t, Flash_kernel_traits<224, 64, 64, 4, cutlass::half_t>>, Is_dropout=true, Is_causal=false, Is_even_N=true, Is_even_K=true, Return_softmax=true]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(34): here
            instantiation of "void run_flash_fwd<Kernel_traits,Is_dropout,Is_causal>(Flash_fwd_params &, cudaStream_t) [with Kernel_traits=Flash_fwd_kernel_traits<224, 64, 64, 4, false, false, cutlass::half_t, Flash_kernel_traits<224, 64, 64, 4, cutlass::half_t>>, Is_dropout=true, Is_causal=false]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(208): here
            instantiation of "void run_mha_fwd_hdim224<T>(Flash_fwd_params &, cudaStream_t) [with T=cutlass::half_t]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_hdim224_fp16_sm80.cu(8): here

/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_kernel.h(276): warning: variable "tQrQ" was declared but never referenced
          detected during:
            instantiation of "void flash::compute_attn_1rowblock<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params,Is_Q_split_K>(const Params &, int, int, int) [with Kernel_traits=Flash_fwd_kernel_traits<224, 64, 64, 4, false, false, cutlass::half_t, Flash_kernel_traits<224, 64, 64, 4, cutlass::half_t>>, Is_dropout=true, Is_causal=false, Is_even_N=true, Is_even_K=true, Return_softmax=true, Params=Flash_fwd_params, Is_Q_split_K=false]" 
(589): here
            instantiation of "void flash::compute_attn<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &) [with Kernel_traits=Flash_fwd_kernel_traits<224, 64, 64, 4, false, false, cutlass::half_t, Flash_kernel_traits<224, 64, 64, 4, cutlass::half_t>>, Is_dropout=true, Is_causal=false, Is_even_N=true, Is_even_K=true, Return_softmax=true, Params=Flash_fwd_params]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(15): here
            instantiation of "void flash_fwd_kernel<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax>(Flash_fwd_params) [with Kernel_traits=Flash_fwd_kernel_traits<224, 64, 64, 4, false, false, cutlass::half_t, Flash_kernel_traits<224, 64, 64, 4, cutlass::half_t>>, Is_dropout=true, Is_causal=false, Is_even_N=true, Is_even_K=true, Return_softmax=true]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(34): here
            instantiation of "void run_flash_fwd<Kernel_traits,Is_dropout,Is_causal>(Flash_fwd_params &, cudaStream_t) [with Kernel_traits=Flash_fwd_kernel_traits<224, 64, 64, 4, false, false, cutlass::half_t, Flash_kernel_traits<224, 64, 64, 4, cutlass::half_t>>, Is_dropout=true, Is_causal=false]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(208): here
            instantiation of "void run_mha_fwd_hdim224<T>(Flash_fwd_params &, cudaStream_t) [with T=cutlass::half_t]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_hdim224_fp16_sm80.cu(8): here

/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_kernel.h(140): warning: variable "MMA_M" was declared but never referenced
          detected during:
            instantiation of "void flash::compute_attn_1rowblock<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params,Is_Q_split_K>(const Params &, int, int, int) [with Kernel_traits=Flash_fwd_kernel_traits<224, 64, 64, 4, false, false, cutlass::half_t, Flash_kernel_traits<224, 64, 64, 4, cutlass::half_t>>, Is_dropout=true, Is_causal=false, Is_even_N=true, Is_even_K=true, Return_softmax=false, Params=Flash_fwd_params, Is_Q_split_K=false]" 
(589): here
            instantiation of "void flash::compute_attn<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &) [with Kernel_traits=Flash_fwd_kernel_traits<224, 64, 64, 4, false, false, cutlass::half_t, Flash_kernel_traits<224, 64, 64, 4, cutlass::half_t>>, Is_dropout=true, Is_causal=false, Is_even_N=true, Is_even_K=true, Return_softmax=false, Params=Flash_fwd_params]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(15): here
            instantiation of "void flash_fwd_kernel<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax>(Flash_fwd_params) [with Kernel_traits=Flash_fwd_kernel_traits<224, 64, 64, 4, false, false, cutlass::half_t, Flash_kernel_traits<224, 64, 64, 4, cutlass::half_t>>, Is_dropout=true, Is_causal=false, Is_even_N=true, Is_even_K=true, Return_softmax=false]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(34): here
            instantiation of "void run_flash_fwd<Kernel_traits,Is_dropout,Is_causal>(Flash_fwd_params &, cudaStream_t) [with Kernel_traits=Flash_fwd_kernel_traits<224, 64, 64, 4, false, false, cutlass::half_t, Flash_kernel_traits<224, 64, 64, 4, cutlass::half_t>>, Is_dropout=true, Is_causal=false]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(208): here
            instantiation of "void run_mha_fwd_hdim224<T>(Flash_fwd_params &, cudaStream_t) [with T=cutlass::half_t]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_hdim224_fp16_sm80.cu(8): here

/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_kernel.h(276): warning: variable "tQrQ" was declared but never referenced
          detected during:
            instantiation of "void flash::compute_attn_1rowblock<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params,Is_Q_split_K>(const Params &, int, int, int) [with Kernel_traits=Flash_fwd_kernel_traits<224, 64, 64, 4, false, false, cutlass::half_t, Flash_kernel_traits<224, 64, 64, 4, cutlass::half_t>>, Is_dropout=true, Is_causal=false, Is_even_N=true, Is_even_K=true, Return_softmax=false, Params=Flash_fwd_params, Is_Q_split_K=false]" 
(589): here
            instantiation of "void flash::compute_attn<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &) [with Kernel_traits=Flash_fwd_kernel_traits<224, 64, 64, 4, false, false, cutlass::half_t, Flash_kernel_traits<224, 64, 64, 4, cutlass::half_t>>, Is_dropout=true, Is_causal=false, Is_even_N=true, Is_even_K=true, Return_softmax=false, Params=Flash_fwd_params]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(15): here
            instantiation of "void flash_fwd_kernel<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax>(Flash_fwd_params) [with Kernel_traits=Flash_fwd_kernel_traits<224, 64, 64, 4, false, false, cutlass::half_t, Flash_kernel_traits<224, 64, 64, 4, cutlass::half_t>>, Is_dropout=true, Is_causal=false, Is_even_N=true, Is_even_K=true, Return_softmax=false]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(34): here
            instantiation of "void run_flash_fwd<Kernel_traits,Is_dropout,Is_causal>(Flash_fwd_params &, cudaStream_t) [with Kernel_traits=Flash_fwd_kernel_traits<224, 64, 64, 4, false, false, cutlass::half_t, Flash_kernel_traits<224, 64, 64, 4, cutlass::half_t>>, Is_dropout=true, Is_causal=false]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(208): here
            instantiation of "void run_mha_fwd_hdim224<T>(Flash_fwd_params &, cudaStream_t) [with T=cutlass::half_t]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_hdim224_fp16_sm80.cu(8): here

/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_kernel.h(140): warning: variable "MMA_M" was declared but never referenced
          detected during:
            instantiation of "void flash::compute_attn_1rowblock<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params,Is_Q_split_K>(const Params &, int, int, int) [with Kernel_traits=Flash_fwd_kernel_traits<224, 64, 64, 4, false, false, cutlass::half_t, Flash_kernel_traits<224, 64, 64, 4, cutlass::half_t>>, Is_dropout=true, Is_causal=false, Is_even_N=true, Is_even_K=false, Return_softmax=true, Params=Flash_fwd_params, Is_Q_split_K=false]" 
(589): here
            instantiation of "void flash::compute_attn<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &) [with Kernel_traits=Flash_fwd_kernel_traits<224, 64, 64, 4, false, false, cutlass::half_t, Flash_kernel_traits<224, 64, 64, 4, cutlass::half_t>>, Is_dropout=true, Is_causal=false, Is_even_N=true, Is_even_K=false, Return_softmax=true, Params=Flash_fwd_params]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(15): here
            instantiation of "void flash_fwd_kernel<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax>(Flash_fwd_params) [with Kernel_traits=Flash_fwd_kernel_traits<224, 64, 64, 4, false, false, cutlass::half_t, Flash_kernel_traits<224, 64, 64, 4, cutlass::half_t>>, Is_dropout=true, Is_causal=false, Is_even_N=true, Is_even_K=false, Return_softmax=true]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(34): here
            instantiation of "void run_flash_fwd<Kernel_traits,Is_dropout,Is_causal>(Flash_fwd_params &, cudaStream_t) [with Kernel_traits=Flash_fwd_kernel_traits<224, 64, 64, 4, false, false, cutlass::half_t, Flash_kernel_traits<224, 64, 64, 4, cutlass::half_t>>, Is_dropout=true, Is_causal=false]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(208): here
            instantiation of "void run_mha_fwd_hdim224<T>(Flash_fwd_params &, cudaStream_t) [with T=cutlass::half_t]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_hdim224_fp16_sm80.cu(8): here

/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_kernel.h(276): warning: variable "tQrQ" was declared but never referenced
          detected during:
            instantiation of "void flash::compute_attn_1rowblock<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params,Is_Q_split_K>(const Params &, int, int, int) [with Kernel_traits=Flash_fwd_kernel_traits<224, 64, 64, 4, false, false, cutlass::half_t, Flash_kernel_traits<224, 64, 64, 4, cutlass::half_t>>, Is_dropout=true, Is_causal=false, Is_even_N=true, Is_even_K=false, Return_softmax=true, Params=Flash_fwd_params, Is_Q_split_K=false]" 
(589): here
            instantiation of "void flash::compute_attn<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &) [with Kernel_traits=Flash_fwd_kernel_traits<224, 64, 64, 4, false, false, cutlass::half_t, Flash_kernel_traits<224, 64, 64, 4, cutlass::half_t>>, Is_dropout=true, Is_causal=false, Is_even_N=true, Is_even_K=false, Return_softmax=true, Params=Flash_fwd_params]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(15): here
            instantiation of "void flash_fwd_kernel<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax>(Flash_fwd_params) [with Kernel_traits=Flash_fwd_kernel_traits<224, 64, 64, 4, false, false, cutlass::half_t, Flash_kernel_traits<224, 64, 64, 4, cutlass::half_t>>, Is_dropout=true, Is_causal=false, Is_even_N=true, Is_even_K=false, Return_softmax=true]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(34): here
            instantiation of "void run_flash_fwd<Kernel_traits,Is_dropout,Is_causal>(Flash_fwd_params &, cudaStream_t) [with Kernel_traits=Flash_fwd_kernel_traits<224, 64, 64, 4, false, false, cutlass::half_t, Flash_kernel_traits<224, 64, 64, 4, cutlass::half_t>>, Is_dropout=true, Is_causal=false]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(208): here
            instantiation of "void run_mha_fwd_hdim224<T>(Flash_fwd_params &, cudaStream_t) [with T=cutlass::half_t]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_hdim224_fp16_sm80.cu(8): here

/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_kernel.h(140): warning: variable "MMA_M" was declared but never referenced
          detected during:
            instantiation of "void flash::compute_attn_1rowblock<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params,Is_Q_split_K>(const Params &, int, int, int) [with Kernel_traits=Flash_fwd_kernel_traits<224, 64, 64, 4, false, false, cutlass::half_t, Flash_kernel_traits<224, 64, 64, 4, cutlass::half_t>>, Is_dropout=true, Is_causal=false, Is_even_N=true, Is_even_K=false, Return_softmax=false, Params=Flash_fwd_params, Is_Q_split_K=false]" 
(589): here
            instantiation of "void flash::compute_attn<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &) [with Kernel_traits=Flash_fwd_kernel_traits<224, 64, 64, 4, false, false, cutlass::half_t, Flash_kernel_traits<224, 64, 64, 4, cutlass::half_t>>, Is_dropout=true, Is_causal=false, Is_even_N=true, Is_even_K=false, Return_softmax=false, Params=Flash_fwd_params]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(15): here
            instantiation of "void flash_fwd_kernel<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax>(Flash_fwd_params) [with Kernel_traits=Flash_fwd_kernel_traits<224, 64, 64, 4, false, false, cutlass::half_t, Flash_kernel_traits<224, 64, 64, 4, cutlass::half_t>>, Is_dropout=true, Is_causal=false, Is_even_N=true, Is_even_K=false, Return_softmax=false]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(34): here
            instantiation of "void run_flash_fwd<Kernel_traits,Is_dropout,Is_causal>(Flash_fwd_params &, cudaStream_t) [with Kernel_traits=Flash_fwd_kernel_traits<224, 64, 64, 4, false, false, cutlass::half_t, Flash_kernel_traits<224, 64, 64, 4, cutlass::half_t>>, Is_dropout=true, Is_causal=false]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(208): here
            instantiation of "void run_mha_fwd_hdim224<T>(Flash_fwd_params &, cudaStream_t) [with T=cutlass::half_t]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_hdim224_fp16_sm80.cu(8): here

/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_kernel.h(276): warning: variable "tQrQ" was declared but never referenced
          detected during:
            instantiation of "void flash::compute_attn_1rowblock<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params,Is_Q_split_K>(const Params &, int, int, int) [with Kernel_traits=Flash_fwd_kernel_traits<224, 64, 64, 4, false, false, cutlass::half_t, Flash_kernel_traits<224, 64, 64, 4, cutlass::half_t>>, Is_dropout=true, Is_causal=false, Is_even_N=true, Is_even_K=false, Return_softmax=false, Params=Flash_fwd_params, Is_Q_split_K=false]" 
(589): here
            instantiation of "void flash::compute_attn<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &) [with Kernel_traits=Flash_fwd_kernel_traits<224, 64, 64, 4, false, false, cutlass::half_t, Flash_kernel_traits<224, 64, 64, 4, cutlass::half_t>>, Is_dropout=true, Is_causal=false, Is_even_N=true, Is_even_K=false, Return_softmax=false, Params=Flash_fwd_params]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(15): here
            instantiation of "void flash_fwd_kernel<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax>(Flash_fwd_params) [with Kernel_traits=Flash_fwd_kernel_traits<224, 64, 64, 4, false, false, cutlass::half_t, Flash_kernel_traits<224, 64, 64, 4, cutlass::half_t>>, Is_dropout=true, Is_causal=false, Is_even_N=true, Is_even_K=false, Return_softmax=false]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(34): here
            instantiation of "void run_flash_fwd<Kernel_traits,Is_dropout,Is_causal>(Flash_fwd_params &, cudaStream_t) [with Kernel_traits=Flash_fwd_kernel_traits<224, 64, 64, 4, false, false, cutlass::half_t, Flash_kernel_traits<224, 64, 64, 4, cutlass::half_t>>, Is_dropout=true, Is_causal=false]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(208): here
            instantiation of "void run_mha_fwd_hdim224<T>(Flash_fwd_params &, cudaStream_t) [with T=cutlass::half_t]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_hdim224_fp16_sm80.cu(8): here

/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_kernel.h(140): warning: variable "MMA_M" was declared but never referenced
          detected during:
            instantiation of "void flash::compute_attn_1rowblock<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params,Is_Q_split_K>(const Params &, int, int, int) [with Kernel_traits=Flash_fwd_kernel_traits<224, 64, 64, 4, false, false, cutlass::half_t, Flash_kernel_traits<224, 64, 64, 4, cutlass::half_t>>, Is_dropout=true, Is_causal=false, Is_even_N=false, Is_even_K=true, Return_softmax=true, Params=Flash_fwd_params, Is_Q_split_K=false]" 
(589): here
            instantiation of "void flash::compute_attn<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &) [with Kernel_traits=Flash_fwd_kernel_traits<224, 64, 64, 4, false, false, cutlass::half_t, Flash_kernel_traits<224, 64, 64, 4, cutlass::half_t>>, Is_dropout=true, Is_causal=false, Is_even_N=false, Is_even_K=true, Return_softmax=true, Params=Flash_fwd_params]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(15): here
            instantiation of "void flash_fwd_kernel<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax>(Flash_fwd_params) [with Kernel_traits=Flash_fwd_kernel_traits<224, 64, 64, 4, false, false, cutlass::half_t, Flash_kernel_traits<224, 64, 64, 4, cutlass::half_t>>, Is_dropout=true, Is_causal=false, Is_even_N=false, Is_even_K=true, Return_softmax=true]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(34): here
            instantiation of "void run_flash_fwd<Kernel_traits,Is_dropout,Is_causal>(Flash_fwd_params &, cudaStream_t) [with Kernel_traits=Flash_fwd_kernel_traits<224, 64, 64, 4, false, false, cutlass::half_t, Flash_kernel_traits<224, 64, 64, 4, cutlass::half_t>>, Is_dropout=true, Is_causal=false]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(208): here
            instantiation of "void run_mha_fwd_hdim224<T>(Flash_fwd_params &, cudaStream_t) [with T=cutlass::half_t]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_hdim224_fp16_sm80.cu(8): here

/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_kernel.h(276): warning: variable "tQrQ" was declared but never referenced
          detected during:
            instantiation of "void flash::compute_attn_1rowblock<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params,Is_Q_split_K>(const Params &, int, int, int) [with Kernel_traits=Flash_fwd_kernel_traits<224, 64, 64, 4, false, false, cutlass::half_t, Flash_kernel_traits<224, 64, 64, 4, cutlass::half_t>>, Is_dropout=true, Is_causal=false, Is_even_N=false, Is_even_K=true, Return_softmax=true, Params=Flash_fwd_params, Is_Q_split_K=false]" 
(589): here
            instantiation of "void flash::compute_attn<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &) [with Kernel_traits=Flash_fwd_kernel_traits<224, 64, 64, 4, false, false, cutlass::half_t, Flash_kernel_traits<224, 64, 64, 4, cutlass::half_t>>, Is_dropout=true, Is_causal=false, Is_even_N=false, Is_even_K=true, Return_softmax=true, Params=Flash_fwd_params]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(15): here
            instantiation of "void flash_fwd_kernel<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax>(Flash_fwd_params) [with Kernel_traits=Flash_fwd_kernel_traits<224, 64, 64, 4, false, false, cutlass::half_t, Flash_kernel_traits<224, 64, 64, 4, cutlass::half_t>>, Is_dropout=true, Is_causal=false, Is_even_N=false, Is_even_K=true, Return_softmax=true]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(34): here
            instantiation of "void run_flash_fwd<Kernel_traits,Is_dropout,Is_causal>(Flash_fwd_params &, cudaStream_t) [with Kernel_traits=Flash_fwd_kernel_traits<224, 64, 64, 4, false, false, cutlass::half_t, Flash_kernel_traits<224, 64, 64, 4, cutlass::half_t>>, Is_dropout=true, Is_causal=false]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(208): here
            instantiation of "void run_mha_fwd_hdim224<T>(Flash_fwd_params &, cudaStream_t) [with T=cutlass::half_t]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_hdim224_fp16_sm80.cu(8): here

/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_kernel.h(140): warning: variable "MMA_M" was declared but never referenced
          detected during:
            instantiation of "void flash::compute_attn_1rowblock<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params,Is_Q_split_K>(const Params &, int, int, int) [with Kernel_traits=Flash_fwd_kernel_traits<224, 64, 64, 4, false, false, cutlass::half_t, Flash_kernel_traits<224, 64, 64, 4, cutlass::half_t>>, Is_dropout=true, Is_causal=false, Is_even_N=false, Is_even_K=true, Return_softmax=false, Params=Flash_fwd_params, Is_Q_split_K=false]" 
(589): here
            instantiation of "void flash::compute_attn<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &) [with Kernel_traits=Flash_fwd_kernel_traits<224, 64, 64, 4, false, false, cutlass::half_t, Flash_kernel_traits<224, 64, 64, 4, cutlass::half_t>>, Is_dropout=true, Is_causal=false, Is_even_N=false, Is_even_K=true, Return_softmax=false, Params=Flash_fwd_params]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(15): here
            instantiation of "void flash_fwd_kernel<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax>(Flash_fwd_params) [with Kernel_traits=Flash_fwd_kernel_traits<224, 64, 64, 4, false, false, cutlass::half_t, Flash_kernel_traits<224, 64, 64, 4, cutlass::half_t>>, Is_dropout=true, Is_causal=false, Is_even_N=false, Is_even_K=true, Return_softmax=false]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(34): here
            instantiation of "void run_flash_fwd<Kernel_traits,Is_dropout,Is_causal>(Flash_fwd_params &, cudaStream_t) [with Kernel_traits=Flash_fwd_kernel_traits<224, 64, 64, 4, false, false, cutlass::half_t, Flash_kernel_traits<224, 64, 64, 4, cutlass::half_t>>, Is_dropout=true, Is_causal=false]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(208): here
            instantiation of "void run_mha_fwd_hdim224<T>(Flash_fwd_params &, cudaStream_t) [with T=cutlass::half_t]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_hdim224_fp16_sm80.cu(8): here

/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_kernel.h(276): warning: variable "tQrQ" was declared but never referenced
          detected during:
            instantiation of "void flash::compute_attn_1rowblock<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params,Is_Q_split_K>(const Params &, int, int, int) [with Kernel_traits=Flash_fwd_kernel_traits<224, 64, 64, 4, false, false, cutlass::half_t, Flash_kernel_traits<224, 64, 64, 4, cutlass::half_t>>, Is_dropout=true, Is_causal=false, Is_even_N=false, Is_even_K=true, Return_softmax=false, Params=Flash_fwd_params, Is_Q_split_K=false]" 
(589): here
            instantiation of "void flash::compute_attn<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &) [with Kernel_traits=Flash_fwd_kernel_traits<224, 64, 64, 4, false, false, cutlass::half_t, Flash_kernel_traits<224, 64, 64, 4, cutlass::half_t>>, Is_dropout=true, Is_causal=false, Is_even_N=false, Is_even_K=true, Return_softmax=false, Params=Flash_fwd_params]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(15): here
            instantiation of "void flash_fwd_kernel<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax>(Flash_fwd_params) [with Kernel_traits=Flash_fwd_kernel_traits<224, 64, 64, 4, false, false, cutlass::half_t, Flash_kernel_traits<224, 64, 64, 4, cutlass::half_t>>, Is_dropout=true, Is_causal=false, Is_even_N=false, Is_even_K=true, Return_softmax=false]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(34): here
            instantiation of "void run_flash_fwd<Kernel_traits,Is_dropout,Is_causal>(Flash_fwd_params &, cudaStream_t) [with Kernel_traits=Flash_fwd_kernel_traits<224, 64, 64, 4, false, false, cutlass::half_t, Flash_kernel_traits<224, 64, 64, 4, cutlass::half_t>>, Is_dropout=true, Is_causal=false]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(208): here
            instantiation of "void run_mha_fwd_hdim224<T>(Flash_fwd_params &, cudaStream_t) [with T=cutlass::half_t]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_hdim224_fp16_sm80.cu(8): here

/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_kernel.h(140): warning: variable "MMA_M" was declared but never referenced
          detected during:
            instantiation of "void flash::compute_attn_1rowblock<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params,Is_Q_split_K>(const Params &, int, int, int) [with Kernel_traits=Flash_fwd_kernel_traits<224, 64, 64, 4, false, false, cutlass::half_t, Flash_kernel_traits<224, 64, 64, 4, cutlass::half_t>>, Is_dropout=true, Is_causal=false, Is_even_N=false, Is_even_K=false, Return_softmax=true, Params=Flash_fwd_params, Is_Q_split_K=false]" 
(589): here
            instantiation of "void flash::compute_attn<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &) [with Kernel_traits=Flash_fwd_kernel_traits<224, 64, 64, 4, false, false, cutlass::half_t, Flash_kernel_traits<224, 64, 64, 4, cutlass::half_t>>, Is_dropout=true, Is_causal=false, Is_even_N=false, Is_even_K=false, Return_softmax=true, Params=Flash_fwd_params]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(15): here
            instantiation of "void flash_fwd_kernel<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax>(Flash_fwd_params) [with Kernel_traits=Flash_fwd_kernel_traits<224, 64, 64, 4, false, false, cutlass::half_t, Flash_kernel_traits<224, 64, 64, 4, cutlass::half_t>>, Is_dropout=true, Is_causal=false, Is_even_N=false, Is_even_K=false, Return_softmax=true]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(34): here
            instantiation of "void run_flash_fwd<Kernel_traits,Is_dropout,Is_causal>(Flash_fwd_params &, cudaStream_t) [with Kernel_traits=Flash_fwd_kernel_traits<224, 64, 64, 4, false, false, cutlass::half_t, Flash_kernel_traits<224, 64, 64, 4, cutlass::half_t>>, Is_dropout=true, Is_causal=false]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(208): here
            instantiation of "void run_mha_fwd_hdim224<T>(Flash_fwd_params &, cudaStream_t) [with T=cutlass::half_t]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_hdim224_fp16_sm80.cu(8): here

/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_kernel.h(276): warning: variable "tQrQ" was declared but never referenced
          detected during:
            instantiation of "void flash::compute_attn_1rowblock<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params,Is_Q_split_K>(const Params &, int, int, int) [with Kernel_traits=Flash_fwd_kernel_traits<224, 64, 64, 4, false, false, cutlass::half_t, Flash_kernel_traits<224, 64, 64, 4, cutlass::half_t>>, Is_dropout=true, Is_causal=false, Is_even_N=false, Is_even_K=false, Return_softmax=true, Params=Flash_fwd_params, Is_Q_split_K=false]" 
(589): here
            instantiation of "void flash::compute_attn<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &) [with Kernel_traits=Flash_fwd_kernel_traits<224, 64, 64, 4, false, false, cutlass::half_t, Flash_kernel_traits<224, 64, 64, 4, cutlass::half_t>>, Is_dropout=true, Is_causal=false, Is_even_N=false, Is_even_K=false, Return_softmax=true, Params=Flash_fwd_params]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(15): here
            instantiation of "void flash_fwd_kernel<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax>(Flash_fwd_params) [with Kernel_traits=Flash_fwd_kernel_traits<224, 64, 64, 4, false, false, cutlass::half_t, Flash_kernel_traits<224, 64, 64, 4, cutlass::half_t>>, Is_dropout=true, Is_causal=false, Is_even_N=false, Is_even_K=false, Return_softmax=true]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(34): here
            instantiation of "void run_flash_fwd<Kernel_traits,Is_dropout,Is_causal>(Flash_fwd_params &, cudaStream_t) [with Kernel_traits=Flash_fwd_kernel_traits<224, 64, 64, 4, false, false, cutlass::half_t, Flash_kernel_traits<224, 64, 64, 4, cutlass::half_t>>, Is_dropout=true, Is_causal=false]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(208): here
            instantiation of "void run_mha_fwd_hdim224<T>(Flash_fwd_params &, cudaStream_t) [with T=cutlass::half_t]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_hdim224_fp16_sm80.cu(8): here

/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_kernel.h(140): warning: variable "MMA_M" was declared but never referenced
          detected during:
            instantiation of "void flash::compute_attn_1rowblock<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params,Is_Q_split_K>(const Params &, int, int, int) [with Kernel_traits=Flash_fwd_kernel_traits<224, 64, 64, 4, false, false, cutlass::half_t, Flash_kernel_traits<224, 64, 64, 4, cutlass::half_t>>, Is_dropout=true, Is_causal=false, Is_even_N=false, Is_even_K=false, Return_softmax=false, Params=Flash_fwd_params, Is_Q_split_K=false]" 
(589): here
            instantiation of "void flash::compute_attn<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &) [with Kernel_traits=Flash_fwd_kernel_traits<224, 64, 64, 4, false, false, cutlass::half_t, Flash_kernel_traits<224, 64, 64, 4, cutlass::half_t>>, Is_dropout=true, Is_causal=false, Is_even_N=false, Is_even_K=false, Return_softmax=false, Params=Flash_fwd_params]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(15): here
            instantiation of "void flash_fwd_kernel<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax>(Flash_fwd_params) [with Kernel_traits=Flash_fwd_kernel_traits<224, 64, 64, 4, false, false, cutlass::half_t, Flash_kernel_traits<224, 64, 64, 4, cutlass::half_t>>, Is_dropout=true, Is_causal=false, Is_even_N=false, Is_even_K=false, Return_softmax=false]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(34): here
            instantiation of "void run_flash_fwd<Kernel_traits,Is_dropout,Is_causal>(Flash_fwd_params &, cudaStream_t) [with Kernel_traits=Flash_fwd_kernel_traits<224, 64, 64, 4, false, false, cutlass::half_t, Flash_kernel_traits<224, 64, 64, 4, cutlass::half_t>>, Is_dropout=true, Is_causal=false]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(208): here
            instantiation of "void run_mha_fwd_hdim224<T>(Flash_fwd_params &, cudaStream_t) [with T=cutlass::half_t]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_hdim224_fp16_sm80.cu(8): here

/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_kernel.h(276): warning: variable "tQrQ" was declared but never referenced
          detected during:
            instantiation of "void flash::compute_attn_1rowblock<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params,Is_Q_split_K>(const Params &, int, int, int) [with Kernel_traits=Flash_fwd_kernel_traits<224, 64, 64, 4, false, false, cutlass::half_t, Flash_kernel_traits<224, 64, 64, 4, cutlass::half_t>>, Is_dropout=true, Is_causal=false, Is_even_N=false, Is_even_K=false, Return_softmax=false, Params=Flash_fwd_params, Is_Q_split_K=false]" 
(589): here
            instantiation of "void flash::compute_attn<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &) [with Kernel_traits=Flash_fwd_kernel_traits<224, 64, 64, 4, false, false, cutlass::half_t, Flash_kernel_traits<224, 64, 64, 4, cutlass::half_t>>, Is_dropout=true, Is_causal=false, Is_even_N=false, Is_even_K=false, Return_softmax=false, Params=Flash_fwd_params]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(15): here
            instantiation of "void flash_fwd_kernel<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax>(Flash_fwd_params) [with Kernel_traits=Flash_fwd_kernel_traits<224, 64, 64, 4, false, false, cutlass::half_t, Flash_kernel_traits<224, 64, 64, 4, cutlass::half_t>>, Is_dropout=true, Is_causal=false, Is_even_N=false, Is_even_K=false, Return_softmax=false]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(34): here
            instantiation of "void run_flash_fwd<Kernel_traits,Is_dropout,Is_causal>(Flash_fwd_params &, cudaStream_t) [with Kernel_traits=Flash_fwd_kernel_traits<224, 64, 64, 4, false, false, cutlass::half_t, Flash_kernel_traits<224, 64, 64, 4, cutlass::half_t>>, Is_dropout=true, Is_causal=false]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(208): here
            instantiation of "void run_mha_fwd_hdim224<T>(Flash_fwd_params &, cudaStream_t) [with T=cutlass::half_t]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_hdim224_fp16_sm80.cu(8): here

/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_kernel.h(140): warning: variable "MMA_M" was declared but never referenced
          detected during:
            instantiation of "void flash::compute_attn_1rowblock<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params,Is_Q_split_K>(const Params &, int, int, int) [with Kernel_traits=Flash_fwd_kernel_traits<224, 128, 64, 8, false, false, cutlass::half_t, Flash_kernel_traits<224, 128, 64, 8, cutlass::half_t>>, Is_dropout=false, Is_causal=true, Is_even_N=true, Is_even_K=true, Return_softmax=false, Params=Flash_fwd_params, Is_Q_split_K=false]" 
(589): here
            instantiation of "void flash::compute_attn<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &) [with Kernel_traits=Flash_fwd_kernel_traits<224, 128, 64, 8, false, false, cutlass::half_t, Flash_kernel_traits<224, 128, 64, 8, cutlass::half_t>>, Is_dropout=false, Is_causal=true, Is_even_N=true, Is_even_K=true, Return_softmax=false, Params=Flash_fwd_params]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(15): here
            instantiation of "void flash_fwd_kernel<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax>(Flash_fwd_params) [with Kernel_traits=Flash_fwd_kernel_traits<224, 128, 64, 8, false, false, cutlass::half_t, Flash_kernel_traits<224, 128, 64, 8, cutlass::half_t>>, Is_dropout=false, Is_causal=true, Is_even_N=true, Is_even_K=true, Return_softmax=false]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(34): here
            instantiation of "void run_flash_fwd<Kernel_traits,Is_dropout,Is_causal>(Flash_fwd_params &, cudaStream_t) [with Kernel_traits=Flash_fwd_kernel_traits<224, 128, 64, 8, false, false, cutlass::half_t, Flash_kernel_traits<224, 128, 64, 8, cutlass::half_t>>, Is_dropout=false, Is_causal=true]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(208): here
            instantiation of "void run_mha_fwd_hdim224<T>(Flash_fwd_params &, cudaStream_t) [with T=cutlass::half_t]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_hdim224_fp16_sm80.cu(8): here

/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_kernel.h(276): warning: variable "tQrQ" was declared but never referenced
          detected during:
            instantiation of "void flash::compute_attn_1rowblock<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params,Is_Q_split_K>(const Params &, int, int, int) [with Kernel_traits=Flash_fwd_kernel_traits<224, 128, 64, 8, false, false, cutlass::half_t, Flash_kernel_traits<224, 128, 64, 8, cutlass::half_t>>, Is_dropout=false, Is_causal=true, Is_even_N=true, Is_even_K=true, Return_softmax=false, Params=Flash_fwd_params, Is_Q_split_K=false]" 
(589): here
            instantiation of "void flash::compute_attn<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &) [with Kernel_traits=Flash_fwd_kernel_traits<224, 128, 64, 8, false, false, cutlass::half_t, Flash_kernel_traits<224, 128, 64, 8, cutlass::half_t>>, Is_dropout=false, Is_causal=true, Is_even_N=true, Is_even_K=true, Return_softmax=false, Params=Flash_fwd_params]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(15): here
            instantiation of "void flash_fwd_kernel<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax>(Flash_fwd_params) [with Kernel_traits=Flash_fwd_kernel_traits<224, 128, 64, 8, false, false, cutlass::half_t, Flash_kernel_traits<224, 128, 64, 8, cutlass::half_t>>, Is_dropout=false, Is_causal=true, Is_even_N=true, Is_even_K=true, Return_softmax=false]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(34): here
            instantiation of "void run_flash_fwd<Kernel_traits,Is_dropout,Is_causal>(Flash_fwd_params &, cudaStream_t) [with Kernel_traits=Flash_fwd_kernel_traits<224, 128, 64, 8, false, false, cutlass::half_t, Flash_kernel_traits<224, 128, 64, 8, cutlass::half_t>>, Is_dropout=false, Is_causal=true]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(208): here
            instantiation of "void run_mha_fwd_hdim224<T>(Flash_fwd_params &, cudaStream_t) [with T=cutlass::half_t]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_hdim224_fp16_sm80.cu(8): here

/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_kernel.h(140): warning: variable "MMA_M" was declared but never referenced
          detected during:
            instantiation of "void flash::compute_attn_1rowblock<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params,Is_Q_split_K>(const Params &, int, int, int) [with Kernel_traits=Flash_fwd_kernel_traits<224, 128, 64, 8, false, false, cutlass::half_t, Flash_kernel_traits<224, 128, 64, 8, cutlass::half_t>>, Is_dropout=false, Is_causal=true, Is_even_N=true, Is_even_K=false, Return_softmax=false, Params=Flash_fwd_params, Is_Q_split_K=false]" 
(589): here
            instantiation of "void flash::compute_attn<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &) [with Kernel_traits=Flash_fwd_kernel_traits<224, 128, 64, 8, false, false, cutlass::half_t, Flash_kernel_traits<224, 128, 64, 8, cutlass::half_t>>, Is_dropout=false, Is_causal=true, Is_even_N=true, Is_even_K=false, Return_softmax=false, Params=Flash_fwd_params]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(15): here
            instantiation of "void flash_fwd_kernel<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax>(Flash_fwd_params) [with Kernel_traits=Flash_fwd_kernel_traits<224, 128, 64, 8, false, false, cutlass::half_t, Flash_kernel_traits<224, 128, 64, 8, cutlass::half_t>>, Is_dropout=false, Is_causal=true, Is_even_N=true, Is_even_K=false, Return_softmax=false]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(34): here
            instantiation of "void run_flash_fwd<Kernel_traits,Is_dropout,Is_causal>(Flash_fwd_params &, cudaStream_t) [with Kernel_traits=Flash_fwd_kernel_traits<224, 128, 64, 8, false, false, cutlass::half_t, Flash_kernel_traits<224, 128, 64, 8, cutlass::half_t>>, Is_dropout=false, Is_causal=true]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(208): here
            instantiation of "void run_mha_fwd_hdim224<T>(Flash_fwd_params &, cudaStream_t) [with T=cutlass::half_t]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_hdim224_fp16_sm80.cu(8): here

/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_kernel.h(276): warning: variable "tQrQ" was declared but never referenced
          detected during:
            instantiation of "void flash::compute_attn_1rowblock<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params,Is_Q_split_K>(const Params &, int, int, int) [with Kernel_traits=Flash_fwd_kernel_traits<224, 128, 64, 8, false, false, cutlass::half_t, Flash_kernel_traits<224, 128, 64, 8, cutlass::half_t>>, Is_dropout=false, Is_causal=true, Is_even_N=true, Is_even_K=false, Return_softmax=false, Params=Flash_fwd_params, Is_Q_split_K=false]" 
(589): here
            instantiation of "void flash::compute_attn<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &) [with Kernel_traits=Flash_fwd_kernel_traits<224, 128, 64, 8, false, false, cutlass::half_t, Flash_kernel_traits<224, 128, 64, 8, cutlass::half_t>>, Is_dropout=false, Is_causal=true, Is_even_N=true, Is_even_K=false, Return_softmax=false, Params=Flash_fwd_params]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(15): here
            instantiation of "void flash_fwd_kernel<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax>(Flash_fwd_params) [with Kernel_traits=Flash_fwd_kernel_traits<224, 128, 64, 8, false, false, cutlass::half_t, Flash_kernel_traits<224, 128, 64, 8, cutlass::half_t>>, Is_dropout=false, Is_causal=true, Is_even_N=true, Is_even_K=false, Return_softmax=false]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(34): here
            instantiation of "void run_flash_fwd<Kernel_traits,Is_dropout,Is_causal>(Flash_fwd_params &, cudaStream_t) [with Kernel_traits=Flash_fwd_kernel_traits<224, 128, 64, 8, false, false, cutlass::half_t, Flash_kernel_traits<224, 128, 64, 8, cutlass::half_t>>, Is_dropout=false, Is_causal=true]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(208): here
            instantiation of "void run_mha_fwd_hdim224<T>(Flash_fwd_params &, cudaStream_t) [with T=cutlass::half_t]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_hdim224_fp16_sm80.cu(8): here

/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_kernel.h(140): warning: variable "MMA_M" was declared but never referenced
          detected during:
            instantiation of "void flash::compute_attn_1rowblock<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params,Is_Q_split_K>(const Params &, int, int, int) [with Kernel_traits=Flash_fwd_kernel_traits<224, 128, 64, 8, false, false, cutlass::half_t, Flash_kernel_traits<224, 128, 64, 8, cutlass::half_t>>, Is_dropout=false, Is_causal=true, Is_even_N=false, Is_even_K=true, Return_softmax=false, Params=Flash_fwd_params, Is_Q_split_K=false]" 
(589): here
            instantiation of "void flash::compute_attn<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &) [with Kernel_traits=Flash_fwd_kernel_traits<224, 128, 64, 8, false, false, cutlass::half_t, Flash_kernel_traits<224, 128, 64, 8, cutlass::half_t>>, Is_dropout=false, Is_causal=true, Is_even_N=false, Is_even_K=true, Return_softmax=false, Params=Flash_fwd_params]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(15): here
            instantiation of "void flash_fwd_kernel<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax>(Flash_fwd_params) [with Kernel_traits=Flash_fwd_kernel_traits<224, 128, 64, 8, false, false, cutlass::half_t, Flash_kernel_traits<224, 128, 64, 8, cutlass::half_t>>, Is_dropout=false, Is_causal=true, Is_even_N=false, Is_even_K=true, Return_softmax=false]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(34): here
            instantiation of "void run_flash_fwd<Kernel_traits,Is_dropout,Is_causal>(Flash_fwd_params &, cudaStream_t) [with Kernel_traits=Flash_fwd_kernel_traits<224, 128, 64, 8, false, false, cutlass::half_t, Flash_kernel_traits<224, 128, 64, 8, cutlass::half_t>>, Is_dropout=false, Is_causal=true]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(208): here
            instantiation of "void run_mha_fwd_hdim224<T>(Flash_fwd_params &, cudaStream_t) [with T=cutlass::half_t]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_hdim224_fp16_sm80.cu(8): here

/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_kernel.h(276): warning: variable "tQrQ" was declared but never referenced
          detected during:
            instantiation of "void flash::compute_attn_1rowblock<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params,Is_Q_split_K>(const Params &, int, int, int) [with Kernel_traits=Flash_fwd_kernel_traits<224, 128, 64, 8, false, false, cutlass::half_t, Flash_kernel_traits<224, 128, 64, 8, cutlass::half_t>>, Is_dropout=false, Is_causal=true, Is_even_N=false, Is_even_K=true, Return_softmax=false, Params=Flash_fwd_params, Is_Q_split_K=false]" 
(589): here
            instantiation of "void flash::compute_attn<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &) [with Kernel_traits=Flash_fwd_kernel_traits<224, 128, 64, 8, false, false, cutlass::half_t, Flash_kernel_traits<224, 128, 64, 8, cutlass::half_t>>, Is_dropout=false, Is_causal=true, Is_even_N=false, Is_even_K=true, Return_softmax=false, Params=Flash_fwd_params]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(15): here
            instantiation of "void flash_fwd_kernel<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax>(Flash_fwd_params) [with Kernel_traits=Flash_fwd_kernel_traits<224, 128, 64, 8, false, false, cutlass::half_t, Flash_kernel_traits<224, 128, 64, 8, cutlass::half_t>>, Is_dropout=false, Is_causal=true, Is_even_N=false, Is_even_K=true, Return_softmax=false]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(34): here
            instantiation of "void run_flash_fwd<Kernel_traits,Is_dropout,Is_causal>(Flash_fwd_params &, cudaStream_t) [with Kernel_traits=Flash_fwd_kernel_traits<224, 128, 64, 8, false, false, cutlass::half_t, Flash_kernel_traits<224, 128, 64, 8, cutlass::half_t>>, Is_dropout=false, Is_causal=true]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(208): here
            instantiation of "void run_mha_fwd_hdim224<T>(Flash_fwd_params &, cudaStream_t) [with T=cutlass::half_t]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_hdim224_fp16_sm80.cu(8): here

/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_kernel.h(140): warning: variable "MMA_M" was declared but never referenced
          detected during:
            instantiation of "void flash::compute_attn_1rowblock<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params,Is_Q_split_K>(const Params &, int, int, int) [with Kernel_traits=Flash_fwd_kernel_traits<224, 128, 64, 8, false, false, cutlass::half_t, Flash_kernel_traits<224, 128, 64, 8, cutlass::half_t>>, Is_dropout=false, Is_causal=true, Is_even_N=false, Is_even_K=false, Return_softmax=false, Params=Flash_fwd_params, Is_Q_split_K=false]" 
(589): here
            instantiation of "void flash::compute_attn<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &) [with Kernel_traits=Flash_fwd_kernel_traits<224, 128, 64, 8, false, false, cutlass::half_t, Flash_kernel_traits<224, 128, 64, 8, cutlass::half_t>>, Is_dropout=false, Is_causal=true, Is_even_N=false, Is_even_K=false, Return_softmax=false, Params=Flash_fwd_params]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(15): here
            instantiation of "void flash_fwd_kernel<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax>(Flash_fwd_params) [with Kernel_traits=Flash_fwd_kernel_traits<224, 128, 64, 8, false, false, cutlass::half_t, Flash_kernel_traits<224, 128, 64, 8, cutlass::half_t>>, Is_dropout=false, Is_causal=true, Is_even_N=false, Is_even_K=false, Return_softmax=false]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(34): here
            instantiation of "void run_flash_fwd<Kernel_traits,Is_dropout,Is_causal>(Flash_fwd_params &, cudaStream_t) [with Kernel_traits=Flash_fwd_kernel_traits<224, 128, 64, 8, false, false, cutlass::half_t, Flash_kernel_traits<224, 128, 64, 8, cutlass::half_t>>, Is_dropout=false, Is_causal=true]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(208): here
            instantiation of "void run_mha_fwd_hdim224<T>(Flash_fwd_params &, cudaStream_t) [with T=cutlass::half_t]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_hdim224_fp16_sm80.cu(8): here

/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_kernel.h(276): warning: variable "tQrQ" was declared but never referenced
          detected during:
            instantiation of "void flash::compute_attn_1rowblock<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params,Is_Q_split_K>(const Params &, int, int, int) [with Kernel_traits=Flash_fwd_kernel_traits<224, 128, 64, 8, false, false, cutlass::half_t, Flash_kernel_traits<224, 128, 64, 8, cutlass::half_t>>, Is_dropout=false, Is_causal=true, Is_even_N=false, Is_even_K=false, Return_softmax=false, Params=Flash_fwd_params, Is_Q_split_K=false]" 
(589): here
            instantiation of "void flash::compute_attn<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &) [with Kernel_traits=Flash_fwd_kernel_traits<224, 128, 64, 8, false, false, cutlass::half_t, Flash_kernel_traits<224, 128, 64, 8, cutlass::half_t>>, Is_dropout=false, Is_causal=true, Is_even_N=false, Is_even_K=false, Return_softmax=false, Params=Flash_fwd_params]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(15): here
            instantiation of "void flash_fwd_kernel<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax>(Flash_fwd_params) [with Kernel_traits=Flash_fwd_kernel_traits<224, 128, 64, 8, false, false, cutlass::half_t, Flash_kernel_traits<224, 128, 64, 8, cutlass::half_t>>, Is_dropout=false, Is_causal=true, Is_even_N=false, Is_even_K=false, Return_softmax=false]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(34): here
            instantiation of "void run_flash_fwd<Kernel_traits,Is_dropout,Is_causal>(Flash_fwd_params &, cudaStream_t) [with Kernel_traits=Flash_fwd_kernel_traits<224, 128, 64, 8, false, false, cutlass::half_t, Flash_kernel_traits<224, 128, 64, 8, cutlass::half_t>>, Is_dropout=false, Is_causal=true]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(208): here
            instantiation of "void run_mha_fwd_hdim224<T>(Flash_fwd_params &, cudaStream_t) [with T=cutlass::half_t]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_hdim224_fp16_sm80.cu(8): here

/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_kernel.h(140): warning: variable "MMA_M" was declared but never referenced
          detected during:
            instantiation of "void flash::compute_attn_1rowblock<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params,Is_Q_split_K>(const Params &, int, int, int) [with Kernel_traits=Flash_fwd_kernel_traits<224, 64, 64, 4, false, false, cutlass::half_t, Flash_kernel_traits<224, 64, 64, 4, cutlass::half_t>>, Is_dropout=false, Is_causal=true, Is_even_N=true, Is_even_K=true, Return_softmax=false, Params=Flash_fwd_params, Is_Q_split_K=false]" 
(589): here
            instantiation of "void flash::compute_attn<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &) [with Kernel_traits=Flash_fwd_kernel_traits<224, 64, 64, 4, false, false, cutlass::half_t, Flash_kernel_traits<224, 64, 64, 4, cutlass::half_t>>, Is_dropout=false, Is_causal=true, Is_even_N=true, Is_even_K=true, Return_softmax=false, Params=Flash_fwd_params]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(15): here
            instantiation of "void flash_fwd_kernel<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax>(Flash_fwd_params) [with Kernel_traits=Flash_fwd_kernel_traits<224, 64, 64, 4, false, false, cutlass::half_t, Flash_kernel_traits<224, 64, 64, 4, cutlass::half_t>>, Is_dropout=false, Is_causal=true, Is_even_N=true, Is_even_K=true, Return_softmax=false]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(34): here
            instantiation of "void run_flash_fwd<Kernel_traits,Is_dropout,Is_causal>(Flash_fwd_params &, cudaStream_t) [with Kernel_traits=Flash_fwd_kernel_traits<224, 64, 64, 4, false, false, cutlass::half_t, Flash_kernel_traits<224, 64, 64, 4, cutlass::half_t>>, Is_dropout=false, Is_causal=true]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(208): here
            instantiation of "void run_mha_fwd_hdim224<T>(Flash_fwd_params &, cudaStream_t) [with T=cutlass::half_t]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_hdim224_fp16_sm80.cu(8): here

/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_kernel.h(276): warning: variable "tQrQ" was declared but never referenced
          detected during:
            instantiation of "void flash::compute_attn_1rowblock<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params,Is_Q_split_K>(const Params &, int, int, int) [with Kernel_traits=Flash_fwd_kernel_traits<224, 64, 64, 4, false, false, cutlass::half_t, Flash_kernel_traits<224, 64, 64, 4, cutlass::half_t>>, Is_dropout=false, Is_causal=true, Is_even_N=true, Is_even_K=true, Return_softmax=false, Params=Flash_fwd_params, Is_Q_split_K=false]" 
(589): here
            instantiation of "void flash::compute_attn<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &) [with Kernel_traits=Flash_fwd_kernel_traits<224, 64, 64, 4, false, false, cutlass::half_t, Flash_kernel_traits<224, 64, 64, 4, cutlass::half_t>>, Is_dropout=false, Is_causal=true, Is_even_N=true, Is_even_K=true, Return_softmax=false, Params=Flash_fwd_params]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(15): here
            instantiation of "void flash_fwd_kernel<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax>(Flash_fwd_params) [with Kernel_traits=Flash_fwd_kernel_traits<224, 64, 64, 4, false, false, cutlass::half_t, Flash_kernel_traits<224, 64, 64, 4, cutlass::half_t>>, Is_dropout=false, Is_causal=true, Is_even_N=true, Is_even_K=true, Return_softmax=false]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(34): here
            instantiation of "void run_flash_fwd<Kernel_traits,Is_dropout,Is_causal>(Flash_fwd_params &, cudaStream_t) [with Kernel_traits=Flash_fwd_kernel_traits<224, 64, 64, 4, false, false, cutlass::half_t, Flash_kernel_traits<224, 64, 64, 4, cutlass::half_t>>, Is_dropout=false, Is_causal=true]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(208): here
            instantiation of "void run_mha_fwd_hdim224<T>(Flash_fwd_params &, cudaStream_t) [with T=cutlass::half_t]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_hdim224_fp16_sm80.cu(8): here

/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_kernel.h(140): warning: variable "MMA_M" was declared but never referenced
          detected during:
            instantiation of "void flash::compute_attn_1rowblock<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params,Is_Q_split_K>(const Params &, int, int, int) [with Kernel_traits=Flash_fwd_kernel_traits<224, 64, 64, 4, false, false, cutlass::half_t, Flash_kernel_traits<224, 64, 64, 4, cutlass::half_t>>, Is_dropout=false, Is_causal=true, Is_even_N=true, Is_even_K=false, Return_softmax=false, Params=Flash_fwd_params, Is_Q_split_K=false]" 
(589): here
            instantiation of "void flash::compute_attn<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &) [with Kernel_traits=Flash_fwd_kernel_traits<224, 64, 64, 4, false, false, cutlass::half_t, Flash_kernel_traits<224, 64, 64, 4, cutlass::half_t>>, Is_dropout=false, Is_causal=true, Is_even_N=true, Is_even_K=false, Return_softmax=false, Params=Flash_fwd_params]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(15): here
            instantiation of "void flash_fwd_kernel<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax>(Flash_fwd_params) [with Kernel_traits=Flash_fwd_kernel_traits<224, 64, 64, 4, false, false, cutlass::half_t, Flash_kernel_traits<224, 64, 64, 4, cutlass::half_t>>, Is_dropout=false, Is_causal=true, Is_even_N=true, Is_even_K=false, Return_softmax=false]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(34): here
            instantiation of "void run_flash_fwd<Kernel_traits,Is_dropout,Is_causal>(Flash_fwd_params &, cudaStream_t) [with Kernel_traits=Flash_fwd_kernel_traits<224, 64, 64, 4, false, false, cutlass::half_t, Flash_kernel_traits<224, 64, 64, 4, cutlass::half_t>>, Is_dropout=false, Is_causal=true]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(208): here
            instantiation of "void run_mha_fwd_hdim224<T>(Flash_fwd_params &, cudaStream_t) [with T=cutlass::half_t]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_hdim224_fp16_sm80.cu(8): here

/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_kernel.h(276): warning: variable "tQrQ" was declared but never referenced
          detected during:
            instantiation of "void flash::compute_attn_1rowblock<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params,Is_Q_split_K>(const Params &, int, int, int) [with Kernel_traits=Flash_fwd_kernel_traits<224, 64, 64, 4, false, false, cutlass::half_t, Flash_kernel_traits<224, 64, 64, 4, cutlass::half_t>>, Is_dropout=false, Is_causal=true, Is_even_N=true, Is_even_K=false, Return_softmax=false, Params=Flash_fwd_params, Is_Q_split_K=false]" 
(589): here
            instantiation of "void flash::compute_attn<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &) [with Kernel_traits=Flash_fwd_kernel_traits<224, 64, 64, 4, false, false, cutlass::half_t, Flash_kernel_traits<224, 64, 64, 4, cutlass::half_t>>, Is_dropout=false, Is_causal=true, Is_even_N=true, Is_even_K=false, Return_softmax=false, Params=Flash_fwd_params]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(15): here
            instantiation of "void flash_fwd_kernel<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax>(Flash_fwd_params) [with Kernel_traits=Flash_fwd_kernel_traits<224, 64, 64, 4, false, false, cutlass::half_t, Flash_kernel_traits<224, 64, 64, 4, cutlass::half_t>>, Is_dropout=false, Is_causal=true, Is_even_N=true, Is_even_K=false, Return_softmax=false]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(34): here
            instantiation of "void run_flash_fwd<Kernel_traits,Is_dropout,Is_causal>(Flash_fwd_params &, cudaStream_t) [with Kernel_traits=Flash_fwd_kernel_traits<224, 64, 64, 4, false, false, cutlass::half_t, Flash_kernel_traits<224, 64, 64, 4, cutlass::half_t>>, Is_dropout=false, Is_causal=true]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(208): here
            instantiation of "void run_mha_fwd_hdim224<T>(Flash_fwd_params &, cudaStream_t) [with T=cutlass::half_t]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_hdim224_fp16_sm80.cu(8): here

/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_kernel.h(140): warning: variable "MMA_M" was declared but never referenced
          detected during:
            instantiation of "void flash::compute_attn_1rowblock<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params,Is_Q_split_K>(const Params &, int, int, int) [with Kernel_traits=Flash_fwd_kernel_traits<224, 64, 64, 4, false, false, cutlass::half_t, Flash_kernel_traits<224, 64, 64, 4, cutlass::half_t>>, Is_dropout=false, Is_causal=true, Is_even_N=false, Is_even_K=true, Return_softmax=false, Params=Flash_fwd_params, Is_Q_split_K=false]" 
(589): here
            instantiation of "void flash::compute_attn<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &) [with Kernel_traits=Flash_fwd_kernel_traits<224, 64, 64, 4, false, false, cutlass::half_t, Flash_kernel_traits<224, 64, 64, 4, cutlass::half_t>>, Is_dropout=false, Is_causal=true, Is_even_N=false, Is_even_K=true, Return_softmax=false, Params=Flash_fwd_params]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(15): here
            instantiation of "void flash_fwd_kernel<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax>(Flash_fwd_params) [with Kernel_traits=Flash_fwd_kernel_traits<224, 64, 64, 4, false, false, cutlass::half_t, Flash_kernel_traits<224, 64, 64, 4, cutlass::half_t>>, Is_dropout=false, Is_causal=true, Is_even_N=false, Is_even_K=true, Return_softmax=false]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(34): here
            instantiation of "void run_flash_fwd<Kernel_traits,Is_dropout,Is_causal>(Flash_fwd_params &, cudaStream_t) [with Kernel_traits=Flash_fwd_kernel_traits<224, 64, 64, 4, false, false, cutlass::half_t, Flash_kernel_traits<224, 64, 64, 4, cutlass::half_t>>, Is_dropout=false, Is_causal=true]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(208): here
            instantiation of "void run_mha_fwd_hdim224<T>(Flash_fwd_params &, cudaStream_t) [with T=cutlass::half_t]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_hdim224_fp16_sm80.cu(8): here

/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_kernel.h(276): warning: variable "tQrQ" was declared but never referenced
          detected during:
            instantiation of "void flash::compute_attn_1rowblock<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params,Is_Q_split_K>(const Params &, int, int, int) [with Kernel_traits=Flash_fwd_kernel_traits<224, 64, 64, 4, false, false, cutlass::half_t, Flash_kernel_traits<224, 64, 64, 4, cutlass::half_t>>, Is_dropout=false, Is_causal=true, Is_even_N=false, Is_even_K=true, Return_softmax=false, Params=Flash_fwd_params, Is_Q_split_K=false]" 
(589): here
            instantiation of "void flash::compute_attn<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &) [with Kernel_traits=Flash_fwd_kernel_traits<224, 64, 64, 4, false, false, cutlass::half_t, Flash_kernel_traits<224, 64, 64, 4, cutlass::half_t>>, Is_dropout=false, Is_causal=true, Is_even_N=false, Is_even_K=true, Return_softmax=false, Params=Flash_fwd_params]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(15): here
            instantiation of "void flash_fwd_kernel<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax>(Flash_fwd_params) [with Kernel_traits=Flash_fwd_kernel_traits<224, 64, 64, 4, false, false, cutlass::half_t, Flash_kernel_traits<224, 64, 64, 4, cutlass::half_t>>, Is_dropout=false, Is_causal=true, Is_even_N=false, Is_even_K=true, Return_softmax=false]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(34): here
            instantiation of "void run_flash_fwd<Kernel_traits,Is_dropout,Is_causal>(Flash_fwd_params &, cudaStream_t) [with Kernel_traits=Flash_fwd_kernel_traits<224, 64, 64, 4, false, false, cutlass::half_t, Flash_kernel_traits<224, 64, 64, 4, cutlass::half_t>>, Is_dropout=false, Is_causal=true]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(208): here
            instantiation of "void run_mha_fwd_hdim224<T>(Flash_fwd_params &, cudaStream_t) [with T=cutlass::half_t]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_hdim224_fp16_sm80.cu(8): here

/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_kernel.h(140): warning: variable "MMA_M" was declared but never referenced
          detected during:
            instantiation of "void flash::compute_attn_1rowblock<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params,Is_Q_split_K>(const Params &, int, int, int) [with Kernel_traits=Flash_fwd_kernel_traits<224, 64, 64, 4, false, false, cutlass::half_t, Flash_kernel_traits<224, 64, 64, 4, cutlass::half_t>>, Is_dropout=false, Is_causal=true, Is_even_N=false, Is_even_K=false, Return_softmax=false, Params=Flash_fwd_params, Is_Q_split_K=false]" 
(589): here
            instantiation of "void flash::compute_attn<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &) [with Kernel_traits=Flash_fwd_kernel_traits<224, 64, 64, 4, false, false, cutlass::half_t, Flash_kernel_traits<224, 64, 64, 4, cutlass::half_t>>, Is_dropout=false, Is_causal=true, Is_even_N=false, Is_even_K=false, Return_softmax=false, Params=Flash_fwd_params]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(15): here
            instantiation of "void flash_fwd_kernel<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax>(Flash_fwd_params) [with Kernel_traits=Flash_fwd_kernel_traits<224, 64, 64, 4, false, false, cutlass::half_t, Flash_kernel_traits<224, 64, 64, 4, cutlass::half_t>>, Is_dropout=false, Is_causal=true, Is_even_N=false, Is_even_K=false, Return_softmax=false]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(34): here
            instantiation of "void run_flash_fwd<Kernel_traits,Is_dropout,Is_causal>(Flash_fwd_params &, cudaStream_t) [with Kernel_traits=Flash_fwd_kernel_traits<224, 64, 64, 4, false, false, cutlass::half_t, Flash_kernel_traits<224, 64, 64, 4, cutlass::half_t>>, Is_dropout=false, Is_causal=true]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(208): here
            instantiation of "void run_mha_fwd_hdim224<T>(Flash_fwd_params &, cudaStream_t) [with T=cutlass::half_t]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_hdim224_fp16_sm80.cu(8): here

/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_kernel.h(276): warning: variable "tQrQ" was declared but never referenced
          detected during:
            instantiation of "void flash::compute_attn_1rowblock<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params,Is_Q_split_K>(const Params &, int, int, int) [with Kernel_traits=Flash_fwd_kernel_traits<224, 64, 64, 4, false, false, cutlass::half_t, Flash_kernel_traits<224, 64, 64, 4, cutlass::half_t>>, Is_dropout=false, Is_causal=true, Is_even_N=false, Is_even_K=false, Return_softmax=false, Params=Flash_fwd_params, Is_Q_split_K=false]" 
(589): here
            instantiation of "void flash::compute_attn<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &) [with Kernel_traits=Flash_fwd_kernel_traits<224, 64, 64, 4, false, false, cutlass::half_t, Flash_kernel_traits<224, 64, 64, 4, cutlass::half_t>>, Is_dropout=false, Is_causal=true, Is_even_N=false, Is_even_K=false, Return_softmax=false, Params=Flash_fwd_params]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(15): here
            instantiation of "void flash_fwd_kernel<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax>(Flash_fwd_params) [with Kernel_traits=Flash_fwd_kernel_traits<224, 64, 64, 4, false, false, cutlass::half_t, Flash_kernel_traits<224, 64, 64, 4, cutlass::half_t>>, Is_dropout=false, Is_causal=true, Is_even_N=false, Is_even_K=false, Return_softmax=false]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(34): here
            instantiation of "void run_flash_fwd<Kernel_traits,Is_dropout,Is_causal>(Flash_fwd_params &, cudaStream_t) [with Kernel_traits=Flash_fwd_kernel_traits<224, 64, 64, 4, false, false, cutlass::half_t, Flash_kernel_traits<224, 64, 64, 4, cutlass::half_t>>, Is_dropout=false, Is_causal=true]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(208): here
            instantiation of "void run_mha_fwd_hdim224<T>(Flash_fwd_params &, cudaStream_t) [with T=cutlass::half_t]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_hdim224_fp16_sm80.cu(8): here

/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_kernel.h(140): warning: variable "MMA_M" was declared but never referenced
          detected during:
            instantiation of "void flash::compute_attn_1rowblock<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params,Is_Q_split_K>(const Params &, int, int, int) [with Kernel_traits=Flash_fwd_kernel_traits<224, 128, 64, 8, false, false, cutlass::half_t, Flash_kernel_traits<224, 128, 64, 8, cutlass::half_t>>, Is_dropout=false, Is_causal=false, Is_even_N=true, Is_even_K=true, Return_softmax=false, Params=Flash_fwd_params, Is_Q_split_K=false]" 
(589): here
            instantiation of "void flash::compute_attn<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &) [with Kernel_traits=Flash_fwd_kernel_traits<224, 128, 64, 8, false, false, cutlass::half_t, Flash_kernel_traits<224, 128, 64, 8, cutlass::half_t>>, Is_dropout=false, Is_causal=false, Is_even_N=true, Is_even_K=true, Return_softmax=false, Params=Flash_fwd_params]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(15): here
            instantiation of "void flash_fwd_kernel<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax>(Flash_fwd_params) [with Kernel_traits=Flash_fwd_kernel_traits<224, 128, 64, 8, false, false, cutlass::half_t, Flash_kernel_traits<224, 128, 64, 8, cutlass::half_t>>, Is_dropout=false, Is_causal=false, Is_even_N=true, Is_even_K=true, Return_softmax=false]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(34): here
            instantiation of "void run_flash_fwd<Kernel_traits,Is_dropout,Is_causal>(Flash_fwd_params &, cudaStream_t) [with Kernel_traits=Flash_fwd_kernel_traits<224, 128, 64, 8, false, false, cutlass::half_t, Flash_kernel_traits<224, 128, 64, 8, cutlass::half_t>>, Is_dropout=false, Is_causal=false]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(208): here
            instantiation of "void run_mha_fwd_hdim224<T>(Flash_fwd_params &, cudaStream_t) [with T=cutlass::half_t]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_hdim224_fp16_sm80.cu(8): here

/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_kernel.h(276): warning: variable "tQrQ" was declared but never referenced
          detected during:
            instantiation of "void flash::compute_attn_1rowblock<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params,Is_Q_split_K>(const Params &, int, int, int) [with Kernel_traits=Flash_fwd_kernel_traits<224, 128, 64, 8, false, false, cutlass::half_t, Flash_kernel_traits<224, 128, 64, 8, cutlass::half_t>>, Is_dropout=false, Is_causal=false, Is_even_N=true, Is_even_K=true, Return_softmax=false, Params=Flash_fwd_params, Is_Q_split_K=false]" 
(589): here
            instantiation of "void flash::compute_attn<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &) [with Kernel_traits=Flash_fwd_kernel_traits<224, 128, 64, 8, false, false, cutlass::half_t, Flash_kernel_traits<224, 128, 64, 8, cutlass::half_t>>, Is_dropout=false, Is_causal=false, Is_even_N=true, Is_even_K=true, Return_softmax=false, Params=Flash_fwd_params]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(15): here
            instantiation of "void flash_fwd_kernel<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax>(Flash_fwd_params) [with Kernel_traits=Flash_fwd_kernel_traits<224, 128, 64, 8, false, false, cutlass::half_t, Flash_kernel_traits<224, 128, 64, 8, cutlass::half_t>>, Is_dropout=false, Is_causal=false, Is_even_N=true, Is_even_K=true, Return_softmax=false]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(34): here
            instantiation of "void run_flash_fwd<Kernel_traits,Is_dropout,Is_causal>(Flash_fwd_params &, cudaStream_t) [with Kernel_traits=Flash_fwd_kernel_traits<224, 128, 64, 8, false, false, cutlass::half_t, Flash_kernel_traits<224, 128, 64, 8, cutlass::half_t>>, Is_dropout=false, Is_causal=false]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(208): here
            instantiation of "void run_mha_fwd_hdim224<T>(Flash_fwd_params &, cudaStream_t) [with T=cutlass::half_t]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_hdim224_fp16_sm80.cu(8): here

/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_kernel.h(140): warning: variable "MMA_M" was declared but never referenced
          detected during:
            instantiation of "void flash::compute_attn_1rowblock<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params,Is_Q_split_K>(const Params &, int, int, int) [with Kernel_traits=Flash_fwd_kernel_traits<224, 128, 64, 8, false, false, cutlass::half_t, Flash_kernel_traits<224, 128, 64, 8, cutlass::half_t>>, Is_dropout=false, Is_causal=false, Is_even_N=true, Is_even_K=false, Return_softmax=false, Params=Flash_fwd_params, Is_Q_split_K=false]" 
(589): here
            instantiation of "void flash::compute_attn<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &) [with Kernel_traits=Flash_fwd_kernel_traits<224, 128, 64, 8, false, false, cutlass::half_t, Flash_kernel_traits<224, 128, 64, 8, cutlass::half_t>>, Is_dropout=false, Is_causal=false, Is_even_N=true, Is_even_K=false, Return_softmax=false, Params=Flash_fwd_params]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(15): here
            instantiation of "void flash_fwd_kernel<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax>(Flash_fwd_params) [with Kernel_traits=Flash_fwd_kernel_traits<224, 128, 64, 8, false, false, cutlass::half_t, Flash_kernel_traits<224, 128, 64, 8, cutlass::half_t>>, Is_dropout=false, Is_causal=false, Is_even_N=true, Is_even_K=false, Return_softmax=false]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(34): here
            instantiation of "void run_flash_fwd<Kernel_traits,Is_dropout,Is_causal>(Flash_fwd_params &, cudaStream_t) [with Kernel_traits=Flash_fwd_kernel_traits<224, 128, 64, 8, false, false, cutlass::half_t, Flash_kernel_traits<224, 128, 64, 8, cutlass::half_t>>, Is_dropout=false, Is_causal=false]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(208): here
            instantiation of "void run_mha_fwd_hdim224<T>(Flash_fwd_params &, cudaStream_t) [with T=cutlass::half_t]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_hdim224_fp16_sm80.cu(8): here

/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_kernel.h(276): warning: variable "tQrQ" was declared but never referenced
          detected during:
            instantiation of "void flash::compute_attn_1rowblock<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params,Is_Q_split_K>(const Params &, int, int, int) [with Kernel_traits=Flash_fwd_kernel_traits<224, 128, 64, 8, false, false, cutlass::half_t, Flash_kernel_traits<224, 128, 64, 8, cutlass::half_t>>, Is_dropout=false, Is_causal=false, Is_even_N=true, Is_even_K=false, Return_softmax=false, Params=Flash_fwd_params, Is_Q_split_K=false]" 
(589): here
            instantiation of "void flash::compute_attn<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &) [with Kernel_traits=Flash_fwd_kernel_traits<224, 128, 64, 8, false, false, cutlass::half_t, Flash_kernel_traits<224, 128, 64, 8, cutlass::half_t>>, Is_dropout=false, Is_causal=false, Is_even_N=true, Is_even_K=false, Return_softmax=false, Params=Flash_fwd_params]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(15): here
            instantiation of "void flash_fwd_kernel<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax>(Flash_fwd_params) [with Kernel_traits=Flash_fwd_kernel_traits<224, 128, 64, 8, false, false, cutlass::half_t, Flash_kernel_traits<224, 128, 64, 8, cutlass::half_t>>, Is_dropout=false, Is_causal=false, Is_even_N=true, Is_even_K=false, Return_softmax=false]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(34): here
            instantiation of "void run_flash_fwd<Kernel_traits,Is_dropout,Is_causal>(Flash_fwd_params &, cudaStream_t) [with Kernel_traits=Flash_fwd_kernel_traits<224, 128, 64, 8, false, false, cutlass::half_t, Flash_kernel_traits<224, 128, 64, 8, cutlass::half_t>>, Is_dropout=false, Is_causal=false]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(208): here
            instantiation of "void run_mha_fwd_hdim224<T>(Flash_fwd_params &, cudaStream_t) [with T=cutlass::half_t]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_hdim224_fp16_sm80.cu(8): here

/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_kernel.h(140): warning: variable "MMA_M" was declared but never referenced
          detected during:
            instantiation of "void flash::compute_attn_1rowblock<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params,Is_Q_split_K>(const Params &, int, int, int) [with Kernel_traits=Flash_fwd_kernel_traits<224, 128, 64, 8, false, false, cutlass::half_t, Flash_kernel_traits<224, 128, 64, 8, cutlass::half_t>>, Is_dropout=false, Is_causal=false, Is_even_N=false, Is_even_K=true, Return_softmax=false, Params=Flash_fwd_params, Is_Q_split_K=false]" 
(589): here
            instantiation of "void flash::compute_attn<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &) [with Kernel_traits=Flash_fwd_kernel_traits<224, 128, 64, 8, false, false, cutlass::half_t, Flash_kernel_traits<224, 128, 64, 8, cutlass::half_t>>, Is_dropout=false, Is_causal=false, Is_even_N=false, Is_even_K=true, Return_softmax=false, Params=Flash_fwd_params]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(15): here
            instantiation of "void flash_fwd_kernel<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax>(Flash_fwd_params) [with Kernel_traits=Flash_fwd_kernel_traits<224, 128, 64, 8, false, false, cutlass::half_t, Flash_kernel_traits<224, 128, 64, 8, cutlass::half_t>>, Is_dropout=false, Is_causal=false, Is_even_N=false, Is_even_K=true, Return_softmax=false]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(34): here
            instantiation of "void run_flash_fwd<Kernel_traits,Is_dropout,Is_causal>(Flash_fwd_params &, cudaStream_t) [with Kernel_traits=Flash_fwd_kernel_traits<224, 128, 64, 8, false, false, cutlass::half_t, Flash_kernel_traits<224, 128, 64, 8, cutlass::half_t>>, Is_dropout=false, Is_causal=false]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(208): here
            instantiation of "void run_mha_fwd_hdim224<T>(Flash_fwd_params &, cudaStream_t) [with T=cutlass::half_t]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_hdim224_fp16_sm80.cu(8): here

/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_kernel.h(276): warning: variable "tQrQ" was declared but never referenced
          detected during:
            instantiation of "void flash::compute_attn_1rowblock<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params,Is_Q_split_K>(const Params &, int, int, int) [with Kernel_traits=Flash_fwd_kernel_traits<224, 128, 64, 8, false, false, cutlass::half_t, Flash_kernel_traits<224, 128, 64, 8, cutlass::half_t>>, Is_dropout=false, Is_causal=false, Is_even_N=false, Is_even_K=true, Return_softmax=false, Params=Flash_fwd_params, Is_Q_split_K=false]" 
(589): here
            instantiation of "void flash::compute_attn<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &) [with Kernel_traits=Flash_fwd_kernel_traits<224, 128, 64, 8, false, false, cutlass::half_t, Flash_kernel_traits<224, 128, 64, 8, cutlass::half_t>>, Is_dropout=false, Is_causal=false, Is_even_N=false, Is_even_K=true, Return_softmax=false, Params=Flash_fwd_params]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(15): here
            instantiation of "void flash_fwd_kernel<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax>(Flash_fwd_params) [with Kernel_traits=Flash_fwd_kernel_traits<224, 128, 64, 8, false, false, cutlass::half_t, Flash_kernel_traits<224, 128, 64, 8, cutlass::half_t>>, Is_dropout=false, Is_causal=false, Is_even_N=false, Is_even_K=true, Return_softmax=false]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(34): here
            instantiation of "void run_flash_fwd<Kernel_traits,Is_dropout,Is_causal>(Flash_fwd_params &, cudaStream_t) [with Kernel_traits=Flash_fwd_kernel_traits<224, 128, 64, 8, false, false, cutlass::half_t, Flash_kernel_traits<224, 128, 64, 8, cutlass::half_t>>, Is_dropout=false, Is_causal=false]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(208): here
            instantiation of "void run_mha_fwd_hdim224<T>(Flash_fwd_params &, cudaStream_t) [with T=cutlass::half_t]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_hdim224_fp16_sm80.cu(8): here

/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_kernel.h(140): warning: variable "MMA_M" was declared but never referenced
          detected during:
            instantiation of "void flash::compute_attn_1rowblock<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params,Is_Q_split_K>(const Params &, int, int, int) [with Kernel_traits=Flash_fwd_kernel_traits<224, 128, 64, 8, false, false, cutlass::half_t, Flash_kernel_traits<224, 128, 64, 8, cutlass::half_t>>, Is_dropout=false, Is_causal=false, Is_even_N=false, Is_even_K=false, Return_softmax=false, Params=Flash_fwd_params, Is_Q_split_K=false]" 
(589): here
            instantiation of "void flash::compute_attn<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &) [with Kernel_traits=Flash_fwd_kernel_traits<224, 128, 64, 8, false, false, cutlass::half_t, Flash_kernel_traits<224, 128, 64, 8, cutlass::half_t>>, Is_dropout=false, Is_causal=false, Is_even_N=false, Is_even_K=false, Return_softmax=false, Params=Flash_fwd_params]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(15): here
            instantiation of "void flash_fwd_kernel<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax>(Flash_fwd_params) [with Kernel_traits=Flash_fwd_kernel_traits<224, 128, 64, 8, false, false, cutlass::half_t, Flash_kernel_traits<224, 128, 64, 8, cutlass::half_t>>, Is_dropout=false, Is_causal=false, Is_even_N=false, Is_even_K=false, Return_softmax=false]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(34): here
            instantiation of "void run_flash_fwd<Kernel_traits,Is_dropout,Is_causal>(Flash_fwd_params &, cudaStream_t) [with Kernel_traits=Flash_fwd_kernel_traits<224, 128, 64, 8, false, false, cutlass::half_t, Flash_kernel_traits<224, 128, 64, 8, cutlass::half_t>>, Is_dropout=false, Is_causal=false]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(208): here
            instantiation of "void run_mha_fwd_hdim224<T>(Flash_fwd_params &, cudaStream_t) [with T=cutlass::half_t]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_hdim224_fp16_sm80.cu(8): here

/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_kernel.h(276): warning: variable "tQrQ" was declared but never referenced
          detected during:
            instantiation of "void flash::compute_attn_1rowblock<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params,Is_Q_split_K>(const Params &, int, int, int) [with Kernel_traits=Flash_fwd_kernel_traits<224, 128, 64, 8, false, false, cutlass::half_t, Flash_kernel_traits<224, 128, 64, 8, cutlass::half_t>>, Is_dropout=false, Is_causal=false, Is_even_N=false, Is_even_K=false, Return_softmax=false, Params=Flash_fwd_params, Is_Q_split_K=false]" 
(589): here
            instantiation of "void flash::compute_attn<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &) [with Kernel_traits=Flash_fwd_kernel_traits<224, 128, 64, 8, false, false, cutlass::half_t, Flash_kernel_traits<224, 128, 64, 8, cutlass::half_t>>, Is_dropout=false, Is_causal=false, Is_even_N=false, Is_even_K=false, Return_softmax=false, Params=Flash_fwd_params]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(15): here
            instantiation of "void flash_fwd_kernel<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax>(Flash_fwd_params) [with Kernel_traits=Flash_fwd_kernel_traits<224, 128, 64, 8, false, false, cutlass::half_t, Flash_kernel_traits<224, 128, 64, 8, cutlass::half_t>>, Is_dropout=false, Is_causal=false, Is_even_N=false, Is_even_K=false, Return_softmax=false]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(34): here
            instantiation of "void run_flash_fwd<Kernel_traits,Is_dropout,Is_causal>(Flash_fwd_params &, cudaStream_t) [with Kernel_traits=Flash_fwd_kernel_traits<224, 128, 64, 8, false, false, cutlass::half_t, Flash_kernel_traits<224, 128, 64, 8, cutlass::half_t>>, Is_dropout=false, Is_causal=false]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(208): here
            instantiation of "void run_mha_fwd_hdim224<T>(Flash_fwd_params &, cudaStream_t) [with T=cutlass::half_t]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_hdim224_fp16_sm80.cu(8): here

/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_kernel.h(140): warning: variable "MMA_M" was declared but never referenced
          detected during:
            instantiation of "void flash::compute_attn_1rowblock<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params,Is_Q_split_K>(const Params &, int, int, int) [with Kernel_traits=Flash_fwd_kernel_traits<224, 64, 64, 4, false, false, cutlass::half_t, Flash_kernel_traits<224, 64, 64, 4, cutlass::half_t>>, Is_dropout=false, Is_causal=false, Is_even_N=true, Is_even_K=true, Return_softmax=false, Params=Flash_fwd_params, Is_Q_split_K=false]" 
(589): here
            instantiation of "void flash::compute_attn<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &) [with Kernel_traits=Flash_fwd_kernel_traits<224, 64, 64, 4, false, false, cutlass::half_t, Flash_kernel_traits<224, 64, 64, 4, cutlass::half_t>>, Is_dropout=false, Is_causal=false, Is_even_N=true, Is_even_K=true, Return_softmax=false, Params=Flash_fwd_params]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(15): here
            instantiation of "void flash_fwd_kernel<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax>(Flash_fwd_params) [with Kernel_traits=Flash_fwd_kernel_traits<224, 64, 64, 4, false, false, cutlass::half_t, Flash_kernel_traits<224, 64, 64, 4, cutlass::half_t>>, Is_dropout=false, Is_causal=false, Is_even_N=true, Is_even_K=true, Return_softmax=false]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(34): here
            instantiation of "void run_flash_fwd<Kernel_traits,Is_dropout,Is_causal>(Flash_fwd_params &, cudaStream_t) [with Kernel_traits=Flash_fwd_kernel_traits<224, 64, 64, 4, false, false, cutlass::half_t, Flash_kernel_traits<224, 64, 64, 4, cutlass::half_t>>, Is_dropout=false, Is_causal=false]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(208): here
            instantiation of "void run_mha_fwd_hdim224<T>(Flash_fwd_params &, cudaStream_t) [with T=cutlass::half_t]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_hdim224_fp16_sm80.cu(8): here

/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_kernel.h(276): warning: variable "tQrQ" was declared but never referenced
          detected during:
            instantiation of "void flash::compute_attn_1rowblock<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params,Is_Q_split_K>(const Params &, int, int, int) [with Kernel_traits=Flash_fwd_kernel_traits<224, 64, 64, 4, false, false, cutlass::half_t, Flash_kernel_traits<224, 64, 64, 4, cutlass::half_t>>, Is_dropout=false, Is_causal=false, Is_even_N=true, Is_even_K=true, Return_softmax=false, Params=Flash_fwd_params, Is_Q_split_K=false]" 
(589): here
            instantiation of "void flash::compute_attn<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &) [with Kernel_traits=Flash_fwd_kernel_traits<224, 64, 64, 4, false, false, cutlass::half_t, Flash_kernel_traits<224, 64, 64, 4, cutlass::half_t>>, Is_dropout=false, Is_causal=false, Is_even_N=true, Is_even_K=true, Return_softmax=false, Params=Flash_fwd_params]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(15): here
            instantiation of "void flash_fwd_kernel<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax>(Flash_fwd_params) [with Kernel_traits=Flash_fwd_kernel_traits<224, 64, 64, 4, false, false, cutlass::half_t, Flash_kernel_traits<224, 64, 64, 4, cutlass::half_t>>, Is_dropout=false, Is_causal=false, Is_even_N=true, Is_even_K=true, Return_softmax=false]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(34): here
            instantiation of "void run_flash_fwd<Kernel_traits,Is_dropout,Is_causal>(Flash_fwd_params &, cudaStream_t) [with Kernel_traits=Flash_fwd_kernel_traits<224, 64, 64, 4, false, false, cutlass::half_t, Flash_kernel_traits<224, 64, 64, 4, cutlass::half_t>>, Is_dropout=false, Is_causal=false]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(208): here
            instantiation of "void run_mha_fwd_hdim224<T>(Flash_fwd_params &, cudaStream_t) [with T=cutlass::half_t]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_hdim224_fp16_sm80.cu(8): here

/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_kernel.h(140): warning: variable "MMA_M" was declared but never referenced
          detected during:
            instantiation of "void flash::compute_attn_1rowblock<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params,Is_Q_split_K>(const Params &, int, int, int) [with Kernel_traits=Flash_fwd_kernel_traits<224, 64, 64, 4, false, false, cutlass::half_t, Flash_kernel_traits<224, 64, 64, 4, cutlass::half_t>>, Is_dropout=false, Is_causal=false, Is_even_N=true, Is_even_K=false, Return_softmax=false, Params=Flash_fwd_params, Is_Q_split_K=false]" 
(589): here
            instantiation of "void flash::compute_attn<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &) [with Kernel_traits=Flash_fwd_kernel_traits<224, 64, 64, 4, false, false, cutlass::half_t, Flash_kernel_traits<224, 64, 64, 4, cutlass::half_t>>, Is_dropout=false, Is_causal=false, Is_even_N=true, Is_even_K=false, Return_softmax=false, Params=Flash_fwd_params]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(15): here
            instantiation of "void flash_fwd_kernel<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax>(Flash_fwd_params) [with Kernel_traits=Flash_fwd_kernel_traits<224, 64, 64, 4, false, false, cutlass::half_t, Flash_kernel_traits<224, 64, 64, 4, cutlass::half_t>>, Is_dropout=false, Is_causal=false, Is_even_N=true, Is_even_K=false, Return_softmax=false]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(34): here
            instantiation of "void run_flash_fwd<Kernel_traits,Is_dropout,Is_causal>(Flash_fwd_params &, cudaStream_t) [with Kernel_traits=Flash_fwd_kernel_traits<224, 64, 64, 4, false, false, cutlass::half_t, Flash_kernel_traits<224, 64, 64, 4, cutlass::half_t>>, Is_dropout=false, Is_causal=false]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(208): here
            instantiation of "void run_mha_fwd_hdim224<T>(Flash_fwd_params &, cudaStream_t) [with T=cutlass::half_t]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_hdim224_fp16_sm80.cu(8): here

/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_kernel.h(276): warning: variable "tQrQ" was declared but never referenced
          detected during:
            instantiation of "void flash::compute_attn_1rowblock<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params,Is_Q_split_K>(const Params &, int, int, int) [with Kernel_traits=Flash_fwd_kernel_traits<224, 64, 64, 4, false, false, cutlass::half_t, Flash_kernel_traits<224, 64, 64, 4, cutlass::half_t>>, Is_dropout=false, Is_causal=false, Is_even_N=true, Is_even_K=false, Return_softmax=false, Params=Flash_fwd_params, Is_Q_split_K=false]" 
(589): here
            instantiation of "void flash::compute_attn<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &) [with Kernel_traits=Flash_fwd_kernel_traits<224, 64, 64, 4, false, false, cutlass::half_t, Flash_kernel_traits<224, 64, 64, 4, cutlass::half_t>>, Is_dropout=false, Is_causal=false, Is_even_N=true, Is_even_K=false, Return_softmax=false, Params=Flash_fwd_params]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(15): here
            instantiation of "void flash_fwd_kernel<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax>(Flash_fwd_params) [with Kernel_traits=Flash_fwd_kernel_traits<224, 64, 64, 4, false, false, cutlass::half_t, Flash_kernel_traits<224, 64, 64, 4, cutlass::half_t>>, Is_dropout=false, Is_causal=false, Is_even_N=true, Is_even_K=false, Return_softmax=false]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(34): here
            instantiation of "void run_flash_fwd<Kernel_traits,Is_dropout,Is_causal>(Flash_fwd_params &, cudaStream_t) [with Kernel_traits=Flash_fwd_kernel_traits<224, 64, 64, 4, false, false, cutlass::half_t, Flash_kernel_traits<224, 64, 64, 4, cutlass::half_t>>, Is_dropout=false, Is_causal=false]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(208): here
            instantiation of "void run_mha_fwd_hdim224<T>(Flash_fwd_params &, cudaStream_t) [with T=cutlass::half_t]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_hdim224_fp16_sm80.cu(8): here

/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_kernel.h(140): warning: variable "MMA_M" was declared but never referenced
          detected during:
            instantiation of "void flash::compute_attn_1rowblock<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params,Is_Q_split_K>(const Params &, int, int, int) [with Kernel_traits=Flash_fwd_kernel_traits<224, 64, 64, 4, false, false, cutlass::half_t, Flash_kernel_traits<224, 64, 64, 4, cutlass::half_t>>, Is_dropout=false, Is_causal=false, Is_even_N=false, Is_even_K=true, Return_softmax=false, Params=Flash_fwd_params, Is_Q_split_K=false]" 
(589): here
            instantiation of "void flash::compute_attn<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &) [with Kernel_traits=Flash_fwd_kernel_traits<224, 64, 64, 4, false, false, cutlass::half_t, Flash_kernel_traits<224, 64, 64, 4, cutlass::half_t>>, Is_dropout=false, Is_causal=false, Is_even_N=false, Is_even_K=true, Return_softmax=false, Params=Flash_fwd_params]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(15): here
            instantiation of "void flash_fwd_kernel<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax>(Flash_fwd_params) [with Kernel_traits=Flash_fwd_kernel_traits<224, 64, 64, 4, false, false, cutlass::half_t, Flash_kernel_traits<224, 64, 64, 4, cutlass::half_t>>, Is_dropout=false, Is_causal=false, Is_even_N=false, Is_even_K=true, Return_softmax=false]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(34): here
            instantiation of "void run_flash_fwd<Kernel_traits,Is_dropout,Is_causal>(Flash_fwd_params &, cudaStream_t) [with Kernel_traits=Flash_fwd_kernel_traits<224, 64, 64, 4, false, false, cutlass::half_t, Flash_kernel_traits<224, 64, 64, 4, cutlass::half_t>>, Is_dropout=false, Is_causal=false]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(208): here
            instantiation of "void run_mha_fwd_hdim224<T>(Flash_fwd_params &, cudaStream_t) [with T=cutlass::half_t]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_hdim224_fp16_sm80.cu(8): here

/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_kernel.h(276): warning: variable "tQrQ" was declared but never referenced
          detected during:
            instantiation of "void flash::compute_attn_1rowblock<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params,Is_Q_split_K>(const Params &, int, int, int) [with Kernel_traits=Flash_fwd_kernel_traits<224, 64, 64, 4, false, false, cutlass::half_t, Flash_kernel_traits<224, 64, 64, 4, cutlass::half_t>>, Is_dropout=false, Is_causal=false, Is_even_N=false, Is_even_K=true, Return_softmax=false, Params=Flash_fwd_params, Is_Q_split_K=false]" 
(589): here
            instantiation of "void flash::compute_attn<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &) [with Kernel_traits=Flash_fwd_kernel_traits<224, 64, 64, 4, false, false, cutlass::half_t, Flash_kernel_traits<224, 64, 64, 4, cutlass::half_t>>, Is_dropout=false, Is_causal=false, Is_even_N=false, Is_even_K=true, Return_softmax=false, Params=Flash_fwd_params]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(15): here
            instantiation of "void flash_fwd_kernel<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax>(Flash_fwd_params) [with Kernel_traits=Flash_fwd_kernel_traits<224, 64, 64, 4, false, false, cutlass::half_t, Flash_kernel_traits<224, 64, 64, 4, cutlass::half_t>>, Is_dropout=false, Is_causal=false, Is_even_N=false, Is_even_K=true, Return_softmax=false]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(34): here
            instantiation of "void run_flash_fwd<Kernel_traits,Is_dropout,Is_causal>(Flash_fwd_params &, cudaStream_t) [with Kernel_traits=Flash_fwd_kernel_traits<224, 64, 64, 4, false, false, cutlass::half_t, Flash_kernel_traits<224, 64, 64, 4, cutlass::half_t>>, Is_dropout=false, Is_causal=false]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(208): here
            instantiation of "void run_mha_fwd_hdim224<T>(Flash_fwd_params &, cudaStream_t) [with T=cutlass::half_t]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_hdim224_fp16_sm80.cu(8): here

/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_kernel.h(140): warning: variable "MMA_M" was declared but never referenced
          detected during:
            instantiation of "void flash::compute_attn_1rowblock<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params,Is_Q_split_K>(const Params &, int, int, int) [with Kernel_traits=Flash_fwd_kernel_traits<224, 64, 64, 4, false, false, cutlass::half_t, Flash_kernel_traits<224, 64, 64, 4, cutlass::half_t>>, Is_dropout=false, Is_causal=false, Is_even_N=false, Is_even_K=false, Return_softmax=false, Params=Flash_fwd_params, Is_Q_split_K=false]" 
(589): here
            instantiation of "void flash::compute_attn<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &) [with Kernel_traits=Flash_fwd_kernel_traits<224, 64, 64, 4, false, false, cutlass::half_t, Flash_kernel_traits<224, 64, 64, 4, cutlass::half_t>>, Is_dropout=false, Is_causal=false, Is_even_N=false, Is_even_K=false, Return_softmax=false, Params=Flash_fwd_params]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(15): here
            instantiation of "void flash_fwd_kernel<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax>(Flash_fwd_params) [with Kernel_traits=Flash_fwd_kernel_traits<224, 64, 64, 4, false, false, cutlass::half_t, Flash_kernel_traits<224, 64, 64, 4, cutlass::half_t>>, Is_dropout=false, Is_causal=false, Is_even_N=false, Is_even_K=false, Return_softmax=false]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(34): here
            instantiation of "void run_flash_fwd<Kernel_traits,Is_dropout,Is_causal>(Flash_fwd_params &, cudaStream_t) [with Kernel_traits=Flash_fwd_kernel_traits<224, 64, 64, 4, false, false, cutlass::half_t, Flash_kernel_traits<224, 64, 64, 4, cutlass::half_t>>, Is_dropout=false, Is_causal=false]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(208): here
            instantiation of "void run_mha_fwd_hdim224<T>(Flash_fwd_params &, cudaStream_t) [with T=cutlass::half_t]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_hdim224_fp16_sm80.cu(8): here

/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_kernel.h(276): warning: variable "tQrQ" was declared but never referenced
          detected during:
            instantiation of "void flash::compute_attn_1rowblock<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params,Is_Q_split_K>(const Params &, int, int, int) [with Kernel_traits=Flash_fwd_kernel_traits<224, 64, 64, 4, false, false, cutlass::half_t, Flash_kernel_traits<224, 64, 64, 4, cutlass::half_t>>, Is_dropout=false, Is_causal=false, Is_even_N=false, Is_even_K=false, Return_softmax=false, Params=Flash_fwd_params, Is_Q_split_K=false]" 
(589): here
            instantiation of "void flash::compute_attn<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &) [with Kernel_traits=Flash_fwd_kernel_traits<224, 64, 64, 4, false, false, cutlass::half_t, Flash_kernel_traits<224, 64, 64, 4, cutlass::half_t>>, Is_dropout=false, Is_causal=false, Is_even_N=false, Is_even_K=false, Return_softmax=false, Params=Flash_fwd_params]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(15): here
            instantiation of "void flash_fwd_kernel<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax>(Flash_fwd_params) [with Kernel_traits=Flash_fwd_kernel_traits<224, 64, 64, 4, false, false, cutlass::half_t, Flash_kernel_traits<224, 64, 64, 4, cutlass::half_t>>, Is_dropout=false, Is_causal=false, Is_even_N=false, Is_even_K=false, Return_softmax=false]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(34): here
            instantiation of "void run_flash_fwd<Kernel_traits,Is_dropout,Is_causal>(Flash_fwd_params &, cudaStream_t) [with Kernel_traits=Flash_fwd_kernel_traits<224, 64, 64, 4, false, false, cutlass::half_t, Flash_kernel_traits<224, 64, 64, 4, cutlass::half_t>>, Is_dropout=false, Is_causal=false]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(208): here
            instantiation of "void run_mha_fwd_hdim224<T>(Flash_fwd_params &, cudaStream_t) [with T=cutlass::half_t]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_hdim224_fp16_sm80.cu(8): here

ptxas info    : 2 bytes gmem
ptxas info    : Compiling entry function '_Z16flash_fwd_kernelI23Flash_fwd_kernel_traitsILi224ELi64ELi64ELi4ELb0ELb0EN7cutlass6half_tE19Flash_kernel_traitsILi224ELi64ELi64ELi4ES2_EELb0ELb0ELb0ELb0ELb0EEv16Flash_fwd_params' for 'sm_80'
ptxas info    : Function properties for _Z16flash_fwd_kernelI23Flash_fwd_kernel_traitsILi224ELi64ELi64ELi4ELb0ELb0EN7cutlass6half_tE19Flash_kernel_traitsILi224ELi64ELi64ELi4ES2_EELb0ELb0ELb0ELb0ELb0EEv16Flash_fwd_params
    0 bytes stack frame, 0 bytes spill stores, 0 bytes spill loads
ptxas info    : Used 218 registers, 584 bytes cmem[0]
ptxas info    : Compiling entry function '_Z16flash_fwd_kernelI23Flash_fwd_kernel_traitsILi224ELi64ELi64ELi4ELb0ELb0EN7cutlass6half_tE19Flash_kernel_traitsILi224ELi64ELi64ELi4ES2_EELb0ELb0ELb0ELb1ELb0EEv16Flash_fwd_params' for 'sm_80'
ptxas info    : Function properties for _Z16flash_fwd_kernelI23Flash_fwd_kernel_traitsILi224ELi64ELi64ELi4ELb0ELb0EN7cutlass6half_tE19Flash_kernel_traitsILi224ELi64ELi64ELi4ES2_EELb0ELb0ELb0ELb1ELb0EEv16Flash_fwd_params
    0 bytes stack frame, 0 bytes spill stores, 0 bytes spill loads
ptxas info    : Used 211 registers, 584 bytes cmem[0]
ptxas info    : Compiling entry function '_Z16flash_fwd_kernelI23Flash_fwd_kernel_traitsILi224ELi64ELi64ELi4ELb0ELb0EN7cutlass6half_tE19Flash_kernel_traitsILi224ELi64ELi64ELi4ES2_EELb0ELb0ELb1ELb0ELb0EEv16Flash_fwd_params' for 'sm_80'
ptxas info    : Function properties for _Z16flash_fwd_kernelI23Flash_fwd_kernel_traitsILi224ELi64ELi64ELi4ELb0ELb0EN7cutlass6half_tE19Flash_kernel_traitsILi224ELi64ELi64ELi4ES2_EELb0ELb0ELb1ELb0ELb0EEv16Flash_fwd_params
    0 bytes stack frame, 0 bytes spill stores, 0 bytes spill loads
ptxas info    : Used 215 registers, 584 bytes cmem[0]
ptxas info    : Compiling entry function '_Z16flash_fwd_kernelI23Flash_fwd_kernel_traitsILi224ELi64ELi64ELi4ELb0ELb0EN7cutlass6half_tE19Flash_kernel_traitsILi224ELi64ELi64ELi4ES2_EELb0ELb0ELb1ELb1ELb0EEv16Flash_fwd_params' for 'sm_80'
ptxas info    : Function properties for _Z16flash_fwd_kernelI23Flash_fwd_kernel_traitsILi224ELi64ELi64ELi4ELb0ELb0EN7cutlass6half_tE19Flash_kernel_traitsILi224ELi64ELi64ELi4ES2_EELb0ELb0ELb1ELb1ELb0EEv16Flash_fwd_params
    0 bytes stack frame, 0 bytes spill stores, 0 bytes spill loads
ptxas info    : Used 211 registers, 584 bytes cmem[0]
ptxas info    : Compiling entry function '_Z16flash_fwd_kernelI23Flash_fwd_kernel_traitsILi224ELi128ELi64ELi8ELb0ELb0EN7cutlass6half_tE19Flash_kernel_traitsILi224ELi128ELi64ELi8ES2_EELb0ELb0ELb0ELb0ELb0EEv16Flash_fwd_params' for 'sm_80'
ptxas info    : Function properties for _Z16flash_fwd_kernelI23Flash_fwd_kernel_traitsILi224ELi128ELi64ELi8ELb0ELb0EN7cutlass6half_tE19Flash_kernel_traitsILi224ELi128ELi64ELi8ES2_EELb0ELb0ELb0ELb0ELb0EEv16Flash_fwd_params
    0 bytes stack frame, 0 bytes spill stores, 0 bytes spill loads
ptxas info    : Used 221 registers, 584 bytes cmem[0]
ptxas info    : Compiling entry function '_Z16flash_fwd_kernelI23Flash_fwd_kernel_traitsILi224ELi128ELi64ELi8ELb0ELb0EN7cutlass6half_tE19Flash_kernel_traitsILi224ELi128ELi64ELi8ES2_EELb0ELb0ELb0ELb1ELb0EEv16Flash_fwd_params' for 'sm_80'
ptxas info    : Function properties for _Z16flash_fwd_kernelI23Flash_fwd_kernel_traitsILi224ELi128ELi64ELi8ELb0ELb0EN7cutlass6half_tE19Flash_kernel_traitsILi224ELi128ELi64ELi8ES2_EELb0ELb0ELb0ELb1ELb0EEv16Flash_fwd_params
    0 bytes stack frame, 0 bytes spill stores, 0 bytes spill loads
ptxas info    : Used 214 registers, 584 bytes cmem[0]
ptxas info    : Compiling entry function '_Z16flash_fwd_kernelI23Flash_fwd_kernel_traitsILi224ELi128ELi64ELi8ELb0ELb0EN7cutlass6half_tE19Flash_kernel_traitsILi224ELi128ELi64ELi8ES2_EELb0ELb0ELb1ELb0ELb0EEv16Flash_fwd_params' for 'sm_80'
ptxas info    : Function properties for _Z16flash_fwd_kernelI23Flash_fwd_kernel_traitsILi224ELi128ELi64ELi8ELb0ELb0EN7cutlass6half_tE19Flash_kernel_traitsILi224ELi128ELi64ELi8ES2_EELb0ELb0ELb1ELb0ELb0EEv16Flash_fwd_params
    0 bytes stack frame, 0 bytes spill stores, 0 bytes spill loads
ptxas info    : Used 218 registers, 584 bytes cmem[0]
ptxas info    : Compiling entry function '_Z16flash_fwd_kernelI23Flash_fwd_kernel_traitsILi224ELi128ELi64ELi8ELb0ELb0EN7cutlass6half_tE19Flash_kernel_traitsILi224ELi128ELi64ELi8ES2_EELb0ELb0ELb1ELb1ELb0EEv16Flash_fwd_params' for 'sm_80'
ptxas info    : Function properties for _Z16flash_fwd_kernelI23Flash_fwd_kernel_traitsILi224ELi128ELi64ELi8ELb0ELb0EN7cutlass6half_tE19Flash_kernel_traitsILi224ELi128ELi64ELi8ES2_EELb0ELb0ELb1ELb1ELb0EEv16Flash_fwd_params
    0 bytes stack frame, 0 bytes spill stores, 0 bytes spill loads
ptxas info    : Used 212 registers, 584 bytes cmem[0]
ptxas info    : Compiling entry function '_Z16flash_fwd_kernelI23Flash_fwd_kernel_traitsILi224ELi64ELi64ELi4ELb0ELb0EN7cutlass6half_tE19Flash_kernel_traitsILi224ELi64ELi64ELi4ES2_EELb0ELb1ELb0ELb0ELb0EEv16Flash_fwd_params' for 'sm_80'
ptxas info    : Function properties for _Z16flash_fwd_kernelI23Flash_fwd_kernel_traitsILi224ELi64ELi64ELi4ELb0ELb0EN7cutlass6half_tE19Flash_kernel_traitsILi224ELi64ELi64ELi4ES2_EELb0ELb1ELb0ELb0ELb0EEv16Flash_fwd_params
    0 bytes stack frame, 0 bytes spill stores, 0 bytes spill loads
ptxas info    : Used 219 registers, 584 bytes cmem[0]
ptxas info    : Compiling entry function '_Z16flash_fwd_kernelI23Flash_fwd_kernel_traitsILi224ELi64ELi64ELi4ELb0ELb0EN7cutlass6half_tE19Flash_kernel_traitsILi224ELi64ELi64ELi4ES2_EELb0ELb1ELb0ELb1ELb0EEv16Flash_fwd_params' for 'sm_80'
ptxas info    : Function properties for _Z16flash_fwd_kernelI23Flash_fwd_kernel_traitsILi224ELi64ELi64ELi4ELb0ELb0EN7cutlass6half_tE19Flash_kernel_traitsILi224ELi64ELi64ELi4ES2_EELb0ELb1ELb0ELb1ELb0EEv16Flash_fwd_params
    0 bytes stack frame, 0 bytes spill stores, 0 bytes spill loads
ptxas info    : Used 212 registers, 584 bytes cmem[0]
ptxas info    : Compiling entry function '_Z16flash_fwd_kernelI23Flash_fwd_kernel_traitsILi224ELi64ELi64ELi4ELb0ELb0EN7cutlass6half_tE19Flash_kernel_traitsILi224ELi64ELi64ELi4ES2_EELb0ELb1ELb1ELb0ELb0EEv16Flash_fwd_params' for 'sm_80'
ptxas info    : Function properties for _Z16flash_fwd_kernelI23Flash_fwd_kernel_traitsILi224ELi64ELi64ELi4ELb0ELb0EN7cutlass6half_tE19Flash_kernel_traitsILi224ELi64ELi64ELi4ES2_EELb0ELb1ELb1ELb0ELb0EEv16Flash_fwd_params
    0 bytes stack frame, 0 bytes spill stores, 0 bytes spill loads
ptxas info    : Used 230 registers, 584 bytes cmem[0]
ptxas info    : Compiling entry function '_Z16flash_fwd_kernelI23Flash_fwd_kernel_traitsILi224ELi64ELi64ELi4ELb0ELb0EN7cutlass6half_tE19Flash_kernel_traitsILi224ELi64ELi64ELi4ES2_EELb0ELb1ELb1ELb1ELb0EEv16Flash_fwd_params' for 'sm_80'
ptxas info    : Function properties for _Z16flash_fwd_kernelI23Flash_fwd_kernel_traitsILi224ELi64ELi64ELi4ELb0ELb0EN7cutlass6half_tE19Flash_kernel_traitsILi224ELi64ELi64ELi4ES2_EELb0ELb1ELb1ELb1ELb0EEv16Flash_fwd_params
    0 bytes stack frame, 0 bytes spill stores, 0 bytes spill loads
ptxas info    : Used 222 registers, 584 bytes cmem[0]
ptxas info    : Compiling entry function '_Z16flash_fwd_kernelI23Flash_fwd_kernel_traitsILi224ELi128ELi64ELi8ELb0ELb0EN7cutlass6half_tE19Flash_kernel_traitsILi224ELi128ELi64ELi8ES2_EELb0ELb1ELb0ELb0ELb0EEv16Flash_fwd_params' for 'sm_80'
ptxas info    : Function properties for _Z16flash_fwd_kernelI23Flash_fwd_kernel_traitsILi224ELi128ELi64ELi8ELb0ELb0EN7cutlass6half_tE19Flash_kernel_traitsILi224ELi128ELi64ELi8ES2_EELb0ELb1ELb0ELb0ELb0EEv16Flash_fwd_params
    0 bytes stack frame, 0 bytes spill stores, 0 bytes spill loads
ptxas info    : Used 234 registers, 584 bytes cmem[0]
ptxas info    : Compiling entry function '_Z16flash_fwd_kernelI23Flash_fwd_kernel_traitsILi224ELi128ELi64ELi8ELb0ELb0EN7cutlass6half_tE19Flash_kernel_traitsILi224ELi128ELi64ELi8ES2_EELb0ELb1ELb0ELb1ELb0EEv16Flash_fwd_params' for 'sm_80'
ptxas info    : Function properties for _Z16flash_fwd_kernelI23Flash_fwd_kernel_traitsILi224ELi128ELi64ELi8ELb0ELb0EN7cutlass6half_tE19Flash_kernel_traitsILi224ELi128ELi64ELi8ES2_EELb0ELb1ELb0ELb1ELb0EEv16Flash_fwd_params
    0 bytes stack frame, 0 bytes spill stores, 0 bytes spill loads
ptxas info    : Used 226 registers, 584 bytes cmem[0]
ptxas info    : Compiling entry function '_Z16flash_fwd_kernelI23Flash_fwd_kernel_traitsILi224ELi128ELi64ELi8ELb0ELb0EN7cutlass6half_tE19Flash_kernel_traitsILi224ELi128ELi64ELi8ES2_EELb0ELb1ELb1ELb0ELb0EEv16Flash_fwd_params' for 'sm_80'
ptxas info    : Function properties for _Z16flash_fwd_kernelI23Flash_fwd_kernel_traitsILi224ELi128ELi64ELi8ELb0ELb0EN7cutlass6half_tE19Flash_kernel_traitsILi224ELi128ELi64ELi8ES2_EELb0ELb1ELb1ELb0ELb0EEv16Flash_fwd_params
    48 bytes stack frame, 80 bytes spill stores, 72 bytes spill loads
ptxas info    : Used 255 registers, 584 bytes cmem[0]
ptxas info    : Compiling entry function '_Z16flash_fwd_kernelI23Flash_fwd_kernel_traitsILi224ELi128ELi64ELi8ELb0ELb0EN7cutlass6half_tE19Flash_kernel_traitsILi224ELi128ELi64ELi8ES2_EELb0ELb1ELb1ELb1ELb0EEv16Flash_fwd_params' for 'sm_80'
ptxas info    : Function properties for _Z16flash_fwd_kernelI23Flash_fwd_kernel_traitsILi224ELi128ELi64ELi8ELb0ELb0EN7cutlass6half_tE19Flash_kernel_traitsILi224ELi128ELi64ELi8ES2_EELb0ELb1ELb1ELb1ELb0EEv16Flash_fwd_params
    0 bytes stack frame, 0 bytes spill stores, 0 bytes spill loads
ptxas info    : Used 228 registers, 584 bytes cmem[0]
ptxas info    : Compiling entry function '_Z16flash_fwd_kernelI23Flash_fwd_kernel_traitsILi224ELi64ELi64ELi4ELb0ELb0EN7cutlass6half_tE19Flash_kernel_traitsILi224ELi64ELi64ELi4ES2_EELb1ELb0ELb0ELb0ELb0EEv16Flash_fwd_params' for 'sm_80'
ptxas info    : Function properties for _Z16flash_fwd_kernelI23Flash_fwd_kernel_traitsILi224ELi64ELi64ELi4ELb0ELb0EN7cutlass6half_tE19Flash_kernel_traitsILi224ELi64ELi64ELi4ES2_EELb1ELb0ELb0ELb0ELb0EEv16Flash_fwd_params
    0 bytes stack frame, 0 bytes spill stores, 0 bytes spill loads
ptxas info    : Used 230 registers, 584 bytes cmem[0]
ptxas info    : Compiling entry function '_Z16flash_fwd_kernelI23Flash_fwd_kernel_traitsILi224ELi64ELi64ELi4ELb0ELb0EN7cutlass6half_tE19Flash_kernel_traitsILi224ELi64ELi64ELi4ES2_EELb1ELb0ELb0ELb0ELb1EEv16Flash_fwd_params' for 'sm_80'
ptxas info    : Function properties for _Z16flash_fwd_kernelI23Flash_fwd_kernel_traitsILi224ELi64ELi64ELi4ELb0ELb0EN7cutlass6half_tE19Flash_kernel_traitsILi224ELi64ELi64ELi4ES2_EELb1ELb0ELb0ELb0ELb1EEv16Flash_fwd_params
    48 bytes stack frame, 60 bytes spill stores, 52 bytes spill loads
ptxas info    : Used 255 registers, 584 bytes cmem[0]
ptxas info    : Compiling entry function '_Z16flash_fwd_kernelI23Flash_fwd_kernel_traitsILi224ELi64ELi64ELi4ELb0ELb0EN7cutlass6half_tE19Flash_kernel_traitsILi224ELi64ELi64ELi4ES2_EELb1ELb0ELb0ELb1ELb0EEv16Flash_fwd_params' for 'sm_80'
ptxas info    : Function properties for _Z16flash_fwd_kernelI23Flash_fwd_kernel_traitsILi224ELi64ELi64ELi4ELb0ELb0EN7cutlass6half_tE19Flash_kernel_traitsILi224ELi64ELi64ELi4ES2_EELb1ELb0ELb0ELb1ELb0EEv16Flash_fwd_params
    0 bytes stack frame, 0 bytes spill stores, 0 bytes spill loads
ptxas info    : Used 222 registers, 584 bytes cmem[0]
ptxas info    : Compiling entry function '_Z16flash_fwd_kernelI23Flash_fwd_kernel_traitsILi224ELi64ELi64ELi4ELb0ELb0EN7cutlass6half_tE19Flash_kernel_traitsILi224ELi64ELi64ELi4ES2_EELb1ELb0ELb0ELb1ELb1EEv16Flash_fwd_params' for 'sm_80'
ptxas info    : Function properties for _Z16flash_fwd_kernelI23Flash_fwd_kernel_traitsILi224ELi64ELi64ELi4ELb0ELb0EN7cutlass6half_tE19Flash_kernel_traitsILi224ELi64ELi64ELi4ES2_EELb1ELb0ELb0ELb1ELb1EEv16Flash_fwd_params
    40 bytes stack frame, 68 bytes spill stores, 44 bytes spill loads
ptxas info    : Used 255 registers, 584 bytes cmem[0]
ptxas info    : Compiling entry function '_Z16flash_fwd_kernelI23Flash_fwd_kernel_traitsILi224ELi64ELi64ELi4ELb0ELb0EN7cutlass6half_tE19Flash_kernel_traitsILi224ELi64ELi64ELi4ES2_EELb1ELb0ELb1ELb0ELb0EEv16Flash_fwd_params' for 'sm_80'
ptxas info    : Function properties for _Z16flash_fwd_kernelI23Flash_fwd_kernel_traitsILi224ELi64ELi64ELi4ELb0ELb0EN7cutlass6half_tE19Flash_kernel_traitsILi224ELi64ELi64ELi4ES2_EELb1ELb0ELb1ELb0ELb0EEv16Flash_fwd_params
    0 bytes stack frame, 0 bytes spill stores, 0 bytes spill loads
ptxas info    : Used 228 registers, 584 bytes cmem[0]
ptxas info    : Compiling entry function '_Z16flash_fwd_kernelI23Flash_fwd_kernel_traitsILi224ELi64ELi64ELi4ELb0ELb0EN7cutlass6half_tE19Flash_kernel_traitsILi224ELi64ELi64ELi4ES2_EELb1ELb0ELb1ELb0ELb1EEv16Flash_fwd_params' for 'sm_80'
ptxas info    : Function properties for _Z16flash_fwd_kernelI23Flash_fwd_kernel_traitsILi224ELi64ELi64ELi4ELb0ELb0EN7cutlass6half_tE19Flash_kernel_traitsILi224ELi64ELi64ELi4ES2_EELb1ELb0ELb1ELb0ELb1EEv16Flash_fwd_params
    64 bytes stack frame, 68 bytes spill stores, 60 bytes spill loads
ptxas info    : Used 255 registers, 584 bytes cmem[0]
ptxas info    : Compiling entry function '_Z16flash_fwd_kernelI23Flash_fwd_kernel_traitsILi224ELi64ELi64ELi4ELb0ELb0EN7cutlass6half_tE19Flash_kernel_traitsILi224ELi64ELi64ELi4ES2_EELb1ELb0ELb1ELb1ELb0EEv16Flash_fwd_params' for 'sm_80'
ptxas info    : Function properties for _Z16flash_fwd_kernelI23Flash_fwd_kernel_traitsILi224ELi64ELi64ELi4ELb0ELb0EN7cutlass6half_tE19Flash_kernel_traitsILi224ELi64ELi64ELi4ES2_EELb1ELb0ELb1ELb1ELb0EEv16Flash_fwd_params
    0 bytes stack frame, 0 bytes spill stores, 0 bytes spill loads
ptxas info    : Used 222 registers, 584 bytes cmem[0]
ptxas info    : Compiling entry function '_Z16flash_fwd_kernelI23Flash_fwd_kernel_traitsILi224ELi64ELi64ELi4ELb0ELb0EN7cutlass6half_tE19Flash_kernel_traitsILi224ELi64ELi64ELi4ES2_EELb1ELb0ELb1ELb1ELb1EEv16Flash_fwd_params' for 'sm_80'
ptxas info    : Function properties for _Z16flash_fwd_kernelI23Flash_fwd_kernel_traitsILi224ELi64ELi64ELi4ELb0ELb0EN7cutlass6half_tE19Flash_kernel_traitsILi224ELi64ELi64ELi4ES2_EELb1ELb0ELb1ELb1ELb1EEv16Flash_fwd_params
    48 bytes stack frame, 62 bytes spill stores, 54 bytes spill loads
ptxas info    : Used 255 registers, 584 bytes cmem[0]
ptxas info    : Compiling entry function '_Z16flash_fwd_kernelI23Flash_fwd_kernel_traitsILi224ELi128ELi64ELi8ELb0ELb0EN7cutlass6half_tE19Flash_kernel_traitsILi224ELi128ELi64ELi8ES2_EELb1ELb0ELb0ELb0ELb0EEv16Flash_fwd_params' for 'sm_80'
ptxas info    : Function properties for _Z16flash_fwd_kernelI23Flash_fwd_kernel_traitsILi224ELi128ELi64ELi8ELb0ELb0EN7cutlass6half_tE19Flash_kernel_traitsILi224ELi128ELi64ELi8ES2_EELb1ELb0ELb0ELb0ELb0EEv16Flash_fwd_params
    0 bytes stack frame, 0 bytes spill stores, 0 bytes spill loads
ptxas info    : Used 234 registers, 584 bytes cmem[0]
ptxas info    : Compiling entry function '_Z16flash_fwd_kernelI23Flash_fwd_kernel_traitsILi224ELi128ELi64ELi8ELb0ELb0EN7cutlass6half_tE19Flash_kernel_traitsILi224ELi128ELi64ELi8ES2_EELb1ELb0ELb0ELb0ELb1EEv16Flash_fwd_params' for 'sm_80'
ptxas info    : Function properties for _Z16flash_fwd_kernelI23Flash_fwd_kernel_traitsILi224ELi128ELi64ELi8ELb0ELb0EN7cutlass6half_tE19Flash_kernel_traitsILi224ELi128ELi64ELi8ES2_EELb1ELb0ELb0ELb0ELb1EEv16Flash_fwd_params
    48 bytes stack frame, 44 bytes spill stores, 44 bytes spill loads
ptxas info    : Used 255 registers, 584 bytes cmem[0]
ptxas info    : Compiling entry function '_Z16flash_fwd_kernelI23Flash_fwd_kernel_traitsILi224ELi128ELi64ELi8ELb0ELb0EN7cutlass6half_tE19Flash_kernel_traitsILi224ELi128ELi64ELi8ES2_EELb1ELb0ELb0ELb1ELb0EEv16Flash_fwd_params' for 'sm_80'
ptxas info    : Function properties for _Z16flash_fwd_kernelI23Flash_fwd_kernel_traitsILi224ELi128ELi64ELi8ELb0ELb0EN7cutlass6half_tE19Flash_kernel_traitsILi224ELi128ELi64ELi8ES2_EELb1ELb0ELb0ELb1ELb0EEv16Flash_fwd_params
    0 bytes stack frame, 0 bytes spill stores, 0 bytes spill loads
ptxas info    : Used 226 registers, 584 bytes cmem[0]
ptxas info    : Compiling entry function '_Z16flash_fwd_kernelI23Flash_fwd_kernel_traitsILi224ELi128ELi64ELi8ELb0ELb0EN7cutlass6half_tE19Flash_kernel_traitsILi224ELi128ELi64ELi8ES2_EELb1ELb0ELb0ELb1ELb1EEv16Flash_fwd_params' for 'sm_80'
ptxas info    : Function properties for _Z16flash_fwd_kernelI23Flash_fwd_kernel_traitsILi224ELi128ELi64ELi8ELb0ELb0EN7cutlass6half_tE19Flash_kernel_traitsILi224ELi128ELi64ELi8ES2_EELb1ELb0ELb0ELb1ELb1EEv16Flash_fwd_params
    40 bytes stack frame, 72 bytes spill stores, 48 bytes spill loads
ptxas info    : Used 255 registers, 584 bytes cmem[0]
ptxas info    : Compiling entry function '_Z16flash_fwd_kernelI23Flash_fwd_kernel_traitsILi224ELi128ELi64ELi8ELb0ELb0EN7cutlass6half_tE19Flash_kernel_traitsILi224ELi128ELi64ELi8ES2_EELb1ELb0ELb1ELb0ELb0EEv16Flash_fwd_params' for 'sm_80'
ptxas info    : Function properties for _Z16flash_fwd_kernelI23Flash_fwd_kernel_traitsILi224ELi128ELi64ELi8ELb0ELb0EN7cutlass6half_tE19Flash_kernel_traitsILi224ELi128ELi64ELi8ES2_EELb1ELb0ELb1ELb0ELb0EEv16Flash_fwd_params
    0 bytes stack frame, 0 bytes spill stores, 0 bytes spill loads
ptxas info    : Used 234 registers, 584 bytes cmem[0]
ptxas info    : Compiling entry function '_Z16flash_fwd_kernelI23Flash_fwd_kernel_traitsILi224ELi128ELi64ELi8ELb0ELb0EN7cutlass6half_tE19Flash_kernel_traitsILi224ELi128ELi64ELi8ES2_EELb1ELb0ELb1ELb0ELb1EEv16Flash_fwd_params' for 'sm_80'
ptxas info    : Function properties for _Z16flash_fwd_kernelI23Flash_fwd_kernel_traitsILi224ELi128ELi64ELi8ELb0ELb0EN7cutlass6half_tE19Flash_kernel_traitsILi224ELi128ELi64ELi8ES2_EELb1ELb0ELb1ELb0ELb1EEv16Flash_fwd_params
    64 bytes stack frame, 56 bytes spill stores, 56 bytes spill loads
ptxas info    : Used 255 registers, 584 bytes cmem[0]
ptxas info    : Compiling entry function '_Z16flash_fwd_kernelI23Flash_fwd_kernel_traitsILi224ELi128ELi64ELi8ELb0ELb0EN7cutlass6half_tE19Flash_kernel_traitsILi224ELi128ELi64ELi8ES2_EELb1ELb0ELb1ELb1ELb0EEv16Flash_fwd_params' for 'sm_80'
ptxas info    : Function properties for _Z16flash_fwd_kernelI23Flash_fwd_kernel_traitsILi224ELi128ELi64ELi8ELb0ELb0EN7cutlass6half_tE19Flash_kernel_traitsILi224ELi128ELi64ELi8ES2_EELb1ELb0ELb1ELb1ELb0EEv16Flash_fwd_params
    0 bytes stack frame, 0 bytes spill stores, 0 bytes spill loads
ptxas info    : Used 222 registers, 584 bytes cmem[0]
ptxas info    : Compiling entry function '_Z16flash_fwd_kernelI23Flash_fwd_kernel_traitsILi224ELi128ELi64ELi8ELb0ELb0EN7cutlass6half_tE19Flash_kernel_traitsILi224ELi128ELi64ELi8ES2_EELb1ELb0ELb1ELb1ELb1EEv16Flash_fwd_params' for 'sm_80'
ptxas info    : Function properties for _Z16flash_fwd_kernelI23Flash_fwd_kernel_traitsILi224ELi128ELi64ELi8ELb0ELb0EN7cutlass6half_tE19Flash_kernel_traitsILi224ELi128ELi64ELi8ES2_EELb1ELb0ELb1ELb1ELb1EEv16Flash_fwd_params
    104 bytes stack frame, 100 bytes spill stores, 92 bytes spill loads
ptxas info    : Used 255 registers, 584 bytes cmem[0]
ptxas info    : Compiling entry function '_Z16flash_fwd_kernelI23Flash_fwd_kernel_traitsILi224ELi64ELi64ELi4ELb0ELb0EN7cutlass6half_tE19Flash_kernel_traitsILi224ELi64ELi64ELi4ES2_EELb1ELb1ELb0ELb0ELb0EEv16Flash_fwd_params' for 'sm_80'
ptxas info    : Function properties for _Z16flash_fwd_kernelI23Flash_fwd_kernel_traitsILi224ELi64ELi64ELi4ELb0ELb0EN7cutlass6half_tE19Flash_kernel_traitsILi224ELi64ELi64ELi4ES2_EELb1ELb1ELb0ELb0ELb0EEv16Flash_fwd_params
    0 bytes stack frame, 0 bytes spill stores, 0 bytes spill loads
ptxas info    : Used 232 registers, 584 bytes cmem[0]
ptxas info    : Compiling entry function '_Z16flash_fwd_kernelI23Flash_fwd_kernel_traitsILi224ELi64ELi64ELi4ELb0ELb0EN7cutlass6half_tE19Flash_kernel_traitsILi224ELi64ELi64ELi4ES2_EELb1ELb1ELb0ELb0ELb1EEv16Flash_fwd_params' for 'sm_80'
ptxas info    : Function properties for _Z16flash_fwd_kernelI23Flash_fwd_kernel_traitsILi224ELi64ELi64ELi4ELb0ELb0EN7cutlass6half_tE19Flash_kernel_traitsILi224ELi64ELi64ELi4ES2_EELb1ELb1ELb0ELb0ELb1EEv16Flash_fwd_params
    48 bytes stack frame, 62 bytes spill stores, 54 bytes spill loads
ptxas info    : Used 255 registers, 584 bytes cmem[0]
ptxas info    : Compiling entry function '_Z16flash_fwd_kernelI23Flash_fwd_kernel_traitsILi224ELi64ELi64ELi4ELb0ELb0EN7cutlass6half_tE19Flash_kernel_traitsILi224ELi64ELi64ELi4ES2_EELb1ELb1ELb0ELb1ELb0EEv16Flash_fwd_params' for 'sm_80'
ptxas info    : Function properties for _Z16flash_fwd_kernelI23Flash_fwd_kernel_traitsILi224ELi64ELi64ELi4ELb0ELb0EN7cutlass6half_tE19Flash_kernel_traitsILi224ELi64ELi64ELi4ES2_EELb1ELb1ELb0ELb1ELb0EEv16Flash_fwd_params
    0 bytes stack frame, 0 bytes spill stores, 0 bytes spill loads
ptxas info    : Used 222 registers, 584 bytes cmem[0]
ptxas info    : Compiling entry function '_Z16flash_fwd_kernelI23Flash_fwd_kernel_traitsILi224ELi64ELi64ELi4ELb0ELb0EN7cutlass6half_tE19Flash_kernel_traitsILi224ELi64ELi64ELi4ES2_EELb1ELb1ELb0ELb1ELb1EEv16Flash_fwd_params' for 'sm_80'
ptxas info    : Function properties for _Z16flash_fwd_kernelI23Flash_fwd_kernel_traitsILi224ELi64ELi64ELi4ELb0ELb0EN7cutlass6half_tE19Flash_kernel_traitsILi224ELi64ELi64ELi4ES2_EELb1ELb1ELb0ELb1ELb1EEv16Flash_fwd_params
    32 bytes stack frame, 56 bytes spill stores, 32 bytes spill loads
ptxas info    : Used 255 registers, 584 bytes cmem[0]
ptxas info    : Compiling entry function '_Z16flash_fwd_kernelI23Flash_fwd_kernel_traitsILi224ELi64ELi64ELi4ELb0ELb0EN7cutlass6half_tE19Flash_kernel_traitsILi224ELi64ELi64ELi4ES2_EELb1ELb1ELb1ELb0ELb0EEv16Flash_fwd_params' for 'sm_80'
ptxas info    : Function properties for _Z16flash_fwd_kernelI23Flash_fwd_kernel_traitsILi224ELi64ELi64ELi4ELb0ELb0EN7cutlass6half_tE19Flash_kernel_traitsILi224ELi64ELi64ELi4ES2_EELb1ELb1ELb1ELb0ELb0EEv16Flash_fwd_params
    0 bytes stack frame, 0 bytes spill stores, 0 bytes spill loads
ptxas info    : Used 238 registers, 584 bytes cmem[0]
ptxas info    : Compiling entry function '_Z16flash_fwd_kernelI23Flash_fwd_kernel_traitsILi224ELi64ELi64ELi4ELb0ELb0EN7cutlass6half_tE19Flash_kernel_traitsILi224ELi64ELi64ELi4ES2_EELb1ELb1ELb1ELb0ELb1EEv16Flash_fwd_params' for 'sm_80'
ptxas info    : Function properties for _Z16flash_fwd_kernelI23Flash_fwd_kernel_traitsILi224ELi64ELi64ELi4ELb0ELb0EN7cutlass6half_tE19Flash_kernel_traitsILi224ELi64ELi64ELi4ES2_EELb1ELb1ELb1ELb0ELb1EEv16Flash_fwd_params
    64 bytes stack frame, 70 bytes spill stores, 62 bytes spill loads
ptxas info    : Used 255 registers, 584 bytes cmem[0]
ptxas info    : Compiling entry function '_Z16flash_fwd_kernelI23Flash_fwd_kernel_traitsILi224ELi64ELi64ELi4ELb0ELb0EN7cutlass6half_tE19Flash_kernel_traitsILi224ELi64ELi64ELi4ES2_EELb1ELb1ELb1ELb1ELb0EEv16Flash_fwd_params' for 'sm_80'
ptxas info    : Function properties for _Z16flash_fwd_kernelI23Flash_fwd_kernel_traitsILi224ELi64ELi64ELi4ELb0ELb0EN7cutlass6half_tE19Flash_kernel_traitsILi224ELi64ELi64ELi4ES2_EELb1ELb1ELb1ELb1ELb0EEv16Flash_fwd_params
    0 bytes stack frame, 0 bytes spill stores, 0 bytes spill loads
ptxas info    : Used 232 registers, 584 bytes cmem[0]
ptxas info    : Compiling entry function '_Z16flash_fwd_kernelI23Flash_fwd_kernel_traitsILi224ELi64ELi64ELi4ELb0ELb0EN7cutlass6half_tE19Flash_kernel_traitsILi224ELi64ELi64ELi4ES2_EELb1ELb1ELb1ELb1ELb1EEv16Flash_fwd_params' for 'sm_80'
ptxas info    : Function properties for _Z16flash_fwd_kernelI23Flash_fwd_kernel_traitsILi224ELi64ELi64ELi4ELb0ELb0EN7cutlass6half_tE19Flash_kernel_traitsILi224ELi64ELi64ELi4ES2_EELb1ELb1ELb1ELb1ELb1EEv16Flash_fwd_params
    32 bytes stack frame, 40 bytes spill stores, 32 bytes spill loads
ptxas info    : Used 255 registers, 584 bytes cmem[0]
ptxas info    : Compiling entry function '_Z16flash_fwd_kernelI23Flash_fwd_kernel_traitsILi224ELi128ELi64ELi8ELb0ELb0EN7cutlass6half_tE19Flash_kernel_traitsILi224ELi128ELi64ELi8ES2_EELb1ELb1ELb0ELb0ELb0EEv16Flash_fwd_params' for 'sm_80'
ptxas info    : Function properties for _Z16flash_fwd_kernelI23Flash_fwd_kernel_traitsILi224ELi128ELi64ELi8ELb0ELb0EN7cutlass6half_tE19Flash_kernel_traitsILi224ELi128ELi64ELi8ES2_EELb1ELb1ELb0ELb0ELb0EEv16Flash_fwd_params
    72 bytes stack frame, 72 bytes spill stores, 72 bytes spill loads
ptxas info    : Used 255 registers, 584 bytes cmem[0]
ptxas info    : Compiling entry function '_Z16flash_fwd_kernelI23Flash_fwd_kernel_traitsILi224ELi128ELi64ELi8ELb0ELb0EN7cutlass6half_tE19Flash_kernel_traitsILi224ELi128ELi64ELi8ES2_EELb1ELb1ELb0ELb0ELb1EEv16Flash_fwd_params' for 'sm_80'
ptxas info    : Function properties for _Z16flash_fwd_kernelI23Flash_fwd_kernel_traitsILi224ELi128ELi64ELi8ELb0ELb0EN7cutlass6half_tE19Flash_kernel_traitsILi224ELi128ELi64ELi8ES2_EELb1ELb1ELb0ELb0ELb1EEv16Flash_fwd_params
    80 bytes stack frame, 106 bytes spill stores, 110 bytes spill loads
ptxas info    : Used 255 registers, 584 bytes cmem[0]
ptxas info    : Compiling entry function '_Z16flash_fwd_kernelI23Flash_fwd_kernel_traitsILi224ELi128ELi64ELi8ELb0ELb0EN7cutlass6half_tE19Flash_kernel_traitsILi224ELi128ELi64ELi8ES2_EELb1ELb1ELb0ELb1ELb0EEv16Flash_fwd_params' for 'sm_80'
ptxas info    : Function properties for _Z16flash_fwd_kernelI23Flash_fwd_kernel_traitsILi224ELi128ELi64ELi8ELb0ELb0EN7cutlass6half_tE19Flash_kernel_traitsILi224ELi128ELi64ELi8ES2_EELb1ELb1ELb0ELb1ELb0EEv16Flash_fwd_params
    0 bytes stack frame, 0 bytes spill stores, 0 bytes spill loads
ptxas info    : Used 251 registers, 584 bytes cmem[0]
ptxas info    : Compiling entry function '_Z16flash_fwd_kernelI23Flash_fwd_kernel_traitsILi224ELi128ELi64ELi8ELb0ELb0EN7cutlass6half_tE19Flash_kernel_traitsILi224ELi128ELi64ELi8ES2_EELb1ELb1ELb0ELb1ELb1EEv16Flash_fwd_params' for 'sm_80'
ptxas info    : Function properties for _Z16flash_fwd_kernelI23Flash_fwd_kernel_traitsILi224ELi128ELi64ELi8ELb0ELb0EN7cutlass6half_tE19Flash_kernel_traitsILi224ELi128ELi64ELi8ES2_EELb1ELb1ELb0ELb1ELb1EEv16Flash_fwd_params
    88 bytes stack frame, 60 bytes spill stores, 78 bytes spill loads
ptxas info    : Used 255 registers, 584 bytes cmem[0]
ptxas info    : Compiling entry function '_Z16flash_fwd_kernelI23Flash_fwd_kernel_traitsILi224ELi128ELi64ELi8ELb0ELb0EN7cutlass6half_tE19Flash_kernel_traitsILi224ELi128ELi64ELi8ES2_EELb1ELb1ELb1ELb0ELb0EEv16Flash_fwd_params' for 'sm_80'
ptxas info    : Function properties for _Z16flash_fwd_kernelI23Flash_fwd_kernel_traitsILi224ELi128ELi64ELi8ELb0ELb0EN7cutlass6half_tE19Flash_kernel_traitsILi224ELi128ELi64ELi8ES2_EELb1ELb1ELb1ELb0ELb0EEv16Flash_fwd_params
    72 bytes stack frame, 84 bytes spill stores, 84 bytes spill loads
ptxas info    : Used 255 registers, 584 bytes cmem[0]
ptxas info    : Compiling entry function '_Z16flash_fwd_kernelI23Flash_fwd_kernel_traitsILi224ELi128ELi64ELi8ELb0ELb0EN7cutlass6half_tE19Flash_kernel_traitsILi224ELi128ELi64ELi8ES2_EELb1ELb1ELb1ELb0ELb1EEv16Flash_fwd_params' for 'sm_80'
ptxas info    : Function properties for _Z16flash_fwd_kernelI23Flash_fwd_kernel_traitsILi224ELi128ELi64ELi8ELb0ELb0EN7cutlass6half_tE19Flash_kernel_traitsILi224ELi128ELi64ELi8ES2_EELb1ELb1ELb1ELb0ELb1EEv16Flash_fwd_params
    56 bytes stack frame, 68 bytes spill stores, 60 bytes spill loads
ptxas info    : Used 255 registers, 584 bytes cmem[0]
ptxas info    : Compiling entry function '_Z16flash_fwd_kernelI23Flash_fwd_kernel_traitsILi224ELi128ELi64ELi8ELb0ELb0EN7cutlass6half_tE19Flash_kernel_traitsILi224ELi128ELi64ELi8ES2_EELb1ELb1ELb1ELb1ELb0EEv16Flash_fwd_params' for 'sm_80'
ptxas info    : Function properties for _Z16flash_fwd_kernelI23Flash_fwd_kernel_traitsILi224ELi128ELi64ELi8ELb0ELb0EN7cutlass6half_tE19Flash_kernel_traitsILi224ELi128ELi64ELi8ES2_EELb1ELb1ELb1ELb1ELb0EEv16Flash_fwd_params
    0 bytes stack frame, 0 bytes spill stores, 0 bytes spill loads
ptxas info    : Used 235 registers, 584 bytes cmem[0]
ptxas info    : Compiling entry function '_Z16flash_fwd_kernelI23Flash_fwd_kernel_traitsILi224ELi128ELi64ELi8ELb0ELb0EN7cutlass6half_tE19Flash_kernel_traitsILi224ELi128ELi64ELi8ES2_EELb1ELb1ELb1ELb1ELb1EEv16Flash_fwd_params' for 'sm_80'
ptxas info    : Function properties for _Z16flash_fwd_kernelI23Flash_fwd_kernel_traitsILi224ELi128ELi64ELi8ELb0ELb0EN7cutlass6half_tE19Flash_kernel_traitsILi224ELi128ELi64ELi8ES2_EELb1ELb1ELb1ELb1ELb1EEv16Flash_fwd_params
    88 bytes stack frame, 64 bytes spill stores, 94 bytes spill loads
ptxas info    : Used 255 registers, 584 bytes cmem[0]
/anaconda/envs/flashattention/lib/python3.9/site-packages/torch/include/c10/core/SymInt.h(84): warning: integer conversion resulted in a change of sign

[23/37] /usr/local/cuda-11.4/bin/nvcc  -I/home/v-feiychen/flash-attention/csrc/flash_attn -I/home/v-feiychen/flash-attention/csrc/flash_attn/src -I/home/v-feiychen/flash-attention/csrc/cutlass/include -I/anaconda/envs/flashattention/lib/python3.9/site-packages/torch/include -I/anaconda/envs/flashattention/lib/python3.9/site-packages/torch/include/torch/csrc/api/include -I/anaconda/envs/flashattention/lib/python3.9/site-packages/torch/include/TH -I/anaconda/envs/flashattention/lib/python3.9/site-packages/torch/include/THC -I/usr/local/cuda-11.4/include -I/anaconda/envs/flashattention/include/python3.9 -c -c /home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_bwd_hdim128_fp16_sm80.cu -o /home/v-feiychen/flash-attention/build/temp.linux-x86_64-cpython-39/csrc/flash_attn/src/flash_bwd_hdim128_fp16_sm80.o -D__CUDA_NO_HALF_OPERATORS__ -D__CUDA_NO_HALF_CONVERSIONS__ -D__CUDA_NO_BFLOAT16_CONVERSIONS__ -D__CUDA_NO_HALF2_OPERATORS__ --expt-relaxed-constexpr --compiler-options ''"'"'-fPIC'"'"'' -O3 -std=c++17 -U__CUDA_NO_HALF_OPERATORS__ -U__CUDA_NO_HALF_CONVERSIONS__ -U__CUDA_NO_HALF2_OPERATORS__ -U__CUDA_NO_BFLOAT16_CONVERSIONS__ --expt-relaxed-constexpr --expt-extended-lambda --use_fast_math --ptxas-options=-v -lineinfo -gencode arch=compute_80,code=sm_80 --threads 4 -DTORCH_API_INCLUDE_EXTENSION_H '-DPYBIND11_COMPILER_TYPE="_gcc"' '-DPYBIND11_STDLIB="_libstdcpp"' '-DPYBIND11_BUILD_ABI="_cxxabi1011"' -DTORCH_EXTENSION_NAME=flash_attn_2_cuda -D_GLIBCXX_USE_CXX11_ABI=0
/anaconda/envs/flashattention/lib/python3.9/site-packages/torch/include/c10/core/SymInt.h(84): warning: integer conversion resulted in a change of sign

ptxas info    : 2 bytes gmem
ptxas info    : Compiling entry function '_Z44flash_bwd_dq_dk_dv_loop_seqk_parallel_kernelI23Flash_bwd_kernel_traitsILi128ELi64ELi64ELi8ELi4ELi2ELi2ELb1ELb0EN7cutlass6half_tE19Flash_kernel_traitsILi128ELi64ELi64ELi8ES2_EELb0ELb0ELb0ELb0EEv16Flash_bwd_params' for 'sm_80'
ptxas info    : Function properties for _Z44flash_bwd_dq_dk_dv_loop_seqk_parallel_kernelI23Flash_bwd_kernel_traitsILi128ELi64ELi64ELi8ELi4ELi2ELi2ELb1ELb0EN7cutlass6half_tE19Flash_kernel_traitsILi128ELi64ELi64ELi8ES2_EELb0ELb0ELb0ELb0EEv16Flash_bwd_params
    0 bytes stack frame, 0 bytes spill stores, 0 bytes spill loads
ptxas info    : Used 242 registers, 696 bytes cmem[0]
ptxas info    : Compiling entry function '_Z44flash_bwd_dq_dk_dv_loop_seqk_parallel_kernelI23Flash_bwd_kernel_traitsILi128ELi64ELi64ELi8ELi4ELi2ELi2ELb1ELb0EN7cutlass6half_tE19Flash_kernel_traitsILi128ELi64ELi64ELi8ES2_EELb0ELb0ELb0ELb1EEv16Flash_bwd_params' for 'sm_80'
ptxas info    : Function properties for _Z44flash_bwd_dq_dk_dv_loop_seqk_parallel_kernelI23Flash_bwd_kernel_traitsILi128ELi64ELi64ELi8ELi4ELi2ELi2ELb1ELb0EN7cutlass6half_tE19Flash_kernel_traitsILi128ELi64ELi64ELi8ES2_EELb0ELb0ELb0ELb1EEv16Flash_bwd_params
    0 bytes stack frame, 0 bytes spill stores, 0 bytes spill loads
ptxas info    : Used 241 registers, 696 bytes cmem[0]
ptxas info    : Compiling entry function '_Z44flash_bwd_dq_dk_dv_loop_seqk_parallel_kernelI23Flash_bwd_kernel_traitsILi128ELi64ELi64ELi8ELi4ELi2ELi2ELb1ELb0EN7cutlass6half_tE19Flash_kernel_traitsILi128ELi64ELi64ELi8ES2_EELb0ELb0ELb1ELb0EEv16Flash_bwd_params' for 'sm_80'
ptxas info    : Function properties for _Z44flash_bwd_dq_dk_dv_loop_seqk_parallel_kernelI23Flash_bwd_kernel_traitsILi128ELi64ELi64ELi8ELi4ELi2ELi2ELb1ELb0EN7cutlass6half_tE19Flash_kernel_traitsILi128ELi64ELi64ELi8ES2_EELb0ELb0ELb1ELb0EEv16Flash_bwd_params
    0 bytes stack frame, 0 bytes spill stores, 0 bytes spill loads
ptxas info    : Used 239 registers, 696 bytes cmem[0]
ptxas info    : Compiling entry function '_Z44flash_bwd_dq_dk_dv_loop_seqk_parallel_kernelI23Flash_bwd_kernel_traitsILi128ELi64ELi64ELi8ELi4ELi2ELi2ELb1ELb0EN7cutlass6half_tE19Flash_kernel_traitsILi128ELi64ELi64ELi8ES2_EELb0ELb0ELb1ELb1EEv16Flash_bwd_params' for 'sm_80'
ptxas info    : Function properties for _Z44flash_bwd_dq_dk_dv_loop_seqk_parallel_kernelI23Flash_bwd_kernel_traitsILi128ELi64ELi64ELi8ELi4ELi2ELi2ELb1ELb0EN7cutlass6half_tE19Flash_kernel_traitsILi128ELi64ELi64ELi8ES2_EELb0ELb0ELb1ELb1EEv16Flash_bwd_params
    0 bytes stack frame, 0 bytes spill stores, 0 bytes spill loads
ptxas info    : Used 236 registers, 696 bytes cmem[0]
ptxas info    : Compiling entry function '_Z44flash_bwd_dq_dk_dv_loop_seqk_parallel_kernelI23Flash_bwd_kernel_traitsILi128ELi64ELi64ELi8ELi4ELi2ELi2ELb1ELb0EN7cutlass6half_tE19Flash_kernel_traitsILi128ELi64ELi64ELi8ES2_EELb0ELb1ELb0ELb0EEv16Flash_bwd_params' for 'sm_80'
ptxas info    : Function properties for _Z44flash_bwd_dq_dk_dv_loop_seqk_parallel_kernelI23Flash_bwd_kernel_traitsILi128ELi64ELi64ELi8ELi4ELi2ELi2ELb1ELb0EN7cutlass6half_tE19Flash_kernel_traitsILi128ELi64ELi64ELi8ES2_EELb0ELb1ELb0ELb0EEv16Flash_bwd_params
    0 bytes stack frame, 0 bytes spill stores, 0 bytes spill loads
ptxas info    : Used 240 registers, 696 bytes cmem[0]
ptxas info    : Compiling entry function '_Z44flash_bwd_dq_dk_dv_loop_seqk_parallel_kernelI23Flash_bwd_kernel_traitsILi128ELi64ELi64ELi8ELi4ELi2ELi2ELb1ELb0EN7cutlass6half_tE19Flash_kernel_traitsILi128ELi64ELi64ELi8ES2_EELb0ELb1ELb0ELb1EEv16Flash_bwd_params' for 'sm_80'
ptxas info    : Function properties for _Z44flash_bwd_dq_dk_dv_loop_seqk_parallel_kernelI23Flash_bwd_kernel_traitsILi128ELi64ELi64ELi8ELi4ELi2ELi2ELb1ELb0EN7cutlass6half_tE19Flash_kernel_traitsILi128ELi64ELi64ELi8ES2_EELb0ELb1ELb0ELb1EEv16Flash_bwd_params
    0 bytes stack frame, 0 bytes spill stores, 0 bytes spill loads
ptxas info    : Used 240 registers, 696 bytes cmem[0]
ptxas info    : Compiling entry function '_Z44flash_bwd_dq_dk_dv_loop_seqk_parallel_kernelI23Flash_bwd_kernel_traitsILi128ELi64ELi64ELi8ELi4ELi2ELi2ELb1ELb0EN7cutlass6half_tE19Flash_kernel_traitsILi128ELi64ELi64ELi8ES2_EELb0ELb1ELb1ELb0EEv16Flash_bwd_params' for 'sm_80'
ptxas info    : Function properties for _Z44flash_bwd_dq_dk_dv_loop_seqk_parallel_kernelI23Flash_bwd_kernel_traitsILi128ELi64ELi64ELi8ELi4ELi2ELi2ELb1ELb0EN7cutlass6half_tE19Flash_kernel_traitsILi128ELi64ELi64ELi8ES2_EELb0ELb1ELb1ELb0EEv16Flash_bwd_params
    0 bytes stack frame, 0 bytes spill stores, 0 bytes spill loads
ptxas info    : Used 238 registers, 696 bytes cmem[0]
ptxas info    : Compiling entry function '_Z44flash_bwd_dq_dk_dv_loop_seqk_parallel_kernelI23Flash_bwd_kernel_traitsILi128ELi64ELi64ELi8ELi4ELi2ELi2ELb1ELb0EN7cutlass6half_tE19Flash_kernel_traitsILi128ELi64ELi64ELi8ES2_EELb0ELb1ELb1ELb1EEv16Flash_bwd_params' for 'sm_80'
ptxas info    : Function properties for _Z44flash_bwd_dq_dk_dv_loop_seqk_parallel_kernelI23Flash_bwd_kernel_traitsILi128ELi64ELi64ELi8ELi4ELi2ELi2ELb1ELb0EN7cutlass6half_tE19Flash_kernel_traitsILi128ELi64ELi64ELi8ES2_EELb0ELb1ELb1ELb1EEv16Flash_bwd_params
    0 bytes stack frame, 0 bytes spill stores, 0 bytes spill loads
ptxas info    : Used 240 registers, 696 bytes cmem[0]
ptxas info    : Compiling entry function '_Z44flash_bwd_dq_dk_dv_loop_seqk_parallel_kernelI23Flash_bwd_kernel_traitsILi128ELi64ELi128ELi8ELi2ELi4ELi2ELb0ELb0EN7cutlass6half_tE19Flash_kernel_traitsILi128ELi64ELi128ELi8ES2_EELb0ELb0ELb0ELb0EEv16Flash_bwd_params' for 'sm_80'
ptxas info    : Function properties for _Z44flash_bwd_dq_dk_dv_loop_seqk_parallel_kernelI23Flash_bwd_kernel_traitsILi128ELi64ELi128ELi8ELi2ELi4ELi2ELb0ELb0EN7cutlass6half_tE19Flash_kernel_traitsILi128ELi64ELi128ELi8ES2_EELb0ELb0ELb0ELb0EEv16Flash_bwd_params
    136 bytes stack frame, 164 bytes spill stores, 152 bytes spill loads
ptxas info    : Used 255 registers, 696 bytes cmem[0]
ptxas info    : Compiling entry function '_Z44flash_bwd_dq_dk_dv_loop_seqk_parallel_kernelI23Flash_bwd_kernel_traitsILi128ELi64ELi128ELi8ELi2ELi4ELi2ELb0ELb0EN7cutlass6half_tE19Flash_kernel_traitsILi128ELi64ELi128ELi8ES2_EELb0ELb0ELb0ELb1EEv16Flash_bwd_params' for 'sm_80'
ptxas info    : Function properties for _Z44flash_bwd_dq_dk_dv_loop_seqk_parallel_kernelI23Flash_bwd_kernel_traitsILi128ELi64ELi128ELi8ELi2ELi4ELi2ELb0ELb0EN7cutlass6half_tE19Flash_kernel_traitsILi128ELi64ELi128ELi8ES2_EELb0ELb0ELb0ELb1EEv16Flash_bwd_params
    136 bytes stack frame, 168 bytes spill stores, 180 bytes spill loads
ptxas info    : Used 255 registers, 696 bytes cmem[0]
ptxas info    : Compiling entry function '_Z44flash_bwd_dq_dk_dv_loop_seqk_parallel_kernelI23Flash_bwd_kernel_traitsILi128ELi64ELi128ELi8ELi2ELi4ELi2ELb0ELb0EN7cutlass6half_tE19Flash_kernel_traitsILi128ELi64ELi128ELi8ES2_EELb0ELb0ELb1ELb0EEv16Flash_bwd_params' for 'sm_80'
ptxas info    : Function properties for _Z44flash_bwd_dq_dk_dv_loop_seqk_parallel_kernelI23Flash_bwd_kernel_traitsILi128ELi64ELi128ELi8ELi2ELi4ELi2ELb0ELb0EN7cutlass6half_tE19Flash_kernel_traitsILi128ELi64ELi128ELi8ES2_EELb0ELb0ELb1ELb0EEv16Flash_bwd_params
    120 bytes stack frame, 156 bytes spill stores, 168 bytes spill loads
ptxas info    : Used 255 registers, 696 bytes cmem[0]
ptxas info    : Compiling entry function '_Z44flash_bwd_dq_dk_dv_loop_seqk_parallel_kernelI23Flash_bwd_kernel_traitsILi128ELi64ELi128ELi8ELi2ELi4ELi2ELb0ELb0EN7cutlass6half_tE19Flash_kernel_traitsILi128ELi64ELi128ELi8ES2_EELb0ELb0ELb1ELb1EEv16Flash_bwd_params' for 'sm_80'
ptxas info    : Function properties for _Z44flash_bwd_dq_dk_dv_loop_seqk_parallel_kernelI23Flash_bwd_kernel_traitsILi128ELi64ELi128ELi8ELi2ELi4ELi2ELb0ELb0EN7cutlass6half_tE19Flash_kernel_traitsILi128ELi64ELi128ELi8ES2_EELb0ELb0ELb1ELb1EEv16Flash_bwd_params
    136 bytes stack frame, 160 bytes spill stores, 176 bytes spill loads
ptxas info    : Used 255 registers, 696 bytes cmem[0]
ptxas info    : Compiling entry function '_Z44flash_bwd_dq_dk_dv_loop_seqk_parallel_kernelI23Flash_bwd_kernel_traitsILi128ELi64ELi128ELi8ELi2ELi4ELi2ELb0ELb0EN7cutlass6half_tE19Flash_kernel_traitsILi128ELi64ELi128ELi8ES2_EELb0ELb1ELb0ELb0EEv16Flash_bwd_params' for 'sm_80'
ptxas info    : Function properties for _Z44flash_bwd_dq_dk_dv_loop_seqk_parallel_kernelI23Flash_bwd_kernel_traitsILi128ELi64ELi128ELi8ELi2ELi4ELi2ELb0ELb0EN7cutlass6half_tE19Flash_kernel_traitsILi128ELi64ELi128ELi8ES2_EELb0ELb1ELb0ELb0EEv16Flash_bwd_params
    120 bytes stack frame, 144 bytes spill stores, 132 bytes spill loads
ptxas info    : Used 255 registers, 696 bytes cmem[0]
ptxas info    : Compiling entry function '_Z44flash_bwd_dq_dk_dv_loop_seqk_parallel_kernelI23Flash_bwd_kernel_traitsILi128ELi64ELi128ELi8ELi2ELi4ELi2ELb0ELb0EN7cutlass6half_tE19Flash_kernel_traitsILi128ELi64ELi128ELi8ES2_EELb0ELb1ELb0ELb1EEv16Flash_bwd_params' for 'sm_80'
ptxas info    : Function properties for _Z44flash_bwd_dq_dk_dv_loop_seqk_parallel_kernelI23Flash_bwd_kernel_traitsILi128ELi64ELi128ELi8ELi2ELi4ELi2ELb0ELb0EN7cutlass6half_tE19Flash_kernel_traitsILi128ELi64ELi128ELi8ES2_EELb0ELb1ELb0ELb1EEv16Flash_bwd_params
    144 bytes stack frame, 168 bytes spill stores, 180 bytes spill loads
ptxas info    : Used 255 registers, 696 bytes cmem[0]
ptxas info    : Compiling entry function '_Z44flash_bwd_dq_dk_dv_loop_seqk_parallel_kernelI23Flash_bwd_kernel_traitsILi128ELi64ELi128ELi8ELi2ELi4ELi2ELb0ELb0EN7cutlass6half_tE19Flash_kernel_traitsILi128ELi64ELi128ELi8ES2_EELb0ELb1ELb1ELb0EEv16Flash_bwd_params' for 'sm_80'
ptxas info    : Function properties for _Z44flash_bwd_dq_dk_dv_loop_seqk_parallel_kernelI23Flash_bwd_kernel_traitsILi128ELi64ELi128ELi8ELi2ELi4ELi2ELb0ELb0EN7cutlass6half_tE19Flash_kernel_traitsILi128ELi64ELi128ELi8ES2_EELb0ELb1ELb1ELb0EEv16Flash_bwd_params
    136 bytes stack frame, 156 bytes spill stores, 152 bytes spill loads
ptxas info    : Used 255 registers, 696 bytes cmem[0]
ptxas info    : Compiling entry function '_Z44flash_bwd_dq_dk_dv_loop_seqk_parallel_kernelI23Flash_bwd_kernel_traitsILi128ELi64ELi128ELi8ELi2ELi4ELi2ELb0ELb0EN7cutlass6half_tE19Flash_kernel_traitsILi128ELi64ELi128ELi8ES2_EELb0ELb1ELb1ELb1EEv16Flash_bwd_params' for 'sm_80'
ptxas info    : Function properties for _Z44flash_bwd_dq_dk_dv_loop_seqk_parallel_kernelI23Flash_bwd_kernel_traitsILi128ELi64ELi128ELi8ELi2ELi4ELi2ELb0ELb0EN7cutlass6half_tE19Flash_kernel_traitsILi128ELi64ELi128ELi8ES2_EELb0ELb1ELb1ELb1EEv16Flash_bwd_params
    144 bytes stack frame, 164 bytes spill stores, 156 bytes spill loads
ptxas info    : Used 255 registers, 696 bytes cmem[0]
ptxas info    : Compiling entry function '_Z27flash_bwd_convert_dq_kernelI23Flash_bwd_kernel_traitsILi128ELi64ELi64ELi8ELi4ELi2ELi2ELb1ELb0EN7cutlass6half_tE19Flash_kernel_traitsILi128ELi64ELi64ELi8ES2_EEEv16Flash_bwd_params' for 'sm_80'
ptxas info    : Function properties for _Z27flash_bwd_convert_dq_kernelI23Flash_bwd_kernel_traitsILi128ELi64ELi64ELi8ELi4ELi2ELi2ELb1ELb0EN7cutlass6half_tE19Flash_kernel_traitsILi128ELi64ELi64ELi8ES2_EEEv16Flash_bwd_params
    0 bytes stack frame, 0 bytes spill stores, 0 bytes spill loads
ptxas info    : Used 32 registers, 696 bytes cmem[0]
ptxas info    : Compiling entry function '_Z44flash_bwd_dq_dk_dv_loop_seqk_parallel_kernelI23Flash_bwd_kernel_traitsILi128ELi64ELi64ELi8ELi4ELi2ELi2ELb1ELb0EN7cutlass6half_tE19Flash_kernel_traitsILi128ELi64ELi64ELi8ES2_EELb1ELb0ELb0ELb0EEv16Flash_bwd_params' for 'sm_80'
ptxas info    : Function properties for _Z44flash_bwd_dq_dk_dv_loop_seqk_parallel_kernelI23Flash_bwd_kernel_traitsILi128ELi64ELi64ELi8ELi4ELi2ELi2ELb1ELb0EN7cutlass6half_tE19Flash_kernel_traitsILi128ELi64ELi64ELi8ES2_EELb1ELb0ELb0ELb0EEv16Flash_bwd_params
    0 bytes stack frame, 0 bytes spill stores, 0 bytes spill loads
ptxas info    : Used 239 registers, 696 bytes cmem[0]
ptxas info    : Compiling entry function '_Z44flash_bwd_dq_dk_dv_loop_seqk_parallel_kernelI23Flash_bwd_kernel_traitsILi128ELi64ELi64ELi8ELi4ELi2ELi2ELb1ELb0EN7cutlass6half_tE19Flash_kernel_traitsILi128ELi64ELi64ELi8ES2_EELb1ELb0ELb0ELb1EEv16Flash_bwd_params' for 'sm_80'
ptxas info    : Function properties for _Z44flash_bwd_dq_dk_dv_loop_seqk_parallel_kernelI23Flash_bwd_kernel_traitsILi128ELi64ELi64ELi8ELi4ELi2ELi2ELb1ELb0EN7cutlass6half_tE19Flash_kernel_traitsILi128ELi64ELi64ELi8ES2_EELb1ELb0ELb0ELb1EEv16Flash_bwd_params
    0 bytes stack frame, 0 bytes spill stores, 0 bytes spill loads
ptxas info    : Used 242 registers, 696 bytes cmem[0]
ptxas info    : Compiling entry function '_Z44flash_bwd_dq_dk_dv_loop_seqk_parallel_kernelI23Flash_bwd_kernel_traitsILi128ELi64ELi64ELi8ELi4ELi2ELi2ELb1ELb0EN7cutlass6half_tE19Flash_kernel_traitsILi128ELi64ELi64ELi8ES2_EELb1ELb0ELb1ELb0EEv16Flash_bwd_params' for 'sm_80'
ptxas info    : Function properties for _Z44flash_bwd_dq_dk_dv_loop_seqk_parallel_kernelI23Flash_bwd_kernel_traitsILi128ELi64ELi64ELi8ELi4ELi2ELi2ELb1ELb0EN7cutlass6half_tE19Flash_kernel_traitsILi128ELi64ELi64ELi8ES2_EELb1ELb0ELb1ELb0EEv16Flash_bwd_params
    0 bytes stack frame, 0 bytes spill stores, 0 bytes spill loads
ptxas info    : Used 240 registers, 696 bytes cmem[0]
ptxas info    : Compiling entry function '_Z44flash_bwd_dq_dk_dv_loop_seqk_parallel_kernelI23Flash_bwd_kernel_traitsILi128ELi64ELi64ELi8ELi4ELi2ELi2ELb1ELb0EN7cutlass6half_tE19Flash_kernel_traitsILi128ELi64ELi64ELi8ES2_EELb1ELb0ELb1ELb1EEv16Flash_bwd_params' for 'sm_80'
ptxas info    : Function properties for _Z44flash_bwd_dq_dk_dv_loop_seqk_parallel_kernelI23Flash_bwd_kernel_traitsILi128ELi64ELi64ELi8ELi4ELi2ELi2ELb1ELb0EN7cutlass6half_tE19Flash_kernel_traitsILi128ELi64ELi64ELi8ES2_EELb1ELb0ELb1ELb1EEv16Flash_bwd_params
    0 bytes stack frame, 0 bytes spill stores, 0 bytes spill loads
ptxas info    : Used 240 registers, 696 bytes cmem[0]
ptxas info    : Compiling entry function '_Z44flash_bwd_dq_dk_dv_loop_seqk_parallel_kernelI23Flash_bwd_kernel_traitsILi128ELi64ELi64ELi8ELi4ELi2ELi2ELb1ELb0EN7cutlass6half_tE19Flash_kernel_traitsILi128ELi64ELi64ELi8ES2_EELb1ELb1ELb0ELb0EEv16Flash_bwd_params' for 'sm_80'
ptxas info    : Function properties for _Z44flash_bwd_dq_dk_dv_loop_seqk_parallel_kernelI23Flash_bwd_kernel_traitsILi128ELi64ELi64ELi8ELi4ELi2ELi2ELb1ELb0EN7cutlass6half_tE19Flash_kernel_traitsILi128ELi64ELi64ELi8ES2_EELb1ELb1ELb0ELb0EEv16Flash_bwd_params
    0 bytes stack frame, 0 bytes spill stores, 0 bytes spill loads
ptxas info    : Used 244 registers, 696 bytes cmem[0]
ptxas info    : Compiling entry function '_Z44flash_bwd_dq_dk_dv_loop_seqk_parallel_kernelI23Flash_bwd_kernel_traitsILi128ELi64ELi64ELi8ELi4ELi2ELi2ELb1ELb0EN7cutlass6half_tE19Flash_kernel_traitsILi128ELi64ELi64ELi8ES2_EELb1ELb1ELb0ELb1EEv16Flash_bwd_params' for 'sm_80'
ptxas info    : Function properties for _Z44flash_bwd_dq_dk_dv_loop_seqk_parallel_kernelI23Flash_bwd_kernel_traitsILi128ELi64ELi64ELi8ELi4ELi2ELi2ELb1ELb0EN7cutlass6half_tE19Flash_kernel_traitsILi128ELi64ELi64ELi8ES2_EELb1ELb1ELb0ELb1EEv16Flash_bwd_params
    0 bytes stack frame, 0 bytes spill stores, 0 bytes spill loads
ptxas info    : Used 241 registers, 696 bytes cmem[0]
ptxas info    : Compiling entry function '_Z44flash_bwd_dq_dk_dv_loop_seqk_parallel_kernelI23Flash_bwd_kernel_traitsILi128ELi64ELi64ELi8ELi4ELi2ELi2ELb1ELb0EN7cutlass6half_tE19Flash_kernel_traitsILi128ELi64ELi64ELi8ES2_EELb1ELb1ELb1ELb0EEv16Flash_bwd_params' for 'sm_80'
ptxas info    : Function properties for _Z44flash_bwd_dq_dk_dv_loop_seqk_parallel_kernelI23Flash_bwd_kernel_traitsILi128ELi64ELi64ELi8ELi4ELi2ELi2ELb1ELb0EN7cutlass6half_tE19Flash_kernel_traitsILi128ELi64ELi64ELi8ES2_EELb1ELb1ELb1ELb0EEv16Flash_bwd_params
    0 bytes stack frame, 0 bytes spill stores, 0 bytes spill loads
ptxas info    : Used 239 registers, 696 bytes cmem[0]
ptxas info    : Compiling entry function '_Z44flash_bwd_dq_dk_dv_loop_seqk_parallel_kernelI23Flash_bwd_kernel_traitsILi128ELi64ELi64ELi8ELi4ELi2ELi2ELb1ELb0EN7cutlass6half_tE19Flash_kernel_traitsILi128ELi64ELi64ELi8ES2_EELb1ELb1ELb1ELb1EEv16Flash_bwd_params' for 'sm_80'
ptxas info    : Function properties for _Z44flash_bwd_dq_dk_dv_loop_seqk_parallel_kernelI23Flash_bwd_kernel_traitsILi128ELi64ELi64ELi8ELi4ELi2ELi2ELb1ELb0EN7cutlass6half_tE19Flash_kernel_traitsILi128ELi64ELi64ELi8ES2_EELb1ELb1ELb1ELb1EEv16Flash_bwd_params
    0 bytes stack frame, 0 bytes spill stores, 0 bytes spill loads
ptxas info    : Used 240 registers, 696 bytes cmem[0]
ptxas info    : Compiling entry function '_Z25flash_bwd_dot_do_o_kernelILb1E23Flash_bwd_kernel_traitsILi128ELi64ELi64ELi8ELi4ELi2ELi2ELb1ELb0EN7cutlass6half_tE19Flash_kernel_traitsILi128ELi64ELi64ELi8ES2_EEEv16Flash_bwd_params' for 'sm_80'
ptxas info    : Function properties for _Z25flash_bwd_dot_do_o_kernelILb1E23Flash_bwd_kernel_traitsILi128ELi64ELi64ELi8ELi4ELi2ELi2ELb1ELb0EN7cutlass6half_tE19Flash_kernel_traitsILi128ELi64ELi64ELi8ES2_EEEv16Flash_bwd_params
    0 bytes stack frame, 0 bytes spill stores, 0 bytes spill loads
ptxas info    : Used 47 registers, 696 bytes cmem[0]
ptxas info    : Compiling entry function '_Z27flash_bwd_convert_dq_kernelI23Flash_bwd_kernel_traitsILi128ELi64ELi128ELi8ELi2ELi4ELi2ELb0ELb0EN7cutlass6half_tE19Flash_kernel_traitsILi128ELi64ELi128ELi8ES2_EEEv16Flash_bwd_params' for 'sm_80'
ptxas info    : Function properties for _Z27flash_bwd_convert_dq_kernelI23Flash_bwd_kernel_traitsILi128ELi64ELi128ELi8ELi2ELi4ELi2ELb0ELb0EN7cutlass6half_tE19Flash_kernel_traitsILi128ELi64ELi128ELi8ES2_EEEv16Flash_bwd_params
    0 bytes stack frame, 0 bytes spill stores, 0 bytes spill loads
ptxas info    : Used 32 registers, 696 bytes cmem[0]
ptxas info    : Compiling entry function '_Z44flash_bwd_dq_dk_dv_loop_seqk_parallel_kernelI23Flash_bwd_kernel_traitsILi128ELi64ELi128ELi8ELi2ELi4ELi2ELb0ELb0EN7cutlass6half_tE19Flash_kernel_traitsILi128ELi64ELi128ELi8ES2_EELb1ELb0ELb0ELb0EEv16Flash_bwd_params' for 'sm_80'
ptxas info    : Function properties for _Z44flash_bwd_dq_dk_dv_loop_seqk_parallel_kernelI23Flash_bwd_kernel_traitsILi128ELi64ELi128ELi8ELi2ELi4ELi2ELb0ELb0EN7cutlass6half_tE19Flash_kernel_traitsILi128ELi64ELi128ELi8ES2_EELb1ELb0ELb0ELb0EEv16Flash_bwd_params
    136 bytes stack frame, 168 bytes spill stores, 184 bytes spill loads
ptxas info    : Used 255 registers, 696 bytes cmem[0]
ptxas info    : Compiling entry function '_Z44flash_bwd_dq_dk_dv_loop_seqk_parallel_kernelI23Flash_bwd_kernel_traitsILi128ELi64ELi128ELi8ELi2ELi4ELi2ELb0ELb0EN7cutlass6half_tE19Flash_kernel_traitsILi128ELi64ELi128ELi8ES2_EELb1ELb0ELb0ELb1EEv16Flash_bwd_params' for 'sm_80'
ptxas info    : Function properties for _Z44flash_bwd_dq_dk_dv_loop_seqk_parallel_kernelI23Flash_bwd_kernel_traitsILi128ELi64ELi128ELi8ELi2ELi4ELi2ELb0ELb0EN7cutlass6half_tE19Flash_kernel_traitsILi128ELi64ELi128ELi8ES2_EELb1ELb0ELb0ELb1EEv16Flash_bwd_params
    128 bytes stack frame, 160 bytes spill stores, 176 bytes spill loads
ptxas info    : Used 255 registers, 696 bytes cmem[0]
ptxas info    : Compiling entry function '_Z44flash_bwd_dq_dk_dv_loop_seqk_parallel_kernelI23Flash_bwd_kernel_traitsILi128ELi64ELi128ELi8ELi2ELi4ELi2ELb0ELb0EN7cutlass6half_tE19Flash_kernel_traitsILi128ELi64ELi128ELi8ES2_EELb1ELb0ELb1ELb0EEv16Flash_bwd_params' for 'sm_80'
ptxas info    : Function properties for _Z44flash_bwd_dq_dk_dv_loop_seqk_parallel_kernelI23Flash_bwd_kernel_traitsILi128ELi64ELi128ELi8ELi2ELi4ELi2ELb0ELb0EN7cutlass6half_tE19Flash_kernel_traitsILi128ELi64ELi128ELi8ES2_EELb1ELb0ELb1ELb0EEv16Flash_bwd_params
    112 bytes stack frame, 140 bytes spill stores, 164 bytes spill loads
ptxas info    : Used 255 registers, 696 bytes cmem[0]
ptxas info    : Compiling entry function '_Z44flash_bwd_dq_dk_dv_loop_seqk_parallel_kernelI23Flash_bwd_kernel_traitsILi128ELi64ELi128ELi8ELi2ELi4ELi2ELb0ELb0EN7cutlass6half_tE19Flash_kernel_traitsILi128ELi64ELi128ELi8ES2_EELb1ELb0ELb1ELb1EEv16Flash_bwd_params' for 'sm_80'
ptxas info    : Function properties for _Z44flash_bwd_dq_dk_dv_loop_seqk_parallel_kernelI23Flash_bwd_kernel_traitsILi128ELi64ELi128ELi8ELi2ELi4ELi2ELb0ELb0EN7cutlass6half_tE19Flash_kernel_traitsILi128ELi64ELi128ELi8ES2_EELb1ELb0ELb1ELb1EEv16Flash_bwd_params
    0 bytes stack frame, 0 bytes spill stores, 0 bytes spill loads
ptxas info    : Used 255 registers, 696 bytes cmem[0]
ptxas info    : Compiling entry function '_Z44flash_bwd_dq_dk_dv_loop_seqk_parallel_kernelI23Flash_bwd_kernel_traitsILi128ELi64ELi128ELi8ELi2ELi4ELi2ELb0ELb0EN7cutlass6half_tE19Flash_kernel_traitsILi128ELi64ELi128ELi8ES2_EELb1ELb1ELb0ELb0EEv16Flash_bwd_params' for 'sm_80'
ptxas info    : Function properties for _Z44flash_bwd_dq_dk_dv_loop_seqk_parallel_kernelI23Flash_bwd_kernel_traitsILi128ELi64ELi128ELi8ELi2ELi4ELi2ELb0ELb0EN7cutlass6half_tE19Flash_kernel_traitsILi128ELi64ELi128ELi8ES2_EELb1ELb1ELb0ELb0EEv16Flash_bwd_params
    24 bytes stack frame, 24 bytes spill stores, 24 bytes spill loads
ptxas info    : Used 255 registers, 696 bytes cmem[0]
ptxas info    : Compiling entry function '_Z44flash_bwd_dq_dk_dv_loop_seqk_parallel_kernelI23Flash_bwd_kernel_traitsILi128ELi64ELi128ELi8ELi2ELi4ELi2ELb0ELb0EN7cutlass6half_tE19Flash_kernel_traitsILi128ELi64ELi128ELi8ES2_EELb1ELb1ELb0ELb1EEv16Flash_bwd_params' for 'sm_80'
ptxas info    : Function properties for _Z44flash_bwd_dq_dk_dv_loop_seqk_parallel_kernelI23Flash_bwd_kernel_traitsILi128ELi64ELi128ELi8ELi2ELi4ELi2ELb0ELb0EN7cutlass6half_tE19Flash_kernel_traitsILi128ELi64ELi128ELi8ES2_EELb1ELb1ELb0ELb1EEv16Flash_bwd_params
    16 bytes stack frame, 16 bytes spill stores, 16 bytes spill loads
ptxas info    : Used 255 registers, 696 bytes cmem[0]
ptxas info    : Compiling entry function '_Z44flash_bwd_dq_dk_dv_loop_seqk_parallel_kernelI23Flash_bwd_kernel_traitsILi128ELi64ELi128ELi8ELi2ELi4ELi2ELb0ELb0EN7cutlass6half_tE19Flash_kernel_traitsILi128ELi64ELi128ELi8ES2_EELb1ELb1ELb1ELb0EEv16Flash_bwd_params' for 'sm_80'
ptxas info    : Function properties for _Z44flash_bwd_dq_dk_dv_loop_seqk_parallel_kernelI23Flash_bwd_kernel_traitsILi128ELi64ELi128ELi8ELi2ELi4ELi2ELb0ELb0EN7cutlass6half_tE19Flash_kernel_traitsILi128ELi64ELi128ELi8ES2_EELb1ELb1ELb1ELb0EEv16Flash_bwd_params
    40 bytes stack frame, 48 bytes spill stores, 40 bytes spill loads
ptxas info    : Used 255 registers, 696 bytes cmem[0]
ptxas info    : Compiling entry function '_Z44flash_bwd_dq_dk_dv_loop_seqk_parallel_kernelI23Flash_bwd_kernel_traitsILi128ELi64ELi128ELi8ELi2ELi4ELi2ELb0ELb0EN7cutlass6half_tE19Flash_kernel_traitsILi128ELi64ELi128ELi8ES2_EELb1ELb1ELb1ELb1EEv16Flash_bwd_params' for 'sm_80'
ptxas info    : Function properties for _Z44flash_bwd_dq_dk_dv_loop_seqk_parallel_kernelI23Flash_bwd_kernel_traitsILi128ELi64ELi128ELi8ELi2ELi4ELi2ELb0ELb0EN7cutlass6half_tE19Flash_kernel_traitsILi128ELi64ELi128ELi8ES2_EELb1ELb1ELb1ELb1EEv16Flash_bwd_params
    16 bytes stack frame, 12 bytes spill stores, 12 bytes spill loads
ptxas info    : Used 255 registers, 696 bytes cmem[0]
ptxas info    : Compiling entry function '_Z25flash_bwd_dot_do_o_kernelILb1E23Flash_bwd_kernel_traitsILi128ELi64ELi128ELi8ELi2ELi4ELi2ELb0ELb0EN7cutlass6half_tE19Flash_kernel_traitsILi128ELi64ELi128ELi8ES2_EEEv16Flash_bwd_params' for 'sm_80'
ptxas info    : Function properties for _Z25flash_bwd_dot_do_o_kernelILb1E23Flash_bwd_kernel_traitsILi128ELi64ELi128ELi8ELi2ELi4ELi2ELb0ELb0EN7cutlass6half_tE19Flash_kernel_traitsILi128ELi64ELi128ELi8ES2_EEEv16Flash_bwd_params
    0 bytes stack frame, 0 bytes spill stores, 0 bytes spill loads
ptxas info    : Used 47 registers, 696 bytes cmem[0]
/anaconda/envs/flashattention/lib/python3.9/site-packages/torch/include/c10/core/SymInt.h(84): warning: integer conversion resulted in a change of sign

[24/37] /usr/local/cuda-11.4/bin/nvcc  -I/home/v-feiychen/flash-attention/csrc/flash_attn -I/home/v-feiychen/flash-attention/csrc/flash_attn/src -I/home/v-feiychen/flash-attention/csrc/cutlass/include -I/anaconda/envs/flashattention/lib/python3.9/site-packages/torch/include -I/anaconda/envs/flashattention/lib/python3.9/site-packages/torch/include/torch/csrc/api/include -I/anaconda/envs/flashattention/lib/python3.9/site-packages/torch/include/TH -I/anaconda/envs/flashattention/lib/python3.9/site-packages/torch/include/THC -I/usr/local/cuda-11.4/include -I/anaconda/envs/flashattention/include/python3.9 -c -c /home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_hdim32_bf16_sm80.cu -o /home/v-feiychen/flash-attention/build/temp.linux-x86_64-cpython-39/csrc/flash_attn/src/flash_fwd_hdim32_bf16_sm80.o -D__CUDA_NO_HALF_OPERATORS__ -D__CUDA_NO_HALF_CONVERSIONS__ -D__CUDA_NO_BFLOAT16_CONVERSIONS__ -D__CUDA_NO_HALF2_OPERATORS__ --expt-relaxed-constexpr --compiler-options ''"'"'-fPIC'"'"'' -O3 -std=c++17 -U__CUDA_NO_HALF_OPERATORS__ -U__CUDA_NO_HALF_CONVERSIONS__ -U__CUDA_NO_HALF2_OPERATORS__ -U__CUDA_NO_BFLOAT16_CONVERSIONS__ --expt-relaxed-constexpr --expt-extended-lambda --use_fast_math --ptxas-options=-v -lineinfo -gencode arch=compute_80,code=sm_80 --threads 4 -DTORCH_API_INCLUDE_EXTENSION_H '-DPYBIND11_COMPILER_TYPE="_gcc"' '-DPYBIND11_STDLIB="_libstdcpp"' '-DPYBIND11_BUILD_ABI="_cxxabi1011"' -DTORCH_EXTENSION_NAME=flash_attn_2_cuda -D_GLIBCXX_USE_CXX11_ABI=0
/anaconda/envs/flashattention/lib/python3.9/site-packages/torch/include/c10/core/SymInt.h(84): warning: integer conversion resulted in a change of sign

/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_kernel.h(140): warning: variable "MMA_M" was declared but never referenced
          detected during:
            instantiation of "void flash::compute_attn_1rowblock<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params,Is_Q_split_K>(const Params &, int, int, int) [with Kernel_traits=Flash_fwd_kernel_traits<32, 128, 128, 4, false, false, cutlass::bfloat16_t, Flash_kernel_traits<32, 128, 128, 4, cutlass::bfloat16_t>>, Is_dropout=true, Is_causal=true, Is_even_N=true, Is_even_K=true, Return_softmax=true, Params=Flash_fwd_params, Is_Q_split_K=false]" 
(589): here
            instantiation of "void flash::compute_attn<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &) [with Kernel_traits=Flash_fwd_kernel_traits<32, 128, 128, 4, false, false, cutlass::bfloat16_t, Flash_kernel_traits<32, 128, 128, 4, cutlass::bfloat16_t>>, Is_dropout=true, Is_causal=true, Is_even_N=true, Is_even_K=true, Return_softmax=true, Params=Flash_fwd_params]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(15): here
            instantiation of "void flash_fwd_kernel<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax>(Flash_fwd_params) [with Kernel_traits=Flash_fwd_kernel_traits<32, 128, 128, 4, false, false, cutlass::bfloat16_t, Flash_kernel_traits<32, 128, 128, 4, cutlass::bfloat16_t>>, Is_dropout=true, Is_causal=true, Is_even_N=true, Is_even_K=true, Return_softmax=true]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(34): here
            instantiation of "void run_flash_fwd<Kernel_traits,Is_dropout,Is_causal>(Flash_fwd_params &, cudaStream_t) [with Kernel_traits=Flash_fwd_kernel_traits<32, 128, 128, 4, false, false, cutlass::bfloat16_t, Flash_kernel_traits<32, 128, 128, 4, cutlass::bfloat16_t>>, Is_dropout=true, Is_causal=true]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(58): here
            instantiation of "void run_mha_fwd_hdim32<T>(Flash_fwd_params &, cudaStream_t) [with T=cutlass::bfloat16_t]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_hdim32_bf16_sm80.cu(9): here

/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_kernel.h(276): warning: variable "tQrQ" was declared but never referenced
          detected during:
            instantiation of "void flash::compute_attn_1rowblock<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params,Is_Q_split_K>(const Params &, int, int, int) [with Kernel_traits=Flash_fwd_kernel_traits<32, 128, 128, 4, false, false, cutlass::bfloat16_t, Flash_kernel_traits<32, 128, 128, 4, cutlass::bfloat16_t>>, Is_dropout=true, Is_causal=true, Is_even_N=true, Is_even_K=true, Return_softmax=true, Params=Flash_fwd_params, Is_Q_split_K=false]" 
(589): here
            instantiation of "void flash::compute_attn<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &) [with Kernel_traits=Flash_fwd_kernel_traits<32, 128, 128, 4, false, false, cutlass::bfloat16_t, Flash_kernel_traits<32, 128, 128, 4, cutlass::bfloat16_t>>, Is_dropout=true, Is_causal=true, Is_even_N=true, Is_even_K=true, Return_softmax=true, Params=Flash_fwd_params]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(15): here
            instantiation of "void flash_fwd_kernel<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax>(Flash_fwd_params) [with Kernel_traits=Flash_fwd_kernel_traits<32, 128, 128, 4, false, false, cutlass::bfloat16_t, Flash_kernel_traits<32, 128, 128, 4, cutlass::bfloat16_t>>, Is_dropout=true, Is_causal=true, Is_even_N=true, Is_even_K=true, Return_softmax=true]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(34): here
            instantiation of "void run_flash_fwd<Kernel_traits,Is_dropout,Is_causal>(Flash_fwd_params &, cudaStream_t) [with Kernel_traits=Flash_fwd_kernel_traits<32, 128, 128, 4, false, false, cutlass::bfloat16_t, Flash_kernel_traits<32, 128, 128, 4, cutlass::bfloat16_t>>, Is_dropout=true, Is_causal=true]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(58): here
            instantiation of "void run_mha_fwd_hdim32<T>(Flash_fwd_params &, cudaStream_t) [with T=cutlass::bfloat16_t]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_hdim32_bf16_sm80.cu(9): here

/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_kernel.h(140): warning: variable "MMA_M" was declared but never referenced
          detected during:
            instantiation of "void flash::compute_attn_1rowblock<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params,Is_Q_split_K>(const Params &, int, int, int) [with Kernel_traits=Flash_fwd_kernel_traits<32, 128, 128, 4, false, false, cutlass::bfloat16_t, Flash_kernel_traits<32, 128, 128, 4, cutlass::bfloat16_t>>, Is_dropout=true, Is_causal=true, Is_even_N=true, Is_even_K=true, Return_softmax=false, Params=Flash_fwd_params, Is_Q_split_K=false]" 
(589): here
            instantiation of "void flash::compute_attn<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &) [with Kernel_traits=Flash_fwd_kernel_traits<32, 128, 128, 4, false, false, cutlass::bfloat16_t, Flash_kernel_traits<32, 128, 128, 4, cutlass::bfloat16_t>>, Is_dropout=true, Is_causal=true, Is_even_N=true, Is_even_K=true, Return_softmax=false, Params=Flash_fwd_params]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(15): here
            instantiation of "void flash_fwd_kernel<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax>(Flash_fwd_params) [with Kernel_traits=Flash_fwd_kernel_traits<32, 128, 128, 4, false, false, cutlass::bfloat16_t, Flash_kernel_traits<32, 128, 128, 4, cutlass::bfloat16_t>>, Is_dropout=true, Is_causal=true, Is_even_N=true, Is_even_K=true, Return_softmax=false]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(34): here
            instantiation of "void run_flash_fwd<Kernel_traits,Is_dropout,Is_causal>(Flash_fwd_params &, cudaStream_t) [with Kernel_traits=Flash_fwd_kernel_traits<32, 128, 128, 4, false, false, cutlass::bfloat16_t, Flash_kernel_traits<32, 128, 128, 4, cutlass::bfloat16_t>>, Is_dropout=true, Is_causal=true]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(58): here
            instantiation of "void run_mha_fwd_hdim32<T>(Flash_fwd_params &, cudaStream_t) [with T=cutlass::bfloat16_t]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_hdim32_bf16_sm80.cu(9): here

/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_kernel.h(276): warning: variable "tQrQ" was declared but never referenced
          detected during:
            instantiation of "void flash::compute_attn_1rowblock<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params,Is_Q_split_K>(const Params &, int, int, int) [with Kernel_traits=Flash_fwd_kernel_traits<32, 128, 128, 4, false, false, cutlass::bfloat16_t, Flash_kernel_traits<32, 128, 128, 4, cutlass::bfloat16_t>>, Is_dropout=true, Is_causal=true, Is_even_N=true, Is_even_K=true, Return_softmax=false, Params=Flash_fwd_params, Is_Q_split_K=false]" 
(589): here
            instantiation of "void flash::compute_attn<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &) [with Kernel_traits=Flash_fwd_kernel_traits<32, 128, 128, 4, false, false, cutlass::bfloat16_t, Flash_kernel_traits<32, 128, 128, 4, cutlass::bfloat16_t>>, Is_dropout=true, Is_causal=true, Is_even_N=true, Is_even_K=true, Return_softmax=false, Params=Flash_fwd_params]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(15): here
            instantiation of "void flash_fwd_kernel<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax>(Flash_fwd_params) [with Kernel_traits=Flash_fwd_kernel_traits<32, 128, 128, 4, false, false, cutlass::bfloat16_t, Flash_kernel_traits<32, 128, 128, 4, cutlass::bfloat16_t>>, Is_dropout=true, Is_causal=true, Is_even_N=true, Is_even_K=true, Return_softmax=false]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(34): here
            instantiation of "void run_flash_fwd<Kernel_traits,Is_dropout,Is_causal>(Flash_fwd_params &, cudaStream_t) [with Kernel_traits=Flash_fwd_kernel_traits<32, 128, 128, 4, false, false, cutlass::bfloat16_t, Flash_kernel_traits<32, 128, 128, 4, cutlass::bfloat16_t>>, Is_dropout=true, Is_causal=true]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(58): here
            instantiation of "void run_mha_fwd_hdim32<T>(Flash_fwd_params &, cudaStream_t) [with T=cutlass::bfloat16_t]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_hdim32_bf16_sm80.cu(9): here

/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_kernel.h(140): warning: variable "MMA_M" was declared but never referenced
          detected during:
            instantiation of "void flash::compute_attn_1rowblock<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params,Is_Q_split_K>(const Params &, int, int, int) [with Kernel_traits=Flash_fwd_kernel_traits<32, 128, 128, 4, false, false, cutlass::bfloat16_t, Flash_kernel_traits<32, 128, 128, 4, cutlass::bfloat16_t>>, Is_dropout=true, Is_causal=true, Is_even_N=true, Is_even_K=false, Return_softmax=true, Params=Flash_fwd_params, Is_Q_split_K=false]" 
(589): here
            instantiation of "void flash::compute_attn<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &) [with Kernel_traits=Flash_fwd_kernel_traits<32, 128, 128, 4, false, false, cutlass::bfloat16_t, Flash_kernel_traits<32, 128, 128, 4, cutlass::bfloat16_t>>, Is_dropout=true, Is_causal=true, Is_even_N=true, Is_even_K=false, Return_softmax=true, Params=Flash_fwd_params]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(15): here
            instantiation of "void flash_fwd_kernel<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax>(Flash_fwd_params) [with Kernel_traits=Flash_fwd_kernel_traits<32, 128, 128, 4, false, false, cutlass::bfloat16_t, Flash_kernel_traits<32, 128, 128, 4, cutlass::bfloat16_t>>, Is_dropout=true, Is_causal=true, Is_even_N=true, Is_even_K=false, Return_softmax=true]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(34): here
            instantiation of "void run_flash_fwd<Kernel_traits,Is_dropout,Is_causal>(Flash_fwd_params &, cudaStream_t) [with Kernel_traits=Flash_fwd_kernel_traits<32, 128, 128, 4, false, false, cutlass::bfloat16_t, Flash_kernel_traits<32, 128, 128, 4, cutlass::bfloat16_t>>, Is_dropout=true, Is_causal=true]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(58): here
            instantiation of "void run_mha_fwd_hdim32<T>(Flash_fwd_params &, cudaStream_t) [with T=cutlass::bfloat16_t]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_hdim32_bf16_sm80.cu(9): here

/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_kernel.h(276): warning: variable "tQrQ" was declared but never referenced
          detected during:
            instantiation of "void flash::compute_attn_1rowblock<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params,Is_Q_split_K>(const Params &, int, int, int) [with Kernel_traits=Flash_fwd_kernel_traits<32, 128, 128, 4, false, false, cutlass::bfloat16_t, Flash_kernel_traits<32, 128, 128, 4, cutlass::bfloat16_t>>, Is_dropout=true, Is_causal=true, Is_even_N=true, Is_even_K=false, Return_softmax=true, Params=Flash_fwd_params, Is_Q_split_K=false]" 
(589): here
            instantiation of "void flash::compute_attn<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &) [with Kernel_traits=Flash_fwd_kernel_traits<32, 128, 128, 4, false, false, cutlass::bfloat16_t, Flash_kernel_traits<32, 128, 128, 4, cutlass::bfloat16_t>>, Is_dropout=true, Is_causal=true, Is_even_N=true, Is_even_K=false, Return_softmax=true, Params=Flash_fwd_params]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(15): here
            instantiation of "void flash_fwd_kernel<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax>(Flash_fwd_params) [with Kernel_traits=Flash_fwd_kernel_traits<32, 128, 128, 4, false, false, cutlass::bfloat16_t, Flash_kernel_traits<32, 128, 128, 4, cutlass::bfloat16_t>>, Is_dropout=true, Is_causal=true, Is_even_N=true, Is_even_K=false, Return_softmax=true]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(34): here
            instantiation of "void run_flash_fwd<Kernel_traits,Is_dropout,Is_causal>(Flash_fwd_params &, cudaStream_t) [with Kernel_traits=Flash_fwd_kernel_traits<32, 128, 128, 4, false, false, cutlass::bfloat16_t, Flash_kernel_traits<32, 128, 128, 4, cutlass::bfloat16_t>>, Is_dropout=true, Is_causal=true]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(58): here
            instantiation of "void run_mha_fwd_hdim32<T>(Flash_fwd_params &, cudaStream_t) [with T=cutlass::bfloat16_t]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_hdim32_bf16_sm80.cu(9): here

/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_kernel.h(140): warning: variable "MMA_M" was declared but never referenced
          detected during:
            instantiation of "void flash::compute_attn_1rowblock<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params,Is_Q_split_K>(const Params &, int, int, int) [with Kernel_traits=Flash_fwd_kernel_traits<32, 128, 128, 4, false, false, cutlass::bfloat16_t, Flash_kernel_traits<32, 128, 128, 4, cutlass::bfloat16_t>>, Is_dropout=true, Is_causal=true, Is_even_N=true, Is_even_K=false, Return_softmax=false, Params=Flash_fwd_params, Is_Q_split_K=false]" 
(589): here
            instantiation of "void flash::compute_attn<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &) [with Kernel_traits=Flash_fwd_kernel_traits<32, 128, 128, 4, false, false, cutlass::bfloat16_t, Flash_kernel_traits<32, 128, 128, 4, cutlass::bfloat16_t>>, Is_dropout=true, Is_causal=true, Is_even_N=true, Is_even_K=false, Return_softmax=false, Params=Flash_fwd_params]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(15): here
            instantiation of "void flash_fwd_kernel<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax>(Flash_fwd_params) [with Kernel_traits=Flash_fwd_kernel_traits<32, 128, 128, 4, false, false, cutlass::bfloat16_t, Flash_kernel_traits<32, 128, 128, 4, cutlass::bfloat16_t>>, Is_dropout=true, Is_causal=true, Is_even_N=true, Is_even_K=false, Return_softmax=false]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(34): here
            instantiation of "void run_flash_fwd<Kernel_traits,Is_dropout,Is_causal>(Flash_fwd_params &, cudaStream_t) [with Kernel_traits=Flash_fwd_kernel_traits<32, 128, 128, 4, false, false, cutlass::bfloat16_t, Flash_kernel_traits<32, 128, 128, 4, cutlass::bfloat16_t>>, Is_dropout=true, Is_causal=true]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(58): here
            instantiation of "void run_mha_fwd_hdim32<T>(Flash_fwd_params &, cudaStream_t) [with T=cutlass::bfloat16_t]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_hdim32_bf16_sm80.cu(9): here

/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_kernel.h(276): warning: variable "tQrQ" was declared but never referenced
          detected during:
            instantiation of "void flash::compute_attn_1rowblock<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params,Is_Q_split_K>(const Params &, int, int, int) [with Kernel_traits=Flash_fwd_kernel_traits<32, 128, 128, 4, false, false, cutlass::bfloat16_t, Flash_kernel_traits<32, 128, 128, 4, cutlass::bfloat16_t>>, Is_dropout=true, Is_causal=true, Is_even_N=true, Is_even_K=false, Return_softmax=false, Params=Flash_fwd_params, Is_Q_split_K=false]" 
(589): here
            instantiation of "void flash::compute_attn<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &) [with Kernel_traits=Flash_fwd_kernel_traits<32, 128, 128, 4, false, false, cutlass::bfloat16_t, Flash_kernel_traits<32, 128, 128, 4, cutlass::bfloat16_t>>, Is_dropout=true, Is_causal=true, Is_even_N=true, Is_even_K=false, Return_softmax=false, Params=Flash_fwd_params]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(15): here
            instantiation of "void flash_fwd_kernel<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax>(Flash_fwd_params) [with Kernel_traits=Flash_fwd_kernel_traits<32, 128, 128, 4, false, false, cutlass::bfloat16_t, Flash_kernel_traits<32, 128, 128, 4, cutlass::bfloat16_t>>, Is_dropout=true, Is_causal=true, Is_even_N=true, Is_even_K=false, Return_softmax=false]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(34): here
            instantiation of "void run_flash_fwd<Kernel_traits,Is_dropout,Is_causal>(Flash_fwd_params &, cudaStream_t) [with Kernel_traits=Flash_fwd_kernel_traits<32, 128, 128, 4, false, false, cutlass::bfloat16_t, Flash_kernel_traits<32, 128, 128, 4, cutlass::bfloat16_t>>, Is_dropout=true, Is_causal=true]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(58): here
            instantiation of "void run_mha_fwd_hdim32<T>(Flash_fwd_params &, cudaStream_t) [with T=cutlass::bfloat16_t]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_hdim32_bf16_sm80.cu(9): here

/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_kernel.h(140): warning: variable "MMA_M" was declared but never referenced
          detected during:
            instantiation of "void flash::compute_attn_1rowblock<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params,Is_Q_split_K>(const Params &, int, int, int) [with Kernel_traits=Flash_fwd_kernel_traits<32, 128, 128, 4, false, false, cutlass::bfloat16_t, Flash_kernel_traits<32, 128, 128, 4, cutlass::bfloat16_t>>, Is_dropout=true, Is_causal=true, Is_even_N=false, Is_even_K=true, Return_softmax=true, Params=Flash_fwd_params, Is_Q_split_K=false]" 
(589): here
            instantiation of "void flash::compute_attn<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &) [with Kernel_traits=Flash_fwd_kernel_traits<32, 128, 128, 4, false, false, cutlass::bfloat16_t, Flash_kernel_traits<32, 128, 128, 4, cutlass::bfloat16_t>>, Is_dropout=true, Is_causal=true, Is_even_N=false, Is_even_K=true, Return_softmax=true, Params=Flash_fwd_params]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(15): here
            instantiation of "void flash_fwd_kernel<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax>(Flash_fwd_params) [with Kernel_traits=Flash_fwd_kernel_traits<32, 128, 128, 4, false, false, cutlass::bfloat16_t, Flash_kernel_traits<32, 128, 128, 4, cutlass::bfloat16_t>>, Is_dropout=true, Is_causal=true, Is_even_N=false, Is_even_K=true, Return_softmax=true]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(34): here
            instantiation of "void run_flash_fwd<Kernel_traits,Is_dropout,Is_causal>(Flash_fwd_params &, cudaStream_t) [with Kernel_traits=Flash_fwd_kernel_traits<32, 128, 128, 4, false, false, cutlass::bfloat16_t, Flash_kernel_traits<32, 128, 128, 4, cutlass::bfloat16_t>>, Is_dropout=true, Is_causal=true]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(58): here
            instantiation of "void run_mha_fwd_hdim32<T>(Flash_fwd_params &, cudaStream_t) [with T=cutlass::bfloat16_t]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_hdim32_bf16_sm80.cu(9): here

/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_kernel.h(276): warning: variable "tQrQ" was declared but never referenced
          detected during:
            instantiation of "void flash::compute_attn_1rowblock<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params,Is_Q_split_K>(const Params &, int, int, int) [with Kernel_traits=Flash_fwd_kernel_traits<32, 128, 128, 4, false, false, cutlass::bfloat16_t, Flash_kernel_traits<32, 128, 128, 4, cutlass::bfloat16_t>>, Is_dropout=true, Is_causal=true, Is_even_N=false, Is_even_K=true, Return_softmax=true, Params=Flash_fwd_params, Is_Q_split_K=false]" 
(589): here
            instantiation of "void flash::compute_attn<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &) [with Kernel_traits=Flash_fwd_kernel_traits<32, 128, 128, 4, false, false, cutlass::bfloat16_t, Flash_kernel_traits<32, 128, 128, 4, cutlass::bfloat16_t>>, Is_dropout=true, Is_causal=true, Is_even_N=false, Is_even_K=true, Return_softmax=true, Params=Flash_fwd_params]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(15): here
            instantiation of "void flash_fwd_kernel<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax>(Flash_fwd_params) [with Kernel_traits=Flash_fwd_kernel_traits<32, 128, 128, 4, false, false, cutlass::bfloat16_t, Flash_kernel_traits<32, 128, 128, 4, cutlass::bfloat16_t>>, Is_dropout=true, Is_causal=true, Is_even_N=false, Is_even_K=true, Return_softmax=true]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(34): here
            instantiation of "void run_flash_fwd<Kernel_traits,Is_dropout,Is_causal>(Flash_fwd_params &, cudaStream_t) [with Kernel_traits=Flash_fwd_kernel_traits<32, 128, 128, 4, false, false, cutlass::bfloat16_t, Flash_kernel_traits<32, 128, 128, 4, cutlass::bfloat16_t>>, Is_dropout=true, Is_causal=true]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(58): here
            instantiation of "void run_mha_fwd_hdim32<T>(Flash_fwd_params &, cudaStream_t) [with T=cutlass::bfloat16_t]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_hdim32_bf16_sm80.cu(9): here

/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_kernel.h(140): warning: variable "MMA_M" was declared but never referenced
          detected during:
            instantiation of "void flash::compute_attn_1rowblock<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params,Is_Q_split_K>(const Params &, int, int, int) [with Kernel_traits=Flash_fwd_kernel_traits<32, 128, 128, 4, false, false, cutlass::bfloat16_t, Flash_kernel_traits<32, 128, 128, 4, cutlass::bfloat16_t>>, Is_dropout=true, Is_causal=true, Is_even_N=false, Is_even_K=true, Return_softmax=false, Params=Flash_fwd_params, Is_Q_split_K=false]" 
(589): here
            instantiation of "void flash::compute_attn<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &) [with Kernel_traits=Flash_fwd_kernel_traits<32, 128, 128, 4, false, false, cutlass::bfloat16_t, Flash_kernel_traits<32, 128, 128, 4, cutlass::bfloat16_t>>, Is_dropout=true, Is_causal=true, Is_even_N=false, Is_even_K=true, Return_softmax=false, Params=Flash_fwd_params]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(15): here
            instantiation of "void flash_fwd_kernel<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax>(Flash_fwd_params) [with Kernel_traits=Flash_fwd_kernel_traits<32, 128, 128, 4, false, false, cutlass::bfloat16_t, Flash_kernel_traits<32, 128, 128, 4, cutlass::bfloat16_t>>, Is_dropout=true, Is_causal=true, Is_even_N=false, Is_even_K=true, Return_softmax=false]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(34): here
            instantiation of "void run_flash_fwd<Kernel_traits,Is_dropout,Is_causal>(Flash_fwd_params &, cudaStream_t) [with Kernel_traits=Flash_fwd_kernel_traits<32, 128, 128, 4, false, false, cutlass::bfloat16_t, Flash_kernel_traits<32, 128, 128, 4, cutlass::bfloat16_t>>, Is_dropout=true, Is_causal=true]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(58): here
            instantiation of "void run_mha_fwd_hdim32<T>(Flash_fwd_params &, cudaStream_t) [with T=cutlass::bfloat16_t]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_hdim32_bf16_sm80.cu(9): here

/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_kernel.h(276): warning: variable "tQrQ" was declared but never referenced
          detected during:
            instantiation of "void flash::compute_attn_1rowblock<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params,Is_Q_split_K>(const Params &, int, int, int) [with Kernel_traits=Flash_fwd_kernel_traits<32, 128, 128, 4, false, false, cutlass::bfloat16_t, Flash_kernel_traits<32, 128, 128, 4, cutlass::bfloat16_t>>, Is_dropout=true, Is_causal=true, Is_even_N=false, Is_even_K=true, Return_softmax=false, Params=Flash_fwd_params, Is_Q_split_K=false]" 
(589): here
            instantiation of "void flash::compute_attn<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &) [with Kernel_traits=Flash_fwd_kernel_traits<32, 128, 128, 4, false, false, cutlass::bfloat16_t, Flash_kernel_traits<32, 128, 128, 4, cutlass::bfloat16_t>>, Is_dropout=true, Is_causal=true, Is_even_N=false, Is_even_K=true, Return_softmax=false, Params=Flash_fwd_params]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(15): here
            instantiation of "void flash_fwd_kernel<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax>(Flash_fwd_params) [with Kernel_traits=Flash_fwd_kernel_traits<32, 128, 128, 4, false, false, cutlass::bfloat16_t, Flash_kernel_traits<32, 128, 128, 4, cutlass::bfloat16_t>>, Is_dropout=true, Is_causal=true, Is_even_N=false, Is_even_K=true, Return_softmax=false]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(34): here
            instantiation of "void run_flash_fwd<Kernel_traits,Is_dropout,Is_causal>(Flash_fwd_params &, cudaStream_t) [with Kernel_traits=Flash_fwd_kernel_traits<32, 128, 128, 4, false, false, cutlass::bfloat16_t, Flash_kernel_traits<32, 128, 128, 4, cutlass::bfloat16_t>>, Is_dropout=true, Is_causal=true]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(58): here
            instantiation of "void run_mha_fwd_hdim32<T>(Flash_fwd_params &, cudaStream_t) [with T=cutlass::bfloat16_t]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_hdim32_bf16_sm80.cu(9): here

/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_kernel.h(140): warning: variable "MMA_M" was declared but never referenced
          detected during:
            instantiation of "void flash::compute_attn_1rowblock<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params,Is_Q_split_K>(const Params &, int, int, int) [with Kernel_traits=Flash_fwd_kernel_traits<32, 128, 128, 4, false, false, cutlass::bfloat16_t, Flash_kernel_traits<32, 128, 128, 4, cutlass::bfloat16_t>>, Is_dropout=true, Is_causal=true, Is_even_N=false, Is_even_K=false, Return_softmax=true, Params=Flash_fwd_params, Is_Q_split_K=false]" 
(589): here
            instantiation of "void flash::compute_attn<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &) [with Kernel_traits=Flash_fwd_kernel_traits<32, 128, 128, 4, false, false, cutlass::bfloat16_t, Flash_kernel_traits<32, 128, 128, 4, cutlass::bfloat16_t>>, Is_dropout=true, Is_causal=true, Is_even_N=false, Is_even_K=false, Return_softmax=true, Params=Flash_fwd_params]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(15): here
            instantiation of "void flash_fwd_kernel<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax>(Flash_fwd_params) [with Kernel_traits=Flash_fwd_kernel_traits<32, 128, 128, 4, false, false, cutlass::bfloat16_t, Flash_kernel_traits<32, 128, 128, 4, cutlass::bfloat16_t>>, Is_dropout=true, Is_causal=true, Is_even_N=false, Is_even_K=false, Return_softmax=true]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(34): here
            instantiation of "void run_flash_fwd<Kernel_traits,Is_dropout,Is_causal>(Flash_fwd_params &, cudaStream_t) [with Kernel_traits=Flash_fwd_kernel_traits<32, 128, 128, 4, false, false, cutlass::bfloat16_t, Flash_kernel_traits<32, 128, 128, 4, cutlass::bfloat16_t>>, Is_dropout=true, Is_causal=true]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(58): here
            instantiation of "void run_mha_fwd_hdim32<T>(Flash_fwd_params &, cudaStream_t) [with T=cutlass::bfloat16_t]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_hdim32_bf16_sm80.cu(9): here

/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_kernel.h(276): warning: variable "tQrQ" was declared but never referenced
          detected during:
            instantiation of "void flash::compute_attn_1rowblock<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params,Is_Q_split_K>(const Params &, int, int, int) [with Kernel_traits=Flash_fwd_kernel_traits<32, 128, 128, 4, false, false, cutlass::bfloat16_t, Flash_kernel_traits<32, 128, 128, 4, cutlass::bfloat16_t>>, Is_dropout=true, Is_causal=true, Is_even_N=false, Is_even_K=false, Return_softmax=true, Params=Flash_fwd_params, Is_Q_split_K=false]" 
(589): here
            instantiation of "void flash::compute_attn<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &) [with Kernel_traits=Flash_fwd_kernel_traits<32, 128, 128, 4, false, false, cutlass::bfloat16_t, Flash_kernel_traits<32, 128, 128, 4, cutlass::bfloat16_t>>, Is_dropout=true, Is_causal=true, Is_even_N=false, Is_even_K=false, Return_softmax=true, Params=Flash_fwd_params]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(15): here
            instantiation of "void flash_fwd_kernel<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax>(Flash_fwd_params) [with Kernel_traits=Flash_fwd_kernel_traits<32, 128, 128, 4, false, false, cutlass::bfloat16_t, Flash_kernel_traits<32, 128, 128, 4, cutlass::bfloat16_t>>, Is_dropout=true, Is_causal=true, Is_even_N=false, Is_even_K=false, Return_softmax=true]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(34): here
            instantiation of "void run_flash_fwd<Kernel_traits,Is_dropout,Is_causal>(Flash_fwd_params &, cudaStream_t) [with Kernel_traits=Flash_fwd_kernel_traits<32, 128, 128, 4, false, false, cutlass::bfloat16_t, Flash_kernel_traits<32, 128, 128, 4, cutlass::bfloat16_t>>, Is_dropout=true, Is_causal=true]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(58): here
            instantiation of "void run_mha_fwd_hdim32<T>(Flash_fwd_params &, cudaStream_t) [with T=cutlass::bfloat16_t]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_hdim32_bf16_sm80.cu(9): here

/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_kernel.h(140): warning: variable "MMA_M" was declared but never referenced
          detected during:
            instantiation of "void flash::compute_attn_1rowblock<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params,Is_Q_split_K>(const Params &, int, int, int) [with Kernel_traits=Flash_fwd_kernel_traits<32, 128, 128, 4, false, false, cutlass::bfloat16_t, Flash_kernel_traits<32, 128, 128, 4, cutlass::bfloat16_t>>, Is_dropout=true, Is_causal=true, Is_even_N=false, Is_even_K=false, Return_softmax=false, Params=Flash_fwd_params, Is_Q_split_K=false]" 
(589): here
            instantiation of "void flash::compute_attn<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &) [with Kernel_traits=Flash_fwd_kernel_traits<32, 128, 128, 4, false, false, cutlass::bfloat16_t, Flash_kernel_traits<32, 128, 128, 4, cutlass::bfloat16_t>>, Is_dropout=true, Is_causal=true, Is_even_N=false, Is_even_K=false, Return_softmax=false, Params=Flash_fwd_params]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(15): here
            instantiation of "void flash_fwd_kernel<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax>(Flash_fwd_params) [with Kernel_traits=Flash_fwd_kernel_traits<32, 128, 128, 4, false, false, cutlass::bfloat16_t, Flash_kernel_traits<32, 128, 128, 4, cutlass::bfloat16_t>>, Is_dropout=true, Is_causal=true, Is_even_N=false, Is_even_K=false, Return_softmax=false]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(34): here
            instantiation of "void run_flash_fwd<Kernel_traits,Is_dropout,Is_causal>(Flash_fwd_params &, cudaStream_t) [with Kernel_traits=Flash_fwd_kernel_traits<32, 128, 128, 4, false, false, cutlass::bfloat16_t, Flash_kernel_traits<32, 128, 128, 4, cutlass::bfloat16_t>>, Is_dropout=true, Is_causal=true]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(58): here
            instantiation of "void run_mha_fwd_hdim32<T>(Flash_fwd_params &, cudaStream_t) [with T=cutlass::bfloat16_t]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_hdim32_bf16_sm80.cu(9): here

/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_kernel.h(276): warning: variable "tQrQ" was declared but never referenced
          detected during:
            instantiation of "void flash::compute_attn_1rowblock<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params,Is_Q_split_K>(const Params &, int, int, int) [with Kernel_traits=Flash_fwd_kernel_traits<32, 128, 128, 4, false, false, cutlass::bfloat16_t, Flash_kernel_traits<32, 128, 128, 4, cutlass::bfloat16_t>>, Is_dropout=true, Is_causal=true, Is_even_N=false, Is_even_K=false, Return_softmax=false, Params=Flash_fwd_params, Is_Q_split_K=false]" 
(589): here
            instantiation of "void flash::compute_attn<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &) [with Kernel_traits=Flash_fwd_kernel_traits<32, 128, 128, 4, false, false, cutlass::bfloat16_t, Flash_kernel_traits<32, 128, 128, 4, cutlass::bfloat16_t>>, Is_dropout=true, Is_causal=true, Is_even_N=false, Is_even_K=false, Return_softmax=false, Params=Flash_fwd_params]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(15): here
            instantiation of "void flash_fwd_kernel<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax>(Flash_fwd_params) [with Kernel_traits=Flash_fwd_kernel_traits<32, 128, 128, 4, false, false, cutlass::bfloat16_t, Flash_kernel_traits<32, 128, 128, 4, cutlass::bfloat16_t>>, Is_dropout=true, Is_causal=true, Is_even_N=false, Is_even_K=false, Return_softmax=false]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(34): here
            instantiation of "void run_flash_fwd<Kernel_traits,Is_dropout,Is_causal>(Flash_fwd_params &, cudaStream_t) [with Kernel_traits=Flash_fwd_kernel_traits<32, 128, 128, 4, false, false, cutlass::bfloat16_t, Flash_kernel_traits<32, 128, 128, 4, cutlass::bfloat16_t>>, Is_dropout=true, Is_causal=true]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(58): here
            instantiation of "void run_mha_fwd_hdim32<T>(Flash_fwd_params &, cudaStream_t) [with T=cutlass::bfloat16_t]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_hdim32_bf16_sm80.cu(9): here

/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_kernel.h(140): warning: variable "MMA_M" was declared but never referenced
          detected during:
            instantiation of "void flash::compute_attn_1rowblock<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params,Is_Q_split_K>(const Params &, int, int, int) [with Kernel_traits=Flash_fwd_kernel_traits<32, 128, 128, 4, false, false, cutlass::bfloat16_t, Flash_kernel_traits<32, 128, 128, 4, cutlass::bfloat16_t>>, Is_dropout=true, Is_causal=false, Is_even_N=true, Is_even_K=true, Return_softmax=true, Params=Flash_fwd_params, Is_Q_split_K=false]" 
(589): here
            instantiation of "void flash::compute_attn<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &) [with Kernel_traits=Flash_fwd_kernel_traits<32, 128, 128, 4, false, false, cutlass::bfloat16_t, Flash_kernel_traits<32, 128, 128, 4, cutlass::bfloat16_t>>, Is_dropout=true, Is_causal=false, Is_even_N=true, Is_even_K=true, Return_softmax=true, Params=Flash_fwd_params]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(15): here
            instantiation of "void flash_fwd_kernel<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax>(Flash_fwd_params) [with Kernel_traits=Flash_fwd_kernel_traits<32, 128, 128, 4, false, false, cutlass::bfloat16_t, Flash_kernel_traits<32, 128, 128, 4, cutlass::bfloat16_t>>, Is_dropout=true, Is_causal=false, Is_even_N=true, Is_even_K=true, Return_softmax=true]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(34): here
            instantiation of "void run_flash_fwd<Kernel_traits,Is_dropout,Is_causal>(Flash_fwd_params &, cudaStream_t) [with Kernel_traits=Flash_fwd_kernel_traits<32, 128, 128, 4, false, false, cutlass::bfloat16_t, Flash_kernel_traits<32, 128, 128, 4, cutlass::bfloat16_t>>, Is_dropout=true, Is_causal=false]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(58): here
            instantiation of "void run_mha_fwd_hdim32<T>(Flash_fwd_params &, cudaStream_t) [with T=cutlass::bfloat16_t]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_hdim32_bf16_sm80.cu(9): here

/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_kernel.h(276): warning: variable "tQrQ" was declared but never referenced
          detected during:
            instantiation of "void flash::compute_attn_1rowblock<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params,Is_Q_split_K>(const Params &, int, int, int) [with Kernel_traits=Flash_fwd_kernel_traits<32, 128, 128, 4, false, false, cutlass::bfloat16_t, Flash_kernel_traits<32, 128, 128, 4, cutlass::bfloat16_t>>, Is_dropout=true, Is_causal=false, Is_even_N=true, Is_even_K=true, Return_softmax=true, Params=Flash_fwd_params, Is_Q_split_K=false]" 
(589): here
            instantiation of "void flash::compute_attn<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &) [with Kernel_traits=Flash_fwd_kernel_traits<32, 128, 128, 4, false, false, cutlass::bfloat16_t, Flash_kernel_traits<32, 128, 128, 4, cutlass::bfloat16_t>>, Is_dropout=true, Is_causal=false, Is_even_N=true, Is_even_K=true, Return_softmax=true, Params=Flash_fwd_params]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(15): here
            instantiation of "void flash_fwd_kernel<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax>(Flash_fwd_params) [with Kernel_traits=Flash_fwd_kernel_traits<32, 128, 128, 4, false, false, cutlass::bfloat16_t, Flash_kernel_traits<32, 128, 128, 4, cutlass::bfloat16_t>>, Is_dropout=true, Is_causal=false, Is_even_N=true, Is_even_K=true, Return_softmax=true]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(34): here
            instantiation of "void run_flash_fwd<Kernel_traits,Is_dropout,Is_causal>(Flash_fwd_params &, cudaStream_t) [with Kernel_traits=Flash_fwd_kernel_traits<32, 128, 128, 4, false, false, cutlass::bfloat16_t, Flash_kernel_traits<32, 128, 128, 4, cutlass::bfloat16_t>>, Is_dropout=true, Is_causal=false]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(58): here
            instantiation of "void run_mha_fwd_hdim32<T>(Flash_fwd_params &, cudaStream_t) [with T=cutlass::bfloat16_t]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_hdim32_bf16_sm80.cu(9): here

/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_kernel.h(140): warning: variable "MMA_M" was declared but never referenced
          detected during:
            instantiation of "void flash::compute_attn_1rowblock<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params,Is_Q_split_K>(const Params &, int, int, int) [with Kernel_traits=Flash_fwd_kernel_traits<32, 128, 128, 4, false, false, cutlass::bfloat16_t, Flash_kernel_traits<32, 128, 128, 4, cutlass::bfloat16_t>>, Is_dropout=true, Is_causal=false, Is_even_N=true, Is_even_K=true, Return_softmax=false, Params=Flash_fwd_params, Is_Q_split_K=false]" 
(589): here
            instantiation of "void flash::compute_attn<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &) [with Kernel_traits=Flash_fwd_kernel_traits<32, 128, 128, 4, false, false, cutlass::bfloat16_t, Flash_kernel_traits<32, 128, 128, 4, cutlass::bfloat16_t>>, Is_dropout=true, Is_causal=false, Is_even_N=true, Is_even_K=true, Return_softmax=false, Params=Flash_fwd_params]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(15): here
            instantiation of "void flash_fwd_kernel<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax>(Flash_fwd_params) [with Kernel_traits=Flash_fwd_kernel_traits<32, 128, 128, 4, false, false, cutlass::bfloat16_t, Flash_kernel_traits<32, 128, 128, 4, cutlass::bfloat16_t>>, Is_dropout=true, Is_causal=false, Is_even_N=true, Is_even_K=true, Return_softmax=false]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(34): here
            instantiation of "void run_flash_fwd<Kernel_traits,Is_dropout,Is_causal>(Flash_fwd_params &, cudaStream_t) [with Kernel_traits=Flash_fwd_kernel_traits<32, 128, 128, 4, false, false, cutlass::bfloat16_t, Flash_kernel_traits<32, 128, 128, 4, cutlass::bfloat16_t>>, Is_dropout=true, Is_causal=false]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(58): here
            instantiation of "void run_mha_fwd_hdim32<T>(Flash_fwd_params &, cudaStream_t) [with T=cutlass::bfloat16_t]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_hdim32_bf16_sm80.cu(9): here

/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_kernel.h(276): warning: variable "tQrQ" was declared but never referenced
          detected during:
            instantiation of "void flash::compute_attn_1rowblock<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params,Is_Q_split_K>(const Params &, int, int, int) [with Kernel_traits=Flash_fwd_kernel_traits<32, 128, 128, 4, false, false, cutlass::bfloat16_t, Flash_kernel_traits<32, 128, 128, 4, cutlass::bfloat16_t>>, Is_dropout=true, Is_causal=false, Is_even_N=true, Is_even_K=true, Return_softmax=false, Params=Flash_fwd_params, Is_Q_split_K=false]" 
(589): here
            instantiation of "void flash::compute_attn<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &) [with Kernel_traits=Flash_fwd_kernel_traits<32, 128, 128, 4, false, false, cutlass::bfloat16_t, Flash_kernel_traits<32, 128, 128, 4, cutlass::bfloat16_t>>, Is_dropout=true, Is_causal=false, Is_even_N=true, Is_even_K=true, Return_softmax=false, Params=Flash_fwd_params]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(15): here
            instantiation of "void flash_fwd_kernel<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax>(Flash_fwd_params) [with Kernel_traits=Flash_fwd_kernel_traits<32, 128, 128, 4, false, false, cutlass::bfloat16_t, Flash_kernel_traits<32, 128, 128, 4, cutlass::bfloat16_t>>, Is_dropout=true, Is_causal=false, Is_even_N=true, Is_even_K=true, Return_softmax=false]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(34): here
            instantiation of "void run_flash_fwd<Kernel_traits,Is_dropout,Is_causal>(Flash_fwd_params &, cudaStream_t) [with Kernel_traits=Flash_fwd_kernel_traits<32, 128, 128, 4, false, false, cutlass::bfloat16_t, Flash_kernel_traits<32, 128, 128, 4, cutlass::bfloat16_t>>, Is_dropout=true, Is_causal=false]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(58): here
            instantiation of "void run_mha_fwd_hdim32<T>(Flash_fwd_params &, cudaStream_t) [with T=cutlass::bfloat16_t]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_hdim32_bf16_sm80.cu(9): here

/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_kernel.h(140): warning: variable "MMA_M" was declared but never referenced
          detected during:
            instantiation of "void flash::compute_attn_1rowblock<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params,Is_Q_split_K>(const Params &, int, int, int) [with Kernel_traits=Flash_fwd_kernel_traits<32, 128, 128, 4, false, false, cutlass::bfloat16_t, Flash_kernel_traits<32, 128, 128, 4, cutlass::bfloat16_t>>, Is_dropout=true, Is_causal=false, Is_even_N=true, Is_even_K=false, Return_softmax=true, Params=Flash_fwd_params, Is_Q_split_K=false]" 
(589): here
            instantiation of "void flash::compute_attn<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &) [with Kernel_traits=Flash_fwd_kernel_traits<32, 128, 128, 4, false, false, cutlass::bfloat16_t, Flash_kernel_traits<32, 128, 128, 4, cutlass::bfloat16_t>>, Is_dropout=true, Is_causal=false, Is_even_N=true, Is_even_K=false, Return_softmax=true, Params=Flash_fwd_params]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(15): here
            instantiation of "void flash_fwd_kernel<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax>(Flash_fwd_params) [with Kernel_traits=Flash_fwd_kernel_traits<32, 128, 128, 4, false, false, cutlass::bfloat16_t, Flash_kernel_traits<32, 128, 128, 4, cutlass::bfloat16_t>>, Is_dropout=true, Is_causal=false, Is_even_N=true, Is_even_K=false, Return_softmax=true]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(34): here
            instantiation of "void run_flash_fwd<Kernel_traits,Is_dropout,Is_causal>(Flash_fwd_params &, cudaStream_t) [with Kernel_traits=Flash_fwd_kernel_traits<32, 128, 128, 4, false, false, cutlass::bfloat16_t, Flash_kernel_traits<32, 128, 128, 4, cutlass::bfloat16_t>>, Is_dropout=true, Is_causal=false]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(58): here
            instantiation of "void run_mha_fwd_hdim32<T>(Flash_fwd_params &, cudaStream_t) [with T=cutlass::bfloat16_t]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_hdim32_bf16_sm80.cu(9): here

/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_kernel.h(276): warning: variable "tQrQ" was declared but never referenced
          detected during:
            instantiation of "void flash::compute_attn_1rowblock<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params,Is_Q_split_K>(const Params &, int, int, int) [with Kernel_traits=Flash_fwd_kernel_traits<32, 128, 128, 4, false, false, cutlass::bfloat16_t, Flash_kernel_traits<32, 128, 128, 4, cutlass::bfloat16_t>>, Is_dropout=true, Is_causal=false, Is_even_N=true, Is_even_K=false, Return_softmax=true, Params=Flash_fwd_params, Is_Q_split_K=false]" 
(589): here
            instantiation of "void flash::compute_attn<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &) [with Kernel_traits=Flash_fwd_kernel_traits<32, 128, 128, 4, false, false, cutlass::bfloat16_t, Flash_kernel_traits<32, 128, 128, 4, cutlass::bfloat16_t>>, Is_dropout=true, Is_causal=false, Is_even_N=true, Is_even_K=false, Return_softmax=true, Params=Flash_fwd_params]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(15): here
            instantiation of "void flash_fwd_kernel<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax>(Flash_fwd_params) [with Kernel_traits=Flash_fwd_kernel_traits<32, 128, 128, 4, false, false, cutlass::bfloat16_t, Flash_kernel_traits<32, 128, 128, 4, cutlass::bfloat16_t>>, Is_dropout=true, Is_causal=false, Is_even_N=true, Is_even_K=false, Return_softmax=true]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(34): here
            instantiation of "void run_flash_fwd<Kernel_traits,Is_dropout,Is_causal>(Flash_fwd_params &, cudaStream_t) [with Kernel_traits=Flash_fwd_kernel_traits<32, 128, 128, 4, false, false, cutlass::bfloat16_t, Flash_kernel_traits<32, 128, 128, 4, cutlass::bfloat16_t>>, Is_dropout=true, Is_causal=false]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(58): here
            instantiation of "void run_mha_fwd_hdim32<T>(Flash_fwd_params &, cudaStream_t) [with T=cutlass::bfloat16_t]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_hdim32_bf16_sm80.cu(9): here

/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_kernel.h(140): warning: variable "MMA_M" was declared but never referenced
          detected during:
            instantiation of "void flash::compute_attn_1rowblock<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params,Is_Q_split_K>(const Params &, int, int, int) [with Kernel_traits=Flash_fwd_kernel_traits<32, 128, 128, 4, false, false, cutlass::bfloat16_t, Flash_kernel_traits<32, 128, 128, 4, cutlass::bfloat16_t>>, Is_dropout=true, Is_causal=false, Is_even_N=true, Is_even_K=false, Return_softmax=false, Params=Flash_fwd_params, Is_Q_split_K=false]" 
(589): here
            instantiation of "void flash::compute_attn<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &) [with Kernel_traits=Flash_fwd_kernel_traits<32, 128, 128, 4, false, false, cutlass::bfloat16_t, Flash_kernel_traits<32, 128, 128, 4, cutlass::bfloat16_t>>, Is_dropout=true, Is_causal=false, Is_even_N=true, Is_even_K=false, Return_softmax=false, Params=Flash_fwd_params]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(15): here
            instantiation of "void flash_fwd_kernel<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax>(Flash_fwd_params) [with Kernel_traits=Flash_fwd_kernel_traits<32, 128, 128, 4, false, false, cutlass::bfloat16_t, Flash_kernel_traits<32, 128, 128, 4, cutlass::bfloat16_t>>, Is_dropout=true, Is_causal=false, Is_even_N=true, Is_even_K=false, Return_softmax=false]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(34): here
            instantiation of "void run_flash_fwd<Kernel_traits,Is_dropout,Is_causal>(Flash_fwd_params &, cudaStream_t) [with Kernel_traits=Flash_fwd_kernel_traits<32, 128, 128, 4, false, false, cutlass::bfloat16_t, Flash_kernel_traits<32, 128, 128, 4, cutlass::bfloat16_t>>, Is_dropout=true, Is_causal=false]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(58): here
            instantiation of "void run_mha_fwd_hdim32<T>(Flash_fwd_params &, cudaStream_t) [with T=cutlass::bfloat16_t]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_hdim32_bf16_sm80.cu(9): here

/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_kernel.h(276): warning: variable "tQrQ" was declared but never referenced
          detected during:
            instantiation of "void flash::compute_attn_1rowblock<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params,Is_Q_split_K>(const Params &, int, int, int) [with Kernel_traits=Flash_fwd_kernel_traits<32, 128, 128, 4, false, false, cutlass::bfloat16_t, Flash_kernel_traits<32, 128, 128, 4, cutlass::bfloat16_t>>, Is_dropout=true, Is_causal=false, Is_even_N=true, Is_even_K=false, Return_softmax=false, Params=Flash_fwd_params, Is_Q_split_K=false]" 
(589): here
            instantiation of "void flash::compute_attn<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &) [with Kernel_traits=Flash_fwd_kernel_traits<32, 128, 128, 4, false, false, cutlass::bfloat16_t, Flash_kernel_traits<32, 128, 128, 4, cutlass::bfloat16_t>>, Is_dropout=true, Is_causal=false, Is_even_N=true, Is_even_K=false, Return_softmax=false, Params=Flash_fwd_params]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(15): here
            instantiation of "void flash_fwd_kernel<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax>(Flash_fwd_params) [with Kernel_traits=Flash_fwd_kernel_traits<32, 128, 128, 4, false, false, cutlass::bfloat16_t, Flash_kernel_traits<32, 128, 128, 4, cutlass::bfloat16_t>>, Is_dropout=true, Is_causal=false, Is_even_N=true, Is_even_K=false, Return_softmax=false]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(34): here
            instantiation of "void run_flash_fwd<Kernel_traits,Is_dropout,Is_causal>(Flash_fwd_params &, cudaStream_t) [with Kernel_traits=Flash_fwd_kernel_traits<32, 128, 128, 4, false, false, cutlass::bfloat16_t, Flash_kernel_traits<32, 128, 128, 4, cutlass::bfloat16_t>>, Is_dropout=true, Is_causal=false]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(58): here
            instantiation of "void run_mha_fwd_hdim32<T>(Flash_fwd_params &, cudaStream_t) [with T=cutlass::bfloat16_t]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_hdim32_bf16_sm80.cu(9): here

/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_kernel.h(140): warning: variable "MMA_M" was declared but never referenced
          detected during:
            instantiation of "void flash::compute_attn_1rowblock<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params,Is_Q_split_K>(const Params &, int, int, int) [with Kernel_traits=Flash_fwd_kernel_traits<32, 128, 128, 4, false, false, cutlass::bfloat16_t, Flash_kernel_traits<32, 128, 128, 4, cutlass::bfloat16_t>>, Is_dropout=true, Is_causal=false, Is_even_N=false, Is_even_K=true, Return_softmax=true, Params=Flash_fwd_params, Is_Q_split_K=false]" 
(589): here
            instantiation of "void flash::compute_attn<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &) [with Kernel_traits=Flash_fwd_kernel_traits<32, 128, 128, 4, false, false, cutlass::bfloat16_t, Flash_kernel_traits<32, 128, 128, 4, cutlass::bfloat16_t>>, Is_dropout=true, Is_causal=false, Is_even_N=false, Is_even_K=true, Return_softmax=true, Params=Flash_fwd_params]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(15): here
            instantiation of "void flash_fwd_kernel<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax>(Flash_fwd_params) [with Kernel_traits=Flash_fwd_kernel_traits<32, 128, 128, 4, false, false, cutlass::bfloat16_t, Flash_kernel_traits<32, 128, 128, 4, cutlass::bfloat16_t>>, Is_dropout=true, Is_causal=false, Is_even_N=false, Is_even_K=true, Return_softmax=true]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(34): here
            instantiation of "void run_flash_fwd<Kernel_traits,Is_dropout,Is_causal>(Flash_fwd_params &, cudaStream_t) [with Kernel_traits=Flash_fwd_kernel_traits<32, 128, 128, 4, false, false, cutlass::bfloat16_t, Flash_kernel_traits<32, 128, 128, 4, cutlass::bfloat16_t>>, Is_dropout=true, Is_causal=false]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(58): here
            instantiation of "void run_mha_fwd_hdim32<T>(Flash_fwd_params &, cudaStream_t) [with T=cutlass::bfloat16_t]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_hdim32_bf16_sm80.cu(9): here

/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_kernel.h(276): warning: variable "tQrQ" was declared but never referenced
          detected during:
            instantiation of "void flash::compute_attn_1rowblock<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params,Is_Q_split_K>(const Params &, int, int, int) [with Kernel_traits=Flash_fwd_kernel_traits<32, 128, 128, 4, false, false, cutlass::bfloat16_t, Flash_kernel_traits<32, 128, 128, 4, cutlass::bfloat16_t>>, Is_dropout=true, Is_causal=false, Is_even_N=false, Is_even_K=true, Return_softmax=true, Params=Flash_fwd_params, Is_Q_split_K=false]" 
(589): here
            instantiation of "void flash::compute_attn<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &) [with Kernel_traits=Flash_fwd_kernel_traits<32, 128, 128, 4, false, false, cutlass::bfloat16_t, Flash_kernel_traits<32, 128, 128, 4, cutlass::bfloat16_t>>, Is_dropout=true, Is_causal=false, Is_even_N=false, Is_even_K=true, Return_softmax=true, Params=Flash_fwd_params]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(15): here
            instantiation of "void flash_fwd_kernel<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax>(Flash_fwd_params) [with Kernel_traits=Flash_fwd_kernel_traits<32, 128, 128, 4, false, false, cutlass::bfloat16_t, Flash_kernel_traits<32, 128, 128, 4, cutlass::bfloat16_t>>, Is_dropout=true, Is_causal=false, Is_even_N=false, Is_even_K=true, Return_softmax=true]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(34): here
            instantiation of "void run_flash_fwd<Kernel_traits,Is_dropout,Is_causal>(Flash_fwd_params &, cudaStream_t) [with Kernel_traits=Flash_fwd_kernel_traits<32, 128, 128, 4, false, false, cutlass::bfloat16_t, Flash_kernel_traits<32, 128, 128, 4, cutlass::bfloat16_t>>, Is_dropout=true, Is_causal=false]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(58): here
            instantiation of "void run_mha_fwd_hdim32<T>(Flash_fwd_params &, cudaStream_t) [with T=cutlass::bfloat16_t]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_hdim32_bf16_sm80.cu(9): here

/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_kernel.h(140): warning: variable "MMA_M" was declared but never referenced
          detected during:
            instantiation of "void flash::compute_attn_1rowblock<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params,Is_Q_split_K>(const Params &, int, int, int) [with Kernel_traits=Flash_fwd_kernel_traits<32, 128, 128, 4, false, false, cutlass::bfloat16_t, Flash_kernel_traits<32, 128, 128, 4, cutlass::bfloat16_t>>, Is_dropout=true, Is_causal=false, Is_even_N=false, Is_even_K=true, Return_softmax=false, Params=Flash_fwd_params, Is_Q_split_K=false]" 
(589): here
            instantiation of "void flash::compute_attn<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &) [with Kernel_traits=Flash_fwd_kernel_traits<32, 128, 128, 4, false, false, cutlass::bfloat16_t, Flash_kernel_traits<32, 128, 128, 4, cutlass::bfloat16_t>>, Is_dropout=true, Is_causal=false, Is_even_N=false, Is_even_K=true, Return_softmax=false, Params=Flash_fwd_params]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(15): here
            instantiation of "void flash_fwd_kernel<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax>(Flash_fwd_params) [with Kernel_traits=Flash_fwd_kernel_traits<32, 128, 128, 4, false, false, cutlass::bfloat16_t, Flash_kernel_traits<32, 128, 128, 4, cutlass::bfloat16_t>>, Is_dropout=true, Is_causal=false, Is_even_N=false, Is_even_K=true, Return_softmax=false]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(34): here
            instantiation of "void run_flash_fwd<Kernel_traits,Is_dropout,Is_causal>(Flash_fwd_params &, cudaStream_t) [with Kernel_traits=Flash_fwd_kernel_traits<32, 128, 128, 4, false, false, cutlass::bfloat16_t, Flash_kernel_traits<32, 128, 128, 4, cutlass::bfloat16_t>>, Is_dropout=true, Is_causal=false]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(58): here
            instantiation of "void run_mha_fwd_hdim32<T>(Flash_fwd_params &, cudaStream_t) [with T=cutlass::bfloat16_t]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_hdim32_bf16_sm80.cu(9): here

/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_kernel.h(276): warning: variable "tQrQ" was declared but never referenced
          detected during:
            instantiation of "void flash::compute_attn_1rowblock<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params,Is_Q_split_K>(const Params &, int, int, int) [with Kernel_traits=Flash_fwd_kernel_traits<32, 128, 128, 4, false, false, cutlass::bfloat16_t, Flash_kernel_traits<32, 128, 128, 4, cutlass::bfloat16_t>>, Is_dropout=true, Is_causal=false, Is_even_N=false, Is_even_K=true, Return_softmax=false, Params=Flash_fwd_params, Is_Q_split_K=false]" 
(589): here
            instantiation of "void flash::compute_attn<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &) [with Kernel_traits=Flash_fwd_kernel_traits<32, 128, 128, 4, false, false, cutlass::bfloat16_t, Flash_kernel_traits<32, 128, 128, 4, cutlass::bfloat16_t>>, Is_dropout=true, Is_causal=false, Is_even_N=false, Is_even_K=true, Return_softmax=false, Params=Flash_fwd_params]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(15): here
            instantiation of "void flash_fwd_kernel<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax>(Flash_fwd_params) [with Kernel_traits=Flash_fwd_kernel_traits<32, 128, 128, 4, false, false, cutlass::bfloat16_t, Flash_kernel_traits<32, 128, 128, 4, cutlass::bfloat16_t>>, Is_dropout=true, Is_causal=false, Is_even_N=false, Is_even_K=true, Return_softmax=false]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(34): here
            instantiation of "void run_flash_fwd<Kernel_traits,Is_dropout,Is_causal>(Flash_fwd_params &, cudaStream_t) [with Kernel_traits=Flash_fwd_kernel_traits<32, 128, 128, 4, false, false, cutlass::bfloat16_t, Flash_kernel_traits<32, 128, 128, 4, cutlass::bfloat16_t>>, Is_dropout=true, Is_causal=false]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(58): here
            instantiation of "void run_mha_fwd_hdim32<T>(Flash_fwd_params &, cudaStream_t) [with T=cutlass::bfloat16_t]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_hdim32_bf16_sm80.cu(9): here

/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_kernel.h(140): warning: variable "MMA_M" was declared but never referenced
          detected during:
            instantiation of "void flash::compute_attn_1rowblock<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params,Is_Q_split_K>(const Params &, int, int, int) [with Kernel_traits=Flash_fwd_kernel_traits<32, 128, 128, 4, false, false, cutlass::bfloat16_t, Flash_kernel_traits<32, 128, 128, 4, cutlass::bfloat16_t>>, Is_dropout=true, Is_causal=false, Is_even_N=false, Is_even_K=false, Return_softmax=true, Params=Flash_fwd_params, Is_Q_split_K=false]" 
(589): here
            instantiation of "void flash::compute_attn<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &) [with Kernel_traits=Flash_fwd_kernel_traits<32, 128, 128, 4, false, false, cutlass::bfloat16_t, Flash_kernel_traits<32, 128, 128, 4, cutlass::bfloat16_t>>, Is_dropout=true, Is_causal=false, Is_even_N=false, Is_even_K=false, Return_softmax=true, Params=Flash_fwd_params]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(15): here
            instantiation of "void flash_fwd_kernel<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax>(Flash_fwd_params) [with Kernel_traits=Flash_fwd_kernel_traits<32, 128, 128, 4, false, false, cutlass::bfloat16_t, Flash_kernel_traits<32, 128, 128, 4, cutlass::bfloat16_t>>, Is_dropout=true, Is_causal=false, Is_even_N=false, Is_even_K=false, Return_softmax=true]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(34): here
            instantiation of "void run_flash_fwd<Kernel_traits,Is_dropout,Is_causal>(Flash_fwd_params &, cudaStream_t) [with Kernel_traits=Flash_fwd_kernel_traits<32, 128, 128, 4, false, false, cutlass::bfloat16_t, Flash_kernel_traits<32, 128, 128, 4, cutlass::bfloat16_t>>, Is_dropout=true, Is_causal=false]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(58): here
            instantiation of "void run_mha_fwd_hdim32<T>(Flash_fwd_params &, cudaStream_t) [with T=cutlass::bfloat16_t]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_hdim32_bf16_sm80.cu(9): here

/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_kernel.h(276): warning: variable "tQrQ" was declared but never referenced
          detected during:
            instantiation of "void flash::compute_attn_1rowblock<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params,Is_Q_split_K>(const Params &, int, int, int) [with Kernel_traits=Flash_fwd_kernel_traits<32, 128, 128, 4, false, false, cutlass::bfloat16_t, Flash_kernel_traits<32, 128, 128, 4, cutlass::bfloat16_t>>, Is_dropout=true, Is_causal=false, Is_even_N=false, Is_even_K=false, Return_softmax=true, Params=Flash_fwd_params, Is_Q_split_K=false]" 
(589): here
            instantiation of "void flash::compute_attn<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &) [with Kernel_traits=Flash_fwd_kernel_traits<32, 128, 128, 4, false, false, cutlass::bfloat16_t, Flash_kernel_traits<32, 128, 128, 4, cutlass::bfloat16_t>>, Is_dropout=true, Is_causal=false, Is_even_N=false, Is_even_K=false, Return_softmax=true, Params=Flash_fwd_params]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(15): here
            instantiation of "void flash_fwd_kernel<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax>(Flash_fwd_params) [with Kernel_traits=Flash_fwd_kernel_traits<32, 128, 128, 4, false, false, cutlass::bfloat16_t, Flash_kernel_traits<32, 128, 128, 4, cutlass::bfloat16_t>>, Is_dropout=true, Is_causal=false, Is_even_N=false, Is_even_K=false, Return_softmax=true]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(34): here
            instantiation of "void run_flash_fwd<Kernel_traits,Is_dropout,Is_causal>(Flash_fwd_params &, cudaStream_t) [with Kernel_traits=Flash_fwd_kernel_traits<32, 128, 128, 4, false, false, cutlass::bfloat16_t, Flash_kernel_traits<32, 128, 128, 4, cutlass::bfloat16_t>>, Is_dropout=true, Is_causal=false]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(58): here
            instantiation of "void run_mha_fwd_hdim32<T>(Flash_fwd_params &, cudaStream_t) [with T=cutlass::bfloat16_t]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_hdim32_bf16_sm80.cu(9): here

/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_kernel.h(140): warning: variable "MMA_M" was declared but never referenced
          detected during:
            instantiation of "void flash::compute_attn_1rowblock<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params,Is_Q_split_K>(const Params &, int, int, int) [with Kernel_traits=Flash_fwd_kernel_traits<32, 128, 128, 4, false, false, cutlass::bfloat16_t, Flash_kernel_traits<32, 128, 128, 4, cutlass::bfloat16_t>>, Is_dropout=true, Is_causal=false, Is_even_N=false, Is_even_K=false, Return_softmax=false, Params=Flash_fwd_params, Is_Q_split_K=false]" 
(589): here
            instantiation of "void flash::compute_attn<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &) [with Kernel_traits=Flash_fwd_kernel_traits<32, 128, 128, 4, false, false, cutlass::bfloat16_t, Flash_kernel_traits<32, 128, 128, 4, cutlass::bfloat16_t>>, Is_dropout=true, Is_causal=false, Is_even_N=false, Is_even_K=false, Return_softmax=false, Params=Flash_fwd_params]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(15): here
            instantiation of "void flash_fwd_kernel<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax>(Flash_fwd_params) [with Kernel_traits=Flash_fwd_kernel_traits<32, 128, 128, 4, false, false, cutlass::bfloat16_t, Flash_kernel_traits<32, 128, 128, 4, cutlass::bfloat16_t>>, Is_dropout=true, Is_causal=false, Is_even_N=false, Is_even_K=false, Return_softmax=false]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(34): here
            instantiation of "void run_flash_fwd<Kernel_traits,Is_dropout,Is_causal>(Flash_fwd_params &, cudaStream_t) [with Kernel_traits=Flash_fwd_kernel_traits<32, 128, 128, 4, false, false, cutlass::bfloat16_t, Flash_kernel_traits<32, 128, 128, 4, cutlass::bfloat16_t>>, Is_dropout=true, Is_causal=false]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(58): here
            instantiation of "void run_mha_fwd_hdim32<T>(Flash_fwd_params &, cudaStream_t) [with T=cutlass::bfloat16_t]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_hdim32_bf16_sm80.cu(9): here

/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_kernel.h(276): warning: variable "tQrQ" was declared but never referenced
          detected during:
            instantiation of "void flash::compute_attn_1rowblock<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params,Is_Q_split_K>(const Params &, int, int, int) [with Kernel_traits=Flash_fwd_kernel_traits<32, 128, 128, 4, false, false, cutlass::bfloat16_t, Flash_kernel_traits<32, 128, 128, 4, cutlass::bfloat16_t>>, Is_dropout=true, Is_causal=false, Is_even_N=false, Is_even_K=false, Return_softmax=false, Params=Flash_fwd_params, Is_Q_split_K=false]" 
(589): here
            instantiation of "void flash::compute_attn<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &) [with Kernel_traits=Flash_fwd_kernel_traits<32, 128, 128, 4, false, false, cutlass::bfloat16_t, Flash_kernel_traits<32, 128, 128, 4, cutlass::bfloat16_t>>, Is_dropout=true, Is_causal=false, Is_even_N=false, Is_even_K=false, Return_softmax=false, Params=Flash_fwd_params]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(15): here
            instantiation of "void flash_fwd_kernel<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax>(Flash_fwd_params) [with Kernel_traits=Flash_fwd_kernel_traits<32, 128, 128, 4, false, false, cutlass::bfloat16_t, Flash_kernel_traits<32, 128, 128, 4, cutlass::bfloat16_t>>, Is_dropout=true, Is_causal=false, Is_even_N=false, Is_even_K=false, Return_softmax=false]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(34): here
            instantiation of "void run_flash_fwd<Kernel_traits,Is_dropout,Is_causal>(Flash_fwd_params &, cudaStream_t) [with Kernel_traits=Flash_fwd_kernel_traits<32, 128, 128, 4, false, false, cutlass::bfloat16_t, Flash_kernel_traits<32, 128, 128, 4, cutlass::bfloat16_t>>, Is_dropout=true, Is_causal=false]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(58): here
            instantiation of "void run_mha_fwd_hdim32<T>(Flash_fwd_params &, cudaStream_t) [with T=cutlass::bfloat16_t]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_hdim32_bf16_sm80.cu(9): here

/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_kernel.h(140): warning: variable "MMA_M" was declared but never referenced
          detected during:
            instantiation of "void flash::compute_attn_1rowblock<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params,Is_Q_split_K>(const Params &, int, int, int) [with Kernel_traits=Flash_fwd_kernel_traits<32, 128, 128, 4, false, false, cutlass::bfloat16_t, Flash_kernel_traits<32, 128, 128, 4, cutlass::bfloat16_t>>, Is_dropout=false, Is_causal=true, Is_even_N=true, Is_even_K=true, Return_softmax=false, Params=Flash_fwd_params, Is_Q_split_K=false]" 
(589): here
            instantiation of "void flash::compute_attn<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &) [with Kernel_traits=Flash_fwd_kernel_traits<32, 128, 128, 4, false, false, cutlass::bfloat16_t, Flash_kernel_traits<32, 128, 128, 4, cutlass::bfloat16_t>>, Is_dropout=false, Is_causal=true, Is_even_N=true, Is_even_K=true, Return_softmax=false, Params=Flash_fwd_params]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(15): here
            instantiation of "void flash_fwd_kernel<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax>(Flash_fwd_params) [with Kernel_traits=Flash_fwd_kernel_traits<32, 128, 128, 4, false, false, cutlass::bfloat16_t, Flash_kernel_traits<32, 128, 128, 4, cutlass::bfloat16_t>>, Is_dropout=false, Is_causal=true, Is_even_N=true, Is_even_K=true, Return_softmax=false]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(34): here
            instantiation of "void run_flash_fwd<Kernel_traits,Is_dropout,Is_causal>(Flash_fwd_params &, cudaStream_t) [with Kernel_traits=Flash_fwd_kernel_traits<32, 128, 128, 4, false, false, cutlass::bfloat16_t, Flash_kernel_traits<32, 128, 128, 4, cutlass::bfloat16_t>>, Is_dropout=false, Is_causal=true]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(58): here
            instantiation of "void run_mha_fwd_hdim32<T>(Flash_fwd_params &, cudaStream_t) [with T=cutlass::bfloat16_t]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_hdim32_bf16_sm80.cu(9): here

/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_kernel.h(276): warning: variable "tQrQ" was declared but never referenced
          detected during:
            instantiation of "void flash::compute_attn_1rowblock<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params,Is_Q_split_K>(const Params &, int, int, int) [with Kernel_traits=Flash_fwd_kernel_traits<32, 128, 128, 4, false, false, cutlass::bfloat16_t, Flash_kernel_traits<32, 128, 128, 4, cutlass::bfloat16_t>>, Is_dropout=false, Is_causal=true, Is_even_N=true, Is_even_K=true, Return_softmax=false, Params=Flash_fwd_params, Is_Q_split_K=false]" 
(589): here
            instantiation of "void flash::compute_attn<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &) [with Kernel_traits=Flash_fwd_kernel_traits<32, 128, 128, 4, false, false, cutlass::bfloat16_t, Flash_kernel_traits<32, 128, 128, 4, cutlass::bfloat16_t>>, Is_dropout=false, Is_causal=true, Is_even_N=true, Is_even_K=true, Return_softmax=false, Params=Flash_fwd_params]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(15): here
            instantiation of "void flash_fwd_kernel<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax>(Flash_fwd_params) [with Kernel_traits=Flash_fwd_kernel_traits<32, 128, 128, 4, false, false, cutlass::bfloat16_t, Flash_kernel_traits<32, 128, 128, 4, cutlass::bfloat16_t>>, Is_dropout=false, Is_causal=true, Is_even_N=true, Is_even_K=true, Return_softmax=false]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(34): here
            instantiation of "void run_flash_fwd<Kernel_traits,Is_dropout,Is_causal>(Flash_fwd_params &, cudaStream_t) [with Kernel_traits=Flash_fwd_kernel_traits<32, 128, 128, 4, false, false, cutlass::bfloat16_t, Flash_kernel_traits<32, 128, 128, 4, cutlass::bfloat16_t>>, Is_dropout=false, Is_causal=true]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(58): here
            instantiation of "void run_mha_fwd_hdim32<T>(Flash_fwd_params &, cudaStream_t) [with T=cutlass::bfloat16_t]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_hdim32_bf16_sm80.cu(9): here

/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_kernel.h(140): warning: variable "MMA_M" was declared but never referenced
          detected during:
            instantiation of "void flash::compute_attn_1rowblock<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params,Is_Q_split_K>(const Params &, int, int, int) [with Kernel_traits=Flash_fwd_kernel_traits<32, 128, 128, 4, false, false, cutlass::bfloat16_t, Flash_kernel_traits<32, 128, 128, 4, cutlass::bfloat16_t>>, Is_dropout=false, Is_causal=true, Is_even_N=true, Is_even_K=false, Return_softmax=false, Params=Flash_fwd_params, Is_Q_split_K=false]" 
(589): here
            instantiation of "void flash::compute_attn<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &) [with Kernel_traits=Flash_fwd_kernel_traits<32, 128, 128, 4, false, false, cutlass::bfloat16_t, Flash_kernel_traits<32, 128, 128, 4, cutlass::bfloat16_t>>, Is_dropout=false, Is_causal=true, Is_even_N=true, Is_even_K=false, Return_softmax=false, Params=Flash_fwd_params]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(15): here
            instantiation of "void flash_fwd_kernel<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax>(Flash_fwd_params) [with Kernel_traits=Flash_fwd_kernel_traits<32, 128, 128, 4, false, false, cutlass::bfloat16_t, Flash_kernel_traits<32, 128, 128, 4, cutlass::bfloat16_t>>, Is_dropout=false, Is_causal=true, Is_even_N=true, Is_even_K=false, Return_softmax=false]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(34): here
            instantiation of "void run_flash_fwd<Kernel_traits,Is_dropout,Is_causal>(Flash_fwd_params &, cudaStream_t) [with Kernel_traits=Flash_fwd_kernel_traits<32, 128, 128, 4, false, false, cutlass::bfloat16_t, Flash_kernel_traits<32, 128, 128, 4, cutlass::bfloat16_t>>, Is_dropout=false, Is_causal=true]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(58): here
            instantiation of "void run_mha_fwd_hdim32<T>(Flash_fwd_params &, cudaStream_t) [with T=cutlass::bfloat16_t]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_hdim32_bf16_sm80.cu(9): here

/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_kernel.h(276): warning: variable "tQrQ" was declared but never referenced
          detected during:
            instantiation of "void flash::compute_attn_1rowblock<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params,Is_Q_split_K>(const Params &, int, int, int) [with Kernel_traits=Flash_fwd_kernel_traits<32, 128, 128, 4, false, false, cutlass::bfloat16_t, Flash_kernel_traits<32, 128, 128, 4, cutlass::bfloat16_t>>, Is_dropout=false, Is_causal=true, Is_even_N=true, Is_even_K=false, Return_softmax=false, Params=Flash_fwd_params, Is_Q_split_K=false]" 
(589): here
            instantiation of "void flash::compute_attn<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &) [with Kernel_traits=Flash_fwd_kernel_traits<32, 128, 128, 4, false, false, cutlass::bfloat16_t, Flash_kernel_traits<32, 128, 128, 4, cutlass::bfloat16_t>>, Is_dropout=false, Is_causal=true, Is_even_N=true, Is_even_K=false, Return_softmax=false, Params=Flash_fwd_params]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(15): here
            instantiation of "void flash_fwd_kernel<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax>(Flash_fwd_params) [with Kernel_traits=Flash_fwd_kernel_traits<32, 128, 128, 4, false, false, cutlass::bfloat16_t, Flash_kernel_traits<32, 128, 128, 4, cutlass::bfloat16_t>>, Is_dropout=false, Is_causal=true, Is_even_N=true, Is_even_K=false, Return_softmax=false]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(34): here
            instantiation of "void run_flash_fwd<Kernel_traits,Is_dropout,Is_causal>(Flash_fwd_params &, cudaStream_t) [with Kernel_traits=Flash_fwd_kernel_traits<32, 128, 128, 4, false, false, cutlass::bfloat16_t, Flash_kernel_traits<32, 128, 128, 4, cutlass::bfloat16_t>>, Is_dropout=false, Is_causal=true]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(58): here
            instantiation of "void run_mha_fwd_hdim32<T>(Flash_fwd_params &, cudaStream_t) [with T=cutlass::bfloat16_t]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_hdim32_bf16_sm80.cu(9): here

/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_kernel.h(140): warning: variable "MMA_M" was declared but never referenced
          detected during:
            instantiation of "void flash::compute_attn_1rowblock<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params,Is_Q_split_K>(const Params &, int, int, int) [with Kernel_traits=Flash_fwd_kernel_traits<32, 128, 128, 4, false, false, cutlass::bfloat16_t, Flash_kernel_traits<32, 128, 128, 4, cutlass::bfloat16_t>>, Is_dropout=false, Is_causal=true, Is_even_N=false, Is_even_K=true, Return_softmax=false, Params=Flash_fwd_params, Is_Q_split_K=false]" 
(589): here
            instantiation of "void flash::compute_attn<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &) [with Kernel_traits=Flash_fwd_kernel_traits<32, 128, 128, 4, false, false, cutlass::bfloat16_t, Flash_kernel_traits<32, 128, 128, 4, cutlass::bfloat16_t>>, Is_dropout=false, Is_causal=true, Is_even_N=false, Is_even_K=true, Return_softmax=false, Params=Flash_fwd_params]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(15): here
            instantiation of "void flash_fwd_kernel<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax>(Flash_fwd_params) [with Kernel_traits=Flash_fwd_kernel_traits<32, 128, 128, 4, false, false, cutlass::bfloat16_t, Flash_kernel_traits<32, 128, 128, 4, cutlass::bfloat16_t>>, Is_dropout=false, Is_causal=true, Is_even_N=false, Is_even_K=true, Return_softmax=false]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(34): here
            instantiation of "void run_flash_fwd<Kernel_traits,Is_dropout,Is_causal>(Flash_fwd_params &, cudaStream_t) [with Kernel_traits=Flash_fwd_kernel_traits<32, 128, 128, 4, false, false, cutlass::bfloat16_t, Flash_kernel_traits<32, 128, 128, 4, cutlass::bfloat16_t>>, Is_dropout=false, Is_causal=true]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(58): here
            instantiation of "void run_mha_fwd_hdim32<T>(Flash_fwd_params &, cudaStream_t) [with T=cutlass::bfloat16_t]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_hdim32_bf16_sm80.cu(9): here

/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_kernel.h(276): warning: variable "tQrQ" was declared but never referenced
          detected during:
            instantiation of "void flash::compute_attn_1rowblock<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params,Is_Q_split_K>(const Params &, int, int, int) [with Kernel_traits=Flash_fwd_kernel_traits<32, 128, 128, 4, false, false, cutlass::bfloat16_t, Flash_kernel_traits<32, 128, 128, 4, cutlass::bfloat16_t>>, Is_dropout=false, Is_causal=true, Is_even_N=false, Is_even_K=true, Return_softmax=false, Params=Flash_fwd_params, Is_Q_split_K=false]" 
(589): here
            instantiation of "void flash::compute_attn<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &) [with Kernel_traits=Flash_fwd_kernel_traits<32, 128, 128, 4, false, false, cutlass::bfloat16_t, Flash_kernel_traits<32, 128, 128, 4, cutlass::bfloat16_t>>, Is_dropout=false, Is_causal=true, Is_even_N=false, Is_even_K=true, Return_softmax=false, Params=Flash_fwd_params]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(15): here
            instantiation of "void flash_fwd_kernel<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax>(Flash_fwd_params) [with Kernel_traits=Flash_fwd_kernel_traits<32, 128, 128, 4, false, false, cutlass::bfloat16_t, Flash_kernel_traits<32, 128, 128, 4, cutlass::bfloat16_t>>, Is_dropout=false, Is_causal=true, Is_even_N=false, Is_even_K=true, Return_softmax=false]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(34): here
            instantiation of "void run_flash_fwd<Kernel_traits,Is_dropout,Is_causal>(Flash_fwd_params &, cudaStream_t) [with Kernel_traits=Flash_fwd_kernel_traits<32, 128, 128, 4, false, false, cutlass::bfloat16_t, Flash_kernel_traits<32, 128, 128, 4, cutlass::bfloat16_t>>, Is_dropout=false, Is_causal=true]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(58): here
            instantiation of "void run_mha_fwd_hdim32<T>(Flash_fwd_params &, cudaStream_t) [with T=cutlass::bfloat16_t]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_hdim32_bf16_sm80.cu(9): here

/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_kernel.h(140): warning: variable "MMA_M" was declared but never referenced
          detected during:
            instantiation of "void flash::compute_attn_1rowblock<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params,Is_Q_split_K>(const Params &, int, int, int) [with Kernel_traits=Flash_fwd_kernel_traits<32, 128, 128, 4, false, false, cutlass::bfloat16_t, Flash_kernel_traits<32, 128, 128, 4, cutlass::bfloat16_t>>, Is_dropout=false, Is_causal=true, Is_even_N=false, Is_even_K=false, Return_softmax=false, Params=Flash_fwd_params, Is_Q_split_K=false]" 
(589): here
            instantiation of "void flash::compute_attn<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &) [with Kernel_traits=Flash_fwd_kernel_traits<32, 128, 128, 4, false, false, cutlass::bfloat16_t, Flash_kernel_traits<32, 128, 128, 4, cutlass::bfloat16_t>>, Is_dropout=false, Is_causal=true, Is_even_N=false, Is_even_K=false, Return_softmax=false, Params=Flash_fwd_params]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(15): here
            instantiation of "void flash_fwd_kernel<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax>(Flash_fwd_params) [with Kernel_traits=Flash_fwd_kernel_traits<32, 128, 128, 4, false, false, cutlass::bfloat16_t, Flash_kernel_traits<32, 128, 128, 4, cutlass::bfloat16_t>>, Is_dropout=false, Is_causal=true, Is_even_N=false, Is_even_K=false, Return_softmax=false]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(34): here
            instantiation of "void run_flash_fwd<Kernel_traits,Is_dropout,Is_causal>(Flash_fwd_params &, cudaStream_t) [with Kernel_traits=Flash_fwd_kernel_traits<32, 128, 128, 4, false, false, cutlass::bfloat16_t, Flash_kernel_traits<32, 128, 128, 4, cutlass::bfloat16_t>>, Is_dropout=false, Is_causal=true]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(58): here
            instantiation of "void run_mha_fwd_hdim32<T>(Flash_fwd_params &, cudaStream_t) [with T=cutlass::bfloat16_t]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_hdim32_bf16_sm80.cu(9): here

/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_kernel.h(276): warning: variable "tQrQ" was declared but never referenced
          detected during:
            instantiation of "void flash::compute_attn_1rowblock<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params,Is_Q_split_K>(const Params &, int, int, int) [with Kernel_traits=Flash_fwd_kernel_traits<32, 128, 128, 4, false, false, cutlass::bfloat16_t, Flash_kernel_traits<32, 128, 128, 4, cutlass::bfloat16_t>>, Is_dropout=false, Is_causal=true, Is_even_N=false, Is_even_K=false, Return_softmax=false, Params=Flash_fwd_params, Is_Q_split_K=false]" 
(589): here
            instantiation of "void flash::compute_attn<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &) [with Kernel_traits=Flash_fwd_kernel_traits<32, 128, 128, 4, false, false, cutlass::bfloat16_t, Flash_kernel_traits<32, 128, 128, 4, cutlass::bfloat16_t>>, Is_dropout=false, Is_causal=true, Is_even_N=false, Is_even_K=false, Return_softmax=false, Params=Flash_fwd_params]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(15): here
            instantiation of "void flash_fwd_kernel<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax>(Flash_fwd_params) [with Kernel_traits=Flash_fwd_kernel_traits<32, 128, 128, 4, false, false, cutlass::bfloat16_t, Flash_kernel_traits<32, 128, 128, 4, cutlass::bfloat16_t>>, Is_dropout=false, Is_causal=true, Is_even_N=false, Is_even_K=false, Return_softmax=false]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(34): here
            instantiation of "void run_flash_fwd<Kernel_traits,Is_dropout,Is_causal>(Flash_fwd_params &, cudaStream_t) [with Kernel_traits=Flash_fwd_kernel_traits<32, 128, 128, 4, false, false, cutlass::bfloat16_t, Flash_kernel_traits<32, 128, 128, 4, cutlass::bfloat16_t>>, Is_dropout=false, Is_causal=true]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(58): here
            instantiation of "void run_mha_fwd_hdim32<T>(Flash_fwd_params &, cudaStream_t) [with T=cutlass::bfloat16_t]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_hdim32_bf16_sm80.cu(9): here

/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_kernel.h(140): warning: variable "MMA_M" was declared but never referenced
          detected during:
            instantiation of "void flash::compute_attn_1rowblock<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params,Is_Q_split_K>(const Params &, int, int, int) [with Kernel_traits=Flash_fwd_kernel_traits<32, 128, 128, 4, false, false, cutlass::bfloat16_t, Flash_kernel_traits<32, 128, 128, 4, cutlass::bfloat16_t>>, Is_dropout=false, Is_causal=false, Is_even_N=true, Is_even_K=true, Return_softmax=false, Params=Flash_fwd_params, Is_Q_split_K=false]" 
(589): here
            instantiation of "void flash::compute_attn<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &) [with Kernel_traits=Flash_fwd_kernel_traits<32, 128, 128, 4, false, false, cutlass::bfloat16_t, Flash_kernel_traits<32, 128, 128, 4, cutlass::bfloat16_t>>, Is_dropout=false, Is_causal=false, Is_even_N=true, Is_even_K=true, Return_softmax=false, Params=Flash_fwd_params]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(15): here
            instantiation of "void flash_fwd_kernel<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax>(Flash_fwd_params) [with Kernel_traits=Flash_fwd_kernel_traits<32, 128, 128, 4, false, false, cutlass::bfloat16_t, Flash_kernel_traits<32, 128, 128, 4, cutlass::bfloat16_t>>, Is_dropout=false, Is_causal=false, Is_even_N=true, Is_even_K=true, Return_softmax=false]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(34): here
            instantiation of "void run_flash_fwd<Kernel_traits,Is_dropout,Is_causal>(Flash_fwd_params &, cudaStream_t) [with Kernel_traits=Flash_fwd_kernel_traits<32, 128, 128, 4, false, false, cutlass::bfloat16_t, Flash_kernel_traits<32, 128, 128, 4, cutlass::bfloat16_t>>, Is_dropout=false, Is_causal=false]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(58): here
            instantiation of "void run_mha_fwd_hdim32<T>(Flash_fwd_params &, cudaStream_t) [with T=cutlass::bfloat16_t]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_hdim32_bf16_sm80.cu(9): here

/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_kernel.h(276): warning: variable "tQrQ" was declared but never referenced
          detected during:
            instantiation of "void flash::compute_attn_1rowblock<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params,Is_Q_split_K>(const Params &, int, int, int) [with Kernel_traits=Flash_fwd_kernel_traits<32, 128, 128, 4, false, false, cutlass::bfloat16_t, Flash_kernel_traits<32, 128, 128, 4, cutlass::bfloat16_t>>, Is_dropout=false, Is_causal=false, Is_even_N=true, Is_even_K=true, Return_softmax=false, Params=Flash_fwd_params, Is_Q_split_K=false]" 
(589): here
            instantiation of "void flash::compute_attn<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &) [with Kernel_traits=Flash_fwd_kernel_traits<32, 128, 128, 4, false, false, cutlass::bfloat16_t, Flash_kernel_traits<32, 128, 128, 4, cutlass::bfloat16_t>>, Is_dropout=false, Is_causal=false, Is_even_N=true, Is_even_K=true, Return_softmax=false, Params=Flash_fwd_params]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(15): here
            instantiation of "void flash_fwd_kernel<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax>(Flash_fwd_params) [with Kernel_traits=Flash_fwd_kernel_traits<32, 128, 128, 4, false, false, cutlass::bfloat16_t, Flash_kernel_traits<32, 128, 128, 4, cutlass::bfloat16_t>>, Is_dropout=false, Is_causal=false, Is_even_N=true, Is_even_K=true, Return_softmax=false]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(34): here
            instantiation of "void run_flash_fwd<Kernel_traits,Is_dropout,Is_causal>(Flash_fwd_params &, cudaStream_t) [with Kernel_traits=Flash_fwd_kernel_traits<32, 128, 128, 4, false, false, cutlass::bfloat16_t, Flash_kernel_traits<32, 128, 128, 4, cutlass::bfloat16_t>>, Is_dropout=false, Is_causal=false]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(58): here
            instantiation of "void run_mha_fwd_hdim32<T>(Flash_fwd_params &, cudaStream_t) [with T=cutlass::bfloat16_t]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_hdim32_bf16_sm80.cu(9): here

/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_kernel.h(140): warning: variable "MMA_M" was declared but never referenced
          detected during:
            instantiation of "void flash::compute_attn_1rowblock<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params,Is_Q_split_K>(const Params &, int, int, int) [with Kernel_traits=Flash_fwd_kernel_traits<32, 128, 128, 4, false, false, cutlass::bfloat16_t, Flash_kernel_traits<32, 128, 128, 4, cutlass::bfloat16_t>>, Is_dropout=false, Is_causal=false, Is_even_N=true, Is_even_K=false, Return_softmax=false, Params=Flash_fwd_params, Is_Q_split_K=false]" 
(589): here
            instantiation of "void flash::compute_attn<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &) [with Kernel_traits=Flash_fwd_kernel_traits<32, 128, 128, 4, false, false, cutlass::bfloat16_t, Flash_kernel_traits<32, 128, 128, 4, cutlass::bfloat16_t>>, Is_dropout=false, Is_causal=false, Is_even_N=true, Is_even_K=false, Return_softmax=false, Params=Flash_fwd_params]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(15): here
            instantiation of "void flash_fwd_kernel<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax>(Flash_fwd_params) [with Kernel_traits=Flash_fwd_kernel_traits<32, 128, 128, 4, false, false, cutlass::bfloat16_t, Flash_kernel_traits<32, 128, 128, 4, cutlass::bfloat16_t>>, Is_dropout=false, Is_causal=false, Is_even_N=true, Is_even_K=false, Return_softmax=false]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(34): here
            instantiation of "void run_flash_fwd<Kernel_traits,Is_dropout,Is_causal>(Flash_fwd_params &, cudaStream_t) [with Kernel_traits=Flash_fwd_kernel_traits<32, 128, 128, 4, false, false, cutlass::bfloat16_t, Flash_kernel_traits<32, 128, 128, 4, cutlass::bfloat16_t>>, Is_dropout=false, Is_causal=false]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(58): here
            instantiation of "void run_mha_fwd_hdim32<T>(Flash_fwd_params &, cudaStream_t) [with T=cutlass::bfloat16_t]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_hdim32_bf16_sm80.cu(9): here

/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_kernel.h(276): warning: variable "tQrQ" was declared but never referenced
          detected during:
            instantiation of "void flash::compute_attn_1rowblock<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params,Is_Q_split_K>(const Params &, int, int, int) [with Kernel_traits=Flash_fwd_kernel_traits<32, 128, 128, 4, false, false, cutlass::bfloat16_t, Flash_kernel_traits<32, 128, 128, 4, cutlass::bfloat16_t>>, Is_dropout=false, Is_causal=false, Is_even_N=true, Is_even_K=false, Return_softmax=false, Params=Flash_fwd_params, Is_Q_split_K=false]" 
(589): here
            instantiation of "void flash::compute_attn<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &) [with Kernel_traits=Flash_fwd_kernel_traits<32, 128, 128, 4, false, false, cutlass::bfloat16_t, Flash_kernel_traits<32, 128, 128, 4, cutlass::bfloat16_t>>, Is_dropout=false, Is_causal=false, Is_even_N=true, Is_even_K=false, Return_softmax=false, Params=Flash_fwd_params]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(15): here
            instantiation of "void flash_fwd_kernel<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax>(Flash_fwd_params) [with Kernel_traits=Flash_fwd_kernel_traits<32, 128, 128, 4, false, false, cutlass::bfloat16_t, Flash_kernel_traits<32, 128, 128, 4, cutlass::bfloat16_t>>, Is_dropout=false, Is_causal=false, Is_even_N=true, Is_even_K=false, Return_softmax=false]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(34): here
            instantiation of "void run_flash_fwd<Kernel_traits,Is_dropout,Is_causal>(Flash_fwd_params &, cudaStream_t) [with Kernel_traits=Flash_fwd_kernel_traits<32, 128, 128, 4, false, false, cutlass::bfloat16_t, Flash_kernel_traits<32, 128, 128, 4, cutlass::bfloat16_t>>, Is_dropout=false, Is_causal=false]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(58): here
            instantiation of "void run_mha_fwd_hdim32<T>(Flash_fwd_params &, cudaStream_t) [with T=cutlass::bfloat16_t]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_hdim32_bf16_sm80.cu(9): here

/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_kernel.h(140): warning: variable "MMA_M" was declared but never referenced
          detected during:
            instantiation of "void flash::compute_attn_1rowblock<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params,Is_Q_split_K>(const Params &, int, int, int) [with Kernel_traits=Flash_fwd_kernel_traits<32, 128, 128, 4, false, false, cutlass::bfloat16_t, Flash_kernel_traits<32, 128, 128, 4, cutlass::bfloat16_t>>, Is_dropout=false, Is_causal=false, Is_even_N=false, Is_even_K=true, Return_softmax=false, Params=Flash_fwd_params, Is_Q_split_K=false]" 
(589): here
            instantiation of "void flash::compute_attn<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &) [with Kernel_traits=Flash_fwd_kernel_traits<32, 128, 128, 4, false, false, cutlass::bfloat16_t, Flash_kernel_traits<32, 128, 128, 4, cutlass::bfloat16_t>>, Is_dropout=false, Is_causal=false, Is_even_N=false, Is_even_K=true, Return_softmax=false, Params=Flash_fwd_params]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(15): here
            instantiation of "void flash_fwd_kernel<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax>(Flash_fwd_params) [with Kernel_traits=Flash_fwd_kernel_traits<32, 128, 128, 4, false, false, cutlass::bfloat16_t, Flash_kernel_traits<32, 128, 128, 4, cutlass::bfloat16_t>>, Is_dropout=false, Is_causal=false, Is_even_N=false, Is_even_K=true, Return_softmax=false]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(34): here
            instantiation of "void run_flash_fwd<Kernel_traits,Is_dropout,Is_causal>(Flash_fwd_params &, cudaStream_t) [with Kernel_traits=Flash_fwd_kernel_traits<32, 128, 128, 4, false, false, cutlass::bfloat16_t, Flash_kernel_traits<32, 128, 128, 4, cutlass::bfloat16_t>>, Is_dropout=false, Is_causal=false]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(58): here
            instantiation of "void run_mha_fwd_hdim32<T>(Flash_fwd_params &, cudaStream_t) [with T=cutlass::bfloat16_t]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_hdim32_bf16_sm80.cu(9): here

/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_kernel.h(276): warning: variable "tQrQ" was declared but never referenced
          detected during:
            instantiation of "void flash::compute_attn_1rowblock<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params,Is_Q_split_K>(const Params &, int, int, int) [with Kernel_traits=Flash_fwd_kernel_traits<32, 128, 128, 4, false, false, cutlass::bfloat16_t, Flash_kernel_traits<32, 128, 128, 4, cutlass::bfloat16_t>>, Is_dropout=false, Is_causal=false, Is_even_N=false, Is_even_K=true, Return_softmax=false, Params=Flash_fwd_params, Is_Q_split_K=false]" 
(589): here
            instantiation of "void flash::compute_attn<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &) [with Kernel_traits=Flash_fwd_kernel_traits<32, 128, 128, 4, false, false, cutlass::bfloat16_t, Flash_kernel_traits<32, 128, 128, 4, cutlass::bfloat16_t>>, Is_dropout=false, Is_causal=false, Is_even_N=false, Is_even_K=true, Return_softmax=false, Params=Flash_fwd_params]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(15): here
            instantiation of "void flash_fwd_kernel<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax>(Flash_fwd_params) [with Kernel_traits=Flash_fwd_kernel_traits<32, 128, 128, 4, false, false, cutlass::bfloat16_t, Flash_kernel_traits<32, 128, 128, 4, cutlass::bfloat16_t>>, Is_dropout=false, Is_causal=false, Is_even_N=false, Is_even_K=true, Return_softmax=false]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(34): here
            instantiation of "void run_flash_fwd<Kernel_traits,Is_dropout,Is_causal>(Flash_fwd_params &, cudaStream_t) [with Kernel_traits=Flash_fwd_kernel_traits<32, 128, 128, 4, false, false, cutlass::bfloat16_t, Flash_kernel_traits<32, 128, 128, 4, cutlass::bfloat16_t>>, Is_dropout=false, Is_causal=false]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(58): here
            instantiation of "void run_mha_fwd_hdim32<T>(Flash_fwd_params &, cudaStream_t) [with T=cutlass::bfloat16_t]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_hdim32_bf16_sm80.cu(9): here

/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_kernel.h(140): warning: variable "MMA_M" was declared but never referenced
          detected during:
            instantiation of "void flash::compute_attn_1rowblock<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params,Is_Q_split_K>(const Params &, int, int, int) [with Kernel_traits=Flash_fwd_kernel_traits<32, 128, 128, 4, false, false, cutlass::bfloat16_t, Flash_kernel_traits<32, 128, 128, 4, cutlass::bfloat16_t>>, Is_dropout=false, Is_causal=false, Is_even_N=false, Is_even_K=false, Return_softmax=false, Params=Flash_fwd_params, Is_Q_split_K=false]" 
(589): here
            instantiation of "void flash::compute_attn<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &) [with Kernel_traits=Flash_fwd_kernel_traits<32, 128, 128, 4, false, false, cutlass::bfloat16_t, Flash_kernel_traits<32, 128, 128, 4, cutlass::bfloat16_t>>, Is_dropout=false, Is_causal=false, Is_even_N=false, Is_even_K=false, Return_softmax=false, Params=Flash_fwd_params]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(15): here
            instantiation of "void flash_fwd_kernel<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax>(Flash_fwd_params) [with Kernel_traits=Flash_fwd_kernel_traits<32, 128, 128, 4, false, false, cutlass::bfloat16_t, Flash_kernel_traits<32, 128, 128, 4, cutlass::bfloat16_t>>, Is_dropout=false, Is_causal=false, Is_even_N=false, Is_even_K=false, Return_softmax=false]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(34): here
            instantiation of "void run_flash_fwd<Kernel_traits,Is_dropout,Is_causal>(Flash_fwd_params &, cudaStream_t) [with Kernel_traits=Flash_fwd_kernel_traits<32, 128, 128, 4, false, false, cutlass::bfloat16_t, Flash_kernel_traits<32, 128, 128, 4, cutlass::bfloat16_t>>, Is_dropout=false, Is_causal=false]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(58): here
            instantiation of "void run_mha_fwd_hdim32<T>(Flash_fwd_params &, cudaStream_t) [with T=cutlass::bfloat16_t]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_hdim32_bf16_sm80.cu(9): here

/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_kernel.h(276): warning: variable "tQrQ" was declared but never referenced
          detected during:
            instantiation of "void flash::compute_attn_1rowblock<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params,Is_Q_split_K>(const Params &, int, int, int) [with Kernel_traits=Flash_fwd_kernel_traits<32, 128, 128, 4, false, false, cutlass::bfloat16_t, Flash_kernel_traits<32, 128, 128, 4, cutlass::bfloat16_t>>, Is_dropout=false, Is_causal=false, Is_even_N=false, Is_even_K=false, Return_softmax=false, Params=Flash_fwd_params, Is_Q_split_K=false]" 
(589): here
            instantiation of "void flash::compute_attn<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &) [with Kernel_traits=Flash_fwd_kernel_traits<32, 128, 128, 4, false, false, cutlass::bfloat16_t, Flash_kernel_traits<32, 128, 128, 4, cutlass::bfloat16_t>>, Is_dropout=false, Is_causal=false, Is_even_N=false, Is_even_K=false, Return_softmax=false, Params=Flash_fwd_params]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(15): here
            instantiation of "void flash_fwd_kernel<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax>(Flash_fwd_params) [with Kernel_traits=Flash_fwd_kernel_traits<32, 128, 128, 4, false, false, cutlass::bfloat16_t, Flash_kernel_traits<32, 128, 128, 4, cutlass::bfloat16_t>>, Is_dropout=false, Is_causal=false, Is_even_N=false, Is_even_K=false, Return_softmax=false]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(34): here
            instantiation of "void run_flash_fwd<Kernel_traits,Is_dropout,Is_causal>(Flash_fwd_params &, cudaStream_t) [with Kernel_traits=Flash_fwd_kernel_traits<32, 128, 128, 4, false, false, cutlass::bfloat16_t, Flash_kernel_traits<32, 128, 128, 4, cutlass::bfloat16_t>>, Is_dropout=false, Is_causal=false]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(58): here
            instantiation of "void run_mha_fwd_hdim32<T>(Flash_fwd_params &, cudaStream_t) [with T=cutlass::bfloat16_t]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_hdim32_bf16_sm80.cu(9): here

ptxas info    : 2 bytes gmem
ptxas info    : Compiling entry function '_Z16flash_fwd_kernelI23Flash_fwd_kernel_traitsILi32ELi128ELi128ELi4ELb0ELb0EN7cutlass10bfloat16_tE19Flash_kernel_traitsILi32ELi128ELi128ELi4ES2_EELb0ELb0ELb0ELb0ELb0EEv16Flash_fwd_params' for 'sm_80'
ptxas info    : Function properties for _Z16flash_fwd_kernelI23Flash_fwd_kernel_traitsILi32ELi128ELi128ELi4ELb0ELb0EN7cutlass10bfloat16_tE19Flash_kernel_traitsILi32ELi128ELi128ELi4ES2_EELb0ELb0ELb0ELb0ELb0EEv16Flash_fwd_params
    0 bytes stack frame, 0 bytes spill stores, 0 bytes spill loads
ptxas info    : Used 254 registers, 584 bytes cmem[0]
ptxas info    : Compiling entry function '_Z16flash_fwd_kernelI23Flash_fwd_kernel_traitsILi32ELi128ELi128ELi4ELb0ELb0EN7cutlass10bfloat16_tE19Flash_kernel_traitsILi32ELi128ELi128ELi4ES2_EELb0ELb0ELb0ELb1ELb0EEv16Flash_fwd_params' for 'sm_80'
ptxas info    : Function properties for _Z16flash_fwd_kernelI23Flash_fwd_kernel_traitsILi32ELi128ELi128ELi4ELb0ELb0EN7cutlass10bfloat16_tE19Flash_kernel_traitsILi32ELi128ELi128ELi4ES2_EELb0ELb0ELb0ELb1ELb0EEv16Flash_fwd_params
    0 bytes stack frame, 0 bytes spill stores, 0 bytes spill loads
ptxas info    : Used 254 registers, 584 bytes cmem[0]
ptxas info    : Compiling entry function '_Z16flash_fwd_kernelI23Flash_fwd_kernel_traitsILi32ELi128ELi128ELi4ELb0ELb0EN7cutlass10bfloat16_tE19Flash_kernel_traitsILi32ELi128ELi128ELi4ES2_EELb0ELb0ELb1ELb0ELb0EEv16Flash_fwd_params' for 'sm_80'
ptxas info    : Function properties for _Z16flash_fwd_kernelI23Flash_fwd_kernel_traitsILi32ELi128ELi128ELi4ELb0ELb0EN7cutlass10bfloat16_tE19Flash_kernel_traitsILi32ELi128ELi128ELi4ES2_EELb0ELb0ELb1ELb0ELb0EEv16Flash_fwd_params
    0 bytes stack frame, 0 bytes spill stores, 0 bytes spill loads
ptxas info    : Used 232 registers, 584 bytes cmem[0]
ptxas info    : Compiling entry function '_Z16flash_fwd_kernelI23Flash_fwd_kernel_traitsILi32ELi128ELi128ELi4ELb0ELb0EN7cutlass10bfloat16_tE19Flash_kernel_traitsILi32ELi128ELi128ELi4ES2_EELb0ELb0ELb1ELb1ELb0EEv16Flash_fwd_params' for 'sm_80'
ptxas info    : Function properties for _Z16flash_fwd_kernelI23Flash_fwd_kernel_traitsILi32ELi128ELi128ELi4ELb0ELb0EN7cutlass10bfloat16_tE19Flash_kernel_traitsILi32ELi128ELi128ELi4ES2_EELb0ELb0ELb1ELb1ELb0EEv16Flash_fwd_params
    0 bytes stack frame, 0 bytes spill stores, 0 bytes spill loads
ptxas info    : Used 236 registers, 584 bytes cmem[0]
ptxas info    : Compiling entry function '_Z16flash_fwd_kernelI23Flash_fwd_kernel_traitsILi32ELi128ELi128ELi4ELb0ELb0EN7cutlass10bfloat16_tE19Flash_kernel_traitsILi32ELi128ELi128ELi4ES2_EELb0ELb1ELb0ELb0ELb0EEv16Flash_fwd_params' for 'sm_80'
ptxas info    : Function properties for _Z16flash_fwd_kernelI23Flash_fwd_kernel_traitsILi32ELi128ELi128ELi4ELb0ELb0EN7cutlass10bfloat16_tE19Flash_kernel_traitsILi32ELi128ELi128ELi4ES2_EELb0ELb1ELb0ELb0ELb0EEv16Flash_fwd_params
    0 bytes stack frame, 0 bytes spill stores, 0 bytes spill loads
ptxas info    : Used 233 registers, 584 bytes cmem[0]
ptxas info    : Compiling entry function '_Z16flash_fwd_kernelI23Flash_fwd_kernel_traitsILi32ELi128ELi128ELi4ELb0ELb0EN7cutlass10bfloat16_tE19Flash_kernel_traitsILi32ELi128ELi128ELi4ES2_EELb0ELb1ELb0ELb1ELb0EEv16Flash_fwd_params' for 'sm_80'
ptxas info    : Function properties for _Z16flash_fwd_kernelI23Flash_fwd_kernel_traitsILi32ELi128ELi128ELi4ELb0ELb0EN7cutlass10bfloat16_tE19Flash_kernel_traitsILi32ELi128ELi128ELi4ES2_EELb0ELb1ELb0ELb1ELb0EEv16Flash_fwd_params
    0 bytes stack frame, 0 bytes spill stores, 0 bytes spill loads
ptxas info    : Used 254 registers, 584 bytes cmem[0]
ptxas info    : Compiling entry function '_Z16flash_fwd_kernelI23Flash_fwd_kernel_traitsILi32ELi128ELi128ELi4ELb0ELb0EN7cutlass10bfloat16_tE19Flash_kernel_traitsILi32ELi128ELi128ELi4ES2_EELb0ELb1ELb1ELb0ELb0EEv16Flash_fwd_params' for 'sm_80'
ptxas info    : Function properties for _Z16flash_fwd_kernelI23Flash_fwd_kernel_traitsILi32ELi128ELi128ELi4ELb0ELb0EN7cutlass10bfloat16_tE19Flash_kernel_traitsILi32ELi128ELi128ELi4ES2_EELb0ELb1ELb1ELb0ELb0EEv16Flash_fwd_params
    0 bytes stack frame, 0 bytes spill stores, 0 bytes spill loads
ptxas info    : Used 254 registers, 584 bytes cmem[0]
ptxas info    : Compiling entry function '_Z16flash_fwd_kernelI23Flash_fwd_kernel_traitsILi32ELi128ELi128ELi4ELb0ELb0EN7cutlass10bfloat16_tE19Flash_kernel_traitsILi32ELi128ELi128ELi4ES2_EELb0ELb1ELb1ELb1ELb0EEv16Flash_fwd_params' for 'sm_80'
ptxas info    : Function properties for _Z16flash_fwd_kernelI23Flash_fwd_kernel_traitsILi32ELi128ELi128ELi4ELb0ELb0EN7cutlass10bfloat16_tE19Flash_kernel_traitsILi32ELi128ELi128ELi4ES2_EELb0ELb1ELb1ELb1ELb0EEv16Flash_fwd_params
    0 bytes stack frame, 0 bytes spill stores, 0 bytes spill loads
ptxas info    : Used 255 registers, 584 bytes cmem[0]
ptxas info    : Compiling entry function '_Z16flash_fwd_kernelI23Flash_fwd_kernel_traitsILi32ELi128ELi128ELi4ELb0ELb0EN7cutlass10bfloat16_tE19Flash_kernel_traitsILi32ELi128ELi128ELi4ES2_EELb1ELb0ELb0ELb0ELb0EEv16Flash_fwd_params' for 'sm_80'
ptxas info    : Function properties for _Z16flash_fwd_kernelI23Flash_fwd_kernel_traitsILi32ELi128ELi128ELi4ELb0ELb0EN7cutlass10bfloat16_tE19Flash_kernel_traitsILi32ELi128ELi128ELi4ES2_EELb1ELb0ELb0ELb0ELb0EEv16Flash_fwd_params
    0 bytes stack frame, 0 bytes spill stores, 0 bytes spill loads
ptxas info    : Used 255 registers, 584 bytes cmem[0]
ptxas info    : Compiling entry function '_Z16flash_fwd_kernelI23Flash_fwd_kernel_traitsILi32ELi128ELi128ELi4ELb0ELb0EN7cutlass10bfloat16_tE19Flash_kernel_traitsILi32ELi128ELi128ELi4ES2_EELb1ELb0ELb0ELb0ELb1EEv16Flash_fwd_params' for 'sm_80'
ptxas info    : Function properties for _Z16flash_fwd_kernelI23Flash_fwd_kernel_traitsILi32ELi128ELi128ELi4ELb0ELb0EN7cutlass10bfloat16_tE19Flash_kernel_traitsILi32ELi128ELi128ELi4ES2_EELb1ELb0ELb0ELb0ELb1EEv16Flash_fwd_params
    176 bytes stack frame, 192 bytes spill stores, 192 bytes spill loads
ptxas info    : Used 255 registers, 584 bytes cmem[0]
ptxas info    : Compiling entry function '_Z16flash_fwd_kernelI23Flash_fwd_kernel_traitsILi32ELi128ELi128ELi4ELb0ELb0EN7cutlass10bfloat16_tE19Flash_kernel_traitsILi32ELi128ELi128ELi4ES2_EELb1ELb0ELb0ELb1ELb0EEv16Flash_fwd_params' for 'sm_80'
ptxas info    : Function properties for _Z16flash_fwd_kernelI23Flash_fwd_kernel_traitsILi32ELi128ELi128ELi4ELb0ELb0EN7cutlass10bfloat16_tE19Flash_kernel_traitsILi32ELi128ELi128ELi4ES2_EELb1ELb0ELb0ELb1ELb0EEv16Flash_fwd_params
    24 bytes stack frame, 20 bytes spill stores, 20 bytes spill loads
ptxas info    : Used 255 registers, 584 bytes cmem[0]
ptxas info    : Compiling entry function '_Z16flash_fwd_kernelI23Flash_fwd_kernel_traitsILi32ELi128ELi128ELi4ELb0ELb0EN7cutlass10bfloat16_tE19Flash_kernel_traitsILi32ELi128ELi128ELi4ES2_EELb1ELb0ELb0ELb1ELb1EEv16Flash_fwd_params' for 'sm_80'
ptxas info    : Function properties for _Z16flash_fwd_kernelI23Flash_fwd_kernel_traitsILi32ELi128ELi128ELi4ELb0ELb0EN7cutlass10bfloat16_tE19Flash_kernel_traitsILi32ELi128ELi128ELi4ES2_EELb1ELb0ELb0ELb1ELb1EEv16Flash_fwd_params
    192 bytes stack frame, 212 bytes spill stores, 196 bytes spill loads
ptxas info    : Used 255 registers, 584 bytes cmem[0]
ptxas info    : Compiling entry function '_Z16flash_fwd_kernelI23Flash_fwd_kernel_traitsILi32ELi128ELi128ELi4ELb0ELb0EN7cutlass10bfloat16_tE19Flash_kernel_traitsILi32ELi128ELi128ELi4ES2_EELb1ELb0ELb1ELb0ELb0EEv16Flash_fwd_params' for 'sm_80'
ptxas info    : Function properties for _Z16flash_fwd_kernelI23Flash_fwd_kernel_traitsILi32ELi128ELi128ELi4ELb0ELb0EN7cutlass10bfloat16_tE19Flash_kernel_traitsILi32ELi128ELi128ELi4ES2_EELb1ELb0ELb1ELb0ELb0EEv16Flash_fwd_params
    48 bytes stack frame, 44 bytes spill stores, 44 bytes spill loads
ptxas info    : Used 255 registers, 584 bytes cmem[0]
ptxas info    : Compiling entry function '_Z16flash_fwd_kernelI23Flash_fwd_kernel_traitsILi32ELi128ELi128ELi4ELb0ELb0EN7cutlass10bfloat16_tE19Flash_kernel_traitsILi32ELi128ELi128ELi4ES2_EELb1ELb0ELb1ELb0ELb1EEv16Flash_fwd_params' for 'sm_80'
ptxas info    : Function properties for _Z16flash_fwd_kernelI23Flash_fwd_kernel_traitsILi32ELi128ELi128ELi4ELb0ELb0EN7cutlass10bfloat16_tE19Flash_kernel_traitsILi32ELi128ELi128ELi4ES2_EELb1ELb0ELb1ELb0ELb1EEv16Flash_fwd_params
    152 bytes stack frame, 164 bytes spill stores, 164 bytes spill loads
ptxas info    : Used 255 registers, 584 bytes cmem[0]
ptxas info    : Compiling entry function '_Z16flash_fwd_kernelI23Flash_fwd_kernel_traitsILi32ELi128ELi128ELi4ELb0ELb0EN7cutlass10bfloat16_tE19Flash_kernel_traitsILi32ELi128ELi128ELi4ES2_EELb1ELb0ELb1ELb1ELb0EEv16Flash_fwd_params' for 'sm_80'
ptxas info    : Function properties for _Z16flash_fwd_kernelI23Flash_fwd_kernel_traitsILi32ELi128ELi128ELi4ELb0ELb0EN7cutlass10bfloat16_tE19Flash_kernel_traitsILi32ELi128ELi128ELi4ES2_EELb1ELb0ELb1ELb1ELb0EEv16Flash_fwd_params
    0 bytes stack frame, 0 bytes spill stores, 0 bytes spill loads
ptxas info    : Used 255 registers, 584 bytes cmem[0]
ptxas info    : Compiling entry function '_Z16flash_fwd_kernelI23Flash_fwd_kernel_traitsILi32ELi128ELi128ELi4ELb0ELb0EN7cutlass10bfloat16_tE19Flash_kernel_traitsILi32ELi128ELi128ELi4ES2_EELb1ELb0ELb1ELb1ELb1EEv16Flash_fwd_params' for 'sm_80'
ptxas info    : Function properties for _Z16flash_fwd_kernelI23Flash_fwd_kernel_traitsILi32ELi128ELi128ELi4ELb0ELb0EN7cutlass10bfloat16_tE19Flash_kernel_traitsILi32ELi128ELi128ELi4ES2_EELb1ELb0ELb1ELb1ELb1EEv16Flash_fwd_params
    168 bytes stack frame, 170 bytes spill stores, 180 bytes spill loads
ptxas info    : Used 255 registers, 584 bytes cmem[0]
ptxas info    : Compiling entry function '_Z16flash_fwd_kernelI23Flash_fwd_kernel_traitsILi32ELi128ELi128ELi4ELb0ELb0EN7cutlass10bfloat16_tE19Flash_kernel_traitsILi32ELi128ELi128ELi4ES2_EELb1ELb1ELb0ELb0ELb0EEv16Flash_fwd_params' for 'sm_80'
ptxas info    : Function properties for _Z16flash_fwd_kernelI23Flash_fwd_kernel_traitsILi32ELi128ELi128ELi4ELb0ELb0EN7cutlass10bfloat16_tE19Flash_kernel_traitsILi32ELi128ELi128ELi4ES2_EELb1ELb1ELb0ELb0ELb0EEv16Flash_fwd_params
    0 bytes stack frame, 0 bytes spill stores, 0 bytes spill loads
ptxas info    : Used 252 registers, 584 bytes cmem[0]
ptxas info    : Compiling entry function '_Z16flash_fwd_kernelI23Flash_fwd_kernel_traitsILi32ELi128ELi128ELi4ELb0ELb0EN7cutlass10bfloat16_tE19Flash_kernel_traitsILi32ELi128ELi128ELi4ES2_EELb1ELb1ELb0ELb0ELb1EEv16Flash_fwd_params' for 'sm_80'
ptxas info    : Function properties for _Z16flash_fwd_kernelI23Flash_fwd_kernel_traitsILi32ELi128ELi128ELi4ELb0ELb0EN7cutlass10bfloat16_tE19Flash_kernel_traitsILi32ELi128ELi128ELi4ES2_EELb1ELb1ELb0ELb0ELb1EEv16Flash_fwd_params
    184 bytes stack frame, 204 bytes spill stores, 204 bytes spill loads
ptxas info    : Used 255 registers, 584 bytes cmem[0]
ptxas info    : Compiling entry function '_Z16flash_fwd_kernelI23Flash_fwd_kernel_traitsILi32ELi128ELi128ELi4ELb0ELb0EN7cutlass10bfloat16_tE19Flash_kernel_traitsILi32ELi128ELi128ELi4ES2_EELb1ELb1ELb0ELb1ELb0EEv16Flash_fwd_params' for 'sm_80'
ptxas info    : Function properties for _Z16flash_fwd_kernelI23Flash_fwd_kernel_traitsILi32ELi128ELi128ELi4ELb0ELb0EN7cutlass10bfloat16_tE19Flash_kernel_traitsILi32ELi128ELi128ELi4ES2_EELb1ELb1ELb0ELb1ELb0EEv16Flash_fwd_params
    24 bytes stack frame, 20 bytes spill stores, 20 bytes spill loads
ptxas info    : Used 255 registers, 584 bytes cmem[0]
ptxas info    : Compiling entry function '_Z16flash_fwd_kernelI23Flash_fwd_kernel_traitsILi32ELi128ELi128ELi4ELb0ELb0EN7cutlass10bfloat16_tE19Flash_kernel_traitsILi32ELi128ELi128ELi4ES2_EELb1ELb1ELb0ELb1ELb1EEv16Flash_fwd_params' for 'sm_80'
ptxas info    : Function properties for _Z16flash_fwd_kernelI23Flash_fwd_kernel_traitsILi32ELi128ELi128ELi4ELb0ELb0EN7cutlass10bfloat16_tE19Flash_kernel_traitsILi32ELi128ELi128ELi4ES2_EELb1ELb1ELb0ELb1ELb1EEv16Flash_fwd_params
    176 bytes stack frame, 212 bytes spill stores, 196 bytes spill loads
ptxas info    : Used 255 registers, 584 bytes cmem[0]
ptxas info    : Compiling entry function '_Z16flash_fwd_kernelI23Flash_fwd_kernel_traitsILi32ELi128ELi128ELi4ELb0ELb0EN7cutlass10bfloat16_tE19Flash_kernel_traitsILi32ELi128ELi128ELi4ES2_EELb1ELb1ELb1ELb0ELb0EEv16Flash_fwd_params' for 'sm_80'
ptxas info    : Function properties for _Z16flash_fwd_kernelI23Flash_fwd_kernel_traitsILi32ELi128ELi128ELi4ELb0ELb0EN7cutlass10bfloat16_tE19Flash_kernel_traitsILi32ELi128ELi128ELi4ES2_EELb1ELb1ELb1ELb0ELb0EEv16Flash_fwd_params
    48 bytes stack frame, 44 bytes spill stores, 44 bytes spill loads
ptxas info    : Used 255 registers, 584 bytes cmem[0]
ptxas info    : Compiling entry function '_Z16flash_fwd_kernelI23Flash_fwd_kernel_traitsILi32ELi128ELi128ELi4ELb0ELb0EN7cutlass10bfloat16_tE19Flash_kernel_traitsILi32ELi128ELi128ELi4ES2_EELb1ELb1ELb1ELb0ELb1EEv16Flash_fwd_params' for 'sm_80'
ptxas info    : Function properties for _Z16flash_fwd_kernelI23Flash_fwd_kernel_traitsILi32ELi128ELi128ELi4ELb0ELb0EN7cutlass10bfloat16_tE19Flash_kernel_traitsILi32ELi128ELi128ELi4ES2_EELb1ELb1ELb1ELb0ELb1EEv16Flash_fwd_params
    184 bytes stack frame, 196 bytes spill stores, 196 bytes spill loads
ptxas info    : Used 255 registers, 584 bytes cmem[0]
ptxas info    : Compiling entry function '_Z16flash_fwd_kernelI23Flash_fwd_kernel_traitsILi32ELi128ELi128ELi4ELb0ELb0EN7cutlass10bfloat16_tE19Flash_kernel_traitsILi32ELi128ELi128ELi4ES2_EELb1ELb1ELb1ELb1ELb0EEv16Flash_fwd_params' for 'sm_80'
ptxas info    : Function properties for _Z16flash_fwd_kernelI23Flash_fwd_kernel_traitsILi32ELi128ELi128ELi4ELb0ELb0EN7cutlass10bfloat16_tE19Flash_kernel_traitsILi32ELi128ELi128ELi4ES2_EELb1ELb1ELb1ELb1ELb0EEv16Flash_fwd_params
    8 bytes stack frame, 4 bytes spill stores, 4 bytes spill loads
ptxas info    : Used 255 registers, 584 bytes cmem[0]
ptxas info    : Compiling entry function '_Z16flash_fwd_kernelI23Flash_fwd_kernel_traitsILi32ELi128ELi128ELi4ELb0ELb0EN7cutlass10bfloat16_tE19Flash_kernel_traitsILi32ELi128ELi128ELi4ES2_EELb1ELb1ELb1ELb1ELb1EEv16Flash_fwd_params' for 'sm_80'
ptxas info    : Function properties for _Z16flash_fwd_kernelI23Flash_fwd_kernel_traitsILi32ELi128ELi128ELi4ELb0ELb0EN7cutlass10bfloat16_tE19Flash_kernel_traitsILi32ELi128ELi128ELi4ES2_EELb1ELb1ELb1ELb1ELb1EEv16Flash_fwd_params
    192 bytes stack frame, 212 bytes spill stores, 212 bytes spill loads
ptxas info    : Used 255 registers, 584 bytes cmem[0]
/anaconda/envs/flashattention/lib/python3.9/site-packages/torch/include/c10/core/SymInt.h(84): warning: integer conversion resulted in a change of sign

[25/37] /usr/local/cuda-11.4/bin/nvcc  -I/home/v-feiychen/flash-attention/csrc/flash_attn -I/home/v-feiychen/flash-attention/csrc/flash_attn/src -I/home/v-feiychen/flash-attention/csrc/cutlass/include -I/anaconda/envs/flashattention/lib/python3.9/site-packages/torch/include -I/anaconda/envs/flashattention/lib/python3.9/site-packages/torch/include/torch/csrc/api/include -I/anaconda/envs/flashattention/lib/python3.9/site-packages/torch/include/TH -I/anaconda/envs/flashattention/lib/python3.9/site-packages/torch/include/THC -I/usr/local/cuda-11.4/include -I/anaconda/envs/flashattention/include/python3.9 -c -c /home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_hdim32_fp16_sm80.cu -o /home/v-feiychen/flash-attention/build/temp.linux-x86_64-cpython-39/csrc/flash_attn/src/flash_fwd_hdim32_fp16_sm80.o -D__CUDA_NO_HALF_OPERATORS__ -D__CUDA_NO_HALF_CONVERSIONS__ -D__CUDA_NO_BFLOAT16_CONVERSIONS__ -D__CUDA_NO_HALF2_OPERATORS__ --expt-relaxed-constexpr --compiler-options ''"'"'-fPIC'"'"'' -O3 -std=c++17 -U__CUDA_NO_HALF_OPERATORS__ -U__CUDA_NO_HALF_CONVERSIONS__ -U__CUDA_NO_HALF2_OPERATORS__ -U__CUDA_NO_BFLOAT16_CONVERSIONS__ --expt-relaxed-constexpr --expt-extended-lambda --use_fast_math --ptxas-options=-v -lineinfo -gencode arch=compute_80,code=sm_80 --threads 4 -DTORCH_API_INCLUDE_EXTENSION_H '-DPYBIND11_COMPILER_TYPE="_gcc"' '-DPYBIND11_STDLIB="_libstdcpp"' '-DPYBIND11_BUILD_ABI="_cxxabi1011"' -DTORCH_EXTENSION_NAME=flash_attn_2_cuda -D_GLIBCXX_USE_CXX11_ABI=0
/anaconda/envs/flashattention/lib/python3.9/site-packages/torch/include/c10/core/SymInt.h(84): warning: integer conversion resulted in a change of sign

/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_kernel.h(140): warning: variable "MMA_M" was declared but never referenced
          detected during:
            instantiation of "void flash::compute_attn_1rowblock<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params,Is_Q_split_K>(const Params &, int, int, int) [with Kernel_traits=Flash_fwd_kernel_traits<32, 128, 128, 4, false, false, cutlass::half_t, Flash_kernel_traits<32, 128, 128, 4, cutlass::half_t>>, Is_dropout=true, Is_causal=true, Is_even_N=true, Is_even_K=true, Return_softmax=true, Params=Flash_fwd_params, Is_Q_split_K=false]" 
(589): here
            instantiation of "void flash::compute_attn<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &) [with Kernel_traits=Flash_fwd_kernel_traits<32, 128, 128, 4, false, false, cutlass::half_t, Flash_kernel_traits<32, 128, 128, 4, cutlass::half_t>>, Is_dropout=true, Is_causal=true, Is_even_N=true, Is_even_K=true, Return_softmax=true, Params=Flash_fwd_params]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(15): here
            instantiation of "void flash_fwd_kernel<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax>(Flash_fwd_params) [with Kernel_traits=Flash_fwd_kernel_traits<32, 128, 128, 4, false, false, cutlass::half_t, Flash_kernel_traits<32, 128, 128, 4, cutlass::half_t>>, Is_dropout=true, Is_causal=true, Is_even_N=true, Is_even_K=true, Return_softmax=true]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(34): here
            instantiation of "void run_flash_fwd<Kernel_traits,Is_dropout,Is_causal>(Flash_fwd_params &, cudaStream_t) [with Kernel_traits=Flash_fwd_kernel_traits<32, 128, 128, 4, false, false, cutlass::half_t, Flash_kernel_traits<32, 128, 128, 4, cutlass::half_t>>, Is_dropout=true, Is_causal=true]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(58): here
            instantiation of "void run_mha_fwd_hdim32<T>(Flash_fwd_params &, cudaStream_t) [with T=cutlass::half_t]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_hdim32_fp16_sm80.cu(22): here

/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_kernel.h(276): warning: variable "tQrQ" was declared but never referenced
          detected during:
            instantiation of "void flash::compute_attn_1rowblock<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params,Is_Q_split_K>(const Params &, int, int, int) [with Kernel_traits=Flash_fwd_kernel_traits<32, 128, 128, 4, false, false, cutlass::half_t, Flash_kernel_traits<32, 128, 128, 4, cutlass::half_t>>, Is_dropout=true, Is_causal=true, Is_even_N=true, Is_even_K=true, Return_softmax=true, Params=Flash_fwd_params, Is_Q_split_K=false]" 
(589): here
            instantiation of "void flash::compute_attn<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &) [with Kernel_traits=Flash_fwd_kernel_traits<32, 128, 128, 4, false, false, cutlass::half_t, Flash_kernel_traits<32, 128, 128, 4, cutlass::half_t>>, Is_dropout=true, Is_causal=true, Is_even_N=true, Is_even_K=true, Return_softmax=true, Params=Flash_fwd_params]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(15): here
            instantiation of "void flash_fwd_kernel<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax>(Flash_fwd_params) [with Kernel_traits=Flash_fwd_kernel_traits<32, 128, 128, 4, false, false, cutlass::half_t, Flash_kernel_traits<32, 128, 128, 4, cutlass::half_t>>, Is_dropout=true, Is_causal=true, Is_even_N=true, Is_even_K=true, Return_softmax=true]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(34): here
            instantiation of "void run_flash_fwd<Kernel_traits,Is_dropout,Is_causal>(Flash_fwd_params &, cudaStream_t) [with Kernel_traits=Flash_fwd_kernel_traits<32, 128, 128, 4, false, false, cutlass::half_t, Flash_kernel_traits<32, 128, 128, 4, cutlass::half_t>>, Is_dropout=true, Is_causal=true]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(58): here
            instantiation of "void run_mha_fwd_hdim32<T>(Flash_fwd_params &, cudaStream_t) [with T=cutlass::half_t]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_hdim32_fp16_sm80.cu(22): here

/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_kernel.h(140): warning: variable "MMA_M" was declared but never referenced
          detected during:
            instantiation of "void flash::compute_attn_1rowblock<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params,Is_Q_split_K>(const Params &, int, int, int) [with Kernel_traits=Flash_fwd_kernel_traits<32, 128, 128, 4, false, false, cutlass::half_t, Flash_kernel_traits<32, 128, 128, 4, cutlass::half_t>>, Is_dropout=true, Is_causal=true, Is_even_N=true, Is_even_K=true, Return_softmax=false, Params=Flash_fwd_params, Is_Q_split_K=false]" 
(589): here
            instantiation of "void flash::compute_attn<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &) [with Kernel_traits=Flash_fwd_kernel_traits<32, 128, 128, 4, false, false, cutlass::half_t, Flash_kernel_traits<32, 128, 128, 4, cutlass::half_t>>, Is_dropout=true, Is_causal=true, Is_even_N=true, Is_even_K=true, Return_softmax=false, Params=Flash_fwd_params]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(15): here
            instantiation of "void flash_fwd_kernel<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax>(Flash_fwd_params) [with Kernel_traits=Flash_fwd_kernel_traits<32, 128, 128, 4, false, false, cutlass::half_t, Flash_kernel_traits<32, 128, 128, 4, cutlass::half_t>>, Is_dropout=true, Is_causal=true, Is_even_N=true, Is_even_K=true, Return_softmax=false]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(34): here
            instantiation of "void run_flash_fwd<Kernel_traits,Is_dropout,Is_causal>(Flash_fwd_params &, cudaStream_t) [with Kernel_traits=Flash_fwd_kernel_traits<32, 128, 128, 4, false, false, cutlass::half_t, Flash_kernel_traits<32, 128, 128, 4, cutlass::half_t>>, Is_dropout=true, Is_causal=true]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(58): here
            instantiation of "void run_mha_fwd_hdim32<T>(Flash_fwd_params &, cudaStream_t) [with T=cutlass::half_t]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_hdim32_fp16_sm80.cu(22): here

/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_kernel.h(276): warning: variable "tQrQ" was declared but never referenced
          detected during:
            instantiation of "void flash::compute_attn_1rowblock<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params,Is_Q_split_K>(const Params &, int, int, int) [with Kernel_traits=Flash_fwd_kernel_traits<32, 128, 128, 4, false, false, cutlass::half_t, Flash_kernel_traits<32, 128, 128, 4, cutlass::half_t>>, Is_dropout=true, Is_causal=true, Is_even_N=true, Is_even_K=true, Return_softmax=false, Params=Flash_fwd_params, Is_Q_split_K=false]" 
(589): here
            instantiation of "void flash::compute_attn<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &) [with Kernel_traits=Flash_fwd_kernel_traits<32, 128, 128, 4, false, false, cutlass::half_t, Flash_kernel_traits<32, 128, 128, 4, cutlass::half_t>>, Is_dropout=true, Is_causal=true, Is_even_N=true, Is_even_K=true, Return_softmax=false, Params=Flash_fwd_params]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(15): here
            instantiation of "void flash_fwd_kernel<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax>(Flash_fwd_params) [with Kernel_traits=Flash_fwd_kernel_traits<32, 128, 128, 4, false, false, cutlass::half_t, Flash_kernel_traits<32, 128, 128, 4, cutlass::half_t>>, Is_dropout=true, Is_causal=true, Is_even_N=true, Is_even_K=true, Return_softmax=false]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(34): here
            instantiation of "void run_flash_fwd<Kernel_traits,Is_dropout,Is_causal>(Flash_fwd_params &, cudaStream_t) [with Kernel_traits=Flash_fwd_kernel_traits<32, 128, 128, 4, false, false, cutlass::half_t, Flash_kernel_traits<32, 128, 128, 4, cutlass::half_t>>, Is_dropout=true, Is_causal=true]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(58): here
            instantiation of "void run_mha_fwd_hdim32<T>(Flash_fwd_params &, cudaStream_t) [with T=cutlass::half_t]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_hdim32_fp16_sm80.cu(22): here

/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_kernel.h(140): warning: variable "MMA_M" was declared but never referenced
          detected during:
            instantiation of "void flash::compute_attn_1rowblock<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params,Is_Q_split_K>(const Params &, int, int, int) [with Kernel_traits=Flash_fwd_kernel_traits<32, 128, 128, 4, false, false, cutlass::half_t, Flash_kernel_traits<32, 128, 128, 4, cutlass::half_t>>, Is_dropout=true, Is_causal=true, Is_even_N=true, Is_even_K=false, Return_softmax=true, Params=Flash_fwd_params, Is_Q_split_K=false]" 
(589): here
            instantiation of "void flash::compute_attn<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &) [with Kernel_traits=Flash_fwd_kernel_traits<32, 128, 128, 4, false, false, cutlass::half_t, Flash_kernel_traits<32, 128, 128, 4, cutlass::half_t>>, Is_dropout=true, Is_causal=true, Is_even_N=true, Is_even_K=false, Return_softmax=true, Params=Flash_fwd_params]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(15): here
            instantiation of "void flash_fwd_kernel<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax>(Flash_fwd_params) [with Kernel_traits=Flash_fwd_kernel_traits<32, 128, 128, 4, false, false, cutlass::half_t, Flash_kernel_traits<32, 128, 128, 4, cutlass::half_t>>, Is_dropout=true, Is_causal=true, Is_even_N=true, Is_even_K=false, Return_softmax=true]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(34): here
            instantiation of "void run_flash_fwd<Kernel_traits,Is_dropout,Is_causal>(Flash_fwd_params &, cudaStream_t) [with Kernel_traits=Flash_fwd_kernel_traits<32, 128, 128, 4, false, false, cutlass::half_t, Flash_kernel_traits<32, 128, 128, 4, cutlass::half_t>>, Is_dropout=true, Is_causal=true]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(58): here
            instantiation of "void run_mha_fwd_hdim32<T>(Flash_fwd_params &, cudaStream_t) [with T=cutlass::half_t]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_hdim32_fp16_sm80.cu(22): here

/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_kernel.h(276): warning: variable "tQrQ" was declared but never referenced
          detected during:
            instantiation of "void flash::compute_attn_1rowblock<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params,Is_Q_split_K>(const Params &, int, int, int) [with Kernel_traits=Flash_fwd_kernel_traits<32, 128, 128, 4, false, false, cutlass::half_t, Flash_kernel_traits<32, 128, 128, 4, cutlass::half_t>>, Is_dropout=true, Is_causal=true, Is_even_N=true, Is_even_K=false, Return_softmax=true, Params=Flash_fwd_params, Is_Q_split_K=false]" 
(589): here
            instantiation of "void flash::compute_attn<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &) [with Kernel_traits=Flash_fwd_kernel_traits<32, 128, 128, 4, false, false, cutlass::half_t, Flash_kernel_traits<32, 128, 128, 4, cutlass::half_t>>, Is_dropout=true, Is_causal=true, Is_even_N=true, Is_even_K=false, Return_softmax=true, Params=Flash_fwd_params]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(15): here
            instantiation of "void flash_fwd_kernel<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax>(Flash_fwd_params) [with Kernel_traits=Flash_fwd_kernel_traits<32, 128, 128, 4, false, false, cutlass::half_t, Flash_kernel_traits<32, 128, 128, 4, cutlass::half_t>>, Is_dropout=true, Is_causal=true, Is_even_N=true, Is_even_K=false, Return_softmax=true]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(34): here
            instantiation of "void run_flash_fwd<Kernel_traits,Is_dropout,Is_causal>(Flash_fwd_params &, cudaStream_t) [with Kernel_traits=Flash_fwd_kernel_traits<32, 128, 128, 4, false, false, cutlass::half_t, Flash_kernel_traits<32, 128, 128, 4, cutlass::half_t>>, Is_dropout=true, Is_causal=true]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(58): here
            instantiation of "void run_mha_fwd_hdim32<T>(Flash_fwd_params &, cudaStream_t) [with T=cutlass::half_t]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_hdim32_fp16_sm80.cu(22): here

/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_kernel.h(140): warning: variable "MMA_M" was declared but never referenced
          detected during:
            instantiation of "void flash::compute_attn_1rowblock<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params,Is_Q_split_K>(const Params &, int, int, int) [with Kernel_traits=Flash_fwd_kernel_traits<32, 128, 128, 4, false, false, cutlass::half_t, Flash_kernel_traits<32, 128, 128, 4, cutlass::half_t>>, Is_dropout=true, Is_causal=true, Is_even_N=true, Is_even_K=false, Return_softmax=false, Params=Flash_fwd_params, Is_Q_split_K=false]" 
(589): here
            instantiation of "void flash::compute_attn<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &) [with Kernel_traits=Flash_fwd_kernel_traits<32, 128, 128, 4, false, false, cutlass::half_t, Flash_kernel_traits<32, 128, 128, 4, cutlass::half_t>>, Is_dropout=true, Is_causal=true, Is_even_N=true, Is_even_K=false, Return_softmax=false, Params=Flash_fwd_params]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(15): here
            instantiation of "void flash_fwd_kernel<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax>(Flash_fwd_params) [with Kernel_traits=Flash_fwd_kernel_traits<32, 128, 128, 4, false, false, cutlass::half_t, Flash_kernel_traits<32, 128, 128, 4, cutlass::half_t>>, Is_dropout=true, Is_causal=true, Is_even_N=true, Is_even_K=false, Return_softmax=false]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(34): here
            instantiation of "void run_flash_fwd<Kernel_traits,Is_dropout,Is_causal>(Flash_fwd_params &, cudaStream_t) [with Kernel_traits=Flash_fwd_kernel_traits<32, 128, 128, 4, false, false, cutlass::half_t, Flash_kernel_traits<32, 128, 128, 4, cutlass::half_t>>, Is_dropout=true, Is_causal=true]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(58): here
            instantiation of "void run_mha_fwd_hdim32<T>(Flash_fwd_params &, cudaStream_t) [with T=cutlass::half_t]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_hdim32_fp16_sm80.cu(22): here

/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_kernel.h(276): warning: variable "tQrQ" was declared but never referenced
          detected during:
            instantiation of "void flash::compute_attn_1rowblock<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params,Is_Q_split_K>(const Params &, int, int, int) [with Kernel_traits=Flash_fwd_kernel_traits<32, 128, 128, 4, false, false, cutlass::half_t, Flash_kernel_traits<32, 128, 128, 4, cutlass::half_t>>, Is_dropout=true, Is_causal=true, Is_even_N=true, Is_even_K=false, Return_softmax=false, Params=Flash_fwd_params, Is_Q_split_K=false]" 
(589): here
            instantiation of "void flash::compute_attn<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &) [with Kernel_traits=Flash_fwd_kernel_traits<32, 128, 128, 4, false, false, cutlass::half_t, Flash_kernel_traits<32, 128, 128, 4, cutlass::half_t>>, Is_dropout=true, Is_causal=true, Is_even_N=true, Is_even_K=false, Return_softmax=false, Params=Flash_fwd_params]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(15): here
            instantiation of "void flash_fwd_kernel<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax>(Flash_fwd_params) [with Kernel_traits=Flash_fwd_kernel_traits<32, 128, 128, 4, false, false, cutlass::half_t, Flash_kernel_traits<32, 128, 128, 4, cutlass::half_t>>, Is_dropout=true, Is_causal=true, Is_even_N=true, Is_even_K=false, Return_softmax=false]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(34): here
            instantiation of "void run_flash_fwd<Kernel_traits,Is_dropout,Is_causal>(Flash_fwd_params &, cudaStream_t) [with Kernel_traits=Flash_fwd_kernel_traits<32, 128, 128, 4, false, false, cutlass::half_t, Flash_kernel_traits<32, 128, 128, 4, cutlass::half_t>>, Is_dropout=true, Is_causal=true]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(58): here
            instantiation of "void run_mha_fwd_hdim32<T>(Flash_fwd_params &, cudaStream_t) [with T=cutlass::half_t]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_hdim32_fp16_sm80.cu(22): here

/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_kernel.h(140): warning: variable "MMA_M" was declared but never referenced
          detected during:
            instantiation of "void flash::compute_attn_1rowblock<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params,Is_Q_split_K>(const Params &, int, int, int) [with Kernel_traits=Flash_fwd_kernel_traits<32, 128, 128, 4, false, false, cutlass::half_t, Flash_kernel_traits<32, 128, 128, 4, cutlass::half_t>>, Is_dropout=true, Is_causal=true, Is_even_N=false, Is_even_K=true, Return_softmax=true, Params=Flash_fwd_params, Is_Q_split_K=false]" 
(589): here
            instantiation of "void flash::compute_attn<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &) [with Kernel_traits=Flash_fwd_kernel_traits<32, 128, 128, 4, false, false, cutlass::half_t, Flash_kernel_traits<32, 128, 128, 4, cutlass::half_t>>, Is_dropout=true, Is_causal=true, Is_even_N=false, Is_even_K=true, Return_softmax=true, Params=Flash_fwd_params]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(15): here
            instantiation of "void flash_fwd_kernel<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax>(Flash_fwd_params) [with Kernel_traits=Flash_fwd_kernel_traits<32, 128, 128, 4, false, false, cutlass::half_t, Flash_kernel_traits<32, 128, 128, 4, cutlass::half_t>>, Is_dropout=true, Is_causal=true, Is_even_N=false, Is_even_K=true, Return_softmax=true]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(34): here
            instantiation of "void run_flash_fwd<Kernel_traits,Is_dropout,Is_causal>(Flash_fwd_params &, cudaStream_t) [with Kernel_traits=Flash_fwd_kernel_traits<32, 128, 128, 4, false, false, cutlass::half_t, Flash_kernel_traits<32, 128, 128, 4, cutlass::half_t>>, Is_dropout=true, Is_causal=true]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(58): here
            instantiation of "void run_mha_fwd_hdim32<T>(Flash_fwd_params &, cudaStream_t) [with T=cutlass::half_t]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_hdim32_fp16_sm80.cu(22): here

/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_kernel.h(276): warning: variable "tQrQ" was declared but never referenced
          detected during:
            instantiation of "void flash::compute_attn_1rowblock<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params,Is_Q_split_K>(const Params &, int, int, int) [with Kernel_traits=Flash_fwd_kernel_traits<32, 128, 128, 4, false, false, cutlass::half_t, Flash_kernel_traits<32, 128, 128, 4, cutlass::half_t>>, Is_dropout=true, Is_causal=true, Is_even_N=false, Is_even_K=true, Return_softmax=true, Params=Flash_fwd_params, Is_Q_split_K=false]" 
(589): here
            instantiation of "void flash::compute_attn<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &) [with Kernel_traits=Flash_fwd_kernel_traits<32, 128, 128, 4, false, false, cutlass::half_t, Flash_kernel_traits<32, 128, 128, 4, cutlass::half_t>>, Is_dropout=true, Is_causal=true, Is_even_N=false, Is_even_K=true, Return_softmax=true, Params=Flash_fwd_params]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(15): here
            instantiation of "void flash_fwd_kernel<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax>(Flash_fwd_params) [with Kernel_traits=Flash_fwd_kernel_traits<32, 128, 128, 4, false, false, cutlass::half_t, Flash_kernel_traits<32, 128, 128, 4, cutlass::half_t>>, Is_dropout=true, Is_causal=true, Is_even_N=false, Is_even_K=true, Return_softmax=true]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(34): here
            instantiation of "void run_flash_fwd<Kernel_traits,Is_dropout,Is_causal>(Flash_fwd_params &, cudaStream_t) [with Kernel_traits=Flash_fwd_kernel_traits<32, 128, 128, 4, false, false, cutlass::half_t, Flash_kernel_traits<32, 128, 128, 4, cutlass::half_t>>, Is_dropout=true, Is_causal=true]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(58): here
            instantiation of "void run_mha_fwd_hdim32<T>(Flash_fwd_params &, cudaStream_t) [with T=cutlass::half_t]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_hdim32_fp16_sm80.cu(22): here

/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_kernel.h(140): warning: variable "MMA_M" was declared but never referenced
          detected during:
            instantiation of "void flash::compute_attn_1rowblock<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params,Is_Q_split_K>(const Params &, int, int, int) [with Kernel_traits=Flash_fwd_kernel_traits<32, 128, 128, 4, false, false, cutlass::half_t, Flash_kernel_traits<32, 128, 128, 4, cutlass::half_t>>, Is_dropout=true, Is_causal=true, Is_even_N=false, Is_even_K=true, Return_softmax=false, Params=Flash_fwd_params, Is_Q_split_K=false]" 
(589): here
            instantiation of "void flash::compute_attn<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &) [with Kernel_traits=Flash_fwd_kernel_traits<32, 128, 128, 4, false, false, cutlass::half_t, Flash_kernel_traits<32, 128, 128, 4, cutlass::half_t>>, Is_dropout=true, Is_causal=true, Is_even_N=false, Is_even_K=true, Return_softmax=false, Params=Flash_fwd_params]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(15): here
            instantiation of "void flash_fwd_kernel<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax>(Flash_fwd_params) [with Kernel_traits=Flash_fwd_kernel_traits<32, 128, 128, 4, false, false, cutlass::half_t, Flash_kernel_traits<32, 128, 128, 4, cutlass::half_t>>, Is_dropout=true, Is_causal=true, Is_even_N=false, Is_even_K=true, Return_softmax=false]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(34): here
            instantiation of "void run_flash_fwd<Kernel_traits,Is_dropout,Is_causal>(Flash_fwd_params &, cudaStream_t) [with Kernel_traits=Flash_fwd_kernel_traits<32, 128, 128, 4, false, false, cutlass::half_t, Flash_kernel_traits<32, 128, 128, 4, cutlass::half_t>>, Is_dropout=true, Is_causal=true]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(58): here
            instantiation of "void run_mha_fwd_hdim32<T>(Flash_fwd_params &, cudaStream_t) [with T=cutlass::half_t]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_hdim32_fp16_sm80.cu(22): here

/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_kernel.h(276): warning: variable "tQrQ" was declared but never referenced
          detected during:
            instantiation of "void flash::compute_attn_1rowblock<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params,Is_Q_split_K>(const Params &, int, int, int) [with Kernel_traits=Flash_fwd_kernel_traits<32, 128, 128, 4, false, false, cutlass::half_t, Flash_kernel_traits<32, 128, 128, 4, cutlass::half_t>>, Is_dropout=true, Is_causal=true, Is_even_N=false, Is_even_K=true, Return_softmax=false, Params=Flash_fwd_params, Is_Q_split_K=false]" 
(589): here
            instantiation of "void flash::compute_attn<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &) [with Kernel_traits=Flash_fwd_kernel_traits<32, 128, 128, 4, false, false, cutlass::half_t, Flash_kernel_traits<32, 128, 128, 4, cutlass::half_t>>, Is_dropout=true, Is_causal=true, Is_even_N=false, Is_even_K=true, Return_softmax=false, Params=Flash_fwd_params]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(15): here
            instantiation of "void flash_fwd_kernel<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax>(Flash_fwd_params) [with Kernel_traits=Flash_fwd_kernel_traits<32, 128, 128, 4, false, false, cutlass::half_t, Flash_kernel_traits<32, 128, 128, 4, cutlass::half_t>>, Is_dropout=true, Is_causal=true, Is_even_N=false, Is_even_K=true, Return_softmax=false]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(34): here
            instantiation of "void run_flash_fwd<Kernel_traits,Is_dropout,Is_causal>(Flash_fwd_params &, cudaStream_t) [with Kernel_traits=Flash_fwd_kernel_traits<32, 128, 128, 4, false, false, cutlass::half_t, Flash_kernel_traits<32, 128, 128, 4, cutlass::half_t>>, Is_dropout=true, Is_causal=true]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(58): here
            instantiation of "void run_mha_fwd_hdim32<T>(Flash_fwd_params &, cudaStream_t) [with T=cutlass::half_t]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_hdim32_fp16_sm80.cu(22): here

/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_kernel.h(140): warning: variable "MMA_M" was declared but never referenced
          detected during:
            instantiation of "void flash::compute_attn_1rowblock<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params,Is_Q_split_K>(const Params &, int, int, int) [with Kernel_traits=Flash_fwd_kernel_traits<32, 128, 128, 4, false, false, cutlass::half_t, Flash_kernel_traits<32, 128, 128, 4, cutlass::half_t>>, Is_dropout=true, Is_causal=true, Is_even_N=false, Is_even_K=false, Return_softmax=true, Params=Flash_fwd_params, Is_Q_split_K=false]" 
(589): here
            instantiation of "void flash::compute_attn<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &) [with Kernel_traits=Flash_fwd_kernel_traits<32, 128, 128, 4, false, false, cutlass::half_t, Flash_kernel_traits<32, 128, 128, 4, cutlass::half_t>>, Is_dropout=true, Is_causal=true, Is_even_N=false, Is_even_K=false, Return_softmax=true, Params=Flash_fwd_params]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(15): here
            instantiation of "void flash_fwd_kernel<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax>(Flash_fwd_params) [with Kernel_traits=Flash_fwd_kernel_traits<32, 128, 128, 4, false, false, cutlass::half_t, Flash_kernel_traits<32, 128, 128, 4, cutlass::half_t>>, Is_dropout=true, Is_causal=true, Is_even_N=false, Is_even_K=false, Return_softmax=true]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(34): here
            instantiation of "void run_flash_fwd<Kernel_traits,Is_dropout,Is_causal>(Flash_fwd_params &, cudaStream_t) [with Kernel_traits=Flash_fwd_kernel_traits<32, 128, 128, 4, false, false, cutlass::half_t, Flash_kernel_traits<32, 128, 128, 4, cutlass::half_t>>, Is_dropout=true, Is_causal=true]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(58): here
            instantiation of "void run_mha_fwd_hdim32<T>(Flash_fwd_params &, cudaStream_t) [with T=cutlass::half_t]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_hdim32_fp16_sm80.cu(22): here

/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_kernel.h(276): warning: variable "tQrQ" was declared but never referenced
          detected during:
            instantiation of "void flash::compute_attn_1rowblock<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params,Is_Q_split_K>(const Params &, int, int, int) [with Kernel_traits=Flash_fwd_kernel_traits<32, 128, 128, 4, false, false, cutlass::half_t, Flash_kernel_traits<32, 128, 128, 4, cutlass::half_t>>, Is_dropout=true, Is_causal=true, Is_even_N=false, Is_even_K=false, Return_softmax=true, Params=Flash_fwd_params, Is_Q_split_K=false]" 
(589): here
            instantiation of "void flash::compute_attn<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &) [with Kernel_traits=Flash_fwd_kernel_traits<32, 128, 128, 4, false, false, cutlass::half_t, Flash_kernel_traits<32, 128, 128, 4, cutlass::half_t>>, Is_dropout=true, Is_causal=true, Is_even_N=false, Is_even_K=false, Return_softmax=true, Params=Flash_fwd_params]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(15): here
            instantiation of "void flash_fwd_kernel<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax>(Flash_fwd_params) [with Kernel_traits=Flash_fwd_kernel_traits<32, 128, 128, 4, false, false, cutlass::half_t, Flash_kernel_traits<32, 128, 128, 4, cutlass::half_t>>, Is_dropout=true, Is_causal=true, Is_even_N=false, Is_even_K=false, Return_softmax=true]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(34): here
            instantiation of "void run_flash_fwd<Kernel_traits,Is_dropout,Is_causal>(Flash_fwd_params &, cudaStream_t) [with Kernel_traits=Flash_fwd_kernel_traits<32, 128, 128, 4, false, false, cutlass::half_t, Flash_kernel_traits<32, 128, 128, 4, cutlass::half_t>>, Is_dropout=true, Is_causal=true]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(58): here
            instantiation of "void run_mha_fwd_hdim32<T>(Flash_fwd_params &, cudaStream_t) [with T=cutlass::half_t]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_hdim32_fp16_sm80.cu(22): here

/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_kernel.h(140): warning: variable "MMA_M" was declared but never referenced
          detected during:
            instantiation of "void flash::compute_attn_1rowblock<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params,Is_Q_split_K>(const Params &, int, int, int) [with Kernel_traits=Flash_fwd_kernel_traits<32, 128, 128, 4, false, false, cutlass::half_t, Flash_kernel_traits<32, 128, 128, 4, cutlass::half_t>>, Is_dropout=true, Is_causal=true, Is_even_N=false, Is_even_K=false, Return_softmax=false, Params=Flash_fwd_params, Is_Q_split_K=false]" 
(589): here
            instantiation of "void flash::compute_attn<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &) [with Kernel_traits=Flash_fwd_kernel_traits<32, 128, 128, 4, false, false, cutlass::half_t, Flash_kernel_traits<32, 128, 128, 4, cutlass::half_t>>, Is_dropout=true, Is_causal=true, Is_even_N=false, Is_even_K=false, Return_softmax=false, Params=Flash_fwd_params]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(15): here
            instantiation of "void flash_fwd_kernel<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax>(Flash_fwd_params) [with Kernel_traits=Flash_fwd_kernel_traits<32, 128, 128, 4, false, false, cutlass::half_t, Flash_kernel_traits<32, 128, 128, 4, cutlass::half_t>>, Is_dropout=true, Is_causal=true, Is_even_N=false, Is_even_K=false, Return_softmax=false]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(34): here
            instantiation of "void run_flash_fwd<Kernel_traits,Is_dropout,Is_causal>(Flash_fwd_params &, cudaStream_t) [with Kernel_traits=Flash_fwd_kernel_traits<32, 128, 128, 4, false, false, cutlass::half_t, Flash_kernel_traits<32, 128, 128, 4, cutlass::half_t>>, Is_dropout=true, Is_causal=true]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(58): here
            instantiation of "void run_mha_fwd_hdim32<T>(Flash_fwd_params &, cudaStream_t) [with T=cutlass::half_t]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_hdim32_fp16_sm80.cu(22): here

/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_kernel.h(276): warning: variable "tQrQ" was declared but never referenced
          detected during:
            instantiation of "void flash::compute_attn_1rowblock<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params,Is_Q_split_K>(const Params &, int, int, int) [with Kernel_traits=Flash_fwd_kernel_traits<32, 128, 128, 4, false, false, cutlass::half_t, Flash_kernel_traits<32, 128, 128, 4, cutlass::half_t>>, Is_dropout=true, Is_causal=true, Is_even_N=false, Is_even_K=false, Return_softmax=false, Params=Flash_fwd_params, Is_Q_split_K=false]" 
(589): here
            instantiation of "void flash::compute_attn<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &) [with Kernel_traits=Flash_fwd_kernel_traits<32, 128, 128, 4, false, false, cutlass::half_t, Flash_kernel_traits<32, 128, 128, 4, cutlass::half_t>>, Is_dropout=true, Is_causal=true, Is_even_N=false, Is_even_K=false, Return_softmax=false, Params=Flash_fwd_params]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(15): here
            instantiation of "void flash_fwd_kernel<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax>(Flash_fwd_params) [with Kernel_traits=Flash_fwd_kernel_traits<32, 128, 128, 4, false, false, cutlass::half_t, Flash_kernel_traits<32, 128, 128, 4, cutlass::half_t>>, Is_dropout=true, Is_causal=true, Is_even_N=false, Is_even_K=false, Return_softmax=false]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(34): here
            instantiation of "void run_flash_fwd<Kernel_traits,Is_dropout,Is_causal>(Flash_fwd_params &, cudaStream_t) [with Kernel_traits=Flash_fwd_kernel_traits<32, 128, 128, 4, false, false, cutlass::half_t, Flash_kernel_traits<32, 128, 128, 4, cutlass::half_t>>, Is_dropout=true, Is_causal=true]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(58): here
            instantiation of "void run_mha_fwd_hdim32<T>(Flash_fwd_params &, cudaStream_t) [with T=cutlass::half_t]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_hdim32_fp16_sm80.cu(22): here

/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_kernel.h(140): warning: variable "MMA_M" was declared but never referenced
          detected during:
            instantiation of "void flash::compute_attn_1rowblock<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params,Is_Q_split_K>(const Params &, int, int, int) [with Kernel_traits=Flash_fwd_kernel_traits<32, 128, 128, 4, false, false, cutlass::half_t, Flash_kernel_traits<32, 128, 128, 4, cutlass::half_t>>, Is_dropout=true, Is_causal=false, Is_even_N=true, Is_even_K=true, Return_softmax=true, Params=Flash_fwd_params, Is_Q_split_K=false]" 
(589): here
            instantiation of "void flash::compute_attn<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &) [with Kernel_traits=Flash_fwd_kernel_traits<32, 128, 128, 4, false, false, cutlass::half_t, Flash_kernel_traits<32, 128, 128, 4, cutlass::half_t>>, Is_dropout=true, Is_causal=false, Is_even_N=true, Is_even_K=true, Return_softmax=true, Params=Flash_fwd_params]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(15): here
            instantiation of "void flash_fwd_kernel<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax>(Flash_fwd_params) [with Kernel_traits=Flash_fwd_kernel_traits<32, 128, 128, 4, false, false, cutlass::half_t, Flash_kernel_traits<32, 128, 128, 4, cutlass::half_t>>, Is_dropout=true, Is_causal=false, Is_even_N=true, Is_even_K=true, Return_softmax=true]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(34): here
            instantiation of "void run_flash_fwd<Kernel_traits,Is_dropout,Is_causal>(Flash_fwd_params &, cudaStream_t) [with Kernel_traits=Flash_fwd_kernel_traits<32, 128, 128, 4, false, false, cutlass::half_t, Flash_kernel_traits<32, 128, 128, 4, cutlass::half_t>>, Is_dropout=true, Is_causal=false]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(58): here
            instantiation of "void run_mha_fwd_hdim32<T>(Flash_fwd_params &, cudaStream_t) [with T=cutlass::half_t]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_hdim32_fp16_sm80.cu(22): here

/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_kernel.h(276): warning: variable "tQrQ" was declared but never referenced
          detected during:
            instantiation of "void flash::compute_attn_1rowblock<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params,Is_Q_split_K>(const Params &, int, int, int) [with Kernel_traits=Flash_fwd_kernel_traits<32, 128, 128, 4, false, false, cutlass::half_t, Flash_kernel_traits<32, 128, 128, 4, cutlass::half_t>>, Is_dropout=true, Is_causal=false, Is_even_N=true, Is_even_K=true, Return_softmax=true, Params=Flash_fwd_params, Is_Q_split_K=false]" 
(589): here
            instantiation of "void flash::compute_attn<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &) [with Kernel_traits=Flash_fwd_kernel_traits<32, 128, 128, 4, false, false, cutlass::half_t, Flash_kernel_traits<32, 128, 128, 4, cutlass::half_t>>, Is_dropout=true, Is_causal=false, Is_even_N=true, Is_even_K=true, Return_softmax=true, Params=Flash_fwd_params]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(15): here
            instantiation of "void flash_fwd_kernel<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax>(Flash_fwd_params) [with Kernel_traits=Flash_fwd_kernel_traits<32, 128, 128, 4, false, false, cutlass::half_t, Flash_kernel_traits<32, 128, 128, 4, cutlass::half_t>>, Is_dropout=true, Is_causal=false, Is_even_N=true, Is_even_K=true, Return_softmax=true]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(34): here
            instantiation of "void run_flash_fwd<Kernel_traits,Is_dropout,Is_causal>(Flash_fwd_params &, cudaStream_t) [with Kernel_traits=Flash_fwd_kernel_traits<32, 128, 128, 4, false, false, cutlass::half_t, Flash_kernel_traits<32, 128, 128, 4, cutlass::half_t>>, Is_dropout=true, Is_causal=false]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(58): here
            instantiation of "void run_mha_fwd_hdim32<T>(Flash_fwd_params &, cudaStream_t) [with T=cutlass::half_t]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_hdim32_fp16_sm80.cu(22): here

/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_kernel.h(140): warning: variable "MMA_M" was declared but never referenced
          detected during:
            instantiation of "void flash::compute_attn_1rowblock<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params,Is_Q_split_K>(const Params &, int, int, int) [with Kernel_traits=Flash_fwd_kernel_traits<32, 128, 128, 4, false, false, cutlass::half_t, Flash_kernel_traits<32, 128, 128, 4, cutlass::half_t>>, Is_dropout=true, Is_causal=false, Is_even_N=true, Is_even_K=true, Return_softmax=false, Params=Flash_fwd_params, Is_Q_split_K=false]" 
(589): here
            instantiation of "void flash::compute_attn<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &) [with Kernel_traits=Flash_fwd_kernel_traits<32, 128, 128, 4, false, false, cutlass::half_t, Flash_kernel_traits<32, 128, 128, 4, cutlass::half_t>>, Is_dropout=true, Is_causal=false, Is_even_N=true, Is_even_K=true, Return_softmax=false, Params=Flash_fwd_params]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(15): here
            instantiation of "void flash_fwd_kernel<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax>(Flash_fwd_params) [with Kernel_traits=Flash_fwd_kernel_traits<32, 128, 128, 4, false, false, cutlass::half_t, Flash_kernel_traits<32, 128, 128, 4, cutlass::half_t>>, Is_dropout=true, Is_causal=false, Is_even_N=true, Is_even_K=true, Return_softmax=false]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(34): here
            instantiation of "void run_flash_fwd<Kernel_traits,Is_dropout,Is_causal>(Flash_fwd_params &, cudaStream_t) [with Kernel_traits=Flash_fwd_kernel_traits<32, 128, 128, 4, false, false, cutlass::half_t, Flash_kernel_traits<32, 128, 128, 4, cutlass::half_t>>, Is_dropout=true, Is_causal=false]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(58): here
            instantiation of "void run_mha_fwd_hdim32<T>(Flash_fwd_params &, cudaStream_t) [with T=cutlass::half_t]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_hdim32_fp16_sm80.cu(22): here

/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_kernel.h(276): warning: variable "tQrQ" was declared but never referenced
          detected during:
            instantiation of "void flash::compute_attn_1rowblock<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params,Is_Q_split_K>(const Params &, int, int, int) [with Kernel_traits=Flash_fwd_kernel_traits<32, 128, 128, 4, false, false, cutlass::half_t, Flash_kernel_traits<32, 128, 128, 4, cutlass::half_t>>, Is_dropout=true, Is_causal=false, Is_even_N=true, Is_even_K=true, Return_softmax=false, Params=Flash_fwd_params, Is_Q_split_K=false]" 
(589): here
            instantiation of "void flash::compute_attn<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &) [with Kernel_traits=Flash_fwd_kernel_traits<32, 128, 128, 4, false, false, cutlass::half_t, Flash_kernel_traits<32, 128, 128, 4, cutlass::half_t>>, Is_dropout=true, Is_causal=false, Is_even_N=true, Is_even_K=true, Return_softmax=false, Params=Flash_fwd_params]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(15): here
            instantiation of "void flash_fwd_kernel<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax>(Flash_fwd_params) [with Kernel_traits=Flash_fwd_kernel_traits<32, 128, 128, 4, false, false, cutlass::half_t, Flash_kernel_traits<32, 128, 128, 4, cutlass::half_t>>, Is_dropout=true, Is_causal=false, Is_even_N=true, Is_even_K=true, Return_softmax=false]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(34): here
            instantiation of "void run_flash_fwd<Kernel_traits,Is_dropout,Is_causal>(Flash_fwd_params &, cudaStream_t) [with Kernel_traits=Flash_fwd_kernel_traits<32, 128, 128, 4, false, false, cutlass::half_t, Flash_kernel_traits<32, 128, 128, 4, cutlass::half_t>>, Is_dropout=true, Is_causal=false]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(58): here
            instantiation of "void run_mha_fwd_hdim32<T>(Flash_fwd_params &, cudaStream_t) [with T=cutlass::half_t]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_hdim32_fp16_sm80.cu(22): here

/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_kernel.h(140): warning: variable "MMA_M" was declared but never referenced
          detected during:
            instantiation of "void flash::compute_attn_1rowblock<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params,Is_Q_split_K>(const Params &, int, int, int) [with Kernel_traits=Flash_fwd_kernel_traits<32, 128, 128, 4, false, false, cutlass::half_t, Flash_kernel_traits<32, 128, 128, 4, cutlass::half_t>>, Is_dropout=true, Is_causal=false, Is_even_N=true, Is_even_K=false, Return_softmax=true, Params=Flash_fwd_params, Is_Q_split_K=false]" 
(589): here
            instantiation of "void flash::compute_attn<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &) [with Kernel_traits=Flash_fwd_kernel_traits<32, 128, 128, 4, false, false, cutlass::half_t, Flash_kernel_traits<32, 128, 128, 4, cutlass::half_t>>, Is_dropout=true, Is_causal=false, Is_even_N=true, Is_even_K=false, Return_softmax=true, Params=Flash_fwd_params]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(15): here
            instantiation of "void flash_fwd_kernel<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax>(Flash_fwd_params) [with Kernel_traits=Flash_fwd_kernel_traits<32, 128, 128, 4, false, false, cutlass::half_t, Flash_kernel_traits<32, 128, 128, 4, cutlass::half_t>>, Is_dropout=true, Is_causal=false, Is_even_N=true, Is_even_K=false, Return_softmax=true]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(34): here
            instantiation of "void run_flash_fwd<Kernel_traits,Is_dropout,Is_causal>(Flash_fwd_params &, cudaStream_t) [with Kernel_traits=Flash_fwd_kernel_traits<32, 128, 128, 4, false, false, cutlass::half_t, Flash_kernel_traits<32, 128, 128, 4, cutlass::half_t>>, Is_dropout=true, Is_causal=false]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(58): here
            instantiation of "void run_mha_fwd_hdim32<T>(Flash_fwd_params &, cudaStream_t) [with T=cutlass::half_t]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_hdim32_fp16_sm80.cu(22): here

/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_kernel.h(276): warning: variable "tQrQ" was declared but never referenced
          detected during:
            instantiation of "void flash::compute_attn_1rowblock<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params,Is_Q_split_K>(const Params &, int, int, int) [with Kernel_traits=Flash_fwd_kernel_traits<32, 128, 128, 4, false, false, cutlass::half_t, Flash_kernel_traits<32, 128, 128, 4, cutlass::half_t>>, Is_dropout=true, Is_causal=false, Is_even_N=true, Is_even_K=false, Return_softmax=true, Params=Flash_fwd_params, Is_Q_split_K=false]" 
(589): here
            instantiation of "void flash::compute_attn<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &) [with Kernel_traits=Flash_fwd_kernel_traits<32, 128, 128, 4, false, false, cutlass::half_t, Flash_kernel_traits<32, 128, 128, 4, cutlass::half_t>>, Is_dropout=true, Is_causal=false, Is_even_N=true, Is_even_K=false, Return_softmax=true, Params=Flash_fwd_params]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(15): here
            instantiation of "void flash_fwd_kernel<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax>(Flash_fwd_params) [with Kernel_traits=Flash_fwd_kernel_traits<32, 128, 128, 4, false, false, cutlass::half_t, Flash_kernel_traits<32, 128, 128, 4, cutlass::half_t>>, Is_dropout=true, Is_causal=false, Is_even_N=true, Is_even_K=false, Return_softmax=true]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(34): here
            instantiation of "void run_flash_fwd<Kernel_traits,Is_dropout,Is_causal>(Flash_fwd_params &, cudaStream_t) [with Kernel_traits=Flash_fwd_kernel_traits<32, 128, 128, 4, false, false, cutlass::half_t, Flash_kernel_traits<32, 128, 128, 4, cutlass::half_t>>, Is_dropout=true, Is_causal=false]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(58): here
            instantiation of "void run_mha_fwd_hdim32<T>(Flash_fwd_params &, cudaStream_t) [with T=cutlass::half_t]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_hdim32_fp16_sm80.cu(22): here

/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_kernel.h(140): warning: variable "MMA_M" was declared but never referenced
          detected during:
            instantiation of "void flash::compute_attn_1rowblock<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params,Is_Q_split_K>(const Params &, int, int, int) [with Kernel_traits=Flash_fwd_kernel_traits<32, 128, 128, 4, false, false, cutlass::half_t, Flash_kernel_traits<32, 128, 128, 4, cutlass::half_t>>, Is_dropout=true, Is_causal=false, Is_even_N=true, Is_even_K=false, Return_softmax=false, Params=Flash_fwd_params, Is_Q_split_K=false]" 
(589): here
            instantiation of "void flash::compute_attn<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &) [with Kernel_traits=Flash_fwd_kernel_traits<32, 128, 128, 4, false, false, cutlass::half_t, Flash_kernel_traits<32, 128, 128, 4, cutlass::half_t>>, Is_dropout=true, Is_causal=false, Is_even_N=true, Is_even_K=false, Return_softmax=false, Params=Flash_fwd_params]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(15): here
            instantiation of "void flash_fwd_kernel<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax>(Flash_fwd_params) [with Kernel_traits=Flash_fwd_kernel_traits<32, 128, 128, 4, false, false, cutlass::half_t, Flash_kernel_traits<32, 128, 128, 4, cutlass::half_t>>, Is_dropout=true, Is_causal=false, Is_even_N=true, Is_even_K=false, Return_softmax=false]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(34): here
            instantiation of "void run_flash_fwd<Kernel_traits,Is_dropout,Is_causal>(Flash_fwd_params &, cudaStream_t) [with Kernel_traits=Flash_fwd_kernel_traits<32, 128, 128, 4, false, false, cutlass::half_t, Flash_kernel_traits<32, 128, 128, 4, cutlass::half_t>>, Is_dropout=true, Is_causal=false]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(58): here
            instantiation of "void run_mha_fwd_hdim32<T>(Flash_fwd_params &, cudaStream_t) [with T=cutlass::half_t]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_hdim32_fp16_sm80.cu(22): here

/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_kernel.h(276): warning: variable "tQrQ" was declared but never referenced
          detected during:
            instantiation of "void flash::compute_attn_1rowblock<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params,Is_Q_split_K>(const Params &, int, int, int) [with Kernel_traits=Flash_fwd_kernel_traits<32, 128, 128, 4, false, false, cutlass::half_t, Flash_kernel_traits<32, 128, 128, 4, cutlass::half_t>>, Is_dropout=true, Is_causal=false, Is_even_N=true, Is_even_K=false, Return_softmax=false, Params=Flash_fwd_params, Is_Q_split_K=false]" 
(589): here
            instantiation of "void flash::compute_attn<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &) [with Kernel_traits=Flash_fwd_kernel_traits<32, 128, 128, 4, false, false, cutlass::half_t, Flash_kernel_traits<32, 128, 128, 4, cutlass::half_t>>, Is_dropout=true, Is_causal=false, Is_even_N=true, Is_even_K=false, Return_softmax=false, Params=Flash_fwd_params]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(15): here
            instantiation of "void flash_fwd_kernel<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax>(Flash_fwd_params) [with Kernel_traits=Flash_fwd_kernel_traits<32, 128, 128, 4, false, false, cutlass::half_t, Flash_kernel_traits<32, 128, 128, 4, cutlass::half_t>>, Is_dropout=true, Is_causal=false, Is_even_N=true, Is_even_K=false, Return_softmax=false]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(34): here
            instantiation of "void run_flash_fwd<Kernel_traits,Is_dropout,Is_causal>(Flash_fwd_params &, cudaStream_t) [with Kernel_traits=Flash_fwd_kernel_traits<32, 128, 128, 4, false, false, cutlass::half_t, Flash_kernel_traits<32, 128, 128, 4, cutlass::half_t>>, Is_dropout=true, Is_causal=false]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(58): here
            instantiation of "void run_mha_fwd_hdim32<T>(Flash_fwd_params &, cudaStream_t) [with T=cutlass::half_t]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_hdim32_fp16_sm80.cu(22): here

/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_kernel.h(140): warning: variable "MMA_M" was declared but never referenced
          detected during:
            instantiation of "void flash::compute_attn_1rowblock<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params,Is_Q_split_K>(const Params &, int, int, int) [with Kernel_traits=Flash_fwd_kernel_traits<32, 128, 128, 4, false, false, cutlass::half_t, Flash_kernel_traits<32, 128, 128, 4, cutlass::half_t>>, Is_dropout=true, Is_causal=false, Is_even_N=false, Is_even_K=true, Return_softmax=true, Params=Flash_fwd_params, Is_Q_split_K=false]" 
(589): here
            instantiation of "void flash::compute_attn<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &) [with Kernel_traits=Flash_fwd_kernel_traits<32, 128, 128, 4, false, false, cutlass::half_t, Flash_kernel_traits<32, 128, 128, 4, cutlass::half_t>>, Is_dropout=true, Is_causal=false, Is_even_N=false, Is_even_K=true, Return_softmax=true, Params=Flash_fwd_params]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(15): here
            instantiation of "void flash_fwd_kernel<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax>(Flash_fwd_params) [with Kernel_traits=Flash_fwd_kernel_traits<32, 128, 128, 4, false, false, cutlass::half_t, Flash_kernel_traits<32, 128, 128, 4, cutlass::half_t>>, Is_dropout=true, Is_causal=false, Is_even_N=false, Is_even_K=true, Return_softmax=true]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(34): here
            instantiation of "void run_flash_fwd<Kernel_traits,Is_dropout,Is_causal>(Flash_fwd_params &, cudaStream_t) [with Kernel_traits=Flash_fwd_kernel_traits<32, 128, 128, 4, false, false, cutlass::half_t, Flash_kernel_traits<32, 128, 128, 4, cutlass::half_t>>, Is_dropout=true, Is_causal=false]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(58): here
            instantiation of "void run_mha_fwd_hdim32<T>(Flash_fwd_params &, cudaStream_t) [with T=cutlass::half_t]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_hdim32_fp16_sm80.cu(22): here

/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_kernel.h(276): warning: variable "tQrQ" was declared but never referenced
          detected during:
            instantiation of "void flash::compute_attn_1rowblock<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params,Is_Q_split_K>(const Params &, int, int, int) [with Kernel_traits=Flash_fwd_kernel_traits<32, 128, 128, 4, false, false, cutlass::half_t, Flash_kernel_traits<32, 128, 128, 4, cutlass::half_t>>, Is_dropout=true, Is_causal=false, Is_even_N=false, Is_even_K=true, Return_softmax=true, Params=Flash_fwd_params, Is_Q_split_K=false]" 
(589): here
            instantiation of "void flash::compute_attn<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &) [with Kernel_traits=Flash_fwd_kernel_traits<32, 128, 128, 4, false, false, cutlass::half_t, Flash_kernel_traits<32, 128, 128, 4, cutlass::half_t>>, Is_dropout=true, Is_causal=false, Is_even_N=false, Is_even_K=true, Return_softmax=true, Params=Flash_fwd_params]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(15): here
            instantiation of "void flash_fwd_kernel<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax>(Flash_fwd_params) [with Kernel_traits=Flash_fwd_kernel_traits<32, 128, 128, 4, false, false, cutlass::half_t, Flash_kernel_traits<32, 128, 128, 4, cutlass::half_t>>, Is_dropout=true, Is_causal=false, Is_even_N=false, Is_even_K=true, Return_softmax=true]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(34): here
            instantiation of "void run_flash_fwd<Kernel_traits,Is_dropout,Is_causal>(Flash_fwd_params &, cudaStream_t) [with Kernel_traits=Flash_fwd_kernel_traits<32, 128, 128, 4, false, false, cutlass::half_t, Flash_kernel_traits<32, 128, 128, 4, cutlass::half_t>>, Is_dropout=true, Is_causal=false]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(58): here
            instantiation of "void run_mha_fwd_hdim32<T>(Flash_fwd_params &, cudaStream_t) [with T=cutlass::half_t]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_hdim32_fp16_sm80.cu(22): here

/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_kernel.h(140): warning: variable "MMA_M" was declared but never referenced
          detected during:
            instantiation of "void flash::compute_attn_1rowblock<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params,Is_Q_split_K>(const Params &, int, int, int) [with Kernel_traits=Flash_fwd_kernel_traits<32, 128, 128, 4, false, false, cutlass::half_t, Flash_kernel_traits<32, 128, 128, 4, cutlass::half_t>>, Is_dropout=true, Is_causal=false, Is_even_N=false, Is_even_K=true, Return_softmax=false, Params=Flash_fwd_params, Is_Q_split_K=false]" 
(589): here
            instantiation of "void flash::compute_attn<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &) [with Kernel_traits=Flash_fwd_kernel_traits<32, 128, 128, 4, false, false, cutlass::half_t, Flash_kernel_traits<32, 128, 128, 4, cutlass::half_t>>, Is_dropout=true, Is_causal=false, Is_even_N=false, Is_even_K=true, Return_softmax=false, Params=Flash_fwd_params]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(15): here
            instantiation of "void flash_fwd_kernel<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax>(Flash_fwd_params) [with Kernel_traits=Flash_fwd_kernel_traits<32, 128, 128, 4, false, false, cutlass::half_t, Flash_kernel_traits<32, 128, 128, 4, cutlass::half_t>>, Is_dropout=true, Is_causal=false, Is_even_N=false, Is_even_K=true, Return_softmax=false]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(34): here
            instantiation of "void run_flash_fwd<Kernel_traits,Is_dropout,Is_causal>(Flash_fwd_params &, cudaStream_t) [with Kernel_traits=Flash_fwd_kernel_traits<32, 128, 128, 4, false, false, cutlass::half_t, Flash_kernel_traits<32, 128, 128, 4, cutlass::half_t>>, Is_dropout=true, Is_causal=false]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(58): here
            instantiation of "void run_mha_fwd_hdim32<T>(Flash_fwd_params &, cudaStream_t) [with T=cutlass::half_t]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_hdim32_fp16_sm80.cu(22): here

/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_kernel.h(276): warning: variable "tQrQ" was declared but never referenced
          detected during:
            instantiation of "void flash::compute_attn_1rowblock<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params,Is_Q_split_K>(const Params &, int, int, int) [with Kernel_traits=Flash_fwd_kernel_traits<32, 128, 128, 4, false, false, cutlass::half_t, Flash_kernel_traits<32, 128, 128, 4, cutlass::half_t>>, Is_dropout=true, Is_causal=false, Is_even_N=false, Is_even_K=true, Return_softmax=false, Params=Flash_fwd_params, Is_Q_split_K=false]" 
(589): here
            instantiation of "void flash::compute_attn<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &) [with Kernel_traits=Flash_fwd_kernel_traits<32, 128, 128, 4, false, false, cutlass::half_t, Flash_kernel_traits<32, 128, 128, 4, cutlass::half_t>>, Is_dropout=true, Is_causal=false, Is_even_N=false, Is_even_K=true, Return_softmax=false, Params=Flash_fwd_params]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(15): here
            instantiation of "void flash_fwd_kernel<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax>(Flash_fwd_params) [with Kernel_traits=Flash_fwd_kernel_traits<32, 128, 128, 4, false, false, cutlass::half_t, Flash_kernel_traits<32, 128, 128, 4, cutlass::half_t>>, Is_dropout=true, Is_causal=false, Is_even_N=false, Is_even_K=true, Return_softmax=false]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(34): here
            instantiation of "void run_flash_fwd<Kernel_traits,Is_dropout,Is_causal>(Flash_fwd_params &, cudaStream_t) [with Kernel_traits=Flash_fwd_kernel_traits<32, 128, 128, 4, false, false, cutlass::half_t, Flash_kernel_traits<32, 128, 128, 4, cutlass::half_t>>, Is_dropout=true, Is_causal=false]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(58): here
            instantiation of "void run_mha_fwd_hdim32<T>(Flash_fwd_params &, cudaStream_t) [with T=cutlass::half_t]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_hdim32_fp16_sm80.cu(22): here

/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_kernel.h(140): warning: variable "MMA_M" was declared but never referenced
          detected during:
            instantiation of "void flash::compute_attn_1rowblock<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params,Is_Q_split_K>(const Params &, int, int, int) [with Kernel_traits=Flash_fwd_kernel_traits<32, 128, 128, 4, false, false, cutlass::half_t, Flash_kernel_traits<32, 128, 128, 4, cutlass::half_t>>, Is_dropout=true, Is_causal=false, Is_even_N=false, Is_even_K=false, Return_softmax=true, Params=Flash_fwd_params, Is_Q_split_K=false]" 
(589): here
            instantiation of "void flash::compute_attn<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &) [with Kernel_traits=Flash_fwd_kernel_traits<32, 128, 128, 4, false, false, cutlass::half_t, Flash_kernel_traits<32, 128, 128, 4, cutlass::half_t>>, Is_dropout=true, Is_causal=false, Is_even_N=false, Is_even_K=false, Return_softmax=true, Params=Flash_fwd_params]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(15): here
            instantiation of "void flash_fwd_kernel<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax>(Flash_fwd_params) [with Kernel_traits=Flash_fwd_kernel_traits<32, 128, 128, 4, false, false, cutlass::half_t, Flash_kernel_traits<32, 128, 128, 4, cutlass::half_t>>, Is_dropout=true, Is_causal=false, Is_even_N=false, Is_even_K=false, Return_softmax=true]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(34): here
            instantiation of "void run_flash_fwd<Kernel_traits,Is_dropout,Is_causal>(Flash_fwd_params &, cudaStream_t) [with Kernel_traits=Flash_fwd_kernel_traits<32, 128, 128, 4, false, false, cutlass::half_t, Flash_kernel_traits<32, 128, 128, 4, cutlass::half_t>>, Is_dropout=true, Is_causal=false]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(58): here
            instantiation of "void run_mha_fwd_hdim32<T>(Flash_fwd_params &, cudaStream_t) [with T=cutlass::half_t]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_hdim32_fp16_sm80.cu(22): here

/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_kernel.h(276): warning: variable "tQrQ" was declared but never referenced
          detected during:
            instantiation of "void flash::compute_attn_1rowblock<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params,Is_Q_split_K>(const Params &, int, int, int) [with Kernel_traits=Flash_fwd_kernel_traits<32, 128, 128, 4, false, false, cutlass::half_t, Flash_kernel_traits<32, 128, 128, 4, cutlass::half_t>>, Is_dropout=true, Is_causal=false, Is_even_N=false, Is_even_K=false, Return_softmax=true, Params=Flash_fwd_params, Is_Q_split_K=false]" 
(589): here
            instantiation of "void flash::compute_attn<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &) [with Kernel_traits=Flash_fwd_kernel_traits<32, 128, 128, 4, false, false, cutlass::half_t, Flash_kernel_traits<32, 128, 128, 4, cutlass::half_t>>, Is_dropout=true, Is_causal=false, Is_even_N=false, Is_even_K=false, Return_softmax=true, Params=Flash_fwd_params]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(15): here
            instantiation of "void flash_fwd_kernel<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax>(Flash_fwd_params) [with Kernel_traits=Flash_fwd_kernel_traits<32, 128, 128, 4, false, false, cutlass::half_t, Flash_kernel_traits<32, 128, 128, 4, cutlass::half_t>>, Is_dropout=true, Is_causal=false, Is_even_N=false, Is_even_K=false, Return_softmax=true]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(34): here
            instantiation of "void run_flash_fwd<Kernel_traits,Is_dropout,Is_causal>(Flash_fwd_params &, cudaStream_t) [with Kernel_traits=Flash_fwd_kernel_traits<32, 128, 128, 4, false, false, cutlass::half_t, Flash_kernel_traits<32, 128, 128, 4, cutlass::half_t>>, Is_dropout=true, Is_causal=false]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(58): here
            instantiation of "void run_mha_fwd_hdim32<T>(Flash_fwd_params &, cudaStream_t) [with T=cutlass::half_t]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_hdim32_fp16_sm80.cu(22): here

/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_kernel.h(140): warning: variable "MMA_M" was declared but never referenced
          detected during:
            instantiation of "void flash::compute_attn_1rowblock<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params,Is_Q_split_K>(const Params &, int, int, int) [with Kernel_traits=Flash_fwd_kernel_traits<32, 128, 128, 4, false, false, cutlass::half_t, Flash_kernel_traits<32, 128, 128, 4, cutlass::half_t>>, Is_dropout=true, Is_causal=false, Is_even_N=false, Is_even_K=false, Return_softmax=false, Params=Flash_fwd_params, Is_Q_split_K=false]" 
(589): here
            instantiation of "void flash::compute_attn<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &) [with Kernel_traits=Flash_fwd_kernel_traits<32, 128, 128, 4, false, false, cutlass::half_t, Flash_kernel_traits<32, 128, 128, 4, cutlass::half_t>>, Is_dropout=true, Is_causal=false, Is_even_N=false, Is_even_K=false, Return_softmax=false, Params=Flash_fwd_params]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(15): here
            instantiation of "void flash_fwd_kernel<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax>(Flash_fwd_params) [with Kernel_traits=Flash_fwd_kernel_traits<32, 128, 128, 4, false, false, cutlass::half_t, Flash_kernel_traits<32, 128, 128, 4, cutlass::half_t>>, Is_dropout=true, Is_causal=false, Is_even_N=false, Is_even_K=false, Return_softmax=false]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(34): here
            instantiation of "void run_flash_fwd<Kernel_traits,Is_dropout,Is_causal>(Flash_fwd_params &, cudaStream_t) [with Kernel_traits=Flash_fwd_kernel_traits<32, 128, 128, 4, false, false, cutlass::half_t, Flash_kernel_traits<32, 128, 128, 4, cutlass::half_t>>, Is_dropout=true, Is_causal=false]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(58): here
            instantiation of "void run_mha_fwd_hdim32<T>(Flash_fwd_params &, cudaStream_t) [with T=cutlass::half_t]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_hdim32_fp16_sm80.cu(22): here

/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_kernel.h(276): warning: variable "tQrQ" was declared but never referenced
          detected during:
            instantiation of "void flash::compute_attn_1rowblock<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params,Is_Q_split_K>(const Params &, int, int, int) [with Kernel_traits=Flash_fwd_kernel_traits<32, 128, 128, 4, false, false, cutlass::half_t, Flash_kernel_traits<32, 128, 128, 4, cutlass::half_t>>, Is_dropout=true, Is_causal=false, Is_even_N=false, Is_even_K=false, Return_softmax=false, Params=Flash_fwd_params, Is_Q_split_K=false]" 
(589): here
            instantiation of "void flash::compute_attn<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &) [with Kernel_traits=Flash_fwd_kernel_traits<32, 128, 128, 4, false, false, cutlass::half_t, Flash_kernel_traits<32, 128, 128, 4, cutlass::half_t>>, Is_dropout=true, Is_causal=false, Is_even_N=false, Is_even_K=false, Return_softmax=false, Params=Flash_fwd_params]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(15): here
            instantiation of "void flash_fwd_kernel<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax>(Flash_fwd_params) [with Kernel_traits=Flash_fwd_kernel_traits<32, 128, 128, 4, false, false, cutlass::half_t, Flash_kernel_traits<32, 128, 128, 4, cutlass::half_t>>, Is_dropout=true, Is_causal=false, Is_even_N=false, Is_even_K=false, Return_softmax=false]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(34): here
            instantiation of "void run_flash_fwd<Kernel_traits,Is_dropout,Is_causal>(Flash_fwd_params &, cudaStream_t) [with Kernel_traits=Flash_fwd_kernel_traits<32, 128, 128, 4, false, false, cutlass::half_t, Flash_kernel_traits<32, 128, 128, 4, cutlass::half_t>>, Is_dropout=true, Is_causal=false]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(58): here
            instantiation of "void run_mha_fwd_hdim32<T>(Flash_fwd_params &, cudaStream_t) [with T=cutlass::half_t]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_hdim32_fp16_sm80.cu(22): here

/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_kernel.h(140): warning: variable "MMA_M" was declared but never referenced
          detected during:
            instantiation of "void flash::compute_attn_1rowblock<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params,Is_Q_split_K>(const Params &, int, int, int) [with Kernel_traits=Flash_fwd_kernel_traits<32, 128, 128, 4, false, false, cutlass::half_t, Flash_kernel_traits<32, 128, 128, 4, cutlass::half_t>>, Is_dropout=false, Is_causal=true, Is_even_N=true, Is_even_K=true, Return_softmax=false, Params=Flash_fwd_params, Is_Q_split_K=false]" 
(589): here
            instantiation of "void flash::compute_attn<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &) [with Kernel_traits=Flash_fwd_kernel_traits<32, 128, 128, 4, false, false, cutlass::half_t, Flash_kernel_traits<32, 128, 128, 4, cutlass::half_t>>, Is_dropout=false, Is_causal=true, Is_even_N=true, Is_even_K=true, Return_softmax=false, Params=Flash_fwd_params]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(15): here
            instantiation of "void flash_fwd_kernel<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax>(Flash_fwd_params) [with Kernel_traits=Flash_fwd_kernel_traits<32, 128, 128, 4, false, false, cutlass::half_t, Flash_kernel_traits<32, 128, 128, 4, cutlass::half_t>>, Is_dropout=false, Is_causal=true, Is_even_N=true, Is_even_K=true, Return_softmax=false]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(34): here
            instantiation of "void run_flash_fwd<Kernel_traits,Is_dropout,Is_causal>(Flash_fwd_params &, cudaStream_t) [with Kernel_traits=Flash_fwd_kernel_traits<32, 128, 128, 4, false, false, cutlass::half_t, Flash_kernel_traits<32, 128, 128, 4, cutlass::half_t>>, Is_dropout=false, Is_causal=true]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(58): here
            instantiation of "void run_mha_fwd_hdim32<T>(Flash_fwd_params &, cudaStream_t) [with T=cutlass::half_t]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_hdim32_fp16_sm80.cu(22): here

/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_kernel.h(276): warning: variable "tQrQ" was declared but never referenced
          detected during:
            instantiation of "void flash::compute_attn_1rowblock<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params,Is_Q_split_K>(const Params &, int, int, int) [with Kernel_traits=Flash_fwd_kernel_traits<32, 128, 128, 4, false, false, cutlass::half_t, Flash_kernel_traits<32, 128, 128, 4, cutlass::half_t>>, Is_dropout=false, Is_causal=true, Is_even_N=true, Is_even_K=true, Return_softmax=false, Params=Flash_fwd_params, Is_Q_split_K=false]" 
(589): here
            instantiation of "void flash::compute_attn<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &) [with Kernel_traits=Flash_fwd_kernel_traits<32, 128, 128, 4, false, false, cutlass::half_t, Flash_kernel_traits<32, 128, 128, 4, cutlass::half_t>>, Is_dropout=false, Is_causal=true, Is_even_N=true, Is_even_K=true, Return_softmax=false, Params=Flash_fwd_params]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(15): here
            instantiation of "void flash_fwd_kernel<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax>(Flash_fwd_params) [with Kernel_traits=Flash_fwd_kernel_traits<32, 128, 128, 4, false, false, cutlass::half_t, Flash_kernel_traits<32, 128, 128, 4, cutlass::half_t>>, Is_dropout=false, Is_causal=true, Is_even_N=true, Is_even_K=true, Return_softmax=false]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(34): here
            instantiation of "void run_flash_fwd<Kernel_traits,Is_dropout,Is_causal>(Flash_fwd_params &, cudaStream_t) [with Kernel_traits=Flash_fwd_kernel_traits<32, 128, 128, 4, false, false, cutlass::half_t, Flash_kernel_traits<32, 128, 128, 4, cutlass::half_t>>, Is_dropout=false, Is_causal=true]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(58): here
            instantiation of "void run_mha_fwd_hdim32<T>(Flash_fwd_params &, cudaStream_t) [with T=cutlass::half_t]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_hdim32_fp16_sm80.cu(22): here

/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_kernel.h(140): warning: variable "MMA_M" was declared but never referenced
          detected during:
            instantiation of "void flash::compute_attn_1rowblock<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params,Is_Q_split_K>(const Params &, int, int, int) [with Kernel_traits=Flash_fwd_kernel_traits<32, 128, 128, 4, false, false, cutlass::half_t, Flash_kernel_traits<32, 128, 128, 4, cutlass::half_t>>, Is_dropout=false, Is_causal=true, Is_even_N=true, Is_even_K=false, Return_softmax=false, Params=Flash_fwd_params, Is_Q_split_K=false]" 
(589): here
            instantiation of "void flash::compute_attn<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &) [with Kernel_traits=Flash_fwd_kernel_traits<32, 128, 128, 4, false, false, cutlass::half_t, Flash_kernel_traits<32, 128, 128, 4, cutlass::half_t>>, Is_dropout=false, Is_causal=true, Is_even_N=true, Is_even_K=false, Return_softmax=false, Params=Flash_fwd_params]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(15): here
            instantiation of "void flash_fwd_kernel<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax>(Flash_fwd_params) [with Kernel_traits=Flash_fwd_kernel_traits<32, 128, 128, 4, false, false, cutlass::half_t, Flash_kernel_traits<32, 128, 128, 4, cutlass::half_t>>, Is_dropout=false, Is_causal=true, Is_even_N=true, Is_even_K=false, Return_softmax=false]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(34): here
            instantiation of "void run_flash_fwd<Kernel_traits,Is_dropout,Is_causal>(Flash_fwd_params &, cudaStream_t) [with Kernel_traits=Flash_fwd_kernel_traits<32, 128, 128, 4, false, false, cutlass::half_t, Flash_kernel_traits<32, 128, 128, 4, cutlass::half_t>>, Is_dropout=false, Is_causal=true]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(58): here
            instantiation of "void run_mha_fwd_hdim32<T>(Flash_fwd_params &, cudaStream_t) [with T=cutlass::half_t]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_hdim32_fp16_sm80.cu(22): here

/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_kernel.h(276): warning: variable "tQrQ" was declared but never referenced
          detected during:
            instantiation of "void flash::compute_attn_1rowblock<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params,Is_Q_split_K>(const Params &, int, int, int) [with Kernel_traits=Flash_fwd_kernel_traits<32, 128, 128, 4, false, false, cutlass::half_t, Flash_kernel_traits<32, 128, 128, 4, cutlass::half_t>>, Is_dropout=false, Is_causal=true, Is_even_N=true, Is_even_K=false, Return_softmax=false, Params=Flash_fwd_params, Is_Q_split_K=false]" 
(589): here
            instantiation of "void flash::compute_attn<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &) [with Kernel_traits=Flash_fwd_kernel_traits<32, 128, 128, 4, false, false, cutlass::half_t, Flash_kernel_traits<32, 128, 128, 4, cutlass::half_t>>, Is_dropout=false, Is_causal=true, Is_even_N=true, Is_even_K=false, Return_softmax=false, Params=Flash_fwd_params]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(15): here
            instantiation of "void flash_fwd_kernel<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax>(Flash_fwd_params) [with Kernel_traits=Flash_fwd_kernel_traits<32, 128, 128, 4, false, false, cutlass::half_t, Flash_kernel_traits<32, 128, 128, 4, cutlass::half_t>>, Is_dropout=false, Is_causal=true, Is_even_N=true, Is_even_K=false, Return_softmax=false]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(34): here
            instantiation of "void run_flash_fwd<Kernel_traits,Is_dropout,Is_causal>(Flash_fwd_params &, cudaStream_t) [with Kernel_traits=Flash_fwd_kernel_traits<32, 128, 128, 4, false, false, cutlass::half_t, Flash_kernel_traits<32, 128, 128, 4, cutlass::half_t>>, Is_dropout=false, Is_causal=true]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(58): here
            instantiation of "void run_mha_fwd_hdim32<T>(Flash_fwd_params &, cudaStream_t) [with T=cutlass::half_t]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_hdim32_fp16_sm80.cu(22): here

/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_kernel.h(140): warning: variable "MMA_M" was declared but never referenced
          detected during:
            instantiation of "void flash::compute_attn_1rowblock<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params,Is_Q_split_K>(const Params &, int, int, int) [with Kernel_traits=Flash_fwd_kernel_traits<32, 128, 128, 4, false, false, cutlass::half_t, Flash_kernel_traits<32, 128, 128, 4, cutlass::half_t>>, Is_dropout=false, Is_causal=true, Is_even_N=false, Is_even_K=true, Return_softmax=false, Params=Flash_fwd_params, Is_Q_split_K=false]" 
(589): here
            instantiation of "void flash::compute_attn<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &) [with Kernel_traits=Flash_fwd_kernel_traits<32, 128, 128, 4, false, false, cutlass::half_t, Flash_kernel_traits<32, 128, 128, 4, cutlass::half_t>>, Is_dropout=false, Is_causal=true, Is_even_N=false, Is_even_K=true, Return_softmax=false, Params=Flash_fwd_params]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(15): here
            instantiation of "void flash_fwd_kernel<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax>(Flash_fwd_params) [with Kernel_traits=Flash_fwd_kernel_traits<32, 128, 128, 4, false, false, cutlass::half_t, Flash_kernel_traits<32, 128, 128, 4, cutlass::half_t>>, Is_dropout=false, Is_causal=true, Is_even_N=false, Is_even_K=true, Return_softmax=false]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(34): here
            instantiation of "void run_flash_fwd<Kernel_traits,Is_dropout,Is_causal>(Flash_fwd_params &, cudaStream_t) [with Kernel_traits=Flash_fwd_kernel_traits<32, 128, 128, 4, false, false, cutlass::half_t, Flash_kernel_traits<32, 128, 128, 4, cutlass::half_t>>, Is_dropout=false, Is_causal=true]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(58): here
            instantiation of "void run_mha_fwd_hdim32<T>(Flash_fwd_params &, cudaStream_t) [with T=cutlass::half_t]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_hdim32_fp16_sm80.cu(22): here

/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_kernel.h(276): warning: variable "tQrQ" was declared but never referenced
          detected during:
            instantiation of "void flash::compute_attn_1rowblock<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params,Is_Q_split_K>(const Params &, int, int, int) [with Kernel_traits=Flash_fwd_kernel_traits<32, 128, 128, 4, false, false, cutlass::half_t, Flash_kernel_traits<32, 128, 128, 4, cutlass::half_t>>, Is_dropout=false, Is_causal=true, Is_even_N=false, Is_even_K=true, Return_softmax=false, Params=Flash_fwd_params, Is_Q_split_K=false]" 
(589): here
            instantiation of "void flash::compute_attn<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &) [with Kernel_traits=Flash_fwd_kernel_traits<32, 128, 128, 4, false, false, cutlass::half_t, Flash_kernel_traits<32, 128, 128, 4, cutlass::half_t>>, Is_dropout=false, Is_causal=true, Is_even_N=false, Is_even_K=true, Return_softmax=false, Params=Flash_fwd_params]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(15): here
            instantiation of "void flash_fwd_kernel<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax>(Flash_fwd_params) [with Kernel_traits=Flash_fwd_kernel_traits<32, 128, 128, 4, false, false, cutlass::half_t, Flash_kernel_traits<32, 128, 128, 4, cutlass::half_t>>, Is_dropout=false, Is_causal=true, Is_even_N=false, Is_even_K=true, Return_softmax=false]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(34): here
            instantiation of "void run_flash_fwd<Kernel_traits,Is_dropout,Is_causal>(Flash_fwd_params &, cudaStream_t) [with Kernel_traits=Flash_fwd_kernel_traits<32, 128, 128, 4, false, false, cutlass::half_t, Flash_kernel_traits<32, 128, 128, 4, cutlass::half_t>>, Is_dropout=false, Is_causal=true]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(58): here
            instantiation of "void run_mha_fwd_hdim32<T>(Flash_fwd_params &, cudaStream_t) [with T=cutlass::half_t]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_hdim32_fp16_sm80.cu(22): here

/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_kernel.h(140): warning: variable "MMA_M" was declared but never referenced
          detected during:
            instantiation of "void flash::compute_attn_1rowblock<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params,Is_Q_split_K>(const Params &, int, int, int) [with Kernel_traits=Flash_fwd_kernel_traits<32, 128, 128, 4, false, false, cutlass::half_t, Flash_kernel_traits<32, 128, 128, 4, cutlass::half_t>>, Is_dropout=false, Is_causal=true, Is_even_N=false, Is_even_K=false, Return_softmax=false, Params=Flash_fwd_params, Is_Q_split_K=false]" 
(589): here
            instantiation of "void flash::compute_attn<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &) [with Kernel_traits=Flash_fwd_kernel_traits<32, 128, 128, 4, false, false, cutlass::half_t, Flash_kernel_traits<32, 128, 128, 4, cutlass::half_t>>, Is_dropout=false, Is_causal=true, Is_even_N=false, Is_even_K=false, Return_softmax=false, Params=Flash_fwd_params]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(15): here
            instantiation of "void flash_fwd_kernel<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax>(Flash_fwd_params) [with Kernel_traits=Flash_fwd_kernel_traits<32, 128, 128, 4, false, false, cutlass::half_t, Flash_kernel_traits<32, 128, 128, 4, cutlass::half_t>>, Is_dropout=false, Is_causal=true, Is_even_N=false, Is_even_K=false, Return_softmax=false]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(34): here
            instantiation of "void run_flash_fwd<Kernel_traits,Is_dropout,Is_causal>(Flash_fwd_params &, cudaStream_t) [with Kernel_traits=Flash_fwd_kernel_traits<32, 128, 128, 4, false, false, cutlass::half_t, Flash_kernel_traits<32, 128, 128, 4, cutlass::half_t>>, Is_dropout=false, Is_causal=true]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(58): here
            instantiation of "void run_mha_fwd_hdim32<T>(Flash_fwd_params &, cudaStream_t) [with T=cutlass::half_t]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_hdim32_fp16_sm80.cu(22): here

/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_kernel.h(276): warning: variable "tQrQ" was declared but never referenced
          detected during:
            instantiation of "void flash::compute_attn_1rowblock<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params,Is_Q_split_K>(const Params &, int, int, int) [with Kernel_traits=Flash_fwd_kernel_traits<32, 128, 128, 4, false, false, cutlass::half_t, Flash_kernel_traits<32, 128, 128, 4, cutlass::half_t>>, Is_dropout=false, Is_causal=true, Is_even_N=false, Is_even_K=false, Return_softmax=false, Params=Flash_fwd_params, Is_Q_split_K=false]" 
(589): here
            instantiation of "void flash::compute_attn<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &) [with Kernel_traits=Flash_fwd_kernel_traits<32, 128, 128, 4, false, false, cutlass::half_t, Flash_kernel_traits<32, 128, 128, 4, cutlass::half_t>>, Is_dropout=false, Is_causal=true, Is_even_N=false, Is_even_K=false, Return_softmax=false, Params=Flash_fwd_params]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(15): here
            instantiation of "void flash_fwd_kernel<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax>(Flash_fwd_params) [with Kernel_traits=Flash_fwd_kernel_traits<32, 128, 128, 4, false, false, cutlass::half_t, Flash_kernel_traits<32, 128, 128, 4, cutlass::half_t>>, Is_dropout=false, Is_causal=true, Is_even_N=false, Is_even_K=false, Return_softmax=false]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(34): here
            instantiation of "void run_flash_fwd<Kernel_traits,Is_dropout,Is_causal>(Flash_fwd_params &, cudaStream_t) [with Kernel_traits=Flash_fwd_kernel_traits<32, 128, 128, 4, false, false, cutlass::half_t, Flash_kernel_traits<32, 128, 128, 4, cutlass::half_t>>, Is_dropout=false, Is_causal=true]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(58): here
            instantiation of "void run_mha_fwd_hdim32<T>(Flash_fwd_params &, cudaStream_t) [with T=cutlass::half_t]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_hdim32_fp16_sm80.cu(22): here

/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_kernel.h(140): warning: variable "MMA_M" was declared but never referenced
          detected during:
            instantiation of "void flash::compute_attn_1rowblock<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params,Is_Q_split_K>(const Params &, int, int, int) [with Kernel_traits=Flash_fwd_kernel_traits<32, 128, 128, 4, false, false, cutlass::half_t, Flash_kernel_traits<32, 128, 128, 4, cutlass::half_t>>, Is_dropout=false, Is_causal=false, Is_even_N=true, Is_even_K=true, Return_softmax=false, Params=Flash_fwd_params, Is_Q_split_K=false]" 
(589): here
            instantiation of "void flash::compute_attn<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &) [with Kernel_traits=Flash_fwd_kernel_traits<32, 128, 128, 4, false, false, cutlass::half_t, Flash_kernel_traits<32, 128, 128, 4, cutlass::half_t>>, Is_dropout=false, Is_causal=false, Is_even_N=true, Is_even_K=true, Return_softmax=false, Params=Flash_fwd_params]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(15): here
            instantiation of "void flash_fwd_kernel<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax>(Flash_fwd_params) [with Kernel_traits=Flash_fwd_kernel_traits<32, 128, 128, 4, false, false, cutlass::half_t, Flash_kernel_traits<32, 128, 128, 4, cutlass::half_t>>, Is_dropout=false, Is_causal=false, Is_even_N=true, Is_even_K=true, Return_softmax=false]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(34): here
            instantiation of "void run_flash_fwd<Kernel_traits,Is_dropout,Is_causal>(Flash_fwd_params &, cudaStream_t) [with Kernel_traits=Flash_fwd_kernel_traits<32, 128, 128, 4, false, false, cutlass::half_t, Flash_kernel_traits<32, 128, 128, 4, cutlass::half_t>>, Is_dropout=false, Is_causal=false]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(58): here
            instantiation of "void run_mha_fwd_hdim32<T>(Flash_fwd_params &, cudaStream_t) [with T=cutlass::half_t]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_hdim32_fp16_sm80.cu(22): here

/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_kernel.h(276): warning: variable "tQrQ" was declared but never referenced
          detected during:
            instantiation of "void flash::compute_attn_1rowblock<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params,Is_Q_split_K>(const Params &, int, int, int) [with Kernel_traits=Flash_fwd_kernel_traits<32, 128, 128, 4, false, false, cutlass::half_t, Flash_kernel_traits<32, 128, 128, 4, cutlass::half_t>>, Is_dropout=false, Is_causal=false, Is_even_N=true, Is_even_K=true, Return_softmax=false, Params=Flash_fwd_params, Is_Q_split_K=false]" 
(589): here
            instantiation of "void flash::compute_attn<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &) [with Kernel_traits=Flash_fwd_kernel_traits<32, 128, 128, 4, false, false, cutlass::half_t, Flash_kernel_traits<32, 128, 128, 4, cutlass::half_t>>, Is_dropout=false, Is_causal=false, Is_even_N=true, Is_even_K=true, Return_softmax=false, Params=Flash_fwd_params]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(15): here
            instantiation of "void flash_fwd_kernel<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax>(Flash_fwd_params) [with Kernel_traits=Flash_fwd_kernel_traits<32, 128, 128, 4, false, false, cutlass::half_t, Flash_kernel_traits<32, 128, 128, 4, cutlass::half_t>>, Is_dropout=false, Is_causal=false, Is_even_N=true, Is_even_K=true, Return_softmax=false]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(34): here
            instantiation of "void run_flash_fwd<Kernel_traits,Is_dropout,Is_causal>(Flash_fwd_params &, cudaStream_t) [with Kernel_traits=Flash_fwd_kernel_traits<32, 128, 128, 4, false, false, cutlass::half_t, Flash_kernel_traits<32, 128, 128, 4, cutlass::half_t>>, Is_dropout=false, Is_causal=false]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(58): here
            instantiation of "void run_mha_fwd_hdim32<T>(Flash_fwd_params &, cudaStream_t) [with T=cutlass::half_t]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_hdim32_fp16_sm80.cu(22): here

/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_kernel.h(140): warning: variable "MMA_M" was declared but never referenced
          detected during:
            instantiation of "void flash::compute_attn_1rowblock<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params,Is_Q_split_K>(const Params &, int, int, int) [with Kernel_traits=Flash_fwd_kernel_traits<32, 128, 128, 4, false, false, cutlass::half_t, Flash_kernel_traits<32, 128, 128, 4, cutlass::half_t>>, Is_dropout=false, Is_causal=false, Is_even_N=true, Is_even_K=false, Return_softmax=false, Params=Flash_fwd_params, Is_Q_split_K=false]" 
(589): here
            instantiation of "void flash::compute_attn<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &) [with Kernel_traits=Flash_fwd_kernel_traits<32, 128, 128, 4, false, false, cutlass::half_t, Flash_kernel_traits<32, 128, 128, 4, cutlass::half_t>>, Is_dropout=false, Is_causal=false, Is_even_N=true, Is_even_K=false, Return_softmax=false, Params=Flash_fwd_params]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(15): here
            instantiation of "void flash_fwd_kernel<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax>(Flash_fwd_params) [with Kernel_traits=Flash_fwd_kernel_traits<32, 128, 128, 4, false, false, cutlass::half_t, Flash_kernel_traits<32, 128, 128, 4, cutlass::half_t>>, Is_dropout=false, Is_causal=false, Is_even_N=true, Is_even_K=false, Return_softmax=false]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(34): here
            instantiation of "void run_flash_fwd<Kernel_traits,Is_dropout,Is_causal>(Flash_fwd_params &, cudaStream_t) [with Kernel_traits=Flash_fwd_kernel_traits<32, 128, 128, 4, false, false, cutlass::half_t, Flash_kernel_traits<32, 128, 128, 4, cutlass::half_t>>, Is_dropout=false, Is_causal=false]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(58): here
            instantiation of "void run_mha_fwd_hdim32<T>(Flash_fwd_params &, cudaStream_t) [with T=cutlass::half_t]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_hdim32_fp16_sm80.cu(22): here

/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_kernel.h(276): warning: variable "tQrQ" was declared but never referenced
          detected during:
            instantiation of "void flash::compute_attn_1rowblock<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params,Is_Q_split_K>(const Params &, int, int, int) [with Kernel_traits=Flash_fwd_kernel_traits<32, 128, 128, 4, false, false, cutlass::half_t, Flash_kernel_traits<32, 128, 128, 4, cutlass::half_t>>, Is_dropout=false, Is_causal=false, Is_even_N=true, Is_even_K=false, Return_softmax=false, Params=Flash_fwd_params, Is_Q_split_K=false]" 
(589): here
            instantiation of "void flash::compute_attn<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &) [with Kernel_traits=Flash_fwd_kernel_traits<32, 128, 128, 4, false, false, cutlass::half_t, Flash_kernel_traits<32, 128, 128, 4, cutlass::half_t>>, Is_dropout=false, Is_causal=false, Is_even_N=true, Is_even_K=false, Return_softmax=false, Params=Flash_fwd_params]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(15): here
            instantiation of "void flash_fwd_kernel<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax>(Flash_fwd_params) [with Kernel_traits=Flash_fwd_kernel_traits<32, 128, 128, 4, false, false, cutlass::half_t, Flash_kernel_traits<32, 128, 128, 4, cutlass::half_t>>, Is_dropout=false, Is_causal=false, Is_even_N=true, Is_even_K=false, Return_softmax=false]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(34): here
            instantiation of "void run_flash_fwd<Kernel_traits,Is_dropout,Is_causal>(Flash_fwd_params &, cudaStream_t) [with Kernel_traits=Flash_fwd_kernel_traits<32, 128, 128, 4, false, false, cutlass::half_t, Flash_kernel_traits<32, 128, 128, 4, cutlass::half_t>>, Is_dropout=false, Is_causal=false]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(58): here
            instantiation of "void run_mha_fwd_hdim32<T>(Flash_fwd_params &, cudaStream_t) [with T=cutlass::half_t]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_hdim32_fp16_sm80.cu(22): here

/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_kernel.h(140): warning: variable "MMA_M" was declared but never referenced
          detected during:
            instantiation of "void flash::compute_attn_1rowblock<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params,Is_Q_split_K>(const Params &, int, int, int) [with Kernel_traits=Flash_fwd_kernel_traits<32, 128, 128, 4, false, false, cutlass::half_t, Flash_kernel_traits<32, 128, 128, 4, cutlass::half_t>>, Is_dropout=false, Is_causal=false, Is_even_N=false, Is_even_K=true, Return_softmax=false, Params=Flash_fwd_params, Is_Q_split_K=false]" 
(589): here
            instantiation of "void flash::compute_attn<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &) [with Kernel_traits=Flash_fwd_kernel_traits<32, 128, 128, 4, false, false, cutlass::half_t, Flash_kernel_traits<32, 128, 128, 4, cutlass::half_t>>, Is_dropout=false, Is_causal=false, Is_even_N=false, Is_even_K=true, Return_softmax=false, Params=Flash_fwd_params]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(15): here
            instantiation of "void flash_fwd_kernel<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax>(Flash_fwd_params) [with Kernel_traits=Flash_fwd_kernel_traits<32, 128, 128, 4, false, false, cutlass::half_t, Flash_kernel_traits<32, 128, 128, 4, cutlass::half_t>>, Is_dropout=false, Is_causal=false, Is_even_N=false, Is_even_K=true, Return_softmax=false]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(34): here
            instantiation of "void run_flash_fwd<Kernel_traits,Is_dropout,Is_causal>(Flash_fwd_params &, cudaStream_t) [with Kernel_traits=Flash_fwd_kernel_traits<32, 128, 128, 4, false, false, cutlass::half_t, Flash_kernel_traits<32, 128, 128, 4, cutlass::half_t>>, Is_dropout=false, Is_causal=false]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(58): here
            instantiation of "void run_mha_fwd_hdim32<T>(Flash_fwd_params &, cudaStream_t) [with T=cutlass::half_t]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_hdim32_fp16_sm80.cu(22): here

/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_kernel.h(276): warning: variable "tQrQ" was declared but never referenced
          detected during:
            instantiation of "void flash::compute_attn_1rowblock<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params,Is_Q_split_K>(const Params &, int, int, int) [with Kernel_traits=Flash_fwd_kernel_traits<32, 128, 128, 4, false, false, cutlass::half_t, Flash_kernel_traits<32, 128, 128, 4, cutlass::half_t>>, Is_dropout=false, Is_causal=false, Is_even_N=false, Is_even_K=true, Return_softmax=false, Params=Flash_fwd_params, Is_Q_split_K=false]" 
(589): here
            instantiation of "void flash::compute_attn<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &) [with Kernel_traits=Flash_fwd_kernel_traits<32, 128, 128, 4, false, false, cutlass::half_t, Flash_kernel_traits<32, 128, 128, 4, cutlass::half_t>>, Is_dropout=false, Is_causal=false, Is_even_N=false, Is_even_K=true, Return_softmax=false, Params=Flash_fwd_params]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(15): here
            instantiation of "void flash_fwd_kernel<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax>(Flash_fwd_params) [with Kernel_traits=Flash_fwd_kernel_traits<32, 128, 128, 4, false, false, cutlass::half_t, Flash_kernel_traits<32, 128, 128, 4, cutlass::half_t>>, Is_dropout=false, Is_causal=false, Is_even_N=false, Is_even_K=true, Return_softmax=false]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(34): here
            instantiation of "void run_flash_fwd<Kernel_traits,Is_dropout,Is_causal>(Flash_fwd_params &, cudaStream_t) [with Kernel_traits=Flash_fwd_kernel_traits<32, 128, 128, 4, false, false, cutlass::half_t, Flash_kernel_traits<32, 128, 128, 4, cutlass::half_t>>, Is_dropout=false, Is_causal=false]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(58): here
            instantiation of "void run_mha_fwd_hdim32<T>(Flash_fwd_params &, cudaStream_t) [with T=cutlass::half_t]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_hdim32_fp16_sm80.cu(22): here

/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_kernel.h(140): warning: variable "MMA_M" was declared but never referenced
          detected during:
            instantiation of "void flash::compute_attn_1rowblock<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params,Is_Q_split_K>(const Params &, int, int, int) [with Kernel_traits=Flash_fwd_kernel_traits<32, 128, 128, 4, false, false, cutlass::half_t, Flash_kernel_traits<32, 128, 128, 4, cutlass::half_t>>, Is_dropout=false, Is_causal=false, Is_even_N=false, Is_even_K=false, Return_softmax=false, Params=Flash_fwd_params, Is_Q_split_K=false]" 
(589): here
            instantiation of "void flash::compute_attn<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &) [with Kernel_traits=Flash_fwd_kernel_traits<32, 128, 128, 4, false, false, cutlass::half_t, Flash_kernel_traits<32, 128, 128, 4, cutlass::half_t>>, Is_dropout=false, Is_causal=false, Is_even_N=false, Is_even_K=false, Return_softmax=false, Params=Flash_fwd_params]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(15): here
            instantiation of "void flash_fwd_kernel<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax>(Flash_fwd_params) [with Kernel_traits=Flash_fwd_kernel_traits<32, 128, 128, 4, false, false, cutlass::half_t, Flash_kernel_traits<32, 128, 128, 4, cutlass::half_t>>, Is_dropout=false, Is_causal=false, Is_even_N=false, Is_even_K=false, Return_softmax=false]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(34): here
            instantiation of "void run_flash_fwd<Kernel_traits,Is_dropout,Is_causal>(Flash_fwd_params &, cudaStream_t) [with Kernel_traits=Flash_fwd_kernel_traits<32, 128, 128, 4, false, false, cutlass::half_t, Flash_kernel_traits<32, 128, 128, 4, cutlass::half_t>>, Is_dropout=false, Is_causal=false]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(58): here
            instantiation of "void run_mha_fwd_hdim32<T>(Flash_fwd_params &, cudaStream_t) [with T=cutlass::half_t]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_hdim32_fp16_sm80.cu(22): here

/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_kernel.h(276): warning: variable "tQrQ" was declared but never referenced
          detected during:
            instantiation of "void flash::compute_attn_1rowblock<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params,Is_Q_split_K>(const Params &, int, int, int) [with Kernel_traits=Flash_fwd_kernel_traits<32, 128, 128, 4, false, false, cutlass::half_t, Flash_kernel_traits<32, 128, 128, 4, cutlass::half_t>>, Is_dropout=false, Is_causal=false, Is_even_N=false, Is_even_K=false, Return_softmax=false, Params=Flash_fwd_params, Is_Q_split_K=false]" 
(589): here
            instantiation of "void flash::compute_attn<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &) [with Kernel_traits=Flash_fwd_kernel_traits<32, 128, 128, 4, false, false, cutlass::half_t, Flash_kernel_traits<32, 128, 128, 4, cutlass::half_t>>, Is_dropout=false, Is_causal=false, Is_even_N=false, Is_even_K=false, Return_softmax=false, Params=Flash_fwd_params]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(15): here
            instantiation of "void flash_fwd_kernel<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax>(Flash_fwd_params) [with Kernel_traits=Flash_fwd_kernel_traits<32, 128, 128, 4, false, false, cutlass::half_t, Flash_kernel_traits<32, 128, 128, 4, cutlass::half_t>>, Is_dropout=false, Is_causal=false, Is_even_N=false, Is_even_K=false, Return_softmax=false]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(34): here
            instantiation of "void run_flash_fwd<Kernel_traits,Is_dropout,Is_causal>(Flash_fwd_params &, cudaStream_t) [with Kernel_traits=Flash_fwd_kernel_traits<32, 128, 128, 4, false, false, cutlass::half_t, Flash_kernel_traits<32, 128, 128, 4, cutlass::half_t>>, Is_dropout=false, Is_causal=false]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(58): here
            instantiation of "void run_mha_fwd_hdim32<T>(Flash_fwd_params &, cudaStream_t) [with T=cutlass::half_t]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_hdim32_fp16_sm80.cu(22): here

ptxas info    : 2 bytes gmem
ptxas info    : Compiling entry function '_Z16flash_fwd_kernelI23Flash_fwd_kernel_traitsILi32ELi128ELi128ELi4ELb0ELb0EN7cutlass6half_tE19Flash_kernel_traitsILi32ELi128ELi128ELi4ES2_EELb0ELb0ELb0ELb0ELb0EEv16Flash_fwd_params' for 'sm_80'
ptxas info    : Function properties for _Z16flash_fwd_kernelI23Flash_fwd_kernel_traitsILi32ELi128ELi128ELi4ELb0ELb0EN7cutlass6half_tE19Flash_kernel_traitsILi32ELi128ELi128ELi4ES2_EELb0ELb0ELb0ELb0ELb0EEv16Flash_fwd_params
    0 bytes stack frame, 0 bytes spill stores, 0 bytes spill loads
ptxas info    : Used 254 registers, 584 bytes cmem[0]
ptxas info    : Compiling entry function '_Z16flash_fwd_kernelI23Flash_fwd_kernel_traitsILi32ELi128ELi128ELi4ELb0ELb0EN7cutlass6half_tE19Flash_kernel_traitsILi32ELi128ELi128ELi4ES2_EELb0ELb0ELb0ELb1ELb0EEv16Flash_fwd_params' for 'sm_80'
ptxas info    : Function properties for _Z16flash_fwd_kernelI23Flash_fwd_kernel_traitsILi32ELi128ELi128ELi4ELb0ELb0EN7cutlass6half_tE19Flash_kernel_traitsILi32ELi128ELi128ELi4ES2_EELb0ELb0ELb0ELb1ELb0EEv16Flash_fwd_params
    0 bytes stack frame, 0 bytes spill stores, 0 bytes spill loads
ptxas info    : Used 254 registers, 584 bytes cmem[0]
ptxas info    : Compiling entry function '_Z16flash_fwd_kernelI23Flash_fwd_kernel_traitsILi32ELi128ELi128ELi4ELb0ELb0EN7cutlass6half_tE19Flash_kernel_traitsILi32ELi128ELi128ELi4ES2_EELb0ELb0ELb1ELb0ELb0EEv16Flash_fwd_params' for 'sm_80'
ptxas info    : Function properties for _Z16flash_fwd_kernelI23Flash_fwd_kernel_traitsILi32ELi128ELi128ELi4ELb0ELb0EN7cutlass6half_tE19Flash_kernel_traitsILi32ELi128ELi128ELi4ES2_EELb0ELb0ELb1ELb0ELb0EEv16Flash_fwd_params
    0 bytes stack frame, 0 bytes spill stores, 0 bytes spill loads
ptxas info    : Used 232 registers, 584 bytes cmem[0]
ptxas info    : Compiling entry function '_Z16flash_fwd_kernelI23Flash_fwd_kernel_traitsILi32ELi128ELi128ELi4ELb0ELb0EN7cutlass6half_tE19Flash_kernel_traitsILi32ELi128ELi128ELi4ES2_EELb0ELb0ELb1ELb1ELb0EEv16Flash_fwd_params' for 'sm_80'
ptxas info    : Function properties for _Z16flash_fwd_kernelI23Flash_fwd_kernel_traitsILi32ELi128ELi128ELi4ELb0ELb0EN7cutlass6half_tE19Flash_kernel_traitsILi32ELi128ELi128ELi4ES2_EELb0ELb0ELb1ELb1ELb0EEv16Flash_fwd_params
    0 bytes stack frame, 0 bytes spill stores, 0 bytes spill loads
ptxas info    : Used 236 registers, 584 bytes cmem[0]
ptxas info    : Compiling entry function '_Z16flash_fwd_kernelI23Flash_fwd_kernel_traitsILi32ELi128ELi128ELi4ELb0ELb0EN7cutlass6half_tE19Flash_kernel_traitsILi32ELi128ELi128ELi4ES2_EELb0ELb1ELb0ELb0ELb0EEv16Flash_fwd_params' for 'sm_80'
ptxas info    : Function properties for _Z16flash_fwd_kernelI23Flash_fwd_kernel_traitsILi32ELi128ELi128ELi4ELb0ELb0EN7cutlass6half_tE19Flash_kernel_traitsILi32ELi128ELi128ELi4ES2_EELb0ELb1ELb0ELb0ELb0EEv16Flash_fwd_params
    0 bytes stack frame, 0 bytes spill stores, 0 bytes spill loads
ptxas info    : Used 233 registers, 584 bytes cmem[0]
ptxas info    : Compiling entry function '_Z16flash_fwd_kernelI23Flash_fwd_kernel_traitsILi32ELi128ELi128ELi4ELb0ELb0EN7cutlass6half_tE19Flash_kernel_traitsILi32ELi128ELi128ELi4ES2_EELb0ELb1ELb0ELb1ELb0EEv16Flash_fwd_params' for 'sm_80'
ptxas info    : Function properties for _Z16flash_fwd_kernelI23Flash_fwd_kernel_traitsILi32ELi128ELi128ELi4ELb0ELb0EN7cutlass6half_tE19Flash_kernel_traitsILi32ELi128ELi128ELi4ES2_EELb0ELb1ELb0ELb1ELb0EEv16Flash_fwd_params
    0 bytes stack frame, 0 bytes spill stores, 0 bytes spill loads
ptxas info    : Used 254 registers, 584 bytes cmem[0]
ptxas info    : Compiling entry function '_Z16flash_fwd_kernelI23Flash_fwd_kernel_traitsILi32ELi128ELi128ELi4ELb0ELb0EN7cutlass6half_tE19Flash_kernel_traitsILi32ELi128ELi128ELi4ES2_EELb0ELb1ELb1ELb0ELb0EEv16Flash_fwd_params' for 'sm_80'
ptxas info    : Function properties for _Z16flash_fwd_kernelI23Flash_fwd_kernel_traitsILi32ELi128ELi128ELi4ELb0ELb0EN7cutlass6half_tE19Flash_kernel_traitsILi32ELi128ELi128ELi4ES2_EELb0ELb1ELb1ELb0ELb0EEv16Flash_fwd_params
    0 bytes stack frame, 0 bytes spill stores, 0 bytes spill loads
ptxas info    : Used 254 registers, 584 bytes cmem[0]
ptxas info    : Compiling entry function '_Z16flash_fwd_kernelI23Flash_fwd_kernel_traitsILi32ELi128ELi128ELi4ELb0ELb0EN7cutlass6half_tE19Flash_kernel_traitsILi32ELi128ELi128ELi4ES2_EELb0ELb1ELb1ELb1ELb0EEv16Flash_fwd_params' for 'sm_80'
ptxas info    : Function properties for _Z16flash_fwd_kernelI23Flash_fwd_kernel_traitsILi32ELi128ELi128ELi4ELb0ELb0EN7cutlass6half_tE19Flash_kernel_traitsILi32ELi128ELi128ELi4ES2_EELb0ELb1ELb1ELb1ELb0EEv16Flash_fwd_params
    0 bytes stack frame, 0 bytes spill stores, 0 bytes spill loads
ptxas info    : Used 255 registers, 584 bytes cmem[0]
ptxas info    : Compiling entry function '_Z16flash_fwd_kernelI23Flash_fwd_kernel_traitsILi32ELi128ELi128ELi4ELb0ELb0EN7cutlass6half_tE19Flash_kernel_traitsILi32ELi128ELi128ELi4ES2_EELb1ELb0ELb0ELb0ELb0EEv16Flash_fwd_params' for 'sm_80'
ptxas info    : Function properties for _Z16flash_fwd_kernelI23Flash_fwd_kernel_traitsILi32ELi128ELi128ELi4ELb0ELb0EN7cutlass6half_tE19Flash_kernel_traitsILi32ELi128ELi128ELi4ES2_EELb1ELb0ELb0ELb0ELb0EEv16Flash_fwd_params
    0 bytes stack frame, 0 bytes spill stores, 0 bytes spill loads
ptxas info    : Used 255 registers, 584 bytes cmem[0]
ptxas info    : Compiling entry function '_Z16flash_fwd_kernelI23Flash_fwd_kernel_traitsILi32ELi128ELi128ELi4ELb0ELb0EN7cutlass6half_tE19Flash_kernel_traitsILi32ELi128ELi128ELi4ES2_EELb1ELb0ELb0ELb0ELb1EEv16Flash_fwd_params' for 'sm_80'
ptxas info    : Function properties for _Z16flash_fwd_kernelI23Flash_fwd_kernel_traitsILi32ELi128ELi128ELi4ELb0ELb0EN7cutlass6half_tE19Flash_kernel_traitsILi32ELi128ELi128ELi4ES2_EELb1ELb0ELb0ELb0ELb1EEv16Flash_fwd_params
    648 bytes stack frame, 404 bytes spill stores, 480 bytes spill loads
ptxas info    : Used 255 registers, 584 bytes cmem[0]
ptxas info    : Compiling entry function '_Z16flash_fwd_kernelI23Flash_fwd_kernel_traitsILi32ELi128ELi128ELi4ELb0ELb0EN7cutlass6half_tE19Flash_kernel_traitsILi32ELi128ELi128ELi4ES2_EELb1ELb0ELb0ELb1ELb0EEv16Flash_fwd_params' for 'sm_80'
ptxas info    : Function properties for _Z16flash_fwd_kernelI23Flash_fwd_kernel_traitsILi32ELi128ELi128ELi4ELb0ELb0EN7cutlass6half_tE19Flash_kernel_traitsILi32ELi128ELi128ELi4ES2_EELb1ELb0ELb0ELb1ELb0EEv16Flash_fwd_params
    24 bytes stack frame, 20 bytes spill stores, 20 bytes spill loads
ptxas info    : Used 255 registers, 584 bytes cmem[0]
ptxas info    : Compiling entry function '_Z16flash_fwd_kernelI23Flash_fwd_kernel_traitsILi32ELi128ELi128ELi4ELb0ELb0EN7cutlass6half_tE19Flash_kernel_traitsILi32ELi128ELi128ELi4ES2_EELb1ELb0ELb0ELb1ELb1EEv16Flash_fwd_params' for 'sm_80'
ptxas info    : Function properties for _Z16flash_fwd_kernelI23Flash_fwd_kernel_traitsILi32ELi128ELi128ELi4ELb0ELb0EN7cutlass6half_tE19Flash_kernel_traitsILi32ELi128ELi128ELi4ES2_EELb1ELb0ELb0ELb1ELb1EEv16Flash_fwd_params
    712 bytes stack frame, 528 bytes spill stores, 586 bytes spill loads
ptxas info    : Used 255 registers, 584 bytes cmem[0]
ptxas info    : Compiling entry function '_Z16flash_fwd_kernelI23Flash_fwd_kernel_traitsILi32ELi128ELi128ELi4ELb0ELb0EN7cutlass6half_tE19Flash_kernel_traitsILi32ELi128ELi128ELi4ES2_EELb1ELb0ELb1ELb0ELb0EEv16Flash_fwd_params' for 'sm_80'
ptxas info    : Function properties for _Z16flash_fwd_kernelI23Flash_fwd_kernel_traitsILi32ELi128ELi128ELi4ELb0ELb0EN7cutlass6half_tE19Flash_kernel_traitsILi32ELi128ELi128ELi4ES2_EELb1ELb0ELb1ELb0ELb0EEv16Flash_fwd_params
    48 bytes stack frame, 44 bytes spill stores, 44 bytes spill loads
ptxas info    : Used 255 registers, 584 bytes cmem[0]
ptxas info    : Compiling entry function '_Z16flash_fwd_kernelI23Flash_fwd_kernel_traitsILi32ELi128ELi128ELi4ELb0ELb0EN7cutlass6half_tE19Flash_kernel_traitsILi32ELi128ELi128ELi4ES2_EELb1ELb0ELb1ELb0ELb1EEv16Flash_fwd_params' for 'sm_80'
ptxas info    : Function properties for _Z16flash_fwd_kernelI23Flash_fwd_kernel_traitsILi32ELi128ELi128ELi4ELb0ELb0EN7cutlass6half_tE19Flash_kernel_traitsILi32ELi128ELi128ELi4ES2_EELb1ELb0ELb1ELb0ELb1EEv16Flash_fwd_params
    648 bytes stack frame, 412 bytes spill stores, 476 bytes spill loads
ptxas info    : Used 255 registers, 584 bytes cmem[0]
ptxas info    : Compiling entry function '_Z16flash_fwd_kernelI23Flash_fwd_kernel_traitsILi32ELi128ELi128ELi4ELb0ELb0EN7cutlass6half_tE19Flash_kernel_traitsILi32ELi128ELi128ELi4ES2_EELb1ELb0ELb1ELb1ELb0EEv16Flash_fwd_params' for 'sm_80'
ptxas info    : Function properties for _Z16flash_fwd_kernelI23Flash_fwd_kernel_traitsILi32ELi128ELi128ELi4ELb0ELb0EN7cutlass6half_tE19Flash_kernel_traitsILi32ELi128ELi128ELi4ES2_EELb1ELb0ELb1ELb1ELb0EEv16Flash_fwd_params
    0 bytes stack frame, 0 bytes spill stores, 0 bytes spill loads
ptxas info    : Used 255 registers, 584 bytes cmem[0]
ptxas info    : Compiling entry function '_Z16flash_fwd_kernelI23Flash_fwd_kernel_traitsILi32ELi128ELi128ELi4ELb0ELb0EN7cutlass6half_tE19Flash_kernel_traitsILi32ELi128ELi128ELi4ES2_EELb1ELb0ELb1ELb1ELb1EEv16Flash_fwd_params' for 'sm_80'
ptxas info    : Function properties for _Z16flash_fwd_kernelI23Flash_fwd_kernel_traitsILi32ELi128ELi128ELi4ELb0ELb0EN7cutlass6half_tE19Flash_kernel_traitsILi32ELi128ELi128ELi4ES2_EELb1ELb0ELb1ELb1ELb1EEv16Flash_fwd_params
    712 bytes stack frame, 488 bytes spill stores, 568 bytes spill loads
ptxas info    : Used 255 registers, 584 bytes cmem[0]
ptxas info    : Compiling entry function '_Z16flash_fwd_kernelI23Flash_fwd_kernel_traitsILi32ELi128ELi128ELi4ELb0ELb0EN7cutlass6half_tE19Flash_kernel_traitsILi32ELi128ELi128ELi4ES2_EELb1ELb1ELb0ELb0ELb0EEv16Flash_fwd_params' for 'sm_80'
ptxas info    : Function properties for _Z16flash_fwd_kernelI23Flash_fwd_kernel_traitsILi32ELi128ELi128ELi4ELb0ELb0EN7cutlass6half_tE19Flash_kernel_traitsILi32ELi128ELi128ELi4ES2_EELb1ELb1ELb0ELb0ELb0EEv16Flash_fwd_params
    0 bytes stack frame, 0 bytes spill stores, 0 bytes spill loads
ptxas info    : Used 252 registers, 584 bytes cmem[0]
ptxas info    : Compiling entry function '_Z16flash_fwd_kernelI23Flash_fwd_kernel_traitsILi32ELi128ELi128ELi4ELb0ELb0EN7cutlass6half_tE19Flash_kernel_traitsILi32ELi128ELi128ELi4ES2_EELb1ELb1ELb0ELb0ELb1EEv16Flash_fwd_params' for 'sm_80'
ptxas info    : Function properties for _Z16flash_fwd_kernelI23Flash_fwd_kernel_traitsILi32ELi128ELi128ELi4ELb0ELb0EN7cutlass6half_tE19Flash_kernel_traitsILi32ELi128ELi128ELi4ES2_EELb1ELb1ELb0ELb0ELb1EEv16Flash_fwd_params
    632 bytes stack frame, 396 bytes spill stores, 474 bytes spill loads
ptxas info    : Used 255 registers, 584 bytes cmem[0]
ptxas info    : Compiling entry function '_Z16flash_fwd_kernelI23Flash_fwd_kernel_traitsILi32ELi128ELi128ELi4ELb0ELb0EN7cutlass6half_tE19Flash_kernel_traitsILi32ELi128ELi128ELi4ES2_EELb1ELb1ELb0ELb1ELb0EEv16Flash_fwd_params' for 'sm_80'
ptxas info    : Function properties for _Z16flash_fwd_kernelI23Flash_fwd_kernel_traitsILi32ELi128ELi128ELi4ELb0ELb0EN7cutlass6half_tE19Flash_kernel_traitsILi32ELi128ELi128ELi4ES2_EELb1ELb1ELb0ELb1ELb0EEv16Flash_fwd_params
    24 bytes stack frame, 20 bytes spill stores, 20 bytes spill loads
ptxas info    : Used 255 registers, 584 bytes cmem[0]
ptxas info    : Compiling entry function '_Z16flash_fwd_kernelI23Flash_fwd_kernel_traitsILi32ELi128ELi128ELi4ELb0ELb0EN7cutlass6half_tE19Flash_kernel_traitsILi32ELi128ELi128ELi4ES2_EELb1ELb1ELb0ELb1ELb1EEv16Flash_fwd_params' for 'sm_80'
ptxas info    : Function properties for _Z16flash_fwd_kernelI23Flash_fwd_kernel_traitsILi32ELi128ELi128ELi4ELb0ELb0EN7cutlass6half_tE19Flash_kernel_traitsILi32ELi128ELi128ELi4ES2_EELb1ELb1ELb0ELb1ELb1EEv16Flash_fwd_params
    712 bytes stack frame, 568 bytes spill stores, 610 bytes spill loads
ptxas info    : Used 255 registers, 584 bytes cmem[0]
ptxas info    : Compiling entry function '_Z16flash_fwd_kernelI23Flash_fwd_kernel_traitsILi32ELi128ELi128ELi4ELb0ELb0EN7cutlass6half_tE19Flash_kernel_traitsILi32ELi128ELi128ELi4ES2_EELb1ELb1ELb1ELb0ELb0EEv16Flash_fwd_params' for 'sm_80'
ptxas info    : Function properties for _Z16flash_fwd_kernelI23Flash_fwd_kernel_traitsILi32ELi128ELi128ELi4ELb0ELb0EN7cutlass6half_tE19Flash_kernel_traitsILi32ELi128ELi128ELi4ES2_EELb1ELb1ELb1ELb0ELb0EEv16Flash_fwd_params
    48 bytes stack frame, 44 bytes spill stores, 44 bytes spill loads
ptxas info    : Used 255 registers, 584 bytes cmem[0]
ptxas info    : Compiling entry function '_Z16flash_fwd_kernelI23Flash_fwd_kernel_traitsILi32ELi128ELi128ELi4ELb0ELb0EN7cutlass6half_tE19Flash_kernel_traitsILi32ELi128ELi128ELi4ES2_EELb1ELb1ELb1ELb0ELb1EEv16Flash_fwd_params' for 'sm_80'
ptxas info    : Function properties for _Z16flash_fwd_kernelI23Flash_fwd_kernel_traitsILi32ELi128ELi128ELi4ELb0ELb0EN7cutlass6half_tE19Flash_kernel_traitsILi32ELi128ELi128ELi4ES2_EELb1ELb1ELb1ELb0ELb1EEv16Flash_fwd_params
    632 bytes stack frame, 428 bytes spill stores, 492 bytes spill loads
ptxas info    : Used 255 registers, 584 bytes cmem[0]
ptxas info    : Compiling entry function '_Z16flash_fwd_kernelI23Flash_fwd_kernel_traitsILi32ELi128ELi128ELi4ELb0ELb0EN7cutlass6half_tE19Flash_kernel_traitsILi32ELi128ELi128ELi4ES2_EELb1ELb1ELb1ELb1ELb0EEv16Flash_fwd_params' for 'sm_80'
ptxas info    : Function properties for _Z16flash_fwd_kernelI23Flash_fwd_kernel_traitsILi32ELi128ELi128ELi4ELb0ELb0EN7cutlass6half_tE19Flash_kernel_traitsILi32ELi128ELi128ELi4ES2_EELb1ELb1ELb1ELb1ELb0EEv16Flash_fwd_params
    8 bytes stack frame, 4 bytes spill stores, 4 bytes spill loads
ptxas info    : Used 255 registers, 584 bytes cmem[0]
ptxas info    : Compiling entry function '_Z16flash_fwd_kernelI23Flash_fwd_kernel_traitsILi32ELi128ELi128ELi4ELb0ELb0EN7cutlass6half_tE19Flash_kernel_traitsILi32ELi128ELi128ELi4ES2_EELb1ELb1ELb1ELb1ELb1EEv16Flash_fwd_params' for 'sm_80'
ptxas info    : Function properties for _Z16flash_fwd_kernelI23Flash_fwd_kernel_traitsILi32ELi128ELi128ELi4ELb0ELb0EN7cutlass6half_tE19Flash_kernel_traitsILi32ELi128ELi128ELi4ES2_EELb1ELb1ELb1ELb1ELb1EEv16Flash_fwd_params
    720 bytes stack frame, 564 bytes spill stores, 630 bytes spill loads
ptxas info    : Used 255 registers, 584 bytes cmem[0]
/anaconda/envs/flashattention/lib/python3.9/site-packages/torch/include/c10/core/SymInt.h(84): warning: integer conversion resulted in a change of sign

[26/37] /usr/local/cuda-11.4/bin/nvcc  -I/home/v-feiychen/flash-attention/csrc/flash_attn -I/home/v-feiychen/flash-attention/csrc/flash_attn/src -I/home/v-feiychen/flash-attention/csrc/cutlass/include -I/anaconda/envs/flashattention/lib/python3.9/site-packages/torch/include -I/anaconda/envs/flashattention/lib/python3.9/site-packages/torch/include/torch/csrc/api/include -I/anaconda/envs/flashattention/lib/python3.9/site-packages/torch/include/TH -I/anaconda/envs/flashattention/lib/python3.9/site-packages/torch/include/THC -I/usr/local/cuda-11.4/include -I/anaconda/envs/flashattention/include/python3.9 -c -c /home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_hdim160_fp16_sm80.cu -o /home/v-feiychen/flash-attention/build/temp.linux-x86_64-cpython-39/csrc/flash_attn/src/flash_fwd_hdim160_fp16_sm80.o -D__CUDA_NO_HALF_OPERATORS__ -D__CUDA_NO_HALF_CONVERSIONS__ -D__CUDA_NO_BFLOAT16_CONVERSIONS__ -D__CUDA_NO_HALF2_OPERATORS__ --expt-relaxed-constexpr --compiler-options ''"'"'-fPIC'"'"'' -O3 -std=c++17 -U__CUDA_NO_HALF_OPERATORS__ -U__CUDA_NO_HALF_CONVERSIONS__ -U__CUDA_NO_HALF2_OPERATORS__ -U__CUDA_NO_BFLOAT16_CONVERSIONS__ --expt-relaxed-constexpr --expt-extended-lambda --use_fast_math --ptxas-options=-v -lineinfo -gencode arch=compute_80,code=sm_80 --threads 4 -DTORCH_API_INCLUDE_EXTENSION_H '-DPYBIND11_COMPILER_TYPE="_gcc"' '-DPYBIND11_STDLIB="_libstdcpp"' '-DPYBIND11_BUILD_ABI="_cxxabi1011"' -DTORCH_EXTENSION_NAME=flash_attn_2_cuda -D_GLIBCXX_USE_CXX11_ABI=0
/anaconda/envs/flashattention/lib/python3.9/site-packages/torch/include/c10/core/SymInt.h(84): warning: integer conversion resulted in a change of sign

/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_kernel.h(140): warning: variable "MMA_M" was declared but never referenced
          detected during:
            instantiation of "void flash::compute_attn_1rowblock<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params,Is_Q_split_K>(const Params &, int, int, int) [with Kernel_traits=Flash_fwd_kernel_traits<160, 64, 64, 4, false, false, cutlass::half_t, Flash_kernel_traits<160, 64, 64, 4, cutlass::half_t>>, Is_dropout=true, Is_causal=true, Is_even_N=true, Is_even_K=true, Return_softmax=true, Params=Flash_fwd_params, Is_Q_split_K=false]" 
(589): here
            instantiation of "void flash::compute_attn<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &) [with Kernel_traits=Flash_fwd_kernel_traits<160, 64, 64, 4, false, false, cutlass::half_t, Flash_kernel_traits<160, 64, 64, 4, cutlass::half_t>>, Is_dropout=true, Is_causal=true, Is_even_N=true, Is_even_K=true, Return_softmax=true, Params=Flash_fwd_params]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(15): here
            instantiation of "void flash_fwd_kernel<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax>(Flash_fwd_params) [with Kernel_traits=Flash_fwd_kernel_traits<160, 64, 64, 4, false, false, cutlass::half_t, Flash_kernel_traits<160, 64, 64, 4, cutlass::half_t>>, Is_dropout=true, Is_causal=true, Is_even_N=true, Is_even_K=true, Return_softmax=true]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(34): here
            instantiation of "void run_flash_fwd<Kernel_traits,Is_dropout,Is_causal>(Flash_fwd_params &, cudaStream_t) [with Kernel_traits=Flash_fwd_kernel_traits<160, 64, 64, 4, false, false, cutlass::half_t, Flash_kernel_traits<160, 64, 64, 4, cutlass::half_t>>, Is_dropout=true, Is_causal=true]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(155): here
            instantiation of "void run_mha_fwd_hdim160<T>(Flash_fwd_params &, cudaStream_t) [with T=cutlass::half_t]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_hdim160_fp16_sm80.cu(26): here

/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_kernel.h(276): warning: variable "tQrQ" was declared but never referenced
          detected during:
            instantiation of "void flash::compute_attn_1rowblock<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params,Is_Q_split_K>(const Params &, int, int, int) [with Kernel_traits=Flash_fwd_kernel_traits<160, 64, 64, 4, false, false, cutlass::half_t, Flash_kernel_traits<160, 64, 64, 4, cutlass::half_t>>, Is_dropout=true, Is_causal=true, Is_even_N=true, Is_even_K=true, Return_softmax=true, Params=Flash_fwd_params, Is_Q_split_K=false]" 
(589): here
            instantiation of "void flash::compute_attn<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &) [with Kernel_traits=Flash_fwd_kernel_traits<160, 64, 64, 4, false, false, cutlass::half_t, Flash_kernel_traits<160, 64, 64, 4, cutlass::half_t>>, Is_dropout=true, Is_causal=true, Is_even_N=true, Is_even_K=true, Return_softmax=true, Params=Flash_fwd_params]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(15): here
            instantiation of "void flash_fwd_kernel<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax>(Flash_fwd_params) [with Kernel_traits=Flash_fwd_kernel_traits<160, 64, 64, 4, false, false, cutlass::half_t, Flash_kernel_traits<160, 64, 64, 4, cutlass::half_t>>, Is_dropout=true, Is_causal=true, Is_even_N=true, Is_even_K=true, Return_softmax=true]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(34): here
            instantiation of "void run_flash_fwd<Kernel_traits,Is_dropout,Is_causal>(Flash_fwd_params &, cudaStream_t) [with Kernel_traits=Flash_fwd_kernel_traits<160, 64, 64, 4, false, false, cutlass::half_t, Flash_kernel_traits<160, 64, 64, 4, cutlass::half_t>>, Is_dropout=true, Is_causal=true]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(155): here
            instantiation of "void run_mha_fwd_hdim160<T>(Flash_fwd_params &, cudaStream_t) [with T=cutlass::half_t]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_hdim160_fp16_sm80.cu(26): here

/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_kernel.h(140): warning: variable "MMA_M" was declared but never referenced
          detected during:
            instantiation of "void flash::compute_attn_1rowblock<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params,Is_Q_split_K>(const Params &, int, int, int) [with Kernel_traits=Flash_fwd_kernel_traits<160, 64, 64, 4, false, false, cutlass::half_t, Flash_kernel_traits<160, 64, 64, 4, cutlass::half_t>>, Is_dropout=true, Is_causal=true, Is_even_N=true, Is_even_K=true, Return_softmax=false, Params=Flash_fwd_params, Is_Q_split_K=false]" 
(589): here
            instantiation of "void flash::compute_attn<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &) [with Kernel_traits=Flash_fwd_kernel_traits<160, 64, 64, 4, false, false, cutlass::half_t, Flash_kernel_traits<160, 64, 64, 4, cutlass::half_t>>, Is_dropout=true, Is_causal=true, Is_even_N=true, Is_even_K=true, Return_softmax=false, Params=Flash_fwd_params]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(15): here
            instantiation of "void flash_fwd_kernel<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax>(Flash_fwd_params) [with Kernel_traits=Flash_fwd_kernel_traits<160, 64, 64, 4, false, false, cutlass::half_t, Flash_kernel_traits<160, 64, 64, 4, cutlass::half_t>>, Is_dropout=true, Is_causal=true, Is_even_N=true, Is_even_K=true, Return_softmax=false]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(34): here
            instantiation of "void run_flash_fwd<Kernel_traits,Is_dropout,Is_causal>(Flash_fwd_params &, cudaStream_t) [with Kernel_traits=Flash_fwd_kernel_traits<160, 64, 64, 4, false, false, cutlass::half_t, Flash_kernel_traits<160, 64, 64, 4, cutlass::half_t>>, Is_dropout=true, Is_causal=true]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(155): here
            instantiation of "void run_mha_fwd_hdim160<T>(Flash_fwd_params &, cudaStream_t) [with T=cutlass::half_t]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_hdim160_fp16_sm80.cu(26): here

/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_kernel.h(276): warning: variable "tQrQ" was declared but never referenced
          detected during:
            instantiation of "void flash::compute_attn_1rowblock<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params,Is_Q_split_K>(const Params &, int, int, int) [with Kernel_traits=Flash_fwd_kernel_traits<160, 64, 64, 4, false, false, cutlass::half_t, Flash_kernel_traits<160, 64, 64, 4, cutlass::half_t>>, Is_dropout=true, Is_causal=true, Is_even_N=true, Is_even_K=true, Return_softmax=false, Params=Flash_fwd_params, Is_Q_split_K=false]" 
(589): here
            instantiation of "void flash::compute_attn<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &) [with Kernel_traits=Flash_fwd_kernel_traits<160, 64, 64, 4, false, false, cutlass::half_t, Flash_kernel_traits<160, 64, 64, 4, cutlass::half_t>>, Is_dropout=true, Is_causal=true, Is_even_N=true, Is_even_K=true, Return_softmax=false, Params=Flash_fwd_params]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(15): here
            instantiation of "void flash_fwd_kernel<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax>(Flash_fwd_params) [with Kernel_traits=Flash_fwd_kernel_traits<160, 64, 64, 4, false, false, cutlass::half_t, Flash_kernel_traits<160, 64, 64, 4, cutlass::half_t>>, Is_dropout=true, Is_causal=true, Is_even_N=true, Is_even_K=true, Return_softmax=false]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(34): here
            instantiation of "void run_flash_fwd<Kernel_traits,Is_dropout,Is_causal>(Flash_fwd_params &, cudaStream_t) [with Kernel_traits=Flash_fwd_kernel_traits<160, 64, 64, 4, false, false, cutlass::half_t, Flash_kernel_traits<160, 64, 64, 4, cutlass::half_t>>, Is_dropout=true, Is_causal=true]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(155): here
            instantiation of "void run_mha_fwd_hdim160<T>(Flash_fwd_params &, cudaStream_t) [with T=cutlass::half_t]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_hdim160_fp16_sm80.cu(26): here

/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_kernel.h(140): warning: variable "MMA_M" was declared but never referenced
          detected during:
            instantiation of "void flash::compute_attn_1rowblock<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params,Is_Q_split_K>(const Params &, int, int, int) [with Kernel_traits=Flash_fwd_kernel_traits<160, 64, 64, 4, false, false, cutlass::half_t, Flash_kernel_traits<160, 64, 64, 4, cutlass::half_t>>, Is_dropout=true, Is_causal=true, Is_even_N=true, Is_even_K=false, Return_softmax=true, Params=Flash_fwd_params, Is_Q_split_K=false]" 
(589): here
            instantiation of "void flash::compute_attn<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &) [with Kernel_traits=Flash_fwd_kernel_traits<160, 64, 64, 4, false, false, cutlass::half_t, Flash_kernel_traits<160, 64, 64, 4, cutlass::half_t>>, Is_dropout=true, Is_causal=true, Is_even_N=true, Is_even_K=false, Return_softmax=true, Params=Flash_fwd_params]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(15): here
            instantiation of "void flash_fwd_kernel<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax>(Flash_fwd_params) [with Kernel_traits=Flash_fwd_kernel_traits<160, 64, 64, 4, false, false, cutlass::half_t, Flash_kernel_traits<160, 64, 64, 4, cutlass::half_t>>, Is_dropout=true, Is_causal=true, Is_even_N=true, Is_even_K=false, Return_softmax=true]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(34): here
            instantiation of "void run_flash_fwd<Kernel_traits,Is_dropout,Is_causal>(Flash_fwd_params &, cudaStream_t) [with Kernel_traits=Flash_fwd_kernel_traits<160, 64, 64, 4, false, false, cutlass::half_t, Flash_kernel_traits<160, 64, 64, 4, cutlass::half_t>>, Is_dropout=true, Is_causal=true]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(155): here
            instantiation of "void run_mha_fwd_hdim160<T>(Flash_fwd_params &, cudaStream_t) [with T=cutlass::half_t]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_hdim160_fp16_sm80.cu(26): here

/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_kernel.h(276): warning: variable "tQrQ" was declared but never referenced
          detected during:
            instantiation of "void flash::compute_attn_1rowblock<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params,Is_Q_split_K>(const Params &, int, int, int) [with Kernel_traits=Flash_fwd_kernel_traits<160, 64, 64, 4, false, false, cutlass::half_t, Flash_kernel_traits<160, 64, 64, 4, cutlass::half_t>>, Is_dropout=true, Is_causal=true, Is_even_N=true, Is_even_K=false, Return_softmax=true, Params=Flash_fwd_params, Is_Q_split_K=false]" 
(589): here
            instantiation of "void flash::compute_attn<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &) [with Kernel_traits=Flash_fwd_kernel_traits<160, 64, 64, 4, false, false, cutlass::half_t, Flash_kernel_traits<160, 64, 64, 4, cutlass::half_t>>, Is_dropout=true, Is_causal=true, Is_even_N=true, Is_even_K=false, Return_softmax=true, Params=Flash_fwd_params]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(15): here
            instantiation of "void flash_fwd_kernel<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax>(Flash_fwd_params) [with Kernel_traits=Flash_fwd_kernel_traits<160, 64, 64, 4, false, false, cutlass::half_t, Flash_kernel_traits<160, 64, 64, 4, cutlass::half_t>>, Is_dropout=true, Is_causal=true, Is_even_N=true, Is_even_K=false, Return_softmax=true]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(34): here
            instantiation of "void run_flash_fwd<Kernel_traits,Is_dropout,Is_causal>(Flash_fwd_params &, cudaStream_t) [with Kernel_traits=Flash_fwd_kernel_traits<160, 64, 64, 4, false, false, cutlass::half_t, Flash_kernel_traits<160, 64, 64, 4, cutlass::half_t>>, Is_dropout=true, Is_causal=true]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(155): here
            instantiation of "void run_mha_fwd_hdim160<T>(Flash_fwd_params &, cudaStream_t) [with T=cutlass::half_t]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_hdim160_fp16_sm80.cu(26): here

/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_kernel.h(140): warning: variable "MMA_M" was declared but never referenced
          detected during:
            instantiation of "void flash::compute_attn_1rowblock<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params,Is_Q_split_K>(const Params &, int, int, int) [with Kernel_traits=Flash_fwd_kernel_traits<160, 64, 64, 4, false, false, cutlass::half_t, Flash_kernel_traits<160, 64, 64, 4, cutlass::half_t>>, Is_dropout=true, Is_causal=true, Is_even_N=true, Is_even_K=false, Return_softmax=false, Params=Flash_fwd_params, Is_Q_split_K=false]" 
(589): here
            instantiation of "void flash::compute_attn<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &) [with Kernel_traits=Flash_fwd_kernel_traits<160, 64, 64, 4, false, false, cutlass::half_t, Flash_kernel_traits<160, 64, 64, 4, cutlass::half_t>>, Is_dropout=true, Is_causal=true, Is_even_N=true, Is_even_K=false, Return_softmax=false, Params=Flash_fwd_params]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(15): here
            instantiation of "void flash_fwd_kernel<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax>(Flash_fwd_params) [with Kernel_traits=Flash_fwd_kernel_traits<160, 64, 64, 4, false, false, cutlass::half_t, Flash_kernel_traits<160, 64, 64, 4, cutlass::half_t>>, Is_dropout=true, Is_causal=true, Is_even_N=true, Is_even_K=false, Return_softmax=false]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(34): here
            instantiation of "void run_flash_fwd<Kernel_traits,Is_dropout,Is_causal>(Flash_fwd_params &, cudaStream_t) [with Kernel_traits=Flash_fwd_kernel_traits<160, 64, 64, 4, false, false, cutlass::half_t, Flash_kernel_traits<160, 64, 64, 4, cutlass::half_t>>, Is_dropout=true, Is_causal=true]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(155): here
            instantiation of "void run_mha_fwd_hdim160<T>(Flash_fwd_params &, cudaStream_t) [with T=cutlass::half_t]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_hdim160_fp16_sm80.cu(26): here

/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_kernel.h(276): warning: variable "tQrQ" was declared but never referenced
          detected during:
            instantiation of "void flash::compute_attn_1rowblock<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params,Is_Q_split_K>(const Params &, int, int, int) [with Kernel_traits=Flash_fwd_kernel_traits<160, 64, 64, 4, false, false, cutlass::half_t, Flash_kernel_traits<160, 64, 64, 4, cutlass::half_t>>, Is_dropout=true, Is_causal=true, Is_even_N=true, Is_even_K=false, Return_softmax=false, Params=Flash_fwd_params, Is_Q_split_K=false]" 
(589): here
            instantiation of "void flash::compute_attn<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &) [with Kernel_traits=Flash_fwd_kernel_traits<160, 64, 64, 4, false, false, cutlass::half_t, Flash_kernel_traits<160, 64, 64, 4, cutlass::half_t>>, Is_dropout=true, Is_causal=true, Is_even_N=true, Is_even_K=false, Return_softmax=false, Params=Flash_fwd_params]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(15): here
            instantiation of "void flash_fwd_kernel<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax>(Flash_fwd_params) [with Kernel_traits=Flash_fwd_kernel_traits<160, 64, 64, 4, false, false, cutlass::half_t, Flash_kernel_traits<160, 64, 64, 4, cutlass::half_t>>, Is_dropout=true, Is_causal=true, Is_even_N=true, Is_even_K=false, Return_softmax=false]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(34): here
            instantiation of "void run_flash_fwd<Kernel_traits,Is_dropout,Is_causal>(Flash_fwd_params &, cudaStream_t) [with Kernel_traits=Flash_fwd_kernel_traits<160, 64, 64, 4, false, false, cutlass::half_t, Flash_kernel_traits<160, 64, 64, 4, cutlass::half_t>>, Is_dropout=true, Is_causal=true]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(155): here
            instantiation of "void run_mha_fwd_hdim160<T>(Flash_fwd_params &, cudaStream_t) [with T=cutlass::half_t]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_hdim160_fp16_sm80.cu(26): here

/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_kernel.h(140): warning: variable "MMA_M" was declared but never referenced
          detected during:
            instantiation of "void flash::compute_attn_1rowblock<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params,Is_Q_split_K>(const Params &, int, int, int) [with Kernel_traits=Flash_fwd_kernel_traits<160, 64, 64, 4, false, false, cutlass::half_t, Flash_kernel_traits<160, 64, 64, 4, cutlass::half_t>>, Is_dropout=true, Is_causal=true, Is_even_N=false, Is_even_K=true, Return_softmax=true, Params=Flash_fwd_params, Is_Q_split_K=false]" 
(589): here
            instantiation of "void flash::compute_attn<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &) [with Kernel_traits=Flash_fwd_kernel_traits<160, 64, 64, 4, false, false, cutlass::half_t, Flash_kernel_traits<160, 64, 64, 4, cutlass::half_t>>, Is_dropout=true, Is_causal=true, Is_even_N=false, Is_even_K=true, Return_softmax=true, Params=Flash_fwd_params]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(15): here
            instantiation of "void flash_fwd_kernel<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax>(Flash_fwd_params) [with Kernel_traits=Flash_fwd_kernel_traits<160, 64, 64, 4, false, false, cutlass::half_t, Flash_kernel_traits<160, 64, 64, 4, cutlass::half_t>>, Is_dropout=true, Is_causal=true, Is_even_N=false, Is_even_K=true, Return_softmax=true]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(34): here
            instantiation of "void run_flash_fwd<Kernel_traits,Is_dropout,Is_causal>(Flash_fwd_params &, cudaStream_t) [with Kernel_traits=Flash_fwd_kernel_traits<160, 64, 64, 4, false, false, cutlass::half_t, Flash_kernel_traits<160, 64, 64, 4, cutlass::half_t>>, Is_dropout=true, Is_causal=true]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(155): here
            instantiation of "void run_mha_fwd_hdim160<T>(Flash_fwd_params &, cudaStream_t) [with T=cutlass::half_t]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_hdim160_fp16_sm80.cu(26): here

/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_kernel.h(276): warning: variable "tQrQ" was declared but never referenced
          detected during:
            instantiation of "void flash::compute_attn_1rowblock<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params,Is_Q_split_K>(const Params &, int, int, int) [with Kernel_traits=Flash_fwd_kernel_traits<160, 64, 64, 4, false, false, cutlass::half_t, Flash_kernel_traits<160, 64, 64, 4, cutlass::half_t>>, Is_dropout=true, Is_causal=true, Is_even_N=false, Is_even_K=true, Return_softmax=true, Params=Flash_fwd_params, Is_Q_split_K=false]" 
(589): here
            instantiation of "void flash::compute_attn<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &) [with Kernel_traits=Flash_fwd_kernel_traits<160, 64, 64, 4, false, false, cutlass::half_t, Flash_kernel_traits<160, 64, 64, 4, cutlass::half_t>>, Is_dropout=true, Is_causal=true, Is_even_N=false, Is_even_K=true, Return_softmax=true, Params=Flash_fwd_params]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(15): here
            instantiation of "void flash_fwd_kernel<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax>(Flash_fwd_params) [with Kernel_traits=Flash_fwd_kernel_traits<160, 64, 64, 4, false, false, cutlass::half_t, Flash_kernel_traits<160, 64, 64, 4, cutlass::half_t>>, Is_dropout=true, Is_causal=true, Is_even_N=false, Is_even_K=true, Return_softmax=true]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(34): here
            instantiation of "void run_flash_fwd<Kernel_traits,Is_dropout,Is_causal>(Flash_fwd_params &, cudaStream_t) [with Kernel_traits=Flash_fwd_kernel_traits<160, 64, 64, 4, false, false, cutlass::half_t, Flash_kernel_traits<160, 64, 64, 4, cutlass::half_t>>, Is_dropout=true, Is_causal=true]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(155): here
            instantiation of "void run_mha_fwd_hdim160<T>(Flash_fwd_params &, cudaStream_t) [with T=cutlass::half_t]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_hdim160_fp16_sm80.cu(26): here

/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_kernel.h(140): warning: variable "MMA_M" was declared but never referenced
          detected during:
            instantiation of "void flash::compute_attn_1rowblock<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params,Is_Q_split_K>(const Params &, int, int, int) [with Kernel_traits=Flash_fwd_kernel_traits<160, 64, 64, 4, false, false, cutlass::half_t, Flash_kernel_traits<160, 64, 64, 4, cutlass::half_t>>, Is_dropout=true, Is_causal=true, Is_even_N=false, Is_even_K=true, Return_softmax=false, Params=Flash_fwd_params, Is_Q_split_K=false]" 
(589): here
            instantiation of "void flash::compute_attn<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &) [with Kernel_traits=Flash_fwd_kernel_traits<160, 64, 64, 4, false, false, cutlass::half_t, Flash_kernel_traits<160, 64, 64, 4, cutlass::half_t>>, Is_dropout=true, Is_causal=true, Is_even_N=false, Is_even_K=true, Return_softmax=false, Params=Flash_fwd_params]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(15): here
            instantiation of "void flash_fwd_kernel<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax>(Flash_fwd_params) [with Kernel_traits=Flash_fwd_kernel_traits<160, 64, 64, 4, false, false, cutlass::half_t, Flash_kernel_traits<160, 64, 64, 4, cutlass::half_t>>, Is_dropout=true, Is_causal=true, Is_even_N=false, Is_even_K=true, Return_softmax=false]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(34): here
            instantiation of "void run_flash_fwd<Kernel_traits,Is_dropout,Is_causal>(Flash_fwd_params &, cudaStream_t) [with Kernel_traits=Flash_fwd_kernel_traits<160, 64, 64, 4, false, false, cutlass::half_t, Flash_kernel_traits<160, 64, 64, 4, cutlass::half_t>>, Is_dropout=true, Is_causal=true]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(155): here
            instantiation of "void run_mha_fwd_hdim160<T>(Flash_fwd_params &, cudaStream_t) [with T=cutlass::half_t]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_hdim160_fp16_sm80.cu(26): here

/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_kernel.h(276): warning: variable "tQrQ" was declared but never referenced
          detected during:
            instantiation of "void flash::compute_attn_1rowblock<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params,Is_Q_split_K>(const Params &, int, int, int) [with Kernel_traits=Flash_fwd_kernel_traits<160, 64, 64, 4, false, false, cutlass::half_t, Flash_kernel_traits<160, 64, 64, 4, cutlass::half_t>>, Is_dropout=true, Is_causal=true, Is_even_N=false, Is_even_K=true, Return_softmax=false, Params=Flash_fwd_params, Is_Q_split_K=false]" 
(589): here
            instantiation of "void flash::compute_attn<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &) [with Kernel_traits=Flash_fwd_kernel_traits<160, 64, 64, 4, false, false, cutlass::half_t, Flash_kernel_traits<160, 64, 64, 4, cutlass::half_t>>, Is_dropout=true, Is_causal=true, Is_even_N=false, Is_even_K=true, Return_softmax=false, Params=Flash_fwd_params]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(15): here
            instantiation of "void flash_fwd_kernel<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax>(Flash_fwd_params) [with Kernel_traits=Flash_fwd_kernel_traits<160, 64, 64, 4, false, false, cutlass::half_t, Flash_kernel_traits<160, 64, 64, 4, cutlass::half_t>>, Is_dropout=true, Is_causal=true, Is_even_N=false, Is_even_K=true, Return_softmax=false]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(34): here
            instantiation of "void run_flash_fwd<Kernel_traits,Is_dropout,Is_causal>(Flash_fwd_params &, cudaStream_t) [with Kernel_traits=Flash_fwd_kernel_traits<160, 64, 64, 4, false, false, cutlass::half_t, Flash_kernel_traits<160, 64, 64, 4, cutlass::half_t>>, Is_dropout=true, Is_causal=true]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(155): here
            instantiation of "void run_mha_fwd_hdim160<T>(Flash_fwd_params &, cudaStream_t) [with T=cutlass::half_t]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_hdim160_fp16_sm80.cu(26): here

/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_kernel.h(140): warning: variable "MMA_M" was declared but never referenced
          detected during:
            instantiation of "void flash::compute_attn_1rowblock<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params,Is_Q_split_K>(const Params &, int, int, int) [with Kernel_traits=Flash_fwd_kernel_traits<160, 64, 64, 4, false, false, cutlass::half_t, Flash_kernel_traits<160, 64, 64, 4, cutlass::half_t>>, Is_dropout=true, Is_causal=true, Is_even_N=false, Is_even_K=false, Return_softmax=true, Params=Flash_fwd_params, Is_Q_split_K=false]" 
(589): here
            instantiation of "void flash::compute_attn<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &) [with Kernel_traits=Flash_fwd_kernel_traits<160, 64, 64, 4, false, false, cutlass::half_t, Flash_kernel_traits<160, 64, 64, 4, cutlass::half_t>>, Is_dropout=true, Is_causal=true, Is_even_N=false, Is_even_K=false, Return_softmax=true, Params=Flash_fwd_params]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(15): here
            instantiation of "void flash_fwd_kernel<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax>(Flash_fwd_params) [with Kernel_traits=Flash_fwd_kernel_traits<160, 64, 64, 4, false, false, cutlass::half_t, Flash_kernel_traits<160, 64, 64, 4, cutlass::half_t>>, Is_dropout=true, Is_causal=true, Is_even_N=false, Is_even_K=false, Return_softmax=true]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(34): here
            instantiation of "void run_flash_fwd<Kernel_traits,Is_dropout,Is_causal>(Flash_fwd_params &, cudaStream_t) [with Kernel_traits=Flash_fwd_kernel_traits<160, 64, 64, 4, false, false, cutlass::half_t, Flash_kernel_traits<160, 64, 64, 4, cutlass::half_t>>, Is_dropout=true, Is_causal=true]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(155): here
            instantiation of "void run_mha_fwd_hdim160<T>(Flash_fwd_params &, cudaStream_t) [with T=cutlass::half_t]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_hdim160_fp16_sm80.cu(26): here

/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_kernel.h(276): warning: variable "tQrQ" was declared but never referenced
          detected during:
            instantiation of "void flash::compute_attn_1rowblock<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params,Is_Q_split_K>(const Params &, int, int, int) [with Kernel_traits=Flash_fwd_kernel_traits<160, 64, 64, 4, false, false, cutlass::half_t, Flash_kernel_traits<160, 64, 64, 4, cutlass::half_t>>, Is_dropout=true, Is_causal=true, Is_even_N=false, Is_even_K=false, Return_softmax=true, Params=Flash_fwd_params, Is_Q_split_K=false]" 
(589): here
            instantiation of "void flash::compute_attn<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &) [with Kernel_traits=Flash_fwd_kernel_traits<160, 64, 64, 4, false, false, cutlass::half_t, Flash_kernel_traits<160, 64, 64, 4, cutlass::half_t>>, Is_dropout=true, Is_causal=true, Is_even_N=false, Is_even_K=false, Return_softmax=true, Params=Flash_fwd_params]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(15): here
            instantiation of "void flash_fwd_kernel<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax>(Flash_fwd_params) [with Kernel_traits=Flash_fwd_kernel_traits<160, 64, 64, 4, false, false, cutlass::half_t, Flash_kernel_traits<160, 64, 64, 4, cutlass::half_t>>, Is_dropout=true, Is_causal=true, Is_even_N=false, Is_even_K=false, Return_softmax=true]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(34): here
            instantiation of "void run_flash_fwd<Kernel_traits,Is_dropout,Is_causal>(Flash_fwd_params &, cudaStream_t) [with Kernel_traits=Flash_fwd_kernel_traits<160, 64, 64, 4, false, false, cutlass::half_t, Flash_kernel_traits<160, 64, 64, 4, cutlass::half_t>>, Is_dropout=true, Is_causal=true]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(155): here
            instantiation of "void run_mha_fwd_hdim160<T>(Flash_fwd_params &, cudaStream_t) [with T=cutlass::half_t]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_hdim160_fp16_sm80.cu(26): here

/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_kernel.h(140): warning: variable "MMA_M" was declared but never referenced
          detected during:
            instantiation of "void flash::compute_attn_1rowblock<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params,Is_Q_split_K>(const Params &, int, int, int) [with Kernel_traits=Flash_fwd_kernel_traits<160, 64, 64, 4, false, false, cutlass::half_t, Flash_kernel_traits<160, 64, 64, 4, cutlass::half_t>>, Is_dropout=true, Is_causal=true, Is_even_N=false, Is_even_K=false, Return_softmax=false, Params=Flash_fwd_params, Is_Q_split_K=false]" 
(589): here
            instantiation of "void flash::compute_attn<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &) [with Kernel_traits=Flash_fwd_kernel_traits<160, 64, 64, 4, false, false, cutlass::half_t, Flash_kernel_traits<160, 64, 64, 4, cutlass::half_t>>, Is_dropout=true, Is_causal=true, Is_even_N=false, Is_even_K=false, Return_softmax=false, Params=Flash_fwd_params]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(15): here
            instantiation of "void flash_fwd_kernel<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax>(Flash_fwd_params) [with Kernel_traits=Flash_fwd_kernel_traits<160, 64, 64, 4, false, false, cutlass::half_t, Flash_kernel_traits<160, 64, 64, 4, cutlass::half_t>>, Is_dropout=true, Is_causal=true, Is_even_N=false, Is_even_K=false, Return_softmax=false]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(34): here
            instantiation of "void run_flash_fwd<Kernel_traits,Is_dropout,Is_causal>(Flash_fwd_params &, cudaStream_t) [with Kernel_traits=Flash_fwd_kernel_traits<160, 64, 64, 4, false, false, cutlass::half_t, Flash_kernel_traits<160, 64, 64, 4, cutlass::half_t>>, Is_dropout=true, Is_causal=true]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(155): here
            instantiation of "void run_mha_fwd_hdim160<T>(Flash_fwd_params &, cudaStream_t) [with T=cutlass::half_t]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_hdim160_fp16_sm80.cu(26): here

/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_kernel.h(276): warning: variable "tQrQ" was declared but never referenced
          detected during:
            instantiation of "void flash::compute_attn_1rowblock<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params,Is_Q_split_K>(const Params &, int, int, int) [with Kernel_traits=Flash_fwd_kernel_traits<160, 64, 64, 4, false, false, cutlass::half_t, Flash_kernel_traits<160, 64, 64, 4, cutlass::half_t>>, Is_dropout=true, Is_causal=true, Is_even_N=false, Is_even_K=false, Return_softmax=false, Params=Flash_fwd_params, Is_Q_split_K=false]" 
(589): here
            instantiation of "void flash::compute_attn<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &) [with Kernel_traits=Flash_fwd_kernel_traits<160, 64, 64, 4, false, false, cutlass::half_t, Flash_kernel_traits<160, 64, 64, 4, cutlass::half_t>>, Is_dropout=true, Is_causal=true, Is_even_N=false, Is_even_K=false, Return_softmax=false, Params=Flash_fwd_params]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(15): here
            instantiation of "void flash_fwd_kernel<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax>(Flash_fwd_params) [with Kernel_traits=Flash_fwd_kernel_traits<160, 64, 64, 4, false, false, cutlass::half_t, Flash_kernel_traits<160, 64, 64, 4, cutlass::half_t>>, Is_dropout=true, Is_causal=true, Is_even_N=false, Is_even_K=false, Return_softmax=false]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(34): here
            instantiation of "void run_flash_fwd<Kernel_traits,Is_dropout,Is_causal>(Flash_fwd_params &, cudaStream_t) [with Kernel_traits=Flash_fwd_kernel_traits<160, 64, 64, 4, false, false, cutlass::half_t, Flash_kernel_traits<160, 64, 64, 4, cutlass::half_t>>, Is_dropout=true, Is_causal=true]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(155): here
            instantiation of "void run_mha_fwd_hdim160<T>(Flash_fwd_params &, cudaStream_t) [with T=cutlass::half_t]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_hdim160_fp16_sm80.cu(26): here

/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_kernel.h(140): warning: variable "MMA_M" was declared but never referenced
          detected during:
            instantiation of "void flash::compute_attn_1rowblock<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params,Is_Q_split_K>(const Params &, int, int, int) [with Kernel_traits=Flash_fwd_kernel_traits<160, 128, 32, 4, false, false, cutlass::half_t, Flash_kernel_traits<160, 128, 32, 4, cutlass::half_t>>, Is_dropout=true, Is_causal=true, Is_even_N=true, Is_even_K=true, Return_softmax=true, Params=Flash_fwd_params, Is_Q_split_K=false]" 
(589): here
            instantiation of "void flash::compute_attn<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &) [with Kernel_traits=Flash_fwd_kernel_traits<160, 128, 32, 4, false, false, cutlass::half_t, Flash_kernel_traits<160, 128, 32, 4, cutlass::half_t>>, Is_dropout=true, Is_causal=true, Is_even_N=true, Is_even_K=true, Return_softmax=true, Params=Flash_fwd_params]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(15): here
            instantiation of "void flash_fwd_kernel<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax>(Flash_fwd_params) [with Kernel_traits=Flash_fwd_kernel_traits<160, 128, 32, 4, false, false, cutlass::half_t, Flash_kernel_traits<160, 128, 32, 4, cutlass::half_t>>, Is_dropout=true, Is_causal=true, Is_even_N=true, Is_even_K=true, Return_softmax=true]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(34): here
            instantiation of "void run_flash_fwd<Kernel_traits,Is_dropout,Is_causal>(Flash_fwd_params &, cudaStream_t) [with Kernel_traits=Flash_fwd_kernel_traits<160, 128, 32, 4, false, false, cutlass::half_t, Flash_kernel_traits<160, 128, 32, 4, cutlass::half_t>>, Is_dropout=true, Is_causal=true]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(155): here
            instantiation of "void run_mha_fwd_hdim160<T>(Flash_fwd_params &, cudaStream_t) [with T=cutlass::half_t]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_hdim160_fp16_sm80.cu(26): here

/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_kernel.h(276): warning: variable "tQrQ" was declared but never referenced
          detected during:
            instantiation of "void flash::compute_attn_1rowblock<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params,Is_Q_split_K>(const Params &, int, int, int) [with Kernel_traits=Flash_fwd_kernel_traits<160, 128, 32, 4, false, false, cutlass::half_t, Flash_kernel_traits<160, 128, 32, 4, cutlass::half_t>>, Is_dropout=true, Is_causal=true, Is_even_N=true, Is_even_K=true, Return_softmax=true, Params=Flash_fwd_params, Is_Q_split_K=false]" 
(589): here
            instantiation of "void flash::compute_attn<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &) [with Kernel_traits=Flash_fwd_kernel_traits<160, 128, 32, 4, false, false, cutlass::half_t, Flash_kernel_traits<160, 128, 32, 4, cutlass::half_t>>, Is_dropout=true, Is_causal=true, Is_even_N=true, Is_even_K=true, Return_softmax=true, Params=Flash_fwd_params]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(15): here
            instantiation of "void flash_fwd_kernel<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax>(Flash_fwd_params) [with Kernel_traits=Flash_fwd_kernel_traits<160, 128, 32, 4, false, false, cutlass::half_t, Flash_kernel_traits<160, 128, 32, 4, cutlass::half_t>>, Is_dropout=true, Is_causal=true, Is_even_N=true, Is_even_K=true, Return_softmax=true]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(34): here
            instantiation of "void run_flash_fwd<Kernel_traits,Is_dropout,Is_causal>(Flash_fwd_params &, cudaStream_t) [with Kernel_traits=Flash_fwd_kernel_traits<160, 128, 32, 4, false, false, cutlass::half_t, Flash_kernel_traits<160, 128, 32, 4, cutlass::half_t>>, Is_dropout=true, Is_causal=true]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(155): here
            instantiation of "void run_mha_fwd_hdim160<T>(Flash_fwd_params &, cudaStream_t) [with T=cutlass::half_t]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_hdim160_fp16_sm80.cu(26): here

/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_kernel.h(140): warning: variable "MMA_M" was declared but never referenced
          detected during:
            instantiation of "void flash::compute_attn_1rowblock<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params,Is_Q_split_K>(const Params &, int, int, int) [with Kernel_traits=Flash_fwd_kernel_traits<160, 128, 32, 4, false, false, cutlass::half_t, Flash_kernel_traits<160, 128, 32, 4, cutlass::half_t>>, Is_dropout=true, Is_causal=true, Is_even_N=true, Is_even_K=true, Return_softmax=false, Params=Flash_fwd_params, Is_Q_split_K=false]" 
(589): here
            instantiation of "void flash::compute_attn<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &) [with Kernel_traits=Flash_fwd_kernel_traits<160, 128, 32, 4, false, false, cutlass::half_t, Flash_kernel_traits<160, 128, 32, 4, cutlass::half_t>>, Is_dropout=true, Is_causal=true, Is_even_N=true, Is_even_K=true, Return_softmax=false, Params=Flash_fwd_params]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(15): here
            instantiation of "void flash_fwd_kernel<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax>(Flash_fwd_params) [with Kernel_traits=Flash_fwd_kernel_traits<160, 128, 32, 4, false, false, cutlass::half_t, Flash_kernel_traits<160, 128, 32, 4, cutlass::half_t>>, Is_dropout=true, Is_causal=true, Is_even_N=true, Is_even_K=true, Return_softmax=false]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(34): here
            instantiation of "void run_flash_fwd<Kernel_traits,Is_dropout,Is_causal>(Flash_fwd_params &, cudaStream_t) [with Kernel_traits=Flash_fwd_kernel_traits<160, 128, 32, 4, false, false, cutlass::half_t, Flash_kernel_traits<160, 128, 32, 4, cutlass::half_t>>, Is_dropout=true, Is_causal=true]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(155): here
            instantiation of "void run_mha_fwd_hdim160<T>(Flash_fwd_params &, cudaStream_t) [with T=cutlass::half_t]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_hdim160_fp16_sm80.cu(26): here

/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_kernel.h(276): warning: variable "tQrQ" was declared but never referenced
          detected during:
            instantiation of "void flash::compute_attn_1rowblock<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params,Is_Q_split_K>(const Params &, int, int, int) [with Kernel_traits=Flash_fwd_kernel_traits<160, 128, 32, 4, false, false, cutlass::half_t, Flash_kernel_traits<160, 128, 32, 4, cutlass::half_t>>, Is_dropout=true, Is_causal=true, Is_even_N=true, Is_even_K=true, Return_softmax=false, Params=Flash_fwd_params, Is_Q_split_K=false]" 
(589): here
            instantiation of "void flash::compute_attn<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &) [with Kernel_traits=Flash_fwd_kernel_traits<160, 128, 32, 4, false, false, cutlass::half_t, Flash_kernel_traits<160, 128, 32, 4, cutlass::half_t>>, Is_dropout=true, Is_causal=true, Is_even_N=true, Is_even_K=true, Return_softmax=false, Params=Flash_fwd_params]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(15): here
            instantiation of "void flash_fwd_kernel<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax>(Flash_fwd_params) [with Kernel_traits=Flash_fwd_kernel_traits<160, 128, 32, 4, false, false, cutlass::half_t, Flash_kernel_traits<160, 128, 32, 4, cutlass::half_t>>, Is_dropout=true, Is_causal=true, Is_even_N=true, Is_even_K=true, Return_softmax=false]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(34): here
            instantiation of "void run_flash_fwd<Kernel_traits,Is_dropout,Is_causal>(Flash_fwd_params &, cudaStream_t) [with Kernel_traits=Flash_fwd_kernel_traits<160, 128, 32, 4, false, false, cutlass::half_t, Flash_kernel_traits<160, 128, 32, 4, cutlass::half_t>>, Is_dropout=true, Is_causal=true]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(155): here
            instantiation of "void run_mha_fwd_hdim160<T>(Flash_fwd_params &, cudaStream_t) [with T=cutlass::half_t]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_hdim160_fp16_sm80.cu(26): here

/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_kernel.h(140): warning: variable "MMA_M" was declared but never referenced
          detected during:
            instantiation of "void flash::compute_attn_1rowblock<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params,Is_Q_split_K>(const Params &, int, int, int) [with Kernel_traits=Flash_fwd_kernel_traits<160, 128, 32, 4, false, false, cutlass::half_t, Flash_kernel_traits<160, 128, 32, 4, cutlass::half_t>>, Is_dropout=true, Is_causal=true, Is_even_N=true, Is_even_K=false, Return_softmax=true, Params=Flash_fwd_params, Is_Q_split_K=false]" 
(589): here
            instantiation of "void flash::compute_attn<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &) [with Kernel_traits=Flash_fwd_kernel_traits<160, 128, 32, 4, false, false, cutlass::half_t, Flash_kernel_traits<160, 128, 32, 4, cutlass::half_t>>, Is_dropout=true, Is_causal=true, Is_even_N=true, Is_even_K=false, Return_softmax=true, Params=Flash_fwd_params]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(15): here
            instantiation of "void flash_fwd_kernel<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax>(Flash_fwd_params) [with Kernel_traits=Flash_fwd_kernel_traits<160, 128, 32, 4, false, false, cutlass::half_t, Flash_kernel_traits<160, 128, 32, 4, cutlass::half_t>>, Is_dropout=true, Is_causal=true, Is_even_N=true, Is_even_K=false, Return_softmax=true]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(34): here
            instantiation of "void run_flash_fwd<Kernel_traits,Is_dropout,Is_causal>(Flash_fwd_params &, cudaStream_t) [with Kernel_traits=Flash_fwd_kernel_traits<160, 128, 32, 4, false, false, cutlass::half_t, Flash_kernel_traits<160, 128, 32, 4, cutlass::half_t>>, Is_dropout=true, Is_causal=true]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(155): here
            instantiation of "void run_mha_fwd_hdim160<T>(Flash_fwd_params &, cudaStream_t) [with T=cutlass::half_t]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_hdim160_fp16_sm80.cu(26): here

/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_kernel.h(276): warning: variable "tQrQ" was declared but never referenced
          detected during:
            instantiation of "void flash::compute_attn_1rowblock<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params,Is_Q_split_K>(const Params &, int, int, int) [with Kernel_traits=Flash_fwd_kernel_traits<160, 128, 32, 4, false, false, cutlass::half_t, Flash_kernel_traits<160, 128, 32, 4, cutlass::half_t>>, Is_dropout=true, Is_causal=true, Is_even_N=true, Is_even_K=false, Return_softmax=true, Params=Flash_fwd_params, Is_Q_split_K=false]" 
(589): here
            instantiation of "void flash::compute_attn<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &) [with Kernel_traits=Flash_fwd_kernel_traits<160, 128, 32, 4, false, false, cutlass::half_t, Flash_kernel_traits<160, 128, 32, 4, cutlass::half_t>>, Is_dropout=true, Is_causal=true, Is_even_N=true, Is_even_K=false, Return_softmax=true, Params=Flash_fwd_params]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(15): here
            instantiation of "void flash_fwd_kernel<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax>(Flash_fwd_params) [with Kernel_traits=Flash_fwd_kernel_traits<160, 128, 32, 4, false, false, cutlass::half_t, Flash_kernel_traits<160, 128, 32, 4, cutlass::half_t>>, Is_dropout=true, Is_causal=true, Is_even_N=true, Is_even_K=false, Return_softmax=true]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(34): here
            instantiation of "void run_flash_fwd<Kernel_traits,Is_dropout,Is_causal>(Flash_fwd_params &, cudaStream_t) [with Kernel_traits=Flash_fwd_kernel_traits<160, 128, 32, 4, false, false, cutlass::half_t, Flash_kernel_traits<160, 128, 32, 4, cutlass::half_t>>, Is_dropout=true, Is_causal=true]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(155): here
            instantiation of "void run_mha_fwd_hdim160<T>(Flash_fwd_params &, cudaStream_t) [with T=cutlass::half_t]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_hdim160_fp16_sm80.cu(26): here

/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_kernel.h(140): warning: variable "MMA_M" was declared but never referenced
          detected during:
            instantiation of "void flash::compute_attn_1rowblock<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params,Is_Q_split_K>(const Params &, int, int, int) [with Kernel_traits=Flash_fwd_kernel_traits<160, 128, 32, 4, false, false, cutlass::half_t, Flash_kernel_traits<160, 128, 32, 4, cutlass::half_t>>, Is_dropout=true, Is_causal=true, Is_even_N=true, Is_even_K=false, Return_softmax=false, Params=Flash_fwd_params, Is_Q_split_K=false]" 
(589): here
            instantiation of "void flash::compute_attn<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &) [with Kernel_traits=Flash_fwd_kernel_traits<160, 128, 32, 4, false, false, cutlass::half_t, Flash_kernel_traits<160, 128, 32, 4, cutlass::half_t>>, Is_dropout=true, Is_causal=true, Is_even_N=true, Is_even_K=false, Return_softmax=false, Params=Flash_fwd_params]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(15): here
            instantiation of "void flash_fwd_kernel<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax>(Flash_fwd_params) [with Kernel_traits=Flash_fwd_kernel_traits<160, 128, 32, 4, false, false, cutlass::half_t, Flash_kernel_traits<160, 128, 32, 4, cutlass::half_t>>, Is_dropout=true, Is_causal=true, Is_even_N=true, Is_even_K=false, Return_softmax=false]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(34): here
            instantiation of "void run_flash_fwd<Kernel_traits,Is_dropout,Is_causal>(Flash_fwd_params &, cudaStream_t) [with Kernel_traits=Flash_fwd_kernel_traits<160, 128, 32, 4, false, false, cutlass::half_t, Flash_kernel_traits<160, 128, 32, 4, cutlass::half_t>>, Is_dropout=true, Is_causal=true]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(155): here
            instantiation of "void run_mha_fwd_hdim160<T>(Flash_fwd_params &, cudaStream_t) [with T=cutlass::half_t]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_hdim160_fp16_sm80.cu(26): here

/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_kernel.h(276): warning: variable "tQrQ" was declared but never referenced
          detected during:
            instantiation of "void flash::compute_attn_1rowblock<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params,Is_Q_split_K>(const Params &, int, int, int) [with Kernel_traits=Flash_fwd_kernel_traits<160, 128, 32, 4, false, false, cutlass::half_t, Flash_kernel_traits<160, 128, 32, 4, cutlass::half_t>>, Is_dropout=true, Is_causal=true, Is_even_N=true, Is_even_K=false, Return_softmax=false, Params=Flash_fwd_params, Is_Q_split_K=false]" 
(589): here
            instantiation of "void flash::compute_attn<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &) [with Kernel_traits=Flash_fwd_kernel_traits<160, 128, 32, 4, false, false, cutlass::half_t, Flash_kernel_traits<160, 128, 32, 4, cutlass::half_t>>, Is_dropout=true, Is_causal=true, Is_even_N=true, Is_even_K=false, Return_softmax=false, Params=Flash_fwd_params]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(15): here
            instantiation of "void flash_fwd_kernel<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax>(Flash_fwd_params) [with Kernel_traits=Flash_fwd_kernel_traits<160, 128, 32, 4, false, false, cutlass::half_t, Flash_kernel_traits<160, 128, 32, 4, cutlass::half_t>>, Is_dropout=true, Is_causal=true, Is_even_N=true, Is_even_K=false, Return_softmax=false]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(34): here
            instantiation of "void run_flash_fwd<Kernel_traits,Is_dropout,Is_causal>(Flash_fwd_params &, cudaStream_t) [with Kernel_traits=Flash_fwd_kernel_traits<160, 128, 32, 4, false, false, cutlass::half_t, Flash_kernel_traits<160, 128, 32, 4, cutlass::half_t>>, Is_dropout=true, Is_causal=true]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(155): here
            instantiation of "void run_mha_fwd_hdim160<T>(Flash_fwd_params &, cudaStream_t) [with T=cutlass::half_t]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_hdim160_fp16_sm80.cu(26): here

/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_kernel.h(140): warning: variable "MMA_M" was declared but never referenced
          detected during:
            instantiation of "void flash::compute_attn_1rowblock<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params,Is_Q_split_K>(const Params &, int, int, int) [with Kernel_traits=Flash_fwd_kernel_traits<160, 128, 32, 4, false, false, cutlass::half_t, Flash_kernel_traits<160, 128, 32, 4, cutlass::half_t>>, Is_dropout=true, Is_causal=true, Is_even_N=false, Is_even_K=true, Return_softmax=true, Params=Flash_fwd_params, Is_Q_split_K=false]" 
(589): here
            instantiation of "void flash::compute_attn<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &) [with Kernel_traits=Flash_fwd_kernel_traits<160, 128, 32, 4, false, false, cutlass::half_t, Flash_kernel_traits<160, 128, 32, 4, cutlass::half_t>>, Is_dropout=true, Is_causal=true, Is_even_N=false, Is_even_K=true, Return_softmax=true, Params=Flash_fwd_params]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(15): here
            instantiation of "void flash_fwd_kernel<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax>(Flash_fwd_params) [with Kernel_traits=Flash_fwd_kernel_traits<160, 128, 32, 4, false, false, cutlass::half_t, Flash_kernel_traits<160, 128, 32, 4, cutlass::half_t>>, Is_dropout=true, Is_causal=true, Is_even_N=false, Is_even_K=true, Return_softmax=true]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(34): here
            instantiation of "void run_flash_fwd<Kernel_traits,Is_dropout,Is_causal>(Flash_fwd_params &, cudaStream_t) [with Kernel_traits=Flash_fwd_kernel_traits<160, 128, 32, 4, false, false, cutlass::half_t, Flash_kernel_traits<160, 128, 32, 4, cutlass::half_t>>, Is_dropout=true, Is_causal=true]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(155): here
            instantiation of "void run_mha_fwd_hdim160<T>(Flash_fwd_params &, cudaStream_t) [with T=cutlass::half_t]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_hdim160_fp16_sm80.cu(26): here

/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_kernel.h(276): warning: variable "tQrQ" was declared but never referenced
          detected during:
            instantiation of "void flash::compute_attn_1rowblock<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params,Is_Q_split_K>(const Params &, int, int, int) [with Kernel_traits=Flash_fwd_kernel_traits<160, 128, 32, 4, false, false, cutlass::half_t, Flash_kernel_traits<160, 128, 32, 4, cutlass::half_t>>, Is_dropout=true, Is_causal=true, Is_even_N=false, Is_even_K=true, Return_softmax=true, Params=Flash_fwd_params, Is_Q_split_K=false]" 
(589): here
            instantiation of "void flash::compute_attn<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &) [with Kernel_traits=Flash_fwd_kernel_traits<160, 128, 32, 4, false, false, cutlass::half_t, Flash_kernel_traits<160, 128, 32, 4, cutlass::half_t>>, Is_dropout=true, Is_causal=true, Is_even_N=false, Is_even_K=true, Return_softmax=true, Params=Flash_fwd_params]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(15): here
            instantiation of "void flash_fwd_kernel<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax>(Flash_fwd_params) [with Kernel_traits=Flash_fwd_kernel_traits<160, 128, 32, 4, false, false, cutlass::half_t, Flash_kernel_traits<160, 128, 32, 4, cutlass::half_t>>, Is_dropout=true, Is_causal=true, Is_even_N=false, Is_even_K=true, Return_softmax=true]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(34): here
            instantiation of "void run_flash_fwd<Kernel_traits,Is_dropout,Is_causal>(Flash_fwd_params &, cudaStream_t) [with Kernel_traits=Flash_fwd_kernel_traits<160, 128, 32, 4, false, false, cutlass::half_t, Flash_kernel_traits<160, 128, 32, 4, cutlass::half_t>>, Is_dropout=true, Is_causal=true]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(155): here
            instantiation of "void run_mha_fwd_hdim160<T>(Flash_fwd_params &, cudaStream_t) [with T=cutlass::half_t]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_hdim160_fp16_sm80.cu(26): here

/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_kernel.h(140): warning: variable "MMA_M" was declared but never referenced
          detected during:
            instantiation of "void flash::compute_attn_1rowblock<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params,Is_Q_split_K>(const Params &, int, int, int) [with Kernel_traits=Flash_fwd_kernel_traits<160, 128, 32, 4, false, false, cutlass::half_t, Flash_kernel_traits<160, 128, 32, 4, cutlass::half_t>>, Is_dropout=true, Is_causal=true, Is_even_N=false, Is_even_K=true, Return_softmax=false, Params=Flash_fwd_params, Is_Q_split_K=false]" 
(589): here
            instantiation of "void flash::compute_attn<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &) [with Kernel_traits=Flash_fwd_kernel_traits<160, 128, 32, 4, false, false, cutlass::half_t, Flash_kernel_traits<160, 128, 32, 4, cutlass::half_t>>, Is_dropout=true, Is_causal=true, Is_even_N=false, Is_even_K=true, Return_softmax=false, Params=Flash_fwd_params]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(15): here
            instantiation of "void flash_fwd_kernel<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax>(Flash_fwd_params) [with Kernel_traits=Flash_fwd_kernel_traits<160, 128, 32, 4, false, false, cutlass::half_t, Flash_kernel_traits<160, 128, 32, 4, cutlass::half_t>>, Is_dropout=true, Is_causal=true, Is_even_N=false, Is_even_K=true, Return_softmax=false]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(34): here
            instantiation of "void run_flash_fwd<Kernel_traits,Is_dropout,Is_causal>(Flash_fwd_params &, cudaStream_t) [with Kernel_traits=Flash_fwd_kernel_traits<160, 128, 32, 4, false, false, cutlass::half_t, Flash_kernel_traits<160, 128, 32, 4, cutlass::half_t>>, Is_dropout=true, Is_causal=true]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(155): here
            instantiation of "void run_mha_fwd_hdim160<T>(Flash_fwd_params &, cudaStream_t) [with T=cutlass::half_t]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_hdim160_fp16_sm80.cu(26): here

/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_kernel.h(276): warning: variable "tQrQ" was declared but never referenced
          detected during:
            instantiation of "void flash::compute_attn_1rowblock<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params,Is_Q_split_K>(const Params &, int, int, int) [with Kernel_traits=Flash_fwd_kernel_traits<160, 128, 32, 4, false, false, cutlass::half_t, Flash_kernel_traits<160, 128, 32, 4, cutlass::half_t>>, Is_dropout=true, Is_causal=true, Is_even_N=false, Is_even_K=true, Return_softmax=false, Params=Flash_fwd_params, Is_Q_split_K=false]" 
(589): here
            instantiation of "void flash::compute_attn<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &) [with Kernel_traits=Flash_fwd_kernel_traits<160, 128, 32, 4, false, false, cutlass::half_t, Flash_kernel_traits<160, 128, 32, 4, cutlass::half_t>>, Is_dropout=true, Is_causal=true, Is_even_N=false, Is_even_K=true, Return_softmax=false, Params=Flash_fwd_params]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(15): here
            instantiation of "void flash_fwd_kernel<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax>(Flash_fwd_params) [with Kernel_traits=Flash_fwd_kernel_traits<160, 128, 32, 4, false, false, cutlass::half_t, Flash_kernel_traits<160, 128, 32, 4, cutlass::half_t>>, Is_dropout=true, Is_causal=true, Is_even_N=false, Is_even_K=true, Return_softmax=false]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(34): here
            instantiation of "void run_flash_fwd<Kernel_traits,Is_dropout,Is_causal>(Flash_fwd_params &, cudaStream_t) [with Kernel_traits=Flash_fwd_kernel_traits<160, 128, 32, 4, false, false, cutlass::half_t, Flash_kernel_traits<160, 128, 32, 4, cutlass::half_t>>, Is_dropout=true, Is_causal=true]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(155): here
            instantiation of "void run_mha_fwd_hdim160<T>(Flash_fwd_params &, cudaStream_t) [with T=cutlass::half_t]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_hdim160_fp16_sm80.cu(26): here

/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_kernel.h(140): warning: variable "MMA_M" was declared but never referenced
          detected during:
            instantiation of "void flash::compute_attn_1rowblock<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params,Is_Q_split_K>(const Params &, int, int, int) [with Kernel_traits=Flash_fwd_kernel_traits<160, 128, 32, 4, false, false, cutlass::half_t, Flash_kernel_traits<160, 128, 32, 4, cutlass::half_t>>, Is_dropout=true, Is_causal=true, Is_even_N=false, Is_even_K=false, Return_softmax=true, Params=Flash_fwd_params, Is_Q_split_K=false]" 
(589): here
            instantiation of "void flash::compute_attn<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &) [with Kernel_traits=Flash_fwd_kernel_traits<160, 128, 32, 4, false, false, cutlass::half_t, Flash_kernel_traits<160, 128, 32, 4, cutlass::half_t>>, Is_dropout=true, Is_causal=true, Is_even_N=false, Is_even_K=false, Return_softmax=true, Params=Flash_fwd_params]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(15): here
            instantiation of "void flash_fwd_kernel<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax>(Flash_fwd_params) [with Kernel_traits=Flash_fwd_kernel_traits<160, 128, 32, 4, false, false, cutlass::half_t, Flash_kernel_traits<160, 128, 32, 4, cutlass::half_t>>, Is_dropout=true, Is_causal=true, Is_even_N=false, Is_even_K=false, Return_softmax=true]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(34): here
            instantiation of "void run_flash_fwd<Kernel_traits,Is_dropout,Is_causal>(Flash_fwd_params &, cudaStream_t) [with Kernel_traits=Flash_fwd_kernel_traits<160, 128, 32, 4, false, false, cutlass::half_t, Flash_kernel_traits<160, 128, 32, 4, cutlass::half_t>>, Is_dropout=true, Is_causal=true]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(155): here
            instantiation of "void run_mha_fwd_hdim160<T>(Flash_fwd_params &, cudaStream_t) [with T=cutlass::half_t]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_hdim160_fp16_sm80.cu(26): here

/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_kernel.h(276): warning: variable "tQrQ" was declared but never referenced
          detected during:
            instantiation of "void flash::compute_attn_1rowblock<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params,Is_Q_split_K>(const Params &, int, int, int) [with Kernel_traits=Flash_fwd_kernel_traits<160, 128, 32, 4, false, false, cutlass::half_t, Flash_kernel_traits<160, 128, 32, 4, cutlass::half_t>>, Is_dropout=true, Is_causal=true, Is_even_N=false, Is_even_K=false, Return_softmax=true, Params=Flash_fwd_params, Is_Q_split_K=false]" 
(589): here
            instantiation of "void flash::compute_attn<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &) [with Kernel_traits=Flash_fwd_kernel_traits<160, 128, 32, 4, false, false, cutlass::half_t, Flash_kernel_traits<160, 128, 32, 4, cutlass::half_t>>, Is_dropout=true, Is_causal=true, Is_even_N=false, Is_even_K=false, Return_softmax=true, Params=Flash_fwd_params]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(15): here
            instantiation of "void flash_fwd_kernel<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax>(Flash_fwd_params) [with Kernel_traits=Flash_fwd_kernel_traits<160, 128, 32, 4, false, false, cutlass::half_t, Flash_kernel_traits<160, 128, 32, 4, cutlass::half_t>>, Is_dropout=true, Is_causal=true, Is_even_N=false, Is_even_K=false, Return_softmax=true]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(34): here
            instantiation of "void run_flash_fwd<Kernel_traits,Is_dropout,Is_causal>(Flash_fwd_params &, cudaStream_t) [with Kernel_traits=Flash_fwd_kernel_traits<160, 128, 32, 4, false, false, cutlass::half_t, Flash_kernel_traits<160, 128, 32, 4, cutlass::half_t>>, Is_dropout=true, Is_causal=true]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(155): here
            instantiation of "void run_mha_fwd_hdim160<T>(Flash_fwd_params &, cudaStream_t) [with T=cutlass::half_t]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_hdim160_fp16_sm80.cu(26): here

/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_kernel.h(140): warning: variable "MMA_M" was declared but never referenced
          detected during:
            instantiation of "void flash::compute_attn_1rowblock<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params,Is_Q_split_K>(const Params &, int, int, int) [with Kernel_traits=Flash_fwd_kernel_traits<160, 128, 32, 4, false, false, cutlass::half_t, Flash_kernel_traits<160, 128, 32, 4, cutlass::half_t>>, Is_dropout=true, Is_causal=true, Is_even_N=false, Is_even_K=false, Return_softmax=false, Params=Flash_fwd_params, Is_Q_split_K=false]" 
(589): here
            instantiation of "void flash::compute_attn<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &) [with Kernel_traits=Flash_fwd_kernel_traits<160, 128, 32, 4, false, false, cutlass::half_t, Flash_kernel_traits<160, 128, 32, 4, cutlass::half_t>>, Is_dropout=true, Is_causal=true, Is_even_N=false, Is_even_K=false, Return_softmax=false, Params=Flash_fwd_params]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(15): here
            instantiation of "void flash_fwd_kernel<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax>(Flash_fwd_params) [with Kernel_traits=Flash_fwd_kernel_traits<160, 128, 32, 4, false, false, cutlass::half_t, Flash_kernel_traits<160, 128, 32, 4, cutlass::half_t>>, Is_dropout=true, Is_causal=true, Is_even_N=false, Is_even_K=false, Return_softmax=false]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(34): here
            instantiation of "void run_flash_fwd<Kernel_traits,Is_dropout,Is_causal>(Flash_fwd_params &, cudaStream_t) [with Kernel_traits=Flash_fwd_kernel_traits<160, 128, 32, 4, false, false, cutlass::half_t, Flash_kernel_traits<160, 128, 32, 4, cutlass::half_t>>, Is_dropout=true, Is_causal=true]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(155): here
            instantiation of "void run_mha_fwd_hdim160<T>(Flash_fwd_params &, cudaStream_t) [with T=cutlass::half_t]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_hdim160_fp16_sm80.cu(26): here

/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_kernel.h(276): warning: variable "tQrQ" was declared but never referenced
          detected during:
            instantiation of "void flash::compute_attn_1rowblock<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params,Is_Q_split_K>(const Params &, int, int, int) [with Kernel_traits=Flash_fwd_kernel_traits<160, 128, 32, 4, false, false, cutlass::half_t, Flash_kernel_traits<160, 128, 32, 4, cutlass::half_t>>, Is_dropout=true, Is_causal=true, Is_even_N=false, Is_even_K=false, Return_softmax=false, Params=Flash_fwd_params, Is_Q_split_K=false]" 
(589): here
            instantiation of "void flash::compute_attn<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &) [with Kernel_traits=Flash_fwd_kernel_traits<160, 128, 32, 4, false, false, cutlass::half_t, Flash_kernel_traits<160, 128, 32, 4, cutlass::half_t>>, Is_dropout=true, Is_causal=true, Is_even_N=false, Is_even_K=false, Return_softmax=false, Params=Flash_fwd_params]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(15): here
            instantiation of "void flash_fwd_kernel<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax>(Flash_fwd_params) [with Kernel_traits=Flash_fwd_kernel_traits<160, 128, 32, 4, false, false, cutlass::half_t, Flash_kernel_traits<160, 128, 32, 4, cutlass::half_t>>, Is_dropout=true, Is_causal=true, Is_even_N=false, Is_even_K=false, Return_softmax=false]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(34): here
            instantiation of "void run_flash_fwd<Kernel_traits,Is_dropout,Is_causal>(Flash_fwd_params &, cudaStream_t) [with Kernel_traits=Flash_fwd_kernel_traits<160, 128, 32, 4, false, false, cutlass::half_t, Flash_kernel_traits<160, 128, 32, 4, cutlass::half_t>>, Is_dropout=true, Is_causal=true]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(155): here
            instantiation of "void run_mha_fwd_hdim160<T>(Flash_fwd_params &, cudaStream_t) [with T=cutlass::half_t]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_hdim160_fp16_sm80.cu(26): here

/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_kernel.h(140): warning: variable "MMA_M" was declared but never referenced
          detected during:
            instantiation of "void flash::compute_attn_1rowblock<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params,Is_Q_split_K>(const Params &, int, int, int) [with Kernel_traits=Flash_fwd_kernel_traits<160, 128, 64, 8, false, false, cutlass::half_t, Flash_kernel_traits<160, 128, 64, 8, cutlass::half_t>>, Is_dropout=true, Is_causal=false, Is_even_N=true, Is_even_K=true, Return_softmax=true, Params=Flash_fwd_params, Is_Q_split_K=false]" 
(589): here
            instantiation of "void flash::compute_attn<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &) [with Kernel_traits=Flash_fwd_kernel_traits<160, 128, 64, 8, false, false, cutlass::half_t, Flash_kernel_traits<160, 128, 64, 8, cutlass::half_t>>, Is_dropout=true, Is_causal=false, Is_even_N=true, Is_even_K=true, Return_softmax=true, Params=Flash_fwd_params]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(15): here
            instantiation of "void flash_fwd_kernel<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax>(Flash_fwd_params) [with Kernel_traits=Flash_fwd_kernel_traits<160, 128, 64, 8, false, false, cutlass::half_t, Flash_kernel_traits<160, 128, 64, 8, cutlass::half_t>>, Is_dropout=true, Is_causal=false, Is_even_N=true, Is_even_K=true, Return_softmax=true]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(34): here
            instantiation of "void run_flash_fwd<Kernel_traits,Is_dropout,Is_causal>(Flash_fwd_params &, cudaStream_t) [with Kernel_traits=Flash_fwd_kernel_traits<160, 128, 64, 8, false, false, cutlass::half_t, Flash_kernel_traits<160, 128, 64, 8, cutlass::half_t>>, Is_dropout=true, Is_causal=false]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(155): here
            instantiation of "void run_mha_fwd_hdim160<T>(Flash_fwd_params &, cudaStream_t) [with T=cutlass::half_t]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_hdim160_fp16_sm80.cu(26): here

/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_kernel.h(276): warning: variable "tQrQ" was declared but never referenced
          detected during:
            instantiation of "void flash::compute_attn_1rowblock<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params,Is_Q_split_K>(const Params &, int, int, int) [with Kernel_traits=Flash_fwd_kernel_traits<160, 128, 64, 8, false, false, cutlass::half_t, Flash_kernel_traits<160, 128, 64, 8, cutlass::half_t>>, Is_dropout=true, Is_causal=false, Is_even_N=true, Is_even_K=true, Return_softmax=true, Params=Flash_fwd_params, Is_Q_split_K=false]" 
(589): here
            instantiation of "void flash::compute_attn<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &) [with Kernel_traits=Flash_fwd_kernel_traits<160, 128, 64, 8, false, false, cutlass::half_t, Flash_kernel_traits<160, 128, 64, 8, cutlass::half_t>>, Is_dropout=true, Is_causal=false, Is_even_N=true, Is_even_K=true, Return_softmax=true, Params=Flash_fwd_params]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(15): here
            instantiation of "void flash_fwd_kernel<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax>(Flash_fwd_params) [with Kernel_traits=Flash_fwd_kernel_traits<160, 128, 64, 8, false, false, cutlass::half_t, Flash_kernel_traits<160, 128, 64, 8, cutlass::half_t>>, Is_dropout=true, Is_causal=false, Is_even_N=true, Is_even_K=true, Return_softmax=true]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(34): here
            instantiation of "void run_flash_fwd<Kernel_traits,Is_dropout,Is_causal>(Flash_fwd_params &, cudaStream_t) [with Kernel_traits=Flash_fwd_kernel_traits<160, 128, 64, 8, false, false, cutlass::half_t, Flash_kernel_traits<160, 128, 64, 8, cutlass::half_t>>, Is_dropout=true, Is_causal=false]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(155): here
            instantiation of "void run_mha_fwd_hdim160<T>(Flash_fwd_params &, cudaStream_t) [with T=cutlass::half_t]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_hdim160_fp16_sm80.cu(26): here

/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_kernel.h(140): warning: variable "MMA_M" was declared but never referenced
          detected during:
            instantiation of "void flash::compute_attn_1rowblock<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params,Is_Q_split_K>(const Params &, int, int, int) [with Kernel_traits=Flash_fwd_kernel_traits<160, 128, 64, 8, false, false, cutlass::half_t, Flash_kernel_traits<160, 128, 64, 8, cutlass::half_t>>, Is_dropout=true, Is_causal=false, Is_even_N=true, Is_even_K=true, Return_softmax=false, Params=Flash_fwd_params, Is_Q_split_K=false]" 
(589): here
            instantiation of "void flash::compute_attn<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &) [with Kernel_traits=Flash_fwd_kernel_traits<160, 128, 64, 8, false, false, cutlass::half_t, Flash_kernel_traits<160, 128, 64, 8, cutlass::half_t>>, Is_dropout=true, Is_causal=false, Is_even_N=true, Is_even_K=true, Return_softmax=false, Params=Flash_fwd_params]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(15): here
            instantiation of "void flash_fwd_kernel<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax>(Flash_fwd_params) [with Kernel_traits=Flash_fwd_kernel_traits<160, 128, 64, 8, false, false, cutlass::half_t, Flash_kernel_traits<160, 128, 64, 8, cutlass::half_t>>, Is_dropout=true, Is_causal=false, Is_even_N=true, Is_even_K=true, Return_softmax=false]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(34): here
            instantiation of "void run_flash_fwd<Kernel_traits,Is_dropout,Is_causal>(Flash_fwd_params &, cudaStream_t) [with Kernel_traits=Flash_fwd_kernel_traits<160, 128, 64, 8, false, false, cutlass::half_t, Flash_kernel_traits<160, 128, 64, 8, cutlass::half_t>>, Is_dropout=true, Is_causal=false]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(155): here
            instantiation of "void run_mha_fwd_hdim160<T>(Flash_fwd_params &, cudaStream_t) [with T=cutlass::half_t]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_hdim160_fp16_sm80.cu(26): here

/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_kernel.h(276): warning: variable "tQrQ" was declared but never referenced
          detected during:
            instantiation of "void flash::compute_attn_1rowblock<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params,Is_Q_split_K>(const Params &, int, int, int) [with Kernel_traits=Flash_fwd_kernel_traits<160, 128, 64, 8, false, false, cutlass::half_t, Flash_kernel_traits<160, 128, 64, 8, cutlass::half_t>>, Is_dropout=true, Is_causal=false, Is_even_N=true, Is_even_K=true, Return_softmax=false, Params=Flash_fwd_params, Is_Q_split_K=false]" 
(589): here
            instantiation of "void flash::compute_attn<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &) [with Kernel_traits=Flash_fwd_kernel_traits<160, 128, 64, 8, false, false, cutlass::half_t, Flash_kernel_traits<160, 128, 64, 8, cutlass::half_t>>, Is_dropout=true, Is_causal=false, Is_even_N=true, Is_even_K=true, Return_softmax=false, Params=Flash_fwd_params]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(15): here
            instantiation of "void flash_fwd_kernel<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax>(Flash_fwd_params) [with Kernel_traits=Flash_fwd_kernel_traits<160, 128, 64, 8, false, false, cutlass::half_t, Flash_kernel_traits<160, 128, 64, 8, cutlass::half_t>>, Is_dropout=true, Is_causal=false, Is_even_N=true, Is_even_K=true, Return_softmax=false]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(34): here
            instantiation of "void run_flash_fwd<Kernel_traits,Is_dropout,Is_causal>(Flash_fwd_params &, cudaStream_t) [with Kernel_traits=Flash_fwd_kernel_traits<160, 128, 64, 8, false, false, cutlass::half_t, Flash_kernel_traits<160, 128, 64, 8, cutlass::half_t>>, Is_dropout=true, Is_causal=false]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(155): here
            instantiation of "void run_mha_fwd_hdim160<T>(Flash_fwd_params &, cudaStream_t) [with T=cutlass::half_t]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_hdim160_fp16_sm80.cu(26): here

/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_kernel.h(140): warning: variable "MMA_M" was declared but never referenced
          detected during:
            instantiation of "void flash::compute_attn_1rowblock<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params,Is_Q_split_K>(const Params &, int, int, int) [with Kernel_traits=Flash_fwd_kernel_traits<160, 128, 64, 8, false, false, cutlass::half_t, Flash_kernel_traits<160, 128, 64, 8, cutlass::half_t>>, Is_dropout=true, Is_causal=false, Is_even_N=true, Is_even_K=false, Return_softmax=true, Params=Flash_fwd_params, Is_Q_split_K=false]" 
(589): here
            instantiation of "void flash::compute_attn<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &) [with Kernel_traits=Flash_fwd_kernel_traits<160, 128, 64, 8, false, false, cutlass::half_t, Flash_kernel_traits<160, 128, 64, 8, cutlass::half_t>>, Is_dropout=true, Is_causal=false, Is_even_N=true, Is_even_K=false, Return_softmax=true, Params=Flash_fwd_params]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(15): here
            instantiation of "void flash_fwd_kernel<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax>(Flash_fwd_params) [with Kernel_traits=Flash_fwd_kernel_traits<160, 128, 64, 8, false, false, cutlass::half_t, Flash_kernel_traits<160, 128, 64, 8, cutlass::half_t>>, Is_dropout=true, Is_causal=false, Is_even_N=true, Is_even_K=false, Return_softmax=true]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(34): here
            instantiation of "void run_flash_fwd<Kernel_traits,Is_dropout,Is_causal>(Flash_fwd_params &, cudaStream_t) [with Kernel_traits=Flash_fwd_kernel_traits<160, 128, 64, 8, false, false, cutlass::half_t, Flash_kernel_traits<160, 128, 64, 8, cutlass::half_t>>, Is_dropout=true, Is_causal=false]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(155): here
            instantiation of "void run_mha_fwd_hdim160<T>(Flash_fwd_params &, cudaStream_t) [with T=cutlass::half_t]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_hdim160_fp16_sm80.cu(26): here

/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_kernel.h(276): warning: variable "tQrQ" was declared but never referenced
          detected during:
            instantiation of "void flash::compute_attn_1rowblock<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params,Is_Q_split_K>(const Params &, int, int, int) [with Kernel_traits=Flash_fwd_kernel_traits<160, 128, 64, 8, false, false, cutlass::half_t, Flash_kernel_traits<160, 128, 64, 8, cutlass::half_t>>, Is_dropout=true, Is_causal=false, Is_even_N=true, Is_even_K=false, Return_softmax=true, Params=Flash_fwd_params, Is_Q_split_K=false]" 
(589): here
            instantiation of "void flash::compute_attn<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &) [with Kernel_traits=Flash_fwd_kernel_traits<160, 128, 64, 8, false, false, cutlass::half_t, Flash_kernel_traits<160, 128, 64, 8, cutlass::half_t>>, Is_dropout=true, Is_causal=false, Is_even_N=true, Is_even_K=false, Return_softmax=true, Params=Flash_fwd_params]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(15): here
            instantiation of "void flash_fwd_kernel<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax>(Flash_fwd_params) [with Kernel_traits=Flash_fwd_kernel_traits<160, 128, 64, 8, false, false, cutlass::half_t, Flash_kernel_traits<160, 128, 64, 8, cutlass::half_t>>, Is_dropout=true, Is_causal=false, Is_even_N=true, Is_even_K=false, Return_softmax=true]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(34): here
            instantiation of "void run_flash_fwd<Kernel_traits,Is_dropout,Is_causal>(Flash_fwd_params &, cudaStream_t) [with Kernel_traits=Flash_fwd_kernel_traits<160, 128, 64, 8, false, false, cutlass::half_t, Flash_kernel_traits<160, 128, 64, 8, cutlass::half_t>>, Is_dropout=true, Is_causal=false]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(155): here
            instantiation of "void run_mha_fwd_hdim160<T>(Flash_fwd_params &, cudaStream_t) [with T=cutlass::half_t]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_hdim160_fp16_sm80.cu(26): here

/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_kernel.h(140): warning: variable "MMA_M" was declared but never referenced
          detected during:
            instantiation of "void flash::compute_attn_1rowblock<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params,Is_Q_split_K>(const Params &, int, int, int) [with Kernel_traits=Flash_fwd_kernel_traits<160, 128, 64, 8, false, false, cutlass::half_t, Flash_kernel_traits<160, 128, 64, 8, cutlass::half_t>>, Is_dropout=true, Is_causal=false, Is_even_N=true, Is_even_K=false, Return_softmax=false, Params=Flash_fwd_params, Is_Q_split_K=false]" 
(589): here
            instantiation of "void flash::compute_attn<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &) [with Kernel_traits=Flash_fwd_kernel_traits<160, 128, 64, 8, false, false, cutlass::half_t, Flash_kernel_traits<160, 128, 64, 8, cutlass::half_t>>, Is_dropout=true, Is_causal=false, Is_even_N=true, Is_even_K=false, Return_softmax=false, Params=Flash_fwd_params]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(15): here
            instantiation of "void flash_fwd_kernel<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax>(Flash_fwd_params) [with Kernel_traits=Flash_fwd_kernel_traits<160, 128, 64, 8, false, false, cutlass::half_t, Flash_kernel_traits<160, 128, 64, 8, cutlass::half_t>>, Is_dropout=true, Is_causal=false, Is_even_N=true, Is_even_K=false, Return_softmax=false]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(34): here
            instantiation of "void run_flash_fwd<Kernel_traits,Is_dropout,Is_causal>(Flash_fwd_params &, cudaStream_t) [with Kernel_traits=Flash_fwd_kernel_traits<160, 128, 64, 8, false, false, cutlass::half_t, Flash_kernel_traits<160, 128, 64, 8, cutlass::half_t>>, Is_dropout=true, Is_causal=false]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(155): here
            instantiation of "void run_mha_fwd_hdim160<T>(Flash_fwd_params &, cudaStream_t) [with T=cutlass::half_t]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_hdim160_fp16_sm80.cu(26): here

/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_kernel.h(276): warning: variable "tQrQ" was declared but never referenced
          detected during:
            instantiation of "void flash::compute_attn_1rowblock<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params,Is_Q_split_K>(const Params &, int, int, int) [with Kernel_traits=Flash_fwd_kernel_traits<160, 128, 64, 8, false, false, cutlass::half_t, Flash_kernel_traits<160, 128, 64, 8, cutlass::half_t>>, Is_dropout=true, Is_causal=false, Is_even_N=true, Is_even_K=false, Return_softmax=false, Params=Flash_fwd_params, Is_Q_split_K=false]" 
(589): here
            instantiation of "void flash::compute_attn<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &) [with Kernel_traits=Flash_fwd_kernel_traits<160, 128, 64, 8, false, false, cutlass::half_t, Flash_kernel_traits<160, 128, 64, 8, cutlass::half_t>>, Is_dropout=true, Is_causal=false, Is_even_N=true, Is_even_K=false, Return_softmax=false, Params=Flash_fwd_params]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(15): here
            instantiation of "void flash_fwd_kernel<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax>(Flash_fwd_params) [with Kernel_traits=Flash_fwd_kernel_traits<160, 128, 64, 8, false, false, cutlass::half_t, Flash_kernel_traits<160, 128, 64, 8, cutlass::half_t>>, Is_dropout=true, Is_causal=false, Is_even_N=true, Is_even_K=false, Return_softmax=false]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(34): here
            instantiation of "void run_flash_fwd<Kernel_traits,Is_dropout,Is_causal>(Flash_fwd_params &, cudaStream_t) [with Kernel_traits=Flash_fwd_kernel_traits<160, 128, 64, 8, false, false, cutlass::half_t, Flash_kernel_traits<160, 128, 64, 8, cutlass::half_t>>, Is_dropout=true, Is_causal=false]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(155): here
            instantiation of "void run_mha_fwd_hdim160<T>(Flash_fwd_params &, cudaStream_t) [with T=cutlass::half_t]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_hdim160_fp16_sm80.cu(26): here

/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_kernel.h(140): warning: variable "MMA_M" was declared but never referenced
          detected during:
            instantiation of "void flash::compute_attn_1rowblock<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params,Is_Q_split_K>(const Params &, int, int, int) [with Kernel_traits=Flash_fwd_kernel_traits<160, 128, 64, 8, false, false, cutlass::half_t, Flash_kernel_traits<160, 128, 64, 8, cutlass::half_t>>, Is_dropout=true, Is_causal=false, Is_even_N=false, Is_even_K=true, Return_softmax=true, Params=Flash_fwd_params, Is_Q_split_K=false]" 
(589): here
            instantiation of "void flash::compute_attn<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &) [with Kernel_traits=Flash_fwd_kernel_traits<160, 128, 64, 8, false, false, cutlass::half_t, Flash_kernel_traits<160, 128, 64, 8, cutlass::half_t>>, Is_dropout=true, Is_causal=false, Is_even_N=false, Is_even_K=true, Return_softmax=true, Params=Flash_fwd_params]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(15): here
            instantiation of "void flash_fwd_kernel<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax>(Flash_fwd_params) [with Kernel_traits=Flash_fwd_kernel_traits<160, 128, 64, 8, false, false, cutlass::half_t, Flash_kernel_traits<160, 128, 64, 8, cutlass::half_t>>, Is_dropout=true, Is_causal=false, Is_even_N=false, Is_even_K=true, Return_softmax=true]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(34): here
            instantiation of "void run_flash_fwd<Kernel_traits,Is_dropout,Is_causal>(Flash_fwd_params &, cudaStream_t) [with Kernel_traits=Flash_fwd_kernel_traits<160, 128, 64, 8, false, false, cutlass::half_t, Flash_kernel_traits<160, 128, 64, 8, cutlass::half_t>>, Is_dropout=true, Is_causal=false]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(155): here
            instantiation of "void run_mha_fwd_hdim160<T>(Flash_fwd_params &, cudaStream_t) [with T=cutlass::half_t]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_hdim160_fp16_sm80.cu(26): here

/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_kernel.h(276): warning: variable "tQrQ" was declared but never referenced
          detected during:
            instantiation of "void flash::compute_attn_1rowblock<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params,Is_Q_split_K>(const Params &, int, int, int) [with Kernel_traits=Flash_fwd_kernel_traits<160, 128, 64, 8, false, false, cutlass::half_t, Flash_kernel_traits<160, 128, 64, 8, cutlass::half_t>>, Is_dropout=true, Is_causal=false, Is_even_N=false, Is_even_K=true, Return_softmax=true, Params=Flash_fwd_params, Is_Q_split_K=false]" 
(589): here
            instantiation of "void flash::compute_attn<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &) [with Kernel_traits=Flash_fwd_kernel_traits<160, 128, 64, 8, false, false, cutlass::half_t, Flash_kernel_traits<160, 128, 64, 8, cutlass::half_t>>, Is_dropout=true, Is_causal=false, Is_even_N=false, Is_even_K=true, Return_softmax=true, Params=Flash_fwd_params]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(15): here
            instantiation of "void flash_fwd_kernel<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax>(Flash_fwd_params) [with Kernel_traits=Flash_fwd_kernel_traits<160, 128, 64, 8, false, false, cutlass::half_t, Flash_kernel_traits<160, 128, 64, 8, cutlass::half_t>>, Is_dropout=true, Is_causal=false, Is_even_N=false, Is_even_K=true, Return_softmax=true]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(34): here
            instantiation of "void run_flash_fwd<Kernel_traits,Is_dropout,Is_causal>(Flash_fwd_params &, cudaStream_t) [with Kernel_traits=Flash_fwd_kernel_traits<160, 128, 64, 8, false, false, cutlass::half_t, Flash_kernel_traits<160, 128, 64, 8, cutlass::half_t>>, Is_dropout=true, Is_causal=false]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(155): here
            instantiation of "void run_mha_fwd_hdim160<T>(Flash_fwd_params &, cudaStream_t) [with T=cutlass::half_t]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_hdim160_fp16_sm80.cu(26): here

/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_kernel.h(140): warning: variable "MMA_M" was declared but never referenced
          detected during:
            instantiation of "void flash::compute_attn_1rowblock<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params,Is_Q_split_K>(const Params &, int, int, int) [with Kernel_traits=Flash_fwd_kernel_traits<160, 128, 64, 8, false, false, cutlass::half_t, Flash_kernel_traits<160, 128, 64, 8, cutlass::half_t>>, Is_dropout=true, Is_causal=false, Is_even_N=false, Is_even_K=true, Return_softmax=false, Params=Flash_fwd_params, Is_Q_split_K=false]" 
(589): here
            instantiation of "void flash::compute_attn<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &) [with Kernel_traits=Flash_fwd_kernel_traits<160, 128, 64, 8, false, false, cutlass::half_t, Flash_kernel_traits<160, 128, 64, 8, cutlass::half_t>>, Is_dropout=true, Is_causal=false, Is_even_N=false, Is_even_K=true, Return_softmax=false, Params=Flash_fwd_params]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(15): here
            instantiation of "void flash_fwd_kernel<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax>(Flash_fwd_params) [with Kernel_traits=Flash_fwd_kernel_traits<160, 128, 64, 8, false, false, cutlass::half_t, Flash_kernel_traits<160, 128, 64, 8, cutlass::half_t>>, Is_dropout=true, Is_causal=false, Is_even_N=false, Is_even_K=true, Return_softmax=false]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(34): here
            instantiation of "void run_flash_fwd<Kernel_traits,Is_dropout,Is_causal>(Flash_fwd_params &, cudaStream_t) [with Kernel_traits=Flash_fwd_kernel_traits<160, 128, 64, 8, false, false, cutlass::half_t, Flash_kernel_traits<160, 128, 64, 8, cutlass::half_t>>, Is_dropout=true, Is_causal=false]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(155): here
            instantiation of "void run_mha_fwd_hdim160<T>(Flash_fwd_params &, cudaStream_t) [with T=cutlass::half_t]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_hdim160_fp16_sm80.cu(26): here

/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_kernel.h(276): warning: variable "tQrQ" was declared but never referenced
          detected during:
            instantiation of "void flash::compute_attn_1rowblock<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params,Is_Q_split_K>(const Params &, int, int, int) [with Kernel_traits=Flash_fwd_kernel_traits<160, 128, 64, 8, false, false, cutlass::half_t, Flash_kernel_traits<160, 128, 64, 8, cutlass::half_t>>, Is_dropout=true, Is_causal=false, Is_even_N=false, Is_even_K=true, Return_softmax=false, Params=Flash_fwd_params, Is_Q_split_K=false]" 
(589): here
            instantiation of "void flash::compute_attn<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &) [with Kernel_traits=Flash_fwd_kernel_traits<160, 128, 64, 8, false, false, cutlass::half_t, Flash_kernel_traits<160, 128, 64, 8, cutlass::half_t>>, Is_dropout=true, Is_causal=false, Is_even_N=false, Is_even_K=true, Return_softmax=false, Params=Flash_fwd_params]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(15): here
            instantiation of "void flash_fwd_kernel<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax>(Flash_fwd_params) [with Kernel_traits=Flash_fwd_kernel_traits<160, 128, 64, 8, false, false, cutlass::half_t, Flash_kernel_traits<160, 128, 64, 8, cutlass::half_t>>, Is_dropout=true, Is_causal=false, Is_even_N=false, Is_even_K=true, Return_softmax=false]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(34): here
            instantiation of "void run_flash_fwd<Kernel_traits,Is_dropout,Is_causal>(Flash_fwd_params &, cudaStream_t) [with Kernel_traits=Flash_fwd_kernel_traits<160, 128, 64, 8, false, false, cutlass::half_t, Flash_kernel_traits<160, 128, 64, 8, cutlass::half_t>>, Is_dropout=true, Is_causal=false]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(155): here
            instantiation of "void run_mha_fwd_hdim160<T>(Flash_fwd_params &, cudaStream_t) [with T=cutlass::half_t]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_hdim160_fp16_sm80.cu(26): here

/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_kernel.h(140): warning: variable "MMA_M" was declared but never referenced
          detected during:
            instantiation of "void flash::compute_attn_1rowblock<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params,Is_Q_split_K>(const Params &, int, int, int) [with Kernel_traits=Flash_fwd_kernel_traits<160, 128, 64, 8, false, false, cutlass::half_t, Flash_kernel_traits<160, 128, 64, 8, cutlass::half_t>>, Is_dropout=true, Is_causal=false, Is_even_N=false, Is_even_K=false, Return_softmax=true, Params=Flash_fwd_params, Is_Q_split_K=false]" 
(589): here
            instantiation of "void flash::compute_attn<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &) [with Kernel_traits=Flash_fwd_kernel_traits<160, 128, 64, 8, false, false, cutlass::half_t, Flash_kernel_traits<160, 128, 64, 8, cutlass::half_t>>, Is_dropout=true, Is_causal=false, Is_even_N=false, Is_even_K=false, Return_softmax=true, Params=Flash_fwd_params]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(15): here
            instantiation of "void flash_fwd_kernel<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax>(Flash_fwd_params) [with Kernel_traits=Flash_fwd_kernel_traits<160, 128, 64, 8, false, false, cutlass::half_t, Flash_kernel_traits<160, 128, 64, 8, cutlass::half_t>>, Is_dropout=true, Is_causal=false, Is_even_N=false, Is_even_K=false, Return_softmax=true]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(34): here
            instantiation of "void run_flash_fwd<Kernel_traits,Is_dropout,Is_causal>(Flash_fwd_params &, cudaStream_t) [with Kernel_traits=Flash_fwd_kernel_traits<160, 128, 64, 8, false, false, cutlass::half_t, Flash_kernel_traits<160, 128, 64, 8, cutlass::half_t>>, Is_dropout=true, Is_causal=false]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(155): here
            instantiation of "void run_mha_fwd_hdim160<T>(Flash_fwd_params &, cudaStream_t) [with T=cutlass::half_t]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_hdim160_fp16_sm80.cu(26): here

/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_kernel.h(276): warning: variable "tQrQ" was declared but never referenced
          detected during:
            instantiation of "void flash::compute_attn_1rowblock<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params,Is_Q_split_K>(const Params &, int, int, int) [with Kernel_traits=Flash_fwd_kernel_traits<160, 128, 64, 8, false, false, cutlass::half_t, Flash_kernel_traits<160, 128, 64, 8, cutlass::half_t>>, Is_dropout=true, Is_causal=false, Is_even_N=false, Is_even_K=false, Return_softmax=true, Params=Flash_fwd_params, Is_Q_split_K=false]" 
(589): here
            instantiation of "void flash::compute_attn<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &) [with Kernel_traits=Flash_fwd_kernel_traits<160, 128, 64, 8, false, false, cutlass::half_t, Flash_kernel_traits<160, 128, 64, 8, cutlass::half_t>>, Is_dropout=true, Is_causal=false, Is_even_N=false, Is_even_K=false, Return_softmax=true, Params=Flash_fwd_params]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(15): here
            instantiation of "void flash_fwd_kernel<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax>(Flash_fwd_params) [with Kernel_traits=Flash_fwd_kernel_traits<160, 128, 64, 8, false, false, cutlass::half_t, Flash_kernel_traits<160, 128, 64, 8, cutlass::half_t>>, Is_dropout=true, Is_causal=false, Is_even_N=false, Is_even_K=false, Return_softmax=true]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(34): here
            instantiation of "void run_flash_fwd<Kernel_traits,Is_dropout,Is_causal>(Flash_fwd_params &, cudaStream_t) [with Kernel_traits=Flash_fwd_kernel_traits<160, 128, 64, 8, false, false, cutlass::half_t, Flash_kernel_traits<160, 128, 64, 8, cutlass::half_t>>, Is_dropout=true, Is_causal=false]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(155): here
            instantiation of "void run_mha_fwd_hdim160<T>(Flash_fwd_params &, cudaStream_t) [with T=cutlass::half_t]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_hdim160_fp16_sm80.cu(26): here

/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_kernel.h(140): warning: variable "MMA_M" was declared but never referenced
          detected during:
            instantiation of "void flash::compute_attn_1rowblock<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params,Is_Q_split_K>(const Params &, int, int, int) [with Kernel_traits=Flash_fwd_kernel_traits<160, 128, 64, 8, false, false, cutlass::half_t, Flash_kernel_traits<160, 128, 64, 8, cutlass::half_t>>, Is_dropout=true, Is_causal=false, Is_even_N=false, Is_even_K=false, Return_softmax=false, Params=Flash_fwd_params, Is_Q_split_K=false]" 
(589): here
            instantiation of "void flash::compute_attn<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &) [with Kernel_traits=Flash_fwd_kernel_traits<160, 128, 64, 8, false, false, cutlass::half_t, Flash_kernel_traits<160, 128, 64, 8, cutlass::half_t>>, Is_dropout=true, Is_causal=false, Is_even_N=false, Is_even_K=false, Return_softmax=false, Params=Flash_fwd_params]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(15): here
            instantiation of "void flash_fwd_kernel<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax>(Flash_fwd_params) [with Kernel_traits=Flash_fwd_kernel_traits<160, 128, 64, 8, false, false, cutlass::half_t, Flash_kernel_traits<160, 128, 64, 8, cutlass::half_t>>, Is_dropout=true, Is_causal=false, Is_even_N=false, Is_even_K=false, Return_softmax=false]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(34): here
            instantiation of "void run_flash_fwd<Kernel_traits,Is_dropout,Is_causal>(Flash_fwd_params &, cudaStream_t) [with Kernel_traits=Flash_fwd_kernel_traits<160, 128, 64, 8, false, false, cutlass::half_t, Flash_kernel_traits<160, 128, 64, 8, cutlass::half_t>>, Is_dropout=true, Is_causal=false]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(155): here
            instantiation of "void run_mha_fwd_hdim160<T>(Flash_fwd_params &, cudaStream_t) [with T=cutlass::half_t]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_hdim160_fp16_sm80.cu(26): here

/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_kernel.h(276): warning: variable "tQrQ" was declared but never referenced
          detected during:
            instantiation of "void flash::compute_attn_1rowblock<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params,Is_Q_split_K>(const Params &, int, int, int) [with Kernel_traits=Flash_fwd_kernel_traits<160, 128, 64, 8, false, false, cutlass::half_t, Flash_kernel_traits<160, 128, 64, 8, cutlass::half_t>>, Is_dropout=true, Is_causal=false, Is_even_N=false, Is_even_K=false, Return_softmax=false, Params=Flash_fwd_params, Is_Q_split_K=false]" 
(589): here
            instantiation of "void flash::compute_attn<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &) [with Kernel_traits=Flash_fwd_kernel_traits<160, 128, 64, 8, false, false, cutlass::half_t, Flash_kernel_traits<160, 128, 64, 8, cutlass::half_t>>, Is_dropout=true, Is_causal=false, Is_even_N=false, Is_even_K=false, Return_softmax=false, Params=Flash_fwd_params]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(15): here
            instantiation of "void flash_fwd_kernel<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax>(Flash_fwd_params) [with Kernel_traits=Flash_fwd_kernel_traits<160, 128, 64, 8, false, false, cutlass::half_t, Flash_kernel_traits<160, 128, 64, 8, cutlass::half_t>>, Is_dropout=true, Is_causal=false, Is_even_N=false, Is_even_K=false, Return_softmax=false]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(34): here
            instantiation of "void run_flash_fwd<Kernel_traits,Is_dropout,Is_causal>(Flash_fwd_params &, cudaStream_t) [with Kernel_traits=Flash_fwd_kernel_traits<160, 128, 64, 8, false, false, cutlass::half_t, Flash_kernel_traits<160, 128, 64, 8, cutlass::half_t>>, Is_dropout=true, Is_causal=false]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(155): here
            instantiation of "void run_mha_fwd_hdim160<T>(Flash_fwd_params &, cudaStream_t) [with T=cutlass::half_t]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_hdim160_fp16_sm80.cu(26): here

/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_kernel.h(140): warning: variable "MMA_M" was declared but never referenced
          detected during:
            instantiation of "void flash::compute_attn_1rowblock<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params,Is_Q_split_K>(const Params &, int, int, int) [with Kernel_traits=Flash_fwd_kernel_traits<160, 128, 32, 4, false, false, cutlass::half_t, Flash_kernel_traits<160, 128, 32, 4, cutlass::half_t>>, Is_dropout=true, Is_causal=false, Is_even_N=true, Is_even_K=true, Return_softmax=true, Params=Flash_fwd_params, Is_Q_split_K=false]" 
(589): here
            instantiation of "void flash::compute_attn<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &) [with Kernel_traits=Flash_fwd_kernel_traits<160, 128, 32, 4, false, false, cutlass::half_t, Flash_kernel_traits<160, 128, 32, 4, cutlass::half_t>>, Is_dropout=true, Is_causal=false, Is_even_N=true, Is_even_K=true, Return_softmax=true, Params=Flash_fwd_params]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(15): here
            instantiation of "void flash_fwd_kernel<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax>(Flash_fwd_params) [with Kernel_traits=Flash_fwd_kernel_traits<160, 128, 32, 4, false, false, cutlass::half_t, Flash_kernel_traits<160, 128, 32, 4, cutlass::half_t>>, Is_dropout=true, Is_causal=false, Is_even_N=true, Is_even_K=true, Return_softmax=true]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(34): here
            instantiation of "void run_flash_fwd<Kernel_traits,Is_dropout,Is_causal>(Flash_fwd_params &, cudaStream_t) [with Kernel_traits=Flash_fwd_kernel_traits<160, 128, 32, 4, false, false, cutlass::half_t, Flash_kernel_traits<160, 128, 32, 4, cutlass::half_t>>, Is_dropout=true, Is_causal=false]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(155): here
            instantiation of "void run_mha_fwd_hdim160<T>(Flash_fwd_params &, cudaStream_t) [with T=cutlass::half_t]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_hdim160_fp16_sm80.cu(26): here

/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_kernel.h(276): warning: variable "tQrQ" was declared but never referenced
          detected during:
            instantiation of "void flash::compute_attn_1rowblock<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params,Is_Q_split_K>(const Params &, int, int, int) [with Kernel_traits=Flash_fwd_kernel_traits<160, 128, 32, 4, false, false, cutlass::half_t, Flash_kernel_traits<160, 128, 32, 4, cutlass::half_t>>, Is_dropout=true, Is_causal=false, Is_even_N=true, Is_even_K=true, Return_softmax=true, Params=Flash_fwd_params, Is_Q_split_K=false]" 
(589): here
            instantiation of "void flash::compute_attn<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &) [with Kernel_traits=Flash_fwd_kernel_traits<160, 128, 32, 4, false, false, cutlass::half_t, Flash_kernel_traits<160, 128, 32, 4, cutlass::half_t>>, Is_dropout=true, Is_causal=false, Is_even_N=true, Is_even_K=true, Return_softmax=true, Params=Flash_fwd_params]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(15): here
            instantiation of "void flash_fwd_kernel<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax>(Flash_fwd_params) [with Kernel_traits=Flash_fwd_kernel_traits<160, 128, 32, 4, false, false, cutlass::half_t, Flash_kernel_traits<160, 128, 32, 4, cutlass::half_t>>, Is_dropout=true, Is_causal=false, Is_even_N=true, Is_even_K=true, Return_softmax=true]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(34): here
            instantiation of "void run_flash_fwd<Kernel_traits,Is_dropout,Is_causal>(Flash_fwd_params &, cudaStream_t) [with Kernel_traits=Flash_fwd_kernel_traits<160, 128, 32, 4, false, false, cutlass::half_t, Flash_kernel_traits<160, 128, 32, 4, cutlass::half_t>>, Is_dropout=true, Is_causal=false]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(155): here
            instantiation of "void run_mha_fwd_hdim160<T>(Flash_fwd_params &, cudaStream_t) [with T=cutlass::half_t]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_hdim160_fp16_sm80.cu(26): here

/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_kernel.h(140): warning: variable "MMA_M" was declared but never referenced
          detected during:
            instantiation of "void flash::compute_attn_1rowblock<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params,Is_Q_split_K>(const Params &, int, int, int) [with Kernel_traits=Flash_fwd_kernel_traits<160, 128, 32, 4, false, false, cutlass::half_t, Flash_kernel_traits<160, 128, 32, 4, cutlass::half_t>>, Is_dropout=true, Is_causal=false, Is_even_N=true, Is_even_K=true, Return_softmax=false, Params=Flash_fwd_params, Is_Q_split_K=false]" 
(589): here
            instantiation of "void flash::compute_attn<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &) [with Kernel_traits=Flash_fwd_kernel_traits<160, 128, 32, 4, false, false, cutlass::half_t, Flash_kernel_traits<160, 128, 32, 4, cutlass::half_t>>, Is_dropout=true, Is_causal=false, Is_even_N=true, Is_even_K=true, Return_softmax=false, Params=Flash_fwd_params]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(15): here
            instantiation of "void flash_fwd_kernel<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax>(Flash_fwd_params) [with Kernel_traits=Flash_fwd_kernel_traits<160, 128, 32, 4, false, false, cutlass::half_t, Flash_kernel_traits<160, 128, 32, 4, cutlass::half_t>>, Is_dropout=true, Is_causal=false, Is_even_N=true, Is_even_K=true, Return_softmax=false]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(34): here
            instantiation of "void run_flash_fwd<Kernel_traits,Is_dropout,Is_causal>(Flash_fwd_params &, cudaStream_t) [with Kernel_traits=Flash_fwd_kernel_traits<160, 128, 32, 4, false, false, cutlass::half_t, Flash_kernel_traits<160, 128, 32, 4, cutlass::half_t>>, Is_dropout=true, Is_causal=false]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(155): here
            instantiation of "void run_mha_fwd_hdim160<T>(Flash_fwd_params &, cudaStream_t) [with T=cutlass::half_t]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_hdim160_fp16_sm80.cu(26): here

/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_kernel.h(276): warning: variable "tQrQ" was declared but never referenced
          detected during:
            instantiation of "void flash::compute_attn_1rowblock<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params,Is_Q_split_K>(const Params &, int, int, int) [with Kernel_traits=Flash_fwd_kernel_traits<160, 128, 32, 4, false, false, cutlass::half_t, Flash_kernel_traits<160, 128, 32, 4, cutlass::half_t>>, Is_dropout=true, Is_causal=false, Is_even_N=true, Is_even_K=true, Return_softmax=false, Params=Flash_fwd_params, Is_Q_split_K=false]" 
(589): here
            instantiation of "void flash::compute_attn<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &) [with Kernel_traits=Flash_fwd_kernel_traits<160, 128, 32, 4, false, false, cutlass::half_t, Flash_kernel_traits<160, 128, 32, 4, cutlass::half_t>>, Is_dropout=true, Is_causal=false, Is_even_N=true, Is_even_K=true, Return_softmax=false, Params=Flash_fwd_params]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(15): here
            instantiation of "void flash_fwd_kernel<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax>(Flash_fwd_params) [with Kernel_traits=Flash_fwd_kernel_traits<160, 128, 32, 4, false, false, cutlass::half_t, Flash_kernel_traits<160, 128, 32, 4, cutlass::half_t>>, Is_dropout=true, Is_causal=false, Is_even_N=true, Is_even_K=true, Return_softmax=false]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(34): here
            instantiation of "void run_flash_fwd<Kernel_traits,Is_dropout,Is_causal>(Flash_fwd_params &, cudaStream_t) [with Kernel_traits=Flash_fwd_kernel_traits<160, 128, 32, 4, false, false, cutlass::half_t, Flash_kernel_traits<160, 128, 32, 4, cutlass::half_t>>, Is_dropout=true, Is_causal=false]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(155): here
            instantiation of "void run_mha_fwd_hdim160<T>(Flash_fwd_params &, cudaStream_t) [with T=cutlass::half_t]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_hdim160_fp16_sm80.cu(26): here

/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_kernel.h(140): warning: variable "MMA_M" was declared but never referenced
          detected during:
            instantiation of "void flash::compute_attn_1rowblock<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params,Is_Q_split_K>(const Params &, int, int, int) [with Kernel_traits=Flash_fwd_kernel_traits<160, 128, 32, 4, false, false, cutlass::half_t, Flash_kernel_traits<160, 128, 32, 4, cutlass::half_t>>, Is_dropout=true, Is_causal=false, Is_even_N=true, Is_even_K=false, Return_softmax=true, Params=Flash_fwd_params, Is_Q_split_K=false]" 
(589): here
            instantiation of "void flash::compute_attn<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &) [with Kernel_traits=Flash_fwd_kernel_traits<160, 128, 32, 4, false, false, cutlass::half_t, Flash_kernel_traits<160, 128, 32, 4, cutlass::half_t>>, Is_dropout=true, Is_causal=false, Is_even_N=true, Is_even_K=false, Return_softmax=true, Params=Flash_fwd_params]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(15): here
            instantiation of "void flash_fwd_kernel<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax>(Flash_fwd_params) [with Kernel_traits=Flash_fwd_kernel_traits<160, 128, 32, 4, false, false, cutlass::half_t, Flash_kernel_traits<160, 128, 32, 4, cutlass::half_t>>, Is_dropout=true, Is_causal=false, Is_even_N=true, Is_even_K=false, Return_softmax=true]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(34): here
            instantiation of "void run_flash_fwd<Kernel_traits,Is_dropout,Is_causal>(Flash_fwd_params &, cudaStream_t) [with Kernel_traits=Flash_fwd_kernel_traits<160, 128, 32, 4, false, false, cutlass::half_t, Flash_kernel_traits<160, 128, 32, 4, cutlass::half_t>>, Is_dropout=true, Is_causal=false]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(155): here
            instantiation of "void run_mha_fwd_hdim160<T>(Flash_fwd_params &, cudaStream_t) [with T=cutlass::half_t]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_hdim160_fp16_sm80.cu(26): here

/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_kernel.h(276): warning: variable "tQrQ" was declared but never referenced
          detected during:
            instantiation of "void flash::compute_attn_1rowblock<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params,Is_Q_split_K>(const Params &, int, int, int) [with Kernel_traits=Flash_fwd_kernel_traits<160, 128, 32, 4, false, false, cutlass::half_t, Flash_kernel_traits<160, 128, 32, 4, cutlass::half_t>>, Is_dropout=true, Is_causal=false, Is_even_N=true, Is_even_K=false, Return_softmax=true, Params=Flash_fwd_params, Is_Q_split_K=false]" 
(589): here
            instantiation of "void flash::compute_attn<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &) [with Kernel_traits=Flash_fwd_kernel_traits<160, 128, 32, 4, false, false, cutlass::half_t, Flash_kernel_traits<160, 128, 32, 4, cutlass::half_t>>, Is_dropout=true, Is_causal=false, Is_even_N=true, Is_even_K=false, Return_softmax=true, Params=Flash_fwd_params]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(15): here
            instantiation of "void flash_fwd_kernel<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax>(Flash_fwd_params) [with Kernel_traits=Flash_fwd_kernel_traits<160, 128, 32, 4, false, false, cutlass::half_t, Flash_kernel_traits<160, 128, 32, 4, cutlass::half_t>>, Is_dropout=true, Is_causal=false, Is_even_N=true, Is_even_K=false, Return_softmax=true]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(34): here
            instantiation of "void run_flash_fwd<Kernel_traits,Is_dropout,Is_causal>(Flash_fwd_params &, cudaStream_t) [with Kernel_traits=Flash_fwd_kernel_traits<160, 128, 32, 4, false, false, cutlass::half_t, Flash_kernel_traits<160, 128, 32, 4, cutlass::half_t>>, Is_dropout=true, Is_causal=false]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(155): here
            instantiation of "void run_mha_fwd_hdim160<T>(Flash_fwd_params &, cudaStream_t) [with T=cutlass::half_t]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_hdim160_fp16_sm80.cu(26): here

/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_kernel.h(140): warning: variable "MMA_M" was declared but never referenced
          detected during:
            instantiation of "void flash::compute_attn_1rowblock<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params,Is_Q_split_K>(const Params &, int, int, int) [with Kernel_traits=Flash_fwd_kernel_traits<160, 128, 32, 4, false, false, cutlass::half_t, Flash_kernel_traits<160, 128, 32, 4, cutlass::half_t>>, Is_dropout=true, Is_causal=false, Is_even_N=true, Is_even_K=false, Return_softmax=false, Params=Flash_fwd_params, Is_Q_split_K=false]" 
(589): here
            instantiation of "void flash::compute_attn<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &) [with Kernel_traits=Flash_fwd_kernel_traits<160, 128, 32, 4, false, false, cutlass::half_t, Flash_kernel_traits<160, 128, 32, 4, cutlass::half_t>>, Is_dropout=true, Is_causal=false, Is_even_N=true, Is_even_K=false, Return_softmax=false, Params=Flash_fwd_params]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(15): here
            instantiation of "void flash_fwd_kernel<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax>(Flash_fwd_params) [with Kernel_traits=Flash_fwd_kernel_traits<160, 128, 32, 4, false, false, cutlass::half_t, Flash_kernel_traits<160, 128, 32, 4, cutlass::half_t>>, Is_dropout=true, Is_causal=false, Is_even_N=true, Is_even_K=false, Return_softmax=false]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(34): here
            instantiation of "void run_flash_fwd<Kernel_traits,Is_dropout,Is_causal>(Flash_fwd_params &, cudaStream_t) [with Kernel_traits=Flash_fwd_kernel_traits<160, 128, 32, 4, false, false, cutlass::half_t, Flash_kernel_traits<160, 128, 32, 4, cutlass::half_t>>, Is_dropout=true, Is_causal=false]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(155): here
            instantiation of "void run_mha_fwd_hdim160<T>(Flash_fwd_params &, cudaStream_t) [with T=cutlass::half_t]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_hdim160_fp16_sm80.cu(26): here

/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_kernel.h(276): warning: variable "tQrQ" was declared but never referenced
          detected during:
            instantiation of "void flash::compute_attn_1rowblock<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params,Is_Q_split_K>(const Params &, int, int, int) [with Kernel_traits=Flash_fwd_kernel_traits<160, 128, 32, 4, false, false, cutlass::half_t, Flash_kernel_traits<160, 128, 32, 4, cutlass::half_t>>, Is_dropout=true, Is_causal=false, Is_even_N=true, Is_even_K=false, Return_softmax=false, Params=Flash_fwd_params, Is_Q_split_K=false]" 
(589): here
            instantiation of "void flash::compute_attn<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &) [with Kernel_traits=Flash_fwd_kernel_traits<160, 128, 32, 4, false, false, cutlass::half_t, Flash_kernel_traits<160, 128, 32, 4, cutlass::half_t>>, Is_dropout=true, Is_causal=false, Is_even_N=true, Is_even_K=false, Return_softmax=false, Params=Flash_fwd_params]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(15): here
            instantiation of "void flash_fwd_kernel<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax>(Flash_fwd_params) [with Kernel_traits=Flash_fwd_kernel_traits<160, 128, 32, 4, false, false, cutlass::half_t, Flash_kernel_traits<160, 128, 32, 4, cutlass::half_t>>, Is_dropout=true, Is_causal=false, Is_even_N=true, Is_even_K=false, Return_softmax=false]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(34): here
            instantiation of "void run_flash_fwd<Kernel_traits,Is_dropout,Is_causal>(Flash_fwd_params &, cudaStream_t) [with Kernel_traits=Flash_fwd_kernel_traits<160, 128, 32, 4, false, false, cutlass::half_t, Flash_kernel_traits<160, 128, 32, 4, cutlass::half_t>>, Is_dropout=true, Is_causal=false]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(155): here
            instantiation of "void run_mha_fwd_hdim160<T>(Flash_fwd_params &, cudaStream_t) [with T=cutlass::half_t]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_hdim160_fp16_sm80.cu(26): here

/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_kernel.h(140): warning: variable "MMA_M" was declared but never referenced
          detected during:
            instantiation of "void flash::compute_attn_1rowblock<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params,Is_Q_split_K>(const Params &, int, int, int) [with Kernel_traits=Flash_fwd_kernel_traits<160, 128, 32, 4, false, false, cutlass::half_t, Flash_kernel_traits<160, 128, 32, 4, cutlass::half_t>>, Is_dropout=true, Is_causal=false, Is_even_N=false, Is_even_K=true, Return_softmax=true, Params=Flash_fwd_params, Is_Q_split_K=false]" 
(589): here
            instantiation of "void flash::compute_attn<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &) [with Kernel_traits=Flash_fwd_kernel_traits<160, 128, 32, 4, false, false, cutlass::half_t, Flash_kernel_traits<160, 128, 32, 4, cutlass::half_t>>, Is_dropout=true, Is_causal=false, Is_even_N=false, Is_even_K=true, Return_softmax=true, Params=Flash_fwd_params]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(15): here
            instantiation of "void flash_fwd_kernel<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax>(Flash_fwd_params) [with Kernel_traits=Flash_fwd_kernel_traits<160, 128, 32, 4, false, false, cutlass::half_t, Flash_kernel_traits<160, 128, 32, 4, cutlass::half_t>>, Is_dropout=true, Is_causal=false, Is_even_N=false, Is_even_K=true, Return_softmax=true]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(34): here
            instantiation of "void run_flash_fwd<Kernel_traits,Is_dropout,Is_causal>(Flash_fwd_params &, cudaStream_t) [with Kernel_traits=Flash_fwd_kernel_traits<160, 128, 32, 4, false, false, cutlass::half_t, Flash_kernel_traits<160, 128, 32, 4, cutlass::half_t>>, Is_dropout=true, Is_causal=false]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(155): here
            instantiation of "void run_mha_fwd_hdim160<T>(Flash_fwd_params &, cudaStream_t) [with T=cutlass::half_t]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_hdim160_fp16_sm80.cu(26): here

/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_kernel.h(276): warning: variable "tQrQ" was declared but never referenced
          detected during:
            instantiation of "void flash::compute_attn_1rowblock<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params,Is_Q_split_K>(const Params &, int, int, int) [with Kernel_traits=Flash_fwd_kernel_traits<160, 128, 32, 4, false, false, cutlass::half_t, Flash_kernel_traits<160, 128, 32, 4, cutlass::half_t>>, Is_dropout=true, Is_causal=false, Is_even_N=false, Is_even_K=true, Return_softmax=true, Params=Flash_fwd_params, Is_Q_split_K=false]" 
(589): here
            instantiation of "void flash::compute_attn<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &) [with Kernel_traits=Flash_fwd_kernel_traits<160, 128, 32, 4, false, false, cutlass::half_t, Flash_kernel_traits<160, 128, 32, 4, cutlass::half_t>>, Is_dropout=true, Is_causal=false, Is_even_N=false, Is_even_K=true, Return_softmax=true, Params=Flash_fwd_params]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(15): here
            instantiation of "void flash_fwd_kernel<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax>(Flash_fwd_params) [with Kernel_traits=Flash_fwd_kernel_traits<160, 128, 32, 4, false, false, cutlass::half_t, Flash_kernel_traits<160, 128, 32, 4, cutlass::half_t>>, Is_dropout=true, Is_causal=false, Is_even_N=false, Is_even_K=true, Return_softmax=true]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(34): here
            instantiation of "void run_flash_fwd<Kernel_traits,Is_dropout,Is_causal>(Flash_fwd_params &, cudaStream_t) [with Kernel_traits=Flash_fwd_kernel_traits<160, 128, 32, 4, false, false, cutlass::half_t, Flash_kernel_traits<160, 128, 32, 4, cutlass::half_t>>, Is_dropout=true, Is_causal=false]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(155): here
            instantiation of "void run_mha_fwd_hdim160<T>(Flash_fwd_params &, cudaStream_t) [with T=cutlass::half_t]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_hdim160_fp16_sm80.cu(26): here

/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_kernel.h(140): warning: variable "MMA_M" was declared but never referenced
          detected during:
            instantiation of "void flash::compute_attn_1rowblock<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params,Is_Q_split_K>(const Params &, int, int, int) [with Kernel_traits=Flash_fwd_kernel_traits<160, 128, 32, 4, false, false, cutlass::half_t, Flash_kernel_traits<160, 128, 32, 4, cutlass::half_t>>, Is_dropout=true, Is_causal=false, Is_even_N=false, Is_even_K=true, Return_softmax=false, Params=Flash_fwd_params, Is_Q_split_K=false]" 
(589): here
            instantiation of "void flash::compute_attn<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &) [with Kernel_traits=Flash_fwd_kernel_traits<160, 128, 32, 4, false, false, cutlass::half_t, Flash_kernel_traits<160, 128, 32, 4, cutlass::half_t>>, Is_dropout=true, Is_causal=false, Is_even_N=false, Is_even_K=true, Return_softmax=false, Params=Flash_fwd_params]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(15): here
            instantiation of "void flash_fwd_kernel<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax>(Flash_fwd_params) [with Kernel_traits=Flash_fwd_kernel_traits<160, 128, 32, 4, false, false, cutlass::half_t, Flash_kernel_traits<160, 128, 32, 4, cutlass::half_t>>, Is_dropout=true, Is_causal=false, Is_even_N=false, Is_even_K=true, Return_softmax=false]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(34): here
            instantiation of "void run_flash_fwd<Kernel_traits,Is_dropout,Is_causal>(Flash_fwd_params &, cudaStream_t) [with Kernel_traits=Flash_fwd_kernel_traits<160, 128, 32, 4, false, false, cutlass::half_t, Flash_kernel_traits<160, 128, 32, 4, cutlass::half_t>>, Is_dropout=true, Is_causal=false]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(155): here
            instantiation of "void run_mha_fwd_hdim160<T>(Flash_fwd_params &, cudaStream_t) [with T=cutlass::half_t]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_hdim160_fp16_sm80.cu(26): here

/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_kernel.h(276): warning: variable "tQrQ" was declared but never referenced
          detected during:
            instantiation of "void flash::compute_attn_1rowblock<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params,Is_Q_split_K>(const Params &, int, int, int) [with Kernel_traits=Flash_fwd_kernel_traits<160, 128, 32, 4, false, false, cutlass::half_t, Flash_kernel_traits<160, 128, 32, 4, cutlass::half_t>>, Is_dropout=true, Is_causal=false, Is_even_N=false, Is_even_K=true, Return_softmax=false, Params=Flash_fwd_params, Is_Q_split_K=false]" 
(589): here
            instantiation of "void flash::compute_attn<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &) [with Kernel_traits=Flash_fwd_kernel_traits<160, 128, 32, 4, false, false, cutlass::half_t, Flash_kernel_traits<160, 128, 32, 4, cutlass::half_t>>, Is_dropout=true, Is_causal=false, Is_even_N=false, Is_even_K=true, Return_softmax=false, Params=Flash_fwd_params]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(15): here
            instantiation of "void flash_fwd_kernel<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax>(Flash_fwd_params) [with Kernel_traits=Flash_fwd_kernel_traits<160, 128, 32, 4, false, false, cutlass::half_t, Flash_kernel_traits<160, 128, 32, 4, cutlass::half_t>>, Is_dropout=true, Is_causal=false, Is_even_N=false, Is_even_K=true, Return_softmax=false]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(34): here
            instantiation of "void run_flash_fwd<Kernel_traits,Is_dropout,Is_causal>(Flash_fwd_params &, cudaStream_t) [with Kernel_traits=Flash_fwd_kernel_traits<160, 128, 32, 4, false, false, cutlass::half_t, Flash_kernel_traits<160, 128, 32, 4, cutlass::half_t>>, Is_dropout=true, Is_causal=false]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(155): here
            instantiation of "void run_mha_fwd_hdim160<T>(Flash_fwd_params &, cudaStream_t) [with T=cutlass::half_t]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_hdim160_fp16_sm80.cu(26): here

/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_kernel.h(140): warning: variable "MMA_M" was declared but never referenced
          detected during:
            instantiation of "void flash::compute_attn_1rowblock<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params,Is_Q_split_K>(const Params &, int, int, int) [with Kernel_traits=Flash_fwd_kernel_traits<160, 128, 32, 4, false, false, cutlass::half_t, Flash_kernel_traits<160, 128, 32, 4, cutlass::half_t>>, Is_dropout=true, Is_causal=false, Is_even_N=false, Is_even_K=false, Return_softmax=true, Params=Flash_fwd_params, Is_Q_split_K=false]" 
(589): here
            instantiation of "void flash::compute_attn<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &) [with Kernel_traits=Flash_fwd_kernel_traits<160, 128, 32, 4, false, false, cutlass::half_t, Flash_kernel_traits<160, 128, 32, 4, cutlass::half_t>>, Is_dropout=true, Is_causal=false, Is_even_N=false, Is_even_K=false, Return_softmax=true, Params=Flash_fwd_params]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(15): here
            instantiation of "void flash_fwd_kernel<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax>(Flash_fwd_params) [with Kernel_traits=Flash_fwd_kernel_traits<160, 128, 32, 4, false, false, cutlass::half_t, Flash_kernel_traits<160, 128, 32, 4, cutlass::half_t>>, Is_dropout=true, Is_causal=false, Is_even_N=false, Is_even_K=false, Return_softmax=true]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(34): here
            instantiation of "void run_flash_fwd<Kernel_traits,Is_dropout,Is_causal>(Flash_fwd_params &, cudaStream_t) [with Kernel_traits=Flash_fwd_kernel_traits<160, 128, 32, 4, false, false, cutlass::half_t, Flash_kernel_traits<160, 128, 32, 4, cutlass::half_t>>, Is_dropout=true, Is_causal=false]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(155): here
            instantiation of "void run_mha_fwd_hdim160<T>(Flash_fwd_params &, cudaStream_t) [with T=cutlass::half_t]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_hdim160_fp16_sm80.cu(26): here

/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_kernel.h(276): warning: variable "tQrQ" was declared but never referenced
          detected during:
            instantiation of "void flash::compute_attn_1rowblock<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params,Is_Q_split_K>(const Params &, int, int, int) [with Kernel_traits=Flash_fwd_kernel_traits<160, 128, 32, 4, false, false, cutlass::half_t, Flash_kernel_traits<160, 128, 32, 4, cutlass::half_t>>, Is_dropout=true, Is_causal=false, Is_even_N=false, Is_even_K=false, Return_softmax=true, Params=Flash_fwd_params, Is_Q_split_K=false]" 
(589): here
            instantiation of "void flash::compute_attn<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &) [with Kernel_traits=Flash_fwd_kernel_traits<160, 128, 32, 4, false, false, cutlass::half_t, Flash_kernel_traits<160, 128, 32, 4, cutlass::half_t>>, Is_dropout=true, Is_causal=false, Is_even_N=false, Is_even_K=false, Return_softmax=true, Params=Flash_fwd_params]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(15): here
            instantiation of "void flash_fwd_kernel<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax>(Flash_fwd_params) [with Kernel_traits=Flash_fwd_kernel_traits<160, 128, 32, 4, false, false, cutlass::half_t, Flash_kernel_traits<160, 128, 32, 4, cutlass::half_t>>, Is_dropout=true, Is_causal=false, Is_even_N=false, Is_even_K=false, Return_softmax=true]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(34): here
            instantiation of "void run_flash_fwd<Kernel_traits,Is_dropout,Is_causal>(Flash_fwd_params &, cudaStream_t) [with Kernel_traits=Flash_fwd_kernel_traits<160, 128, 32, 4, false, false, cutlass::half_t, Flash_kernel_traits<160, 128, 32, 4, cutlass::half_t>>, Is_dropout=true, Is_causal=false]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(155): here
            instantiation of "void run_mha_fwd_hdim160<T>(Flash_fwd_params &, cudaStream_t) [with T=cutlass::half_t]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_hdim160_fp16_sm80.cu(26): here

/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_kernel.h(140): warning: variable "MMA_M" was declared but never referenced
          detected during:
            instantiation of "void flash::compute_attn_1rowblock<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params,Is_Q_split_K>(const Params &, int, int, int) [with Kernel_traits=Flash_fwd_kernel_traits<160, 128, 32, 4, false, false, cutlass::half_t, Flash_kernel_traits<160, 128, 32, 4, cutlass::half_t>>, Is_dropout=true, Is_causal=false, Is_even_N=false, Is_even_K=false, Return_softmax=false, Params=Flash_fwd_params, Is_Q_split_K=false]" 
(589): here
            instantiation of "void flash::compute_attn<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &) [with Kernel_traits=Flash_fwd_kernel_traits<160, 128, 32, 4, false, false, cutlass::half_t, Flash_kernel_traits<160, 128, 32, 4, cutlass::half_t>>, Is_dropout=true, Is_causal=false, Is_even_N=false, Is_even_K=false, Return_softmax=false, Params=Flash_fwd_params]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(15): here
            instantiation of "void flash_fwd_kernel<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax>(Flash_fwd_params) [with Kernel_traits=Flash_fwd_kernel_traits<160, 128, 32, 4, false, false, cutlass::half_t, Flash_kernel_traits<160, 128, 32, 4, cutlass::half_t>>, Is_dropout=true, Is_causal=false, Is_even_N=false, Is_even_K=false, Return_softmax=false]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(34): here
            instantiation of "void run_flash_fwd<Kernel_traits,Is_dropout,Is_causal>(Flash_fwd_params &, cudaStream_t) [with Kernel_traits=Flash_fwd_kernel_traits<160, 128, 32, 4, false, false, cutlass::half_t, Flash_kernel_traits<160, 128, 32, 4, cutlass::half_t>>, Is_dropout=true, Is_causal=false]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(155): here
            instantiation of "void run_mha_fwd_hdim160<T>(Flash_fwd_params &, cudaStream_t) [with T=cutlass::half_t]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_hdim160_fp16_sm80.cu(26): here

/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_kernel.h(276): warning: variable "tQrQ" was declared but never referenced
          detected during:
            instantiation of "void flash::compute_attn_1rowblock<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params,Is_Q_split_K>(const Params &, int, int, int) [with Kernel_traits=Flash_fwd_kernel_traits<160, 128, 32, 4, false, false, cutlass::half_t, Flash_kernel_traits<160, 128, 32, 4, cutlass::half_t>>, Is_dropout=true, Is_causal=false, Is_even_N=false, Is_even_K=false, Return_softmax=false, Params=Flash_fwd_params, Is_Q_split_K=false]" 
(589): here
            instantiation of "void flash::compute_attn<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &) [with Kernel_traits=Flash_fwd_kernel_traits<160, 128, 32, 4, false, false, cutlass::half_t, Flash_kernel_traits<160, 128, 32, 4, cutlass::half_t>>, Is_dropout=true, Is_causal=false, Is_even_N=false, Is_even_K=false, Return_softmax=false, Params=Flash_fwd_params]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(15): here
            instantiation of "void flash_fwd_kernel<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax>(Flash_fwd_params) [with Kernel_traits=Flash_fwd_kernel_traits<160, 128, 32, 4, false, false, cutlass::half_t, Flash_kernel_traits<160, 128, 32, 4, cutlass::half_t>>, Is_dropout=true, Is_causal=false, Is_even_N=false, Is_even_K=false, Return_softmax=false]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(34): here
            instantiation of "void run_flash_fwd<Kernel_traits,Is_dropout,Is_causal>(Flash_fwd_params &, cudaStream_t) [with Kernel_traits=Flash_fwd_kernel_traits<160, 128, 32, 4, false, false, cutlass::half_t, Flash_kernel_traits<160, 128, 32, 4, cutlass::half_t>>, Is_dropout=true, Is_causal=false]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(155): here
            instantiation of "void run_mha_fwd_hdim160<T>(Flash_fwd_params &, cudaStream_t) [with T=cutlass::half_t]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_hdim160_fp16_sm80.cu(26): here

/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_kernel.h(140): warning: variable "MMA_M" was declared but never referenced
          detected during:
            instantiation of "void flash::compute_attn_1rowblock<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params,Is_Q_split_K>(const Params &, int, int, int) [with Kernel_traits=Flash_fwd_kernel_traits<160, 64, 64, 4, false, false, cutlass::half_t, Flash_kernel_traits<160, 64, 64, 4, cutlass::half_t>>, Is_dropout=false, Is_causal=true, Is_even_N=true, Is_even_K=true, Return_softmax=false, Params=Flash_fwd_params, Is_Q_split_K=false]" 
(589): here
            instantiation of "void flash::compute_attn<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &) [with Kernel_traits=Flash_fwd_kernel_traits<160, 64, 64, 4, false, false, cutlass::half_t, Flash_kernel_traits<160, 64, 64, 4, cutlass::half_t>>, Is_dropout=false, Is_causal=true, Is_even_N=true, Is_even_K=true, Return_softmax=false, Params=Flash_fwd_params]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(15): here
            instantiation of "void flash_fwd_kernel<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax>(Flash_fwd_params) [with Kernel_traits=Flash_fwd_kernel_traits<160, 64, 64, 4, false, false, cutlass::half_t, Flash_kernel_traits<160, 64, 64, 4, cutlass::half_t>>, Is_dropout=false, Is_causal=true, Is_even_N=true, Is_even_K=true, Return_softmax=false]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(34): here
            instantiation of "void run_flash_fwd<Kernel_traits,Is_dropout,Is_causal>(Flash_fwd_params &, cudaStream_t) [with Kernel_traits=Flash_fwd_kernel_traits<160, 64, 64, 4, false, false, cutlass::half_t, Flash_kernel_traits<160, 64, 64, 4, cutlass::half_t>>, Is_dropout=false, Is_causal=true]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(155): here
            instantiation of "void run_mha_fwd_hdim160<T>(Flash_fwd_params &, cudaStream_t) [with T=cutlass::half_t]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_hdim160_fp16_sm80.cu(26): here

/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_kernel.h(276): warning: variable "tQrQ" was declared but never referenced
          detected during:
            instantiation of "void flash::compute_attn_1rowblock<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params,Is_Q_split_K>(const Params &, int, int, int) [with Kernel_traits=Flash_fwd_kernel_traits<160, 64, 64, 4, false, false, cutlass::half_t, Flash_kernel_traits<160, 64, 64, 4, cutlass::half_t>>, Is_dropout=false, Is_causal=true, Is_even_N=true, Is_even_K=true, Return_softmax=false, Params=Flash_fwd_params, Is_Q_split_K=false]" 
(589): here
            instantiation of "void flash::compute_attn<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &) [with Kernel_traits=Flash_fwd_kernel_traits<160, 64, 64, 4, false, false, cutlass::half_t, Flash_kernel_traits<160, 64, 64, 4, cutlass::half_t>>, Is_dropout=false, Is_causal=true, Is_even_N=true, Is_even_K=true, Return_softmax=false, Params=Flash_fwd_params]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(15): here
            instantiation of "void flash_fwd_kernel<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax>(Flash_fwd_params) [with Kernel_traits=Flash_fwd_kernel_traits<160, 64, 64, 4, false, false, cutlass::half_t, Flash_kernel_traits<160, 64, 64, 4, cutlass::half_t>>, Is_dropout=false, Is_causal=true, Is_even_N=true, Is_even_K=true, Return_softmax=false]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(34): here
            instantiation of "void run_flash_fwd<Kernel_traits,Is_dropout,Is_causal>(Flash_fwd_params &, cudaStream_t) [with Kernel_traits=Flash_fwd_kernel_traits<160, 64, 64, 4, false, false, cutlass::half_t, Flash_kernel_traits<160, 64, 64, 4, cutlass::half_t>>, Is_dropout=false, Is_causal=true]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(155): here
            instantiation of "void run_mha_fwd_hdim160<T>(Flash_fwd_params &, cudaStream_t) [with T=cutlass::half_t]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_hdim160_fp16_sm80.cu(26): here

/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_kernel.h(140): warning: variable "MMA_M" was declared but never referenced
          detected during:
            instantiation of "void flash::compute_attn_1rowblock<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params,Is_Q_split_K>(const Params &, int, int, int) [with Kernel_traits=Flash_fwd_kernel_traits<160, 64, 64, 4, false, false, cutlass::half_t, Flash_kernel_traits<160, 64, 64, 4, cutlass::half_t>>, Is_dropout=false, Is_causal=true, Is_even_N=true, Is_even_K=false, Return_softmax=false, Params=Flash_fwd_params, Is_Q_split_K=false]" 
(589): here
            instantiation of "void flash::compute_attn<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &) [with Kernel_traits=Flash_fwd_kernel_traits<160, 64, 64, 4, false, false, cutlass::half_t, Flash_kernel_traits<160, 64, 64, 4, cutlass::half_t>>, Is_dropout=false, Is_causal=true, Is_even_N=true, Is_even_K=false, Return_softmax=false, Params=Flash_fwd_params]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(15): here
            instantiation of "void flash_fwd_kernel<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax>(Flash_fwd_params) [with Kernel_traits=Flash_fwd_kernel_traits<160, 64, 64, 4, false, false, cutlass::half_t, Flash_kernel_traits<160, 64, 64, 4, cutlass::half_t>>, Is_dropout=false, Is_causal=true, Is_even_N=true, Is_even_K=false, Return_softmax=false]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(34): here
            instantiation of "void run_flash_fwd<Kernel_traits,Is_dropout,Is_causal>(Flash_fwd_params &, cudaStream_t) [with Kernel_traits=Flash_fwd_kernel_traits<160, 64, 64, 4, false, false, cutlass::half_t, Flash_kernel_traits<160, 64, 64, 4, cutlass::half_t>>, Is_dropout=false, Is_causal=true]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(155): here
            instantiation of "void run_mha_fwd_hdim160<T>(Flash_fwd_params &, cudaStream_t) [with T=cutlass::half_t]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_hdim160_fp16_sm80.cu(26): here

/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_kernel.h(276): warning: variable "tQrQ" was declared but never referenced
          detected during:
            instantiation of "void flash::compute_attn_1rowblock<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params,Is_Q_split_K>(const Params &, int, int, int) [with Kernel_traits=Flash_fwd_kernel_traits<160, 64, 64, 4, false, false, cutlass::half_t, Flash_kernel_traits<160, 64, 64, 4, cutlass::half_t>>, Is_dropout=false, Is_causal=true, Is_even_N=true, Is_even_K=false, Return_softmax=false, Params=Flash_fwd_params, Is_Q_split_K=false]" 
(589): here
            instantiation of "void flash::compute_attn<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &) [with Kernel_traits=Flash_fwd_kernel_traits<160, 64, 64, 4, false, false, cutlass::half_t, Flash_kernel_traits<160, 64, 64, 4, cutlass::half_t>>, Is_dropout=false, Is_causal=true, Is_even_N=true, Is_even_K=false, Return_softmax=false, Params=Flash_fwd_params]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(15): here
            instantiation of "void flash_fwd_kernel<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax>(Flash_fwd_params) [with Kernel_traits=Flash_fwd_kernel_traits<160, 64, 64, 4, false, false, cutlass::half_t, Flash_kernel_traits<160, 64, 64, 4, cutlass::half_t>>, Is_dropout=false, Is_causal=true, Is_even_N=true, Is_even_K=false, Return_softmax=false]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(34): here
            instantiation of "void run_flash_fwd<Kernel_traits,Is_dropout,Is_causal>(Flash_fwd_params &, cudaStream_t) [with Kernel_traits=Flash_fwd_kernel_traits<160, 64, 64, 4, false, false, cutlass::half_t, Flash_kernel_traits<160, 64, 64, 4, cutlass::half_t>>, Is_dropout=false, Is_causal=true]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(155): here
            instantiation of "void run_mha_fwd_hdim160<T>(Flash_fwd_params &, cudaStream_t) [with T=cutlass::half_t]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_hdim160_fp16_sm80.cu(26): here

/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_kernel.h(140): warning: variable "MMA_M" was declared but never referenced
          detected during:
            instantiation of "void flash::compute_attn_1rowblock<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params,Is_Q_split_K>(const Params &, int, int, int) [with Kernel_traits=Flash_fwd_kernel_traits<160, 64, 64, 4, false, false, cutlass::half_t, Flash_kernel_traits<160, 64, 64, 4, cutlass::half_t>>, Is_dropout=false, Is_causal=true, Is_even_N=false, Is_even_K=true, Return_softmax=false, Params=Flash_fwd_params, Is_Q_split_K=false]" 
(589): here
            instantiation of "void flash::compute_attn<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &) [with Kernel_traits=Flash_fwd_kernel_traits<160, 64, 64, 4, false, false, cutlass::half_t, Flash_kernel_traits<160, 64, 64, 4, cutlass::half_t>>, Is_dropout=false, Is_causal=true, Is_even_N=false, Is_even_K=true, Return_softmax=false, Params=Flash_fwd_params]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(15): here
            instantiation of "void flash_fwd_kernel<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax>(Flash_fwd_params) [with Kernel_traits=Flash_fwd_kernel_traits<160, 64, 64, 4, false, false, cutlass::half_t, Flash_kernel_traits<160, 64, 64, 4, cutlass::half_t>>, Is_dropout=false, Is_causal=true, Is_even_N=false, Is_even_K=true, Return_softmax=false]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(34): here
            instantiation of "void run_flash_fwd<Kernel_traits,Is_dropout,Is_causal>(Flash_fwd_params &, cudaStream_t) [with Kernel_traits=Flash_fwd_kernel_traits<160, 64, 64, 4, false, false, cutlass::half_t, Flash_kernel_traits<160, 64, 64, 4, cutlass::half_t>>, Is_dropout=false, Is_causal=true]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(155): here
            instantiation of "void run_mha_fwd_hdim160<T>(Flash_fwd_params &, cudaStream_t) [with T=cutlass::half_t]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_hdim160_fp16_sm80.cu(26): here

/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_kernel.h(276): warning: variable "tQrQ" was declared but never referenced
          detected during:
            instantiation of "void flash::compute_attn_1rowblock<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params,Is_Q_split_K>(const Params &, int, int, int) [with Kernel_traits=Flash_fwd_kernel_traits<160, 64, 64, 4, false, false, cutlass::half_t, Flash_kernel_traits<160, 64, 64, 4, cutlass::half_t>>, Is_dropout=false, Is_causal=true, Is_even_N=false, Is_even_K=true, Return_softmax=false, Params=Flash_fwd_params, Is_Q_split_K=false]" 
(589): here
            instantiation of "void flash::compute_attn<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &) [with Kernel_traits=Flash_fwd_kernel_traits<160, 64, 64, 4, false, false, cutlass::half_t, Flash_kernel_traits<160, 64, 64, 4, cutlass::half_t>>, Is_dropout=false, Is_causal=true, Is_even_N=false, Is_even_K=true, Return_softmax=false, Params=Flash_fwd_params]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(15): here
            instantiation of "void flash_fwd_kernel<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax>(Flash_fwd_params) [with Kernel_traits=Flash_fwd_kernel_traits<160, 64, 64, 4, false, false, cutlass::half_t, Flash_kernel_traits<160, 64, 64, 4, cutlass::half_t>>, Is_dropout=false, Is_causal=true, Is_even_N=false, Is_even_K=true, Return_softmax=false]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(34): here
            instantiation of "void run_flash_fwd<Kernel_traits,Is_dropout,Is_causal>(Flash_fwd_params &, cudaStream_t) [with Kernel_traits=Flash_fwd_kernel_traits<160, 64, 64, 4, false, false, cutlass::half_t, Flash_kernel_traits<160, 64, 64, 4, cutlass::half_t>>, Is_dropout=false, Is_causal=true]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(155): here
            instantiation of "void run_mha_fwd_hdim160<T>(Flash_fwd_params &, cudaStream_t) [with T=cutlass::half_t]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_hdim160_fp16_sm80.cu(26): here

/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_kernel.h(140): warning: variable "MMA_M" was declared but never referenced
          detected during:
            instantiation of "void flash::compute_attn_1rowblock<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params,Is_Q_split_K>(const Params &, int, int, int) [with Kernel_traits=Flash_fwd_kernel_traits<160, 64, 64, 4, false, false, cutlass::half_t, Flash_kernel_traits<160, 64, 64, 4, cutlass::half_t>>, Is_dropout=false, Is_causal=true, Is_even_N=false, Is_even_K=false, Return_softmax=false, Params=Flash_fwd_params, Is_Q_split_K=false]" 
(589): here
            instantiation of "void flash::compute_attn<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &) [with Kernel_traits=Flash_fwd_kernel_traits<160, 64, 64, 4, false, false, cutlass::half_t, Flash_kernel_traits<160, 64, 64, 4, cutlass::half_t>>, Is_dropout=false, Is_causal=true, Is_even_N=false, Is_even_K=false, Return_softmax=false, Params=Flash_fwd_params]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(15): here
            instantiation of "void flash_fwd_kernel<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax>(Flash_fwd_params) [with Kernel_traits=Flash_fwd_kernel_traits<160, 64, 64, 4, false, false, cutlass::half_t, Flash_kernel_traits<160, 64, 64, 4, cutlass::half_t>>, Is_dropout=false, Is_causal=true, Is_even_N=false, Is_even_K=false, Return_softmax=false]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(34): here
            instantiation of "void run_flash_fwd<Kernel_traits,Is_dropout,Is_causal>(Flash_fwd_params &, cudaStream_t) [with Kernel_traits=Flash_fwd_kernel_traits<160, 64, 64, 4, false, false, cutlass::half_t, Flash_kernel_traits<160, 64, 64, 4, cutlass::half_t>>, Is_dropout=false, Is_causal=true]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(155): here
            instantiation of "void run_mha_fwd_hdim160<T>(Flash_fwd_params &, cudaStream_t) [with T=cutlass::half_t]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_hdim160_fp16_sm80.cu(26): here

/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_kernel.h(276): warning: variable "tQrQ" was declared but never referenced
          detected during:
            instantiation of "void flash::compute_attn_1rowblock<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params,Is_Q_split_K>(const Params &, int, int, int) [with Kernel_traits=Flash_fwd_kernel_traits<160, 64, 64, 4, false, false, cutlass::half_t, Flash_kernel_traits<160, 64, 64, 4, cutlass::half_t>>, Is_dropout=false, Is_causal=true, Is_even_N=false, Is_even_K=false, Return_softmax=false, Params=Flash_fwd_params, Is_Q_split_K=false]" 
(589): here
            instantiation of "void flash::compute_attn<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &) [with Kernel_traits=Flash_fwd_kernel_traits<160, 64, 64, 4, false, false, cutlass::half_t, Flash_kernel_traits<160, 64, 64, 4, cutlass::half_t>>, Is_dropout=false, Is_causal=true, Is_even_N=false, Is_even_K=false, Return_softmax=false, Params=Flash_fwd_params]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(15): here
            instantiation of "void flash_fwd_kernel<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax>(Flash_fwd_params) [with Kernel_traits=Flash_fwd_kernel_traits<160, 64, 64, 4, false, false, cutlass::half_t, Flash_kernel_traits<160, 64, 64, 4, cutlass::half_t>>, Is_dropout=false, Is_causal=true, Is_even_N=false, Is_even_K=false, Return_softmax=false]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(34): here
            instantiation of "void run_flash_fwd<Kernel_traits,Is_dropout,Is_causal>(Flash_fwd_params &, cudaStream_t) [with Kernel_traits=Flash_fwd_kernel_traits<160, 64, 64, 4, false, false, cutlass::half_t, Flash_kernel_traits<160, 64, 64, 4, cutlass::half_t>>, Is_dropout=false, Is_causal=true]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(155): here
            instantiation of "void run_mha_fwd_hdim160<T>(Flash_fwd_params &, cudaStream_t) [with T=cutlass::half_t]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_hdim160_fp16_sm80.cu(26): here

/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_kernel.h(140): warning: variable "MMA_M" was declared but never referenced
          detected during:
            instantiation of "void flash::compute_attn_1rowblock<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params,Is_Q_split_K>(const Params &, int, int, int) [with Kernel_traits=Flash_fwd_kernel_traits<160, 128, 32, 4, false, false, cutlass::half_t, Flash_kernel_traits<160, 128, 32, 4, cutlass::half_t>>, Is_dropout=false, Is_causal=true, Is_even_N=true, Is_even_K=true, Return_softmax=false, Params=Flash_fwd_params, Is_Q_split_K=false]" 
(589): here
            instantiation of "void flash::compute_attn<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &) [with Kernel_traits=Flash_fwd_kernel_traits<160, 128, 32, 4, false, false, cutlass::half_t, Flash_kernel_traits<160, 128, 32, 4, cutlass::half_t>>, Is_dropout=false, Is_causal=true, Is_even_N=true, Is_even_K=true, Return_softmax=false, Params=Flash_fwd_params]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(15): here
            instantiation of "void flash_fwd_kernel<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax>(Flash_fwd_params) [with Kernel_traits=Flash_fwd_kernel_traits<160, 128, 32, 4, false, false, cutlass::half_t, Flash_kernel_traits<160, 128, 32, 4, cutlass::half_t>>, Is_dropout=false, Is_causal=true, Is_even_N=true, Is_even_K=true, Return_softmax=false]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(34): here
            instantiation of "void run_flash_fwd<Kernel_traits,Is_dropout,Is_causal>(Flash_fwd_params &, cudaStream_t) [with Kernel_traits=Flash_fwd_kernel_traits<160, 128, 32, 4, false, false, cutlass::half_t, Flash_kernel_traits<160, 128, 32, 4, cutlass::half_t>>, Is_dropout=false, Is_causal=true]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(155): here
            instantiation of "void run_mha_fwd_hdim160<T>(Flash_fwd_params &, cudaStream_t) [with T=cutlass::half_t]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_hdim160_fp16_sm80.cu(26): here

/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_kernel.h(276): warning: variable "tQrQ" was declared but never referenced
          detected during:
            instantiation of "void flash::compute_attn_1rowblock<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params,Is_Q_split_K>(const Params &, int, int, int) [with Kernel_traits=Flash_fwd_kernel_traits<160, 128, 32, 4, false, false, cutlass::half_t, Flash_kernel_traits<160, 128, 32, 4, cutlass::half_t>>, Is_dropout=false, Is_causal=true, Is_even_N=true, Is_even_K=true, Return_softmax=false, Params=Flash_fwd_params, Is_Q_split_K=false]" 
(589): here
            instantiation of "void flash::compute_attn<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &) [with Kernel_traits=Flash_fwd_kernel_traits<160, 128, 32, 4, false, false, cutlass::half_t, Flash_kernel_traits<160, 128, 32, 4, cutlass::half_t>>, Is_dropout=false, Is_causal=true, Is_even_N=true, Is_even_K=true, Return_softmax=false, Params=Flash_fwd_params]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(15): here
            instantiation of "void flash_fwd_kernel<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax>(Flash_fwd_params) [with Kernel_traits=Flash_fwd_kernel_traits<160, 128, 32, 4, false, false, cutlass::half_t, Flash_kernel_traits<160, 128, 32, 4, cutlass::half_t>>, Is_dropout=false, Is_causal=true, Is_even_N=true, Is_even_K=true, Return_softmax=false]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(34): here
            instantiation of "void run_flash_fwd<Kernel_traits,Is_dropout,Is_causal>(Flash_fwd_params &, cudaStream_t) [with Kernel_traits=Flash_fwd_kernel_traits<160, 128, 32, 4, false, false, cutlass::half_t, Flash_kernel_traits<160, 128, 32, 4, cutlass::half_t>>, Is_dropout=false, Is_causal=true]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(155): here
            instantiation of "void run_mha_fwd_hdim160<T>(Flash_fwd_params &, cudaStream_t) [with T=cutlass::half_t]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_hdim160_fp16_sm80.cu(26): here

/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_kernel.h(140): warning: variable "MMA_M" was declared but never referenced
          detected during:
            instantiation of "void flash::compute_attn_1rowblock<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params,Is_Q_split_K>(const Params &, int, int, int) [with Kernel_traits=Flash_fwd_kernel_traits<160, 128, 32, 4, false, false, cutlass::half_t, Flash_kernel_traits<160, 128, 32, 4, cutlass::half_t>>, Is_dropout=false, Is_causal=true, Is_even_N=true, Is_even_K=false, Return_softmax=false, Params=Flash_fwd_params, Is_Q_split_K=false]" 
(589): here
            instantiation of "void flash::compute_attn<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &) [with Kernel_traits=Flash_fwd_kernel_traits<160, 128, 32, 4, false, false, cutlass::half_t, Flash_kernel_traits<160, 128, 32, 4, cutlass::half_t>>, Is_dropout=false, Is_causal=true, Is_even_N=true, Is_even_K=false, Return_softmax=false, Params=Flash_fwd_params]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(15): here
            instantiation of "void flash_fwd_kernel<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax>(Flash_fwd_params) [with Kernel_traits=Flash_fwd_kernel_traits<160, 128, 32, 4, false, false, cutlass::half_t, Flash_kernel_traits<160, 128, 32, 4, cutlass::half_t>>, Is_dropout=false, Is_causal=true, Is_even_N=true, Is_even_K=false, Return_softmax=false]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(34): here
            instantiation of "void run_flash_fwd<Kernel_traits,Is_dropout,Is_causal>(Flash_fwd_params &, cudaStream_t) [with Kernel_traits=Flash_fwd_kernel_traits<160, 128, 32, 4, false, false, cutlass::half_t, Flash_kernel_traits<160, 128, 32, 4, cutlass::half_t>>, Is_dropout=false, Is_causal=true]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(155): here
            instantiation of "void run_mha_fwd_hdim160<T>(Flash_fwd_params &, cudaStream_t) [with T=cutlass::half_t]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_hdim160_fp16_sm80.cu(26): here

/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_kernel.h(276): warning: variable "tQrQ" was declared but never referenced
          detected during:
            instantiation of "void flash::compute_attn_1rowblock<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params,Is_Q_split_K>(const Params &, int, int, int) [with Kernel_traits=Flash_fwd_kernel_traits<160, 128, 32, 4, false, false, cutlass::half_t, Flash_kernel_traits<160, 128, 32, 4, cutlass::half_t>>, Is_dropout=false, Is_causal=true, Is_even_N=true, Is_even_K=false, Return_softmax=false, Params=Flash_fwd_params, Is_Q_split_K=false]" 
(589): here
            instantiation of "void flash::compute_attn<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &) [with Kernel_traits=Flash_fwd_kernel_traits<160, 128, 32, 4, false, false, cutlass::half_t, Flash_kernel_traits<160, 128, 32, 4, cutlass::half_t>>, Is_dropout=false, Is_causal=true, Is_even_N=true, Is_even_K=false, Return_softmax=false, Params=Flash_fwd_params]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(15): here
            instantiation of "void flash_fwd_kernel<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax>(Flash_fwd_params) [with Kernel_traits=Flash_fwd_kernel_traits<160, 128, 32, 4, false, false, cutlass::half_t, Flash_kernel_traits<160, 128, 32, 4, cutlass::half_t>>, Is_dropout=false, Is_causal=true, Is_even_N=true, Is_even_K=false, Return_softmax=false]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(34): here
            instantiation of "void run_flash_fwd<Kernel_traits,Is_dropout,Is_causal>(Flash_fwd_params &, cudaStream_t) [with Kernel_traits=Flash_fwd_kernel_traits<160, 128, 32, 4, false, false, cutlass::half_t, Flash_kernel_traits<160, 128, 32, 4, cutlass::half_t>>, Is_dropout=false, Is_causal=true]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(155): here
            instantiation of "void run_mha_fwd_hdim160<T>(Flash_fwd_params &, cudaStream_t) [with T=cutlass::half_t]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_hdim160_fp16_sm80.cu(26): here

/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_kernel.h(140): warning: variable "MMA_M" was declared but never referenced
          detected during:
            instantiation of "void flash::compute_attn_1rowblock<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params,Is_Q_split_K>(const Params &, int, int, int) [with Kernel_traits=Flash_fwd_kernel_traits<160, 128, 32, 4, false, false, cutlass::half_t, Flash_kernel_traits<160, 128, 32, 4, cutlass::half_t>>, Is_dropout=false, Is_causal=true, Is_even_N=false, Is_even_K=true, Return_softmax=false, Params=Flash_fwd_params, Is_Q_split_K=false]" 
(589): here
            instantiation of "void flash::compute_attn<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &) [with Kernel_traits=Flash_fwd_kernel_traits<160, 128, 32, 4, false, false, cutlass::half_t, Flash_kernel_traits<160, 128, 32, 4, cutlass::half_t>>, Is_dropout=false, Is_causal=true, Is_even_N=false, Is_even_K=true, Return_softmax=false, Params=Flash_fwd_params]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(15): here
            instantiation of "void flash_fwd_kernel<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax>(Flash_fwd_params) [with Kernel_traits=Flash_fwd_kernel_traits<160, 128, 32, 4, false, false, cutlass::half_t, Flash_kernel_traits<160, 128, 32, 4, cutlass::half_t>>, Is_dropout=false, Is_causal=true, Is_even_N=false, Is_even_K=true, Return_softmax=false]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(34): here
            instantiation of "void run_flash_fwd<Kernel_traits,Is_dropout,Is_causal>(Flash_fwd_params &, cudaStream_t) [with Kernel_traits=Flash_fwd_kernel_traits<160, 128, 32, 4, false, false, cutlass::half_t, Flash_kernel_traits<160, 128, 32, 4, cutlass::half_t>>, Is_dropout=false, Is_causal=true]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(155): here
            instantiation of "void run_mha_fwd_hdim160<T>(Flash_fwd_params &, cudaStream_t) [with T=cutlass::half_t]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_hdim160_fp16_sm80.cu(26): here

/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_kernel.h(276): warning: variable "tQrQ" was declared but never referenced
          detected during:
            instantiation of "void flash::compute_attn_1rowblock<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params,Is_Q_split_K>(const Params &, int, int, int) [with Kernel_traits=Flash_fwd_kernel_traits<160, 128, 32, 4, false, false, cutlass::half_t, Flash_kernel_traits<160, 128, 32, 4, cutlass::half_t>>, Is_dropout=false, Is_causal=true, Is_even_N=false, Is_even_K=true, Return_softmax=false, Params=Flash_fwd_params, Is_Q_split_K=false]" 
(589): here
            instantiation of "void flash::compute_attn<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &) [with Kernel_traits=Flash_fwd_kernel_traits<160, 128, 32, 4, false, false, cutlass::half_t, Flash_kernel_traits<160, 128, 32, 4, cutlass::half_t>>, Is_dropout=false, Is_causal=true, Is_even_N=false, Is_even_K=true, Return_softmax=false, Params=Flash_fwd_params]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(15): here
            instantiation of "void flash_fwd_kernel<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax>(Flash_fwd_params) [with Kernel_traits=Flash_fwd_kernel_traits<160, 128, 32, 4, false, false, cutlass::half_t, Flash_kernel_traits<160, 128, 32, 4, cutlass::half_t>>, Is_dropout=false, Is_causal=true, Is_even_N=false, Is_even_K=true, Return_softmax=false]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(34): here
            instantiation of "void run_flash_fwd<Kernel_traits,Is_dropout,Is_causal>(Flash_fwd_params &, cudaStream_t) [with Kernel_traits=Flash_fwd_kernel_traits<160, 128, 32, 4, false, false, cutlass::half_t, Flash_kernel_traits<160, 128, 32, 4, cutlass::half_t>>, Is_dropout=false, Is_causal=true]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(155): here
            instantiation of "void run_mha_fwd_hdim160<T>(Flash_fwd_params &, cudaStream_t) [with T=cutlass::half_t]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_hdim160_fp16_sm80.cu(26): here

/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_kernel.h(140): warning: variable "MMA_M" was declared but never referenced
          detected during:
            instantiation of "void flash::compute_attn_1rowblock<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params,Is_Q_split_K>(const Params &, int, int, int) [with Kernel_traits=Flash_fwd_kernel_traits<160, 128, 32, 4, false, false, cutlass::half_t, Flash_kernel_traits<160, 128, 32, 4, cutlass::half_t>>, Is_dropout=false, Is_causal=true, Is_even_N=false, Is_even_K=false, Return_softmax=false, Params=Flash_fwd_params, Is_Q_split_K=false]" 
(589): here
            instantiation of "void flash::compute_attn<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &) [with Kernel_traits=Flash_fwd_kernel_traits<160, 128, 32, 4, false, false, cutlass::half_t, Flash_kernel_traits<160, 128, 32, 4, cutlass::half_t>>, Is_dropout=false, Is_causal=true, Is_even_N=false, Is_even_K=false, Return_softmax=false, Params=Flash_fwd_params]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(15): here
            instantiation of "void flash_fwd_kernel<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax>(Flash_fwd_params) [with Kernel_traits=Flash_fwd_kernel_traits<160, 128, 32, 4, false, false, cutlass::half_t, Flash_kernel_traits<160, 128, 32, 4, cutlass::half_t>>, Is_dropout=false, Is_causal=true, Is_even_N=false, Is_even_K=false, Return_softmax=false]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(34): here
            instantiation of "void run_flash_fwd<Kernel_traits,Is_dropout,Is_causal>(Flash_fwd_params &, cudaStream_t) [with Kernel_traits=Flash_fwd_kernel_traits<160, 128, 32, 4, false, false, cutlass::half_t, Flash_kernel_traits<160, 128, 32, 4, cutlass::half_t>>, Is_dropout=false, Is_causal=true]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(155): here
            instantiation of "void run_mha_fwd_hdim160<T>(Flash_fwd_params &, cudaStream_t) [with T=cutlass::half_t]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_hdim160_fp16_sm80.cu(26): here

/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_kernel.h(276): warning: variable "tQrQ" was declared but never referenced
          detected during:
            instantiation of "void flash::compute_attn_1rowblock<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params,Is_Q_split_K>(const Params &, int, int, int) [with Kernel_traits=Flash_fwd_kernel_traits<160, 128, 32, 4, false, false, cutlass::half_t, Flash_kernel_traits<160, 128, 32, 4, cutlass::half_t>>, Is_dropout=false, Is_causal=true, Is_even_N=false, Is_even_K=false, Return_softmax=false, Params=Flash_fwd_params, Is_Q_split_K=false]" 
(589): here
            instantiation of "void flash::compute_attn<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &) [with Kernel_traits=Flash_fwd_kernel_traits<160, 128, 32, 4, false, false, cutlass::half_t, Flash_kernel_traits<160, 128, 32, 4, cutlass::half_t>>, Is_dropout=false, Is_causal=true, Is_even_N=false, Is_even_K=false, Return_softmax=false, Params=Flash_fwd_params]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(15): here
            instantiation of "void flash_fwd_kernel<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax>(Flash_fwd_params) [with Kernel_traits=Flash_fwd_kernel_traits<160, 128, 32, 4, false, false, cutlass::half_t, Flash_kernel_traits<160, 128, 32, 4, cutlass::half_t>>, Is_dropout=false, Is_causal=true, Is_even_N=false, Is_even_K=false, Return_softmax=false]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(34): here
            instantiation of "void run_flash_fwd<Kernel_traits,Is_dropout,Is_causal>(Flash_fwd_params &, cudaStream_t) [with Kernel_traits=Flash_fwd_kernel_traits<160, 128, 32, 4, false, false, cutlass::half_t, Flash_kernel_traits<160, 128, 32, 4, cutlass::half_t>>, Is_dropout=false, Is_causal=true]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(155): here
            instantiation of "void run_mha_fwd_hdim160<T>(Flash_fwd_params &, cudaStream_t) [with T=cutlass::half_t]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_hdim160_fp16_sm80.cu(26): here

/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_kernel.h(140): warning: variable "MMA_M" was declared but never referenced
          detected during:
            instantiation of "void flash::compute_attn_1rowblock<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params,Is_Q_split_K>(const Params &, int, int, int) [with Kernel_traits=Flash_fwd_kernel_traits<160, 128, 64, 8, false, false, cutlass::half_t, Flash_kernel_traits<160, 128, 64, 8, cutlass::half_t>>, Is_dropout=false, Is_causal=false, Is_even_N=true, Is_even_K=true, Return_softmax=false, Params=Flash_fwd_params, Is_Q_split_K=false]" 
(589): here
            instantiation of "void flash::compute_attn<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &) [with Kernel_traits=Flash_fwd_kernel_traits<160, 128, 64, 8, false, false, cutlass::half_t, Flash_kernel_traits<160, 128, 64, 8, cutlass::half_t>>, Is_dropout=false, Is_causal=false, Is_even_N=true, Is_even_K=true, Return_softmax=false, Params=Flash_fwd_params]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(15): here
            instantiation of "void flash_fwd_kernel<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax>(Flash_fwd_params) [with Kernel_traits=Flash_fwd_kernel_traits<160, 128, 64, 8, false, false, cutlass::half_t, Flash_kernel_traits<160, 128, 64, 8, cutlass::half_t>>, Is_dropout=false, Is_causal=false, Is_even_N=true, Is_even_K=true, Return_softmax=false]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(34): here
            instantiation of "void run_flash_fwd<Kernel_traits,Is_dropout,Is_causal>(Flash_fwd_params &, cudaStream_t) [with Kernel_traits=Flash_fwd_kernel_traits<160, 128, 64, 8, false, false, cutlass::half_t, Flash_kernel_traits<160, 128, 64, 8, cutlass::half_t>>, Is_dropout=false, Is_causal=false]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(155): here
            instantiation of "void run_mha_fwd_hdim160<T>(Flash_fwd_params &, cudaStream_t) [with T=cutlass::half_t]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_hdim160_fp16_sm80.cu(26): here

/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_kernel.h(276): warning: variable "tQrQ" was declared but never referenced
          detected during:
            instantiation of "void flash::compute_attn_1rowblock<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params,Is_Q_split_K>(const Params &, int, int, int) [with Kernel_traits=Flash_fwd_kernel_traits<160, 128, 64, 8, false, false, cutlass::half_t, Flash_kernel_traits<160, 128, 64, 8, cutlass::half_t>>, Is_dropout=false, Is_causal=false, Is_even_N=true, Is_even_K=true, Return_softmax=false, Params=Flash_fwd_params, Is_Q_split_K=false]" 
(589): here
            instantiation of "void flash::compute_attn<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &) [with Kernel_traits=Flash_fwd_kernel_traits<160, 128, 64, 8, false, false, cutlass::half_t, Flash_kernel_traits<160, 128, 64, 8, cutlass::half_t>>, Is_dropout=false, Is_causal=false, Is_even_N=true, Is_even_K=true, Return_softmax=false, Params=Flash_fwd_params]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(15): here
            instantiation of "void flash_fwd_kernel<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax>(Flash_fwd_params) [with Kernel_traits=Flash_fwd_kernel_traits<160, 128, 64, 8, false, false, cutlass::half_t, Flash_kernel_traits<160, 128, 64, 8, cutlass::half_t>>, Is_dropout=false, Is_causal=false, Is_even_N=true, Is_even_K=true, Return_softmax=false]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(34): here
            instantiation of "void run_flash_fwd<Kernel_traits,Is_dropout,Is_causal>(Flash_fwd_params &, cudaStream_t) [with Kernel_traits=Flash_fwd_kernel_traits<160, 128, 64, 8, false, false, cutlass::half_t, Flash_kernel_traits<160, 128, 64, 8, cutlass::half_t>>, Is_dropout=false, Is_causal=false]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(155): here
            instantiation of "void run_mha_fwd_hdim160<T>(Flash_fwd_params &, cudaStream_t) [with T=cutlass::half_t]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_hdim160_fp16_sm80.cu(26): here

/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_kernel.h(140): warning: variable "MMA_M" was declared but never referenced
          detected during:
            instantiation of "void flash::compute_attn_1rowblock<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params,Is_Q_split_K>(const Params &, int, int, int) [with Kernel_traits=Flash_fwd_kernel_traits<160, 128, 64, 8, false, false, cutlass::half_t, Flash_kernel_traits<160, 128, 64, 8, cutlass::half_t>>, Is_dropout=false, Is_causal=false, Is_even_N=true, Is_even_K=false, Return_softmax=false, Params=Flash_fwd_params, Is_Q_split_K=false]" 
(589): here
            instantiation of "void flash::compute_attn<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &) [with Kernel_traits=Flash_fwd_kernel_traits<160, 128, 64, 8, false, false, cutlass::half_t, Flash_kernel_traits<160, 128, 64, 8, cutlass::half_t>>, Is_dropout=false, Is_causal=false, Is_even_N=true, Is_even_K=false, Return_softmax=false, Params=Flash_fwd_params]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(15): here
            instantiation of "void flash_fwd_kernel<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax>(Flash_fwd_params) [with Kernel_traits=Flash_fwd_kernel_traits<160, 128, 64, 8, false, false, cutlass::half_t, Flash_kernel_traits<160, 128, 64, 8, cutlass::half_t>>, Is_dropout=false, Is_causal=false, Is_even_N=true, Is_even_K=false, Return_softmax=false]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(34): here
            instantiation of "void run_flash_fwd<Kernel_traits,Is_dropout,Is_causal>(Flash_fwd_params &, cudaStream_t) [with Kernel_traits=Flash_fwd_kernel_traits<160, 128, 64, 8, false, false, cutlass::half_t, Flash_kernel_traits<160, 128, 64, 8, cutlass::half_t>>, Is_dropout=false, Is_causal=false]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(155): here
            instantiation of "void run_mha_fwd_hdim160<T>(Flash_fwd_params &, cudaStream_t) [with T=cutlass::half_t]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_hdim160_fp16_sm80.cu(26): here

/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_kernel.h(276): warning: variable "tQrQ" was declared but never referenced
          detected during:
            instantiation of "void flash::compute_attn_1rowblock<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params,Is_Q_split_K>(const Params &, int, int, int) [with Kernel_traits=Flash_fwd_kernel_traits<160, 128, 64, 8, false, false, cutlass::half_t, Flash_kernel_traits<160, 128, 64, 8, cutlass::half_t>>, Is_dropout=false, Is_causal=false, Is_even_N=true, Is_even_K=false, Return_softmax=false, Params=Flash_fwd_params, Is_Q_split_K=false]" 
(589): here
            instantiation of "void flash::compute_attn<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &) [with Kernel_traits=Flash_fwd_kernel_traits<160, 128, 64, 8, false, false, cutlass::half_t, Flash_kernel_traits<160, 128, 64, 8, cutlass::half_t>>, Is_dropout=false, Is_causal=false, Is_even_N=true, Is_even_K=false, Return_softmax=false, Params=Flash_fwd_params]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(15): here
            instantiation of "void flash_fwd_kernel<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax>(Flash_fwd_params) [with Kernel_traits=Flash_fwd_kernel_traits<160, 128, 64, 8, false, false, cutlass::half_t, Flash_kernel_traits<160, 128, 64, 8, cutlass::half_t>>, Is_dropout=false, Is_causal=false, Is_even_N=true, Is_even_K=false, Return_softmax=false]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(34): here
            instantiation of "void run_flash_fwd<Kernel_traits,Is_dropout,Is_causal>(Flash_fwd_params &, cudaStream_t) [with Kernel_traits=Flash_fwd_kernel_traits<160, 128, 64, 8, false, false, cutlass::half_t, Flash_kernel_traits<160, 128, 64, 8, cutlass::half_t>>, Is_dropout=false, Is_causal=false]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(155): here
            instantiation of "void run_mha_fwd_hdim160<T>(Flash_fwd_params &, cudaStream_t) [with T=cutlass::half_t]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_hdim160_fp16_sm80.cu(26): here

/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_kernel.h(140): warning: variable "MMA_M" was declared but never referenced
          detected during:
            instantiation of "void flash::compute_attn_1rowblock<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params,Is_Q_split_K>(const Params &, int, int, int) [with Kernel_traits=Flash_fwd_kernel_traits<160, 128, 64, 8, false, false, cutlass::half_t, Flash_kernel_traits<160, 128, 64, 8, cutlass::half_t>>, Is_dropout=false, Is_causal=false, Is_even_N=false, Is_even_K=true, Return_softmax=false, Params=Flash_fwd_params, Is_Q_split_K=false]" 
(589): here
            instantiation of "void flash::compute_attn<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &) [with Kernel_traits=Flash_fwd_kernel_traits<160, 128, 64, 8, false, false, cutlass::half_t, Flash_kernel_traits<160, 128, 64, 8, cutlass::half_t>>, Is_dropout=false, Is_causal=false, Is_even_N=false, Is_even_K=true, Return_softmax=false, Params=Flash_fwd_params]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(15): here
            instantiation of "void flash_fwd_kernel<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax>(Flash_fwd_params) [with Kernel_traits=Flash_fwd_kernel_traits<160, 128, 64, 8, false, false, cutlass::half_t, Flash_kernel_traits<160, 128, 64, 8, cutlass::half_t>>, Is_dropout=false, Is_causal=false, Is_even_N=false, Is_even_K=true, Return_softmax=false]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(34): here
            instantiation of "void run_flash_fwd<Kernel_traits,Is_dropout,Is_causal>(Flash_fwd_params &, cudaStream_t) [with Kernel_traits=Flash_fwd_kernel_traits<160, 128, 64, 8, false, false, cutlass::half_t, Flash_kernel_traits<160, 128, 64, 8, cutlass::half_t>>, Is_dropout=false, Is_causal=false]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(155): here
            instantiation of "void run_mha_fwd_hdim160<T>(Flash_fwd_params &, cudaStream_t) [with T=cutlass::half_t]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_hdim160_fp16_sm80.cu(26): here

/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_kernel.h(276): warning: variable "tQrQ" was declared but never referenced
          detected during:
            instantiation of "void flash::compute_attn_1rowblock<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params,Is_Q_split_K>(const Params &, int, int, int) [with Kernel_traits=Flash_fwd_kernel_traits<160, 128, 64, 8, false, false, cutlass::half_t, Flash_kernel_traits<160, 128, 64, 8, cutlass::half_t>>, Is_dropout=false, Is_causal=false, Is_even_N=false, Is_even_K=true, Return_softmax=false, Params=Flash_fwd_params, Is_Q_split_K=false]" 
(589): here
            instantiation of "void flash::compute_attn<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &) [with Kernel_traits=Flash_fwd_kernel_traits<160, 128, 64, 8, false, false, cutlass::half_t, Flash_kernel_traits<160, 128, 64, 8, cutlass::half_t>>, Is_dropout=false, Is_causal=false, Is_even_N=false, Is_even_K=true, Return_softmax=false, Params=Flash_fwd_params]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(15): here
            instantiation of "void flash_fwd_kernel<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax>(Flash_fwd_params) [with Kernel_traits=Flash_fwd_kernel_traits<160, 128, 64, 8, false, false, cutlass::half_t, Flash_kernel_traits<160, 128, 64, 8, cutlass::half_t>>, Is_dropout=false, Is_causal=false, Is_even_N=false, Is_even_K=true, Return_softmax=false]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(34): here
            instantiation of "void run_flash_fwd<Kernel_traits,Is_dropout,Is_causal>(Flash_fwd_params &, cudaStream_t) [with Kernel_traits=Flash_fwd_kernel_traits<160, 128, 64, 8, false, false, cutlass::half_t, Flash_kernel_traits<160, 128, 64, 8, cutlass::half_t>>, Is_dropout=false, Is_causal=false]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(155): here
            instantiation of "void run_mha_fwd_hdim160<T>(Flash_fwd_params &, cudaStream_t) [with T=cutlass::half_t]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_hdim160_fp16_sm80.cu(26): here

/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_kernel.h(140): warning: variable "MMA_M" was declared but never referenced
          detected during:
            instantiation of "void flash::compute_attn_1rowblock<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params,Is_Q_split_K>(const Params &, int, int, int) [with Kernel_traits=Flash_fwd_kernel_traits<160, 128, 64, 8, false, false, cutlass::half_t, Flash_kernel_traits<160, 128, 64, 8, cutlass::half_t>>, Is_dropout=false, Is_causal=false, Is_even_N=false, Is_even_K=false, Return_softmax=false, Params=Flash_fwd_params, Is_Q_split_K=false]" 
(589): here
            instantiation of "void flash::compute_attn<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &) [with Kernel_traits=Flash_fwd_kernel_traits<160, 128, 64, 8, false, false, cutlass::half_t, Flash_kernel_traits<160, 128, 64, 8, cutlass::half_t>>, Is_dropout=false, Is_causal=false, Is_even_N=false, Is_even_K=false, Return_softmax=false, Params=Flash_fwd_params]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(15): here
            instantiation of "void flash_fwd_kernel<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax>(Flash_fwd_params) [with Kernel_traits=Flash_fwd_kernel_traits<160, 128, 64, 8, false, false, cutlass::half_t, Flash_kernel_traits<160, 128, 64, 8, cutlass::half_t>>, Is_dropout=false, Is_causal=false, Is_even_N=false, Is_even_K=false, Return_softmax=false]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(34): here
            instantiation of "void run_flash_fwd<Kernel_traits,Is_dropout,Is_causal>(Flash_fwd_params &, cudaStream_t) [with Kernel_traits=Flash_fwd_kernel_traits<160, 128, 64, 8, false, false, cutlass::half_t, Flash_kernel_traits<160, 128, 64, 8, cutlass::half_t>>, Is_dropout=false, Is_causal=false]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(155): here
            instantiation of "void run_mha_fwd_hdim160<T>(Flash_fwd_params &, cudaStream_t) [with T=cutlass::half_t]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_hdim160_fp16_sm80.cu(26): here

/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_kernel.h(276): warning: variable "tQrQ" was declared but never referenced
          detected during:
            instantiation of "void flash::compute_attn_1rowblock<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params,Is_Q_split_K>(const Params &, int, int, int) [with Kernel_traits=Flash_fwd_kernel_traits<160, 128, 64, 8, false, false, cutlass::half_t, Flash_kernel_traits<160, 128, 64, 8, cutlass::half_t>>, Is_dropout=false, Is_causal=false, Is_even_N=false, Is_even_K=false, Return_softmax=false, Params=Flash_fwd_params, Is_Q_split_K=false]" 
(589): here
            instantiation of "void flash::compute_attn<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &) [with Kernel_traits=Flash_fwd_kernel_traits<160, 128, 64, 8, false, false, cutlass::half_t, Flash_kernel_traits<160, 128, 64, 8, cutlass::half_t>>, Is_dropout=false, Is_causal=false, Is_even_N=false, Is_even_K=false, Return_softmax=false, Params=Flash_fwd_params]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(15): here
            instantiation of "void flash_fwd_kernel<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax>(Flash_fwd_params) [with Kernel_traits=Flash_fwd_kernel_traits<160, 128, 64, 8, false, false, cutlass::half_t, Flash_kernel_traits<160, 128, 64, 8, cutlass::half_t>>, Is_dropout=false, Is_causal=false, Is_even_N=false, Is_even_K=false, Return_softmax=false]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(34): here
            instantiation of "void run_flash_fwd<Kernel_traits,Is_dropout,Is_causal>(Flash_fwd_params &, cudaStream_t) [with Kernel_traits=Flash_fwd_kernel_traits<160, 128, 64, 8, false, false, cutlass::half_t, Flash_kernel_traits<160, 128, 64, 8, cutlass::half_t>>, Is_dropout=false, Is_causal=false]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(155): here
            instantiation of "void run_mha_fwd_hdim160<T>(Flash_fwd_params &, cudaStream_t) [with T=cutlass::half_t]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_hdim160_fp16_sm80.cu(26): here

/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_kernel.h(140): warning: variable "MMA_M" was declared but never referenced
          detected during:
            instantiation of "void flash::compute_attn_1rowblock<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params,Is_Q_split_K>(const Params &, int, int, int) [with Kernel_traits=Flash_fwd_kernel_traits<160, 128, 32, 4, false, false, cutlass::half_t, Flash_kernel_traits<160, 128, 32, 4, cutlass::half_t>>, Is_dropout=false, Is_causal=false, Is_even_N=true, Is_even_K=true, Return_softmax=false, Params=Flash_fwd_params, Is_Q_split_K=false]" 
(589): here
            instantiation of "void flash::compute_attn<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &) [with Kernel_traits=Flash_fwd_kernel_traits<160, 128, 32, 4, false, false, cutlass::half_t, Flash_kernel_traits<160, 128, 32, 4, cutlass::half_t>>, Is_dropout=false, Is_causal=false, Is_even_N=true, Is_even_K=true, Return_softmax=false, Params=Flash_fwd_params]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(15): here
            instantiation of "void flash_fwd_kernel<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax>(Flash_fwd_params) [with Kernel_traits=Flash_fwd_kernel_traits<160, 128, 32, 4, false, false, cutlass::half_t, Flash_kernel_traits<160, 128, 32, 4, cutlass::half_t>>, Is_dropout=false, Is_causal=false, Is_even_N=true, Is_even_K=true, Return_softmax=false]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(34): here
            instantiation of "void run_flash_fwd<Kernel_traits,Is_dropout,Is_causal>(Flash_fwd_params &, cudaStream_t) [with Kernel_traits=Flash_fwd_kernel_traits<160, 128, 32, 4, false, false, cutlass::half_t, Flash_kernel_traits<160, 128, 32, 4, cutlass::half_t>>, Is_dropout=false, Is_causal=false]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(155): here
            instantiation of "void run_mha_fwd_hdim160<T>(Flash_fwd_params &, cudaStream_t) [with T=cutlass::half_t]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_hdim160_fp16_sm80.cu(26): here

/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_kernel.h(276): warning: variable "tQrQ" was declared but never referenced
          detected during:
            instantiation of "void flash::compute_attn_1rowblock<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params,Is_Q_split_K>(const Params &, int, int, int) [with Kernel_traits=Flash_fwd_kernel_traits<160, 128, 32, 4, false, false, cutlass::half_t, Flash_kernel_traits<160, 128, 32, 4, cutlass::half_t>>, Is_dropout=false, Is_causal=false, Is_even_N=true, Is_even_K=true, Return_softmax=false, Params=Flash_fwd_params, Is_Q_split_K=false]" 
(589): here
            instantiation of "void flash::compute_attn<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &) [with Kernel_traits=Flash_fwd_kernel_traits<160, 128, 32, 4, false, false, cutlass::half_t, Flash_kernel_traits<160, 128, 32, 4, cutlass::half_t>>, Is_dropout=false, Is_causal=false, Is_even_N=true, Is_even_K=true, Return_softmax=false, Params=Flash_fwd_params]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(15): here
            instantiation of "void flash_fwd_kernel<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax>(Flash_fwd_params) [with Kernel_traits=Flash_fwd_kernel_traits<160, 128, 32, 4, false, false, cutlass::half_t, Flash_kernel_traits<160, 128, 32, 4, cutlass::half_t>>, Is_dropout=false, Is_causal=false, Is_even_N=true, Is_even_K=true, Return_softmax=false]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(34): here
            instantiation of "void run_flash_fwd<Kernel_traits,Is_dropout,Is_causal>(Flash_fwd_params &, cudaStream_t) [with Kernel_traits=Flash_fwd_kernel_traits<160, 128, 32, 4, false, false, cutlass::half_t, Flash_kernel_traits<160, 128, 32, 4, cutlass::half_t>>, Is_dropout=false, Is_causal=false]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(155): here
            instantiation of "void run_mha_fwd_hdim160<T>(Flash_fwd_params &, cudaStream_t) [with T=cutlass::half_t]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_hdim160_fp16_sm80.cu(26): here

/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_kernel.h(140): warning: variable "MMA_M" was declared but never referenced
          detected during:
            instantiation of "void flash::compute_attn_1rowblock<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params,Is_Q_split_K>(const Params &, int, int, int) [with Kernel_traits=Flash_fwd_kernel_traits<160, 128, 32, 4, false, false, cutlass::half_t, Flash_kernel_traits<160, 128, 32, 4, cutlass::half_t>>, Is_dropout=false, Is_causal=false, Is_even_N=true, Is_even_K=false, Return_softmax=false, Params=Flash_fwd_params, Is_Q_split_K=false]" 
(589): here
            instantiation of "void flash::compute_attn<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &) [with Kernel_traits=Flash_fwd_kernel_traits<160, 128, 32, 4, false, false, cutlass::half_t, Flash_kernel_traits<160, 128, 32, 4, cutlass::half_t>>, Is_dropout=false, Is_causal=false, Is_even_N=true, Is_even_K=false, Return_softmax=false, Params=Flash_fwd_params]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(15): here
            instantiation of "void flash_fwd_kernel<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax>(Flash_fwd_params) [with Kernel_traits=Flash_fwd_kernel_traits<160, 128, 32, 4, false, false, cutlass::half_t, Flash_kernel_traits<160, 128, 32, 4, cutlass::half_t>>, Is_dropout=false, Is_causal=false, Is_even_N=true, Is_even_K=false, Return_softmax=false]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(34): here
            instantiation of "void run_flash_fwd<Kernel_traits,Is_dropout,Is_causal>(Flash_fwd_params &, cudaStream_t) [with Kernel_traits=Flash_fwd_kernel_traits<160, 128, 32, 4, false, false, cutlass::half_t, Flash_kernel_traits<160, 128, 32, 4, cutlass::half_t>>, Is_dropout=false, Is_causal=false]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(155): here
            instantiation of "void run_mha_fwd_hdim160<T>(Flash_fwd_params &, cudaStream_t) [with T=cutlass::half_t]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_hdim160_fp16_sm80.cu(26): here

/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_kernel.h(276): warning: variable "tQrQ" was declared but never referenced
          detected during:
            instantiation of "void flash::compute_attn_1rowblock<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params,Is_Q_split_K>(const Params &, int, int, int) [with Kernel_traits=Flash_fwd_kernel_traits<160, 128, 32, 4, false, false, cutlass::half_t, Flash_kernel_traits<160, 128, 32, 4, cutlass::half_t>>, Is_dropout=false, Is_causal=false, Is_even_N=true, Is_even_K=false, Return_softmax=false, Params=Flash_fwd_params, Is_Q_split_K=false]" 
(589): here
            instantiation of "void flash::compute_attn<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &) [with Kernel_traits=Flash_fwd_kernel_traits<160, 128, 32, 4, false, false, cutlass::half_t, Flash_kernel_traits<160, 128, 32, 4, cutlass::half_t>>, Is_dropout=false, Is_causal=false, Is_even_N=true, Is_even_K=false, Return_softmax=false, Params=Flash_fwd_params]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(15): here
            instantiation of "void flash_fwd_kernel<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax>(Flash_fwd_params) [with Kernel_traits=Flash_fwd_kernel_traits<160, 128, 32, 4, false, false, cutlass::half_t, Flash_kernel_traits<160, 128, 32, 4, cutlass::half_t>>, Is_dropout=false, Is_causal=false, Is_even_N=true, Is_even_K=false, Return_softmax=false]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(34): here
            instantiation of "void run_flash_fwd<Kernel_traits,Is_dropout,Is_causal>(Flash_fwd_params &, cudaStream_t) [with Kernel_traits=Flash_fwd_kernel_traits<160, 128, 32, 4, false, false, cutlass::half_t, Flash_kernel_traits<160, 128, 32, 4, cutlass::half_t>>, Is_dropout=false, Is_causal=false]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(155): here
            instantiation of "void run_mha_fwd_hdim160<T>(Flash_fwd_params &, cudaStream_t) [with T=cutlass::half_t]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_hdim160_fp16_sm80.cu(26): here

/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_kernel.h(140): warning: variable "MMA_M" was declared but never referenced
          detected during:
            instantiation of "void flash::compute_attn_1rowblock<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params,Is_Q_split_K>(const Params &, int, int, int) [with Kernel_traits=Flash_fwd_kernel_traits<160, 128, 32, 4, false, false, cutlass::half_t, Flash_kernel_traits<160, 128, 32, 4, cutlass::half_t>>, Is_dropout=false, Is_causal=false, Is_even_N=false, Is_even_K=true, Return_softmax=false, Params=Flash_fwd_params, Is_Q_split_K=false]" 
(589): here
            instantiation of "void flash::compute_attn<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &) [with Kernel_traits=Flash_fwd_kernel_traits<160, 128, 32, 4, false, false, cutlass::half_t, Flash_kernel_traits<160, 128, 32, 4, cutlass::half_t>>, Is_dropout=false, Is_causal=false, Is_even_N=false, Is_even_K=true, Return_softmax=false, Params=Flash_fwd_params]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(15): here
            instantiation of "void flash_fwd_kernel<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax>(Flash_fwd_params) [with Kernel_traits=Flash_fwd_kernel_traits<160, 128, 32, 4, false, false, cutlass::half_t, Flash_kernel_traits<160, 128, 32, 4, cutlass::half_t>>, Is_dropout=false, Is_causal=false, Is_even_N=false, Is_even_K=true, Return_softmax=false]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(34): here
            instantiation of "void run_flash_fwd<Kernel_traits,Is_dropout,Is_causal>(Flash_fwd_params &, cudaStream_t) [with Kernel_traits=Flash_fwd_kernel_traits<160, 128, 32, 4, false, false, cutlass::half_t, Flash_kernel_traits<160, 128, 32, 4, cutlass::half_t>>, Is_dropout=false, Is_causal=false]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(155): here
            instantiation of "void run_mha_fwd_hdim160<T>(Flash_fwd_params &, cudaStream_t) [with T=cutlass::half_t]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_hdim160_fp16_sm80.cu(26): here

/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_kernel.h(276): warning: variable "tQrQ" was declared but never referenced
          detected during:
            instantiation of "void flash::compute_attn_1rowblock<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params,Is_Q_split_K>(const Params &, int, int, int) [with Kernel_traits=Flash_fwd_kernel_traits<160, 128, 32, 4, false, false, cutlass::half_t, Flash_kernel_traits<160, 128, 32, 4, cutlass::half_t>>, Is_dropout=false, Is_causal=false, Is_even_N=false, Is_even_K=true, Return_softmax=false, Params=Flash_fwd_params, Is_Q_split_K=false]" 
(589): here
            instantiation of "void flash::compute_attn<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &) [with Kernel_traits=Flash_fwd_kernel_traits<160, 128, 32, 4, false, false, cutlass::half_t, Flash_kernel_traits<160, 128, 32, 4, cutlass::half_t>>, Is_dropout=false, Is_causal=false, Is_even_N=false, Is_even_K=true, Return_softmax=false, Params=Flash_fwd_params]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(15): here
            instantiation of "void flash_fwd_kernel<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax>(Flash_fwd_params) [with Kernel_traits=Flash_fwd_kernel_traits<160, 128, 32, 4, false, false, cutlass::half_t, Flash_kernel_traits<160, 128, 32, 4, cutlass::half_t>>, Is_dropout=false, Is_causal=false, Is_even_N=false, Is_even_K=true, Return_softmax=false]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(34): here
            instantiation of "void run_flash_fwd<Kernel_traits,Is_dropout,Is_causal>(Flash_fwd_params &, cudaStream_t) [with Kernel_traits=Flash_fwd_kernel_traits<160, 128, 32, 4, false, false, cutlass::half_t, Flash_kernel_traits<160, 128, 32, 4, cutlass::half_t>>, Is_dropout=false, Is_causal=false]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(155): here
            instantiation of "void run_mha_fwd_hdim160<T>(Flash_fwd_params &, cudaStream_t) [with T=cutlass::half_t]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_hdim160_fp16_sm80.cu(26): here

/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_kernel.h(140): warning: variable "MMA_M" was declared but never referenced
          detected during:
            instantiation of "void flash::compute_attn_1rowblock<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params,Is_Q_split_K>(const Params &, int, int, int) [with Kernel_traits=Flash_fwd_kernel_traits<160, 128, 32, 4, false, false, cutlass::half_t, Flash_kernel_traits<160, 128, 32, 4, cutlass::half_t>>, Is_dropout=false, Is_causal=false, Is_even_N=false, Is_even_K=false, Return_softmax=false, Params=Flash_fwd_params, Is_Q_split_K=false]" 
(589): here
            instantiation of "void flash::compute_attn<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &) [with Kernel_traits=Flash_fwd_kernel_traits<160, 128, 32, 4, false, false, cutlass::half_t, Flash_kernel_traits<160, 128, 32, 4, cutlass::half_t>>, Is_dropout=false, Is_causal=false, Is_even_N=false, Is_even_K=false, Return_softmax=false, Params=Flash_fwd_params]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(15): here
            instantiation of "void flash_fwd_kernel<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax>(Flash_fwd_params) [with Kernel_traits=Flash_fwd_kernel_traits<160, 128, 32, 4, false, false, cutlass::half_t, Flash_kernel_traits<160, 128, 32, 4, cutlass::half_t>>, Is_dropout=false, Is_causal=false, Is_even_N=false, Is_even_K=false, Return_softmax=false]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(34): here
            instantiation of "void run_flash_fwd<Kernel_traits,Is_dropout,Is_causal>(Flash_fwd_params &, cudaStream_t) [with Kernel_traits=Flash_fwd_kernel_traits<160, 128, 32, 4, false, false, cutlass::half_t, Flash_kernel_traits<160, 128, 32, 4, cutlass::half_t>>, Is_dropout=false, Is_causal=false]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(155): here
            instantiation of "void run_mha_fwd_hdim160<T>(Flash_fwd_params &, cudaStream_t) [with T=cutlass::half_t]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_hdim160_fp16_sm80.cu(26): here

/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_kernel.h(276): warning: variable "tQrQ" was declared but never referenced
          detected during:
            instantiation of "void flash::compute_attn_1rowblock<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params,Is_Q_split_K>(const Params &, int, int, int) [with Kernel_traits=Flash_fwd_kernel_traits<160, 128, 32, 4, false, false, cutlass::half_t, Flash_kernel_traits<160, 128, 32, 4, cutlass::half_t>>, Is_dropout=false, Is_causal=false, Is_even_N=false, Is_even_K=false, Return_softmax=false, Params=Flash_fwd_params, Is_Q_split_K=false]" 
(589): here
            instantiation of "void flash::compute_attn<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &) [with Kernel_traits=Flash_fwd_kernel_traits<160, 128, 32, 4, false, false, cutlass::half_t, Flash_kernel_traits<160, 128, 32, 4, cutlass::half_t>>, Is_dropout=false, Is_causal=false, Is_even_N=false, Is_even_K=false, Return_softmax=false, Params=Flash_fwd_params]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(15): here
            instantiation of "void flash_fwd_kernel<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax>(Flash_fwd_params) [with Kernel_traits=Flash_fwd_kernel_traits<160, 128, 32, 4, false, false, cutlass::half_t, Flash_kernel_traits<160, 128, 32, 4, cutlass::half_t>>, Is_dropout=false, Is_causal=false, Is_even_N=false, Is_even_K=false, Return_softmax=false]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(34): here
            instantiation of "void run_flash_fwd<Kernel_traits,Is_dropout,Is_causal>(Flash_fwd_params &, cudaStream_t) [with Kernel_traits=Flash_fwd_kernel_traits<160, 128, 32, 4, false, false, cutlass::half_t, Flash_kernel_traits<160, 128, 32, 4, cutlass::half_t>>, Is_dropout=false, Is_causal=false]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(155): here
            instantiation of "void run_mha_fwd_hdim160<T>(Flash_fwd_params &, cudaStream_t) [with T=cutlass::half_t]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_hdim160_fp16_sm80.cu(26): here

ptxas info    : 2 bytes gmem
ptxas info    : Compiling entry function '_Z16flash_fwd_kernelI23Flash_fwd_kernel_traitsILi160ELi128ELi32ELi4ELb0ELb0EN7cutlass6half_tE19Flash_kernel_traitsILi160ELi128ELi32ELi4ES2_EELb0ELb0ELb0ELb0ELb0EEv16Flash_fwd_params' for 'sm_80'
ptxas info    : Function properties for _Z16flash_fwd_kernelI23Flash_fwd_kernel_traitsILi160ELi128ELi32ELi4ELb0ELb0EN7cutlass6half_tE19Flash_kernel_traitsILi160ELi128ELi32ELi4ES2_EELb0ELb0ELb0ELb0ELb0EEv16Flash_fwd_params
    32 bytes stack frame, 48 bytes spill stores, 48 bytes spill loads
ptxas info    : Used 255 registers, 584 bytes cmem[0]
ptxas info    : Compiling entry function '_Z16flash_fwd_kernelI23Flash_fwd_kernel_traitsILi160ELi128ELi32ELi4ELb0ELb0EN7cutlass6half_tE19Flash_kernel_traitsILi160ELi128ELi32ELi4ES2_EELb0ELb0ELb0ELb1ELb0EEv16Flash_fwd_params' for 'sm_80'
ptxas info    : Function properties for _Z16flash_fwd_kernelI23Flash_fwd_kernel_traitsILi160ELi128ELi32ELi4ELb0ELb0EN7cutlass6half_tE19Flash_kernel_traitsILi160ELi128ELi32ELi4ES2_EELb0ELb0ELb0ELb1ELb0EEv16Flash_fwd_params
    48 bytes stack frame, 80 bytes spill stores, 64 bytes spill loads
ptxas info    : Used 255 registers, 584 bytes cmem[0]
ptxas info    : Compiling entry function '_Z16flash_fwd_kernelI23Flash_fwd_kernel_traitsILi160ELi128ELi32ELi4ELb0ELb0EN7cutlass6half_tE19Flash_kernel_traitsILi160ELi128ELi32ELi4ES2_EELb0ELb0ELb1ELb0ELb0EEv16Flash_fwd_params' for 'sm_80'
ptxas info    : Function properties for _Z16flash_fwd_kernelI23Flash_fwd_kernel_traitsILi160ELi128ELi32ELi4ELb0ELb0EN7cutlass6half_tE19Flash_kernel_traitsILi160ELi128ELi32ELi4ES2_EELb0ELb0ELb1ELb0ELb0EEv16Flash_fwd_params
    24 bytes stack frame, 36 bytes spill stores, 36 bytes spill loads
ptxas info    : Used 255 registers, 584 bytes cmem[0]
ptxas info    : Compiling entry function '_Z16flash_fwd_kernelI23Flash_fwd_kernel_traitsILi160ELi128ELi32ELi4ELb0ELb0EN7cutlass6half_tE19Flash_kernel_traitsILi160ELi128ELi32ELi4ES2_EELb0ELb0ELb1ELb1ELb0EEv16Flash_fwd_params' for 'sm_80'
ptxas info    : Function properties for _Z16flash_fwd_kernelI23Flash_fwd_kernel_traitsILi160ELi128ELi32ELi4ELb0ELb0EN7cutlass6half_tE19Flash_kernel_traitsILi160ELi128ELi32ELi4ES2_EELb0ELb0ELb1ELb1ELb0EEv16Flash_fwd_params
    24 bytes stack frame, 40 bytes spill stores, 40 bytes spill loads
ptxas info    : Used 255 registers, 584 bytes cmem[0]
ptxas info    : Compiling entry function '_Z16flash_fwd_kernelI23Flash_fwd_kernel_traitsILi160ELi128ELi64ELi8ELb0ELb0EN7cutlass6half_tE19Flash_kernel_traitsILi160ELi128ELi64ELi8ES2_EELb0ELb0ELb0ELb0ELb0EEv16Flash_fwd_params' for 'sm_80'
ptxas info    : Function properties for _Z16flash_fwd_kernelI23Flash_fwd_kernel_traitsILi160ELi128ELi64ELi8ELb0ELb0EN7cutlass6half_tE19Flash_kernel_traitsILi160ELi128ELi64ELi8ES2_EELb0ELb0ELb0ELb0ELb0EEv16Flash_fwd_params
    0 bytes stack frame, 0 bytes spill stores, 0 bytes spill loads
ptxas info    : Used 165 registers, 584 bytes cmem[0]
ptxas info    : Compiling entry function '_Z16flash_fwd_kernelI23Flash_fwd_kernel_traitsILi160ELi128ELi64ELi8ELb0ELb0EN7cutlass6half_tE19Flash_kernel_traitsILi160ELi128ELi64ELi8ES2_EELb0ELb0ELb0ELb1ELb0EEv16Flash_fwd_params' for 'sm_80'
ptxas info    : Function properties for _Z16flash_fwd_kernelI23Flash_fwd_kernel_traitsILi160ELi128ELi64ELi8ELb0ELb0EN7cutlass6half_tE19Flash_kernel_traitsILi160ELi128ELi64ELi8ES2_EELb0ELb0ELb0ELb1ELb0EEv16Flash_fwd_params
    0 bytes stack frame, 0 bytes spill stores, 0 bytes spill loads
ptxas info    : Used 163 registers, 584 bytes cmem[0]
ptxas info    : Compiling entry function '_Z16flash_fwd_kernelI23Flash_fwd_kernel_traitsILi160ELi128ELi64ELi8ELb0ELb0EN7cutlass6half_tE19Flash_kernel_traitsILi160ELi128ELi64ELi8ES2_EELb0ELb0ELb1ELb0ELb0EEv16Flash_fwd_params' for 'sm_80'
ptxas info    : Function properties for _Z16flash_fwd_kernelI23Flash_fwd_kernel_traitsILi160ELi128ELi64ELi8ELb0ELb0EN7cutlass6half_tE19Flash_kernel_traitsILi160ELi128ELi64ELi8ES2_EELb0ELb0ELb1ELb0ELb0EEv16Flash_fwd_params
    0 bytes stack frame, 0 bytes spill stores, 0 bytes spill loads
ptxas info    : Used 171 registers, 584 bytes cmem[0]
ptxas info    : Compiling entry function '_Z16flash_fwd_kernelI23Flash_fwd_kernel_traitsILi160ELi128ELi64ELi8ELb0ELb0EN7cutlass6half_tE19Flash_kernel_traitsILi160ELi128ELi64ELi8ES2_EELb0ELb0ELb1ELb1ELb0EEv16Flash_fwd_params' for 'sm_80'
ptxas info    : Function properties for _Z16flash_fwd_kernelI23Flash_fwd_kernel_traitsILi160ELi128ELi64ELi8ELb0ELb0EN7cutlass6half_tE19Flash_kernel_traitsILi160ELi128ELi64ELi8ES2_EELb0ELb0ELb1ELb1ELb0EEv16Flash_fwd_params
    0 bytes stack frame, 0 bytes spill stores, 0 bytes spill loads
ptxas info    : Used 170 registers, 584 bytes cmem[0]
ptxas info    : Compiling entry function '_Z16flash_fwd_kernelI23Flash_fwd_kernel_traitsILi160ELi128ELi32ELi4ELb0ELb0EN7cutlass6half_tE19Flash_kernel_traitsILi160ELi128ELi32ELi4ES2_EELb0ELb1ELb0ELb0ELb0EEv16Flash_fwd_params' for 'sm_80'
ptxas info    : Function properties for _Z16flash_fwd_kernelI23Flash_fwd_kernel_traitsILi160ELi128ELi32ELi4ELb0ELb0EN7cutlass6half_tE19Flash_kernel_traitsILi160ELi128ELi32ELi4ES2_EELb0ELb1ELb0ELb0ELb0EEv16Flash_fwd_params
    56 bytes stack frame, 132 bytes spill stores, 112 bytes spill loads
ptxas info    : Used 255 registers, 584 bytes cmem[0]
ptxas info    : Compiling entry function '_Z16flash_fwd_kernelI23Flash_fwd_kernel_traitsILi160ELi128ELi32ELi4ELb0ELb0EN7cutlass6half_tE19Flash_kernel_traitsILi160ELi128ELi32ELi4ES2_EELb0ELb1ELb0ELb1ELb0EEv16Flash_fwd_params' for 'sm_80'
ptxas info    : Function properties for _Z16flash_fwd_kernelI23Flash_fwd_kernel_traitsILi160ELi128ELi32ELi4ELb0ELb0EN7cutlass6half_tE19Flash_kernel_traitsILi160ELi128ELi32ELi4ES2_EELb0ELb1ELb0ELb1ELb0EEv16Flash_fwd_params
    40 bytes stack frame, 156 bytes spill stores, 96 bytes spill loads
ptxas info    : Used 255 registers, 584 bytes cmem[0]
ptxas info    : Compiling entry function '_Z16flash_fwd_kernelI23Flash_fwd_kernel_traitsILi160ELi128ELi32ELi4ELb0ELb0EN7cutlass6half_tE19Flash_kernel_traitsILi160ELi128ELi32ELi4ES2_EELb0ELb1ELb1ELb0ELb0EEv16Flash_fwd_params' for 'sm_80'
ptxas info    : Function properties for _Z16flash_fwd_kernelI23Flash_fwd_kernel_traitsILi160ELi128ELi32ELi4ELb0ELb0EN7cutlass6half_tE19Flash_kernel_traitsILi160ELi128ELi32ELi4ES2_EELb0ELb1ELb1ELb0ELb0EEv16Flash_fwd_params
    24 bytes stack frame, 84 bytes spill stores, 44 bytes spill loads
ptxas info    : Used 255 registers, 584 bytes cmem[0]
ptxas info    : Compiling entry function '_Z16flash_fwd_kernelI23Flash_fwd_kernel_traitsILi160ELi128ELi32ELi4ELb0ELb0EN7cutlass6half_tE19Flash_kernel_traitsILi160ELi128ELi32ELi4ES2_EELb0ELb1ELb1ELb1ELb0EEv16Flash_fwd_params' for 'sm_80'
ptxas info    : Function properties for _Z16flash_fwd_kernelI23Flash_fwd_kernel_traitsILi160ELi128ELi32ELi4ELb0ELb0EN7cutlass6half_tE19Flash_kernel_traitsILi160ELi128ELi32ELi4ES2_EELb0ELb1ELb1ELb1ELb0EEv16Flash_fwd_params
    56 bytes stack frame, 176 bytes spill stores, 124 bytes spill loads
ptxas info    : Used 255 registers, 584 bytes cmem[0]
ptxas info    : Compiling entry function '_Z16flash_fwd_kernelI23Flash_fwd_kernel_traitsILi160ELi64ELi64ELi4ELb0ELb0EN7cutlass6half_tE19Flash_kernel_traitsILi160ELi64ELi64ELi4ES2_EELb0ELb1ELb0ELb0ELb0EEv16Flash_fwd_params' for 'sm_80'
ptxas info    : Function properties for _Z16flash_fwd_kernelI23Flash_fwd_kernel_traitsILi160ELi64ELi64ELi4ELb0ELb0EN7cutlass6half_tE19Flash_kernel_traitsILi160ELi64ELi64ELi4ES2_EELb0ELb1ELb0ELb0ELb0EEv16Flash_fwd_params
    0 bytes stack frame, 0 bytes spill stores, 0 bytes spill loads
ptxas info    : Used 171 registers, 584 bytes cmem[0]
ptxas info    : Compiling entry function '_Z16flash_fwd_kernelI23Flash_fwd_kernel_traitsILi160ELi64ELi64ELi4ELb0ELb0EN7cutlass6half_tE19Flash_kernel_traitsILi160ELi64ELi64ELi4ES2_EELb0ELb1ELb0ELb1ELb0EEv16Flash_fwd_params' for 'sm_80'
ptxas info    : Function properties for _Z16flash_fwd_kernelI23Flash_fwd_kernel_traitsILi160ELi64ELi64ELi4ELb0ELb0EN7cutlass6half_tE19Flash_kernel_traitsILi160ELi64ELi64ELi4ES2_EELb0ELb1ELb0ELb1ELb0EEv16Flash_fwd_params
    0 bytes stack frame, 0 bytes spill stores, 0 bytes spill loads
ptxas info    : Used 167 registers, 584 bytes cmem[0]
ptxas info    : Compiling entry function '_Z16flash_fwd_kernelI23Flash_fwd_kernel_traitsILi160ELi64ELi64ELi4ELb0ELb0EN7cutlass6half_tE19Flash_kernel_traitsILi160ELi64ELi64ELi4ES2_EELb0ELb1ELb1ELb0ELb0EEv16Flash_fwd_params' for 'sm_80'
ptxas info    : Function properties for _Z16flash_fwd_kernelI23Flash_fwd_kernel_traitsILi160ELi64ELi64ELi4ELb0ELb0EN7cutlass6half_tE19Flash_kernel_traitsILi160ELi64ELi64ELi4ES2_EELb0ELb1ELb1ELb0ELb0EEv16Flash_fwd_params
    0 bytes stack frame, 0 bytes spill stores, 0 bytes spill loads
ptxas info    : Used 168 registers, 584 bytes cmem[0]
ptxas info    : Compiling entry function '_Z16flash_fwd_kernelI23Flash_fwd_kernel_traitsILi160ELi64ELi64ELi4ELb0ELb0EN7cutlass6half_tE19Flash_kernel_traitsILi160ELi64ELi64ELi4ES2_EELb0ELb1ELb1ELb1ELb0EEv16Flash_fwd_params' for 'sm_80'
ptxas info    : Function properties for _Z16flash_fwd_kernelI23Flash_fwd_kernel_traitsILi160ELi64ELi64ELi4ELb0ELb0EN7cutlass6half_tE19Flash_kernel_traitsILi160ELi64ELi64ELi4ES2_EELb0ELb1ELb1ELb1ELb0EEv16Flash_fwd_params
    0 bytes stack frame, 0 bytes spill stores, 0 bytes spill loads
ptxas info    : Used 172 registers, 584 bytes cmem[0]
ptxas info    : Compiling entry function '_Z16flash_fwd_kernelI23Flash_fwd_kernel_traitsILi160ELi128ELi32ELi4ELb0ELb0EN7cutlass6half_tE19Flash_kernel_traitsILi160ELi128ELi32ELi4ES2_EELb1ELb0ELb0ELb0ELb0EEv16Flash_fwd_params' for 'sm_80'
ptxas info    : Function properties for _Z16flash_fwd_kernelI23Flash_fwd_kernel_traitsILi160ELi128ELi32ELi4ELb0ELb0EN7cutlass6half_tE19Flash_kernel_traitsILi160ELi128ELi32ELi4ES2_EELb1ELb0ELb0ELb0ELb0EEv16Flash_fwd_params
    32 bytes stack frame, 32 bytes spill stores, 32 bytes spill loads
ptxas info    : Used 255 registers, 584 bytes cmem[0]
ptxas info    : Compiling entry function '_Z16flash_fwd_kernelI23Flash_fwd_kernel_traitsILi160ELi128ELi32ELi4ELb0ELb0EN7cutlass6half_tE19Flash_kernel_traitsILi160ELi128ELi32ELi4ES2_EELb1ELb0ELb0ELb0ELb1EEv16Flash_fwd_params' for 'sm_80'
ptxas info    : Function properties for _Z16flash_fwd_kernelI23Flash_fwd_kernel_traitsILi160ELi128ELi32ELi4ELb0ELb0EN7cutlass6half_tE19Flash_kernel_traitsILi160ELi128ELi32ELi4ES2_EELb1ELb0ELb0ELb0ELb1EEv16Flash_fwd_params
    144 bytes stack frame, 106 bytes spill stores, 106 bytes spill loads
ptxas info    : Used 255 registers, 584 bytes cmem[0]
ptxas info    : Compiling entry function '_Z16flash_fwd_kernelI23Flash_fwd_kernel_traitsILi160ELi128ELi32ELi4ELb0ELb0EN7cutlass6half_tE19Flash_kernel_traitsILi160ELi128ELi32ELi4ES2_EELb1ELb0ELb0ELb1ELb0EEv16Flash_fwd_params' for 'sm_80'
ptxas info    : Function properties for _Z16flash_fwd_kernelI23Flash_fwd_kernel_traitsILi160ELi128ELi32ELi4ELb0ELb0EN7cutlass6half_tE19Flash_kernel_traitsILi160ELi128ELi32ELi4ES2_EELb1ELb0ELb0ELb1ELb0EEv16Flash_fwd_params
    32 bytes stack frame, 32 bytes spill stores, 32 bytes spill loads
ptxas info    : Used 255 registers, 584 bytes cmem[0]
ptxas info    : Compiling entry function '_Z16flash_fwd_kernelI23Flash_fwd_kernel_traitsILi160ELi128ELi32ELi4ELb0ELb0EN7cutlass6half_tE19Flash_kernel_traitsILi160ELi128ELi32ELi4ES2_EELb1ELb0ELb0ELb1ELb1EEv16Flash_fwd_params' for 'sm_80'
ptxas info    : Function properties for _Z16flash_fwd_kernelI23Flash_fwd_kernel_traitsILi160ELi128ELi32ELi4ELb0ELb0EN7cutlass6half_tE19Flash_kernel_traitsILi160ELi128ELi32ELi4ES2_EELb1ELb0ELb0ELb1ELb1EEv16Flash_fwd_params
    128 bytes stack frame, 94 bytes spill stores, 100 bytes spill loads
ptxas info    : Used 255 registers, 584 bytes cmem[0]
ptxas info    : Compiling entry function '_Z16flash_fwd_kernelI23Flash_fwd_kernel_traitsILi160ELi128ELi32ELi4ELb0ELb0EN7cutlass6half_tE19Flash_kernel_traitsILi160ELi128ELi32ELi4ES2_EELb1ELb0ELb1ELb0ELb0EEv16Flash_fwd_params' for 'sm_80'
ptxas info    : Function properties for _Z16flash_fwd_kernelI23Flash_fwd_kernel_traitsILi160ELi128ELi32ELi4ELb0ELb0EN7cutlass6half_tE19Flash_kernel_traitsILi160ELi128ELi32ELi4ES2_EELb1ELb0ELb1ELb0ELb0EEv16Flash_fwd_params
    32 bytes stack frame, 32 bytes spill stores, 32 bytes spill loads
ptxas info    : Used 255 registers, 584 bytes cmem[0]
ptxas info    : Compiling entry function '_Z16flash_fwd_kernelI23Flash_fwd_kernel_traitsILi160ELi128ELi32ELi4ELb0ELb0EN7cutlass6half_tE19Flash_kernel_traitsILi160ELi128ELi32ELi4ES2_EELb1ELb0ELb1ELb0ELb1EEv16Flash_fwd_params' for 'sm_80'
ptxas info    : Function properties for _Z16flash_fwd_kernelI23Flash_fwd_kernel_traitsILi160ELi128ELi32ELi4ELb0ELb0EN7cutlass6half_tE19Flash_kernel_traitsILi160ELi128ELi32ELi4ES2_EELb1ELb0ELb1ELb0ELb1EEv16Flash_fwd_params
    168 bytes stack frame, 120 bytes spill stores, 120 bytes spill loads
ptxas info    : Used 255 registers, 584 bytes cmem[0]
ptxas info    : Compiling entry function '_Z16flash_fwd_kernelI23Flash_fwd_kernel_traitsILi160ELi128ELi32ELi4ELb0ELb0EN7cutlass6half_tE19Flash_kernel_traitsILi160ELi128ELi32ELi4ES2_EELb1ELb0ELb1ELb1ELb0EEv16Flash_fwd_params' for 'sm_80'
ptxas info    : Function properties for _Z16flash_fwd_kernelI23Flash_fwd_kernel_traitsILi160ELi128ELi32ELi4ELb0ELb0EN7cutlass6half_tE19Flash_kernel_traitsILi160ELi128ELi32ELi4ES2_EELb1ELb0ELb1ELb1ELb0EEv16Flash_fwd_params
    16 bytes stack frame, 16 bytes spill stores, 16 bytes spill loads
ptxas info    : Used 255 registers, 584 bytes cmem[0]
ptxas info    : Compiling entry function '_Z16flash_fwd_kernelI23Flash_fwd_kernel_traitsILi160ELi128ELi32ELi4ELb0ELb0EN7cutlass6half_tE19Flash_kernel_traitsILi160ELi128ELi32ELi4ES2_EELb1ELb0ELb1ELb1ELb1EEv16Flash_fwd_params' for 'sm_80'
ptxas info    : Function properties for _Z16flash_fwd_kernelI23Flash_fwd_kernel_traitsILi160ELi128ELi32ELi4ELb0ELb0EN7cutlass6half_tE19Flash_kernel_traitsILi160ELi128ELi32ELi4ES2_EELb1ELb0ELb1ELb1ELb1EEv16Flash_fwd_params
    120 bytes stack frame, 82 bytes spill stores, 98 bytes spill loads
ptxas info    : Used 255 registers, 584 bytes cmem[0]
ptxas info    : Compiling entry function '_Z16flash_fwd_kernelI23Flash_fwd_kernel_traitsILi160ELi128ELi64ELi8ELb0ELb0EN7cutlass6half_tE19Flash_kernel_traitsILi160ELi128ELi64ELi8ES2_EELb1ELb0ELb0ELb0ELb0EEv16Flash_fwd_params' for 'sm_80'
ptxas info    : Function properties for _Z16flash_fwd_kernelI23Flash_fwd_kernel_traitsILi160ELi128ELi64ELi8ELb0ELb0EN7cutlass6half_tE19Flash_kernel_traitsILi160ELi128ELi64ELi8ES2_EELb1ELb0ELb0ELb0ELb0EEv16Flash_fwd_params
    0 bytes stack frame, 0 bytes spill stores, 0 bytes spill loads
ptxas info    : Used 176 registers, 584 bytes cmem[0]
ptxas info    : Compiling entry function '_Z16flash_fwd_kernelI23Flash_fwd_kernel_traitsILi160ELi128ELi64ELi8ELb0ELb0EN7cutlass6half_tE19Flash_kernel_traitsILi160ELi128ELi64ELi8ES2_EELb1ELb0ELb0ELb0ELb1EEv16Flash_fwd_params' for 'sm_80'
ptxas info    : Function properties for _Z16flash_fwd_kernelI23Flash_fwd_kernel_traitsILi160ELi128ELi64ELi8ELb0ELb0EN7cutlass6half_tE19Flash_kernel_traitsILi160ELi128ELi64ELi8ES2_EELb1ELb0ELb0ELb0ELb1EEv16Flash_fwd_params
    0 bytes stack frame, 0 bytes spill stores, 0 bytes spill loads
ptxas info    : Used 252 registers, 584 bytes cmem[0]
ptxas info    : Compiling entry function '_Z16flash_fwd_kernelI23Flash_fwd_kernel_traitsILi160ELi128ELi64ELi8ELb0ELb0EN7cutlass6half_tE19Flash_kernel_traitsILi160ELi128ELi64ELi8ES2_EELb1ELb0ELb0ELb1ELb0EEv16Flash_fwd_params' for 'sm_80'
ptxas info    : Function properties for _Z16flash_fwd_kernelI23Flash_fwd_kernel_traitsILi160ELi128ELi64ELi8ELb0ELb0EN7cutlass6half_tE19Flash_kernel_traitsILi160ELi128ELi64ELi8ES2_EELb1ELb0ELb0ELb1ELb0EEv16Flash_fwd_params
    0 bytes stack frame, 0 bytes spill stores, 0 bytes spill loads
ptxas info    : Used 176 registers, 584 bytes cmem[0]
ptxas info    : Compiling entry function '_Z16flash_fwd_kernelI23Flash_fwd_kernel_traitsILi160ELi128ELi64ELi8ELb0ELb0EN7cutlass6half_tE19Flash_kernel_traitsILi160ELi128ELi64ELi8ES2_EELb1ELb0ELb0ELb1ELb1EEv16Flash_fwd_params' for 'sm_80'
ptxas info    : Function properties for _Z16flash_fwd_kernelI23Flash_fwd_kernel_traitsILi160ELi128ELi64ELi8ELb0ELb0EN7cutlass6half_tE19Flash_kernel_traitsILi160ELi128ELi64ELi8ES2_EELb1ELb0ELb0ELb1ELb1EEv16Flash_fwd_params
    0 bytes stack frame, 0 bytes spill stores, 0 bytes spill loads
ptxas info    : Used 253 registers, 584 bytes cmem[0]
ptxas info    : Compiling entry function '_Z16flash_fwd_kernelI23Flash_fwd_kernel_traitsILi160ELi128ELi64ELi8ELb0ELb0EN7cutlass6half_tE19Flash_kernel_traitsILi160ELi128ELi64ELi8ES2_EELb1ELb0ELb1ELb0ELb0EEv16Flash_fwd_params' for 'sm_80'
ptxas info    : Function properties for _Z16flash_fwd_kernelI23Flash_fwd_kernel_traitsILi160ELi128ELi64ELi8ELb0ELb0EN7cutlass6half_tE19Flash_kernel_traitsILi160ELi128ELi64ELi8ES2_EELb1ELb0ELb1ELb0ELb0EEv16Flash_fwd_params
    0 bytes stack frame, 0 bytes spill stores, 0 bytes spill loads
ptxas info    : Used 172 registers, 584 bytes cmem[0]
ptxas info    : Compiling entry function '_Z16flash_fwd_kernelI23Flash_fwd_kernel_traitsILi160ELi128ELi64ELi8ELb0ELb0EN7cutlass6half_tE19Flash_kernel_traitsILi160ELi128ELi64ELi8ES2_EELb1ELb0ELb1ELb0ELb1EEv16Flash_fwd_params' for 'sm_80'
ptxas info    : Function properties for _Z16flash_fwd_kernelI23Flash_fwd_kernel_traitsILi160ELi128ELi64ELi8ELb0ELb0EN7cutlass6half_tE19Flash_kernel_traitsILi160ELi128ELi64ELi8ES2_EELb1ELb0ELb1ELb0ELb1EEv16Flash_fwd_params
    0 bytes stack frame, 0 bytes spill stores, 0 bytes spill loads
ptxas info    : Used 250 registers, 584 bytes cmem[0]
ptxas info    : Compiling entry function '_Z16flash_fwd_kernelI23Flash_fwd_kernel_traitsILi160ELi128ELi64ELi8ELb0ELb0EN7cutlass6half_tE19Flash_kernel_traitsILi160ELi128ELi64ELi8ES2_EELb1ELb0ELb1ELb1ELb0EEv16Flash_fwd_params' for 'sm_80'
ptxas info    : Function properties for _Z16flash_fwd_kernelI23Flash_fwd_kernel_traitsILi160ELi128ELi64ELi8ELb0ELb0EN7cutlass6half_tE19Flash_kernel_traitsILi160ELi128ELi64ELi8ES2_EELb1ELb0ELb1ELb1ELb0EEv16Flash_fwd_params
    0 bytes stack frame, 0 bytes spill stores, 0 bytes spill loads
ptxas info    : Used 168 registers, 584 bytes cmem[0]
ptxas info    : Compiling entry function '_Z16flash_fwd_kernelI23Flash_fwd_kernel_traitsILi160ELi128ELi64ELi8ELb0ELb0EN7cutlass6half_tE19Flash_kernel_traitsILi160ELi128ELi64ELi8ES2_EELb1ELb0ELb1ELb1ELb1EEv16Flash_fwd_params' for 'sm_80'
ptxas info    : Function properties for _Z16flash_fwd_kernelI23Flash_fwd_kernel_traitsILi160ELi128ELi64ELi8ELb0ELb0EN7cutlass6half_tE19Flash_kernel_traitsILi160ELi128ELi64ELi8ES2_EELb1ELb0ELb1ELb1ELb1EEv16Flash_fwd_params
    0 bytes stack frame, 0 bytes spill stores, 0 bytes spill loads
ptxas info    : Used 247 registers, 584 bytes cmem[0]
ptxas info    : Compiling entry function '_Z16flash_fwd_kernelI23Flash_fwd_kernel_traitsILi160ELi128ELi32ELi4ELb0ELb0EN7cutlass6half_tE19Flash_kernel_traitsILi160ELi128ELi32ELi4ES2_EELb1ELb1ELb0ELb0ELb0EEv16Flash_fwd_params' for 'sm_80'
ptxas info    : Function properties for _Z16flash_fwd_kernelI23Flash_fwd_kernel_traitsILi160ELi128ELi32ELi4ELb0ELb0EN7cutlass6half_tE19Flash_kernel_traitsILi160ELi128ELi32ELi4ES2_EELb1ELb1ELb0ELb0ELb0EEv16Flash_fwd_params
    152 bytes stack frame, 204 bytes spill stores, 268 bytes spill loads
ptxas info    : Used 255 registers, 584 bytes cmem[0]
ptxas info    : Compiling entry function '_Z16flash_fwd_kernelI23Flash_fwd_kernel_traitsILi160ELi128ELi32ELi4ELb0ELb0EN7cutlass6half_tE19Flash_kernel_traitsILi160ELi128ELi32ELi4ES2_EELb1ELb1ELb0ELb0ELb1EEv16Flash_fwd_params' for 'sm_80'
ptxas info    : Function properties for _Z16flash_fwd_kernelI23Flash_fwd_kernel_traitsILi160ELi128ELi32ELi4ELb0ELb0EN7cutlass6half_tE19Flash_kernel_traitsILi160ELi128ELi32ELi4ES2_EELb1ELb1ELb0ELb0ELb1EEv16Flash_fwd_params
    256 bytes stack frame, 428 bytes spill stores, 588 bytes spill loads
ptxas info    : Used 255 registers, 584 bytes cmem[0]
ptxas info    : Compiling entry function '_Z16flash_fwd_kernelI23Flash_fwd_kernel_traitsILi160ELi128ELi32ELi4ELb0ELb0EN7cutlass6half_tE19Flash_kernel_traitsILi160ELi128ELi32ELi4ES2_EELb1ELb1ELb0ELb1ELb0EEv16Flash_fwd_params' for 'sm_80'
ptxas info    : Function properties for _Z16flash_fwd_kernelI23Flash_fwd_kernel_traitsILi160ELi128ELi32ELi4ELb0ELb0EN7cutlass6half_tE19Flash_kernel_traitsILi160ELi128ELi32ELi4ES2_EELb1ELb1ELb0ELb1ELb0EEv16Flash_fwd_params
    112 bytes stack frame, 252 bytes spill stores, 228 bytes spill loads
ptxas info    : Used 255 registers, 584 bytes cmem[0]
ptxas info    : Compiling entry function '_Z16flash_fwd_kernelI23Flash_fwd_kernel_traitsILi160ELi128ELi32ELi4ELb0ELb0EN7cutlass6half_tE19Flash_kernel_traitsILi160ELi128ELi32ELi4ES2_EELb1ELb1ELb0ELb1ELb1EEv16Flash_fwd_params' for 'sm_80'
ptxas info    : Function properties for _Z16flash_fwd_kernelI23Flash_fwd_kernel_traitsILi160ELi128ELi32ELi4ELb0ELb0EN7cutlass6half_tE19Flash_kernel_traitsILi160ELi128ELi32ELi4ES2_EELb1ELb1ELb0ELb1ELb1EEv16Flash_fwd_params
    264 bytes stack frame, 422 bytes spill stores, 554 bytes spill loads
ptxas info    : Used 255 registers, 584 bytes cmem[0]
ptxas info    : Compiling entry function '_Z16flash_fwd_kernelI23Flash_fwd_kernel_traitsILi160ELi128ELi32ELi4ELb0ELb0EN7cutlass6half_tE19Flash_kernel_traitsILi160ELi128ELi32ELi4ES2_EELb1ELb1ELb1ELb0ELb0EEv16Flash_fwd_params' for 'sm_80'
ptxas info    : Function properties for _Z16flash_fwd_kernelI23Flash_fwd_kernel_traitsILi160ELi128ELi32ELi4ELb0ELb0EN7cutlass6half_tE19Flash_kernel_traitsILi160ELi128ELi32ELi4ES2_EELb1ELb1ELb1ELb0ELb0EEv16Flash_fwd_params
    88 bytes stack frame, 144 bytes spill stores, 168 bytes spill loads
ptxas info    : Used 255 registers, 584 bytes cmem[0]
ptxas info    : Compiling entry function '_Z16flash_fwd_kernelI23Flash_fwd_kernel_traitsILi160ELi128ELi32ELi4ELb0ELb0EN7cutlass6half_tE19Flash_kernel_traitsILi160ELi128ELi32ELi4ES2_EELb1ELb1ELb1ELb0ELb1EEv16Flash_fwd_params' for 'sm_80'
ptxas info    : Function properties for _Z16flash_fwd_kernelI23Flash_fwd_kernel_traitsILi160ELi128ELi32ELi4ELb0ELb0EN7cutlass6half_tE19Flash_kernel_traitsILi160ELi128ELi32ELi4ES2_EELb1ELb1ELb1ELb0ELb1EEv16Flash_fwd_params
    208 bytes stack frame, 366 bytes spill stores, 488 bytes spill loads
ptxas info    : Used 255 registers, 584 bytes cmem[0]
ptxas info    : Compiling entry function '_Z16flash_fwd_kernelI23Flash_fwd_kernel_traitsILi160ELi128ELi32ELi4ELb0ELb0EN7cutlass6half_tE19Flash_kernel_traitsILi160ELi128ELi32ELi4ES2_EELb1ELb1ELb1ELb1ELb0EEv16Flash_fwd_params' for 'sm_80'
ptxas info    : Function properties for _Z16flash_fwd_kernelI23Flash_fwd_kernel_traitsILi160ELi128ELi32ELi4ELb0ELb0EN7cutlass6half_tE19Flash_kernel_traitsILi160ELi128ELi32ELi4ES2_EELb1ELb1ELb1ELb1ELb0EEv16Flash_fwd_params
    120 bytes stack frame, 196 bytes spill stores, 236 bytes spill loads
ptxas info    : Used 255 registers, 584 bytes cmem[0]
ptxas info    : Compiling entry function '_Z16flash_fwd_kernelI23Flash_fwd_kernel_traitsILi160ELi128ELi32ELi4ELb0ELb0EN7cutlass6half_tE19Flash_kernel_traitsILi160ELi128ELi32ELi4ES2_EELb1ELb1ELb1ELb1ELb1EEv16Flash_fwd_params' for 'sm_80'
ptxas info    : Function properties for _Z16flash_fwd_kernelI23Flash_fwd_kernel_traitsILi160ELi128ELi32ELi4ELb0ELb0EN7cutlass6half_tE19Flash_kernel_traitsILi160ELi128ELi32ELi4ES2_EELb1ELb1ELb1ELb1ELb1EEv16Flash_fwd_params
    288 bytes stack frame, 508 bytes spill stores, 648 bytes spill loads
ptxas info    : Used 255 registers, 584 bytes cmem[0]
ptxas info    : Compiling entry function '_Z16flash_fwd_kernelI23Flash_fwd_kernel_traitsILi160ELi64ELi64ELi4ELb0ELb0EN7cutlass6half_tE19Flash_kernel_traitsILi160ELi64ELi64ELi4ES2_EELb1ELb1ELb0ELb0ELb0EEv16Flash_fwd_params' for 'sm_80'
ptxas info    : Function properties for _Z16flash_fwd_kernelI23Flash_fwd_kernel_traitsILi160ELi64ELi64ELi4ELb0ELb0EN7cutlass6half_tE19Flash_kernel_traitsILi160ELi64ELi64ELi4ES2_EELb1ELb1ELb0ELb0ELb0EEv16Flash_fwd_params
    0 bytes stack frame, 0 bytes spill stores, 0 bytes spill loads
ptxas info    : Used 176 registers, 584 bytes cmem[0]
ptxas info    : Compiling entry function '_Z16flash_fwd_kernelI23Flash_fwd_kernel_traitsILi160ELi64ELi64ELi4ELb0ELb0EN7cutlass6half_tE19Flash_kernel_traitsILi160ELi64ELi64ELi4ES2_EELb1ELb1ELb0ELb0ELb1EEv16Flash_fwd_params' for 'sm_80'
ptxas info    : Function properties for _Z16flash_fwd_kernelI23Flash_fwd_kernel_traitsILi160ELi64ELi64ELi4ELb0ELb0EN7cutlass6half_tE19Flash_kernel_traitsILi160ELi64ELi64ELi4ES2_EELb1ELb1ELb0ELb0ELb1EEv16Flash_fwd_params
    0 bytes stack frame, 0 bytes spill stores, 0 bytes spill loads
ptxas info    : Used 252 registers, 584 bytes cmem[0]
ptxas info    : Compiling entry function '_Z16flash_fwd_kernelI23Flash_fwd_kernel_traitsILi160ELi64ELi64ELi4ELb0ELb0EN7cutlass6half_tE19Flash_kernel_traitsILi160ELi64ELi64ELi4ES2_EELb1ELb1ELb0ELb1ELb0EEv16Flash_fwd_params' for 'sm_80'
ptxas info    : Function properties for _Z16flash_fwd_kernelI23Flash_fwd_kernel_traitsILi160ELi64ELi64ELi4ELb0ELb0EN7cutlass6half_tE19Flash_kernel_traitsILi160ELi64ELi64ELi4ES2_EELb1ELb1ELb0ELb1ELb0EEv16Flash_fwd_params
    0 bytes stack frame, 0 bytes spill stores, 0 bytes spill loads
ptxas info    : Used 182 registers, 584 bytes cmem[0]
ptxas info    : Compiling entry function '_Z16flash_fwd_kernelI23Flash_fwd_kernel_traitsILi160ELi64ELi64ELi4ELb0ELb0EN7cutlass6half_tE19Flash_kernel_traitsILi160ELi64ELi64ELi4ES2_EELb1ELb1ELb0ELb1ELb1EEv16Flash_fwd_params' for 'sm_80'
ptxas info    : Function properties for _Z16flash_fwd_kernelI23Flash_fwd_kernel_traitsILi160ELi64ELi64ELi4ELb0ELb0EN7cutlass6half_tE19Flash_kernel_traitsILi160ELi64ELi64ELi4ES2_EELb1ELb1ELb0ELb1ELb1EEv16Flash_fwd_params
    0 bytes stack frame, 0 bytes spill stores, 0 bytes spill loads
ptxas info    : Used 253 registers, 584 bytes cmem[0]
ptxas info    : Compiling entry function '_Z16flash_fwd_kernelI23Flash_fwd_kernel_traitsILi160ELi64ELi64ELi4ELb0ELb0EN7cutlass6half_tE19Flash_kernel_traitsILi160ELi64ELi64ELi4ES2_EELb1ELb1ELb1ELb0ELb0EEv16Flash_fwd_params' for 'sm_80'
ptxas info    : Function properties for _Z16flash_fwd_kernelI23Flash_fwd_kernel_traitsILi160ELi64ELi64ELi4ELb0ELb0EN7cutlass6half_tE19Flash_kernel_traitsILi160ELi64ELi64ELi4ES2_EELb1ELb1ELb1ELb0ELb0EEv16Flash_fwd_params
    0 bytes stack frame, 0 bytes spill stores, 0 bytes spill loads
ptxas info    : Used 182 registers, 584 bytes cmem[0]
ptxas info    : Compiling entry function '_Z16flash_fwd_kernelI23Flash_fwd_kernel_traitsILi160ELi64ELi64ELi4ELb0ELb0EN7cutlass6half_tE19Flash_kernel_traitsILi160ELi64ELi64ELi4ES2_EELb1ELb1ELb1ELb0ELb1EEv16Flash_fwd_params' for 'sm_80'
ptxas info    : Function properties for _Z16flash_fwd_kernelI23Flash_fwd_kernel_traitsILi160ELi64ELi64ELi4ELb0ELb0EN7cutlass6half_tE19Flash_kernel_traitsILi160ELi64ELi64ELi4ES2_EELb1ELb1ELb1ELb0ELb1EEv16Flash_fwd_params
    0 bytes stack frame, 0 bytes spill stores, 0 bytes spill loads
ptxas info    : Used 250 registers, 584 bytes cmem[0]
ptxas info    : Compiling entry function '_Z16flash_fwd_kernelI23Flash_fwd_kernel_traitsILi160ELi64ELi64ELi4ELb0ELb0EN7cutlass6half_tE19Flash_kernel_traitsILi160ELi64ELi64ELi4ES2_EELb1ELb1ELb1ELb1ELb0EEv16Flash_fwd_params' for 'sm_80'
ptxas info    : Function properties for _Z16flash_fwd_kernelI23Flash_fwd_kernel_traitsILi160ELi64ELi64ELi4ELb0ELb0EN7cutlass6half_tE19Flash_kernel_traitsILi160ELi64ELi64ELi4ES2_EELb1ELb1ELb1ELb1ELb0EEv16Flash_fwd_params
    0 bytes stack frame, 0 bytes spill stores, 0 bytes spill loads
ptxas info    : Used 188 registers, 584 bytes cmem[0]
ptxas info    : Compiling entry function '_Z16flash_fwd_kernelI23Flash_fwd_kernel_traitsILi160ELi64ELi64ELi4ELb0ELb0EN7cutlass6half_tE19Flash_kernel_traitsILi160ELi64ELi64ELi4ES2_EELb1ELb1ELb1ELb1ELb1EEv16Flash_fwd_params' for 'sm_80'
ptxas info    : Function properties for _Z16flash_fwd_kernelI23Flash_fwd_kernel_traitsILi160ELi64ELi64ELi4ELb0ELb0EN7cutlass6half_tE19Flash_kernel_traitsILi160ELi64ELi64ELi4ES2_EELb1ELb1ELb1ELb1ELb1EEv16Flash_fwd_params
    0 bytes stack frame, 0 bytes spill stores, 0 bytes spill loads
ptxas info    : Used 253 registers, 584 bytes cmem[0]
/anaconda/envs/flashattention/lib/python3.9/site-packages/torch/include/c10/core/SymInt.h(84): warning: integer conversion resulted in a change of sign

[27/37] /usr/local/cuda-11.4/bin/nvcc  -I/home/v-feiychen/flash-attention/csrc/flash_attn -I/home/v-feiychen/flash-attention/csrc/flash_attn/src -I/home/v-feiychen/flash-attention/csrc/cutlass/include -I/anaconda/envs/flashattention/lib/python3.9/site-packages/torch/include -I/anaconda/envs/flashattention/lib/python3.9/site-packages/torch/include/torch/csrc/api/include -I/anaconda/envs/flashattention/lib/python3.9/site-packages/torch/include/TH -I/anaconda/envs/flashattention/lib/python3.9/site-packages/torch/include/THC -I/usr/local/cuda-11.4/include -I/anaconda/envs/flashattention/include/python3.9 -c -c /home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_hdim160_bf16_sm80.cu -o /home/v-feiychen/flash-attention/build/temp.linux-x86_64-cpython-39/csrc/flash_attn/src/flash_fwd_hdim160_bf16_sm80.o -D__CUDA_NO_HALF_OPERATORS__ -D__CUDA_NO_HALF_CONVERSIONS__ -D__CUDA_NO_BFLOAT16_CONVERSIONS__ -D__CUDA_NO_HALF2_OPERATORS__ --expt-relaxed-constexpr --compiler-options ''"'"'-fPIC'"'"'' -O3 -std=c++17 -U__CUDA_NO_HALF_OPERATORS__ -U__CUDA_NO_HALF_CONVERSIONS__ -U__CUDA_NO_HALF2_OPERATORS__ -U__CUDA_NO_BFLOAT16_CONVERSIONS__ --expt-relaxed-constexpr --expt-extended-lambda --use_fast_math --ptxas-options=-v -lineinfo -gencode arch=compute_80,code=sm_80 --threads 4 -DTORCH_API_INCLUDE_EXTENSION_H '-DPYBIND11_COMPILER_TYPE="_gcc"' '-DPYBIND11_STDLIB="_libstdcpp"' '-DPYBIND11_BUILD_ABI="_cxxabi1011"' -DTORCH_EXTENSION_NAME=flash_attn_2_cuda -D_GLIBCXX_USE_CXX11_ABI=0
/anaconda/envs/flashattention/lib/python3.9/site-packages/torch/include/c10/core/SymInt.h(84): warning: integer conversion resulted in a change of sign

/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_kernel.h(140): warning: variable "MMA_M" was declared but never referenced
          detected during:
            instantiation of "void flash::compute_attn_1rowblock<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params,Is_Q_split_K>(const Params &, int, int, int) [with Kernel_traits=Flash_fwd_kernel_traits<160, 64, 64, 4, false, false, cutlass::bfloat16_t, Flash_kernel_traits<160, 64, 64, 4, cutlass::bfloat16_t>>, Is_dropout=true, Is_causal=true, Is_even_N=true, Is_even_K=true, Return_softmax=true, Params=Flash_fwd_params, Is_Q_split_K=false]" 
(589): here
            instantiation of "void flash::compute_attn<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &) [with Kernel_traits=Flash_fwd_kernel_traits<160, 64, 64, 4, false, false, cutlass::bfloat16_t, Flash_kernel_traits<160, 64, 64, 4, cutlass::bfloat16_t>>, Is_dropout=true, Is_causal=true, Is_even_N=true, Is_even_K=true, Return_softmax=true, Params=Flash_fwd_params]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(15): here
            instantiation of "void flash_fwd_kernel<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax>(Flash_fwd_params) [with Kernel_traits=Flash_fwd_kernel_traits<160, 64, 64, 4, false, false, cutlass::bfloat16_t, Flash_kernel_traits<160, 64, 64, 4, cutlass::bfloat16_t>>, Is_dropout=true, Is_causal=true, Is_even_N=true, Is_even_K=true, Return_softmax=true]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(34): here
            instantiation of "void run_flash_fwd<Kernel_traits,Is_dropout,Is_causal>(Flash_fwd_params &, cudaStream_t) [with Kernel_traits=Flash_fwd_kernel_traits<160, 64, 64, 4, false, false, cutlass::bfloat16_t, Flash_kernel_traits<160, 64, 64, 4, cutlass::bfloat16_t>>, Is_dropout=true, Is_causal=true]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(155): here
            instantiation of "void run_mha_fwd_hdim160<T>(Flash_fwd_params &, cudaStream_t) [with T=cutlass::bfloat16_t]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_hdim160_bf16_sm80.cu(16): here

/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_kernel.h(276): warning: variable "tQrQ" was declared but never referenced
          detected during:
            instantiation of "void flash::compute_attn_1rowblock<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params,Is_Q_split_K>(const Params &, int, int, int) [with Kernel_traits=Flash_fwd_kernel_traits<160, 64, 64, 4, false, false, cutlass::bfloat16_t, Flash_kernel_traits<160, 64, 64, 4, cutlass::bfloat16_t>>, Is_dropout=true, Is_causal=true, Is_even_N=true, Is_even_K=true, Return_softmax=true, Params=Flash_fwd_params, Is_Q_split_K=false]" 
(589): here
            instantiation of "void flash::compute_attn<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &) [with Kernel_traits=Flash_fwd_kernel_traits<160, 64, 64, 4, false, false, cutlass::bfloat16_t, Flash_kernel_traits<160, 64, 64, 4, cutlass::bfloat16_t>>, Is_dropout=true, Is_causal=true, Is_even_N=true, Is_even_K=true, Return_softmax=true, Params=Flash_fwd_params]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(15): here
            instantiation of "void flash_fwd_kernel<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax>(Flash_fwd_params) [with Kernel_traits=Flash_fwd_kernel_traits<160, 64, 64, 4, false, false, cutlass::bfloat16_t, Flash_kernel_traits<160, 64, 64, 4, cutlass::bfloat16_t>>, Is_dropout=true, Is_causal=true, Is_even_N=true, Is_even_K=true, Return_softmax=true]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(34): here
            instantiation of "void run_flash_fwd<Kernel_traits,Is_dropout,Is_causal>(Flash_fwd_params &, cudaStream_t) [with Kernel_traits=Flash_fwd_kernel_traits<160, 64, 64, 4, false, false, cutlass::bfloat16_t, Flash_kernel_traits<160, 64, 64, 4, cutlass::bfloat16_t>>, Is_dropout=true, Is_causal=true]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(155): here
            instantiation of "void run_mha_fwd_hdim160<T>(Flash_fwd_params &, cudaStream_t) [with T=cutlass::bfloat16_t]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_hdim160_bf16_sm80.cu(16): here

/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_kernel.h(140): warning: variable "MMA_M" was declared but never referenced
          detected during:
            instantiation of "void flash::compute_attn_1rowblock<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params,Is_Q_split_K>(const Params &, int, int, int) [with Kernel_traits=Flash_fwd_kernel_traits<160, 64, 64, 4, false, false, cutlass::bfloat16_t, Flash_kernel_traits<160, 64, 64, 4, cutlass::bfloat16_t>>, Is_dropout=true, Is_causal=true, Is_even_N=true, Is_even_K=true, Return_softmax=false, Params=Flash_fwd_params, Is_Q_split_K=false]" 
(589): here
            instantiation of "void flash::compute_attn<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &) [with Kernel_traits=Flash_fwd_kernel_traits<160, 64, 64, 4, false, false, cutlass::bfloat16_t, Flash_kernel_traits<160, 64, 64, 4, cutlass::bfloat16_t>>, Is_dropout=true, Is_causal=true, Is_even_N=true, Is_even_K=true, Return_softmax=false, Params=Flash_fwd_params]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(15): here
            instantiation of "void flash_fwd_kernel<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax>(Flash_fwd_params) [with Kernel_traits=Flash_fwd_kernel_traits<160, 64, 64, 4, false, false, cutlass::bfloat16_t, Flash_kernel_traits<160, 64, 64, 4, cutlass::bfloat16_t>>, Is_dropout=true, Is_causal=true, Is_even_N=true, Is_even_K=true, Return_softmax=false]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(34): here
            instantiation of "void run_flash_fwd<Kernel_traits,Is_dropout,Is_causal>(Flash_fwd_params &, cudaStream_t) [with Kernel_traits=Flash_fwd_kernel_traits<160, 64, 64, 4, false, false, cutlass::bfloat16_t, Flash_kernel_traits<160, 64, 64, 4, cutlass::bfloat16_t>>, Is_dropout=true, Is_causal=true]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(155): here
            instantiation of "void run_mha_fwd_hdim160<T>(Flash_fwd_params &, cudaStream_t) [with T=cutlass::bfloat16_t]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_hdim160_bf16_sm80.cu(16): here

/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_kernel.h(276): warning: variable "tQrQ" was declared but never referenced
          detected during:
            instantiation of "void flash::compute_attn_1rowblock<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params,Is_Q_split_K>(const Params &, int, int, int) [with Kernel_traits=Flash_fwd_kernel_traits<160, 64, 64, 4, false, false, cutlass::bfloat16_t, Flash_kernel_traits<160, 64, 64, 4, cutlass::bfloat16_t>>, Is_dropout=true, Is_causal=true, Is_even_N=true, Is_even_K=true, Return_softmax=false, Params=Flash_fwd_params, Is_Q_split_K=false]" 
(589): here
            instantiation of "void flash::compute_attn<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &) [with Kernel_traits=Flash_fwd_kernel_traits<160, 64, 64, 4, false, false, cutlass::bfloat16_t, Flash_kernel_traits<160, 64, 64, 4, cutlass::bfloat16_t>>, Is_dropout=true, Is_causal=true, Is_even_N=true, Is_even_K=true, Return_softmax=false, Params=Flash_fwd_params]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(15): here
            instantiation of "void flash_fwd_kernel<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax>(Flash_fwd_params) [with Kernel_traits=Flash_fwd_kernel_traits<160, 64, 64, 4, false, false, cutlass::bfloat16_t, Flash_kernel_traits<160, 64, 64, 4, cutlass::bfloat16_t>>, Is_dropout=true, Is_causal=true, Is_even_N=true, Is_even_K=true, Return_softmax=false]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(34): here
            instantiation of "void run_flash_fwd<Kernel_traits,Is_dropout,Is_causal>(Flash_fwd_params &, cudaStream_t) [with Kernel_traits=Flash_fwd_kernel_traits<160, 64, 64, 4, false, false, cutlass::bfloat16_t, Flash_kernel_traits<160, 64, 64, 4, cutlass::bfloat16_t>>, Is_dropout=true, Is_causal=true]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(155): here
            instantiation of "void run_mha_fwd_hdim160<T>(Flash_fwd_params &, cudaStream_t) [with T=cutlass::bfloat16_t]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_hdim160_bf16_sm80.cu(16): here

/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_kernel.h(140): warning: variable "MMA_M" was declared but never referenced
          detected during:
            instantiation of "void flash::compute_attn_1rowblock<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params,Is_Q_split_K>(const Params &, int, int, int) [with Kernel_traits=Flash_fwd_kernel_traits<160, 64, 64, 4, false, false, cutlass::bfloat16_t, Flash_kernel_traits<160, 64, 64, 4, cutlass::bfloat16_t>>, Is_dropout=true, Is_causal=true, Is_even_N=true, Is_even_K=false, Return_softmax=true, Params=Flash_fwd_params, Is_Q_split_K=false]" 
(589): here
            instantiation of "void flash::compute_attn<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &) [with Kernel_traits=Flash_fwd_kernel_traits<160, 64, 64, 4, false, false, cutlass::bfloat16_t, Flash_kernel_traits<160, 64, 64, 4, cutlass::bfloat16_t>>, Is_dropout=true, Is_causal=true, Is_even_N=true, Is_even_K=false, Return_softmax=true, Params=Flash_fwd_params]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(15): here
            instantiation of "void flash_fwd_kernel<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax>(Flash_fwd_params) [with Kernel_traits=Flash_fwd_kernel_traits<160, 64, 64, 4, false, false, cutlass::bfloat16_t, Flash_kernel_traits<160, 64, 64, 4, cutlass::bfloat16_t>>, Is_dropout=true, Is_causal=true, Is_even_N=true, Is_even_K=false, Return_softmax=true]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(34): here
            instantiation of "void run_flash_fwd<Kernel_traits,Is_dropout,Is_causal>(Flash_fwd_params &, cudaStream_t) [with Kernel_traits=Flash_fwd_kernel_traits<160, 64, 64, 4, false, false, cutlass::bfloat16_t, Flash_kernel_traits<160, 64, 64, 4, cutlass::bfloat16_t>>, Is_dropout=true, Is_causal=true]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(155): here
            instantiation of "void run_mha_fwd_hdim160<T>(Flash_fwd_params &, cudaStream_t) [with T=cutlass::bfloat16_t]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_hdim160_bf16_sm80.cu(16): here

/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_kernel.h(276): warning: variable "tQrQ" was declared but never referenced
          detected during:
            instantiation of "void flash::compute_attn_1rowblock<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params,Is_Q_split_K>(const Params &, int, int, int) [with Kernel_traits=Flash_fwd_kernel_traits<160, 64, 64, 4, false, false, cutlass::bfloat16_t, Flash_kernel_traits<160, 64, 64, 4, cutlass::bfloat16_t>>, Is_dropout=true, Is_causal=true, Is_even_N=true, Is_even_K=false, Return_softmax=true, Params=Flash_fwd_params, Is_Q_split_K=false]" 
(589): here
            instantiation of "void flash::compute_attn<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &) [with Kernel_traits=Flash_fwd_kernel_traits<160, 64, 64, 4, false, false, cutlass::bfloat16_t, Flash_kernel_traits<160, 64, 64, 4, cutlass::bfloat16_t>>, Is_dropout=true, Is_causal=true, Is_even_N=true, Is_even_K=false, Return_softmax=true, Params=Flash_fwd_params]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(15): here
            instantiation of "void flash_fwd_kernel<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax>(Flash_fwd_params) [with Kernel_traits=Flash_fwd_kernel_traits<160, 64, 64, 4, false, false, cutlass::bfloat16_t, Flash_kernel_traits<160, 64, 64, 4, cutlass::bfloat16_t>>, Is_dropout=true, Is_causal=true, Is_even_N=true, Is_even_K=false, Return_softmax=true]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(34): here
            instantiation of "void run_flash_fwd<Kernel_traits,Is_dropout,Is_causal>(Flash_fwd_params &, cudaStream_t) [with Kernel_traits=Flash_fwd_kernel_traits<160, 64, 64, 4, false, false, cutlass::bfloat16_t, Flash_kernel_traits<160, 64, 64, 4, cutlass::bfloat16_t>>, Is_dropout=true, Is_causal=true]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(155): here
            instantiation of "void run_mha_fwd_hdim160<T>(Flash_fwd_params &, cudaStream_t) [with T=cutlass::bfloat16_t]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_hdim160_bf16_sm80.cu(16): here

/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_kernel.h(140): warning: variable "MMA_M" was declared but never referenced
          detected during:
            instantiation of "void flash::compute_attn_1rowblock<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params,Is_Q_split_K>(const Params &, int, int, int) [with Kernel_traits=Flash_fwd_kernel_traits<160, 64, 64, 4, false, false, cutlass::bfloat16_t, Flash_kernel_traits<160, 64, 64, 4, cutlass::bfloat16_t>>, Is_dropout=true, Is_causal=true, Is_even_N=true, Is_even_K=false, Return_softmax=false, Params=Flash_fwd_params, Is_Q_split_K=false]" 
(589): here
            instantiation of "void flash::compute_attn<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &) [with Kernel_traits=Flash_fwd_kernel_traits<160, 64, 64, 4, false, false, cutlass::bfloat16_t, Flash_kernel_traits<160, 64, 64, 4, cutlass::bfloat16_t>>, Is_dropout=true, Is_causal=true, Is_even_N=true, Is_even_K=false, Return_softmax=false, Params=Flash_fwd_params]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(15): here
            instantiation of "void flash_fwd_kernel<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax>(Flash_fwd_params) [with Kernel_traits=Flash_fwd_kernel_traits<160, 64, 64, 4, false, false, cutlass::bfloat16_t, Flash_kernel_traits<160, 64, 64, 4, cutlass::bfloat16_t>>, Is_dropout=true, Is_causal=true, Is_even_N=true, Is_even_K=false, Return_softmax=false]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(34): here
            instantiation of "void run_flash_fwd<Kernel_traits,Is_dropout,Is_causal>(Flash_fwd_params &, cudaStream_t) [with Kernel_traits=Flash_fwd_kernel_traits<160, 64, 64, 4, false, false, cutlass::bfloat16_t, Flash_kernel_traits<160, 64, 64, 4, cutlass::bfloat16_t>>, Is_dropout=true, Is_causal=true]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(155): here
            instantiation of "void run_mha_fwd_hdim160<T>(Flash_fwd_params &, cudaStream_t) [with T=cutlass::bfloat16_t]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_hdim160_bf16_sm80.cu(16): here

/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_kernel.h(276): warning: variable "tQrQ" was declared but never referenced
          detected during:
            instantiation of "void flash::compute_attn_1rowblock<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params,Is_Q_split_K>(const Params &, int, int, int) [with Kernel_traits=Flash_fwd_kernel_traits<160, 64, 64, 4, false, false, cutlass::bfloat16_t, Flash_kernel_traits<160, 64, 64, 4, cutlass::bfloat16_t>>, Is_dropout=true, Is_causal=true, Is_even_N=true, Is_even_K=false, Return_softmax=false, Params=Flash_fwd_params, Is_Q_split_K=false]" 
(589): here
            instantiation of "void flash::compute_attn<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &) [with Kernel_traits=Flash_fwd_kernel_traits<160, 64, 64, 4, false, false, cutlass::bfloat16_t, Flash_kernel_traits<160, 64, 64, 4, cutlass::bfloat16_t>>, Is_dropout=true, Is_causal=true, Is_even_N=true, Is_even_K=false, Return_softmax=false, Params=Flash_fwd_params]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(15): here
            instantiation of "void flash_fwd_kernel<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax>(Flash_fwd_params) [with Kernel_traits=Flash_fwd_kernel_traits<160, 64, 64, 4, false, false, cutlass::bfloat16_t, Flash_kernel_traits<160, 64, 64, 4, cutlass::bfloat16_t>>, Is_dropout=true, Is_causal=true, Is_even_N=true, Is_even_K=false, Return_softmax=false]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(34): here
            instantiation of "void run_flash_fwd<Kernel_traits,Is_dropout,Is_causal>(Flash_fwd_params &, cudaStream_t) [with Kernel_traits=Flash_fwd_kernel_traits<160, 64, 64, 4, false, false, cutlass::bfloat16_t, Flash_kernel_traits<160, 64, 64, 4, cutlass::bfloat16_t>>, Is_dropout=true, Is_causal=true]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(155): here
            instantiation of "void run_mha_fwd_hdim160<T>(Flash_fwd_params &, cudaStream_t) [with T=cutlass::bfloat16_t]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_hdim160_bf16_sm80.cu(16): here

/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_kernel.h(140): warning: variable "MMA_M" was declared but never referenced
          detected during:
            instantiation of "void flash::compute_attn_1rowblock<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params,Is_Q_split_K>(const Params &, int, int, int) [with Kernel_traits=Flash_fwd_kernel_traits<160, 64, 64, 4, false, false, cutlass::bfloat16_t, Flash_kernel_traits<160, 64, 64, 4, cutlass::bfloat16_t>>, Is_dropout=true, Is_causal=true, Is_even_N=false, Is_even_K=true, Return_softmax=true, Params=Flash_fwd_params, Is_Q_split_K=false]" 
(589): here
            instantiation of "void flash::compute_attn<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &) [with Kernel_traits=Flash_fwd_kernel_traits<160, 64, 64, 4, false, false, cutlass::bfloat16_t, Flash_kernel_traits<160, 64, 64, 4, cutlass::bfloat16_t>>, Is_dropout=true, Is_causal=true, Is_even_N=false, Is_even_K=true, Return_softmax=true, Params=Flash_fwd_params]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(15): here
            instantiation of "void flash_fwd_kernel<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax>(Flash_fwd_params) [with Kernel_traits=Flash_fwd_kernel_traits<160, 64, 64, 4, false, false, cutlass::bfloat16_t, Flash_kernel_traits<160, 64, 64, 4, cutlass::bfloat16_t>>, Is_dropout=true, Is_causal=true, Is_even_N=false, Is_even_K=true, Return_softmax=true]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(34): here
            instantiation of "void run_flash_fwd<Kernel_traits,Is_dropout,Is_causal>(Flash_fwd_params &, cudaStream_t) [with Kernel_traits=Flash_fwd_kernel_traits<160, 64, 64, 4, false, false, cutlass::bfloat16_t, Flash_kernel_traits<160, 64, 64, 4, cutlass::bfloat16_t>>, Is_dropout=true, Is_causal=true]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(155): here
            instantiation of "void run_mha_fwd_hdim160<T>(Flash_fwd_params &, cudaStream_t) [with T=cutlass::bfloat16_t]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_hdim160_bf16_sm80.cu(16): here

/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_kernel.h(276): warning: variable "tQrQ" was declared but never referenced
          detected during:
            instantiation of "void flash::compute_attn_1rowblock<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params,Is_Q_split_K>(const Params &, int, int, int) [with Kernel_traits=Flash_fwd_kernel_traits<160, 64, 64, 4, false, false, cutlass::bfloat16_t, Flash_kernel_traits<160, 64, 64, 4, cutlass::bfloat16_t>>, Is_dropout=true, Is_causal=true, Is_even_N=false, Is_even_K=true, Return_softmax=true, Params=Flash_fwd_params, Is_Q_split_K=false]" 
(589): here
            instantiation of "void flash::compute_attn<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &) [with Kernel_traits=Flash_fwd_kernel_traits<160, 64, 64, 4, false, false, cutlass::bfloat16_t, Flash_kernel_traits<160, 64, 64, 4, cutlass::bfloat16_t>>, Is_dropout=true, Is_causal=true, Is_even_N=false, Is_even_K=true, Return_softmax=true, Params=Flash_fwd_params]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(15): here
            instantiation of "void flash_fwd_kernel<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax>(Flash_fwd_params) [with Kernel_traits=Flash_fwd_kernel_traits<160, 64, 64, 4, false, false, cutlass::bfloat16_t, Flash_kernel_traits<160, 64, 64, 4, cutlass::bfloat16_t>>, Is_dropout=true, Is_causal=true, Is_even_N=false, Is_even_K=true, Return_softmax=true]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(34): here
            instantiation of "void run_flash_fwd<Kernel_traits,Is_dropout,Is_causal>(Flash_fwd_params &, cudaStream_t) [with Kernel_traits=Flash_fwd_kernel_traits<160, 64, 64, 4, false, false, cutlass::bfloat16_t, Flash_kernel_traits<160, 64, 64, 4, cutlass::bfloat16_t>>, Is_dropout=true, Is_causal=true]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(155): here
            instantiation of "void run_mha_fwd_hdim160<T>(Flash_fwd_params &, cudaStream_t) [with T=cutlass::bfloat16_t]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_hdim160_bf16_sm80.cu(16): here

/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_kernel.h(140): warning: variable "MMA_M" was declared but never referenced
          detected during:
            instantiation of "void flash::compute_attn_1rowblock<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params,Is_Q_split_K>(const Params &, int, int, int) [with Kernel_traits=Flash_fwd_kernel_traits<160, 64, 64, 4, false, false, cutlass::bfloat16_t, Flash_kernel_traits<160, 64, 64, 4, cutlass::bfloat16_t>>, Is_dropout=true, Is_causal=true, Is_even_N=false, Is_even_K=true, Return_softmax=false, Params=Flash_fwd_params, Is_Q_split_K=false]" 
(589): here
            instantiation of "void flash::compute_attn<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &) [with Kernel_traits=Flash_fwd_kernel_traits<160, 64, 64, 4, false, false, cutlass::bfloat16_t, Flash_kernel_traits<160, 64, 64, 4, cutlass::bfloat16_t>>, Is_dropout=true, Is_causal=true, Is_even_N=false, Is_even_K=true, Return_softmax=false, Params=Flash_fwd_params]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(15): here
            instantiation of "void flash_fwd_kernel<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax>(Flash_fwd_params) [with Kernel_traits=Flash_fwd_kernel_traits<160, 64, 64, 4, false, false, cutlass::bfloat16_t, Flash_kernel_traits<160, 64, 64, 4, cutlass::bfloat16_t>>, Is_dropout=true, Is_causal=true, Is_even_N=false, Is_even_K=true, Return_softmax=false]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(34): here
            instantiation of "void run_flash_fwd<Kernel_traits,Is_dropout,Is_causal>(Flash_fwd_params &, cudaStream_t) [with Kernel_traits=Flash_fwd_kernel_traits<160, 64, 64, 4, false, false, cutlass::bfloat16_t, Flash_kernel_traits<160, 64, 64, 4, cutlass::bfloat16_t>>, Is_dropout=true, Is_causal=true]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(155): here
            instantiation of "void run_mha_fwd_hdim160<T>(Flash_fwd_params &, cudaStream_t) [with T=cutlass::bfloat16_t]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_hdim160_bf16_sm80.cu(16): here

/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_kernel.h(276): warning: variable "tQrQ" was declared but never referenced
          detected during:
            instantiation of "void flash::compute_attn_1rowblock<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params,Is_Q_split_K>(const Params &, int, int, int) [with Kernel_traits=Flash_fwd_kernel_traits<160, 64, 64, 4, false, false, cutlass::bfloat16_t, Flash_kernel_traits<160, 64, 64, 4, cutlass::bfloat16_t>>, Is_dropout=true, Is_causal=true, Is_even_N=false, Is_even_K=true, Return_softmax=false, Params=Flash_fwd_params, Is_Q_split_K=false]" 
(589): here
            instantiation of "void flash::compute_attn<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &) [with Kernel_traits=Flash_fwd_kernel_traits<160, 64, 64, 4, false, false, cutlass::bfloat16_t, Flash_kernel_traits<160, 64, 64, 4, cutlass::bfloat16_t>>, Is_dropout=true, Is_causal=true, Is_even_N=false, Is_even_K=true, Return_softmax=false, Params=Flash_fwd_params]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(15): here
            instantiation of "void flash_fwd_kernel<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax>(Flash_fwd_params) [with Kernel_traits=Flash_fwd_kernel_traits<160, 64, 64, 4, false, false, cutlass::bfloat16_t, Flash_kernel_traits<160, 64, 64, 4, cutlass::bfloat16_t>>, Is_dropout=true, Is_causal=true, Is_even_N=false, Is_even_K=true, Return_softmax=false]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(34): here
            instantiation of "void run_flash_fwd<Kernel_traits,Is_dropout,Is_causal>(Flash_fwd_params &, cudaStream_t) [with Kernel_traits=Flash_fwd_kernel_traits<160, 64, 64, 4, false, false, cutlass::bfloat16_t, Flash_kernel_traits<160, 64, 64, 4, cutlass::bfloat16_t>>, Is_dropout=true, Is_causal=true]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(155): here
            instantiation of "void run_mha_fwd_hdim160<T>(Flash_fwd_params &, cudaStream_t) [with T=cutlass::bfloat16_t]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_hdim160_bf16_sm80.cu(16): here

/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_kernel.h(140): warning: variable "MMA_M" was declared but never referenced
          detected during:
            instantiation of "void flash::compute_attn_1rowblock<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params,Is_Q_split_K>(const Params &, int, int, int) [with Kernel_traits=Flash_fwd_kernel_traits<160, 64, 64, 4, false, false, cutlass::bfloat16_t, Flash_kernel_traits<160, 64, 64, 4, cutlass::bfloat16_t>>, Is_dropout=true, Is_causal=true, Is_even_N=false, Is_even_K=false, Return_softmax=true, Params=Flash_fwd_params, Is_Q_split_K=false]" 
(589): here
            instantiation of "void flash::compute_attn<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &) [with Kernel_traits=Flash_fwd_kernel_traits<160, 64, 64, 4, false, false, cutlass::bfloat16_t, Flash_kernel_traits<160, 64, 64, 4, cutlass::bfloat16_t>>, Is_dropout=true, Is_causal=true, Is_even_N=false, Is_even_K=false, Return_softmax=true, Params=Flash_fwd_params]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(15): here
            instantiation of "void flash_fwd_kernel<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax>(Flash_fwd_params) [with Kernel_traits=Flash_fwd_kernel_traits<160, 64, 64, 4, false, false, cutlass::bfloat16_t, Flash_kernel_traits<160, 64, 64, 4, cutlass::bfloat16_t>>, Is_dropout=true, Is_causal=true, Is_even_N=false, Is_even_K=false, Return_softmax=true]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(34): here
            instantiation of "void run_flash_fwd<Kernel_traits,Is_dropout,Is_causal>(Flash_fwd_params &, cudaStream_t) [with Kernel_traits=Flash_fwd_kernel_traits<160, 64, 64, 4, false, false, cutlass::bfloat16_t, Flash_kernel_traits<160, 64, 64, 4, cutlass::bfloat16_t>>, Is_dropout=true, Is_causal=true]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(155): here
            instantiation of "void run_mha_fwd_hdim160<T>(Flash_fwd_params &, cudaStream_t) [with T=cutlass::bfloat16_t]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_hdim160_bf16_sm80.cu(16): here

/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_kernel.h(276): warning: variable "tQrQ" was declared but never referenced
          detected during:
            instantiation of "void flash::compute_attn_1rowblock<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params,Is_Q_split_K>(const Params &, int, int, int) [with Kernel_traits=Flash_fwd_kernel_traits<160, 64, 64, 4, false, false, cutlass::bfloat16_t, Flash_kernel_traits<160, 64, 64, 4, cutlass::bfloat16_t>>, Is_dropout=true, Is_causal=true, Is_even_N=false, Is_even_K=false, Return_softmax=true, Params=Flash_fwd_params, Is_Q_split_K=false]" 
(589): here
            instantiation of "void flash::compute_attn<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &) [with Kernel_traits=Flash_fwd_kernel_traits<160, 64, 64, 4, false, false, cutlass::bfloat16_t, Flash_kernel_traits<160, 64, 64, 4, cutlass::bfloat16_t>>, Is_dropout=true, Is_causal=true, Is_even_N=false, Is_even_K=false, Return_softmax=true, Params=Flash_fwd_params]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(15): here
            instantiation of "void flash_fwd_kernel<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax>(Flash_fwd_params) [with Kernel_traits=Flash_fwd_kernel_traits<160, 64, 64, 4, false, false, cutlass::bfloat16_t, Flash_kernel_traits<160, 64, 64, 4, cutlass::bfloat16_t>>, Is_dropout=true, Is_causal=true, Is_even_N=false, Is_even_K=false, Return_softmax=true]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(34): here
            instantiation of "void run_flash_fwd<Kernel_traits,Is_dropout,Is_causal>(Flash_fwd_params &, cudaStream_t) [with Kernel_traits=Flash_fwd_kernel_traits<160, 64, 64, 4, false, false, cutlass::bfloat16_t, Flash_kernel_traits<160, 64, 64, 4, cutlass::bfloat16_t>>, Is_dropout=true, Is_causal=true]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(155): here
            instantiation of "void run_mha_fwd_hdim160<T>(Flash_fwd_params &, cudaStream_t) [with T=cutlass::bfloat16_t]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_hdim160_bf16_sm80.cu(16): here

/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_kernel.h(140): warning: variable "MMA_M" was declared but never referenced
          detected during:
            instantiation of "void flash::compute_attn_1rowblock<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params,Is_Q_split_K>(const Params &, int, int, int) [with Kernel_traits=Flash_fwd_kernel_traits<160, 64, 64, 4, false, false, cutlass::bfloat16_t, Flash_kernel_traits<160, 64, 64, 4, cutlass::bfloat16_t>>, Is_dropout=true, Is_causal=true, Is_even_N=false, Is_even_K=false, Return_softmax=false, Params=Flash_fwd_params, Is_Q_split_K=false]" 
(589): here
            instantiation of "void flash::compute_attn<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &) [with Kernel_traits=Flash_fwd_kernel_traits<160, 64, 64, 4, false, false, cutlass::bfloat16_t, Flash_kernel_traits<160, 64, 64, 4, cutlass::bfloat16_t>>, Is_dropout=true, Is_causal=true, Is_even_N=false, Is_even_K=false, Return_softmax=false, Params=Flash_fwd_params]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(15): here
            instantiation of "void flash_fwd_kernel<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax>(Flash_fwd_params) [with Kernel_traits=Flash_fwd_kernel_traits<160, 64, 64, 4, false, false, cutlass::bfloat16_t, Flash_kernel_traits<160, 64, 64, 4, cutlass::bfloat16_t>>, Is_dropout=true, Is_causal=true, Is_even_N=false, Is_even_K=false, Return_softmax=false]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(34): here
            instantiation of "void run_flash_fwd<Kernel_traits,Is_dropout,Is_causal>(Flash_fwd_params &, cudaStream_t) [with Kernel_traits=Flash_fwd_kernel_traits<160, 64, 64, 4, false, false, cutlass::bfloat16_t, Flash_kernel_traits<160, 64, 64, 4, cutlass::bfloat16_t>>, Is_dropout=true, Is_causal=true]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(155): here
            instantiation of "void run_mha_fwd_hdim160<T>(Flash_fwd_params &, cudaStream_t) [with T=cutlass::bfloat16_t]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_hdim160_bf16_sm80.cu(16): here

/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_kernel.h(276): warning: variable "tQrQ" was declared but never referenced
          detected during:
            instantiation of "void flash::compute_attn_1rowblock<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params,Is_Q_split_K>(const Params &, int, int, int) [with Kernel_traits=Flash_fwd_kernel_traits<160, 64, 64, 4, false, false, cutlass::bfloat16_t, Flash_kernel_traits<160, 64, 64, 4, cutlass::bfloat16_t>>, Is_dropout=true, Is_causal=true, Is_even_N=false, Is_even_K=false, Return_softmax=false, Params=Flash_fwd_params, Is_Q_split_K=false]" 
(589): here
            instantiation of "void flash::compute_attn<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &) [with Kernel_traits=Flash_fwd_kernel_traits<160, 64, 64, 4, false, false, cutlass::bfloat16_t, Flash_kernel_traits<160, 64, 64, 4, cutlass::bfloat16_t>>, Is_dropout=true, Is_causal=true, Is_even_N=false, Is_even_K=false, Return_softmax=false, Params=Flash_fwd_params]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(15): here
            instantiation of "void flash_fwd_kernel<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax>(Flash_fwd_params) [with Kernel_traits=Flash_fwd_kernel_traits<160, 64, 64, 4, false, false, cutlass::bfloat16_t, Flash_kernel_traits<160, 64, 64, 4, cutlass::bfloat16_t>>, Is_dropout=true, Is_causal=true, Is_even_N=false, Is_even_K=false, Return_softmax=false]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(34): here
            instantiation of "void run_flash_fwd<Kernel_traits,Is_dropout,Is_causal>(Flash_fwd_params &, cudaStream_t) [with Kernel_traits=Flash_fwd_kernel_traits<160, 64, 64, 4, false, false, cutlass::bfloat16_t, Flash_kernel_traits<160, 64, 64, 4, cutlass::bfloat16_t>>, Is_dropout=true, Is_causal=true]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(155): here
            instantiation of "void run_mha_fwd_hdim160<T>(Flash_fwd_params &, cudaStream_t) [with T=cutlass::bfloat16_t]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_hdim160_bf16_sm80.cu(16): here

/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_kernel.h(140): warning: variable "MMA_M" was declared but never referenced
          detected during:
            instantiation of "void flash::compute_attn_1rowblock<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params,Is_Q_split_K>(const Params &, int, int, int) [with Kernel_traits=Flash_fwd_kernel_traits<160, 128, 32, 4, false, false, cutlass::bfloat16_t, Flash_kernel_traits<160, 128, 32, 4, cutlass::bfloat16_t>>, Is_dropout=true, Is_causal=true, Is_even_N=true, Is_even_K=true, Return_softmax=true, Params=Flash_fwd_params, Is_Q_split_K=false]" 
(589): here
            instantiation of "void flash::compute_attn<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &) [with Kernel_traits=Flash_fwd_kernel_traits<160, 128, 32, 4, false, false, cutlass::bfloat16_t, Flash_kernel_traits<160, 128, 32, 4, cutlass::bfloat16_t>>, Is_dropout=true, Is_causal=true, Is_even_N=true, Is_even_K=true, Return_softmax=true, Params=Flash_fwd_params]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(15): here
            instantiation of "void flash_fwd_kernel<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax>(Flash_fwd_params) [with Kernel_traits=Flash_fwd_kernel_traits<160, 128, 32, 4, false, false, cutlass::bfloat16_t, Flash_kernel_traits<160, 128, 32, 4, cutlass::bfloat16_t>>, Is_dropout=true, Is_causal=true, Is_even_N=true, Is_even_K=true, Return_softmax=true]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(34): here
            instantiation of "void run_flash_fwd<Kernel_traits,Is_dropout,Is_causal>(Flash_fwd_params &, cudaStream_t) [with Kernel_traits=Flash_fwd_kernel_traits<160, 128, 32, 4, false, false, cutlass::bfloat16_t, Flash_kernel_traits<160, 128, 32, 4, cutlass::bfloat16_t>>, Is_dropout=true, Is_causal=true]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(155): here
            instantiation of "void run_mha_fwd_hdim160<T>(Flash_fwd_params &, cudaStream_t) [with T=cutlass::bfloat16_t]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_hdim160_bf16_sm80.cu(16): here

/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_kernel.h(276): warning: variable "tQrQ" was declared but never referenced
          detected during:
            instantiation of "void flash::compute_attn_1rowblock<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params,Is_Q_split_K>(const Params &, int, int, int) [with Kernel_traits=Flash_fwd_kernel_traits<160, 128, 32, 4, false, false, cutlass::bfloat16_t, Flash_kernel_traits<160, 128, 32, 4, cutlass::bfloat16_t>>, Is_dropout=true, Is_causal=true, Is_even_N=true, Is_even_K=true, Return_softmax=true, Params=Flash_fwd_params, Is_Q_split_K=false]" 
(589): here
            instantiation of "void flash::compute_attn<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &) [with Kernel_traits=Flash_fwd_kernel_traits<160, 128, 32, 4, false, false, cutlass::bfloat16_t, Flash_kernel_traits<160, 128, 32, 4, cutlass::bfloat16_t>>, Is_dropout=true, Is_causal=true, Is_even_N=true, Is_even_K=true, Return_softmax=true, Params=Flash_fwd_params]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(15): here
            instantiation of "void flash_fwd_kernel<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax>(Flash_fwd_params) [with Kernel_traits=Flash_fwd_kernel_traits<160, 128, 32, 4, false, false, cutlass::bfloat16_t, Flash_kernel_traits<160, 128, 32, 4, cutlass::bfloat16_t>>, Is_dropout=true, Is_causal=true, Is_even_N=true, Is_even_K=true, Return_softmax=true]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(34): here
            instantiation of "void run_flash_fwd<Kernel_traits,Is_dropout,Is_causal>(Flash_fwd_params &, cudaStream_t) [with Kernel_traits=Flash_fwd_kernel_traits<160, 128, 32, 4, false, false, cutlass::bfloat16_t, Flash_kernel_traits<160, 128, 32, 4, cutlass::bfloat16_t>>, Is_dropout=true, Is_causal=true]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(155): here
            instantiation of "void run_mha_fwd_hdim160<T>(Flash_fwd_params &, cudaStream_t) [with T=cutlass::bfloat16_t]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_hdim160_bf16_sm80.cu(16): here

/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_kernel.h(140): warning: variable "MMA_M" was declared but never referenced
          detected during:
            instantiation of "void flash::compute_attn_1rowblock<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params,Is_Q_split_K>(const Params &, int, int, int) [with Kernel_traits=Flash_fwd_kernel_traits<160, 128, 32, 4, false, false, cutlass::bfloat16_t, Flash_kernel_traits<160, 128, 32, 4, cutlass::bfloat16_t>>, Is_dropout=true, Is_causal=true, Is_even_N=true, Is_even_K=true, Return_softmax=false, Params=Flash_fwd_params, Is_Q_split_K=false]" 
(589): here
            instantiation of "void flash::compute_attn<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &) [with Kernel_traits=Flash_fwd_kernel_traits<160, 128, 32, 4, false, false, cutlass::bfloat16_t, Flash_kernel_traits<160, 128, 32, 4, cutlass::bfloat16_t>>, Is_dropout=true, Is_causal=true, Is_even_N=true, Is_even_K=true, Return_softmax=false, Params=Flash_fwd_params]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(15): here
            instantiation of "void flash_fwd_kernel<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax>(Flash_fwd_params) [with Kernel_traits=Flash_fwd_kernel_traits<160, 128, 32, 4, false, false, cutlass::bfloat16_t, Flash_kernel_traits<160, 128, 32, 4, cutlass::bfloat16_t>>, Is_dropout=true, Is_causal=true, Is_even_N=true, Is_even_K=true, Return_softmax=false]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(34): here
            instantiation of "void run_flash_fwd<Kernel_traits,Is_dropout,Is_causal>(Flash_fwd_params &, cudaStream_t) [with Kernel_traits=Flash_fwd_kernel_traits<160, 128, 32, 4, false, false, cutlass::bfloat16_t, Flash_kernel_traits<160, 128, 32, 4, cutlass::bfloat16_t>>, Is_dropout=true, Is_causal=true]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(155): here
            instantiation of "void run_mha_fwd_hdim160<T>(Flash_fwd_params &, cudaStream_t) [with T=cutlass::bfloat16_t]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_hdim160_bf16_sm80.cu(16): here

/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_kernel.h(276): warning: variable "tQrQ" was declared but never referenced
          detected during:
            instantiation of "void flash::compute_attn_1rowblock<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params,Is_Q_split_K>(const Params &, int, int, int) [with Kernel_traits=Flash_fwd_kernel_traits<160, 128, 32, 4, false, false, cutlass::bfloat16_t, Flash_kernel_traits<160, 128, 32, 4, cutlass::bfloat16_t>>, Is_dropout=true, Is_causal=true, Is_even_N=true, Is_even_K=true, Return_softmax=false, Params=Flash_fwd_params, Is_Q_split_K=false]" 
(589): here
            instantiation of "void flash::compute_attn<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &) [with Kernel_traits=Flash_fwd_kernel_traits<160, 128, 32, 4, false, false, cutlass::bfloat16_t, Flash_kernel_traits<160, 128, 32, 4, cutlass::bfloat16_t>>, Is_dropout=true, Is_causal=true, Is_even_N=true, Is_even_K=true, Return_softmax=false, Params=Flash_fwd_params]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(15): here
            instantiation of "void flash_fwd_kernel<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax>(Flash_fwd_params) [with Kernel_traits=Flash_fwd_kernel_traits<160, 128, 32, 4, false, false, cutlass::bfloat16_t, Flash_kernel_traits<160, 128, 32, 4, cutlass::bfloat16_t>>, Is_dropout=true, Is_causal=true, Is_even_N=true, Is_even_K=true, Return_softmax=false]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(34): here
            instantiation of "void run_flash_fwd<Kernel_traits,Is_dropout,Is_causal>(Flash_fwd_params &, cudaStream_t) [with Kernel_traits=Flash_fwd_kernel_traits<160, 128, 32, 4, false, false, cutlass::bfloat16_t, Flash_kernel_traits<160, 128, 32, 4, cutlass::bfloat16_t>>, Is_dropout=true, Is_causal=true]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(155): here
            instantiation of "void run_mha_fwd_hdim160<T>(Flash_fwd_params &, cudaStream_t) [with T=cutlass::bfloat16_t]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_hdim160_bf16_sm80.cu(16): here

/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_kernel.h(140): warning: variable "MMA_M" was declared but never referenced
          detected during:
            instantiation of "void flash::compute_attn_1rowblock<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params,Is_Q_split_K>(const Params &, int, int, int) [with Kernel_traits=Flash_fwd_kernel_traits<160, 128, 32, 4, false, false, cutlass::bfloat16_t, Flash_kernel_traits<160, 128, 32, 4, cutlass::bfloat16_t>>, Is_dropout=true, Is_causal=true, Is_even_N=true, Is_even_K=false, Return_softmax=true, Params=Flash_fwd_params, Is_Q_split_K=false]" 
(589): here
            instantiation of "void flash::compute_attn<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &) [with Kernel_traits=Flash_fwd_kernel_traits<160, 128, 32, 4, false, false, cutlass::bfloat16_t, Flash_kernel_traits<160, 128, 32, 4, cutlass::bfloat16_t>>, Is_dropout=true, Is_causal=true, Is_even_N=true, Is_even_K=false, Return_softmax=true, Params=Flash_fwd_params]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(15): here
            instantiation of "void flash_fwd_kernel<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax>(Flash_fwd_params) [with Kernel_traits=Flash_fwd_kernel_traits<160, 128, 32, 4, false, false, cutlass::bfloat16_t, Flash_kernel_traits<160, 128, 32, 4, cutlass::bfloat16_t>>, Is_dropout=true, Is_causal=true, Is_even_N=true, Is_even_K=false, Return_softmax=true]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(34): here
            instantiation of "void run_flash_fwd<Kernel_traits,Is_dropout,Is_causal>(Flash_fwd_params &, cudaStream_t) [with Kernel_traits=Flash_fwd_kernel_traits<160, 128, 32, 4, false, false, cutlass::bfloat16_t, Flash_kernel_traits<160, 128, 32, 4, cutlass::bfloat16_t>>, Is_dropout=true, Is_causal=true]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(155): here
            instantiation of "void run_mha_fwd_hdim160<T>(Flash_fwd_params &, cudaStream_t) [with T=cutlass::bfloat16_t]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_hdim160_bf16_sm80.cu(16): here

/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_kernel.h(276): warning: variable "tQrQ" was declared but never referenced
          detected during:
            instantiation of "void flash::compute_attn_1rowblock<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params,Is_Q_split_K>(const Params &, int, int, int) [with Kernel_traits=Flash_fwd_kernel_traits<160, 128, 32, 4, false, false, cutlass::bfloat16_t, Flash_kernel_traits<160, 128, 32, 4, cutlass::bfloat16_t>>, Is_dropout=true, Is_causal=true, Is_even_N=true, Is_even_K=false, Return_softmax=true, Params=Flash_fwd_params, Is_Q_split_K=false]" 
(589): here
            instantiation of "void flash::compute_attn<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &) [with Kernel_traits=Flash_fwd_kernel_traits<160, 128, 32, 4, false, false, cutlass::bfloat16_t, Flash_kernel_traits<160, 128, 32, 4, cutlass::bfloat16_t>>, Is_dropout=true, Is_causal=true, Is_even_N=true, Is_even_K=false, Return_softmax=true, Params=Flash_fwd_params]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(15): here
            instantiation of "void flash_fwd_kernel<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax>(Flash_fwd_params) [with Kernel_traits=Flash_fwd_kernel_traits<160, 128, 32, 4, false, false, cutlass::bfloat16_t, Flash_kernel_traits<160, 128, 32, 4, cutlass::bfloat16_t>>, Is_dropout=true, Is_causal=true, Is_even_N=true, Is_even_K=false, Return_softmax=true]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(34): here
            instantiation of "void run_flash_fwd<Kernel_traits,Is_dropout,Is_causal>(Flash_fwd_params &, cudaStream_t) [with Kernel_traits=Flash_fwd_kernel_traits<160, 128, 32, 4, false, false, cutlass::bfloat16_t, Flash_kernel_traits<160, 128, 32, 4, cutlass::bfloat16_t>>, Is_dropout=true, Is_causal=true]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(155): here
            instantiation of "void run_mha_fwd_hdim160<T>(Flash_fwd_params &, cudaStream_t) [with T=cutlass::bfloat16_t]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_hdim160_bf16_sm80.cu(16): here

/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_kernel.h(140): warning: variable "MMA_M" was declared but never referenced
          detected during:
            instantiation of "void flash::compute_attn_1rowblock<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params,Is_Q_split_K>(const Params &, int, int, int) [with Kernel_traits=Flash_fwd_kernel_traits<160, 128, 32, 4, false, false, cutlass::bfloat16_t, Flash_kernel_traits<160, 128, 32, 4, cutlass::bfloat16_t>>, Is_dropout=true, Is_causal=true, Is_even_N=true, Is_even_K=false, Return_softmax=false, Params=Flash_fwd_params, Is_Q_split_K=false]" 
(589): here
            instantiation of "void flash::compute_attn<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &) [with Kernel_traits=Flash_fwd_kernel_traits<160, 128, 32, 4, false, false, cutlass::bfloat16_t, Flash_kernel_traits<160, 128, 32, 4, cutlass::bfloat16_t>>, Is_dropout=true, Is_causal=true, Is_even_N=true, Is_even_K=false, Return_softmax=false, Params=Flash_fwd_params]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(15): here
            instantiation of "void flash_fwd_kernel<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax>(Flash_fwd_params) [with Kernel_traits=Flash_fwd_kernel_traits<160, 128, 32, 4, false, false, cutlass::bfloat16_t, Flash_kernel_traits<160, 128, 32, 4, cutlass::bfloat16_t>>, Is_dropout=true, Is_causal=true, Is_even_N=true, Is_even_K=false, Return_softmax=false]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(34): here
            instantiation of "void run_flash_fwd<Kernel_traits,Is_dropout,Is_causal>(Flash_fwd_params &, cudaStream_t) [with Kernel_traits=Flash_fwd_kernel_traits<160, 128, 32, 4, false, false, cutlass::bfloat16_t, Flash_kernel_traits<160, 128, 32, 4, cutlass::bfloat16_t>>, Is_dropout=true, Is_causal=true]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(155): here
            instantiation of "void run_mha_fwd_hdim160<T>(Flash_fwd_params &, cudaStream_t) [with T=cutlass::bfloat16_t]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_hdim160_bf16_sm80.cu(16): here

/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_kernel.h(276): warning: variable "tQrQ" was declared but never referenced
          detected during:
            instantiation of "void flash::compute_attn_1rowblock<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params,Is_Q_split_K>(const Params &, int, int, int) [with Kernel_traits=Flash_fwd_kernel_traits<160, 128, 32, 4, false, false, cutlass::bfloat16_t, Flash_kernel_traits<160, 128, 32, 4, cutlass::bfloat16_t>>, Is_dropout=true, Is_causal=true, Is_even_N=true, Is_even_K=false, Return_softmax=false, Params=Flash_fwd_params, Is_Q_split_K=false]" 
(589): here
            instantiation of "void flash::compute_attn<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &) [with Kernel_traits=Flash_fwd_kernel_traits<160, 128, 32, 4, false, false, cutlass::bfloat16_t, Flash_kernel_traits<160, 128, 32, 4, cutlass::bfloat16_t>>, Is_dropout=true, Is_causal=true, Is_even_N=true, Is_even_K=false, Return_softmax=false, Params=Flash_fwd_params]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(15): here
            instantiation of "void flash_fwd_kernel<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax>(Flash_fwd_params) [with Kernel_traits=Flash_fwd_kernel_traits<160, 128, 32, 4, false, false, cutlass::bfloat16_t, Flash_kernel_traits<160, 128, 32, 4, cutlass::bfloat16_t>>, Is_dropout=true, Is_causal=true, Is_even_N=true, Is_even_K=false, Return_softmax=false]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(34): here
            instantiation of "void run_flash_fwd<Kernel_traits,Is_dropout,Is_causal>(Flash_fwd_params &, cudaStream_t) [with Kernel_traits=Flash_fwd_kernel_traits<160, 128, 32, 4, false, false, cutlass::bfloat16_t, Flash_kernel_traits<160, 128, 32, 4, cutlass::bfloat16_t>>, Is_dropout=true, Is_causal=true]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(155): here
            instantiation of "void run_mha_fwd_hdim160<T>(Flash_fwd_params &, cudaStream_t) [with T=cutlass::bfloat16_t]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_hdim160_bf16_sm80.cu(16): here

/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_kernel.h(140): warning: variable "MMA_M" was declared but never referenced
          detected during:
            instantiation of "void flash::compute_attn_1rowblock<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params,Is_Q_split_K>(const Params &, int, int, int) [with Kernel_traits=Flash_fwd_kernel_traits<160, 128, 32, 4, false, false, cutlass::bfloat16_t, Flash_kernel_traits<160, 128, 32, 4, cutlass::bfloat16_t>>, Is_dropout=true, Is_causal=true, Is_even_N=false, Is_even_K=true, Return_softmax=true, Params=Flash_fwd_params, Is_Q_split_K=false]" 
(589): here
            instantiation of "void flash::compute_attn<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &) [with Kernel_traits=Flash_fwd_kernel_traits<160, 128, 32, 4, false, false, cutlass::bfloat16_t, Flash_kernel_traits<160, 128, 32, 4, cutlass::bfloat16_t>>, Is_dropout=true, Is_causal=true, Is_even_N=false, Is_even_K=true, Return_softmax=true, Params=Flash_fwd_params]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(15): here
            instantiation of "void flash_fwd_kernel<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax>(Flash_fwd_params) [with Kernel_traits=Flash_fwd_kernel_traits<160, 128, 32, 4, false, false, cutlass::bfloat16_t, Flash_kernel_traits<160, 128, 32, 4, cutlass::bfloat16_t>>, Is_dropout=true, Is_causal=true, Is_even_N=false, Is_even_K=true, Return_softmax=true]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(34): here
            instantiation of "void run_flash_fwd<Kernel_traits,Is_dropout,Is_causal>(Flash_fwd_params &, cudaStream_t) [with Kernel_traits=Flash_fwd_kernel_traits<160, 128, 32, 4, false, false, cutlass::bfloat16_t, Flash_kernel_traits<160, 128, 32, 4, cutlass::bfloat16_t>>, Is_dropout=true, Is_causal=true]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(155): here
            instantiation of "void run_mha_fwd_hdim160<T>(Flash_fwd_params &, cudaStream_t) [with T=cutlass::bfloat16_t]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_hdim160_bf16_sm80.cu(16): here

/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_kernel.h(276): warning: variable "tQrQ" was declared but never referenced
          detected during:
            instantiation of "void flash::compute_attn_1rowblock<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params,Is_Q_split_K>(const Params &, int, int, int) [with Kernel_traits=Flash_fwd_kernel_traits<160, 128, 32, 4, false, false, cutlass::bfloat16_t, Flash_kernel_traits<160, 128, 32, 4, cutlass::bfloat16_t>>, Is_dropout=true, Is_causal=true, Is_even_N=false, Is_even_K=true, Return_softmax=true, Params=Flash_fwd_params, Is_Q_split_K=false]" 
(589): here
            instantiation of "void flash::compute_attn<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &) [with Kernel_traits=Flash_fwd_kernel_traits<160, 128, 32, 4, false, false, cutlass::bfloat16_t, Flash_kernel_traits<160, 128, 32, 4, cutlass::bfloat16_t>>, Is_dropout=true, Is_causal=true, Is_even_N=false, Is_even_K=true, Return_softmax=true, Params=Flash_fwd_params]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(15): here
            instantiation of "void flash_fwd_kernel<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax>(Flash_fwd_params) [with Kernel_traits=Flash_fwd_kernel_traits<160, 128, 32, 4, false, false, cutlass::bfloat16_t, Flash_kernel_traits<160, 128, 32, 4, cutlass::bfloat16_t>>, Is_dropout=true, Is_causal=true, Is_even_N=false, Is_even_K=true, Return_softmax=true]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(34): here
            instantiation of "void run_flash_fwd<Kernel_traits,Is_dropout,Is_causal>(Flash_fwd_params &, cudaStream_t) [with Kernel_traits=Flash_fwd_kernel_traits<160, 128, 32, 4, false, false, cutlass::bfloat16_t, Flash_kernel_traits<160, 128, 32, 4, cutlass::bfloat16_t>>, Is_dropout=true, Is_causal=true]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(155): here
            instantiation of "void run_mha_fwd_hdim160<T>(Flash_fwd_params &, cudaStream_t) [with T=cutlass::bfloat16_t]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_hdim160_bf16_sm80.cu(16): here

/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_kernel.h(140): warning: variable "MMA_M" was declared but never referenced
          detected during:
            instantiation of "void flash::compute_attn_1rowblock<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params,Is_Q_split_K>(const Params &, int, int, int) [with Kernel_traits=Flash_fwd_kernel_traits<160, 128, 32, 4, false, false, cutlass::bfloat16_t, Flash_kernel_traits<160, 128, 32, 4, cutlass::bfloat16_t>>, Is_dropout=true, Is_causal=true, Is_even_N=false, Is_even_K=true, Return_softmax=false, Params=Flash_fwd_params, Is_Q_split_K=false]" 
(589): here
            instantiation of "void flash::compute_attn<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &) [with Kernel_traits=Flash_fwd_kernel_traits<160, 128, 32, 4, false, false, cutlass::bfloat16_t, Flash_kernel_traits<160, 128, 32, 4, cutlass::bfloat16_t>>, Is_dropout=true, Is_causal=true, Is_even_N=false, Is_even_K=true, Return_softmax=false, Params=Flash_fwd_params]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(15): here
            instantiation of "void flash_fwd_kernel<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax>(Flash_fwd_params) [with Kernel_traits=Flash_fwd_kernel_traits<160, 128, 32, 4, false, false, cutlass::bfloat16_t, Flash_kernel_traits<160, 128, 32, 4, cutlass::bfloat16_t>>, Is_dropout=true, Is_causal=true, Is_even_N=false, Is_even_K=true, Return_softmax=false]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(34): here
            instantiation of "void run_flash_fwd<Kernel_traits,Is_dropout,Is_causal>(Flash_fwd_params &, cudaStream_t) [with Kernel_traits=Flash_fwd_kernel_traits<160, 128, 32, 4, false, false, cutlass::bfloat16_t, Flash_kernel_traits<160, 128, 32, 4, cutlass::bfloat16_t>>, Is_dropout=true, Is_causal=true]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(155): here
            instantiation of "void run_mha_fwd_hdim160<T>(Flash_fwd_params &, cudaStream_t) [with T=cutlass::bfloat16_t]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_hdim160_bf16_sm80.cu(16): here

/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_kernel.h(276): warning: variable "tQrQ" was declared but never referenced
          detected during:
            instantiation of "void flash::compute_attn_1rowblock<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params,Is_Q_split_K>(const Params &, int, int, int) [with Kernel_traits=Flash_fwd_kernel_traits<160, 128, 32, 4, false, false, cutlass::bfloat16_t, Flash_kernel_traits<160, 128, 32, 4, cutlass::bfloat16_t>>, Is_dropout=true, Is_causal=true, Is_even_N=false, Is_even_K=true, Return_softmax=false, Params=Flash_fwd_params, Is_Q_split_K=false]" 
(589): here
            instantiation of "void flash::compute_attn<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &) [with Kernel_traits=Flash_fwd_kernel_traits<160, 128, 32, 4, false, false, cutlass::bfloat16_t, Flash_kernel_traits<160, 128, 32, 4, cutlass::bfloat16_t>>, Is_dropout=true, Is_causal=true, Is_even_N=false, Is_even_K=true, Return_softmax=false, Params=Flash_fwd_params]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(15): here
            instantiation of "void flash_fwd_kernel<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax>(Flash_fwd_params) [with Kernel_traits=Flash_fwd_kernel_traits<160, 128, 32, 4, false, false, cutlass::bfloat16_t, Flash_kernel_traits<160, 128, 32, 4, cutlass::bfloat16_t>>, Is_dropout=true, Is_causal=true, Is_even_N=false, Is_even_K=true, Return_softmax=false]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(34): here
            instantiation of "void run_flash_fwd<Kernel_traits,Is_dropout,Is_causal>(Flash_fwd_params &, cudaStream_t) [with Kernel_traits=Flash_fwd_kernel_traits<160, 128, 32, 4, false, false, cutlass::bfloat16_t, Flash_kernel_traits<160, 128, 32, 4, cutlass::bfloat16_t>>, Is_dropout=true, Is_causal=true]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(155): here
            instantiation of "void run_mha_fwd_hdim160<T>(Flash_fwd_params &, cudaStream_t) [with T=cutlass::bfloat16_t]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_hdim160_bf16_sm80.cu(16): here

/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_kernel.h(140): warning: variable "MMA_M" was declared but never referenced
          detected during:
            instantiation of "void flash::compute_attn_1rowblock<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params,Is_Q_split_K>(const Params &, int, int, int) [with Kernel_traits=Flash_fwd_kernel_traits<160, 128, 32, 4, false, false, cutlass::bfloat16_t, Flash_kernel_traits<160, 128, 32, 4, cutlass::bfloat16_t>>, Is_dropout=true, Is_causal=true, Is_even_N=false, Is_even_K=false, Return_softmax=true, Params=Flash_fwd_params, Is_Q_split_K=false]" 
(589): here
            instantiation of "void flash::compute_attn<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &) [with Kernel_traits=Flash_fwd_kernel_traits<160, 128, 32, 4, false, false, cutlass::bfloat16_t, Flash_kernel_traits<160, 128, 32, 4, cutlass::bfloat16_t>>, Is_dropout=true, Is_causal=true, Is_even_N=false, Is_even_K=false, Return_softmax=true, Params=Flash_fwd_params]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(15): here
            instantiation of "void flash_fwd_kernel<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax>(Flash_fwd_params) [with Kernel_traits=Flash_fwd_kernel_traits<160, 128, 32, 4, false, false, cutlass::bfloat16_t, Flash_kernel_traits<160, 128, 32, 4, cutlass::bfloat16_t>>, Is_dropout=true, Is_causal=true, Is_even_N=false, Is_even_K=false, Return_softmax=true]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(34): here
            instantiation of "void run_flash_fwd<Kernel_traits,Is_dropout,Is_causal>(Flash_fwd_params &, cudaStream_t) [with Kernel_traits=Flash_fwd_kernel_traits<160, 128, 32, 4, false, false, cutlass::bfloat16_t, Flash_kernel_traits<160, 128, 32, 4, cutlass::bfloat16_t>>, Is_dropout=true, Is_causal=true]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(155): here
            instantiation of "void run_mha_fwd_hdim160<T>(Flash_fwd_params &, cudaStream_t) [with T=cutlass::bfloat16_t]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_hdim160_bf16_sm80.cu(16): here

/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_kernel.h(276): warning: variable "tQrQ" was declared but never referenced
          detected during:
            instantiation of "void flash::compute_attn_1rowblock<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params,Is_Q_split_K>(const Params &, int, int, int) [with Kernel_traits=Flash_fwd_kernel_traits<160, 128, 32, 4, false, false, cutlass::bfloat16_t, Flash_kernel_traits<160, 128, 32, 4, cutlass::bfloat16_t>>, Is_dropout=true, Is_causal=true, Is_even_N=false, Is_even_K=false, Return_softmax=true, Params=Flash_fwd_params, Is_Q_split_K=false]" 
(589): here
            instantiation of "void flash::compute_attn<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &) [with Kernel_traits=Flash_fwd_kernel_traits<160, 128, 32, 4, false, false, cutlass::bfloat16_t, Flash_kernel_traits<160, 128, 32, 4, cutlass::bfloat16_t>>, Is_dropout=true, Is_causal=true, Is_even_N=false, Is_even_K=false, Return_softmax=true, Params=Flash_fwd_params]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(15): here
            instantiation of "void flash_fwd_kernel<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax>(Flash_fwd_params) [with Kernel_traits=Flash_fwd_kernel_traits<160, 128, 32, 4, false, false, cutlass::bfloat16_t, Flash_kernel_traits<160, 128, 32, 4, cutlass::bfloat16_t>>, Is_dropout=true, Is_causal=true, Is_even_N=false, Is_even_K=false, Return_softmax=true]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(34): here
            instantiation of "void run_flash_fwd<Kernel_traits,Is_dropout,Is_causal>(Flash_fwd_params &, cudaStream_t) [with Kernel_traits=Flash_fwd_kernel_traits<160, 128, 32, 4, false, false, cutlass::bfloat16_t, Flash_kernel_traits<160, 128, 32, 4, cutlass::bfloat16_t>>, Is_dropout=true, Is_causal=true]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(155): here
            instantiation of "void run_mha_fwd_hdim160<T>(Flash_fwd_params &, cudaStream_t) [with T=cutlass::bfloat16_t]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_hdim160_bf16_sm80.cu(16): here

/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_kernel.h(140): warning: variable "MMA_M" was declared but never referenced
          detected during:
            instantiation of "void flash::compute_attn_1rowblock<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params,Is_Q_split_K>(const Params &, int, int, int) [with Kernel_traits=Flash_fwd_kernel_traits<160, 128, 32, 4, false, false, cutlass::bfloat16_t, Flash_kernel_traits<160, 128, 32, 4, cutlass::bfloat16_t>>, Is_dropout=true, Is_causal=true, Is_even_N=false, Is_even_K=false, Return_softmax=false, Params=Flash_fwd_params, Is_Q_split_K=false]" 
(589): here
            instantiation of "void flash::compute_attn<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &) [with Kernel_traits=Flash_fwd_kernel_traits<160, 128, 32, 4, false, false, cutlass::bfloat16_t, Flash_kernel_traits<160, 128, 32, 4, cutlass::bfloat16_t>>, Is_dropout=true, Is_causal=true, Is_even_N=false, Is_even_K=false, Return_softmax=false, Params=Flash_fwd_params]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(15): here
            instantiation of "void flash_fwd_kernel<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax>(Flash_fwd_params) [with Kernel_traits=Flash_fwd_kernel_traits<160, 128, 32, 4, false, false, cutlass::bfloat16_t, Flash_kernel_traits<160, 128, 32, 4, cutlass::bfloat16_t>>, Is_dropout=true, Is_causal=true, Is_even_N=false, Is_even_K=false, Return_softmax=false]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(34): here
            instantiation of "void run_flash_fwd<Kernel_traits,Is_dropout,Is_causal>(Flash_fwd_params &, cudaStream_t) [with Kernel_traits=Flash_fwd_kernel_traits<160, 128, 32, 4, false, false, cutlass::bfloat16_t, Flash_kernel_traits<160, 128, 32, 4, cutlass::bfloat16_t>>, Is_dropout=true, Is_causal=true]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(155): here
            instantiation of "void run_mha_fwd_hdim160<T>(Flash_fwd_params &, cudaStream_t) [with T=cutlass::bfloat16_t]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_hdim160_bf16_sm80.cu(16): here

/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_kernel.h(276): warning: variable "tQrQ" was declared but never referenced
          detected during:
            instantiation of "void flash::compute_attn_1rowblock<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params,Is_Q_split_K>(const Params &, int, int, int) [with Kernel_traits=Flash_fwd_kernel_traits<160, 128, 32, 4, false, false, cutlass::bfloat16_t, Flash_kernel_traits<160, 128, 32, 4, cutlass::bfloat16_t>>, Is_dropout=true, Is_causal=true, Is_even_N=false, Is_even_K=false, Return_softmax=false, Params=Flash_fwd_params, Is_Q_split_K=false]" 
(589): here
            instantiation of "void flash::compute_attn<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &) [with Kernel_traits=Flash_fwd_kernel_traits<160, 128, 32, 4, false, false, cutlass::bfloat16_t, Flash_kernel_traits<160, 128, 32, 4, cutlass::bfloat16_t>>, Is_dropout=true, Is_causal=true, Is_even_N=false, Is_even_K=false, Return_softmax=false, Params=Flash_fwd_params]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(15): here
            instantiation of "void flash_fwd_kernel<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax>(Flash_fwd_params) [with Kernel_traits=Flash_fwd_kernel_traits<160, 128, 32, 4, false, false, cutlass::bfloat16_t, Flash_kernel_traits<160, 128, 32, 4, cutlass::bfloat16_t>>, Is_dropout=true, Is_causal=true, Is_even_N=false, Is_even_K=false, Return_softmax=false]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(34): here
            instantiation of "void run_flash_fwd<Kernel_traits,Is_dropout,Is_causal>(Flash_fwd_params &, cudaStream_t) [with Kernel_traits=Flash_fwd_kernel_traits<160, 128, 32, 4, false, false, cutlass::bfloat16_t, Flash_kernel_traits<160, 128, 32, 4, cutlass::bfloat16_t>>, Is_dropout=true, Is_causal=true]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(155): here
            instantiation of "void run_mha_fwd_hdim160<T>(Flash_fwd_params &, cudaStream_t) [with T=cutlass::bfloat16_t]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_hdim160_bf16_sm80.cu(16): here

/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_kernel.h(140): warning: variable "MMA_M" was declared but never referenced
          detected during:
            instantiation of "void flash::compute_attn_1rowblock<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params,Is_Q_split_K>(const Params &, int, int, int) [with Kernel_traits=Flash_fwd_kernel_traits<160, 128, 64, 8, false, false, cutlass::bfloat16_t, Flash_kernel_traits<160, 128, 64, 8, cutlass::bfloat16_t>>, Is_dropout=true, Is_causal=false, Is_even_N=true, Is_even_K=true, Return_softmax=true, Params=Flash_fwd_params, Is_Q_split_K=false]" 
(589): here
            instantiation of "void flash::compute_attn<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &) [with Kernel_traits=Flash_fwd_kernel_traits<160, 128, 64, 8, false, false, cutlass::bfloat16_t, Flash_kernel_traits<160, 128, 64, 8, cutlass::bfloat16_t>>, Is_dropout=true, Is_causal=false, Is_even_N=true, Is_even_K=true, Return_softmax=true, Params=Flash_fwd_params]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(15): here
            instantiation of "void flash_fwd_kernel<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax>(Flash_fwd_params) [with Kernel_traits=Flash_fwd_kernel_traits<160, 128, 64, 8, false, false, cutlass::bfloat16_t, Flash_kernel_traits<160, 128, 64, 8, cutlass::bfloat16_t>>, Is_dropout=true, Is_causal=false, Is_even_N=true, Is_even_K=true, Return_softmax=true]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(34): here
            instantiation of "void run_flash_fwd<Kernel_traits,Is_dropout,Is_causal>(Flash_fwd_params &, cudaStream_t) [with Kernel_traits=Flash_fwd_kernel_traits<160, 128, 64, 8, false, false, cutlass::bfloat16_t, Flash_kernel_traits<160, 128, 64, 8, cutlass::bfloat16_t>>, Is_dropout=true, Is_causal=false]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(155): here
            instantiation of "void run_mha_fwd_hdim160<T>(Flash_fwd_params &, cudaStream_t) [with T=cutlass::bfloat16_t]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_hdim160_bf16_sm80.cu(16): here

/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_kernel.h(276): warning: variable "tQrQ" was declared but never referenced
          detected during:
            instantiation of "void flash::compute_attn_1rowblock<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params,Is_Q_split_K>(const Params &, int, int, int) [with Kernel_traits=Flash_fwd_kernel_traits<160, 128, 64, 8, false, false, cutlass::bfloat16_t, Flash_kernel_traits<160, 128, 64, 8, cutlass::bfloat16_t>>, Is_dropout=true, Is_causal=false, Is_even_N=true, Is_even_K=true, Return_softmax=true, Params=Flash_fwd_params, Is_Q_split_K=false]" 
(589): here
            instantiation of "void flash::compute_attn<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &) [with Kernel_traits=Flash_fwd_kernel_traits<160, 128, 64, 8, false, false, cutlass::bfloat16_t, Flash_kernel_traits<160, 128, 64, 8, cutlass::bfloat16_t>>, Is_dropout=true, Is_causal=false, Is_even_N=true, Is_even_K=true, Return_softmax=true, Params=Flash_fwd_params]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(15): here
            instantiation of "void flash_fwd_kernel<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax>(Flash_fwd_params) [with Kernel_traits=Flash_fwd_kernel_traits<160, 128, 64, 8, false, false, cutlass::bfloat16_t, Flash_kernel_traits<160, 128, 64, 8, cutlass::bfloat16_t>>, Is_dropout=true, Is_causal=false, Is_even_N=true, Is_even_K=true, Return_softmax=true]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(34): here
            instantiation of "void run_flash_fwd<Kernel_traits,Is_dropout,Is_causal>(Flash_fwd_params &, cudaStream_t) [with Kernel_traits=Flash_fwd_kernel_traits<160, 128, 64, 8, false, false, cutlass::bfloat16_t, Flash_kernel_traits<160, 128, 64, 8, cutlass::bfloat16_t>>, Is_dropout=true, Is_causal=false]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(155): here
            instantiation of "void run_mha_fwd_hdim160<T>(Flash_fwd_params &, cudaStream_t) [with T=cutlass::bfloat16_t]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_hdim160_bf16_sm80.cu(16): here

/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_kernel.h(140): warning: variable "MMA_M" was declared but never referenced
          detected during:
            instantiation of "void flash::compute_attn_1rowblock<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params,Is_Q_split_K>(const Params &, int, int, int) [with Kernel_traits=Flash_fwd_kernel_traits<160, 128, 64, 8, false, false, cutlass::bfloat16_t, Flash_kernel_traits<160, 128, 64, 8, cutlass::bfloat16_t>>, Is_dropout=true, Is_causal=false, Is_even_N=true, Is_even_K=true, Return_softmax=false, Params=Flash_fwd_params, Is_Q_split_K=false]" 
(589): here
            instantiation of "void flash::compute_attn<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &) [with Kernel_traits=Flash_fwd_kernel_traits<160, 128, 64, 8, false, false, cutlass::bfloat16_t, Flash_kernel_traits<160, 128, 64, 8, cutlass::bfloat16_t>>, Is_dropout=true, Is_causal=false, Is_even_N=true, Is_even_K=true, Return_softmax=false, Params=Flash_fwd_params]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(15): here
            instantiation of "void flash_fwd_kernel<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax>(Flash_fwd_params) [with Kernel_traits=Flash_fwd_kernel_traits<160, 128, 64, 8, false, false, cutlass::bfloat16_t, Flash_kernel_traits<160, 128, 64, 8, cutlass::bfloat16_t>>, Is_dropout=true, Is_causal=false, Is_even_N=true, Is_even_K=true, Return_softmax=false]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(34): here
            instantiation of "void run_flash_fwd<Kernel_traits,Is_dropout,Is_causal>(Flash_fwd_params &, cudaStream_t) [with Kernel_traits=Flash_fwd_kernel_traits<160, 128, 64, 8, false, false, cutlass::bfloat16_t, Flash_kernel_traits<160, 128, 64, 8, cutlass::bfloat16_t>>, Is_dropout=true, Is_causal=false]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(155): here
            instantiation of "void run_mha_fwd_hdim160<T>(Flash_fwd_params &, cudaStream_t) [with T=cutlass::bfloat16_t]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_hdim160_bf16_sm80.cu(16): here

/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_kernel.h(276): warning: variable "tQrQ" was declared but never referenced
          detected during:
            instantiation of "void flash::compute_attn_1rowblock<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params,Is_Q_split_K>(const Params &, int, int, int) [with Kernel_traits=Flash_fwd_kernel_traits<160, 128, 64, 8, false, false, cutlass::bfloat16_t, Flash_kernel_traits<160, 128, 64, 8, cutlass::bfloat16_t>>, Is_dropout=true, Is_causal=false, Is_even_N=true, Is_even_K=true, Return_softmax=false, Params=Flash_fwd_params, Is_Q_split_K=false]" 
(589): here
            instantiation of "void flash::compute_attn<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &) [with Kernel_traits=Flash_fwd_kernel_traits<160, 128, 64, 8, false, false, cutlass::bfloat16_t, Flash_kernel_traits<160, 128, 64, 8, cutlass::bfloat16_t>>, Is_dropout=true, Is_causal=false, Is_even_N=true, Is_even_K=true, Return_softmax=false, Params=Flash_fwd_params]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(15): here
            instantiation of "void flash_fwd_kernel<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax>(Flash_fwd_params) [with Kernel_traits=Flash_fwd_kernel_traits<160, 128, 64, 8, false, false, cutlass::bfloat16_t, Flash_kernel_traits<160, 128, 64, 8, cutlass::bfloat16_t>>, Is_dropout=true, Is_causal=false, Is_even_N=true, Is_even_K=true, Return_softmax=false]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(34): here
            instantiation of "void run_flash_fwd<Kernel_traits,Is_dropout,Is_causal>(Flash_fwd_params &, cudaStream_t) [with Kernel_traits=Flash_fwd_kernel_traits<160, 128, 64, 8, false, false, cutlass::bfloat16_t, Flash_kernel_traits<160, 128, 64, 8, cutlass::bfloat16_t>>, Is_dropout=true, Is_causal=false]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(155): here
            instantiation of "void run_mha_fwd_hdim160<T>(Flash_fwd_params &, cudaStream_t) [with T=cutlass::bfloat16_t]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_hdim160_bf16_sm80.cu(16): here

/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_kernel.h(140): warning: variable "MMA_M" was declared but never referenced
          detected during:
            instantiation of "void flash::compute_attn_1rowblock<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params,Is_Q_split_K>(const Params &, int, int, int) [with Kernel_traits=Flash_fwd_kernel_traits<160, 128, 64, 8, false, false, cutlass::bfloat16_t, Flash_kernel_traits<160, 128, 64, 8, cutlass::bfloat16_t>>, Is_dropout=true, Is_causal=false, Is_even_N=true, Is_even_K=false, Return_softmax=true, Params=Flash_fwd_params, Is_Q_split_K=false]" 
(589): here
            instantiation of "void flash::compute_attn<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &) [with Kernel_traits=Flash_fwd_kernel_traits<160, 128, 64, 8, false, false, cutlass::bfloat16_t, Flash_kernel_traits<160, 128, 64, 8, cutlass::bfloat16_t>>, Is_dropout=true, Is_causal=false, Is_even_N=true, Is_even_K=false, Return_softmax=true, Params=Flash_fwd_params]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(15): here
            instantiation of "void flash_fwd_kernel<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax>(Flash_fwd_params) [with Kernel_traits=Flash_fwd_kernel_traits<160, 128, 64, 8, false, false, cutlass::bfloat16_t, Flash_kernel_traits<160, 128, 64, 8, cutlass::bfloat16_t>>, Is_dropout=true, Is_causal=false, Is_even_N=true, Is_even_K=false, Return_softmax=true]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(34): here
            instantiation of "void run_flash_fwd<Kernel_traits,Is_dropout,Is_causal>(Flash_fwd_params &, cudaStream_t) [with Kernel_traits=Flash_fwd_kernel_traits<160, 128, 64, 8, false, false, cutlass::bfloat16_t, Flash_kernel_traits<160, 128, 64, 8, cutlass::bfloat16_t>>, Is_dropout=true, Is_causal=false]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(155): here
            instantiation of "void run_mha_fwd_hdim160<T>(Flash_fwd_params &, cudaStream_t) [with T=cutlass::bfloat16_t]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_hdim160_bf16_sm80.cu(16): here

/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_kernel.h(276): warning: variable "tQrQ" was declared but never referenced
          detected during:
            instantiation of "void flash::compute_attn_1rowblock<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params,Is_Q_split_K>(const Params &, int, int, int) [with Kernel_traits=Flash_fwd_kernel_traits<160, 128, 64, 8, false, false, cutlass::bfloat16_t, Flash_kernel_traits<160, 128, 64, 8, cutlass::bfloat16_t>>, Is_dropout=true, Is_causal=false, Is_even_N=true, Is_even_K=false, Return_softmax=true, Params=Flash_fwd_params, Is_Q_split_K=false]" 
(589): here
            instantiation of "void flash::compute_attn<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &) [with Kernel_traits=Flash_fwd_kernel_traits<160, 128, 64, 8, false, false, cutlass::bfloat16_t, Flash_kernel_traits<160, 128, 64, 8, cutlass::bfloat16_t>>, Is_dropout=true, Is_causal=false, Is_even_N=true, Is_even_K=false, Return_softmax=true, Params=Flash_fwd_params]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(15): here
            instantiation of "void flash_fwd_kernel<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax>(Flash_fwd_params) [with Kernel_traits=Flash_fwd_kernel_traits<160, 128, 64, 8, false, false, cutlass::bfloat16_t, Flash_kernel_traits<160, 128, 64, 8, cutlass::bfloat16_t>>, Is_dropout=true, Is_causal=false, Is_even_N=true, Is_even_K=false, Return_softmax=true]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(34): here
            instantiation of "void run_flash_fwd<Kernel_traits,Is_dropout,Is_causal>(Flash_fwd_params &, cudaStream_t) [with Kernel_traits=Flash_fwd_kernel_traits<160, 128, 64, 8, false, false, cutlass::bfloat16_t, Flash_kernel_traits<160, 128, 64, 8, cutlass::bfloat16_t>>, Is_dropout=true, Is_causal=false]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(155): here
            instantiation of "void run_mha_fwd_hdim160<T>(Flash_fwd_params &, cudaStream_t) [with T=cutlass::bfloat16_t]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_hdim160_bf16_sm80.cu(16): here

/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_kernel.h(140): warning: variable "MMA_M" was declared but never referenced
          detected during:
            instantiation of "void flash::compute_attn_1rowblock<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params,Is_Q_split_K>(const Params &, int, int, int) [with Kernel_traits=Flash_fwd_kernel_traits<160, 128, 64, 8, false, false, cutlass::bfloat16_t, Flash_kernel_traits<160, 128, 64, 8, cutlass::bfloat16_t>>, Is_dropout=true, Is_causal=false, Is_even_N=true, Is_even_K=false, Return_softmax=false, Params=Flash_fwd_params, Is_Q_split_K=false]" 
(589): here
            instantiation of "void flash::compute_attn<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &) [with Kernel_traits=Flash_fwd_kernel_traits<160, 128, 64, 8, false, false, cutlass::bfloat16_t, Flash_kernel_traits<160, 128, 64, 8, cutlass::bfloat16_t>>, Is_dropout=true, Is_causal=false, Is_even_N=true, Is_even_K=false, Return_softmax=false, Params=Flash_fwd_params]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(15): here
            instantiation of "void flash_fwd_kernel<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax>(Flash_fwd_params) [with Kernel_traits=Flash_fwd_kernel_traits<160, 128, 64, 8, false, false, cutlass::bfloat16_t, Flash_kernel_traits<160, 128, 64, 8, cutlass::bfloat16_t>>, Is_dropout=true, Is_causal=false, Is_even_N=true, Is_even_K=false, Return_softmax=false]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(34): here
            instantiation of "void run_flash_fwd<Kernel_traits,Is_dropout,Is_causal>(Flash_fwd_params &, cudaStream_t) [with Kernel_traits=Flash_fwd_kernel_traits<160, 128, 64, 8, false, false, cutlass::bfloat16_t, Flash_kernel_traits<160, 128, 64, 8, cutlass::bfloat16_t>>, Is_dropout=true, Is_causal=false]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(155): here
            instantiation of "void run_mha_fwd_hdim160<T>(Flash_fwd_params &, cudaStream_t) [with T=cutlass::bfloat16_t]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_hdim160_bf16_sm80.cu(16): here

/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_kernel.h(276): warning: variable "tQrQ" was declared but never referenced
          detected during:
            instantiation of "void flash::compute_attn_1rowblock<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params,Is_Q_split_K>(const Params &, int, int, int) [with Kernel_traits=Flash_fwd_kernel_traits<160, 128, 64, 8, false, false, cutlass::bfloat16_t, Flash_kernel_traits<160, 128, 64, 8, cutlass::bfloat16_t>>, Is_dropout=true, Is_causal=false, Is_even_N=true, Is_even_K=false, Return_softmax=false, Params=Flash_fwd_params, Is_Q_split_K=false]" 
(589): here
            instantiation of "void flash::compute_attn<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &) [with Kernel_traits=Flash_fwd_kernel_traits<160, 128, 64, 8, false, false, cutlass::bfloat16_t, Flash_kernel_traits<160, 128, 64, 8, cutlass::bfloat16_t>>, Is_dropout=true, Is_causal=false, Is_even_N=true, Is_even_K=false, Return_softmax=false, Params=Flash_fwd_params]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(15): here
            instantiation of "void flash_fwd_kernel<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax>(Flash_fwd_params) [with Kernel_traits=Flash_fwd_kernel_traits<160, 128, 64, 8, false, false, cutlass::bfloat16_t, Flash_kernel_traits<160, 128, 64, 8, cutlass::bfloat16_t>>, Is_dropout=true, Is_causal=false, Is_even_N=true, Is_even_K=false, Return_softmax=false]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(34): here
            instantiation of "void run_flash_fwd<Kernel_traits,Is_dropout,Is_causal>(Flash_fwd_params &, cudaStream_t) [with Kernel_traits=Flash_fwd_kernel_traits<160, 128, 64, 8, false, false, cutlass::bfloat16_t, Flash_kernel_traits<160, 128, 64, 8, cutlass::bfloat16_t>>, Is_dropout=true, Is_causal=false]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(155): here
            instantiation of "void run_mha_fwd_hdim160<T>(Flash_fwd_params &, cudaStream_t) [with T=cutlass::bfloat16_t]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_hdim160_bf16_sm80.cu(16): here

/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_kernel.h(140): warning: variable "MMA_M" was declared but never referenced
          detected during:
            instantiation of "void flash::compute_attn_1rowblock<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params,Is_Q_split_K>(const Params &, int, int, int) [with Kernel_traits=Flash_fwd_kernel_traits<160, 128, 64, 8, false, false, cutlass::bfloat16_t, Flash_kernel_traits<160, 128, 64, 8, cutlass::bfloat16_t>>, Is_dropout=true, Is_causal=false, Is_even_N=false, Is_even_K=true, Return_softmax=true, Params=Flash_fwd_params, Is_Q_split_K=false]" 
(589): here
            instantiation of "void flash::compute_attn<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &) [with Kernel_traits=Flash_fwd_kernel_traits<160, 128, 64, 8, false, false, cutlass::bfloat16_t, Flash_kernel_traits<160, 128, 64, 8, cutlass::bfloat16_t>>, Is_dropout=true, Is_causal=false, Is_even_N=false, Is_even_K=true, Return_softmax=true, Params=Flash_fwd_params]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(15): here
            instantiation of "void flash_fwd_kernel<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax>(Flash_fwd_params) [with Kernel_traits=Flash_fwd_kernel_traits<160, 128, 64, 8, false, false, cutlass::bfloat16_t, Flash_kernel_traits<160, 128, 64, 8, cutlass::bfloat16_t>>, Is_dropout=true, Is_causal=false, Is_even_N=false, Is_even_K=true, Return_softmax=true]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(34): here
            instantiation of "void run_flash_fwd<Kernel_traits,Is_dropout,Is_causal>(Flash_fwd_params &, cudaStream_t) [with Kernel_traits=Flash_fwd_kernel_traits<160, 128, 64, 8, false, false, cutlass::bfloat16_t, Flash_kernel_traits<160, 128, 64, 8, cutlass::bfloat16_t>>, Is_dropout=true, Is_causal=false]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(155): here
            instantiation of "void run_mha_fwd_hdim160<T>(Flash_fwd_params &, cudaStream_t) [with T=cutlass::bfloat16_t]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_hdim160_bf16_sm80.cu(16): here

/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_kernel.h(276): warning: variable "tQrQ" was declared but never referenced
          detected during:
            instantiation of "void flash::compute_attn_1rowblock<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params,Is_Q_split_K>(const Params &, int, int, int) [with Kernel_traits=Flash_fwd_kernel_traits<160, 128, 64, 8, false, false, cutlass::bfloat16_t, Flash_kernel_traits<160, 128, 64, 8, cutlass::bfloat16_t>>, Is_dropout=true, Is_causal=false, Is_even_N=false, Is_even_K=true, Return_softmax=true, Params=Flash_fwd_params, Is_Q_split_K=false]" 
(589): here
            instantiation of "void flash::compute_attn<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &) [with Kernel_traits=Flash_fwd_kernel_traits<160, 128, 64, 8, false, false, cutlass::bfloat16_t, Flash_kernel_traits<160, 128, 64, 8, cutlass::bfloat16_t>>, Is_dropout=true, Is_causal=false, Is_even_N=false, Is_even_K=true, Return_softmax=true, Params=Flash_fwd_params]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(15): here
            instantiation of "void flash_fwd_kernel<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax>(Flash_fwd_params) [with Kernel_traits=Flash_fwd_kernel_traits<160, 128, 64, 8, false, false, cutlass::bfloat16_t, Flash_kernel_traits<160, 128, 64, 8, cutlass::bfloat16_t>>, Is_dropout=true, Is_causal=false, Is_even_N=false, Is_even_K=true, Return_softmax=true]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(34): here
            instantiation of "void run_flash_fwd<Kernel_traits,Is_dropout,Is_causal>(Flash_fwd_params &, cudaStream_t) [with Kernel_traits=Flash_fwd_kernel_traits<160, 128, 64, 8, false, false, cutlass::bfloat16_t, Flash_kernel_traits<160, 128, 64, 8, cutlass::bfloat16_t>>, Is_dropout=true, Is_causal=false]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(155): here
            instantiation of "void run_mha_fwd_hdim160<T>(Flash_fwd_params &, cudaStream_t) [with T=cutlass::bfloat16_t]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_hdim160_bf16_sm80.cu(16): here

/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_kernel.h(140): warning: variable "MMA_M" was declared but never referenced
          detected during:
            instantiation of "void flash::compute_attn_1rowblock<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params,Is_Q_split_K>(const Params &, int, int, int) [with Kernel_traits=Flash_fwd_kernel_traits<160, 128, 64, 8, false, false, cutlass::bfloat16_t, Flash_kernel_traits<160, 128, 64, 8, cutlass::bfloat16_t>>, Is_dropout=true, Is_causal=false, Is_even_N=false, Is_even_K=true, Return_softmax=false, Params=Flash_fwd_params, Is_Q_split_K=false]" 
(589): here
            instantiation of "void flash::compute_attn<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &) [with Kernel_traits=Flash_fwd_kernel_traits<160, 128, 64, 8, false, false, cutlass::bfloat16_t, Flash_kernel_traits<160, 128, 64, 8, cutlass::bfloat16_t>>, Is_dropout=true, Is_causal=false, Is_even_N=false, Is_even_K=true, Return_softmax=false, Params=Flash_fwd_params]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(15): here
            instantiation of "void flash_fwd_kernel<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax>(Flash_fwd_params) [with Kernel_traits=Flash_fwd_kernel_traits<160, 128, 64, 8, false, false, cutlass::bfloat16_t, Flash_kernel_traits<160, 128, 64, 8, cutlass::bfloat16_t>>, Is_dropout=true, Is_causal=false, Is_even_N=false, Is_even_K=true, Return_softmax=false]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(34): here
            instantiation of "void run_flash_fwd<Kernel_traits,Is_dropout,Is_causal>(Flash_fwd_params &, cudaStream_t) [with Kernel_traits=Flash_fwd_kernel_traits<160, 128, 64, 8, false, false, cutlass::bfloat16_t, Flash_kernel_traits<160, 128, 64, 8, cutlass::bfloat16_t>>, Is_dropout=true, Is_causal=false]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(155): here
            instantiation of "void run_mha_fwd_hdim160<T>(Flash_fwd_params &, cudaStream_t) [with T=cutlass::bfloat16_t]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_hdim160_bf16_sm80.cu(16): here

/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_kernel.h(276): warning: variable "tQrQ" was declared but never referenced
          detected during:
            instantiation of "void flash::compute_attn_1rowblock<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params,Is_Q_split_K>(const Params &, int, int, int) [with Kernel_traits=Flash_fwd_kernel_traits<160, 128, 64, 8, false, false, cutlass::bfloat16_t, Flash_kernel_traits<160, 128, 64, 8, cutlass::bfloat16_t>>, Is_dropout=true, Is_causal=false, Is_even_N=false, Is_even_K=true, Return_softmax=false, Params=Flash_fwd_params, Is_Q_split_K=false]" 
(589): here
            instantiation of "void flash::compute_attn<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &) [with Kernel_traits=Flash_fwd_kernel_traits<160, 128, 64, 8, false, false, cutlass::bfloat16_t, Flash_kernel_traits<160, 128, 64, 8, cutlass::bfloat16_t>>, Is_dropout=true, Is_causal=false, Is_even_N=false, Is_even_K=true, Return_softmax=false, Params=Flash_fwd_params]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(15): here
            instantiation of "void flash_fwd_kernel<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax>(Flash_fwd_params) [with Kernel_traits=Flash_fwd_kernel_traits<160, 128, 64, 8, false, false, cutlass::bfloat16_t, Flash_kernel_traits<160, 128, 64, 8, cutlass::bfloat16_t>>, Is_dropout=true, Is_causal=false, Is_even_N=false, Is_even_K=true, Return_softmax=false]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(34): here
            instantiation of "void run_flash_fwd<Kernel_traits,Is_dropout,Is_causal>(Flash_fwd_params &, cudaStream_t) [with Kernel_traits=Flash_fwd_kernel_traits<160, 128, 64, 8, false, false, cutlass::bfloat16_t, Flash_kernel_traits<160, 128, 64, 8, cutlass::bfloat16_t>>, Is_dropout=true, Is_causal=false]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(155): here
            instantiation of "void run_mha_fwd_hdim160<T>(Flash_fwd_params &, cudaStream_t) [with T=cutlass::bfloat16_t]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_hdim160_bf16_sm80.cu(16): here

/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_kernel.h(140): warning: variable "MMA_M" was declared but never referenced
          detected during:
            instantiation of "void flash::compute_attn_1rowblock<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params,Is_Q_split_K>(const Params &, int, int, int) [with Kernel_traits=Flash_fwd_kernel_traits<160, 128, 64, 8, false, false, cutlass::bfloat16_t, Flash_kernel_traits<160, 128, 64, 8, cutlass::bfloat16_t>>, Is_dropout=true, Is_causal=false, Is_even_N=false, Is_even_K=false, Return_softmax=true, Params=Flash_fwd_params, Is_Q_split_K=false]" 
(589): here
            instantiation of "void flash::compute_attn<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &) [with Kernel_traits=Flash_fwd_kernel_traits<160, 128, 64, 8, false, false, cutlass::bfloat16_t, Flash_kernel_traits<160, 128, 64, 8, cutlass::bfloat16_t>>, Is_dropout=true, Is_causal=false, Is_even_N=false, Is_even_K=false, Return_softmax=true, Params=Flash_fwd_params]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(15): here
            instantiation of "void flash_fwd_kernel<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax>(Flash_fwd_params) [with Kernel_traits=Flash_fwd_kernel_traits<160, 128, 64, 8, false, false, cutlass::bfloat16_t, Flash_kernel_traits<160, 128, 64, 8, cutlass::bfloat16_t>>, Is_dropout=true, Is_causal=false, Is_even_N=false, Is_even_K=false, Return_softmax=true]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(34): here
            instantiation of "void run_flash_fwd<Kernel_traits,Is_dropout,Is_causal>(Flash_fwd_params &, cudaStream_t) [with Kernel_traits=Flash_fwd_kernel_traits<160, 128, 64, 8, false, false, cutlass::bfloat16_t, Flash_kernel_traits<160, 128, 64, 8, cutlass::bfloat16_t>>, Is_dropout=true, Is_causal=false]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(155): here
            instantiation of "void run_mha_fwd_hdim160<T>(Flash_fwd_params &, cudaStream_t) [with T=cutlass::bfloat16_t]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_hdim160_bf16_sm80.cu(16): here

/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_kernel.h(276): warning: variable "tQrQ" was declared but never referenced
          detected during:
            instantiation of "void flash::compute_attn_1rowblock<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params,Is_Q_split_K>(const Params &, int, int, int) [with Kernel_traits=Flash_fwd_kernel_traits<160, 128, 64, 8, false, false, cutlass::bfloat16_t, Flash_kernel_traits<160, 128, 64, 8, cutlass::bfloat16_t>>, Is_dropout=true, Is_causal=false, Is_even_N=false, Is_even_K=false, Return_softmax=true, Params=Flash_fwd_params, Is_Q_split_K=false]" 
(589): here
            instantiation of "void flash::compute_attn<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &) [with Kernel_traits=Flash_fwd_kernel_traits<160, 128, 64, 8, false, false, cutlass::bfloat16_t, Flash_kernel_traits<160, 128, 64, 8, cutlass::bfloat16_t>>, Is_dropout=true, Is_causal=false, Is_even_N=false, Is_even_K=false, Return_softmax=true, Params=Flash_fwd_params]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(15): here
            instantiation of "void flash_fwd_kernel<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax>(Flash_fwd_params) [with Kernel_traits=Flash_fwd_kernel_traits<160, 128, 64, 8, false, false, cutlass::bfloat16_t, Flash_kernel_traits<160, 128, 64, 8, cutlass::bfloat16_t>>, Is_dropout=true, Is_causal=false, Is_even_N=false, Is_even_K=false, Return_softmax=true]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(34): here
            instantiation of "void run_flash_fwd<Kernel_traits,Is_dropout,Is_causal>(Flash_fwd_params &, cudaStream_t) [with Kernel_traits=Flash_fwd_kernel_traits<160, 128, 64, 8, false, false, cutlass::bfloat16_t, Flash_kernel_traits<160, 128, 64, 8, cutlass::bfloat16_t>>, Is_dropout=true, Is_causal=false]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(155): here
            instantiation of "void run_mha_fwd_hdim160<T>(Flash_fwd_params &, cudaStream_t) [with T=cutlass::bfloat16_t]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_hdim160_bf16_sm80.cu(16): here

/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_kernel.h(140): warning: variable "MMA_M" was declared but never referenced
          detected during:
            instantiation of "void flash::compute_attn_1rowblock<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params,Is_Q_split_K>(const Params &, int, int, int) [with Kernel_traits=Flash_fwd_kernel_traits<160, 128, 64, 8, false, false, cutlass::bfloat16_t, Flash_kernel_traits<160, 128, 64, 8, cutlass::bfloat16_t>>, Is_dropout=true, Is_causal=false, Is_even_N=false, Is_even_K=false, Return_softmax=false, Params=Flash_fwd_params, Is_Q_split_K=false]" 
(589): here
            instantiation of "void flash::compute_attn<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &) [with Kernel_traits=Flash_fwd_kernel_traits<160, 128, 64, 8, false, false, cutlass::bfloat16_t, Flash_kernel_traits<160, 128, 64, 8, cutlass::bfloat16_t>>, Is_dropout=true, Is_causal=false, Is_even_N=false, Is_even_K=false, Return_softmax=false, Params=Flash_fwd_params]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(15): here
            instantiation of "void flash_fwd_kernel<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax>(Flash_fwd_params) [with Kernel_traits=Flash_fwd_kernel_traits<160, 128, 64, 8, false, false, cutlass::bfloat16_t, Flash_kernel_traits<160, 128, 64, 8, cutlass::bfloat16_t>>, Is_dropout=true, Is_causal=false, Is_even_N=false, Is_even_K=false, Return_softmax=false]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(34): here
            instantiation of "void run_flash_fwd<Kernel_traits,Is_dropout,Is_causal>(Flash_fwd_params &, cudaStream_t) [with Kernel_traits=Flash_fwd_kernel_traits<160, 128, 64, 8, false, false, cutlass::bfloat16_t, Flash_kernel_traits<160, 128, 64, 8, cutlass::bfloat16_t>>, Is_dropout=true, Is_causal=false]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(155): here
            instantiation of "void run_mha_fwd_hdim160<T>(Flash_fwd_params &, cudaStream_t) [with T=cutlass::bfloat16_t]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_hdim160_bf16_sm80.cu(16): here

/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_kernel.h(276): warning: variable "tQrQ" was declared but never referenced
          detected during:
            instantiation of "void flash::compute_attn_1rowblock<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params,Is_Q_split_K>(const Params &, int, int, int) [with Kernel_traits=Flash_fwd_kernel_traits<160, 128, 64, 8, false, false, cutlass::bfloat16_t, Flash_kernel_traits<160, 128, 64, 8, cutlass::bfloat16_t>>, Is_dropout=true, Is_causal=false, Is_even_N=false, Is_even_K=false, Return_softmax=false, Params=Flash_fwd_params, Is_Q_split_K=false]" 
(589): here
            instantiation of "void flash::compute_attn<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &) [with Kernel_traits=Flash_fwd_kernel_traits<160, 128, 64, 8, false, false, cutlass::bfloat16_t, Flash_kernel_traits<160, 128, 64, 8, cutlass::bfloat16_t>>, Is_dropout=true, Is_causal=false, Is_even_N=false, Is_even_K=false, Return_softmax=false, Params=Flash_fwd_params]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(15): here
            instantiation of "void flash_fwd_kernel<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax>(Flash_fwd_params) [with Kernel_traits=Flash_fwd_kernel_traits<160, 128, 64, 8, false, false, cutlass::bfloat16_t, Flash_kernel_traits<160, 128, 64, 8, cutlass::bfloat16_t>>, Is_dropout=true, Is_causal=false, Is_even_N=false, Is_even_K=false, Return_softmax=false]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(34): here
            instantiation of "void run_flash_fwd<Kernel_traits,Is_dropout,Is_causal>(Flash_fwd_params &, cudaStream_t) [with Kernel_traits=Flash_fwd_kernel_traits<160, 128, 64, 8, false, false, cutlass::bfloat16_t, Flash_kernel_traits<160, 128, 64, 8, cutlass::bfloat16_t>>, Is_dropout=true, Is_causal=false]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(155): here
            instantiation of "void run_mha_fwd_hdim160<T>(Flash_fwd_params &, cudaStream_t) [with T=cutlass::bfloat16_t]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_hdim160_bf16_sm80.cu(16): here

/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_kernel.h(140): warning: variable "MMA_M" was declared but never referenced
          detected during:
            instantiation of "void flash::compute_attn_1rowblock<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params,Is_Q_split_K>(const Params &, int, int, int) [with Kernel_traits=Flash_fwd_kernel_traits<160, 128, 32, 4, false, false, cutlass::bfloat16_t, Flash_kernel_traits<160, 128, 32, 4, cutlass::bfloat16_t>>, Is_dropout=true, Is_causal=false, Is_even_N=true, Is_even_K=true, Return_softmax=true, Params=Flash_fwd_params, Is_Q_split_K=false]" 
(589): here
            instantiation of "void flash::compute_attn<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &) [with Kernel_traits=Flash_fwd_kernel_traits<160, 128, 32, 4, false, false, cutlass::bfloat16_t, Flash_kernel_traits<160, 128, 32, 4, cutlass::bfloat16_t>>, Is_dropout=true, Is_causal=false, Is_even_N=true, Is_even_K=true, Return_softmax=true, Params=Flash_fwd_params]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(15): here
            instantiation of "void flash_fwd_kernel<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax>(Flash_fwd_params) [with Kernel_traits=Flash_fwd_kernel_traits<160, 128, 32, 4, false, false, cutlass::bfloat16_t, Flash_kernel_traits<160, 128, 32, 4, cutlass::bfloat16_t>>, Is_dropout=true, Is_causal=false, Is_even_N=true, Is_even_K=true, Return_softmax=true]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(34): here
            instantiation of "void run_flash_fwd<Kernel_traits,Is_dropout,Is_causal>(Flash_fwd_params &, cudaStream_t) [with Kernel_traits=Flash_fwd_kernel_traits<160, 128, 32, 4, false, false, cutlass::bfloat16_t, Flash_kernel_traits<160, 128, 32, 4, cutlass::bfloat16_t>>, Is_dropout=true, Is_causal=false]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(155): here
            instantiation of "void run_mha_fwd_hdim160<T>(Flash_fwd_params &, cudaStream_t) [with T=cutlass::bfloat16_t]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_hdim160_bf16_sm80.cu(16): here

/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_kernel.h(276): warning: variable "tQrQ" was declared but never referenced
          detected during:
            instantiation of "void flash::compute_attn_1rowblock<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params,Is_Q_split_K>(const Params &, int, int, int) [with Kernel_traits=Flash_fwd_kernel_traits<160, 128, 32, 4, false, false, cutlass::bfloat16_t, Flash_kernel_traits<160, 128, 32, 4, cutlass::bfloat16_t>>, Is_dropout=true, Is_causal=false, Is_even_N=true, Is_even_K=true, Return_softmax=true, Params=Flash_fwd_params, Is_Q_split_K=false]" 
(589): here
            instantiation of "void flash::compute_attn<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &) [with Kernel_traits=Flash_fwd_kernel_traits<160, 128, 32, 4, false, false, cutlass::bfloat16_t, Flash_kernel_traits<160, 128, 32, 4, cutlass::bfloat16_t>>, Is_dropout=true, Is_causal=false, Is_even_N=true, Is_even_K=true, Return_softmax=true, Params=Flash_fwd_params]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(15): here
            instantiation of "void flash_fwd_kernel<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax>(Flash_fwd_params) [with Kernel_traits=Flash_fwd_kernel_traits<160, 128, 32, 4, false, false, cutlass::bfloat16_t, Flash_kernel_traits<160, 128, 32, 4, cutlass::bfloat16_t>>, Is_dropout=true, Is_causal=false, Is_even_N=true, Is_even_K=true, Return_softmax=true]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(34): here
            instantiation of "void run_flash_fwd<Kernel_traits,Is_dropout,Is_causal>(Flash_fwd_params &, cudaStream_t) [with Kernel_traits=Flash_fwd_kernel_traits<160, 128, 32, 4, false, false, cutlass::bfloat16_t, Flash_kernel_traits<160, 128, 32, 4, cutlass::bfloat16_t>>, Is_dropout=true, Is_causal=false]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(155): here
            instantiation of "void run_mha_fwd_hdim160<T>(Flash_fwd_params &, cudaStream_t) [with T=cutlass::bfloat16_t]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_hdim160_bf16_sm80.cu(16): here

/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_kernel.h(140): warning: variable "MMA_M" was declared but never referenced
          detected during:
            instantiation of "void flash::compute_attn_1rowblock<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params,Is_Q_split_K>(const Params &, int, int, int) [with Kernel_traits=Flash_fwd_kernel_traits<160, 128, 32, 4, false, false, cutlass::bfloat16_t, Flash_kernel_traits<160, 128, 32, 4, cutlass::bfloat16_t>>, Is_dropout=true, Is_causal=false, Is_even_N=true, Is_even_K=true, Return_softmax=false, Params=Flash_fwd_params, Is_Q_split_K=false]" 
(589): here
            instantiation of "void flash::compute_attn<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &) [with Kernel_traits=Flash_fwd_kernel_traits<160, 128, 32, 4, false, false, cutlass::bfloat16_t, Flash_kernel_traits<160, 128, 32, 4, cutlass::bfloat16_t>>, Is_dropout=true, Is_causal=false, Is_even_N=true, Is_even_K=true, Return_softmax=false, Params=Flash_fwd_params]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(15): here
            instantiation of "void flash_fwd_kernel<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax>(Flash_fwd_params) [with Kernel_traits=Flash_fwd_kernel_traits<160, 128, 32, 4, false, false, cutlass::bfloat16_t, Flash_kernel_traits<160, 128, 32, 4, cutlass::bfloat16_t>>, Is_dropout=true, Is_causal=false, Is_even_N=true, Is_even_K=true, Return_softmax=false]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(34): here
            instantiation of "void run_flash_fwd<Kernel_traits,Is_dropout,Is_causal>(Flash_fwd_params &, cudaStream_t) [with Kernel_traits=Flash_fwd_kernel_traits<160, 128, 32, 4, false, false, cutlass::bfloat16_t, Flash_kernel_traits<160, 128, 32, 4, cutlass::bfloat16_t>>, Is_dropout=true, Is_causal=false]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(155): here
            instantiation of "void run_mha_fwd_hdim160<T>(Flash_fwd_params &, cudaStream_t) [with T=cutlass::bfloat16_t]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_hdim160_bf16_sm80.cu(16): here

/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_kernel.h(276): warning: variable "tQrQ" was declared but never referenced
          detected during:
            instantiation of "void flash::compute_attn_1rowblock<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params,Is_Q_split_K>(const Params &, int, int, int) [with Kernel_traits=Flash_fwd_kernel_traits<160, 128, 32, 4, false, false, cutlass::bfloat16_t, Flash_kernel_traits<160, 128, 32, 4, cutlass::bfloat16_t>>, Is_dropout=true, Is_causal=false, Is_even_N=true, Is_even_K=true, Return_softmax=false, Params=Flash_fwd_params, Is_Q_split_K=false]" 
(589): here
            instantiation of "void flash::compute_attn<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &) [with Kernel_traits=Flash_fwd_kernel_traits<160, 128, 32, 4, false, false, cutlass::bfloat16_t, Flash_kernel_traits<160, 128, 32, 4, cutlass::bfloat16_t>>, Is_dropout=true, Is_causal=false, Is_even_N=true, Is_even_K=true, Return_softmax=false, Params=Flash_fwd_params]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(15): here
            instantiation of "void flash_fwd_kernel<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax>(Flash_fwd_params) [with Kernel_traits=Flash_fwd_kernel_traits<160, 128, 32, 4, false, false, cutlass::bfloat16_t, Flash_kernel_traits<160, 128, 32, 4, cutlass::bfloat16_t>>, Is_dropout=true, Is_causal=false, Is_even_N=true, Is_even_K=true, Return_softmax=false]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(34): here
            instantiation of "void run_flash_fwd<Kernel_traits,Is_dropout,Is_causal>(Flash_fwd_params &, cudaStream_t) [with Kernel_traits=Flash_fwd_kernel_traits<160, 128, 32, 4, false, false, cutlass::bfloat16_t, Flash_kernel_traits<160, 128, 32, 4, cutlass::bfloat16_t>>, Is_dropout=true, Is_causal=false]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(155): here
            instantiation of "void run_mha_fwd_hdim160<T>(Flash_fwd_params &, cudaStream_t) [with T=cutlass::bfloat16_t]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_hdim160_bf16_sm80.cu(16): here

/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_kernel.h(140): warning: variable "MMA_M" was declared but never referenced
          detected during:
            instantiation of "void flash::compute_attn_1rowblock<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params,Is_Q_split_K>(const Params &, int, int, int) [with Kernel_traits=Flash_fwd_kernel_traits<160, 128, 32, 4, false, false, cutlass::bfloat16_t, Flash_kernel_traits<160, 128, 32, 4, cutlass::bfloat16_t>>, Is_dropout=true, Is_causal=false, Is_even_N=true, Is_even_K=false, Return_softmax=true, Params=Flash_fwd_params, Is_Q_split_K=false]" 
(589): here
            instantiation of "void flash::compute_attn<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &) [with Kernel_traits=Flash_fwd_kernel_traits<160, 128, 32, 4, false, false, cutlass::bfloat16_t, Flash_kernel_traits<160, 128, 32, 4, cutlass::bfloat16_t>>, Is_dropout=true, Is_causal=false, Is_even_N=true, Is_even_K=false, Return_softmax=true, Params=Flash_fwd_params]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(15): here
            instantiation of "void flash_fwd_kernel<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax>(Flash_fwd_params) [with Kernel_traits=Flash_fwd_kernel_traits<160, 128, 32, 4, false, false, cutlass::bfloat16_t, Flash_kernel_traits<160, 128, 32, 4, cutlass::bfloat16_t>>, Is_dropout=true, Is_causal=false, Is_even_N=true, Is_even_K=false, Return_softmax=true]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(34): here
            instantiation of "void run_flash_fwd<Kernel_traits,Is_dropout,Is_causal>(Flash_fwd_params &, cudaStream_t) [with Kernel_traits=Flash_fwd_kernel_traits<160, 128, 32, 4, false, false, cutlass::bfloat16_t, Flash_kernel_traits<160, 128, 32, 4, cutlass::bfloat16_t>>, Is_dropout=true, Is_causal=false]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(155): here
            instantiation of "void run_mha_fwd_hdim160<T>(Flash_fwd_params &, cudaStream_t) [with T=cutlass::bfloat16_t]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_hdim160_bf16_sm80.cu(16): here

/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_kernel.h(276): warning: variable "tQrQ" was declared but never referenced
          detected during:
            instantiation of "void flash::compute_attn_1rowblock<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params,Is_Q_split_K>(const Params &, int, int, int) [with Kernel_traits=Flash_fwd_kernel_traits<160, 128, 32, 4, false, false, cutlass::bfloat16_t, Flash_kernel_traits<160, 128, 32, 4, cutlass::bfloat16_t>>, Is_dropout=true, Is_causal=false, Is_even_N=true, Is_even_K=false, Return_softmax=true, Params=Flash_fwd_params, Is_Q_split_K=false]" 
(589): here
            instantiation of "void flash::compute_attn<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &) [with Kernel_traits=Flash_fwd_kernel_traits<160, 128, 32, 4, false, false, cutlass::bfloat16_t, Flash_kernel_traits<160, 128, 32, 4, cutlass::bfloat16_t>>, Is_dropout=true, Is_causal=false, Is_even_N=true, Is_even_K=false, Return_softmax=true, Params=Flash_fwd_params]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(15): here
            instantiation of "void flash_fwd_kernel<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax>(Flash_fwd_params) [with Kernel_traits=Flash_fwd_kernel_traits<160, 128, 32, 4, false, false, cutlass::bfloat16_t, Flash_kernel_traits<160, 128, 32, 4, cutlass::bfloat16_t>>, Is_dropout=true, Is_causal=false, Is_even_N=true, Is_even_K=false, Return_softmax=true]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(34): here
            instantiation of "void run_flash_fwd<Kernel_traits,Is_dropout,Is_causal>(Flash_fwd_params &, cudaStream_t) [with Kernel_traits=Flash_fwd_kernel_traits<160, 128, 32, 4, false, false, cutlass::bfloat16_t, Flash_kernel_traits<160, 128, 32, 4, cutlass::bfloat16_t>>, Is_dropout=true, Is_causal=false]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(155): here
            instantiation of "void run_mha_fwd_hdim160<T>(Flash_fwd_params &, cudaStream_t) [with T=cutlass::bfloat16_t]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_hdim160_bf16_sm80.cu(16): here

/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_kernel.h(140): warning: variable "MMA_M" was declared but never referenced
          detected during:
            instantiation of "void flash::compute_attn_1rowblock<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params,Is_Q_split_K>(const Params &, int, int, int) [with Kernel_traits=Flash_fwd_kernel_traits<160, 128, 32, 4, false, false, cutlass::bfloat16_t, Flash_kernel_traits<160, 128, 32, 4, cutlass::bfloat16_t>>, Is_dropout=true, Is_causal=false, Is_even_N=true, Is_even_K=false, Return_softmax=false, Params=Flash_fwd_params, Is_Q_split_K=false]" 
(589): here
            instantiation of "void flash::compute_attn<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &) [with Kernel_traits=Flash_fwd_kernel_traits<160, 128, 32, 4, false, false, cutlass::bfloat16_t, Flash_kernel_traits<160, 128, 32, 4, cutlass::bfloat16_t>>, Is_dropout=true, Is_causal=false, Is_even_N=true, Is_even_K=false, Return_softmax=false, Params=Flash_fwd_params]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(15): here
            instantiation of "void flash_fwd_kernel<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax>(Flash_fwd_params) [with Kernel_traits=Flash_fwd_kernel_traits<160, 128, 32, 4, false, false, cutlass::bfloat16_t, Flash_kernel_traits<160, 128, 32, 4, cutlass::bfloat16_t>>, Is_dropout=true, Is_causal=false, Is_even_N=true, Is_even_K=false, Return_softmax=false]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(34): here
            instantiation of "void run_flash_fwd<Kernel_traits,Is_dropout,Is_causal>(Flash_fwd_params &, cudaStream_t) [with Kernel_traits=Flash_fwd_kernel_traits<160, 128, 32, 4, false, false, cutlass::bfloat16_t, Flash_kernel_traits<160, 128, 32, 4, cutlass::bfloat16_t>>, Is_dropout=true, Is_causal=false]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(155): here
            instantiation of "void run_mha_fwd_hdim160<T>(Flash_fwd_params &, cudaStream_t) [with T=cutlass::bfloat16_t]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_hdim160_bf16_sm80.cu(16): here

/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_kernel.h(276): warning: variable "tQrQ" was declared but never referenced
          detected during:
            instantiation of "void flash::compute_attn_1rowblock<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params,Is_Q_split_K>(const Params &, int, int, int) [with Kernel_traits=Flash_fwd_kernel_traits<160, 128, 32, 4, false, false, cutlass::bfloat16_t, Flash_kernel_traits<160, 128, 32, 4, cutlass::bfloat16_t>>, Is_dropout=true, Is_causal=false, Is_even_N=true, Is_even_K=false, Return_softmax=false, Params=Flash_fwd_params, Is_Q_split_K=false]" 
(589): here
            instantiation of "void flash::compute_attn<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &) [with Kernel_traits=Flash_fwd_kernel_traits<160, 128, 32, 4, false, false, cutlass::bfloat16_t, Flash_kernel_traits<160, 128, 32, 4, cutlass::bfloat16_t>>, Is_dropout=true, Is_causal=false, Is_even_N=true, Is_even_K=false, Return_softmax=false, Params=Flash_fwd_params]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(15): here
            instantiation of "void flash_fwd_kernel<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax>(Flash_fwd_params) [with Kernel_traits=Flash_fwd_kernel_traits<160, 128, 32, 4, false, false, cutlass::bfloat16_t, Flash_kernel_traits<160, 128, 32, 4, cutlass::bfloat16_t>>, Is_dropout=true, Is_causal=false, Is_even_N=true, Is_even_K=false, Return_softmax=false]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(34): here
            instantiation of "void run_flash_fwd<Kernel_traits,Is_dropout,Is_causal>(Flash_fwd_params &, cudaStream_t) [with Kernel_traits=Flash_fwd_kernel_traits<160, 128, 32, 4, false, false, cutlass::bfloat16_t, Flash_kernel_traits<160, 128, 32, 4, cutlass::bfloat16_t>>, Is_dropout=true, Is_causal=false]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(155): here
            instantiation of "void run_mha_fwd_hdim160<T>(Flash_fwd_params &, cudaStream_t) [with T=cutlass::bfloat16_t]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_hdim160_bf16_sm80.cu(16): here

/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_kernel.h(140): warning: variable "MMA_M" was declared but never referenced
          detected during:
            instantiation of "void flash::compute_attn_1rowblock<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params,Is_Q_split_K>(const Params &, int, int, int) [with Kernel_traits=Flash_fwd_kernel_traits<160, 128, 32, 4, false, false, cutlass::bfloat16_t, Flash_kernel_traits<160, 128, 32, 4, cutlass::bfloat16_t>>, Is_dropout=true, Is_causal=false, Is_even_N=false, Is_even_K=true, Return_softmax=true, Params=Flash_fwd_params, Is_Q_split_K=false]" 
(589): here
            instantiation of "void flash::compute_attn<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &) [with Kernel_traits=Flash_fwd_kernel_traits<160, 128, 32, 4, false, false, cutlass::bfloat16_t, Flash_kernel_traits<160, 128, 32, 4, cutlass::bfloat16_t>>, Is_dropout=true, Is_causal=false, Is_even_N=false, Is_even_K=true, Return_softmax=true, Params=Flash_fwd_params]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(15): here
            instantiation of "void flash_fwd_kernel<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax>(Flash_fwd_params) [with Kernel_traits=Flash_fwd_kernel_traits<160, 128, 32, 4, false, false, cutlass::bfloat16_t, Flash_kernel_traits<160, 128, 32, 4, cutlass::bfloat16_t>>, Is_dropout=true, Is_causal=false, Is_even_N=false, Is_even_K=true, Return_softmax=true]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(34): here
            instantiation of "void run_flash_fwd<Kernel_traits,Is_dropout,Is_causal>(Flash_fwd_params &, cudaStream_t) [with Kernel_traits=Flash_fwd_kernel_traits<160, 128, 32, 4, false, false, cutlass::bfloat16_t, Flash_kernel_traits<160, 128, 32, 4, cutlass::bfloat16_t>>, Is_dropout=true, Is_causal=false]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(155): here
            instantiation of "void run_mha_fwd_hdim160<T>(Flash_fwd_params &, cudaStream_t) [with T=cutlass::bfloat16_t]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_hdim160_bf16_sm80.cu(16): here

/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_kernel.h(276): warning: variable "tQrQ" was declared but never referenced
          detected during:
            instantiation of "void flash::compute_attn_1rowblock<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params,Is_Q_split_K>(const Params &, int, int, int) [with Kernel_traits=Flash_fwd_kernel_traits<160, 128, 32, 4, false, false, cutlass::bfloat16_t, Flash_kernel_traits<160, 128, 32, 4, cutlass::bfloat16_t>>, Is_dropout=true, Is_causal=false, Is_even_N=false, Is_even_K=true, Return_softmax=true, Params=Flash_fwd_params, Is_Q_split_K=false]" 
(589): here
            instantiation of "void flash::compute_attn<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &) [with Kernel_traits=Flash_fwd_kernel_traits<160, 128, 32, 4, false, false, cutlass::bfloat16_t, Flash_kernel_traits<160, 128, 32, 4, cutlass::bfloat16_t>>, Is_dropout=true, Is_causal=false, Is_even_N=false, Is_even_K=true, Return_softmax=true, Params=Flash_fwd_params]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(15): here
            instantiation of "void flash_fwd_kernel<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax>(Flash_fwd_params) [with Kernel_traits=Flash_fwd_kernel_traits<160, 128, 32, 4, false, false, cutlass::bfloat16_t, Flash_kernel_traits<160, 128, 32, 4, cutlass::bfloat16_t>>, Is_dropout=true, Is_causal=false, Is_even_N=false, Is_even_K=true, Return_softmax=true]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(34): here
            instantiation of "void run_flash_fwd<Kernel_traits,Is_dropout,Is_causal>(Flash_fwd_params &, cudaStream_t) [with Kernel_traits=Flash_fwd_kernel_traits<160, 128, 32, 4, false, false, cutlass::bfloat16_t, Flash_kernel_traits<160, 128, 32, 4, cutlass::bfloat16_t>>, Is_dropout=true, Is_causal=false]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(155): here
            instantiation of "void run_mha_fwd_hdim160<T>(Flash_fwd_params &, cudaStream_t) [with T=cutlass::bfloat16_t]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_hdim160_bf16_sm80.cu(16): here

/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_kernel.h(140): warning: variable "MMA_M" was declared but never referenced
          detected during:
            instantiation of "void flash::compute_attn_1rowblock<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params,Is_Q_split_K>(const Params &, int, int, int) [with Kernel_traits=Flash_fwd_kernel_traits<160, 128, 32, 4, false, false, cutlass::bfloat16_t, Flash_kernel_traits<160, 128, 32, 4, cutlass::bfloat16_t>>, Is_dropout=true, Is_causal=false, Is_even_N=false, Is_even_K=true, Return_softmax=false, Params=Flash_fwd_params, Is_Q_split_K=false]" 
(589): here
            instantiation of "void flash::compute_attn<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &) [with Kernel_traits=Flash_fwd_kernel_traits<160, 128, 32, 4, false, false, cutlass::bfloat16_t, Flash_kernel_traits<160, 128, 32, 4, cutlass::bfloat16_t>>, Is_dropout=true, Is_causal=false, Is_even_N=false, Is_even_K=true, Return_softmax=false, Params=Flash_fwd_params]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(15): here
            instantiation of "void flash_fwd_kernel<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax>(Flash_fwd_params) [with Kernel_traits=Flash_fwd_kernel_traits<160, 128, 32, 4, false, false, cutlass::bfloat16_t, Flash_kernel_traits<160, 128, 32, 4, cutlass::bfloat16_t>>, Is_dropout=true, Is_causal=false, Is_even_N=false, Is_even_K=true, Return_softmax=false]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(34): here
            instantiation of "void run_flash_fwd<Kernel_traits,Is_dropout,Is_causal>(Flash_fwd_params &, cudaStream_t) [with Kernel_traits=Flash_fwd_kernel_traits<160, 128, 32, 4, false, false, cutlass::bfloat16_t, Flash_kernel_traits<160, 128, 32, 4, cutlass::bfloat16_t>>, Is_dropout=true, Is_causal=false]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(155): here
            instantiation of "void run_mha_fwd_hdim160<T>(Flash_fwd_params &, cudaStream_t) [with T=cutlass::bfloat16_t]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_hdim160_bf16_sm80.cu(16): here

/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_kernel.h(276): warning: variable "tQrQ" was declared but never referenced
          detected during:
            instantiation of "void flash::compute_attn_1rowblock<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params,Is_Q_split_K>(const Params &, int, int, int) [with Kernel_traits=Flash_fwd_kernel_traits<160, 128, 32, 4, false, false, cutlass::bfloat16_t, Flash_kernel_traits<160, 128, 32, 4, cutlass::bfloat16_t>>, Is_dropout=true, Is_causal=false, Is_even_N=false, Is_even_K=true, Return_softmax=false, Params=Flash_fwd_params, Is_Q_split_K=false]" 
(589): here
            instantiation of "void flash::compute_attn<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &) [with Kernel_traits=Flash_fwd_kernel_traits<160, 128, 32, 4, false, false, cutlass::bfloat16_t, Flash_kernel_traits<160, 128, 32, 4, cutlass::bfloat16_t>>, Is_dropout=true, Is_causal=false, Is_even_N=false, Is_even_K=true, Return_softmax=false, Params=Flash_fwd_params]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(15): here
            instantiation of "void flash_fwd_kernel<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax>(Flash_fwd_params) [with Kernel_traits=Flash_fwd_kernel_traits<160, 128, 32, 4, false, false, cutlass::bfloat16_t, Flash_kernel_traits<160, 128, 32, 4, cutlass::bfloat16_t>>, Is_dropout=true, Is_causal=false, Is_even_N=false, Is_even_K=true, Return_softmax=false]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(34): here
            instantiation of "void run_flash_fwd<Kernel_traits,Is_dropout,Is_causal>(Flash_fwd_params &, cudaStream_t) [with Kernel_traits=Flash_fwd_kernel_traits<160, 128, 32, 4, false, false, cutlass::bfloat16_t, Flash_kernel_traits<160, 128, 32, 4, cutlass::bfloat16_t>>, Is_dropout=true, Is_causal=false]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(155): here
            instantiation of "void run_mha_fwd_hdim160<T>(Flash_fwd_params &, cudaStream_t) [with T=cutlass::bfloat16_t]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_hdim160_bf16_sm80.cu(16): here

/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_kernel.h(140): warning: variable "MMA_M" was declared but never referenced
          detected during:
            instantiation of "void flash::compute_attn_1rowblock<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params,Is_Q_split_K>(const Params &, int, int, int) [with Kernel_traits=Flash_fwd_kernel_traits<160, 128, 32, 4, false, false, cutlass::bfloat16_t, Flash_kernel_traits<160, 128, 32, 4, cutlass::bfloat16_t>>, Is_dropout=true, Is_causal=false, Is_even_N=false, Is_even_K=false, Return_softmax=true, Params=Flash_fwd_params, Is_Q_split_K=false]" 
(589): here
            instantiation of "void flash::compute_attn<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &) [with Kernel_traits=Flash_fwd_kernel_traits<160, 128, 32, 4, false, false, cutlass::bfloat16_t, Flash_kernel_traits<160, 128, 32, 4, cutlass::bfloat16_t>>, Is_dropout=true, Is_causal=false, Is_even_N=false, Is_even_K=false, Return_softmax=true, Params=Flash_fwd_params]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(15): here
            instantiation of "void flash_fwd_kernel<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax>(Flash_fwd_params) [with Kernel_traits=Flash_fwd_kernel_traits<160, 128, 32, 4, false, false, cutlass::bfloat16_t, Flash_kernel_traits<160, 128, 32, 4, cutlass::bfloat16_t>>, Is_dropout=true, Is_causal=false, Is_even_N=false, Is_even_K=false, Return_softmax=true]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(34): here
            instantiation of "void run_flash_fwd<Kernel_traits,Is_dropout,Is_causal>(Flash_fwd_params &, cudaStream_t) [with Kernel_traits=Flash_fwd_kernel_traits<160, 128, 32, 4, false, false, cutlass::bfloat16_t, Flash_kernel_traits<160, 128, 32, 4, cutlass::bfloat16_t>>, Is_dropout=true, Is_causal=false]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(155): here
            instantiation of "void run_mha_fwd_hdim160<T>(Flash_fwd_params &, cudaStream_t) [with T=cutlass::bfloat16_t]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_hdim160_bf16_sm80.cu(16): here

/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_kernel.h(276): warning: variable "tQrQ" was declared but never referenced
          detected during:
            instantiation of "void flash::compute_attn_1rowblock<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params,Is_Q_split_K>(const Params &, int, int, int) [with Kernel_traits=Flash_fwd_kernel_traits<160, 128, 32, 4, false, false, cutlass::bfloat16_t, Flash_kernel_traits<160, 128, 32, 4, cutlass::bfloat16_t>>, Is_dropout=true, Is_causal=false, Is_even_N=false, Is_even_K=false, Return_softmax=true, Params=Flash_fwd_params, Is_Q_split_K=false]" 
(589): here
            instantiation of "void flash::compute_attn<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &) [with Kernel_traits=Flash_fwd_kernel_traits<160, 128, 32, 4, false, false, cutlass::bfloat16_t, Flash_kernel_traits<160, 128, 32, 4, cutlass::bfloat16_t>>, Is_dropout=true, Is_causal=false, Is_even_N=false, Is_even_K=false, Return_softmax=true, Params=Flash_fwd_params]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(15): here
            instantiation of "void flash_fwd_kernel<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax>(Flash_fwd_params) [with Kernel_traits=Flash_fwd_kernel_traits<160, 128, 32, 4, false, false, cutlass::bfloat16_t, Flash_kernel_traits<160, 128, 32, 4, cutlass::bfloat16_t>>, Is_dropout=true, Is_causal=false, Is_even_N=false, Is_even_K=false, Return_softmax=true]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(34): here
            instantiation of "void run_flash_fwd<Kernel_traits,Is_dropout,Is_causal>(Flash_fwd_params &, cudaStream_t) [with Kernel_traits=Flash_fwd_kernel_traits<160, 128, 32, 4, false, false, cutlass::bfloat16_t, Flash_kernel_traits<160, 128, 32, 4, cutlass::bfloat16_t>>, Is_dropout=true, Is_causal=false]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(155): here
            instantiation of "void run_mha_fwd_hdim160<T>(Flash_fwd_params &, cudaStream_t) [with T=cutlass::bfloat16_t]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_hdim160_bf16_sm80.cu(16): here

/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_kernel.h(140): warning: variable "MMA_M" was declared but never referenced
          detected during:
            instantiation of "void flash::compute_attn_1rowblock<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params,Is_Q_split_K>(const Params &, int, int, int) [with Kernel_traits=Flash_fwd_kernel_traits<160, 128, 32, 4, false, false, cutlass::bfloat16_t, Flash_kernel_traits<160, 128, 32, 4, cutlass::bfloat16_t>>, Is_dropout=true, Is_causal=false, Is_even_N=false, Is_even_K=false, Return_softmax=false, Params=Flash_fwd_params, Is_Q_split_K=false]" 
(589): here
            instantiation of "void flash::compute_attn<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &) [with Kernel_traits=Flash_fwd_kernel_traits<160, 128, 32, 4, false, false, cutlass::bfloat16_t, Flash_kernel_traits<160, 128, 32, 4, cutlass::bfloat16_t>>, Is_dropout=true, Is_causal=false, Is_even_N=false, Is_even_K=false, Return_softmax=false, Params=Flash_fwd_params]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(15): here
            instantiation of "void flash_fwd_kernel<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax>(Flash_fwd_params) [with Kernel_traits=Flash_fwd_kernel_traits<160, 128, 32, 4, false, false, cutlass::bfloat16_t, Flash_kernel_traits<160, 128, 32, 4, cutlass::bfloat16_t>>, Is_dropout=true, Is_causal=false, Is_even_N=false, Is_even_K=false, Return_softmax=false]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(34): here
            instantiation of "void run_flash_fwd<Kernel_traits,Is_dropout,Is_causal>(Flash_fwd_params &, cudaStream_t) [with Kernel_traits=Flash_fwd_kernel_traits<160, 128, 32, 4, false, false, cutlass::bfloat16_t, Flash_kernel_traits<160, 128, 32, 4, cutlass::bfloat16_t>>, Is_dropout=true, Is_causal=false]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(155): here
            instantiation of "void run_mha_fwd_hdim160<T>(Flash_fwd_params &, cudaStream_t) [with T=cutlass::bfloat16_t]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_hdim160_bf16_sm80.cu(16): here

/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_kernel.h(276): warning: variable "tQrQ" was declared but never referenced
          detected during:
            instantiation of "void flash::compute_attn_1rowblock<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params,Is_Q_split_K>(const Params &, int, int, int) [with Kernel_traits=Flash_fwd_kernel_traits<160, 128, 32, 4, false, false, cutlass::bfloat16_t, Flash_kernel_traits<160, 128, 32, 4, cutlass::bfloat16_t>>, Is_dropout=true, Is_causal=false, Is_even_N=false, Is_even_K=false, Return_softmax=false, Params=Flash_fwd_params, Is_Q_split_K=false]" 
(589): here
            instantiation of "void flash::compute_attn<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &) [with Kernel_traits=Flash_fwd_kernel_traits<160, 128, 32, 4, false, false, cutlass::bfloat16_t, Flash_kernel_traits<160, 128, 32, 4, cutlass::bfloat16_t>>, Is_dropout=true, Is_causal=false, Is_even_N=false, Is_even_K=false, Return_softmax=false, Params=Flash_fwd_params]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(15): here
            instantiation of "void flash_fwd_kernel<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax>(Flash_fwd_params) [with Kernel_traits=Flash_fwd_kernel_traits<160, 128, 32, 4, false, false, cutlass::bfloat16_t, Flash_kernel_traits<160, 128, 32, 4, cutlass::bfloat16_t>>, Is_dropout=true, Is_causal=false, Is_even_N=false, Is_even_K=false, Return_softmax=false]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(34): here
            instantiation of "void run_flash_fwd<Kernel_traits,Is_dropout,Is_causal>(Flash_fwd_params &, cudaStream_t) [with Kernel_traits=Flash_fwd_kernel_traits<160, 128, 32, 4, false, false, cutlass::bfloat16_t, Flash_kernel_traits<160, 128, 32, 4, cutlass::bfloat16_t>>, Is_dropout=true, Is_causal=false]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(155): here
            instantiation of "void run_mha_fwd_hdim160<T>(Flash_fwd_params &, cudaStream_t) [with T=cutlass::bfloat16_t]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_hdim160_bf16_sm80.cu(16): here

/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_kernel.h(140): warning: variable "MMA_M" was declared but never referenced
          detected during:
            instantiation of "void flash::compute_attn_1rowblock<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params,Is_Q_split_K>(const Params &, int, int, int) [with Kernel_traits=Flash_fwd_kernel_traits<160, 64, 64, 4, false, false, cutlass::bfloat16_t, Flash_kernel_traits<160, 64, 64, 4, cutlass::bfloat16_t>>, Is_dropout=false, Is_causal=true, Is_even_N=true, Is_even_K=true, Return_softmax=false, Params=Flash_fwd_params, Is_Q_split_K=false]" 
(589): here
            instantiation of "void flash::compute_attn<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &) [with Kernel_traits=Flash_fwd_kernel_traits<160, 64, 64, 4, false, false, cutlass::bfloat16_t, Flash_kernel_traits<160, 64, 64, 4, cutlass::bfloat16_t>>, Is_dropout=false, Is_causal=true, Is_even_N=true, Is_even_K=true, Return_softmax=false, Params=Flash_fwd_params]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(15): here
            instantiation of "void flash_fwd_kernel<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax>(Flash_fwd_params) [with Kernel_traits=Flash_fwd_kernel_traits<160, 64, 64, 4, false, false, cutlass::bfloat16_t, Flash_kernel_traits<160, 64, 64, 4, cutlass::bfloat16_t>>, Is_dropout=false, Is_causal=true, Is_even_N=true, Is_even_K=true, Return_softmax=false]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(34): here
            instantiation of "void run_flash_fwd<Kernel_traits,Is_dropout,Is_causal>(Flash_fwd_params &, cudaStream_t) [with Kernel_traits=Flash_fwd_kernel_traits<160, 64, 64, 4, false, false, cutlass::bfloat16_t, Flash_kernel_traits<160, 64, 64, 4, cutlass::bfloat16_t>>, Is_dropout=false, Is_causal=true]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(155): here
            instantiation of "void run_mha_fwd_hdim160<T>(Flash_fwd_params &, cudaStream_t) [with T=cutlass::bfloat16_t]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_hdim160_bf16_sm80.cu(16): here

/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_kernel.h(276): warning: variable "tQrQ" was declared but never referenced
          detected during:
            instantiation of "void flash::compute_attn_1rowblock<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params,Is_Q_split_K>(const Params &, int, int, int) [with Kernel_traits=Flash_fwd_kernel_traits<160, 64, 64, 4, false, false, cutlass::bfloat16_t, Flash_kernel_traits<160, 64, 64, 4, cutlass::bfloat16_t>>, Is_dropout=false, Is_causal=true, Is_even_N=true, Is_even_K=true, Return_softmax=false, Params=Flash_fwd_params, Is_Q_split_K=false]" 
(589): here
            instantiation of "void flash::compute_attn<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &) [with Kernel_traits=Flash_fwd_kernel_traits<160, 64, 64, 4, false, false, cutlass::bfloat16_t, Flash_kernel_traits<160, 64, 64, 4, cutlass::bfloat16_t>>, Is_dropout=false, Is_causal=true, Is_even_N=true, Is_even_K=true, Return_softmax=false, Params=Flash_fwd_params]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(15): here
            instantiation of "void flash_fwd_kernel<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax>(Flash_fwd_params) [with Kernel_traits=Flash_fwd_kernel_traits<160, 64, 64, 4, false, false, cutlass::bfloat16_t, Flash_kernel_traits<160, 64, 64, 4, cutlass::bfloat16_t>>, Is_dropout=false, Is_causal=true, Is_even_N=true, Is_even_K=true, Return_softmax=false]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(34): here
            instantiation of "void run_flash_fwd<Kernel_traits,Is_dropout,Is_causal>(Flash_fwd_params &, cudaStream_t) [with Kernel_traits=Flash_fwd_kernel_traits<160, 64, 64, 4, false, false, cutlass::bfloat16_t, Flash_kernel_traits<160, 64, 64, 4, cutlass::bfloat16_t>>, Is_dropout=false, Is_causal=true]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(155): here
            instantiation of "void run_mha_fwd_hdim160<T>(Flash_fwd_params &, cudaStream_t) [with T=cutlass::bfloat16_t]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_hdim160_bf16_sm80.cu(16): here

/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_kernel.h(140): warning: variable "MMA_M" was declared but never referenced
          detected during:
            instantiation of "void flash::compute_attn_1rowblock<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params,Is_Q_split_K>(const Params &, int, int, int) [with Kernel_traits=Flash_fwd_kernel_traits<160, 64, 64, 4, false, false, cutlass::bfloat16_t, Flash_kernel_traits<160, 64, 64, 4, cutlass::bfloat16_t>>, Is_dropout=false, Is_causal=true, Is_even_N=true, Is_even_K=false, Return_softmax=false, Params=Flash_fwd_params, Is_Q_split_K=false]" 
(589): here
            instantiation of "void flash::compute_attn<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &) [with Kernel_traits=Flash_fwd_kernel_traits<160, 64, 64, 4, false, false, cutlass::bfloat16_t, Flash_kernel_traits<160, 64, 64, 4, cutlass::bfloat16_t>>, Is_dropout=false, Is_causal=true, Is_even_N=true, Is_even_K=false, Return_softmax=false, Params=Flash_fwd_params]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(15): here
            instantiation of "void flash_fwd_kernel<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax>(Flash_fwd_params) [with Kernel_traits=Flash_fwd_kernel_traits<160, 64, 64, 4, false, false, cutlass::bfloat16_t, Flash_kernel_traits<160, 64, 64, 4, cutlass::bfloat16_t>>, Is_dropout=false, Is_causal=true, Is_even_N=true, Is_even_K=false, Return_softmax=false]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(34): here
            instantiation of "void run_flash_fwd<Kernel_traits,Is_dropout,Is_causal>(Flash_fwd_params &, cudaStream_t) [with Kernel_traits=Flash_fwd_kernel_traits<160, 64, 64, 4, false, false, cutlass::bfloat16_t, Flash_kernel_traits<160, 64, 64, 4, cutlass::bfloat16_t>>, Is_dropout=false, Is_causal=true]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(155): here
            instantiation of "void run_mha_fwd_hdim160<T>(Flash_fwd_params &, cudaStream_t) [with T=cutlass::bfloat16_t]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_hdim160_bf16_sm80.cu(16): here

/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_kernel.h(276): warning: variable "tQrQ" was declared but never referenced
          detected during:
            instantiation of "void flash::compute_attn_1rowblock<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params,Is_Q_split_K>(const Params &, int, int, int) [with Kernel_traits=Flash_fwd_kernel_traits<160, 64, 64, 4, false, false, cutlass::bfloat16_t, Flash_kernel_traits<160, 64, 64, 4, cutlass::bfloat16_t>>, Is_dropout=false, Is_causal=true, Is_even_N=true, Is_even_K=false, Return_softmax=false, Params=Flash_fwd_params, Is_Q_split_K=false]" 
(589): here
            instantiation of "void flash::compute_attn<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &) [with Kernel_traits=Flash_fwd_kernel_traits<160, 64, 64, 4, false, false, cutlass::bfloat16_t, Flash_kernel_traits<160, 64, 64, 4, cutlass::bfloat16_t>>, Is_dropout=false, Is_causal=true, Is_even_N=true, Is_even_K=false, Return_softmax=false, Params=Flash_fwd_params]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(15): here
            instantiation of "void flash_fwd_kernel<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax>(Flash_fwd_params) [with Kernel_traits=Flash_fwd_kernel_traits<160, 64, 64, 4, false, false, cutlass::bfloat16_t, Flash_kernel_traits<160, 64, 64, 4, cutlass::bfloat16_t>>, Is_dropout=false, Is_causal=true, Is_even_N=true, Is_even_K=false, Return_softmax=false]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(34): here
            instantiation of "void run_flash_fwd<Kernel_traits,Is_dropout,Is_causal>(Flash_fwd_params &, cudaStream_t) [with Kernel_traits=Flash_fwd_kernel_traits<160, 64, 64, 4, false, false, cutlass::bfloat16_t, Flash_kernel_traits<160, 64, 64, 4, cutlass::bfloat16_t>>, Is_dropout=false, Is_causal=true]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(155): here
            instantiation of "void run_mha_fwd_hdim160<T>(Flash_fwd_params &, cudaStream_t) [with T=cutlass::bfloat16_t]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_hdim160_bf16_sm80.cu(16): here

/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_kernel.h(140): warning: variable "MMA_M" was declared but never referenced
          detected during:
            instantiation of "void flash::compute_attn_1rowblock<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params,Is_Q_split_K>(const Params &, int, int, int) [with Kernel_traits=Flash_fwd_kernel_traits<160, 64, 64, 4, false, false, cutlass::bfloat16_t, Flash_kernel_traits<160, 64, 64, 4, cutlass::bfloat16_t>>, Is_dropout=false, Is_causal=true, Is_even_N=false, Is_even_K=true, Return_softmax=false, Params=Flash_fwd_params, Is_Q_split_K=false]" 
(589): here
            instantiation of "void flash::compute_attn<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &) [with Kernel_traits=Flash_fwd_kernel_traits<160, 64, 64, 4, false, false, cutlass::bfloat16_t, Flash_kernel_traits<160, 64, 64, 4, cutlass::bfloat16_t>>, Is_dropout=false, Is_causal=true, Is_even_N=false, Is_even_K=true, Return_softmax=false, Params=Flash_fwd_params]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(15): here
            instantiation of "void flash_fwd_kernel<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax>(Flash_fwd_params) [with Kernel_traits=Flash_fwd_kernel_traits<160, 64, 64, 4, false, false, cutlass::bfloat16_t, Flash_kernel_traits<160, 64, 64, 4, cutlass::bfloat16_t>>, Is_dropout=false, Is_causal=true, Is_even_N=false, Is_even_K=true, Return_softmax=false]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(34): here
            instantiation of "void run_flash_fwd<Kernel_traits,Is_dropout,Is_causal>(Flash_fwd_params &, cudaStream_t) [with Kernel_traits=Flash_fwd_kernel_traits<160, 64, 64, 4, false, false, cutlass::bfloat16_t, Flash_kernel_traits<160, 64, 64, 4, cutlass::bfloat16_t>>, Is_dropout=false, Is_causal=true]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(155): here
            instantiation of "void run_mha_fwd_hdim160<T>(Flash_fwd_params &, cudaStream_t) [with T=cutlass::bfloat16_t]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_hdim160_bf16_sm80.cu(16): here

/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_kernel.h(276): warning: variable "tQrQ" was declared but never referenced
          detected during:
            instantiation of "void flash::compute_attn_1rowblock<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params,Is_Q_split_K>(const Params &, int, int, int) [with Kernel_traits=Flash_fwd_kernel_traits<160, 64, 64, 4, false, false, cutlass::bfloat16_t, Flash_kernel_traits<160, 64, 64, 4, cutlass::bfloat16_t>>, Is_dropout=false, Is_causal=true, Is_even_N=false, Is_even_K=true, Return_softmax=false, Params=Flash_fwd_params, Is_Q_split_K=false]" 
(589): here
            instantiation of "void flash::compute_attn<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &) [with Kernel_traits=Flash_fwd_kernel_traits<160, 64, 64, 4, false, false, cutlass::bfloat16_t, Flash_kernel_traits<160, 64, 64, 4, cutlass::bfloat16_t>>, Is_dropout=false, Is_causal=true, Is_even_N=false, Is_even_K=true, Return_softmax=false, Params=Flash_fwd_params]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(15): here
            instantiation of "void flash_fwd_kernel<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax>(Flash_fwd_params) [with Kernel_traits=Flash_fwd_kernel_traits<160, 64, 64, 4, false, false, cutlass::bfloat16_t, Flash_kernel_traits<160, 64, 64, 4, cutlass::bfloat16_t>>, Is_dropout=false, Is_causal=true, Is_even_N=false, Is_even_K=true, Return_softmax=false]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(34): here
            instantiation of "void run_flash_fwd<Kernel_traits,Is_dropout,Is_causal>(Flash_fwd_params &, cudaStream_t) [with Kernel_traits=Flash_fwd_kernel_traits<160, 64, 64, 4, false, false, cutlass::bfloat16_t, Flash_kernel_traits<160, 64, 64, 4, cutlass::bfloat16_t>>, Is_dropout=false, Is_causal=true]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(155): here
            instantiation of "void run_mha_fwd_hdim160<T>(Flash_fwd_params &, cudaStream_t) [with T=cutlass::bfloat16_t]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_hdim160_bf16_sm80.cu(16): here

/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_kernel.h(140): warning: variable "MMA_M" was declared but never referenced
          detected during:
            instantiation of "void flash::compute_attn_1rowblock<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params,Is_Q_split_K>(const Params &, int, int, int) [with Kernel_traits=Flash_fwd_kernel_traits<160, 64, 64, 4, false, false, cutlass::bfloat16_t, Flash_kernel_traits<160, 64, 64, 4, cutlass::bfloat16_t>>, Is_dropout=false, Is_causal=true, Is_even_N=false, Is_even_K=false, Return_softmax=false, Params=Flash_fwd_params, Is_Q_split_K=false]" 
(589): here
            instantiation of "void flash::compute_attn<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &) [with Kernel_traits=Flash_fwd_kernel_traits<160, 64, 64, 4, false, false, cutlass::bfloat16_t, Flash_kernel_traits<160, 64, 64, 4, cutlass::bfloat16_t>>, Is_dropout=false, Is_causal=true, Is_even_N=false, Is_even_K=false, Return_softmax=false, Params=Flash_fwd_params]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(15): here
            instantiation of "void flash_fwd_kernel<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax>(Flash_fwd_params) [with Kernel_traits=Flash_fwd_kernel_traits<160, 64, 64, 4, false, false, cutlass::bfloat16_t, Flash_kernel_traits<160, 64, 64, 4, cutlass::bfloat16_t>>, Is_dropout=false, Is_causal=true, Is_even_N=false, Is_even_K=false, Return_softmax=false]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(34): here
            instantiation of "void run_flash_fwd<Kernel_traits,Is_dropout,Is_causal>(Flash_fwd_params &, cudaStream_t) [with Kernel_traits=Flash_fwd_kernel_traits<160, 64, 64, 4, false, false, cutlass::bfloat16_t, Flash_kernel_traits<160, 64, 64, 4, cutlass::bfloat16_t>>, Is_dropout=false, Is_causal=true]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(155): here
            instantiation of "void run_mha_fwd_hdim160<T>(Flash_fwd_params &, cudaStream_t) [with T=cutlass::bfloat16_t]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_hdim160_bf16_sm80.cu(16): here

/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_kernel.h(276): warning: variable "tQrQ" was declared but never referenced
          detected during:
            instantiation of "void flash::compute_attn_1rowblock<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params,Is_Q_split_K>(const Params &, int, int, int) [with Kernel_traits=Flash_fwd_kernel_traits<160, 64, 64, 4, false, false, cutlass::bfloat16_t, Flash_kernel_traits<160, 64, 64, 4, cutlass::bfloat16_t>>, Is_dropout=false, Is_causal=true, Is_even_N=false, Is_even_K=false, Return_softmax=false, Params=Flash_fwd_params, Is_Q_split_K=false]" 
(589): here
            instantiation of "void flash::compute_attn<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &) [with Kernel_traits=Flash_fwd_kernel_traits<160, 64, 64, 4, false, false, cutlass::bfloat16_t, Flash_kernel_traits<160, 64, 64, 4, cutlass::bfloat16_t>>, Is_dropout=false, Is_causal=true, Is_even_N=false, Is_even_K=false, Return_softmax=false, Params=Flash_fwd_params]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(15): here
            instantiation of "void flash_fwd_kernel<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax>(Flash_fwd_params) [with Kernel_traits=Flash_fwd_kernel_traits<160, 64, 64, 4, false, false, cutlass::bfloat16_t, Flash_kernel_traits<160, 64, 64, 4, cutlass::bfloat16_t>>, Is_dropout=false, Is_causal=true, Is_even_N=false, Is_even_K=false, Return_softmax=false]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(34): here
            instantiation of "void run_flash_fwd<Kernel_traits,Is_dropout,Is_causal>(Flash_fwd_params &, cudaStream_t) [with Kernel_traits=Flash_fwd_kernel_traits<160, 64, 64, 4, false, false, cutlass::bfloat16_t, Flash_kernel_traits<160, 64, 64, 4, cutlass::bfloat16_t>>, Is_dropout=false, Is_causal=true]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(155): here
            instantiation of "void run_mha_fwd_hdim160<T>(Flash_fwd_params &, cudaStream_t) [with T=cutlass::bfloat16_t]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_hdim160_bf16_sm80.cu(16): here

/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_kernel.h(140): warning: variable "MMA_M" was declared but never referenced
          detected during:
            instantiation of "void flash::compute_attn_1rowblock<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params,Is_Q_split_K>(const Params &, int, int, int) [with Kernel_traits=Flash_fwd_kernel_traits<160, 128, 32, 4, false, false, cutlass::bfloat16_t, Flash_kernel_traits<160, 128, 32, 4, cutlass::bfloat16_t>>, Is_dropout=false, Is_causal=true, Is_even_N=true, Is_even_K=true, Return_softmax=false, Params=Flash_fwd_params, Is_Q_split_K=false]" 
(589): here
            instantiation of "void flash::compute_attn<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &) [with Kernel_traits=Flash_fwd_kernel_traits<160, 128, 32, 4, false, false, cutlass::bfloat16_t, Flash_kernel_traits<160, 128, 32, 4, cutlass::bfloat16_t>>, Is_dropout=false, Is_causal=true, Is_even_N=true, Is_even_K=true, Return_softmax=false, Params=Flash_fwd_params]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(15): here
            instantiation of "void flash_fwd_kernel<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax>(Flash_fwd_params) [with Kernel_traits=Flash_fwd_kernel_traits<160, 128, 32, 4, false, false, cutlass::bfloat16_t, Flash_kernel_traits<160, 128, 32, 4, cutlass::bfloat16_t>>, Is_dropout=false, Is_causal=true, Is_even_N=true, Is_even_K=true, Return_softmax=false]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(34): here
            instantiation of "void run_flash_fwd<Kernel_traits,Is_dropout,Is_causal>(Flash_fwd_params &, cudaStream_t) [with Kernel_traits=Flash_fwd_kernel_traits<160, 128, 32, 4, false, false, cutlass::bfloat16_t, Flash_kernel_traits<160, 128, 32, 4, cutlass::bfloat16_t>>, Is_dropout=false, Is_causal=true]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(155): here
            instantiation of "void run_mha_fwd_hdim160<T>(Flash_fwd_params &, cudaStream_t) [with T=cutlass::bfloat16_t]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_hdim160_bf16_sm80.cu(16): here

/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_kernel.h(276): warning: variable "tQrQ" was declared but never referenced
          detected during:
            instantiation of "void flash::compute_attn_1rowblock<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params,Is_Q_split_K>(const Params &, int, int, int) [with Kernel_traits=Flash_fwd_kernel_traits<160, 128, 32, 4, false, false, cutlass::bfloat16_t, Flash_kernel_traits<160, 128, 32, 4, cutlass::bfloat16_t>>, Is_dropout=false, Is_causal=true, Is_even_N=true, Is_even_K=true, Return_softmax=false, Params=Flash_fwd_params, Is_Q_split_K=false]" 
(589): here
            instantiation of "void flash::compute_attn<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &) [with Kernel_traits=Flash_fwd_kernel_traits<160, 128, 32, 4, false, false, cutlass::bfloat16_t, Flash_kernel_traits<160, 128, 32, 4, cutlass::bfloat16_t>>, Is_dropout=false, Is_causal=true, Is_even_N=true, Is_even_K=true, Return_softmax=false, Params=Flash_fwd_params]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(15): here
            instantiation of "void flash_fwd_kernel<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax>(Flash_fwd_params) [with Kernel_traits=Flash_fwd_kernel_traits<160, 128, 32, 4, false, false, cutlass::bfloat16_t, Flash_kernel_traits<160, 128, 32, 4, cutlass::bfloat16_t>>, Is_dropout=false, Is_causal=true, Is_even_N=true, Is_even_K=true, Return_softmax=false]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(34): here
            instantiation of "void run_flash_fwd<Kernel_traits,Is_dropout,Is_causal>(Flash_fwd_params &, cudaStream_t) [with Kernel_traits=Flash_fwd_kernel_traits<160, 128, 32, 4, false, false, cutlass::bfloat16_t, Flash_kernel_traits<160, 128, 32, 4, cutlass::bfloat16_t>>, Is_dropout=false, Is_causal=true]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(155): here
            instantiation of "void run_mha_fwd_hdim160<T>(Flash_fwd_params &, cudaStream_t) [with T=cutlass::bfloat16_t]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_hdim160_bf16_sm80.cu(16): here

/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_kernel.h(140): warning: variable "MMA_M" was declared but never referenced
          detected during:
            instantiation of "void flash::compute_attn_1rowblock<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params,Is_Q_split_K>(const Params &, int, int, int) [with Kernel_traits=Flash_fwd_kernel_traits<160, 128, 32, 4, false, false, cutlass::bfloat16_t, Flash_kernel_traits<160, 128, 32, 4, cutlass::bfloat16_t>>, Is_dropout=false, Is_causal=true, Is_even_N=true, Is_even_K=false, Return_softmax=false, Params=Flash_fwd_params, Is_Q_split_K=false]" 
(589): here
            instantiation of "void flash::compute_attn<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &) [with Kernel_traits=Flash_fwd_kernel_traits<160, 128, 32, 4, false, false, cutlass::bfloat16_t, Flash_kernel_traits<160, 128, 32, 4, cutlass::bfloat16_t>>, Is_dropout=false, Is_causal=true, Is_even_N=true, Is_even_K=false, Return_softmax=false, Params=Flash_fwd_params]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(15): here
            instantiation of "void flash_fwd_kernel<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax>(Flash_fwd_params) [with Kernel_traits=Flash_fwd_kernel_traits<160, 128, 32, 4, false, false, cutlass::bfloat16_t, Flash_kernel_traits<160, 128, 32, 4, cutlass::bfloat16_t>>, Is_dropout=false, Is_causal=true, Is_even_N=true, Is_even_K=false, Return_softmax=false]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(34): here
            instantiation of "void run_flash_fwd<Kernel_traits,Is_dropout,Is_causal>(Flash_fwd_params &, cudaStream_t) [with Kernel_traits=Flash_fwd_kernel_traits<160, 128, 32, 4, false, false, cutlass::bfloat16_t, Flash_kernel_traits<160, 128, 32, 4, cutlass::bfloat16_t>>, Is_dropout=false, Is_causal=true]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(155): here
            instantiation of "void run_mha_fwd_hdim160<T>(Flash_fwd_params &, cudaStream_t) [with T=cutlass::bfloat16_t]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_hdim160_bf16_sm80.cu(16): here

/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_kernel.h(276): warning: variable "tQrQ" was declared but never referenced
          detected during:
            instantiation of "void flash::compute_attn_1rowblock<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params,Is_Q_split_K>(const Params &, int, int, int) [with Kernel_traits=Flash_fwd_kernel_traits<160, 128, 32, 4, false, false, cutlass::bfloat16_t, Flash_kernel_traits<160, 128, 32, 4, cutlass::bfloat16_t>>, Is_dropout=false, Is_causal=true, Is_even_N=true, Is_even_K=false, Return_softmax=false, Params=Flash_fwd_params, Is_Q_split_K=false]" 
(589): here
            instantiation of "void flash::compute_attn<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &) [with Kernel_traits=Flash_fwd_kernel_traits<160, 128, 32, 4, false, false, cutlass::bfloat16_t, Flash_kernel_traits<160, 128, 32, 4, cutlass::bfloat16_t>>, Is_dropout=false, Is_causal=true, Is_even_N=true, Is_even_K=false, Return_softmax=false, Params=Flash_fwd_params]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(15): here
            instantiation of "void flash_fwd_kernel<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax>(Flash_fwd_params) [with Kernel_traits=Flash_fwd_kernel_traits<160, 128, 32, 4, false, false, cutlass::bfloat16_t, Flash_kernel_traits<160, 128, 32, 4, cutlass::bfloat16_t>>, Is_dropout=false, Is_causal=true, Is_even_N=true, Is_even_K=false, Return_softmax=false]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(34): here
            instantiation of "void run_flash_fwd<Kernel_traits,Is_dropout,Is_causal>(Flash_fwd_params &, cudaStream_t) [with Kernel_traits=Flash_fwd_kernel_traits<160, 128, 32, 4, false, false, cutlass::bfloat16_t, Flash_kernel_traits<160, 128, 32, 4, cutlass::bfloat16_t>>, Is_dropout=false, Is_causal=true]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(155): here
            instantiation of "void run_mha_fwd_hdim160<T>(Flash_fwd_params &, cudaStream_t) [with T=cutlass::bfloat16_t]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_hdim160_bf16_sm80.cu(16): here

/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_kernel.h(140): warning: variable "MMA_M" was declared but never referenced
          detected during:
            instantiation of "void flash::compute_attn_1rowblock<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params,Is_Q_split_K>(const Params &, int, int, int) [with Kernel_traits=Flash_fwd_kernel_traits<160, 128, 32, 4, false, false, cutlass::bfloat16_t, Flash_kernel_traits<160, 128, 32, 4, cutlass::bfloat16_t>>, Is_dropout=false, Is_causal=true, Is_even_N=false, Is_even_K=true, Return_softmax=false, Params=Flash_fwd_params, Is_Q_split_K=false]" 
(589): here
            instantiation of "void flash::compute_attn<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &) [with Kernel_traits=Flash_fwd_kernel_traits<160, 128, 32, 4, false, false, cutlass::bfloat16_t, Flash_kernel_traits<160, 128, 32, 4, cutlass::bfloat16_t>>, Is_dropout=false, Is_causal=true, Is_even_N=false, Is_even_K=true, Return_softmax=false, Params=Flash_fwd_params]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(15): here
            instantiation of "void flash_fwd_kernel<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax>(Flash_fwd_params) [with Kernel_traits=Flash_fwd_kernel_traits<160, 128, 32, 4, false, false, cutlass::bfloat16_t, Flash_kernel_traits<160, 128, 32, 4, cutlass::bfloat16_t>>, Is_dropout=false, Is_causal=true, Is_even_N=false, Is_even_K=true, Return_softmax=false]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(34): here
            instantiation of "void run_flash_fwd<Kernel_traits,Is_dropout,Is_causal>(Flash_fwd_params &, cudaStream_t) [with Kernel_traits=Flash_fwd_kernel_traits<160, 128, 32, 4, false, false, cutlass::bfloat16_t, Flash_kernel_traits<160, 128, 32, 4, cutlass::bfloat16_t>>, Is_dropout=false, Is_causal=true]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(155): here
            instantiation of "void run_mha_fwd_hdim160<T>(Flash_fwd_params &, cudaStream_t) [with T=cutlass::bfloat16_t]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_hdim160_bf16_sm80.cu(16): here

/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_kernel.h(276): warning: variable "tQrQ" was declared but never referenced
          detected during:
            instantiation of "void flash::compute_attn_1rowblock<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params,Is_Q_split_K>(const Params &, int, int, int) [with Kernel_traits=Flash_fwd_kernel_traits<160, 128, 32, 4, false, false, cutlass::bfloat16_t, Flash_kernel_traits<160, 128, 32, 4, cutlass::bfloat16_t>>, Is_dropout=false, Is_causal=true, Is_even_N=false, Is_even_K=true, Return_softmax=false, Params=Flash_fwd_params, Is_Q_split_K=false]" 
(589): here
            instantiation of "void flash::compute_attn<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &) [with Kernel_traits=Flash_fwd_kernel_traits<160, 128, 32, 4, false, false, cutlass::bfloat16_t, Flash_kernel_traits<160, 128, 32, 4, cutlass::bfloat16_t>>, Is_dropout=false, Is_causal=true, Is_even_N=false, Is_even_K=true, Return_softmax=false, Params=Flash_fwd_params]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(15): here
            instantiation of "void flash_fwd_kernel<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax>(Flash_fwd_params) [with Kernel_traits=Flash_fwd_kernel_traits<160, 128, 32, 4, false, false, cutlass::bfloat16_t, Flash_kernel_traits<160, 128, 32, 4, cutlass::bfloat16_t>>, Is_dropout=false, Is_causal=true, Is_even_N=false, Is_even_K=true, Return_softmax=false]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(34): here
            instantiation of "void run_flash_fwd<Kernel_traits,Is_dropout,Is_causal>(Flash_fwd_params &, cudaStream_t) [with Kernel_traits=Flash_fwd_kernel_traits<160, 128, 32, 4, false, false, cutlass::bfloat16_t, Flash_kernel_traits<160, 128, 32, 4, cutlass::bfloat16_t>>, Is_dropout=false, Is_causal=true]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(155): here
            instantiation of "void run_mha_fwd_hdim160<T>(Flash_fwd_params &, cudaStream_t) [with T=cutlass::bfloat16_t]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_hdim160_bf16_sm80.cu(16): here

/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_kernel.h(140): warning: variable "MMA_M" was declared but never referenced
          detected during:
            instantiation of "void flash::compute_attn_1rowblock<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params,Is_Q_split_K>(const Params &, int, int, int) [with Kernel_traits=Flash_fwd_kernel_traits<160, 128, 32, 4, false, false, cutlass::bfloat16_t, Flash_kernel_traits<160, 128, 32, 4, cutlass::bfloat16_t>>, Is_dropout=false, Is_causal=true, Is_even_N=false, Is_even_K=false, Return_softmax=false, Params=Flash_fwd_params, Is_Q_split_K=false]" 
(589): here
            instantiation of "void flash::compute_attn<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &) [with Kernel_traits=Flash_fwd_kernel_traits<160, 128, 32, 4, false, false, cutlass::bfloat16_t, Flash_kernel_traits<160, 128, 32, 4, cutlass::bfloat16_t>>, Is_dropout=false, Is_causal=true, Is_even_N=false, Is_even_K=false, Return_softmax=false, Params=Flash_fwd_params]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(15): here
            instantiation of "void flash_fwd_kernel<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax>(Flash_fwd_params) [with Kernel_traits=Flash_fwd_kernel_traits<160, 128, 32, 4, false, false, cutlass::bfloat16_t, Flash_kernel_traits<160, 128, 32, 4, cutlass::bfloat16_t>>, Is_dropout=false, Is_causal=true, Is_even_N=false, Is_even_K=false, Return_softmax=false]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(34): here
            instantiation of "void run_flash_fwd<Kernel_traits,Is_dropout,Is_causal>(Flash_fwd_params &, cudaStream_t) [with Kernel_traits=Flash_fwd_kernel_traits<160, 128, 32, 4, false, false, cutlass::bfloat16_t, Flash_kernel_traits<160, 128, 32, 4, cutlass::bfloat16_t>>, Is_dropout=false, Is_causal=true]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(155): here
            instantiation of "void run_mha_fwd_hdim160<T>(Flash_fwd_params &, cudaStream_t) [with T=cutlass::bfloat16_t]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_hdim160_bf16_sm80.cu(16): here

/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_kernel.h(276): warning: variable "tQrQ" was declared but never referenced
          detected during:
            instantiation of "void flash::compute_attn_1rowblock<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params,Is_Q_split_K>(const Params &, int, int, int) [with Kernel_traits=Flash_fwd_kernel_traits<160, 128, 32, 4, false, false, cutlass::bfloat16_t, Flash_kernel_traits<160, 128, 32, 4, cutlass::bfloat16_t>>, Is_dropout=false, Is_causal=true, Is_even_N=false, Is_even_K=false, Return_softmax=false, Params=Flash_fwd_params, Is_Q_split_K=false]" 
(589): here
            instantiation of "void flash::compute_attn<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &) [with Kernel_traits=Flash_fwd_kernel_traits<160, 128, 32, 4, false, false, cutlass::bfloat16_t, Flash_kernel_traits<160, 128, 32, 4, cutlass::bfloat16_t>>, Is_dropout=false, Is_causal=true, Is_even_N=false, Is_even_K=false, Return_softmax=false, Params=Flash_fwd_params]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(15): here
            instantiation of "void flash_fwd_kernel<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax>(Flash_fwd_params) [with Kernel_traits=Flash_fwd_kernel_traits<160, 128, 32, 4, false, false, cutlass::bfloat16_t, Flash_kernel_traits<160, 128, 32, 4, cutlass::bfloat16_t>>, Is_dropout=false, Is_causal=true, Is_even_N=false, Is_even_K=false, Return_softmax=false]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(34): here
            instantiation of "void run_flash_fwd<Kernel_traits,Is_dropout,Is_causal>(Flash_fwd_params &, cudaStream_t) [with Kernel_traits=Flash_fwd_kernel_traits<160, 128, 32, 4, false, false, cutlass::bfloat16_t, Flash_kernel_traits<160, 128, 32, 4, cutlass::bfloat16_t>>, Is_dropout=false, Is_causal=true]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(155): here
            instantiation of "void run_mha_fwd_hdim160<T>(Flash_fwd_params &, cudaStream_t) [with T=cutlass::bfloat16_t]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_hdim160_bf16_sm80.cu(16): here

/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_kernel.h(140): warning: variable "MMA_M" was declared but never referenced
          detected during:
            instantiation of "void flash::compute_attn_1rowblock<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params,Is_Q_split_K>(const Params &, int, int, int) [with Kernel_traits=Flash_fwd_kernel_traits<160, 128, 64, 8, false, false, cutlass::bfloat16_t, Flash_kernel_traits<160, 128, 64, 8, cutlass::bfloat16_t>>, Is_dropout=false, Is_causal=false, Is_even_N=true, Is_even_K=true, Return_softmax=false, Params=Flash_fwd_params, Is_Q_split_K=false]" 
(589): here
            instantiation of "void flash::compute_attn<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &) [with Kernel_traits=Flash_fwd_kernel_traits<160, 128, 64, 8, false, false, cutlass::bfloat16_t, Flash_kernel_traits<160, 128, 64, 8, cutlass::bfloat16_t>>, Is_dropout=false, Is_causal=false, Is_even_N=true, Is_even_K=true, Return_softmax=false, Params=Flash_fwd_params]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(15): here
            instantiation of "void flash_fwd_kernel<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax>(Flash_fwd_params) [with Kernel_traits=Flash_fwd_kernel_traits<160, 128, 64, 8, false, false, cutlass::bfloat16_t, Flash_kernel_traits<160, 128, 64, 8, cutlass::bfloat16_t>>, Is_dropout=false, Is_causal=false, Is_even_N=true, Is_even_K=true, Return_softmax=false]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(34): here
            instantiation of "void run_flash_fwd<Kernel_traits,Is_dropout,Is_causal>(Flash_fwd_params &, cudaStream_t) [with Kernel_traits=Flash_fwd_kernel_traits<160, 128, 64, 8, false, false, cutlass::bfloat16_t, Flash_kernel_traits<160, 128, 64, 8, cutlass::bfloat16_t>>, Is_dropout=false, Is_causal=false]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(155): here
            instantiation of "void run_mha_fwd_hdim160<T>(Flash_fwd_params &, cudaStream_t) [with T=cutlass::bfloat16_t]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_hdim160_bf16_sm80.cu(16): here

/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_kernel.h(276): warning: variable "tQrQ" was declared but never referenced
          detected during:
            instantiation of "void flash::compute_attn_1rowblock<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params,Is_Q_split_K>(const Params &, int, int, int) [with Kernel_traits=Flash_fwd_kernel_traits<160, 128, 64, 8, false, false, cutlass::bfloat16_t, Flash_kernel_traits<160, 128, 64, 8, cutlass::bfloat16_t>>, Is_dropout=false, Is_causal=false, Is_even_N=true, Is_even_K=true, Return_softmax=false, Params=Flash_fwd_params, Is_Q_split_K=false]" 
(589): here
            instantiation of "void flash::compute_attn<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &) [with Kernel_traits=Flash_fwd_kernel_traits<160, 128, 64, 8, false, false, cutlass::bfloat16_t, Flash_kernel_traits<160, 128, 64, 8, cutlass::bfloat16_t>>, Is_dropout=false, Is_causal=false, Is_even_N=true, Is_even_K=true, Return_softmax=false, Params=Flash_fwd_params]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(15): here
            instantiation of "void flash_fwd_kernel<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax>(Flash_fwd_params) [with Kernel_traits=Flash_fwd_kernel_traits<160, 128, 64, 8, false, false, cutlass::bfloat16_t, Flash_kernel_traits<160, 128, 64, 8, cutlass::bfloat16_t>>, Is_dropout=false, Is_causal=false, Is_even_N=true, Is_even_K=true, Return_softmax=false]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(34): here
            instantiation of "void run_flash_fwd<Kernel_traits,Is_dropout,Is_causal>(Flash_fwd_params &, cudaStream_t) [with Kernel_traits=Flash_fwd_kernel_traits<160, 128, 64, 8, false, false, cutlass::bfloat16_t, Flash_kernel_traits<160, 128, 64, 8, cutlass::bfloat16_t>>, Is_dropout=false, Is_causal=false]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(155): here
            instantiation of "void run_mha_fwd_hdim160<T>(Flash_fwd_params &, cudaStream_t) [with T=cutlass::bfloat16_t]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_hdim160_bf16_sm80.cu(16): here

/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_kernel.h(140): warning: variable "MMA_M" was declared but never referenced
          detected during:
            instantiation of "void flash::compute_attn_1rowblock<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params,Is_Q_split_K>(const Params &, int, int, int) [with Kernel_traits=Flash_fwd_kernel_traits<160, 128, 64, 8, false, false, cutlass::bfloat16_t, Flash_kernel_traits<160, 128, 64, 8, cutlass::bfloat16_t>>, Is_dropout=false, Is_causal=false, Is_even_N=true, Is_even_K=false, Return_softmax=false, Params=Flash_fwd_params, Is_Q_split_K=false]" 
(589): here
            instantiation of "void flash::compute_attn<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &) [with Kernel_traits=Flash_fwd_kernel_traits<160, 128, 64, 8, false, false, cutlass::bfloat16_t, Flash_kernel_traits<160, 128, 64, 8, cutlass::bfloat16_t>>, Is_dropout=false, Is_causal=false, Is_even_N=true, Is_even_K=false, Return_softmax=false, Params=Flash_fwd_params]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(15): here
            instantiation of "void flash_fwd_kernel<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax>(Flash_fwd_params) [with Kernel_traits=Flash_fwd_kernel_traits<160, 128, 64, 8, false, false, cutlass::bfloat16_t, Flash_kernel_traits<160, 128, 64, 8, cutlass::bfloat16_t>>, Is_dropout=false, Is_causal=false, Is_even_N=true, Is_even_K=false, Return_softmax=false]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(34): here
            instantiation of "void run_flash_fwd<Kernel_traits,Is_dropout,Is_causal>(Flash_fwd_params &, cudaStream_t) [with Kernel_traits=Flash_fwd_kernel_traits<160, 128, 64, 8, false, false, cutlass::bfloat16_t, Flash_kernel_traits<160, 128, 64, 8, cutlass::bfloat16_t>>, Is_dropout=false, Is_causal=false]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(155): here
            instantiation of "void run_mha_fwd_hdim160<T>(Flash_fwd_params &, cudaStream_t) [with T=cutlass::bfloat16_t]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_hdim160_bf16_sm80.cu(16): here

/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_kernel.h(276): warning: variable "tQrQ" was declared but never referenced
          detected during:
            instantiation of "void flash::compute_attn_1rowblock<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params,Is_Q_split_K>(const Params &, int, int, int) [with Kernel_traits=Flash_fwd_kernel_traits<160, 128, 64, 8, false, false, cutlass::bfloat16_t, Flash_kernel_traits<160, 128, 64, 8, cutlass::bfloat16_t>>, Is_dropout=false, Is_causal=false, Is_even_N=true, Is_even_K=false, Return_softmax=false, Params=Flash_fwd_params, Is_Q_split_K=false]" 
(589): here
            instantiation of "void flash::compute_attn<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &) [with Kernel_traits=Flash_fwd_kernel_traits<160, 128, 64, 8, false, false, cutlass::bfloat16_t, Flash_kernel_traits<160, 128, 64, 8, cutlass::bfloat16_t>>, Is_dropout=false, Is_causal=false, Is_even_N=true, Is_even_K=false, Return_softmax=false, Params=Flash_fwd_params]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(15): here
            instantiation of "void flash_fwd_kernel<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax>(Flash_fwd_params) [with Kernel_traits=Flash_fwd_kernel_traits<160, 128, 64, 8, false, false, cutlass::bfloat16_t, Flash_kernel_traits<160, 128, 64, 8, cutlass::bfloat16_t>>, Is_dropout=false, Is_causal=false, Is_even_N=true, Is_even_K=false, Return_softmax=false]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(34): here
            instantiation of "void run_flash_fwd<Kernel_traits,Is_dropout,Is_causal>(Flash_fwd_params &, cudaStream_t) [with Kernel_traits=Flash_fwd_kernel_traits<160, 128, 64, 8, false, false, cutlass::bfloat16_t, Flash_kernel_traits<160, 128, 64, 8, cutlass::bfloat16_t>>, Is_dropout=false, Is_causal=false]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(155): here
            instantiation of "void run_mha_fwd_hdim160<T>(Flash_fwd_params &, cudaStream_t) [with T=cutlass::bfloat16_t]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_hdim160_bf16_sm80.cu(16): here

/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_kernel.h(140): warning: variable "MMA_M" was declared but never referenced
          detected during:
            instantiation of "void flash::compute_attn_1rowblock<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params,Is_Q_split_K>(const Params &, int, int, int) [with Kernel_traits=Flash_fwd_kernel_traits<160, 128, 64, 8, false, false, cutlass::bfloat16_t, Flash_kernel_traits<160, 128, 64, 8, cutlass::bfloat16_t>>, Is_dropout=false, Is_causal=false, Is_even_N=false, Is_even_K=true, Return_softmax=false, Params=Flash_fwd_params, Is_Q_split_K=false]" 
(589): here
            instantiation of "void flash::compute_attn<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &) [with Kernel_traits=Flash_fwd_kernel_traits<160, 128, 64, 8, false, false, cutlass::bfloat16_t, Flash_kernel_traits<160, 128, 64, 8, cutlass::bfloat16_t>>, Is_dropout=false, Is_causal=false, Is_even_N=false, Is_even_K=true, Return_softmax=false, Params=Flash_fwd_params]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(15): here
            instantiation of "void flash_fwd_kernel<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax>(Flash_fwd_params) [with Kernel_traits=Flash_fwd_kernel_traits<160, 128, 64, 8, false, false, cutlass::bfloat16_t, Flash_kernel_traits<160, 128, 64, 8, cutlass::bfloat16_t>>, Is_dropout=false, Is_causal=false, Is_even_N=false, Is_even_K=true, Return_softmax=false]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(34): here
            instantiation of "void run_flash_fwd<Kernel_traits,Is_dropout,Is_causal>(Flash_fwd_params &, cudaStream_t) [with Kernel_traits=Flash_fwd_kernel_traits<160, 128, 64, 8, false, false, cutlass::bfloat16_t, Flash_kernel_traits<160, 128, 64, 8, cutlass::bfloat16_t>>, Is_dropout=false, Is_causal=false]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(155): here
            instantiation of "void run_mha_fwd_hdim160<T>(Flash_fwd_params &, cudaStream_t) [with T=cutlass::bfloat16_t]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_hdim160_bf16_sm80.cu(16): here

/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_kernel.h(276): warning: variable "tQrQ" was declared but never referenced
          detected during:
            instantiation of "void flash::compute_attn_1rowblock<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params,Is_Q_split_K>(const Params &, int, int, int) [with Kernel_traits=Flash_fwd_kernel_traits<160, 128, 64, 8, false, false, cutlass::bfloat16_t, Flash_kernel_traits<160, 128, 64, 8, cutlass::bfloat16_t>>, Is_dropout=false, Is_causal=false, Is_even_N=false, Is_even_K=true, Return_softmax=false, Params=Flash_fwd_params, Is_Q_split_K=false]" 
(589): here
            instantiation of "void flash::compute_attn<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &) [with Kernel_traits=Flash_fwd_kernel_traits<160, 128, 64, 8, false, false, cutlass::bfloat16_t, Flash_kernel_traits<160, 128, 64, 8, cutlass::bfloat16_t>>, Is_dropout=false, Is_causal=false, Is_even_N=false, Is_even_K=true, Return_softmax=false, Params=Flash_fwd_params]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(15): here
            instantiation of "void flash_fwd_kernel<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax>(Flash_fwd_params) [with Kernel_traits=Flash_fwd_kernel_traits<160, 128, 64, 8, false, false, cutlass::bfloat16_t, Flash_kernel_traits<160, 128, 64, 8, cutlass::bfloat16_t>>, Is_dropout=false, Is_causal=false, Is_even_N=false, Is_even_K=true, Return_softmax=false]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(34): here
            instantiation of "void run_flash_fwd<Kernel_traits,Is_dropout,Is_causal>(Flash_fwd_params &, cudaStream_t) [with Kernel_traits=Flash_fwd_kernel_traits<160, 128, 64, 8, false, false, cutlass::bfloat16_t, Flash_kernel_traits<160, 128, 64, 8, cutlass::bfloat16_t>>, Is_dropout=false, Is_causal=false]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(155): here
            instantiation of "void run_mha_fwd_hdim160<T>(Flash_fwd_params &, cudaStream_t) [with T=cutlass::bfloat16_t]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_hdim160_bf16_sm80.cu(16): here

/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_kernel.h(140): warning: variable "MMA_M" was declared but never referenced
          detected during:
            instantiation of "void flash::compute_attn_1rowblock<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params,Is_Q_split_K>(const Params &, int, int, int) [with Kernel_traits=Flash_fwd_kernel_traits<160, 128, 64, 8, false, false, cutlass::bfloat16_t, Flash_kernel_traits<160, 128, 64, 8, cutlass::bfloat16_t>>, Is_dropout=false, Is_causal=false, Is_even_N=false, Is_even_K=false, Return_softmax=false, Params=Flash_fwd_params, Is_Q_split_K=false]" 
(589): here
            instantiation of "void flash::compute_attn<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &) [with Kernel_traits=Flash_fwd_kernel_traits<160, 128, 64, 8, false, false, cutlass::bfloat16_t, Flash_kernel_traits<160, 128, 64, 8, cutlass::bfloat16_t>>, Is_dropout=false, Is_causal=false, Is_even_N=false, Is_even_K=false, Return_softmax=false, Params=Flash_fwd_params]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(15): here
            instantiation of "void flash_fwd_kernel<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax>(Flash_fwd_params) [with Kernel_traits=Flash_fwd_kernel_traits<160, 128, 64, 8, false, false, cutlass::bfloat16_t, Flash_kernel_traits<160, 128, 64, 8, cutlass::bfloat16_t>>, Is_dropout=false, Is_causal=false, Is_even_N=false, Is_even_K=false, Return_softmax=false]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(34): here
            instantiation of "void run_flash_fwd<Kernel_traits,Is_dropout,Is_causal>(Flash_fwd_params &, cudaStream_t) [with Kernel_traits=Flash_fwd_kernel_traits<160, 128, 64, 8, false, false, cutlass::bfloat16_t, Flash_kernel_traits<160, 128, 64, 8, cutlass::bfloat16_t>>, Is_dropout=false, Is_causal=false]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(155): here
            instantiation of "void run_mha_fwd_hdim160<T>(Flash_fwd_params &, cudaStream_t) [with T=cutlass::bfloat16_t]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_hdim160_bf16_sm80.cu(16): here

/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_kernel.h(276): warning: variable "tQrQ" was declared but never referenced
          detected during:
            instantiation of "void flash::compute_attn_1rowblock<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params,Is_Q_split_K>(const Params &, int, int, int) [with Kernel_traits=Flash_fwd_kernel_traits<160, 128, 64, 8, false, false, cutlass::bfloat16_t, Flash_kernel_traits<160, 128, 64, 8, cutlass::bfloat16_t>>, Is_dropout=false, Is_causal=false, Is_even_N=false, Is_even_K=false, Return_softmax=false, Params=Flash_fwd_params, Is_Q_split_K=false]" 
(589): here
            instantiation of "void flash::compute_attn<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &) [with Kernel_traits=Flash_fwd_kernel_traits<160, 128, 64, 8, false, false, cutlass::bfloat16_t, Flash_kernel_traits<160, 128, 64, 8, cutlass::bfloat16_t>>, Is_dropout=false, Is_causal=false, Is_even_N=false, Is_even_K=false, Return_softmax=false, Params=Flash_fwd_params]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(15): here
            instantiation of "void flash_fwd_kernel<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax>(Flash_fwd_params) [with Kernel_traits=Flash_fwd_kernel_traits<160, 128, 64, 8, false, false, cutlass::bfloat16_t, Flash_kernel_traits<160, 128, 64, 8, cutlass::bfloat16_t>>, Is_dropout=false, Is_causal=false, Is_even_N=false, Is_even_K=false, Return_softmax=false]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(34): here
            instantiation of "void run_flash_fwd<Kernel_traits,Is_dropout,Is_causal>(Flash_fwd_params &, cudaStream_t) [with Kernel_traits=Flash_fwd_kernel_traits<160, 128, 64, 8, false, false, cutlass::bfloat16_t, Flash_kernel_traits<160, 128, 64, 8, cutlass::bfloat16_t>>, Is_dropout=false, Is_causal=false]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(155): here
            instantiation of "void run_mha_fwd_hdim160<T>(Flash_fwd_params &, cudaStream_t) [with T=cutlass::bfloat16_t]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_hdim160_bf16_sm80.cu(16): here

/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_kernel.h(140): warning: variable "MMA_M" was declared but never referenced
          detected during:
            instantiation of "void flash::compute_attn_1rowblock<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params,Is_Q_split_K>(const Params &, int, int, int) [with Kernel_traits=Flash_fwd_kernel_traits<160, 128, 32, 4, false, false, cutlass::bfloat16_t, Flash_kernel_traits<160, 128, 32, 4, cutlass::bfloat16_t>>, Is_dropout=false, Is_causal=false, Is_even_N=true, Is_even_K=true, Return_softmax=false, Params=Flash_fwd_params, Is_Q_split_K=false]" 
(589): here
            instantiation of "void flash::compute_attn<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &) [with Kernel_traits=Flash_fwd_kernel_traits<160, 128, 32, 4, false, false, cutlass::bfloat16_t, Flash_kernel_traits<160, 128, 32, 4, cutlass::bfloat16_t>>, Is_dropout=false, Is_causal=false, Is_even_N=true, Is_even_K=true, Return_softmax=false, Params=Flash_fwd_params]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(15): here
            instantiation of "void flash_fwd_kernel<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax>(Flash_fwd_params) [with Kernel_traits=Flash_fwd_kernel_traits<160, 128, 32, 4, false, false, cutlass::bfloat16_t, Flash_kernel_traits<160, 128, 32, 4, cutlass::bfloat16_t>>, Is_dropout=false, Is_causal=false, Is_even_N=true, Is_even_K=true, Return_softmax=false]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(34): here
            instantiation of "void run_flash_fwd<Kernel_traits,Is_dropout,Is_causal>(Flash_fwd_params &, cudaStream_t) [with Kernel_traits=Flash_fwd_kernel_traits<160, 128, 32, 4, false, false, cutlass::bfloat16_t, Flash_kernel_traits<160, 128, 32, 4, cutlass::bfloat16_t>>, Is_dropout=false, Is_causal=false]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(155): here
            instantiation of "void run_mha_fwd_hdim160<T>(Flash_fwd_params &, cudaStream_t) [with T=cutlass::bfloat16_t]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_hdim160_bf16_sm80.cu(16): here

/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_kernel.h(276): warning: variable "tQrQ" was declared but never referenced
          detected during:
            instantiation of "void flash::compute_attn_1rowblock<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params,Is_Q_split_K>(const Params &, int, int, int) [with Kernel_traits=Flash_fwd_kernel_traits<160, 128, 32, 4, false, false, cutlass::bfloat16_t, Flash_kernel_traits<160, 128, 32, 4, cutlass::bfloat16_t>>, Is_dropout=false, Is_causal=false, Is_even_N=true, Is_even_K=true, Return_softmax=false, Params=Flash_fwd_params, Is_Q_split_K=false]" 
(589): here
            instantiation of "void flash::compute_attn<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &) [with Kernel_traits=Flash_fwd_kernel_traits<160, 128, 32, 4, false, false, cutlass::bfloat16_t, Flash_kernel_traits<160, 128, 32, 4, cutlass::bfloat16_t>>, Is_dropout=false, Is_causal=false, Is_even_N=true, Is_even_K=true, Return_softmax=false, Params=Flash_fwd_params]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(15): here
            instantiation of "void flash_fwd_kernel<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax>(Flash_fwd_params) [with Kernel_traits=Flash_fwd_kernel_traits<160, 128, 32, 4, false, false, cutlass::bfloat16_t, Flash_kernel_traits<160, 128, 32, 4, cutlass::bfloat16_t>>, Is_dropout=false, Is_causal=false, Is_even_N=true, Is_even_K=true, Return_softmax=false]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(34): here
            instantiation of "void run_flash_fwd<Kernel_traits,Is_dropout,Is_causal>(Flash_fwd_params &, cudaStream_t) [with Kernel_traits=Flash_fwd_kernel_traits<160, 128, 32, 4, false, false, cutlass::bfloat16_t, Flash_kernel_traits<160, 128, 32, 4, cutlass::bfloat16_t>>, Is_dropout=false, Is_causal=false]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(155): here
            instantiation of "void run_mha_fwd_hdim160<T>(Flash_fwd_params &, cudaStream_t) [with T=cutlass::bfloat16_t]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_hdim160_bf16_sm80.cu(16): here

/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_kernel.h(140): warning: variable "MMA_M" was declared but never referenced
          detected during:
            instantiation of "void flash::compute_attn_1rowblock<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params,Is_Q_split_K>(const Params &, int, int, int) [with Kernel_traits=Flash_fwd_kernel_traits<160, 128, 32, 4, false, false, cutlass::bfloat16_t, Flash_kernel_traits<160, 128, 32, 4, cutlass::bfloat16_t>>, Is_dropout=false, Is_causal=false, Is_even_N=true, Is_even_K=false, Return_softmax=false, Params=Flash_fwd_params, Is_Q_split_K=false]" 
(589): here
            instantiation of "void flash::compute_attn<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &) [with Kernel_traits=Flash_fwd_kernel_traits<160, 128, 32, 4, false, false, cutlass::bfloat16_t, Flash_kernel_traits<160, 128, 32, 4, cutlass::bfloat16_t>>, Is_dropout=false, Is_causal=false, Is_even_N=true, Is_even_K=false, Return_softmax=false, Params=Flash_fwd_params]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(15): here
            instantiation of "void flash_fwd_kernel<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax>(Flash_fwd_params) [with Kernel_traits=Flash_fwd_kernel_traits<160, 128, 32, 4, false, false, cutlass::bfloat16_t, Flash_kernel_traits<160, 128, 32, 4, cutlass::bfloat16_t>>, Is_dropout=false, Is_causal=false, Is_even_N=true, Is_even_K=false, Return_softmax=false]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(34): here
            instantiation of "void run_flash_fwd<Kernel_traits,Is_dropout,Is_causal>(Flash_fwd_params &, cudaStream_t) [with Kernel_traits=Flash_fwd_kernel_traits<160, 128, 32, 4, false, false, cutlass::bfloat16_t, Flash_kernel_traits<160, 128, 32, 4, cutlass::bfloat16_t>>, Is_dropout=false, Is_causal=false]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(155): here
            instantiation of "void run_mha_fwd_hdim160<T>(Flash_fwd_params &, cudaStream_t) [with T=cutlass::bfloat16_t]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_hdim160_bf16_sm80.cu(16): here

/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_kernel.h(276): warning: variable "tQrQ" was declared but never referenced
          detected during:
            instantiation of "void flash::compute_attn_1rowblock<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params,Is_Q_split_K>(const Params &, int, int, int) [with Kernel_traits=Flash_fwd_kernel_traits<160, 128, 32, 4, false, false, cutlass::bfloat16_t, Flash_kernel_traits<160, 128, 32, 4, cutlass::bfloat16_t>>, Is_dropout=false, Is_causal=false, Is_even_N=true, Is_even_K=false, Return_softmax=false, Params=Flash_fwd_params, Is_Q_split_K=false]" 
(589): here
            instantiation of "void flash::compute_attn<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &) [with Kernel_traits=Flash_fwd_kernel_traits<160, 128, 32, 4, false, false, cutlass::bfloat16_t, Flash_kernel_traits<160, 128, 32, 4, cutlass::bfloat16_t>>, Is_dropout=false, Is_causal=false, Is_even_N=true, Is_even_K=false, Return_softmax=false, Params=Flash_fwd_params]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(15): here
            instantiation of "void flash_fwd_kernel<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax>(Flash_fwd_params) [with Kernel_traits=Flash_fwd_kernel_traits<160, 128, 32, 4, false, false, cutlass::bfloat16_t, Flash_kernel_traits<160, 128, 32, 4, cutlass::bfloat16_t>>, Is_dropout=false, Is_causal=false, Is_even_N=true, Is_even_K=false, Return_softmax=false]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(34): here
            instantiation of "void run_flash_fwd<Kernel_traits,Is_dropout,Is_causal>(Flash_fwd_params &, cudaStream_t) [with Kernel_traits=Flash_fwd_kernel_traits<160, 128, 32, 4, false, false, cutlass::bfloat16_t, Flash_kernel_traits<160, 128, 32, 4, cutlass::bfloat16_t>>, Is_dropout=false, Is_causal=false]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(155): here
            instantiation of "void run_mha_fwd_hdim160<T>(Flash_fwd_params &, cudaStream_t) [with T=cutlass::bfloat16_t]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_hdim160_bf16_sm80.cu(16): here

/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_kernel.h(140): warning: variable "MMA_M" was declared but never referenced
          detected during:
            instantiation of "void flash::compute_attn_1rowblock<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params,Is_Q_split_K>(const Params &, int, int, int) [with Kernel_traits=Flash_fwd_kernel_traits<160, 128, 32, 4, false, false, cutlass::bfloat16_t, Flash_kernel_traits<160, 128, 32, 4, cutlass::bfloat16_t>>, Is_dropout=false, Is_causal=false, Is_even_N=false, Is_even_K=true, Return_softmax=false, Params=Flash_fwd_params, Is_Q_split_K=false]" 
(589): here
            instantiation of "void flash::compute_attn<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &) [with Kernel_traits=Flash_fwd_kernel_traits<160, 128, 32, 4, false, false, cutlass::bfloat16_t, Flash_kernel_traits<160, 128, 32, 4, cutlass::bfloat16_t>>, Is_dropout=false, Is_causal=false, Is_even_N=false, Is_even_K=true, Return_softmax=false, Params=Flash_fwd_params]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(15): here
            instantiation of "void flash_fwd_kernel<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax>(Flash_fwd_params) [with Kernel_traits=Flash_fwd_kernel_traits<160, 128, 32, 4, false, false, cutlass::bfloat16_t, Flash_kernel_traits<160, 128, 32, 4, cutlass::bfloat16_t>>, Is_dropout=false, Is_causal=false, Is_even_N=false, Is_even_K=true, Return_softmax=false]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(34): here
            instantiation of "void run_flash_fwd<Kernel_traits,Is_dropout,Is_causal>(Flash_fwd_params &, cudaStream_t) [with Kernel_traits=Flash_fwd_kernel_traits<160, 128, 32, 4, false, false, cutlass::bfloat16_t, Flash_kernel_traits<160, 128, 32, 4, cutlass::bfloat16_t>>, Is_dropout=false, Is_causal=false]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(155): here
            instantiation of "void run_mha_fwd_hdim160<T>(Flash_fwd_params &, cudaStream_t) [with T=cutlass::bfloat16_t]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_hdim160_bf16_sm80.cu(16): here

/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_kernel.h(276): warning: variable "tQrQ" was declared but never referenced
          detected during:
            instantiation of "void flash::compute_attn_1rowblock<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params,Is_Q_split_K>(const Params &, int, int, int) [with Kernel_traits=Flash_fwd_kernel_traits<160, 128, 32, 4, false, false, cutlass::bfloat16_t, Flash_kernel_traits<160, 128, 32, 4, cutlass::bfloat16_t>>, Is_dropout=false, Is_causal=false, Is_even_N=false, Is_even_K=true, Return_softmax=false, Params=Flash_fwd_params, Is_Q_split_K=false]" 
(589): here
            instantiation of "void flash::compute_attn<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &) [with Kernel_traits=Flash_fwd_kernel_traits<160, 128, 32, 4, false, false, cutlass::bfloat16_t, Flash_kernel_traits<160, 128, 32, 4, cutlass::bfloat16_t>>, Is_dropout=false, Is_causal=false, Is_even_N=false, Is_even_K=true, Return_softmax=false, Params=Flash_fwd_params]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(15): here
            instantiation of "void flash_fwd_kernel<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax>(Flash_fwd_params) [with Kernel_traits=Flash_fwd_kernel_traits<160, 128, 32, 4, false, false, cutlass::bfloat16_t, Flash_kernel_traits<160, 128, 32, 4, cutlass::bfloat16_t>>, Is_dropout=false, Is_causal=false, Is_even_N=false, Is_even_K=true, Return_softmax=false]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(34): here
            instantiation of "void run_flash_fwd<Kernel_traits,Is_dropout,Is_causal>(Flash_fwd_params &, cudaStream_t) [with Kernel_traits=Flash_fwd_kernel_traits<160, 128, 32, 4, false, false, cutlass::bfloat16_t, Flash_kernel_traits<160, 128, 32, 4, cutlass::bfloat16_t>>, Is_dropout=false, Is_causal=false]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(155): here
            instantiation of "void run_mha_fwd_hdim160<T>(Flash_fwd_params &, cudaStream_t) [with T=cutlass::bfloat16_t]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_hdim160_bf16_sm80.cu(16): here

/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_kernel.h(140): warning: variable "MMA_M" was declared but never referenced
          detected during:
            instantiation of "void flash::compute_attn_1rowblock<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params,Is_Q_split_K>(const Params &, int, int, int) [with Kernel_traits=Flash_fwd_kernel_traits<160, 128, 32, 4, false, false, cutlass::bfloat16_t, Flash_kernel_traits<160, 128, 32, 4, cutlass::bfloat16_t>>, Is_dropout=false, Is_causal=false, Is_even_N=false, Is_even_K=false, Return_softmax=false, Params=Flash_fwd_params, Is_Q_split_K=false]" 
(589): here
            instantiation of "void flash::compute_attn<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &) [with Kernel_traits=Flash_fwd_kernel_traits<160, 128, 32, 4, false, false, cutlass::bfloat16_t, Flash_kernel_traits<160, 128, 32, 4, cutlass::bfloat16_t>>, Is_dropout=false, Is_causal=false, Is_even_N=false, Is_even_K=false, Return_softmax=false, Params=Flash_fwd_params]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(15): here
            instantiation of "void flash_fwd_kernel<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax>(Flash_fwd_params) [with Kernel_traits=Flash_fwd_kernel_traits<160, 128, 32, 4, false, false, cutlass::bfloat16_t, Flash_kernel_traits<160, 128, 32, 4, cutlass::bfloat16_t>>, Is_dropout=false, Is_causal=false, Is_even_N=false, Is_even_K=false, Return_softmax=false]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(34): here
            instantiation of "void run_flash_fwd<Kernel_traits,Is_dropout,Is_causal>(Flash_fwd_params &, cudaStream_t) [with Kernel_traits=Flash_fwd_kernel_traits<160, 128, 32, 4, false, false, cutlass::bfloat16_t, Flash_kernel_traits<160, 128, 32, 4, cutlass::bfloat16_t>>, Is_dropout=false, Is_causal=false]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(155): here
            instantiation of "void run_mha_fwd_hdim160<T>(Flash_fwd_params &, cudaStream_t) [with T=cutlass::bfloat16_t]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_hdim160_bf16_sm80.cu(16): here

/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_kernel.h(276): warning: variable "tQrQ" was declared but never referenced
          detected during:
            instantiation of "void flash::compute_attn_1rowblock<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params,Is_Q_split_K>(const Params &, int, int, int) [with Kernel_traits=Flash_fwd_kernel_traits<160, 128, 32, 4, false, false, cutlass::bfloat16_t, Flash_kernel_traits<160, 128, 32, 4, cutlass::bfloat16_t>>, Is_dropout=false, Is_causal=false, Is_even_N=false, Is_even_K=false, Return_softmax=false, Params=Flash_fwd_params, Is_Q_split_K=false]" 
(589): here
            instantiation of "void flash::compute_attn<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &) [with Kernel_traits=Flash_fwd_kernel_traits<160, 128, 32, 4, false, false, cutlass::bfloat16_t, Flash_kernel_traits<160, 128, 32, 4, cutlass::bfloat16_t>>, Is_dropout=false, Is_causal=false, Is_even_N=false, Is_even_K=false, Return_softmax=false, Params=Flash_fwd_params]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(15): here
            instantiation of "void flash_fwd_kernel<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax>(Flash_fwd_params) [with Kernel_traits=Flash_fwd_kernel_traits<160, 128, 32, 4, false, false, cutlass::bfloat16_t, Flash_kernel_traits<160, 128, 32, 4, cutlass::bfloat16_t>>, Is_dropout=false, Is_causal=false, Is_even_N=false, Is_even_K=false, Return_softmax=false]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(34): here
            instantiation of "void run_flash_fwd<Kernel_traits,Is_dropout,Is_causal>(Flash_fwd_params &, cudaStream_t) [with Kernel_traits=Flash_fwd_kernel_traits<160, 128, 32, 4, false, false, cutlass::bfloat16_t, Flash_kernel_traits<160, 128, 32, 4, cutlass::bfloat16_t>>, Is_dropout=false, Is_causal=false]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(155): here
            instantiation of "void run_mha_fwd_hdim160<T>(Flash_fwd_params &, cudaStream_t) [with T=cutlass::bfloat16_t]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_hdim160_bf16_sm80.cu(16): here

ptxas info    : 2 bytes gmem
ptxas info    : Compiling entry function '_Z16flash_fwd_kernelI23Flash_fwd_kernel_traitsILi160ELi128ELi32ELi4ELb0ELb0EN7cutlass10bfloat16_tE19Flash_kernel_traitsILi160ELi128ELi32ELi4ES2_EELb0ELb0ELb0ELb0ELb0EEv16Flash_fwd_params' for 'sm_80'
ptxas info    : Function properties for _Z16flash_fwd_kernelI23Flash_fwd_kernel_traitsILi160ELi128ELi32ELi4ELb0ELb0EN7cutlass10bfloat16_tE19Flash_kernel_traitsILi160ELi128ELi32ELi4ES2_EELb0ELb0ELb0ELb0ELb0EEv16Flash_fwd_params
    32 bytes stack frame, 48 bytes spill stores, 48 bytes spill loads
ptxas info    : Used 255 registers, 584 bytes cmem[0]
ptxas info    : Compiling entry function '_Z16flash_fwd_kernelI23Flash_fwd_kernel_traitsILi160ELi128ELi32ELi4ELb0ELb0EN7cutlass10bfloat16_tE19Flash_kernel_traitsILi160ELi128ELi32ELi4ES2_EELb0ELb0ELb0ELb1ELb0EEv16Flash_fwd_params' for 'sm_80'
ptxas info    : Function properties for _Z16flash_fwd_kernelI23Flash_fwd_kernel_traitsILi160ELi128ELi32ELi4ELb0ELb0EN7cutlass10bfloat16_tE19Flash_kernel_traitsILi160ELi128ELi32ELi4ES2_EELb0ELb0ELb0ELb1ELb0EEv16Flash_fwd_params
    48 bytes stack frame, 80 bytes spill stores, 64 bytes spill loads
ptxas info    : Used 255 registers, 584 bytes cmem[0]
ptxas info    : Compiling entry function '_Z16flash_fwd_kernelI23Flash_fwd_kernel_traitsILi160ELi128ELi32ELi4ELb0ELb0EN7cutlass10bfloat16_tE19Flash_kernel_traitsILi160ELi128ELi32ELi4ES2_EELb0ELb0ELb1ELb0ELb0EEv16Flash_fwd_params' for 'sm_80'
ptxas info    : Function properties for _Z16flash_fwd_kernelI23Flash_fwd_kernel_traitsILi160ELi128ELi32ELi4ELb0ELb0EN7cutlass10bfloat16_tE19Flash_kernel_traitsILi160ELi128ELi32ELi4ES2_EELb0ELb0ELb1ELb0ELb0EEv16Flash_fwd_params
    24 bytes stack frame, 36 bytes spill stores, 36 bytes spill loads
ptxas info    : Used 255 registers, 584 bytes cmem[0]
ptxas info    : Compiling entry function '_Z16flash_fwd_kernelI23Flash_fwd_kernel_traitsILi160ELi128ELi32ELi4ELb0ELb0EN7cutlass10bfloat16_tE19Flash_kernel_traitsILi160ELi128ELi32ELi4ES2_EELb0ELb0ELb1ELb1ELb0EEv16Flash_fwd_params' for 'sm_80'
ptxas info    : Function properties for _Z16flash_fwd_kernelI23Flash_fwd_kernel_traitsILi160ELi128ELi32ELi4ELb0ELb0EN7cutlass10bfloat16_tE19Flash_kernel_traitsILi160ELi128ELi32ELi4ES2_EELb0ELb0ELb1ELb1ELb0EEv16Flash_fwd_params
    24 bytes stack frame, 40 bytes spill stores, 40 bytes spill loads
ptxas info    : Used 255 registers, 584 bytes cmem[0]
ptxas info    : Compiling entry function '_Z16flash_fwd_kernelI23Flash_fwd_kernel_traitsILi160ELi128ELi64ELi8ELb0ELb0EN7cutlass10bfloat16_tE19Flash_kernel_traitsILi160ELi128ELi64ELi8ES2_EELb0ELb0ELb0ELb0ELb0EEv16Flash_fwd_params' for 'sm_80'
ptxas info    : Function properties for _Z16flash_fwd_kernelI23Flash_fwd_kernel_traitsILi160ELi128ELi64ELi8ELb0ELb0EN7cutlass10bfloat16_tE19Flash_kernel_traitsILi160ELi128ELi64ELi8ES2_EELb0ELb0ELb0ELb0ELb0EEv16Flash_fwd_params
    0 bytes stack frame, 0 bytes spill stores, 0 bytes spill loads
ptxas info    : Used 165 registers, 584 bytes cmem[0]
ptxas info    : Compiling entry function '_Z16flash_fwd_kernelI23Flash_fwd_kernel_traitsILi160ELi128ELi64ELi8ELb0ELb0EN7cutlass10bfloat16_tE19Flash_kernel_traitsILi160ELi128ELi64ELi8ES2_EELb0ELb0ELb0ELb1ELb0EEv16Flash_fwd_params' for 'sm_80'
ptxas info    : Function properties for _Z16flash_fwd_kernelI23Flash_fwd_kernel_traitsILi160ELi128ELi64ELi8ELb0ELb0EN7cutlass10bfloat16_tE19Flash_kernel_traitsILi160ELi128ELi64ELi8ES2_EELb0ELb0ELb0ELb1ELb0EEv16Flash_fwd_params
    0 bytes stack frame, 0 bytes spill stores, 0 bytes spill loads
ptxas info    : Used 163 registers, 584 bytes cmem[0]
ptxas info    : Compiling entry function '_Z16flash_fwd_kernelI23Flash_fwd_kernel_traitsILi160ELi128ELi64ELi8ELb0ELb0EN7cutlass10bfloat16_tE19Flash_kernel_traitsILi160ELi128ELi64ELi8ES2_EELb0ELb0ELb1ELb0ELb0EEv16Flash_fwd_params' for 'sm_80'
ptxas info    : Function properties for _Z16flash_fwd_kernelI23Flash_fwd_kernel_traitsILi160ELi128ELi64ELi8ELb0ELb0EN7cutlass10bfloat16_tE19Flash_kernel_traitsILi160ELi128ELi64ELi8ES2_EELb0ELb0ELb1ELb0ELb0EEv16Flash_fwd_params
    0 bytes stack frame, 0 bytes spill stores, 0 bytes spill loads
ptxas info    : Used 171 registers, 584 bytes cmem[0]
ptxas info    : Compiling entry function '_Z16flash_fwd_kernelI23Flash_fwd_kernel_traitsILi160ELi128ELi64ELi8ELb0ELb0EN7cutlass10bfloat16_tE19Flash_kernel_traitsILi160ELi128ELi64ELi8ES2_EELb0ELb0ELb1ELb1ELb0EEv16Flash_fwd_params' for 'sm_80'
ptxas info    : Function properties for _Z16flash_fwd_kernelI23Flash_fwd_kernel_traitsILi160ELi128ELi64ELi8ELb0ELb0EN7cutlass10bfloat16_tE19Flash_kernel_traitsILi160ELi128ELi64ELi8ES2_EELb0ELb0ELb1ELb1ELb0EEv16Flash_fwd_params
    0 bytes stack frame, 0 bytes spill stores, 0 bytes spill loads
ptxas info    : Used 170 registers, 584 bytes cmem[0]
ptxas info    : Compiling entry function '_Z16flash_fwd_kernelI23Flash_fwd_kernel_traitsILi160ELi128ELi32ELi4ELb0ELb0EN7cutlass10bfloat16_tE19Flash_kernel_traitsILi160ELi128ELi32ELi4ES2_EELb0ELb1ELb0ELb0ELb0EEv16Flash_fwd_params' for 'sm_80'
ptxas info    : Function properties for _Z16flash_fwd_kernelI23Flash_fwd_kernel_traitsILi160ELi128ELi32ELi4ELb0ELb0EN7cutlass10bfloat16_tE19Flash_kernel_traitsILi160ELi128ELi32ELi4ES2_EELb0ELb1ELb0ELb0ELb0EEv16Flash_fwd_params
    56 bytes stack frame, 132 bytes spill stores, 112 bytes spill loads
ptxas info    : Used 255 registers, 584 bytes cmem[0]
ptxas info    : Compiling entry function '_Z16flash_fwd_kernelI23Flash_fwd_kernel_traitsILi160ELi128ELi32ELi4ELb0ELb0EN7cutlass10bfloat16_tE19Flash_kernel_traitsILi160ELi128ELi32ELi4ES2_EELb0ELb1ELb0ELb1ELb0EEv16Flash_fwd_params' for 'sm_80'
ptxas info    : Function properties for _Z16flash_fwd_kernelI23Flash_fwd_kernel_traitsILi160ELi128ELi32ELi4ELb0ELb0EN7cutlass10bfloat16_tE19Flash_kernel_traitsILi160ELi128ELi32ELi4ES2_EELb0ELb1ELb0ELb1ELb0EEv16Flash_fwd_params
    40 bytes stack frame, 156 bytes spill stores, 96 bytes spill loads
ptxas info    : Used 255 registers, 584 bytes cmem[0]
ptxas info    : Compiling entry function '_Z16flash_fwd_kernelI23Flash_fwd_kernel_traitsILi160ELi128ELi32ELi4ELb0ELb0EN7cutlass10bfloat16_tE19Flash_kernel_traitsILi160ELi128ELi32ELi4ES2_EELb0ELb1ELb1ELb0ELb0EEv16Flash_fwd_params' for 'sm_80'
ptxas info    : Function properties for _Z16flash_fwd_kernelI23Flash_fwd_kernel_traitsILi160ELi128ELi32ELi4ELb0ELb0EN7cutlass10bfloat16_tE19Flash_kernel_traitsILi160ELi128ELi32ELi4ES2_EELb0ELb1ELb1ELb0ELb0EEv16Flash_fwd_params
    24 bytes stack frame, 84 bytes spill stores, 44 bytes spill loads
ptxas info    : Used 255 registers, 584 bytes cmem[0]
ptxas info    : Compiling entry function '_Z16flash_fwd_kernelI23Flash_fwd_kernel_traitsILi160ELi128ELi32ELi4ELb0ELb0EN7cutlass10bfloat16_tE19Flash_kernel_traitsILi160ELi128ELi32ELi4ES2_EELb0ELb1ELb1ELb1ELb0EEv16Flash_fwd_params' for 'sm_80'
ptxas info    : Function properties for _Z16flash_fwd_kernelI23Flash_fwd_kernel_traitsILi160ELi128ELi32ELi4ELb0ELb0EN7cutlass10bfloat16_tE19Flash_kernel_traitsILi160ELi128ELi32ELi4ES2_EELb0ELb1ELb1ELb1ELb0EEv16Flash_fwd_params
    56 bytes stack frame, 176 bytes spill stores, 124 bytes spill loads
ptxas info    : Used 255 registers, 584 bytes cmem[0]
ptxas info    : Compiling entry function '_Z16flash_fwd_kernelI23Flash_fwd_kernel_traitsILi160ELi64ELi64ELi4ELb0ELb0EN7cutlass10bfloat16_tE19Flash_kernel_traitsILi160ELi64ELi64ELi4ES2_EELb0ELb1ELb0ELb0ELb0EEv16Flash_fwd_params' for 'sm_80'
ptxas info    : Function properties for _Z16flash_fwd_kernelI23Flash_fwd_kernel_traitsILi160ELi64ELi64ELi4ELb0ELb0EN7cutlass10bfloat16_tE19Flash_kernel_traitsILi160ELi64ELi64ELi4ES2_EELb0ELb1ELb0ELb0ELb0EEv16Flash_fwd_params
    0 bytes stack frame, 0 bytes spill stores, 0 bytes spill loads
ptxas info    : Used 171 registers, 584 bytes cmem[0]
ptxas info    : Compiling entry function '_Z16flash_fwd_kernelI23Flash_fwd_kernel_traitsILi160ELi64ELi64ELi4ELb0ELb0EN7cutlass10bfloat16_tE19Flash_kernel_traitsILi160ELi64ELi64ELi4ES2_EELb0ELb1ELb0ELb1ELb0EEv16Flash_fwd_params' for 'sm_80'
ptxas info    : Function properties for _Z16flash_fwd_kernelI23Flash_fwd_kernel_traitsILi160ELi64ELi64ELi4ELb0ELb0EN7cutlass10bfloat16_tE19Flash_kernel_traitsILi160ELi64ELi64ELi4ES2_EELb0ELb1ELb0ELb1ELb0EEv16Flash_fwd_params
    0 bytes stack frame, 0 bytes spill stores, 0 bytes spill loads
ptxas info    : Used 167 registers, 584 bytes cmem[0]
ptxas info    : Compiling entry function '_Z16flash_fwd_kernelI23Flash_fwd_kernel_traitsILi160ELi64ELi64ELi4ELb0ELb0EN7cutlass10bfloat16_tE19Flash_kernel_traitsILi160ELi64ELi64ELi4ES2_EELb0ELb1ELb1ELb0ELb0EEv16Flash_fwd_params' for 'sm_80'
ptxas info    : Function properties for _Z16flash_fwd_kernelI23Flash_fwd_kernel_traitsILi160ELi64ELi64ELi4ELb0ELb0EN7cutlass10bfloat16_tE19Flash_kernel_traitsILi160ELi64ELi64ELi4ES2_EELb0ELb1ELb1ELb0ELb0EEv16Flash_fwd_params
    0 bytes stack frame, 0 bytes spill stores, 0 bytes spill loads
ptxas info    : Used 168 registers, 584 bytes cmem[0]
ptxas info    : Compiling entry function '_Z16flash_fwd_kernelI23Flash_fwd_kernel_traitsILi160ELi64ELi64ELi4ELb0ELb0EN7cutlass10bfloat16_tE19Flash_kernel_traitsILi160ELi64ELi64ELi4ES2_EELb0ELb1ELb1ELb1ELb0EEv16Flash_fwd_params' for 'sm_80'
ptxas info    : Function properties for _Z16flash_fwd_kernelI23Flash_fwd_kernel_traitsILi160ELi64ELi64ELi4ELb0ELb0EN7cutlass10bfloat16_tE19Flash_kernel_traitsILi160ELi64ELi64ELi4ES2_EELb0ELb1ELb1ELb1ELb0EEv16Flash_fwd_params
    0 bytes stack frame, 0 bytes spill stores, 0 bytes spill loads
ptxas info    : Used 172 registers, 584 bytes cmem[0]
ptxas info    : Compiling entry function '_Z16flash_fwd_kernelI23Flash_fwd_kernel_traitsILi160ELi128ELi32ELi4ELb0ELb0EN7cutlass10bfloat16_tE19Flash_kernel_traitsILi160ELi128ELi32ELi4ES2_EELb1ELb0ELb0ELb0ELb0EEv16Flash_fwd_params' for 'sm_80'
ptxas info    : Function properties for _Z16flash_fwd_kernelI23Flash_fwd_kernel_traitsILi160ELi128ELi32ELi4ELb0ELb0EN7cutlass10bfloat16_tE19Flash_kernel_traitsILi160ELi128ELi32ELi4ES2_EELb1ELb0ELb0ELb0ELb0EEv16Flash_fwd_params
    32 bytes stack frame, 32 bytes spill stores, 32 bytes spill loads
ptxas info    : Used 255 registers, 584 bytes cmem[0]
ptxas info    : Compiling entry function '_Z16flash_fwd_kernelI23Flash_fwd_kernel_traitsILi160ELi128ELi32ELi4ELb0ELb0EN7cutlass10bfloat16_tE19Flash_kernel_traitsILi160ELi128ELi32ELi4ES2_EELb1ELb0ELb0ELb0ELb1EEv16Flash_fwd_params' for 'sm_80'
ptxas info    : Function properties for _Z16flash_fwd_kernelI23Flash_fwd_kernel_traitsILi160ELi128ELi32ELi4ELb0ELb0EN7cutlass10bfloat16_tE19Flash_kernel_traitsILi160ELi128ELi32ELi4ES2_EELb1ELb0ELb0ELb0ELb1EEv16Flash_fwd_params
    32 bytes stack frame, 36 bytes spill stores, 36 bytes spill loads
ptxas info    : Used 255 registers, 584 bytes cmem[0]
ptxas info    : Compiling entry function '_Z16flash_fwd_kernelI23Flash_fwd_kernel_traitsILi160ELi128ELi32ELi4ELb0ELb0EN7cutlass10bfloat16_tE19Flash_kernel_traitsILi160ELi128ELi32ELi4ES2_EELb1ELb0ELb0ELb1ELb0EEv16Flash_fwd_params' for 'sm_80'
ptxas info    : Function properties for _Z16flash_fwd_kernelI23Flash_fwd_kernel_traitsILi160ELi128ELi32ELi4ELb0ELb0EN7cutlass10bfloat16_tE19Flash_kernel_traitsILi160ELi128ELi32ELi4ES2_EELb1ELb0ELb0ELb1ELb0EEv16Flash_fwd_params
    32 bytes stack frame, 32 bytes spill stores, 32 bytes spill loads
ptxas info    : Used 255 registers, 584 bytes cmem[0]
ptxas info    : Compiling entry function '_Z16flash_fwd_kernelI23Flash_fwd_kernel_traitsILi160ELi128ELi32ELi4ELb0ELb0EN7cutlass10bfloat16_tE19Flash_kernel_traitsILi160ELi128ELi32ELi4ES2_EELb1ELb0ELb0ELb1ELb1EEv16Flash_fwd_params' for 'sm_80'
ptxas info    : Function properties for _Z16flash_fwd_kernelI23Flash_fwd_kernel_traitsILi160ELi128ELi32ELi4ELb0ELb0EN7cutlass10bfloat16_tE19Flash_kernel_traitsILi160ELi128ELi32ELi4ES2_EELb1ELb0ELb0ELb1ELb1EEv16Flash_fwd_params
    32 bytes stack frame, 32 bytes spill stores, 32 bytes spill loads
ptxas info    : Used 255 registers, 584 bytes cmem[0]
ptxas info    : Compiling entry function '_Z16flash_fwd_kernelI23Flash_fwd_kernel_traitsILi160ELi128ELi32ELi4ELb0ELb0EN7cutlass10bfloat16_tE19Flash_kernel_traitsILi160ELi128ELi32ELi4ES2_EELb1ELb0ELb1ELb0ELb0EEv16Flash_fwd_params' for 'sm_80'
ptxas info    : Function properties for _Z16flash_fwd_kernelI23Flash_fwd_kernel_traitsILi160ELi128ELi32ELi4ELb0ELb0EN7cutlass10bfloat16_tE19Flash_kernel_traitsILi160ELi128ELi32ELi4ES2_EELb1ELb0ELb1ELb0ELb0EEv16Flash_fwd_params
    32 bytes stack frame, 32 bytes spill stores, 32 bytes spill loads
ptxas info    : Used 255 registers, 584 bytes cmem[0]
ptxas info    : Compiling entry function '_Z16flash_fwd_kernelI23Flash_fwd_kernel_traitsILi160ELi128ELi32ELi4ELb0ELb0EN7cutlass10bfloat16_tE19Flash_kernel_traitsILi160ELi128ELi32ELi4ES2_EELb1ELb0ELb1ELb0ELb1EEv16Flash_fwd_params' for 'sm_80'
ptxas info    : Function properties for _Z16flash_fwd_kernelI23Flash_fwd_kernel_traitsILi160ELi128ELi32ELi4ELb0ELb0EN7cutlass10bfloat16_tE19Flash_kernel_traitsILi160ELi128ELi32ELi4ES2_EELb1ELb0ELb1ELb0ELb1EEv16Flash_fwd_params
    8 bytes stack frame, 8 bytes spill stores, 8 bytes spill loads
ptxas info    : Used 255 registers, 584 bytes cmem[0]
ptxas info    : Compiling entry function '_Z16flash_fwd_kernelI23Flash_fwd_kernel_traitsILi160ELi128ELi32ELi4ELb0ELb0EN7cutlass10bfloat16_tE19Flash_kernel_traitsILi160ELi128ELi32ELi4ES2_EELb1ELb0ELb1ELb1ELb0EEv16Flash_fwd_params' for 'sm_80'
ptxas info    : Function properties for _Z16flash_fwd_kernelI23Flash_fwd_kernel_traitsILi160ELi128ELi32ELi4ELb0ELb0EN7cutlass10bfloat16_tE19Flash_kernel_traitsILi160ELi128ELi32ELi4ES2_EELb1ELb0ELb1ELb1ELb0EEv16Flash_fwd_params
    16 bytes stack frame, 16 bytes spill stores, 16 bytes spill loads
ptxas info    : Used 255 registers, 584 bytes cmem[0]
ptxas info    : Compiling entry function '_Z16flash_fwd_kernelI23Flash_fwd_kernel_traitsILi160ELi128ELi32ELi4ELb0ELb0EN7cutlass10bfloat16_tE19Flash_kernel_traitsILi160ELi128ELi32ELi4ES2_EELb1ELb0ELb1ELb1ELb1EEv16Flash_fwd_params' for 'sm_80'
ptxas info    : Function properties for _Z16flash_fwd_kernelI23Flash_fwd_kernel_traitsILi160ELi128ELi32ELi4ELb0ELb0EN7cutlass10bfloat16_tE19Flash_kernel_traitsILi160ELi128ELi32ELi4ES2_EELb1ELb0ELb1ELb1ELb1EEv16Flash_fwd_params
    8 bytes stack frame, 8 bytes spill stores, 8 bytes spill loads
ptxas info    : Used 255 registers, 584 bytes cmem[0]
ptxas info    : Compiling entry function '_Z16flash_fwd_kernelI23Flash_fwd_kernel_traitsILi160ELi128ELi64ELi8ELb0ELb0EN7cutlass10bfloat16_tE19Flash_kernel_traitsILi160ELi128ELi64ELi8ES2_EELb1ELb0ELb0ELb0ELb0EEv16Flash_fwd_params' for 'sm_80'
ptxas info    : Function properties for _Z16flash_fwd_kernelI23Flash_fwd_kernel_traitsILi160ELi128ELi64ELi8ELb0ELb0EN7cutlass10bfloat16_tE19Flash_kernel_traitsILi160ELi128ELi64ELi8ES2_EELb1ELb0ELb0ELb0ELb0EEv16Flash_fwd_params
    0 bytes stack frame, 0 bytes spill stores, 0 bytes spill loads
ptxas info    : Used 176 registers, 584 bytes cmem[0]
ptxas info    : Compiling entry function '_Z16flash_fwd_kernelI23Flash_fwd_kernel_traitsILi160ELi128ELi64ELi8ELb0ELb0EN7cutlass10bfloat16_tE19Flash_kernel_traitsILi160ELi128ELi64ELi8ES2_EELb1ELb0ELb0ELb0ELb1EEv16Flash_fwd_params' for 'sm_80'
ptxas info    : Function properties for _Z16flash_fwd_kernelI23Flash_fwd_kernel_traitsILi160ELi128ELi64ELi8ELb0ELb0EN7cutlass10bfloat16_tE19Flash_kernel_traitsILi160ELi128ELi64ELi8ES2_EELb1ELb0ELb0ELb0ELb1EEv16Flash_fwd_params
    0 bytes stack frame, 0 bytes spill stores, 0 bytes spill loads
ptxas info    : Used 176 registers, 584 bytes cmem[0]
ptxas info    : Compiling entry function '_Z16flash_fwd_kernelI23Flash_fwd_kernel_traitsILi160ELi128ELi64ELi8ELb0ELb0EN7cutlass10bfloat16_tE19Flash_kernel_traitsILi160ELi128ELi64ELi8ES2_EELb1ELb0ELb0ELb1ELb0EEv16Flash_fwd_params' for 'sm_80'
ptxas info    : Function properties for _Z16flash_fwd_kernelI23Flash_fwd_kernel_traitsILi160ELi128ELi64ELi8ELb0ELb0EN7cutlass10bfloat16_tE19Flash_kernel_traitsILi160ELi128ELi64ELi8ES2_EELb1ELb0ELb0ELb1ELb0EEv16Flash_fwd_params
    0 bytes stack frame, 0 bytes spill stores, 0 bytes spill loads
ptxas info    : Used 176 registers, 584 bytes cmem[0]
ptxas info    : Compiling entry function '_Z16flash_fwd_kernelI23Flash_fwd_kernel_traitsILi160ELi128ELi64ELi8ELb0ELb0EN7cutlass10bfloat16_tE19Flash_kernel_traitsILi160ELi128ELi64ELi8ES2_EELb1ELb0ELb0ELb1ELb1EEv16Flash_fwd_params' for 'sm_80'
ptxas info    : Function properties for _Z16flash_fwd_kernelI23Flash_fwd_kernel_traitsILi160ELi128ELi64ELi8ELb0ELb0EN7cutlass10bfloat16_tE19Flash_kernel_traitsILi160ELi128ELi64ELi8ES2_EELb1ELb0ELb0ELb1ELb1EEv16Flash_fwd_params
    0 bytes stack frame, 0 bytes spill stores, 0 bytes spill loads
ptxas info    : Used 174 registers, 584 bytes cmem[0]
ptxas info    : Compiling entry function '_Z16flash_fwd_kernelI23Flash_fwd_kernel_traitsILi160ELi128ELi64ELi8ELb0ELb0EN7cutlass10bfloat16_tE19Flash_kernel_traitsILi160ELi128ELi64ELi8ES2_EELb1ELb0ELb1ELb0ELb0EEv16Flash_fwd_params' for 'sm_80'
ptxas info    : Function properties for _Z16flash_fwd_kernelI23Flash_fwd_kernel_traitsILi160ELi128ELi64ELi8ELb0ELb0EN7cutlass10bfloat16_tE19Flash_kernel_traitsILi160ELi128ELi64ELi8ES2_EELb1ELb0ELb1ELb0ELb0EEv16Flash_fwd_params
    0 bytes stack frame, 0 bytes spill stores, 0 bytes spill loads
ptxas info    : Used 172 registers, 584 bytes cmem[0]
ptxas info    : Compiling entry function '_Z16flash_fwd_kernelI23Flash_fwd_kernel_traitsILi160ELi128ELi64ELi8ELb0ELb0EN7cutlass10bfloat16_tE19Flash_kernel_traitsILi160ELi128ELi64ELi8ES2_EELb1ELb0ELb1ELb0ELb1EEv16Flash_fwd_params' for 'sm_80'
ptxas info    : Function properties for _Z16flash_fwd_kernelI23Flash_fwd_kernel_traitsILi160ELi128ELi64ELi8ELb0ELb0EN7cutlass10bfloat16_tE19Flash_kernel_traitsILi160ELi128ELi64ELi8ES2_EELb1ELb0ELb1ELb0ELb1EEv16Flash_fwd_params
    0 bytes stack frame, 0 bytes spill stores, 0 bytes spill loads
ptxas info    : Used 176 registers, 584 bytes cmem[0]
ptxas info    : Compiling entry function '_Z16flash_fwd_kernelI23Flash_fwd_kernel_traitsILi160ELi128ELi64ELi8ELb0ELb0EN7cutlass10bfloat16_tE19Flash_kernel_traitsILi160ELi128ELi64ELi8ES2_EELb1ELb0ELb1ELb1ELb0EEv16Flash_fwd_params' for 'sm_80'
ptxas info    : Function properties for _Z16flash_fwd_kernelI23Flash_fwd_kernel_traitsILi160ELi128ELi64ELi8ELb0ELb0EN7cutlass10bfloat16_tE19Flash_kernel_traitsILi160ELi128ELi64ELi8ES2_EELb1ELb0ELb1ELb1ELb0EEv16Flash_fwd_params
    0 bytes stack frame, 0 bytes spill stores, 0 bytes spill loads
ptxas info    : Used 168 registers, 584 bytes cmem[0]
ptxas info    : Compiling entry function '_Z16flash_fwd_kernelI23Flash_fwd_kernel_traitsILi160ELi128ELi64ELi8ELb0ELb0EN7cutlass10bfloat16_tE19Flash_kernel_traitsILi160ELi128ELi64ELi8ES2_EELb1ELb0ELb1ELb1ELb1EEv16Flash_fwd_params' for 'sm_80'
ptxas info    : Function properties for _Z16flash_fwd_kernelI23Flash_fwd_kernel_traitsILi160ELi128ELi64ELi8ELb0ELb0EN7cutlass10bfloat16_tE19Flash_kernel_traitsILi160ELi128ELi64ELi8ES2_EELb1ELb0ELb1ELb1ELb1EEv16Flash_fwd_params
    0 bytes stack frame, 0 bytes spill stores, 0 bytes spill loads
ptxas info    : Used 176 registers, 584 bytes cmem[0]
ptxas info    : Compiling entry function '_Z16flash_fwd_kernelI23Flash_fwd_kernel_traitsILi160ELi128ELi32ELi4ELb0ELb0EN7cutlass10bfloat16_tE19Flash_kernel_traitsILi160ELi128ELi32ELi4ES2_EELb1ELb1ELb0ELb0ELb0EEv16Flash_fwd_params' for 'sm_80'
ptxas info    : Function properties for _Z16flash_fwd_kernelI23Flash_fwd_kernel_traitsILi160ELi128ELi32ELi4ELb0ELb0EN7cutlass10bfloat16_tE19Flash_kernel_traitsILi160ELi128ELi32ELi4ES2_EELb1ELb1ELb0ELb0ELb0EEv16Flash_fwd_params
    152 bytes stack frame, 204 bytes spill stores, 268 bytes spill loads
ptxas info    : Used 255 registers, 584 bytes cmem[0]
ptxas info    : Compiling entry function '_Z16flash_fwd_kernelI23Flash_fwd_kernel_traitsILi160ELi128ELi32ELi4ELb0ELb0EN7cutlass10bfloat16_tE19Flash_kernel_traitsILi160ELi128ELi32ELi4ES2_EELb1ELb1ELb0ELb0ELb1EEv16Flash_fwd_params' for 'sm_80'
ptxas info    : Function properties for _Z16flash_fwd_kernelI23Flash_fwd_kernel_traitsILi160ELi128ELi32ELi4ELb0ELb0EN7cutlass10bfloat16_tE19Flash_kernel_traitsILi160ELi128ELi32ELi4ES2_EELb1ELb1ELb0ELb0ELb1EEv16Flash_fwd_params
    232 bytes stack frame, 332 bytes spill stores, 440 bytes spill loads
ptxas info    : Used 255 registers, 584 bytes cmem[0]
ptxas info    : Compiling entry function '_Z16flash_fwd_kernelI23Flash_fwd_kernel_traitsILi160ELi128ELi32ELi4ELb0ELb0EN7cutlass10bfloat16_tE19Flash_kernel_traitsILi160ELi128ELi32ELi4ES2_EELb1ELb1ELb0ELb1ELb0EEv16Flash_fwd_params' for 'sm_80'
ptxas info    : Function properties for _Z16flash_fwd_kernelI23Flash_fwd_kernel_traitsILi160ELi128ELi32ELi4ELb0ELb0EN7cutlass10bfloat16_tE19Flash_kernel_traitsILi160ELi128ELi32ELi4ES2_EELb1ELb1ELb0ELb1ELb0EEv16Flash_fwd_params
    112 bytes stack frame, 252 bytes spill stores, 228 bytes spill loads
ptxas info    : Used 255 registers, 584 bytes cmem[0]
ptxas info    : Compiling entry function '_Z16flash_fwd_kernelI23Flash_fwd_kernel_traitsILi160ELi128ELi32ELi4ELb0ELb0EN7cutlass10bfloat16_tE19Flash_kernel_traitsILi160ELi128ELi32ELi4ES2_EELb1ELb1ELb0ELb1ELb1EEv16Flash_fwd_params' for 'sm_80'
ptxas info    : Function properties for _Z16flash_fwd_kernelI23Flash_fwd_kernel_traitsILi160ELi128ELi32ELi4ELb0ELb0EN7cutlass10bfloat16_tE19Flash_kernel_traitsILi160ELi128ELi32ELi4ES2_EELb1ELb1ELb0ELb1ELb1EEv16Flash_fwd_params
    216 bytes stack frame, 344 bytes spill stores, 360 bytes spill loads
ptxas info    : Used 255 registers, 584 bytes cmem[0]
ptxas info    : Compiling entry function '_Z16flash_fwd_kernelI23Flash_fwd_kernel_traitsILi160ELi128ELi32ELi4ELb0ELb0EN7cutlass10bfloat16_tE19Flash_kernel_traitsILi160ELi128ELi32ELi4ES2_EELb1ELb1ELb1ELb0ELb0EEv16Flash_fwd_params' for 'sm_80'
ptxas info    : Function properties for _Z16flash_fwd_kernelI23Flash_fwd_kernel_traitsILi160ELi128ELi32ELi4ELb0ELb0EN7cutlass10bfloat16_tE19Flash_kernel_traitsILi160ELi128ELi32ELi4ES2_EELb1ELb1ELb1ELb0ELb0EEv16Flash_fwd_params
    88 bytes stack frame, 144 bytes spill stores, 168 bytes spill loads
ptxas info    : Used 255 registers, 584 bytes cmem[0]
ptxas info    : Compiling entry function '_Z16flash_fwd_kernelI23Flash_fwd_kernel_traitsILi160ELi128ELi32ELi4ELb0ELb0EN7cutlass10bfloat16_tE19Flash_kernel_traitsILi160ELi128ELi32ELi4ES2_EELb1ELb1ELb1ELb0ELb1EEv16Flash_fwd_params' for 'sm_80'
ptxas info    : Function properties for _Z16flash_fwd_kernelI23Flash_fwd_kernel_traitsILi160ELi128ELi32ELi4ELb0ELb0EN7cutlass10bfloat16_tE19Flash_kernel_traitsILi160ELi128ELi32ELi4ES2_EELb1ELb1ELb1ELb0ELb1EEv16Flash_fwd_params
    160 bytes stack frame, 240 bytes spill stores, 324 bytes spill loads
ptxas info    : Used 255 registers, 584 bytes cmem[0]
ptxas info    : Compiling entry function '_Z16flash_fwd_kernelI23Flash_fwd_kernel_traitsILi160ELi128ELi32ELi4ELb0ELb0EN7cutlass10bfloat16_tE19Flash_kernel_traitsILi160ELi128ELi32ELi4ES2_EELb1ELb1ELb1ELb1ELb0EEv16Flash_fwd_params' for 'sm_80'
ptxas info    : Function properties for _Z16flash_fwd_kernelI23Flash_fwd_kernel_traitsILi160ELi128ELi32ELi4ELb0ELb0EN7cutlass10bfloat16_tE19Flash_kernel_traitsILi160ELi128ELi32ELi4ES2_EELb1ELb1ELb1ELb1ELb0EEv16Flash_fwd_params
    120 bytes stack frame, 196 bytes spill stores, 236 bytes spill loads
ptxas info    : Used 255 registers, 584 bytes cmem[0]
ptxas info    : Compiling entry function '_Z16flash_fwd_kernelI23Flash_fwd_kernel_traitsILi160ELi128ELi32ELi4ELb0ELb0EN7cutlass10bfloat16_tE19Flash_kernel_traitsILi160ELi128ELi32ELi4ES2_EELb1ELb1ELb1ELb1ELb1EEv16Flash_fwd_params' for 'sm_80'
ptxas info    : Function properties for _Z16flash_fwd_kernelI23Flash_fwd_kernel_traitsILi160ELi128ELi32ELi4ELb0ELb0EN7cutlass10bfloat16_tE19Flash_kernel_traitsILi160ELi128ELi32ELi4ES2_EELb1ELb1ELb1ELb1ELb1EEv16Flash_fwd_params
    208 bytes stack frame, 296 bytes spill stores, 376 bytes spill loads
ptxas info    : Used 255 registers, 584 bytes cmem[0]
ptxas info    : Compiling entry function '_Z16flash_fwd_kernelI23Flash_fwd_kernel_traitsILi160ELi64ELi64ELi4ELb0ELb0EN7cutlass10bfloat16_tE19Flash_kernel_traitsILi160ELi64ELi64ELi4ES2_EELb1ELb1ELb0ELb0ELb0EEv16Flash_fwd_params' for 'sm_80'
ptxas info    : Function properties for _Z16flash_fwd_kernelI23Flash_fwd_kernel_traitsILi160ELi64ELi64ELi4ELb0ELb0EN7cutlass10bfloat16_tE19Flash_kernel_traitsILi160ELi64ELi64ELi4ES2_EELb1ELb1ELb0ELb0ELb0EEv16Flash_fwd_params
    0 bytes stack frame, 0 bytes spill stores, 0 bytes spill loads
ptxas info    : Used 176 registers, 584 bytes cmem[0]
ptxas info    : Compiling entry function '_Z16flash_fwd_kernelI23Flash_fwd_kernel_traitsILi160ELi64ELi64ELi4ELb0ELb0EN7cutlass10bfloat16_tE19Flash_kernel_traitsILi160ELi64ELi64ELi4ES2_EELb1ELb1ELb0ELb0ELb1EEv16Flash_fwd_params' for 'sm_80'
ptxas info    : Function properties for _Z16flash_fwd_kernelI23Flash_fwd_kernel_traitsILi160ELi64ELi64ELi4ELb0ELb0EN7cutlass10bfloat16_tE19Flash_kernel_traitsILi160ELi64ELi64ELi4ES2_EELb1ELb1ELb0ELb0ELb1EEv16Flash_fwd_params
    0 bytes stack frame, 0 bytes spill stores, 0 bytes spill loads
ptxas info    : Used 176 registers, 584 bytes cmem[0]
ptxas info    : Compiling entry function '_Z16flash_fwd_kernelI23Flash_fwd_kernel_traitsILi160ELi64ELi64ELi4ELb0ELb0EN7cutlass10bfloat16_tE19Flash_kernel_traitsILi160ELi64ELi64ELi4ES2_EELb1ELb1ELb0ELb1ELb0EEv16Flash_fwd_params' for 'sm_80'
ptxas info    : Function properties for _Z16flash_fwd_kernelI23Flash_fwd_kernel_traitsILi160ELi64ELi64ELi4ELb0ELb0EN7cutlass10bfloat16_tE19Flash_kernel_traitsILi160ELi64ELi64ELi4ES2_EELb1ELb1ELb0ELb1ELb0EEv16Flash_fwd_params
    0 bytes stack frame, 0 bytes spill stores, 0 bytes spill loads
ptxas info    : Used 182 registers, 584 bytes cmem[0]
ptxas info    : Compiling entry function '_Z16flash_fwd_kernelI23Flash_fwd_kernel_traitsILi160ELi64ELi64ELi4ELb0ELb0EN7cutlass10bfloat16_tE19Flash_kernel_traitsILi160ELi64ELi64ELi4ES2_EELb1ELb1ELb0ELb1ELb1EEv16Flash_fwd_params' for 'sm_80'
ptxas info    : Function properties for _Z16flash_fwd_kernelI23Flash_fwd_kernel_traitsILi160ELi64ELi64ELi4ELb0ELb0EN7cutlass10bfloat16_tE19Flash_kernel_traitsILi160ELi64ELi64ELi4ES2_EELb1ELb1ELb0ELb1ELb1EEv16Flash_fwd_params
    0 bytes stack frame, 0 bytes spill stores, 0 bytes spill loads
ptxas info    : Used 174 registers, 584 bytes cmem[0]
ptxas info    : Compiling entry function '_Z16flash_fwd_kernelI23Flash_fwd_kernel_traitsILi160ELi64ELi64ELi4ELb0ELb0EN7cutlass10bfloat16_tE19Flash_kernel_traitsILi160ELi64ELi64ELi4ES2_EELb1ELb1ELb1ELb0ELb0EEv16Flash_fwd_params' for 'sm_80'
ptxas info    : Function properties for _Z16flash_fwd_kernelI23Flash_fwd_kernel_traitsILi160ELi64ELi64ELi4ELb0ELb0EN7cutlass10bfloat16_tE19Flash_kernel_traitsILi160ELi64ELi64ELi4ES2_EELb1ELb1ELb1ELb0ELb0EEv16Flash_fwd_params
    0 bytes stack frame, 0 bytes spill stores, 0 bytes spill loads
ptxas info    : Used 182 registers, 584 bytes cmem[0]
ptxas info    : Compiling entry function '_Z16flash_fwd_kernelI23Flash_fwd_kernel_traitsILi160ELi64ELi64ELi4ELb0ELb0EN7cutlass10bfloat16_tE19Flash_kernel_traitsILi160ELi64ELi64ELi4ES2_EELb1ELb1ELb1ELb0ELb1EEv16Flash_fwd_params' for 'sm_80'
ptxas info    : Function properties for _Z16flash_fwd_kernelI23Flash_fwd_kernel_traitsILi160ELi64ELi64ELi4ELb0ELb0EN7cutlass10bfloat16_tE19Flash_kernel_traitsILi160ELi64ELi64ELi4ES2_EELb1ELb1ELb1ELb0ELb1EEv16Flash_fwd_params
    0 bytes stack frame, 0 bytes spill stores, 0 bytes spill loads
ptxas info    : Used 178 registers, 584 bytes cmem[0]
ptxas info    : Compiling entry function '_Z16flash_fwd_kernelI23Flash_fwd_kernel_traitsILi160ELi64ELi64ELi4ELb0ELb0EN7cutlass10bfloat16_tE19Flash_kernel_traitsILi160ELi64ELi64ELi4ES2_EELb1ELb1ELb1ELb1ELb0EEv16Flash_fwd_params' for 'sm_80'
ptxas info    : Function properties for _Z16flash_fwd_kernelI23Flash_fwd_kernel_traitsILi160ELi64ELi64ELi4ELb0ELb0EN7cutlass10bfloat16_tE19Flash_kernel_traitsILi160ELi64ELi64ELi4ES2_EELb1ELb1ELb1ELb1ELb0EEv16Flash_fwd_params
    0 bytes stack frame, 0 bytes spill stores, 0 bytes spill loads
ptxas info    : Used 188 registers, 584 bytes cmem[0]
ptxas info    : Compiling entry function '_Z16flash_fwd_kernelI23Flash_fwd_kernel_traitsILi160ELi64ELi64ELi4ELb0ELb0EN7cutlass10bfloat16_tE19Flash_kernel_traitsILi160ELi64ELi64ELi4ES2_EELb1ELb1ELb1ELb1ELb1EEv16Flash_fwd_params' for 'sm_80'
ptxas info    : Function properties for _Z16flash_fwd_kernelI23Flash_fwd_kernel_traitsILi160ELi64ELi64ELi4ELb0ELb0EN7cutlass10bfloat16_tE19Flash_kernel_traitsILi160ELi64ELi64ELi4ES2_EELb1ELb1ELb1ELb1ELb1EEv16Flash_fwd_params
    0 bytes stack frame, 0 bytes spill stores, 0 bytes spill loads
ptxas info    : Used 174 registers, 584 bytes cmem[0]
/anaconda/envs/flashattention/lib/python3.9/site-packages/torch/include/c10/core/SymInt.h(84): warning: integer conversion resulted in a change of sign

[28/37] /usr/local/cuda-11.4/bin/nvcc  -I/home/v-feiychen/flash-attention/csrc/flash_attn -I/home/v-feiychen/flash-attention/csrc/flash_attn/src -I/home/v-feiychen/flash-attention/csrc/cutlass/include -I/anaconda/envs/flashattention/lib/python3.9/site-packages/torch/include -I/anaconda/envs/flashattention/lib/python3.9/site-packages/torch/include/torch/csrc/api/include -I/anaconda/envs/flashattention/lib/python3.9/site-packages/torch/include/TH -I/anaconda/envs/flashattention/lib/python3.9/site-packages/torch/include/THC -I/usr/local/cuda-11.4/include -I/anaconda/envs/flashattention/include/python3.9 -c -c /home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_hdim64_fp16_sm80.cu -o /home/v-feiychen/flash-attention/build/temp.linux-x86_64-cpython-39/csrc/flash_attn/src/flash_fwd_hdim64_fp16_sm80.o -D__CUDA_NO_HALF_OPERATORS__ -D__CUDA_NO_HALF_CONVERSIONS__ -D__CUDA_NO_BFLOAT16_CONVERSIONS__ -D__CUDA_NO_HALF2_OPERATORS__ --expt-relaxed-constexpr --compiler-options ''"'"'-fPIC'"'"'' -O3 -std=c++17 -U__CUDA_NO_HALF_OPERATORS__ -U__CUDA_NO_HALF_CONVERSIONS__ -U__CUDA_NO_HALF2_OPERATORS__ -U__CUDA_NO_BFLOAT16_CONVERSIONS__ --expt-relaxed-constexpr --expt-extended-lambda --use_fast_math --ptxas-options=-v -lineinfo -gencode arch=compute_80,code=sm_80 --threads 4 -DTORCH_API_INCLUDE_EXTENSION_H '-DPYBIND11_COMPILER_TYPE="_gcc"' '-DPYBIND11_STDLIB="_libstdcpp"' '-DPYBIND11_BUILD_ABI="_cxxabi1011"' -DTORCH_EXTENSION_NAME=flash_attn_2_cuda -D_GLIBCXX_USE_CXX11_ABI=0
/anaconda/envs/flashattention/lib/python3.9/site-packages/torch/include/c10/core/SymInt.h(84): warning: integer conversion resulted in a change of sign

/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_kernel.h(140): warning: variable "MMA_M" was declared but never referenced
          detected during:
            instantiation of "void flash::compute_attn_1rowblock<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params,Is_Q_split_K>(const Params &, int, int, int) [with Kernel_traits=Flash_fwd_kernel_traits<64, 128, 64, 4, false, false, cutlass::half_t, Flash_kernel_traits<64, 128, 64, 4, cutlass::half_t>>, Is_dropout=true, Is_causal=true, Is_even_N=true, Is_even_K=true, Return_softmax=true, Params=Flash_fwd_params, Is_Q_split_K=false]" 
(589): here
            instantiation of "void flash::compute_attn<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &) [with Kernel_traits=Flash_fwd_kernel_traits<64, 128, 64, 4, false, false, cutlass::half_t, Flash_kernel_traits<64, 128, 64, 4, cutlass::half_t>>, Is_dropout=true, Is_causal=true, Is_even_N=true, Is_even_K=true, Return_softmax=true, Params=Flash_fwd_params]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(15): here
            instantiation of "void flash_fwd_kernel<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax>(Flash_fwd_params) [with Kernel_traits=Flash_fwd_kernel_traits<64, 128, 64, 4, false, false, cutlass::half_t, Flash_kernel_traits<64, 128, 64, 4, cutlass::half_t>>, Is_dropout=true, Is_causal=true, Is_even_N=true, Is_even_K=true, Return_softmax=true]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(34): here
            instantiation of "void run_flash_fwd<Kernel_traits,Is_dropout,Is_causal>(Flash_fwd_params &, cudaStream_t) [with Kernel_traits=Flash_fwd_kernel_traits<64, 128, 64, 4, false, false, cutlass::half_t, Flash_kernel_traits<64, 128, 64, 4, cutlass::half_t>>, Is_dropout=true, Is_causal=true]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(68): here
            instantiation of "void run_mha_fwd_hdim64<T>(Flash_fwd_params &, cudaStream_t) [with T=cutlass::half_t]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_hdim64_fp16_sm80.cu(25): here

/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_kernel.h(276): warning: variable "tQrQ" was declared but never referenced
          detected during:
            instantiation of "void flash::compute_attn_1rowblock<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params,Is_Q_split_K>(const Params &, int, int, int) [with Kernel_traits=Flash_fwd_kernel_traits<64, 128, 64, 4, false, false, cutlass::half_t, Flash_kernel_traits<64, 128, 64, 4, cutlass::half_t>>, Is_dropout=true, Is_causal=true, Is_even_N=true, Is_even_K=true, Return_softmax=true, Params=Flash_fwd_params, Is_Q_split_K=false]" 
(589): here
            instantiation of "void flash::compute_attn<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &) [with Kernel_traits=Flash_fwd_kernel_traits<64, 128, 64, 4, false, false, cutlass::half_t, Flash_kernel_traits<64, 128, 64, 4, cutlass::half_t>>, Is_dropout=true, Is_causal=true, Is_even_N=true, Is_even_K=true, Return_softmax=true, Params=Flash_fwd_params]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(15): here
            instantiation of "void flash_fwd_kernel<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax>(Flash_fwd_params) [with Kernel_traits=Flash_fwd_kernel_traits<64, 128, 64, 4, false, false, cutlass::half_t, Flash_kernel_traits<64, 128, 64, 4, cutlass::half_t>>, Is_dropout=true, Is_causal=true, Is_even_N=true, Is_even_K=true, Return_softmax=true]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(34): here
            instantiation of "void run_flash_fwd<Kernel_traits,Is_dropout,Is_causal>(Flash_fwd_params &, cudaStream_t) [with Kernel_traits=Flash_fwd_kernel_traits<64, 128, 64, 4, false, false, cutlass::half_t, Flash_kernel_traits<64, 128, 64, 4, cutlass::half_t>>, Is_dropout=true, Is_causal=true]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(68): here
            instantiation of "void run_mha_fwd_hdim64<T>(Flash_fwd_params &, cudaStream_t) [with T=cutlass::half_t]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_hdim64_fp16_sm80.cu(25): here

/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_kernel.h(140): warning: variable "MMA_M" was declared but never referenced
          detected during:
            instantiation of "void flash::compute_attn_1rowblock<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params,Is_Q_split_K>(const Params &, int, int, int) [with Kernel_traits=Flash_fwd_kernel_traits<64, 128, 64, 4, false, false, cutlass::half_t, Flash_kernel_traits<64, 128, 64, 4, cutlass::half_t>>, Is_dropout=true, Is_causal=true, Is_even_N=true, Is_even_K=true, Return_softmax=false, Params=Flash_fwd_params, Is_Q_split_K=false]" 
(589): here
            instantiation of "void flash::compute_attn<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &) [with Kernel_traits=Flash_fwd_kernel_traits<64, 128, 64, 4, false, false, cutlass::half_t, Flash_kernel_traits<64, 128, 64, 4, cutlass::half_t>>, Is_dropout=true, Is_causal=true, Is_even_N=true, Is_even_K=true, Return_softmax=false, Params=Flash_fwd_params]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(15): here
            instantiation of "void flash_fwd_kernel<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax>(Flash_fwd_params) [with Kernel_traits=Flash_fwd_kernel_traits<64, 128, 64, 4, false, false, cutlass::half_t, Flash_kernel_traits<64, 128, 64, 4, cutlass::half_t>>, Is_dropout=true, Is_causal=true, Is_even_N=true, Is_even_K=true, Return_softmax=false]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(34): here
            instantiation of "void run_flash_fwd<Kernel_traits,Is_dropout,Is_causal>(Flash_fwd_params &, cudaStream_t) [with Kernel_traits=Flash_fwd_kernel_traits<64, 128, 64, 4, false, false, cutlass::half_t, Flash_kernel_traits<64, 128, 64, 4, cutlass::half_t>>, Is_dropout=true, Is_causal=true]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(68): here
            instantiation of "void run_mha_fwd_hdim64<T>(Flash_fwd_params &, cudaStream_t) [with T=cutlass::half_t]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_hdim64_fp16_sm80.cu(25): here

/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_kernel.h(276): warning: variable "tQrQ" was declared but never referenced
          detected during:
            instantiation of "void flash::compute_attn_1rowblock<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params,Is_Q_split_K>(const Params &, int, int, int) [with Kernel_traits=Flash_fwd_kernel_traits<64, 128, 64, 4, false, false, cutlass::half_t, Flash_kernel_traits<64, 128, 64, 4, cutlass::half_t>>, Is_dropout=true, Is_causal=true, Is_even_N=true, Is_even_K=true, Return_softmax=false, Params=Flash_fwd_params, Is_Q_split_K=false]" 
(589): here
            instantiation of "void flash::compute_attn<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &) [with Kernel_traits=Flash_fwd_kernel_traits<64, 128, 64, 4, false, false, cutlass::half_t, Flash_kernel_traits<64, 128, 64, 4, cutlass::half_t>>, Is_dropout=true, Is_causal=true, Is_even_N=true, Is_even_K=true, Return_softmax=false, Params=Flash_fwd_params]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(15): here
            instantiation of "void flash_fwd_kernel<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax>(Flash_fwd_params) [with Kernel_traits=Flash_fwd_kernel_traits<64, 128, 64, 4, false, false, cutlass::half_t, Flash_kernel_traits<64, 128, 64, 4, cutlass::half_t>>, Is_dropout=true, Is_causal=true, Is_even_N=true, Is_even_K=true, Return_softmax=false]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(34): here
            instantiation of "void run_flash_fwd<Kernel_traits,Is_dropout,Is_causal>(Flash_fwd_params &, cudaStream_t) [with Kernel_traits=Flash_fwd_kernel_traits<64, 128, 64, 4, false, false, cutlass::half_t, Flash_kernel_traits<64, 128, 64, 4, cutlass::half_t>>, Is_dropout=true, Is_causal=true]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(68): here
            instantiation of "void run_mha_fwd_hdim64<T>(Flash_fwd_params &, cudaStream_t) [with T=cutlass::half_t]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_hdim64_fp16_sm80.cu(25): here

/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_kernel.h(140): warning: variable "MMA_M" was declared but never referenced
          detected during:
            instantiation of "void flash::compute_attn_1rowblock<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params,Is_Q_split_K>(const Params &, int, int, int) [with Kernel_traits=Flash_fwd_kernel_traits<64, 128, 64, 4, false, false, cutlass::half_t, Flash_kernel_traits<64, 128, 64, 4, cutlass::half_t>>, Is_dropout=true, Is_causal=true, Is_even_N=true, Is_even_K=false, Return_softmax=true, Params=Flash_fwd_params, Is_Q_split_K=false]" 
(589): here
            instantiation of "void flash::compute_attn<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &) [with Kernel_traits=Flash_fwd_kernel_traits<64, 128, 64, 4, false, false, cutlass::half_t, Flash_kernel_traits<64, 128, 64, 4, cutlass::half_t>>, Is_dropout=true, Is_causal=true, Is_even_N=true, Is_even_K=false, Return_softmax=true, Params=Flash_fwd_params]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(15): here
            instantiation of "void flash_fwd_kernel<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax>(Flash_fwd_params) [with Kernel_traits=Flash_fwd_kernel_traits<64, 128, 64, 4, false, false, cutlass::half_t, Flash_kernel_traits<64, 128, 64, 4, cutlass::half_t>>, Is_dropout=true, Is_causal=true, Is_even_N=true, Is_even_K=false, Return_softmax=true]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(34): here
            instantiation of "void run_flash_fwd<Kernel_traits,Is_dropout,Is_causal>(Flash_fwd_params &, cudaStream_t) [with Kernel_traits=Flash_fwd_kernel_traits<64, 128, 64, 4, false, false, cutlass::half_t, Flash_kernel_traits<64, 128, 64, 4, cutlass::half_t>>, Is_dropout=true, Is_causal=true]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(68): here
            instantiation of "void run_mha_fwd_hdim64<T>(Flash_fwd_params &, cudaStream_t) [with T=cutlass::half_t]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_hdim64_fp16_sm80.cu(25): here

/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_kernel.h(276): warning: variable "tQrQ" was declared but never referenced
          detected during:
            instantiation of "void flash::compute_attn_1rowblock<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params,Is_Q_split_K>(const Params &, int, int, int) [with Kernel_traits=Flash_fwd_kernel_traits<64, 128, 64, 4, false, false, cutlass::half_t, Flash_kernel_traits<64, 128, 64, 4, cutlass::half_t>>, Is_dropout=true, Is_causal=true, Is_even_N=true, Is_even_K=false, Return_softmax=true, Params=Flash_fwd_params, Is_Q_split_K=false]" 
(589): here
            instantiation of "void flash::compute_attn<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &) [with Kernel_traits=Flash_fwd_kernel_traits<64, 128, 64, 4, false, false, cutlass::half_t, Flash_kernel_traits<64, 128, 64, 4, cutlass::half_t>>, Is_dropout=true, Is_causal=true, Is_even_N=true, Is_even_K=false, Return_softmax=true, Params=Flash_fwd_params]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(15): here
            instantiation of "void flash_fwd_kernel<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax>(Flash_fwd_params) [with Kernel_traits=Flash_fwd_kernel_traits<64, 128, 64, 4, false, false, cutlass::half_t, Flash_kernel_traits<64, 128, 64, 4, cutlass::half_t>>, Is_dropout=true, Is_causal=true, Is_even_N=true, Is_even_K=false, Return_softmax=true]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(34): here
            instantiation of "void run_flash_fwd<Kernel_traits,Is_dropout,Is_causal>(Flash_fwd_params &, cudaStream_t) [with Kernel_traits=Flash_fwd_kernel_traits<64, 128, 64, 4, false, false, cutlass::half_t, Flash_kernel_traits<64, 128, 64, 4, cutlass::half_t>>, Is_dropout=true, Is_causal=true]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(68): here
            instantiation of "void run_mha_fwd_hdim64<T>(Flash_fwd_params &, cudaStream_t) [with T=cutlass::half_t]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_hdim64_fp16_sm80.cu(25): here

/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_kernel.h(140): warning: variable "MMA_M" was declared but never referenced
          detected during:
            instantiation of "void flash::compute_attn_1rowblock<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params,Is_Q_split_K>(const Params &, int, int, int) [with Kernel_traits=Flash_fwd_kernel_traits<64, 128, 64, 4, false, false, cutlass::half_t, Flash_kernel_traits<64, 128, 64, 4, cutlass::half_t>>, Is_dropout=true, Is_causal=true, Is_even_N=true, Is_even_K=false, Return_softmax=false, Params=Flash_fwd_params, Is_Q_split_K=false]" 
(589): here
            instantiation of "void flash::compute_attn<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &) [with Kernel_traits=Flash_fwd_kernel_traits<64, 128, 64, 4, false, false, cutlass::half_t, Flash_kernel_traits<64, 128, 64, 4, cutlass::half_t>>, Is_dropout=true, Is_causal=true, Is_even_N=true, Is_even_K=false, Return_softmax=false, Params=Flash_fwd_params]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(15): here
            instantiation of "void flash_fwd_kernel<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax>(Flash_fwd_params) [with Kernel_traits=Flash_fwd_kernel_traits<64, 128, 64, 4, false, false, cutlass::half_t, Flash_kernel_traits<64, 128, 64, 4, cutlass::half_t>>, Is_dropout=true, Is_causal=true, Is_even_N=true, Is_even_K=false, Return_softmax=false]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(34): here
            instantiation of "void run_flash_fwd<Kernel_traits,Is_dropout,Is_causal>(Flash_fwd_params &, cudaStream_t) [with Kernel_traits=Flash_fwd_kernel_traits<64, 128, 64, 4, false, false, cutlass::half_t, Flash_kernel_traits<64, 128, 64, 4, cutlass::half_t>>, Is_dropout=true, Is_causal=true]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(68): here
            instantiation of "void run_mha_fwd_hdim64<T>(Flash_fwd_params &, cudaStream_t) [with T=cutlass::half_t]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_hdim64_fp16_sm80.cu(25): here

/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_kernel.h(276): warning: variable "tQrQ" was declared but never referenced
          detected during:
            instantiation of "void flash::compute_attn_1rowblock<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params,Is_Q_split_K>(const Params &, int, int, int) [with Kernel_traits=Flash_fwd_kernel_traits<64, 128, 64, 4, false, false, cutlass::half_t, Flash_kernel_traits<64, 128, 64, 4, cutlass::half_t>>, Is_dropout=true, Is_causal=true, Is_even_N=true, Is_even_K=false, Return_softmax=false, Params=Flash_fwd_params, Is_Q_split_K=false]" 
(589): here
            instantiation of "void flash::compute_attn<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &) [with Kernel_traits=Flash_fwd_kernel_traits<64, 128, 64, 4, false, false, cutlass::half_t, Flash_kernel_traits<64, 128, 64, 4, cutlass::half_t>>, Is_dropout=true, Is_causal=true, Is_even_N=true, Is_even_K=false, Return_softmax=false, Params=Flash_fwd_params]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(15): here
            instantiation of "void flash_fwd_kernel<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax>(Flash_fwd_params) [with Kernel_traits=Flash_fwd_kernel_traits<64, 128, 64, 4, false, false, cutlass::half_t, Flash_kernel_traits<64, 128, 64, 4, cutlass::half_t>>, Is_dropout=true, Is_causal=true, Is_even_N=true, Is_even_K=false, Return_softmax=false]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(34): here
            instantiation of "void run_flash_fwd<Kernel_traits,Is_dropout,Is_causal>(Flash_fwd_params &, cudaStream_t) [with Kernel_traits=Flash_fwd_kernel_traits<64, 128, 64, 4, false, false, cutlass::half_t, Flash_kernel_traits<64, 128, 64, 4, cutlass::half_t>>, Is_dropout=true, Is_causal=true]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(68): here
            instantiation of "void run_mha_fwd_hdim64<T>(Flash_fwd_params &, cudaStream_t) [with T=cutlass::half_t]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_hdim64_fp16_sm80.cu(25): here

/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_kernel.h(140): warning: variable "MMA_M" was declared but never referenced
          detected during:
            instantiation of "void flash::compute_attn_1rowblock<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params,Is_Q_split_K>(const Params &, int, int, int) [with Kernel_traits=Flash_fwd_kernel_traits<64, 128, 64, 4, false, false, cutlass::half_t, Flash_kernel_traits<64, 128, 64, 4, cutlass::half_t>>, Is_dropout=true, Is_causal=true, Is_even_N=false, Is_even_K=true, Return_softmax=true, Params=Flash_fwd_params, Is_Q_split_K=false]" 
(589): here
            instantiation of "void flash::compute_attn<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &) [with Kernel_traits=Flash_fwd_kernel_traits<64, 128, 64, 4, false, false, cutlass::half_t, Flash_kernel_traits<64, 128, 64, 4, cutlass::half_t>>, Is_dropout=true, Is_causal=true, Is_even_N=false, Is_even_K=true, Return_softmax=true, Params=Flash_fwd_params]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(15): here
            instantiation of "void flash_fwd_kernel<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax>(Flash_fwd_params) [with Kernel_traits=Flash_fwd_kernel_traits<64, 128, 64, 4, false, false, cutlass::half_t, Flash_kernel_traits<64, 128, 64, 4, cutlass::half_t>>, Is_dropout=true, Is_causal=true, Is_even_N=false, Is_even_K=true, Return_softmax=true]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(34): here
            instantiation of "void run_flash_fwd<Kernel_traits,Is_dropout,Is_causal>(Flash_fwd_params &, cudaStream_t) [with Kernel_traits=Flash_fwd_kernel_traits<64, 128, 64, 4, false, false, cutlass::half_t, Flash_kernel_traits<64, 128, 64, 4, cutlass::half_t>>, Is_dropout=true, Is_causal=true]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(68): here
            instantiation of "void run_mha_fwd_hdim64<T>(Flash_fwd_params &, cudaStream_t) [with T=cutlass::half_t]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_hdim64_fp16_sm80.cu(25): here

/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_kernel.h(276): warning: variable "tQrQ" was declared but never referenced
          detected during:
            instantiation of "void flash::compute_attn_1rowblock<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params,Is_Q_split_K>(const Params &, int, int, int) [with Kernel_traits=Flash_fwd_kernel_traits<64, 128, 64, 4, false, false, cutlass::half_t, Flash_kernel_traits<64, 128, 64, 4, cutlass::half_t>>, Is_dropout=true, Is_causal=true, Is_even_N=false, Is_even_K=true, Return_softmax=true, Params=Flash_fwd_params, Is_Q_split_K=false]" 
(589): here
            instantiation of "void flash::compute_attn<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &) [with Kernel_traits=Flash_fwd_kernel_traits<64, 128, 64, 4, false, false, cutlass::half_t, Flash_kernel_traits<64, 128, 64, 4, cutlass::half_t>>, Is_dropout=true, Is_causal=true, Is_even_N=false, Is_even_K=true, Return_softmax=true, Params=Flash_fwd_params]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(15): here
            instantiation of "void flash_fwd_kernel<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax>(Flash_fwd_params) [with Kernel_traits=Flash_fwd_kernel_traits<64, 128, 64, 4, false, false, cutlass::half_t, Flash_kernel_traits<64, 128, 64, 4, cutlass::half_t>>, Is_dropout=true, Is_causal=true, Is_even_N=false, Is_even_K=true, Return_softmax=true]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(34): here
            instantiation of "void run_flash_fwd<Kernel_traits,Is_dropout,Is_causal>(Flash_fwd_params &, cudaStream_t) [with Kernel_traits=Flash_fwd_kernel_traits<64, 128, 64, 4, false, false, cutlass::half_t, Flash_kernel_traits<64, 128, 64, 4, cutlass::half_t>>, Is_dropout=true, Is_causal=true]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(68): here
            instantiation of "void run_mha_fwd_hdim64<T>(Flash_fwd_params &, cudaStream_t) [with T=cutlass::half_t]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_hdim64_fp16_sm80.cu(25): here

/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_kernel.h(140): warning: variable "MMA_M" was declared but never referenced
          detected during:
            instantiation of "void flash::compute_attn_1rowblock<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params,Is_Q_split_K>(const Params &, int, int, int) [with Kernel_traits=Flash_fwd_kernel_traits<64, 128, 64, 4, false, false, cutlass::half_t, Flash_kernel_traits<64, 128, 64, 4, cutlass::half_t>>, Is_dropout=true, Is_causal=true, Is_even_N=false, Is_even_K=true, Return_softmax=false, Params=Flash_fwd_params, Is_Q_split_K=false]" 
(589): here
            instantiation of "void flash::compute_attn<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &) [with Kernel_traits=Flash_fwd_kernel_traits<64, 128, 64, 4, false, false, cutlass::half_t, Flash_kernel_traits<64, 128, 64, 4, cutlass::half_t>>, Is_dropout=true, Is_causal=true, Is_even_N=false, Is_even_K=true, Return_softmax=false, Params=Flash_fwd_params]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(15): here
            instantiation of "void flash_fwd_kernel<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax>(Flash_fwd_params) [with Kernel_traits=Flash_fwd_kernel_traits<64, 128, 64, 4, false, false, cutlass::half_t, Flash_kernel_traits<64, 128, 64, 4, cutlass::half_t>>, Is_dropout=true, Is_causal=true, Is_even_N=false, Is_even_K=true, Return_softmax=false]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(34): here
            instantiation of "void run_flash_fwd<Kernel_traits,Is_dropout,Is_causal>(Flash_fwd_params &, cudaStream_t) [with Kernel_traits=Flash_fwd_kernel_traits<64, 128, 64, 4, false, false, cutlass::half_t, Flash_kernel_traits<64, 128, 64, 4, cutlass::half_t>>, Is_dropout=true, Is_causal=true]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(68): here
            instantiation of "void run_mha_fwd_hdim64<T>(Flash_fwd_params &, cudaStream_t) [with T=cutlass::half_t]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_hdim64_fp16_sm80.cu(25): here

/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_kernel.h(276): warning: variable "tQrQ" was declared but never referenced
          detected during:
            instantiation of "void flash::compute_attn_1rowblock<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params,Is_Q_split_K>(const Params &, int, int, int) [with Kernel_traits=Flash_fwd_kernel_traits<64, 128, 64, 4, false, false, cutlass::half_t, Flash_kernel_traits<64, 128, 64, 4, cutlass::half_t>>, Is_dropout=true, Is_causal=true, Is_even_N=false, Is_even_K=true, Return_softmax=false, Params=Flash_fwd_params, Is_Q_split_K=false]" 
(589): here
            instantiation of "void flash::compute_attn<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &) [with Kernel_traits=Flash_fwd_kernel_traits<64, 128, 64, 4, false, false, cutlass::half_t, Flash_kernel_traits<64, 128, 64, 4, cutlass::half_t>>, Is_dropout=true, Is_causal=true, Is_even_N=false, Is_even_K=true, Return_softmax=false, Params=Flash_fwd_params]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(15): here
            instantiation of "void flash_fwd_kernel<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax>(Flash_fwd_params) [with Kernel_traits=Flash_fwd_kernel_traits<64, 128, 64, 4, false, false, cutlass::half_t, Flash_kernel_traits<64, 128, 64, 4, cutlass::half_t>>, Is_dropout=true, Is_causal=true, Is_even_N=false, Is_even_K=true, Return_softmax=false]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(34): here
            instantiation of "void run_flash_fwd<Kernel_traits,Is_dropout,Is_causal>(Flash_fwd_params &, cudaStream_t) [with Kernel_traits=Flash_fwd_kernel_traits<64, 128, 64, 4, false, false, cutlass::half_t, Flash_kernel_traits<64, 128, 64, 4, cutlass::half_t>>, Is_dropout=true, Is_causal=true]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(68): here
            instantiation of "void run_mha_fwd_hdim64<T>(Flash_fwd_params &, cudaStream_t) [with T=cutlass::half_t]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_hdim64_fp16_sm80.cu(25): here

/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_kernel.h(140): warning: variable "MMA_M" was declared but never referenced
          detected during:
            instantiation of "void flash::compute_attn_1rowblock<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params,Is_Q_split_K>(const Params &, int, int, int) [with Kernel_traits=Flash_fwd_kernel_traits<64, 128, 64, 4, false, false, cutlass::half_t, Flash_kernel_traits<64, 128, 64, 4, cutlass::half_t>>, Is_dropout=true, Is_causal=true, Is_even_N=false, Is_even_K=false, Return_softmax=true, Params=Flash_fwd_params, Is_Q_split_K=false]" 
(589): here
            instantiation of "void flash::compute_attn<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &) [with Kernel_traits=Flash_fwd_kernel_traits<64, 128, 64, 4, false, false, cutlass::half_t, Flash_kernel_traits<64, 128, 64, 4, cutlass::half_t>>, Is_dropout=true, Is_causal=true, Is_even_N=false, Is_even_K=false, Return_softmax=true, Params=Flash_fwd_params]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(15): here
            instantiation of "void flash_fwd_kernel<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax>(Flash_fwd_params) [with Kernel_traits=Flash_fwd_kernel_traits<64, 128, 64, 4, false, false, cutlass::half_t, Flash_kernel_traits<64, 128, 64, 4, cutlass::half_t>>, Is_dropout=true, Is_causal=true, Is_even_N=false, Is_even_K=false, Return_softmax=true]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(34): here
            instantiation of "void run_flash_fwd<Kernel_traits,Is_dropout,Is_causal>(Flash_fwd_params &, cudaStream_t) [with Kernel_traits=Flash_fwd_kernel_traits<64, 128, 64, 4, false, false, cutlass::half_t, Flash_kernel_traits<64, 128, 64, 4, cutlass::half_t>>, Is_dropout=true, Is_causal=true]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(68): here
            instantiation of "void run_mha_fwd_hdim64<T>(Flash_fwd_params &, cudaStream_t) [with T=cutlass::half_t]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_hdim64_fp16_sm80.cu(25): here

/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_kernel.h(276): warning: variable "tQrQ" was declared but never referenced
          detected during:
            instantiation of "void flash::compute_attn_1rowblock<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params,Is_Q_split_K>(const Params &, int, int, int) [with Kernel_traits=Flash_fwd_kernel_traits<64, 128, 64, 4, false, false, cutlass::half_t, Flash_kernel_traits<64, 128, 64, 4, cutlass::half_t>>, Is_dropout=true, Is_causal=true, Is_even_N=false, Is_even_K=false, Return_softmax=true, Params=Flash_fwd_params, Is_Q_split_K=false]" 
(589): here
            instantiation of "void flash::compute_attn<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &) [with Kernel_traits=Flash_fwd_kernel_traits<64, 128, 64, 4, false, false, cutlass::half_t, Flash_kernel_traits<64, 128, 64, 4, cutlass::half_t>>, Is_dropout=true, Is_causal=true, Is_even_N=false, Is_even_K=false, Return_softmax=true, Params=Flash_fwd_params]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(15): here
            instantiation of "void flash_fwd_kernel<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax>(Flash_fwd_params) [with Kernel_traits=Flash_fwd_kernel_traits<64, 128, 64, 4, false, false, cutlass::half_t, Flash_kernel_traits<64, 128, 64, 4, cutlass::half_t>>, Is_dropout=true, Is_causal=true, Is_even_N=false, Is_even_K=false, Return_softmax=true]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(34): here
            instantiation of "void run_flash_fwd<Kernel_traits,Is_dropout,Is_causal>(Flash_fwd_params &, cudaStream_t) [with Kernel_traits=Flash_fwd_kernel_traits<64, 128, 64, 4, false, false, cutlass::half_t, Flash_kernel_traits<64, 128, 64, 4, cutlass::half_t>>, Is_dropout=true, Is_causal=true]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(68): here
            instantiation of "void run_mha_fwd_hdim64<T>(Flash_fwd_params &, cudaStream_t) [with T=cutlass::half_t]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_hdim64_fp16_sm80.cu(25): here

/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_kernel.h(140): warning: variable "MMA_M" was declared but never referenced
          detected during:
            instantiation of "void flash::compute_attn_1rowblock<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params,Is_Q_split_K>(const Params &, int, int, int) [with Kernel_traits=Flash_fwd_kernel_traits<64, 128, 64, 4, false, false, cutlass::half_t, Flash_kernel_traits<64, 128, 64, 4, cutlass::half_t>>, Is_dropout=true, Is_causal=true, Is_even_N=false, Is_even_K=false, Return_softmax=false, Params=Flash_fwd_params, Is_Q_split_K=false]" 
(589): here
            instantiation of "void flash::compute_attn<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &) [with Kernel_traits=Flash_fwd_kernel_traits<64, 128, 64, 4, false, false, cutlass::half_t, Flash_kernel_traits<64, 128, 64, 4, cutlass::half_t>>, Is_dropout=true, Is_causal=true, Is_even_N=false, Is_even_K=false, Return_softmax=false, Params=Flash_fwd_params]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(15): here
            instantiation of "void flash_fwd_kernel<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax>(Flash_fwd_params) [with Kernel_traits=Flash_fwd_kernel_traits<64, 128, 64, 4, false, false, cutlass::half_t, Flash_kernel_traits<64, 128, 64, 4, cutlass::half_t>>, Is_dropout=true, Is_causal=true, Is_even_N=false, Is_even_K=false, Return_softmax=false]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(34): here
            instantiation of "void run_flash_fwd<Kernel_traits,Is_dropout,Is_causal>(Flash_fwd_params &, cudaStream_t) [with Kernel_traits=Flash_fwd_kernel_traits<64, 128, 64, 4, false, false, cutlass::half_t, Flash_kernel_traits<64, 128, 64, 4, cutlass::half_t>>, Is_dropout=true, Is_causal=true]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(68): here
            instantiation of "void run_mha_fwd_hdim64<T>(Flash_fwd_params &, cudaStream_t) [with T=cutlass::half_t]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_hdim64_fp16_sm80.cu(25): here

/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_kernel.h(276): warning: variable "tQrQ" was declared but never referenced
          detected during:
            instantiation of "void flash::compute_attn_1rowblock<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params,Is_Q_split_K>(const Params &, int, int, int) [with Kernel_traits=Flash_fwd_kernel_traits<64, 128, 64, 4, false, false, cutlass::half_t, Flash_kernel_traits<64, 128, 64, 4, cutlass::half_t>>, Is_dropout=true, Is_causal=true, Is_even_N=false, Is_even_K=false, Return_softmax=false, Params=Flash_fwd_params, Is_Q_split_K=false]" 
(589): here
            instantiation of "void flash::compute_attn<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &) [with Kernel_traits=Flash_fwd_kernel_traits<64, 128, 64, 4, false, false, cutlass::half_t, Flash_kernel_traits<64, 128, 64, 4, cutlass::half_t>>, Is_dropout=true, Is_causal=true, Is_even_N=false, Is_even_K=false, Return_softmax=false, Params=Flash_fwd_params]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(15): here
            instantiation of "void flash_fwd_kernel<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax>(Flash_fwd_params) [with Kernel_traits=Flash_fwd_kernel_traits<64, 128, 64, 4, false, false, cutlass::half_t, Flash_kernel_traits<64, 128, 64, 4, cutlass::half_t>>, Is_dropout=true, Is_causal=true, Is_even_N=false, Is_even_K=false, Return_softmax=false]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(34): here
            instantiation of "void run_flash_fwd<Kernel_traits,Is_dropout,Is_causal>(Flash_fwd_params &, cudaStream_t) [with Kernel_traits=Flash_fwd_kernel_traits<64, 128, 64, 4, false, false, cutlass::half_t, Flash_kernel_traits<64, 128, 64, 4, cutlass::half_t>>, Is_dropout=true, Is_causal=true]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(68): here
            instantiation of "void run_mha_fwd_hdim64<T>(Flash_fwd_params &, cudaStream_t) [with T=cutlass::half_t]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_hdim64_fp16_sm80.cu(25): here

/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_kernel.h(140): warning: variable "MMA_M" was declared but never referenced
          detected during:
            instantiation of "void flash::compute_attn_1rowblock<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params,Is_Q_split_K>(const Params &, int, int, int) [with Kernel_traits=Flash_fwd_kernel_traits<64, 128, 64, 4, false, false, cutlass::half_t, Flash_kernel_traits<64, 128, 64, 4, cutlass::half_t>>, Is_dropout=true, Is_causal=false, Is_even_N=true, Is_even_K=true, Return_softmax=true, Params=Flash_fwd_params, Is_Q_split_K=false]" 
(589): here
            instantiation of "void flash::compute_attn<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &) [with Kernel_traits=Flash_fwd_kernel_traits<64, 128, 64, 4, false, false, cutlass::half_t, Flash_kernel_traits<64, 128, 64, 4, cutlass::half_t>>, Is_dropout=true, Is_causal=false, Is_even_N=true, Is_even_K=true, Return_softmax=true, Params=Flash_fwd_params]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(15): here
            instantiation of "void flash_fwd_kernel<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax>(Flash_fwd_params) [with Kernel_traits=Flash_fwd_kernel_traits<64, 128, 64, 4, false, false, cutlass::half_t, Flash_kernel_traits<64, 128, 64, 4, cutlass::half_t>>, Is_dropout=true, Is_causal=false, Is_even_N=true, Is_even_K=true, Return_softmax=true]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(34): here
            instantiation of "void run_flash_fwd<Kernel_traits,Is_dropout,Is_causal>(Flash_fwd_params &, cudaStream_t) [with Kernel_traits=Flash_fwd_kernel_traits<64, 128, 64, 4, false, false, cutlass::half_t, Flash_kernel_traits<64, 128, 64, 4, cutlass::half_t>>, Is_dropout=true, Is_causal=false]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(68): here
            instantiation of "void run_mha_fwd_hdim64<T>(Flash_fwd_params &, cudaStream_t) [with T=cutlass::half_t]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_hdim64_fp16_sm80.cu(25): here

/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_kernel.h(276): warning: variable "tQrQ" was declared but never referenced
          detected during:
            instantiation of "void flash::compute_attn_1rowblock<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params,Is_Q_split_K>(const Params &, int, int, int) [with Kernel_traits=Flash_fwd_kernel_traits<64, 128, 64, 4, false, false, cutlass::half_t, Flash_kernel_traits<64, 128, 64, 4, cutlass::half_t>>, Is_dropout=true, Is_causal=false, Is_even_N=true, Is_even_K=true, Return_softmax=true, Params=Flash_fwd_params, Is_Q_split_K=false]" 
(589): here
            instantiation of "void flash::compute_attn<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &) [with Kernel_traits=Flash_fwd_kernel_traits<64, 128, 64, 4, false, false, cutlass::half_t, Flash_kernel_traits<64, 128, 64, 4, cutlass::half_t>>, Is_dropout=true, Is_causal=false, Is_even_N=true, Is_even_K=true, Return_softmax=true, Params=Flash_fwd_params]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(15): here
            instantiation of "void flash_fwd_kernel<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax>(Flash_fwd_params) [with Kernel_traits=Flash_fwd_kernel_traits<64, 128, 64, 4, false, false, cutlass::half_t, Flash_kernel_traits<64, 128, 64, 4, cutlass::half_t>>, Is_dropout=true, Is_causal=false, Is_even_N=true, Is_even_K=true, Return_softmax=true]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(34): here
            instantiation of "void run_flash_fwd<Kernel_traits,Is_dropout,Is_causal>(Flash_fwd_params &, cudaStream_t) [with Kernel_traits=Flash_fwd_kernel_traits<64, 128, 64, 4, false, false, cutlass::half_t, Flash_kernel_traits<64, 128, 64, 4, cutlass::half_t>>, Is_dropout=true, Is_causal=false]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(68): here
            instantiation of "void run_mha_fwd_hdim64<T>(Flash_fwd_params &, cudaStream_t) [with T=cutlass::half_t]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_hdim64_fp16_sm80.cu(25): here

/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_kernel.h(140): warning: variable "MMA_M" was declared but never referenced
          detected during:
            instantiation of "void flash::compute_attn_1rowblock<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params,Is_Q_split_K>(const Params &, int, int, int) [with Kernel_traits=Flash_fwd_kernel_traits<64, 128, 64, 4, false, false, cutlass::half_t, Flash_kernel_traits<64, 128, 64, 4, cutlass::half_t>>, Is_dropout=true, Is_causal=false, Is_even_N=true, Is_even_K=true, Return_softmax=false, Params=Flash_fwd_params, Is_Q_split_K=false]" 
(589): here
            instantiation of "void flash::compute_attn<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &) [with Kernel_traits=Flash_fwd_kernel_traits<64, 128, 64, 4, false, false, cutlass::half_t, Flash_kernel_traits<64, 128, 64, 4, cutlass::half_t>>, Is_dropout=true, Is_causal=false, Is_even_N=true, Is_even_K=true, Return_softmax=false, Params=Flash_fwd_params]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(15): here
            instantiation of "void flash_fwd_kernel<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax>(Flash_fwd_params) [with Kernel_traits=Flash_fwd_kernel_traits<64, 128, 64, 4, false, false, cutlass::half_t, Flash_kernel_traits<64, 128, 64, 4, cutlass::half_t>>, Is_dropout=true, Is_causal=false, Is_even_N=true, Is_even_K=true, Return_softmax=false]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(34): here
            instantiation of "void run_flash_fwd<Kernel_traits,Is_dropout,Is_causal>(Flash_fwd_params &, cudaStream_t) [with Kernel_traits=Flash_fwd_kernel_traits<64, 128, 64, 4, false, false, cutlass::half_t, Flash_kernel_traits<64, 128, 64, 4, cutlass::half_t>>, Is_dropout=true, Is_causal=false]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(68): here
            instantiation of "void run_mha_fwd_hdim64<T>(Flash_fwd_params &, cudaStream_t) [with T=cutlass::half_t]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_hdim64_fp16_sm80.cu(25): here

/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_kernel.h(276): warning: variable "tQrQ" was declared but never referenced
          detected during:
            instantiation of "void flash::compute_attn_1rowblock<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params,Is_Q_split_K>(const Params &, int, int, int) [with Kernel_traits=Flash_fwd_kernel_traits<64, 128, 64, 4, false, false, cutlass::half_t, Flash_kernel_traits<64, 128, 64, 4, cutlass::half_t>>, Is_dropout=true, Is_causal=false, Is_even_N=true, Is_even_K=true, Return_softmax=false, Params=Flash_fwd_params, Is_Q_split_K=false]" 
(589): here
            instantiation of "void flash::compute_attn<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &) [with Kernel_traits=Flash_fwd_kernel_traits<64, 128, 64, 4, false, false, cutlass::half_t, Flash_kernel_traits<64, 128, 64, 4, cutlass::half_t>>, Is_dropout=true, Is_causal=false, Is_even_N=true, Is_even_K=true, Return_softmax=false, Params=Flash_fwd_params]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(15): here
            instantiation of "void flash_fwd_kernel<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax>(Flash_fwd_params) [with Kernel_traits=Flash_fwd_kernel_traits<64, 128, 64, 4, false, false, cutlass::half_t, Flash_kernel_traits<64, 128, 64, 4, cutlass::half_t>>, Is_dropout=true, Is_causal=false, Is_even_N=true, Is_even_K=true, Return_softmax=false]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(34): here
            instantiation of "void run_flash_fwd<Kernel_traits,Is_dropout,Is_causal>(Flash_fwd_params &, cudaStream_t) [with Kernel_traits=Flash_fwd_kernel_traits<64, 128, 64, 4, false, false, cutlass::half_t, Flash_kernel_traits<64, 128, 64, 4, cutlass::half_t>>, Is_dropout=true, Is_causal=false]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(68): here
            instantiation of "void run_mha_fwd_hdim64<T>(Flash_fwd_params &, cudaStream_t) [with T=cutlass::half_t]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_hdim64_fp16_sm80.cu(25): here

/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_kernel.h(140): warning: variable "MMA_M" was declared but never referenced
          detected during:
            instantiation of "void flash::compute_attn_1rowblock<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params,Is_Q_split_K>(const Params &, int, int, int) [with Kernel_traits=Flash_fwd_kernel_traits<64, 128, 64, 4, false, false, cutlass::half_t, Flash_kernel_traits<64, 128, 64, 4, cutlass::half_t>>, Is_dropout=true, Is_causal=false, Is_even_N=true, Is_even_K=false, Return_softmax=true, Params=Flash_fwd_params, Is_Q_split_K=false]" 
(589): here
            instantiation of "void flash::compute_attn<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &) [with Kernel_traits=Flash_fwd_kernel_traits<64, 128, 64, 4, false, false, cutlass::half_t, Flash_kernel_traits<64, 128, 64, 4, cutlass::half_t>>, Is_dropout=true, Is_causal=false, Is_even_N=true, Is_even_K=false, Return_softmax=true, Params=Flash_fwd_params]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(15): here
            instantiation of "void flash_fwd_kernel<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax>(Flash_fwd_params) [with Kernel_traits=Flash_fwd_kernel_traits<64, 128, 64, 4, false, false, cutlass::half_t, Flash_kernel_traits<64, 128, 64, 4, cutlass::half_t>>, Is_dropout=true, Is_causal=false, Is_even_N=true, Is_even_K=false, Return_softmax=true]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(34): here
            instantiation of "void run_flash_fwd<Kernel_traits,Is_dropout,Is_causal>(Flash_fwd_params &, cudaStream_t) [with Kernel_traits=Flash_fwd_kernel_traits<64, 128, 64, 4, false, false, cutlass::half_t, Flash_kernel_traits<64, 128, 64, 4, cutlass::half_t>>, Is_dropout=true, Is_causal=false]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(68): here
            instantiation of "void run_mha_fwd_hdim64<T>(Flash_fwd_params &, cudaStream_t) [with T=cutlass::half_t]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_hdim64_fp16_sm80.cu(25): here

/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_kernel.h(276): warning: variable "tQrQ" was declared but never referenced
          detected during:
            instantiation of "void flash::compute_attn_1rowblock<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params,Is_Q_split_K>(const Params &, int, int, int) [with Kernel_traits=Flash_fwd_kernel_traits<64, 128, 64, 4, false, false, cutlass::half_t, Flash_kernel_traits<64, 128, 64, 4, cutlass::half_t>>, Is_dropout=true, Is_causal=false, Is_even_N=true, Is_even_K=false, Return_softmax=true, Params=Flash_fwd_params, Is_Q_split_K=false]" 
(589): here
            instantiation of "void flash::compute_attn<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &) [with Kernel_traits=Flash_fwd_kernel_traits<64, 128, 64, 4, false, false, cutlass::half_t, Flash_kernel_traits<64, 128, 64, 4, cutlass::half_t>>, Is_dropout=true, Is_causal=false, Is_even_N=true, Is_even_K=false, Return_softmax=true, Params=Flash_fwd_params]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(15): here
            instantiation of "void flash_fwd_kernel<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax>(Flash_fwd_params) [with Kernel_traits=Flash_fwd_kernel_traits<64, 128, 64, 4, false, false, cutlass::half_t, Flash_kernel_traits<64, 128, 64, 4, cutlass::half_t>>, Is_dropout=true, Is_causal=false, Is_even_N=true, Is_even_K=false, Return_softmax=true]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(34): here
            instantiation of "void run_flash_fwd<Kernel_traits,Is_dropout,Is_causal>(Flash_fwd_params &, cudaStream_t) [with Kernel_traits=Flash_fwd_kernel_traits<64, 128, 64, 4, false, false, cutlass::half_t, Flash_kernel_traits<64, 128, 64, 4, cutlass::half_t>>, Is_dropout=true, Is_causal=false]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(68): here
            instantiation of "void run_mha_fwd_hdim64<T>(Flash_fwd_params &, cudaStream_t) [with T=cutlass::half_t]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_hdim64_fp16_sm80.cu(25): here

/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_kernel.h(140): warning: variable "MMA_M" was declared but never referenced
          detected during:
            instantiation of "void flash::compute_attn_1rowblock<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params,Is_Q_split_K>(const Params &, int, int, int) [with Kernel_traits=Flash_fwd_kernel_traits<64, 128, 64, 4, false, false, cutlass::half_t, Flash_kernel_traits<64, 128, 64, 4, cutlass::half_t>>, Is_dropout=true, Is_causal=false, Is_even_N=true, Is_even_K=false, Return_softmax=false, Params=Flash_fwd_params, Is_Q_split_K=false]" 
(589): here
            instantiation of "void flash::compute_attn<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &) [with Kernel_traits=Flash_fwd_kernel_traits<64, 128, 64, 4, false, false, cutlass::half_t, Flash_kernel_traits<64, 128, 64, 4, cutlass::half_t>>, Is_dropout=true, Is_causal=false, Is_even_N=true, Is_even_K=false, Return_softmax=false, Params=Flash_fwd_params]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(15): here
            instantiation of "void flash_fwd_kernel<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax>(Flash_fwd_params) [with Kernel_traits=Flash_fwd_kernel_traits<64, 128, 64, 4, false, false, cutlass::half_t, Flash_kernel_traits<64, 128, 64, 4, cutlass::half_t>>, Is_dropout=true, Is_causal=false, Is_even_N=true, Is_even_K=false, Return_softmax=false]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(34): here
            instantiation of "void run_flash_fwd<Kernel_traits,Is_dropout,Is_causal>(Flash_fwd_params &, cudaStream_t) [with Kernel_traits=Flash_fwd_kernel_traits<64, 128, 64, 4, false, false, cutlass::half_t, Flash_kernel_traits<64, 128, 64, 4, cutlass::half_t>>, Is_dropout=true, Is_causal=false]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(68): here
            instantiation of "void run_mha_fwd_hdim64<T>(Flash_fwd_params &, cudaStream_t) [with T=cutlass::half_t]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_hdim64_fp16_sm80.cu(25): here

/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_kernel.h(276): warning: variable "tQrQ" was declared but never referenced
          detected during:
            instantiation of "void flash::compute_attn_1rowblock<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params,Is_Q_split_K>(const Params &, int, int, int) [with Kernel_traits=Flash_fwd_kernel_traits<64, 128, 64, 4, false, false, cutlass::half_t, Flash_kernel_traits<64, 128, 64, 4, cutlass::half_t>>, Is_dropout=true, Is_causal=false, Is_even_N=true, Is_even_K=false, Return_softmax=false, Params=Flash_fwd_params, Is_Q_split_K=false]" 
(589): here
            instantiation of "void flash::compute_attn<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &) [with Kernel_traits=Flash_fwd_kernel_traits<64, 128, 64, 4, false, false, cutlass::half_t, Flash_kernel_traits<64, 128, 64, 4, cutlass::half_t>>, Is_dropout=true, Is_causal=false, Is_even_N=true, Is_even_K=false, Return_softmax=false, Params=Flash_fwd_params]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(15): here
            instantiation of "void flash_fwd_kernel<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax>(Flash_fwd_params) [with Kernel_traits=Flash_fwd_kernel_traits<64, 128, 64, 4, false, false, cutlass::half_t, Flash_kernel_traits<64, 128, 64, 4, cutlass::half_t>>, Is_dropout=true, Is_causal=false, Is_even_N=true, Is_even_K=false, Return_softmax=false]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(34): here
            instantiation of "void run_flash_fwd<Kernel_traits,Is_dropout,Is_causal>(Flash_fwd_params &, cudaStream_t) [with Kernel_traits=Flash_fwd_kernel_traits<64, 128, 64, 4, false, false, cutlass::half_t, Flash_kernel_traits<64, 128, 64, 4, cutlass::half_t>>, Is_dropout=true, Is_causal=false]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(68): here
            instantiation of "void run_mha_fwd_hdim64<T>(Flash_fwd_params &, cudaStream_t) [with T=cutlass::half_t]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_hdim64_fp16_sm80.cu(25): here

/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_kernel.h(140): warning: variable "MMA_M" was declared but never referenced
          detected during:
            instantiation of "void flash::compute_attn_1rowblock<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params,Is_Q_split_K>(const Params &, int, int, int) [with Kernel_traits=Flash_fwd_kernel_traits<64, 128, 64, 4, false, false, cutlass::half_t, Flash_kernel_traits<64, 128, 64, 4, cutlass::half_t>>, Is_dropout=true, Is_causal=false, Is_even_N=false, Is_even_K=true, Return_softmax=true, Params=Flash_fwd_params, Is_Q_split_K=false]" 
(589): here
            instantiation of "void flash::compute_attn<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &) [with Kernel_traits=Flash_fwd_kernel_traits<64, 128, 64, 4, false, false, cutlass::half_t, Flash_kernel_traits<64, 128, 64, 4, cutlass::half_t>>, Is_dropout=true, Is_causal=false, Is_even_N=false, Is_even_K=true, Return_softmax=true, Params=Flash_fwd_params]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(15): here
            instantiation of "void flash_fwd_kernel<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax>(Flash_fwd_params) [with Kernel_traits=Flash_fwd_kernel_traits<64, 128, 64, 4, false, false, cutlass::half_t, Flash_kernel_traits<64, 128, 64, 4, cutlass::half_t>>, Is_dropout=true, Is_causal=false, Is_even_N=false, Is_even_K=true, Return_softmax=true]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(34): here
            instantiation of "void run_flash_fwd<Kernel_traits,Is_dropout,Is_causal>(Flash_fwd_params &, cudaStream_t) [with Kernel_traits=Flash_fwd_kernel_traits<64, 128, 64, 4, false, false, cutlass::half_t, Flash_kernel_traits<64, 128, 64, 4, cutlass::half_t>>, Is_dropout=true, Is_causal=false]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(68): here
            instantiation of "void run_mha_fwd_hdim64<T>(Flash_fwd_params &, cudaStream_t) [with T=cutlass::half_t]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_hdim64_fp16_sm80.cu(25): here

/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_kernel.h(276): warning: variable "tQrQ" was declared but never referenced
          detected during:
            instantiation of "void flash::compute_attn_1rowblock<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params,Is_Q_split_K>(const Params &, int, int, int) [with Kernel_traits=Flash_fwd_kernel_traits<64, 128, 64, 4, false, false, cutlass::half_t, Flash_kernel_traits<64, 128, 64, 4, cutlass::half_t>>, Is_dropout=true, Is_causal=false, Is_even_N=false, Is_even_K=true, Return_softmax=true, Params=Flash_fwd_params, Is_Q_split_K=false]" 
(589): here
            instantiation of "void flash::compute_attn<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &) [with Kernel_traits=Flash_fwd_kernel_traits<64, 128, 64, 4, false, false, cutlass::half_t, Flash_kernel_traits<64, 128, 64, 4, cutlass::half_t>>, Is_dropout=true, Is_causal=false, Is_even_N=false, Is_even_K=true, Return_softmax=true, Params=Flash_fwd_params]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(15): here
            instantiation of "void flash_fwd_kernel<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax>(Flash_fwd_params) [with Kernel_traits=Flash_fwd_kernel_traits<64, 128, 64, 4, false, false, cutlass::half_t, Flash_kernel_traits<64, 128, 64, 4, cutlass::half_t>>, Is_dropout=true, Is_causal=false, Is_even_N=false, Is_even_K=true, Return_softmax=true]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(34): here
            instantiation of "void run_flash_fwd<Kernel_traits,Is_dropout,Is_causal>(Flash_fwd_params &, cudaStream_t) [with Kernel_traits=Flash_fwd_kernel_traits<64, 128, 64, 4, false, false, cutlass::half_t, Flash_kernel_traits<64, 128, 64, 4, cutlass::half_t>>, Is_dropout=true, Is_causal=false]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(68): here
            instantiation of "void run_mha_fwd_hdim64<T>(Flash_fwd_params &, cudaStream_t) [with T=cutlass::half_t]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_hdim64_fp16_sm80.cu(25): here

/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_kernel.h(140): warning: variable "MMA_M" was declared but never referenced
          detected during:
            instantiation of "void flash::compute_attn_1rowblock<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params,Is_Q_split_K>(const Params &, int, int, int) [with Kernel_traits=Flash_fwd_kernel_traits<64, 128, 64, 4, false, false, cutlass::half_t, Flash_kernel_traits<64, 128, 64, 4, cutlass::half_t>>, Is_dropout=true, Is_causal=false, Is_even_N=false, Is_even_K=true, Return_softmax=false, Params=Flash_fwd_params, Is_Q_split_K=false]" 
(589): here
            instantiation of "void flash::compute_attn<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &) [with Kernel_traits=Flash_fwd_kernel_traits<64, 128, 64, 4, false, false, cutlass::half_t, Flash_kernel_traits<64, 128, 64, 4, cutlass::half_t>>, Is_dropout=true, Is_causal=false, Is_even_N=false, Is_even_K=true, Return_softmax=false, Params=Flash_fwd_params]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(15): here
            instantiation of "void flash_fwd_kernel<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax>(Flash_fwd_params) [with Kernel_traits=Flash_fwd_kernel_traits<64, 128, 64, 4, false, false, cutlass::half_t, Flash_kernel_traits<64, 128, 64, 4, cutlass::half_t>>, Is_dropout=true, Is_causal=false, Is_even_N=false, Is_even_K=true, Return_softmax=false]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(34): here
            instantiation of "void run_flash_fwd<Kernel_traits,Is_dropout,Is_causal>(Flash_fwd_params &, cudaStream_t) [with Kernel_traits=Flash_fwd_kernel_traits<64, 128, 64, 4, false, false, cutlass::half_t, Flash_kernel_traits<64, 128, 64, 4, cutlass::half_t>>, Is_dropout=true, Is_causal=false]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(68): here
            instantiation of "void run_mha_fwd_hdim64<T>(Flash_fwd_params &, cudaStream_t) [with T=cutlass::half_t]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_hdim64_fp16_sm80.cu(25): here

/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_kernel.h(276): warning: variable "tQrQ" was declared but never referenced
          detected during:
            instantiation of "void flash::compute_attn_1rowblock<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params,Is_Q_split_K>(const Params &, int, int, int) [with Kernel_traits=Flash_fwd_kernel_traits<64, 128, 64, 4, false, false, cutlass::half_t, Flash_kernel_traits<64, 128, 64, 4, cutlass::half_t>>, Is_dropout=true, Is_causal=false, Is_even_N=false, Is_even_K=true, Return_softmax=false, Params=Flash_fwd_params, Is_Q_split_K=false]" 
(589): here
            instantiation of "void flash::compute_attn<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &) [with Kernel_traits=Flash_fwd_kernel_traits<64, 128, 64, 4, false, false, cutlass::half_t, Flash_kernel_traits<64, 128, 64, 4, cutlass::half_t>>, Is_dropout=true, Is_causal=false, Is_even_N=false, Is_even_K=true, Return_softmax=false, Params=Flash_fwd_params]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(15): here
            instantiation of "void flash_fwd_kernel<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax>(Flash_fwd_params) [with Kernel_traits=Flash_fwd_kernel_traits<64, 128, 64, 4, false, false, cutlass::half_t, Flash_kernel_traits<64, 128, 64, 4, cutlass::half_t>>, Is_dropout=true, Is_causal=false, Is_even_N=false, Is_even_K=true, Return_softmax=false]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(34): here
            instantiation of "void run_flash_fwd<Kernel_traits,Is_dropout,Is_causal>(Flash_fwd_params &, cudaStream_t) [with Kernel_traits=Flash_fwd_kernel_traits<64, 128, 64, 4, false, false, cutlass::half_t, Flash_kernel_traits<64, 128, 64, 4, cutlass::half_t>>, Is_dropout=true, Is_causal=false]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(68): here
            instantiation of "void run_mha_fwd_hdim64<T>(Flash_fwd_params &, cudaStream_t) [with T=cutlass::half_t]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_hdim64_fp16_sm80.cu(25): here

/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_kernel.h(140): warning: variable "MMA_M" was declared but never referenced
          detected during:
            instantiation of "void flash::compute_attn_1rowblock<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params,Is_Q_split_K>(const Params &, int, int, int) [with Kernel_traits=Flash_fwd_kernel_traits<64, 128, 64, 4, false, false, cutlass::half_t, Flash_kernel_traits<64, 128, 64, 4, cutlass::half_t>>, Is_dropout=true, Is_causal=false, Is_even_N=false, Is_even_K=false, Return_softmax=true, Params=Flash_fwd_params, Is_Q_split_K=false]" 
(589): here
            instantiation of "void flash::compute_attn<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &) [with Kernel_traits=Flash_fwd_kernel_traits<64, 128, 64, 4, false, false, cutlass::half_t, Flash_kernel_traits<64, 128, 64, 4, cutlass::half_t>>, Is_dropout=true, Is_causal=false, Is_even_N=false, Is_even_K=false, Return_softmax=true, Params=Flash_fwd_params]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(15): here
            instantiation of "void flash_fwd_kernel<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax>(Flash_fwd_params) [with Kernel_traits=Flash_fwd_kernel_traits<64, 128, 64, 4, false, false, cutlass::half_t, Flash_kernel_traits<64, 128, 64, 4, cutlass::half_t>>, Is_dropout=true, Is_causal=false, Is_even_N=false, Is_even_K=false, Return_softmax=true]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(34): here
            instantiation of "void run_flash_fwd<Kernel_traits,Is_dropout,Is_causal>(Flash_fwd_params &, cudaStream_t) [with Kernel_traits=Flash_fwd_kernel_traits<64, 128, 64, 4, false, false, cutlass::half_t, Flash_kernel_traits<64, 128, 64, 4, cutlass::half_t>>, Is_dropout=true, Is_causal=false]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(68): here
            instantiation of "void run_mha_fwd_hdim64<T>(Flash_fwd_params &, cudaStream_t) [with T=cutlass::half_t]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_hdim64_fp16_sm80.cu(25): here

/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_kernel.h(276): warning: variable "tQrQ" was declared but never referenced
          detected during:
            instantiation of "void flash::compute_attn_1rowblock<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params,Is_Q_split_K>(const Params &, int, int, int) [with Kernel_traits=Flash_fwd_kernel_traits<64, 128, 64, 4, false, false, cutlass::half_t, Flash_kernel_traits<64, 128, 64, 4, cutlass::half_t>>, Is_dropout=true, Is_causal=false, Is_even_N=false, Is_even_K=false, Return_softmax=true, Params=Flash_fwd_params, Is_Q_split_K=false]" 
(589): here
            instantiation of "void flash::compute_attn<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &) [with Kernel_traits=Flash_fwd_kernel_traits<64, 128, 64, 4, false, false, cutlass::half_t, Flash_kernel_traits<64, 128, 64, 4, cutlass::half_t>>, Is_dropout=true, Is_causal=false, Is_even_N=false, Is_even_K=false, Return_softmax=true, Params=Flash_fwd_params]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(15): here
            instantiation of "void flash_fwd_kernel<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax>(Flash_fwd_params) [with Kernel_traits=Flash_fwd_kernel_traits<64, 128, 64, 4, false, false, cutlass::half_t, Flash_kernel_traits<64, 128, 64, 4, cutlass::half_t>>, Is_dropout=true, Is_causal=false, Is_even_N=false, Is_even_K=false, Return_softmax=true]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(34): here
            instantiation of "void run_flash_fwd<Kernel_traits,Is_dropout,Is_causal>(Flash_fwd_params &, cudaStream_t) [with Kernel_traits=Flash_fwd_kernel_traits<64, 128, 64, 4, false, false, cutlass::half_t, Flash_kernel_traits<64, 128, 64, 4, cutlass::half_t>>, Is_dropout=true, Is_causal=false]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(68): here
            instantiation of "void run_mha_fwd_hdim64<T>(Flash_fwd_params &, cudaStream_t) [with T=cutlass::half_t]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_hdim64_fp16_sm80.cu(25): here

/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_kernel.h(140): warning: variable "MMA_M" was declared but never referenced
          detected during:
            instantiation of "void flash::compute_attn_1rowblock<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params,Is_Q_split_K>(const Params &, int, int, int) [with Kernel_traits=Flash_fwd_kernel_traits<64, 128, 64, 4, false, false, cutlass::half_t, Flash_kernel_traits<64, 128, 64, 4, cutlass::half_t>>, Is_dropout=true, Is_causal=false, Is_even_N=false, Is_even_K=false, Return_softmax=false, Params=Flash_fwd_params, Is_Q_split_K=false]" 
(589): here
            instantiation of "void flash::compute_attn<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &) [with Kernel_traits=Flash_fwd_kernel_traits<64, 128, 64, 4, false, false, cutlass::half_t, Flash_kernel_traits<64, 128, 64, 4, cutlass::half_t>>, Is_dropout=true, Is_causal=false, Is_even_N=false, Is_even_K=false, Return_softmax=false, Params=Flash_fwd_params]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(15): here
            instantiation of "void flash_fwd_kernel<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax>(Flash_fwd_params) [with Kernel_traits=Flash_fwd_kernel_traits<64, 128, 64, 4, false, false, cutlass::half_t, Flash_kernel_traits<64, 128, 64, 4, cutlass::half_t>>, Is_dropout=true, Is_causal=false, Is_even_N=false, Is_even_K=false, Return_softmax=false]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(34): here
            instantiation of "void run_flash_fwd<Kernel_traits,Is_dropout,Is_causal>(Flash_fwd_params &, cudaStream_t) [with Kernel_traits=Flash_fwd_kernel_traits<64, 128, 64, 4, false, false, cutlass::half_t, Flash_kernel_traits<64, 128, 64, 4, cutlass::half_t>>, Is_dropout=true, Is_causal=false]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(68): here
            instantiation of "void run_mha_fwd_hdim64<T>(Flash_fwd_params &, cudaStream_t) [with T=cutlass::half_t]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_hdim64_fp16_sm80.cu(25): here

/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_kernel.h(276): warning: variable "tQrQ" was declared but never referenced
          detected during:
            instantiation of "void flash::compute_attn_1rowblock<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params,Is_Q_split_K>(const Params &, int, int, int) [with Kernel_traits=Flash_fwd_kernel_traits<64, 128, 64, 4, false, false, cutlass::half_t, Flash_kernel_traits<64, 128, 64, 4, cutlass::half_t>>, Is_dropout=true, Is_causal=false, Is_even_N=false, Is_even_K=false, Return_softmax=false, Params=Flash_fwd_params, Is_Q_split_K=false]" 
(589): here
            instantiation of "void flash::compute_attn<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &) [with Kernel_traits=Flash_fwd_kernel_traits<64, 128, 64, 4, false, false, cutlass::half_t, Flash_kernel_traits<64, 128, 64, 4, cutlass::half_t>>, Is_dropout=true, Is_causal=false, Is_even_N=false, Is_even_K=false, Return_softmax=false, Params=Flash_fwd_params]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(15): here
            instantiation of "void flash_fwd_kernel<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax>(Flash_fwd_params) [with Kernel_traits=Flash_fwd_kernel_traits<64, 128, 64, 4, false, false, cutlass::half_t, Flash_kernel_traits<64, 128, 64, 4, cutlass::half_t>>, Is_dropout=true, Is_causal=false, Is_even_N=false, Is_even_K=false, Return_softmax=false]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(34): here
            instantiation of "void run_flash_fwd<Kernel_traits,Is_dropout,Is_causal>(Flash_fwd_params &, cudaStream_t) [with Kernel_traits=Flash_fwd_kernel_traits<64, 128, 64, 4, false, false, cutlass::half_t, Flash_kernel_traits<64, 128, 64, 4, cutlass::half_t>>, Is_dropout=true, Is_causal=false]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(68): here
            instantiation of "void run_mha_fwd_hdim64<T>(Flash_fwd_params &, cudaStream_t) [with T=cutlass::half_t]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_hdim64_fp16_sm80.cu(25): here

/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_kernel.h(140): warning: variable "MMA_M" was declared but never referenced
          detected during:
            instantiation of "void flash::compute_attn_1rowblock<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params,Is_Q_split_K>(const Params &, int, int, int) [with Kernel_traits=Flash_fwd_kernel_traits<64, 128, 128, 4, false, false, cutlass::half_t, Flash_kernel_traits<64, 128, 128, 4, cutlass::half_t>>, Is_dropout=false, Is_causal=true, Is_even_N=true, Is_even_K=true, Return_softmax=false, Params=Flash_fwd_params, Is_Q_split_K=false]" 
(589): here
            instantiation of "void flash::compute_attn<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &) [with Kernel_traits=Flash_fwd_kernel_traits<64, 128, 128, 4, false, false, cutlass::half_t, Flash_kernel_traits<64, 128, 128, 4, cutlass::half_t>>, Is_dropout=false, Is_causal=true, Is_even_N=true, Is_even_K=true, Return_softmax=false, Params=Flash_fwd_params]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(15): here
            instantiation of "void flash_fwd_kernel<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax>(Flash_fwd_params) [with Kernel_traits=Flash_fwd_kernel_traits<64, 128, 128, 4, false, false, cutlass::half_t, Flash_kernel_traits<64, 128, 128, 4, cutlass::half_t>>, Is_dropout=false, Is_causal=true, Is_even_N=true, Is_even_K=true, Return_softmax=false]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(34): here
            instantiation of "void run_flash_fwd<Kernel_traits,Is_dropout,Is_causal>(Flash_fwd_params &, cudaStream_t) [with Kernel_traits=Flash_fwd_kernel_traits<64, 128, 128, 4, false, false, cutlass::half_t, Flash_kernel_traits<64, 128, 128, 4, cutlass::half_t>>, Is_dropout=false, Is_causal=true]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(68): here
            instantiation of "void run_mha_fwd_hdim64<T>(Flash_fwd_params &, cudaStream_t) [with T=cutlass::half_t]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_hdim64_fp16_sm80.cu(25): here

/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_kernel.h(276): warning: variable "tQrQ" was declared but never referenced
          detected during:
            instantiation of "void flash::compute_attn_1rowblock<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params,Is_Q_split_K>(const Params &, int, int, int) [with Kernel_traits=Flash_fwd_kernel_traits<64, 128, 128, 4, false, false, cutlass::half_t, Flash_kernel_traits<64, 128, 128, 4, cutlass::half_t>>, Is_dropout=false, Is_causal=true, Is_even_N=true, Is_even_K=true, Return_softmax=false, Params=Flash_fwd_params, Is_Q_split_K=false]" 
(589): here
            instantiation of "void flash::compute_attn<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &) [with Kernel_traits=Flash_fwd_kernel_traits<64, 128, 128, 4, false, false, cutlass::half_t, Flash_kernel_traits<64, 128, 128, 4, cutlass::half_t>>, Is_dropout=false, Is_causal=true, Is_even_N=true, Is_even_K=true, Return_softmax=false, Params=Flash_fwd_params]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(15): here
            instantiation of "void flash_fwd_kernel<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax>(Flash_fwd_params) [with Kernel_traits=Flash_fwd_kernel_traits<64, 128, 128, 4, false, false, cutlass::half_t, Flash_kernel_traits<64, 128, 128, 4, cutlass::half_t>>, Is_dropout=false, Is_causal=true, Is_even_N=true, Is_even_K=true, Return_softmax=false]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(34): here
            instantiation of "void run_flash_fwd<Kernel_traits,Is_dropout,Is_causal>(Flash_fwd_params &, cudaStream_t) [with Kernel_traits=Flash_fwd_kernel_traits<64, 128, 128, 4, false, false, cutlass::half_t, Flash_kernel_traits<64, 128, 128, 4, cutlass::half_t>>, Is_dropout=false, Is_causal=true]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(68): here
            instantiation of "void run_mha_fwd_hdim64<T>(Flash_fwd_params &, cudaStream_t) [with T=cutlass::half_t]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_hdim64_fp16_sm80.cu(25): here

/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_kernel.h(140): warning: variable "MMA_M" was declared but never referenced
          detected during:
            instantiation of "void flash::compute_attn_1rowblock<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params,Is_Q_split_K>(const Params &, int, int, int) [with Kernel_traits=Flash_fwd_kernel_traits<64, 128, 128, 4, false, false, cutlass::half_t, Flash_kernel_traits<64, 128, 128, 4, cutlass::half_t>>, Is_dropout=false, Is_causal=true, Is_even_N=true, Is_even_K=false, Return_softmax=false, Params=Flash_fwd_params, Is_Q_split_K=false]" 
(589): here
            instantiation of "void flash::compute_attn<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &) [with Kernel_traits=Flash_fwd_kernel_traits<64, 128, 128, 4, false, false, cutlass::half_t, Flash_kernel_traits<64, 128, 128, 4, cutlass::half_t>>, Is_dropout=false, Is_causal=true, Is_even_N=true, Is_even_K=false, Return_softmax=false, Params=Flash_fwd_params]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(15): here
            instantiation of "void flash_fwd_kernel<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax>(Flash_fwd_params) [with Kernel_traits=Flash_fwd_kernel_traits<64, 128, 128, 4, false, false, cutlass::half_t, Flash_kernel_traits<64, 128, 128, 4, cutlass::half_t>>, Is_dropout=false, Is_causal=true, Is_even_N=true, Is_even_K=false, Return_softmax=false]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(34): here
            instantiation of "void run_flash_fwd<Kernel_traits,Is_dropout,Is_causal>(Flash_fwd_params &, cudaStream_t) [with Kernel_traits=Flash_fwd_kernel_traits<64, 128, 128, 4, false, false, cutlass::half_t, Flash_kernel_traits<64, 128, 128, 4, cutlass::half_t>>, Is_dropout=false, Is_causal=true]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(68): here
            instantiation of "void run_mha_fwd_hdim64<T>(Flash_fwd_params &, cudaStream_t) [with T=cutlass::half_t]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_hdim64_fp16_sm80.cu(25): here

/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_kernel.h(276): warning: variable "tQrQ" was declared but never referenced
          detected during:
            instantiation of "void flash::compute_attn_1rowblock<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params,Is_Q_split_K>(const Params &, int, int, int) [with Kernel_traits=Flash_fwd_kernel_traits<64, 128, 128, 4, false, false, cutlass::half_t, Flash_kernel_traits<64, 128, 128, 4, cutlass::half_t>>, Is_dropout=false, Is_causal=true, Is_even_N=true, Is_even_K=false, Return_softmax=false, Params=Flash_fwd_params, Is_Q_split_K=false]" 
(589): here
            instantiation of "void flash::compute_attn<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &) [with Kernel_traits=Flash_fwd_kernel_traits<64, 128, 128, 4, false, false, cutlass::half_t, Flash_kernel_traits<64, 128, 128, 4, cutlass::half_t>>, Is_dropout=false, Is_causal=true, Is_even_N=true, Is_even_K=false, Return_softmax=false, Params=Flash_fwd_params]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(15): here
            instantiation of "void flash_fwd_kernel<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax>(Flash_fwd_params) [with Kernel_traits=Flash_fwd_kernel_traits<64, 128, 128, 4, false, false, cutlass::half_t, Flash_kernel_traits<64, 128, 128, 4, cutlass::half_t>>, Is_dropout=false, Is_causal=true, Is_even_N=true, Is_even_K=false, Return_softmax=false]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(34): here
            instantiation of "void run_flash_fwd<Kernel_traits,Is_dropout,Is_causal>(Flash_fwd_params &, cudaStream_t) [with Kernel_traits=Flash_fwd_kernel_traits<64, 128, 128, 4, false, false, cutlass::half_t, Flash_kernel_traits<64, 128, 128, 4, cutlass::half_t>>, Is_dropout=false, Is_causal=true]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(68): here
            instantiation of "void run_mha_fwd_hdim64<T>(Flash_fwd_params &, cudaStream_t) [with T=cutlass::half_t]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_hdim64_fp16_sm80.cu(25): here

/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_kernel.h(140): warning: variable "MMA_M" was declared but never referenced
          detected during:
            instantiation of "void flash::compute_attn_1rowblock<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params,Is_Q_split_K>(const Params &, int, int, int) [with Kernel_traits=Flash_fwd_kernel_traits<64, 128, 128, 4, false, false, cutlass::half_t, Flash_kernel_traits<64, 128, 128, 4, cutlass::half_t>>, Is_dropout=false, Is_causal=true, Is_even_N=false, Is_even_K=true, Return_softmax=false, Params=Flash_fwd_params, Is_Q_split_K=false]" 
(589): here
            instantiation of "void flash::compute_attn<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &) [with Kernel_traits=Flash_fwd_kernel_traits<64, 128, 128, 4, false, false, cutlass::half_t, Flash_kernel_traits<64, 128, 128, 4, cutlass::half_t>>, Is_dropout=false, Is_causal=true, Is_even_N=false, Is_even_K=true, Return_softmax=false, Params=Flash_fwd_params]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(15): here
            instantiation of "void flash_fwd_kernel<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax>(Flash_fwd_params) [with Kernel_traits=Flash_fwd_kernel_traits<64, 128, 128, 4, false, false, cutlass::half_t, Flash_kernel_traits<64, 128, 128, 4, cutlass::half_t>>, Is_dropout=false, Is_causal=true, Is_even_N=false, Is_even_K=true, Return_softmax=false]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(34): here
            instantiation of "void run_flash_fwd<Kernel_traits,Is_dropout,Is_causal>(Flash_fwd_params &, cudaStream_t) [with Kernel_traits=Flash_fwd_kernel_traits<64, 128, 128, 4, false, false, cutlass::half_t, Flash_kernel_traits<64, 128, 128, 4, cutlass::half_t>>, Is_dropout=false, Is_causal=true]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(68): here
            instantiation of "void run_mha_fwd_hdim64<T>(Flash_fwd_params &, cudaStream_t) [with T=cutlass::half_t]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_hdim64_fp16_sm80.cu(25): here

/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_kernel.h(276): warning: variable "tQrQ" was declared but never referenced
          detected during:
            instantiation of "void flash::compute_attn_1rowblock<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params,Is_Q_split_K>(const Params &, int, int, int) [with Kernel_traits=Flash_fwd_kernel_traits<64, 128, 128, 4, false, false, cutlass::half_t, Flash_kernel_traits<64, 128, 128, 4, cutlass::half_t>>, Is_dropout=false, Is_causal=true, Is_even_N=false, Is_even_K=true, Return_softmax=false, Params=Flash_fwd_params, Is_Q_split_K=false]" 
(589): here
            instantiation of "void flash::compute_attn<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &) [with Kernel_traits=Flash_fwd_kernel_traits<64, 128, 128, 4, false, false, cutlass::half_t, Flash_kernel_traits<64, 128, 128, 4, cutlass::half_t>>, Is_dropout=false, Is_causal=true, Is_even_N=false, Is_even_K=true, Return_softmax=false, Params=Flash_fwd_params]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(15): here
            instantiation of "void flash_fwd_kernel<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax>(Flash_fwd_params) [with Kernel_traits=Flash_fwd_kernel_traits<64, 128, 128, 4, false, false, cutlass::half_t, Flash_kernel_traits<64, 128, 128, 4, cutlass::half_t>>, Is_dropout=false, Is_causal=true, Is_even_N=false, Is_even_K=true, Return_softmax=false]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(34): here
            instantiation of "void run_flash_fwd<Kernel_traits,Is_dropout,Is_causal>(Flash_fwd_params &, cudaStream_t) [with Kernel_traits=Flash_fwd_kernel_traits<64, 128, 128, 4, false, false, cutlass::half_t, Flash_kernel_traits<64, 128, 128, 4, cutlass::half_t>>, Is_dropout=false, Is_causal=true]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(68): here
            instantiation of "void run_mha_fwd_hdim64<T>(Flash_fwd_params &, cudaStream_t) [with T=cutlass::half_t]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_hdim64_fp16_sm80.cu(25): here

/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_kernel.h(140): warning: variable "MMA_M" was declared but never referenced
          detected during:
            instantiation of "void flash::compute_attn_1rowblock<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params,Is_Q_split_K>(const Params &, int, int, int) [with Kernel_traits=Flash_fwd_kernel_traits<64, 128, 128, 4, false, false, cutlass::half_t, Flash_kernel_traits<64, 128, 128, 4, cutlass::half_t>>, Is_dropout=false, Is_causal=true, Is_even_N=false, Is_even_K=false, Return_softmax=false, Params=Flash_fwd_params, Is_Q_split_K=false]" 
(589): here
            instantiation of "void flash::compute_attn<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &) [with Kernel_traits=Flash_fwd_kernel_traits<64, 128, 128, 4, false, false, cutlass::half_t, Flash_kernel_traits<64, 128, 128, 4, cutlass::half_t>>, Is_dropout=false, Is_causal=true, Is_even_N=false, Is_even_K=false, Return_softmax=false, Params=Flash_fwd_params]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(15): here
            instantiation of "void flash_fwd_kernel<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax>(Flash_fwd_params) [with Kernel_traits=Flash_fwd_kernel_traits<64, 128, 128, 4, false, false, cutlass::half_t, Flash_kernel_traits<64, 128, 128, 4, cutlass::half_t>>, Is_dropout=false, Is_causal=true, Is_even_N=false, Is_even_K=false, Return_softmax=false]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(34): here
            instantiation of "void run_flash_fwd<Kernel_traits,Is_dropout,Is_causal>(Flash_fwd_params &, cudaStream_t) [with Kernel_traits=Flash_fwd_kernel_traits<64, 128, 128, 4, false, false, cutlass::half_t, Flash_kernel_traits<64, 128, 128, 4, cutlass::half_t>>, Is_dropout=false, Is_causal=true]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(68): here
            instantiation of "void run_mha_fwd_hdim64<T>(Flash_fwd_params &, cudaStream_t) [with T=cutlass::half_t]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_hdim64_fp16_sm80.cu(25): here

/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_kernel.h(276): warning: variable "tQrQ" was declared but never referenced
          detected during:
            instantiation of "void flash::compute_attn_1rowblock<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params,Is_Q_split_K>(const Params &, int, int, int) [with Kernel_traits=Flash_fwd_kernel_traits<64, 128, 128, 4, false, false, cutlass::half_t, Flash_kernel_traits<64, 128, 128, 4, cutlass::half_t>>, Is_dropout=false, Is_causal=true, Is_even_N=false, Is_even_K=false, Return_softmax=false, Params=Flash_fwd_params, Is_Q_split_K=false]" 
(589): here
            instantiation of "void flash::compute_attn<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &) [with Kernel_traits=Flash_fwd_kernel_traits<64, 128, 128, 4, false, false, cutlass::half_t, Flash_kernel_traits<64, 128, 128, 4, cutlass::half_t>>, Is_dropout=false, Is_causal=true, Is_even_N=false, Is_even_K=false, Return_softmax=false, Params=Flash_fwd_params]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(15): here
            instantiation of "void flash_fwd_kernel<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax>(Flash_fwd_params) [with Kernel_traits=Flash_fwd_kernel_traits<64, 128, 128, 4, false, false, cutlass::half_t, Flash_kernel_traits<64, 128, 128, 4, cutlass::half_t>>, Is_dropout=false, Is_causal=true, Is_even_N=false, Is_even_K=false, Return_softmax=false]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(34): here
            instantiation of "void run_flash_fwd<Kernel_traits,Is_dropout,Is_causal>(Flash_fwd_params &, cudaStream_t) [with Kernel_traits=Flash_fwd_kernel_traits<64, 128, 128, 4, false, false, cutlass::half_t, Flash_kernel_traits<64, 128, 128, 4, cutlass::half_t>>, Is_dropout=false, Is_causal=true]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(68): here
            instantiation of "void run_mha_fwd_hdim64<T>(Flash_fwd_params &, cudaStream_t) [with T=cutlass::half_t]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_hdim64_fp16_sm80.cu(25): here

/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_kernel.h(140): warning: variable "MMA_M" was declared but never referenced
          detected during:
            instantiation of "void flash::compute_attn_1rowblock<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params,Is_Q_split_K>(const Params &, int, int, int) [with Kernel_traits=Flash_fwd_kernel_traits<64, 128, 128, 4, false, false, cutlass::half_t, Flash_kernel_traits<64, 128, 128, 4, cutlass::half_t>>, Is_dropout=false, Is_causal=false, Is_even_N=true, Is_even_K=true, Return_softmax=false, Params=Flash_fwd_params, Is_Q_split_K=false]" 
(589): here
            instantiation of "void flash::compute_attn<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &) [with Kernel_traits=Flash_fwd_kernel_traits<64, 128, 128, 4, false, false, cutlass::half_t, Flash_kernel_traits<64, 128, 128, 4, cutlass::half_t>>, Is_dropout=false, Is_causal=false, Is_even_N=true, Is_even_K=true, Return_softmax=false, Params=Flash_fwd_params]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(15): here
            instantiation of "void flash_fwd_kernel<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax>(Flash_fwd_params) [with Kernel_traits=Flash_fwd_kernel_traits<64, 128, 128, 4, false, false, cutlass::half_t, Flash_kernel_traits<64, 128, 128, 4, cutlass::half_t>>, Is_dropout=false, Is_causal=false, Is_even_N=true, Is_even_K=true, Return_softmax=false]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(34): here
            instantiation of "void run_flash_fwd<Kernel_traits,Is_dropout,Is_causal>(Flash_fwd_params &, cudaStream_t) [with Kernel_traits=Flash_fwd_kernel_traits<64, 128, 128, 4, false, false, cutlass::half_t, Flash_kernel_traits<64, 128, 128, 4, cutlass::half_t>>, Is_dropout=false, Is_causal=false]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(68): here
            instantiation of "void run_mha_fwd_hdim64<T>(Flash_fwd_params &, cudaStream_t) [with T=cutlass::half_t]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_hdim64_fp16_sm80.cu(25): here

/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_kernel.h(276): warning: variable "tQrQ" was declared but never referenced
          detected during:
            instantiation of "void flash::compute_attn_1rowblock<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params,Is_Q_split_K>(const Params &, int, int, int) [with Kernel_traits=Flash_fwd_kernel_traits<64, 128, 128, 4, false, false, cutlass::half_t, Flash_kernel_traits<64, 128, 128, 4, cutlass::half_t>>, Is_dropout=false, Is_causal=false, Is_even_N=true, Is_even_K=true, Return_softmax=false, Params=Flash_fwd_params, Is_Q_split_K=false]" 
(589): here
            instantiation of "void flash::compute_attn<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &) [with Kernel_traits=Flash_fwd_kernel_traits<64, 128, 128, 4, false, false, cutlass::half_t, Flash_kernel_traits<64, 128, 128, 4, cutlass::half_t>>, Is_dropout=false, Is_causal=false, Is_even_N=true, Is_even_K=true, Return_softmax=false, Params=Flash_fwd_params]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(15): here
            instantiation of "void flash_fwd_kernel<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax>(Flash_fwd_params) [with Kernel_traits=Flash_fwd_kernel_traits<64, 128, 128, 4, false, false, cutlass::half_t, Flash_kernel_traits<64, 128, 128, 4, cutlass::half_t>>, Is_dropout=false, Is_causal=false, Is_even_N=true, Is_even_K=true, Return_softmax=false]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(34): here
            instantiation of "void run_flash_fwd<Kernel_traits,Is_dropout,Is_causal>(Flash_fwd_params &, cudaStream_t) [with Kernel_traits=Flash_fwd_kernel_traits<64, 128, 128, 4, false, false, cutlass::half_t, Flash_kernel_traits<64, 128, 128, 4, cutlass::half_t>>, Is_dropout=false, Is_causal=false]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(68): here
            instantiation of "void run_mha_fwd_hdim64<T>(Flash_fwd_params &, cudaStream_t) [with T=cutlass::half_t]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_hdim64_fp16_sm80.cu(25): here

/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_kernel.h(140): warning: variable "MMA_M" was declared but never referenced
          detected during:
            instantiation of "void flash::compute_attn_1rowblock<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params,Is_Q_split_K>(const Params &, int, int, int) [with Kernel_traits=Flash_fwd_kernel_traits<64, 128, 128, 4, false, false, cutlass::half_t, Flash_kernel_traits<64, 128, 128, 4, cutlass::half_t>>, Is_dropout=false, Is_causal=false, Is_even_N=true, Is_even_K=false, Return_softmax=false, Params=Flash_fwd_params, Is_Q_split_K=false]" 
(589): here
            instantiation of "void flash::compute_attn<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &) [with Kernel_traits=Flash_fwd_kernel_traits<64, 128, 128, 4, false, false, cutlass::half_t, Flash_kernel_traits<64, 128, 128, 4, cutlass::half_t>>, Is_dropout=false, Is_causal=false, Is_even_N=true, Is_even_K=false, Return_softmax=false, Params=Flash_fwd_params]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(15): here
            instantiation of "void flash_fwd_kernel<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax>(Flash_fwd_params) [with Kernel_traits=Flash_fwd_kernel_traits<64, 128, 128, 4, false, false, cutlass::half_t, Flash_kernel_traits<64, 128, 128, 4, cutlass::half_t>>, Is_dropout=false, Is_causal=false, Is_even_N=true, Is_even_K=false, Return_softmax=false]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(34): here
            instantiation of "void run_flash_fwd<Kernel_traits,Is_dropout,Is_causal>(Flash_fwd_params &, cudaStream_t) [with Kernel_traits=Flash_fwd_kernel_traits<64, 128, 128, 4, false, false, cutlass::half_t, Flash_kernel_traits<64, 128, 128, 4, cutlass::half_t>>, Is_dropout=false, Is_causal=false]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(68): here
            instantiation of "void run_mha_fwd_hdim64<T>(Flash_fwd_params &, cudaStream_t) [with T=cutlass::half_t]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_hdim64_fp16_sm80.cu(25): here

/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_kernel.h(276): warning: variable "tQrQ" was declared but never referenced
          detected during:
            instantiation of "void flash::compute_attn_1rowblock<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params,Is_Q_split_K>(const Params &, int, int, int) [with Kernel_traits=Flash_fwd_kernel_traits<64, 128, 128, 4, false, false, cutlass::half_t, Flash_kernel_traits<64, 128, 128, 4, cutlass::half_t>>, Is_dropout=false, Is_causal=false, Is_even_N=true, Is_even_K=false, Return_softmax=false, Params=Flash_fwd_params, Is_Q_split_K=false]" 
(589): here
            instantiation of "void flash::compute_attn<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &) [with Kernel_traits=Flash_fwd_kernel_traits<64, 128, 128, 4, false, false, cutlass::half_t, Flash_kernel_traits<64, 128, 128, 4, cutlass::half_t>>, Is_dropout=false, Is_causal=false, Is_even_N=true, Is_even_K=false, Return_softmax=false, Params=Flash_fwd_params]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(15): here
            instantiation of "void flash_fwd_kernel<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax>(Flash_fwd_params) [with Kernel_traits=Flash_fwd_kernel_traits<64, 128, 128, 4, false, false, cutlass::half_t, Flash_kernel_traits<64, 128, 128, 4, cutlass::half_t>>, Is_dropout=false, Is_causal=false, Is_even_N=true, Is_even_K=false, Return_softmax=false]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(34): here
            instantiation of "void run_flash_fwd<Kernel_traits,Is_dropout,Is_causal>(Flash_fwd_params &, cudaStream_t) [with Kernel_traits=Flash_fwd_kernel_traits<64, 128, 128, 4, false, false, cutlass::half_t, Flash_kernel_traits<64, 128, 128, 4, cutlass::half_t>>, Is_dropout=false, Is_causal=false]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(68): here
            instantiation of "void run_mha_fwd_hdim64<T>(Flash_fwd_params &, cudaStream_t) [with T=cutlass::half_t]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_hdim64_fp16_sm80.cu(25): here

/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_kernel.h(140): warning: variable "MMA_M" was declared but never referenced
          detected during:
            instantiation of "void flash::compute_attn_1rowblock<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params,Is_Q_split_K>(const Params &, int, int, int) [with Kernel_traits=Flash_fwd_kernel_traits<64, 128, 128, 4, false, false, cutlass::half_t, Flash_kernel_traits<64, 128, 128, 4, cutlass::half_t>>, Is_dropout=false, Is_causal=false, Is_even_N=false, Is_even_K=true, Return_softmax=false, Params=Flash_fwd_params, Is_Q_split_K=false]" 
(589): here
            instantiation of "void flash::compute_attn<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &) [with Kernel_traits=Flash_fwd_kernel_traits<64, 128, 128, 4, false, false, cutlass::half_t, Flash_kernel_traits<64, 128, 128, 4, cutlass::half_t>>, Is_dropout=false, Is_causal=false, Is_even_N=false, Is_even_K=true, Return_softmax=false, Params=Flash_fwd_params]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(15): here
            instantiation of "void flash_fwd_kernel<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax>(Flash_fwd_params) [with Kernel_traits=Flash_fwd_kernel_traits<64, 128, 128, 4, false, false, cutlass::half_t, Flash_kernel_traits<64, 128, 128, 4, cutlass::half_t>>, Is_dropout=false, Is_causal=false, Is_even_N=false, Is_even_K=true, Return_softmax=false]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(34): here
            instantiation of "void run_flash_fwd<Kernel_traits,Is_dropout,Is_causal>(Flash_fwd_params &, cudaStream_t) [with Kernel_traits=Flash_fwd_kernel_traits<64, 128, 128, 4, false, false, cutlass::half_t, Flash_kernel_traits<64, 128, 128, 4, cutlass::half_t>>, Is_dropout=false, Is_causal=false]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(68): here
            instantiation of "void run_mha_fwd_hdim64<T>(Flash_fwd_params &, cudaStream_t) [with T=cutlass::half_t]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_hdim64_fp16_sm80.cu(25): here

/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_kernel.h(276): warning: variable "tQrQ" was declared but never referenced
          detected during:
            instantiation of "void flash::compute_attn_1rowblock<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params,Is_Q_split_K>(const Params &, int, int, int) [with Kernel_traits=Flash_fwd_kernel_traits<64, 128, 128, 4, false, false, cutlass::half_t, Flash_kernel_traits<64, 128, 128, 4, cutlass::half_t>>, Is_dropout=false, Is_causal=false, Is_even_N=false, Is_even_K=true, Return_softmax=false, Params=Flash_fwd_params, Is_Q_split_K=false]" 
(589): here
            instantiation of "void flash::compute_attn<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &) [with Kernel_traits=Flash_fwd_kernel_traits<64, 128, 128, 4, false, false, cutlass::half_t, Flash_kernel_traits<64, 128, 128, 4, cutlass::half_t>>, Is_dropout=false, Is_causal=false, Is_even_N=false, Is_even_K=true, Return_softmax=false, Params=Flash_fwd_params]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(15): here
            instantiation of "void flash_fwd_kernel<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax>(Flash_fwd_params) [with Kernel_traits=Flash_fwd_kernel_traits<64, 128, 128, 4, false, false, cutlass::half_t, Flash_kernel_traits<64, 128, 128, 4, cutlass::half_t>>, Is_dropout=false, Is_causal=false, Is_even_N=false, Is_even_K=true, Return_softmax=false]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(34): here
            instantiation of "void run_flash_fwd<Kernel_traits,Is_dropout,Is_causal>(Flash_fwd_params &, cudaStream_t) [with Kernel_traits=Flash_fwd_kernel_traits<64, 128, 128, 4, false, false, cutlass::half_t, Flash_kernel_traits<64, 128, 128, 4, cutlass::half_t>>, Is_dropout=false, Is_causal=false]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(68): here
            instantiation of "void run_mha_fwd_hdim64<T>(Flash_fwd_params &, cudaStream_t) [with T=cutlass::half_t]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_hdim64_fp16_sm80.cu(25): here

/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_kernel.h(140): warning: variable "MMA_M" was declared but never referenced
          detected during:
            instantiation of "void flash::compute_attn_1rowblock<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params,Is_Q_split_K>(const Params &, int, int, int) [with Kernel_traits=Flash_fwd_kernel_traits<64, 128, 128, 4, false, false, cutlass::half_t, Flash_kernel_traits<64, 128, 128, 4, cutlass::half_t>>, Is_dropout=false, Is_causal=false, Is_even_N=false, Is_even_K=false, Return_softmax=false, Params=Flash_fwd_params, Is_Q_split_K=false]" 
(589): here
            instantiation of "void flash::compute_attn<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &) [with Kernel_traits=Flash_fwd_kernel_traits<64, 128, 128, 4, false, false, cutlass::half_t, Flash_kernel_traits<64, 128, 128, 4, cutlass::half_t>>, Is_dropout=false, Is_causal=false, Is_even_N=false, Is_even_K=false, Return_softmax=false, Params=Flash_fwd_params]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(15): here
            instantiation of "void flash_fwd_kernel<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax>(Flash_fwd_params) [with Kernel_traits=Flash_fwd_kernel_traits<64, 128, 128, 4, false, false, cutlass::half_t, Flash_kernel_traits<64, 128, 128, 4, cutlass::half_t>>, Is_dropout=false, Is_causal=false, Is_even_N=false, Is_even_K=false, Return_softmax=false]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(34): here
            instantiation of "void run_flash_fwd<Kernel_traits,Is_dropout,Is_causal>(Flash_fwd_params &, cudaStream_t) [with Kernel_traits=Flash_fwd_kernel_traits<64, 128, 128, 4, false, false, cutlass::half_t, Flash_kernel_traits<64, 128, 128, 4, cutlass::half_t>>, Is_dropout=false, Is_causal=false]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(68): here
            instantiation of "void run_mha_fwd_hdim64<T>(Flash_fwd_params &, cudaStream_t) [with T=cutlass::half_t]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_hdim64_fp16_sm80.cu(25): here

/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_kernel.h(276): warning: variable "tQrQ" was declared but never referenced
          detected during:
            instantiation of "void flash::compute_attn_1rowblock<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params,Is_Q_split_K>(const Params &, int, int, int) [with Kernel_traits=Flash_fwd_kernel_traits<64, 128, 128, 4, false, false, cutlass::half_t, Flash_kernel_traits<64, 128, 128, 4, cutlass::half_t>>, Is_dropout=false, Is_causal=false, Is_even_N=false, Is_even_K=false, Return_softmax=false, Params=Flash_fwd_params, Is_Q_split_K=false]" 
(589): here
            instantiation of "void flash::compute_attn<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &) [with Kernel_traits=Flash_fwd_kernel_traits<64, 128, 128, 4, false, false, cutlass::half_t, Flash_kernel_traits<64, 128, 128, 4, cutlass::half_t>>, Is_dropout=false, Is_causal=false, Is_even_N=false, Is_even_K=false, Return_softmax=false, Params=Flash_fwd_params]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(15): here
            instantiation of "void flash_fwd_kernel<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax>(Flash_fwd_params) [with Kernel_traits=Flash_fwd_kernel_traits<64, 128, 128, 4, false, false, cutlass::half_t, Flash_kernel_traits<64, 128, 128, 4, cutlass::half_t>>, Is_dropout=false, Is_causal=false, Is_even_N=false, Is_even_K=false, Return_softmax=false]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(34): here
            instantiation of "void run_flash_fwd<Kernel_traits,Is_dropout,Is_causal>(Flash_fwd_params &, cudaStream_t) [with Kernel_traits=Flash_fwd_kernel_traits<64, 128, 128, 4, false, false, cutlass::half_t, Flash_kernel_traits<64, 128, 128, 4, cutlass::half_t>>, Is_dropout=false, Is_causal=false]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(68): here
            instantiation of "void run_mha_fwd_hdim64<T>(Flash_fwd_params &, cudaStream_t) [with T=cutlass::half_t]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_hdim64_fp16_sm80.cu(25): here

ptxas info    : 2 bytes gmem
ptxas info    : Compiling entry function '_Z16flash_fwd_kernelI23Flash_fwd_kernel_traitsILi64ELi128ELi128ELi4ELb0ELb0EN7cutlass6half_tE19Flash_kernel_traitsILi64ELi128ELi128ELi4ES2_EELb0ELb0ELb0ELb0ELb0EEv16Flash_fwd_params' for 'sm_80'
ptxas info    : Function properties for _Z16flash_fwd_kernelI23Flash_fwd_kernel_traitsILi64ELi128ELi128ELi4ELb0ELb0EN7cutlass6half_tE19Flash_kernel_traitsILi64ELi128ELi128ELi4ES2_EELb0ELb0ELb0ELb0ELb0EEv16Flash_fwd_params
    40 bytes stack frame, 56 bytes spill stores, 56 bytes spill loads
ptxas info    : Used 255 registers, 584 bytes cmem[0]
ptxas info    : Compiling entry function '_Z16flash_fwd_kernelI23Flash_fwd_kernel_traitsILi64ELi128ELi128ELi4ELb0ELb0EN7cutlass6half_tE19Flash_kernel_traitsILi64ELi128ELi128ELi4ES2_EELb0ELb0ELb0ELb1ELb0EEv16Flash_fwd_params' for 'sm_80'
ptxas info    : Function properties for _Z16flash_fwd_kernelI23Flash_fwd_kernel_traitsILi64ELi128ELi128ELi4ELb0ELb0EN7cutlass6half_tE19Flash_kernel_traitsILi64ELi128ELi128ELi4ES2_EELb0ELb0ELb0ELb1ELb0EEv16Flash_fwd_params
    48 bytes stack frame, 76 bytes spill stores, 60 bytes spill loads
ptxas info    : Used 255 registers, 584 bytes cmem[0]
ptxas info    : Compiling entry function '_Z16flash_fwd_kernelI23Flash_fwd_kernel_traitsILi64ELi128ELi128ELi4ELb0ELb0EN7cutlass6half_tE19Flash_kernel_traitsILi64ELi128ELi128ELi4ES2_EELb0ELb0ELb1ELb0ELb0EEv16Flash_fwd_params' for 'sm_80'
ptxas info    : Function properties for _Z16flash_fwd_kernelI23Flash_fwd_kernel_traitsILi64ELi128ELi128ELi4ELb0ELb0EN7cutlass6half_tE19Flash_kernel_traitsILi64ELi128ELi128ELi4ES2_EELb0ELb0ELb1ELb0ELb0EEv16Flash_fwd_params
    40 bytes stack frame, 48 bytes spill stores, 48 bytes spill loads
ptxas info    : Used 255 registers, 584 bytes cmem[0]
ptxas info    : Compiling entry function '_Z16flash_fwd_kernelI23Flash_fwd_kernel_traitsILi64ELi128ELi128ELi4ELb0ELb0EN7cutlass6half_tE19Flash_kernel_traitsILi64ELi128ELi128ELi4ES2_EELb0ELb0ELb1ELb1ELb0EEv16Flash_fwd_params' for 'sm_80'
ptxas info    : Function properties for _Z16flash_fwd_kernelI23Flash_fwd_kernel_traitsILi64ELi128ELi128ELi4ELb0ELb0EN7cutlass6half_tE19Flash_kernel_traitsILi64ELi128ELi128ELi4ES2_EELb0ELb0ELb1ELb1ELb0EEv16Flash_fwd_params
    40 bytes stack frame, 52 bytes spill stores, 52 bytes spill loads
ptxas info    : Used 255 registers, 584 bytes cmem[0]
ptxas info    : Compiling entry function '_Z16flash_fwd_kernelI23Flash_fwd_kernel_traitsILi64ELi128ELi128ELi4ELb0ELb0EN7cutlass6half_tE19Flash_kernel_traitsILi64ELi128ELi128ELi4ES2_EELb0ELb1ELb0ELb0ELb0EEv16Flash_fwd_params' for 'sm_80'
ptxas info    : Function properties for _Z16flash_fwd_kernelI23Flash_fwd_kernel_traitsILi64ELi128ELi128ELi4ELb0ELb0EN7cutlass6half_tE19Flash_kernel_traitsILi64ELi128ELi128ELi4ES2_EELb0ELb1ELb0ELb0ELb0EEv16Flash_fwd_params
    40 bytes stack frame, 56 bytes spill stores, 56 bytes spill loads
ptxas info    : Used 255 registers, 584 bytes cmem[0]
ptxas info    : Compiling entry function '_Z16flash_fwd_kernelI23Flash_fwd_kernel_traitsILi64ELi128ELi128ELi4ELb0ELb0EN7cutlass6half_tE19Flash_kernel_traitsILi64ELi128ELi128ELi4ES2_EELb0ELb1ELb0ELb1ELb0EEv16Flash_fwd_params' for 'sm_80'
ptxas info    : Function properties for _Z16flash_fwd_kernelI23Flash_fwd_kernel_traitsILi64ELi128ELi128ELi4ELb0ELb0EN7cutlass6half_tE19Flash_kernel_traitsILi64ELi128ELi128ELi4ES2_EELb0ELb1ELb0ELb1ELb0EEv16Flash_fwd_params
    48 bytes stack frame, 80 bytes spill stores, 64 bytes spill loads
ptxas info    : Used 255 registers, 584 bytes cmem[0]
ptxas info    : Compiling entry function '_Z16flash_fwd_kernelI23Flash_fwd_kernel_traitsILi64ELi128ELi128ELi4ELb0ELb0EN7cutlass6half_tE19Flash_kernel_traitsILi64ELi128ELi128ELi4ES2_EELb0ELb1ELb1ELb0ELb0EEv16Flash_fwd_params' for 'sm_80'
ptxas info    : Function properties for _Z16flash_fwd_kernelI23Flash_fwd_kernel_traitsILi64ELi128ELi128ELi4ELb0ELb0EN7cutlass6half_tE19Flash_kernel_traitsILi64ELi128ELi128ELi4ES2_EELb0ELb1ELb1ELb0ELb0EEv16Flash_fwd_params
    40 bytes stack frame, 56 bytes spill stores, 56 bytes spill loads
ptxas info    : Used 255 registers, 584 bytes cmem[0]
ptxas info    : Compiling entry function '_Z16flash_fwd_kernelI23Flash_fwd_kernel_traitsILi64ELi128ELi128ELi4ELb0ELb0EN7cutlass6half_tE19Flash_kernel_traitsILi64ELi128ELi128ELi4ES2_EELb0ELb1ELb1ELb1ELb0EEv16Flash_fwd_params' for 'sm_80'
ptxas info    : Function properties for _Z16flash_fwd_kernelI23Flash_fwd_kernel_traitsILi64ELi128ELi128ELi4ELb0ELb0EN7cutlass6half_tE19Flash_kernel_traitsILi64ELi128ELi128ELi4ES2_EELb0ELb1ELb1ELb1ELb0EEv16Flash_fwd_params
    40 bytes stack frame, 52 bytes spill stores, 52 bytes spill loads
ptxas info    : Used 255 registers, 584 bytes cmem[0]
ptxas info    : Compiling entry function '_Z16flash_fwd_kernelI23Flash_fwd_kernel_traitsILi64ELi128ELi64ELi4ELb0ELb0EN7cutlass6half_tE19Flash_kernel_traitsILi64ELi128ELi64ELi4ES2_EELb1ELb0ELb0ELb0ELb0EEv16Flash_fwd_params' for 'sm_80'
ptxas info    : Function properties for _Z16flash_fwd_kernelI23Flash_fwd_kernel_traitsILi64ELi128ELi64ELi4ELb0ELb0EN7cutlass6half_tE19Flash_kernel_traitsILi64ELi128ELi64ELi4ES2_EELb1ELb0ELb0ELb0ELb0EEv16Flash_fwd_params
    0 bytes stack frame, 0 bytes spill stores, 0 bytes spill loads
ptxas info    : Used 242 registers, 584 bytes cmem[0]
ptxas info    : Compiling entry function '_Z16flash_fwd_kernelI23Flash_fwd_kernel_traitsILi64ELi128ELi64ELi4ELb0ELb0EN7cutlass6half_tE19Flash_kernel_traitsILi64ELi128ELi64ELi4ES2_EELb1ELb0ELb0ELb0ELb1EEv16Flash_fwd_params' for 'sm_80'
ptxas info    : Function properties for _Z16flash_fwd_kernelI23Flash_fwd_kernel_traitsILi64ELi128ELi64ELi4ELb0ELb0EN7cutlass6half_tE19Flash_kernel_traitsILi64ELi128ELi64ELi4ES2_EELb1ELb0ELb0ELb0ELb1EEv16Flash_fwd_params
    160 bytes stack frame, 126 bytes spill stores, 126 bytes spill loads
ptxas info    : Used 255 registers, 584 bytes cmem[0]
ptxas info    : Compiling entry function '_Z16flash_fwd_kernelI23Flash_fwd_kernel_traitsILi64ELi128ELi64ELi4ELb0ELb0EN7cutlass6half_tE19Flash_kernel_traitsILi64ELi128ELi64ELi4ES2_EELb1ELb0ELb0ELb1ELb0EEv16Flash_fwd_params' for 'sm_80'
ptxas info    : Function properties for _Z16flash_fwd_kernelI23Flash_fwd_kernel_traitsILi64ELi128ELi64ELi4ELb0ELb0EN7cutlass6half_tE19Flash_kernel_traitsILi64ELi128ELi64ELi4ES2_EELb1ELb0ELb0ELb1ELb0EEv16Flash_fwd_params
    0 bytes stack frame, 0 bytes spill stores, 0 bytes spill loads
ptxas info    : Used 236 registers, 584 bytes cmem[0]
ptxas info    : Compiling entry function '_Z16flash_fwd_kernelI23Flash_fwd_kernel_traitsILi64ELi128ELi64ELi4ELb0ELb0EN7cutlass6half_tE19Flash_kernel_traitsILi64ELi128ELi64ELi4ES2_EELb1ELb0ELb0ELb1ELb1EEv16Flash_fwd_params' for 'sm_80'
ptxas info    : Function properties for _Z16flash_fwd_kernelI23Flash_fwd_kernel_traitsILi64ELi128ELi64ELi4ELb0ELb0EN7cutlass6half_tE19Flash_kernel_traitsILi64ELi128ELi64ELi4ES2_EELb1ELb0ELb0ELb1ELb1EEv16Flash_fwd_params
    216 bytes stack frame, 212 bytes spill stores, 218 bytes spill loads
ptxas info    : Used 255 registers, 584 bytes cmem[0]
ptxas info    : Compiling entry function '_Z16flash_fwd_kernelI23Flash_fwd_kernel_traitsILi64ELi128ELi64ELi4ELb0ELb0EN7cutlass6half_tE19Flash_kernel_traitsILi64ELi128ELi64ELi4ES2_EELb1ELb0ELb1ELb0ELb0EEv16Flash_fwd_params' for 'sm_80'
ptxas info    : Function properties for _Z16flash_fwd_kernelI23Flash_fwd_kernel_traitsILi64ELi128ELi64ELi4ELb0ELb0EN7cutlass6half_tE19Flash_kernel_traitsILi64ELi128ELi64ELi4ES2_EELb1ELb0ELb1ELb0ELb0EEv16Flash_fwd_params
    0 bytes stack frame, 0 bytes spill stores, 0 bytes spill loads
ptxas info    : Used 236 registers, 584 bytes cmem[0]
ptxas info    : Compiling entry function '_Z16flash_fwd_kernelI23Flash_fwd_kernel_traitsILi64ELi128ELi64ELi4ELb0ELb0EN7cutlass6half_tE19Flash_kernel_traitsILi64ELi128ELi64ELi4ES2_EELb1ELb0ELb1ELb0ELb1EEv16Flash_fwd_params' for 'sm_80'
ptxas info    : Function properties for _Z16flash_fwd_kernelI23Flash_fwd_kernel_traitsILi64ELi128ELi64ELi4ELb0ELb0EN7cutlass6half_tE19Flash_kernel_traitsILi64ELi128ELi64ELi4ES2_EELb1ELb0ELb1ELb0ELb1EEv16Flash_fwd_params
    136 bytes stack frame, 116 bytes spill stores, 116 bytes spill loads
ptxas info    : Used 255 registers, 584 bytes cmem[0]
ptxas info    : Compiling entry function '_Z16flash_fwd_kernelI23Flash_fwd_kernel_traitsILi64ELi128ELi64ELi4ELb0ELb0EN7cutlass6half_tE19Flash_kernel_traitsILi64ELi128ELi64ELi4ES2_EELb1ELb0ELb1ELb1ELb0EEv16Flash_fwd_params' for 'sm_80'
ptxas info    : Function properties for _Z16flash_fwd_kernelI23Flash_fwd_kernel_traitsILi64ELi128ELi64ELi4ELb0ELb0EN7cutlass6half_tE19Flash_kernel_traitsILi64ELi128ELi64ELi4ES2_EELb1ELb0ELb1ELb1ELb0EEv16Flash_fwd_params
    0 bytes stack frame, 0 bytes spill stores, 0 bytes spill loads
ptxas info    : Used 238 registers, 584 bytes cmem[0]
ptxas info    : Compiling entry function '_Z16flash_fwd_kernelI23Flash_fwd_kernel_traitsILi64ELi128ELi64ELi4ELb0ELb0EN7cutlass6half_tE19Flash_kernel_traitsILi64ELi128ELi64ELi4ES2_EELb1ELb0ELb1ELb1ELb1EEv16Flash_fwd_params' for 'sm_80'
ptxas info    : Function properties for _Z16flash_fwd_kernelI23Flash_fwd_kernel_traitsILi64ELi128ELi64ELi4ELb0ELb0EN7cutlass6half_tE19Flash_kernel_traitsILi64ELi128ELi64ELi4ES2_EELb1ELb0ELb1ELb1ELb1EEv16Flash_fwd_params
    192 bytes stack frame, 164 bytes spill stores, 184 bytes spill loads
ptxas info    : Used 255 registers, 584 bytes cmem[0]
ptxas info    : Compiling entry function '_Z16flash_fwd_kernelI23Flash_fwd_kernel_traitsILi64ELi128ELi64ELi4ELb0ELb0EN7cutlass6half_tE19Flash_kernel_traitsILi64ELi128ELi64ELi4ES2_EELb1ELb1ELb0ELb0ELb0EEv16Flash_fwd_params' for 'sm_80'
ptxas info    : Function properties for _Z16flash_fwd_kernelI23Flash_fwd_kernel_traitsILi64ELi128ELi64ELi4ELb0ELb0EN7cutlass6half_tE19Flash_kernel_traitsILi64ELi128ELi64ELi4ES2_EELb1ELb1ELb0ELb0ELb0EEv16Flash_fwd_params
    0 bytes stack frame, 0 bytes spill stores, 0 bytes spill loads
ptxas info    : Used 250 registers, 584 bytes cmem[0]
ptxas info    : Compiling entry function '_Z16flash_fwd_kernelI23Flash_fwd_kernel_traitsILi64ELi128ELi64ELi4ELb0ELb0EN7cutlass6half_tE19Flash_kernel_traitsILi64ELi128ELi64ELi4ES2_EELb1ELb1ELb0ELb0ELb1EEv16Flash_fwd_params' for 'sm_80'
ptxas info    : Function properties for _Z16flash_fwd_kernelI23Flash_fwd_kernel_traitsILi64ELi128ELi64ELi4ELb0ELb0EN7cutlass6half_tE19Flash_kernel_traitsILi64ELi128ELi64ELi4ES2_EELb1ELb1ELb0ELb0ELb1EEv16Flash_fwd_params
    160 bytes stack frame, 178 bytes spill stores, 202 bytes spill loads
ptxas info    : Used 255 registers, 584 bytes cmem[0]
ptxas info    : Compiling entry function '_Z16flash_fwd_kernelI23Flash_fwd_kernel_traitsILi64ELi128ELi64ELi4ELb0ELb0EN7cutlass6half_tE19Flash_kernel_traitsILi64ELi128ELi64ELi4ES2_EELb1ELb1ELb0ELb1ELb0EEv16Flash_fwd_params' for 'sm_80'
ptxas info    : Function properties for _Z16flash_fwd_kernelI23Flash_fwd_kernel_traitsILi64ELi128ELi64ELi4ELb0ELb0EN7cutlass6half_tE19Flash_kernel_traitsILi64ELi128ELi64ELi4ES2_EELb1ELb1ELb0ELb1ELb0EEv16Flash_fwd_params
    0 bytes stack frame, 0 bytes spill stores, 0 bytes spill loads
ptxas info    : Used 244 registers, 584 bytes cmem[0]
ptxas info    : Compiling entry function '_Z16flash_fwd_kernelI23Flash_fwd_kernel_traitsILi64ELi128ELi64ELi4ELb0ELb0EN7cutlass6half_tE19Flash_kernel_traitsILi64ELi128ELi64ELi4ES2_EELb1ELb1ELb0ELb1ELb1EEv16Flash_fwd_params' for 'sm_80'
ptxas info    : Function properties for _Z16flash_fwd_kernelI23Flash_fwd_kernel_traitsILi64ELi128ELi64ELi4ELb0ELb0EN7cutlass6half_tE19Flash_kernel_traitsILi64ELi128ELi64ELi4ES2_EELb1ELb1ELb0ELb1ELb1EEv16Flash_fwd_params
    216 bytes stack frame, 212 bytes spill stores, 236 bytes spill loads
ptxas info    : Used 255 registers, 584 bytes cmem[0]
ptxas info    : Compiling entry function '_Z16flash_fwd_kernelI23Flash_fwd_kernel_traitsILi64ELi128ELi64ELi4ELb0ELb0EN7cutlass6half_tE19Flash_kernel_traitsILi64ELi128ELi64ELi4ES2_EELb1ELb1ELb1ELb0ELb0EEv16Flash_fwd_params' for 'sm_80'
ptxas info    : Function properties for _Z16flash_fwd_kernelI23Flash_fwd_kernel_traitsILi64ELi128ELi64ELi4ELb0ELb0EN7cutlass6half_tE19Flash_kernel_traitsILi64ELi128ELi64ELi4ES2_EELb1ELb1ELb1ELb0ELb0EEv16Flash_fwd_params
    0 bytes stack frame, 0 bytes spill stores, 0 bytes spill loads
ptxas info    : Used 248 registers, 584 bytes cmem[0]
ptxas info    : Compiling entry function '_Z16flash_fwd_kernelI23Flash_fwd_kernel_traitsILi64ELi128ELi64ELi4ELb0ELb0EN7cutlass6half_tE19Flash_kernel_traitsILi64ELi128ELi64ELi4ES2_EELb1ELb1ELb1ELb0ELb1EEv16Flash_fwd_params' for 'sm_80'
ptxas info    : Function properties for _Z16flash_fwd_kernelI23Flash_fwd_kernel_traitsILi64ELi128ELi64ELi4ELb0ELb0EN7cutlass6half_tE19Flash_kernel_traitsILi64ELi128ELi64ELi4ES2_EELb1ELb1ELb1ELb0ELb1EEv16Flash_fwd_params
    152 bytes stack frame, 142 bytes spill stores, 182 bytes spill loads
ptxas info    : Used 255 registers, 584 bytes cmem[0]
ptxas info    : Compiling entry function '_Z16flash_fwd_kernelI23Flash_fwd_kernel_traitsILi64ELi128ELi64ELi4ELb0ELb0EN7cutlass6half_tE19Flash_kernel_traitsILi64ELi128ELi64ELi4ES2_EELb1ELb1ELb1ELb1ELb0EEv16Flash_fwd_params' for 'sm_80'
ptxas info    : Function properties for _Z16flash_fwd_kernelI23Flash_fwd_kernel_traitsILi64ELi128ELi64ELi4ELb0ELb0EN7cutlass6half_tE19Flash_kernel_traitsILi64ELi128ELi64ELi4ES2_EELb1ELb1ELb1ELb1ELb0EEv16Flash_fwd_params
    0 bytes stack frame, 0 bytes spill stores, 0 bytes spill loads
ptxas info    : Used 246 registers, 584 bytes cmem[0]
ptxas info    : Compiling entry function '_Z16flash_fwd_kernelI23Flash_fwd_kernel_traitsILi64ELi128ELi64ELi4ELb0ELb0EN7cutlass6half_tE19Flash_kernel_traitsILi64ELi128ELi64ELi4ES2_EELb1ELb1ELb1ELb1ELb1EEv16Flash_fwd_params' for 'sm_80'
ptxas info    : Function properties for _Z16flash_fwd_kernelI23Flash_fwd_kernel_traitsILi64ELi128ELi64ELi4ELb0ELb0EN7cutlass6half_tE19Flash_kernel_traitsILi64ELi128ELi64ELi4ES2_EELb1ELb1ELb1ELb1ELb1EEv16Flash_fwd_params
    160 bytes stack frame, 152 bytes spill stores, 190 bytes spill loads
ptxas info    : Used 255 registers, 584 bytes cmem[0]
/anaconda/envs/flashattention/lib/python3.9/site-packages/torch/include/c10/core/SymInt.h(84): warning: integer conversion resulted in a change of sign

[29/37] /usr/local/cuda-11.4/bin/nvcc  -I/home/v-feiychen/flash-attention/csrc/flash_attn -I/home/v-feiychen/flash-attention/csrc/flash_attn/src -I/home/v-feiychen/flash-attention/csrc/cutlass/include -I/anaconda/envs/flashattention/lib/python3.9/site-packages/torch/include -I/anaconda/envs/flashattention/lib/python3.9/site-packages/torch/include/torch/csrc/api/include -I/anaconda/envs/flashattention/lib/python3.9/site-packages/torch/include/TH -I/anaconda/envs/flashattention/lib/python3.9/site-packages/torch/include/THC -I/usr/local/cuda-11.4/include -I/anaconda/envs/flashattention/include/python3.9 -c -c /home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_hdim64_bf16_sm80.cu -o /home/v-feiychen/flash-attention/build/temp.linux-x86_64-cpython-39/csrc/flash_attn/src/flash_fwd_hdim64_bf16_sm80.o -D__CUDA_NO_HALF_OPERATORS__ -D__CUDA_NO_HALF_CONVERSIONS__ -D__CUDA_NO_BFLOAT16_CONVERSIONS__ -D__CUDA_NO_HALF2_OPERATORS__ --expt-relaxed-constexpr --compiler-options ''"'"'-fPIC'"'"'' -O3 -std=c++17 -U__CUDA_NO_HALF_OPERATORS__ -U__CUDA_NO_HALF_CONVERSIONS__ -U__CUDA_NO_HALF2_OPERATORS__ -U__CUDA_NO_BFLOAT16_CONVERSIONS__ --expt-relaxed-constexpr --expt-extended-lambda --use_fast_math --ptxas-options=-v -lineinfo -gencode arch=compute_80,code=sm_80 --threads 4 -DTORCH_API_INCLUDE_EXTENSION_H '-DPYBIND11_COMPILER_TYPE="_gcc"' '-DPYBIND11_STDLIB="_libstdcpp"' '-DPYBIND11_BUILD_ABI="_cxxabi1011"' -DTORCH_EXTENSION_NAME=flash_attn_2_cuda -D_GLIBCXX_USE_CXX11_ABI=0
/anaconda/envs/flashattention/lib/python3.9/site-packages/torch/include/c10/core/SymInt.h(84): warning: integer conversion resulted in a change of sign

/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_kernel.h(140): warning: variable "MMA_M" was declared but never referenced
          detected during:
            instantiation of "void flash::compute_attn_1rowblock<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params,Is_Q_split_K>(const Params &, int, int, int) [with Kernel_traits=Flash_fwd_kernel_traits<64, 128, 64, 4, false, false, cutlass::bfloat16_t, Flash_kernel_traits<64, 128, 64, 4, cutlass::bfloat16_t>>, Is_dropout=true, Is_causal=true, Is_even_N=true, Is_even_K=true, Return_softmax=true, Params=Flash_fwd_params, Is_Q_split_K=false]" 
(589): here
            instantiation of "void flash::compute_attn<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &) [with Kernel_traits=Flash_fwd_kernel_traits<64, 128, 64, 4, false, false, cutlass::bfloat16_t, Flash_kernel_traits<64, 128, 64, 4, cutlass::bfloat16_t>>, Is_dropout=true, Is_causal=true, Is_even_N=true, Is_even_K=true, Return_softmax=true, Params=Flash_fwd_params]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(15): here
            instantiation of "void flash_fwd_kernel<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax>(Flash_fwd_params) [with Kernel_traits=Flash_fwd_kernel_traits<64, 128, 64, 4, false, false, cutlass::bfloat16_t, Flash_kernel_traits<64, 128, 64, 4, cutlass::bfloat16_t>>, Is_dropout=true, Is_causal=true, Is_even_N=true, Is_even_K=true, Return_softmax=true]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(34): here
            instantiation of "void run_flash_fwd<Kernel_traits,Is_dropout,Is_causal>(Flash_fwd_params &, cudaStream_t) [with Kernel_traits=Flash_fwd_kernel_traits<64, 128, 64, 4, false, false, cutlass::bfloat16_t, Flash_kernel_traits<64, 128, 64, 4, cutlass::bfloat16_t>>, Is_dropout=true, Is_causal=true]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(68): here
            instantiation of "void run_mha_fwd_hdim64<T>(Flash_fwd_params &, cudaStream_t) [with T=cutlass::bfloat16_t]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_hdim64_bf16_sm80.cu(18): here

/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_kernel.h(276): warning: variable "tQrQ" was declared but never referenced
          detected during:
            instantiation of "void flash::compute_attn_1rowblock<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params,Is_Q_split_K>(const Params &, int, int, int) [with Kernel_traits=Flash_fwd_kernel_traits<64, 128, 64, 4, false, false, cutlass::bfloat16_t, Flash_kernel_traits<64, 128, 64, 4, cutlass::bfloat16_t>>, Is_dropout=true, Is_causal=true, Is_even_N=true, Is_even_K=true, Return_softmax=true, Params=Flash_fwd_params, Is_Q_split_K=false]" 
(589): here
            instantiation of "void flash::compute_attn<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &) [with Kernel_traits=Flash_fwd_kernel_traits<64, 128, 64, 4, false, false, cutlass::bfloat16_t, Flash_kernel_traits<64, 128, 64, 4, cutlass::bfloat16_t>>, Is_dropout=true, Is_causal=true, Is_even_N=true, Is_even_K=true, Return_softmax=true, Params=Flash_fwd_params]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(15): here
            instantiation of "void flash_fwd_kernel<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax>(Flash_fwd_params) [with Kernel_traits=Flash_fwd_kernel_traits<64, 128, 64, 4, false, false, cutlass::bfloat16_t, Flash_kernel_traits<64, 128, 64, 4, cutlass::bfloat16_t>>, Is_dropout=true, Is_causal=true, Is_even_N=true, Is_even_K=true, Return_softmax=true]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(34): here
            instantiation of "void run_flash_fwd<Kernel_traits,Is_dropout,Is_causal>(Flash_fwd_params &, cudaStream_t) [with Kernel_traits=Flash_fwd_kernel_traits<64, 128, 64, 4, false, false, cutlass::bfloat16_t, Flash_kernel_traits<64, 128, 64, 4, cutlass::bfloat16_t>>, Is_dropout=true, Is_causal=true]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(68): here
            instantiation of "void run_mha_fwd_hdim64<T>(Flash_fwd_params &, cudaStream_t) [with T=cutlass::bfloat16_t]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_hdim64_bf16_sm80.cu(18): here

/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_kernel.h(140): warning: variable "MMA_M" was declared but never referenced
          detected during:
            instantiation of "void flash::compute_attn_1rowblock<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params,Is_Q_split_K>(const Params &, int, int, int) [with Kernel_traits=Flash_fwd_kernel_traits<64, 128, 64, 4, false, false, cutlass::bfloat16_t, Flash_kernel_traits<64, 128, 64, 4, cutlass::bfloat16_t>>, Is_dropout=true, Is_causal=true, Is_even_N=true, Is_even_K=true, Return_softmax=false, Params=Flash_fwd_params, Is_Q_split_K=false]" 
(589): here
            instantiation of "void flash::compute_attn<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &) [with Kernel_traits=Flash_fwd_kernel_traits<64, 128, 64, 4, false, false, cutlass::bfloat16_t, Flash_kernel_traits<64, 128, 64, 4, cutlass::bfloat16_t>>, Is_dropout=true, Is_causal=true, Is_even_N=true, Is_even_K=true, Return_softmax=false, Params=Flash_fwd_params]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(15): here
            instantiation of "void flash_fwd_kernel<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax>(Flash_fwd_params) [with Kernel_traits=Flash_fwd_kernel_traits<64, 128, 64, 4, false, false, cutlass::bfloat16_t, Flash_kernel_traits<64, 128, 64, 4, cutlass::bfloat16_t>>, Is_dropout=true, Is_causal=true, Is_even_N=true, Is_even_K=true, Return_softmax=false]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(34): here
            instantiation of "void run_flash_fwd<Kernel_traits,Is_dropout,Is_causal>(Flash_fwd_params &, cudaStream_t) [with Kernel_traits=Flash_fwd_kernel_traits<64, 128, 64, 4, false, false, cutlass::bfloat16_t, Flash_kernel_traits<64, 128, 64, 4, cutlass::bfloat16_t>>, Is_dropout=true, Is_causal=true]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(68): here
            instantiation of "void run_mha_fwd_hdim64<T>(Flash_fwd_params &, cudaStream_t) [with T=cutlass::bfloat16_t]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_hdim64_bf16_sm80.cu(18): here

/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_kernel.h(276): warning: variable "tQrQ" was declared but never referenced
          detected during:
            instantiation of "void flash::compute_attn_1rowblock<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params,Is_Q_split_K>(const Params &, int, int, int) [with Kernel_traits=Flash_fwd_kernel_traits<64, 128, 64, 4, false, false, cutlass::bfloat16_t, Flash_kernel_traits<64, 128, 64, 4, cutlass::bfloat16_t>>, Is_dropout=true, Is_causal=true, Is_even_N=true, Is_even_K=true, Return_softmax=false, Params=Flash_fwd_params, Is_Q_split_K=false]" 
(589): here
            instantiation of "void flash::compute_attn<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &) [with Kernel_traits=Flash_fwd_kernel_traits<64, 128, 64, 4, false, false, cutlass::bfloat16_t, Flash_kernel_traits<64, 128, 64, 4, cutlass::bfloat16_t>>, Is_dropout=true, Is_causal=true, Is_even_N=true, Is_even_K=true, Return_softmax=false, Params=Flash_fwd_params]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(15): here
            instantiation of "void flash_fwd_kernel<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax>(Flash_fwd_params) [with Kernel_traits=Flash_fwd_kernel_traits<64, 128, 64, 4, false, false, cutlass::bfloat16_t, Flash_kernel_traits<64, 128, 64, 4, cutlass::bfloat16_t>>, Is_dropout=true, Is_causal=true, Is_even_N=true, Is_even_K=true, Return_softmax=false]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(34): here
            instantiation of "void run_flash_fwd<Kernel_traits,Is_dropout,Is_causal>(Flash_fwd_params &, cudaStream_t) [with Kernel_traits=Flash_fwd_kernel_traits<64, 128, 64, 4, false, false, cutlass::bfloat16_t, Flash_kernel_traits<64, 128, 64, 4, cutlass::bfloat16_t>>, Is_dropout=true, Is_causal=true]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(68): here
            instantiation of "void run_mha_fwd_hdim64<T>(Flash_fwd_params &, cudaStream_t) [with T=cutlass::bfloat16_t]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_hdim64_bf16_sm80.cu(18): here

/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_kernel.h(140): warning: variable "MMA_M" was declared but never referenced
          detected during:
            instantiation of "void flash::compute_attn_1rowblock<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params,Is_Q_split_K>(const Params &, int, int, int) [with Kernel_traits=Flash_fwd_kernel_traits<64, 128, 64, 4, false, false, cutlass::bfloat16_t, Flash_kernel_traits<64, 128, 64, 4, cutlass::bfloat16_t>>, Is_dropout=true, Is_causal=true, Is_even_N=true, Is_even_K=false, Return_softmax=true, Params=Flash_fwd_params, Is_Q_split_K=false]" 
(589): here
            instantiation of "void flash::compute_attn<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &) [with Kernel_traits=Flash_fwd_kernel_traits<64, 128, 64, 4, false, false, cutlass::bfloat16_t, Flash_kernel_traits<64, 128, 64, 4, cutlass::bfloat16_t>>, Is_dropout=true, Is_causal=true, Is_even_N=true, Is_even_K=false, Return_softmax=true, Params=Flash_fwd_params]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(15): here
            instantiation of "void flash_fwd_kernel<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax>(Flash_fwd_params) [with Kernel_traits=Flash_fwd_kernel_traits<64, 128, 64, 4, false, false, cutlass::bfloat16_t, Flash_kernel_traits<64, 128, 64, 4, cutlass::bfloat16_t>>, Is_dropout=true, Is_causal=true, Is_even_N=true, Is_even_K=false, Return_softmax=true]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(34): here
            instantiation of "void run_flash_fwd<Kernel_traits,Is_dropout,Is_causal>(Flash_fwd_params &, cudaStream_t) [with Kernel_traits=Flash_fwd_kernel_traits<64, 128, 64, 4, false, false, cutlass::bfloat16_t, Flash_kernel_traits<64, 128, 64, 4, cutlass::bfloat16_t>>, Is_dropout=true, Is_causal=true]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(68): here
            instantiation of "void run_mha_fwd_hdim64<T>(Flash_fwd_params &, cudaStream_t) [with T=cutlass::bfloat16_t]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_hdim64_bf16_sm80.cu(18): here

/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_kernel.h(276): warning: variable "tQrQ" was declared but never referenced
          detected during:
            instantiation of "void flash::compute_attn_1rowblock<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params,Is_Q_split_K>(const Params &, int, int, int) [with Kernel_traits=Flash_fwd_kernel_traits<64, 128, 64, 4, false, false, cutlass::bfloat16_t, Flash_kernel_traits<64, 128, 64, 4, cutlass::bfloat16_t>>, Is_dropout=true, Is_causal=true, Is_even_N=true, Is_even_K=false, Return_softmax=true, Params=Flash_fwd_params, Is_Q_split_K=false]" 
(589): here
            instantiation of "void flash::compute_attn<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &) [with Kernel_traits=Flash_fwd_kernel_traits<64, 128, 64, 4, false, false, cutlass::bfloat16_t, Flash_kernel_traits<64, 128, 64, 4, cutlass::bfloat16_t>>, Is_dropout=true, Is_causal=true, Is_even_N=true, Is_even_K=false, Return_softmax=true, Params=Flash_fwd_params]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(15): here
            instantiation of "void flash_fwd_kernel<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax>(Flash_fwd_params) [with Kernel_traits=Flash_fwd_kernel_traits<64, 128, 64, 4, false, false, cutlass::bfloat16_t, Flash_kernel_traits<64, 128, 64, 4, cutlass::bfloat16_t>>, Is_dropout=true, Is_causal=true, Is_even_N=true, Is_even_K=false, Return_softmax=true]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(34): here
            instantiation of "void run_flash_fwd<Kernel_traits,Is_dropout,Is_causal>(Flash_fwd_params &, cudaStream_t) [with Kernel_traits=Flash_fwd_kernel_traits<64, 128, 64, 4, false, false, cutlass::bfloat16_t, Flash_kernel_traits<64, 128, 64, 4, cutlass::bfloat16_t>>, Is_dropout=true, Is_causal=true]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(68): here
            instantiation of "void run_mha_fwd_hdim64<T>(Flash_fwd_params &, cudaStream_t) [with T=cutlass::bfloat16_t]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_hdim64_bf16_sm80.cu(18): here

/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_kernel.h(140): warning: variable "MMA_M" was declared but never referenced
          detected during:
            instantiation of "void flash::compute_attn_1rowblock<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params,Is_Q_split_K>(const Params &, int, int, int) [with Kernel_traits=Flash_fwd_kernel_traits<64, 128, 64, 4, false, false, cutlass::bfloat16_t, Flash_kernel_traits<64, 128, 64, 4, cutlass::bfloat16_t>>, Is_dropout=true, Is_causal=true, Is_even_N=true, Is_even_K=false, Return_softmax=false, Params=Flash_fwd_params, Is_Q_split_K=false]" 
(589): here
            instantiation of "void flash::compute_attn<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &) [with Kernel_traits=Flash_fwd_kernel_traits<64, 128, 64, 4, false, false, cutlass::bfloat16_t, Flash_kernel_traits<64, 128, 64, 4, cutlass::bfloat16_t>>, Is_dropout=true, Is_causal=true, Is_even_N=true, Is_even_K=false, Return_softmax=false, Params=Flash_fwd_params]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(15): here
            instantiation of "void flash_fwd_kernel<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax>(Flash_fwd_params) [with Kernel_traits=Flash_fwd_kernel_traits<64, 128, 64, 4, false, false, cutlass::bfloat16_t, Flash_kernel_traits<64, 128, 64, 4, cutlass::bfloat16_t>>, Is_dropout=true, Is_causal=true, Is_even_N=true, Is_even_K=false, Return_softmax=false]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(34): here
            instantiation of "void run_flash_fwd<Kernel_traits,Is_dropout,Is_causal>(Flash_fwd_params &, cudaStream_t) [with Kernel_traits=Flash_fwd_kernel_traits<64, 128, 64, 4, false, false, cutlass::bfloat16_t, Flash_kernel_traits<64, 128, 64, 4, cutlass::bfloat16_t>>, Is_dropout=true, Is_causal=true]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(68): here
            instantiation of "void run_mha_fwd_hdim64<T>(Flash_fwd_params &, cudaStream_t) [with T=cutlass::bfloat16_t]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_hdim64_bf16_sm80.cu(18): here

/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_kernel.h(276): warning: variable "tQrQ" was declared but never referenced
          detected during:
            instantiation of "void flash::compute_attn_1rowblock<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params,Is_Q_split_K>(const Params &, int, int, int) [with Kernel_traits=Flash_fwd_kernel_traits<64, 128, 64, 4, false, false, cutlass::bfloat16_t, Flash_kernel_traits<64, 128, 64, 4, cutlass::bfloat16_t>>, Is_dropout=true, Is_causal=true, Is_even_N=true, Is_even_K=false, Return_softmax=false, Params=Flash_fwd_params, Is_Q_split_K=false]" 
(589): here
            instantiation of "void flash::compute_attn<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &) [with Kernel_traits=Flash_fwd_kernel_traits<64, 128, 64, 4, false, false, cutlass::bfloat16_t, Flash_kernel_traits<64, 128, 64, 4, cutlass::bfloat16_t>>, Is_dropout=true, Is_causal=true, Is_even_N=true, Is_even_K=false, Return_softmax=false, Params=Flash_fwd_params]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(15): here
            instantiation of "void flash_fwd_kernel<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax>(Flash_fwd_params) [with Kernel_traits=Flash_fwd_kernel_traits<64, 128, 64, 4, false, false, cutlass::bfloat16_t, Flash_kernel_traits<64, 128, 64, 4, cutlass::bfloat16_t>>, Is_dropout=true, Is_causal=true, Is_even_N=true, Is_even_K=false, Return_softmax=false]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(34): here
            instantiation of "void run_flash_fwd<Kernel_traits,Is_dropout,Is_causal>(Flash_fwd_params &, cudaStream_t) [with Kernel_traits=Flash_fwd_kernel_traits<64, 128, 64, 4, false, false, cutlass::bfloat16_t, Flash_kernel_traits<64, 128, 64, 4, cutlass::bfloat16_t>>, Is_dropout=true, Is_causal=true]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(68): here
            instantiation of "void run_mha_fwd_hdim64<T>(Flash_fwd_params &, cudaStream_t) [with T=cutlass::bfloat16_t]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_hdim64_bf16_sm80.cu(18): here

/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_kernel.h(140): warning: variable "MMA_M" was declared but never referenced
          detected during:
            instantiation of "void flash::compute_attn_1rowblock<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params,Is_Q_split_K>(const Params &, int, int, int) [with Kernel_traits=Flash_fwd_kernel_traits<64, 128, 64, 4, false, false, cutlass::bfloat16_t, Flash_kernel_traits<64, 128, 64, 4, cutlass::bfloat16_t>>, Is_dropout=true, Is_causal=true, Is_even_N=false, Is_even_K=true, Return_softmax=true, Params=Flash_fwd_params, Is_Q_split_K=false]" 
(589): here
            instantiation of "void flash::compute_attn<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &) [with Kernel_traits=Flash_fwd_kernel_traits<64, 128, 64, 4, false, false, cutlass::bfloat16_t, Flash_kernel_traits<64, 128, 64, 4, cutlass::bfloat16_t>>, Is_dropout=true, Is_causal=true, Is_even_N=false, Is_even_K=true, Return_softmax=true, Params=Flash_fwd_params]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(15): here
            instantiation of "void flash_fwd_kernel<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax>(Flash_fwd_params) [with Kernel_traits=Flash_fwd_kernel_traits<64, 128, 64, 4, false, false, cutlass::bfloat16_t, Flash_kernel_traits<64, 128, 64, 4, cutlass::bfloat16_t>>, Is_dropout=true, Is_causal=true, Is_even_N=false, Is_even_K=true, Return_softmax=true]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(34): here
            instantiation of "void run_flash_fwd<Kernel_traits,Is_dropout,Is_causal>(Flash_fwd_params &, cudaStream_t) [with Kernel_traits=Flash_fwd_kernel_traits<64, 128, 64, 4, false, false, cutlass::bfloat16_t, Flash_kernel_traits<64, 128, 64, 4, cutlass::bfloat16_t>>, Is_dropout=true, Is_causal=true]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(68): here
            instantiation of "void run_mha_fwd_hdim64<T>(Flash_fwd_params &, cudaStream_t) [with T=cutlass::bfloat16_t]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_hdim64_bf16_sm80.cu(18): here

/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_kernel.h(276): warning: variable "tQrQ" was declared but never referenced
          detected during:
            instantiation of "void flash::compute_attn_1rowblock<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params,Is_Q_split_K>(const Params &, int, int, int) [with Kernel_traits=Flash_fwd_kernel_traits<64, 128, 64, 4, false, false, cutlass::bfloat16_t, Flash_kernel_traits<64, 128, 64, 4, cutlass::bfloat16_t>>, Is_dropout=true, Is_causal=true, Is_even_N=false, Is_even_K=true, Return_softmax=true, Params=Flash_fwd_params, Is_Q_split_K=false]" 
(589): here
            instantiation of "void flash::compute_attn<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &) [with Kernel_traits=Flash_fwd_kernel_traits<64, 128, 64, 4, false, false, cutlass::bfloat16_t, Flash_kernel_traits<64, 128, 64, 4, cutlass::bfloat16_t>>, Is_dropout=true, Is_causal=true, Is_even_N=false, Is_even_K=true, Return_softmax=true, Params=Flash_fwd_params]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(15): here
            instantiation of "void flash_fwd_kernel<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax>(Flash_fwd_params) [with Kernel_traits=Flash_fwd_kernel_traits<64, 128, 64, 4, false, false, cutlass::bfloat16_t, Flash_kernel_traits<64, 128, 64, 4, cutlass::bfloat16_t>>, Is_dropout=true, Is_causal=true, Is_even_N=false, Is_even_K=true, Return_softmax=true]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(34): here
            instantiation of "void run_flash_fwd<Kernel_traits,Is_dropout,Is_causal>(Flash_fwd_params &, cudaStream_t) [with Kernel_traits=Flash_fwd_kernel_traits<64, 128, 64, 4, false, false, cutlass::bfloat16_t, Flash_kernel_traits<64, 128, 64, 4, cutlass::bfloat16_t>>, Is_dropout=true, Is_causal=true]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(68): here
            instantiation of "void run_mha_fwd_hdim64<T>(Flash_fwd_params &, cudaStream_t) [with T=cutlass::bfloat16_t]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_hdim64_bf16_sm80.cu(18): here

/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_kernel.h(140): warning: variable "MMA_M" was declared but never referenced
          detected during:
            instantiation of "void flash::compute_attn_1rowblock<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params,Is_Q_split_K>(const Params &, int, int, int) [with Kernel_traits=Flash_fwd_kernel_traits<64, 128, 64, 4, false, false, cutlass::bfloat16_t, Flash_kernel_traits<64, 128, 64, 4, cutlass::bfloat16_t>>, Is_dropout=true, Is_causal=true, Is_even_N=false, Is_even_K=true, Return_softmax=false, Params=Flash_fwd_params, Is_Q_split_K=false]" 
(589): here
            instantiation of "void flash::compute_attn<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &) [with Kernel_traits=Flash_fwd_kernel_traits<64, 128, 64, 4, false, false, cutlass::bfloat16_t, Flash_kernel_traits<64, 128, 64, 4, cutlass::bfloat16_t>>, Is_dropout=true, Is_causal=true, Is_even_N=false, Is_even_K=true, Return_softmax=false, Params=Flash_fwd_params]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(15): here
            instantiation of "void flash_fwd_kernel<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax>(Flash_fwd_params) [with Kernel_traits=Flash_fwd_kernel_traits<64, 128, 64, 4, false, false, cutlass::bfloat16_t, Flash_kernel_traits<64, 128, 64, 4, cutlass::bfloat16_t>>, Is_dropout=true, Is_causal=true, Is_even_N=false, Is_even_K=true, Return_softmax=false]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(34): here
            instantiation of "void run_flash_fwd<Kernel_traits,Is_dropout,Is_causal>(Flash_fwd_params &, cudaStream_t) [with Kernel_traits=Flash_fwd_kernel_traits<64, 128, 64, 4, false, false, cutlass::bfloat16_t, Flash_kernel_traits<64, 128, 64, 4, cutlass::bfloat16_t>>, Is_dropout=true, Is_causal=true]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(68): here
            instantiation of "void run_mha_fwd_hdim64<T>(Flash_fwd_params &, cudaStream_t) [with T=cutlass::bfloat16_t]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_hdim64_bf16_sm80.cu(18): here

/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_kernel.h(276): warning: variable "tQrQ" was declared but never referenced
          detected during:
            instantiation of "void flash::compute_attn_1rowblock<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params,Is_Q_split_K>(const Params &, int, int, int) [with Kernel_traits=Flash_fwd_kernel_traits<64, 128, 64, 4, false, false, cutlass::bfloat16_t, Flash_kernel_traits<64, 128, 64, 4, cutlass::bfloat16_t>>, Is_dropout=true, Is_causal=true, Is_even_N=false, Is_even_K=true, Return_softmax=false, Params=Flash_fwd_params, Is_Q_split_K=false]" 
(589): here
            instantiation of "void flash::compute_attn<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &) [with Kernel_traits=Flash_fwd_kernel_traits<64, 128, 64, 4, false, false, cutlass::bfloat16_t, Flash_kernel_traits<64, 128, 64, 4, cutlass::bfloat16_t>>, Is_dropout=true, Is_causal=true, Is_even_N=false, Is_even_K=true, Return_softmax=false, Params=Flash_fwd_params]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(15): here
            instantiation of "void flash_fwd_kernel<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax>(Flash_fwd_params) [with Kernel_traits=Flash_fwd_kernel_traits<64, 128, 64, 4, false, false, cutlass::bfloat16_t, Flash_kernel_traits<64, 128, 64, 4, cutlass::bfloat16_t>>, Is_dropout=true, Is_causal=true, Is_even_N=false, Is_even_K=true, Return_softmax=false]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(34): here
            instantiation of "void run_flash_fwd<Kernel_traits,Is_dropout,Is_causal>(Flash_fwd_params &, cudaStream_t) [with Kernel_traits=Flash_fwd_kernel_traits<64, 128, 64, 4, false, false, cutlass::bfloat16_t, Flash_kernel_traits<64, 128, 64, 4, cutlass::bfloat16_t>>, Is_dropout=true, Is_causal=true]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(68): here
            instantiation of "void run_mha_fwd_hdim64<T>(Flash_fwd_params &, cudaStream_t) [with T=cutlass::bfloat16_t]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_hdim64_bf16_sm80.cu(18): here

/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_kernel.h(140): warning: variable "MMA_M" was declared but never referenced
          detected during:
            instantiation of "void flash::compute_attn_1rowblock<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params,Is_Q_split_K>(const Params &, int, int, int) [with Kernel_traits=Flash_fwd_kernel_traits<64, 128, 64, 4, false, false, cutlass::bfloat16_t, Flash_kernel_traits<64, 128, 64, 4, cutlass::bfloat16_t>>, Is_dropout=true, Is_causal=true, Is_even_N=false, Is_even_K=false, Return_softmax=true, Params=Flash_fwd_params, Is_Q_split_K=false]" 
(589): here
            instantiation of "void flash::compute_attn<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &) [with Kernel_traits=Flash_fwd_kernel_traits<64, 128, 64, 4, false, false, cutlass::bfloat16_t, Flash_kernel_traits<64, 128, 64, 4, cutlass::bfloat16_t>>, Is_dropout=true, Is_causal=true, Is_even_N=false, Is_even_K=false, Return_softmax=true, Params=Flash_fwd_params]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(15): here
            instantiation of "void flash_fwd_kernel<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax>(Flash_fwd_params) [with Kernel_traits=Flash_fwd_kernel_traits<64, 128, 64, 4, false, false, cutlass::bfloat16_t, Flash_kernel_traits<64, 128, 64, 4, cutlass::bfloat16_t>>, Is_dropout=true, Is_causal=true, Is_even_N=false, Is_even_K=false, Return_softmax=true]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(34): here
            instantiation of "void run_flash_fwd<Kernel_traits,Is_dropout,Is_causal>(Flash_fwd_params &, cudaStream_t) [with Kernel_traits=Flash_fwd_kernel_traits<64, 128, 64, 4, false, false, cutlass::bfloat16_t, Flash_kernel_traits<64, 128, 64, 4, cutlass::bfloat16_t>>, Is_dropout=true, Is_causal=true]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(68): here
            instantiation of "void run_mha_fwd_hdim64<T>(Flash_fwd_params &, cudaStream_t) [with T=cutlass::bfloat16_t]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_hdim64_bf16_sm80.cu(18): here

/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_kernel.h(276): warning: variable "tQrQ" was declared but never referenced
          detected during:
            instantiation of "void flash::compute_attn_1rowblock<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params,Is_Q_split_K>(const Params &, int, int, int) [with Kernel_traits=Flash_fwd_kernel_traits<64, 128, 64, 4, false, false, cutlass::bfloat16_t, Flash_kernel_traits<64, 128, 64, 4, cutlass::bfloat16_t>>, Is_dropout=true, Is_causal=true, Is_even_N=false, Is_even_K=false, Return_softmax=true, Params=Flash_fwd_params, Is_Q_split_K=false]" 
(589): here
            instantiation of "void flash::compute_attn<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &) [with Kernel_traits=Flash_fwd_kernel_traits<64, 128, 64, 4, false, false, cutlass::bfloat16_t, Flash_kernel_traits<64, 128, 64, 4, cutlass::bfloat16_t>>, Is_dropout=true, Is_causal=true, Is_even_N=false, Is_even_K=false, Return_softmax=true, Params=Flash_fwd_params]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(15): here
            instantiation of "void flash_fwd_kernel<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax>(Flash_fwd_params) [with Kernel_traits=Flash_fwd_kernel_traits<64, 128, 64, 4, false, false, cutlass::bfloat16_t, Flash_kernel_traits<64, 128, 64, 4, cutlass::bfloat16_t>>, Is_dropout=true, Is_causal=true, Is_even_N=false, Is_even_K=false, Return_softmax=true]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(34): here
            instantiation of "void run_flash_fwd<Kernel_traits,Is_dropout,Is_causal>(Flash_fwd_params &, cudaStream_t) [with Kernel_traits=Flash_fwd_kernel_traits<64, 128, 64, 4, false, false, cutlass::bfloat16_t, Flash_kernel_traits<64, 128, 64, 4, cutlass::bfloat16_t>>, Is_dropout=true, Is_causal=true]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(68): here
            instantiation of "void run_mha_fwd_hdim64<T>(Flash_fwd_params &, cudaStream_t) [with T=cutlass::bfloat16_t]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_hdim64_bf16_sm80.cu(18): here

/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_kernel.h(140): warning: variable "MMA_M" was declared but never referenced
          detected during:
            instantiation of "void flash::compute_attn_1rowblock<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params,Is_Q_split_K>(const Params &, int, int, int) [with Kernel_traits=Flash_fwd_kernel_traits<64, 128, 64, 4, false, false, cutlass::bfloat16_t, Flash_kernel_traits<64, 128, 64, 4, cutlass::bfloat16_t>>, Is_dropout=true, Is_causal=true, Is_even_N=false, Is_even_K=false, Return_softmax=false, Params=Flash_fwd_params, Is_Q_split_K=false]" 
(589): here
            instantiation of "void flash::compute_attn<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &) [with Kernel_traits=Flash_fwd_kernel_traits<64, 128, 64, 4, false, false, cutlass::bfloat16_t, Flash_kernel_traits<64, 128, 64, 4, cutlass::bfloat16_t>>, Is_dropout=true, Is_causal=true, Is_even_N=false, Is_even_K=false, Return_softmax=false, Params=Flash_fwd_params]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(15): here
            instantiation of "void flash_fwd_kernel<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax>(Flash_fwd_params) [with Kernel_traits=Flash_fwd_kernel_traits<64, 128, 64, 4, false, false, cutlass::bfloat16_t, Flash_kernel_traits<64, 128, 64, 4, cutlass::bfloat16_t>>, Is_dropout=true, Is_causal=true, Is_even_N=false, Is_even_K=false, Return_softmax=false]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(34): here
            instantiation of "void run_flash_fwd<Kernel_traits,Is_dropout,Is_causal>(Flash_fwd_params &, cudaStream_t) [with Kernel_traits=Flash_fwd_kernel_traits<64, 128, 64, 4, false, false, cutlass::bfloat16_t, Flash_kernel_traits<64, 128, 64, 4, cutlass::bfloat16_t>>, Is_dropout=true, Is_causal=true]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(68): here
            instantiation of "void run_mha_fwd_hdim64<T>(Flash_fwd_params &, cudaStream_t) [with T=cutlass::bfloat16_t]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_hdim64_bf16_sm80.cu(18): here

/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_kernel.h(276): warning: variable "tQrQ" was declared but never referenced
          detected during:
            instantiation of "void flash::compute_attn_1rowblock<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params,Is_Q_split_K>(const Params &, int, int, int) [with Kernel_traits=Flash_fwd_kernel_traits<64, 128, 64, 4, false, false, cutlass::bfloat16_t, Flash_kernel_traits<64, 128, 64, 4, cutlass::bfloat16_t>>, Is_dropout=true, Is_causal=true, Is_even_N=false, Is_even_K=false, Return_softmax=false, Params=Flash_fwd_params, Is_Q_split_K=false]" 
(589): here
            instantiation of "void flash::compute_attn<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &) [with Kernel_traits=Flash_fwd_kernel_traits<64, 128, 64, 4, false, false, cutlass::bfloat16_t, Flash_kernel_traits<64, 128, 64, 4, cutlass::bfloat16_t>>, Is_dropout=true, Is_causal=true, Is_even_N=false, Is_even_K=false, Return_softmax=false, Params=Flash_fwd_params]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(15): here
            instantiation of "void flash_fwd_kernel<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax>(Flash_fwd_params) [with Kernel_traits=Flash_fwd_kernel_traits<64, 128, 64, 4, false, false, cutlass::bfloat16_t, Flash_kernel_traits<64, 128, 64, 4, cutlass::bfloat16_t>>, Is_dropout=true, Is_causal=true, Is_even_N=false, Is_even_K=false, Return_softmax=false]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(34): here
            instantiation of "void run_flash_fwd<Kernel_traits,Is_dropout,Is_causal>(Flash_fwd_params &, cudaStream_t) [with Kernel_traits=Flash_fwd_kernel_traits<64, 128, 64, 4, false, false, cutlass::bfloat16_t, Flash_kernel_traits<64, 128, 64, 4, cutlass::bfloat16_t>>, Is_dropout=true, Is_causal=true]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(68): here
            instantiation of "void run_mha_fwd_hdim64<T>(Flash_fwd_params &, cudaStream_t) [with T=cutlass::bfloat16_t]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_hdim64_bf16_sm80.cu(18): here

/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_kernel.h(140): warning: variable "MMA_M" was declared but never referenced
          detected during:
            instantiation of "void flash::compute_attn_1rowblock<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params,Is_Q_split_K>(const Params &, int, int, int) [with Kernel_traits=Flash_fwd_kernel_traits<64, 128, 64, 4, false, false, cutlass::bfloat16_t, Flash_kernel_traits<64, 128, 64, 4, cutlass::bfloat16_t>>, Is_dropout=true, Is_causal=false, Is_even_N=true, Is_even_K=true, Return_softmax=true, Params=Flash_fwd_params, Is_Q_split_K=false]" 
(589): here
            instantiation of "void flash::compute_attn<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &) [with Kernel_traits=Flash_fwd_kernel_traits<64, 128, 64, 4, false, false, cutlass::bfloat16_t, Flash_kernel_traits<64, 128, 64, 4, cutlass::bfloat16_t>>, Is_dropout=true, Is_causal=false, Is_even_N=true, Is_even_K=true, Return_softmax=true, Params=Flash_fwd_params]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(15): here
            instantiation of "void flash_fwd_kernel<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax>(Flash_fwd_params) [with Kernel_traits=Flash_fwd_kernel_traits<64, 128, 64, 4, false, false, cutlass::bfloat16_t, Flash_kernel_traits<64, 128, 64, 4, cutlass::bfloat16_t>>, Is_dropout=true, Is_causal=false, Is_even_N=true, Is_even_K=true, Return_softmax=true]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(34): here
            instantiation of "void run_flash_fwd<Kernel_traits,Is_dropout,Is_causal>(Flash_fwd_params &, cudaStream_t) [with Kernel_traits=Flash_fwd_kernel_traits<64, 128, 64, 4, false, false, cutlass::bfloat16_t, Flash_kernel_traits<64, 128, 64, 4, cutlass::bfloat16_t>>, Is_dropout=true, Is_causal=false]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(68): here
            instantiation of "void run_mha_fwd_hdim64<T>(Flash_fwd_params &, cudaStream_t) [with T=cutlass::bfloat16_t]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_hdim64_bf16_sm80.cu(18): here

/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_kernel.h(276): warning: variable "tQrQ" was declared but never referenced
          detected during:
            instantiation of "void flash::compute_attn_1rowblock<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params,Is_Q_split_K>(const Params &, int, int, int) [with Kernel_traits=Flash_fwd_kernel_traits<64, 128, 64, 4, false, false, cutlass::bfloat16_t, Flash_kernel_traits<64, 128, 64, 4, cutlass::bfloat16_t>>, Is_dropout=true, Is_causal=false, Is_even_N=true, Is_even_K=true, Return_softmax=true, Params=Flash_fwd_params, Is_Q_split_K=false]" 
(589): here
            instantiation of "void flash::compute_attn<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &) [with Kernel_traits=Flash_fwd_kernel_traits<64, 128, 64, 4, false, false, cutlass::bfloat16_t, Flash_kernel_traits<64, 128, 64, 4, cutlass::bfloat16_t>>, Is_dropout=true, Is_causal=false, Is_even_N=true, Is_even_K=true, Return_softmax=true, Params=Flash_fwd_params]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(15): here
            instantiation of "void flash_fwd_kernel<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax>(Flash_fwd_params) [with Kernel_traits=Flash_fwd_kernel_traits<64, 128, 64, 4, false, false, cutlass::bfloat16_t, Flash_kernel_traits<64, 128, 64, 4, cutlass::bfloat16_t>>, Is_dropout=true, Is_causal=false, Is_even_N=true, Is_even_K=true, Return_softmax=true]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(34): here
            instantiation of "void run_flash_fwd<Kernel_traits,Is_dropout,Is_causal>(Flash_fwd_params &, cudaStream_t) [with Kernel_traits=Flash_fwd_kernel_traits<64, 128, 64, 4, false, false, cutlass::bfloat16_t, Flash_kernel_traits<64, 128, 64, 4, cutlass::bfloat16_t>>, Is_dropout=true, Is_causal=false]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(68): here
            instantiation of "void run_mha_fwd_hdim64<T>(Flash_fwd_params &, cudaStream_t) [with T=cutlass::bfloat16_t]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_hdim64_bf16_sm80.cu(18): here

/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_kernel.h(140): warning: variable "MMA_M" was declared but never referenced
          detected during:
            instantiation of "void flash::compute_attn_1rowblock<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params,Is_Q_split_K>(const Params &, int, int, int) [with Kernel_traits=Flash_fwd_kernel_traits<64, 128, 64, 4, false, false, cutlass::bfloat16_t, Flash_kernel_traits<64, 128, 64, 4, cutlass::bfloat16_t>>, Is_dropout=true, Is_causal=false, Is_even_N=true, Is_even_K=true, Return_softmax=false, Params=Flash_fwd_params, Is_Q_split_K=false]" 
(589): here
            instantiation of "void flash::compute_attn<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &) [with Kernel_traits=Flash_fwd_kernel_traits<64, 128, 64, 4, false, false, cutlass::bfloat16_t, Flash_kernel_traits<64, 128, 64, 4, cutlass::bfloat16_t>>, Is_dropout=true, Is_causal=false, Is_even_N=true, Is_even_K=true, Return_softmax=false, Params=Flash_fwd_params]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(15): here
            instantiation of "void flash_fwd_kernel<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax>(Flash_fwd_params) [with Kernel_traits=Flash_fwd_kernel_traits<64, 128, 64, 4, false, false, cutlass::bfloat16_t, Flash_kernel_traits<64, 128, 64, 4, cutlass::bfloat16_t>>, Is_dropout=true, Is_causal=false, Is_even_N=true, Is_even_K=true, Return_softmax=false]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(34): here
            instantiation of "void run_flash_fwd<Kernel_traits,Is_dropout,Is_causal>(Flash_fwd_params &, cudaStream_t) [with Kernel_traits=Flash_fwd_kernel_traits<64, 128, 64, 4, false, false, cutlass::bfloat16_t, Flash_kernel_traits<64, 128, 64, 4, cutlass::bfloat16_t>>, Is_dropout=true, Is_causal=false]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(68): here
            instantiation of "void run_mha_fwd_hdim64<T>(Flash_fwd_params &, cudaStream_t) [with T=cutlass::bfloat16_t]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_hdim64_bf16_sm80.cu(18): here

/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_kernel.h(276): warning: variable "tQrQ" was declared but never referenced
          detected during:
            instantiation of "void flash::compute_attn_1rowblock<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params,Is_Q_split_K>(const Params &, int, int, int) [with Kernel_traits=Flash_fwd_kernel_traits<64, 128, 64, 4, false, false, cutlass::bfloat16_t, Flash_kernel_traits<64, 128, 64, 4, cutlass::bfloat16_t>>, Is_dropout=true, Is_causal=false, Is_even_N=true, Is_even_K=true, Return_softmax=false, Params=Flash_fwd_params, Is_Q_split_K=false]" 
(589): here
            instantiation of "void flash::compute_attn<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &) [with Kernel_traits=Flash_fwd_kernel_traits<64, 128, 64, 4, false, false, cutlass::bfloat16_t, Flash_kernel_traits<64, 128, 64, 4, cutlass::bfloat16_t>>, Is_dropout=true, Is_causal=false, Is_even_N=true, Is_even_K=true, Return_softmax=false, Params=Flash_fwd_params]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(15): here
            instantiation of "void flash_fwd_kernel<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax>(Flash_fwd_params) [with Kernel_traits=Flash_fwd_kernel_traits<64, 128, 64, 4, false, false, cutlass::bfloat16_t, Flash_kernel_traits<64, 128, 64, 4, cutlass::bfloat16_t>>, Is_dropout=true, Is_causal=false, Is_even_N=true, Is_even_K=true, Return_softmax=false]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(34): here
            instantiation of "void run_flash_fwd<Kernel_traits,Is_dropout,Is_causal>(Flash_fwd_params &, cudaStream_t) [with Kernel_traits=Flash_fwd_kernel_traits<64, 128, 64, 4, false, false, cutlass::bfloat16_t, Flash_kernel_traits<64, 128, 64, 4, cutlass::bfloat16_t>>, Is_dropout=true, Is_causal=false]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(68): here
            instantiation of "void run_mha_fwd_hdim64<T>(Flash_fwd_params &, cudaStream_t) [with T=cutlass::bfloat16_t]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_hdim64_bf16_sm80.cu(18): here

/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_kernel.h(140): warning: variable "MMA_M" was declared but never referenced
          detected during:
            instantiation of "void flash::compute_attn_1rowblock<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params,Is_Q_split_K>(const Params &, int, int, int) [with Kernel_traits=Flash_fwd_kernel_traits<64, 128, 64, 4, false, false, cutlass::bfloat16_t, Flash_kernel_traits<64, 128, 64, 4, cutlass::bfloat16_t>>, Is_dropout=true, Is_causal=false, Is_even_N=true, Is_even_K=false, Return_softmax=true, Params=Flash_fwd_params, Is_Q_split_K=false]" 
(589): here
            instantiation of "void flash::compute_attn<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &) [with Kernel_traits=Flash_fwd_kernel_traits<64, 128, 64, 4, false, false, cutlass::bfloat16_t, Flash_kernel_traits<64, 128, 64, 4, cutlass::bfloat16_t>>, Is_dropout=true, Is_causal=false, Is_even_N=true, Is_even_K=false, Return_softmax=true, Params=Flash_fwd_params]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(15): here
            instantiation of "void flash_fwd_kernel<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax>(Flash_fwd_params) [with Kernel_traits=Flash_fwd_kernel_traits<64, 128, 64, 4, false, false, cutlass::bfloat16_t, Flash_kernel_traits<64, 128, 64, 4, cutlass::bfloat16_t>>, Is_dropout=true, Is_causal=false, Is_even_N=true, Is_even_K=false, Return_softmax=true]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(34): here
            instantiation of "void run_flash_fwd<Kernel_traits,Is_dropout,Is_causal>(Flash_fwd_params &, cudaStream_t) [with Kernel_traits=Flash_fwd_kernel_traits<64, 128, 64, 4, false, false, cutlass::bfloat16_t, Flash_kernel_traits<64, 128, 64, 4, cutlass::bfloat16_t>>, Is_dropout=true, Is_causal=false]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(68): here
            instantiation of "void run_mha_fwd_hdim64<T>(Flash_fwd_params &, cudaStream_t) [with T=cutlass::bfloat16_t]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_hdim64_bf16_sm80.cu(18): here

/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_kernel.h(276): warning: variable "tQrQ" was declared but never referenced
          detected during:
            instantiation of "void flash::compute_attn_1rowblock<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params,Is_Q_split_K>(const Params &, int, int, int) [with Kernel_traits=Flash_fwd_kernel_traits<64, 128, 64, 4, false, false, cutlass::bfloat16_t, Flash_kernel_traits<64, 128, 64, 4, cutlass::bfloat16_t>>, Is_dropout=true, Is_causal=false, Is_even_N=true, Is_even_K=false, Return_softmax=true, Params=Flash_fwd_params, Is_Q_split_K=false]" 
(589): here
            instantiation of "void flash::compute_attn<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &) [with Kernel_traits=Flash_fwd_kernel_traits<64, 128, 64, 4, false, false, cutlass::bfloat16_t, Flash_kernel_traits<64, 128, 64, 4, cutlass::bfloat16_t>>, Is_dropout=true, Is_causal=false, Is_even_N=true, Is_even_K=false, Return_softmax=true, Params=Flash_fwd_params]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(15): here
            instantiation of "void flash_fwd_kernel<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax>(Flash_fwd_params) [with Kernel_traits=Flash_fwd_kernel_traits<64, 128, 64, 4, false, false, cutlass::bfloat16_t, Flash_kernel_traits<64, 128, 64, 4, cutlass::bfloat16_t>>, Is_dropout=true, Is_causal=false, Is_even_N=true, Is_even_K=false, Return_softmax=true]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(34): here
            instantiation of "void run_flash_fwd<Kernel_traits,Is_dropout,Is_causal>(Flash_fwd_params &, cudaStream_t) [with Kernel_traits=Flash_fwd_kernel_traits<64, 128, 64, 4, false, false, cutlass::bfloat16_t, Flash_kernel_traits<64, 128, 64, 4, cutlass::bfloat16_t>>, Is_dropout=true, Is_causal=false]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(68): here
            instantiation of "void run_mha_fwd_hdim64<T>(Flash_fwd_params &, cudaStream_t) [with T=cutlass::bfloat16_t]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_hdim64_bf16_sm80.cu(18): here

/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_kernel.h(140): warning: variable "MMA_M" was declared but never referenced
          detected during:
            instantiation of "void flash::compute_attn_1rowblock<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params,Is_Q_split_K>(const Params &, int, int, int) [with Kernel_traits=Flash_fwd_kernel_traits<64, 128, 64, 4, false, false, cutlass::bfloat16_t, Flash_kernel_traits<64, 128, 64, 4, cutlass::bfloat16_t>>, Is_dropout=true, Is_causal=false, Is_even_N=true, Is_even_K=false, Return_softmax=false, Params=Flash_fwd_params, Is_Q_split_K=false]" 
(589): here
            instantiation of "void flash::compute_attn<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &) [with Kernel_traits=Flash_fwd_kernel_traits<64, 128, 64, 4, false, false, cutlass::bfloat16_t, Flash_kernel_traits<64, 128, 64, 4, cutlass::bfloat16_t>>, Is_dropout=true, Is_causal=false, Is_even_N=true, Is_even_K=false, Return_softmax=false, Params=Flash_fwd_params]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(15): here
            instantiation of "void flash_fwd_kernel<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax>(Flash_fwd_params) [with Kernel_traits=Flash_fwd_kernel_traits<64, 128, 64, 4, false, false, cutlass::bfloat16_t, Flash_kernel_traits<64, 128, 64, 4, cutlass::bfloat16_t>>, Is_dropout=true, Is_causal=false, Is_even_N=true, Is_even_K=false, Return_softmax=false]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(34): here
            instantiation of "void run_flash_fwd<Kernel_traits,Is_dropout,Is_causal>(Flash_fwd_params &, cudaStream_t) [with Kernel_traits=Flash_fwd_kernel_traits<64, 128, 64, 4, false, false, cutlass::bfloat16_t, Flash_kernel_traits<64, 128, 64, 4, cutlass::bfloat16_t>>, Is_dropout=true, Is_causal=false]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(68): here
            instantiation of "void run_mha_fwd_hdim64<T>(Flash_fwd_params &, cudaStream_t) [with T=cutlass::bfloat16_t]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_hdim64_bf16_sm80.cu(18): here

/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_kernel.h(276): warning: variable "tQrQ" was declared but never referenced
          detected during:
            instantiation of "void flash::compute_attn_1rowblock<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params,Is_Q_split_K>(const Params &, int, int, int) [with Kernel_traits=Flash_fwd_kernel_traits<64, 128, 64, 4, false, false, cutlass::bfloat16_t, Flash_kernel_traits<64, 128, 64, 4, cutlass::bfloat16_t>>, Is_dropout=true, Is_causal=false, Is_even_N=true, Is_even_K=false, Return_softmax=false, Params=Flash_fwd_params, Is_Q_split_K=false]" 
(589): here
            instantiation of "void flash::compute_attn<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &) [with Kernel_traits=Flash_fwd_kernel_traits<64, 128, 64, 4, false, false, cutlass::bfloat16_t, Flash_kernel_traits<64, 128, 64, 4, cutlass::bfloat16_t>>, Is_dropout=true, Is_causal=false, Is_even_N=true, Is_even_K=false, Return_softmax=false, Params=Flash_fwd_params]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(15): here
            instantiation of "void flash_fwd_kernel<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax>(Flash_fwd_params) [with Kernel_traits=Flash_fwd_kernel_traits<64, 128, 64, 4, false, false, cutlass::bfloat16_t, Flash_kernel_traits<64, 128, 64, 4, cutlass::bfloat16_t>>, Is_dropout=true, Is_causal=false, Is_even_N=true, Is_even_K=false, Return_softmax=false]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(34): here
            instantiation of "void run_flash_fwd<Kernel_traits,Is_dropout,Is_causal>(Flash_fwd_params &, cudaStream_t) [with Kernel_traits=Flash_fwd_kernel_traits<64, 128, 64, 4, false, false, cutlass::bfloat16_t, Flash_kernel_traits<64, 128, 64, 4, cutlass::bfloat16_t>>, Is_dropout=true, Is_causal=false]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(68): here
            instantiation of "void run_mha_fwd_hdim64<T>(Flash_fwd_params &, cudaStream_t) [with T=cutlass::bfloat16_t]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_hdim64_bf16_sm80.cu(18): here

/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_kernel.h(140): warning: variable "MMA_M" was declared but never referenced
          detected during:
            instantiation of "void flash::compute_attn_1rowblock<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params,Is_Q_split_K>(const Params &, int, int, int) [with Kernel_traits=Flash_fwd_kernel_traits<64, 128, 64, 4, false, false, cutlass::bfloat16_t, Flash_kernel_traits<64, 128, 64, 4, cutlass::bfloat16_t>>, Is_dropout=true, Is_causal=false, Is_even_N=false, Is_even_K=true, Return_softmax=true, Params=Flash_fwd_params, Is_Q_split_K=false]" 
(589): here
            instantiation of "void flash::compute_attn<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &) [with Kernel_traits=Flash_fwd_kernel_traits<64, 128, 64, 4, false, false, cutlass::bfloat16_t, Flash_kernel_traits<64, 128, 64, 4, cutlass::bfloat16_t>>, Is_dropout=true, Is_causal=false, Is_even_N=false, Is_even_K=true, Return_softmax=true, Params=Flash_fwd_params]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(15): here
            instantiation of "void flash_fwd_kernel<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax>(Flash_fwd_params) [with Kernel_traits=Flash_fwd_kernel_traits<64, 128, 64, 4, false, false, cutlass::bfloat16_t, Flash_kernel_traits<64, 128, 64, 4, cutlass::bfloat16_t>>, Is_dropout=true, Is_causal=false, Is_even_N=false, Is_even_K=true, Return_softmax=true]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(34): here
            instantiation of "void run_flash_fwd<Kernel_traits,Is_dropout,Is_causal>(Flash_fwd_params &, cudaStream_t) [with Kernel_traits=Flash_fwd_kernel_traits<64, 128, 64, 4, false, false, cutlass::bfloat16_t, Flash_kernel_traits<64, 128, 64, 4, cutlass::bfloat16_t>>, Is_dropout=true, Is_causal=false]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(68): here
            instantiation of "void run_mha_fwd_hdim64<T>(Flash_fwd_params &, cudaStream_t) [with T=cutlass::bfloat16_t]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_hdim64_bf16_sm80.cu(18): here

/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_kernel.h(276): warning: variable "tQrQ" was declared but never referenced
          detected during:
            instantiation of "void flash::compute_attn_1rowblock<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params,Is_Q_split_K>(const Params &, int, int, int) [with Kernel_traits=Flash_fwd_kernel_traits<64, 128, 64, 4, false, false, cutlass::bfloat16_t, Flash_kernel_traits<64, 128, 64, 4, cutlass::bfloat16_t>>, Is_dropout=true, Is_causal=false, Is_even_N=false, Is_even_K=true, Return_softmax=true, Params=Flash_fwd_params, Is_Q_split_K=false]" 
(589): here
            instantiation of "void flash::compute_attn<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &) [with Kernel_traits=Flash_fwd_kernel_traits<64, 128, 64, 4, false, false, cutlass::bfloat16_t, Flash_kernel_traits<64, 128, 64, 4, cutlass::bfloat16_t>>, Is_dropout=true, Is_causal=false, Is_even_N=false, Is_even_K=true, Return_softmax=true, Params=Flash_fwd_params]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(15): here
            instantiation of "void flash_fwd_kernel<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax>(Flash_fwd_params) [with Kernel_traits=Flash_fwd_kernel_traits<64, 128, 64, 4, false, false, cutlass::bfloat16_t, Flash_kernel_traits<64, 128, 64, 4, cutlass::bfloat16_t>>, Is_dropout=true, Is_causal=false, Is_even_N=false, Is_even_K=true, Return_softmax=true]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(34): here
            instantiation of "void run_flash_fwd<Kernel_traits,Is_dropout,Is_causal>(Flash_fwd_params &, cudaStream_t) [with Kernel_traits=Flash_fwd_kernel_traits<64, 128, 64, 4, false, false, cutlass::bfloat16_t, Flash_kernel_traits<64, 128, 64, 4, cutlass::bfloat16_t>>, Is_dropout=true, Is_causal=false]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(68): here
            instantiation of "void run_mha_fwd_hdim64<T>(Flash_fwd_params &, cudaStream_t) [with T=cutlass::bfloat16_t]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_hdim64_bf16_sm80.cu(18): here

/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_kernel.h(140): warning: variable "MMA_M" was declared but never referenced
          detected during:
            instantiation of "void flash::compute_attn_1rowblock<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params,Is_Q_split_K>(const Params &, int, int, int) [with Kernel_traits=Flash_fwd_kernel_traits<64, 128, 64, 4, false, false, cutlass::bfloat16_t, Flash_kernel_traits<64, 128, 64, 4, cutlass::bfloat16_t>>, Is_dropout=true, Is_causal=false, Is_even_N=false, Is_even_K=true, Return_softmax=false, Params=Flash_fwd_params, Is_Q_split_K=false]" 
(589): here
            instantiation of "void flash::compute_attn<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &) [with Kernel_traits=Flash_fwd_kernel_traits<64, 128, 64, 4, false, false, cutlass::bfloat16_t, Flash_kernel_traits<64, 128, 64, 4, cutlass::bfloat16_t>>, Is_dropout=true, Is_causal=false, Is_even_N=false, Is_even_K=true, Return_softmax=false, Params=Flash_fwd_params]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(15): here
            instantiation of "void flash_fwd_kernel<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax>(Flash_fwd_params) [with Kernel_traits=Flash_fwd_kernel_traits<64, 128, 64, 4, false, false, cutlass::bfloat16_t, Flash_kernel_traits<64, 128, 64, 4, cutlass::bfloat16_t>>, Is_dropout=true, Is_causal=false, Is_even_N=false, Is_even_K=true, Return_softmax=false]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(34): here
            instantiation of "void run_flash_fwd<Kernel_traits,Is_dropout,Is_causal>(Flash_fwd_params &, cudaStream_t) [with Kernel_traits=Flash_fwd_kernel_traits<64, 128, 64, 4, false, false, cutlass::bfloat16_t, Flash_kernel_traits<64, 128, 64, 4, cutlass::bfloat16_t>>, Is_dropout=true, Is_causal=false]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(68): here
            instantiation of "void run_mha_fwd_hdim64<T>(Flash_fwd_params &, cudaStream_t) [with T=cutlass::bfloat16_t]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_hdim64_bf16_sm80.cu(18): here

/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_kernel.h(276): warning: variable "tQrQ" was declared but never referenced
          detected during:
            instantiation of "void flash::compute_attn_1rowblock<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params,Is_Q_split_K>(const Params &, int, int, int) [with Kernel_traits=Flash_fwd_kernel_traits<64, 128, 64, 4, false, false, cutlass::bfloat16_t, Flash_kernel_traits<64, 128, 64, 4, cutlass::bfloat16_t>>, Is_dropout=true, Is_causal=false, Is_even_N=false, Is_even_K=true, Return_softmax=false, Params=Flash_fwd_params, Is_Q_split_K=false]" 
(589): here
            instantiation of "void flash::compute_attn<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &) [with Kernel_traits=Flash_fwd_kernel_traits<64, 128, 64, 4, false, false, cutlass::bfloat16_t, Flash_kernel_traits<64, 128, 64, 4, cutlass::bfloat16_t>>, Is_dropout=true, Is_causal=false, Is_even_N=false, Is_even_K=true, Return_softmax=false, Params=Flash_fwd_params]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(15): here
            instantiation of "void flash_fwd_kernel<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax>(Flash_fwd_params) [with Kernel_traits=Flash_fwd_kernel_traits<64, 128, 64, 4, false, false, cutlass::bfloat16_t, Flash_kernel_traits<64, 128, 64, 4, cutlass::bfloat16_t>>, Is_dropout=true, Is_causal=false, Is_even_N=false, Is_even_K=true, Return_softmax=false]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(34): here
            instantiation of "void run_flash_fwd<Kernel_traits,Is_dropout,Is_causal>(Flash_fwd_params &, cudaStream_t) [with Kernel_traits=Flash_fwd_kernel_traits<64, 128, 64, 4, false, false, cutlass::bfloat16_t, Flash_kernel_traits<64, 128, 64, 4, cutlass::bfloat16_t>>, Is_dropout=true, Is_causal=false]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(68): here
            instantiation of "void run_mha_fwd_hdim64<T>(Flash_fwd_params &, cudaStream_t) [with T=cutlass::bfloat16_t]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_hdim64_bf16_sm80.cu(18): here

/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_kernel.h(140): warning: variable "MMA_M" was declared but never referenced
          detected during:
            instantiation of "void flash::compute_attn_1rowblock<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params,Is_Q_split_K>(const Params &, int, int, int) [with Kernel_traits=Flash_fwd_kernel_traits<64, 128, 64, 4, false, false, cutlass::bfloat16_t, Flash_kernel_traits<64, 128, 64, 4, cutlass::bfloat16_t>>, Is_dropout=true, Is_causal=false, Is_even_N=false, Is_even_K=false, Return_softmax=true, Params=Flash_fwd_params, Is_Q_split_K=false]" 
(589): here
            instantiation of "void flash::compute_attn<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &) [with Kernel_traits=Flash_fwd_kernel_traits<64, 128, 64, 4, false, false, cutlass::bfloat16_t, Flash_kernel_traits<64, 128, 64, 4, cutlass::bfloat16_t>>, Is_dropout=true, Is_causal=false, Is_even_N=false, Is_even_K=false, Return_softmax=true, Params=Flash_fwd_params]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(15): here
            instantiation of "void flash_fwd_kernel<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax>(Flash_fwd_params) [with Kernel_traits=Flash_fwd_kernel_traits<64, 128, 64, 4, false, false, cutlass::bfloat16_t, Flash_kernel_traits<64, 128, 64, 4, cutlass::bfloat16_t>>, Is_dropout=true, Is_causal=false, Is_even_N=false, Is_even_K=false, Return_softmax=true]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(34): here
            instantiation of "void run_flash_fwd<Kernel_traits,Is_dropout,Is_causal>(Flash_fwd_params &, cudaStream_t) [with Kernel_traits=Flash_fwd_kernel_traits<64, 128, 64, 4, false, false, cutlass::bfloat16_t, Flash_kernel_traits<64, 128, 64, 4, cutlass::bfloat16_t>>, Is_dropout=true, Is_causal=false]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(68): here
            instantiation of "void run_mha_fwd_hdim64<T>(Flash_fwd_params &, cudaStream_t) [with T=cutlass::bfloat16_t]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_hdim64_bf16_sm80.cu(18): here

/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_kernel.h(276): warning: variable "tQrQ" was declared but never referenced
          detected during:
            instantiation of "void flash::compute_attn_1rowblock<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params,Is_Q_split_K>(const Params &, int, int, int) [with Kernel_traits=Flash_fwd_kernel_traits<64, 128, 64, 4, false, false, cutlass::bfloat16_t, Flash_kernel_traits<64, 128, 64, 4, cutlass::bfloat16_t>>, Is_dropout=true, Is_causal=false, Is_even_N=false, Is_even_K=false, Return_softmax=true, Params=Flash_fwd_params, Is_Q_split_K=false]" 
(589): here
            instantiation of "void flash::compute_attn<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &) [with Kernel_traits=Flash_fwd_kernel_traits<64, 128, 64, 4, false, false, cutlass::bfloat16_t, Flash_kernel_traits<64, 128, 64, 4, cutlass::bfloat16_t>>, Is_dropout=true, Is_causal=false, Is_even_N=false, Is_even_K=false, Return_softmax=true, Params=Flash_fwd_params]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(15): here
            instantiation of "void flash_fwd_kernel<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax>(Flash_fwd_params) [with Kernel_traits=Flash_fwd_kernel_traits<64, 128, 64, 4, false, false, cutlass::bfloat16_t, Flash_kernel_traits<64, 128, 64, 4, cutlass::bfloat16_t>>, Is_dropout=true, Is_causal=false, Is_even_N=false, Is_even_K=false, Return_softmax=true]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(34): here
            instantiation of "void run_flash_fwd<Kernel_traits,Is_dropout,Is_causal>(Flash_fwd_params &, cudaStream_t) [with Kernel_traits=Flash_fwd_kernel_traits<64, 128, 64, 4, false, false, cutlass::bfloat16_t, Flash_kernel_traits<64, 128, 64, 4, cutlass::bfloat16_t>>, Is_dropout=true, Is_causal=false]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(68): here
            instantiation of "void run_mha_fwd_hdim64<T>(Flash_fwd_params &, cudaStream_t) [with T=cutlass::bfloat16_t]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_hdim64_bf16_sm80.cu(18): here

/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_kernel.h(140): warning: variable "MMA_M" was declared but never referenced
          detected during:
            instantiation of "void flash::compute_attn_1rowblock<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params,Is_Q_split_K>(const Params &, int, int, int) [with Kernel_traits=Flash_fwd_kernel_traits<64, 128, 64, 4, false, false, cutlass::bfloat16_t, Flash_kernel_traits<64, 128, 64, 4, cutlass::bfloat16_t>>, Is_dropout=true, Is_causal=false, Is_even_N=false, Is_even_K=false, Return_softmax=false, Params=Flash_fwd_params, Is_Q_split_K=false]" 
(589): here
            instantiation of "void flash::compute_attn<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &) [with Kernel_traits=Flash_fwd_kernel_traits<64, 128, 64, 4, false, false, cutlass::bfloat16_t, Flash_kernel_traits<64, 128, 64, 4, cutlass::bfloat16_t>>, Is_dropout=true, Is_causal=false, Is_even_N=false, Is_even_K=false, Return_softmax=false, Params=Flash_fwd_params]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(15): here
            instantiation of "void flash_fwd_kernel<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax>(Flash_fwd_params) [with Kernel_traits=Flash_fwd_kernel_traits<64, 128, 64, 4, false, false, cutlass::bfloat16_t, Flash_kernel_traits<64, 128, 64, 4, cutlass::bfloat16_t>>, Is_dropout=true, Is_causal=false, Is_even_N=false, Is_even_K=false, Return_softmax=false]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(34): here
            instantiation of "void run_flash_fwd<Kernel_traits,Is_dropout,Is_causal>(Flash_fwd_params &, cudaStream_t) [with Kernel_traits=Flash_fwd_kernel_traits<64, 128, 64, 4, false, false, cutlass::bfloat16_t, Flash_kernel_traits<64, 128, 64, 4, cutlass::bfloat16_t>>, Is_dropout=true, Is_causal=false]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(68): here
            instantiation of "void run_mha_fwd_hdim64<T>(Flash_fwd_params &, cudaStream_t) [with T=cutlass::bfloat16_t]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_hdim64_bf16_sm80.cu(18): here

/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_kernel.h(276): warning: variable "tQrQ" was declared but never referenced
          detected during:
            instantiation of "void flash::compute_attn_1rowblock<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params,Is_Q_split_K>(const Params &, int, int, int) [with Kernel_traits=Flash_fwd_kernel_traits<64, 128, 64, 4, false, false, cutlass::bfloat16_t, Flash_kernel_traits<64, 128, 64, 4, cutlass::bfloat16_t>>, Is_dropout=true, Is_causal=false, Is_even_N=false, Is_even_K=false, Return_softmax=false, Params=Flash_fwd_params, Is_Q_split_K=false]" 
(589): here
            instantiation of "void flash::compute_attn<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &) [with Kernel_traits=Flash_fwd_kernel_traits<64, 128, 64, 4, false, false, cutlass::bfloat16_t, Flash_kernel_traits<64, 128, 64, 4, cutlass::bfloat16_t>>, Is_dropout=true, Is_causal=false, Is_even_N=false, Is_even_K=false, Return_softmax=false, Params=Flash_fwd_params]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(15): here
            instantiation of "void flash_fwd_kernel<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax>(Flash_fwd_params) [with Kernel_traits=Flash_fwd_kernel_traits<64, 128, 64, 4, false, false, cutlass::bfloat16_t, Flash_kernel_traits<64, 128, 64, 4, cutlass::bfloat16_t>>, Is_dropout=true, Is_causal=false, Is_even_N=false, Is_even_K=false, Return_softmax=false]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(34): here
            instantiation of "void run_flash_fwd<Kernel_traits,Is_dropout,Is_causal>(Flash_fwd_params &, cudaStream_t) [with Kernel_traits=Flash_fwd_kernel_traits<64, 128, 64, 4, false, false, cutlass::bfloat16_t, Flash_kernel_traits<64, 128, 64, 4, cutlass::bfloat16_t>>, Is_dropout=true, Is_causal=false]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(68): here
            instantiation of "void run_mha_fwd_hdim64<T>(Flash_fwd_params &, cudaStream_t) [with T=cutlass::bfloat16_t]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_hdim64_bf16_sm80.cu(18): here

/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_kernel.h(140): warning: variable "MMA_M" was declared but never referenced
          detected during:
            instantiation of "void flash::compute_attn_1rowblock<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params,Is_Q_split_K>(const Params &, int, int, int) [with Kernel_traits=Flash_fwd_kernel_traits<64, 128, 128, 4, false, false, cutlass::bfloat16_t, Flash_kernel_traits<64, 128, 128, 4, cutlass::bfloat16_t>>, Is_dropout=false, Is_causal=true, Is_even_N=true, Is_even_K=true, Return_softmax=false, Params=Flash_fwd_params, Is_Q_split_K=false]" 
(589): here
            instantiation of "void flash::compute_attn<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &) [with Kernel_traits=Flash_fwd_kernel_traits<64, 128, 128, 4, false, false, cutlass::bfloat16_t, Flash_kernel_traits<64, 128, 128, 4, cutlass::bfloat16_t>>, Is_dropout=false, Is_causal=true, Is_even_N=true, Is_even_K=true, Return_softmax=false, Params=Flash_fwd_params]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(15): here
            instantiation of "void flash_fwd_kernel<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax>(Flash_fwd_params) [with Kernel_traits=Flash_fwd_kernel_traits<64, 128, 128, 4, false, false, cutlass::bfloat16_t, Flash_kernel_traits<64, 128, 128, 4, cutlass::bfloat16_t>>, Is_dropout=false, Is_causal=true, Is_even_N=true, Is_even_K=true, Return_softmax=false]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(34): here
            instantiation of "void run_flash_fwd<Kernel_traits,Is_dropout,Is_causal>(Flash_fwd_params &, cudaStream_t) [with Kernel_traits=Flash_fwd_kernel_traits<64, 128, 128, 4, false, false, cutlass::bfloat16_t, Flash_kernel_traits<64, 128, 128, 4, cutlass::bfloat16_t>>, Is_dropout=false, Is_causal=true]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(68): here
            instantiation of "void run_mha_fwd_hdim64<T>(Flash_fwd_params &, cudaStream_t) [with T=cutlass::bfloat16_t]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_hdim64_bf16_sm80.cu(18): here

/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_kernel.h(276): warning: variable "tQrQ" was declared but never referenced
          detected during:
            instantiation of "void flash::compute_attn_1rowblock<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params,Is_Q_split_K>(const Params &, int, int, int) [with Kernel_traits=Flash_fwd_kernel_traits<64, 128, 128, 4, false, false, cutlass::bfloat16_t, Flash_kernel_traits<64, 128, 128, 4, cutlass::bfloat16_t>>, Is_dropout=false, Is_causal=true, Is_even_N=true, Is_even_K=true, Return_softmax=false, Params=Flash_fwd_params, Is_Q_split_K=false]" 
(589): here
            instantiation of "void flash::compute_attn<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &) [with Kernel_traits=Flash_fwd_kernel_traits<64, 128, 128, 4, false, false, cutlass::bfloat16_t, Flash_kernel_traits<64, 128, 128, 4, cutlass::bfloat16_t>>, Is_dropout=false, Is_causal=true, Is_even_N=true, Is_even_K=true, Return_softmax=false, Params=Flash_fwd_params]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(15): here
            instantiation of "void flash_fwd_kernel<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax>(Flash_fwd_params) [with Kernel_traits=Flash_fwd_kernel_traits<64, 128, 128, 4, false, false, cutlass::bfloat16_t, Flash_kernel_traits<64, 128, 128, 4, cutlass::bfloat16_t>>, Is_dropout=false, Is_causal=true, Is_even_N=true, Is_even_K=true, Return_softmax=false]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(34): here
            instantiation of "void run_flash_fwd<Kernel_traits,Is_dropout,Is_causal>(Flash_fwd_params &, cudaStream_t) [with Kernel_traits=Flash_fwd_kernel_traits<64, 128, 128, 4, false, false, cutlass::bfloat16_t, Flash_kernel_traits<64, 128, 128, 4, cutlass::bfloat16_t>>, Is_dropout=false, Is_causal=true]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(68): here
            instantiation of "void run_mha_fwd_hdim64<T>(Flash_fwd_params &, cudaStream_t) [with T=cutlass::bfloat16_t]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_hdim64_bf16_sm80.cu(18): here

/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_kernel.h(140): warning: variable "MMA_M" was declared but never referenced
          detected during:
            instantiation of "void flash::compute_attn_1rowblock<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params,Is_Q_split_K>(const Params &, int, int, int) [with Kernel_traits=Flash_fwd_kernel_traits<64, 128, 128, 4, false, false, cutlass::bfloat16_t, Flash_kernel_traits<64, 128, 128, 4, cutlass::bfloat16_t>>, Is_dropout=false, Is_causal=true, Is_even_N=true, Is_even_K=false, Return_softmax=false, Params=Flash_fwd_params, Is_Q_split_K=false]" 
(589): here
            instantiation of "void flash::compute_attn<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &) [with Kernel_traits=Flash_fwd_kernel_traits<64, 128, 128, 4, false, false, cutlass::bfloat16_t, Flash_kernel_traits<64, 128, 128, 4, cutlass::bfloat16_t>>, Is_dropout=false, Is_causal=true, Is_even_N=true, Is_even_K=false, Return_softmax=false, Params=Flash_fwd_params]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(15): here
            instantiation of "void flash_fwd_kernel<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax>(Flash_fwd_params) [with Kernel_traits=Flash_fwd_kernel_traits<64, 128, 128, 4, false, false, cutlass::bfloat16_t, Flash_kernel_traits<64, 128, 128, 4, cutlass::bfloat16_t>>, Is_dropout=false, Is_causal=true, Is_even_N=true, Is_even_K=false, Return_softmax=false]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(34): here
            instantiation of "void run_flash_fwd<Kernel_traits,Is_dropout,Is_causal>(Flash_fwd_params &, cudaStream_t) [with Kernel_traits=Flash_fwd_kernel_traits<64, 128, 128, 4, false, false, cutlass::bfloat16_t, Flash_kernel_traits<64, 128, 128, 4, cutlass::bfloat16_t>>, Is_dropout=false, Is_causal=true]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(68): here
            instantiation of "void run_mha_fwd_hdim64<T>(Flash_fwd_params &, cudaStream_t) [with T=cutlass::bfloat16_t]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_hdim64_bf16_sm80.cu(18): here

/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_kernel.h(276): warning: variable "tQrQ" was declared but never referenced
          detected during:
            instantiation of "void flash::compute_attn_1rowblock<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params,Is_Q_split_K>(const Params &, int, int, int) [with Kernel_traits=Flash_fwd_kernel_traits<64, 128, 128, 4, false, false, cutlass::bfloat16_t, Flash_kernel_traits<64, 128, 128, 4, cutlass::bfloat16_t>>, Is_dropout=false, Is_causal=true, Is_even_N=true, Is_even_K=false, Return_softmax=false, Params=Flash_fwd_params, Is_Q_split_K=false]" 
(589): here
            instantiation of "void flash::compute_attn<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &) [with Kernel_traits=Flash_fwd_kernel_traits<64, 128, 128, 4, false, false, cutlass::bfloat16_t, Flash_kernel_traits<64, 128, 128, 4, cutlass::bfloat16_t>>, Is_dropout=false, Is_causal=true, Is_even_N=true, Is_even_K=false, Return_softmax=false, Params=Flash_fwd_params]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(15): here
            instantiation of "void flash_fwd_kernel<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax>(Flash_fwd_params) [with Kernel_traits=Flash_fwd_kernel_traits<64, 128, 128, 4, false, false, cutlass::bfloat16_t, Flash_kernel_traits<64, 128, 128, 4, cutlass::bfloat16_t>>, Is_dropout=false, Is_causal=true, Is_even_N=true, Is_even_K=false, Return_softmax=false]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(34): here
            instantiation of "void run_flash_fwd<Kernel_traits,Is_dropout,Is_causal>(Flash_fwd_params &, cudaStream_t) [with Kernel_traits=Flash_fwd_kernel_traits<64, 128, 128, 4, false, false, cutlass::bfloat16_t, Flash_kernel_traits<64, 128, 128, 4, cutlass::bfloat16_t>>, Is_dropout=false, Is_causal=true]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(68): here
            instantiation of "void run_mha_fwd_hdim64<T>(Flash_fwd_params &, cudaStream_t) [with T=cutlass::bfloat16_t]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_hdim64_bf16_sm80.cu(18): here

/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_kernel.h(140): warning: variable "MMA_M" was declared but never referenced
          detected during:
            instantiation of "void flash::compute_attn_1rowblock<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params,Is_Q_split_K>(const Params &, int, int, int) [with Kernel_traits=Flash_fwd_kernel_traits<64, 128, 128, 4, false, false, cutlass::bfloat16_t, Flash_kernel_traits<64, 128, 128, 4, cutlass::bfloat16_t>>, Is_dropout=false, Is_causal=true, Is_even_N=false, Is_even_K=true, Return_softmax=false, Params=Flash_fwd_params, Is_Q_split_K=false]" 
(589): here
            instantiation of "void flash::compute_attn<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &) [with Kernel_traits=Flash_fwd_kernel_traits<64, 128, 128, 4, false, false, cutlass::bfloat16_t, Flash_kernel_traits<64, 128, 128, 4, cutlass::bfloat16_t>>, Is_dropout=false, Is_causal=true, Is_even_N=false, Is_even_K=true, Return_softmax=false, Params=Flash_fwd_params]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(15): here
            instantiation of "void flash_fwd_kernel<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax>(Flash_fwd_params) [with Kernel_traits=Flash_fwd_kernel_traits<64, 128, 128, 4, false, false, cutlass::bfloat16_t, Flash_kernel_traits<64, 128, 128, 4, cutlass::bfloat16_t>>, Is_dropout=false, Is_causal=true, Is_even_N=false, Is_even_K=true, Return_softmax=false]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(34): here
            instantiation of "void run_flash_fwd<Kernel_traits,Is_dropout,Is_causal>(Flash_fwd_params &, cudaStream_t) [with Kernel_traits=Flash_fwd_kernel_traits<64, 128, 128, 4, false, false, cutlass::bfloat16_t, Flash_kernel_traits<64, 128, 128, 4, cutlass::bfloat16_t>>, Is_dropout=false, Is_causal=true]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(68): here
            instantiation of "void run_mha_fwd_hdim64<T>(Flash_fwd_params &, cudaStream_t) [with T=cutlass::bfloat16_t]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_hdim64_bf16_sm80.cu(18): here

/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_kernel.h(276): warning: variable "tQrQ" was declared but never referenced
          detected during:
            instantiation of "void flash::compute_attn_1rowblock<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params,Is_Q_split_K>(const Params &, int, int, int) [with Kernel_traits=Flash_fwd_kernel_traits<64, 128, 128, 4, false, false, cutlass::bfloat16_t, Flash_kernel_traits<64, 128, 128, 4, cutlass::bfloat16_t>>, Is_dropout=false, Is_causal=true, Is_even_N=false, Is_even_K=true, Return_softmax=false, Params=Flash_fwd_params, Is_Q_split_K=false]" 
(589): here
            instantiation of "void flash::compute_attn<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &) [with Kernel_traits=Flash_fwd_kernel_traits<64, 128, 128, 4, false, false, cutlass::bfloat16_t, Flash_kernel_traits<64, 128, 128, 4, cutlass::bfloat16_t>>, Is_dropout=false, Is_causal=true, Is_even_N=false, Is_even_K=true, Return_softmax=false, Params=Flash_fwd_params]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(15): here
            instantiation of "void flash_fwd_kernel<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax>(Flash_fwd_params) [with Kernel_traits=Flash_fwd_kernel_traits<64, 128, 128, 4, false, false, cutlass::bfloat16_t, Flash_kernel_traits<64, 128, 128, 4, cutlass::bfloat16_t>>, Is_dropout=false, Is_causal=true, Is_even_N=false, Is_even_K=true, Return_softmax=false]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(34): here
            instantiation of "void run_flash_fwd<Kernel_traits,Is_dropout,Is_causal>(Flash_fwd_params &, cudaStream_t) [with Kernel_traits=Flash_fwd_kernel_traits<64, 128, 128, 4, false, false, cutlass::bfloat16_t, Flash_kernel_traits<64, 128, 128, 4, cutlass::bfloat16_t>>, Is_dropout=false, Is_causal=true]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(68): here
            instantiation of "void run_mha_fwd_hdim64<T>(Flash_fwd_params &, cudaStream_t) [with T=cutlass::bfloat16_t]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_hdim64_bf16_sm80.cu(18): here

/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_kernel.h(140): warning: variable "MMA_M" was declared but never referenced
          detected during:
            instantiation of "void flash::compute_attn_1rowblock<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params,Is_Q_split_K>(const Params &, int, int, int) [with Kernel_traits=Flash_fwd_kernel_traits<64, 128, 128, 4, false, false, cutlass::bfloat16_t, Flash_kernel_traits<64, 128, 128, 4, cutlass::bfloat16_t>>, Is_dropout=false, Is_causal=true, Is_even_N=false, Is_even_K=false, Return_softmax=false, Params=Flash_fwd_params, Is_Q_split_K=false]" 
(589): here
            instantiation of "void flash::compute_attn<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &) [with Kernel_traits=Flash_fwd_kernel_traits<64, 128, 128, 4, false, false, cutlass::bfloat16_t, Flash_kernel_traits<64, 128, 128, 4, cutlass::bfloat16_t>>, Is_dropout=false, Is_causal=true, Is_even_N=false, Is_even_K=false, Return_softmax=false, Params=Flash_fwd_params]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(15): here
            instantiation of "void flash_fwd_kernel<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax>(Flash_fwd_params) [with Kernel_traits=Flash_fwd_kernel_traits<64, 128, 128, 4, false, false, cutlass::bfloat16_t, Flash_kernel_traits<64, 128, 128, 4, cutlass::bfloat16_t>>, Is_dropout=false, Is_causal=true, Is_even_N=false, Is_even_K=false, Return_softmax=false]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(34): here
            instantiation of "void run_flash_fwd<Kernel_traits,Is_dropout,Is_causal>(Flash_fwd_params &, cudaStream_t) [with Kernel_traits=Flash_fwd_kernel_traits<64, 128, 128, 4, false, false, cutlass::bfloat16_t, Flash_kernel_traits<64, 128, 128, 4, cutlass::bfloat16_t>>, Is_dropout=false, Is_causal=true]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(68): here
            instantiation of "void run_mha_fwd_hdim64<T>(Flash_fwd_params &, cudaStream_t) [with T=cutlass::bfloat16_t]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_hdim64_bf16_sm80.cu(18): here

/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_kernel.h(276): warning: variable "tQrQ" was declared but never referenced
          detected during:
            instantiation of "void flash::compute_attn_1rowblock<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params,Is_Q_split_K>(const Params &, int, int, int) [with Kernel_traits=Flash_fwd_kernel_traits<64, 128, 128, 4, false, false, cutlass::bfloat16_t, Flash_kernel_traits<64, 128, 128, 4, cutlass::bfloat16_t>>, Is_dropout=false, Is_causal=true, Is_even_N=false, Is_even_K=false, Return_softmax=false, Params=Flash_fwd_params, Is_Q_split_K=false]" 
(589): here
            instantiation of "void flash::compute_attn<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &) [with Kernel_traits=Flash_fwd_kernel_traits<64, 128, 128, 4, false, false, cutlass::bfloat16_t, Flash_kernel_traits<64, 128, 128, 4, cutlass::bfloat16_t>>, Is_dropout=false, Is_causal=true, Is_even_N=false, Is_even_K=false, Return_softmax=false, Params=Flash_fwd_params]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(15): here
            instantiation of "void flash_fwd_kernel<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax>(Flash_fwd_params) [with Kernel_traits=Flash_fwd_kernel_traits<64, 128, 128, 4, false, false, cutlass::bfloat16_t, Flash_kernel_traits<64, 128, 128, 4, cutlass::bfloat16_t>>, Is_dropout=false, Is_causal=true, Is_even_N=false, Is_even_K=false, Return_softmax=false]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(34): here
            instantiation of "void run_flash_fwd<Kernel_traits,Is_dropout,Is_causal>(Flash_fwd_params &, cudaStream_t) [with Kernel_traits=Flash_fwd_kernel_traits<64, 128, 128, 4, false, false, cutlass::bfloat16_t, Flash_kernel_traits<64, 128, 128, 4, cutlass::bfloat16_t>>, Is_dropout=false, Is_causal=true]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(68): here
            instantiation of "void run_mha_fwd_hdim64<T>(Flash_fwd_params &, cudaStream_t) [with T=cutlass::bfloat16_t]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_hdim64_bf16_sm80.cu(18): here

/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_kernel.h(140): warning: variable "MMA_M" was declared but never referenced
          detected during:
            instantiation of "void flash::compute_attn_1rowblock<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params,Is_Q_split_K>(const Params &, int, int, int) [with Kernel_traits=Flash_fwd_kernel_traits<64, 128, 128, 4, false, false, cutlass::bfloat16_t, Flash_kernel_traits<64, 128, 128, 4, cutlass::bfloat16_t>>, Is_dropout=false, Is_causal=false, Is_even_N=true, Is_even_K=true, Return_softmax=false, Params=Flash_fwd_params, Is_Q_split_K=false]" 
(589): here
            instantiation of "void flash::compute_attn<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &) [with Kernel_traits=Flash_fwd_kernel_traits<64, 128, 128, 4, false, false, cutlass::bfloat16_t, Flash_kernel_traits<64, 128, 128, 4, cutlass::bfloat16_t>>, Is_dropout=false, Is_causal=false, Is_even_N=true, Is_even_K=true, Return_softmax=false, Params=Flash_fwd_params]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(15): here
            instantiation of "void flash_fwd_kernel<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax>(Flash_fwd_params) [with Kernel_traits=Flash_fwd_kernel_traits<64, 128, 128, 4, false, false, cutlass::bfloat16_t, Flash_kernel_traits<64, 128, 128, 4, cutlass::bfloat16_t>>, Is_dropout=false, Is_causal=false, Is_even_N=true, Is_even_K=true, Return_softmax=false]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(34): here
            instantiation of "void run_flash_fwd<Kernel_traits,Is_dropout,Is_causal>(Flash_fwd_params &, cudaStream_t) [with Kernel_traits=Flash_fwd_kernel_traits<64, 128, 128, 4, false, false, cutlass::bfloat16_t, Flash_kernel_traits<64, 128, 128, 4, cutlass::bfloat16_t>>, Is_dropout=false, Is_causal=false]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(68): here
            instantiation of "void run_mha_fwd_hdim64<T>(Flash_fwd_params &, cudaStream_t) [with T=cutlass::bfloat16_t]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_hdim64_bf16_sm80.cu(18): here

/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_kernel.h(276): warning: variable "tQrQ" was declared but never referenced
          detected during:
            instantiation of "void flash::compute_attn_1rowblock<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params,Is_Q_split_K>(const Params &, int, int, int) [with Kernel_traits=Flash_fwd_kernel_traits<64, 128, 128, 4, false, false, cutlass::bfloat16_t, Flash_kernel_traits<64, 128, 128, 4, cutlass::bfloat16_t>>, Is_dropout=false, Is_causal=false, Is_even_N=true, Is_even_K=true, Return_softmax=false, Params=Flash_fwd_params, Is_Q_split_K=false]" 
(589): here
            instantiation of "void flash::compute_attn<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &) [with Kernel_traits=Flash_fwd_kernel_traits<64, 128, 128, 4, false, false, cutlass::bfloat16_t, Flash_kernel_traits<64, 128, 128, 4, cutlass::bfloat16_t>>, Is_dropout=false, Is_causal=false, Is_even_N=true, Is_even_K=true, Return_softmax=false, Params=Flash_fwd_params]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(15): here
            instantiation of "void flash_fwd_kernel<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax>(Flash_fwd_params) [with Kernel_traits=Flash_fwd_kernel_traits<64, 128, 128, 4, false, false, cutlass::bfloat16_t, Flash_kernel_traits<64, 128, 128, 4, cutlass::bfloat16_t>>, Is_dropout=false, Is_causal=false, Is_even_N=true, Is_even_K=true, Return_softmax=false]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(34): here
            instantiation of "void run_flash_fwd<Kernel_traits,Is_dropout,Is_causal>(Flash_fwd_params &, cudaStream_t) [with Kernel_traits=Flash_fwd_kernel_traits<64, 128, 128, 4, false, false, cutlass::bfloat16_t, Flash_kernel_traits<64, 128, 128, 4, cutlass::bfloat16_t>>, Is_dropout=false, Is_causal=false]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(68): here
            instantiation of "void run_mha_fwd_hdim64<T>(Flash_fwd_params &, cudaStream_t) [with T=cutlass::bfloat16_t]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_hdim64_bf16_sm80.cu(18): here

/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_kernel.h(140): warning: variable "MMA_M" was declared but never referenced
          detected during:
            instantiation of "void flash::compute_attn_1rowblock<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params,Is_Q_split_K>(const Params &, int, int, int) [with Kernel_traits=Flash_fwd_kernel_traits<64, 128, 128, 4, false, false, cutlass::bfloat16_t, Flash_kernel_traits<64, 128, 128, 4, cutlass::bfloat16_t>>, Is_dropout=false, Is_causal=false, Is_even_N=true, Is_even_K=false, Return_softmax=false, Params=Flash_fwd_params, Is_Q_split_K=false]" 
(589): here
            instantiation of "void flash::compute_attn<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &) [with Kernel_traits=Flash_fwd_kernel_traits<64, 128, 128, 4, false, false, cutlass::bfloat16_t, Flash_kernel_traits<64, 128, 128, 4, cutlass::bfloat16_t>>, Is_dropout=false, Is_causal=false, Is_even_N=true, Is_even_K=false, Return_softmax=false, Params=Flash_fwd_params]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(15): here
            instantiation of "void flash_fwd_kernel<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax>(Flash_fwd_params) [with Kernel_traits=Flash_fwd_kernel_traits<64, 128, 128, 4, false, false, cutlass::bfloat16_t, Flash_kernel_traits<64, 128, 128, 4, cutlass::bfloat16_t>>, Is_dropout=false, Is_causal=false, Is_even_N=true, Is_even_K=false, Return_softmax=false]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(34): here
            instantiation of "void run_flash_fwd<Kernel_traits,Is_dropout,Is_causal>(Flash_fwd_params &, cudaStream_t) [with Kernel_traits=Flash_fwd_kernel_traits<64, 128, 128, 4, false, false, cutlass::bfloat16_t, Flash_kernel_traits<64, 128, 128, 4, cutlass::bfloat16_t>>, Is_dropout=false, Is_causal=false]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(68): here
            instantiation of "void run_mha_fwd_hdim64<T>(Flash_fwd_params &, cudaStream_t) [with T=cutlass::bfloat16_t]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_hdim64_bf16_sm80.cu(18): here

/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_kernel.h(276): warning: variable "tQrQ" was declared but never referenced
          detected during:
            instantiation of "void flash::compute_attn_1rowblock<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params,Is_Q_split_K>(const Params &, int, int, int) [with Kernel_traits=Flash_fwd_kernel_traits<64, 128, 128, 4, false, false, cutlass::bfloat16_t, Flash_kernel_traits<64, 128, 128, 4, cutlass::bfloat16_t>>, Is_dropout=false, Is_causal=false, Is_even_N=true, Is_even_K=false, Return_softmax=false, Params=Flash_fwd_params, Is_Q_split_K=false]" 
(589): here
            instantiation of "void flash::compute_attn<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &) [with Kernel_traits=Flash_fwd_kernel_traits<64, 128, 128, 4, false, false, cutlass::bfloat16_t, Flash_kernel_traits<64, 128, 128, 4, cutlass::bfloat16_t>>, Is_dropout=false, Is_causal=false, Is_even_N=true, Is_even_K=false, Return_softmax=false, Params=Flash_fwd_params]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(15): here
            instantiation of "void flash_fwd_kernel<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax>(Flash_fwd_params) [with Kernel_traits=Flash_fwd_kernel_traits<64, 128, 128, 4, false, false, cutlass::bfloat16_t, Flash_kernel_traits<64, 128, 128, 4, cutlass::bfloat16_t>>, Is_dropout=false, Is_causal=false, Is_even_N=true, Is_even_K=false, Return_softmax=false]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(34): here
            instantiation of "void run_flash_fwd<Kernel_traits,Is_dropout,Is_causal>(Flash_fwd_params &, cudaStream_t) [with Kernel_traits=Flash_fwd_kernel_traits<64, 128, 128, 4, false, false, cutlass::bfloat16_t, Flash_kernel_traits<64, 128, 128, 4, cutlass::bfloat16_t>>, Is_dropout=false, Is_causal=false]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(68): here
            instantiation of "void run_mha_fwd_hdim64<T>(Flash_fwd_params &, cudaStream_t) [with T=cutlass::bfloat16_t]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_hdim64_bf16_sm80.cu(18): here

/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_kernel.h(140): warning: variable "MMA_M" was declared but never referenced
          detected during:
            instantiation of "void flash::compute_attn_1rowblock<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params,Is_Q_split_K>(const Params &, int, int, int) [with Kernel_traits=Flash_fwd_kernel_traits<64, 128, 128, 4, false, false, cutlass::bfloat16_t, Flash_kernel_traits<64, 128, 128, 4, cutlass::bfloat16_t>>, Is_dropout=false, Is_causal=false, Is_even_N=false, Is_even_K=true, Return_softmax=false, Params=Flash_fwd_params, Is_Q_split_K=false]" 
(589): here
            instantiation of "void flash::compute_attn<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &) [with Kernel_traits=Flash_fwd_kernel_traits<64, 128, 128, 4, false, false, cutlass::bfloat16_t, Flash_kernel_traits<64, 128, 128, 4, cutlass::bfloat16_t>>, Is_dropout=false, Is_causal=false, Is_even_N=false, Is_even_K=true, Return_softmax=false, Params=Flash_fwd_params]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(15): here
            instantiation of "void flash_fwd_kernel<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax>(Flash_fwd_params) [with Kernel_traits=Flash_fwd_kernel_traits<64, 128, 128, 4, false, false, cutlass::bfloat16_t, Flash_kernel_traits<64, 128, 128, 4, cutlass::bfloat16_t>>, Is_dropout=false, Is_causal=false, Is_even_N=false, Is_even_K=true, Return_softmax=false]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(34): here
            instantiation of "void run_flash_fwd<Kernel_traits,Is_dropout,Is_causal>(Flash_fwd_params &, cudaStream_t) [with Kernel_traits=Flash_fwd_kernel_traits<64, 128, 128, 4, false, false, cutlass::bfloat16_t, Flash_kernel_traits<64, 128, 128, 4, cutlass::bfloat16_t>>, Is_dropout=false, Is_causal=false]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(68): here
            instantiation of "void run_mha_fwd_hdim64<T>(Flash_fwd_params &, cudaStream_t) [with T=cutlass::bfloat16_t]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_hdim64_bf16_sm80.cu(18): here

/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_kernel.h(276): warning: variable "tQrQ" was declared but never referenced
          detected during:
            instantiation of "void flash::compute_attn_1rowblock<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params,Is_Q_split_K>(const Params &, int, int, int) [with Kernel_traits=Flash_fwd_kernel_traits<64, 128, 128, 4, false, false, cutlass::bfloat16_t, Flash_kernel_traits<64, 128, 128, 4, cutlass::bfloat16_t>>, Is_dropout=false, Is_causal=false, Is_even_N=false, Is_even_K=true, Return_softmax=false, Params=Flash_fwd_params, Is_Q_split_K=false]" 
(589): here
            instantiation of "void flash::compute_attn<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &) [with Kernel_traits=Flash_fwd_kernel_traits<64, 128, 128, 4, false, false, cutlass::bfloat16_t, Flash_kernel_traits<64, 128, 128, 4, cutlass::bfloat16_t>>, Is_dropout=false, Is_causal=false, Is_even_N=false, Is_even_K=true, Return_softmax=false, Params=Flash_fwd_params]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(15): here
            instantiation of "void flash_fwd_kernel<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax>(Flash_fwd_params) [with Kernel_traits=Flash_fwd_kernel_traits<64, 128, 128, 4, false, false, cutlass::bfloat16_t, Flash_kernel_traits<64, 128, 128, 4, cutlass::bfloat16_t>>, Is_dropout=false, Is_causal=false, Is_even_N=false, Is_even_K=true, Return_softmax=false]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(34): here
            instantiation of "void run_flash_fwd<Kernel_traits,Is_dropout,Is_causal>(Flash_fwd_params &, cudaStream_t) [with Kernel_traits=Flash_fwd_kernel_traits<64, 128, 128, 4, false, false, cutlass::bfloat16_t, Flash_kernel_traits<64, 128, 128, 4, cutlass::bfloat16_t>>, Is_dropout=false, Is_causal=false]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(68): here
            instantiation of "void run_mha_fwd_hdim64<T>(Flash_fwd_params &, cudaStream_t) [with T=cutlass::bfloat16_t]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_hdim64_bf16_sm80.cu(18): here

/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_kernel.h(140): warning: variable "MMA_M" was declared but never referenced
          detected during:
            instantiation of "void flash::compute_attn_1rowblock<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params,Is_Q_split_K>(const Params &, int, int, int) [with Kernel_traits=Flash_fwd_kernel_traits<64, 128, 128, 4, false, false, cutlass::bfloat16_t, Flash_kernel_traits<64, 128, 128, 4, cutlass::bfloat16_t>>, Is_dropout=false, Is_causal=false, Is_even_N=false, Is_even_K=false, Return_softmax=false, Params=Flash_fwd_params, Is_Q_split_K=false]" 
(589): here
            instantiation of "void flash::compute_attn<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &) [with Kernel_traits=Flash_fwd_kernel_traits<64, 128, 128, 4, false, false, cutlass::bfloat16_t, Flash_kernel_traits<64, 128, 128, 4, cutlass::bfloat16_t>>, Is_dropout=false, Is_causal=false, Is_even_N=false, Is_even_K=false, Return_softmax=false, Params=Flash_fwd_params]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(15): here
            instantiation of "void flash_fwd_kernel<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax>(Flash_fwd_params) [with Kernel_traits=Flash_fwd_kernel_traits<64, 128, 128, 4, false, false, cutlass::bfloat16_t, Flash_kernel_traits<64, 128, 128, 4, cutlass::bfloat16_t>>, Is_dropout=false, Is_causal=false, Is_even_N=false, Is_even_K=false, Return_softmax=false]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(34): here
            instantiation of "void run_flash_fwd<Kernel_traits,Is_dropout,Is_causal>(Flash_fwd_params &, cudaStream_t) [with Kernel_traits=Flash_fwd_kernel_traits<64, 128, 128, 4, false, false, cutlass::bfloat16_t, Flash_kernel_traits<64, 128, 128, 4, cutlass::bfloat16_t>>, Is_dropout=false, Is_causal=false]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(68): here
            instantiation of "void run_mha_fwd_hdim64<T>(Flash_fwd_params &, cudaStream_t) [with T=cutlass::bfloat16_t]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_hdim64_bf16_sm80.cu(18): here

/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_kernel.h(276): warning: variable "tQrQ" was declared but never referenced
          detected during:
            instantiation of "void flash::compute_attn_1rowblock<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params,Is_Q_split_K>(const Params &, int, int, int) [with Kernel_traits=Flash_fwd_kernel_traits<64, 128, 128, 4, false, false, cutlass::bfloat16_t, Flash_kernel_traits<64, 128, 128, 4, cutlass::bfloat16_t>>, Is_dropout=false, Is_causal=false, Is_even_N=false, Is_even_K=false, Return_softmax=false, Params=Flash_fwd_params, Is_Q_split_K=false]" 
(589): here
            instantiation of "void flash::compute_attn<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &) [with Kernel_traits=Flash_fwd_kernel_traits<64, 128, 128, 4, false, false, cutlass::bfloat16_t, Flash_kernel_traits<64, 128, 128, 4, cutlass::bfloat16_t>>, Is_dropout=false, Is_causal=false, Is_even_N=false, Is_even_K=false, Return_softmax=false, Params=Flash_fwd_params]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(15): here
            instantiation of "void flash_fwd_kernel<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax>(Flash_fwd_params) [with Kernel_traits=Flash_fwd_kernel_traits<64, 128, 128, 4, false, false, cutlass::bfloat16_t, Flash_kernel_traits<64, 128, 128, 4, cutlass::bfloat16_t>>, Is_dropout=false, Is_causal=false, Is_even_N=false, Is_even_K=false, Return_softmax=false]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(34): here
            instantiation of "void run_flash_fwd<Kernel_traits,Is_dropout,Is_causal>(Flash_fwd_params &, cudaStream_t) [with Kernel_traits=Flash_fwd_kernel_traits<64, 128, 128, 4, false, false, cutlass::bfloat16_t, Flash_kernel_traits<64, 128, 128, 4, cutlass::bfloat16_t>>, Is_dropout=false, Is_causal=false]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(68): here
            instantiation of "void run_mha_fwd_hdim64<T>(Flash_fwd_params &, cudaStream_t) [with T=cutlass::bfloat16_t]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_hdim64_bf16_sm80.cu(18): here

ptxas info    : 2 bytes gmem
ptxas info    : Compiling entry function '_Z16flash_fwd_kernelI23Flash_fwd_kernel_traitsILi64ELi128ELi128ELi4ELb0ELb0EN7cutlass10bfloat16_tE19Flash_kernel_traitsILi64ELi128ELi128ELi4ES2_EELb0ELb0ELb0ELb0ELb0EEv16Flash_fwd_params' for 'sm_80'
ptxas info    : Function properties for _Z16flash_fwd_kernelI23Flash_fwd_kernel_traitsILi64ELi128ELi128ELi4ELb0ELb0EN7cutlass10bfloat16_tE19Flash_kernel_traitsILi64ELi128ELi128ELi4ES2_EELb0ELb0ELb0ELb0ELb0EEv16Flash_fwd_params
    40 bytes stack frame, 56 bytes spill stores, 56 bytes spill loads
ptxas info    : Used 255 registers, 584 bytes cmem[0]
ptxas info    : Compiling entry function '_Z16flash_fwd_kernelI23Flash_fwd_kernel_traitsILi64ELi128ELi128ELi4ELb0ELb0EN7cutlass10bfloat16_tE19Flash_kernel_traitsILi64ELi128ELi128ELi4ES2_EELb0ELb0ELb0ELb1ELb0EEv16Flash_fwd_params' for 'sm_80'
ptxas info    : Function properties for _Z16flash_fwd_kernelI23Flash_fwd_kernel_traitsILi64ELi128ELi128ELi4ELb0ELb0EN7cutlass10bfloat16_tE19Flash_kernel_traitsILi64ELi128ELi128ELi4ES2_EELb0ELb0ELb0ELb1ELb0EEv16Flash_fwd_params
    48 bytes stack frame, 76 bytes spill stores, 60 bytes spill loads
ptxas info    : Used 255 registers, 584 bytes cmem[0]
ptxas info    : Compiling entry function '_Z16flash_fwd_kernelI23Flash_fwd_kernel_traitsILi64ELi128ELi128ELi4ELb0ELb0EN7cutlass10bfloat16_tE19Flash_kernel_traitsILi64ELi128ELi128ELi4ES2_EELb0ELb0ELb1ELb0ELb0EEv16Flash_fwd_params' for 'sm_80'
ptxas info    : Function properties for _Z16flash_fwd_kernelI23Flash_fwd_kernel_traitsILi64ELi128ELi128ELi4ELb0ELb0EN7cutlass10bfloat16_tE19Flash_kernel_traitsILi64ELi128ELi128ELi4ES2_EELb0ELb0ELb1ELb0ELb0EEv16Flash_fwd_params
    40 bytes stack frame, 48 bytes spill stores, 48 bytes spill loads
ptxas info    : Used 255 registers, 584 bytes cmem[0]
ptxas info    : Compiling entry function '_Z16flash_fwd_kernelI23Flash_fwd_kernel_traitsILi64ELi128ELi128ELi4ELb0ELb0EN7cutlass10bfloat16_tE19Flash_kernel_traitsILi64ELi128ELi128ELi4ES2_EELb0ELb0ELb1ELb1ELb0EEv16Flash_fwd_params' for 'sm_80'
ptxas info    : Function properties for _Z16flash_fwd_kernelI23Flash_fwd_kernel_traitsILi64ELi128ELi128ELi4ELb0ELb0EN7cutlass10bfloat16_tE19Flash_kernel_traitsILi64ELi128ELi128ELi4ES2_EELb0ELb0ELb1ELb1ELb0EEv16Flash_fwd_params
    40 bytes stack frame, 52 bytes spill stores, 52 bytes spill loads
ptxas info    : Used 255 registers, 584 bytes cmem[0]
ptxas info    : Compiling entry function '_Z16flash_fwd_kernelI23Flash_fwd_kernel_traitsILi64ELi128ELi128ELi4ELb0ELb0EN7cutlass10bfloat16_tE19Flash_kernel_traitsILi64ELi128ELi128ELi4ES2_EELb0ELb1ELb0ELb0ELb0EEv16Flash_fwd_params' for 'sm_80'
ptxas info    : Function properties for _Z16flash_fwd_kernelI23Flash_fwd_kernel_traitsILi64ELi128ELi128ELi4ELb0ELb0EN7cutlass10bfloat16_tE19Flash_kernel_traitsILi64ELi128ELi128ELi4ES2_EELb0ELb1ELb0ELb0ELb0EEv16Flash_fwd_params
    40 bytes stack frame, 56 bytes spill stores, 56 bytes spill loads
ptxas info    : Used 255 registers, 584 bytes cmem[0]
ptxas info    : Compiling entry function '_Z16flash_fwd_kernelI23Flash_fwd_kernel_traitsILi64ELi128ELi128ELi4ELb0ELb0EN7cutlass10bfloat16_tE19Flash_kernel_traitsILi64ELi128ELi128ELi4ES2_EELb0ELb1ELb0ELb1ELb0EEv16Flash_fwd_params' for 'sm_80'
ptxas info    : Function properties for _Z16flash_fwd_kernelI23Flash_fwd_kernel_traitsILi64ELi128ELi128ELi4ELb0ELb0EN7cutlass10bfloat16_tE19Flash_kernel_traitsILi64ELi128ELi128ELi4ES2_EELb0ELb1ELb0ELb1ELb0EEv16Flash_fwd_params
    48 bytes stack frame, 80 bytes spill stores, 64 bytes spill loads
ptxas info    : Used 255 registers, 584 bytes cmem[0]
ptxas info    : Compiling entry function '_Z16flash_fwd_kernelI23Flash_fwd_kernel_traitsILi64ELi128ELi128ELi4ELb0ELb0EN7cutlass10bfloat16_tE19Flash_kernel_traitsILi64ELi128ELi128ELi4ES2_EELb0ELb1ELb1ELb0ELb0EEv16Flash_fwd_params' for 'sm_80'
ptxas info    : Function properties for _Z16flash_fwd_kernelI23Flash_fwd_kernel_traitsILi64ELi128ELi128ELi4ELb0ELb0EN7cutlass10bfloat16_tE19Flash_kernel_traitsILi64ELi128ELi128ELi4ES2_EELb0ELb1ELb1ELb0ELb0EEv16Flash_fwd_params
    40 bytes stack frame, 56 bytes spill stores, 56 bytes spill loads
ptxas info    : Used 255 registers, 584 bytes cmem[0]
ptxas info    : Compiling entry function '_Z16flash_fwd_kernelI23Flash_fwd_kernel_traitsILi64ELi128ELi128ELi4ELb0ELb0EN7cutlass10bfloat16_tE19Flash_kernel_traitsILi64ELi128ELi128ELi4ES2_EELb0ELb1ELb1ELb1ELb0EEv16Flash_fwd_params' for 'sm_80'
ptxas info    : Function properties for _Z16flash_fwd_kernelI23Flash_fwd_kernel_traitsILi64ELi128ELi128ELi4ELb0ELb0EN7cutlass10bfloat16_tE19Flash_kernel_traitsILi64ELi128ELi128ELi4ES2_EELb0ELb1ELb1ELb1ELb0EEv16Flash_fwd_params
    40 bytes stack frame, 52 bytes spill stores, 52 bytes spill loads
ptxas info    : Used 255 registers, 584 bytes cmem[0]
ptxas info    : Compiling entry function '_Z16flash_fwd_kernelI23Flash_fwd_kernel_traitsILi64ELi128ELi64ELi4ELb0ELb0EN7cutlass10bfloat16_tE19Flash_kernel_traitsILi64ELi128ELi64ELi4ES2_EELb1ELb0ELb0ELb0ELb0EEv16Flash_fwd_params' for 'sm_80'
ptxas info    : Function properties for _Z16flash_fwd_kernelI23Flash_fwd_kernel_traitsILi64ELi128ELi64ELi4ELb0ELb0EN7cutlass10bfloat16_tE19Flash_kernel_traitsILi64ELi128ELi64ELi4ES2_EELb1ELb0ELb0ELb0ELb0EEv16Flash_fwd_params
    0 bytes stack frame, 0 bytes spill stores, 0 bytes spill loads
ptxas info    : Used 242 registers, 584 bytes cmem[0]
ptxas info    : Compiling entry function '_Z16flash_fwd_kernelI23Flash_fwd_kernel_traitsILi64ELi128ELi64ELi4ELb0ELb0EN7cutlass10bfloat16_tE19Flash_kernel_traitsILi64ELi128ELi64ELi4ES2_EELb1ELb0ELb0ELb0ELb1EEv16Flash_fwd_params' for 'sm_80'
ptxas info    : Function properties for _Z16flash_fwd_kernelI23Flash_fwd_kernel_traitsILi64ELi128ELi64ELi4ELb0ELb0EN7cutlass10bfloat16_tE19Flash_kernel_traitsILi64ELi128ELi64ELi4ES2_EELb1ELb0ELb0ELb0ELb1EEv16Flash_fwd_params
    48 bytes stack frame, 44 bytes spill stores, 44 bytes spill loads
ptxas info    : Used 255 registers, 584 bytes cmem[0]
ptxas info    : Compiling entry function '_Z16flash_fwd_kernelI23Flash_fwd_kernel_traitsILi64ELi128ELi64ELi4ELb0ELb0EN7cutlass10bfloat16_tE19Flash_kernel_traitsILi64ELi128ELi64ELi4ES2_EELb1ELb0ELb0ELb1ELb0EEv16Flash_fwd_params' for 'sm_80'
ptxas info    : Function properties for _Z16flash_fwd_kernelI23Flash_fwd_kernel_traitsILi64ELi128ELi64ELi4ELb0ELb0EN7cutlass10bfloat16_tE19Flash_kernel_traitsILi64ELi128ELi64ELi4ES2_EELb1ELb0ELb0ELb1ELb0EEv16Flash_fwd_params
    0 bytes stack frame, 0 bytes spill stores, 0 bytes spill loads
ptxas info    : Used 236 registers, 584 bytes cmem[0]
ptxas info    : Compiling entry function '_Z16flash_fwd_kernelI23Flash_fwd_kernel_traitsILi64ELi128ELi64ELi4ELb0ELb0EN7cutlass10bfloat16_tE19Flash_kernel_traitsILi64ELi128ELi64ELi4ES2_EELb1ELb0ELb0ELb1ELb1EEv16Flash_fwd_params' for 'sm_80'
ptxas info    : Function properties for _Z16flash_fwd_kernelI23Flash_fwd_kernel_traitsILi64ELi128ELi64ELi4ELb0ELb0EN7cutlass10bfloat16_tE19Flash_kernel_traitsILi64ELi128ELi64ELi4ES2_EELb1ELb0ELb0ELb1ELb1EEv16Flash_fwd_params
    32 bytes stack frame, 32 bytes spill stores, 32 bytes spill loads
ptxas info    : Used 255 registers, 584 bytes cmem[0]
ptxas info    : Compiling entry function '_Z16flash_fwd_kernelI23Flash_fwd_kernel_traitsILi64ELi128ELi64ELi4ELb0ELb0EN7cutlass10bfloat16_tE19Flash_kernel_traitsILi64ELi128ELi64ELi4ES2_EELb1ELb0ELb1ELb0ELb0EEv16Flash_fwd_params' for 'sm_80'
ptxas info    : Function properties for _Z16flash_fwd_kernelI23Flash_fwd_kernel_traitsILi64ELi128ELi64ELi4ELb0ELb0EN7cutlass10bfloat16_tE19Flash_kernel_traitsILi64ELi128ELi64ELi4ES2_EELb1ELb0ELb1ELb0ELb0EEv16Flash_fwd_params
    0 bytes stack frame, 0 bytes spill stores, 0 bytes spill loads
ptxas info    : Used 236 registers, 584 bytes cmem[0]
ptxas info    : Compiling entry function '_Z16flash_fwd_kernelI23Flash_fwd_kernel_traitsILi64ELi128ELi64ELi4ELb0ELb0EN7cutlass10bfloat16_tE19Flash_kernel_traitsILi64ELi128ELi64ELi4ES2_EELb1ELb0ELb1ELb0ELb1EEv16Flash_fwd_params' for 'sm_80'
ptxas info    : Function properties for _Z16flash_fwd_kernelI23Flash_fwd_kernel_traitsILi64ELi128ELi64ELi4ELb0ELb0EN7cutlass10bfloat16_tE19Flash_kernel_traitsILi64ELi128ELi64ELi4ES2_EELb1ELb0ELb1ELb0ELb1EEv16Flash_fwd_params
    40 bytes stack frame, 40 bytes spill stores, 40 bytes spill loads
ptxas info    : Used 255 registers, 584 bytes cmem[0]
ptxas info    : Compiling entry function '_Z16flash_fwd_kernelI23Flash_fwd_kernel_traitsILi64ELi128ELi64ELi4ELb0ELb0EN7cutlass10bfloat16_tE19Flash_kernel_traitsILi64ELi128ELi64ELi4ES2_EELb1ELb0ELb1ELb1ELb0EEv16Flash_fwd_params' for 'sm_80'
ptxas info    : Function properties for _Z16flash_fwd_kernelI23Flash_fwd_kernel_traitsILi64ELi128ELi64ELi4ELb0ELb0EN7cutlass10bfloat16_tE19Flash_kernel_traitsILi64ELi128ELi64ELi4ES2_EELb1ELb0ELb1ELb1ELb0EEv16Flash_fwd_params
    0 bytes stack frame, 0 bytes spill stores, 0 bytes spill loads
ptxas info    : Used 238 registers, 584 bytes cmem[0]
ptxas info    : Compiling entry function '_Z16flash_fwd_kernelI23Flash_fwd_kernel_traitsILi64ELi128ELi64ELi4ELb0ELb0EN7cutlass10bfloat16_tE19Flash_kernel_traitsILi64ELi128ELi64ELi4ES2_EELb1ELb0ELb1ELb1ELb1EEv16Flash_fwd_params' for 'sm_80'
ptxas info    : Function properties for _Z16flash_fwd_kernelI23Flash_fwd_kernel_traitsILi64ELi128ELi64ELi4ELb0ELb0EN7cutlass10bfloat16_tE19Flash_kernel_traitsILi64ELi128ELi64ELi4ES2_EELb1ELb0ELb1ELb1ELb1EEv16Flash_fwd_params
    0 bytes stack frame, 0 bytes spill stores, 0 bytes spill loads
ptxas info    : Used 255 registers, 584 bytes cmem[0]
ptxas info    : Compiling entry function '_Z16flash_fwd_kernelI23Flash_fwd_kernel_traitsILi64ELi128ELi64ELi4ELb0ELb0EN7cutlass10bfloat16_tE19Flash_kernel_traitsILi64ELi128ELi64ELi4ES2_EELb1ELb1ELb0ELb0ELb0EEv16Flash_fwd_params' for 'sm_80'
ptxas info    : Function properties for _Z16flash_fwd_kernelI23Flash_fwd_kernel_traitsILi64ELi128ELi64ELi4ELb0ELb0EN7cutlass10bfloat16_tE19Flash_kernel_traitsILi64ELi128ELi64ELi4ES2_EELb1ELb1ELb0ELb0ELb0EEv16Flash_fwd_params
    0 bytes stack frame, 0 bytes spill stores, 0 bytes spill loads
ptxas info    : Used 250 registers, 584 bytes cmem[0]
ptxas info    : Compiling entry function '_Z16flash_fwd_kernelI23Flash_fwd_kernel_traitsILi64ELi128ELi64ELi4ELb0ELb0EN7cutlass10bfloat16_tE19Flash_kernel_traitsILi64ELi128ELi64ELi4ES2_EELb1ELb1ELb0ELb0ELb1EEv16Flash_fwd_params' for 'sm_80'
ptxas info    : Function properties for _Z16flash_fwd_kernelI23Flash_fwd_kernel_traitsILi64ELi128ELi64ELi4ELb0ELb0EN7cutlass10bfloat16_tE19Flash_kernel_traitsILi64ELi128ELi64ELi4ES2_EELb1ELb1ELb0ELb0ELb1EEv16Flash_fwd_params
    40 bytes stack frame, 36 bytes spill stores, 36 bytes spill loads
ptxas info    : Used 255 registers, 584 bytes cmem[0]
ptxas info    : Compiling entry function '_Z16flash_fwd_kernelI23Flash_fwd_kernel_traitsILi64ELi128ELi64ELi4ELb0ELb0EN7cutlass10bfloat16_tE19Flash_kernel_traitsILi64ELi128ELi64ELi4ES2_EELb1ELb1ELb0ELb1ELb0EEv16Flash_fwd_params' for 'sm_80'
ptxas info    : Function properties for _Z16flash_fwd_kernelI23Flash_fwd_kernel_traitsILi64ELi128ELi64ELi4ELb0ELb0EN7cutlass10bfloat16_tE19Flash_kernel_traitsILi64ELi128ELi64ELi4ES2_EELb1ELb1ELb0ELb1ELb0EEv16Flash_fwd_params
    0 bytes stack frame, 0 bytes spill stores, 0 bytes spill loads
ptxas info    : Used 244 registers, 584 bytes cmem[0]
ptxas info    : Compiling entry function '_Z16flash_fwd_kernelI23Flash_fwd_kernel_traitsILi64ELi128ELi64ELi4ELb0ELb0EN7cutlass10bfloat16_tE19Flash_kernel_traitsILi64ELi128ELi64ELi4ES2_EELb1ELb1ELb0ELb1ELb1EEv16Flash_fwd_params' for 'sm_80'
ptxas info    : Function properties for _Z16flash_fwd_kernelI23Flash_fwd_kernel_traitsILi64ELi128ELi64ELi4ELb0ELb0EN7cutlass10bfloat16_tE19Flash_kernel_traitsILi64ELi128ELi64ELi4ES2_EELb1ELb1ELb0ELb1ELb1EEv16Flash_fwd_params
    56 bytes stack frame, 56 bytes spill stores, 56 bytes spill loads
ptxas info    : Used 255 registers, 584 bytes cmem[0]
ptxas info    : Compiling entry function '_Z16flash_fwd_kernelI23Flash_fwd_kernel_traitsILi64ELi128ELi64ELi4ELb0ELb0EN7cutlass10bfloat16_tE19Flash_kernel_traitsILi64ELi128ELi64ELi4ES2_EELb1ELb1ELb1ELb0ELb0EEv16Flash_fwd_params' for 'sm_80'
ptxas info    : Function properties for _Z16flash_fwd_kernelI23Flash_fwd_kernel_traitsILi64ELi128ELi64ELi4ELb0ELb0EN7cutlass10bfloat16_tE19Flash_kernel_traitsILi64ELi128ELi64ELi4ES2_EELb1ELb1ELb1ELb0ELb0EEv16Flash_fwd_params
    0 bytes stack frame, 0 bytes spill stores, 0 bytes spill loads
ptxas info    : Used 248 registers, 584 bytes cmem[0]
ptxas info    : Compiling entry function '_Z16flash_fwd_kernelI23Flash_fwd_kernel_traitsILi64ELi128ELi64ELi4ELb0ELb0EN7cutlass10bfloat16_tE19Flash_kernel_traitsILi64ELi128ELi64ELi4ES2_EELb1ELb1ELb1ELb0ELb1EEv16Flash_fwd_params' for 'sm_80'
ptxas info    : Function properties for _Z16flash_fwd_kernelI23Flash_fwd_kernel_traitsILi64ELi128ELi64ELi4ELb0ELb0EN7cutlass10bfloat16_tE19Flash_kernel_traitsILi64ELi128ELi64ELi4ES2_EELb1ELb1ELb1ELb0ELb1EEv16Flash_fwd_params
    32 bytes stack frame, 32 bytes spill stores, 32 bytes spill loads
ptxas info    : Used 255 registers, 584 bytes cmem[0]
ptxas info    : Compiling entry function '_Z16flash_fwd_kernelI23Flash_fwd_kernel_traitsILi64ELi128ELi64ELi4ELb0ELb0EN7cutlass10bfloat16_tE19Flash_kernel_traitsILi64ELi128ELi64ELi4ES2_EELb1ELb1ELb1ELb1ELb0EEv16Flash_fwd_params' for 'sm_80'
ptxas info    : Function properties for _Z16flash_fwd_kernelI23Flash_fwd_kernel_traitsILi64ELi128ELi64ELi4ELb0ELb0EN7cutlass10bfloat16_tE19Flash_kernel_traitsILi64ELi128ELi64ELi4ES2_EELb1ELb1ELb1ELb1ELb0EEv16Flash_fwd_params
    0 bytes stack frame, 0 bytes spill stores, 0 bytes spill loads
ptxas info    : Used 246 registers, 584 bytes cmem[0]
ptxas info    : Compiling entry function '_Z16flash_fwd_kernelI23Flash_fwd_kernel_traitsILi64ELi128ELi64ELi4ELb0ELb0EN7cutlass10bfloat16_tE19Flash_kernel_traitsILi64ELi128ELi64ELi4ES2_EELb1ELb1ELb1ELb1ELb1EEv16Flash_fwd_params' for 'sm_80'
ptxas info    : Function properties for _Z16flash_fwd_kernelI23Flash_fwd_kernel_traitsILi64ELi128ELi64ELi4ELb0ELb0EN7cutlass10bfloat16_tE19Flash_kernel_traitsILi64ELi128ELi64ELi4ES2_EELb1ELb1ELb1ELb1ELb1EEv16Flash_fwd_params
    24 bytes stack frame, 24 bytes spill stores, 24 bytes spill loads
ptxas info    : Used 255 registers, 584 bytes cmem[0]
/anaconda/envs/flashattention/lib/python3.9/site-packages/torch/include/c10/core/SymInt.h(84): warning: integer conversion resulted in a change of sign

[30/37] /usr/local/cuda-11.4/bin/nvcc  -I/home/v-feiychen/flash-attention/csrc/flash_attn -I/home/v-feiychen/flash-attention/csrc/flash_attn/src -I/home/v-feiychen/flash-attention/csrc/cutlass/include -I/anaconda/envs/flashattention/lib/python3.9/site-packages/torch/include -I/anaconda/envs/flashattention/lib/python3.9/site-packages/torch/include/torch/csrc/api/include -I/anaconda/envs/flashattention/lib/python3.9/site-packages/torch/include/TH -I/anaconda/envs/flashattention/lib/python3.9/site-packages/torch/include/THC -I/usr/local/cuda-11.4/include -I/anaconda/envs/flashattention/include/python3.9 -c -c /home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_bwd_hdim512_fp16_sm80.cu -o /home/v-feiychen/flash-attention/build/temp.linux-x86_64-cpython-39/csrc/flash_attn/src/flash_bwd_hdim512_fp16_sm80.o -D__CUDA_NO_HALF_OPERATORS__ -D__CUDA_NO_HALF_CONVERSIONS__ -D__CUDA_NO_BFLOAT16_CONVERSIONS__ -D__CUDA_NO_HALF2_OPERATORS__ --expt-relaxed-constexpr --compiler-options ''"'"'-fPIC'"'"'' -O3 -std=c++17 -U__CUDA_NO_HALF_OPERATORS__ -U__CUDA_NO_HALF_CONVERSIONS__ -U__CUDA_NO_HALF2_OPERATORS__ -U__CUDA_NO_BFLOAT16_CONVERSIONS__ --expt-relaxed-constexpr --expt-extended-lambda --use_fast_math --ptxas-options=-v -lineinfo -gencode arch=compute_80,code=sm_80 --threads 4 -DTORCH_API_INCLUDE_EXTENSION_H '-DPYBIND11_COMPILER_TYPE="_gcc"' '-DPYBIND11_STDLIB="_libstdcpp"' '-DPYBIND11_BUILD_ABI="_cxxabi1011"' -DTORCH_EXTENSION_NAME=flash_attn_2_cuda -D_GLIBCXX_USE_CXX11_ABI=0
/anaconda/envs/flashattention/lib/python3.9/site-packages/torch/include/c10/core/SymInt.h(84): warning: integer conversion resulted in a change of sign

ptxas info    : 2 bytes gmem
ptxas info    : Compiling entry function '_Z44flash_bwd_dq_dk_dv_loop_seqk_parallel_kernelI23Flash_bwd_kernel_traitsILi512ELi32ELi64ELi4ELi2ELi2ELi2ELb0ELb1EN7cutlass6half_tE19Flash_kernel_traitsILi512ELi32ELi64ELi4ES2_EELb0ELb0ELb0ELb0EEv16Flash_bwd_params' for 'sm_80'
ptxas info    : Function properties for _Z44flash_bwd_dq_dk_dv_loop_seqk_parallel_kernelI23Flash_bwd_kernel_traitsILi512ELi32ELi64ELi4ELi2ELi2ELi2ELb0ELb1EN7cutlass6half_tE19Flash_kernel_traitsILi512ELi32ELi64ELi4ES2_EELb0ELb0ELb0ELb0EEv16Flash_bwd_params
    3048 bytes stack frame, 0 bytes spill stores, 0 bytes spill loads
ptxas info    : Used 255 registers, 696 bytes cmem[0]
ptxas info    : Function properties for _ZN5flash26compute_dq_dk_dv_1colblockI23Flash_bwd_kernel_traitsILi512ELi32ELi64ELi4ELi2ELi2ELi2ELb0ELb1EN7cutlass6half_tE19Flash_kernel_traitsILi512ELi32ELi64ELi4ES3_EELb0ELb0ELb0ELb0ELb0ELb0ELb1E16Flash_bwd_paramsEEvRKT7_iii
    0 bytes stack frame, 7340 bytes spill stores, 5384 bytes spill loads
ptxas info    : Compiling entry function '_Z44flash_bwd_dq_dk_dv_loop_seqk_parallel_kernelI23Flash_bwd_kernel_traitsILi512ELi32ELi64ELi4ELi2ELi2ELi2ELb0ELb1EN7cutlass6half_tE19Flash_kernel_traitsILi512ELi32ELi64ELi4ES2_EELb0ELb0ELb0ELb1EEv16Flash_bwd_params' for 'sm_80'
ptxas info    : Function properties for _Z44flash_bwd_dq_dk_dv_loop_seqk_parallel_kernelI23Flash_bwd_kernel_traitsILi512ELi32ELi64ELi4ELi2ELi2ELi2ELb0ELb1EN7cutlass6half_tE19Flash_kernel_traitsILi512ELi32ELi64ELi4ES2_EELb0ELb0ELb0ELb1EEv16Flash_bwd_params
    2696 bytes stack frame, 7536 bytes spill stores, 5516 bytes spill loads
ptxas info    : Used 255 registers, 696 bytes cmem[0]
ptxas info    : Compiling entry function '_Z44flash_bwd_dq_dk_dv_loop_seqk_parallel_kernelI23Flash_bwd_kernel_traitsILi512ELi32ELi64ELi4ELi2ELi2ELi2ELb0ELb1EN7cutlass6half_tE19Flash_kernel_traitsILi512ELi32ELi64ELi4ES2_EELb0ELb0ELb1ELb0EEv16Flash_bwd_params' for 'sm_80'
ptxas info    : Function properties for _Z44flash_bwd_dq_dk_dv_loop_seqk_parallel_kernelI23Flash_bwd_kernel_traitsILi512ELi32ELi64ELi4ELi2ELi2ELi2ELb0ELb1EN7cutlass6half_tE19Flash_kernel_traitsILi512ELi32ELi64ELi4ES2_EELb0ELb0ELb1ELb0EEv16Flash_bwd_params
    2704 bytes stack frame, 7772 bytes spill stores, 5776 bytes spill loads
ptxas info    : Used 255 registers, 696 bytes cmem[0]
ptxas info    : Compiling entry function '_Z44flash_bwd_dq_dk_dv_loop_seqk_parallel_kernelI23Flash_bwd_kernel_traitsILi512ELi32ELi64ELi4ELi2ELi2ELi2ELb0ELb1EN7cutlass6half_tE19Flash_kernel_traitsILi512ELi32ELi64ELi4ES2_EELb0ELb0ELb1ELb1EEv16Flash_bwd_params' for 'sm_80'
ptxas info    : Function properties for _Z44flash_bwd_dq_dk_dv_loop_seqk_parallel_kernelI23Flash_bwd_kernel_traitsILi512ELi32ELi64ELi4ELi2ELi2ELi2ELb0ELb1EN7cutlass6half_tE19Flash_kernel_traitsILi512ELi32ELi64ELi4ES2_EELb0ELb0ELb1ELb1EEv16Flash_bwd_params
    2688 bytes stack frame, 7520 bytes spill stores, 5492 bytes spill loads
ptxas info    : Used 255 registers, 696 bytes cmem[0]
ptxas info    : Compiling entry function '_Z44flash_bwd_dq_dk_dv_loop_seqk_parallel_kernelI23Flash_bwd_kernel_traitsILi512ELi32ELi64ELi4ELi2ELi2ELi2ELb0ELb1EN7cutlass6half_tE19Flash_kernel_traitsILi512ELi32ELi64ELi4ES2_EELb0ELb1ELb0ELb0EEv16Flash_bwd_params' for 'sm_80'
ptxas info    : Function properties for _Z44flash_bwd_dq_dk_dv_loop_seqk_parallel_kernelI23Flash_bwd_kernel_traitsILi512ELi32ELi64ELi4ELi2ELi2ELi2ELb0ELb1EN7cutlass6half_tE19Flash_kernel_traitsILi512ELi32ELi64ELi4ES2_EELb0ELb1ELb0ELb0EEv16Flash_bwd_params
    4304 bytes stack frame, 5400 bytes spill stores, 5448 bytes spill loads
ptxas info    : Used 255 registers, 696 bytes cmem[0]
ptxas info    : Function properties for _ZN5flash4gemmILb0ELb0EN4cute6TensorINS1_13array_alignedIfLm16ELm16EEENS1_6LayoutINS1_5tupleIJNS6_IJNS1_8constantIiLi2EEES8_EEENS7_IiLi1EEENS7_IiLi4EEEEEENS6_IJNS6_IJSA_S8_EEENS7_IiLi0EEESB_EEEEEEENS2_INS3_IN7cutlass6half_tELm256ELm16EEENS5_INS6_IJNS6_IJS8_S8_S8_EEESA_NS6_IJS9_NS7_IiLi8EEEEEEEEENS6_IJNS6_IJSA_S8_SB_EEESE_NS6_IJNS6_IJSM_NS7_IiLi16EEEEEENS7_IiLi32EEEEEEEEEEEEENS2_INS3_ISJ_Lm512ELm16EEENS5_INS6_IJS9_SB_SN_EEENS6_IJSD_SB_NS6_IJNS6_IJSQ_SS_EEENS7_IiLi64EEEEEEEEEEEEENS2_INS1_10ViewEngineINS1_8smem_ptrISJ_EEEENS5_INS6_IJNS6_IJSM_SA_EEESA_SN_EEENS6_IJNS6_IJSA_SE_EEESE_NS6_IJNS6_IJiiEEENS7_IiLi2048EEEEEEEEEEEEENS2_IS18_NS5_INS6_IJS19_S8_SN_EEENS6_IJS1B_NS7_IiLi1024EEENS6_IJS1C_NS7_IiLi4096EEEEEEEEEEEEENS1_8TiledMMAINS1_8MMA_AtomIJNS1_28SM80_16x8x16_F32F16F16F32_TNEEEENS5_INS6_IJS8_S8_SA_EEENS6_IJSA_S8_SE_EEEEENS5_INS6_IJSA_S8_SA_EEENS6_IJSE_SA_SE_EEEEENS6_IJNS1_10UnderscoreES1Z_S1Z_EEEEENS1_9TiledCopyINS1_9Copy_AtomIJNS1_17SM75_U32x4_LDSM_NESJ_EEENS5_INS6_IJNS6_IJSB_SM_S8_S8_EEENS6_IJSL_NS6_IJSA_SA_EEEEEEEEENS6_IJNS6_IJS10_SA_SQ_SE_EEENS6_IJNS6_IJSS_SM_NS7_IiLi256EEEEEENS6_IJSE_SE_EEEEEEEEEEENS6_IJSS_SQ_EEEEENS22_IS25_NS5_INS6_IJS26_NS6_IJS9_NS6_IJS8_SA_EEEEEEEEENS6_IJNS6_IJS10_SA_SE_SM_EEENS6_IJNS6_IJSS_S2B_EEENS6_IJSQ_SE_EEEEEEEEEEENS6_IJNS5_INS6_IJSM_S8_S8_EEENS6_IJSA_SS_SM_EEEEENS5_ISQ_SA_EEEEEEENS1_7ThrCopyIS2I_iEENS2Y_IS2X_iEEEEvRT1_RT2_RT3_RKT4_RKT5_T6_T7_T8_T9_T10_
    0 bytes stack frame, 0 bytes spill stores, 0 bytes spill loads
ptxas info    : Compiling entry function '_Z44flash_bwd_dq_dk_dv_loop_seqk_parallel_kernelI23Flash_bwd_kernel_traitsILi512ELi32ELi64ELi4ELi2ELi2ELi2ELb0ELb1EN7cutlass6half_tE19Flash_kernel_traitsILi512ELi32ELi64ELi4ES2_EELb0ELb1ELb0ELb1EEv16Flash_bwd_params' for 'sm_80'
ptxas info    : Function properties for _Z44flash_bwd_dq_dk_dv_loop_seqk_parallel_kernelI23Flash_bwd_kernel_traitsILi512ELi32ELi64ELi4ELi2ELi2ELi2ELb0ELb1EN7cutlass6half_tE19Flash_kernel_traitsILi512ELi32ELi64ELi4ES2_EELb0ELb1ELb0ELb1EEv16Flash_bwd_params
    2696 bytes stack frame, 5492 bytes spill stores, 5520 bytes spill loads
ptxas info    : Used 255 registers, 696 bytes cmem[0]
ptxas info    : Compiling entry function '_Z44flash_bwd_dq_dk_dv_loop_seqk_parallel_kernelI23Flash_bwd_kernel_traitsILi512ELi32ELi64ELi4ELi2ELi2ELi2ELb0ELb1EN7cutlass6half_tE19Flash_kernel_traitsILi512ELi32ELi64ELi4ES2_EELb0ELb1ELb1ELb0EEv16Flash_bwd_params' for 'sm_80'
ptxas info    : Function properties for _Z44flash_bwd_dq_dk_dv_loop_seqk_parallel_kernelI23Flash_bwd_kernel_traitsILi512ELi32ELi64ELi4ELi2ELi2ELi2ELb0ELb1EN7cutlass6half_tE19Flash_kernel_traitsILi512ELi32ELi64ELi4ES2_EELb0ELb1ELb1ELb0EEv16Flash_bwd_params
    2768 bytes stack frame, 5776 bytes spill stores, 5824 bytes spill loads
ptxas info    : Used 255 registers, 696 bytes cmem[0]
ptxas info    : Compiling entry function '_Z44flash_bwd_dq_dk_dv_loop_seqk_parallel_kernelI23Flash_bwd_kernel_traitsILi512ELi32ELi64ELi4ELi2ELi2ELi2ELb0ELb1EN7cutlass6half_tE19Flash_kernel_traitsILi512ELi32ELi64ELi4ES2_EELb0ELb1ELb1ELb1EEv16Flash_bwd_params' for 'sm_80'
ptxas info    : Function properties for _Z44flash_bwd_dq_dk_dv_loop_seqk_parallel_kernelI23Flash_bwd_kernel_traitsILi512ELi32ELi64ELi4ELi2ELi2ELi2ELb0ELb1EN7cutlass6half_tE19Flash_kernel_traitsILi512ELi32ELi64ELi4ES2_EELb0ELb1ELb1ELb1EEv16Flash_bwd_params
    2696 bytes stack frame, 5480 bytes spill stores, 5508 bytes spill loads
ptxas info    : Used 255 registers, 696 bytes cmem[0]
ptxas info    : Compiling entry function '_Z44flash_bwd_dq_dk_dv_loop_seqk_parallel_kernelI23Flash_bwd_kernel_traitsILi512ELi32ELi64ELi4ELi2ELi2ELi2ELb0ELb0EN7cutlass6half_tE19Flash_kernel_traitsILi512ELi32ELi64ELi4ES2_EELb0ELb0ELb0ELb0EEv16Flash_bwd_params' for 'sm_80'
ptxas info    : Function properties for _Z44flash_bwd_dq_dk_dv_loop_seqk_parallel_kernelI23Flash_bwd_kernel_traitsILi512ELi32ELi64ELi4ELi2ELi2ELi2ELb0ELb0EN7cutlass6half_tE19Flash_kernel_traitsILi512ELi32ELi64ELi4ES2_EELb0ELb0ELb0ELb0EEv16Flash_bwd_params
    3016 bytes stack frame, 0 bytes spill stores, 0 bytes spill loads
ptxas info    : Used 255 registers, 696 bytes cmem[0]
ptxas info    : Function properties for _ZN5flash26compute_dq_dk_dv_1colblockI23Flash_bwd_kernel_traitsILi512ELi32ELi64ELi4ELi2ELi2ELi2ELb0ELb0EN7cutlass6half_tE19Flash_kernel_traitsILi512ELi32ELi64ELi4ES3_EELb0ELb0ELb0ELb0ELb0ELb0ELb1E16Flash_bwd_paramsEEvRKT7_iii
    0 bytes stack frame, 7156 bytes spill stores, 5236 bytes spill loads
ptxas info    : Compiling entry function '_Z44flash_bwd_dq_dk_dv_loop_seqk_parallel_kernelI23Flash_bwd_kernel_traitsILi512ELi32ELi64ELi4ELi2ELi2ELi2ELb0ELb0EN7cutlass6half_tE19Flash_kernel_traitsILi512ELi32ELi64ELi4ES2_EELb0ELb0ELb0ELb1EEv16Flash_bwd_params' for 'sm_80'
ptxas info    : Function properties for _Z44flash_bwd_dq_dk_dv_loop_seqk_parallel_kernelI23Flash_bwd_kernel_traitsILi512ELi32ELi64ELi4ELi2ELi2ELi2ELb0ELb0EN7cutlass6half_tE19Flash_kernel_traitsILi512ELi32ELi64ELi4ES2_EELb0ELb0ELb0ELb1EEv16Flash_bwd_params
    2504 bytes stack frame, 7272 bytes spill stores, 5232 bytes spill loads
ptxas info    : Used 255 registers, 696 bytes cmem[0]
ptxas info    : Compiling entry function '_Z44flash_bwd_dq_dk_dv_loop_seqk_parallel_kernelI23Flash_bwd_kernel_traitsILi512ELi32ELi64ELi4ELi2ELi2ELi2ELb0ELb0EN7cutlass6half_tE19Flash_kernel_traitsILi512ELi32ELi64ELi4ES2_EELb0ELb0ELb1ELb0EEv16Flash_bwd_params' for 'sm_80'
ptxas info    : Function properties for _Z44flash_bwd_dq_dk_dv_loop_seqk_parallel_kernelI23Flash_bwd_kernel_traitsILi512ELi32ELi64ELi4ELi2ELi2ELi2ELb0ELb0EN7cutlass6half_tE19Flash_kernel_traitsILi512ELi32ELi64ELi4ES2_EELb0ELb0ELb1ELb0EEv16Flash_bwd_params
    2752 bytes stack frame, 7600 bytes spill stores, 5592 bytes spill loads
ptxas info    : Used 255 registers, 696 bytes cmem[0]
ptxas info    : Compiling entry function '_Z44flash_bwd_dq_dk_dv_loop_seqk_parallel_kernelI23Flash_bwd_kernel_traitsILi512ELi32ELi64ELi4ELi2ELi2ELi2ELb0ELb0EN7cutlass6half_tE19Flash_kernel_traitsILi512ELi32ELi64ELi4ES2_EELb0ELb0ELb1ELb1EEv16Flash_bwd_params' for 'sm_80'
ptxas info    : Function properties for _Z44flash_bwd_dq_dk_dv_loop_seqk_parallel_kernelI23Flash_bwd_kernel_traitsILi512ELi32ELi64ELi4ELi2ELi2ELi2ELb0ELb0EN7cutlass6half_tE19Flash_kernel_traitsILi512ELi32ELi64ELi4ES2_EELb0ELb0ELb1ELb1EEv16Flash_bwd_params
    2512 bytes stack frame, 7304 bytes spill stores, 5264 bytes spill loads
ptxas info    : Used 255 registers, 696 bytes cmem[0]
ptxas info    : Compiling entry function '_Z44flash_bwd_dq_dk_dv_loop_seqk_parallel_kernelI23Flash_bwd_kernel_traitsILi512ELi32ELi64ELi4ELi2ELi2ELi2ELb0ELb0EN7cutlass6half_tE19Flash_kernel_traitsILi512ELi32ELi64ELi4ES2_EELb0ELb1ELb0ELb0EEv16Flash_bwd_params' for 'sm_80'
ptxas info    : Function properties for _Z44flash_bwd_dq_dk_dv_loop_seqk_parallel_kernelI23Flash_bwd_kernel_traitsILi512ELi32ELi64ELi4ELi2ELi2ELi2ELb0ELb0EN7cutlass6half_tE19Flash_kernel_traitsILi512ELi32ELi64ELi4ES2_EELb0ELb1ELb0ELb0EEv16Flash_bwd_params
    4224 bytes stack frame, 5340 bytes spill stores, 4932 bytes spill loads
ptxas info    : Used 255 registers, 696 bytes cmem[0]
ptxas info    : Function properties for _ZN5flash4gemmILb0ELb0EN4cute6TensorINS1_13array_alignedIfLm16ELm16EEENS1_6LayoutINS1_5tupleIJNS6_IJNS1_8constantIiLi2EEES8_EEENS7_IiLi1EEENS7_IiLi4EEEEEENS6_IJNS6_IJSA_S8_EEENS7_IiLi0EEESB_EEEEEEENS2_INS3_IN7cutlass6half_tELm256ELm16EEENS5_INS6_IJNS6_IJS8_S8_S8_EEESA_NS6_IJS9_NS7_IiLi8EEEEEEEEENS6_IJNS6_IJSA_S8_SB_EEESE_NS6_IJNS6_IJSM_NS7_IiLi16EEEEEENS7_IiLi32EEEEEEEEEEEEENS2_INS3_ISJ_Lm512ELm16EEENS5_INS6_IJS9_SB_SN_EEENS6_IJSD_SB_NS6_IJNS6_IJSQ_SS_EEENS7_IiLi64EEEEEEEEEEEEENS2_INS1_10ViewEngineINS1_8smem_ptrISJ_EEEENS5_INS6_IJNS6_IJSM_SA_EEESA_SN_EEENS6_IJNS6_IJSA_SE_EEESE_NS6_IJNS6_IJiiEEENS7_IiLi2048EEEEEEEEEEEEENS2_IS18_NS5_INS6_IJS19_S8_SN_EEENS6_IJS1B_NS7_IiLi1024EEENS6_IJS1C_NS7_IiLi4096EEEEEEEEEEEEENS1_8TiledMMAINS1_8MMA_AtomIJNS1_28SM80_16x8x16_F32F16F16F32_TNEEEENS5_INS6_IJS8_S8_SA_EEENS6_IJSA_S8_SE_EEEEENS5_INS6_IJSA_S8_SA_EEENS6_IJSE_SA_SE_EEEEENS6_IJNS1_10UnderscoreES1Z_S1Z_EEEEENS1_9TiledCopyINS1_9Copy_AtomIJNS1_17SM75_U32x4_LDSM_NESJ_EEENS5_INS6_IJNS6_IJSB_SM_S8_S8_EEENS6_IJSL_NS6_IJSA_SA_EEEEEEEEENS6_IJNS6_IJS10_SA_SQ_SE_EEENS6_IJNS6_IJSS_SM_NS7_IiLi256EEEEEENS6_IJSE_SE_EEEEEEEEEEENS6_IJSS_SQ_EEEEENS22_IS25_NS5_INS6_IJS26_NS6_IJS9_NS6_IJS8_SA_EEEEEEEEENS6_IJNS6_IJS10_SA_SE_SM_EEENS6_IJNS6_IJSS_S2B_EEENS6_IJSQ_SE_EEEEEEEEEEENS6_IJNS5_INS6_IJSM_S8_S8_EEENS6_IJSA_SS_SM_EEEEENS5_ISQ_SA_EEEEEEENS1_7ThrCopyIS2I_iEENS2Y_IS2X_iEEEEvRT1_RT2_RT3_RKT4_RKT5_T6_T7_T8_T9_T10_
    0 bytes stack frame, 0 bytes spill stores, 0 bytes spill loads
ptxas info    : Compiling entry function '_Z44flash_bwd_dq_dk_dv_loop_seqk_parallel_kernelI23Flash_bwd_kernel_traitsILi512ELi32ELi64ELi4ELi2ELi2ELi2ELb0ELb0EN7cutlass6half_tE19Flash_kernel_traitsILi512ELi32ELi64ELi4ES2_EELb0ELb1ELb0ELb1EEv16Flash_bwd_params' for 'sm_80'
ptxas info    : Function properties for _Z44flash_bwd_dq_dk_dv_loop_seqk_parallel_kernelI23Flash_bwd_kernel_traitsILi512ELi32ELi64ELi4ELi2ELi2ELi2ELb0ELb0EN7cutlass6half_tE19Flash_kernel_traitsILi512ELi32ELi64ELi4ES2_EELb0ELb1ELb0ELb1EEv16Flash_bwd_params
    2536 bytes stack frame, 5228 bytes spill stores, 4760 bytes spill loads
ptxas info    : Used 255 registers, 696 bytes cmem[0]
ptxas info    : Compiling entry function '_Z44flash_bwd_dq_dk_dv_loop_seqk_parallel_kernelI23Flash_bwd_kernel_traitsILi512ELi32ELi64ELi4ELi2ELi2ELi2ELb0ELb0EN7cutlass6half_tE19Flash_kernel_traitsILi512ELi32ELi64ELi4ES2_EELb0ELb1ELb1ELb0EEv16Flash_bwd_params' for 'sm_80'
ptxas info    : Function properties for _Z44flash_bwd_dq_dk_dv_loop_seqk_parallel_kernelI23Flash_bwd_kernel_traitsILi512ELi32ELi64ELi4ELi2ELi2ELi2ELb0ELb0EN7cutlass6half_tE19Flash_kernel_traitsILi512ELi32ELi64ELi4ES2_EELb0ELb1ELb1ELb0EEv16Flash_bwd_params
    2864 bytes stack frame, 5628 bytes spill stores, 5272 bytes spill loads
ptxas info    : Used 255 registers, 696 bytes cmem[0]
ptxas info    : Compiling entry function '_Z44flash_bwd_dq_dk_dv_loop_seqk_parallel_kernelI23Flash_bwd_kernel_traitsILi512ELi32ELi64ELi4ELi2ELi2ELi2ELb0ELb0EN7cutlass6half_tE19Flash_kernel_traitsILi512ELi32ELi64ELi4ES2_EELb0ELb1ELb1ELb1EEv16Flash_bwd_params' for 'sm_80'
ptxas info    : Function properties for _Z44flash_bwd_dq_dk_dv_loop_seqk_parallel_kernelI23Flash_bwd_kernel_traitsILi512ELi32ELi64ELi4ELi2ELi2ELi2ELb0ELb0EN7cutlass6half_tE19Flash_kernel_traitsILi512ELi32ELi64ELi4ES2_EELb0ELb1ELb1ELb1EEv16Flash_bwd_params
    2520 bytes stack frame, 5280 bytes spill stores, 4796 bytes spill loads
ptxas info    : Used 255 registers, 696 bytes cmem[0]
ptxas info    : Compiling entry function '_Z27flash_bwd_convert_dq_kernelI23Flash_bwd_kernel_traitsILi512ELi32ELi64ELi4ELi2ELi2ELi2ELb0ELb1EN7cutlass6half_tE19Flash_kernel_traitsILi512ELi32ELi64ELi4ES2_EEEv16Flash_bwd_params' for 'sm_80'
ptxas info    : Function properties for _Z27flash_bwd_convert_dq_kernelI23Flash_bwd_kernel_traitsILi512ELi32ELi64ELi4ELi2ELi2ELi2ELb0ELb1EN7cutlass6half_tE19Flash_kernel_traitsILi512ELi32ELi64ELi4ES2_EEEv16Flash_bwd_params
    0 bytes stack frame, 0 bytes spill stores, 0 bytes spill loads
ptxas info    : Used 56 registers, 696 bytes cmem[0]
ptxas info    : Compiling entry function '_Z44flash_bwd_dq_dk_dv_loop_seqk_parallel_kernelI23Flash_bwd_kernel_traitsILi512ELi32ELi64ELi4ELi2ELi2ELi2ELb0ELb1EN7cutlass6half_tE19Flash_kernel_traitsILi512ELi32ELi64ELi4ES2_EELb1ELb0ELb0ELb0EEv16Flash_bwd_params' for 'sm_80'
ptxas info    : Function properties for _Z44flash_bwd_dq_dk_dv_loop_seqk_parallel_kernelI23Flash_bwd_kernel_traitsILi512ELi32ELi64ELi4ELi2ELi2ELi2ELb0ELb1EN7cutlass6half_tE19Flash_kernel_traitsILi512ELi32ELi64ELi4ES2_EELb1ELb0ELb0ELb0EEv16Flash_bwd_params
    2992 bytes stack frame, 0 bytes spill stores, 0 bytes spill loads
ptxas info    : Used 255 registers, 696 bytes cmem[0]
ptxas info    : Function properties for _ZN5flash26compute_dq_dk_dv_1colblockI23Flash_bwd_kernel_traitsILi512ELi32ELi64ELi4ELi2ELi2ELi2ELb0ELb1EN7cutlass6half_tE19Flash_kernel_traitsILi512ELi32ELi64ELi4ES3_EELb1ELb0ELb0ELb0ELb0ELb0ELb1E16Flash_bwd_paramsEEvRKT7_iii
    0 bytes stack frame, 7448 bytes spill stores, 5512 bytes spill loads
ptxas info    : Compiling entry function '_Z44flash_bwd_dq_dk_dv_loop_seqk_parallel_kernelI23Flash_bwd_kernel_traitsILi512ELi32ELi64ELi4ELi2ELi2ELi2ELb0ELb1EN7cutlass6half_tE19Flash_kernel_traitsILi512ELi32ELi64ELi4ES2_EELb1ELb0ELb0ELb1EEv16Flash_bwd_params' for 'sm_80'
ptxas info    : Function properties for _Z44flash_bwd_dq_dk_dv_loop_seqk_parallel_kernelI23Flash_bwd_kernel_traitsILi512ELi32ELi64ELi4ELi2ELi2ELi2ELb0ELb1EN7cutlass6half_tE19Flash_kernel_traitsILi512ELi32ELi64ELi4ES2_EELb1ELb0ELb0ELb1EEv16Flash_bwd_params
    2808 bytes stack frame, 7984 bytes spill stores, 5972 bytes spill loads
ptxas info    : Used 255 registers, 696 bytes cmem[0]
ptxas info    : Compiling entry function '_Z44flash_bwd_dq_dk_dv_loop_seqk_parallel_kernelI23Flash_bwd_kernel_traitsILi512ELi32ELi64ELi4ELi2ELi2ELi2ELb0ELb1EN7cutlass6half_tE19Flash_kernel_traitsILi512ELi32ELi64ELi4ES2_EELb1ELb0ELb1ELb0EEv16Flash_bwd_params' for 'sm_80'
ptxas info    : Function properties for _Z44flash_bwd_dq_dk_dv_loop_seqk_parallel_kernelI23Flash_bwd_kernel_traitsILi512ELi32ELi64ELi4ELi2ELi2ELi2ELb0ELb1EN7cutlass6half_tE19Flash_kernel_traitsILi512ELi32ELi64ELi4ES2_EELb1ELb0ELb1ELb0EEv16Flash_bwd_params
    2704 bytes stack frame, 7788 bytes spill stores, 5792 bytes spill loads
ptxas info    : Used 255 registers, 696 bytes cmem[0]
ptxas info    : Compiling entry function '_Z44flash_bwd_dq_dk_dv_loop_seqk_parallel_kernelI23Flash_bwd_kernel_traitsILi512ELi32ELi64ELi4ELi2ELi2ELi2ELb0ELb1EN7cutlass6half_tE19Flash_kernel_traitsILi512ELi32ELi64ELi4ES2_EELb1ELb0ELb1ELb1EEv16Flash_bwd_params' for 'sm_80'
ptxas info    : Function properties for _Z44flash_bwd_dq_dk_dv_loop_seqk_parallel_kernelI23Flash_bwd_kernel_traitsILi512ELi32ELi64ELi4ELi2ELi2ELi2ELb0ELb1EN7cutlass6half_tE19Flash_kernel_traitsILi512ELi32ELi64ELi4ES2_EELb1ELb0ELb1ELb1EEv16Flash_bwd_params
    2800 bytes stack frame, 7940 bytes spill stores, 5912 bytes spill loads
ptxas info    : Used 255 registers, 696 bytes cmem[0]
ptxas info    : Compiling entry function '_Z44flash_bwd_dq_dk_dv_loop_seqk_parallel_kernelI23Flash_bwd_kernel_traitsILi512ELi32ELi64ELi4ELi2ELi2ELi2ELb0ELb1EN7cutlass6half_tE19Flash_kernel_traitsILi512ELi32ELi64ELi4ES2_EELb1ELb1ELb0ELb0EEv16Flash_bwd_params' for 'sm_80'
ptxas info    : Function properties for _Z44flash_bwd_dq_dk_dv_loop_seqk_parallel_kernelI23Flash_bwd_kernel_traitsILi512ELi32ELi64ELi4ELi2ELi2ELi2ELb0ELb1EN7cutlass6half_tE19Flash_kernel_traitsILi512ELi32ELi64ELi4ES2_EELb1ELb1ELb0ELb0EEv16Flash_bwd_params
    4336 bytes stack frame, 5840 bytes spill stores, 5884 bytes spill loads
ptxas info    : Used 255 registers, 696 bytes cmem[0]
ptxas info    : Function properties for _ZN5flash4gemmILb0ELb0EN4cute6TensorINS1_13array_alignedIfLm16ELm16EEENS1_6LayoutINS1_5tupleIJNS6_IJNS1_8constantIiLi2EEES8_EEENS7_IiLi1EEENS7_IiLi4EEEEEENS6_IJNS6_IJSA_S8_EEENS7_IiLi0EEESB_EEEEEEENS2_INS3_IN7cutlass6half_tELm256ELm16EEENS5_INS6_IJNS6_IJS8_S8_S8_EEESA_NS6_IJS9_NS7_IiLi8EEEEEEEEENS6_IJNS6_IJSA_S8_SB_EEESE_NS6_IJNS6_IJSM_NS7_IiLi16EEEEEENS7_IiLi32EEEEEEEEEEEEENS2_INS3_ISJ_Lm512ELm16EEENS5_INS6_IJS9_SB_SN_EEENS6_IJSD_SB_NS6_IJNS6_IJSQ_SS_EEENS7_IiLi64EEEEEEEEEEEEENS2_INS1_10ViewEngineINS1_8smem_ptrISJ_EEEENS5_INS6_IJNS6_IJSM_SA_EEESA_SN_EEENS6_IJNS6_IJSA_SE_EEESE_NS6_IJNS6_IJiiEEENS7_IiLi2048EEEEEEEEEEEEENS2_IS18_NS5_INS6_IJS19_S8_SN_EEENS6_IJS1B_NS7_IiLi1024EEENS6_IJS1C_NS7_IiLi4096EEEEEEEEEEEEENS1_8TiledMMAINS1_8MMA_AtomIJNS1_28SM80_16x8x16_F32F16F16F32_TNEEEENS5_INS6_IJS8_S8_SA_EEENS6_IJSA_S8_SE_EEEEENS5_INS6_IJSA_S8_SA_EEENS6_IJSE_SA_SE_EEEEENS6_IJNS1_10UnderscoreES1Z_S1Z_EEEEENS1_9TiledCopyINS1_9Copy_AtomIJNS1_17SM75_U32x4_LDSM_NESJ_EEENS5_INS6_IJNS6_IJSB_SM_S8_S8_EEENS6_IJSL_NS6_IJSA_SA_EEEEEEEEENS6_IJNS6_IJS10_SA_SQ_SE_EEENS6_IJNS6_IJSS_SM_NS7_IiLi256EEEEEENS6_IJSE_SE_EEEEEEEEEEENS6_IJSS_SQ_EEEEENS22_IS25_NS5_INS6_IJS26_NS6_IJS9_NS6_IJS8_SA_EEEEEEEEENS6_IJNS6_IJS10_SA_SE_SM_EEENS6_IJNS6_IJSS_S2B_EEENS6_IJSQ_SE_EEEEEEEEEEENS6_IJNS5_INS6_IJSM_S8_S8_EEENS6_IJSA_SS_SM_EEEEENS5_ISQ_SA_EEEEEEENS1_7ThrCopyIS2I_iEENS2Y_IS2X_iEEEEvRT1_RT2_RT3_RKT4_RKT5_T6_T7_T8_T9_T10_
    0 bytes stack frame, 0 bytes spill stores, 0 bytes spill loads
ptxas info    : Compiling entry function '_Z44flash_bwd_dq_dk_dv_loop_seqk_parallel_kernelI23Flash_bwd_kernel_traitsILi512ELi32ELi64ELi4ELi2ELi2ELi2ELb0ELb1EN7cutlass6half_tE19Flash_kernel_traitsILi512ELi32ELi64ELi4ES2_EELb1ELb1ELb0ELb1EEv16Flash_bwd_params' for 'sm_80'
ptxas info    : Function properties for _Z44flash_bwd_dq_dk_dv_loop_seqk_parallel_kernelI23Flash_bwd_kernel_traitsILi512ELi32ELi64ELi4ELi2ELi2ELi2ELb0ELb1EN7cutlass6half_tE19Flash_kernel_traitsILi512ELi32ELi64ELi4ES2_EELb1ELb1ELb0ELb1EEv16Flash_bwd_params
    2808 bytes stack frame, 5936 bytes spill stores, 5972 bytes spill loads
ptxas info    : Used 255 registers, 696 bytes cmem[0]
ptxas info    : Compiling entry function '_Z44flash_bwd_dq_dk_dv_loop_seqk_parallel_kernelI23Flash_bwd_kernel_traitsILi512ELi32ELi64ELi4ELi2ELi2ELi2ELb0ELb1EN7cutlass6half_tE19Flash_kernel_traitsILi512ELi32ELi64ELi4ES2_EELb1ELb1ELb1ELb0EEv16Flash_bwd_params' for 'sm_80'
ptxas info    : Function properties for _Z44flash_bwd_dq_dk_dv_loop_seqk_parallel_kernelI23Flash_bwd_kernel_traitsILi512ELi32ELi64ELi4ELi2ELi2ELi2ELb0ELb1EN7cutlass6half_tE19Flash_kernel_traitsILi512ELi32ELi64ELi4ES2_EELb1ELb1ELb1ELb0EEv16Flash_bwd_params
    2720 bytes stack frame, 5816 bytes spill stores, 5864 bytes spill loads
ptxas info    : Used 255 registers, 696 bytes cmem[0]
ptxas info    : Compiling entry function '_Z44flash_bwd_dq_dk_dv_loop_seqk_parallel_kernelI23Flash_bwd_kernel_traitsILi512ELi32ELi64ELi4ELi2ELi2ELi2ELb0ELb1EN7cutlass6half_tE19Flash_kernel_traitsILi512ELi32ELi64ELi4ES2_EELb1ELb1ELb1ELb1EEv16Flash_bwd_params' for 'sm_80'
ptxas info    : Function properties for _Z44flash_bwd_dq_dk_dv_loop_seqk_parallel_kernelI23Flash_bwd_kernel_traitsILi512ELi32ELi64ELi4ELi2ELi2ELi2ELb0ELb1EN7cutlass6half_tE19Flash_kernel_traitsILi512ELi32ELi64ELi4ES2_EELb1ELb1ELb1ELb1EEv16Flash_bwd_params
    2792 bytes stack frame, 5960 bytes spill stores, 5996 bytes spill loads
ptxas info    : Used 255 registers, 696 bytes cmem[0]
ptxas info    : Compiling entry function '_Z25flash_bwd_dot_do_o_kernelILb1E23Flash_bwd_kernel_traitsILi512ELi32ELi64ELi4ELi2ELi2ELi2ELb0ELb1EN7cutlass6half_tE19Flash_kernel_traitsILi512ELi32ELi64ELi4ES2_EEEv16Flash_bwd_params' for 'sm_80'
ptxas info    : Function properties for _Z25flash_bwd_dot_do_o_kernelILb1E23Flash_bwd_kernel_traitsILi512ELi32ELi64ELi4ELi2ELi2ELi2ELb0ELb1EN7cutlass6half_tE19Flash_kernel_traitsILi512ELi32ELi64ELi4ES2_EEEv16Flash_bwd_params
    0 bytes stack frame, 0 bytes spill stores, 0 bytes spill loads
ptxas info    : Used 168 registers, 696 bytes cmem[0]
ptxas info    : Compiling entry function '_Z27flash_bwd_convert_dq_kernelI23Flash_bwd_kernel_traitsILi512ELi32ELi64ELi4ELi2ELi2ELi2ELb0ELb0EN7cutlass6half_tE19Flash_kernel_traitsILi512ELi32ELi64ELi4ES2_EEEv16Flash_bwd_params' for 'sm_80'
ptxas info    : Function properties for _Z27flash_bwd_convert_dq_kernelI23Flash_bwd_kernel_traitsILi512ELi32ELi64ELi4ELi2ELi2ELi2ELb0ELb0EN7cutlass6half_tE19Flash_kernel_traitsILi512ELi32ELi64ELi4ES2_EEEv16Flash_bwd_params
    0 bytes stack frame, 0 bytes spill stores, 0 bytes spill loads
ptxas info    : Used 56 registers, 696 bytes cmem[0]
ptxas info    : Compiling entry function '_Z44flash_bwd_dq_dk_dv_loop_seqk_parallel_kernelI23Flash_bwd_kernel_traitsILi512ELi32ELi64ELi4ELi2ELi2ELi2ELb0ELb0EN7cutlass6half_tE19Flash_kernel_traitsILi512ELi32ELi64ELi4ES2_EELb1ELb0ELb0ELb0EEv16Flash_bwd_params' for 'sm_80'
ptxas info    : Function properties for _Z44flash_bwd_dq_dk_dv_loop_seqk_parallel_kernelI23Flash_bwd_kernel_traitsILi512ELi32ELi64ELi4ELi2ELi2ELi2ELb0ELb0EN7cutlass6half_tE19Flash_kernel_traitsILi512ELi32ELi64ELi4ES2_EELb1ELb0ELb0ELb0EEv16Flash_bwd_params
    2936 bytes stack frame, 0 bytes spill stores, 0 bytes spill loads
ptxas info    : Used 255 registers, 696 bytes cmem[0]
ptxas info    : Function properties for _ZN5flash26compute_dq_dk_dv_1colblockI23Flash_bwd_kernel_traitsILi512ELi32ELi64ELi4ELi2ELi2ELi2ELb0ELb0EN7cutlass6half_tE19Flash_kernel_traitsILi512ELi32ELi64ELi4ES3_EELb1ELb0ELb0ELb0ELb0ELb0ELb1E16Flash_bwd_paramsEEvRKT7_iii
    0 bytes stack frame, 7412 bytes spill stores, 5476 bytes spill loads
ptxas info    : Compiling entry function '_Z44flash_bwd_dq_dk_dv_loop_seqk_parallel_kernelI23Flash_bwd_kernel_traitsILi512ELi32ELi64ELi4ELi2ELi2ELi2ELb0ELb0EN7cutlass6half_tE19Flash_kernel_traitsILi512ELi32ELi64ELi4ES2_EELb1ELb0ELb0ELb1EEv16Flash_bwd_params' for 'sm_80'
ptxas info    : Function properties for _Z44flash_bwd_dq_dk_dv_loop_seqk_parallel_kernelI23Flash_bwd_kernel_traitsILi512ELi32ELi64ELi4ELi2ELi2ELi2ELb0ELb0EN7cutlass6half_tE19Flash_kernel_traitsILi512ELi32ELi64ELi4ES2_EELb1ELb0ELb0ELb1EEv16Flash_bwd_params
    2528 bytes stack frame, 7716 bytes spill stores, 5688 bytes spill loads
ptxas info    : Used 255 registers, 696 bytes cmem[0]
ptxas info    : Compiling entry function '_Z44flash_bwd_dq_dk_dv_loop_seqk_parallel_kernelI23Flash_bwd_kernel_traitsILi512ELi32ELi64ELi4ELi2ELi2ELi2ELb0ELb0EN7cutlass6half_tE19Flash_kernel_traitsILi512ELi32ELi64ELi4ES2_EELb1ELb0ELb1ELb0EEv16Flash_bwd_params' for 'sm_80'
ptxas info    : Function properties for _Z44flash_bwd_dq_dk_dv_loop_seqk_parallel_kernelI23Flash_bwd_kernel_traitsILi512ELi32ELi64ELi4ELi2ELi2ELi2ELb0ELb0EN7cutlass6half_tE19Flash_kernel_traitsILi512ELi32ELi64ELi4ES2_EELb1ELb0ELb1ELb0EEv16Flash_bwd_params
    2624 bytes stack frame, 7608 bytes spill stores, 5600 bytes spill loads
ptxas info    : Used 255 registers, 696 bytes cmem[0]
ptxas info    : Compiling entry function '_Z44flash_bwd_dq_dk_dv_loop_seqk_parallel_kernelI23Flash_bwd_kernel_traitsILi512ELi32ELi64ELi4ELi2ELi2ELi2ELb0ELb0EN7cutlass6half_tE19Flash_kernel_traitsILi512ELi32ELi64ELi4ES2_EELb1ELb0ELb1ELb1EEv16Flash_bwd_params' for 'sm_80'
ptxas info    : Function properties for _Z44flash_bwd_dq_dk_dv_loop_seqk_parallel_kernelI23Flash_bwd_kernel_traitsILi512ELi32ELi64ELi4ELi2ELi2ELi2ELb0ELb0EN7cutlass6half_tE19Flash_kernel_traitsILi512ELi32ELi64ELi4ES2_EELb1ELb0ELb1ELb1EEv16Flash_bwd_params
    2552 bytes stack frame, 7788 bytes spill stores, 5752 bytes spill loads
ptxas info    : Used 255 registers, 696 bytes cmem[0]
ptxas info    : Compiling entry function '_Z44flash_bwd_dq_dk_dv_loop_seqk_parallel_kernelI23Flash_bwd_kernel_traitsILi512ELi32ELi64ELi4ELi2ELi2ELi2ELb0ELb0EN7cutlass6half_tE19Flash_kernel_traitsILi512ELi32ELi64ELi4ES2_EELb1ELb1ELb0ELb0EEv16Flash_bwd_params' for 'sm_80'
ptxas info    : Function properties for _Z44flash_bwd_dq_dk_dv_loop_seqk_parallel_kernelI23Flash_bwd_kernel_traitsILi512ELi32ELi64ELi4ELi2ELi2ELi2ELb0ELb0EN7cutlass6half_tE19Flash_kernel_traitsILi512ELi32ELi64ELi4ES2_EELb1ELb1ELb0ELb0EEv16Flash_bwd_params
    4208 bytes stack frame, 5780 bytes spill stores, 5312 bytes spill loads
ptxas info    : Used 255 registers, 696 bytes cmem[0]
ptxas info    : Function properties for _ZN5flash4gemmILb0ELb0EN4cute6TensorINS1_13array_alignedIfLm16ELm16EEENS1_6LayoutINS1_5tupleIJNS6_IJNS1_8constantIiLi2EEES8_EEENS7_IiLi1EEENS7_IiLi4EEEEEENS6_IJNS6_IJSA_S8_EEENS7_IiLi0EEESB_EEEEEEENS2_INS3_IN7cutlass6half_tELm256ELm16EEENS5_INS6_IJNS6_IJS8_S8_S8_EEESA_NS6_IJS9_NS7_IiLi8EEEEEEEEENS6_IJNS6_IJSA_S8_SB_EEESE_NS6_IJNS6_IJSM_NS7_IiLi16EEEEEENS7_IiLi32EEEEEEEEEEEEENS2_INS3_ISJ_Lm512ELm16EEENS5_INS6_IJS9_SB_SN_EEENS6_IJSD_SB_NS6_IJNS6_IJSQ_SS_EEENS7_IiLi64EEEEEEEEEEEEENS2_INS1_10ViewEngineINS1_8smem_ptrISJ_EEEENS5_INS6_IJNS6_IJSM_SA_EEESA_SN_EEENS6_IJNS6_IJSA_SE_EEESE_NS6_IJNS6_IJiiEEENS7_IiLi2048EEEEEEEEEEEEENS2_IS18_NS5_INS6_IJS19_S8_SN_EEENS6_IJS1B_NS7_IiLi1024EEENS6_IJS1C_NS7_IiLi4096EEEEEEEEEEEEENS1_8TiledMMAINS1_8MMA_AtomIJNS1_28SM80_16x8x16_F32F16F16F32_TNEEEENS5_INS6_IJS8_S8_SA_EEENS6_IJSA_S8_SE_EEEEENS5_INS6_IJSA_S8_SA_EEENS6_IJSE_SA_SE_EEEEENS6_IJNS1_10UnderscoreES1Z_S1Z_EEEEENS1_9TiledCopyINS1_9Copy_AtomIJNS1_17SM75_U32x4_LDSM_NESJ_EEENS5_INS6_IJNS6_IJSB_SM_S8_S8_EEENS6_IJSL_NS6_IJSA_SA_EEEEEEEEENS6_IJNS6_IJS10_SA_SQ_SE_EEENS6_IJNS6_IJSS_SM_NS7_IiLi256EEEEEENS6_IJSE_SE_EEEEEEEEEEENS6_IJSS_SQ_EEEEENS22_IS25_NS5_INS6_IJS26_NS6_IJS9_NS6_IJS8_SA_EEEEEEEEENS6_IJNS6_IJS10_SA_SE_SM_EEENS6_IJNS6_IJSS_S2B_EEENS6_IJSQ_SE_EEEEEEEEEEENS6_IJNS5_INS6_IJSM_S8_S8_EEENS6_IJSA_SS_SM_EEEEENS5_ISQ_SA_EEEEEEENS1_7ThrCopyIS2I_iEENS2Y_IS2X_iEEEEvRT1_RT2_RT3_RKT4_RKT5_T6_T7_T8_T9_T10_
    0 bytes stack frame, 0 bytes spill stores, 0 bytes spill loads
ptxas info    : Compiling entry function '_Z44flash_bwd_dq_dk_dv_loop_seqk_parallel_kernelI23Flash_bwd_kernel_traitsILi512ELi32ELi64ELi4ELi2ELi2ELi2ELb0ELb0EN7cutlass6half_tE19Flash_kernel_traitsILi512ELi32ELi64ELi4ES2_EELb1ELb1ELb0ELb1EEv16Flash_bwd_params' for 'sm_80'
ptxas info    : Function properties for _Z44flash_bwd_dq_dk_dv_loop_seqk_parallel_kernelI23Flash_bwd_kernel_traitsILi512ELi32ELi64ELi4ELi2ELi2ELi2ELb0ELb0EN7cutlass6half_tE19Flash_kernel_traitsILi512ELi32ELi64ELi4ES2_EELb1ELb1ELb0ELb1EEv16Flash_bwd_params
    2536 bytes stack frame, 5672 bytes spill stores, 5204 bytes spill loads
ptxas info    : Used 255 registers, 696 bytes cmem[0]
ptxas info    : Compiling entry function '_Z44flash_bwd_dq_dk_dv_loop_seqk_parallel_kernelI23Flash_bwd_kernel_traitsILi512ELi32ELi64ELi4ELi2ELi2ELi2ELb0ELb0EN7cutlass6half_tE19Flash_kernel_traitsILi512ELi32ELi64ELi4ES2_EELb1ELb1ELb1ELb0EEv16Flash_bwd_params' for 'sm_80'
ptxas info    : Function properties for _Z44flash_bwd_dq_dk_dv_loop_seqk_parallel_kernelI23Flash_bwd_kernel_traitsILi512ELi32ELi64ELi4ELi2ELi2ELi2ELb0ELb0EN7cutlass6half_tE19Flash_kernel_traitsILi512ELi32ELi64ELi4ES2_EELb1ELb1ELb1ELb0EEv16Flash_bwd_params
    2728 bytes stack frame, 5668 bytes spill stores, 5108 bytes spill loads
ptxas info    : Used 255 registers, 696 bytes cmem[0]
ptxas info    : Compiling entry function '_Z44flash_bwd_dq_dk_dv_loop_seqk_parallel_kernelI23Flash_bwd_kernel_traitsILi512ELi32ELi64ELi4ELi2ELi2ELi2ELb0ELb0EN7cutlass6half_tE19Flash_kernel_traitsILi512ELi32ELi64ELi4ES2_EELb1ELb1ELb1ELb1EEv16Flash_bwd_params' for 'sm_80'
ptxas info    : Function properties for _Z44flash_bwd_dq_dk_dv_loop_seqk_parallel_kernelI23Flash_bwd_kernel_traitsILi512ELi32ELi64ELi4ELi2ELi2ELi2ELb0ELb0EN7cutlass6half_tE19Flash_kernel_traitsILi512ELi32ELi64ELi4ES2_EELb1ELb1ELb1ELb1EEv16Flash_bwd_params
    2520 bytes stack frame, 5744 bytes spill stores, 5244 bytes spill loads
ptxas info    : Used 255 registers, 696 bytes cmem[0]
ptxas info    : Compiling entry function '_Z25flash_bwd_dot_do_o_kernelILb1E23Flash_bwd_kernel_traitsILi512ELi32ELi64ELi4ELi2ELi2ELi2ELb0ELb0EN7cutlass6half_tE19Flash_kernel_traitsILi512ELi32ELi64ELi4ES2_EEEv16Flash_bwd_params' for 'sm_80'
ptxas info    : Function properties for _Z25flash_bwd_dot_do_o_kernelILb1E23Flash_bwd_kernel_traitsILi512ELi32ELi64ELi4ELi2ELi2ELi2ELb0ELb0EN7cutlass6half_tE19Flash_kernel_traitsILi512ELi32ELi64ELi4ES2_EEEv16Flash_bwd_params
    0 bytes stack frame, 0 bytes spill stores, 0 bytes spill loads
ptxas info    : Used 168 registers, 696 bytes cmem[0]
/anaconda/envs/flashattention/lib/python3.9/site-packages/torch/include/c10/core/SymInt.h(84): warning: integer conversion resulted in a change of sign

[31/37] /usr/local/cuda-11.4/bin/nvcc  -I/home/v-feiychen/flash-attention/csrc/flash_attn -I/home/v-feiychen/flash-attention/csrc/flash_attn/src -I/home/v-feiychen/flash-attention/csrc/cutlass/include -I/anaconda/envs/flashattention/lib/python3.9/site-packages/torch/include -I/anaconda/envs/flashattention/lib/python3.9/site-packages/torch/include/torch/csrc/api/include -I/anaconda/envs/flashattention/lib/python3.9/site-packages/torch/include/TH -I/anaconda/envs/flashattention/lib/python3.9/site-packages/torch/include/THC -I/usr/local/cuda-11.4/include -I/anaconda/envs/flashattention/include/python3.9 -c -c /home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_hdim96_bf16_sm80.cu -o /home/v-feiychen/flash-attention/build/temp.linux-x86_64-cpython-39/csrc/flash_attn/src/flash_fwd_hdim96_bf16_sm80.o -D__CUDA_NO_HALF_OPERATORS__ -D__CUDA_NO_HALF_CONVERSIONS__ -D__CUDA_NO_BFLOAT16_CONVERSIONS__ -D__CUDA_NO_HALF2_OPERATORS__ --expt-relaxed-constexpr --compiler-options ''"'"'-fPIC'"'"'' -O3 -std=c++17 -U__CUDA_NO_HALF_OPERATORS__ -U__CUDA_NO_HALF_CONVERSIONS__ -U__CUDA_NO_HALF2_OPERATORS__ -U__CUDA_NO_BFLOAT16_CONVERSIONS__ --expt-relaxed-constexpr --expt-extended-lambda --use_fast_math --ptxas-options=-v -lineinfo -gencode arch=compute_80,code=sm_80 --threads 4 -DTORCH_API_INCLUDE_EXTENSION_H '-DPYBIND11_COMPILER_TYPE="_gcc"' '-DPYBIND11_STDLIB="_libstdcpp"' '-DPYBIND11_BUILD_ABI="_cxxabi1011"' -DTORCH_EXTENSION_NAME=flash_attn_2_cuda -D_GLIBCXX_USE_CXX11_ABI=0
/anaconda/envs/flashattention/lib/python3.9/site-packages/torch/include/c10/core/SymInt.h(84): warning: integer conversion resulted in a change of sign

/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_kernel.h(140): warning: variable "MMA_M" was declared but never referenced
          detected during:
            instantiation of "void flash::compute_attn_1rowblock<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params,Is_Q_split_K>(const Params &, int, int, int) [with Kernel_traits=Flash_fwd_kernel_traits<96, 64, 64, 4, false, false, cutlass::bfloat16_t, Flash_kernel_traits<96, 64, 64, 4, cutlass::bfloat16_t>>, Is_dropout=true, Is_causal=true, Is_even_N=true, Is_even_K=true, Return_softmax=true, Params=Flash_fwd_params, Is_Q_split_K=false]" 
(589): here
            instantiation of "void flash::compute_attn<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &) [with Kernel_traits=Flash_fwd_kernel_traits<96, 64, 64, 4, false, false, cutlass::bfloat16_t, Flash_kernel_traits<96, 64, 64, 4, cutlass::bfloat16_t>>, Is_dropout=true, Is_causal=true, Is_even_N=true, Is_even_K=true, Return_softmax=true, Params=Flash_fwd_params]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(15): here
            instantiation of "void flash_fwd_kernel<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax>(Flash_fwd_params) [with Kernel_traits=Flash_fwd_kernel_traits<96, 64, 64, 4, false, false, cutlass::bfloat16_t, Flash_kernel_traits<96, 64, 64, 4, cutlass::bfloat16_t>>, Is_dropout=true, Is_causal=true, Is_even_N=true, Is_even_K=true, Return_softmax=true]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(34): here
            instantiation of "void run_flash_fwd<Kernel_traits,Is_dropout,Is_causal>(Flash_fwd_params &, cudaStream_t) [with Kernel_traits=Flash_fwd_kernel_traits<96, 64, 64, 4, false, false, cutlass::bfloat16_t, Flash_kernel_traits<96, 64, 64, 4, cutlass::bfloat16_t>>, Is_dropout=true, Is_causal=true]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(92): here
            instantiation of "void run_mha_fwd_hdim96<T>(Flash_fwd_params &, cudaStream_t) [with T=cutlass::bfloat16_t]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_hdim96_bf16_sm80.cu(16): here

/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_kernel.h(276): warning: variable "tQrQ" was declared but never referenced
          detected during:
            instantiation of "void flash::compute_attn_1rowblock<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params,Is_Q_split_K>(const Params &, int, int, int) [with Kernel_traits=Flash_fwd_kernel_traits<96, 64, 64, 4, false, false, cutlass::bfloat16_t, Flash_kernel_traits<96, 64, 64, 4, cutlass::bfloat16_t>>, Is_dropout=true, Is_causal=true, Is_even_N=true, Is_even_K=true, Return_softmax=true, Params=Flash_fwd_params, Is_Q_split_K=false]" 
(589): here
            instantiation of "void flash::compute_attn<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &) [with Kernel_traits=Flash_fwd_kernel_traits<96, 64, 64, 4, false, false, cutlass::bfloat16_t, Flash_kernel_traits<96, 64, 64, 4, cutlass::bfloat16_t>>, Is_dropout=true, Is_causal=true, Is_even_N=true, Is_even_K=true, Return_softmax=true, Params=Flash_fwd_params]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(15): here
            instantiation of "void flash_fwd_kernel<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax>(Flash_fwd_params) [with Kernel_traits=Flash_fwd_kernel_traits<96, 64, 64, 4, false, false, cutlass::bfloat16_t, Flash_kernel_traits<96, 64, 64, 4, cutlass::bfloat16_t>>, Is_dropout=true, Is_causal=true, Is_even_N=true, Is_even_K=true, Return_softmax=true]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(34): here
            instantiation of "void run_flash_fwd<Kernel_traits,Is_dropout,Is_causal>(Flash_fwd_params &, cudaStream_t) [with Kernel_traits=Flash_fwd_kernel_traits<96, 64, 64, 4, false, false, cutlass::bfloat16_t, Flash_kernel_traits<96, 64, 64, 4, cutlass::bfloat16_t>>, Is_dropout=true, Is_causal=true]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(92): here
            instantiation of "void run_mha_fwd_hdim96<T>(Flash_fwd_params &, cudaStream_t) [with T=cutlass::bfloat16_t]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_hdim96_bf16_sm80.cu(16): here

/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_kernel.h(140): warning: variable "MMA_M" was declared but never referenced
          detected during:
            instantiation of "void flash::compute_attn_1rowblock<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params,Is_Q_split_K>(const Params &, int, int, int) [with Kernel_traits=Flash_fwd_kernel_traits<96, 64, 64, 4, false, false, cutlass::bfloat16_t, Flash_kernel_traits<96, 64, 64, 4, cutlass::bfloat16_t>>, Is_dropout=true, Is_causal=true, Is_even_N=true, Is_even_K=true, Return_softmax=false, Params=Flash_fwd_params, Is_Q_split_K=false]" 
(589): here
            instantiation of "void flash::compute_attn<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &) [with Kernel_traits=Flash_fwd_kernel_traits<96, 64, 64, 4, false, false, cutlass::bfloat16_t, Flash_kernel_traits<96, 64, 64, 4, cutlass::bfloat16_t>>, Is_dropout=true, Is_causal=true, Is_even_N=true, Is_even_K=true, Return_softmax=false, Params=Flash_fwd_params]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(15): here
            instantiation of "void flash_fwd_kernel<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax>(Flash_fwd_params) [with Kernel_traits=Flash_fwd_kernel_traits<96, 64, 64, 4, false, false, cutlass::bfloat16_t, Flash_kernel_traits<96, 64, 64, 4, cutlass::bfloat16_t>>, Is_dropout=true, Is_causal=true, Is_even_N=true, Is_even_K=true, Return_softmax=false]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(34): here
            instantiation of "void run_flash_fwd<Kernel_traits,Is_dropout,Is_causal>(Flash_fwd_params &, cudaStream_t) [with Kernel_traits=Flash_fwd_kernel_traits<96, 64, 64, 4, false, false, cutlass::bfloat16_t, Flash_kernel_traits<96, 64, 64, 4, cutlass::bfloat16_t>>, Is_dropout=true, Is_causal=true]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(92): here
            instantiation of "void run_mha_fwd_hdim96<T>(Flash_fwd_params &, cudaStream_t) [with T=cutlass::bfloat16_t]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_hdim96_bf16_sm80.cu(16): here

/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_kernel.h(276): warning: variable "tQrQ" was declared but never referenced
          detected during:
            instantiation of "void flash::compute_attn_1rowblock<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params,Is_Q_split_K>(const Params &, int, int, int) [with Kernel_traits=Flash_fwd_kernel_traits<96, 64, 64, 4, false, false, cutlass::bfloat16_t, Flash_kernel_traits<96, 64, 64, 4, cutlass::bfloat16_t>>, Is_dropout=true, Is_causal=true, Is_even_N=true, Is_even_K=true, Return_softmax=false, Params=Flash_fwd_params, Is_Q_split_K=false]" 
(589): here
            instantiation of "void flash::compute_attn<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &) [with Kernel_traits=Flash_fwd_kernel_traits<96, 64, 64, 4, false, false, cutlass::bfloat16_t, Flash_kernel_traits<96, 64, 64, 4, cutlass::bfloat16_t>>, Is_dropout=true, Is_causal=true, Is_even_N=true, Is_even_K=true, Return_softmax=false, Params=Flash_fwd_params]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(15): here
            instantiation of "void flash_fwd_kernel<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax>(Flash_fwd_params) [with Kernel_traits=Flash_fwd_kernel_traits<96, 64, 64, 4, false, false, cutlass::bfloat16_t, Flash_kernel_traits<96, 64, 64, 4, cutlass::bfloat16_t>>, Is_dropout=true, Is_causal=true, Is_even_N=true, Is_even_K=true, Return_softmax=false]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(34): here
            instantiation of "void run_flash_fwd<Kernel_traits,Is_dropout,Is_causal>(Flash_fwd_params &, cudaStream_t) [with Kernel_traits=Flash_fwd_kernel_traits<96, 64, 64, 4, false, false, cutlass::bfloat16_t, Flash_kernel_traits<96, 64, 64, 4, cutlass::bfloat16_t>>, Is_dropout=true, Is_causal=true]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(92): here
            instantiation of "void run_mha_fwd_hdim96<T>(Flash_fwd_params &, cudaStream_t) [with T=cutlass::bfloat16_t]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_hdim96_bf16_sm80.cu(16): here

/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_kernel.h(140): warning: variable "MMA_M" was declared but never referenced
          detected during:
            instantiation of "void flash::compute_attn_1rowblock<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params,Is_Q_split_K>(const Params &, int, int, int) [with Kernel_traits=Flash_fwd_kernel_traits<96, 64, 64, 4, false, false, cutlass::bfloat16_t, Flash_kernel_traits<96, 64, 64, 4, cutlass::bfloat16_t>>, Is_dropout=true, Is_causal=true, Is_even_N=true, Is_even_K=false, Return_softmax=true, Params=Flash_fwd_params, Is_Q_split_K=false]" 
(589): here
            instantiation of "void flash::compute_attn<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &) [with Kernel_traits=Flash_fwd_kernel_traits<96, 64, 64, 4, false, false, cutlass::bfloat16_t, Flash_kernel_traits<96, 64, 64, 4, cutlass::bfloat16_t>>, Is_dropout=true, Is_causal=true, Is_even_N=true, Is_even_K=false, Return_softmax=true, Params=Flash_fwd_params]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(15): here
            instantiation of "void flash_fwd_kernel<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax>(Flash_fwd_params) [with Kernel_traits=Flash_fwd_kernel_traits<96, 64, 64, 4, false, false, cutlass::bfloat16_t, Flash_kernel_traits<96, 64, 64, 4, cutlass::bfloat16_t>>, Is_dropout=true, Is_causal=true, Is_even_N=true, Is_even_K=false, Return_softmax=true]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(34): here
            instantiation of "void run_flash_fwd<Kernel_traits,Is_dropout,Is_causal>(Flash_fwd_params &, cudaStream_t) [with Kernel_traits=Flash_fwd_kernel_traits<96, 64, 64, 4, false, false, cutlass::bfloat16_t, Flash_kernel_traits<96, 64, 64, 4, cutlass::bfloat16_t>>, Is_dropout=true, Is_causal=true]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(92): here
            instantiation of "void run_mha_fwd_hdim96<T>(Flash_fwd_params &, cudaStream_t) [with T=cutlass::bfloat16_t]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_hdim96_bf16_sm80.cu(16): here

/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_kernel.h(276): warning: variable "tQrQ" was declared but never referenced
          detected during:
            instantiation of "void flash::compute_attn_1rowblock<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params,Is_Q_split_K>(const Params &, int, int, int) [with Kernel_traits=Flash_fwd_kernel_traits<96, 64, 64, 4, false, false, cutlass::bfloat16_t, Flash_kernel_traits<96, 64, 64, 4, cutlass::bfloat16_t>>, Is_dropout=true, Is_causal=true, Is_even_N=true, Is_even_K=false, Return_softmax=true, Params=Flash_fwd_params, Is_Q_split_K=false]" 
(589): here
            instantiation of "void flash::compute_attn<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &) [with Kernel_traits=Flash_fwd_kernel_traits<96, 64, 64, 4, false, false, cutlass::bfloat16_t, Flash_kernel_traits<96, 64, 64, 4, cutlass::bfloat16_t>>, Is_dropout=true, Is_causal=true, Is_even_N=true, Is_even_K=false, Return_softmax=true, Params=Flash_fwd_params]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(15): here
            instantiation of "void flash_fwd_kernel<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax>(Flash_fwd_params) [with Kernel_traits=Flash_fwd_kernel_traits<96, 64, 64, 4, false, false, cutlass::bfloat16_t, Flash_kernel_traits<96, 64, 64, 4, cutlass::bfloat16_t>>, Is_dropout=true, Is_causal=true, Is_even_N=true, Is_even_K=false, Return_softmax=true]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(34): here
            instantiation of "void run_flash_fwd<Kernel_traits,Is_dropout,Is_causal>(Flash_fwd_params &, cudaStream_t) [with Kernel_traits=Flash_fwd_kernel_traits<96, 64, 64, 4, false, false, cutlass::bfloat16_t, Flash_kernel_traits<96, 64, 64, 4, cutlass::bfloat16_t>>, Is_dropout=true, Is_causal=true]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(92): here
            instantiation of "void run_mha_fwd_hdim96<T>(Flash_fwd_params &, cudaStream_t) [with T=cutlass::bfloat16_t]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_hdim96_bf16_sm80.cu(16): here

/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_kernel.h(140): warning: variable "MMA_M" was declared but never referenced
          detected during:
            instantiation of "void flash::compute_attn_1rowblock<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params,Is_Q_split_K>(const Params &, int, int, int) [with Kernel_traits=Flash_fwd_kernel_traits<96, 64, 64, 4, false, false, cutlass::bfloat16_t, Flash_kernel_traits<96, 64, 64, 4, cutlass::bfloat16_t>>, Is_dropout=true, Is_causal=true, Is_even_N=true, Is_even_K=false, Return_softmax=false, Params=Flash_fwd_params, Is_Q_split_K=false]" 
(589): here
            instantiation of "void flash::compute_attn<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &) [with Kernel_traits=Flash_fwd_kernel_traits<96, 64, 64, 4, false, false, cutlass::bfloat16_t, Flash_kernel_traits<96, 64, 64, 4, cutlass::bfloat16_t>>, Is_dropout=true, Is_causal=true, Is_even_N=true, Is_even_K=false, Return_softmax=false, Params=Flash_fwd_params]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(15): here
            instantiation of "void flash_fwd_kernel<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax>(Flash_fwd_params) [with Kernel_traits=Flash_fwd_kernel_traits<96, 64, 64, 4, false, false, cutlass::bfloat16_t, Flash_kernel_traits<96, 64, 64, 4, cutlass::bfloat16_t>>, Is_dropout=true, Is_causal=true, Is_even_N=true, Is_even_K=false, Return_softmax=false]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(34): here
            instantiation of "void run_flash_fwd<Kernel_traits,Is_dropout,Is_causal>(Flash_fwd_params &, cudaStream_t) [with Kernel_traits=Flash_fwd_kernel_traits<96, 64, 64, 4, false, false, cutlass::bfloat16_t, Flash_kernel_traits<96, 64, 64, 4, cutlass::bfloat16_t>>, Is_dropout=true, Is_causal=true]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(92): here
            instantiation of "void run_mha_fwd_hdim96<T>(Flash_fwd_params &, cudaStream_t) [with T=cutlass::bfloat16_t]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_hdim96_bf16_sm80.cu(16): here

/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_kernel.h(276): warning: variable "tQrQ" was declared but never referenced
          detected during:
            instantiation of "void flash::compute_attn_1rowblock<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params,Is_Q_split_K>(const Params &, int, int, int) [with Kernel_traits=Flash_fwd_kernel_traits<96, 64, 64, 4, false, false, cutlass::bfloat16_t, Flash_kernel_traits<96, 64, 64, 4, cutlass::bfloat16_t>>, Is_dropout=true, Is_causal=true, Is_even_N=true, Is_even_K=false, Return_softmax=false, Params=Flash_fwd_params, Is_Q_split_K=false]" 
(589): here
            instantiation of "void flash::compute_attn<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &) [with Kernel_traits=Flash_fwd_kernel_traits<96, 64, 64, 4, false, false, cutlass::bfloat16_t, Flash_kernel_traits<96, 64, 64, 4, cutlass::bfloat16_t>>, Is_dropout=true, Is_causal=true, Is_even_N=true, Is_even_K=false, Return_softmax=false, Params=Flash_fwd_params]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(15): here
            instantiation of "void flash_fwd_kernel<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax>(Flash_fwd_params) [with Kernel_traits=Flash_fwd_kernel_traits<96, 64, 64, 4, false, false, cutlass::bfloat16_t, Flash_kernel_traits<96, 64, 64, 4, cutlass::bfloat16_t>>, Is_dropout=true, Is_causal=true, Is_even_N=true, Is_even_K=false, Return_softmax=false]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(34): here
            instantiation of "void run_flash_fwd<Kernel_traits,Is_dropout,Is_causal>(Flash_fwd_params &, cudaStream_t) [with Kernel_traits=Flash_fwd_kernel_traits<96, 64, 64, 4, false, false, cutlass::bfloat16_t, Flash_kernel_traits<96, 64, 64, 4, cutlass::bfloat16_t>>, Is_dropout=true, Is_causal=true]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(92): here
            instantiation of "void run_mha_fwd_hdim96<T>(Flash_fwd_params &, cudaStream_t) [with T=cutlass::bfloat16_t]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_hdim96_bf16_sm80.cu(16): here

/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_kernel.h(140): warning: variable "MMA_M" was declared but never referenced
          detected during:
            instantiation of "void flash::compute_attn_1rowblock<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params,Is_Q_split_K>(const Params &, int, int, int) [with Kernel_traits=Flash_fwd_kernel_traits<96, 64, 64, 4, false, false, cutlass::bfloat16_t, Flash_kernel_traits<96, 64, 64, 4, cutlass::bfloat16_t>>, Is_dropout=true, Is_causal=true, Is_even_N=false, Is_even_K=true, Return_softmax=true, Params=Flash_fwd_params, Is_Q_split_K=false]" 
(589): here
            instantiation of "void flash::compute_attn<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &) [with Kernel_traits=Flash_fwd_kernel_traits<96, 64, 64, 4, false, false, cutlass::bfloat16_t, Flash_kernel_traits<96, 64, 64, 4, cutlass::bfloat16_t>>, Is_dropout=true, Is_causal=true, Is_even_N=false, Is_even_K=true, Return_softmax=true, Params=Flash_fwd_params]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(15): here
            instantiation of "void flash_fwd_kernel<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax>(Flash_fwd_params) [with Kernel_traits=Flash_fwd_kernel_traits<96, 64, 64, 4, false, false, cutlass::bfloat16_t, Flash_kernel_traits<96, 64, 64, 4, cutlass::bfloat16_t>>, Is_dropout=true, Is_causal=true, Is_even_N=false, Is_even_K=true, Return_softmax=true]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(34): here
            instantiation of "void run_flash_fwd<Kernel_traits,Is_dropout,Is_causal>(Flash_fwd_params &, cudaStream_t) [with Kernel_traits=Flash_fwd_kernel_traits<96, 64, 64, 4, false, false, cutlass::bfloat16_t, Flash_kernel_traits<96, 64, 64, 4, cutlass::bfloat16_t>>, Is_dropout=true, Is_causal=true]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(92): here
            instantiation of "void run_mha_fwd_hdim96<T>(Flash_fwd_params &, cudaStream_t) [with T=cutlass::bfloat16_t]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_hdim96_bf16_sm80.cu(16): here

/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_kernel.h(276): warning: variable "tQrQ" was declared but never referenced
          detected during:
            instantiation of "void flash::compute_attn_1rowblock<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params,Is_Q_split_K>(const Params &, int, int, int) [with Kernel_traits=Flash_fwd_kernel_traits<96, 64, 64, 4, false, false, cutlass::bfloat16_t, Flash_kernel_traits<96, 64, 64, 4, cutlass::bfloat16_t>>, Is_dropout=true, Is_causal=true, Is_even_N=false, Is_even_K=true, Return_softmax=true, Params=Flash_fwd_params, Is_Q_split_K=false]" 
(589): here
            instantiation of "void flash::compute_attn<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &) [with Kernel_traits=Flash_fwd_kernel_traits<96, 64, 64, 4, false, false, cutlass::bfloat16_t, Flash_kernel_traits<96, 64, 64, 4, cutlass::bfloat16_t>>, Is_dropout=true, Is_causal=true, Is_even_N=false, Is_even_K=true, Return_softmax=true, Params=Flash_fwd_params]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(15): here
            instantiation of "void flash_fwd_kernel<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax>(Flash_fwd_params) [with Kernel_traits=Flash_fwd_kernel_traits<96, 64, 64, 4, false, false, cutlass::bfloat16_t, Flash_kernel_traits<96, 64, 64, 4, cutlass::bfloat16_t>>, Is_dropout=true, Is_causal=true, Is_even_N=false, Is_even_K=true, Return_softmax=true]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(34): here
            instantiation of "void run_flash_fwd<Kernel_traits,Is_dropout,Is_causal>(Flash_fwd_params &, cudaStream_t) [with Kernel_traits=Flash_fwd_kernel_traits<96, 64, 64, 4, false, false, cutlass::bfloat16_t, Flash_kernel_traits<96, 64, 64, 4, cutlass::bfloat16_t>>, Is_dropout=true, Is_causal=true]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(92): here
            instantiation of "void run_mha_fwd_hdim96<T>(Flash_fwd_params &, cudaStream_t) [with T=cutlass::bfloat16_t]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_hdim96_bf16_sm80.cu(16): here

/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_kernel.h(140): warning: variable "MMA_M" was declared but never referenced
          detected during:
            instantiation of "void flash::compute_attn_1rowblock<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params,Is_Q_split_K>(const Params &, int, int, int) [with Kernel_traits=Flash_fwd_kernel_traits<96, 64, 64, 4, false, false, cutlass::bfloat16_t, Flash_kernel_traits<96, 64, 64, 4, cutlass::bfloat16_t>>, Is_dropout=true, Is_causal=true, Is_even_N=false, Is_even_K=true, Return_softmax=false, Params=Flash_fwd_params, Is_Q_split_K=false]" 
(589): here
            instantiation of "void flash::compute_attn<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &) [with Kernel_traits=Flash_fwd_kernel_traits<96, 64, 64, 4, false, false, cutlass::bfloat16_t, Flash_kernel_traits<96, 64, 64, 4, cutlass::bfloat16_t>>, Is_dropout=true, Is_causal=true, Is_even_N=false, Is_even_K=true, Return_softmax=false, Params=Flash_fwd_params]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(15): here
            instantiation of "void flash_fwd_kernel<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax>(Flash_fwd_params) [with Kernel_traits=Flash_fwd_kernel_traits<96, 64, 64, 4, false, false, cutlass::bfloat16_t, Flash_kernel_traits<96, 64, 64, 4, cutlass::bfloat16_t>>, Is_dropout=true, Is_causal=true, Is_even_N=false, Is_even_K=true, Return_softmax=false]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(34): here
            instantiation of "void run_flash_fwd<Kernel_traits,Is_dropout,Is_causal>(Flash_fwd_params &, cudaStream_t) [with Kernel_traits=Flash_fwd_kernel_traits<96, 64, 64, 4, false, false, cutlass::bfloat16_t, Flash_kernel_traits<96, 64, 64, 4, cutlass::bfloat16_t>>, Is_dropout=true, Is_causal=true]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(92): here
            instantiation of "void run_mha_fwd_hdim96<T>(Flash_fwd_params &, cudaStream_t) [with T=cutlass::bfloat16_t]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_hdim96_bf16_sm80.cu(16): here

/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_kernel.h(276): warning: variable "tQrQ" was declared but never referenced
          detected during:
            instantiation of "void flash::compute_attn_1rowblock<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params,Is_Q_split_K>(const Params &, int, int, int) [with Kernel_traits=Flash_fwd_kernel_traits<96, 64, 64, 4, false, false, cutlass::bfloat16_t, Flash_kernel_traits<96, 64, 64, 4, cutlass::bfloat16_t>>, Is_dropout=true, Is_causal=true, Is_even_N=false, Is_even_K=true, Return_softmax=false, Params=Flash_fwd_params, Is_Q_split_K=false]" 
(589): here
            instantiation of "void flash::compute_attn<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &) [with Kernel_traits=Flash_fwd_kernel_traits<96, 64, 64, 4, false, false, cutlass::bfloat16_t, Flash_kernel_traits<96, 64, 64, 4, cutlass::bfloat16_t>>, Is_dropout=true, Is_causal=true, Is_even_N=false, Is_even_K=true, Return_softmax=false, Params=Flash_fwd_params]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(15): here
            instantiation of "void flash_fwd_kernel<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax>(Flash_fwd_params) [with Kernel_traits=Flash_fwd_kernel_traits<96, 64, 64, 4, false, false, cutlass::bfloat16_t, Flash_kernel_traits<96, 64, 64, 4, cutlass::bfloat16_t>>, Is_dropout=true, Is_causal=true, Is_even_N=false, Is_even_K=true, Return_softmax=false]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(34): here
            instantiation of "void run_flash_fwd<Kernel_traits,Is_dropout,Is_causal>(Flash_fwd_params &, cudaStream_t) [with Kernel_traits=Flash_fwd_kernel_traits<96, 64, 64, 4, false, false, cutlass::bfloat16_t, Flash_kernel_traits<96, 64, 64, 4, cutlass::bfloat16_t>>, Is_dropout=true, Is_causal=true]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(92): here
            instantiation of "void run_mha_fwd_hdim96<T>(Flash_fwd_params &, cudaStream_t) [with T=cutlass::bfloat16_t]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_hdim96_bf16_sm80.cu(16): here

/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_kernel.h(140): warning: variable "MMA_M" was declared but never referenced
          detected during:
            instantiation of "void flash::compute_attn_1rowblock<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params,Is_Q_split_K>(const Params &, int, int, int) [with Kernel_traits=Flash_fwd_kernel_traits<96, 64, 64, 4, false, false, cutlass::bfloat16_t, Flash_kernel_traits<96, 64, 64, 4, cutlass::bfloat16_t>>, Is_dropout=true, Is_causal=true, Is_even_N=false, Is_even_K=false, Return_softmax=true, Params=Flash_fwd_params, Is_Q_split_K=false]" 
(589): here
            instantiation of "void flash::compute_attn<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &) [with Kernel_traits=Flash_fwd_kernel_traits<96, 64, 64, 4, false, false, cutlass::bfloat16_t, Flash_kernel_traits<96, 64, 64, 4, cutlass::bfloat16_t>>, Is_dropout=true, Is_causal=true, Is_even_N=false, Is_even_K=false, Return_softmax=true, Params=Flash_fwd_params]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(15): here
            instantiation of "void flash_fwd_kernel<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax>(Flash_fwd_params) [with Kernel_traits=Flash_fwd_kernel_traits<96, 64, 64, 4, false, false, cutlass::bfloat16_t, Flash_kernel_traits<96, 64, 64, 4, cutlass::bfloat16_t>>, Is_dropout=true, Is_causal=true, Is_even_N=false, Is_even_K=false, Return_softmax=true]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(34): here
            instantiation of "void run_flash_fwd<Kernel_traits,Is_dropout,Is_causal>(Flash_fwd_params &, cudaStream_t) [with Kernel_traits=Flash_fwd_kernel_traits<96, 64, 64, 4, false, false, cutlass::bfloat16_t, Flash_kernel_traits<96, 64, 64, 4, cutlass::bfloat16_t>>, Is_dropout=true, Is_causal=true]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(92): here
            instantiation of "void run_mha_fwd_hdim96<T>(Flash_fwd_params &, cudaStream_t) [with T=cutlass::bfloat16_t]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_hdim96_bf16_sm80.cu(16): here

/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_kernel.h(276): warning: variable "tQrQ" was declared but never referenced
          detected during:
            instantiation of "void flash::compute_attn_1rowblock<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params,Is_Q_split_K>(const Params &, int, int, int) [with Kernel_traits=Flash_fwd_kernel_traits<96, 64, 64, 4, false, false, cutlass::bfloat16_t, Flash_kernel_traits<96, 64, 64, 4, cutlass::bfloat16_t>>, Is_dropout=true, Is_causal=true, Is_even_N=false, Is_even_K=false, Return_softmax=true, Params=Flash_fwd_params, Is_Q_split_K=false]" 
(589): here
            instantiation of "void flash::compute_attn<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &) [with Kernel_traits=Flash_fwd_kernel_traits<96, 64, 64, 4, false, false, cutlass::bfloat16_t, Flash_kernel_traits<96, 64, 64, 4, cutlass::bfloat16_t>>, Is_dropout=true, Is_causal=true, Is_even_N=false, Is_even_K=false, Return_softmax=true, Params=Flash_fwd_params]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(15): here
            instantiation of "void flash_fwd_kernel<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax>(Flash_fwd_params) [with Kernel_traits=Flash_fwd_kernel_traits<96, 64, 64, 4, false, false, cutlass::bfloat16_t, Flash_kernel_traits<96, 64, 64, 4, cutlass::bfloat16_t>>, Is_dropout=true, Is_causal=true, Is_even_N=false, Is_even_K=false, Return_softmax=true]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(34): here
            instantiation of "void run_flash_fwd<Kernel_traits,Is_dropout,Is_causal>(Flash_fwd_params &, cudaStream_t) [with Kernel_traits=Flash_fwd_kernel_traits<96, 64, 64, 4, false, false, cutlass::bfloat16_t, Flash_kernel_traits<96, 64, 64, 4, cutlass::bfloat16_t>>, Is_dropout=true, Is_causal=true]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(92): here
            instantiation of "void run_mha_fwd_hdim96<T>(Flash_fwd_params &, cudaStream_t) [with T=cutlass::bfloat16_t]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_hdim96_bf16_sm80.cu(16): here

/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_kernel.h(140): warning: variable "MMA_M" was declared but never referenced
          detected during:
            instantiation of "void flash::compute_attn_1rowblock<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params,Is_Q_split_K>(const Params &, int, int, int) [with Kernel_traits=Flash_fwd_kernel_traits<96, 64, 64, 4, false, false, cutlass::bfloat16_t, Flash_kernel_traits<96, 64, 64, 4, cutlass::bfloat16_t>>, Is_dropout=true, Is_causal=true, Is_even_N=false, Is_even_K=false, Return_softmax=false, Params=Flash_fwd_params, Is_Q_split_K=false]" 
(589): here
            instantiation of "void flash::compute_attn<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &) [with Kernel_traits=Flash_fwd_kernel_traits<96, 64, 64, 4, false, false, cutlass::bfloat16_t, Flash_kernel_traits<96, 64, 64, 4, cutlass::bfloat16_t>>, Is_dropout=true, Is_causal=true, Is_even_N=false, Is_even_K=false, Return_softmax=false, Params=Flash_fwd_params]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(15): here
            instantiation of "void flash_fwd_kernel<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax>(Flash_fwd_params) [with Kernel_traits=Flash_fwd_kernel_traits<96, 64, 64, 4, false, false, cutlass::bfloat16_t, Flash_kernel_traits<96, 64, 64, 4, cutlass::bfloat16_t>>, Is_dropout=true, Is_causal=true, Is_even_N=false, Is_even_K=false, Return_softmax=false]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(34): here
            instantiation of "void run_flash_fwd<Kernel_traits,Is_dropout,Is_causal>(Flash_fwd_params &, cudaStream_t) [with Kernel_traits=Flash_fwd_kernel_traits<96, 64, 64, 4, false, false, cutlass::bfloat16_t, Flash_kernel_traits<96, 64, 64, 4, cutlass::bfloat16_t>>, Is_dropout=true, Is_causal=true]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(92): here
            instantiation of "void run_mha_fwd_hdim96<T>(Flash_fwd_params &, cudaStream_t) [with T=cutlass::bfloat16_t]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_hdim96_bf16_sm80.cu(16): here

/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_kernel.h(276): warning: variable "tQrQ" was declared but never referenced
          detected during:
            instantiation of "void flash::compute_attn_1rowblock<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params,Is_Q_split_K>(const Params &, int, int, int) [with Kernel_traits=Flash_fwd_kernel_traits<96, 64, 64, 4, false, false, cutlass::bfloat16_t, Flash_kernel_traits<96, 64, 64, 4, cutlass::bfloat16_t>>, Is_dropout=true, Is_causal=true, Is_even_N=false, Is_even_K=false, Return_softmax=false, Params=Flash_fwd_params, Is_Q_split_K=false]" 
(589): here
            instantiation of "void flash::compute_attn<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &) [with Kernel_traits=Flash_fwd_kernel_traits<96, 64, 64, 4, false, false, cutlass::bfloat16_t, Flash_kernel_traits<96, 64, 64, 4, cutlass::bfloat16_t>>, Is_dropout=true, Is_causal=true, Is_even_N=false, Is_even_K=false, Return_softmax=false, Params=Flash_fwd_params]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(15): here
            instantiation of "void flash_fwd_kernel<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax>(Flash_fwd_params) [with Kernel_traits=Flash_fwd_kernel_traits<96, 64, 64, 4, false, false, cutlass::bfloat16_t, Flash_kernel_traits<96, 64, 64, 4, cutlass::bfloat16_t>>, Is_dropout=true, Is_causal=true, Is_even_N=false, Is_even_K=false, Return_softmax=false]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(34): here
            instantiation of "void run_flash_fwd<Kernel_traits,Is_dropout,Is_causal>(Flash_fwd_params &, cudaStream_t) [with Kernel_traits=Flash_fwd_kernel_traits<96, 64, 64, 4, false, false, cutlass::bfloat16_t, Flash_kernel_traits<96, 64, 64, 4, cutlass::bfloat16_t>>, Is_dropout=true, Is_causal=true]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(92): here
            instantiation of "void run_mha_fwd_hdim96<T>(Flash_fwd_params &, cudaStream_t) [with T=cutlass::bfloat16_t]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_hdim96_bf16_sm80.cu(16): here

/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_kernel.h(140): warning: variable "MMA_M" was declared but never referenced
          detected during:
            instantiation of "void flash::compute_attn_1rowblock<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params,Is_Q_split_K>(const Params &, int, int, int) [with Kernel_traits=Flash_fwd_kernel_traits<96, 128, 64, 4, false, false, cutlass::bfloat16_t, Flash_kernel_traits<96, 128, 64, 4, cutlass::bfloat16_t>>, Is_dropout=true, Is_causal=true, Is_even_N=true, Is_even_K=true, Return_softmax=true, Params=Flash_fwd_params, Is_Q_split_K=false]" 
(589): here
            instantiation of "void flash::compute_attn<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &) [with Kernel_traits=Flash_fwd_kernel_traits<96, 128, 64, 4, false, false, cutlass::bfloat16_t, Flash_kernel_traits<96, 128, 64, 4, cutlass::bfloat16_t>>, Is_dropout=true, Is_causal=true, Is_even_N=true, Is_even_K=true, Return_softmax=true, Params=Flash_fwd_params]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(15): here
            instantiation of "void flash_fwd_kernel<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax>(Flash_fwd_params) [with Kernel_traits=Flash_fwd_kernel_traits<96, 128, 64, 4, false, false, cutlass::bfloat16_t, Flash_kernel_traits<96, 128, 64, 4, cutlass::bfloat16_t>>, Is_dropout=true, Is_causal=true, Is_even_N=true, Is_even_K=true, Return_softmax=true]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(34): here
            instantiation of "void run_flash_fwd<Kernel_traits,Is_dropout,Is_causal>(Flash_fwd_params &, cudaStream_t) [with Kernel_traits=Flash_fwd_kernel_traits<96, 128, 64, 4, false, false, cutlass::bfloat16_t, Flash_kernel_traits<96, 128, 64, 4, cutlass::bfloat16_t>>, Is_dropout=true, Is_causal=true]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(92): here
            instantiation of "void run_mha_fwd_hdim96<T>(Flash_fwd_params &, cudaStream_t) [with T=cutlass::bfloat16_t]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_hdim96_bf16_sm80.cu(16): here

/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_kernel.h(276): warning: variable "tQrQ" was declared but never referenced
          detected during:
            instantiation of "void flash::compute_attn_1rowblock<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params,Is_Q_split_K>(const Params &, int, int, int) [with Kernel_traits=Flash_fwd_kernel_traits<96, 128, 64, 4, false, false, cutlass::bfloat16_t, Flash_kernel_traits<96, 128, 64, 4, cutlass::bfloat16_t>>, Is_dropout=true, Is_causal=true, Is_even_N=true, Is_even_K=true, Return_softmax=true, Params=Flash_fwd_params, Is_Q_split_K=false]" 
(589): here
            instantiation of "void flash::compute_attn<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &) [with Kernel_traits=Flash_fwd_kernel_traits<96, 128, 64, 4, false, false, cutlass::bfloat16_t, Flash_kernel_traits<96, 128, 64, 4, cutlass::bfloat16_t>>, Is_dropout=true, Is_causal=true, Is_even_N=true, Is_even_K=true, Return_softmax=true, Params=Flash_fwd_params]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(15): here
            instantiation of "void flash_fwd_kernel<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax>(Flash_fwd_params) [with Kernel_traits=Flash_fwd_kernel_traits<96, 128, 64, 4, false, false, cutlass::bfloat16_t, Flash_kernel_traits<96, 128, 64, 4, cutlass::bfloat16_t>>, Is_dropout=true, Is_causal=true, Is_even_N=true, Is_even_K=true, Return_softmax=true]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(34): here
            instantiation of "void run_flash_fwd<Kernel_traits,Is_dropout,Is_causal>(Flash_fwd_params &, cudaStream_t) [with Kernel_traits=Flash_fwd_kernel_traits<96, 128, 64, 4, false, false, cutlass::bfloat16_t, Flash_kernel_traits<96, 128, 64, 4, cutlass::bfloat16_t>>, Is_dropout=true, Is_causal=true]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(92): here
            instantiation of "void run_mha_fwd_hdim96<T>(Flash_fwd_params &, cudaStream_t) [with T=cutlass::bfloat16_t]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_hdim96_bf16_sm80.cu(16): here

/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_kernel.h(140): warning: variable "MMA_M" was declared but never referenced
          detected during:
            instantiation of "void flash::compute_attn_1rowblock<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params,Is_Q_split_K>(const Params &, int, int, int) [with Kernel_traits=Flash_fwd_kernel_traits<96, 128, 64, 4, false, false, cutlass::bfloat16_t, Flash_kernel_traits<96, 128, 64, 4, cutlass::bfloat16_t>>, Is_dropout=true, Is_causal=true, Is_even_N=true, Is_even_K=true, Return_softmax=false, Params=Flash_fwd_params, Is_Q_split_K=false]" 
(589): here
            instantiation of "void flash::compute_attn<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &) [with Kernel_traits=Flash_fwd_kernel_traits<96, 128, 64, 4, false, false, cutlass::bfloat16_t, Flash_kernel_traits<96, 128, 64, 4, cutlass::bfloat16_t>>, Is_dropout=true, Is_causal=true, Is_even_N=true, Is_even_K=true, Return_softmax=false, Params=Flash_fwd_params]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(15): here
            instantiation of "void flash_fwd_kernel<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax>(Flash_fwd_params) [with Kernel_traits=Flash_fwd_kernel_traits<96, 128, 64, 4, false, false, cutlass::bfloat16_t, Flash_kernel_traits<96, 128, 64, 4, cutlass::bfloat16_t>>, Is_dropout=true, Is_causal=true, Is_even_N=true, Is_even_K=true, Return_softmax=false]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(34): here
            instantiation of "void run_flash_fwd<Kernel_traits,Is_dropout,Is_causal>(Flash_fwd_params &, cudaStream_t) [with Kernel_traits=Flash_fwd_kernel_traits<96, 128, 64, 4, false, false, cutlass::bfloat16_t, Flash_kernel_traits<96, 128, 64, 4, cutlass::bfloat16_t>>, Is_dropout=true, Is_causal=true]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(92): here
            instantiation of "void run_mha_fwd_hdim96<T>(Flash_fwd_params &, cudaStream_t) [with T=cutlass::bfloat16_t]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_hdim96_bf16_sm80.cu(16): here

/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_kernel.h(276): warning: variable "tQrQ" was declared but never referenced
          detected during:
            instantiation of "void flash::compute_attn_1rowblock<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params,Is_Q_split_K>(const Params &, int, int, int) [with Kernel_traits=Flash_fwd_kernel_traits<96, 128, 64, 4, false, false, cutlass::bfloat16_t, Flash_kernel_traits<96, 128, 64, 4, cutlass::bfloat16_t>>, Is_dropout=true, Is_causal=true, Is_even_N=true, Is_even_K=true, Return_softmax=false, Params=Flash_fwd_params, Is_Q_split_K=false]" 
(589): here
            instantiation of "void flash::compute_attn<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &) [with Kernel_traits=Flash_fwd_kernel_traits<96, 128, 64, 4, false, false, cutlass::bfloat16_t, Flash_kernel_traits<96, 128, 64, 4, cutlass::bfloat16_t>>, Is_dropout=true, Is_causal=true, Is_even_N=true, Is_even_K=true, Return_softmax=false, Params=Flash_fwd_params]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(15): here
            instantiation of "void flash_fwd_kernel<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax>(Flash_fwd_params) [with Kernel_traits=Flash_fwd_kernel_traits<96, 128, 64, 4, false, false, cutlass::bfloat16_t, Flash_kernel_traits<96, 128, 64, 4, cutlass::bfloat16_t>>, Is_dropout=true, Is_causal=true, Is_even_N=true, Is_even_K=true, Return_softmax=false]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(34): here
            instantiation of "void run_flash_fwd<Kernel_traits,Is_dropout,Is_causal>(Flash_fwd_params &, cudaStream_t) [with Kernel_traits=Flash_fwd_kernel_traits<96, 128, 64, 4, false, false, cutlass::bfloat16_t, Flash_kernel_traits<96, 128, 64, 4, cutlass::bfloat16_t>>, Is_dropout=true, Is_causal=true]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(92): here
            instantiation of "void run_mha_fwd_hdim96<T>(Flash_fwd_params &, cudaStream_t) [with T=cutlass::bfloat16_t]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_hdim96_bf16_sm80.cu(16): here

/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_kernel.h(140): warning: variable "MMA_M" was declared but never referenced
          detected during:
            instantiation of "void flash::compute_attn_1rowblock<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params,Is_Q_split_K>(const Params &, int, int, int) [with Kernel_traits=Flash_fwd_kernel_traits<96, 128, 64, 4, false, false, cutlass::bfloat16_t, Flash_kernel_traits<96, 128, 64, 4, cutlass::bfloat16_t>>, Is_dropout=true, Is_causal=true, Is_even_N=true, Is_even_K=false, Return_softmax=true, Params=Flash_fwd_params, Is_Q_split_K=false]" 
(589): here
            instantiation of "void flash::compute_attn<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &) [with Kernel_traits=Flash_fwd_kernel_traits<96, 128, 64, 4, false, false, cutlass::bfloat16_t, Flash_kernel_traits<96, 128, 64, 4, cutlass::bfloat16_t>>, Is_dropout=true, Is_causal=true, Is_even_N=true, Is_even_K=false, Return_softmax=true, Params=Flash_fwd_params]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(15): here
            instantiation of "void flash_fwd_kernel<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax>(Flash_fwd_params) [with Kernel_traits=Flash_fwd_kernel_traits<96, 128, 64, 4, false, false, cutlass::bfloat16_t, Flash_kernel_traits<96, 128, 64, 4, cutlass::bfloat16_t>>, Is_dropout=true, Is_causal=true, Is_even_N=true, Is_even_K=false, Return_softmax=true]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(34): here
            instantiation of "void run_flash_fwd<Kernel_traits,Is_dropout,Is_causal>(Flash_fwd_params &, cudaStream_t) [with Kernel_traits=Flash_fwd_kernel_traits<96, 128, 64, 4, false, false, cutlass::bfloat16_t, Flash_kernel_traits<96, 128, 64, 4, cutlass::bfloat16_t>>, Is_dropout=true, Is_causal=true]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(92): here
            instantiation of "void run_mha_fwd_hdim96<T>(Flash_fwd_params &, cudaStream_t) [with T=cutlass::bfloat16_t]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_hdim96_bf16_sm80.cu(16): here

/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_kernel.h(276): warning: variable "tQrQ" was declared but never referenced
          detected during:
            instantiation of "void flash::compute_attn_1rowblock<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params,Is_Q_split_K>(const Params &, int, int, int) [with Kernel_traits=Flash_fwd_kernel_traits<96, 128, 64, 4, false, false, cutlass::bfloat16_t, Flash_kernel_traits<96, 128, 64, 4, cutlass::bfloat16_t>>, Is_dropout=true, Is_causal=true, Is_even_N=true, Is_even_K=false, Return_softmax=true, Params=Flash_fwd_params, Is_Q_split_K=false]" 
(589): here
            instantiation of "void flash::compute_attn<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &) [with Kernel_traits=Flash_fwd_kernel_traits<96, 128, 64, 4, false, false, cutlass::bfloat16_t, Flash_kernel_traits<96, 128, 64, 4, cutlass::bfloat16_t>>, Is_dropout=true, Is_causal=true, Is_even_N=true, Is_even_K=false, Return_softmax=true, Params=Flash_fwd_params]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(15): here
            instantiation of "void flash_fwd_kernel<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax>(Flash_fwd_params) [with Kernel_traits=Flash_fwd_kernel_traits<96, 128, 64, 4, false, false, cutlass::bfloat16_t, Flash_kernel_traits<96, 128, 64, 4, cutlass::bfloat16_t>>, Is_dropout=true, Is_causal=true, Is_even_N=true, Is_even_K=false, Return_softmax=true]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(34): here
            instantiation of "void run_flash_fwd<Kernel_traits,Is_dropout,Is_causal>(Flash_fwd_params &, cudaStream_t) [with Kernel_traits=Flash_fwd_kernel_traits<96, 128, 64, 4, false, false, cutlass::bfloat16_t, Flash_kernel_traits<96, 128, 64, 4, cutlass::bfloat16_t>>, Is_dropout=true, Is_causal=true]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(92): here
            instantiation of "void run_mha_fwd_hdim96<T>(Flash_fwd_params &, cudaStream_t) [with T=cutlass::bfloat16_t]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_hdim96_bf16_sm80.cu(16): here

/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_kernel.h(140): warning: variable "MMA_M" was declared but never referenced
          detected during:
            instantiation of "void flash::compute_attn_1rowblock<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params,Is_Q_split_K>(const Params &, int, int, int) [with Kernel_traits=Flash_fwd_kernel_traits<96, 128, 64, 4, false, false, cutlass::bfloat16_t, Flash_kernel_traits<96, 128, 64, 4, cutlass::bfloat16_t>>, Is_dropout=true, Is_causal=true, Is_even_N=true, Is_even_K=false, Return_softmax=false, Params=Flash_fwd_params, Is_Q_split_K=false]" 
(589): here
            instantiation of "void flash::compute_attn<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &) [with Kernel_traits=Flash_fwd_kernel_traits<96, 128, 64, 4, false, false, cutlass::bfloat16_t, Flash_kernel_traits<96, 128, 64, 4, cutlass::bfloat16_t>>, Is_dropout=true, Is_causal=true, Is_even_N=true, Is_even_K=false, Return_softmax=false, Params=Flash_fwd_params]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(15): here
            instantiation of "void flash_fwd_kernel<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax>(Flash_fwd_params) [with Kernel_traits=Flash_fwd_kernel_traits<96, 128, 64, 4, false, false, cutlass::bfloat16_t, Flash_kernel_traits<96, 128, 64, 4, cutlass::bfloat16_t>>, Is_dropout=true, Is_causal=true, Is_even_N=true, Is_even_K=false, Return_softmax=false]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(34): here
            instantiation of "void run_flash_fwd<Kernel_traits,Is_dropout,Is_causal>(Flash_fwd_params &, cudaStream_t) [with Kernel_traits=Flash_fwd_kernel_traits<96, 128, 64, 4, false, false, cutlass::bfloat16_t, Flash_kernel_traits<96, 128, 64, 4, cutlass::bfloat16_t>>, Is_dropout=true, Is_causal=true]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(92): here
            instantiation of "void run_mha_fwd_hdim96<T>(Flash_fwd_params &, cudaStream_t) [with T=cutlass::bfloat16_t]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_hdim96_bf16_sm80.cu(16): here

/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_kernel.h(276): warning: variable "tQrQ" was declared but never referenced
          detected during:
            instantiation of "void flash::compute_attn_1rowblock<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params,Is_Q_split_K>(const Params &, int, int, int) [with Kernel_traits=Flash_fwd_kernel_traits<96, 128, 64, 4, false, false, cutlass::bfloat16_t, Flash_kernel_traits<96, 128, 64, 4, cutlass::bfloat16_t>>, Is_dropout=true, Is_causal=true, Is_even_N=true, Is_even_K=false, Return_softmax=false, Params=Flash_fwd_params, Is_Q_split_K=false]" 
(589): here
            instantiation of "void flash::compute_attn<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &) [with Kernel_traits=Flash_fwd_kernel_traits<96, 128, 64, 4, false, false, cutlass::bfloat16_t, Flash_kernel_traits<96, 128, 64, 4, cutlass::bfloat16_t>>, Is_dropout=true, Is_causal=true, Is_even_N=true, Is_even_K=false, Return_softmax=false, Params=Flash_fwd_params]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(15): here
            instantiation of "void flash_fwd_kernel<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax>(Flash_fwd_params) [with Kernel_traits=Flash_fwd_kernel_traits<96, 128, 64, 4, false, false, cutlass::bfloat16_t, Flash_kernel_traits<96, 128, 64, 4, cutlass::bfloat16_t>>, Is_dropout=true, Is_causal=true, Is_even_N=true, Is_even_K=false, Return_softmax=false]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(34): here
            instantiation of "void run_flash_fwd<Kernel_traits,Is_dropout,Is_causal>(Flash_fwd_params &, cudaStream_t) [with Kernel_traits=Flash_fwd_kernel_traits<96, 128, 64, 4, false, false, cutlass::bfloat16_t, Flash_kernel_traits<96, 128, 64, 4, cutlass::bfloat16_t>>, Is_dropout=true, Is_causal=true]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(92): here
            instantiation of "void run_mha_fwd_hdim96<T>(Flash_fwd_params &, cudaStream_t) [with T=cutlass::bfloat16_t]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_hdim96_bf16_sm80.cu(16): here

/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_kernel.h(140): warning: variable "MMA_M" was declared but never referenced
          detected during:
            instantiation of "void flash::compute_attn_1rowblock<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params,Is_Q_split_K>(const Params &, int, int, int) [with Kernel_traits=Flash_fwd_kernel_traits<96, 128, 64, 4, false, false, cutlass::bfloat16_t, Flash_kernel_traits<96, 128, 64, 4, cutlass::bfloat16_t>>, Is_dropout=true, Is_causal=true, Is_even_N=false, Is_even_K=true, Return_softmax=true, Params=Flash_fwd_params, Is_Q_split_K=false]" 
(589): here
            instantiation of "void flash::compute_attn<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &) [with Kernel_traits=Flash_fwd_kernel_traits<96, 128, 64, 4, false, false, cutlass::bfloat16_t, Flash_kernel_traits<96, 128, 64, 4, cutlass::bfloat16_t>>, Is_dropout=true, Is_causal=true, Is_even_N=false, Is_even_K=true, Return_softmax=true, Params=Flash_fwd_params]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(15): here
            instantiation of "void flash_fwd_kernel<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax>(Flash_fwd_params) [with Kernel_traits=Flash_fwd_kernel_traits<96, 128, 64, 4, false, false, cutlass::bfloat16_t, Flash_kernel_traits<96, 128, 64, 4, cutlass::bfloat16_t>>, Is_dropout=true, Is_causal=true, Is_even_N=false, Is_even_K=true, Return_softmax=true]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(34): here
            instantiation of "void run_flash_fwd<Kernel_traits,Is_dropout,Is_causal>(Flash_fwd_params &, cudaStream_t) [with Kernel_traits=Flash_fwd_kernel_traits<96, 128, 64, 4, false, false, cutlass::bfloat16_t, Flash_kernel_traits<96, 128, 64, 4, cutlass::bfloat16_t>>, Is_dropout=true, Is_causal=true]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(92): here
            instantiation of "void run_mha_fwd_hdim96<T>(Flash_fwd_params &, cudaStream_t) [with T=cutlass::bfloat16_t]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_hdim96_bf16_sm80.cu(16): here

/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_kernel.h(276): warning: variable "tQrQ" was declared but never referenced
          detected during:
            instantiation of "void flash::compute_attn_1rowblock<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params,Is_Q_split_K>(const Params &, int, int, int) [with Kernel_traits=Flash_fwd_kernel_traits<96, 128, 64, 4, false, false, cutlass::bfloat16_t, Flash_kernel_traits<96, 128, 64, 4, cutlass::bfloat16_t>>, Is_dropout=true, Is_causal=true, Is_even_N=false, Is_even_K=true, Return_softmax=true, Params=Flash_fwd_params, Is_Q_split_K=false]" 
(589): here
            instantiation of "void flash::compute_attn<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &) [with Kernel_traits=Flash_fwd_kernel_traits<96, 128, 64, 4, false, false, cutlass::bfloat16_t, Flash_kernel_traits<96, 128, 64, 4, cutlass::bfloat16_t>>, Is_dropout=true, Is_causal=true, Is_even_N=false, Is_even_K=true, Return_softmax=true, Params=Flash_fwd_params]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(15): here
            instantiation of "void flash_fwd_kernel<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax>(Flash_fwd_params) [with Kernel_traits=Flash_fwd_kernel_traits<96, 128, 64, 4, false, false, cutlass::bfloat16_t, Flash_kernel_traits<96, 128, 64, 4, cutlass::bfloat16_t>>, Is_dropout=true, Is_causal=true, Is_even_N=false, Is_even_K=true, Return_softmax=true]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(34): here
            instantiation of "void run_flash_fwd<Kernel_traits,Is_dropout,Is_causal>(Flash_fwd_params &, cudaStream_t) [with Kernel_traits=Flash_fwd_kernel_traits<96, 128, 64, 4, false, false, cutlass::bfloat16_t, Flash_kernel_traits<96, 128, 64, 4, cutlass::bfloat16_t>>, Is_dropout=true, Is_causal=true]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(92): here
            instantiation of "void run_mha_fwd_hdim96<T>(Flash_fwd_params &, cudaStream_t) [with T=cutlass::bfloat16_t]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_hdim96_bf16_sm80.cu(16): here

/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_kernel.h(140): warning: variable "MMA_M" was declared but never referenced
          detected during:
            instantiation of "void flash::compute_attn_1rowblock<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params,Is_Q_split_K>(const Params &, int, int, int) [with Kernel_traits=Flash_fwd_kernel_traits<96, 128, 64, 4, false, false, cutlass::bfloat16_t, Flash_kernel_traits<96, 128, 64, 4, cutlass::bfloat16_t>>, Is_dropout=true, Is_causal=true, Is_even_N=false, Is_even_K=true, Return_softmax=false, Params=Flash_fwd_params, Is_Q_split_K=false]" 
(589): here
            instantiation of "void flash::compute_attn<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &) [with Kernel_traits=Flash_fwd_kernel_traits<96, 128, 64, 4, false, false, cutlass::bfloat16_t, Flash_kernel_traits<96, 128, 64, 4, cutlass::bfloat16_t>>, Is_dropout=true, Is_causal=true, Is_even_N=false, Is_even_K=true, Return_softmax=false, Params=Flash_fwd_params]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(15): here
            instantiation of "void flash_fwd_kernel<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax>(Flash_fwd_params) [with Kernel_traits=Flash_fwd_kernel_traits<96, 128, 64, 4, false, false, cutlass::bfloat16_t, Flash_kernel_traits<96, 128, 64, 4, cutlass::bfloat16_t>>, Is_dropout=true, Is_causal=true, Is_even_N=false, Is_even_K=true, Return_softmax=false]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(34): here
            instantiation of "void run_flash_fwd<Kernel_traits,Is_dropout,Is_causal>(Flash_fwd_params &, cudaStream_t) [with Kernel_traits=Flash_fwd_kernel_traits<96, 128, 64, 4, false, false, cutlass::bfloat16_t, Flash_kernel_traits<96, 128, 64, 4, cutlass::bfloat16_t>>, Is_dropout=true, Is_causal=true]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(92): here
            instantiation of "void run_mha_fwd_hdim96<T>(Flash_fwd_params &, cudaStream_t) [with T=cutlass::bfloat16_t]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_hdim96_bf16_sm80.cu(16): here

/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_kernel.h(276): warning: variable "tQrQ" was declared but never referenced
          detected during:
            instantiation of "void flash::compute_attn_1rowblock<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params,Is_Q_split_K>(const Params &, int, int, int) [with Kernel_traits=Flash_fwd_kernel_traits<96, 128, 64, 4, false, false, cutlass::bfloat16_t, Flash_kernel_traits<96, 128, 64, 4, cutlass::bfloat16_t>>, Is_dropout=true, Is_causal=true, Is_even_N=false, Is_even_K=true, Return_softmax=false, Params=Flash_fwd_params, Is_Q_split_K=false]" 
(589): here
            instantiation of "void flash::compute_attn<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &) [with Kernel_traits=Flash_fwd_kernel_traits<96, 128, 64, 4, false, false, cutlass::bfloat16_t, Flash_kernel_traits<96, 128, 64, 4, cutlass::bfloat16_t>>, Is_dropout=true, Is_causal=true, Is_even_N=false, Is_even_K=true, Return_softmax=false, Params=Flash_fwd_params]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(15): here
            instantiation of "void flash_fwd_kernel<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax>(Flash_fwd_params) [with Kernel_traits=Flash_fwd_kernel_traits<96, 128, 64, 4, false, false, cutlass::bfloat16_t, Flash_kernel_traits<96, 128, 64, 4, cutlass::bfloat16_t>>, Is_dropout=true, Is_causal=true, Is_even_N=false, Is_even_K=true, Return_softmax=false]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(34): here
            instantiation of "void run_flash_fwd<Kernel_traits,Is_dropout,Is_causal>(Flash_fwd_params &, cudaStream_t) [with Kernel_traits=Flash_fwd_kernel_traits<96, 128, 64, 4, false, false, cutlass::bfloat16_t, Flash_kernel_traits<96, 128, 64, 4, cutlass::bfloat16_t>>, Is_dropout=true, Is_causal=true]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(92): here
            instantiation of "void run_mha_fwd_hdim96<T>(Flash_fwd_params &, cudaStream_t) [with T=cutlass::bfloat16_t]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_hdim96_bf16_sm80.cu(16): here

/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_kernel.h(140): warning: variable "MMA_M" was declared but never referenced
          detected during:
            instantiation of "void flash::compute_attn_1rowblock<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params,Is_Q_split_K>(const Params &, int, int, int) [with Kernel_traits=Flash_fwd_kernel_traits<96, 128, 64, 4, false, false, cutlass::bfloat16_t, Flash_kernel_traits<96, 128, 64, 4, cutlass::bfloat16_t>>, Is_dropout=true, Is_causal=true, Is_even_N=false, Is_even_K=false, Return_softmax=true, Params=Flash_fwd_params, Is_Q_split_K=false]" 
(589): here
            instantiation of "void flash::compute_attn<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &) [with Kernel_traits=Flash_fwd_kernel_traits<96, 128, 64, 4, false, false, cutlass::bfloat16_t, Flash_kernel_traits<96, 128, 64, 4, cutlass::bfloat16_t>>, Is_dropout=true, Is_causal=true, Is_even_N=false, Is_even_K=false, Return_softmax=true, Params=Flash_fwd_params]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(15): here
            instantiation of "void flash_fwd_kernel<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax>(Flash_fwd_params) [with Kernel_traits=Flash_fwd_kernel_traits<96, 128, 64, 4, false, false, cutlass::bfloat16_t, Flash_kernel_traits<96, 128, 64, 4, cutlass::bfloat16_t>>, Is_dropout=true, Is_causal=true, Is_even_N=false, Is_even_K=false, Return_softmax=true]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(34): here
            instantiation of "void run_flash_fwd<Kernel_traits,Is_dropout,Is_causal>(Flash_fwd_params &, cudaStream_t) [with Kernel_traits=Flash_fwd_kernel_traits<96, 128, 64, 4, false, false, cutlass::bfloat16_t, Flash_kernel_traits<96, 128, 64, 4, cutlass::bfloat16_t>>, Is_dropout=true, Is_causal=true]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(92): here
            instantiation of "void run_mha_fwd_hdim96<T>(Flash_fwd_params &, cudaStream_t) [with T=cutlass::bfloat16_t]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_hdim96_bf16_sm80.cu(16): here

/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_kernel.h(276): warning: variable "tQrQ" was declared but never referenced
          detected during:
            instantiation of "void flash::compute_attn_1rowblock<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params,Is_Q_split_K>(const Params &, int, int, int) [with Kernel_traits=Flash_fwd_kernel_traits<96, 128, 64, 4, false, false, cutlass::bfloat16_t, Flash_kernel_traits<96, 128, 64, 4, cutlass::bfloat16_t>>, Is_dropout=true, Is_causal=true, Is_even_N=false, Is_even_K=false, Return_softmax=true, Params=Flash_fwd_params, Is_Q_split_K=false]" 
(589): here
            instantiation of "void flash::compute_attn<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &) [with Kernel_traits=Flash_fwd_kernel_traits<96, 128, 64, 4, false, false, cutlass::bfloat16_t, Flash_kernel_traits<96, 128, 64, 4, cutlass::bfloat16_t>>, Is_dropout=true, Is_causal=true, Is_even_N=false, Is_even_K=false, Return_softmax=true, Params=Flash_fwd_params]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(15): here
            instantiation of "void flash_fwd_kernel<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax>(Flash_fwd_params) [with Kernel_traits=Flash_fwd_kernel_traits<96, 128, 64, 4, false, false, cutlass::bfloat16_t, Flash_kernel_traits<96, 128, 64, 4, cutlass::bfloat16_t>>, Is_dropout=true, Is_causal=true, Is_even_N=false, Is_even_K=false, Return_softmax=true]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(34): here
            instantiation of "void run_flash_fwd<Kernel_traits,Is_dropout,Is_causal>(Flash_fwd_params &, cudaStream_t) [with Kernel_traits=Flash_fwd_kernel_traits<96, 128, 64, 4, false, false, cutlass::bfloat16_t, Flash_kernel_traits<96, 128, 64, 4, cutlass::bfloat16_t>>, Is_dropout=true, Is_causal=true]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(92): here
            instantiation of "void run_mha_fwd_hdim96<T>(Flash_fwd_params &, cudaStream_t) [with T=cutlass::bfloat16_t]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_hdim96_bf16_sm80.cu(16): here

/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_kernel.h(140): warning: variable "MMA_M" was declared but never referenced
          detected during:
            instantiation of "void flash::compute_attn_1rowblock<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params,Is_Q_split_K>(const Params &, int, int, int) [with Kernel_traits=Flash_fwd_kernel_traits<96, 128, 64, 4, false, false, cutlass::bfloat16_t, Flash_kernel_traits<96, 128, 64, 4, cutlass::bfloat16_t>>, Is_dropout=true, Is_causal=true, Is_even_N=false, Is_even_K=false, Return_softmax=false, Params=Flash_fwd_params, Is_Q_split_K=false]" 
(589): here
            instantiation of "void flash::compute_attn<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &) [with Kernel_traits=Flash_fwd_kernel_traits<96, 128, 64, 4, false, false, cutlass::bfloat16_t, Flash_kernel_traits<96, 128, 64, 4, cutlass::bfloat16_t>>, Is_dropout=true, Is_causal=true, Is_even_N=false, Is_even_K=false, Return_softmax=false, Params=Flash_fwd_params]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(15): here
            instantiation of "void flash_fwd_kernel<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax>(Flash_fwd_params) [with Kernel_traits=Flash_fwd_kernel_traits<96, 128, 64, 4, false, false, cutlass::bfloat16_t, Flash_kernel_traits<96, 128, 64, 4, cutlass::bfloat16_t>>, Is_dropout=true, Is_causal=true, Is_even_N=false, Is_even_K=false, Return_softmax=false]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(34): here
            instantiation of "void run_flash_fwd<Kernel_traits,Is_dropout,Is_causal>(Flash_fwd_params &, cudaStream_t) [with Kernel_traits=Flash_fwd_kernel_traits<96, 128, 64, 4, false, false, cutlass::bfloat16_t, Flash_kernel_traits<96, 128, 64, 4, cutlass::bfloat16_t>>, Is_dropout=true, Is_causal=true]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(92): here
            instantiation of "void run_mha_fwd_hdim96<T>(Flash_fwd_params &, cudaStream_t) [with T=cutlass::bfloat16_t]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_hdim96_bf16_sm80.cu(16): here

/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_kernel.h(276): warning: variable "tQrQ" was declared but never referenced
          detected during:
            instantiation of "void flash::compute_attn_1rowblock<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params,Is_Q_split_K>(const Params &, int, int, int) [with Kernel_traits=Flash_fwd_kernel_traits<96, 128, 64, 4, false, false, cutlass::bfloat16_t, Flash_kernel_traits<96, 128, 64, 4, cutlass::bfloat16_t>>, Is_dropout=true, Is_causal=true, Is_even_N=false, Is_even_K=false, Return_softmax=false, Params=Flash_fwd_params, Is_Q_split_K=false]" 
(589): here
            instantiation of "void flash::compute_attn<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &) [with Kernel_traits=Flash_fwd_kernel_traits<96, 128, 64, 4, false, false, cutlass::bfloat16_t, Flash_kernel_traits<96, 128, 64, 4, cutlass::bfloat16_t>>, Is_dropout=true, Is_causal=true, Is_even_N=false, Is_even_K=false, Return_softmax=false, Params=Flash_fwd_params]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(15): here
            instantiation of "void flash_fwd_kernel<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax>(Flash_fwd_params) [with Kernel_traits=Flash_fwd_kernel_traits<96, 128, 64, 4, false, false, cutlass::bfloat16_t, Flash_kernel_traits<96, 128, 64, 4, cutlass::bfloat16_t>>, Is_dropout=true, Is_causal=true, Is_even_N=false, Is_even_K=false, Return_softmax=false]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(34): here
            instantiation of "void run_flash_fwd<Kernel_traits,Is_dropout,Is_causal>(Flash_fwd_params &, cudaStream_t) [with Kernel_traits=Flash_fwd_kernel_traits<96, 128, 64, 4, false, false, cutlass::bfloat16_t, Flash_kernel_traits<96, 128, 64, 4, cutlass::bfloat16_t>>, Is_dropout=true, Is_causal=true]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(92): here
            instantiation of "void run_mha_fwd_hdim96<T>(Flash_fwd_params &, cudaStream_t) [with T=cutlass::bfloat16_t]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_hdim96_bf16_sm80.cu(16): here

/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_kernel.h(140): warning: variable "MMA_M" was declared but never referenced
          detected during:
            instantiation of "void flash::compute_attn_1rowblock<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params,Is_Q_split_K>(const Params &, int, int, int) [with Kernel_traits=Flash_fwd_kernel_traits<96, 128, 64, 4, false, false, cutlass::bfloat16_t, Flash_kernel_traits<96, 128, 64, 4, cutlass::bfloat16_t>>, Is_dropout=true, Is_causal=false, Is_even_N=true, Is_even_K=true, Return_softmax=true, Params=Flash_fwd_params, Is_Q_split_K=false]" 
(589): here
            instantiation of "void flash::compute_attn<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &) [with Kernel_traits=Flash_fwd_kernel_traits<96, 128, 64, 4, false, false, cutlass::bfloat16_t, Flash_kernel_traits<96, 128, 64, 4, cutlass::bfloat16_t>>, Is_dropout=true, Is_causal=false, Is_even_N=true, Is_even_K=true, Return_softmax=true, Params=Flash_fwd_params]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(15): here
            instantiation of "void flash_fwd_kernel<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax>(Flash_fwd_params) [with Kernel_traits=Flash_fwd_kernel_traits<96, 128, 64, 4, false, false, cutlass::bfloat16_t, Flash_kernel_traits<96, 128, 64, 4, cutlass::bfloat16_t>>, Is_dropout=true, Is_causal=false, Is_even_N=true, Is_even_K=true, Return_softmax=true]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(34): here
            instantiation of "void run_flash_fwd<Kernel_traits,Is_dropout,Is_causal>(Flash_fwd_params &, cudaStream_t) [with Kernel_traits=Flash_fwd_kernel_traits<96, 128, 64, 4, false, false, cutlass::bfloat16_t, Flash_kernel_traits<96, 128, 64, 4, cutlass::bfloat16_t>>, Is_dropout=true, Is_causal=false]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(92): here
            instantiation of "void run_mha_fwd_hdim96<T>(Flash_fwd_params &, cudaStream_t) [with T=cutlass::bfloat16_t]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_hdim96_bf16_sm80.cu(16): here

/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_kernel.h(276): warning: variable "tQrQ" was declared but never referenced
          detected during:
            instantiation of "void flash::compute_attn_1rowblock<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params,Is_Q_split_K>(const Params &, int, int, int) [with Kernel_traits=Flash_fwd_kernel_traits<96, 128, 64, 4, false, false, cutlass::bfloat16_t, Flash_kernel_traits<96, 128, 64, 4, cutlass::bfloat16_t>>, Is_dropout=true, Is_causal=false, Is_even_N=true, Is_even_K=true, Return_softmax=true, Params=Flash_fwd_params, Is_Q_split_K=false]" 
(589): here
            instantiation of "void flash::compute_attn<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &) [with Kernel_traits=Flash_fwd_kernel_traits<96, 128, 64, 4, false, false, cutlass::bfloat16_t, Flash_kernel_traits<96, 128, 64, 4, cutlass::bfloat16_t>>, Is_dropout=true, Is_causal=false, Is_even_N=true, Is_even_K=true, Return_softmax=true, Params=Flash_fwd_params]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(15): here
            instantiation of "void flash_fwd_kernel<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax>(Flash_fwd_params) [with Kernel_traits=Flash_fwd_kernel_traits<96, 128, 64, 4, false, false, cutlass::bfloat16_t, Flash_kernel_traits<96, 128, 64, 4, cutlass::bfloat16_t>>, Is_dropout=true, Is_causal=false, Is_even_N=true, Is_even_K=true, Return_softmax=true]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(34): here
            instantiation of "void run_flash_fwd<Kernel_traits,Is_dropout,Is_causal>(Flash_fwd_params &, cudaStream_t) [with Kernel_traits=Flash_fwd_kernel_traits<96, 128, 64, 4, false, false, cutlass::bfloat16_t, Flash_kernel_traits<96, 128, 64, 4, cutlass::bfloat16_t>>, Is_dropout=true, Is_causal=false]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(92): here
            instantiation of "void run_mha_fwd_hdim96<T>(Flash_fwd_params &, cudaStream_t) [with T=cutlass::bfloat16_t]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_hdim96_bf16_sm80.cu(16): here

/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_kernel.h(140): warning: variable "MMA_M" was declared but never referenced
          detected during:
            instantiation of "void flash::compute_attn_1rowblock<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params,Is_Q_split_K>(const Params &, int, int, int) [with Kernel_traits=Flash_fwd_kernel_traits<96, 128, 64, 4, false, false, cutlass::bfloat16_t, Flash_kernel_traits<96, 128, 64, 4, cutlass::bfloat16_t>>, Is_dropout=true, Is_causal=false, Is_even_N=true, Is_even_K=true, Return_softmax=false, Params=Flash_fwd_params, Is_Q_split_K=false]" 
(589): here
            instantiation of "void flash::compute_attn<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &) [with Kernel_traits=Flash_fwd_kernel_traits<96, 128, 64, 4, false, false, cutlass::bfloat16_t, Flash_kernel_traits<96, 128, 64, 4, cutlass::bfloat16_t>>, Is_dropout=true, Is_causal=false, Is_even_N=true, Is_even_K=true, Return_softmax=false, Params=Flash_fwd_params]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(15): here
            instantiation of "void flash_fwd_kernel<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax>(Flash_fwd_params) [with Kernel_traits=Flash_fwd_kernel_traits<96, 128, 64, 4, false, false, cutlass::bfloat16_t, Flash_kernel_traits<96, 128, 64, 4, cutlass::bfloat16_t>>, Is_dropout=true, Is_causal=false, Is_even_N=true, Is_even_K=true, Return_softmax=false]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(34): here
            instantiation of "void run_flash_fwd<Kernel_traits,Is_dropout,Is_causal>(Flash_fwd_params &, cudaStream_t) [with Kernel_traits=Flash_fwd_kernel_traits<96, 128, 64, 4, false, false, cutlass::bfloat16_t, Flash_kernel_traits<96, 128, 64, 4, cutlass::bfloat16_t>>, Is_dropout=true, Is_causal=false]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(92): here
            instantiation of "void run_mha_fwd_hdim96<T>(Flash_fwd_params &, cudaStream_t) [with T=cutlass::bfloat16_t]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_hdim96_bf16_sm80.cu(16): here

/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_kernel.h(276): warning: variable "tQrQ" was declared but never referenced
          detected during:
            instantiation of "void flash::compute_attn_1rowblock<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params,Is_Q_split_K>(const Params &, int, int, int) [with Kernel_traits=Flash_fwd_kernel_traits<96, 128, 64, 4, false, false, cutlass::bfloat16_t, Flash_kernel_traits<96, 128, 64, 4, cutlass::bfloat16_t>>, Is_dropout=true, Is_causal=false, Is_even_N=true, Is_even_K=true, Return_softmax=false, Params=Flash_fwd_params, Is_Q_split_K=false]" 
(589): here
            instantiation of "void flash::compute_attn<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &) [with Kernel_traits=Flash_fwd_kernel_traits<96, 128, 64, 4, false, false, cutlass::bfloat16_t, Flash_kernel_traits<96, 128, 64, 4, cutlass::bfloat16_t>>, Is_dropout=true, Is_causal=false, Is_even_N=true, Is_even_K=true, Return_softmax=false, Params=Flash_fwd_params]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(15): here
            instantiation of "void flash_fwd_kernel<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax>(Flash_fwd_params) [with Kernel_traits=Flash_fwd_kernel_traits<96, 128, 64, 4, false, false, cutlass::bfloat16_t, Flash_kernel_traits<96, 128, 64, 4, cutlass::bfloat16_t>>, Is_dropout=true, Is_causal=false, Is_even_N=true, Is_even_K=true, Return_softmax=false]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(34): here
            instantiation of "void run_flash_fwd<Kernel_traits,Is_dropout,Is_causal>(Flash_fwd_params &, cudaStream_t) [with Kernel_traits=Flash_fwd_kernel_traits<96, 128, 64, 4, false, false, cutlass::bfloat16_t, Flash_kernel_traits<96, 128, 64, 4, cutlass::bfloat16_t>>, Is_dropout=true, Is_causal=false]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(92): here
            instantiation of "void run_mha_fwd_hdim96<T>(Flash_fwd_params &, cudaStream_t) [with T=cutlass::bfloat16_t]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_hdim96_bf16_sm80.cu(16): here

/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_kernel.h(140): warning: variable "MMA_M" was declared but never referenced
          detected during:
            instantiation of "void flash::compute_attn_1rowblock<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params,Is_Q_split_K>(const Params &, int, int, int) [with Kernel_traits=Flash_fwd_kernel_traits<96, 128, 64, 4, false, false, cutlass::bfloat16_t, Flash_kernel_traits<96, 128, 64, 4, cutlass::bfloat16_t>>, Is_dropout=true, Is_causal=false, Is_even_N=true, Is_even_K=false, Return_softmax=true, Params=Flash_fwd_params, Is_Q_split_K=false]" 
(589): here
            instantiation of "void flash::compute_attn<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &) [with Kernel_traits=Flash_fwd_kernel_traits<96, 128, 64, 4, false, false, cutlass::bfloat16_t, Flash_kernel_traits<96, 128, 64, 4, cutlass::bfloat16_t>>, Is_dropout=true, Is_causal=false, Is_even_N=true, Is_even_K=false, Return_softmax=true, Params=Flash_fwd_params]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(15): here
            instantiation of "void flash_fwd_kernel<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax>(Flash_fwd_params) [with Kernel_traits=Flash_fwd_kernel_traits<96, 128, 64, 4, false, false, cutlass::bfloat16_t, Flash_kernel_traits<96, 128, 64, 4, cutlass::bfloat16_t>>, Is_dropout=true, Is_causal=false, Is_even_N=true, Is_even_K=false, Return_softmax=true]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(34): here
            instantiation of "void run_flash_fwd<Kernel_traits,Is_dropout,Is_causal>(Flash_fwd_params &, cudaStream_t) [with Kernel_traits=Flash_fwd_kernel_traits<96, 128, 64, 4, false, false, cutlass::bfloat16_t, Flash_kernel_traits<96, 128, 64, 4, cutlass::bfloat16_t>>, Is_dropout=true, Is_causal=false]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(92): here
            instantiation of "void run_mha_fwd_hdim96<T>(Flash_fwd_params &, cudaStream_t) [with T=cutlass::bfloat16_t]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_hdim96_bf16_sm80.cu(16): here

/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_kernel.h(276): warning: variable "tQrQ" was declared but never referenced
          detected during:
            instantiation of "void flash::compute_attn_1rowblock<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params,Is_Q_split_K>(const Params &, int, int, int) [with Kernel_traits=Flash_fwd_kernel_traits<96, 128, 64, 4, false, false, cutlass::bfloat16_t, Flash_kernel_traits<96, 128, 64, 4, cutlass::bfloat16_t>>, Is_dropout=true, Is_causal=false, Is_even_N=true, Is_even_K=false, Return_softmax=true, Params=Flash_fwd_params, Is_Q_split_K=false]" 
(589): here
            instantiation of "void flash::compute_attn<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &) [with Kernel_traits=Flash_fwd_kernel_traits<96, 128, 64, 4, false, false, cutlass::bfloat16_t, Flash_kernel_traits<96, 128, 64, 4, cutlass::bfloat16_t>>, Is_dropout=true, Is_causal=false, Is_even_N=true, Is_even_K=false, Return_softmax=true, Params=Flash_fwd_params]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(15): here
            instantiation of "void flash_fwd_kernel<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax>(Flash_fwd_params) [with Kernel_traits=Flash_fwd_kernel_traits<96, 128, 64, 4, false, false, cutlass::bfloat16_t, Flash_kernel_traits<96, 128, 64, 4, cutlass::bfloat16_t>>, Is_dropout=true, Is_causal=false, Is_even_N=true, Is_even_K=false, Return_softmax=true]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(34): here
            instantiation of "void run_flash_fwd<Kernel_traits,Is_dropout,Is_causal>(Flash_fwd_params &, cudaStream_t) [with Kernel_traits=Flash_fwd_kernel_traits<96, 128, 64, 4, false, false, cutlass::bfloat16_t, Flash_kernel_traits<96, 128, 64, 4, cutlass::bfloat16_t>>, Is_dropout=true, Is_causal=false]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(92): here
            instantiation of "void run_mha_fwd_hdim96<T>(Flash_fwd_params &, cudaStream_t) [with T=cutlass::bfloat16_t]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_hdim96_bf16_sm80.cu(16): here

/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_kernel.h(140): warning: variable "MMA_M" was declared but never referenced
          detected during:
            instantiation of "void flash::compute_attn_1rowblock<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params,Is_Q_split_K>(const Params &, int, int, int) [with Kernel_traits=Flash_fwd_kernel_traits<96, 128, 64, 4, false, false, cutlass::bfloat16_t, Flash_kernel_traits<96, 128, 64, 4, cutlass::bfloat16_t>>, Is_dropout=true, Is_causal=false, Is_even_N=true, Is_even_K=false, Return_softmax=false, Params=Flash_fwd_params, Is_Q_split_K=false]" 
(589): here
            instantiation of "void flash::compute_attn<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &) [with Kernel_traits=Flash_fwd_kernel_traits<96, 128, 64, 4, false, false, cutlass::bfloat16_t, Flash_kernel_traits<96, 128, 64, 4, cutlass::bfloat16_t>>, Is_dropout=true, Is_causal=false, Is_even_N=true, Is_even_K=false, Return_softmax=false, Params=Flash_fwd_params]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(15): here
            instantiation of "void flash_fwd_kernel<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax>(Flash_fwd_params) [with Kernel_traits=Flash_fwd_kernel_traits<96, 128, 64, 4, false, false, cutlass::bfloat16_t, Flash_kernel_traits<96, 128, 64, 4, cutlass::bfloat16_t>>, Is_dropout=true, Is_causal=false, Is_even_N=true, Is_even_K=false, Return_softmax=false]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(34): here
            instantiation of "void run_flash_fwd<Kernel_traits,Is_dropout,Is_causal>(Flash_fwd_params &, cudaStream_t) [with Kernel_traits=Flash_fwd_kernel_traits<96, 128, 64, 4, false, false, cutlass::bfloat16_t, Flash_kernel_traits<96, 128, 64, 4, cutlass::bfloat16_t>>, Is_dropout=true, Is_causal=false]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(92): here
            instantiation of "void run_mha_fwd_hdim96<T>(Flash_fwd_params &, cudaStream_t) [with T=cutlass::bfloat16_t]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_hdim96_bf16_sm80.cu(16): here

/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_kernel.h(276): warning: variable "tQrQ" was declared but never referenced
          detected during:
            instantiation of "void flash::compute_attn_1rowblock<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params,Is_Q_split_K>(const Params &, int, int, int) [with Kernel_traits=Flash_fwd_kernel_traits<96, 128, 64, 4, false, false, cutlass::bfloat16_t, Flash_kernel_traits<96, 128, 64, 4, cutlass::bfloat16_t>>, Is_dropout=true, Is_causal=false, Is_even_N=true, Is_even_K=false, Return_softmax=false, Params=Flash_fwd_params, Is_Q_split_K=false]" 
(589): here
            instantiation of "void flash::compute_attn<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &) [with Kernel_traits=Flash_fwd_kernel_traits<96, 128, 64, 4, false, false, cutlass::bfloat16_t, Flash_kernel_traits<96, 128, 64, 4, cutlass::bfloat16_t>>, Is_dropout=true, Is_causal=false, Is_even_N=true, Is_even_K=false, Return_softmax=false, Params=Flash_fwd_params]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(15): here
            instantiation of "void flash_fwd_kernel<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax>(Flash_fwd_params) [with Kernel_traits=Flash_fwd_kernel_traits<96, 128, 64, 4, false, false, cutlass::bfloat16_t, Flash_kernel_traits<96, 128, 64, 4, cutlass::bfloat16_t>>, Is_dropout=true, Is_causal=false, Is_even_N=true, Is_even_K=false, Return_softmax=false]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(34): here
            instantiation of "void run_flash_fwd<Kernel_traits,Is_dropout,Is_causal>(Flash_fwd_params &, cudaStream_t) [with Kernel_traits=Flash_fwd_kernel_traits<96, 128, 64, 4, false, false, cutlass::bfloat16_t, Flash_kernel_traits<96, 128, 64, 4, cutlass::bfloat16_t>>, Is_dropout=true, Is_causal=false]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(92): here
            instantiation of "void run_mha_fwd_hdim96<T>(Flash_fwd_params &, cudaStream_t) [with T=cutlass::bfloat16_t]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_hdim96_bf16_sm80.cu(16): here

/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_kernel.h(140): warning: variable "MMA_M" was declared but never referenced
          detected during:
            instantiation of "void flash::compute_attn_1rowblock<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params,Is_Q_split_K>(const Params &, int, int, int) [with Kernel_traits=Flash_fwd_kernel_traits<96, 128, 64, 4, false, false, cutlass::bfloat16_t, Flash_kernel_traits<96, 128, 64, 4, cutlass::bfloat16_t>>, Is_dropout=true, Is_causal=false, Is_even_N=false, Is_even_K=true, Return_softmax=true, Params=Flash_fwd_params, Is_Q_split_K=false]" 
(589): here
            instantiation of "void flash::compute_attn<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &) [with Kernel_traits=Flash_fwd_kernel_traits<96, 128, 64, 4, false, false, cutlass::bfloat16_t, Flash_kernel_traits<96, 128, 64, 4, cutlass::bfloat16_t>>, Is_dropout=true, Is_causal=false, Is_even_N=false, Is_even_K=true, Return_softmax=true, Params=Flash_fwd_params]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(15): here
            instantiation of "void flash_fwd_kernel<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax>(Flash_fwd_params) [with Kernel_traits=Flash_fwd_kernel_traits<96, 128, 64, 4, false, false, cutlass::bfloat16_t, Flash_kernel_traits<96, 128, 64, 4, cutlass::bfloat16_t>>, Is_dropout=true, Is_causal=false, Is_even_N=false, Is_even_K=true, Return_softmax=true]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(34): here
            instantiation of "void run_flash_fwd<Kernel_traits,Is_dropout,Is_causal>(Flash_fwd_params &, cudaStream_t) [with Kernel_traits=Flash_fwd_kernel_traits<96, 128, 64, 4, false, false, cutlass::bfloat16_t, Flash_kernel_traits<96, 128, 64, 4, cutlass::bfloat16_t>>, Is_dropout=true, Is_causal=false]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(92): here
            instantiation of "void run_mha_fwd_hdim96<T>(Flash_fwd_params &, cudaStream_t) [with T=cutlass::bfloat16_t]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_hdim96_bf16_sm80.cu(16): here

/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_kernel.h(276): warning: variable "tQrQ" was declared but never referenced
          detected during:
            instantiation of "void flash::compute_attn_1rowblock<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params,Is_Q_split_K>(const Params &, int, int, int) [with Kernel_traits=Flash_fwd_kernel_traits<96, 128, 64, 4, false, false, cutlass::bfloat16_t, Flash_kernel_traits<96, 128, 64, 4, cutlass::bfloat16_t>>, Is_dropout=true, Is_causal=false, Is_even_N=false, Is_even_K=true, Return_softmax=true, Params=Flash_fwd_params, Is_Q_split_K=false]" 
(589): here
            instantiation of "void flash::compute_attn<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &) [with Kernel_traits=Flash_fwd_kernel_traits<96, 128, 64, 4, false, false, cutlass::bfloat16_t, Flash_kernel_traits<96, 128, 64, 4, cutlass::bfloat16_t>>, Is_dropout=true, Is_causal=false, Is_even_N=false, Is_even_K=true, Return_softmax=true, Params=Flash_fwd_params]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(15): here
            instantiation of "void flash_fwd_kernel<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax>(Flash_fwd_params) [with Kernel_traits=Flash_fwd_kernel_traits<96, 128, 64, 4, false, false, cutlass::bfloat16_t, Flash_kernel_traits<96, 128, 64, 4, cutlass::bfloat16_t>>, Is_dropout=true, Is_causal=false, Is_even_N=false, Is_even_K=true, Return_softmax=true]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(34): here
            instantiation of "void run_flash_fwd<Kernel_traits,Is_dropout,Is_causal>(Flash_fwd_params &, cudaStream_t) [with Kernel_traits=Flash_fwd_kernel_traits<96, 128, 64, 4, false, false, cutlass::bfloat16_t, Flash_kernel_traits<96, 128, 64, 4, cutlass::bfloat16_t>>, Is_dropout=true, Is_causal=false]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(92): here
            instantiation of "void run_mha_fwd_hdim96<T>(Flash_fwd_params &, cudaStream_t) [with T=cutlass::bfloat16_t]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_hdim96_bf16_sm80.cu(16): here

/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_kernel.h(140): warning: variable "MMA_M" was declared but never referenced
          detected during:
            instantiation of "void flash::compute_attn_1rowblock<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params,Is_Q_split_K>(const Params &, int, int, int) [with Kernel_traits=Flash_fwd_kernel_traits<96, 128, 64, 4, false, false, cutlass::bfloat16_t, Flash_kernel_traits<96, 128, 64, 4, cutlass::bfloat16_t>>, Is_dropout=true, Is_causal=false, Is_even_N=false, Is_even_K=true, Return_softmax=false, Params=Flash_fwd_params, Is_Q_split_K=false]" 
(589): here
            instantiation of "void flash::compute_attn<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &) [with Kernel_traits=Flash_fwd_kernel_traits<96, 128, 64, 4, false, false, cutlass::bfloat16_t, Flash_kernel_traits<96, 128, 64, 4, cutlass::bfloat16_t>>, Is_dropout=true, Is_causal=false, Is_even_N=false, Is_even_K=true, Return_softmax=false, Params=Flash_fwd_params]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(15): here
            instantiation of "void flash_fwd_kernel<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax>(Flash_fwd_params) [with Kernel_traits=Flash_fwd_kernel_traits<96, 128, 64, 4, false, false, cutlass::bfloat16_t, Flash_kernel_traits<96, 128, 64, 4, cutlass::bfloat16_t>>, Is_dropout=true, Is_causal=false, Is_even_N=false, Is_even_K=true, Return_softmax=false]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(34): here
            instantiation of "void run_flash_fwd<Kernel_traits,Is_dropout,Is_causal>(Flash_fwd_params &, cudaStream_t) [with Kernel_traits=Flash_fwd_kernel_traits<96, 128, 64, 4, false, false, cutlass::bfloat16_t, Flash_kernel_traits<96, 128, 64, 4, cutlass::bfloat16_t>>, Is_dropout=true, Is_causal=false]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(92): here
            instantiation of "void run_mha_fwd_hdim96<T>(Flash_fwd_params &, cudaStream_t) [with T=cutlass::bfloat16_t]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_hdim96_bf16_sm80.cu(16): here

/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_kernel.h(276): warning: variable "tQrQ" was declared but never referenced
          detected during:
            instantiation of "void flash::compute_attn_1rowblock<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params,Is_Q_split_K>(const Params &, int, int, int) [with Kernel_traits=Flash_fwd_kernel_traits<96, 128, 64, 4, false, false, cutlass::bfloat16_t, Flash_kernel_traits<96, 128, 64, 4, cutlass::bfloat16_t>>, Is_dropout=true, Is_causal=false, Is_even_N=false, Is_even_K=true, Return_softmax=false, Params=Flash_fwd_params, Is_Q_split_K=false]" 
(589): here
            instantiation of "void flash::compute_attn<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &) [with Kernel_traits=Flash_fwd_kernel_traits<96, 128, 64, 4, false, false, cutlass::bfloat16_t, Flash_kernel_traits<96, 128, 64, 4, cutlass::bfloat16_t>>, Is_dropout=true, Is_causal=false, Is_even_N=false, Is_even_K=true, Return_softmax=false, Params=Flash_fwd_params]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(15): here
            instantiation of "void flash_fwd_kernel<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax>(Flash_fwd_params) [with Kernel_traits=Flash_fwd_kernel_traits<96, 128, 64, 4, false, false, cutlass::bfloat16_t, Flash_kernel_traits<96, 128, 64, 4, cutlass::bfloat16_t>>, Is_dropout=true, Is_causal=false, Is_even_N=false, Is_even_K=true, Return_softmax=false]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(34): here
            instantiation of "void run_flash_fwd<Kernel_traits,Is_dropout,Is_causal>(Flash_fwd_params &, cudaStream_t) [with Kernel_traits=Flash_fwd_kernel_traits<96, 128, 64, 4, false, false, cutlass::bfloat16_t, Flash_kernel_traits<96, 128, 64, 4, cutlass::bfloat16_t>>, Is_dropout=true, Is_causal=false]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(92): here
            instantiation of "void run_mha_fwd_hdim96<T>(Flash_fwd_params &, cudaStream_t) [with T=cutlass::bfloat16_t]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_hdim96_bf16_sm80.cu(16): here

/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_kernel.h(140): warning: variable "MMA_M" was declared but never referenced
          detected during:
            instantiation of "void flash::compute_attn_1rowblock<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params,Is_Q_split_K>(const Params &, int, int, int) [with Kernel_traits=Flash_fwd_kernel_traits<96, 128, 64, 4, false, false, cutlass::bfloat16_t, Flash_kernel_traits<96, 128, 64, 4, cutlass::bfloat16_t>>, Is_dropout=true, Is_causal=false, Is_even_N=false, Is_even_K=false, Return_softmax=true, Params=Flash_fwd_params, Is_Q_split_K=false]" 
(589): here
            instantiation of "void flash::compute_attn<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &) [with Kernel_traits=Flash_fwd_kernel_traits<96, 128, 64, 4, false, false, cutlass::bfloat16_t, Flash_kernel_traits<96, 128, 64, 4, cutlass::bfloat16_t>>, Is_dropout=true, Is_causal=false, Is_even_N=false, Is_even_K=false, Return_softmax=true, Params=Flash_fwd_params]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(15): here
            instantiation of "void flash_fwd_kernel<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax>(Flash_fwd_params) [with Kernel_traits=Flash_fwd_kernel_traits<96, 128, 64, 4, false, false, cutlass::bfloat16_t, Flash_kernel_traits<96, 128, 64, 4, cutlass::bfloat16_t>>, Is_dropout=true, Is_causal=false, Is_even_N=false, Is_even_K=false, Return_softmax=true]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(34): here
            instantiation of "void run_flash_fwd<Kernel_traits,Is_dropout,Is_causal>(Flash_fwd_params &, cudaStream_t) [with Kernel_traits=Flash_fwd_kernel_traits<96, 128, 64, 4, false, false, cutlass::bfloat16_t, Flash_kernel_traits<96, 128, 64, 4, cutlass::bfloat16_t>>, Is_dropout=true, Is_causal=false]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(92): here
            instantiation of "void run_mha_fwd_hdim96<T>(Flash_fwd_params &, cudaStream_t) [with T=cutlass::bfloat16_t]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_hdim96_bf16_sm80.cu(16): here

/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_kernel.h(276): warning: variable "tQrQ" was declared but never referenced
          detected during:
            instantiation of "void flash::compute_attn_1rowblock<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params,Is_Q_split_K>(const Params &, int, int, int) [with Kernel_traits=Flash_fwd_kernel_traits<96, 128, 64, 4, false, false, cutlass::bfloat16_t, Flash_kernel_traits<96, 128, 64, 4, cutlass::bfloat16_t>>, Is_dropout=true, Is_causal=false, Is_even_N=false, Is_even_K=false, Return_softmax=true, Params=Flash_fwd_params, Is_Q_split_K=false]" 
(589): here
            instantiation of "void flash::compute_attn<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &) [with Kernel_traits=Flash_fwd_kernel_traits<96, 128, 64, 4, false, false, cutlass::bfloat16_t, Flash_kernel_traits<96, 128, 64, 4, cutlass::bfloat16_t>>, Is_dropout=true, Is_causal=false, Is_even_N=false, Is_even_K=false, Return_softmax=true, Params=Flash_fwd_params]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(15): here
            instantiation of "void flash_fwd_kernel<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax>(Flash_fwd_params) [with Kernel_traits=Flash_fwd_kernel_traits<96, 128, 64, 4, false, false, cutlass::bfloat16_t, Flash_kernel_traits<96, 128, 64, 4, cutlass::bfloat16_t>>, Is_dropout=true, Is_causal=false, Is_even_N=false, Is_even_K=false, Return_softmax=true]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(34): here
            instantiation of "void run_flash_fwd<Kernel_traits,Is_dropout,Is_causal>(Flash_fwd_params &, cudaStream_t) [with Kernel_traits=Flash_fwd_kernel_traits<96, 128, 64, 4, false, false, cutlass::bfloat16_t, Flash_kernel_traits<96, 128, 64, 4, cutlass::bfloat16_t>>, Is_dropout=true, Is_causal=false]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(92): here
            instantiation of "void run_mha_fwd_hdim96<T>(Flash_fwd_params &, cudaStream_t) [with T=cutlass::bfloat16_t]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_hdim96_bf16_sm80.cu(16): here

/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_kernel.h(140): warning: variable "MMA_M" was declared but never referenced
          detected during:
            instantiation of "void flash::compute_attn_1rowblock<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params,Is_Q_split_K>(const Params &, int, int, int) [with Kernel_traits=Flash_fwd_kernel_traits<96, 128, 64, 4, false, false, cutlass::bfloat16_t, Flash_kernel_traits<96, 128, 64, 4, cutlass::bfloat16_t>>, Is_dropout=true, Is_causal=false, Is_even_N=false, Is_even_K=false, Return_softmax=false, Params=Flash_fwd_params, Is_Q_split_K=false]" 
(589): here
            instantiation of "void flash::compute_attn<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &) [with Kernel_traits=Flash_fwd_kernel_traits<96, 128, 64, 4, false, false, cutlass::bfloat16_t, Flash_kernel_traits<96, 128, 64, 4, cutlass::bfloat16_t>>, Is_dropout=true, Is_causal=false, Is_even_N=false, Is_even_K=false, Return_softmax=false, Params=Flash_fwd_params]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(15): here
            instantiation of "void flash_fwd_kernel<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax>(Flash_fwd_params) [with Kernel_traits=Flash_fwd_kernel_traits<96, 128, 64, 4, false, false, cutlass::bfloat16_t, Flash_kernel_traits<96, 128, 64, 4, cutlass::bfloat16_t>>, Is_dropout=true, Is_causal=false, Is_even_N=false, Is_even_K=false, Return_softmax=false]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(34): here
            instantiation of "void run_flash_fwd<Kernel_traits,Is_dropout,Is_causal>(Flash_fwd_params &, cudaStream_t) [with Kernel_traits=Flash_fwd_kernel_traits<96, 128, 64, 4, false, false, cutlass::bfloat16_t, Flash_kernel_traits<96, 128, 64, 4, cutlass::bfloat16_t>>, Is_dropout=true, Is_causal=false]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(92): here
            instantiation of "void run_mha_fwd_hdim96<T>(Flash_fwd_params &, cudaStream_t) [with T=cutlass::bfloat16_t]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_hdim96_bf16_sm80.cu(16): here

/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_kernel.h(276): warning: variable "tQrQ" was declared but never referenced
          detected during:
            instantiation of "void flash::compute_attn_1rowblock<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params,Is_Q_split_K>(const Params &, int, int, int) [with Kernel_traits=Flash_fwd_kernel_traits<96, 128, 64, 4, false, false, cutlass::bfloat16_t, Flash_kernel_traits<96, 128, 64, 4, cutlass::bfloat16_t>>, Is_dropout=true, Is_causal=false, Is_even_N=false, Is_even_K=false, Return_softmax=false, Params=Flash_fwd_params, Is_Q_split_K=false]" 
(589): here
            instantiation of "void flash::compute_attn<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &) [with Kernel_traits=Flash_fwd_kernel_traits<96, 128, 64, 4, false, false, cutlass::bfloat16_t, Flash_kernel_traits<96, 128, 64, 4, cutlass::bfloat16_t>>, Is_dropout=true, Is_causal=false, Is_even_N=false, Is_even_K=false, Return_softmax=false, Params=Flash_fwd_params]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(15): here
            instantiation of "void flash_fwd_kernel<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax>(Flash_fwd_params) [with Kernel_traits=Flash_fwd_kernel_traits<96, 128, 64, 4, false, false, cutlass::bfloat16_t, Flash_kernel_traits<96, 128, 64, 4, cutlass::bfloat16_t>>, Is_dropout=true, Is_causal=false, Is_even_N=false, Is_even_K=false, Return_softmax=false]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(34): here
            instantiation of "void run_flash_fwd<Kernel_traits,Is_dropout,Is_causal>(Flash_fwd_params &, cudaStream_t) [with Kernel_traits=Flash_fwd_kernel_traits<96, 128, 64, 4, false, false, cutlass::bfloat16_t, Flash_kernel_traits<96, 128, 64, 4, cutlass::bfloat16_t>>, Is_dropout=true, Is_causal=false]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(92): here
            instantiation of "void run_mha_fwd_hdim96<T>(Flash_fwd_params &, cudaStream_t) [with T=cutlass::bfloat16_t]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_hdim96_bf16_sm80.cu(16): here

/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_kernel.h(140): warning: variable "MMA_M" was declared but never referenced
          detected during:
            instantiation of "void flash::compute_attn_1rowblock<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params,Is_Q_split_K>(const Params &, int, int, int) [with Kernel_traits=Flash_fwd_kernel_traits<96, 64, 64, 4, false, false, cutlass::bfloat16_t, Flash_kernel_traits<96, 64, 64, 4, cutlass::bfloat16_t>>, Is_dropout=false, Is_causal=true, Is_even_N=true, Is_even_K=true, Return_softmax=false, Params=Flash_fwd_params, Is_Q_split_K=false]" 
(589): here
            instantiation of "void flash::compute_attn<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &) [with Kernel_traits=Flash_fwd_kernel_traits<96, 64, 64, 4, false, false, cutlass::bfloat16_t, Flash_kernel_traits<96, 64, 64, 4, cutlass::bfloat16_t>>, Is_dropout=false, Is_causal=true, Is_even_N=true, Is_even_K=true, Return_softmax=false, Params=Flash_fwd_params]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(15): here
            instantiation of "void flash_fwd_kernel<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax>(Flash_fwd_params) [with Kernel_traits=Flash_fwd_kernel_traits<96, 64, 64, 4, false, false, cutlass::bfloat16_t, Flash_kernel_traits<96, 64, 64, 4, cutlass::bfloat16_t>>, Is_dropout=false, Is_causal=true, Is_even_N=true, Is_even_K=true, Return_softmax=false]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(34): here
            instantiation of "void run_flash_fwd<Kernel_traits,Is_dropout,Is_causal>(Flash_fwd_params &, cudaStream_t) [with Kernel_traits=Flash_fwd_kernel_traits<96, 64, 64, 4, false, false, cutlass::bfloat16_t, Flash_kernel_traits<96, 64, 64, 4, cutlass::bfloat16_t>>, Is_dropout=false, Is_causal=true]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(92): here
            instantiation of "void run_mha_fwd_hdim96<T>(Flash_fwd_params &, cudaStream_t) [with T=cutlass::bfloat16_t]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_hdim96_bf16_sm80.cu(16): here

/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_kernel.h(276): warning: variable "tQrQ" was declared but never referenced
          detected during:
            instantiation of "void flash::compute_attn_1rowblock<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params,Is_Q_split_K>(const Params &, int, int, int) [with Kernel_traits=Flash_fwd_kernel_traits<96, 64, 64, 4, false, false, cutlass::bfloat16_t, Flash_kernel_traits<96, 64, 64, 4, cutlass::bfloat16_t>>, Is_dropout=false, Is_causal=true, Is_even_N=true, Is_even_K=true, Return_softmax=false, Params=Flash_fwd_params, Is_Q_split_K=false]" 
(589): here
            instantiation of "void flash::compute_attn<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &) [with Kernel_traits=Flash_fwd_kernel_traits<96, 64, 64, 4, false, false, cutlass::bfloat16_t, Flash_kernel_traits<96, 64, 64, 4, cutlass::bfloat16_t>>, Is_dropout=false, Is_causal=true, Is_even_N=true, Is_even_K=true, Return_softmax=false, Params=Flash_fwd_params]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(15): here
            instantiation of "void flash_fwd_kernel<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax>(Flash_fwd_params) [with Kernel_traits=Flash_fwd_kernel_traits<96, 64, 64, 4, false, false, cutlass::bfloat16_t, Flash_kernel_traits<96, 64, 64, 4, cutlass::bfloat16_t>>, Is_dropout=false, Is_causal=true, Is_even_N=true, Is_even_K=true, Return_softmax=false]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(34): here
            instantiation of "void run_flash_fwd<Kernel_traits,Is_dropout,Is_causal>(Flash_fwd_params &, cudaStream_t) [with Kernel_traits=Flash_fwd_kernel_traits<96, 64, 64, 4, false, false, cutlass::bfloat16_t, Flash_kernel_traits<96, 64, 64, 4, cutlass::bfloat16_t>>, Is_dropout=false, Is_causal=true]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(92): here
            instantiation of "void run_mha_fwd_hdim96<T>(Flash_fwd_params &, cudaStream_t) [with T=cutlass::bfloat16_t]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_hdim96_bf16_sm80.cu(16): here

/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_kernel.h(140): warning: variable "MMA_M" was declared but never referenced
          detected during:
            instantiation of "void flash::compute_attn_1rowblock<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params,Is_Q_split_K>(const Params &, int, int, int) [with Kernel_traits=Flash_fwd_kernel_traits<96, 64, 64, 4, false, false, cutlass::bfloat16_t, Flash_kernel_traits<96, 64, 64, 4, cutlass::bfloat16_t>>, Is_dropout=false, Is_causal=true, Is_even_N=true, Is_even_K=false, Return_softmax=false, Params=Flash_fwd_params, Is_Q_split_K=false]" 
(589): here
            instantiation of "void flash::compute_attn<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &) [with Kernel_traits=Flash_fwd_kernel_traits<96, 64, 64, 4, false, false, cutlass::bfloat16_t, Flash_kernel_traits<96, 64, 64, 4, cutlass::bfloat16_t>>, Is_dropout=false, Is_causal=true, Is_even_N=true, Is_even_K=false, Return_softmax=false, Params=Flash_fwd_params]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(15): here
            instantiation of "void flash_fwd_kernel<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax>(Flash_fwd_params) [with Kernel_traits=Flash_fwd_kernel_traits<96, 64, 64, 4, false, false, cutlass::bfloat16_t, Flash_kernel_traits<96, 64, 64, 4, cutlass::bfloat16_t>>, Is_dropout=false, Is_causal=true, Is_even_N=true, Is_even_K=false, Return_softmax=false]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(34): here
            instantiation of "void run_flash_fwd<Kernel_traits,Is_dropout,Is_causal>(Flash_fwd_params &, cudaStream_t) [with Kernel_traits=Flash_fwd_kernel_traits<96, 64, 64, 4, false, false, cutlass::bfloat16_t, Flash_kernel_traits<96, 64, 64, 4, cutlass::bfloat16_t>>, Is_dropout=false, Is_causal=true]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(92): here
            instantiation of "void run_mha_fwd_hdim96<T>(Flash_fwd_params &, cudaStream_t) [with T=cutlass::bfloat16_t]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_hdim96_bf16_sm80.cu(16): here

/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_kernel.h(276): warning: variable "tQrQ" was declared but never referenced
          detected during:
            instantiation of "void flash::compute_attn_1rowblock<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params,Is_Q_split_K>(const Params &, int, int, int) [with Kernel_traits=Flash_fwd_kernel_traits<96, 64, 64, 4, false, false, cutlass::bfloat16_t, Flash_kernel_traits<96, 64, 64, 4, cutlass::bfloat16_t>>, Is_dropout=false, Is_causal=true, Is_even_N=true, Is_even_K=false, Return_softmax=false, Params=Flash_fwd_params, Is_Q_split_K=false]" 
(589): here
            instantiation of "void flash::compute_attn<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &) [with Kernel_traits=Flash_fwd_kernel_traits<96, 64, 64, 4, false, false, cutlass::bfloat16_t, Flash_kernel_traits<96, 64, 64, 4, cutlass::bfloat16_t>>, Is_dropout=false, Is_causal=true, Is_even_N=true, Is_even_K=false, Return_softmax=false, Params=Flash_fwd_params]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(15): here
            instantiation of "void flash_fwd_kernel<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax>(Flash_fwd_params) [with Kernel_traits=Flash_fwd_kernel_traits<96, 64, 64, 4, false, false, cutlass::bfloat16_t, Flash_kernel_traits<96, 64, 64, 4, cutlass::bfloat16_t>>, Is_dropout=false, Is_causal=true, Is_even_N=true, Is_even_K=false, Return_softmax=false]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(34): here
            instantiation of "void run_flash_fwd<Kernel_traits,Is_dropout,Is_causal>(Flash_fwd_params &, cudaStream_t) [with Kernel_traits=Flash_fwd_kernel_traits<96, 64, 64, 4, false, false, cutlass::bfloat16_t, Flash_kernel_traits<96, 64, 64, 4, cutlass::bfloat16_t>>, Is_dropout=false, Is_causal=true]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(92): here
            instantiation of "void run_mha_fwd_hdim96<T>(Flash_fwd_params &, cudaStream_t) [with T=cutlass::bfloat16_t]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_hdim96_bf16_sm80.cu(16): here

/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_kernel.h(140): warning: variable "MMA_M" was declared but never referenced
          detected during:
            instantiation of "void flash::compute_attn_1rowblock<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params,Is_Q_split_K>(const Params &, int, int, int) [with Kernel_traits=Flash_fwd_kernel_traits<96, 64, 64, 4, false, false, cutlass::bfloat16_t, Flash_kernel_traits<96, 64, 64, 4, cutlass::bfloat16_t>>, Is_dropout=false, Is_causal=true, Is_even_N=false, Is_even_K=true, Return_softmax=false, Params=Flash_fwd_params, Is_Q_split_K=false]" 
(589): here
            instantiation of "void flash::compute_attn<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &) [with Kernel_traits=Flash_fwd_kernel_traits<96, 64, 64, 4, false, false, cutlass::bfloat16_t, Flash_kernel_traits<96, 64, 64, 4, cutlass::bfloat16_t>>, Is_dropout=false, Is_causal=true, Is_even_N=false, Is_even_K=true, Return_softmax=false, Params=Flash_fwd_params]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(15): here
            instantiation of "void flash_fwd_kernel<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax>(Flash_fwd_params) [with Kernel_traits=Flash_fwd_kernel_traits<96, 64, 64, 4, false, false, cutlass::bfloat16_t, Flash_kernel_traits<96, 64, 64, 4, cutlass::bfloat16_t>>, Is_dropout=false, Is_causal=true, Is_even_N=false, Is_even_K=true, Return_softmax=false]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(34): here
            instantiation of "void run_flash_fwd<Kernel_traits,Is_dropout,Is_causal>(Flash_fwd_params &, cudaStream_t) [with Kernel_traits=Flash_fwd_kernel_traits<96, 64, 64, 4, false, false, cutlass::bfloat16_t, Flash_kernel_traits<96, 64, 64, 4, cutlass::bfloat16_t>>, Is_dropout=false, Is_causal=true]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(92): here
            instantiation of "void run_mha_fwd_hdim96<T>(Flash_fwd_params &, cudaStream_t) [with T=cutlass::bfloat16_t]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_hdim96_bf16_sm80.cu(16): here

/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_kernel.h(276): warning: variable "tQrQ" was declared but never referenced
          detected during:
            instantiation of "void flash::compute_attn_1rowblock<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params,Is_Q_split_K>(const Params &, int, int, int) [with Kernel_traits=Flash_fwd_kernel_traits<96, 64, 64, 4, false, false, cutlass::bfloat16_t, Flash_kernel_traits<96, 64, 64, 4, cutlass::bfloat16_t>>, Is_dropout=false, Is_causal=true, Is_even_N=false, Is_even_K=true, Return_softmax=false, Params=Flash_fwd_params, Is_Q_split_K=false]" 
(589): here
            instantiation of "void flash::compute_attn<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &) [with Kernel_traits=Flash_fwd_kernel_traits<96, 64, 64, 4, false, false, cutlass::bfloat16_t, Flash_kernel_traits<96, 64, 64, 4, cutlass::bfloat16_t>>, Is_dropout=false, Is_causal=true, Is_even_N=false, Is_even_K=true, Return_softmax=false, Params=Flash_fwd_params]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(15): here
            instantiation of "void flash_fwd_kernel<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax>(Flash_fwd_params) [with Kernel_traits=Flash_fwd_kernel_traits<96, 64, 64, 4, false, false, cutlass::bfloat16_t, Flash_kernel_traits<96, 64, 64, 4, cutlass::bfloat16_t>>, Is_dropout=false, Is_causal=true, Is_even_N=false, Is_even_K=true, Return_softmax=false]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(34): here
            instantiation of "void run_flash_fwd<Kernel_traits,Is_dropout,Is_causal>(Flash_fwd_params &, cudaStream_t) [with Kernel_traits=Flash_fwd_kernel_traits<96, 64, 64, 4, false, false, cutlass::bfloat16_t, Flash_kernel_traits<96, 64, 64, 4, cutlass::bfloat16_t>>, Is_dropout=false, Is_causal=true]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(92): here
            instantiation of "void run_mha_fwd_hdim96<T>(Flash_fwd_params &, cudaStream_t) [with T=cutlass::bfloat16_t]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_hdim96_bf16_sm80.cu(16): here

/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_kernel.h(140): warning: variable "MMA_M" was declared but never referenced
          detected during:
            instantiation of "void flash::compute_attn_1rowblock<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params,Is_Q_split_K>(const Params &, int, int, int) [with Kernel_traits=Flash_fwd_kernel_traits<96, 64, 64, 4, false, false, cutlass::bfloat16_t, Flash_kernel_traits<96, 64, 64, 4, cutlass::bfloat16_t>>, Is_dropout=false, Is_causal=true, Is_even_N=false, Is_even_K=false, Return_softmax=false, Params=Flash_fwd_params, Is_Q_split_K=false]" 
(589): here
            instantiation of "void flash::compute_attn<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &) [with Kernel_traits=Flash_fwd_kernel_traits<96, 64, 64, 4, false, false, cutlass::bfloat16_t, Flash_kernel_traits<96, 64, 64, 4, cutlass::bfloat16_t>>, Is_dropout=false, Is_causal=true, Is_even_N=false, Is_even_K=false, Return_softmax=false, Params=Flash_fwd_params]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(15): here
            instantiation of "void flash_fwd_kernel<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax>(Flash_fwd_params) [with Kernel_traits=Flash_fwd_kernel_traits<96, 64, 64, 4, false, false, cutlass::bfloat16_t, Flash_kernel_traits<96, 64, 64, 4, cutlass::bfloat16_t>>, Is_dropout=false, Is_causal=true, Is_even_N=false, Is_even_K=false, Return_softmax=false]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(34): here
            instantiation of "void run_flash_fwd<Kernel_traits,Is_dropout,Is_causal>(Flash_fwd_params &, cudaStream_t) [with Kernel_traits=Flash_fwd_kernel_traits<96, 64, 64, 4, false, false, cutlass::bfloat16_t, Flash_kernel_traits<96, 64, 64, 4, cutlass::bfloat16_t>>, Is_dropout=false, Is_causal=true]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(92): here
            instantiation of "void run_mha_fwd_hdim96<T>(Flash_fwd_params &, cudaStream_t) [with T=cutlass::bfloat16_t]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_hdim96_bf16_sm80.cu(16): here

/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_kernel.h(276): warning: variable "tQrQ" was declared but never referenced
          detected during:
            instantiation of "void flash::compute_attn_1rowblock<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params,Is_Q_split_K>(const Params &, int, int, int) [with Kernel_traits=Flash_fwd_kernel_traits<96, 64, 64, 4, false, false, cutlass::bfloat16_t, Flash_kernel_traits<96, 64, 64, 4, cutlass::bfloat16_t>>, Is_dropout=false, Is_causal=true, Is_even_N=false, Is_even_K=false, Return_softmax=false, Params=Flash_fwd_params, Is_Q_split_K=false]" 
(589): here
            instantiation of "void flash::compute_attn<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &) [with Kernel_traits=Flash_fwd_kernel_traits<96, 64, 64, 4, false, false, cutlass::bfloat16_t, Flash_kernel_traits<96, 64, 64, 4, cutlass::bfloat16_t>>, Is_dropout=false, Is_causal=true, Is_even_N=false, Is_even_K=false, Return_softmax=false, Params=Flash_fwd_params]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(15): here
            instantiation of "void flash_fwd_kernel<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax>(Flash_fwd_params) [with Kernel_traits=Flash_fwd_kernel_traits<96, 64, 64, 4, false, false, cutlass::bfloat16_t, Flash_kernel_traits<96, 64, 64, 4, cutlass::bfloat16_t>>, Is_dropout=false, Is_causal=true, Is_even_N=false, Is_even_K=false, Return_softmax=false]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(34): here
            instantiation of "void run_flash_fwd<Kernel_traits,Is_dropout,Is_causal>(Flash_fwd_params &, cudaStream_t) [with Kernel_traits=Flash_fwd_kernel_traits<96, 64, 64, 4, false, false, cutlass::bfloat16_t, Flash_kernel_traits<96, 64, 64, 4, cutlass::bfloat16_t>>, Is_dropout=false, Is_causal=true]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(92): here
            instantiation of "void run_mha_fwd_hdim96<T>(Flash_fwd_params &, cudaStream_t) [with T=cutlass::bfloat16_t]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_hdim96_bf16_sm80.cu(16): here

/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_kernel.h(140): warning: variable "MMA_M" was declared but never referenced
          detected during:
            instantiation of "void flash::compute_attn_1rowblock<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params,Is_Q_split_K>(const Params &, int, int, int) [with Kernel_traits=Flash_fwd_kernel_traits<96, 128, 64, 4, false, false, cutlass::bfloat16_t, Flash_kernel_traits<96, 128, 64, 4, cutlass::bfloat16_t>>, Is_dropout=false, Is_causal=true, Is_even_N=true, Is_even_K=true, Return_softmax=false, Params=Flash_fwd_params, Is_Q_split_K=false]" 
(589): here
            instantiation of "void flash::compute_attn<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &) [with Kernel_traits=Flash_fwd_kernel_traits<96, 128, 64, 4, false, false, cutlass::bfloat16_t, Flash_kernel_traits<96, 128, 64, 4, cutlass::bfloat16_t>>, Is_dropout=false, Is_causal=true, Is_even_N=true, Is_even_K=true, Return_softmax=false, Params=Flash_fwd_params]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(15): here
            instantiation of "void flash_fwd_kernel<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax>(Flash_fwd_params) [with Kernel_traits=Flash_fwd_kernel_traits<96, 128, 64, 4, false, false, cutlass::bfloat16_t, Flash_kernel_traits<96, 128, 64, 4, cutlass::bfloat16_t>>, Is_dropout=false, Is_causal=true, Is_even_N=true, Is_even_K=true, Return_softmax=false]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(34): here
            instantiation of "void run_flash_fwd<Kernel_traits,Is_dropout,Is_causal>(Flash_fwd_params &, cudaStream_t) [with Kernel_traits=Flash_fwd_kernel_traits<96, 128, 64, 4, false, false, cutlass::bfloat16_t, Flash_kernel_traits<96, 128, 64, 4, cutlass::bfloat16_t>>, Is_dropout=false, Is_causal=true]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(92): here
            instantiation of "void run_mha_fwd_hdim96<T>(Flash_fwd_params &, cudaStream_t) [with T=cutlass::bfloat16_t]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_hdim96_bf16_sm80.cu(16): here

/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_kernel.h(276): warning: variable "tQrQ" was declared but never referenced
          detected during:
            instantiation of "void flash::compute_attn_1rowblock<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params,Is_Q_split_K>(const Params &, int, int, int) [with Kernel_traits=Flash_fwd_kernel_traits<96, 128, 64, 4, false, false, cutlass::bfloat16_t, Flash_kernel_traits<96, 128, 64, 4, cutlass::bfloat16_t>>, Is_dropout=false, Is_causal=true, Is_even_N=true, Is_even_K=true, Return_softmax=false, Params=Flash_fwd_params, Is_Q_split_K=false]" 
(589): here
            instantiation of "void flash::compute_attn<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &) [with Kernel_traits=Flash_fwd_kernel_traits<96, 128, 64, 4, false, false, cutlass::bfloat16_t, Flash_kernel_traits<96, 128, 64, 4, cutlass::bfloat16_t>>, Is_dropout=false, Is_causal=true, Is_even_N=true, Is_even_K=true, Return_softmax=false, Params=Flash_fwd_params]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(15): here
            instantiation of "void flash_fwd_kernel<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax>(Flash_fwd_params) [with Kernel_traits=Flash_fwd_kernel_traits<96, 128, 64, 4, false, false, cutlass::bfloat16_t, Flash_kernel_traits<96, 128, 64, 4, cutlass::bfloat16_t>>, Is_dropout=false, Is_causal=true, Is_even_N=true, Is_even_K=true, Return_softmax=false]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(34): here
            instantiation of "void run_flash_fwd<Kernel_traits,Is_dropout,Is_causal>(Flash_fwd_params &, cudaStream_t) [with Kernel_traits=Flash_fwd_kernel_traits<96, 128, 64, 4, false, false, cutlass::bfloat16_t, Flash_kernel_traits<96, 128, 64, 4, cutlass::bfloat16_t>>, Is_dropout=false, Is_causal=true]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(92): here
            instantiation of "void run_mha_fwd_hdim96<T>(Flash_fwd_params &, cudaStream_t) [with T=cutlass::bfloat16_t]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_hdim96_bf16_sm80.cu(16): here

/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_kernel.h(140): warning: variable "MMA_M" was declared but never referenced
          detected during:
            instantiation of "void flash::compute_attn_1rowblock<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params,Is_Q_split_K>(const Params &, int, int, int) [with Kernel_traits=Flash_fwd_kernel_traits<96, 128, 64, 4, false, false, cutlass::bfloat16_t, Flash_kernel_traits<96, 128, 64, 4, cutlass::bfloat16_t>>, Is_dropout=false, Is_causal=true, Is_even_N=true, Is_even_K=false, Return_softmax=false, Params=Flash_fwd_params, Is_Q_split_K=false]" 
(589): here
            instantiation of "void flash::compute_attn<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &) [with Kernel_traits=Flash_fwd_kernel_traits<96, 128, 64, 4, false, false, cutlass::bfloat16_t, Flash_kernel_traits<96, 128, 64, 4, cutlass::bfloat16_t>>, Is_dropout=false, Is_causal=true, Is_even_N=true, Is_even_K=false, Return_softmax=false, Params=Flash_fwd_params]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(15): here
            instantiation of "void flash_fwd_kernel<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax>(Flash_fwd_params) [with Kernel_traits=Flash_fwd_kernel_traits<96, 128, 64, 4, false, false, cutlass::bfloat16_t, Flash_kernel_traits<96, 128, 64, 4, cutlass::bfloat16_t>>, Is_dropout=false, Is_causal=true, Is_even_N=true, Is_even_K=false, Return_softmax=false]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(34): here
            instantiation of "void run_flash_fwd<Kernel_traits,Is_dropout,Is_causal>(Flash_fwd_params &, cudaStream_t) [with Kernel_traits=Flash_fwd_kernel_traits<96, 128, 64, 4, false, false, cutlass::bfloat16_t, Flash_kernel_traits<96, 128, 64, 4, cutlass::bfloat16_t>>, Is_dropout=false, Is_causal=true]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(92): here
            instantiation of "void run_mha_fwd_hdim96<T>(Flash_fwd_params &, cudaStream_t) [with T=cutlass::bfloat16_t]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_hdim96_bf16_sm80.cu(16): here

/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_kernel.h(276): warning: variable "tQrQ" was declared but never referenced
          detected during:
            instantiation of "void flash::compute_attn_1rowblock<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params,Is_Q_split_K>(const Params &, int, int, int) [with Kernel_traits=Flash_fwd_kernel_traits<96, 128, 64, 4, false, false, cutlass::bfloat16_t, Flash_kernel_traits<96, 128, 64, 4, cutlass::bfloat16_t>>, Is_dropout=false, Is_causal=true, Is_even_N=true, Is_even_K=false, Return_softmax=false, Params=Flash_fwd_params, Is_Q_split_K=false]" 
(589): here
            instantiation of "void flash::compute_attn<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &) [with Kernel_traits=Flash_fwd_kernel_traits<96, 128, 64, 4, false, false, cutlass::bfloat16_t, Flash_kernel_traits<96, 128, 64, 4, cutlass::bfloat16_t>>, Is_dropout=false, Is_causal=true, Is_even_N=true, Is_even_K=false, Return_softmax=false, Params=Flash_fwd_params]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(15): here
            instantiation of "void flash_fwd_kernel<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax>(Flash_fwd_params) [with Kernel_traits=Flash_fwd_kernel_traits<96, 128, 64, 4, false, false, cutlass::bfloat16_t, Flash_kernel_traits<96, 128, 64, 4, cutlass::bfloat16_t>>, Is_dropout=false, Is_causal=true, Is_even_N=true, Is_even_K=false, Return_softmax=false]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(34): here
            instantiation of "void run_flash_fwd<Kernel_traits,Is_dropout,Is_causal>(Flash_fwd_params &, cudaStream_t) [with Kernel_traits=Flash_fwd_kernel_traits<96, 128, 64, 4, false, false, cutlass::bfloat16_t, Flash_kernel_traits<96, 128, 64, 4, cutlass::bfloat16_t>>, Is_dropout=false, Is_causal=true]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(92): here
            instantiation of "void run_mha_fwd_hdim96<T>(Flash_fwd_params &, cudaStream_t) [with T=cutlass::bfloat16_t]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_hdim96_bf16_sm80.cu(16): here

/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_kernel.h(140): warning: variable "MMA_M" was declared but never referenced
          detected during:
            instantiation of "void flash::compute_attn_1rowblock<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params,Is_Q_split_K>(const Params &, int, int, int) [with Kernel_traits=Flash_fwd_kernel_traits<96, 128, 64, 4, false, false, cutlass::bfloat16_t, Flash_kernel_traits<96, 128, 64, 4, cutlass::bfloat16_t>>, Is_dropout=false, Is_causal=true, Is_even_N=false, Is_even_K=true, Return_softmax=false, Params=Flash_fwd_params, Is_Q_split_K=false]" 
(589): here
            instantiation of "void flash::compute_attn<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &) [with Kernel_traits=Flash_fwd_kernel_traits<96, 128, 64, 4, false, false, cutlass::bfloat16_t, Flash_kernel_traits<96, 128, 64, 4, cutlass::bfloat16_t>>, Is_dropout=false, Is_causal=true, Is_even_N=false, Is_even_K=true, Return_softmax=false, Params=Flash_fwd_params]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(15): here
            instantiation of "void flash_fwd_kernel<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax>(Flash_fwd_params) [with Kernel_traits=Flash_fwd_kernel_traits<96, 128, 64, 4, false, false, cutlass::bfloat16_t, Flash_kernel_traits<96, 128, 64, 4, cutlass::bfloat16_t>>, Is_dropout=false, Is_causal=true, Is_even_N=false, Is_even_K=true, Return_softmax=false]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(34): here
            instantiation of "void run_flash_fwd<Kernel_traits,Is_dropout,Is_causal>(Flash_fwd_params &, cudaStream_t) [with Kernel_traits=Flash_fwd_kernel_traits<96, 128, 64, 4, false, false, cutlass::bfloat16_t, Flash_kernel_traits<96, 128, 64, 4, cutlass::bfloat16_t>>, Is_dropout=false, Is_causal=true]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(92): here
            instantiation of "void run_mha_fwd_hdim96<T>(Flash_fwd_params &, cudaStream_t) [with T=cutlass::bfloat16_t]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_hdim96_bf16_sm80.cu(16): here

/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_kernel.h(276): warning: variable "tQrQ" was declared but never referenced
          detected during:
            instantiation of "void flash::compute_attn_1rowblock<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params,Is_Q_split_K>(const Params &, int, int, int) [with Kernel_traits=Flash_fwd_kernel_traits<96, 128, 64, 4, false, false, cutlass::bfloat16_t, Flash_kernel_traits<96, 128, 64, 4, cutlass::bfloat16_t>>, Is_dropout=false, Is_causal=true, Is_even_N=false, Is_even_K=true, Return_softmax=false, Params=Flash_fwd_params, Is_Q_split_K=false]" 
(589): here
            instantiation of "void flash::compute_attn<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &) [with Kernel_traits=Flash_fwd_kernel_traits<96, 128, 64, 4, false, false, cutlass::bfloat16_t, Flash_kernel_traits<96, 128, 64, 4, cutlass::bfloat16_t>>, Is_dropout=false, Is_causal=true, Is_even_N=false, Is_even_K=true, Return_softmax=false, Params=Flash_fwd_params]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(15): here
            instantiation of "void flash_fwd_kernel<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax>(Flash_fwd_params) [with Kernel_traits=Flash_fwd_kernel_traits<96, 128, 64, 4, false, false, cutlass::bfloat16_t, Flash_kernel_traits<96, 128, 64, 4, cutlass::bfloat16_t>>, Is_dropout=false, Is_causal=true, Is_even_N=false, Is_even_K=true, Return_softmax=false]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(34): here
            instantiation of "void run_flash_fwd<Kernel_traits,Is_dropout,Is_causal>(Flash_fwd_params &, cudaStream_t) [with Kernel_traits=Flash_fwd_kernel_traits<96, 128, 64, 4, false, false, cutlass::bfloat16_t, Flash_kernel_traits<96, 128, 64, 4, cutlass::bfloat16_t>>, Is_dropout=false, Is_causal=true]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(92): here
            instantiation of "void run_mha_fwd_hdim96<T>(Flash_fwd_params &, cudaStream_t) [with T=cutlass::bfloat16_t]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_hdim96_bf16_sm80.cu(16): here

/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_kernel.h(140): warning: variable "MMA_M" was declared but never referenced
          detected during:
            instantiation of "void flash::compute_attn_1rowblock<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params,Is_Q_split_K>(const Params &, int, int, int) [with Kernel_traits=Flash_fwd_kernel_traits<96, 128, 64, 4, false, false, cutlass::bfloat16_t, Flash_kernel_traits<96, 128, 64, 4, cutlass::bfloat16_t>>, Is_dropout=false, Is_causal=true, Is_even_N=false, Is_even_K=false, Return_softmax=false, Params=Flash_fwd_params, Is_Q_split_K=false]" 
(589): here
            instantiation of "void flash::compute_attn<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &) [with Kernel_traits=Flash_fwd_kernel_traits<96, 128, 64, 4, false, false, cutlass::bfloat16_t, Flash_kernel_traits<96, 128, 64, 4, cutlass::bfloat16_t>>, Is_dropout=false, Is_causal=true, Is_even_N=false, Is_even_K=false, Return_softmax=false, Params=Flash_fwd_params]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(15): here
            instantiation of "void flash_fwd_kernel<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax>(Flash_fwd_params) [with Kernel_traits=Flash_fwd_kernel_traits<96, 128, 64, 4, false, false, cutlass::bfloat16_t, Flash_kernel_traits<96, 128, 64, 4, cutlass::bfloat16_t>>, Is_dropout=false, Is_causal=true, Is_even_N=false, Is_even_K=false, Return_softmax=false]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(34): here
            instantiation of "void run_flash_fwd<Kernel_traits,Is_dropout,Is_causal>(Flash_fwd_params &, cudaStream_t) [with Kernel_traits=Flash_fwd_kernel_traits<96, 128, 64, 4, false, false, cutlass::bfloat16_t, Flash_kernel_traits<96, 128, 64, 4, cutlass::bfloat16_t>>, Is_dropout=false, Is_causal=true]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(92): here
            instantiation of "void run_mha_fwd_hdim96<T>(Flash_fwd_params &, cudaStream_t) [with T=cutlass::bfloat16_t]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_hdim96_bf16_sm80.cu(16): here

/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_kernel.h(276): warning: variable "tQrQ" was declared but never referenced
          detected during:
            instantiation of "void flash::compute_attn_1rowblock<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params,Is_Q_split_K>(const Params &, int, int, int) [with Kernel_traits=Flash_fwd_kernel_traits<96, 128, 64, 4, false, false, cutlass::bfloat16_t, Flash_kernel_traits<96, 128, 64, 4, cutlass::bfloat16_t>>, Is_dropout=false, Is_causal=true, Is_even_N=false, Is_even_K=false, Return_softmax=false, Params=Flash_fwd_params, Is_Q_split_K=false]" 
(589): here
            instantiation of "void flash::compute_attn<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &) [with Kernel_traits=Flash_fwd_kernel_traits<96, 128, 64, 4, false, false, cutlass::bfloat16_t, Flash_kernel_traits<96, 128, 64, 4, cutlass::bfloat16_t>>, Is_dropout=false, Is_causal=true, Is_even_N=false, Is_even_K=false, Return_softmax=false, Params=Flash_fwd_params]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(15): here
            instantiation of "void flash_fwd_kernel<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax>(Flash_fwd_params) [with Kernel_traits=Flash_fwd_kernel_traits<96, 128, 64, 4, false, false, cutlass::bfloat16_t, Flash_kernel_traits<96, 128, 64, 4, cutlass::bfloat16_t>>, Is_dropout=false, Is_causal=true, Is_even_N=false, Is_even_K=false, Return_softmax=false]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(34): here
            instantiation of "void run_flash_fwd<Kernel_traits,Is_dropout,Is_causal>(Flash_fwd_params &, cudaStream_t) [with Kernel_traits=Flash_fwd_kernel_traits<96, 128, 64, 4, false, false, cutlass::bfloat16_t, Flash_kernel_traits<96, 128, 64, 4, cutlass::bfloat16_t>>, Is_dropout=false, Is_causal=true]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(92): here
            instantiation of "void run_mha_fwd_hdim96<T>(Flash_fwd_params &, cudaStream_t) [with T=cutlass::bfloat16_t]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_hdim96_bf16_sm80.cu(16): here

/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_kernel.h(140): warning: variable "MMA_M" was declared but never referenced
          detected during:
            instantiation of "void flash::compute_attn_1rowblock<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params,Is_Q_split_K>(const Params &, int, int, int) [with Kernel_traits=Flash_fwd_kernel_traits<96, 128, 64, 4, false, false, cutlass::bfloat16_t, Flash_kernel_traits<96, 128, 64, 4, cutlass::bfloat16_t>>, Is_dropout=false, Is_causal=false, Is_even_N=true, Is_even_K=true, Return_softmax=false, Params=Flash_fwd_params, Is_Q_split_K=false]" 
(589): here
            instantiation of "void flash::compute_attn<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &) [with Kernel_traits=Flash_fwd_kernel_traits<96, 128, 64, 4, false, false, cutlass::bfloat16_t, Flash_kernel_traits<96, 128, 64, 4, cutlass::bfloat16_t>>, Is_dropout=false, Is_causal=false, Is_even_N=true, Is_even_K=true, Return_softmax=false, Params=Flash_fwd_params]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(15): here
            instantiation of "void flash_fwd_kernel<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax>(Flash_fwd_params) [with Kernel_traits=Flash_fwd_kernel_traits<96, 128, 64, 4, false, false, cutlass::bfloat16_t, Flash_kernel_traits<96, 128, 64, 4, cutlass::bfloat16_t>>, Is_dropout=false, Is_causal=false, Is_even_N=true, Is_even_K=true, Return_softmax=false]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(34): here
            instantiation of "void run_flash_fwd<Kernel_traits,Is_dropout,Is_causal>(Flash_fwd_params &, cudaStream_t) [with Kernel_traits=Flash_fwd_kernel_traits<96, 128, 64, 4, false, false, cutlass::bfloat16_t, Flash_kernel_traits<96, 128, 64, 4, cutlass::bfloat16_t>>, Is_dropout=false, Is_causal=false]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(92): here
            instantiation of "void run_mha_fwd_hdim96<T>(Flash_fwd_params &, cudaStream_t) [with T=cutlass::bfloat16_t]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_hdim96_bf16_sm80.cu(16): here

/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_kernel.h(276): warning: variable "tQrQ" was declared but never referenced
          detected during:
            instantiation of "void flash::compute_attn_1rowblock<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params,Is_Q_split_K>(const Params &, int, int, int) [with Kernel_traits=Flash_fwd_kernel_traits<96, 128, 64, 4, false, false, cutlass::bfloat16_t, Flash_kernel_traits<96, 128, 64, 4, cutlass::bfloat16_t>>, Is_dropout=false, Is_causal=false, Is_even_N=true, Is_even_K=true, Return_softmax=false, Params=Flash_fwd_params, Is_Q_split_K=false]" 
(589): here
            instantiation of "void flash::compute_attn<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &) [with Kernel_traits=Flash_fwd_kernel_traits<96, 128, 64, 4, false, false, cutlass::bfloat16_t, Flash_kernel_traits<96, 128, 64, 4, cutlass::bfloat16_t>>, Is_dropout=false, Is_causal=false, Is_even_N=true, Is_even_K=true, Return_softmax=false, Params=Flash_fwd_params]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(15): here
            instantiation of "void flash_fwd_kernel<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax>(Flash_fwd_params) [with Kernel_traits=Flash_fwd_kernel_traits<96, 128, 64, 4, false, false, cutlass::bfloat16_t, Flash_kernel_traits<96, 128, 64, 4, cutlass::bfloat16_t>>, Is_dropout=false, Is_causal=false, Is_even_N=true, Is_even_K=true, Return_softmax=false]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(34): here
            instantiation of "void run_flash_fwd<Kernel_traits,Is_dropout,Is_causal>(Flash_fwd_params &, cudaStream_t) [with Kernel_traits=Flash_fwd_kernel_traits<96, 128, 64, 4, false, false, cutlass::bfloat16_t, Flash_kernel_traits<96, 128, 64, 4, cutlass::bfloat16_t>>, Is_dropout=false, Is_causal=false]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(92): here
            instantiation of "void run_mha_fwd_hdim96<T>(Flash_fwd_params &, cudaStream_t) [with T=cutlass::bfloat16_t]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_hdim96_bf16_sm80.cu(16): here

/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_kernel.h(140): warning: variable "MMA_M" was declared but never referenced
          detected during:
            instantiation of "void flash::compute_attn_1rowblock<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params,Is_Q_split_K>(const Params &, int, int, int) [with Kernel_traits=Flash_fwd_kernel_traits<96, 128, 64, 4, false, false, cutlass::bfloat16_t, Flash_kernel_traits<96, 128, 64, 4, cutlass::bfloat16_t>>, Is_dropout=false, Is_causal=false, Is_even_N=true, Is_even_K=false, Return_softmax=false, Params=Flash_fwd_params, Is_Q_split_K=false]" 
(589): here
            instantiation of "void flash::compute_attn<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &) [with Kernel_traits=Flash_fwd_kernel_traits<96, 128, 64, 4, false, false, cutlass::bfloat16_t, Flash_kernel_traits<96, 128, 64, 4, cutlass::bfloat16_t>>, Is_dropout=false, Is_causal=false, Is_even_N=true, Is_even_K=false, Return_softmax=false, Params=Flash_fwd_params]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(15): here
            instantiation of "void flash_fwd_kernel<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax>(Flash_fwd_params) [with Kernel_traits=Flash_fwd_kernel_traits<96, 128, 64, 4, false, false, cutlass::bfloat16_t, Flash_kernel_traits<96, 128, 64, 4, cutlass::bfloat16_t>>, Is_dropout=false, Is_causal=false, Is_even_N=true, Is_even_K=false, Return_softmax=false]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(34): here
            instantiation of "void run_flash_fwd<Kernel_traits,Is_dropout,Is_causal>(Flash_fwd_params &, cudaStream_t) [with Kernel_traits=Flash_fwd_kernel_traits<96, 128, 64, 4, false, false, cutlass::bfloat16_t, Flash_kernel_traits<96, 128, 64, 4, cutlass::bfloat16_t>>, Is_dropout=false, Is_causal=false]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(92): here
            instantiation of "void run_mha_fwd_hdim96<T>(Flash_fwd_params &, cudaStream_t) [with T=cutlass::bfloat16_t]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_hdim96_bf16_sm80.cu(16): here

/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_kernel.h(276): warning: variable "tQrQ" was declared but never referenced
          detected during:
            instantiation of "void flash::compute_attn_1rowblock<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params,Is_Q_split_K>(const Params &, int, int, int) [with Kernel_traits=Flash_fwd_kernel_traits<96, 128, 64, 4, false, false, cutlass::bfloat16_t, Flash_kernel_traits<96, 128, 64, 4, cutlass::bfloat16_t>>, Is_dropout=false, Is_causal=false, Is_even_N=true, Is_even_K=false, Return_softmax=false, Params=Flash_fwd_params, Is_Q_split_K=false]" 
(589): here
            instantiation of "void flash::compute_attn<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &) [with Kernel_traits=Flash_fwd_kernel_traits<96, 128, 64, 4, false, false, cutlass::bfloat16_t, Flash_kernel_traits<96, 128, 64, 4, cutlass::bfloat16_t>>, Is_dropout=false, Is_causal=false, Is_even_N=true, Is_even_K=false, Return_softmax=false, Params=Flash_fwd_params]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(15): here
            instantiation of "void flash_fwd_kernel<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax>(Flash_fwd_params) [with Kernel_traits=Flash_fwd_kernel_traits<96, 128, 64, 4, false, false, cutlass::bfloat16_t, Flash_kernel_traits<96, 128, 64, 4, cutlass::bfloat16_t>>, Is_dropout=false, Is_causal=false, Is_even_N=true, Is_even_K=false, Return_softmax=false]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(34): here
            instantiation of "void run_flash_fwd<Kernel_traits,Is_dropout,Is_causal>(Flash_fwd_params &, cudaStream_t) [with Kernel_traits=Flash_fwd_kernel_traits<96, 128, 64, 4, false, false, cutlass::bfloat16_t, Flash_kernel_traits<96, 128, 64, 4, cutlass::bfloat16_t>>, Is_dropout=false, Is_causal=false]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(92): here
            instantiation of "void run_mha_fwd_hdim96<T>(Flash_fwd_params &, cudaStream_t) [with T=cutlass::bfloat16_t]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_hdim96_bf16_sm80.cu(16): here

/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_kernel.h(140): warning: variable "MMA_M" was declared but never referenced
          detected during:
            instantiation of "void flash::compute_attn_1rowblock<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params,Is_Q_split_K>(const Params &, int, int, int) [with Kernel_traits=Flash_fwd_kernel_traits<96, 128, 64, 4, false, false, cutlass::bfloat16_t, Flash_kernel_traits<96, 128, 64, 4, cutlass::bfloat16_t>>, Is_dropout=false, Is_causal=false, Is_even_N=false, Is_even_K=true, Return_softmax=false, Params=Flash_fwd_params, Is_Q_split_K=false]" 
(589): here
            instantiation of "void flash::compute_attn<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &) [with Kernel_traits=Flash_fwd_kernel_traits<96, 128, 64, 4, false, false, cutlass::bfloat16_t, Flash_kernel_traits<96, 128, 64, 4, cutlass::bfloat16_t>>, Is_dropout=false, Is_causal=false, Is_even_N=false, Is_even_K=true, Return_softmax=false, Params=Flash_fwd_params]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(15): here
            instantiation of "void flash_fwd_kernel<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax>(Flash_fwd_params) [with Kernel_traits=Flash_fwd_kernel_traits<96, 128, 64, 4, false, false, cutlass::bfloat16_t, Flash_kernel_traits<96, 128, 64, 4, cutlass::bfloat16_t>>, Is_dropout=false, Is_causal=false, Is_even_N=false, Is_even_K=true, Return_softmax=false]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(34): here
            instantiation of "void run_flash_fwd<Kernel_traits,Is_dropout,Is_causal>(Flash_fwd_params &, cudaStream_t) [with Kernel_traits=Flash_fwd_kernel_traits<96, 128, 64, 4, false, false, cutlass::bfloat16_t, Flash_kernel_traits<96, 128, 64, 4, cutlass::bfloat16_t>>, Is_dropout=false, Is_causal=false]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(92): here
            instantiation of "void run_mha_fwd_hdim96<T>(Flash_fwd_params &, cudaStream_t) [with T=cutlass::bfloat16_t]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_hdim96_bf16_sm80.cu(16): here

/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_kernel.h(276): warning: variable "tQrQ" was declared but never referenced
          detected during:
            instantiation of "void flash::compute_attn_1rowblock<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params,Is_Q_split_K>(const Params &, int, int, int) [with Kernel_traits=Flash_fwd_kernel_traits<96, 128, 64, 4, false, false, cutlass::bfloat16_t, Flash_kernel_traits<96, 128, 64, 4, cutlass::bfloat16_t>>, Is_dropout=false, Is_causal=false, Is_even_N=false, Is_even_K=true, Return_softmax=false, Params=Flash_fwd_params, Is_Q_split_K=false]" 
(589): here
            instantiation of "void flash::compute_attn<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &) [with Kernel_traits=Flash_fwd_kernel_traits<96, 128, 64, 4, false, false, cutlass::bfloat16_t, Flash_kernel_traits<96, 128, 64, 4, cutlass::bfloat16_t>>, Is_dropout=false, Is_causal=false, Is_even_N=false, Is_even_K=true, Return_softmax=false, Params=Flash_fwd_params]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(15): here
            instantiation of "void flash_fwd_kernel<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax>(Flash_fwd_params) [with Kernel_traits=Flash_fwd_kernel_traits<96, 128, 64, 4, false, false, cutlass::bfloat16_t, Flash_kernel_traits<96, 128, 64, 4, cutlass::bfloat16_t>>, Is_dropout=false, Is_causal=false, Is_even_N=false, Is_even_K=true, Return_softmax=false]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(34): here
            instantiation of "void run_flash_fwd<Kernel_traits,Is_dropout,Is_causal>(Flash_fwd_params &, cudaStream_t) [with Kernel_traits=Flash_fwd_kernel_traits<96, 128, 64, 4, false, false, cutlass::bfloat16_t, Flash_kernel_traits<96, 128, 64, 4, cutlass::bfloat16_t>>, Is_dropout=false, Is_causal=false]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(92): here
            instantiation of "void run_mha_fwd_hdim96<T>(Flash_fwd_params &, cudaStream_t) [with T=cutlass::bfloat16_t]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_hdim96_bf16_sm80.cu(16): here

/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_kernel.h(140): warning: variable "MMA_M" was declared but never referenced
          detected during:
            instantiation of "void flash::compute_attn_1rowblock<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params,Is_Q_split_K>(const Params &, int, int, int) [with Kernel_traits=Flash_fwd_kernel_traits<96, 128, 64, 4, false, false, cutlass::bfloat16_t, Flash_kernel_traits<96, 128, 64, 4, cutlass::bfloat16_t>>, Is_dropout=false, Is_causal=false, Is_even_N=false, Is_even_K=false, Return_softmax=false, Params=Flash_fwd_params, Is_Q_split_K=false]" 
(589): here
            instantiation of "void flash::compute_attn<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &) [with Kernel_traits=Flash_fwd_kernel_traits<96, 128, 64, 4, false, false, cutlass::bfloat16_t, Flash_kernel_traits<96, 128, 64, 4, cutlass::bfloat16_t>>, Is_dropout=false, Is_causal=false, Is_even_N=false, Is_even_K=false, Return_softmax=false, Params=Flash_fwd_params]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(15): here
            instantiation of "void flash_fwd_kernel<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax>(Flash_fwd_params) [with Kernel_traits=Flash_fwd_kernel_traits<96, 128, 64, 4, false, false, cutlass::bfloat16_t, Flash_kernel_traits<96, 128, 64, 4, cutlass::bfloat16_t>>, Is_dropout=false, Is_causal=false, Is_even_N=false, Is_even_K=false, Return_softmax=false]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(34): here
            instantiation of "void run_flash_fwd<Kernel_traits,Is_dropout,Is_causal>(Flash_fwd_params &, cudaStream_t) [with Kernel_traits=Flash_fwd_kernel_traits<96, 128, 64, 4, false, false, cutlass::bfloat16_t, Flash_kernel_traits<96, 128, 64, 4, cutlass::bfloat16_t>>, Is_dropout=false, Is_causal=false]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(92): here
            instantiation of "void run_mha_fwd_hdim96<T>(Flash_fwd_params &, cudaStream_t) [with T=cutlass::bfloat16_t]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_hdim96_bf16_sm80.cu(16): here

/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_kernel.h(276): warning: variable "tQrQ" was declared but never referenced
          detected during:
            instantiation of "void flash::compute_attn_1rowblock<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params,Is_Q_split_K>(const Params &, int, int, int) [with Kernel_traits=Flash_fwd_kernel_traits<96, 128, 64, 4, false, false, cutlass::bfloat16_t, Flash_kernel_traits<96, 128, 64, 4, cutlass::bfloat16_t>>, Is_dropout=false, Is_causal=false, Is_even_N=false, Is_even_K=false, Return_softmax=false, Params=Flash_fwd_params, Is_Q_split_K=false]" 
(589): here
            instantiation of "void flash::compute_attn<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &) [with Kernel_traits=Flash_fwd_kernel_traits<96, 128, 64, 4, false, false, cutlass::bfloat16_t, Flash_kernel_traits<96, 128, 64, 4, cutlass::bfloat16_t>>, Is_dropout=false, Is_causal=false, Is_even_N=false, Is_even_K=false, Return_softmax=false, Params=Flash_fwd_params]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(15): here
            instantiation of "void flash_fwd_kernel<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax>(Flash_fwd_params) [with Kernel_traits=Flash_fwd_kernel_traits<96, 128, 64, 4, false, false, cutlass::bfloat16_t, Flash_kernel_traits<96, 128, 64, 4, cutlass::bfloat16_t>>, Is_dropout=false, Is_causal=false, Is_even_N=false, Is_even_K=false, Return_softmax=false]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(34): here
            instantiation of "void run_flash_fwd<Kernel_traits,Is_dropout,Is_causal>(Flash_fwd_params &, cudaStream_t) [with Kernel_traits=Flash_fwd_kernel_traits<96, 128, 64, 4, false, false, cutlass::bfloat16_t, Flash_kernel_traits<96, 128, 64, 4, cutlass::bfloat16_t>>, Is_dropout=false, Is_causal=false]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(92): here
            instantiation of "void run_mha_fwd_hdim96<T>(Flash_fwd_params &, cudaStream_t) [with T=cutlass::bfloat16_t]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_hdim96_bf16_sm80.cu(16): here

ptxas info    : 2 bytes gmem
ptxas info    : Compiling entry function '_Z16flash_fwd_kernelI23Flash_fwd_kernel_traitsILi96ELi128ELi64ELi4ELb0ELb0EN7cutlass10bfloat16_tE19Flash_kernel_traitsILi96ELi128ELi64ELi4ES2_EELb0ELb0ELb0ELb0ELb0EEv16Flash_fwd_params' for 'sm_80'
ptxas info    : Function properties for _Z16flash_fwd_kernelI23Flash_fwd_kernel_traitsILi96ELi128ELi64ELi4ELb0ELb0EN7cutlass10bfloat16_tE19Flash_kernel_traitsILi96ELi128ELi64ELi4ES2_EELb0ELb0ELb0ELb0ELb0EEv16Flash_fwd_params
    0 bytes stack frame, 0 bytes spill stores, 0 bytes spill loads
ptxas info    : Used 240 registers, 584 bytes cmem[0]
ptxas info    : Compiling entry function '_Z16flash_fwd_kernelI23Flash_fwd_kernel_traitsILi96ELi128ELi64ELi4ELb0ELb0EN7cutlass10bfloat16_tE19Flash_kernel_traitsILi96ELi128ELi64ELi4ES2_EELb0ELb0ELb0ELb1ELb0EEv16Flash_fwd_params' for 'sm_80'
ptxas info    : Function properties for _Z16flash_fwd_kernelI23Flash_fwd_kernel_traitsILi96ELi128ELi64ELi4ELb0ELb0EN7cutlass10bfloat16_tE19Flash_kernel_traitsILi96ELi128ELi64ELi4ES2_EELb0ELb0ELb0ELb1ELb0EEv16Flash_fwd_params
    0 bytes stack frame, 0 bytes spill stores, 0 bytes spill loads
ptxas info    : Used 237 registers, 584 bytes cmem[0]
ptxas info    : Compiling entry function '_Z16flash_fwd_kernelI23Flash_fwd_kernel_traitsILi96ELi128ELi64ELi4ELb0ELb0EN7cutlass10bfloat16_tE19Flash_kernel_traitsILi96ELi128ELi64ELi4ES2_EELb0ELb0ELb1ELb0ELb0EEv16Flash_fwd_params' for 'sm_80'
ptxas info    : Function properties for _Z16flash_fwd_kernelI23Flash_fwd_kernel_traitsILi96ELi128ELi64ELi4ELb0ELb0EN7cutlass10bfloat16_tE19Flash_kernel_traitsILi96ELi128ELi64ELi4ES2_EELb0ELb0ELb1ELb0ELb0EEv16Flash_fwd_params
    0 bytes stack frame, 0 bytes spill stores, 0 bytes spill loads
ptxas info    : Used 239 registers, 584 bytes cmem[0]
ptxas info    : Compiling entry function '_Z16flash_fwd_kernelI23Flash_fwd_kernel_traitsILi96ELi128ELi64ELi4ELb0ELb0EN7cutlass10bfloat16_tE19Flash_kernel_traitsILi96ELi128ELi64ELi4ES2_EELb0ELb0ELb1ELb1ELb0EEv16Flash_fwd_params' for 'sm_80'
ptxas info    : Function properties for _Z16flash_fwd_kernelI23Flash_fwd_kernel_traitsILi96ELi128ELi64ELi4ELb0ELb0EN7cutlass10bfloat16_tE19Flash_kernel_traitsILi96ELi128ELi64ELi4ES2_EELb0ELb0ELb1ELb1ELb0EEv16Flash_fwd_params
    0 bytes stack frame, 0 bytes spill stores, 0 bytes spill loads
ptxas info    : Used 237 registers, 584 bytes cmem[0]
ptxas info    : Compiling entry function '_Z16flash_fwd_kernelI23Flash_fwd_kernel_traitsILi96ELi128ELi64ELi4ELb0ELb0EN7cutlass10bfloat16_tE19Flash_kernel_traitsILi96ELi128ELi64ELi4ES2_EELb0ELb1ELb0ELb0ELb0EEv16Flash_fwd_params' for 'sm_80'
ptxas info    : Function properties for _Z16flash_fwd_kernelI23Flash_fwd_kernel_traitsILi96ELi128ELi64ELi4ELb0ELb0EN7cutlass10bfloat16_tE19Flash_kernel_traitsILi96ELi128ELi64ELi4ES2_EELb0ELb1ELb0ELb0ELb0EEv16Flash_fwd_params
    0 bytes stack frame, 0 bytes spill stores, 0 bytes spill loads
ptxas info    : Used 255 registers, 584 bytes cmem[0]
ptxas info    : Compiling entry function '_Z16flash_fwd_kernelI23Flash_fwd_kernel_traitsILi96ELi128ELi64ELi4ELb0ELb0EN7cutlass10bfloat16_tE19Flash_kernel_traitsILi96ELi128ELi64ELi4ES2_EELb0ELb1ELb0ELb1ELb0EEv16Flash_fwd_params' for 'sm_80'
ptxas info    : Function properties for _Z16flash_fwd_kernelI23Flash_fwd_kernel_traitsILi96ELi128ELi64ELi4ELb0ELb0EN7cutlass10bfloat16_tE19Flash_kernel_traitsILi96ELi128ELi64ELi4ES2_EELb0ELb1ELb0ELb1ELb0EEv16Flash_fwd_params
    0 bytes stack frame, 0 bytes spill stores, 0 bytes spill loads
ptxas info    : Used 254 registers, 584 bytes cmem[0]
ptxas info    : Compiling entry function '_Z16flash_fwd_kernelI23Flash_fwd_kernel_traitsILi96ELi128ELi64ELi4ELb0ELb0EN7cutlass10bfloat16_tE19Flash_kernel_traitsILi96ELi128ELi64ELi4ES2_EELb0ELb1ELb1ELb0ELb0EEv16Flash_fwd_params' for 'sm_80'
ptxas info    : Function properties for _Z16flash_fwd_kernelI23Flash_fwd_kernel_traitsILi96ELi128ELi64ELi4ELb0ELb0EN7cutlass10bfloat16_tE19Flash_kernel_traitsILi96ELi128ELi64ELi4ES2_EELb0ELb1ELb1ELb0ELb0EEv16Flash_fwd_params
    0 bytes stack frame, 0 bytes spill stores, 0 bytes spill loads
ptxas info    : Used 254 registers, 584 bytes cmem[0]
ptxas info    : Compiling entry function '_Z16flash_fwd_kernelI23Flash_fwd_kernel_traitsILi96ELi128ELi64ELi4ELb0ELb0EN7cutlass10bfloat16_tE19Flash_kernel_traitsILi96ELi128ELi64ELi4ES2_EELb0ELb1ELb1ELb1ELb0EEv16Flash_fwd_params' for 'sm_80'
ptxas info    : Function properties for _Z16flash_fwd_kernelI23Flash_fwd_kernel_traitsILi96ELi128ELi64ELi4ELb0ELb0EN7cutlass10bfloat16_tE19Flash_kernel_traitsILi96ELi128ELi64ELi4ES2_EELb0ELb1ELb1ELb1ELb0EEv16Flash_fwd_params
    0 bytes stack frame, 0 bytes spill stores, 0 bytes spill loads
ptxas info    : Used 255 registers, 584 bytes cmem[0]
ptxas info    : Compiling entry function '_Z16flash_fwd_kernelI23Flash_fwd_kernel_traitsILi96ELi64ELi64ELi4ELb0ELb0EN7cutlass10bfloat16_tE19Flash_kernel_traitsILi96ELi64ELi64ELi4ES2_EELb0ELb1ELb0ELb0ELb0EEv16Flash_fwd_params' for 'sm_80'
ptxas info    : Function properties for _Z16flash_fwd_kernelI23Flash_fwd_kernel_traitsILi96ELi64ELi64ELi4ELb0ELb0EN7cutlass10bfloat16_tE19Flash_kernel_traitsILi96ELi64ELi64ELi4ES2_EELb0ELb1ELb0ELb0ELb0EEv16Flash_fwd_params
    0 bytes stack frame, 0 bytes spill stores, 0 bytes spill loads
ptxas info    : Used 133 registers, 584 bytes cmem[0]
ptxas info    : Compiling entry function '_Z16flash_fwd_kernelI23Flash_fwd_kernel_traitsILi96ELi64ELi64ELi4ELb0ELb0EN7cutlass10bfloat16_tE19Flash_kernel_traitsILi96ELi64ELi64ELi4ES2_EELb0ELb1ELb0ELb1ELb0EEv16Flash_fwd_params' for 'sm_80'
ptxas info    : Function properties for _Z16flash_fwd_kernelI23Flash_fwd_kernel_traitsILi96ELi64ELi64ELi4ELb0ELb0EN7cutlass10bfloat16_tE19Flash_kernel_traitsILi96ELi64ELi64ELi4ES2_EELb0ELb1ELb0ELb1ELb0EEv16Flash_fwd_params
    0 bytes stack frame, 0 bytes spill stores, 0 bytes spill loads
ptxas info    : Used 130 registers, 584 bytes cmem[0]
ptxas info    : Compiling entry function '_Z16flash_fwd_kernelI23Flash_fwd_kernel_traitsILi96ELi64ELi64ELi4ELb0ELb0EN7cutlass10bfloat16_tE19Flash_kernel_traitsILi96ELi64ELi64ELi4ES2_EELb0ELb1ELb1ELb0ELb0EEv16Flash_fwd_params' for 'sm_80'
ptxas info    : Function properties for _Z16flash_fwd_kernelI23Flash_fwd_kernel_traitsILi96ELi64ELi64ELi4ELb0ELb0EN7cutlass10bfloat16_tE19Flash_kernel_traitsILi96ELi64ELi64ELi4ES2_EELb0ELb1ELb1ELb0ELb0EEv16Flash_fwd_params
    0 bytes stack frame, 0 bytes spill stores, 0 bytes spill loads
ptxas info    : Used 130 registers, 584 bytes cmem[0]
ptxas info    : Compiling entry function '_Z16flash_fwd_kernelI23Flash_fwd_kernel_traitsILi96ELi64ELi64ELi4ELb0ELb0EN7cutlass10bfloat16_tE19Flash_kernel_traitsILi96ELi64ELi64ELi4ES2_EELb0ELb1ELb1ELb1ELb0EEv16Flash_fwd_params' for 'sm_80'
ptxas info    : Function properties for _Z16flash_fwd_kernelI23Flash_fwd_kernel_traitsILi96ELi64ELi64ELi4ELb0ELb0EN7cutlass10bfloat16_tE19Flash_kernel_traitsILi96ELi64ELi64ELi4ES2_EELb0ELb1ELb1ELb1ELb0EEv16Flash_fwd_params
    0 bytes stack frame, 0 bytes spill stores, 0 bytes spill loads
ptxas info    : Used 133 registers, 584 bytes cmem[0]
ptxas info    : Compiling entry function '_Z16flash_fwd_kernelI23Flash_fwd_kernel_traitsILi96ELi128ELi64ELi4ELb0ELb0EN7cutlass10bfloat16_tE19Flash_kernel_traitsILi96ELi128ELi64ELi4ES2_EELb1ELb0ELb0ELb0ELb0EEv16Flash_fwd_params' for 'sm_80'
ptxas info    : Function properties for _Z16flash_fwd_kernelI23Flash_fwd_kernel_traitsILi96ELi128ELi64ELi4ELb0ELb0EN7cutlass10bfloat16_tE19Flash_kernel_traitsILi96ELi128ELi64ELi4ES2_EELb1ELb0ELb0ELb0ELb0EEv16Flash_fwd_params
    16 bytes stack frame, 16 bytes spill stores, 16 bytes spill loads
ptxas info    : Used 255 registers, 584 bytes cmem[0]
ptxas info    : Compiling entry function '_Z16flash_fwd_kernelI23Flash_fwd_kernel_traitsILi96ELi128ELi64ELi4ELb0ELb0EN7cutlass10bfloat16_tE19Flash_kernel_traitsILi96ELi128ELi64ELi4ES2_EELb1ELb0ELb0ELb0ELb1EEv16Flash_fwd_params' for 'sm_80'
ptxas info    : Function properties for _Z16flash_fwd_kernelI23Flash_fwd_kernel_traitsILi96ELi128ELi64ELi4ELb0ELb0EN7cutlass10bfloat16_tE19Flash_kernel_traitsILi96ELi128ELi64ELi4ES2_EELb1ELb0ELb0ELb0ELb1EEv16Flash_fwd_params
    40 bytes stack frame, 36 bytes spill stores, 36 bytes spill loads
ptxas info    : Used 255 registers, 584 bytes cmem[0]
ptxas info    : Compiling entry function '_Z16flash_fwd_kernelI23Flash_fwd_kernel_traitsILi96ELi128ELi64ELi4ELb0ELb0EN7cutlass10bfloat16_tE19Flash_kernel_traitsILi96ELi128ELi64ELi4ES2_EELb1ELb0ELb0ELb1ELb0EEv16Flash_fwd_params' for 'sm_80'
ptxas info    : Function properties for _Z16flash_fwd_kernelI23Flash_fwd_kernel_traitsILi96ELi128ELi64ELi4ELb0ELb0EN7cutlass10bfloat16_tE19Flash_kernel_traitsILi96ELi128ELi64ELi4ES2_EELb1ELb0ELb0ELb1ELb0EEv16Flash_fwd_params
    0 bytes stack frame, 0 bytes spill stores, 0 bytes spill loads
ptxas info    : Used 255 registers, 584 bytes cmem[0]
ptxas info    : Compiling entry function '_Z16flash_fwd_kernelI23Flash_fwd_kernel_traitsILi96ELi128ELi64ELi4ELb0ELb0EN7cutlass10bfloat16_tE19Flash_kernel_traitsILi96ELi128ELi64ELi4ES2_EELb1ELb0ELb0ELb1ELb1EEv16Flash_fwd_params' for 'sm_80'
ptxas info    : Function properties for _Z16flash_fwd_kernelI23Flash_fwd_kernel_traitsILi96ELi128ELi64ELi4ELb0ELb0EN7cutlass10bfloat16_tE19Flash_kernel_traitsILi96ELi128ELi64ELi4ES2_EELb1ELb0ELb0ELb1ELb1EEv16Flash_fwd_params
    40 bytes stack frame, 36 bytes spill stores, 36 bytes spill loads
ptxas info    : Used 255 registers, 584 bytes cmem[0]
ptxas info    : Compiling entry function '_Z16flash_fwd_kernelI23Flash_fwd_kernel_traitsILi96ELi128ELi64ELi4ELb0ELb0EN7cutlass10bfloat16_tE19Flash_kernel_traitsILi96ELi128ELi64ELi4ES2_EELb1ELb0ELb1ELb0ELb0EEv16Flash_fwd_params' for 'sm_80'
ptxas info    : Function properties for _Z16flash_fwd_kernelI23Flash_fwd_kernel_traitsILi96ELi128ELi64ELi4ELb0ELb0EN7cutlass10bfloat16_tE19Flash_kernel_traitsILi96ELi128ELi64ELi4ES2_EELb1ELb0ELb1ELb0ELb0EEv16Flash_fwd_params
    0 bytes stack frame, 0 bytes spill stores, 0 bytes spill loads
ptxas info    : Used 255 registers, 584 bytes cmem[0]
ptxas info    : Compiling entry function '_Z16flash_fwd_kernelI23Flash_fwd_kernel_traitsILi96ELi128ELi64ELi4ELb0ELb0EN7cutlass10bfloat16_tE19Flash_kernel_traitsILi96ELi128ELi64ELi4ES2_EELb1ELb0ELb1ELb0ELb1EEv16Flash_fwd_params' for 'sm_80'
ptxas info    : Function properties for _Z16flash_fwd_kernelI23Flash_fwd_kernel_traitsILi96ELi128ELi64ELi4ELb0ELb0EN7cutlass10bfloat16_tE19Flash_kernel_traitsILi96ELi128ELi64ELi4ES2_EELb1ELb0ELb1ELb0ELb1EEv16Flash_fwd_params
    8 bytes stack frame, 8 bytes spill stores, 8 bytes spill loads
ptxas info    : Used 255 registers, 584 bytes cmem[0]
ptxas info    : Compiling entry function '_Z16flash_fwd_kernelI23Flash_fwd_kernel_traitsILi96ELi128ELi64ELi4ELb0ELb0EN7cutlass10bfloat16_tE19Flash_kernel_traitsILi96ELi128ELi64ELi4ES2_EELb1ELb0ELb1ELb1ELb0EEv16Flash_fwd_params' for 'sm_80'
ptxas info    : Function properties for _Z16flash_fwd_kernelI23Flash_fwd_kernel_traitsILi96ELi128ELi64ELi4ELb0ELb0EN7cutlass10bfloat16_tE19Flash_kernel_traitsILi96ELi128ELi64ELi4ES2_EELb1ELb0ELb1ELb1ELb0EEv16Flash_fwd_params
    0 bytes stack frame, 0 bytes spill stores, 0 bytes spill loads
ptxas info    : Used 255 registers, 584 bytes cmem[0]
ptxas info    : Compiling entry function '_Z16flash_fwd_kernelI23Flash_fwd_kernel_traitsILi96ELi128ELi64ELi4ELb0ELb0EN7cutlass10bfloat16_tE19Flash_kernel_traitsILi96ELi128ELi64ELi4ES2_EELb1ELb0ELb1ELb1ELb1EEv16Flash_fwd_params' for 'sm_80'
ptxas info    : Function properties for _Z16flash_fwd_kernelI23Flash_fwd_kernel_traitsILi96ELi128ELi64ELi4ELb0ELb0EN7cutlass10bfloat16_tE19Flash_kernel_traitsILi96ELi128ELi64ELi4ES2_EELb1ELb0ELb1ELb1ELb1EEv16Flash_fwd_params
    16 bytes stack frame, 24 bytes spill stores, 16 bytes spill loads
ptxas info    : Used 255 registers, 584 bytes cmem[0]
ptxas info    : Compiling entry function '_Z16flash_fwd_kernelI23Flash_fwd_kernel_traitsILi96ELi128ELi64ELi4ELb0ELb0EN7cutlass10bfloat16_tE19Flash_kernel_traitsILi96ELi128ELi64ELi4ES2_EELb1ELb1ELb0ELb0ELb0EEv16Flash_fwd_params' for 'sm_80'
ptxas info    : Function properties for _Z16flash_fwd_kernelI23Flash_fwd_kernel_traitsILi96ELi128ELi64ELi4ELb0ELb0EN7cutlass10bfloat16_tE19Flash_kernel_traitsILi96ELi128ELi64ELi4ES2_EELb1ELb1ELb0ELb0ELb0EEv16Flash_fwd_params
    48 bytes stack frame, 44 bytes spill stores, 44 bytes spill loads
ptxas info    : Used 255 registers, 584 bytes cmem[0]
ptxas info    : Compiling entry function '_Z16flash_fwd_kernelI23Flash_fwd_kernel_traitsILi96ELi128ELi64ELi4ELb0ELb0EN7cutlass10bfloat16_tE19Flash_kernel_traitsILi96ELi128ELi64ELi4ES2_EELb1ELb1ELb0ELb0ELb1EEv16Flash_fwd_params' for 'sm_80'
ptxas info    : Function properties for _Z16flash_fwd_kernelI23Flash_fwd_kernel_traitsILi96ELi128ELi64ELi4ELb0ELb0EN7cutlass10bfloat16_tE19Flash_kernel_traitsILi96ELi128ELi64ELi4ES2_EELb1ELb1ELb0ELb0ELb1EEv16Flash_fwd_params
    120 bytes stack frame, 140 bytes spill stores, 140 bytes spill loads
ptxas info    : Used 255 registers, 584 bytes cmem[0]
ptxas info    : Compiling entry function '_Z16flash_fwd_kernelI23Flash_fwd_kernel_traitsILi96ELi128ELi64ELi4ELb0ELb0EN7cutlass10bfloat16_tE19Flash_kernel_traitsILi96ELi128ELi64ELi4ES2_EELb1ELb1ELb0ELb1ELb0EEv16Flash_fwd_params' for 'sm_80'
ptxas info    : Function properties for _Z16flash_fwd_kernelI23Flash_fwd_kernel_traitsILi96ELi128ELi64ELi4ELb0ELb0EN7cutlass10bfloat16_tE19Flash_kernel_traitsILi96ELi128ELi64ELi4ES2_EELb1ELb1ELb0ELb1ELb0EEv16Flash_fwd_params
    8 bytes stack frame, 8 bytes spill stores, 8 bytes spill loads
ptxas info    : Used 255 registers, 584 bytes cmem[0]
ptxas info    : Compiling entry function '_Z16flash_fwd_kernelI23Flash_fwd_kernel_traitsILi96ELi128ELi64ELi4ELb0ELb0EN7cutlass10bfloat16_tE19Flash_kernel_traitsILi96ELi128ELi64ELi4ES2_EELb1ELb1ELb0ELb1ELb1EEv16Flash_fwd_params' for 'sm_80'
ptxas info    : Function properties for _Z16flash_fwd_kernelI23Flash_fwd_kernel_traitsILi96ELi128ELi64ELi4ELb0ELb0EN7cutlass10bfloat16_tE19Flash_kernel_traitsILi96ELi128ELi64ELi4ES2_EELb1ELb1ELb0ELb1ELb1EEv16Flash_fwd_params
    144 bytes stack frame, 164 bytes spill stores, 168 bytes spill loads
ptxas info    : Used 255 registers, 584 bytes cmem[0]
ptxas info    : Compiling entry function '_Z16flash_fwd_kernelI23Flash_fwd_kernel_traitsILi96ELi128ELi64ELi4ELb0ELb0EN7cutlass10bfloat16_tE19Flash_kernel_traitsILi96ELi128ELi64ELi4ES2_EELb1ELb1ELb1ELb0ELb0EEv16Flash_fwd_params' for 'sm_80'
ptxas info    : Function properties for _Z16flash_fwd_kernelI23Flash_fwd_kernel_traitsILi96ELi128ELi64ELi4ELb0ELb0EN7cutlass10bfloat16_tE19Flash_kernel_traitsILi96ELi128ELi64ELi4ES2_EELb1ELb1ELb1ELb0ELb0EEv16Flash_fwd_params
    40 bytes stack frame, 32 bytes spill stores, 32 bytes spill loads
ptxas info    : Used 255 registers, 584 bytes cmem[0]
ptxas info    : Compiling entry function '_Z16flash_fwd_kernelI23Flash_fwd_kernel_traitsILi96ELi128ELi64ELi4ELb0ELb0EN7cutlass10bfloat16_tE19Flash_kernel_traitsILi96ELi128ELi64ELi4ES2_EELb1ELb1ELb1ELb0ELb1EEv16Flash_fwd_params' for 'sm_80'
ptxas info    : Function properties for _Z16flash_fwd_kernelI23Flash_fwd_kernel_traitsILi96ELi128ELi64ELi4ELb0ELb0EN7cutlass10bfloat16_tE19Flash_kernel_traitsILi96ELi128ELi64ELi4ES2_EELb1ELb1ELb1ELb0ELb1EEv16Flash_fwd_params
    64 bytes stack frame, 96 bytes spill stores, 88 bytes spill loads
ptxas info    : Used 255 registers, 584 bytes cmem[0]
ptxas info    : Compiling entry function '_Z16flash_fwd_kernelI23Flash_fwd_kernel_traitsILi96ELi128ELi64ELi4ELb0ELb0EN7cutlass10bfloat16_tE19Flash_kernel_traitsILi96ELi128ELi64ELi4ES2_EELb1ELb1ELb1ELb1ELb0EEv16Flash_fwd_params' for 'sm_80'
ptxas info    : Function properties for _Z16flash_fwd_kernelI23Flash_fwd_kernel_traitsILi96ELi128ELi64ELi4ELb0ELb0EN7cutlass10bfloat16_tE19Flash_kernel_traitsILi96ELi128ELi64ELi4ES2_EELb1ELb1ELb1ELb1ELb0EEv16Flash_fwd_params
    40 bytes stack frame, 44 bytes spill stores, 40 bytes spill loads
ptxas info    : Used 255 registers, 584 bytes cmem[0]
ptxas info    : Compiling entry function '_Z16flash_fwd_kernelI23Flash_fwd_kernel_traitsILi96ELi128ELi64ELi4ELb0ELb0EN7cutlass10bfloat16_tE19Flash_kernel_traitsILi96ELi128ELi64ELi4ES2_EELb1ELb1ELb1ELb1ELb1EEv16Flash_fwd_params' for 'sm_80'
ptxas info    : Function properties for _Z16flash_fwd_kernelI23Flash_fwd_kernel_traitsILi96ELi128ELi64ELi4ELb0ELb0EN7cutlass10bfloat16_tE19Flash_kernel_traitsILi96ELi128ELi64ELi4ES2_EELb1ELb1ELb1ELb1ELb1EEv16Flash_fwd_params
    24 bytes stack frame, 52 bytes spill stores, 36 bytes spill loads
ptxas info    : Used 255 registers, 584 bytes cmem[0]
ptxas info    : Compiling entry function '_Z16flash_fwd_kernelI23Flash_fwd_kernel_traitsILi96ELi64ELi64ELi4ELb0ELb0EN7cutlass10bfloat16_tE19Flash_kernel_traitsILi96ELi64ELi64ELi4ES2_EELb1ELb1ELb0ELb0ELb0EEv16Flash_fwd_params' for 'sm_80'
ptxas info    : Function properties for _Z16flash_fwd_kernelI23Flash_fwd_kernel_traitsILi96ELi64ELi64ELi4ELb0ELb0EN7cutlass10bfloat16_tE19Flash_kernel_traitsILi96ELi64ELi64ELi4ES2_EELb1ELb1ELb0ELb0ELb0EEv16Flash_fwd_params
    0 bytes stack frame, 0 bytes spill stores, 0 bytes spill loads
ptxas info    : Used 146 registers, 584 bytes cmem[0]
ptxas info    : Compiling entry function '_Z16flash_fwd_kernelI23Flash_fwd_kernel_traitsILi96ELi64ELi64ELi4ELb0ELb0EN7cutlass10bfloat16_tE19Flash_kernel_traitsILi96ELi64ELi64ELi4ES2_EELb1ELb1ELb0ELb0ELb1EEv16Flash_fwd_params' for 'sm_80'
ptxas info    : Function properties for _Z16flash_fwd_kernelI23Flash_fwd_kernel_traitsILi96ELi64ELi64ELi4ELb0ELb0EN7cutlass10bfloat16_tE19Flash_kernel_traitsILi96ELi64ELi64ELi4ES2_EELb1ELb1ELb0ELb0ELb1EEv16Flash_fwd_params
    0 bytes stack frame, 0 bytes spill stores, 0 bytes spill loads
ptxas info    : Used 174 registers, 584 bytes cmem[0]
ptxas info    : Compiling entry function '_Z16flash_fwd_kernelI23Flash_fwd_kernel_traitsILi96ELi64ELi64ELi4ELb0ELb0EN7cutlass10bfloat16_tE19Flash_kernel_traitsILi96ELi64ELi64ELi4ES2_EELb1ELb1ELb0ELb1ELb0EEv16Flash_fwd_params' for 'sm_80'
ptxas info    : Function properties for _Z16flash_fwd_kernelI23Flash_fwd_kernel_traitsILi96ELi64ELi64ELi4ELb0ELb0EN7cutlass10bfloat16_tE19Flash_kernel_traitsILi96ELi64ELi64ELi4ES2_EELb1ELb1ELb0ELb1ELb0EEv16Flash_fwd_params
    0 bytes stack frame, 0 bytes spill stores, 0 bytes spill loads
ptxas info    : Used 140 registers, 584 bytes cmem[0]
ptxas info    : Compiling entry function '_Z16flash_fwd_kernelI23Flash_fwd_kernel_traitsILi96ELi64ELi64ELi4ELb0ELb0EN7cutlass10bfloat16_tE19Flash_kernel_traitsILi96ELi64ELi64ELi4ES2_EELb1ELb1ELb0ELb1ELb1EEv16Flash_fwd_params' for 'sm_80'
ptxas info    : Function properties for _Z16flash_fwd_kernelI23Flash_fwd_kernel_traitsILi96ELi64ELi64ELi4ELb0ELb0EN7cutlass10bfloat16_tE19Flash_kernel_traitsILi96ELi64ELi64ELi4ES2_EELb1ELb1ELb0ELb1ELb1EEv16Flash_fwd_params
    0 bytes stack frame, 0 bytes spill stores, 0 bytes spill loads
ptxas info    : Used 138 registers, 584 bytes cmem[0]
ptxas info    : Compiling entry function '_Z16flash_fwd_kernelI23Flash_fwd_kernel_traitsILi96ELi64ELi64ELi4ELb0ELb0EN7cutlass10bfloat16_tE19Flash_kernel_traitsILi96ELi64ELi64ELi4ES2_EELb1ELb1ELb1ELb0ELb0EEv16Flash_fwd_params' for 'sm_80'
ptxas info    : Function properties for _Z16flash_fwd_kernelI23Flash_fwd_kernel_traitsILi96ELi64ELi64ELi4ELb0ELb0EN7cutlass10bfloat16_tE19Flash_kernel_traitsILi96ELi64ELi64ELi4ES2_EELb1ELb1ELb1ELb0ELb0EEv16Flash_fwd_params
    0 bytes stack frame, 0 bytes spill stores, 0 bytes spill loads
ptxas info    : Used 148 registers, 584 bytes cmem[0]
ptxas info    : Compiling entry function '_Z16flash_fwd_kernelI23Flash_fwd_kernel_traitsILi96ELi64ELi64ELi4ELb0ELb0EN7cutlass10bfloat16_tE19Flash_kernel_traitsILi96ELi64ELi64ELi4ES2_EELb1ELb1ELb1ELb0ELb1EEv16Flash_fwd_params' for 'sm_80'
ptxas info    : Function properties for _Z16flash_fwd_kernelI23Flash_fwd_kernel_traitsILi96ELi64ELi64ELi4ELb0ELb0EN7cutlass10bfloat16_tE19Flash_kernel_traitsILi96ELi64ELi64ELi4ES2_EELb1ELb1ELb1ELb0ELb1EEv16Flash_fwd_params
    0 bytes stack frame, 0 bytes spill stores, 0 bytes spill loads
ptxas info    : Used 134 registers, 584 bytes cmem[0]
ptxas info    : Compiling entry function '_Z16flash_fwd_kernelI23Flash_fwd_kernel_traitsILi96ELi64ELi64ELi4ELb0ELb0EN7cutlass10bfloat16_tE19Flash_kernel_traitsILi96ELi64ELi64ELi4ES2_EELb1ELb1ELb1ELb1ELb0EEv16Flash_fwd_params' for 'sm_80'
ptxas info    : Function properties for _Z16flash_fwd_kernelI23Flash_fwd_kernel_traitsILi96ELi64ELi64ELi4ELb0ELb0EN7cutlass10bfloat16_tE19Flash_kernel_traitsILi96ELi64ELi64ELi4ES2_EELb1ELb1ELb1ELb1ELb0EEv16Flash_fwd_params
    0 bytes stack frame, 0 bytes spill stores, 0 bytes spill loads
ptxas info    : Used 142 registers, 584 bytes cmem[0]
ptxas info    : Compiling entry function '_Z16flash_fwd_kernelI23Flash_fwd_kernel_traitsILi96ELi64ELi64ELi4ELb0ELb0EN7cutlass10bfloat16_tE19Flash_kernel_traitsILi96ELi64ELi64ELi4ES2_EELb1ELb1ELb1ELb1ELb1EEv16Flash_fwd_params' for 'sm_80'
ptxas info    : Function properties for _Z16flash_fwd_kernelI23Flash_fwd_kernel_traitsILi96ELi64ELi64ELi4ELb0ELb0EN7cutlass10bfloat16_tE19Flash_kernel_traitsILi96ELi64ELi64ELi4ES2_EELb1ELb1ELb1ELb1ELb1EEv16Flash_fwd_params
    0 bytes stack frame, 0 bytes spill stores, 0 bytes spill loads
ptxas info    : Used 136 registers, 584 bytes cmem[0]
/anaconda/envs/flashattention/lib/python3.9/site-packages/torch/include/c10/core/SymInt.h(84): warning: integer conversion resulted in a change of sign

[32/37] /usr/local/cuda-11.4/bin/nvcc  -I/home/v-feiychen/flash-attention/csrc/flash_attn -I/home/v-feiychen/flash-attention/csrc/flash_attn/src -I/home/v-feiychen/flash-attention/csrc/cutlass/include -I/anaconda/envs/flashattention/lib/python3.9/site-packages/torch/include -I/anaconda/envs/flashattention/lib/python3.9/site-packages/torch/include/torch/csrc/api/include -I/anaconda/envs/flashattention/lib/python3.9/site-packages/torch/include/TH -I/anaconda/envs/flashattention/lib/python3.9/site-packages/torch/include/THC -I/usr/local/cuda-11.4/include -I/anaconda/envs/flashattention/include/python3.9 -c -c /home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_hdim96_fp16_sm80.cu -o /home/v-feiychen/flash-attention/build/temp.linux-x86_64-cpython-39/csrc/flash_attn/src/flash_fwd_hdim96_fp16_sm80.o -D__CUDA_NO_HALF_OPERATORS__ -D__CUDA_NO_HALF_CONVERSIONS__ -D__CUDA_NO_BFLOAT16_CONVERSIONS__ -D__CUDA_NO_HALF2_OPERATORS__ --expt-relaxed-constexpr --compiler-options ''"'"'-fPIC'"'"'' -O3 -std=c++17 -U__CUDA_NO_HALF_OPERATORS__ -U__CUDA_NO_HALF_CONVERSIONS__ -U__CUDA_NO_HALF2_OPERATORS__ -U__CUDA_NO_BFLOAT16_CONVERSIONS__ --expt-relaxed-constexpr --expt-extended-lambda --use_fast_math --ptxas-options=-v -lineinfo -gencode arch=compute_80,code=sm_80 --threads 4 -DTORCH_API_INCLUDE_EXTENSION_H '-DPYBIND11_COMPILER_TYPE="_gcc"' '-DPYBIND11_STDLIB="_libstdcpp"' '-DPYBIND11_BUILD_ABI="_cxxabi1011"' -DTORCH_EXTENSION_NAME=flash_attn_2_cuda -D_GLIBCXX_USE_CXX11_ABI=0
/anaconda/envs/flashattention/lib/python3.9/site-packages/torch/include/c10/core/SymInt.h(84): warning: integer conversion resulted in a change of sign

/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_kernel.h(140): warning: variable "MMA_M" was declared but never referenced
          detected during:
            instantiation of "void flash::compute_attn_1rowblock<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params,Is_Q_split_K>(const Params &, int, int, int) [with Kernel_traits=Flash_fwd_kernel_traits<96, 64, 64, 4, false, false, cutlass::half_t, Flash_kernel_traits<96, 64, 64, 4, cutlass::half_t>>, Is_dropout=true, Is_causal=true, Is_even_N=true, Is_even_K=true, Return_softmax=true, Params=Flash_fwd_params, Is_Q_split_K=false]" 
(589): here
            instantiation of "void flash::compute_attn<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &) [with Kernel_traits=Flash_fwd_kernel_traits<96, 64, 64, 4, false, false, cutlass::half_t, Flash_kernel_traits<96, 64, 64, 4, cutlass::half_t>>, Is_dropout=true, Is_causal=true, Is_even_N=true, Is_even_K=true, Return_softmax=true, Params=Flash_fwd_params]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(15): here
            instantiation of "void flash_fwd_kernel<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax>(Flash_fwd_params) [with Kernel_traits=Flash_fwd_kernel_traits<96, 64, 64, 4, false, false, cutlass::half_t, Flash_kernel_traits<96, 64, 64, 4, cutlass::half_t>>, Is_dropout=true, Is_causal=true, Is_even_N=true, Is_even_K=true, Return_softmax=true]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(34): here
            instantiation of "void run_flash_fwd<Kernel_traits,Is_dropout,Is_causal>(Flash_fwd_params &, cudaStream_t) [with Kernel_traits=Flash_fwd_kernel_traits<96, 64, 64, 4, false, false, cutlass::half_t, Flash_kernel_traits<96, 64, 64, 4, cutlass::half_t>>, Is_dropout=true, Is_causal=true]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(92): here
            instantiation of "void run_mha_fwd_hdim96<T>(Flash_fwd_params &, cudaStream_t) [with T=cutlass::half_t]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_hdim96_fp16_sm80.cu(22): here

/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_kernel.h(276): warning: variable "tQrQ" was declared but never referenced
          detected during:
            instantiation of "void flash::compute_attn_1rowblock<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params,Is_Q_split_K>(const Params &, int, int, int) [with Kernel_traits=Flash_fwd_kernel_traits<96, 64, 64, 4, false, false, cutlass::half_t, Flash_kernel_traits<96, 64, 64, 4, cutlass::half_t>>, Is_dropout=true, Is_causal=true, Is_even_N=true, Is_even_K=true, Return_softmax=true, Params=Flash_fwd_params, Is_Q_split_K=false]" 
(589): here
            instantiation of "void flash::compute_attn<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &) [with Kernel_traits=Flash_fwd_kernel_traits<96, 64, 64, 4, false, false, cutlass::half_t, Flash_kernel_traits<96, 64, 64, 4, cutlass::half_t>>, Is_dropout=true, Is_causal=true, Is_even_N=true, Is_even_K=true, Return_softmax=true, Params=Flash_fwd_params]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(15): here
            instantiation of "void flash_fwd_kernel<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax>(Flash_fwd_params) [with Kernel_traits=Flash_fwd_kernel_traits<96, 64, 64, 4, false, false, cutlass::half_t, Flash_kernel_traits<96, 64, 64, 4, cutlass::half_t>>, Is_dropout=true, Is_causal=true, Is_even_N=true, Is_even_K=true, Return_softmax=true]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(34): here
            instantiation of "void run_flash_fwd<Kernel_traits,Is_dropout,Is_causal>(Flash_fwd_params &, cudaStream_t) [with Kernel_traits=Flash_fwd_kernel_traits<96, 64, 64, 4, false, false, cutlass::half_t, Flash_kernel_traits<96, 64, 64, 4, cutlass::half_t>>, Is_dropout=true, Is_causal=true]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(92): here
            instantiation of "void run_mha_fwd_hdim96<T>(Flash_fwd_params &, cudaStream_t) [with T=cutlass::half_t]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_hdim96_fp16_sm80.cu(22): here

/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_kernel.h(140): warning: variable "MMA_M" was declared but never referenced
          detected during:
            instantiation of "void flash::compute_attn_1rowblock<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params,Is_Q_split_K>(const Params &, int, int, int) [with Kernel_traits=Flash_fwd_kernel_traits<96, 64, 64, 4, false, false, cutlass::half_t, Flash_kernel_traits<96, 64, 64, 4, cutlass::half_t>>, Is_dropout=true, Is_causal=true, Is_even_N=true, Is_even_K=true, Return_softmax=false, Params=Flash_fwd_params, Is_Q_split_K=false]" 
(589): here
            instantiation of "void flash::compute_attn<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &) [with Kernel_traits=Flash_fwd_kernel_traits<96, 64, 64, 4, false, false, cutlass::half_t, Flash_kernel_traits<96, 64, 64, 4, cutlass::half_t>>, Is_dropout=true, Is_causal=true, Is_even_N=true, Is_even_K=true, Return_softmax=false, Params=Flash_fwd_params]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(15): here
            instantiation of "void flash_fwd_kernel<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax>(Flash_fwd_params) [with Kernel_traits=Flash_fwd_kernel_traits<96, 64, 64, 4, false, false, cutlass::half_t, Flash_kernel_traits<96, 64, 64, 4, cutlass::half_t>>, Is_dropout=true, Is_causal=true, Is_even_N=true, Is_even_K=true, Return_softmax=false]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(34): here
            instantiation of "void run_flash_fwd<Kernel_traits,Is_dropout,Is_causal>(Flash_fwd_params &, cudaStream_t) [with Kernel_traits=Flash_fwd_kernel_traits<96, 64, 64, 4, false, false, cutlass::half_t, Flash_kernel_traits<96, 64, 64, 4, cutlass::half_t>>, Is_dropout=true, Is_causal=true]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(92): here
            instantiation of "void run_mha_fwd_hdim96<T>(Flash_fwd_params &, cudaStream_t) [with T=cutlass::half_t]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_hdim96_fp16_sm80.cu(22): here

/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_kernel.h(276): warning: variable "tQrQ" was declared but never referenced
          detected during:
            instantiation of "void flash::compute_attn_1rowblock<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params,Is_Q_split_K>(const Params &, int, int, int) [with Kernel_traits=Flash_fwd_kernel_traits<96, 64, 64, 4, false, false, cutlass::half_t, Flash_kernel_traits<96, 64, 64, 4, cutlass::half_t>>, Is_dropout=true, Is_causal=true, Is_even_N=true, Is_even_K=true, Return_softmax=false, Params=Flash_fwd_params, Is_Q_split_K=false]" 
(589): here
            instantiation of "void flash::compute_attn<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &) [with Kernel_traits=Flash_fwd_kernel_traits<96, 64, 64, 4, false, false, cutlass::half_t, Flash_kernel_traits<96, 64, 64, 4, cutlass::half_t>>, Is_dropout=true, Is_causal=true, Is_even_N=true, Is_even_K=true, Return_softmax=false, Params=Flash_fwd_params]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(15): here
            instantiation of "void flash_fwd_kernel<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax>(Flash_fwd_params) [with Kernel_traits=Flash_fwd_kernel_traits<96, 64, 64, 4, false, false, cutlass::half_t, Flash_kernel_traits<96, 64, 64, 4, cutlass::half_t>>, Is_dropout=true, Is_causal=true, Is_even_N=true, Is_even_K=true, Return_softmax=false]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(34): here
            instantiation of "void run_flash_fwd<Kernel_traits,Is_dropout,Is_causal>(Flash_fwd_params &, cudaStream_t) [with Kernel_traits=Flash_fwd_kernel_traits<96, 64, 64, 4, false, false, cutlass::half_t, Flash_kernel_traits<96, 64, 64, 4, cutlass::half_t>>, Is_dropout=true, Is_causal=true]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(92): here
            instantiation of "void run_mha_fwd_hdim96<T>(Flash_fwd_params &, cudaStream_t) [with T=cutlass::half_t]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_hdim96_fp16_sm80.cu(22): here

/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_kernel.h(140): warning: variable "MMA_M" was declared but never referenced
          detected during:
            instantiation of "void flash::compute_attn_1rowblock<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params,Is_Q_split_K>(const Params &, int, int, int) [with Kernel_traits=Flash_fwd_kernel_traits<96, 64, 64, 4, false, false, cutlass::half_t, Flash_kernel_traits<96, 64, 64, 4, cutlass::half_t>>, Is_dropout=true, Is_causal=true, Is_even_N=true, Is_even_K=false, Return_softmax=true, Params=Flash_fwd_params, Is_Q_split_K=false]" 
(589): here
            instantiation of "void flash::compute_attn<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &) [with Kernel_traits=Flash_fwd_kernel_traits<96, 64, 64, 4, false, false, cutlass::half_t, Flash_kernel_traits<96, 64, 64, 4, cutlass::half_t>>, Is_dropout=true, Is_causal=true, Is_even_N=true, Is_even_K=false, Return_softmax=true, Params=Flash_fwd_params]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(15): here
            instantiation of "void flash_fwd_kernel<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax>(Flash_fwd_params) [with Kernel_traits=Flash_fwd_kernel_traits<96, 64, 64, 4, false, false, cutlass::half_t, Flash_kernel_traits<96, 64, 64, 4, cutlass::half_t>>, Is_dropout=true, Is_causal=true, Is_even_N=true, Is_even_K=false, Return_softmax=true]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(34): here
            instantiation of "void run_flash_fwd<Kernel_traits,Is_dropout,Is_causal>(Flash_fwd_params &, cudaStream_t) [with Kernel_traits=Flash_fwd_kernel_traits<96, 64, 64, 4, false, false, cutlass::half_t, Flash_kernel_traits<96, 64, 64, 4, cutlass::half_t>>, Is_dropout=true, Is_causal=true]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(92): here
            instantiation of "void run_mha_fwd_hdim96<T>(Flash_fwd_params &, cudaStream_t) [with T=cutlass::half_t]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_hdim96_fp16_sm80.cu(22): here

/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_kernel.h(276): warning: variable "tQrQ" was declared but never referenced
          detected during:
            instantiation of "void flash::compute_attn_1rowblock<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params,Is_Q_split_K>(const Params &, int, int, int) [with Kernel_traits=Flash_fwd_kernel_traits<96, 64, 64, 4, false, false, cutlass::half_t, Flash_kernel_traits<96, 64, 64, 4, cutlass::half_t>>, Is_dropout=true, Is_causal=true, Is_even_N=true, Is_even_K=false, Return_softmax=true, Params=Flash_fwd_params, Is_Q_split_K=false]" 
(589): here
            instantiation of "void flash::compute_attn<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &) [with Kernel_traits=Flash_fwd_kernel_traits<96, 64, 64, 4, false, false, cutlass::half_t, Flash_kernel_traits<96, 64, 64, 4, cutlass::half_t>>, Is_dropout=true, Is_causal=true, Is_even_N=true, Is_even_K=false, Return_softmax=true, Params=Flash_fwd_params]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(15): here
            instantiation of "void flash_fwd_kernel<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax>(Flash_fwd_params) [with Kernel_traits=Flash_fwd_kernel_traits<96, 64, 64, 4, false, false, cutlass::half_t, Flash_kernel_traits<96, 64, 64, 4, cutlass::half_t>>, Is_dropout=true, Is_causal=true, Is_even_N=true, Is_even_K=false, Return_softmax=true]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(34): here
            instantiation of "void run_flash_fwd<Kernel_traits,Is_dropout,Is_causal>(Flash_fwd_params &, cudaStream_t) [with Kernel_traits=Flash_fwd_kernel_traits<96, 64, 64, 4, false, false, cutlass::half_t, Flash_kernel_traits<96, 64, 64, 4, cutlass::half_t>>, Is_dropout=true, Is_causal=true]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(92): here
            instantiation of "void run_mha_fwd_hdim96<T>(Flash_fwd_params &, cudaStream_t) [with T=cutlass::half_t]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_hdim96_fp16_sm80.cu(22): here

/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_kernel.h(140): warning: variable "MMA_M" was declared but never referenced
          detected during:
            instantiation of "void flash::compute_attn_1rowblock<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params,Is_Q_split_K>(const Params &, int, int, int) [with Kernel_traits=Flash_fwd_kernel_traits<96, 64, 64, 4, false, false, cutlass::half_t, Flash_kernel_traits<96, 64, 64, 4, cutlass::half_t>>, Is_dropout=true, Is_causal=true, Is_even_N=true, Is_even_K=false, Return_softmax=false, Params=Flash_fwd_params, Is_Q_split_K=false]" 
(589): here
            instantiation of "void flash::compute_attn<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &) [with Kernel_traits=Flash_fwd_kernel_traits<96, 64, 64, 4, false, false, cutlass::half_t, Flash_kernel_traits<96, 64, 64, 4, cutlass::half_t>>, Is_dropout=true, Is_causal=true, Is_even_N=true, Is_even_K=false, Return_softmax=false, Params=Flash_fwd_params]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(15): here
            instantiation of "void flash_fwd_kernel<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax>(Flash_fwd_params) [with Kernel_traits=Flash_fwd_kernel_traits<96, 64, 64, 4, false, false, cutlass::half_t, Flash_kernel_traits<96, 64, 64, 4, cutlass::half_t>>, Is_dropout=true, Is_causal=true, Is_even_N=true, Is_even_K=false, Return_softmax=false]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(34): here
            instantiation of "void run_flash_fwd<Kernel_traits,Is_dropout,Is_causal>(Flash_fwd_params &, cudaStream_t) [with Kernel_traits=Flash_fwd_kernel_traits<96, 64, 64, 4, false, false, cutlass::half_t, Flash_kernel_traits<96, 64, 64, 4, cutlass::half_t>>, Is_dropout=true, Is_causal=true]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(92): here
            instantiation of "void run_mha_fwd_hdim96<T>(Flash_fwd_params &, cudaStream_t) [with T=cutlass::half_t]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_hdim96_fp16_sm80.cu(22): here

/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_kernel.h(276): warning: variable "tQrQ" was declared but never referenced
          detected during:
            instantiation of "void flash::compute_attn_1rowblock<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params,Is_Q_split_K>(const Params &, int, int, int) [with Kernel_traits=Flash_fwd_kernel_traits<96, 64, 64, 4, false, false, cutlass::half_t, Flash_kernel_traits<96, 64, 64, 4, cutlass::half_t>>, Is_dropout=true, Is_causal=true, Is_even_N=true, Is_even_K=false, Return_softmax=false, Params=Flash_fwd_params, Is_Q_split_K=false]" 
(589): here
            instantiation of "void flash::compute_attn<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &) [with Kernel_traits=Flash_fwd_kernel_traits<96, 64, 64, 4, false, false, cutlass::half_t, Flash_kernel_traits<96, 64, 64, 4, cutlass::half_t>>, Is_dropout=true, Is_causal=true, Is_even_N=true, Is_even_K=false, Return_softmax=false, Params=Flash_fwd_params]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(15): here
            instantiation of "void flash_fwd_kernel<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax>(Flash_fwd_params) [with Kernel_traits=Flash_fwd_kernel_traits<96, 64, 64, 4, false, false, cutlass::half_t, Flash_kernel_traits<96, 64, 64, 4, cutlass::half_t>>, Is_dropout=true, Is_causal=true, Is_even_N=true, Is_even_K=false, Return_softmax=false]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(34): here
            instantiation of "void run_flash_fwd<Kernel_traits,Is_dropout,Is_causal>(Flash_fwd_params &, cudaStream_t) [with Kernel_traits=Flash_fwd_kernel_traits<96, 64, 64, 4, false, false, cutlass::half_t, Flash_kernel_traits<96, 64, 64, 4, cutlass::half_t>>, Is_dropout=true, Is_causal=true]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(92): here
            instantiation of "void run_mha_fwd_hdim96<T>(Flash_fwd_params &, cudaStream_t) [with T=cutlass::half_t]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_hdim96_fp16_sm80.cu(22): here

/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_kernel.h(140): warning: variable "MMA_M" was declared but never referenced
          detected during:
            instantiation of "void flash::compute_attn_1rowblock<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params,Is_Q_split_K>(const Params &, int, int, int) [with Kernel_traits=Flash_fwd_kernel_traits<96, 64, 64, 4, false, false, cutlass::half_t, Flash_kernel_traits<96, 64, 64, 4, cutlass::half_t>>, Is_dropout=true, Is_causal=true, Is_even_N=false, Is_even_K=true, Return_softmax=true, Params=Flash_fwd_params, Is_Q_split_K=false]" 
(589): here
            instantiation of "void flash::compute_attn<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &) [with Kernel_traits=Flash_fwd_kernel_traits<96, 64, 64, 4, false, false, cutlass::half_t, Flash_kernel_traits<96, 64, 64, 4, cutlass::half_t>>, Is_dropout=true, Is_causal=true, Is_even_N=false, Is_even_K=true, Return_softmax=true, Params=Flash_fwd_params]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(15): here
            instantiation of "void flash_fwd_kernel<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax>(Flash_fwd_params) [with Kernel_traits=Flash_fwd_kernel_traits<96, 64, 64, 4, false, false, cutlass::half_t, Flash_kernel_traits<96, 64, 64, 4, cutlass::half_t>>, Is_dropout=true, Is_causal=true, Is_even_N=false, Is_even_K=true, Return_softmax=true]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(34): here
            instantiation of "void run_flash_fwd<Kernel_traits,Is_dropout,Is_causal>(Flash_fwd_params &, cudaStream_t) [with Kernel_traits=Flash_fwd_kernel_traits<96, 64, 64, 4, false, false, cutlass::half_t, Flash_kernel_traits<96, 64, 64, 4, cutlass::half_t>>, Is_dropout=true, Is_causal=true]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(92): here
            instantiation of "void run_mha_fwd_hdim96<T>(Flash_fwd_params &, cudaStream_t) [with T=cutlass::half_t]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_hdim96_fp16_sm80.cu(22): here

/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_kernel.h(276): warning: variable "tQrQ" was declared but never referenced
          detected during:
            instantiation of "void flash::compute_attn_1rowblock<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params,Is_Q_split_K>(const Params &, int, int, int) [with Kernel_traits=Flash_fwd_kernel_traits<96, 64, 64, 4, false, false, cutlass::half_t, Flash_kernel_traits<96, 64, 64, 4, cutlass::half_t>>, Is_dropout=true, Is_causal=true, Is_even_N=false, Is_even_K=true, Return_softmax=true, Params=Flash_fwd_params, Is_Q_split_K=false]" 
(589): here
            instantiation of "void flash::compute_attn<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &) [with Kernel_traits=Flash_fwd_kernel_traits<96, 64, 64, 4, false, false, cutlass::half_t, Flash_kernel_traits<96, 64, 64, 4, cutlass::half_t>>, Is_dropout=true, Is_causal=true, Is_even_N=false, Is_even_K=true, Return_softmax=true, Params=Flash_fwd_params]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(15): here
            instantiation of "void flash_fwd_kernel<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax>(Flash_fwd_params) [with Kernel_traits=Flash_fwd_kernel_traits<96, 64, 64, 4, false, false, cutlass::half_t, Flash_kernel_traits<96, 64, 64, 4, cutlass::half_t>>, Is_dropout=true, Is_causal=true, Is_even_N=false, Is_even_K=true, Return_softmax=true]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(34): here
            instantiation of "void run_flash_fwd<Kernel_traits,Is_dropout,Is_causal>(Flash_fwd_params &, cudaStream_t) [with Kernel_traits=Flash_fwd_kernel_traits<96, 64, 64, 4, false, false, cutlass::half_t, Flash_kernel_traits<96, 64, 64, 4, cutlass::half_t>>, Is_dropout=true, Is_causal=true]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(92): here
            instantiation of "void run_mha_fwd_hdim96<T>(Flash_fwd_params &, cudaStream_t) [with T=cutlass::half_t]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_hdim96_fp16_sm80.cu(22): here

/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_kernel.h(140): warning: variable "MMA_M" was declared but never referenced
          detected during:
            instantiation of "void flash::compute_attn_1rowblock<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params,Is_Q_split_K>(const Params &, int, int, int) [with Kernel_traits=Flash_fwd_kernel_traits<96, 64, 64, 4, false, false, cutlass::half_t, Flash_kernel_traits<96, 64, 64, 4, cutlass::half_t>>, Is_dropout=true, Is_causal=true, Is_even_N=false, Is_even_K=true, Return_softmax=false, Params=Flash_fwd_params, Is_Q_split_K=false]" 
(589): here
            instantiation of "void flash::compute_attn<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &) [with Kernel_traits=Flash_fwd_kernel_traits<96, 64, 64, 4, false, false, cutlass::half_t, Flash_kernel_traits<96, 64, 64, 4, cutlass::half_t>>, Is_dropout=true, Is_causal=true, Is_even_N=false, Is_even_K=true, Return_softmax=false, Params=Flash_fwd_params]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(15): here
            instantiation of "void flash_fwd_kernel<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax>(Flash_fwd_params) [with Kernel_traits=Flash_fwd_kernel_traits<96, 64, 64, 4, false, false, cutlass::half_t, Flash_kernel_traits<96, 64, 64, 4, cutlass::half_t>>, Is_dropout=true, Is_causal=true, Is_even_N=false, Is_even_K=true, Return_softmax=false]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(34): here
            instantiation of "void run_flash_fwd<Kernel_traits,Is_dropout,Is_causal>(Flash_fwd_params &, cudaStream_t) [with Kernel_traits=Flash_fwd_kernel_traits<96, 64, 64, 4, false, false, cutlass::half_t, Flash_kernel_traits<96, 64, 64, 4, cutlass::half_t>>, Is_dropout=true, Is_causal=true]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(92): here
            instantiation of "void run_mha_fwd_hdim96<T>(Flash_fwd_params &, cudaStream_t) [with T=cutlass::half_t]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_hdim96_fp16_sm80.cu(22): here

/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_kernel.h(276): warning: variable "tQrQ" was declared but never referenced
          detected during:
            instantiation of "void flash::compute_attn_1rowblock<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params,Is_Q_split_K>(const Params &, int, int, int) [with Kernel_traits=Flash_fwd_kernel_traits<96, 64, 64, 4, false, false, cutlass::half_t, Flash_kernel_traits<96, 64, 64, 4, cutlass::half_t>>, Is_dropout=true, Is_causal=true, Is_even_N=false, Is_even_K=true, Return_softmax=false, Params=Flash_fwd_params, Is_Q_split_K=false]" 
(589): here
            instantiation of "void flash::compute_attn<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &) [with Kernel_traits=Flash_fwd_kernel_traits<96, 64, 64, 4, false, false, cutlass::half_t, Flash_kernel_traits<96, 64, 64, 4, cutlass::half_t>>, Is_dropout=true, Is_causal=true, Is_even_N=false, Is_even_K=true, Return_softmax=false, Params=Flash_fwd_params]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(15): here
            instantiation of "void flash_fwd_kernel<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax>(Flash_fwd_params) [with Kernel_traits=Flash_fwd_kernel_traits<96, 64, 64, 4, false, false, cutlass::half_t, Flash_kernel_traits<96, 64, 64, 4, cutlass::half_t>>, Is_dropout=true, Is_causal=true, Is_even_N=false, Is_even_K=true, Return_softmax=false]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(34): here
            instantiation of "void run_flash_fwd<Kernel_traits,Is_dropout,Is_causal>(Flash_fwd_params &, cudaStream_t) [with Kernel_traits=Flash_fwd_kernel_traits<96, 64, 64, 4, false, false, cutlass::half_t, Flash_kernel_traits<96, 64, 64, 4, cutlass::half_t>>, Is_dropout=true, Is_causal=true]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(92): here
            instantiation of "void run_mha_fwd_hdim96<T>(Flash_fwd_params &, cudaStream_t) [with T=cutlass::half_t]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_hdim96_fp16_sm80.cu(22): here

/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_kernel.h(140): warning: variable "MMA_M" was declared but never referenced
          detected during:
            instantiation of "void flash::compute_attn_1rowblock<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params,Is_Q_split_K>(const Params &, int, int, int) [with Kernel_traits=Flash_fwd_kernel_traits<96, 64, 64, 4, false, false, cutlass::half_t, Flash_kernel_traits<96, 64, 64, 4, cutlass::half_t>>, Is_dropout=true, Is_causal=true, Is_even_N=false, Is_even_K=false, Return_softmax=true, Params=Flash_fwd_params, Is_Q_split_K=false]" 
(589): here
            instantiation of "void flash::compute_attn<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &) [with Kernel_traits=Flash_fwd_kernel_traits<96, 64, 64, 4, false, false, cutlass::half_t, Flash_kernel_traits<96, 64, 64, 4, cutlass::half_t>>, Is_dropout=true, Is_causal=true, Is_even_N=false, Is_even_K=false, Return_softmax=true, Params=Flash_fwd_params]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(15): here
            instantiation of "void flash_fwd_kernel<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax>(Flash_fwd_params) [with Kernel_traits=Flash_fwd_kernel_traits<96, 64, 64, 4, false, false, cutlass::half_t, Flash_kernel_traits<96, 64, 64, 4, cutlass::half_t>>, Is_dropout=true, Is_causal=true, Is_even_N=false, Is_even_K=false, Return_softmax=true]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(34): here
            instantiation of "void run_flash_fwd<Kernel_traits,Is_dropout,Is_causal>(Flash_fwd_params &, cudaStream_t) [with Kernel_traits=Flash_fwd_kernel_traits<96, 64, 64, 4, false, false, cutlass::half_t, Flash_kernel_traits<96, 64, 64, 4, cutlass::half_t>>, Is_dropout=true, Is_causal=true]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(92): here
            instantiation of "void run_mha_fwd_hdim96<T>(Flash_fwd_params &, cudaStream_t) [with T=cutlass::half_t]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_hdim96_fp16_sm80.cu(22): here

/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_kernel.h(276): warning: variable "tQrQ" was declared but never referenced
          detected during:
            instantiation of "void flash::compute_attn_1rowblock<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params,Is_Q_split_K>(const Params &, int, int, int) [with Kernel_traits=Flash_fwd_kernel_traits<96, 64, 64, 4, false, false, cutlass::half_t, Flash_kernel_traits<96, 64, 64, 4, cutlass::half_t>>, Is_dropout=true, Is_causal=true, Is_even_N=false, Is_even_K=false, Return_softmax=true, Params=Flash_fwd_params, Is_Q_split_K=false]" 
(589): here
            instantiation of "void flash::compute_attn<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &) [with Kernel_traits=Flash_fwd_kernel_traits<96, 64, 64, 4, false, false, cutlass::half_t, Flash_kernel_traits<96, 64, 64, 4, cutlass::half_t>>, Is_dropout=true, Is_causal=true, Is_even_N=false, Is_even_K=false, Return_softmax=true, Params=Flash_fwd_params]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(15): here
            instantiation of "void flash_fwd_kernel<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax>(Flash_fwd_params) [with Kernel_traits=Flash_fwd_kernel_traits<96, 64, 64, 4, false, false, cutlass::half_t, Flash_kernel_traits<96, 64, 64, 4, cutlass::half_t>>, Is_dropout=true, Is_causal=true, Is_even_N=false, Is_even_K=false, Return_softmax=true]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(34): here
            instantiation of "void run_flash_fwd<Kernel_traits,Is_dropout,Is_causal>(Flash_fwd_params &, cudaStream_t) [with Kernel_traits=Flash_fwd_kernel_traits<96, 64, 64, 4, false, false, cutlass::half_t, Flash_kernel_traits<96, 64, 64, 4, cutlass::half_t>>, Is_dropout=true, Is_causal=true]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(92): here
            instantiation of "void run_mha_fwd_hdim96<T>(Flash_fwd_params &, cudaStream_t) [with T=cutlass::half_t]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_hdim96_fp16_sm80.cu(22): here

/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_kernel.h(140): warning: variable "MMA_M" was declared but never referenced
          detected during:
            instantiation of "void flash::compute_attn_1rowblock<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params,Is_Q_split_K>(const Params &, int, int, int) [with Kernel_traits=Flash_fwd_kernel_traits<96, 64, 64, 4, false, false, cutlass::half_t, Flash_kernel_traits<96, 64, 64, 4, cutlass::half_t>>, Is_dropout=true, Is_causal=true, Is_even_N=false, Is_even_K=false, Return_softmax=false, Params=Flash_fwd_params, Is_Q_split_K=false]" 
(589): here
            instantiation of "void flash::compute_attn<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &) [with Kernel_traits=Flash_fwd_kernel_traits<96, 64, 64, 4, false, false, cutlass::half_t, Flash_kernel_traits<96, 64, 64, 4, cutlass::half_t>>, Is_dropout=true, Is_causal=true, Is_even_N=false, Is_even_K=false, Return_softmax=false, Params=Flash_fwd_params]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(15): here
            instantiation of "void flash_fwd_kernel<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax>(Flash_fwd_params) [with Kernel_traits=Flash_fwd_kernel_traits<96, 64, 64, 4, false, false, cutlass::half_t, Flash_kernel_traits<96, 64, 64, 4, cutlass::half_t>>, Is_dropout=true, Is_causal=true, Is_even_N=false, Is_even_K=false, Return_softmax=false]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(34): here
            instantiation of "void run_flash_fwd<Kernel_traits,Is_dropout,Is_causal>(Flash_fwd_params &, cudaStream_t) [with Kernel_traits=Flash_fwd_kernel_traits<96, 64, 64, 4, false, false, cutlass::half_t, Flash_kernel_traits<96, 64, 64, 4, cutlass::half_t>>, Is_dropout=true, Is_causal=true]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(92): here
            instantiation of "void run_mha_fwd_hdim96<T>(Flash_fwd_params &, cudaStream_t) [with T=cutlass::half_t]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_hdim96_fp16_sm80.cu(22): here

/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_kernel.h(276): warning: variable "tQrQ" was declared but never referenced
          detected during:
            instantiation of "void flash::compute_attn_1rowblock<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params,Is_Q_split_K>(const Params &, int, int, int) [with Kernel_traits=Flash_fwd_kernel_traits<96, 64, 64, 4, false, false, cutlass::half_t, Flash_kernel_traits<96, 64, 64, 4, cutlass::half_t>>, Is_dropout=true, Is_causal=true, Is_even_N=false, Is_even_K=false, Return_softmax=false, Params=Flash_fwd_params, Is_Q_split_K=false]" 
(589): here
            instantiation of "void flash::compute_attn<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &) [with Kernel_traits=Flash_fwd_kernel_traits<96, 64, 64, 4, false, false, cutlass::half_t, Flash_kernel_traits<96, 64, 64, 4, cutlass::half_t>>, Is_dropout=true, Is_causal=true, Is_even_N=false, Is_even_K=false, Return_softmax=false, Params=Flash_fwd_params]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(15): here
            instantiation of "void flash_fwd_kernel<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax>(Flash_fwd_params) [with Kernel_traits=Flash_fwd_kernel_traits<96, 64, 64, 4, false, false, cutlass::half_t, Flash_kernel_traits<96, 64, 64, 4, cutlass::half_t>>, Is_dropout=true, Is_causal=true, Is_even_N=false, Is_even_K=false, Return_softmax=false]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(34): here
            instantiation of "void run_flash_fwd<Kernel_traits,Is_dropout,Is_causal>(Flash_fwd_params &, cudaStream_t) [with Kernel_traits=Flash_fwd_kernel_traits<96, 64, 64, 4, false, false, cutlass::half_t, Flash_kernel_traits<96, 64, 64, 4, cutlass::half_t>>, Is_dropout=true, Is_causal=true]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(92): here
            instantiation of "void run_mha_fwd_hdim96<T>(Flash_fwd_params &, cudaStream_t) [with T=cutlass::half_t]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_hdim96_fp16_sm80.cu(22): here

/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_kernel.h(140): warning: variable "MMA_M" was declared but never referenced
          detected during:
            instantiation of "void flash::compute_attn_1rowblock<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params,Is_Q_split_K>(const Params &, int, int, int) [with Kernel_traits=Flash_fwd_kernel_traits<96, 128, 64, 4, false, false, cutlass::half_t, Flash_kernel_traits<96, 128, 64, 4, cutlass::half_t>>, Is_dropout=true, Is_causal=true, Is_even_N=true, Is_even_K=true, Return_softmax=true, Params=Flash_fwd_params, Is_Q_split_K=false]" 
(589): here
            instantiation of "void flash::compute_attn<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &) [with Kernel_traits=Flash_fwd_kernel_traits<96, 128, 64, 4, false, false, cutlass::half_t, Flash_kernel_traits<96, 128, 64, 4, cutlass::half_t>>, Is_dropout=true, Is_causal=true, Is_even_N=true, Is_even_K=true, Return_softmax=true, Params=Flash_fwd_params]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(15): here
            instantiation of "void flash_fwd_kernel<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax>(Flash_fwd_params) [with Kernel_traits=Flash_fwd_kernel_traits<96, 128, 64, 4, false, false, cutlass::half_t, Flash_kernel_traits<96, 128, 64, 4, cutlass::half_t>>, Is_dropout=true, Is_causal=true, Is_even_N=true, Is_even_K=true, Return_softmax=true]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(34): here
            instantiation of "void run_flash_fwd<Kernel_traits,Is_dropout,Is_causal>(Flash_fwd_params &, cudaStream_t) [with Kernel_traits=Flash_fwd_kernel_traits<96, 128, 64, 4, false, false, cutlass::half_t, Flash_kernel_traits<96, 128, 64, 4, cutlass::half_t>>, Is_dropout=true, Is_causal=true]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(92): here
            instantiation of "void run_mha_fwd_hdim96<T>(Flash_fwd_params &, cudaStream_t) [with T=cutlass::half_t]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_hdim96_fp16_sm80.cu(22): here

/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_kernel.h(276): warning: variable "tQrQ" was declared but never referenced
          detected during:
            instantiation of "void flash::compute_attn_1rowblock<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params,Is_Q_split_K>(const Params &, int, int, int) [with Kernel_traits=Flash_fwd_kernel_traits<96, 128, 64, 4, false, false, cutlass::half_t, Flash_kernel_traits<96, 128, 64, 4, cutlass::half_t>>, Is_dropout=true, Is_causal=true, Is_even_N=true, Is_even_K=true, Return_softmax=true, Params=Flash_fwd_params, Is_Q_split_K=false]" 
(589): here
            instantiation of "void flash::compute_attn<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &) [with Kernel_traits=Flash_fwd_kernel_traits<96, 128, 64, 4, false, false, cutlass::half_t, Flash_kernel_traits<96, 128, 64, 4, cutlass::half_t>>, Is_dropout=true, Is_causal=true, Is_even_N=true, Is_even_K=true, Return_softmax=true, Params=Flash_fwd_params]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(15): here
            instantiation of "void flash_fwd_kernel<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax>(Flash_fwd_params) [with Kernel_traits=Flash_fwd_kernel_traits<96, 128, 64, 4, false, false, cutlass::half_t, Flash_kernel_traits<96, 128, 64, 4, cutlass::half_t>>, Is_dropout=true, Is_causal=true, Is_even_N=true, Is_even_K=true, Return_softmax=true]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(34): here
            instantiation of "void run_flash_fwd<Kernel_traits,Is_dropout,Is_causal>(Flash_fwd_params &, cudaStream_t) [with Kernel_traits=Flash_fwd_kernel_traits<96, 128, 64, 4, false, false, cutlass::half_t, Flash_kernel_traits<96, 128, 64, 4, cutlass::half_t>>, Is_dropout=true, Is_causal=true]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(92): here
            instantiation of "void run_mha_fwd_hdim96<T>(Flash_fwd_params &, cudaStream_t) [with T=cutlass::half_t]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_hdim96_fp16_sm80.cu(22): here

/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_kernel.h(140): warning: variable "MMA_M" was declared but never referenced
          detected during:
            instantiation of "void flash::compute_attn_1rowblock<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params,Is_Q_split_K>(const Params &, int, int, int) [with Kernel_traits=Flash_fwd_kernel_traits<96, 128, 64, 4, false, false, cutlass::half_t, Flash_kernel_traits<96, 128, 64, 4, cutlass::half_t>>, Is_dropout=true, Is_causal=true, Is_even_N=true, Is_even_K=true, Return_softmax=false, Params=Flash_fwd_params, Is_Q_split_K=false]" 
(589): here
            instantiation of "void flash::compute_attn<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &) [with Kernel_traits=Flash_fwd_kernel_traits<96, 128, 64, 4, false, false, cutlass::half_t, Flash_kernel_traits<96, 128, 64, 4, cutlass::half_t>>, Is_dropout=true, Is_causal=true, Is_even_N=true, Is_even_K=true, Return_softmax=false, Params=Flash_fwd_params]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(15): here
            instantiation of "void flash_fwd_kernel<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax>(Flash_fwd_params) [with Kernel_traits=Flash_fwd_kernel_traits<96, 128, 64, 4, false, false, cutlass::half_t, Flash_kernel_traits<96, 128, 64, 4, cutlass::half_t>>, Is_dropout=true, Is_causal=true, Is_even_N=true, Is_even_K=true, Return_softmax=false]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(34): here
            instantiation of "void run_flash_fwd<Kernel_traits,Is_dropout,Is_causal>(Flash_fwd_params &, cudaStream_t) [with Kernel_traits=Flash_fwd_kernel_traits<96, 128, 64, 4, false, false, cutlass::half_t, Flash_kernel_traits<96, 128, 64, 4, cutlass::half_t>>, Is_dropout=true, Is_causal=true]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(92): here
            instantiation of "void run_mha_fwd_hdim96<T>(Flash_fwd_params &, cudaStream_t) [with T=cutlass::half_t]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_hdim96_fp16_sm80.cu(22): here

/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_kernel.h(276): warning: variable "tQrQ" was declared but never referenced
          detected during:
            instantiation of "void flash::compute_attn_1rowblock<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params,Is_Q_split_K>(const Params &, int, int, int) [with Kernel_traits=Flash_fwd_kernel_traits<96, 128, 64, 4, false, false, cutlass::half_t, Flash_kernel_traits<96, 128, 64, 4, cutlass::half_t>>, Is_dropout=true, Is_causal=true, Is_even_N=true, Is_even_K=true, Return_softmax=false, Params=Flash_fwd_params, Is_Q_split_K=false]" 
(589): here
            instantiation of "void flash::compute_attn<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &) [with Kernel_traits=Flash_fwd_kernel_traits<96, 128, 64, 4, false, false, cutlass::half_t, Flash_kernel_traits<96, 128, 64, 4, cutlass::half_t>>, Is_dropout=true, Is_causal=true, Is_even_N=true, Is_even_K=true, Return_softmax=false, Params=Flash_fwd_params]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(15): here
            instantiation of "void flash_fwd_kernel<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax>(Flash_fwd_params) [with Kernel_traits=Flash_fwd_kernel_traits<96, 128, 64, 4, false, false, cutlass::half_t, Flash_kernel_traits<96, 128, 64, 4, cutlass::half_t>>, Is_dropout=true, Is_causal=true, Is_even_N=true, Is_even_K=true, Return_softmax=false]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(34): here
            instantiation of "void run_flash_fwd<Kernel_traits,Is_dropout,Is_causal>(Flash_fwd_params &, cudaStream_t) [with Kernel_traits=Flash_fwd_kernel_traits<96, 128, 64, 4, false, false, cutlass::half_t, Flash_kernel_traits<96, 128, 64, 4, cutlass::half_t>>, Is_dropout=true, Is_causal=true]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(92): here
            instantiation of "void run_mha_fwd_hdim96<T>(Flash_fwd_params &, cudaStream_t) [with T=cutlass::half_t]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_hdim96_fp16_sm80.cu(22): here

/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_kernel.h(140): warning: variable "MMA_M" was declared but never referenced
          detected during:
            instantiation of "void flash::compute_attn_1rowblock<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params,Is_Q_split_K>(const Params &, int, int, int) [with Kernel_traits=Flash_fwd_kernel_traits<96, 128, 64, 4, false, false, cutlass::half_t, Flash_kernel_traits<96, 128, 64, 4, cutlass::half_t>>, Is_dropout=true, Is_causal=true, Is_even_N=true, Is_even_K=false, Return_softmax=true, Params=Flash_fwd_params, Is_Q_split_K=false]" 
(589): here
            instantiation of "void flash::compute_attn<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &) [with Kernel_traits=Flash_fwd_kernel_traits<96, 128, 64, 4, false, false, cutlass::half_t, Flash_kernel_traits<96, 128, 64, 4, cutlass::half_t>>, Is_dropout=true, Is_causal=true, Is_even_N=true, Is_even_K=false, Return_softmax=true, Params=Flash_fwd_params]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(15): here
            instantiation of "void flash_fwd_kernel<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax>(Flash_fwd_params) [with Kernel_traits=Flash_fwd_kernel_traits<96, 128, 64, 4, false, false, cutlass::half_t, Flash_kernel_traits<96, 128, 64, 4, cutlass::half_t>>, Is_dropout=true, Is_causal=true, Is_even_N=true, Is_even_K=false, Return_softmax=true]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(34): here
            instantiation of "void run_flash_fwd<Kernel_traits,Is_dropout,Is_causal>(Flash_fwd_params &, cudaStream_t) [with Kernel_traits=Flash_fwd_kernel_traits<96, 128, 64, 4, false, false, cutlass::half_t, Flash_kernel_traits<96, 128, 64, 4, cutlass::half_t>>, Is_dropout=true, Is_causal=true]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(92): here
            instantiation of "void run_mha_fwd_hdim96<T>(Flash_fwd_params &, cudaStream_t) [with T=cutlass::half_t]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_hdim96_fp16_sm80.cu(22): here

/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_kernel.h(276): warning: variable "tQrQ" was declared but never referenced
          detected during:
            instantiation of "void flash::compute_attn_1rowblock<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params,Is_Q_split_K>(const Params &, int, int, int) [with Kernel_traits=Flash_fwd_kernel_traits<96, 128, 64, 4, false, false, cutlass::half_t, Flash_kernel_traits<96, 128, 64, 4, cutlass::half_t>>, Is_dropout=true, Is_causal=true, Is_even_N=true, Is_even_K=false, Return_softmax=true, Params=Flash_fwd_params, Is_Q_split_K=false]" 
(589): here
            instantiation of "void flash::compute_attn<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &) [with Kernel_traits=Flash_fwd_kernel_traits<96, 128, 64, 4, false, false, cutlass::half_t, Flash_kernel_traits<96, 128, 64, 4, cutlass::half_t>>, Is_dropout=true, Is_causal=true, Is_even_N=true, Is_even_K=false, Return_softmax=true, Params=Flash_fwd_params]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(15): here
            instantiation of "void flash_fwd_kernel<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax>(Flash_fwd_params) [with Kernel_traits=Flash_fwd_kernel_traits<96, 128, 64, 4, false, false, cutlass::half_t, Flash_kernel_traits<96, 128, 64, 4, cutlass::half_t>>, Is_dropout=true, Is_causal=true, Is_even_N=true, Is_even_K=false, Return_softmax=true]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(34): here
            instantiation of "void run_flash_fwd<Kernel_traits,Is_dropout,Is_causal>(Flash_fwd_params &, cudaStream_t) [with Kernel_traits=Flash_fwd_kernel_traits<96, 128, 64, 4, false, false, cutlass::half_t, Flash_kernel_traits<96, 128, 64, 4, cutlass::half_t>>, Is_dropout=true, Is_causal=true]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(92): here
            instantiation of "void run_mha_fwd_hdim96<T>(Flash_fwd_params &, cudaStream_t) [with T=cutlass::half_t]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_hdim96_fp16_sm80.cu(22): here

/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_kernel.h(140): warning: variable "MMA_M" was declared but never referenced
          detected during:
            instantiation of "void flash::compute_attn_1rowblock<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params,Is_Q_split_K>(const Params &, int, int, int) [with Kernel_traits=Flash_fwd_kernel_traits<96, 128, 64, 4, false, false, cutlass::half_t, Flash_kernel_traits<96, 128, 64, 4, cutlass::half_t>>, Is_dropout=true, Is_causal=true, Is_even_N=true, Is_even_K=false, Return_softmax=false, Params=Flash_fwd_params, Is_Q_split_K=false]" 
(589): here
            instantiation of "void flash::compute_attn<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &) [with Kernel_traits=Flash_fwd_kernel_traits<96, 128, 64, 4, false, false, cutlass::half_t, Flash_kernel_traits<96, 128, 64, 4, cutlass::half_t>>, Is_dropout=true, Is_causal=true, Is_even_N=true, Is_even_K=false, Return_softmax=false, Params=Flash_fwd_params]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(15): here
            instantiation of "void flash_fwd_kernel<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax>(Flash_fwd_params) [with Kernel_traits=Flash_fwd_kernel_traits<96, 128, 64, 4, false, false, cutlass::half_t, Flash_kernel_traits<96, 128, 64, 4, cutlass::half_t>>, Is_dropout=true, Is_causal=true, Is_even_N=true, Is_even_K=false, Return_softmax=false]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(34): here
            instantiation of "void run_flash_fwd<Kernel_traits,Is_dropout,Is_causal>(Flash_fwd_params &, cudaStream_t) [with Kernel_traits=Flash_fwd_kernel_traits<96, 128, 64, 4, false, false, cutlass::half_t, Flash_kernel_traits<96, 128, 64, 4, cutlass::half_t>>, Is_dropout=true, Is_causal=true]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(92): here
            instantiation of "void run_mha_fwd_hdim96<T>(Flash_fwd_params &, cudaStream_t) [with T=cutlass::half_t]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_hdim96_fp16_sm80.cu(22): here

/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_kernel.h(276): warning: variable "tQrQ" was declared but never referenced
          detected during:
            instantiation of "void flash::compute_attn_1rowblock<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params,Is_Q_split_K>(const Params &, int, int, int) [with Kernel_traits=Flash_fwd_kernel_traits<96, 128, 64, 4, false, false, cutlass::half_t, Flash_kernel_traits<96, 128, 64, 4, cutlass::half_t>>, Is_dropout=true, Is_causal=true, Is_even_N=true, Is_even_K=false, Return_softmax=false, Params=Flash_fwd_params, Is_Q_split_K=false]" 
(589): here
            instantiation of "void flash::compute_attn<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &) [with Kernel_traits=Flash_fwd_kernel_traits<96, 128, 64, 4, false, false, cutlass::half_t, Flash_kernel_traits<96, 128, 64, 4, cutlass::half_t>>, Is_dropout=true, Is_causal=true, Is_even_N=true, Is_even_K=false, Return_softmax=false, Params=Flash_fwd_params]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(15): here
            instantiation of "void flash_fwd_kernel<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax>(Flash_fwd_params) [with Kernel_traits=Flash_fwd_kernel_traits<96, 128, 64, 4, false, false, cutlass::half_t, Flash_kernel_traits<96, 128, 64, 4, cutlass::half_t>>, Is_dropout=true, Is_causal=true, Is_even_N=true, Is_even_K=false, Return_softmax=false]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(34): here
            instantiation of "void run_flash_fwd<Kernel_traits,Is_dropout,Is_causal>(Flash_fwd_params &, cudaStream_t) [with Kernel_traits=Flash_fwd_kernel_traits<96, 128, 64, 4, false, false, cutlass::half_t, Flash_kernel_traits<96, 128, 64, 4, cutlass::half_t>>, Is_dropout=true, Is_causal=true]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(92): here
            instantiation of "void run_mha_fwd_hdim96<T>(Flash_fwd_params &, cudaStream_t) [with T=cutlass::half_t]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_hdim96_fp16_sm80.cu(22): here

/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_kernel.h(140): warning: variable "MMA_M" was declared but never referenced
          detected during:
            instantiation of "void flash::compute_attn_1rowblock<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params,Is_Q_split_K>(const Params &, int, int, int) [with Kernel_traits=Flash_fwd_kernel_traits<96, 128, 64, 4, false, false, cutlass::half_t, Flash_kernel_traits<96, 128, 64, 4, cutlass::half_t>>, Is_dropout=true, Is_causal=true, Is_even_N=false, Is_even_K=true, Return_softmax=true, Params=Flash_fwd_params, Is_Q_split_K=false]" 
(589): here
            instantiation of "void flash::compute_attn<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &) [with Kernel_traits=Flash_fwd_kernel_traits<96, 128, 64, 4, false, false, cutlass::half_t, Flash_kernel_traits<96, 128, 64, 4, cutlass::half_t>>, Is_dropout=true, Is_causal=true, Is_even_N=false, Is_even_K=true, Return_softmax=true, Params=Flash_fwd_params]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(15): here
            instantiation of "void flash_fwd_kernel<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax>(Flash_fwd_params) [with Kernel_traits=Flash_fwd_kernel_traits<96, 128, 64, 4, false, false, cutlass::half_t, Flash_kernel_traits<96, 128, 64, 4, cutlass::half_t>>, Is_dropout=true, Is_causal=true, Is_even_N=false, Is_even_K=true, Return_softmax=true]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(34): here
            instantiation of "void run_flash_fwd<Kernel_traits,Is_dropout,Is_causal>(Flash_fwd_params &, cudaStream_t) [with Kernel_traits=Flash_fwd_kernel_traits<96, 128, 64, 4, false, false, cutlass::half_t, Flash_kernel_traits<96, 128, 64, 4, cutlass::half_t>>, Is_dropout=true, Is_causal=true]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(92): here
            instantiation of "void run_mha_fwd_hdim96<T>(Flash_fwd_params &, cudaStream_t) [with T=cutlass::half_t]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_hdim96_fp16_sm80.cu(22): here

/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_kernel.h(276): warning: variable "tQrQ" was declared but never referenced
          detected during:
            instantiation of "void flash::compute_attn_1rowblock<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params,Is_Q_split_K>(const Params &, int, int, int) [with Kernel_traits=Flash_fwd_kernel_traits<96, 128, 64, 4, false, false, cutlass::half_t, Flash_kernel_traits<96, 128, 64, 4, cutlass::half_t>>, Is_dropout=true, Is_causal=true, Is_even_N=false, Is_even_K=true, Return_softmax=true, Params=Flash_fwd_params, Is_Q_split_K=false]" 
(589): here
            instantiation of "void flash::compute_attn<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &) [with Kernel_traits=Flash_fwd_kernel_traits<96, 128, 64, 4, false, false, cutlass::half_t, Flash_kernel_traits<96, 128, 64, 4, cutlass::half_t>>, Is_dropout=true, Is_causal=true, Is_even_N=false, Is_even_K=true, Return_softmax=true, Params=Flash_fwd_params]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(15): here
            instantiation of "void flash_fwd_kernel<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax>(Flash_fwd_params) [with Kernel_traits=Flash_fwd_kernel_traits<96, 128, 64, 4, false, false, cutlass::half_t, Flash_kernel_traits<96, 128, 64, 4, cutlass::half_t>>, Is_dropout=true, Is_causal=true, Is_even_N=false, Is_even_K=true, Return_softmax=true]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(34): here
            instantiation of "void run_flash_fwd<Kernel_traits,Is_dropout,Is_causal>(Flash_fwd_params &, cudaStream_t) [with Kernel_traits=Flash_fwd_kernel_traits<96, 128, 64, 4, false, false, cutlass::half_t, Flash_kernel_traits<96, 128, 64, 4, cutlass::half_t>>, Is_dropout=true, Is_causal=true]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(92): here
            instantiation of "void run_mha_fwd_hdim96<T>(Flash_fwd_params &, cudaStream_t) [with T=cutlass::half_t]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_hdim96_fp16_sm80.cu(22): here

/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_kernel.h(140): warning: variable "MMA_M" was declared but never referenced
          detected during:
            instantiation of "void flash::compute_attn_1rowblock<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params,Is_Q_split_K>(const Params &, int, int, int) [with Kernel_traits=Flash_fwd_kernel_traits<96, 128, 64, 4, false, false, cutlass::half_t, Flash_kernel_traits<96, 128, 64, 4, cutlass::half_t>>, Is_dropout=true, Is_causal=true, Is_even_N=false, Is_even_K=true, Return_softmax=false, Params=Flash_fwd_params, Is_Q_split_K=false]" 
(589): here
            instantiation of "void flash::compute_attn<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &) [with Kernel_traits=Flash_fwd_kernel_traits<96, 128, 64, 4, false, false, cutlass::half_t, Flash_kernel_traits<96, 128, 64, 4, cutlass::half_t>>, Is_dropout=true, Is_causal=true, Is_even_N=false, Is_even_K=true, Return_softmax=false, Params=Flash_fwd_params]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(15): here
            instantiation of "void flash_fwd_kernel<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax>(Flash_fwd_params) [with Kernel_traits=Flash_fwd_kernel_traits<96, 128, 64, 4, false, false, cutlass::half_t, Flash_kernel_traits<96, 128, 64, 4, cutlass::half_t>>, Is_dropout=true, Is_causal=true, Is_even_N=false, Is_even_K=true, Return_softmax=false]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(34): here
            instantiation of "void run_flash_fwd<Kernel_traits,Is_dropout,Is_causal>(Flash_fwd_params &, cudaStream_t) [with Kernel_traits=Flash_fwd_kernel_traits<96, 128, 64, 4, false, false, cutlass::half_t, Flash_kernel_traits<96, 128, 64, 4, cutlass::half_t>>, Is_dropout=true, Is_causal=true]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(92): here
            instantiation of "void run_mha_fwd_hdim96<T>(Flash_fwd_params &, cudaStream_t) [with T=cutlass::half_t]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_hdim96_fp16_sm80.cu(22): here

/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_kernel.h(276): warning: variable "tQrQ" was declared but never referenced
          detected during:
            instantiation of "void flash::compute_attn_1rowblock<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params,Is_Q_split_K>(const Params &, int, int, int) [with Kernel_traits=Flash_fwd_kernel_traits<96, 128, 64, 4, false, false, cutlass::half_t, Flash_kernel_traits<96, 128, 64, 4, cutlass::half_t>>, Is_dropout=true, Is_causal=true, Is_even_N=false, Is_even_K=true, Return_softmax=false, Params=Flash_fwd_params, Is_Q_split_K=false]" 
(589): here
            instantiation of "void flash::compute_attn<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &) [with Kernel_traits=Flash_fwd_kernel_traits<96, 128, 64, 4, false, false, cutlass::half_t, Flash_kernel_traits<96, 128, 64, 4, cutlass::half_t>>, Is_dropout=true, Is_causal=true, Is_even_N=false, Is_even_K=true, Return_softmax=false, Params=Flash_fwd_params]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(15): here
            instantiation of "void flash_fwd_kernel<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax>(Flash_fwd_params) [with Kernel_traits=Flash_fwd_kernel_traits<96, 128, 64, 4, false, false, cutlass::half_t, Flash_kernel_traits<96, 128, 64, 4, cutlass::half_t>>, Is_dropout=true, Is_causal=true, Is_even_N=false, Is_even_K=true, Return_softmax=false]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(34): here
            instantiation of "void run_flash_fwd<Kernel_traits,Is_dropout,Is_causal>(Flash_fwd_params &, cudaStream_t) [with Kernel_traits=Flash_fwd_kernel_traits<96, 128, 64, 4, false, false, cutlass::half_t, Flash_kernel_traits<96, 128, 64, 4, cutlass::half_t>>, Is_dropout=true, Is_causal=true]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(92): here
            instantiation of "void run_mha_fwd_hdim96<T>(Flash_fwd_params &, cudaStream_t) [with T=cutlass::half_t]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_hdim96_fp16_sm80.cu(22): here

/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_kernel.h(140): warning: variable "MMA_M" was declared but never referenced
          detected during:
            instantiation of "void flash::compute_attn_1rowblock<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params,Is_Q_split_K>(const Params &, int, int, int) [with Kernel_traits=Flash_fwd_kernel_traits<96, 128, 64, 4, false, false, cutlass::half_t, Flash_kernel_traits<96, 128, 64, 4, cutlass::half_t>>, Is_dropout=true, Is_causal=true, Is_even_N=false, Is_even_K=false, Return_softmax=true, Params=Flash_fwd_params, Is_Q_split_K=false]" 
(589): here
            instantiation of "void flash::compute_attn<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &) [with Kernel_traits=Flash_fwd_kernel_traits<96, 128, 64, 4, false, false, cutlass::half_t, Flash_kernel_traits<96, 128, 64, 4, cutlass::half_t>>, Is_dropout=true, Is_causal=true, Is_even_N=false, Is_even_K=false, Return_softmax=true, Params=Flash_fwd_params]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(15): here
            instantiation of "void flash_fwd_kernel<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax>(Flash_fwd_params) [with Kernel_traits=Flash_fwd_kernel_traits<96, 128, 64, 4, false, false, cutlass::half_t, Flash_kernel_traits<96, 128, 64, 4, cutlass::half_t>>, Is_dropout=true, Is_causal=true, Is_even_N=false, Is_even_K=false, Return_softmax=true]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(34): here
            instantiation of "void run_flash_fwd<Kernel_traits,Is_dropout,Is_causal>(Flash_fwd_params &, cudaStream_t) [with Kernel_traits=Flash_fwd_kernel_traits<96, 128, 64, 4, false, false, cutlass::half_t, Flash_kernel_traits<96, 128, 64, 4, cutlass::half_t>>, Is_dropout=true, Is_causal=true]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(92): here
            instantiation of "void run_mha_fwd_hdim96<T>(Flash_fwd_params &, cudaStream_t) [with T=cutlass::half_t]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_hdim96_fp16_sm80.cu(22): here

/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_kernel.h(276): warning: variable "tQrQ" was declared but never referenced
          detected during:
            instantiation of "void flash::compute_attn_1rowblock<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params,Is_Q_split_K>(const Params &, int, int, int) [with Kernel_traits=Flash_fwd_kernel_traits<96, 128, 64, 4, false, false, cutlass::half_t, Flash_kernel_traits<96, 128, 64, 4, cutlass::half_t>>, Is_dropout=true, Is_causal=true, Is_even_N=false, Is_even_K=false, Return_softmax=true, Params=Flash_fwd_params, Is_Q_split_K=false]" 
(589): here
            instantiation of "void flash::compute_attn<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &) [with Kernel_traits=Flash_fwd_kernel_traits<96, 128, 64, 4, false, false, cutlass::half_t, Flash_kernel_traits<96, 128, 64, 4, cutlass::half_t>>, Is_dropout=true, Is_causal=true, Is_even_N=false, Is_even_K=false, Return_softmax=true, Params=Flash_fwd_params]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(15): here
            instantiation of "void flash_fwd_kernel<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax>(Flash_fwd_params) [with Kernel_traits=Flash_fwd_kernel_traits<96, 128, 64, 4, false, false, cutlass::half_t, Flash_kernel_traits<96, 128, 64, 4, cutlass::half_t>>, Is_dropout=true, Is_causal=true, Is_even_N=false, Is_even_K=false, Return_softmax=true]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(34): here
            instantiation of "void run_flash_fwd<Kernel_traits,Is_dropout,Is_causal>(Flash_fwd_params &, cudaStream_t) [with Kernel_traits=Flash_fwd_kernel_traits<96, 128, 64, 4, false, false, cutlass::half_t, Flash_kernel_traits<96, 128, 64, 4, cutlass::half_t>>, Is_dropout=true, Is_causal=true]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(92): here
            instantiation of "void run_mha_fwd_hdim96<T>(Flash_fwd_params &, cudaStream_t) [with T=cutlass::half_t]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_hdim96_fp16_sm80.cu(22): here

/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_kernel.h(140): warning: variable "MMA_M" was declared but never referenced
          detected during:
            instantiation of "void flash::compute_attn_1rowblock<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params,Is_Q_split_K>(const Params &, int, int, int) [with Kernel_traits=Flash_fwd_kernel_traits<96, 128, 64, 4, false, false, cutlass::half_t, Flash_kernel_traits<96, 128, 64, 4, cutlass::half_t>>, Is_dropout=true, Is_causal=true, Is_even_N=false, Is_even_K=false, Return_softmax=false, Params=Flash_fwd_params, Is_Q_split_K=false]" 
(589): here
            instantiation of "void flash::compute_attn<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &) [with Kernel_traits=Flash_fwd_kernel_traits<96, 128, 64, 4, false, false, cutlass::half_t, Flash_kernel_traits<96, 128, 64, 4, cutlass::half_t>>, Is_dropout=true, Is_causal=true, Is_even_N=false, Is_even_K=false, Return_softmax=false, Params=Flash_fwd_params]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(15): here
            instantiation of "void flash_fwd_kernel<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax>(Flash_fwd_params) [with Kernel_traits=Flash_fwd_kernel_traits<96, 128, 64, 4, false, false, cutlass::half_t, Flash_kernel_traits<96, 128, 64, 4, cutlass::half_t>>, Is_dropout=true, Is_causal=true, Is_even_N=false, Is_even_K=false, Return_softmax=false]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(34): here
            instantiation of "void run_flash_fwd<Kernel_traits,Is_dropout,Is_causal>(Flash_fwd_params &, cudaStream_t) [with Kernel_traits=Flash_fwd_kernel_traits<96, 128, 64, 4, false, false, cutlass::half_t, Flash_kernel_traits<96, 128, 64, 4, cutlass::half_t>>, Is_dropout=true, Is_causal=true]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(92): here
            instantiation of "void run_mha_fwd_hdim96<T>(Flash_fwd_params &, cudaStream_t) [with T=cutlass::half_t]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_hdim96_fp16_sm80.cu(22): here

/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_kernel.h(276): warning: variable "tQrQ" was declared but never referenced
          detected during:
            instantiation of "void flash::compute_attn_1rowblock<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params,Is_Q_split_K>(const Params &, int, int, int) [with Kernel_traits=Flash_fwd_kernel_traits<96, 128, 64, 4, false, false, cutlass::half_t, Flash_kernel_traits<96, 128, 64, 4, cutlass::half_t>>, Is_dropout=true, Is_causal=true, Is_even_N=false, Is_even_K=false, Return_softmax=false, Params=Flash_fwd_params, Is_Q_split_K=false]" 
(589): here
            instantiation of "void flash::compute_attn<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &) [with Kernel_traits=Flash_fwd_kernel_traits<96, 128, 64, 4, false, false, cutlass::half_t, Flash_kernel_traits<96, 128, 64, 4, cutlass::half_t>>, Is_dropout=true, Is_causal=true, Is_even_N=false, Is_even_K=false, Return_softmax=false, Params=Flash_fwd_params]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(15): here
            instantiation of "void flash_fwd_kernel<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax>(Flash_fwd_params) [with Kernel_traits=Flash_fwd_kernel_traits<96, 128, 64, 4, false, false, cutlass::half_t, Flash_kernel_traits<96, 128, 64, 4, cutlass::half_t>>, Is_dropout=true, Is_causal=true, Is_even_N=false, Is_even_K=false, Return_softmax=false]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(34): here
            instantiation of "void run_flash_fwd<Kernel_traits,Is_dropout,Is_causal>(Flash_fwd_params &, cudaStream_t) [with Kernel_traits=Flash_fwd_kernel_traits<96, 128, 64, 4, false, false, cutlass::half_t, Flash_kernel_traits<96, 128, 64, 4, cutlass::half_t>>, Is_dropout=true, Is_causal=true]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(92): here
            instantiation of "void run_mha_fwd_hdim96<T>(Flash_fwd_params &, cudaStream_t) [with T=cutlass::half_t]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_hdim96_fp16_sm80.cu(22): here

/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_kernel.h(140): warning: variable "MMA_M" was declared but never referenced
          detected during:
            instantiation of "void flash::compute_attn_1rowblock<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params,Is_Q_split_K>(const Params &, int, int, int) [with Kernel_traits=Flash_fwd_kernel_traits<96, 128, 64, 4, false, false, cutlass::half_t, Flash_kernel_traits<96, 128, 64, 4, cutlass::half_t>>, Is_dropout=true, Is_causal=false, Is_even_N=true, Is_even_K=true, Return_softmax=true, Params=Flash_fwd_params, Is_Q_split_K=false]" 
(589): here
            instantiation of "void flash::compute_attn<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &) [with Kernel_traits=Flash_fwd_kernel_traits<96, 128, 64, 4, false, false, cutlass::half_t, Flash_kernel_traits<96, 128, 64, 4, cutlass::half_t>>, Is_dropout=true, Is_causal=false, Is_even_N=true, Is_even_K=true, Return_softmax=true, Params=Flash_fwd_params]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(15): here
            instantiation of "void flash_fwd_kernel<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax>(Flash_fwd_params) [with Kernel_traits=Flash_fwd_kernel_traits<96, 128, 64, 4, false, false, cutlass::half_t, Flash_kernel_traits<96, 128, 64, 4, cutlass::half_t>>, Is_dropout=true, Is_causal=false, Is_even_N=true, Is_even_K=true, Return_softmax=true]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(34): here
            instantiation of "void run_flash_fwd<Kernel_traits,Is_dropout,Is_causal>(Flash_fwd_params &, cudaStream_t) [with Kernel_traits=Flash_fwd_kernel_traits<96, 128, 64, 4, false, false, cutlass::half_t, Flash_kernel_traits<96, 128, 64, 4, cutlass::half_t>>, Is_dropout=true, Is_causal=false]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(92): here
            instantiation of "void run_mha_fwd_hdim96<T>(Flash_fwd_params &, cudaStream_t) [with T=cutlass::half_t]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_hdim96_fp16_sm80.cu(22): here

/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_kernel.h(276): warning: variable "tQrQ" was declared but never referenced
          detected during:
            instantiation of "void flash::compute_attn_1rowblock<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params,Is_Q_split_K>(const Params &, int, int, int) [with Kernel_traits=Flash_fwd_kernel_traits<96, 128, 64, 4, false, false, cutlass::half_t, Flash_kernel_traits<96, 128, 64, 4, cutlass::half_t>>, Is_dropout=true, Is_causal=false, Is_even_N=true, Is_even_K=true, Return_softmax=true, Params=Flash_fwd_params, Is_Q_split_K=false]" 
(589): here
            instantiation of "void flash::compute_attn<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &) [with Kernel_traits=Flash_fwd_kernel_traits<96, 128, 64, 4, false, false, cutlass::half_t, Flash_kernel_traits<96, 128, 64, 4, cutlass::half_t>>, Is_dropout=true, Is_causal=false, Is_even_N=true, Is_even_K=true, Return_softmax=true, Params=Flash_fwd_params]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(15): here
            instantiation of "void flash_fwd_kernel<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax>(Flash_fwd_params) [with Kernel_traits=Flash_fwd_kernel_traits<96, 128, 64, 4, false, false, cutlass::half_t, Flash_kernel_traits<96, 128, 64, 4, cutlass::half_t>>, Is_dropout=true, Is_causal=false, Is_even_N=true, Is_even_K=true, Return_softmax=true]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(34): here
            instantiation of "void run_flash_fwd<Kernel_traits,Is_dropout,Is_causal>(Flash_fwd_params &, cudaStream_t) [with Kernel_traits=Flash_fwd_kernel_traits<96, 128, 64, 4, false, false, cutlass::half_t, Flash_kernel_traits<96, 128, 64, 4, cutlass::half_t>>, Is_dropout=true, Is_causal=false]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(92): here
            instantiation of "void run_mha_fwd_hdim96<T>(Flash_fwd_params &, cudaStream_t) [with T=cutlass::half_t]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_hdim96_fp16_sm80.cu(22): here

/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_kernel.h(140): warning: variable "MMA_M" was declared but never referenced
          detected during:
            instantiation of "void flash::compute_attn_1rowblock<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params,Is_Q_split_K>(const Params &, int, int, int) [with Kernel_traits=Flash_fwd_kernel_traits<96, 128, 64, 4, false, false, cutlass::half_t, Flash_kernel_traits<96, 128, 64, 4, cutlass::half_t>>, Is_dropout=true, Is_causal=false, Is_even_N=true, Is_even_K=true, Return_softmax=false, Params=Flash_fwd_params, Is_Q_split_K=false]" 
(589): here
            instantiation of "void flash::compute_attn<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &) [with Kernel_traits=Flash_fwd_kernel_traits<96, 128, 64, 4, false, false, cutlass::half_t, Flash_kernel_traits<96, 128, 64, 4, cutlass::half_t>>, Is_dropout=true, Is_causal=false, Is_even_N=true, Is_even_K=true, Return_softmax=false, Params=Flash_fwd_params]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(15): here
            instantiation of "void flash_fwd_kernel<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax>(Flash_fwd_params) [with Kernel_traits=Flash_fwd_kernel_traits<96, 128, 64, 4, false, false, cutlass::half_t, Flash_kernel_traits<96, 128, 64, 4, cutlass::half_t>>, Is_dropout=true, Is_causal=false, Is_even_N=true, Is_even_K=true, Return_softmax=false]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(34): here
            instantiation of "void run_flash_fwd<Kernel_traits,Is_dropout,Is_causal>(Flash_fwd_params &, cudaStream_t) [with Kernel_traits=Flash_fwd_kernel_traits<96, 128, 64, 4, false, false, cutlass::half_t, Flash_kernel_traits<96, 128, 64, 4, cutlass::half_t>>, Is_dropout=true, Is_causal=false]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(92): here
            instantiation of "void run_mha_fwd_hdim96<T>(Flash_fwd_params &, cudaStream_t) [with T=cutlass::half_t]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_hdim96_fp16_sm80.cu(22): here

/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_kernel.h(276): warning: variable "tQrQ" was declared but never referenced
          detected during:
            instantiation of "void flash::compute_attn_1rowblock<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params,Is_Q_split_K>(const Params &, int, int, int) [with Kernel_traits=Flash_fwd_kernel_traits<96, 128, 64, 4, false, false, cutlass::half_t, Flash_kernel_traits<96, 128, 64, 4, cutlass::half_t>>, Is_dropout=true, Is_causal=false, Is_even_N=true, Is_even_K=true, Return_softmax=false, Params=Flash_fwd_params, Is_Q_split_K=false]" 
(589): here
            instantiation of "void flash::compute_attn<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &) [with Kernel_traits=Flash_fwd_kernel_traits<96, 128, 64, 4, false, false, cutlass::half_t, Flash_kernel_traits<96, 128, 64, 4, cutlass::half_t>>, Is_dropout=true, Is_causal=false, Is_even_N=true, Is_even_K=true, Return_softmax=false, Params=Flash_fwd_params]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(15): here
            instantiation of "void flash_fwd_kernel<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax>(Flash_fwd_params) [with Kernel_traits=Flash_fwd_kernel_traits<96, 128, 64, 4, false, false, cutlass::half_t, Flash_kernel_traits<96, 128, 64, 4, cutlass::half_t>>, Is_dropout=true, Is_causal=false, Is_even_N=true, Is_even_K=true, Return_softmax=false]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(34): here
            instantiation of "void run_flash_fwd<Kernel_traits,Is_dropout,Is_causal>(Flash_fwd_params &, cudaStream_t) [with Kernel_traits=Flash_fwd_kernel_traits<96, 128, 64, 4, false, false, cutlass::half_t, Flash_kernel_traits<96, 128, 64, 4, cutlass::half_t>>, Is_dropout=true, Is_causal=false]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(92): here
            instantiation of "void run_mha_fwd_hdim96<T>(Flash_fwd_params &, cudaStream_t) [with T=cutlass::half_t]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_hdim96_fp16_sm80.cu(22): here

/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_kernel.h(140): warning: variable "MMA_M" was declared but never referenced
          detected during:
            instantiation of "void flash::compute_attn_1rowblock<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params,Is_Q_split_K>(const Params &, int, int, int) [with Kernel_traits=Flash_fwd_kernel_traits<96, 128, 64, 4, false, false, cutlass::half_t, Flash_kernel_traits<96, 128, 64, 4, cutlass::half_t>>, Is_dropout=true, Is_causal=false, Is_even_N=true, Is_even_K=false, Return_softmax=true, Params=Flash_fwd_params, Is_Q_split_K=false]" 
(589): here
            instantiation of "void flash::compute_attn<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &) [with Kernel_traits=Flash_fwd_kernel_traits<96, 128, 64, 4, false, false, cutlass::half_t, Flash_kernel_traits<96, 128, 64, 4, cutlass::half_t>>, Is_dropout=true, Is_causal=false, Is_even_N=true, Is_even_K=false, Return_softmax=true, Params=Flash_fwd_params]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(15): here
            instantiation of "void flash_fwd_kernel<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax>(Flash_fwd_params) [with Kernel_traits=Flash_fwd_kernel_traits<96, 128, 64, 4, false, false, cutlass::half_t, Flash_kernel_traits<96, 128, 64, 4, cutlass::half_t>>, Is_dropout=true, Is_causal=false, Is_even_N=true, Is_even_K=false, Return_softmax=true]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(34): here
            instantiation of "void run_flash_fwd<Kernel_traits,Is_dropout,Is_causal>(Flash_fwd_params &, cudaStream_t) [with Kernel_traits=Flash_fwd_kernel_traits<96, 128, 64, 4, false, false, cutlass::half_t, Flash_kernel_traits<96, 128, 64, 4, cutlass::half_t>>, Is_dropout=true, Is_causal=false]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(92): here
            instantiation of "void run_mha_fwd_hdim96<T>(Flash_fwd_params &, cudaStream_t) [with T=cutlass::half_t]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_hdim96_fp16_sm80.cu(22): here

/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_kernel.h(276): warning: variable "tQrQ" was declared but never referenced
          detected during:
            instantiation of "void flash::compute_attn_1rowblock<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params,Is_Q_split_K>(const Params &, int, int, int) [with Kernel_traits=Flash_fwd_kernel_traits<96, 128, 64, 4, false, false, cutlass::half_t, Flash_kernel_traits<96, 128, 64, 4, cutlass::half_t>>, Is_dropout=true, Is_causal=false, Is_even_N=true, Is_even_K=false, Return_softmax=true, Params=Flash_fwd_params, Is_Q_split_K=false]" 
(589): here
            instantiation of "void flash::compute_attn<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &) [with Kernel_traits=Flash_fwd_kernel_traits<96, 128, 64, 4, false, false, cutlass::half_t, Flash_kernel_traits<96, 128, 64, 4, cutlass::half_t>>, Is_dropout=true, Is_causal=false, Is_even_N=true, Is_even_K=false, Return_softmax=true, Params=Flash_fwd_params]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(15): here
            instantiation of "void flash_fwd_kernel<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax>(Flash_fwd_params) [with Kernel_traits=Flash_fwd_kernel_traits<96, 128, 64, 4, false, false, cutlass::half_t, Flash_kernel_traits<96, 128, 64, 4, cutlass::half_t>>, Is_dropout=true, Is_causal=false, Is_even_N=true, Is_even_K=false, Return_softmax=true]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(34): here
            instantiation of "void run_flash_fwd<Kernel_traits,Is_dropout,Is_causal>(Flash_fwd_params &, cudaStream_t) [with Kernel_traits=Flash_fwd_kernel_traits<96, 128, 64, 4, false, false, cutlass::half_t, Flash_kernel_traits<96, 128, 64, 4, cutlass::half_t>>, Is_dropout=true, Is_causal=false]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(92): here
            instantiation of "void run_mha_fwd_hdim96<T>(Flash_fwd_params &, cudaStream_t) [with T=cutlass::half_t]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_hdim96_fp16_sm80.cu(22): here

/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_kernel.h(140): warning: variable "MMA_M" was declared but never referenced
          detected during:
            instantiation of "void flash::compute_attn_1rowblock<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params,Is_Q_split_K>(const Params &, int, int, int) [with Kernel_traits=Flash_fwd_kernel_traits<96, 128, 64, 4, false, false, cutlass::half_t, Flash_kernel_traits<96, 128, 64, 4, cutlass::half_t>>, Is_dropout=true, Is_causal=false, Is_even_N=true, Is_even_K=false, Return_softmax=false, Params=Flash_fwd_params, Is_Q_split_K=false]" 
(589): here
            instantiation of "void flash::compute_attn<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &) [with Kernel_traits=Flash_fwd_kernel_traits<96, 128, 64, 4, false, false, cutlass::half_t, Flash_kernel_traits<96, 128, 64, 4, cutlass::half_t>>, Is_dropout=true, Is_causal=false, Is_even_N=true, Is_even_K=false, Return_softmax=false, Params=Flash_fwd_params]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(15): here
            instantiation of "void flash_fwd_kernel<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax>(Flash_fwd_params) [with Kernel_traits=Flash_fwd_kernel_traits<96, 128, 64, 4, false, false, cutlass::half_t, Flash_kernel_traits<96, 128, 64, 4, cutlass::half_t>>, Is_dropout=true, Is_causal=false, Is_even_N=true, Is_even_K=false, Return_softmax=false]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(34): here
            instantiation of "void run_flash_fwd<Kernel_traits,Is_dropout,Is_causal>(Flash_fwd_params &, cudaStream_t) [with Kernel_traits=Flash_fwd_kernel_traits<96, 128, 64, 4, false, false, cutlass::half_t, Flash_kernel_traits<96, 128, 64, 4, cutlass::half_t>>, Is_dropout=true, Is_causal=false]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(92): here
            instantiation of "void run_mha_fwd_hdim96<T>(Flash_fwd_params &, cudaStream_t) [with T=cutlass::half_t]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_hdim96_fp16_sm80.cu(22): here

/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_kernel.h(276): warning: variable "tQrQ" was declared but never referenced
          detected during:
            instantiation of "void flash::compute_attn_1rowblock<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params,Is_Q_split_K>(const Params &, int, int, int) [with Kernel_traits=Flash_fwd_kernel_traits<96, 128, 64, 4, false, false, cutlass::half_t, Flash_kernel_traits<96, 128, 64, 4, cutlass::half_t>>, Is_dropout=true, Is_causal=false, Is_even_N=true, Is_even_K=false, Return_softmax=false, Params=Flash_fwd_params, Is_Q_split_K=false]" 
(589): here
            instantiation of "void flash::compute_attn<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &) [with Kernel_traits=Flash_fwd_kernel_traits<96, 128, 64, 4, false, false, cutlass::half_t, Flash_kernel_traits<96, 128, 64, 4, cutlass::half_t>>, Is_dropout=true, Is_causal=false, Is_even_N=true, Is_even_K=false, Return_softmax=false, Params=Flash_fwd_params]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(15): here
            instantiation of "void flash_fwd_kernel<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax>(Flash_fwd_params) [with Kernel_traits=Flash_fwd_kernel_traits<96, 128, 64, 4, false, false, cutlass::half_t, Flash_kernel_traits<96, 128, 64, 4, cutlass::half_t>>, Is_dropout=true, Is_causal=false, Is_even_N=true, Is_even_K=false, Return_softmax=false]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(34): here
            instantiation of "void run_flash_fwd<Kernel_traits,Is_dropout,Is_causal>(Flash_fwd_params &, cudaStream_t) [with Kernel_traits=Flash_fwd_kernel_traits<96, 128, 64, 4, false, false, cutlass::half_t, Flash_kernel_traits<96, 128, 64, 4, cutlass::half_t>>, Is_dropout=true, Is_causal=false]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(92): here
            instantiation of "void run_mha_fwd_hdim96<T>(Flash_fwd_params &, cudaStream_t) [with T=cutlass::half_t]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_hdim96_fp16_sm80.cu(22): here

/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_kernel.h(140): warning: variable "MMA_M" was declared but never referenced
          detected during:
            instantiation of "void flash::compute_attn_1rowblock<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params,Is_Q_split_K>(const Params &, int, int, int) [with Kernel_traits=Flash_fwd_kernel_traits<96, 128, 64, 4, false, false, cutlass::half_t, Flash_kernel_traits<96, 128, 64, 4, cutlass::half_t>>, Is_dropout=true, Is_causal=false, Is_even_N=false, Is_even_K=true, Return_softmax=true, Params=Flash_fwd_params, Is_Q_split_K=false]" 
(589): here
            instantiation of "void flash::compute_attn<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &) [with Kernel_traits=Flash_fwd_kernel_traits<96, 128, 64, 4, false, false, cutlass::half_t, Flash_kernel_traits<96, 128, 64, 4, cutlass::half_t>>, Is_dropout=true, Is_causal=false, Is_even_N=false, Is_even_K=true, Return_softmax=true, Params=Flash_fwd_params]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(15): here
            instantiation of "void flash_fwd_kernel<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax>(Flash_fwd_params) [with Kernel_traits=Flash_fwd_kernel_traits<96, 128, 64, 4, false, false, cutlass::half_t, Flash_kernel_traits<96, 128, 64, 4, cutlass::half_t>>, Is_dropout=true, Is_causal=false, Is_even_N=false, Is_even_K=true, Return_softmax=true]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(34): here
            instantiation of "void run_flash_fwd<Kernel_traits,Is_dropout,Is_causal>(Flash_fwd_params &, cudaStream_t) [with Kernel_traits=Flash_fwd_kernel_traits<96, 128, 64, 4, false, false, cutlass::half_t, Flash_kernel_traits<96, 128, 64, 4, cutlass::half_t>>, Is_dropout=true, Is_causal=false]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(92): here
            instantiation of "void run_mha_fwd_hdim96<T>(Flash_fwd_params &, cudaStream_t) [with T=cutlass::half_t]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_hdim96_fp16_sm80.cu(22): here

/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_kernel.h(276): warning: variable "tQrQ" was declared but never referenced
          detected during:
            instantiation of "void flash::compute_attn_1rowblock<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params,Is_Q_split_K>(const Params &, int, int, int) [with Kernel_traits=Flash_fwd_kernel_traits<96, 128, 64, 4, false, false, cutlass::half_t, Flash_kernel_traits<96, 128, 64, 4, cutlass::half_t>>, Is_dropout=true, Is_causal=false, Is_even_N=false, Is_even_K=true, Return_softmax=true, Params=Flash_fwd_params, Is_Q_split_K=false]" 
(589): here
            instantiation of "void flash::compute_attn<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &) [with Kernel_traits=Flash_fwd_kernel_traits<96, 128, 64, 4, false, false, cutlass::half_t, Flash_kernel_traits<96, 128, 64, 4, cutlass::half_t>>, Is_dropout=true, Is_causal=false, Is_even_N=false, Is_even_K=true, Return_softmax=true, Params=Flash_fwd_params]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(15): here
            instantiation of "void flash_fwd_kernel<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax>(Flash_fwd_params) [with Kernel_traits=Flash_fwd_kernel_traits<96, 128, 64, 4, false, false, cutlass::half_t, Flash_kernel_traits<96, 128, 64, 4, cutlass::half_t>>, Is_dropout=true, Is_causal=false, Is_even_N=false, Is_even_K=true, Return_softmax=true]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(34): here
            instantiation of "void run_flash_fwd<Kernel_traits,Is_dropout,Is_causal>(Flash_fwd_params &, cudaStream_t) [with Kernel_traits=Flash_fwd_kernel_traits<96, 128, 64, 4, false, false, cutlass::half_t, Flash_kernel_traits<96, 128, 64, 4, cutlass::half_t>>, Is_dropout=true, Is_causal=false]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(92): here
            instantiation of "void run_mha_fwd_hdim96<T>(Flash_fwd_params &, cudaStream_t) [with T=cutlass::half_t]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_hdim96_fp16_sm80.cu(22): here

/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_kernel.h(140): warning: variable "MMA_M" was declared but never referenced
          detected during:
            instantiation of "void flash::compute_attn_1rowblock<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params,Is_Q_split_K>(const Params &, int, int, int) [with Kernel_traits=Flash_fwd_kernel_traits<96, 128, 64, 4, false, false, cutlass::half_t, Flash_kernel_traits<96, 128, 64, 4, cutlass::half_t>>, Is_dropout=true, Is_causal=false, Is_even_N=false, Is_even_K=true, Return_softmax=false, Params=Flash_fwd_params, Is_Q_split_K=false]" 
(589): here
            instantiation of "void flash::compute_attn<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &) [with Kernel_traits=Flash_fwd_kernel_traits<96, 128, 64, 4, false, false, cutlass::half_t, Flash_kernel_traits<96, 128, 64, 4, cutlass::half_t>>, Is_dropout=true, Is_causal=false, Is_even_N=false, Is_even_K=true, Return_softmax=false, Params=Flash_fwd_params]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(15): here
            instantiation of "void flash_fwd_kernel<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax>(Flash_fwd_params) [with Kernel_traits=Flash_fwd_kernel_traits<96, 128, 64, 4, false, false, cutlass::half_t, Flash_kernel_traits<96, 128, 64, 4, cutlass::half_t>>, Is_dropout=true, Is_causal=false, Is_even_N=false, Is_even_K=true, Return_softmax=false]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(34): here
            instantiation of "void run_flash_fwd<Kernel_traits,Is_dropout,Is_causal>(Flash_fwd_params &, cudaStream_t) [with Kernel_traits=Flash_fwd_kernel_traits<96, 128, 64, 4, false, false, cutlass::half_t, Flash_kernel_traits<96, 128, 64, 4, cutlass::half_t>>, Is_dropout=true, Is_causal=false]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(92): here
            instantiation of "void run_mha_fwd_hdim96<T>(Flash_fwd_params &, cudaStream_t) [with T=cutlass::half_t]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_hdim96_fp16_sm80.cu(22): here

/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_kernel.h(276): warning: variable "tQrQ" was declared but never referenced
          detected during:
            instantiation of "void flash::compute_attn_1rowblock<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params,Is_Q_split_K>(const Params &, int, int, int) [with Kernel_traits=Flash_fwd_kernel_traits<96, 128, 64, 4, false, false, cutlass::half_t, Flash_kernel_traits<96, 128, 64, 4, cutlass::half_t>>, Is_dropout=true, Is_causal=false, Is_even_N=false, Is_even_K=true, Return_softmax=false, Params=Flash_fwd_params, Is_Q_split_K=false]" 
(589): here
            instantiation of "void flash::compute_attn<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &) [with Kernel_traits=Flash_fwd_kernel_traits<96, 128, 64, 4, false, false, cutlass::half_t, Flash_kernel_traits<96, 128, 64, 4, cutlass::half_t>>, Is_dropout=true, Is_causal=false, Is_even_N=false, Is_even_K=true, Return_softmax=false, Params=Flash_fwd_params]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(15): here
            instantiation of "void flash_fwd_kernel<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax>(Flash_fwd_params) [with Kernel_traits=Flash_fwd_kernel_traits<96, 128, 64, 4, false, false, cutlass::half_t, Flash_kernel_traits<96, 128, 64, 4, cutlass::half_t>>, Is_dropout=true, Is_causal=false, Is_even_N=false, Is_even_K=true, Return_softmax=false]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(34): here
            instantiation of "void run_flash_fwd<Kernel_traits,Is_dropout,Is_causal>(Flash_fwd_params &, cudaStream_t) [with Kernel_traits=Flash_fwd_kernel_traits<96, 128, 64, 4, false, false, cutlass::half_t, Flash_kernel_traits<96, 128, 64, 4, cutlass::half_t>>, Is_dropout=true, Is_causal=false]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(92): here
            instantiation of "void run_mha_fwd_hdim96<T>(Flash_fwd_params &, cudaStream_t) [with T=cutlass::half_t]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_hdim96_fp16_sm80.cu(22): here

/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_kernel.h(140): warning: variable "MMA_M" was declared but never referenced
          detected during:
            instantiation of "void flash::compute_attn_1rowblock<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params,Is_Q_split_K>(const Params &, int, int, int) [with Kernel_traits=Flash_fwd_kernel_traits<96, 128, 64, 4, false, false, cutlass::half_t, Flash_kernel_traits<96, 128, 64, 4, cutlass::half_t>>, Is_dropout=true, Is_causal=false, Is_even_N=false, Is_even_K=false, Return_softmax=true, Params=Flash_fwd_params, Is_Q_split_K=false]" 
(589): here
            instantiation of "void flash::compute_attn<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &) [with Kernel_traits=Flash_fwd_kernel_traits<96, 128, 64, 4, false, false, cutlass::half_t, Flash_kernel_traits<96, 128, 64, 4, cutlass::half_t>>, Is_dropout=true, Is_causal=false, Is_even_N=false, Is_even_K=false, Return_softmax=true, Params=Flash_fwd_params]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(15): here
            instantiation of "void flash_fwd_kernel<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax>(Flash_fwd_params) [with Kernel_traits=Flash_fwd_kernel_traits<96, 128, 64, 4, false, false, cutlass::half_t, Flash_kernel_traits<96, 128, 64, 4, cutlass::half_t>>, Is_dropout=true, Is_causal=false, Is_even_N=false, Is_even_K=false, Return_softmax=true]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(34): here
            instantiation of "void run_flash_fwd<Kernel_traits,Is_dropout,Is_causal>(Flash_fwd_params &, cudaStream_t) [with Kernel_traits=Flash_fwd_kernel_traits<96, 128, 64, 4, false, false, cutlass::half_t, Flash_kernel_traits<96, 128, 64, 4, cutlass::half_t>>, Is_dropout=true, Is_causal=false]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(92): here
            instantiation of "void run_mha_fwd_hdim96<T>(Flash_fwd_params &, cudaStream_t) [with T=cutlass::half_t]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_hdim96_fp16_sm80.cu(22): here

/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_kernel.h(276): warning: variable "tQrQ" was declared but never referenced
          detected during:
            instantiation of "void flash::compute_attn_1rowblock<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params,Is_Q_split_K>(const Params &, int, int, int) [with Kernel_traits=Flash_fwd_kernel_traits<96, 128, 64, 4, false, false, cutlass::half_t, Flash_kernel_traits<96, 128, 64, 4, cutlass::half_t>>, Is_dropout=true, Is_causal=false, Is_even_N=false, Is_even_K=false, Return_softmax=true, Params=Flash_fwd_params, Is_Q_split_K=false]" 
(589): here
            instantiation of "void flash::compute_attn<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &) [with Kernel_traits=Flash_fwd_kernel_traits<96, 128, 64, 4, false, false, cutlass::half_t, Flash_kernel_traits<96, 128, 64, 4, cutlass::half_t>>, Is_dropout=true, Is_causal=false, Is_even_N=false, Is_even_K=false, Return_softmax=true, Params=Flash_fwd_params]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(15): here
            instantiation of "void flash_fwd_kernel<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax>(Flash_fwd_params) [with Kernel_traits=Flash_fwd_kernel_traits<96, 128, 64, 4, false, false, cutlass::half_t, Flash_kernel_traits<96, 128, 64, 4, cutlass::half_t>>, Is_dropout=true, Is_causal=false, Is_even_N=false, Is_even_K=false, Return_softmax=true]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(34): here
            instantiation of "void run_flash_fwd<Kernel_traits,Is_dropout,Is_causal>(Flash_fwd_params &, cudaStream_t) [with Kernel_traits=Flash_fwd_kernel_traits<96, 128, 64, 4, false, false, cutlass::half_t, Flash_kernel_traits<96, 128, 64, 4, cutlass::half_t>>, Is_dropout=true, Is_causal=false]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(92): here
            instantiation of "void run_mha_fwd_hdim96<T>(Flash_fwd_params &, cudaStream_t) [with T=cutlass::half_t]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_hdim96_fp16_sm80.cu(22): here

/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_kernel.h(140): warning: variable "MMA_M" was declared but never referenced
          detected during:
            instantiation of "void flash::compute_attn_1rowblock<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params,Is_Q_split_K>(const Params &, int, int, int) [with Kernel_traits=Flash_fwd_kernel_traits<96, 128, 64, 4, false, false, cutlass::half_t, Flash_kernel_traits<96, 128, 64, 4, cutlass::half_t>>, Is_dropout=true, Is_causal=false, Is_even_N=false, Is_even_K=false, Return_softmax=false, Params=Flash_fwd_params, Is_Q_split_K=false]" 
(589): here
            instantiation of "void flash::compute_attn<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &) [with Kernel_traits=Flash_fwd_kernel_traits<96, 128, 64, 4, false, false, cutlass::half_t, Flash_kernel_traits<96, 128, 64, 4, cutlass::half_t>>, Is_dropout=true, Is_causal=false, Is_even_N=false, Is_even_K=false, Return_softmax=false, Params=Flash_fwd_params]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(15): here
            instantiation of "void flash_fwd_kernel<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax>(Flash_fwd_params) [with Kernel_traits=Flash_fwd_kernel_traits<96, 128, 64, 4, false, false, cutlass::half_t, Flash_kernel_traits<96, 128, 64, 4, cutlass::half_t>>, Is_dropout=true, Is_causal=false, Is_even_N=false, Is_even_K=false, Return_softmax=false]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(34): here
            instantiation of "void run_flash_fwd<Kernel_traits,Is_dropout,Is_causal>(Flash_fwd_params &, cudaStream_t) [with Kernel_traits=Flash_fwd_kernel_traits<96, 128, 64, 4, false, false, cutlass::half_t, Flash_kernel_traits<96, 128, 64, 4, cutlass::half_t>>, Is_dropout=true, Is_causal=false]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(92): here
            instantiation of "void run_mha_fwd_hdim96<T>(Flash_fwd_params &, cudaStream_t) [with T=cutlass::half_t]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_hdim96_fp16_sm80.cu(22): here

/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_kernel.h(276): warning: variable "tQrQ" was declared but never referenced
          detected during:
            instantiation of "void flash::compute_attn_1rowblock<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params,Is_Q_split_K>(const Params &, int, int, int) [with Kernel_traits=Flash_fwd_kernel_traits<96, 128, 64, 4, false, false, cutlass::half_t, Flash_kernel_traits<96, 128, 64, 4, cutlass::half_t>>, Is_dropout=true, Is_causal=false, Is_even_N=false, Is_even_K=false, Return_softmax=false, Params=Flash_fwd_params, Is_Q_split_K=false]" 
(589): here
            instantiation of "void flash::compute_attn<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &) [with Kernel_traits=Flash_fwd_kernel_traits<96, 128, 64, 4, false, false, cutlass::half_t, Flash_kernel_traits<96, 128, 64, 4, cutlass::half_t>>, Is_dropout=true, Is_causal=false, Is_even_N=false, Is_even_K=false, Return_softmax=false, Params=Flash_fwd_params]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(15): here
            instantiation of "void flash_fwd_kernel<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax>(Flash_fwd_params) [with Kernel_traits=Flash_fwd_kernel_traits<96, 128, 64, 4, false, false, cutlass::half_t, Flash_kernel_traits<96, 128, 64, 4, cutlass::half_t>>, Is_dropout=true, Is_causal=false, Is_even_N=false, Is_even_K=false, Return_softmax=false]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(34): here
            instantiation of "void run_flash_fwd<Kernel_traits,Is_dropout,Is_causal>(Flash_fwd_params &, cudaStream_t) [with Kernel_traits=Flash_fwd_kernel_traits<96, 128, 64, 4, false, false, cutlass::half_t, Flash_kernel_traits<96, 128, 64, 4, cutlass::half_t>>, Is_dropout=true, Is_causal=false]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(92): here
            instantiation of "void run_mha_fwd_hdim96<T>(Flash_fwd_params &, cudaStream_t) [with T=cutlass::half_t]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_hdim96_fp16_sm80.cu(22): here

/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_kernel.h(140): warning: variable "MMA_M" was declared but never referenced
          detected during:
            instantiation of "void flash::compute_attn_1rowblock<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params,Is_Q_split_K>(const Params &, int, int, int) [with Kernel_traits=Flash_fwd_kernel_traits<96, 64, 64, 4, false, false, cutlass::half_t, Flash_kernel_traits<96, 64, 64, 4, cutlass::half_t>>, Is_dropout=false, Is_causal=true, Is_even_N=true, Is_even_K=true, Return_softmax=false, Params=Flash_fwd_params, Is_Q_split_K=false]" 
(589): here
            instantiation of "void flash::compute_attn<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &) [with Kernel_traits=Flash_fwd_kernel_traits<96, 64, 64, 4, false, false, cutlass::half_t, Flash_kernel_traits<96, 64, 64, 4, cutlass::half_t>>, Is_dropout=false, Is_causal=true, Is_even_N=true, Is_even_K=true, Return_softmax=false, Params=Flash_fwd_params]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(15): here
            instantiation of "void flash_fwd_kernel<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax>(Flash_fwd_params) [with Kernel_traits=Flash_fwd_kernel_traits<96, 64, 64, 4, false, false, cutlass::half_t, Flash_kernel_traits<96, 64, 64, 4, cutlass::half_t>>, Is_dropout=false, Is_causal=true, Is_even_N=true, Is_even_K=true, Return_softmax=false]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(34): here
            instantiation of "void run_flash_fwd<Kernel_traits,Is_dropout,Is_causal>(Flash_fwd_params &, cudaStream_t) [with Kernel_traits=Flash_fwd_kernel_traits<96, 64, 64, 4, false, false, cutlass::half_t, Flash_kernel_traits<96, 64, 64, 4, cutlass::half_t>>, Is_dropout=false, Is_causal=true]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(92): here
            instantiation of "void run_mha_fwd_hdim96<T>(Flash_fwd_params &, cudaStream_t) [with T=cutlass::half_t]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_hdim96_fp16_sm80.cu(22): here

/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_kernel.h(276): warning: variable "tQrQ" was declared but never referenced
          detected during:
            instantiation of "void flash::compute_attn_1rowblock<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params,Is_Q_split_K>(const Params &, int, int, int) [with Kernel_traits=Flash_fwd_kernel_traits<96, 64, 64, 4, false, false, cutlass::half_t, Flash_kernel_traits<96, 64, 64, 4, cutlass::half_t>>, Is_dropout=false, Is_causal=true, Is_even_N=true, Is_even_K=true, Return_softmax=false, Params=Flash_fwd_params, Is_Q_split_K=false]" 
(589): here
            instantiation of "void flash::compute_attn<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &) [with Kernel_traits=Flash_fwd_kernel_traits<96, 64, 64, 4, false, false, cutlass::half_t, Flash_kernel_traits<96, 64, 64, 4, cutlass::half_t>>, Is_dropout=false, Is_causal=true, Is_even_N=true, Is_even_K=true, Return_softmax=false, Params=Flash_fwd_params]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(15): here
            instantiation of "void flash_fwd_kernel<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax>(Flash_fwd_params) [with Kernel_traits=Flash_fwd_kernel_traits<96, 64, 64, 4, false, false, cutlass::half_t, Flash_kernel_traits<96, 64, 64, 4, cutlass::half_t>>, Is_dropout=false, Is_causal=true, Is_even_N=true, Is_even_K=true, Return_softmax=false]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(34): here
            instantiation of "void run_flash_fwd<Kernel_traits,Is_dropout,Is_causal>(Flash_fwd_params &, cudaStream_t) [with Kernel_traits=Flash_fwd_kernel_traits<96, 64, 64, 4, false, false, cutlass::half_t, Flash_kernel_traits<96, 64, 64, 4, cutlass::half_t>>, Is_dropout=false, Is_causal=true]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(92): here
            instantiation of "void run_mha_fwd_hdim96<T>(Flash_fwd_params &, cudaStream_t) [with T=cutlass::half_t]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_hdim96_fp16_sm80.cu(22): here

/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_kernel.h(140): warning: variable "MMA_M" was declared but never referenced
          detected during:
            instantiation of "void flash::compute_attn_1rowblock<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params,Is_Q_split_K>(const Params &, int, int, int) [with Kernel_traits=Flash_fwd_kernel_traits<96, 64, 64, 4, false, false, cutlass::half_t, Flash_kernel_traits<96, 64, 64, 4, cutlass::half_t>>, Is_dropout=false, Is_causal=true, Is_even_N=true, Is_even_K=false, Return_softmax=false, Params=Flash_fwd_params, Is_Q_split_K=false]" 
(589): here
            instantiation of "void flash::compute_attn<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &) [with Kernel_traits=Flash_fwd_kernel_traits<96, 64, 64, 4, false, false, cutlass::half_t, Flash_kernel_traits<96, 64, 64, 4, cutlass::half_t>>, Is_dropout=false, Is_causal=true, Is_even_N=true, Is_even_K=false, Return_softmax=false, Params=Flash_fwd_params]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(15): here
            instantiation of "void flash_fwd_kernel<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax>(Flash_fwd_params) [with Kernel_traits=Flash_fwd_kernel_traits<96, 64, 64, 4, false, false, cutlass::half_t, Flash_kernel_traits<96, 64, 64, 4, cutlass::half_t>>, Is_dropout=false, Is_causal=true, Is_even_N=true, Is_even_K=false, Return_softmax=false]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(34): here
            instantiation of "void run_flash_fwd<Kernel_traits,Is_dropout,Is_causal>(Flash_fwd_params &, cudaStream_t) [with Kernel_traits=Flash_fwd_kernel_traits<96, 64, 64, 4, false, false, cutlass::half_t, Flash_kernel_traits<96, 64, 64, 4, cutlass::half_t>>, Is_dropout=false, Is_causal=true]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(92): here
            instantiation of "void run_mha_fwd_hdim96<T>(Flash_fwd_params &, cudaStream_t) [with T=cutlass::half_t]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_hdim96_fp16_sm80.cu(22): here

/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_kernel.h(276): warning: variable "tQrQ" was declared but never referenced
          detected during:
            instantiation of "void flash::compute_attn_1rowblock<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params,Is_Q_split_K>(const Params &, int, int, int) [with Kernel_traits=Flash_fwd_kernel_traits<96, 64, 64, 4, false, false, cutlass::half_t, Flash_kernel_traits<96, 64, 64, 4, cutlass::half_t>>, Is_dropout=false, Is_causal=true, Is_even_N=true, Is_even_K=false, Return_softmax=false, Params=Flash_fwd_params, Is_Q_split_K=false]" 
(589): here
            instantiation of "void flash::compute_attn<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &) [with Kernel_traits=Flash_fwd_kernel_traits<96, 64, 64, 4, false, false, cutlass::half_t, Flash_kernel_traits<96, 64, 64, 4, cutlass::half_t>>, Is_dropout=false, Is_causal=true, Is_even_N=true, Is_even_K=false, Return_softmax=false, Params=Flash_fwd_params]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(15): here
            instantiation of "void flash_fwd_kernel<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax>(Flash_fwd_params) [with Kernel_traits=Flash_fwd_kernel_traits<96, 64, 64, 4, false, false, cutlass::half_t, Flash_kernel_traits<96, 64, 64, 4, cutlass::half_t>>, Is_dropout=false, Is_causal=true, Is_even_N=true, Is_even_K=false, Return_softmax=false]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(34): here
            instantiation of "void run_flash_fwd<Kernel_traits,Is_dropout,Is_causal>(Flash_fwd_params &, cudaStream_t) [with Kernel_traits=Flash_fwd_kernel_traits<96, 64, 64, 4, false, false, cutlass::half_t, Flash_kernel_traits<96, 64, 64, 4, cutlass::half_t>>, Is_dropout=false, Is_causal=true]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(92): here
            instantiation of "void run_mha_fwd_hdim96<T>(Flash_fwd_params &, cudaStream_t) [with T=cutlass::half_t]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_hdim96_fp16_sm80.cu(22): here

/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_kernel.h(140): warning: variable "MMA_M" was declared but never referenced
          detected during:
            instantiation of "void flash::compute_attn_1rowblock<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params,Is_Q_split_K>(const Params &, int, int, int) [with Kernel_traits=Flash_fwd_kernel_traits<96, 64, 64, 4, false, false, cutlass::half_t, Flash_kernel_traits<96, 64, 64, 4, cutlass::half_t>>, Is_dropout=false, Is_causal=true, Is_even_N=false, Is_even_K=true, Return_softmax=false, Params=Flash_fwd_params, Is_Q_split_K=false]" 
(589): here
            instantiation of "void flash::compute_attn<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &) [with Kernel_traits=Flash_fwd_kernel_traits<96, 64, 64, 4, false, false, cutlass::half_t, Flash_kernel_traits<96, 64, 64, 4, cutlass::half_t>>, Is_dropout=false, Is_causal=true, Is_even_N=false, Is_even_K=true, Return_softmax=false, Params=Flash_fwd_params]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(15): here
            instantiation of "void flash_fwd_kernel<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax>(Flash_fwd_params) [with Kernel_traits=Flash_fwd_kernel_traits<96, 64, 64, 4, false, false, cutlass::half_t, Flash_kernel_traits<96, 64, 64, 4, cutlass::half_t>>, Is_dropout=false, Is_causal=true, Is_even_N=false, Is_even_K=true, Return_softmax=false]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(34): here
            instantiation of "void run_flash_fwd<Kernel_traits,Is_dropout,Is_causal>(Flash_fwd_params &, cudaStream_t) [with Kernel_traits=Flash_fwd_kernel_traits<96, 64, 64, 4, false, false, cutlass::half_t, Flash_kernel_traits<96, 64, 64, 4, cutlass::half_t>>, Is_dropout=false, Is_causal=true]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(92): here
            instantiation of "void run_mha_fwd_hdim96<T>(Flash_fwd_params &, cudaStream_t) [with T=cutlass::half_t]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_hdim96_fp16_sm80.cu(22): here

/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_kernel.h(276): warning: variable "tQrQ" was declared but never referenced
          detected during:
            instantiation of "void flash::compute_attn_1rowblock<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params,Is_Q_split_K>(const Params &, int, int, int) [with Kernel_traits=Flash_fwd_kernel_traits<96, 64, 64, 4, false, false, cutlass::half_t, Flash_kernel_traits<96, 64, 64, 4, cutlass::half_t>>, Is_dropout=false, Is_causal=true, Is_even_N=false, Is_even_K=true, Return_softmax=false, Params=Flash_fwd_params, Is_Q_split_K=false]" 
(589): here
            instantiation of "void flash::compute_attn<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &) [with Kernel_traits=Flash_fwd_kernel_traits<96, 64, 64, 4, false, false, cutlass::half_t, Flash_kernel_traits<96, 64, 64, 4, cutlass::half_t>>, Is_dropout=false, Is_causal=true, Is_even_N=false, Is_even_K=true, Return_softmax=false, Params=Flash_fwd_params]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(15): here
            instantiation of "void flash_fwd_kernel<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax>(Flash_fwd_params) [with Kernel_traits=Flash_fwd_kernel_traits<96, 64, 64, 4, false, false, cutlass::half_t, Flash_kernel_traits<96, 64, 64, 4, cutlass::half_t>>, Is_dropout=false, Is_causal=true, Is_even_N=false, Is_even_K=true, Return_softmax=false]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(34): here
            instantiation of "void run_flash_fwd<Kernel_traits,Is_dropout,Is_causal>(Flash_fwd_params &, cudaStream_t) [with Kernel_traits=Flash_fwd_kernel_traits<96, 64, 64, 4, false, false, cutlass::half_t, Flash_kernel_traits<96, 64, 64, 4, cutlass::half_t>>, Is_dropout=false, Is_causal=true]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(92): here
            instantiation of "void run_mha_fwd_hdim96<T>(Flash_fwd_params &, cudaStream_t) [with T=cutlass::half_t]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_hdim96_fp16_sm80.cu(22): here

/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_kernel.h(140): warning: variable "MMA_M" was declared but never referenced
          detected during:
            instantiation of "void flash::compute_attn_1rowblock<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params,Is_Q_split_K>(const Params &, int, int, int) [with Kernel_traits=Flash_fwd_kernel_traits<96, 64, 64, 4, false, false, cutlass::half_t, Flash_kernel_traits<96, 64, 64, 4, cutlass::half_t>>, Is_dropout=false, Is_causal=true, Is_even_N=false, Is_even_K=false, Return_softmax=false, Params=Flash_fwd_params, Is_Q_split_K=false]" 
(589): here
            instantiation of "void flash::compute_attn<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &) [with Kernel_traits=Flash_fwd_kernel_traits<96, 64, 64, 4, false, false, cutlass::half_t, Flash_kernel_traits<96, 64, 64, 4, cutlass::half_t>>, Is_dropout=false, Is_causal=true, Is_even_N=false, Is_even_K=false, Return_softmax=false, Params=Flash_fwd_params]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(15): here
            instantiation of "void flash_fwd_kernel<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax>(Flash_fwd_params) [with Kernel_traits=Flash_fwd_kernel_traits<96, 64, 64, 4, false, false, cutlass::half_t, Flash_kernel_traits<96, 64, 64, 4, cutlass::half_t>>, Is_dropout=false, Is_causal=true, Is_even_N=false, Is_even_K=false, Return_softmax=false]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(34): here
            instantiation of "void run_flash_fwd<Kernel_traits,Is_dropout,Is_causal>(Flash_fwd_params &, cudaStream_t) [with Kernel_traits=Flash_fwd_kernel_traits<96, 64, 64, 4, false, false, cutlass::half_t, Flash_kernel_traits<96, 64, 64, 4, cutlass::half_t>>, Is_dropout=false, Is_causal=true]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(92): here
            instantiation of "void run_mha_fwd_hdim96<T>(Flash_fwd_params &, cudaStream_t) [with T=cutlass::half_t]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_hdim96_fp16_sm80.cu(22): here

/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_kernel.h(276): warning: variable "tQrQ" was declared but never referenced
          detected during:
            instantiation of "void flash::compute_attn_1rowblock<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params,Is_Q_split_K>(const Params &, int, int, int) [with Kernel_traits=Flash_fwd_kernel_traits<96, 64, 64, 4, false, false, cutlass::half_t, Flash_kernel_traits<96, 64, 64, 4, cutlass::half_t>>, Is_dropout=false, Is_causal=true, Is_even_N=false, Is_even_K=false, Return_softmax=false, Params=Flash_fwd_params, Is_Q_split_K=false]" 
(589): here
            instantiation of "void flash::compute_attn<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &) [with Kernel_traits=Flash_fwd_kernel_traits<96, 64, 64, 4, false, false, cutlass::half_t, Flash_kernel_traits<96, 64, 64, 4, cutlass::half_t>>, Is_dropout=false, Is_causal=true, Is_even_N=false, Is_even_K=false, Return_softmax=false, Params=Flash_fwd_params]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(15): here
            instantiation of "void flash_fwd_kernel<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax>(Flash_fwd_params) [with Kernel_traits=Flash_fwd_kernel_traits<96, 64, 64, 4, false, false, cutlass::half_t, Flash_kernel_traits<96, 64, 64, 4, cutlass::half_t>>, Is_dropout=false, Is_causal=true, Is_even_N=false, Is_even_K=false, Return_softmax=false]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(34): here
            instantiation of "void run_flash_fwd<Kernel_traits,Is_dropout,Is_causal>(Flash_fwd_params &, cudaStream_t) [with Kernel_traits=Flash_fwd_kernel_traits<96, 64, 64, 4, false, false, cutlass::half_t, Flash_kernel_traits<96, 64, 64, 4, cutlass::half_t>>, Is_dropout=false, Is_causal=true]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(92): here
            instantiation of "void run_mha_fwd_hdim96<T>(Flash_fwd_params &, cudaStream_t) [with T=cutlass::half_t]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_hdim96_fp16_sm80.cu(22): here

/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_kernel.h(140): warning: variable "MMA_M" was declared but never referenced
          detected during:
            instantiation of "void flash::compute_attn_1rowblock<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params,Is_Q_split_K>(const Params &, int, int, int) [with Kernel_traits=Flash_fwd_kernel_traits<96, 128, 64, 4, false, false, cutlass::half_t, Flash_kernel_traits<96, 128, 64, 4, cutlass::half_t>>, Is_dropout=false, Is_causal=true, Is_even_N=true, Is_even_K=true, Return_softmax=false, Params=Flash_fwd_params, Is_Q_split_K=false]" 
(589): here
            instantiation of "void flash::compute_attn<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &) [with Kernel_traits=Flash_fwd_kernel_traits<96, 128, 64, 4, false, false, cutlass::half_t, Flash_kernel_traits<96, 128, 64, 4, cutlass::half_t>>, Is_dropout=false, Is_causal=true, Is_even_N=true, Is_even_K=true, Return_softmax=false, Params=Flash_fwd_params]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(15): here
            instantiation of "void flash_fwd_kernel<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax>(Flash_fwd_params) [with Kernel_traits=Flash_fwd_kernel_traits<96, 128, 64, 4, false, false, cutlass::half_t, Flash_kernel_traits<96, 128, 64, 4, cutlass::half_t>>, Is_dropout=false, Is_causal=true, Is_even_N=true, Is_even_K=true, Return_softmax=false]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(34): here
            instantiation of "void run_flash_fwd<Kernel_traits,Is_dropout,Is_causal>(Flash_fwd_params &, cudaStream_t) [with Kernel_traits=Flash_fwd_kernel_traits<96, 128, 64, 4, false, false, cutlass::half_t, Flash_kernel_traits<96, 128, 64, 4, cutlass::half_t>>, Is_dropout=false, Is_causal=true]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(92): here
            instantiation of "void run_mha_fwd_hdim96<T>(Flash_fwd_params &, cudaStream_t) [with T=cutlass::half_t]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_hdim96_fp16_sm80.cu(22): here

/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_kernel.h(276): warning: variable "tQrQ" was declared but never referenced
          detected during:
            instantiation of "void flash::compute_attn_1rowblock<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params,Is_Q_split_K>(const Params &, int, int, int) [with Kernel_traits=Flash_fwd_kernel_traits<96, 128, 64, 4, false, false, cutlass::half_t, Flash_kernel_traits<96, 128, 64, 4, cutlass::half_t>>, Is_dropout=false, Is_causal=true, Is_even_N=true, Is_even_K=true, Return_softmax=false, Params=Flash_fwd_params, Is_Q_split_K=false]" 
(589): here
            instantiation of "void flash::compute_attn<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &) [with Kernel_traits=Flash_fwd_kernel_traits<96, 128, 64, 4, false, false, cutlass::half_t, Flash_kernel_traits<96, 128, 64, 4, cutlass::half_t>>, Is_dropout=false, Is_causal=true, Is_even_N=true, Is_even_K=true, Return_softmax=false, Params=Flash_fwd_params]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(15): here
            instantiation of "void flash_fwd_kernel<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax>(Flash_fwd_params) [with Kernel_traits=Flash_fwd_kernel_traits<96, 128, 64, 4, false, false, cutlass::half_t, Flash_kernel_traits<96, 128, 64, 4, cutlass::half_t>>, Is_dropout=false, Is_causal=true, Is_even_N=true, Is_even_K=true, Return_softmax=false]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(34): here
            instantiation of "void run_flash_fwd<Kernel_traits,Is_dropout,Is_causal>(Flash_fwd_params &, cudaStream_t) [with Kernel_traits=Flash_fwd_kernel_traits<96, 128, 64, 4, false, false, cutlass::half_t, Flash_kernel_traits<96, 128, 64, 4, cutlass::half_t>>, Is_dropout=false, Is_causal=true]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(92): here
            instantiation of "void run_mha_fwd_hdim96<T>(Flash_fwd_params &, cudaStream_t) [with T=cutlass::half_t]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_hdim96_fp16_sm80.cu(22): here

/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_kernel.h(140): warning: variable "MMA_M" was declared but never referenced
          detected during:
            instantiation of "void flash::compute_attn_1rowblock<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params,Is_Q_split_K>(const Params &, int, int, int) [with Kernel_traits=Flash_fwd_kernel_traits<96, 128, 64, 4, false, false, cutlass::half_t, Flash_kernel_traits<96, 128, 64, 4, cutlass::half_t>>, Is_dropout=false, Is_causal=true, Is_even_N=true, Is_even_K=false, Return_softmax=false, Params=Flash_fwd_params, Is_Q_split_K=false]" 
(589): here
            instantiation of "void flash::compute_attn<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &) [with Kernel_traits=Flash_fwd_kernel_traits<96, 128, 64, 4, false, false, cutlass::half_t, Flash_kernel_traits<96, 128, 64, 4, cutlass::half_t>>, Is_dropout=false, Is_causal=true, Is_even_N=true, Is_even_K=false, Return_softmax=false, Params=Flash_fwd_params]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(15): here
            instantiation of "void flash_fwd_kernel<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax>(Flash_fwd_params) [with Kernel_traits=Flash_fwd_kernel_traits<96, 128, 64, 4, false, false, cutlass::half_t, Flash_kernel_traits<96, 128, 64, 4, cutlass::half_t>>, Is_dropout=false, Is_causal=true, Is_even_N=true, Is_even_K=false, Return_softmax=false]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(34): here
            instantiation of "void run_flash_fwd<Kernel_traits,Is_dropout,Is_causal>(Flash_fwd_params &, cudaStream_t) [with Kernel_traits=Flash_fwd_kernel_traits<96, 128, 64, 4, false, false, cutlass::half_t, Flash_kernel_traits<96, 128, 64, 4, cutlass::half_t>>, Is_dropout=false, Is_causal=true]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(92): here
            instantiation of "void run_mha_fwd_hdim96<T>(Flash_fwd_params &, cudaStream_t) [with T=cutlass::half_t]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_hdim96_fp16_sm80.cu(22): here

/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_kernel.h(276): warning: variable "tQrQ" was declared but never referenced
          detected during:
            instantiation of "void flash::compute_attn_1rowblock<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params,Is_Q_split_K>(const Params &, int, int, int) [with Kernel_traits=Flash_fwd_kernel_traits<96, 128, 64, 4, false, false, cutlass::half_t, Flash_kernel_traits<96, 128, 64, 4, cutlass::half_t>>, Is_dropout=false, Is_causal=true, Is_even_N=true, Is_even_K=false, Return_softmax=false, Params=Flash_fwd_params, Is_Q_split_K=false]" 
(589): here
            instantiation of "void flash::compute_attn<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &) [with Kernel_traits=Flash_fwd_kernel_traits<96, 128, 64, 4, false, false, cutlass::half_t, Flash_kernel_traits<96, 128, 64, 4, cutlass::half_t>>, Is_dropout=false, Is_causal=true, Is_even_N=true, Is_even_K=false, Return_softmax=false, Params=Flash_fwd_params]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(15): here
            instantiation of "void flash_fwd_kernel<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax>(Flash_fwd_params) [with Kernel_traits=Flash_fwd_kernel_traits<96, 128, 64, 4, false, false, cutlass::half_t, Flash_kernel_traits<96, 128, 64, 4, cutlass::half_t>>, Is_dropout=false, Is_causal=true, Is_even_N=true, Is_even_K=false, Return_softmax=false]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(34): here
            instantiation of "void run_flash_fwd<Kernel_traits,Is_dropout,Is_causal>(Flash_fwd_params &, cudaStream_t) [with Kernel_traits=Flash_fwd_kernel_traits<96, 128, 64, 4, false, false, cutlass::half_t, Flash_kernel_traits<96, 128, 64, 4, cutlass::half_t>>, Is_dropout=false, Is_causal=true]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(92): here
            instantiation of "void run_mha_fwd_hdim96<T>(Flash_fwd_params &, cudaStream_t) [with T=cutlass::half_t]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_hdim96_fp16_sm80.cu(22): here

/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_kernel.h(140): warning: variable "MMA_M" was declared but never referenced
          detected during:
            instantiation of "void flash::compute_attn_1rowblock<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params,Is_Q_split_K>(const Params &, int, int, int) [with Kernel_traits=Flash_fwd_kernel_traits<96, 128, 64, 4, false, false, cutlass::half_t, Flash_kernel_traits<96, 128, 64, 4, cutlass::half_t>>, Is_dropout=false, Is_causal=true, Is_even_N=false, Is_even_K=true, Return_softmax=false, Params=Flash_fwd_params, Is_Q_split_K=false]" 
(589): here
            instantiation of "void flash::compute_attn<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &) [with Kernel_traits=Flash_fwd_kernel_traits<96, 128, 64, 4, false, false, cutlass::half_t, Flash_kernel_traits<96, 128, 64, 4, cutlass::half_t>>, Is_dropout=false, Is_causal=true, Is_even_N=false, Is_even_K=true, Return_softmax=false, Params=Flash_fwd_params]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(15): here
            instantiation of "void flash_fwd_kernel<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax>(Flash_fwd_params) [with Kernel_traits=Flash_fwd_kernel_traits<96, 128, 64, 4, false, false, cutlass::half_t, Flash_kernel_traits<96, 128, 64, 4, cutlass::half_t>>, Is_dropout=false, Is_causal=true, Is_even_N=false, Is_even_K=true, Return_softmax=false]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(34): here
            instantiation of "void run_flash_fwd<Kernel_traits,Is_dropout,Is_causal>(Flash_fwd_params &, cudaStream_t) [with Kernel_traits=Flash_fwd_kernel_traits<96, 128, 64, 4, false, false, cutlass::half_t, Flash_kernel_traits<96, 128, 64, 4, cutlass::half_t>>, Is_dropout=false, Is_causal=true]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(92): here
            instantiation of "void run_mha_fwd_hdim96<T>(Flash_fwd_params &, cudaStream_t) [with T=cutlass::half_t]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_hdim96_fp16_sm80.cu(22): here

/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_kernel.h(276): warning: variable "tQrQ" was declared but never referenced
          detected during:
            instantiation of "void flash::compute_attn_1rowblock<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params,Is_Q_split_K>(const Params &, int, int, int) [with Kernel_traits=Flash_fwd_kernel_traits<96, 128, 64, 4, false, false, cutlass::half_t, Flash_kernel_traits<96, 128, 64, 4, cutlass::half_t>>, Is_dropout=false, Is_causal=true, Is_even_N=false, Is_even_K=true, Return_softmax=false, Params=Flash_fwd_params, Is_Q_split_K=false]" 
(589): here
            instantiation of "void flash::compute_attn<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &) [with Kernel_traits=Flash_fwd_kernel_traits<96, 128, 64, 4, false, false, cutlass::half_t, Flash_kernel_traits<96, 128, 64, 4, cutlass::half_t>>, Is_dropout=false, Is_causal=true, Is_even_N=false, Is_even_K=true, Return_softmax=false, Params=Flash_fwd_params]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(15): here
            instantiation of "void flash_fwd_kernel<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax>(Flash_fwd_params) [with Kernel_traits=Flash_fwd_kernel_traits<96, 128, 64, 4, false, false, cutlass::half_t, Flash_kernel_traits<96, 128, 64, 4, cutlass::half_t>>, Is_dropout=false, Is_causal=true, Is_even_N=false, Is_even_K=true, Return_softmax=false]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(34): here
            instantiation of "void run_flash_fwd<Kernel_traits,Is_dropout,Is_causal>(Flash_fwd_params &, cudaStream_t) [with Kernel_traits=Flash_fwd_kernel_traits<96, 128, 64, 4, false, false, cutlass::half_t, Flash_kernel_traits<96, 128, 64, 4, cutlass::half_t>>, Is_dropout=false, Is_causal=true]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(92): here
            instantiation of "void run_mha_fwd_hdim96<T>(Flash_fwd_params &, cudaStream_t) [with T=cutlass::half_t]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_hdim96_fp16_sm80.cu(22): here

/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_kernel.h(140): warning: variable "MMA_M" was declared but never referenced
          detected during:
            instantiation of "void flash::compute_attn_1rowblock<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params,Is_Q_split_K>(const Params &, int, int, int) [with Kernel_traits=Flash_fwd_kernel_traits<96, 128, 64, 4, false, false, cutlass::half_t, Flash_kernel_traits<96, 128, 64, 4, cutlass::half_t>>, Is_dropout=false, Is_causal=true, Is_even_N=false, Is_even_K=false, Return_softmax=false, Params=Flash_fwd_params, Is_Q_split_K=false]" 
(589): here
            instantiation of "void flash::compute_attn<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &) [with Kernel_traits=Flash_fwd_kernel_traits<96, 128, 64, 4, false, false, cutlass::half_t, Flash_kernel_traits<96, 128, 64, 4, cutlass::half_t>>, Is_dropout=false, Is_causal=true, Is_even_N=false, Is_even_K=false, Return_softmax=false, Params=Flash_fwd_params]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(15): here
            instantiation of "void flash_fwd_kernel<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax>(Flash_fwd_params) [with Kernel_traits=Flash_fwd_kernel_traits<96, 128, 64, 4, false, false, cutlass::half_t, Flash_kernel_traits<96, 128, 64, 4, cutlass::half_t>>, Is_dropout=false, Is_causal=true, Is_even_N=false, Is_even_K=false, Return_softmax=false]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(34): here
            instantiation of "void run_flash_fwd<Kernel_traits,Is_dropout,Is_causal>(Flash_fwd_params &, cudaStream_t) [with Kernel_traits=Flash_fwd_kernel_traits<96, 128, 64, 4, false, false, cutlass::half_t, Flash_kernel_traits<96, 128, 64, 4, cutlass::half_t>>, Is_dropout=false, Is_causal=true]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(92): here
            instantiation of "void run_mha_fwd_hdim96<T>(Flash_fwd_params &, cudaStream_t) [with T=cutlass::half_t]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_hdim96_fp16_sm80.cu(22): here

/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_kernel.h(276): warning: variable "tQrQ" was declared but never referenced
          detected during:
            instantiation of "void flash::compute_attn_1rowblock<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params,Is_Q_split_K>(const Params &, int, int, int) [with Kernel_traits=Flash_fwd_kernel_traits<96, 128, 64, 4, false, false, cutlass::half_t, Flash_kernel_traits<96, 128, 64, 4, cutlass::half_t>>, Is_dropout=false, Is_causal=true, Is_even_N=false, Is_even_K=false, Return_softmax=false, Params=Flash_fwd_params, Is_Q_split_K=false]" 
(589): here
            instantiation of "void flash::compute_attn<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &) [with Kernel_traits=Flash_fwd_kernel_traits<96, 128, 64, 4, false, false, cutlass::half_t, Flash_kernel_traits<96, 128, 64, 4, cutlass::half_t>>, Is_dropout=false, Is_causal=true, Is_even_N=false, Is_even_K=false, Return_softmax=false, Params=Flash_fwd_params]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(15): here
            instantiation of "void flash_fwd_kernel<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax>(Flash_fwd_params) [with Kernel_traits=Flash_fwd_kernel_traits<96, 128, 64, 4, false, false, cutlass::half_t, Flash_kernel_traits<96, 128, 64, 4, cutlass::half_t>>, Is_dropout=false, Is_causal=true, Is_even_N=false, Is_even_K=false, Return_softmax=false]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(34): here
            instantiation of "void run_flash_fwd<Kernel_traits,Is_dropout,Is_causal>(Flash_fwd_params &, cudaStream_t) [with Kernel_traits=Flash_fwd_kernel_traits<96, 128, 64, 4, false, false, cutlass::half_t, Flash_kernel_traits<96, 128, 64, 4, cutlass::half_t>>, Is_dropout=false, Is_causal=true]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(92): here
            instantiation of "void run_mha_fwd_hdim96<T>(Flash_fwd_params &, cudaStream_t) [with T=cutlass::half_t]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_hdim96_fp16_sm80.cu(22): here

/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_kernel.h(140): warning: variable "MMA_M" was declared but never referenced
          detected during:
            instantiation of "void flash::compute_attn_1rowblock<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params,Is_Q_split_K>(const Params &, int, int, int) [with Kernel_traits=Flash_fwd_kernel_traits<96, 128, 64, 4, false, false, cutlass::half_t, Flash_kernel_traits<96, 128, 64, 4, cutlass::half_t>>, Is_dropout=false, Is_causal=false, Is_even_N=true, Is_even_K=true, Return_softmax=false, Params=Flash_fwd_params, Is_Q_split_K=false]" 
(589): here
            instantiation of "void flash::compute_attn<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &) [with Kernel_traits=Flash_fwd_kernel_traits<96, 128, 64, 4, false, false, cutlass::half_t, Flash_kernel_traits<96, 128, 64, 4, cutlass::half_t>>, Is_dropout=false, Is_causal=false, Is_even_N=true, Is_even_K=true, Return_softmax=false, Params=Flash_fwd_params]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(15): here
            instantiation of "void flash_fwd_kernel<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax>(Flash_fwd_params) [with Kernel_traits=Flash_fwd_kernel_traits<96, 128, 64, 4, false, false, cutlass::half_t, Flash_kernel_traits<96, 128, 64, 4, cutlass::half_t>>, Is_dropout=false, Is_causal=false, Is_even_N=true, Is_even_K=true, Return_softmax=false]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(34): here
            instantiation of "void run_flash_fwd<Kernel_traits,Is_dropout,Is_causal>(Flash_fwd_params &, cudaStream_t) [with Kernel_traits=Flash_fwd_kernel_traits<96, 128, 64, 4, false, false, cutlass::half_t, Flash_kernel_traits<96, 128, 64, 4, cutlass::half_t>>, Is_dropout=false, Is_causal=false]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(92): here
            instantiation of "void run_mha_fwd_hdim96<T>(Flash_fwd_params &, cudaStream_t) [with T=cutlass::half_t]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_hdim96_fp16_sm80.cu(22): here

/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_kernel.h(276): warning: variable "tQrQ" was declared but never referenced
          detected during:
            instantiation of "void flash::compute_attn_1rowblock<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params,Is_Q_split_K>(const Params &, int, int, int) [with Kernel_traits=Flash_fwd_kernel_traits<96, 128, 64, 4, false, false, cutlass::half_t, Flash_kernel_traits<96, 128, 64, 4, cutlass::half_t>>, Is_dropout=false, Is_causal=false, Is_even_N=true, Is_even_K=true, Return_softmax=false, Params=Flash_fwd_params, Is_Q_split_K=false]" 
(589): here
            instantiation of "void flash::compute_attn<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &) [with Kernel_traits=Flash_fwd_kernel_traits<96, 128, 64, 4, false, false, cutlass::half_t, Flash_kernel_traits<96, 128, 64, 4, cutlass::half_t>>, Is_dropout=false, Is_causal=false, Is_even_N=true, Is_even_K=true, Return_softmax=false, Params=Flash_fwd_params]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(15): here
            instantiation of "void flash_fwd_kernel<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax>(Flash_fwd_params) [with Kernel_traits=Flash_fwd_kernel_traits<96, 128, 64, 4, false, false, cutlass::half_t, Flash_kernel_traits<96, 128, 64, 4, cutlass::half_t>>, Is_dropout=false, Is_causal=false, Is_even_N=true, Is_even_K=true, Return_softmax=false]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(34): here
            instantiation of "void run_flash_fwd<Kernel_traits,Is_dropout,Is_causal>(Flash_fwd_params &, cudaStream_t) [with Kernel_traits=Flash_fwd_kernel_traits<96, 128, 64, 4, false, false, cutlass::half_t, Flash_kernel_traits<96, 128, 64, 4, cutlass::half_t>>, Is_dropout=false, Is_causal=false]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(92): here
            instantiation of "void run_mha_fwd_hdim96<T>(Flash_fwd_params &, cudaStream_t) [with T=cutlass::half_t]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_hdim96_fp16_sm80.cu(22): here

/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_kernel.h(140): warning: variable "MMA_M" was declared but never referenced
          detected during:
            instantiation of "void flash::compute_attn_1rowblock<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params,Is_Q_split_K>(const Params &, int, int, int) [with Kernel_traits=Flash_fwd_kernel_traits<96, 128, 64, 4, false, false, cutlass::half_t, Flash_kernel_traits<96, 128, 64, 4, cutlass::half_t>>, Is_dropout=false, Is_causal=false, Is_even_N=true, Is_even_K=false, Return_softmax=false, Params=Flash_fwd_params, Is_Q_split_K=false]" 
(589): here
            instantiation of "void flash::compute_attn<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &) [with Kernel_traits=Flash_fwd_kernel_traits<96, 128, 64, 4, false, false, cutlass::half_t, Flash_kernel_traits<96, 128, 64, 4, cutlass::half_t>>, Is_dropout=false, Is_causal=false, Is_even_N=true, Is_even_K=false, Return_softmax=false, Params=Flash_fwd_params]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(15): here
            instantiation of "void flash_fwd_kernel<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax>(Flash_fwd_params) [with Kernel_traits=Flash_fwd_kernel_traits<96, 128, 64, 4, false, false, cutlass::half_t, Flash_kernel_traits<96, 128, 64, 4, cutlass::half_t>>, Is_dropout=false, Is_causal=false, Is_even_N=true, Is_even_K=false, Return_softmax=false]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(34): here
            instantiation of "void run_flash_fwd<Kernel_traits,Is_dropout,Is_causal>(Flash_fwd_params &, cudaStream_t) [with Kernel_traits=Flash_fwd_kernel_traits<96, 128, 64, 4, false, false, cutlass::half_t, Flash_kernel_traits<96, 128, 64, 4, cutlass::half_t>>, Is_dropout=false, Is_causal=false]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(92): here
            instantiation of "void run_mha_fwd_hdim96<T>(Flash_fwd_params &, cudaStream_t) [with T=cutlass::half_t]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_hdim96_fp16_sm80.cu(22): here

/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_kernel.h(276): warning: variable "tQrQ" was declared but never referenced
          detected during:
            instantiation of "void flash::compute_attn_1rowblock<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params,Is_Q_split_K>(const Params &, int, int, int) [with Kernel_traits=Flash_fwd_kernel_traits<96, 128, 64, 4, false, false, cutlass::half_t, Flash_kernel_traits<96, 128, 64, 4, cutlass::half_t>>, Is_dropout=false, Is_causal=false, Is_even_N=true, Is_even_K=false, Return_softmax=false, Params=Flash_fwd_params, Is_Q_split_K=false]" 
(589): here
            instantiation of "void flash::compute_attn<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &) [with Kernel_traits=Flash_fwd_kernel_traits<96, 128, 64, 4, false, false, cutlass::half_t, Flash_kernel_traits<96, 128, 64, 4, cutlass::half_t>>, Is_dropout=false, Is_causal=false, Is_even_N=true, Is_even_K=false, Return_softmax=false, Params=Flash_fwd_params]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(15): here
            instantiation of "void flash_fwd_kernel<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax>(Flash_fwd_params) [with Kernel_traits=Flash_fwd_kernel_traits<96, 128, 64, 4, false, false, cutlass::half_t, Flash_kernel_traits<96, 128, 64, 4, cutlass::half_t>>, Is_dropout=false, Is_causal=false, Is_even_N=true, Is_even_K=false, Return_softmax=false]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(34): here
            instantiation of "void run_flash_fwd<Kernel_traits,Is_dropout,Is_causal>(Flash_fwd_params &, cudaStream_t) [with Kernel_traits=Flash_fwd_kernel_traits<96, 128, 64, 4, false, false, cutlass::half_t, Flash_kernel_traits<96, 128, 64, 4, cutlass::half_t>>, Is_dropout=false, Is_causal=false]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(92): here
            instantiation of "void run_mha_fwd_hdim96<T>(Flash_fwd_params &, cudaStream_t) [with T=cutlass::half_t]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_hdim96_fp16_sm80.cu(22): here

/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_kernel.h(140): warning: variable "MMA_M" was declared but never referenced
          detected during:
            instantiation of "void flash::compute_attn_1rowblock<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params,Is_Q_split_K>(const Params &, int, int, int) [with Kernel_traits=Flash_fwd_kernel_traits<96, 128, 64, 4, false, false, cutlass::half_t, Flash_kernel_traits<96, 128, 64, 4, cutlass::half_t>>, Is_dropout=false, Is_causal=false, Is_even_N=false, Is_even_K=true, Return_softmax=false, Params=Flash_fwd_params, Is_Q_split_K=false]" 
(589): here
            instantiation of "void flash::compute_attn<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &) [with Kernel_traits=Flash_fwd_kernel_traits<96, 128, 64, 4, false, false, cutlass::half_t, Flash_kernel_traits<96, 128, 64, 4, cutlass::half_t>>, Is_dropout=false, Is_causal=false, Is_even_N=false, Is_even_K=true, Return_softmax=false, Params=Flash_fwd_params]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(15): here
            instantiation of "void flash_fwd_kernel<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax>(Flash_fwd_params) [with Kernel_traits=Flash_fwd_kernel_traits<96, 128, 64, 4, false, false, cutlass::half_t, Flash_kernel_traits<96, 128, 64, 4, cutlass::half_t>>, Is_dropout=false, Is_causal=false, Is_even_N=false, Is_even_K=true, Return_softmax=false]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(34): here
            instantiation of "void run_flash_fwd<Kernel_traits,Is_dropout,Is_causal>(Flash_fwd_params &, cudaStream_t) [with Kernel_traits=Flash_fwd_kernel_traits<96, 128, 64, 4, false, false, cutlass::half_t, Flash_kernel_traits<96, 128, 64, 4, cutlass::half_t>>, Is_dropout=false, Is_causal=false]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(92): here
            instantiation of "void run_mha_fwd_hdim96<T>(Flash_fwd_params &, cudaStream_t) [with T=cutlass::half_t]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_hdim96_fp16_sm80.cu(22): here

/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_kernel.h(276): warning: variable "tQrQ" was declared but never referenced
          detected during:
            instantiation of "void flash::compute_attn_1rowblock<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params,Is_Q_split_K>(const Params &, int, int, int) [with Kernel_traits=Flash_fwd_kernel_traits<96, 128, 64, 4, false, false, cutlass::half_t, Flash_kernel_traits<96, 128, 64, 4, cutlass::half_t>>, Is_dropout=false, Is_causal=false, Is_even_N=false, Is_even_K=true, Return_softmax=false, Params=Flash_fwd_params, Is_Q_split_K=false]" 
(589): here
            instantiation of "void flash::compute_attn<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &) [with Kernel_traits=Flash_fwd_kernel_traits<96, 128, 64, 4, false, false, cutlass::half_t, Flash_kernel_traits<96, 128, 64, 4, cutlass::half_t>>, Is_dropout=false, Is_causal=false, Is_even_N=false, Is_even_K=true, Return_softmax=false, Params=Flash_fwd_params]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(15): here
            instantiation of "void flash_fwd_kernel<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax>(Flash_fwd_params) [with Kernel_traits=Flash_fwd_kernel_traits<96, 128, 64, 4, false, false, cutlass::half_t, Flash_kernel_traits<96, 128, 64, 4, cutlass::half_t>>, Is_dropout=false, Is_causal=false, Is_even_N=false, Is_even_K=true, Return_softmax=false]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(34): here
            instantiation of "void run_flash_fwd<Kernel_traits,Is_dropout,Is_causal>(Flash_fwd_params &, cudaStream_t) [with Kernel_traits=Flash_fwd_kernel_traits<96, 128, 64, 4, false, false, cutlass::half_t, Flash_kernel_traits<96, 128, 64, 4, cutlass::half_t>>, Is_dropout=false, Is_causal=false]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(92): here
            instantiation of "void run_mha_fwd_hdim96<T>(Flash_fwd_params &, cudaStream_t) [with T=cutlass::half_t]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_hdim96_fp16_sm80.cu(22): here

/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_kernel.h(140): warning: variable "MMA_M" was declared but never referenced
          detected during:
            instantiation of "void flash::compute_attn_1rowblock<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params,Is_Q_split_K>(const Params &, int, int, int) [with Kernel_traits=Flash_fwd_kernel_traits<96, 128, 64, 4, false, false, cutlass::half_t, Flash_kernel_traits<96, 128, 64, 4, cutlass::half_t>>, Is_dropout=false, Is_causal=false, Is_even_N=false, Is_even_K=false, Return_softmax=false, Params=Flash_fwd_params, Is_Q_split_K=false]" 
(589): here
            instantiation of "void flash::compute_attn<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &) [with Kernel_traits=Flash_fwd_kernel_traits<96, 128, 64, 4, false, false, cutlass::half_t, Flash_kernel_traits<96, 128, 64, 4, cutlass::half_t>>, Is_dropout=false, Is_causal=false, Is_even_N=false, Is_even_K=false, Return_softmax=false, Params=Flash_fwd_params]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(15): here
            instantiation of "void flash_fwd_kernel<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax>(Flash_fwd_params) [with Kernel_traits=Flash_fwd_kernel_traits<96, 128, 64, 4, false, false, cutlass::half_t, Flash_kernel_traits<96, 128, 64, 4, cutlass::half_t>>, Is_dropout=false, Is_causal=false, Is_even_N=false, Is_even_K=false, Return_softmax=false]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(34): here
            instantiation of "void run_flash_fwd<Kernel_traits,Is_dropout,Is_causal>(Flash_fwd_params &, cudaStream_t) [with Kernel_traits=Flash_fwd_kernel_traits<96, 128, 64, 4, false, false, cutlass::half_t, Flash_kernel_traits<96, 128, 64, 4, cutlass::half_t>>, Is_dropout=false, Is_causal=false]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(92): here
            instantiation of "void run_mha_fwd_hdim96<T>(Flash_fwd_params &, cudaStream_t) [with T=cutlass::half_t]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_hdim96_fp16_sm80.cu(22): here

/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_kernel.h(276): warning: variable "tQrQ" was declared but never referenced
          detected during:
            instantiation of "void flash::compute_attn_1rowblock<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params,Is_Q_split_K>(const Params &, int, int, int) [with Kernel_traits=Flash_fwd_kernel_traits<96, 128, 64, 4, false, false, cutlass::half_t, Flash_kernel_traits<96, 128, 64, 4, cutlass::half_t>>, Is_dropout=false, Is_causal=false, Is_even_N=false, Is_even_K=false, Return_softmax=false, Params=Flash_fwd_params, Is_Q_split_K=false]" 
(589): here
            instantiation of "void flash::compute_attn<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &) [with Kernel_traits=Flash_fwd_kernel_traits<96, 128, 64, 4, false, false, cutlass::half_t, Flash_kernel_traits<96, 128, 64, 4, cutlass::half_t>>, Is_dropout=false, Is_causal=false, Is_even_N=false, Is_even_K=false, Return_softmax=false, Params=Flash_fwd_params]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(15): here
            instantiation of "void flash_fwd_kernel<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax>(Flash_fwd_params) [with Kernel_traits=Flash_fwd_kernel_traits<96, 128, 64, 4, false, false, cutlass::half_t, Flash_kernel_traits<96, 128, 64, 4, cutlass::half_t>>, Is_dropout=false, Is_causal=false, Is_even_N=false, Is_even_K=false, Return_softmax=false]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(34): here
            instantiation of "void run_flash_fwd<Kernel_traits,Is_dropout,Is_causal>(Flash_fwd_params &, cudaStream_t) [with Kernel_traits=Flash_fwd_kernel_traits<96, 128, 64, 4, false, false, cutlass::half_t, Flash_kernel_traits<96, 128, 64, 4, cutlass::half_t>>, Is_dropout=false, Is_causal=false]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(92): here
            instantiation of "void run_mha_fwd_hdim96<T>(Flash_fwd_params &, cudaStream_t) [with T=cutlass::half_t]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_hdim96_fp16_sm80.cu(22): here

ptxas info    : 2 bytes gmem
ptxas info    : Compiling entry function '_Z16flash_fwd_kernelI23Flash_fwd_kernel_traitsILi96ELi128ELi64ELi4ELb0ELb0EN7cutlass6half_tE19Flash_kernel_traitsILi96ELi128ELi64ELi4ES2_EELb0ELb0ELb0ELb0ELb0EEv16Flash_fwd_params' for 'sm_80'
ptxas info    : Function properties for _Z16flash_fwd_kernelI23Flash_fwd_kernel_traitsILi96ELi128ELi64ELi4ELb0ELb0EN7cutlass6half_tE19Flash_kernel_traitsILi96ELi128ELi64ELi4ES2_EELb0ELb0ELb0ELb0ELb0EEv16Flash_fwd_params
    0 bytes stack frame, 0 bytes spill stores, 0 bytes spill loads
ptxas info    : Used 240 registers, 584 bytes cmem[0]
ptxas info    : Compiling entry function '_Z16flash_fwd_kernelI23Flash_fwd_kernel_traitsILi96ELi128ELi64ELi4ELb0ELb0EN7cutlass6half_tE19Flash_kernel_traitsILi96ELi128ELi64ELi4ES2_EELb0ELb0ELb0ELb1ELb0EEv16Flash_fwd_params' for 'sm_80'
ptxas info    : Function properties for _Z16flash_fwd_kernelI23Flash_fwd_kernel_traitsILi96ELi128ELi64ELi4ELb0ELb0EN7cutlass6half_tE19Flash_kernel_traitsILi96ELi128ELi64ELi4ES2_EELb0ELb0ELb0ELb1ELb0EEv16Flash_fwd_params
    0 bytes stack frame, 0 bytes spill stores, 0 bytes spill loads
ptxas info    : Used 237 registers, 584 bytes cmem[0]
ptxas info    : Compiling entry function '_Z16flash_fwd_kernelI23Flash_fwd_kernel_traitsILi96ELi128ELi64ELi4ELb0ELb0EN7cutlass6half_tE19Flash_kernel_traitsILi96ELi128ELi64ELi4ES2_EELb0ELb0ELb1ELb0ELb0EEv16Flash_fwd_params' for 'sm_80'
ptxas info    : Function properties for _Z16flash_fwd_kernelI23Flash_fwd_kernel_traitsILi96ELi128ELi64ELi4ELb0ELb0EN7cutlass6half_tE19Flash_kernel_traitsILi96ELi128ELi64ELi4ES2_EELb0ELb0ELb1ELb0ELb0EEv16Flash_fwd_params
    0 bytes stack frame, 0 bytes spill stores, 0 bytes spill loads
ptxas info    : Used 239 registers, 584 bytes cmem[0]
ptxas info    : Compiling entry function '_Z16flash_fwd_kernelI23Flash_fwd_kernel_traitsILi96ELi128ELi64ELi4ELb0ELb0EN7cutlass6half_tE19Flash_kernel_traitsILi96ELi128ELi64ELi4ES2_EELb0ELb0ELb1ELb1ELb0EEv16Flash_fwd_params' for 'sm_80'
ptxas info    : Function properties for _Z16flash_fwd_kernelI23Flash_fwd_kernel_traitsILi96ELi128ELi64ELi4ELb0ELb0EN7cutlass6half_tE19Flash_kernel_traitsILi96ELi128ELi64ELi4ES2_EELb0ELb0ELb1ELb1ELb0EEv16Flash_fwd_params
    0 bytes stack frame, 0 bytes spill stores, 0 bytes spill loads
ptxas info    : Used 237 registers, 584 bytes cmem[0]
ptxas info    : Compiling entry function '_Z16flash_fwd_kernelI23Flash_fwd_kernel_traitsILi96ELi128ELi64ELi4ELb0ELb0EN7cutlass6half_tE19Flash_kernel_traitsILi96ELi128ELi64ELi4ES2_EELb0ELb1ELb0ELb0ELb0EEv16Flash_fwd_params' for 'sm_80'
ptxas info    : Function properties for _Z16flash_fwd_kernelI23Flash_fwd_kernel_traitsILi96ELi128ELi64ELi4ELb0ELb0EN7cutlass6half_tE19Flash_kernel_traitsILi96ELi128ELi64ELi4ES2_EELb0ELb1ELb0ELb0ELb0EEv16Flash_fwd_params
    0 bytes stack frame, 0 bytes spill stores, 0 bytes spill loads
ptxas info    : Used 255 registers, 584 bytes cmem[0]
ptxas info    : Compiling entry function '_Z16flash_fwd_kernelI23Flash_fwd_kernel_traitsILi96ELi128ELi64ELi4ELb0ELb0EN7cutlass6half_tE19Flash_kernel_traitsILi96ELi128ELi64ELi4ES2_EELb0ELb1ELb0ELb1ELb0EEv16Flash_fwd_params' for 'sm_80'
ptxas info    : Function properties for _Z16flash_fwd_kernelI23Flash_fwd_kernel_traitsILi96ELi128ELi64ELi4ELb0ELb0EN7cutlass6half_tE19Flash_kernel_traitsILi96ELi128ELi64ELi4ES2_EELb0ELb1ELb0ELb1ELb0EEv16Flash_fwd_params
    0 bytes stack frame, 0 bytes spill stores, 0 bytes spill loads
ptxas info    : Used 254 registers, 584 bytes cmem[0]
ptxas info    : Compiling entry function '_Z16flash_fwd_kernelI23Flash_fwd_kernel_traitsILi96ELi128ELi64ELi4ELb0ELb0EN7cutlass6half_tE19Flash_kernel_traitsILi96ELi128ELi64ELi4ES2_EELb0ELb1ELb1ELb0ELb0EEv16Flash_fwd_params' for 'sm_80'
ptxas info    : Function properties for _Z16flash_fwd_kernelI23Flash_fwd_kernel_traitsILi96ELi128ELi64ELi4ELb0ELb0EN7cutlass6half_tE19Flash_kernel_traitsILi96ELi128ELi64ELi4ES2_EELb0ELb1ELb1ELb0ELb0EEv16Flash_fwd_params
    0 bytes stack frame, 0 bytes spill stores, 0 bytes spill loads
ptxas info    : Used 254 registers, 584 bytes cmem[0]
ptxas info    : Compiling entry function '_Z16flash_fwd_kernelI23Flash_fwd_kernel_traitsILi96ELi128ELi64ELi4ELb0ELb0EN7cutlass6half_tE19Flash_kernel_traitsILi96ELi128ELi64ELi4ES2_EELb0ELb1ELb1ELb1ELb0EEv16Flash_fwd_params' for 'sm_80'
ptxas info    : Function properties for _Z16flash_fwd_kernelI23Flash_fwd_kernel_traitsILi96ELi128ELi64ELi4ELb0ELb0EN7cutlass6half_tE19Flash_kernel_traitsILi96ELi128ELi64ELi4ES2_EELb0ELb1ELb1ELb1ELb0EEv16Flash_fwd_params
    0 bytes stack frame, 0 bytes spill stores, 0 bytes spill loads
ptxas info    : Used 255 registers, 584 bytes cmem[0]
ptxas info    : Compiling entry function '_Z16flash_fwd_kernelI23Flash_fwd_kernel_traitsILi96ELi64ELi64ELi4ELb0ELb0EN7cutlass6half_tE19Flash_kernel_traitsILi96ELi64ELi64ELi4ES2_EELb0ELb1ELb0ELb0ELb0EEv16Flash_fwd_params' for 'sm_80'
ptxas info    : Function properties for _Z16flash_fwd_kernelI23Flash_fwd_kernel_traitsILi96ELi64ELi64ELi4ELb0ELb0EN7cutlass6half_tE19Flash_kernel_traitsILi96ELi64ELi64ELi4ES2_EELb0ELb1ELb0ELb0ELb0EEv16Flash_fwd_params
    0 bytes stack frame, 0 bytes spill stores, 0 bytes spill loads
ptxas info    : Used 133 registers, 584 bytes cmem[0]
ptxas info    : Compiling entry function '_Z16flash_fwd_kernelI23Flash_fwd_kernel_traitsILi96ELi64ELi64ELi4ELb0ELb0EN7cutlass6half_tE19Flash_kernel_traitsILi96ELi64ELi64ELi4ES2_EELb0ELb1ELb0ELb1ELb0EEv16Flash_fwd_params' for 'sm_80'
ptxas info    : Function properties for _Z16flash_fwd_kernelI23Flash_fwd_kernel_traitsILi96ELi64ELi64ELi4ELb0ELb0EN7cutlass6half_tE19Flash_kernel_traitsILi96ELi64ELi64ELi4ES2_EELb0ELb1ELb0ELb1ELb0EEv16Flash_fwd_params
    0 bytes stack frame, 0 bytes spill stores, 0 bytes spill loads
ptxas info    : Used 130 registers, 584 bytes cmem[0]
ptxas info    : Compiling entry function '_Z16flash_fwd_kernelI23Flash_fwd_kernel_traitsILi96ELi64ELi64ELi4ELb0ELb0EN7cutlass6half_tE19Flash_kernel_traitsILi96ELi64ELi64ELi4ES2_EELb0ELb1ELb1ELb0ELb0EEv16Flash_fwd_params' for 'sm_80'
ptxas info    : Function properties for _Z16flash_fwd_kernelI23Flash_fwd_kernel_traitsILi96ELi64ELi64ELi4ELb0ELb0EN7cutlass6half_tE19Flash_kernel_traitsILi96ELi64ELi64ELi4ES2_EELb0ELb1ELb1ELb0ELb0EEv16Flash_fwd_params
    0 bytes stack frame, 0 bytes spill stores, 0 bytes spill loads
ptxas info    : Used 130 registers, 584 bytes cmem[0]
ptxas info    : Compiling entry function '_Z16flash_fwd_kernelI23Flash_fwd_kernel_traitsILi96ELi64ELi64ELi4ELb0ELb0EN7cutlass6half_tE19Flash_kernel_traitsILi96ELi64ELi64ELi4ES2_EELb0ELb1ELb1ELb1ELb0EEv16Flash_fwd_params' for 'sm_80'
ptxas info    : Function properties for _Z16flash_fwd_kernelI23Flash_fwd_kernel_traitsILi96ELi64ELi64ELi4ELb0ELb0EN7cutlass6half_tE19Flash_kernel_traitsILi96ELi64ELi64ELi4ES2_EELb0ELb1ELb1ELb1ELb0EEv16Flash_fwd_params
    0 bytes stack frame, 0 bytes spill stores, 0 bytes spill loads
ptxas info    : Used 133 registers, 584 bytes cmem[0]
ptxas info    : Compiling entry function '_Z16flash_fwd_kernelI23Flash_fwd_kernel_traitsILi96ELi128ELi64ELi4ELb0ELb0EN7cutlass6half_tE19Flash_kernel_traitsILi96ELi128ELi64ELi4ES2_EELb1ELb0ELb0ELb0ELb0EEv16Flash_fwd_params' for 'sm_80'
ptxas info    : Function properties for _Z16flash_fwd_kernelI23Flash_fwd_kernel_traitsILi96ELi128ELi64ELi4ELb0ELb0EN7cutlass6half_tE19Flash_kernel_traitsILi96ELi128ELi64ELi4ES2_EELb1ELb0ELb0ELb0ELb0EEv16Flash_fwd_params
    16 bytes stack frame, 16 bytes spill stores, 16 bytes spill loads
ptxas info    : Used 255 registers, 584 bytes cmem[0]
ptxas info    : Compiling entry function '_Z16flash_fwd_kernelI23Flash_fwd_kernel_traitsILi96ELi128ELi64ELi4ELb0ELb0EN7cutlass6half_tE19Flash_kernel_traitsILi96ELi128ELi64ELi4ES2_EELb1ELb0ELb0ELb0ELb1EEv16Flash_fwd_params' for 'sm_80'
ptxas info    : Function properties for _Z16flash_fwd_kernelI23Flash_fwd_kernel_traitsILi96ELi128ELi64ELi4ELb0ELb0EN7cutlass6half_tE19Flash_kernel_traitsILi96ELi128ELi64ELi4ES2_EELb1ELb0ELb0ELb0ELb1EEv16Flash_fwd_params
    256 bytes stack frame, 184 bytes spill stores, 184 bytes spill loads
ptxas info    : Used 255 registers, 584 bytes cmem[0]
ptxas info    : Compiling entry function '_Z16flash_fwd_kernelI23Flash_fwd_kernel_traitsILi96ELi128ELi64ELi4ELb0ELb0EN7cutlass6half_tE19Flash_kernel_traitsILi96ELi128ELi64ELi4ES2_EELb1ELb0ELb0ELb1ELb0EEv16Flash_fwd_params' for 'sm_80'
ptxas info    : Function properties for _Z16flash_fwd_kernelI23Flash_fwd_kernel_traitsILi96ELi128ELi64ELi4ELb0ELb0EN7cutlass6half_tE19Flash_kernel_traitsILi96ELi128ELi64ELi4ES2_EELb1ELb0ELb0ELb1ELb0EEv16Flash_fwd_params
    0 bytes stack frame, 0 bytes spill stores, 0 bytes spill loads
ptxas info    : Used 255 registers, 584 bytes cmem[0]
ptxas info    : Compiling entry function '_Z16flash_fwd_kernelI23Flash_fwd_kernel_traitsILi96ELi128ELi64ELi4ELb0ELb0EN7cutlass6half_tE19Flash_kernel_traitsILi96ELi128ELi64ELi4ES2_EELb1ELb0ELb0ELb1ELb1EEv16Flash_fwd_params' for 'sm_80'
ptxas info    : Function properties for _Z16flash_fwd_kernelI23Flash_fwd_kernel_traitsILi96ELi128ELi64ELi4ELb0ELb0EN7cutlass6half_tE19Flash_kernel_traitsILi96ELi128ELi64ELi4ES2_EELb1ELb0ELb0ELb1ELb1EEv16Flash_fwd_params
    184 bytes stack frame, 146 bytes spill stores, 130 bytes spill loads
ptxas info    : Used 255 registers, 584 bytes cmem[0]
ptxas info    : Compiling entry function '_Z16flash_fwd_kernelI23Flash_fwd_kernel_traitsILi96ELi128ELi64ELi4ELb0ELb0EN7cutlass6half_tE19Flash_kernel_traitsILi96ELi128ELi64ELi4ES2_EELb1ELb0ELb1ELb0ELb0EEv16Flash_fwd_params' for 'sm_80'
ptxas info    : Function properties for _Z16flash_fwd_kernelI23Flash_fwd_kernel_traitsILi96ELi128ELi64ELi4ELb0ELb0EN7cutlass6half_tE19Flash_kernel_traitsILi96ELi128ELi64ELi4ES2_EELb1ELb0ELb1ELb0ELb0EEv16Flash_fwd_params
    0 bytes stack frame, 0 bytes spill stores, 0 bytes spill loads
ptxas info    : Used 255 registers, 584 bytes cmem[0]
ptxas info    : Compiling entry function '_Z16flash_fwd_kernelI23Flash_fwd_kernel_traitsILi96ELi128ELi64ELi4ELb0ELb0EN7cutlass6half_tE19Flash_kernel_traitsILi96ELi128ELi64ELi4ES2_EELb1ELb0ELb1ELb0ELb1EEv16Flash_fwd_params' for 'sm_80'
ptxas info    : Function properties for _Z16flash_fwd_kernelI23Flash_fwd_kernel_traitsILi96ELi128ELi64ELi4ELb0ELb0EN7cutlass6half_tE19Flash_kernel_traitsILi96ELi128ELi64ELi4ES2_EELb1ELb0ELb1ELb0ELb1EEv16Flash_fwd_params
    200 bytes stack frame, 132 bytes spill stores, 132 bytes spill loads
ptxas info    : Used 255 registers, 584 bytes cmem[0]
ptxas info    : Compiling entry function '_Z16flash_fwd_kernelI23Flash_fwd_kernel_traitsILi96ELi128ELi64ELi4ELb0ELb0EN7cutlass6half_tE19Flash_kernel_traitsILi96ELi128ELi64ELi4ES2_EELb1ELb0ELb1ELb1ELb0EEv16Flash_fwd_params' for 'sm_80'
ptxas info    : Function properties for _Z16flash_fwd_kernelI23Flash_fwd_kernel_traitsILi96ELi128ELi64ELi4ELb0ELb0EN7cutlass6half_tE19Flash_kernel_traitsILi96ELi128ELi64ELi4ES2_EELb1ELb0ELb1ELb1ELb0EEv16Flash_fwd_params
    0 bytes stack frame, 0 bytes spill stores, 0 bytes spill loads
ptxas info    : Used 255 registers, 584 bytes cmem[0]
ptxas info    : Compiling entry function '_Z16flash_fwd_kernelI23Flash_fwd_kernel_traitsILi96ELi128ELi64ELi4ELb0ELb0EN7cutlass6half_tE19Flash_kernel_traitsILi96ELi128ELi64ELi4ES2_EELb1ELb0ELb1ELb1ELb1EEv16Flash_fwd_params' for 'sm_80'
ptxas info    : Function properties for _Z16flash_fwd_kernelI23Flash_fwd_kernel_traitsILi96ELi128ELi64ELi4ELb0ELb0EN7cutlass6half_tE19Flash_kernel_traitsILi96ELi128ELi64ELi4ES2_EELb1ELb0ELb1ELb1ELb1EEv16Flash_fwd_params
    240 bytes stack frame, 200 bytes spill stores, 200 bytes spill loads
ptxas info    : Used 255 registers, 584 bytes cmem[0]
ptxas info    : Compiling entry function '_Z16flash_fwd_kernelI23Flash_fwd_kernel_traitsILi96ELi128ELi64ELi4ELb0ELb0EN7cutlass6half_tE19Flash_kernel_traitsILi96ELi128ELi64ELi4ES2_EELb1ELb1ELb0ELb0ELb0EEv16Flash_fwd_params' for 'sm_80'
ptxas info    : Function properties for _Z16flash_fwd_kernelI23Flash_fwd_kernel_traitsILi96ELi128ELi64ELi4ELb0ELb0EN7cutlass6half_tE19Flash_kernel_traitsILi96ELi128ELi64ELi4ES2_EELb1ELb1ELb0ELb0ELb0EEv16Flash_fwd_params
    48 bytes stack frame, 44 bytes spill stores, 44 bytes spill loads
ptxas info    : Used 255 registers, 584 bytes cmem[0]
ptxas info    : Compiling entry function '_Z16flash_fwd_kernelI23Flash_fwd_kernel_traitsILi96ELi128ELi64ELi4ELb0ELb0EN7cutlass6half_tE19Flash_kernel_traitsILi96ELi128ELi64ELi4ES2_EELb1ELb1ELb0ELb0ELb1EEv16Flash_fwd_params' for 'sm_80'
ptxas info    : Function properties for _Z16flash_fwd_kernelI23Flash_fwd_kernel_traitsILi96ELi128ELi64ELi4ELb0ELb0EN7cutlass6half_tE19Flash_kernel_traitsILi96ELi128ELi64ELi4ES2_EELb1ELb1ELb0ELb0ELb1EEv16Flash_fwd_params
    224 bytes stack frame, 252 bytes spill stores, 288 bytes spill loads
ptxas info    : Used 255 registers, 584 bytes cmem[0]
ptxas info    : Compiling entry function '_Z16flash_fwd_kernelI23Flash_fwd_kernel_traitsILi96ELi128ELi64ELi4ELb0ELb0EN7cutlass6half_tE19Flash_kernel_traitsILi96ELi128ELi64ELi4ES2_EELb1ELb1ELb0ELb1ELb0EEv16Flash_fwd_params' for 'sm_80'
ptxas info    : Function properties for _Z16flash_fwd_kernelI23Flash_fwd_kernel_traitsILi96ELi128ELi64ELi4ELb0ELb0EN7cutlass6half_tE19Flash_kernel_traitsILi96ELi128ELi64ELi4ES2_EELb1ELb1ELb0ELb1ELb0EEv16Flash_fwd_params
    8 bytes stack frame, 8 bytes spill stores, 8 bytes spill loads
ptxas info    : Used 255 registers, 584 bytes cmem[0]
ptxas info    : Compiling entry function '_Z16flash_fwd_kernelI23Flash_fwd_kernel_traitsILi96ELi128ELi64ELi4ELb0ELb0EN7cutlass6half_tE19Flash_kernel_traitsILi96ELi128ELi64ELi4ES2_EELb1ELb1ELb0ELb1ELb1EEv16Flash_fwd_params' for 'sm_80'
ptxas info    : Function properties for _Z16flash_fwd_kernelI23Flash_fwd_kernel_traitsILi96ELi128ELi64ELi4ELb0ELb0EN7cutlass6half_tE19Flash_kernel_traitsILi96ELi128ELi64ELi4ES2_EELb1ELb1ELb0ELb1ELb1EEv16Flash_fwd_params
    208 bytes stack frame, 272 bytes spill stores, 294 bytes spill loads
ptxas info    : Used 255 registers, 584 bytes cmem[0]
ptxas info    : Compiling entry function '_Z16flash_fwd_kernelI23Flash_fwd_kernel_traitsILi96ELi128ELi64ELi4ELb0ELb0EN7cutlass6half_tE19Flash_kernel_traitsILi96ELi128ELi64ELi4ES2_EELb1ELb1ELb1ELb0ELb0EEv16Flash_fwd_params' for 'sm_80'
ptxas info    : Function properties for _Z16flash_fwd_kernelI23Flash_fwd_kernel_traitsILi96ELi128ELi64ELi4ELb0ELb0EN7cutlass6half_tE19Flash_kernel_traitsILi96ELi128ELi64ELi4ES2_EELb1ELb1ELb1ELb0ELb0EEv16Flash_fwd_params
    40 bytes stack frame, 32 bytes spill stores, 32 bytes spill loads
ptxas info    : Used 255 registers, 584 bytes cmem[0]
ptxas info    : Compiling entry function '_Z16flash_fwd_kernelI23Flash_fwd_kernel_traitsILi96ELi128ELi64ELi4ELb0ELb0EN7cutlass6half_tE19Flash_kernel_traitsILi96ELi128ELi64ELi4ES2_EELb1ELb1ELb1ELb0ELb1EEv16Flash_fwd_params' for 'sm_80'
ptxas info    : Function properties for _Z16flash_fwd_kernelI23Flash_fwd_kernel_traitsILi96ELi128ELi64ELi4ELb0ELb0EN7cutlass6half_tE19Flash_kernel_traitsILi96ELi128ELi64ELi4ES2_EELb1ELb1ELb1ELb0ELb1EEv16Flash_fwd_params
    272 bytes stack frame, 250 bytes spill stores, 254 bytes spill loads
ptxas info    : Used 255 registers, 584 bytes cmem[0]
ptxas info    : Compiling entry function '_Z16flash_fwd_kernelI23Flash_fwd_kernel_traitsILi96ELi128ELi64ELi4ELb0ELb0EN7cutlass6half_tE19Flash_kernel_traitsILi96ELi128ELi64ELi4ES2_EELb1ELb1ELb1ELb1ELb0EEv16Flash_fwd_params' for 'sm_80'
ptxas info    : Function properties for _Z16flash_fwd_kernelI23Flash_fwd_kernel_traitsILi96ELi128ELi64ELi4ELb0ELb0EN7cutlass6half_tE19Flash_kernel_traitsILi96ELi128ELi64ELi4ES2_EELb1ELb1ELb1ELb1ELb0EEv16Flash_fwd_params
    40 bytes stack frame, 44 bytes spill stores, 40 bytes spill loads
ptxas info    : Used 255 registers, 584 bytes cmem[0]
ptxas info    : Compiling entry function '_Z16flash_fwd_kernelI23Flash_fwd_kernel_traitsILi96ELi128ELi64ELi4ELb0ELb0EN7cutlass6half_tE19Flash_kernel_traitsILi96ELi128ELi64ELi4ES2_EELb1ELb1ELb1ELb1ELb1EEv16Flash_fwd_params' for 'sm_80'
ptxas info    : Function properties for _Z16flash_fwd_kernelI23Flash_fwd_kernel_traitsILi96ELi128ELi64ELi4ELb0ELb0EN7cutlass6half_tE19Flash_kernel_traitsILi96ELi128ELi64ELi4ES2_EELb1ELb1ELb1ELb1ELb1EEv16Flash_fwd_params
    264 bytes stack frame, 250 bytes spill stores, 268 bytes spill loads
ptxas info    : Used 255 registers, 584 bytes cmem[0]
ptxas info    : Compiling entry function '_Z16flash_fwd_kernelI23Flash_fwd_kernel_traitsILi96ELi64ELi64ELi4ELb0ELb0EN7cutlass6half_tE19Flash_kernel_traitsILi96ELi64ELi64ELi4ES2_EELb1ELb1ELb0ELb0ELb0EEv16Flash_fwd_params' for 'sm_80'
ptxas info    : Function properties for _Z16flash_fwd_kernelI23Flash_fwd_kernel_traitsILi96ELi64ELi64ELi4ELb0ELb0EN7cutlass6half_tE19Flash_kernel_traitsILi96ELi64ELi64ELi4ES2_EELb1ELb1ELb0ELb0ELb0EEv16Flash_fwd_params
    0 bytes stack frame, 0 bytes spill stores, 0 bytes spill loads
ptxas info    : Used 146 registers, 584 bytes cmem[0]
ptxas info    : Compiling entry function '_Z16flash_fwd_kernelI23Flash_fwd_kernel_traitsILi96ELi64ELi64ELi4ELb0ELb0EN7cutlass6half_tE19Flash_kernel_traitsILi96ELi64ELi64ELi4ES2_EELb1ELb1ELb0ELb0ELb1EEv16Flash_fwd_params' for 'sm_80'
ptxas info    : Function properties for _Z16flash_fwd_kernelI23Flash_fwd_kernel_traitsILi96ELi64ELi64ELi4ELb0ELb0EN7cutlass6half_tE19Flash_kernel_traitsILi96ELi64ELi64ELi4ES2_EELb1ELb1ELb0ELb0ELb1EEv16Flash_fwd_params
    0 bytes stack frame, 0 bytes spill stores, 0 bytes spill loads
ptxas info    : Used 192 registers, 584 bytes cmem[0]
ptxas info    : Compiling entry function '_Z16flash_fwd_kernelI23Flash_fwd_kernel_traitsILi96ELi64ELi64ELi4ELb0ELb0EN7cutlass6half_tE19Flash_kernel_traitsILi96ELi64ELi64ELi4ES2_EELb1ELb1ELb0ELb1ELb0EEv16Flash_fwd_params' for 'sm_80'
ptxas info    : Function properties for _Z16flash_fwd_kernelI23Flash_fwd_kernel_traitsILi96ELi64ELi64ELi4ELb0ELb0EN7cutlass6half_tE19Flash_kernel_traitsILi96ELi64ELi64ELi4ES2_EELb1ELb1ELb0ELb1ELb0EEv16Flash_fwd_params
    0 bytes stack frame, 0 bytes spill stores, 0 bytes spill loads
ptxas info    : Used 140 registers, 584 bytes cmem[0]
ptxas info    : Compiling entry function '_Z16flash_fwd_kernelI23Flash_fwd_kernel_traitsILi96ELi64ELi64ELi4ELb0ELb0EN7cutlass6half_tE19Flash_kernel_traitsILi96ELi64ELi64ELi4ES2_EELb1ELb1ELb0ELb1ELb1EEv16Flash_fwd_params' for 'sm_80'
ptxas info    : Function properties for _Z16flash_fwd_kernelI23Flash_fwd_kernel_traitsILi96ELi64ELi64ELi4ELb0ELb0EN7cutlass6half_tE19Flash_kernel_traitsILi96ELi64ELi64ELi4ES2_EELb1ELb1ELb0ELb1ELb1EEv16Flash_fwd_params
    0 bytes stack frame, 0 bytes spill stores, 0 bytes spill loads
ptxas info    : Used 190 registers, 584 bytes cmem[0]
ptxas info    : Compiling entry function '_Z16flash_fwd_kernelI23Flash_fwd_kernel_traitsILi96ELi64ELi64ELi4ELb0ELb0EN7cutlass6half_tE19Flash_kernel_traitsILi96ELi64ELi64ELi4ES2_EELb1ELb1ELb1ELb0ELb0EEv16Flash_fwd_params' for 'sm_80'
ptxas info    : Function properties for _Z16flash_fwd_kernelI23Flash_fwd_kernel_traitsILi96ELi64ELi64ELi4ELb0ELb0EN7cutlass6half_tE19Flash_kernel_traitsILi96ELi64ELi64ELi4ES2_EELb1ELb1ELb1ELb0ELb0EEv16Flash_fwd_params
    0 bytes stack frame, 0 bytes spill stores, 0 bytes spill loads
ptxas info    : Used 148 registers, 584 bytes cmem[0]
ptxas info    : Compiling entry function '_Z16flash_fwd_kernelI23Flash_fwd_kernel_traitsILi96ELi64ELi64ELi4ELb0ELb0EN7cutlass6half_tE19Flash_kernel_traitsILi96ELi64ELi64ELi4ES2_EELb1ELb1ELb1ELb0ELb1EEv16Flash_fwd_params' for 'sm_80'
ptxas info    : Function properties for _Z16flash_fwd_kernelI23Flash_fwd_kernel_traitsILi96ELi64ELi64ELi4ELb0ELb0EN7cutlass6half_tE19Flash_kernel_traitsILi96ELi64ELi64ELi4ES2_EELb1ELb1ELb1ELb0ELb1EEv16Flash_fwd_params
    0 bytes stack frame, 0 bytes spill stores, 0 bytes spill loads
ptxas info    : Used 188 registers, 584 bytes cmem[0]
ptxas info    : Compiling entry function '_Z16flash_fwd_kernelI23Flash_fwd_kernel_traitsILi96ELi64ELi64ELi4ELb0ELb0EN7cutlass6half_tE19Flash_kernel_traitsILi96ELi64ELi64ELi4ES2_EELb1ELb1ELb1ELb1ELb0EEv16Flash_fwd_params' for 'sm_80'
ptxas info    : Function properties for _Z16flash_fwd_kernelI23Flash_fwd_kernel_traitsILi96ELi64ELi64ELi4ELb0ELb0EN7cutlass6half_tE19Flash_kernel_traitsILi96ELi64ELi64ELi4ES2_EELb1ELb1ELb1ELb1ELb0EEv16Flash_fwd_params
    0 bytes stack frame, 0 bytes spill stores, 0 bytes spill loads
ptxas info    : Used 142 registers, 584 bytes cmem[0]
ptxas info    : Compiling entry function '_Z16flash_fwd_kernelI23Flash_fwd_kernel_traitsILi96ELi64ELi64ELi4ELb0ELb0EN7cutlass6half_tE19Flash_kernel_traitsILi96ELi64ELi64ELi4ES2_EELb1ELb1ELb1ELb1ELb1EEv16Flash_fwd_params' for 'sm_80'
ptxas info    : Function properties for _Z16flash_fwd_kernelI23Flash_fwd_kernel_traitsILi96ELi64ELi64ELi4ELb0ELb0EN7cutlass6half_tE19Flash_kernel_traitsILi96ELi64ELi64ELi4ES2_EELb1ELb1ELb1ELb1ELb1EEv16Flash_fwd_params
    0 bytes stack frame, 0 bytes spill stores, 0 bytes spill loads
ptxas info    : Used 186 registers, 584 bytes cmem[0]
/anaconda/envs/flashattention/lib/python3.9/site-packages/torch/include/c10/core/SymInt.h(84): warning: integer conversion resulted in a change of sign

[33/37] /usr/local/cuda-11.4/bin/nvcc  -I/home/v-feiychen/flash-attention/csrc/flash_attn -I/home/v-feiychen/flash-attention/csrc/flash_attn/src -I/home/v-feiychen/flash-attention/csrc/cutlass/include -I/anaconda/envs/flashattention/lib/python3.9/site-packages/torch/include -I/anaconda/envs/flashattention/lib/python3.9/site-packages/torch/include/torch/csrc/api/include -I/anaconda/envs/flashattention/lib/python3.9/site-packages/torch/include/TH -I/anaconda/envs/flashattention/lib/python3.9/site-packages/torch/include/THC -I/usr/local/cuda-11.4/include -I/anaconda/envs/flashattention/include/python3.9 -c -c /home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_bwd_hdim512_bf16_sm80.cu -o /home/v-feiychen/flash-attention/build/temp.linux-x86_64-cpython-39/csrc/flash_attn/src/flash_bwd_hdim512_bf16_sm80.o -D__CUDA_NO_HALF_OPERATORS__ -D__CUDA_NO_HALF_CONVERSIONS__ -D__CUDA_NO_BFLOAT16_CONVERSIONS__ -D__CUDA_NO_HALF2_OPERATORS__ --expt-relaxed-constexpr --compiler-options ''"'"'-fPIC'"'"'' -O3 -std=c++17 -U__CUDA_NO_HALF_OPERATORS__ -U__CUDA_NO_HALF_CONVERSIONS__ -U__CUDA_NO_HALF2_OPERATORS__ -U__CUDA_NO_BFLOAT16_CONVERSIONS__ --expt-relaxed-constexpr --expt-extended-lambda --use_fast_math --ptxas-options=-v -lineinfo -gencode arch=compute_80,code=sm_80 --threads 4 -DTORCH_API_INCLUDE_EXTENSION_H '-DPYBIND11_COMPILER_TYPE="_gcc"' '-DPYBIND11_STDLIB="_libstdcpp"' '-DPYBIND11_BUILD_ABI="_cxxabi1011"' -DTORCH_EXTENSION_NAME=flash_attn_2_cuda -D_GLIBCXX_USE_CXX11_ABI=0
/anaconda/envs/flashattention/lib/python3.9/site-packages/torch/include/c10/core/SymInt.h(84): warning: integer conversion resulted in a change of sign

ptxas info    : 2 bytes gmem
ptxas info    : Compiling entry function '_Z44flash_bwd_dq_dk_dv_loop_seqk_parallel_kernelI23Flash_bwd_kernel_traitsILi512ELi32ELi64ELi4ELi2ELi2ELi2ELb0ELb1EN7cutlass10bfloat16_tE19Flash_kernel_traitsILi512ELi32ELi64ELi4ES2_EELb0ELb0ELb0ELb0EEv16Flash_bwd_params' for 'sm_80'
ptxas info    : Function properties for _Z44flash_bwd_dq_dk_dv_loop_seqk_parallel_kernelI23Flash_bwd_kernel_traitsILi512ELi32ELi64ELi4ELi2ELi2ELi2ELb0ELb1EN7cutlass10bfloat16_tE19Flash_kernel_traitsILi512ELi32ELi64ELi4ES2_EELb0ELb0ELb0ELb0EEv16Flash_bwd_params
    3048 bytes stack frame, 0 bytes spill stores, 0 bytes spill loads
ptxas info    : Used 255 registers, 696 bytes cmem[0]
ptxas info    : Function properties for _ZN5flash26compute_dq_dk_dv_1colblockI23Flash_bwd_kernel_traitsILi512ELi32ELi64ELi4ELi2ELi2ELi2ELb0ELb1EN7cutlass10bfloat16_tE19Flash_kernel_traitsILi512ELi32ELi64ELi4ES3_EELb0ELb0ELb0ELb0ELb0ELb0ELb1E16Flash_bwd_paramsEEvRKT7_iii
    0 bytes stack frame, 7340 bytes spill stores, 5384 bytes spill loads
ptxas info    : Compiling entry function '_Z44flash_bwd_dq_dk_dv_loop_seqk_parallel_kernelI23Flash_bwd_kernel_traitsILi512ELi32ELi64ELi4ELi2ELi2ELi2ELb0ELb1EN7cutlass10bfloat16_tE19Flash_kernel_traitsILi512ELi32ELi64ELi4ES2_EELb0ELb0ELb0ELb1EEv16Flash_bwd_params' for 'sm_80'
ptxas info    : Function properties for _Z44flash_bwd_dq_dk_dv_loop_seqk_parallel_kernelI23Flash_bwd_kernel_traitsILi512ELi32ELi64ELi4ELi2ELi2ELi2ELb0ELb1EN7cutlass10bfloat16_tE19Flash_kernel_traitsILi512ELi32ELi64ELi4ES2_EELb0ELb0ELb0ELb1EEv16Flash_bwd_params
    2696 bytes stack frame, 7536 bytes spill stores, 5516 bytes spill loads
ptxas info    : Used 255 registers, 696 bytes cmem[0]
ptxas info    : Compiling entry function '_Z44flash_bwd_dq_dk_dv_loop_seqk_parallel_kernelI23Flash_bwd_kernel_traitsILi512ELi32ELi64ELi4ELi2ELi2ELi2ELb0ELb1EN7cutlass10bfloat16_tE19Flash_kernel_traitsILi512ELi32ELi64ELi4ES2_EELb0ELb0ELb1ELb0EEv16Flash_bwd_params' for 'sm_80'
ptxas info    : Function properties for _Z44flash_bwd_dq_dk_dv_loop_seqk_parallel_kernelI23Flash_bwd_kernel_traitsILi512ELi32ELi64ELi4ELi2ELi2ELi2ELb0ELb1EN7cutlass10bfloat16_tE19Flash_kernel_traitsILi512ELi32ELi64ELi4ES2_EELb0ELb0ELb1ELb0EEv16Flash_bwd_params
    2704 bytes stack frame, 7772 bytes spill stores, 5776 bytes spill loads
ptxas info    : Used 255 registers, 696 bytes cmem[0]
ptxas info    : Compiling entry function '_Z44flash_bwd_dq_dk_dv_loop_seqk_parallel_kernelI23Flash_bwd_kernel_traitsILi512ELi32ELi64ELi4ELi2ELi2ELi2ELb0ELb1EN7cutlass10bfloat16_tE19Flash_kernel_traitsILi512ELi32ELi64ELi4ES2_EELb0ELb0ELb1ELb1EEv16Flash_bwd_params' for 'sm_80'
ptxas info    : Function properties for _Z44flash_bwd_dq_dk_dv_loop_seqk_parallel_kernelI23Flash_bwd_kernel_traitsILi512ELi32ELi64ELi4ELi2ELi2ELi2ELb0ELb1EN7cutlass10bfloat16_tE19Flash_kernel_traitsILi512ELi32ELi64ELi4ES2_EELb0ELb0ELb1ELb1EEv16Flash_bwd_params
    2688 bytes stack frame, 7520 bytes spill stores, 5492 bytes spill loads
ptxas info    : Used 255 registers, 696 bytes cmem[0]
ptxas info    : Compiling entry function '_Z44flash_bwd_dq_dk_dv_loop_seqk_parallel_kernelI23Flash_bwd_kernel_traitsILi512ELi32ELi64ELi4ELi2ELi2ELi2ELb0ELb1EN7cutlass10bfloat16_tE19Flash_kernel_traitsILi512ELi32ELi64ELi4ES2_EELb0ELb1ELb0ELb0EEv16Flash_bwd_params' for 'sm_80'
ptxas info    : Function properties for _Z44flash_bwd_dq_dk_dv_loop_seqk_parallel_kernelI23Flash_bwd_kernel_traitsILi512ELi32ELi64ELi4ELi2ELi2ELi2ELb0ELb1EN7cutlass10bfloat16_tE19Flash_kernel_traitsILi512ELi32ELi64ELi4ES2_EELb0ELb1ELb0ELb0EEv16Flash_bwd_params
    4304 bytes stack frame, 5400 bytes spill stores, 5448 bytes spill loads
ptxas info    : Used 255 registers, 696 bytes cmem[0]
ptxas info    : Function properties for _ZN5flash4gemmILb0ELb0EN4cute6TensorINS1_13array_alignedIfLm16ELm16EEENS1_6LayoutINS1_5tupleIJNS6_IJNS1_8constantIiLi2EEES8_EEENS7_IiLi1EEENS7_IiLi4EEEEEENS6_IJNS6_IJSA_S8_EEENS7_IiLi0EEESB_EEEEEEENS2_INS3_IN7cutlass10bfloat16_tELm256ELm16EEENS5_INS6_IJNS6_IJS8_S8_S8_EEESA_NS6_IJS9_NS7_IiLi8EEEEEEEEENS6_IJNS6_IJSA_S8_SB_EEESE_NS6_IJNS6_IJSM_NS7_IiLi16EEEEEENS7_IiLi32EEEEEEEEEEEEENS2_INS3_ISJ_Lm512ELm16EEENS5_INS6_IJS9_SB_SN_EEENS6_IJSD_SB_NS6_IJNS6_IJSQ_SS_EEENS7_IiLi64EEEEEEEEEEEEENS2_INS1_10ViewEngineINS1_8smem_ptrISJ_EEEENS5_INS6_IJNS6_IJSM_SA_EEESA_SN_EEENS6_IJNS6_IJSA_SE_EEESE_NS6_IJNS6_IJiiEEENS7_IiLi2048EEEEEEEEEEEEENS2_IS18_NS5_INS6_IJS19_S8_SN_EEENS6_IJS1B_NS7_IiLi1024EEENS6_IJS1C_NS7_IiLi4096EEEEEEEEEEEEENS1_8TiledMMAINS1_8MMA_AtomIJNS1_30SM80_16x8x16_F32BF16BF16F32_TNEEEENS5_INS6_IJS8_S8_SA_EEENS6_IJSA_S8_SE_EEEEENS5_INS6_IJSA_S8_SA_EEENS6_IJSE_SA_SE_EEEEENS6_IJNS1_10UnderscoreES1Z_S1Z_EEEEENS1_9TiledCopyINS1_9Copy_AtomIJNS1_17SM75_U32x4_LDSM_NESJ_EEENS5_INS6_IJNS6_IJSB_SM_S8_S8_EEENS6_IJSL_NS6_IJSA_SA_EEEEEEEEENS6_IJNS6_IJS10_SA_SQ_SE_EEENS6_IJNS6_IJSS_SM_NS7_IiLi256EEEEEENS6_IJSE_SE_EEEEEEEEEEENS6_IJSS_SQ_EEEEENS22_IS25_NS5_INS6_IJS26_NS6_IJS9_NS6_IJS8_SA_EEEEEEEEENS6_IJNS6_IJS10_SA_SE_SM_EEENS6_IJNS6_IJSS_S2B_EEENS6_IJSQ_SE_EEEEEEEEEEENS6_IJNS5_INS6_IJSM_S8_S8_EEENS6_IJSA_SS_SM_EEEEENS5_ISQ_SA_EEEEEEENS1_7ThrCopyIS2I_iEENS2Y_IS2X_iEEEEvRT1_RT2_RT3_RKT4_RKT5_T6_T7_T8_T9_T10_
    0 bytes stack frame, 0 bytes spill stores, 0 bytes spill loads
ptxas info    : Compiling entry function '_Z44flash_bwd_dq_dk_dv_loop_seqk_parallel_kernelI23Flash_bwd_kernel_traitsILi512ELi32ELi64ELi4ELi2ELi2ELi2ELb0ELb1EN7cutlass10bfloat16_tE19Flash_kernel_traitsILi512ELi32ELi64ELi4ES2_EELb0ELb1ELb0ELb1EEv16Flash_bwd_params' for 'sm_80'
ptxas info    : Function properties for _Z44flash_bwd_dq_dk_dv_loop_seqk_parallel_kernelI23Flash_bwd_kernel_traitsILi512ELi32ELi64ELi4ELi2ELi2ELi2ELb0ELb1EN7cutlass10bfloat16_tE19Flash_kernel_traitsILi512ELi32ELi64ELi4ES2_EELb0ELb1ELb0ELb1EEv16Flash_bwd_params
    2696 bytes stack frame, 5492 bytes spill stores, 5520 bytes spill loads
ptxas info    : Used 255 registers, 696 bytes cmem[0]
ptxas info    : Compiling entry function '_Z44flash_bwd_dq_dk_dv_loop_seqk_parallel_kernelI23Flash_bwd_kernel_traitsILi512ELi32ELi64ELi4ELi2ELi2ELi2ELb0ELb1EN7cutlass10bfloat16_tE19Flash_kernel_traitsILi512ELi32ELi64ELi4ES2_EELb0ELb1ELb1ELb0EEv16Flash_bwd_params' for 'sm_80'
ptxas info    : Function properties for _Z44flash_bwd_dq_dk_dv_loop_seqk_parallel_kernelI23Flash_bwd_kernel_traitsILi512ELi32ELi64ELi4ELi2ELi2ELi2ELb0ELb1EN7cutlass10bfloat16_tE19Flash_kernel_traitsILi512ELi32ELi64ELi4ES2_EELb0ELb1ELb1ELb0EEv16Flash_bwd_params
    2768 bytes stack frame, 5776 bytes spill stores, 5824 bytes spill loads
ptxas info    : Used 255 registers, 696 bytes cmem[0]
ptxas info    : Compiling entry function '_Z44flash_bwd_dq_dk_dv_loop_seqk_parallel_kernelI23Flash_bwd_kernel_traitsILi512ELi32ELi64ELi4ELi2ELi2ELi2ELb0ELb1EN7cutlass10bfloat16_tE19Flash_kernel_traitsILi512ELi32ELi64ELi4ES2_EELb0ELb1ELb1ELb1EEv16Flash_bwd_params' for 'sm_80'
ptxas info    : Function properties for _Z44flash_bwd_dq_dk_dv_loop_seqk_parallel_kernelI23Flash_bwd_kernel_traitsILi512ELi32ELi64ELi4ELi2ELi2ELi2ELb0ELb1EN7cutlass10bfloat16_tE19Flash_kernel_traitsILi512ELi32ELi64ELi4ES2_EELb0ELb1ELb1ELb1EEv16Flash_bwd_params
    2696 bytes stack frame, 5480 bytes spill stores, 5508 bytes spill loads
ptxas info    : Used 255 registers, 696 bytes cmem[0]
ptxas info    : Compiling entry function '_Z44flash_bwd_dq_dk_dv_loop_seqk_parallel_kernelI23Flash_bwd_kernel_traitsILi512ELi32ELi64ELi4ELi2ELi2ELi2ELb0ELb0EN7cutlass10bfloat16_tE19Flash_kernel_traitsILi512ELi32ELi64ELi4ES2_EELb0ELb0ELb0ELb0EEv16Flash_bwd_params' for 'sm_80'
ptxas info    : Function properties for _Z44flash_bwd_dq_dk_dv_loop_seqk_parallel_kernelI23Flash_bwd_kernel_traitsILi512ELi32ELi64ELi4ELi2ELi2ELi2ELb0ELb0EN7cutlass10bfloat16_tE19Flash_kernel_traitsILi512ELi32ELi64ELi4ES2_EELb0ELb0ELb0ELb0EEv16Flash_bwd_params
    3016 bytes stack frame, 0 bytes spill stores, 0 bytes spill loads
ptxas info    : Used 255 registers, 696 bytes cmem[0]
ptxas info    : Function properties for _ZN5flash26compute_dq_dk_dv_1colblockI23Flash_bwd_kernel_traitsILi512ELi32ELi64ELi4ELi2ELi2ELi2ELb0ELb0EN7cutlass10bfloat16_tE19Flash_kernel_traitsILi512ELi32ELi64ELi4ES3_EELb0ELb0ELb0ELb0ELb0ELb0ELb1E16Flash_bwd_paramsEEvRKT7_iii
    0 bytes stack frame, 7156 bytes spill stores, 5236 bytes spill loads
ptxas info    : Compiling entry function '_Z44flash_bwd_dq_dk_dv_loop_seqk_parallel_kernelI23Flash_bwd_kernel_traitsILi512ELi32ELi64ELi4ELi2ELi2ELi2ELb0ELb0EN7cutlass10bfloat16_tE19Flash_kernel_traitsILi512ELi32ELi64ELi4ES2_EELb0ELb0ELb0ELb1EEv16Flash_bwd_params' for 'sm_80'
ptxas info    : Function properties for _Z44flash_bwd_dq_dk_dv_loop_seqk_parallel_kernelI23Flash_bwd_kernel_traitsILi512ELi32ELi64ELi4ELi2ELi2ELi2ELb0ELb0EN7cutlass10bfloat16_tE19Flash_kernel_traitsILi512ELi32ELi64ELi4ES2_EELb0ELb0ELb0ELb1EEv16Flash_bwd_params
    2504 bytes stack frame, 7272 bytes spill stores, 5232 bytes spill loads
ptxas info    : Used 255 registers, 696 bytes cmem[0]
ptxas info    : Compiling entry function '_Z44flash_bwd_dq_dk_dv_loop_seqk_parallel_kernelI23Flash_bwd_kernel_traitsILi512ELi32ELi64ELi4ELi2ELi2ELi2ELb0ELb0EN7cutlass10bfloat16_tE19Flash_kernel_traitsILi512ELi32ELi64ELi4ES2_EELb0ELb0ELb1ELb0EEv16Flash_bwd_params' for 'sm_80'
ptxas info    : Function properties for _Z44flash_bwd_dq_dk_dv_loop_seqk_parallel_kernelI23Flash_bwd_kernel_traitsILi512ELi32ELi64ELi4ELi2ELi2ELi2ELb0ELb0EN7cutlass10bfloat16_tE19Flash_kernel_traitsILi512ELi32ELi64ELi4ES2_EELb0ELb0ELb1ELb0EEv16Flash_bwd_params
    2752 bytes stack frame, 7600 bytes spill stores, 5592 bytes spill loads
ptxas info    : Used 255 registers, 696 bytes cmem[0]
ptxas info    : Compiling entry function '_Z44flash_bwd_dq_dk_dv_loop_seqk_parallel_kernelI23Flash_bwd_kernel_traitsILi512ELi32ELi64ELi4ELi2ELi2ELi2ELb0ELb0EN7cutlass10bfloat16_tE19Flash_kernel_traitsILi512ELi32ELi64ELi4ES2_EELb0ELb0ELb1ELb1EEv16Flash_bwd_params' for 'sm_80'
ptxas info    : Function properties for _Z44flash_bwd_dq_dk_dv_loop_seqk_parallel_kernelI23Flash_bwd_kernel_traitsILi512ELi32ELi64ELi4ELi2ELi2ELi2ELb0ELb0EN7cutlass10bfloat16_tE19Flash_kernel_traitsILi512ELi32ELi64ELi4ES2_EELb0ELb0ELb1ELb1EEv16Flash_bwd_params
    2512 bytes stack frame, 7304 bytes spill stores, 5264 bytes spill loads
ptxas info    : Used 255 registers, 696 bytes cmem[0]
ptxas info    : Compiling entry function '_Z44flash_bwd_dq_dk_dv_loop_seqk_parallel_kernelI23Flash_bwd_kernel_traitsILi512ELi32ELi64ELi4ELi2ELi2ELi2ELb0ELb0EN7cutlass10bfloat16_tE19Flash_kernel_traitsILi512ELi32ELi64ELi4ES2_EELb0ELb1ELb0ELb0EEv16Flash_bwd_params' for 'sm_80'
ptxas info    : Function properties for _Z44flash_bwd_dq_dk_dv_loop_seqk_parallel_kernelI23Flash_bwd_kernel_traitsILi512ELi32ELi64ELi4ELi2ELi2ELi2ELb0ELb0EN7cutlass10bfloat16_tE19Flash_kernel_traitsILi512ELi32ELi64ELi4ES2_EELb0ELb1ELb0ELb0EEv16Flash_bwd_params
    4224 bytes stack frame, 5340 bytes spill stores, 4932 bytes spill loads
ptxas info    : Used 255 registers, 696 bytes cmem[0]
ptxas info    : Function properties for _ZN5flash4gemmILb0ELb0EN4cute6TensorINS1_13array_alignedIfLm16ELm16EEENS1_6LayoutINS1_5tupleIJNS6_IJNS1_8constantIiLi2EEES8_EEENS7_IiLi1EEENS7_IiLi4EEEEEENS6_IJNS6_IJSA_S8_EEENS7_IiLi0EEESB_EEEEEEENS2_INS3_IN7cutlass10bfloat16_tELm256ELm16EEENS5_INS6_IJNS6_IJS8_S8_S8_EEESA_NS6_IJS9_NS7_IiLi8EEEEEEEEENS6_IJNS6_IJSA_S8_SB_EEESE_NS6_IJNS6_IJSM_NS7_IiLi16EEEEEENS7_IiLi32EEEEEEEEEEEEENS2_INS3_ISJ_Lm512ELm16EEENS5_INS6_IJS9_SB_SN_EEENS6_IJSD_SB_NS6_IJNS6_IJSQ_SS_EEENS7_IiLi64EEEEEEEEEEEEENS2_INS1_10ViewEngineINS1_8smem_ptrISJ_EEEENS5_INS6_IJNS6_IJSM_SA_EEESA_SN_EEENS6_IJNS6_IJSA_SE_EEESE_NS6_IJNS6_IJiiEEENS7_IiLi2048EEEEEEEEEEEEENS2_IS18_NS5_INS6_IJS19_S8_SN_EEENS6_IJS1B_NS7_IiLi1024EEENS6_IJS1C_NS7_IiLi4096EEEEEEEEEEEEENS1_8TiledMMAINS1_8MMA_AtomIJNS1_30SM80_16x8x16_F32BF16BF16F32_TNEEEENS5_INS6_IJS8_S8_SA_EEENS6_IJSA_S8_SE_EEEEENS5_INS6_IJSA_S8_SA_EEENS6_IJSE_SA_SE_EEEEENS6_IJNS1_10UnderscoreES1Z_S1Z_EEEEENS1_9TiledCopyINS1_9Copy_AtomIJNS1_17SM75_U32x4_LDSM_NESJ_EEENS5_INS6_IJNS6_IJSB_SM_S8_S8_EEENS6_IJSL_NS6_IJSA_SA_EEEEEEEEENS6_IJNS6_IJS10_SA_SQ_SE_EEENS6_IJNS6_IJSS_SM_NS7_IiLi256EEEEEENS6_IJSE_SE_EEEEEEEEEEENS6_IJSS_SQ_EEEEENS22_IS25_NS5_INS6_IJS26_NS6_IJS9_NS6_IJS8_SA_EEEEEEEEENS6_IJNS6_IJS10_SA_SE_SM_EEENS6_IJNS6_IJSS_S2B_EEENS6_IJSQ_SE_EEEEEEEEEEENS6_IJNS5_INS6_IJSM_S8_S8_EEENS6_IJSA_SS_SM_EEEEENS5_ISQ_SA_EEEEEEENS1_7ThrCopyIS2I_iEENS2Y_IS2X_iEEEEvRT1_RT2_RT3_RKT4_RKT5_T6_T7_T8_T9_T10_
    0 bytes stack frame, 0 bytes spill stores, 0 bytes spill loads
ptxas info    : Compiling entry function '_Z44flash_bwd_dq_dk_dv_loop_seqk_parallel_kernelI23Flash_bwd_kernel_traitsILi512ELi32ELi64ELi4ELi2ELi2ELi2ELb0ELb0EN7cutlass10bfloat16_tE19Flash_kernel_traitsILi512ELi32ELi64ELi4ES2_EELb0ELb1ELb0ELb1EEv16Flash_bwd_params' for 'sm_80'
ptxas info    : Function properties for _Z44flash_bwd_dq_dk_dv_loop_seqk_parallel_kernelI23Flash_bwd_kernel_traitsILi512ELi32ELi64ELi4ELi2ELi2ELi2ELb0ELb0EN7cutlass10bfloat16_tE19Flash_kernel_traitsILi512ELi32ELi64ELi4ES2_EELb0ELb1ELb0ELb1EEv16Flash_bwd_params
    2536 bytes stack frame, 5228 bytes spill stores, 4760 bytes spill loads
ptxas info    : Used 255 registers, 696 bytes cmem[0]
ptxas info    : Compiling entry function '_Z44flash_bwd_dq_dk_dv_loop_seqk_parallel_kernelI23Flash_bwd_kernel_traitsILi512ELi32ELi64ELi4ELi2ELi2ELi2ELb0ELb0EN7cutlass10bfloat16_tE19Flash_kernel_traitsILi512ELi32ELi64ELi4ES2_EELb0ELb1ELb1ELb0EEv16Flash_bwd_params' for 'sm_80'
ptxas info    : Function properties for _Z44flash_bwd_dq_dk_dv_loop_seqk_parallel_kernelI23Flash_bwd_kernel_traitsILi512ELi32ELi64ELi4ELi2ELi2ELi2ELb0ELb0EN7cutlass10bfloat16_tE19Flash_kernel_traitsILi512ELi32ELi64ELi4ES2_EELb0ELb1ELb1ELb0EEv16Flash_bwd_params
    2864 bytes stack frame, 5628 bytes spill stores, 5272 bytes spill loads
ptxas info    : Used 255 registers, 696 bytes cmem[0]
ptxas info    : Compiling entry function '_Z44flash_bwd_dq_dk_dv_loop_seqk_parallel_kernelI23Flash_bwd_kernel_traitsILi512ELi32ELi64ELi4ELi2ELi2ELi2ELb0ELb0EN7cutlass10bfloat16_tE19Flash_kernel_traitsILi512ELi32ELi64ELi4ES2_EELb0ELb1ELb1ELb1EEv16Flash_bwd_params' for 'sm_80'
ptxas info    : Function properties for _Z44flash_bwd_dq_dk_dv_loop_seqk_parallel_kernelI23Flash_bwd_kernel_traitsILi512ELi32ELi64ELi4ELi2ELi2ELi2ELb0ELb0EN7cutlass10bfloat16_tE19Flash_kernel_traitsILi512ELi32ELi64ELi4ES2_EELb0ELb1ELb1ELb1EEv16Flash_bwd_params
    2520 bytes stack frame, 5280 bytes spill stores, 4796 bytes spill loads
ptxas info    : Used 255 registers, 696 bytes cmem[0]
ptxas info    : Compiling entry function '_Z27flash_bwd_convert_dq_kernelI23Flash_bwd_kernel_traitsILi512ELi32ELi64ELi4ELi2ELi2ELi2ELb0ELb1EN7cutlass10bfloat16_tE19Flash_kernel_traitsILi512ELi32ELi64ELi4ES2_EEEv16Flash_bwd_params' for 'sm_80'
ptxas info    : Function properties for _Z27flash_bwd_convert_dq_kernelI23Flash_bwd_kernel_traitsILi512ELi32ELi64ELi4ELi2ELi2ELi2ELb0ELb1EN7cutlass10bfloat16_tE19Flash_kernel_traitsILi512ELi32ELi64ELi4ES2_EEEv16Flash_bwd_params
    0 bytes stack frame, 0 bytes spill stores, 0 bytes spill loads
ptxas info    : Used 56 registers, 696 bytes cmem[0]
ptxas info    : Compiling entry function '_Z44flash_bwd_dq_dk_dv_loop_seqk_parallel_kernelI23Flash_bwd_kernel_traitsILi512ELi32ELi64ELi4ELi2ELi2ELi2ELb0ELb1EN7cutlass10bfloat16_tE19Flash_kernel_traitsILi512ELi32ELi64ELi4ES2_EELb1ELb0ELb0ELb0EEv16Flash_bwd_params' for 'sm_80'
ptxas info    : Function properties for _Z44flash_bwd_dq_dk_dv_loop_seqk_parallel_kernelI23Flash_bwd_kernel_traitsILi512ELi32ELi64ELi4ELi2ELi2ELi2ELb0ELb1EN7cutlass10bfloat16_tE19Flash_kernel_traitsILi512ELi32ELi64ELi4ES2_EELb1ELb0ELb0ELb0EEv16Flash_bwd_params
    2992 bytes stack frame, 0 bytes spill stores, 0 bytes spill loads
ptxas info    : Used 255 registers, 696 bytes cmem[0]
ptxas info    : Function properties for _ZN5flash26compute_dq_dk_dv_1colblockI23Flash_bwd_kernel_traitsILi512ELi32ELi64ELi4ELi2ELi2ELi2ELb0ELb1EN7cutlass10bfloat16_tE19Flash_kernel_traitsILi512ELi32ELi64ELi4ES3_EELb1ELb0ELb0ELb0ELb0ELb0ELb1E16Flash_bwd_paramsEEvRKT7_iii
    0 bytes stack frame, 7448 bytes spill stores, 5512 bytes spill loads
ptxas info    : Compiling entry function '_Z44flash_bwd_dq_dk_dv_loop_seqk_parallel_kernelI23Flash_bwd_kernel_traitsILi512ELi32ELi64ELi4ELi2ELi2ELi2ELb0ELb1EN7cutlass10bfloat16_tE19Flash_kernel_traitsILi512ELi32ELi64ELi4ES2_EELb1ELb0ELb0ELb1EEv16Flash_bwd_params' for 'sm_80'
ptxas info    : Function properties for _Z44flash_bwd_dq_dk_dv_loop_seqk_parallel_kernelI23Flash_bwd_kernel_traitsILi512ELi32ELi64ELi4ELi2ELi2ELi2ELb0ELb1EN7cutlass10bfloat16_tE19Flash_kernel_traitsILi512ELi32ELi64ELi4ES2_EELb1ELb0ELb0ELb1EEv16Flash_bwd_params
    2808 bytes stack frame, 7984 bytes spill stores, 5972 bytes spill loads
ptxas info    : Used 255 registers, 696 bytes cmem[0]
ptxas info    : Compiling entry function '_Z44flash_bwd_dq_dk_dv_loop_seqk_parallel_kernelI23Flash_bwd_kernel_traitsILi512ELi32ELi64ELi4ELi2ELi2ELi2ELb0ELb1EN7cutlass10bfloat16_tE19Flash_kernel_traitsILi512ELi32ELi64ELi4ES2_EELb1ELb0ELb1ELb0EEv16Flash_bwd_params' for 'sm_80'
ptxas info    : Function properties for _Z44flash_bwd_dq_dk_dv_loop_seqk_parallel_kernelI23Flash_bwd_kernel_traitsILi512ELi32ELi64ELi4ELi2ELi2ELi2ELb0ELb1EN7cutlass10bfloat16_tE19Flash_kernel_traitsILi512ELi32ELi64ELi4ES2_EELb1ELb0ELb1ELb0EEv16Flash_bwd_params
    2704 bytes stack frame, 7788 bytes spill stores, 5792 bytes spill loads
ptxas info    : Used 255 registers, 696 bytes cmem[0]
ptxas info    : Compiling entry function '_Z44flash_bwd_dq_dk_dv_loop_seqk_parallel_kernelI23Flash_bwd_kernel_traitsILi512ELi32ELi64ELi4ELi2ELi2ELi2ELb0ELb1EN7cutlass10bfloat16_tE19Flash_kernel_traitsILi512ELi32ELi64ELi4ES2_EELb1ELb0ELb1ELb1EEv16Flash_bwd_params' for 'sm_80'
ptxas info    : Function properties for _Z44flash_bwd_dq_dk_dv_loop_seqk_parallel_kernelI23Flash_bwd_kernel_traitsILi512ELi32ELi64ELi4ELi2ELi2ELi2ELb0ELb1EN7cutlass10bfloat16_tE19Flash_kernel_traitsILi512ELi32ELi64ELi4ES2_EELb1ELb0ELb1ELb1EEv16Flash_bwd_params
    2800 bytes stack frame, 7940 bytes spill stores, 5912 bytes spill loads
ptxas info    : Used 255 registers, 696 bytes cmem[0]
ptxas info    : Compiling entry function '_Z44flash_bwd_dq_dk_dv_loop_seqk_parallel_kernelI23Flash_bwd_kernel_traitsILi512ELi32ELi64ELi4ELi2ELi2ELi2ELb0ELb1EN7cutlass10bfloat16_tE19Flash_kernel_traitsILi512ELi32ELi64ELi4ES2_EELb1ELb1ELb0ELb0EEv16Flash_bwd_params' for 'sm_80'
ptxas info    : Function properties for _Z44flash_bwd_dq_dk_dv_loop_seqk_parallel_kernelI23Flash_bwd_kernel_traitsILi512ELi32ELi64ELi4ELi2ELi2ELi2ELb0ELb1EN7cutlass10bfloat16_tE19Flash_kernel_traitsILi512ELi32ELi64ELi4ES2_EELb1ELb1ELb0ELb0EEv16Flash_bwd_params
    4336 bytes stack frame, 5840 bytes spill stores, 5884 bytes spill loads
ptxas info    : Used 255 registers, 696 bytes cmem[0]
ptxas info    : Function properties for _ZN5flash4gemmILb0ELb0EN4cute6TensorINS1_13array_alignedIfLm16ELm16EEENS1_6LayoutINS1_5tupleIJNS6_IJNS1_8constantIiLi2EEES8_EEENS7_IiLi1EEENS7_IiLi4EEEEEENS6_IJNS6_IJSA_S8_EEENS7_IiLi0EEESB_EEEEEEENS2_INS3_IN7cutlass10bfloat16_tELm256ELm16EEENS5_INS6_IJNS6_IJS8_S8_S8_EEESA_NS6_IJS9_NS7_IiLi8EEEEEEEEENS6_IJNS6_IJSA_S8_SB_EEESE_NS6_IJNS6_IJSM_NS7_IiLi16EEEEEENS7_IiLi32EEEEEEEEEEEEENS2_INS3_ISJ_Lm512ELm16EEENS5_INS6_IJS9_SB_SN_EEENS6_IJSD_SB_NS6_IJNS6_IJSQ_SS_EEENS7_IiLi64EEEEEEEEEEEEENS2_INS1_10ViewEngineINS1_8smem_ptrISJ_EEEENS5_INS6_IJNS6_IJSM_SA_EEESA_SN_EEENS6_IJNS6_IJSA_SE_EEESE_NS6_IJNS6_IJiiEEENS7_IiLi2048EEEEEEEEEEEEENS2_IS18_NS5_INS6_IJS19_S8_SN_EEENS6_IJS1B_NS7_IiLi1024EEENS6_IJS1C_NS7_IiLi4096EEEEEEEEEEEEENS1_8TiledMMAINS1_8MMA_AtomIJNS1_30SM80_16x8x16_F32BF16BF16F32_TNEEEENS5_INS6_IJS8_S8_SA_EEENS6_IJSA_S8_SE_EEEEENS5_INS6_IJSA_S8_SA_EEENS6_IJSE_SA_SE_EEEEENS6_IJNS1_10UnderscoreES1Z_S1Z_EEEEENS1_9TiledCopyINS1_9Copy_AtomIJNS1_17SM75_U32x4_LDSM_NESJ_EEENS5_INS6_IJNS6_IJSB_SM_S8_S8_EEENS6_IJSL_NS6_IJSA_SA_EEEEEEEEENS6_IJNS6_IJS10_SA_SQ_SE_EEENS6_IJNS6_IJSS_SM_NS7_IiLi256EEEEEENS6_IJSE_SE_EEEEEEEEEEENS6_IJSS_SQ_EEEEENS22_IS25_NS5_INS6_IJS26_NS6_IJS9_NS6_IJS8_SA_EEEEEEEEENS6_IJNS6_IJS10_SA_SE_SM_EEENS6_IJNS6_IJSS_S2B_EEENS6_IJSQ_SE_EEEEEEEEEEENS6_IJNS5_INS6_IJSM_S8_S8_EEENS6_IJSA_SS_SM_EEEEENS5_ISQ_SA_EEEEEEENS1_7ThrCopyIS2I_iEENS2Y_IS2X_iEEEEvRT1_RT2_RT3_RKT4_RKT5_T6_T7_T8_T9_T10_
    0 bytes stack frame, 0 bytes spill stores, 0 bytes spill loads
ptxas info    : Compiling entry function '_Z44flash_bwd_dq_dk_dv_loop_seqk_parallel_kernelI23Flash_bwd_kernel_traitsILi512ELi32ELi64ELi4ELi2ELi2ELi2ELb0ELb1EN7cutlass10bfloat16_tE19Flash_kernel_traitsILi512ELi32ELi64ELi4ES2_EELb1ELb1ELb0ELb1EEv16Flash_bwd_params' for 'sm_80'
ptxas info    : Function properties for _Z44flash_bwd_dq_dk_dv_loop_seqk_parallel_kernelI23Flash_bwd_kernel_traitsILi512ELi32ELi64ELi4ELi2ELi2ELi2ELb0ELb1EN7cutlass10bfloat16_tE19Flash_kernel_traitsILi512ELi32ELi64ELi4ES2_EELb1ELb1ELb0ELb1EEv16Flash_bwd_params
    2808 bytes stack frame, 5936 bytes spill stores, 5972 bytes spill loads
ptxas info    : Used 255 registers, 696 bytes cmem[0]
ptxas info    : Compiling entry function '_Z44flash_bwd_dq_dk_dv_loop_seqk_parallel_kernelI23Flash_bwd_kernel_traitsILi512ELi32ELi64ELi4ELi2ELi2ELi2ELb0ELb1EN7cutlass10bfloat16_tE19Flash_kernel_traitsILi512ELi32ELi64ELi4ES2_EELb1ELb1ELb1ELb0EEv16Flash_bwd_params' for 'sm_80'
ptxas info    : Function properties for _Z44flash_bwd_dq_dk_dv_loop_seqk_parallel_kernelI23Flash_bwd_kernel_traitsILi512ELi32ELi64ELi4ELi2ELi2ELi2ELb0ELb1EN7cutlass10bfloat16_tE19Flash_kernel_traitsILi512ELi32ELi64ELi4ES2_EELb1ELb1ELb1ELb0EEv16Flash_bwd_params
    2720 bytes stack frame, 5816 bytes spill stores, 5864 bytes spill loads
ptxas info    : Used 255 registers, 696 bytes cmem[0]
ptxas info    : Compiling entry function '_Z44flash_bwd_dq_dk_dv_loop_seqk_parallel_kernelI23Flash_bwd_kernel_traitsILi512ELi32ELi64ELi4ELi2ELi2ELi2ELb0ELb1EN7cutlass10bfloat16_tE19Flash_kernel_traitsILi512ELi32ELi64ELi4ES2_EELb1ELb1ELb1ELb1EEv16Flash_bwd_params' for 'sm_80'
ptxas info    : Function properties for _Z44flash_bwd_dq_dk_dv_loop_seqk_parallel_kernelI23Flash_bwd_kernel_traitsILi512ELi32ELi64ELi4ELi2ELi2ELi2ELb0ELb1EN7cutlass10bfloat16_tE19Flash_kernel_traitsILi512ELi32ELi64ELi4ES2_EELb1ELb1ELb1ELb1EEv16Flash_bwd_params
    2792 bytes stack frame, 5960 bytes spill stores, 5996 bytes spill loads
ptxas info    : Used 255 registers, 696 bytes cmem[0]
ptxas info    : Compiling entry function '_Z25flash_bwd_dot_do_o_kernelILb1E23Flash_bwd_kernel_traitsILi512ELi32ELi64ELi4ELi2ELi2ELi2ELb0ELb1EN7cutlass10bfloat16_tE19Flash_kernel_traitsILi512ELi32ELi64ELi4ES2_EEEv16Flash_bwd_params' for 'sm_80'
ptxas info    : Function properties for _Z25flash_bwd_dot_do_o_kernelILb1E23Flash_bwd_kernel_traitsILi512ELi32ELi64ELi4ELi2ELi2ELi2ELb0ELb1EN7cutlass10bfloat16_tE19Flash_kernel_traitsILi512ELi32ELi64ELi4ES2_EEEv16Flash_bwd_params
    0 bytes stack frame, 0 bytes spill stores, 0 bytes spill loads
ptxas info    : Used 168 registers, 696 bytes cmem[0]
ptxas info    : Compiling entry function '_Z27flash_bwd_convert_dq_kernelI23Flash_bwd_kernel_traitsILi512ELi32ELi64ELi4ELi2ELi2ELi2ELb0ELb0EN7cutlass10bfloat16_tE19Flash_kernel_traitsILi512ELi32ELi64ELi4ES2_EEEv16Flash_bwd_params' for 'sm_80'
ptxas info    : Function properties for _Z27flash_bwd_convert_dq_kernelI23Flash_bwd_kernel_traitsILi512ELi32ELi64ELi4ELi2ELi2ELi2ELb0ELb0EN7cutlass10bfloat16_tE19Flash_kernel_traitsILi512ELi32ELi64ELi4ES2_EEEv16Flash_bwd_params
    0 bytes stack frame, 0 bytes spill stores, 0 bytes spill loads
ptxas info    : Used 56 registers, 696 bytes cmem[0]
ptxas info    : Compiling entry function '_Z44flash_bwd_dq_dk_dv_loop_seqk_parallel_kernelI23Flash_bwd_kernel_traitsILi512ELi32ELi64ELi4ELi2ELi2ELi2ELb0ELb0EN7cutlass10bfloat16_tE19Flash_kernel_traitsILi512ELi32ELi64ELi4ES2_EELb1ELb0ELb0ELb0EEv16Flash_bwd_params' for 'sm_80'
ptxas info    : Function properties for _Z44flash_bwd_dq_dk_dv_loop_seqk_parallel_kernelI23Flash_bwd_kernel_traitsILi512ELi32ELi64ELi4ELi2ELi2ELi2ELb0ELb0EN7cutlass10bfloat16_tE19Flash_kernel_traitsILi512ELi32ELi64ELi4ES2_EELb1ELb0ELb0ELb0EEv16Flash_bwd_params
    2936 bytes stack frame, 0 bytes spill stores, 0 bytes spill loads
ptxas info    : Used 255 registers, 696 bytes cmem[0]
ptxas info    : Function properties for _ZN5flash26compute_dq_dk_dv_1colblockI23Flash_bwd_kernel_traitsILi512ELi32ELi64ELi4ELi2ELi2ELi2ELb0ELb0EN7cutlass10bfloat16_tE19Flash_kernel_traitsILi512ELi32ELi64ELi4ES3_EELb1ELb0ELb0ELb0ELb0ELb0ELb1E16Flash_bwd_paramsEEvRKT7_iii
    0 bytes stack frame, 7412 bytes spill stores, 5476 bytes spill loads
ptxas info    : Compiling entry function '_Z44flash_bwd_dq_dk_dv_loop_seqk_parallel_kernelI23Flash_bwd_kernel_traitsILi512ELi32ELi64ELi4ELi2ELi2ELi2ELb0ELb0EN7cutlass10bfloat16_tE19Flash_kernel_traitsILi512ELi32ELi64ELi4ES2_EELb1ELb0ELb0ELb1EEv16Flash_bwd_params' for 'sm_80'
ptxas info    : Function properties for _Z44flash_bwd_dq_dk_dv_loop_seqk_parallel_kernelI23Flash_bwd_kernel_traitsILi512ELi32ELi64ELi4ELi2ELi2ELi2ELb0ELb0EN7cutlass10bfloat16_tE19Flash_kernel_traitsILi512ELi32ELi64ELi4ES2_EELb1ELb0ELb0ELb1EEv16Flash_bwd_params
    2528 bytes stack frame, 7716 bytes spill stores, 5688 bytes spill loads
ptxas info    : Used 255 registers, 696 bytes cmem[0]
ptxas info    : Compiling entry function '_Z44flash_bwd_dq_dk_dv_loop_seqk_parallel_kernelI23Flash_bwd_kernel_traitsILi512ELi32ELi64ELi4ELi2ELi2ELi2ELb0ELb0EN7cutlass10bfloat16_tE19Flash_kernel_traitsILi512ELi32ELi64ELi4ES2_EELb1ELb0ELb1ELb0EEv16Flash_bwd_params' for 'sm_80'
ptxas info    : Function properties for _Z44flash_bwd_dq_dk_dv_loop_seqk_parallel_kernelI23Flash_bwd_kernel_traitsILi512ELi32ELi64ELi4ELi2ELi2ELi2ELb0ELb0EN7cutlass10bfloat16_tE19Flash_kernel_traitsILi512ELi32ELi64ELi4ES2_EELb1ELb0ELb1ELb0EEv16Flash_bwd_params
    2624 bytes stack frame, 7608 bytes spill stores, 5600 bytes spill loads
ptxas info    : Used 255 registers, 696 bytes cmem[0]
ptxas info    : Compiling entry function '_Z44flash_bwd_dq_dk_dv_loop_seqk_parallel_kernelI23Flash_bwd_kernel_traitsILi512ELi32ELi64ELi4ELi2ELi2ELi2ELb0ELb0EN7cutlass10bfloat16_tE19Flash_kernel_traitsILi512ELi32ELi64ELi4ES2_EELb1ELb0ELb1ELb1EEv16Flash_bwd_params' for 'sm_80'
ptxas info    : Function properties for _Z44flash_bwd_dq_dk_dv_loop_seqk_parallel_kernelI23Flash_bwd_kernel_traitsILi512ELi32ELi64ELi4ELi2ELi2ELi2ELb0ELb0EN7cutlass10bfloat16_tE19Flash_kernel_traitsILi512ELi32ELi64ELi4ES2_EELb1ELb0ELb1ELb1EEv16Flash_bwd_params
    2552 bytes stack frame, 7788 bytes spill stores, 5752 bytes spill loads
ptxas info    : Used 255 registers, 696 bytes cmem[0]
ptxas info    : Compiling entry function '_Z44flash_bwd_dq_dk_dv_loop_seqk_parallel_kernelI23Flash_bwd_kernel_traitsILi512ELi32ELi64ELi4ELi2ELi2ELi2ELb0ELb0EN7cutlass10bfloat16_tE19Flash_kernel_traitsILi512ELi32ELi64ELi4ES2_EELb1ELb1ELb0ELb0EEv16Flash_bwd_params' for 'sm_80'
ptxas info    : Function properties for _Z44flash_bwd_dq_dk_dv_loop_seqk_parallel_kernelI23Flash_bwd_kernel_traitsILi512ELi32ELi64ELi4ELi2ELi2ELi2ELb0ELb0EN7cutlass10bfloat16_tE19Flash_kernel_traitsILi512ELi32ELi64ELi4ES2_EELb1ELb1ELb0ELb0EEv16Flash_bwd_params
    4208 bytes stack frame, 5780 bytes spill stores, 5312 bytes spill loads
ptxas info    : Used 255 registers, 696 bytes cmem[0]
ptxas info    : Function properties for _ZN5flash4gemmILb0ELb0EN4cute6TensorINS1_13array_alignedIfLm16ELm16EEENS1_6LayoutINS1_5tupleIJNS6_IJNS1_8constantIiLi2EEES8_EEENS7_IiLi1EEENS7_IiLi4EEEEEENS6_IJNS6_IJSA_S8_EEENS7_IiLi0EEESB_EEEEEEENS2_INS3_IN7cutlass10bfloat16_tELm256ELm16EEENS5_INS6_IJNS6_IJS8_S8_S8_EEESA_NS6_IJS9_NS7_IiLi8EEEEEEEEENS6_IJNS6_IJSA_S8_SB_EEESE_NS6_IJNS6_IJSM_NS7_IiLi16EEEEEENS7_IiLi32EEEEEEEEEEEEENS2_INS3_ISJ_Lm512ELm16EEENS5_INS6_IJS9_SB_SN_EEENS6_IJSD_SB_NS6_IJNS6_IJSQ_SS_EEENS7_IiLi64EEEEEEEEEEEEENS2_INS1_10ViewEngineINS1_8smem_ptrISJ_EEEENS5_INS6_IJNS6_IJSM_SA_EEESA_SN_EEENS6_IJNS6_IJSA_SE_EEESE_NS6_IJNS6_IJiiEEENS7_IiLi2048EEEEEEEEEEEEENS2_IS18_NS5_INS6_IJS19_S8_SN_EEENS6_IJS1B_NS7_IiLi1024EEENS6_IJS1C_NS7_IiLi4096EEEEEEEEEEEEENS1_8TiledMMAINS1_8MMA_AtomIJNS1_30SM80_16x8x16_F32BF16BF16F32_TNEEEENS5_INS6_IJS8_S8_SA_EEENS6_IJSA_S8_SE_EEEEENS5_INS6_IJSA_S8_SA_EEENS6_IJSE_SA_SE_EEEEENS6_IJNS1_10UnderscoreES1Z_S1Z_EEEEENS1_9TiledCopyINS1_9Copy_AtomIJNS1_17SM75_U32x4_LDSM_NESJ_EEENS5_INS6_IJNS6_IJSB_SM_S8_S8_EEENS6_IJSL_NS6_IJSA_SA_EEEEEEEEENS6_IJNS6_IJS10_SA_SQ_SE_EEENS6_IJNS6_IJSS_SM_NS7_IiLi256EEEEEENS6_IJSE_SE_EEEEEEEEEEENS6_IJSS_SQ_EEEEENS22_IS25_NS5_INS6_IJS26_NS6_IJS9_NS6_IJS8_SA_EEEEEEEEENS6_IJNS6_IJS10_SA_SE_SM_EEENS6_IJNS6_IJSS_S2B_EEENS6_IJSQ_SE_EEEEEEEEEEENS6_IJNS5_INS6_IJSM_S8_S8_EEENS6_IJSA_SS_SM_EEEEENS5_ISQ_SA_EEEEEEENS1_7ThrCopyIS2I_iEENS2Y_IS2X_iEEEEvRT1_RT2_RT3_RKT4_RKT5_T6_T7_T8_T9_T10_
    0 bytes stack frame, 0 bytes spill stores, 0 bytes spill loads
ptxas info    : Compiling entry function '_Z44flash_bwd_dq_dk_dv_loop_seqk_parallel_kernelI23Flash_bwd_kernel_traitsILi512ELi32ELi64ELi4ELi2ELi2ELi2ELb0ELb0EN7cutlass10bfloat16_tE19Flash_kernel_traitsILi512ELi32ELi64ELi4ES2_EELb1ELb1ELb0ELb1EEv16Flash_bwd_params' for 'sm_80'
ptxas info    : Function properties for _Z44flash_bwd_dq_dk_dv_loop_seqk_parallel_kernelI23Flash_bwd_kernel_traitsILi512ELi32ELi64ELi4ELi2ELi2ELi2ELb0ELb0EN7cutlass10bfloat16_tE19Flash_kernel_traitsILi512ELi32ELi64ELi4ES2_EELb1ELb1ELb0ELb1EEv16Flash_bwd_params
    2536 bytes stack frame, 5672 bytes spill stores, 5204 bytes spill loads
ptxas info    : Used 255 registers, 696 bytes cmem[0]
ptxas info    : Compiling entry function '_Z44flash_bwd_dq_dk_dv_loop_seqk_parallel_kernelI23Flash_bwd_kernel_traitsILi512ELi32ELi64ELi4ELi2ELi2ELi2ELb0ELb0EN7cutlass10bfloat16_tE19Flash_kernel_traitsILi512ELi32ELi64ELi4ES2_EELb1ELb1ELb1ELb0EEv16Flash_bwd_params' for 'sm_80'
ptxas info    : Function properties for _Z44flash_bwd_dq_dk_dv_loop_seqk_parallel_kernelI23Flash_bwd_kernel_traitsILi512ELi32ELi64ELi4ELi2ELi2ELi2ELb0ELb0EN7cutlass10bfloat16_tE19Flash_kernel_traitsILi512ELi32ELi64ELi4ES2_EELb1ELb1ELb1ELb0EEv16Flash_bwd_params
    2728 bytes stack frame, 5668 bytes spill stores, 5108 bytes spill loads
ptxas info    : Used 255 registers, 696 bytes cmem[0]
ptxas info    : Compiling entry function '_Z44flash_bwd_dq_dk_dv_loop_seqk_parallel_kernelI23Flash_bwd_kernel_traitsILi512ELi32ELi64ELi4ELi2ELi2ELi2ELb0ELb0EN7cutlass10bfloat16_tE19Flash_kernel_traitsILi512ELi32ELi64ELi4ES2_EELb1ELb1ELb1ELb1EEv16Flash_bwd_params' for 'sm_80'
ptxas info    : Function properties for _Z44flash_bwd_dq_dk_dv_loop_seqk_parallel_kernelI23Flash_bwd_kernel_traitsILi512ELi32ELi64ELi4ELi2ELi2ELi2ELb0ELb0EN7cutlass10bfloat16_tE19Flash_kernel_traitsILi512ELi32ELi64ELi4ES2_EELb1ELb1ELb1ELb1EEv16Flash_bwd_params
    2520 bytes stack frame, 5744 bytes spill stores, 5244 bytes spill loads
ptxas info    : Used 255 registers, 696 bytes cmem[0]
ptxas info    : Compiling entry function '_Z25flash_bwd_dot_do_o_kernelILb1E23Flash_bwd_kernel_traitsILi512ELi32ELi64ELi4ELi2ELi2ELi2ELb0ELb0EN7cutlass10bfloat16_tE19Flash_kernel_traitsILi512ELi32ELi64ELi4ES2_EEEv16Flash_bwd_params' for 'sm_80'
ptxas info    : Function properties for _Z25flash_bwd_dot_do_o_kernelILb1E23Flash_bwd_kernel_traitsILi512ELi32ELi64ELi4ELi2ELi2ELi2ELb0ELb0EN7cutlass10bfloat16_tE19Flash_kernel_traitsILi512ELi32ELi64ELi4ES2_EEEv16Flash_bwd_params
    0 bytes stack frame, 0 bytes spill stores, 0 bytes spill loads
ptxas info    : Used 168 registers, 696 bytes cmem[0]
/anaconda/envs/flashattention/lib/python3.9/site-packages/torch/include/c10/core/SymInt.h(84): warning: integer conversion resulted in a change of sign

[34/37] /usr/local/cuda-11.4/bin/nvcc  -I/home/v-feiychen/flash-attention/csrc/flash_attn -I/home/v-feiychen/flash-attention/csrc/flash_attn/src -I/home/v-feiychen/flash-attention/csrc/cutlass/include -I/anaconda/envs/flashattention/lib/python3.9/site-packages/torch/include -I/anaconda/envs/flashattention/lib/python3.9/site-packages/torch/include/torch/csrc/api/include -I/anaconda/envs/flashattention/lib/python3.9/site-packages/torch/include/TH -I/anaconda/envs/flashattention/lib/python3.9/site-packages/torch/include/THC -I/usr/local/cuda-11.4/include -I/anaconda/envs/flashattention/include/python3.9 -c -c /home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_hdim256_bf16_sm80.cu -o /home/v-feiychen/flash-attention/build/temp.linux-x86_64-cpython-39/csrc/flash_attn/src/flash_fwd_hdim256_bf16_sm80.o -D__CUDA_NO_HALF_OPERATORS__ -D__CUDA_NO_HALF_CONVERSIONS__ -D__CUDA_NO_BFLOAT16_CONVERSIONS__ -D__CUDA_NO_HALF2_OPERATORS__ --expt-relaxed-constexpr --compiler-options ''"'"'-fPIC'"'"'' -O3 -std=c++17 -U__CUDA_NO_HALF_OPERATORS__ -U__CUDA_NO_HALF_CONVERSIONS__ -U__CUDA_NO_HALF2_OPERATORS__ -U__CUDA_NO_BFLOAT16_CONVERSIONS__ --expt-relaxed-constexpr --expt-extended-lambda --use_fast_math --ptxas-options=-v -lineinfo -gencode arch=compute_80,code=sm_80 --threads 4 -DTORCH_API_INCLUDE_EXTENSION_H '-DPYBIND11_COMPILER_TYPE="_gcc"' '-DPYBIND11_STDLIB="_libstdcpp"' '-DPYBIND11_BUILD_ABI="_cxxabi1011"' -DTORCH_EXTENSION_NAME=flash_attn_2_cuda -D_GLIBCXX_USE_CXX11_ABI=0
/anaconda/envs/flashattention/lib/python3.9/site-packages/torch/include/c10/core/SymInt.h(84): warning: integer conversion resulted in a change of sign

/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_kernel.h(140): warning: variable "MMA_M" was declared but never referenced
          detected during:
            instantiation of "void flash::compute_attn_1rowblock<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params,Is_Q_split_K>(const Params &, int, int, int) [with Kernel_traits=Flash_fwd_kernel_traits<256, 128, 64, 8, false, false, cutlass::bfloat16_t, Flash_kernel_traits<256, 128, 64, 8, cutlass::bfloat16_t>>, Is_dropout=true, Is_causal=true, Is_even_N=true, Is_even_K=true, Return_softmax=true, Params=Flash_fwd_params, Is_Q_split_K=false]" 
(589): here
            instantiation of "void flash::compute_attn<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &) [with Kernel_traits=Flash_fwd_kernel_traits<256, 128, 64, 8, false, false, cutlass::bfloat16_t, Flash_kernel_traits<256, 128, 64, 8, cutlass::bfloat16_t>>, Is_dropout=true, Is_causal=true, Is_even_N=true, Is_even_K=true, Return_softmax=true, Params=Flash_fwd_params]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(15): here
            instantiation of "void flash_fwd_kernel<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax>(Flash_fwd_params) [with Kernel_traits=Flash_fwd_kernel_traits<256, 128, 64, 8, false, false, cutlass::bfloat16_t, Flash_kernel_traits<256, 128, 64, 8, cutlass::bfloat16_t>>, Is_dropout=true, Is_causal=true, Is_even_N=true, Is_even_K=true, Return_softmax=true]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(34): here
            instantiation of "void run_flash_fwd<Kernel_traits,Is_dropout,Is_causal>(Flash_fwd_params &, cudaStream_t) [with Kernel_traits=Flash_fwd_kernel_traits<256, 128, 64, 8, false, false, cutlass::bfloat16_t, Flash_kernel_traits<256, 128, 64, 8, cutlass::bfloat16_t>>, Is_dropout=true, Is_causal=true]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(236): here
            instantiation of "void run_mha_fwd_hdim256<T>(Flash_fwd_params &, cudaStream_t) [with T=cutlass::bfloat16_t]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_hdim256_bf16_sm80.cu(8): here

/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_kernel.h(276): warning: variable "tQrQ" was declared but never referenced
          detected during:
            instantiation of "void flash::compute_attn_1rowblock<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params,Is_Q_split_K>(const Params &, int, int, int) [with Kernel_traits=Flash_fwd_kernel_traits<256, 128, 64, 8, false, false, cutlass::bfloat16_t, Flash_kernel_traits<256, 128, 64, 8, cutlass::bfloat16_t>>, Is_dropout=true, Is_causal=true, Is_even_N=true, Is_even_K=true, Return_softmax=true, Params=Flash_fwd_params, Is_Q_split_K=false]" 
(589): here
            instantiation of "void flash::compute_attn<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &) [with Kernel_traits=Flash_fwd_kernel_traits<256, 128, 64, 8, false, false, cutlass::bfloat16_t, Flash_kernel_traits<256, 128, 64, 8, cutlass::bfloat16_t>>, Is_dropout=true, Is_causal=true, Is_even_N=true, Is_even_K=true, Return_softmax=true, Params=Flash_fwd_params]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(15): here
            instantiation of "void flash_fwd_kernel<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax>(Flash_fwd_params) [with Kernel_traits=Flash_fwd_kernel_traits<256, 128, 64, 8, false, false, cutlass::bfloat16_t, Flash_kernel_traits<256, 128, 64, 8, cutlass::bfloat16_t>>, Is_dropout=true, Is_causal=true, Is_even_N=true, Is_even_K=true, Return_softmax=true]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(34): here
            instantiation of "void run_flash_fwd<Kernel_traits,Is_dropout,Is_causal>(Flash_fwd_params &, cudaStream_t) [with Kernel_traits=Flash_fwd_kernel_traits<256, 128, 64, 8, false, false, cutlass::bfloat16_t, Flash_kernel_traits<256, 128, 64, 8, cutlass::bfloat16_t>>, Is_dropout=true, Is_causal=true]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(236): here
            instantiation of "void run_mha_fwd_hdim256<T>(Flash_fwd_params &, cudaStream_t) [with T=cutlass::bfloat16_t]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_hdim256_bf16_sm80.cu(8): here

/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_kernel.h(140): warning: variable "MMA_M" was declared but never referenced
          detected during:
            instantiation of "void flash::compute_attn_1rowblock<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params,Is_Q_split_K>(const Params &, int, int, int) [with Kernel_traits=Flash_fwd_kernel_traits<256, 128, 64, 8, false, false, cutlass::bfloat16_t, Flash_kernel_traits<256, 128, 64, 8, cutlass::bfloat16_t>>, Is_dropout=true, Is_causal=true, Is_even_N=true, Is_even_K=true, Return_softmax=false, Params=Flash_fwd_params, Is_Q_split_K=false]" 
(589): here
            instantiation of "void flash::compute_attn<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &) [with Kernel_traits=Flash_fwd_kernel_traits<256, 128, 64, 8, false, false, cutlass::bfloat16_t, Flash_kernel_traits<256, 128, 64, 8, cutlass::bfloat16_t>>, Is_dropout=true, Is_causal=true, Is_even_N=true, Is_even_K=true, Return_softmax=false, Params=Flash_fwd_params]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(15): here
            instantiation of "void flash_fwd_kernel<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax>(Flash_fwd_params) [with Kernel_traits=Flash_fwd_kernel_traits<256, 128, 64, 8, false, false, cutlass::bfloat16_t, Flash_kernel_traits<256, 128, 64, 8, cutlass::bfloat16_t>>, Is_dropout=true, Is_causal=true, Is_even_N=true, Is_even_K=true, Return_softmax=false]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(34): here
            instantiation of "void run_flash_fwd<Kernel_traits,Is_dropout,Is_causal>(Flash_fwd_params &, cudaStream_t) [with Kernel_traits=Flash_fwd_kernel_traits<256, 128, 64, 8, false, false, cutlass::bfloat16_t, Flash_kernel_traits<256, 128, 64, 8, cutlass::bfloat16_t>>, Is_dropout=true, Is_causal=true]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(236): here
            instantiation of "void run_mha_fwd_hdim256<T>(Flash_fwd_params &, cudaStream_t) [with T=cutlass::bfloat16_t]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_hdim256_bf16_sm80.cu(8): here

/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_kernel.h(276): warning: variable "tQrQ" was declared but never referenced
          detected during:
            instantiation of "void flash::compute_attn_1rowblock<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params,Is_Q_split_K>(const Params &, int, int, int) [with Kernel_traits=Flash_fwd_kernel_traits<256, 128, 64, 8, false, false, cutlass::bfloat16_t, Flash_kernel_traits<256, 128, 64, 8, cutlass::bfloat16_t>>, Is_dropout=true, Is_causal=true, Is_even_N=true, Is_even_K=true, Return_softmax=false, Params=Flash_fwd_params, Is_Q_split_K=false]" 
(589): here
            instantiation of "void flash::compute_attn<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &) [with Kernel_traits=Flash_fwd_kernel_traits<256, 128, 64, 8, false, false, cutlass::bfloat16_t, Flash_kernel_traits<256, 128, 64, 8, cutlass::bfloat16_t>>, Is_dropout=true, Is_causal=true, Is_even_N=true, Is_even_K=true, Return_softmax=false, Params=Flash_fwd_params]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(15): here
            instantiation of "void flash_fwd_kernel<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax>(Flash_fwd_params) [with Kernel_traits=Flash_fwd_kernel_traits<256, 128, 64, 8, false, false, cutlass::bfloat16_t, Flash_kernel_traits<256, 128, 64, 8, cutlass::bfloat16_t>>, Is_dropout=true, Is_causal=true, Is_even_N=true, Is_even_K=true, Return_softmax=false]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(34): here
            instantiation of "void run_flash_fwd<Kernel_traits,Is_dropout,Is_causal>(Flash_fwd_params &, cudaStream_t) [with Kernel_traits=Flash_fwd_kernel_traits<256, 128, 64, 8, false, false, cutlass::bfloat16_t, Flash_kernel_traits<256, 128, 64, 8, cutlass::bfloat16_t>>, Is_dropout=true, Is_causal=true]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(236): here
            instantiation of "void run_mha_fwd_hdim256<T>(Flash_fwd_params &, cudaStream_t) [with T=cutlass::bfloat16_t]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_hdim256_bf16_sm80.cu(8): here

/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_kernel.h(140): warning: variable "MMA_M" was declared but never referenced
          detected during:
            instantiation of "void flash::compute_attn_1rowblock<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params,Is_Q_split_K>(const Params &, int, int, int) [with Kernel_traits=Flash_fwd_kernel_traits<256, 128, 64, 8, false, false, cutlass::bfloat16_t, Flash_kernel_traits<256, 128, 64, 8, cutlass::bfloat16_t>>, Is_dropout=true, Is_causal=true, Is_even_N=true, Is_even_K=false, Return_softmax=true, Params=Flash_fwd_params, Is_Q_split_K=false]" 
(589): here
            instantiation of "void flash::compute_attn<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &) [with Kernel_traits=Flash_fwd_kernel_traits<256, 128, 64, 8, false, false, cutlass::bfloat16_t, Flash_kernel_traits<256, 128, 64, 8, cutlass::bfloat16_t>>, Is_dropout=true, Is_causal=true, Is_even_N=true, Is_even_K=false, Return_softmax=true, Params=Flash_fwd_params]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(15): here
            instantiation of "void flash_fwd_kernel<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax>(Flash_fwd_params) [with Kernel_traits=Flash_fwd_kernel_traits<256, 128, 64, 8, false, false, cutlass::bfloat16_t, Flash_kernel_traits<256, 128, 64, 8, cutlass::bfloat16_t>>, Is_dropout=true, Is_causal=true, Is_even_N=true, Is_even_K=false, Return_softmax=true]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(34): here
            instantiation of "void run_flash_fwd<Kernel_traits,Is_dropout,Is_causal>(Flash_fwd_params &, cudaStream_t) [with Kernel_traits=Flash_fwd_kernel_traits<256, 128, 64, 8, false, false, cutlass::bfloat16_t, Flash_kernel_traits<256, 128, 64, 8, cutlass::bfloat16_t>>, Is_dropout=true, Is_causal=true]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(236): here
            instantiation of "void run_mha_fwd_hdim256<T>(Flash_fwd_params &, cudaStream_t) [with T=cutlass::bfloat16_t]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_hdim256_bf16_sm80.cu(8): here

/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_kernel.h(276): warning: variable "tQrQ" was declared but never referenced
          detected during:
            instantiation of "void flash::compute_attn_1rowblock<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params,Is_Q_split_K>(const Params &, int, int, int) [with Kernel_traits=Flash_fwd_kernel_traits<256, 128, 64, 8, false, false, cutlass::bfloat16_t, Flash_kernel_traits<256, 128, 64, 8, cutlass::bfloat16_t>>, Is_dropout=true, Is_causal=true, Is_even_N=true, Is_even_K=false, Return_softmax=true, Params=Flash_fwd_params, Is_Q_split_K=false]" 
(589): here
            instantiation of "void flash::compute_attn<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &) [with Kernel_traits=Flash_fwd_kernel_traits<256, 128, 64, 8, false, false, cutlass::bfloat16_t, Flash_kernel_traits<256, 128, 64, 8, cutlass::bfloat16_t>>, Is_dropout=true, Is_causal=true, Is_even_N=true, Is_even_K=false, Return_softmax=true, Params=Flash_fwd_params]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(15): here
            instantiation of "void flash_fwd_kernel<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax>(Flash_fwd_params) [with Kernel_traits=Flash_fwd_kernel_traits<256, 128, 64, 8, false, false, cutlass::bfloat16_t, Flash_kernel_traits<256, 128, 64, 8, cutlass::bfloat16_t>>, Is_dropout=true, Is_causal=true, Is_even_N=true, Is_even_K=false, Return_softmax=true]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(34): here
            instantiation of "void run_flash_fwd<Kernel_traits,Is_dropout,Is_causal>(Flash_fwd_params &, cudaStream_t) [with Kernel_traits=Flash_fwd_kernel_traits<256, 128, 64, 8, false, false, cutlass::bfloat16_t, Flash_kernel_traits<256, 128, 64, 8, cutlass::bfloat16_t>>, Is_dropout=true, Is_causal=true]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(236): here
            instantiation of "void run_mha_fwd_hdim256<T>(Flash_fwd_params &, cudaStream_t) [with T=cutlass::bfloat16_t]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_hdim256_bf16_sm80.cu(8): here

/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_kernel.h(140): warning: variable "MMA_M" was declared but never referenced
          detected during:
            instantiation of "void flash::compute_attn_1rowblock<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params,Is_Q_split_K>(const Params &, int, int, int) [with Kernel_traits=Flash_fwd_kernel_traits<256, 128, 64, 8, false, false, cutlass::bfloat16_t, Flash_kernel_traits<256, 128, 64, 8, cutlass::bfloat16_t>>, Is_dropout=true, Is_causal=true, Is_even_N=true, Is_even_K=false, Return_softmax=false, Params=Flash_fwd_params, Is_Q_split_K=false]" 
(589): here
            instantiation of "void flash::compute_attn<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &) [with Kernel_traits=Flash_fwd_kernel_traits<256, 128, 64, 8, false, false, cutlass::bfloat16_t, Flash_kernel_traits<256, 128, 64, 8, cutlass::bfloat16_t>>, Is_dropout=true, Is_causal=true, Is_even_N=true, Is_even_K=false, Return_softmax=false, Params=Flash_fwd_params]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(15): here
            instantiation of "void flash_fwd_kernel<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax>(Flash_fwd_params) [with Kernel_traits=Flash_fwd_kernel_traits<256, 128, 64, 8, false, false, cutlass::bfloat16_t, Flash_kernel_traits<256, 128, 64, 8, cutlass::bfloat16_t>>, Is_dropout=true, Is_causal=true, Is_even_N=true, Is_even_K=false, Return_softmax=false]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(34): here
            instantiation of "void run_flash_fwd<Kernel_traits,Is_dropout,Is_causal>(Flash_fwd_params &, cudaStream_t) [with Kernel_traits=Flash_fwd_kernel_traits<256, 128, 64, 8, false, false, cutlass::bfloat16_t, Flash_kernel_traits<256, 128, 64, 8, cutlass::bfloat16_t>>, Is_dropout=true, Is_causal=true]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(236): here
            instantiation of "void run_mha_fwd_hdim256<T>(Flash_fwd_params &, cudaStream_t) [with T=cutlass::bfloat16_t]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_hdim256_bf16_sm80.cu(8): here

/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_kernel.h(276): warning: variable "tQrQ" was declared but never referenced
          detected during:
            instantiation of "void flash::compute_attn_1rowblock<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params,Is_Q_split_K>(const Params &, int, int, int) [with Kernel_traits=Flash_fwd_kernel_traits<256, 128, 64, 8, false, false, cutlass::bfloat16_t, Flash_kernel_traits<256, 128, 64, 8, cutlass::bfloat16_t>>, Is_dropout=true, Is_causal=true, Is_even_N=true, Is_even_K=false, Return_softmax=false, Params=Flash_fwd_params, Is_Q_split_K=false]" 
(589): here
            instantiation of "void flash::compute_attn<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &) [with Kernel_traits=Flash_fwd_kernel_traits<256, 128, 64, 8, false, false, cutlass::bfloat16_t, Flash_kernel_traits<256, 128, 64, 8, cutlass::bfloat16_t>>, Is_dropout=true, Is_causal=true, Is_even_N=true, Is_even_K=false, Return_softmax=false, Params=Flash_fwd_params]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(15): here
            instantiation of "void flash_fwd_kernel<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax>(Flash_fwd_params) [with Kernel_traits=Flash_fwd_kernel_traits<256, 128, 64, 8, false, false, cutlass::bfloat16_t, Flash_kernel_traits<256, 128, 64, 8, cutlass::bfloat16_t>>, Is_dropout=true, Is_causal=true, Is_even_N=true, Is_even_K=false, Return_softmax=false]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(34): here
            instantiation of "void run_flash_fwd<Kernel_traits,Is_dropout,Is_causal>(Flash_fwd_params &, cudaStream_t) [with Kernel_traits=Flash_fwd_kernel_traits<256, 128, 64, 8, false, false, cutlass::bfloat16_t, Flash_kernel_traits<256, 128, 64, 8, cutlass::bfloat16_t>>, Is_dropout=true, Is_causal=true]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(236): here
            instantiation of "void run_mha_fwd_hdim256<T>(Flash_fwd_params &, cudaStream_t) [with T=cutlass::bfloat16_t]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_hdim256_bf16_sm80.cu(8): here

/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_kernel.h(140): warning: variable "MMA_M" was declared but never referenced
          detected during:
            instantiation of "void flash::compute_attn_1rowblock<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params,Is_Q_split_K>(const Params &, int, int, int) [with Kernel_traits=Flash_fwd_kernel_traits<256, 128, 64, 8, false, false, cutlass::bfloat16_t, Flash_kernel_traits<256, 128, 64, 8, cutlass::bfloat16_t>>, Is_dropout=true, Is_causal=true, Is_even_N=false, Is_even_K=true, Return_softmax=true, Params=Flash_fwd_params, Is_Q_split_K=false]" 
(589): here
            instantiation of "void flash::compute_attn<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &) [with Kernel_traits=Flash_fwd_kernel_traits<256, 128, 64, 8, false, false, cutlass::bfloat16_t, Flash_kernel_traits<256, 128, 64, 8, cutlass::bfloat16_t>>, Is_dropout=true, Is_causal=true, Is_even_N=false, Is_even_K=true, Return_softmax=true, Params=Flash_fwd_params]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(15): here
            instantiation of "void flash_fwd_kernel<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax>(Flash_fwd_params) [with Kernel_traits=Flash_fwd_kernel_traits<256, 128, 64, 8, false, false, cutlass::bfloat16_t, Flash_kernel_traits<256, 128, 64, 8, cutlass::bfloat16_t>>, Is_dropout=true, Is_causal=true, Is_even_N=false, Is_even_K=true, Return_softmax=true]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(34): here
            instantiation of "void run_flash_fwd<Kernel_traits,Is_dropout,Is_causal>(Flash_fwd_params &, cudaStream_t) [with Kernel_traits=Flash_fwd_kernel_traits<256, 128, 64, 8, false, false, cutlass::bfloat16_t, Flash_kernel_traits<256, 128, 64, 8, cutlass::bfloat16_t>>, Is_dropout=true, Is_causal=true]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(236): here
            instantiation of "void run_mha_fwd_hdim256<T>(Flash_fwd_params &, cudaStream_t) [with T=cutlass::bfloat16_t]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_hdim256_bf16_sm80.cu(8): here

/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_kernel.h(276): warning: variable "tQrQ" was declared but never referenced
          detected during:
            instantiation of "void flash::compute_attn_1rowblock<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params,Is_Q_split_K>(const Params &, int, int, int) [with Kernel_traits=Flash_fwd_kernel_traits<256, 128, 64, 8, false, false, cutlass::bfloat16_t, Flash_kernel_traits<256, 128, 64, 8, cutlass::bfloat16_t>>, Is_dropout=true, Is_causal=true, Is_even_N=false, Is_even_K=true, Return_softmax=true, Params=Flash_fwd_params, Is_Q_split_K=false]" 
(589): here
            instantiation of "void flash::compute_attn<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &) [with Kernel_traits=Flash_fwd_kernel_traits<256, 128, 64, 8, false, false, cutlass::bfloat16_t, Flash_kernel_traits<256, 128, 64, 8, cutlass::bfloat16_t>>, Is_dropout=true, Is_causal=true, Is_even_N=false, Is_even_K=true, Return_softmax=true, Params=Flash_fwd_params]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(15): here
            instantiation of "void flash_fwd_kernel<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax>(Flash_fwd_params) [with Kernel_traits=Flash_fwd_kernel_traits<256, 128, 64, 8, false, false, cutlass::bfloat16_t, Flash_kernel_traits<256, 128, 64, 8, cutlass::bfloat16_t>>, Is_dropout=true, Is_causal=true, Is_even_N=false, Is_even_K=true, Return_softmax=true]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(34): here
            instantiation of "void run_flash_fwd<Kernel_traits,Is_dropout,Is_causal>(Flash_fwd_params &, cudaStream_t) [with Kernel_traits=Flash_fwd_kernel_traits<256, 128, 64, 8, false, false, cutlass::bfloat16_t, Flash_kernel_traits<256, 128, 64, 8, cutlass::bfloat16_t>>, Is_dropout=true, Is_causal=true]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(236): here
            instantiation of "void run_mha_fwd_hdim256<T>(Flash_fwd_params &, cudaStream_t) [with T=cutlass::bfloat16_t]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_hdim256_bf16_sm80.cu(8): here

/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_kernel.h(140): warning: variable "MMA_M" was declared but never referenced
          detected during:
            instantiation of "void flash::compute_attn_1rowblock<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params,Is_Q_split_K>(const Params &, int, int, int) [with Kernel_traits=Flash_fwd_kernel_traits<256, 128, 64, 8, false, false, cutlass::bfloat16_t, Flash_kernel_traits<256, 128, 64, 8, cutlass::bfloat16_t>>, Is_dropout=true, Is_causal=true, Is_even_N=false, Is_even_K=true, Return_softmax=false, Params=Flash_fwd_params, Is_Q_split_K=false]" 
(589): here
            instantiation of "void flash::compute_attn<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &) [with Kernel_traits=Flash_fwd_kernel_traits<256, 128, 64, 8, false, false, cutlass::bfloat16_t, Flash_kernel_traits<256, 128, 64, 8, cutlass::bfloat16_t>>, Is_dropout=true, Is_causal=true, Is_even_N=false, Is_even_K=true, Return_softmax=false, Params=Flash_fwd_params]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(15): here
            instantiation of "void flash_fwd_kernel<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax>(Flash_fwd_params) [with Kernel_traits=Flash_fwd_kernel_traits<256, 128, 64, 8, false, false, cutlass::bfloat16_t, Flash_kernel_traits<256, 128, 64, 8, cutlass::bfloat16_t>>, Is_dropout=true, Is_causal=true, Is_even_N=false, Is_even_K=true, Return_softmax=false]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(34): here
            instantiation of "void run_flash_fwd<Kernel_traits,Is_dropout,Is_causal>(Flash_fwd_params &, cudaStream_t) [with Kernel_traits=Flash_fwd_kernel_traits<256, 128, 64, 8, false, false, cutlass::bfloat16_t, Flash_kernel_traits<256, 128, 64, 8, cutlass::bfloat16_t>>, Is_dropout=true, Is_causal=true]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(236): here
            instantiation of "void run_mha_fwd_hdim256<T>(Flash_fwd_params &, cudaStream_t) [with T=cutlass::bfloat16_t]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_hdim256_bf16_sm80.cu(8): here

/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_kernel.h(276): warning: variable "tQrQ" was declared but never referenced
          detected during:
            instantiation of "void flash::compute_attn_1rowblock<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params,Is_Q_split_K>(const Params &, int, int, int) [with Kernel_traits=Flash_fwd_kernel_traits<256, 128, 64, 8, false, false, cutlass::bfloat16_t, Flash_kernel_traits<256, 128, 64, 8, cutlass::bfloat16_t>>, Is_dropout=true, Is_causal=true, Is_even_N=false, Is_even_K=true, Return_softmax=false, Params=Flash_fwd_params, Is_Q_split_K=false]" 
(589): here
            instantiation of "void flash::compute_attn<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &) [with Kernel_traits=Flash_fwd_kernel_traits<256, 128, 64, 8, false, false, cutlass::bfloat16_t, Flash_kernel_traits<256, 128, 64, 8, cutlass::bfloat16_t>>, Is_dropout=true, Is_causal=true, Is_even_N=false, Is_even_K=true, Return_softmax=false, Params=Flash_fwd_params]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(15): here
            instantiation of "void flash_fwd_kernel<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax>(Flash_fwd_params) [with Kernel_traits=Flash_fwd_kernel_traits<256, 128, 64, 8, false, false, cutlass::bfloat16_t, Flash_kernel_traits<256, 128, 64, 8, cutlass::bfloat16_t>>, Is_dropout=true, Is_causal=true, Is_even_N=false, Is_even_K=true, Return_softmax=false]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(34): here
            instantiation of "void run_flash_fwd<Kernel_traits,Is_dropout,Is_causal>(Flash_fwd_params &, cudaStream_t) [with Kernel_traits=Flash_fwd_kernel_traits<256, 128, 64, 8, false, false, cutlass::bfloat16_t, Flash_kernel_traits<256, 128, 64, 8, cutlass::bfloat16_t>>, Is_dropout=true, Is_causal=true]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(236): here
            instantiation of "void run_mha_fwd_hdim256<T>(Flash_fwd_params &, cudaStream_t) [with T=cutlass::bfloat16_t]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_hdim256_bf16_sm80.cu(8): here

/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_kernel.h(140): warning: variable "MMA_M" was declared but never referenced
          detected during:
            instantiation of "void flash::compute_attn_1rowblock<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params,Is_Q_split_K>(const Params &, int, int, int) [with Kernel_traits=Flash_fwd_kernel_traits<256, 128, 64, 8, false, false, cutlass::bfloat16_t, Flash_kernel_traits<256, 128, 64, 8, cutlass::bfloat16_t>>, Is_dropout=true, Is_causal=true, Is_even_N=false, Is_even_K=false, Return_softmax=true, Params=Flash_fwd_params, Is_Q_split_K=false]" 
(589): here
            instantiation of "void flash::compute_attn<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &) [with Kernel_traits=Flash_fwd_kernel_traits<256, 128, 64, 8, false, false, cutlass::bfloat16_t, Flash_kernel_traits<256, 128, 64, 8, cutlass::bfloat16_t>>, Is_dropout=true, Is_causal=true, Is_even_N=false, Is_even_K=false, Return_softmax=true, Params=Flash_fwd_params]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(15): here
            instantiation of "void flash_fwd_kernel<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax>(Flash_fwd_params) [with Kernel_traits=Flash_fwd_kernel_traits<256, 128, 64, 8, false, false, cutlass::bfloat16_t, Flash_kernel_traits<256, 128, 64, 8, cutlass::bfloat16_t>>, Is_dropout=true, Is_causal=true, Is_even_N=false, Is_even_K=false, Return_softmax=true]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(34): here
            instantiation of "void run_flash_fwd<Kernel_traits,Is_dropout,Is_causal>(Flash_fwd_params &, cudaStream_t) [with Kernel_traits=Flash_fwd_kernel_traits<256, 128, 64, 8, false, false, cutlass::bfloat16_t, Flash_kernel_traits<256, 128, 64, 8, cutlass::bfloat16_t>>, Is_dropout=true, Is_causal=true]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(236): here
            instantiation of "void run_mha_fwd_hdim256<T>(Flash_fwd_params &, cudaStream_t) [with T=cutlass::bfloat16_t]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_hdim256_bf16_sm80.cu(8): here

/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_kernel.h(276): warning: variable "tQrQ" was declared but never referenced
          detected during:
            instantiation of "void flash::compute_attn_1rowblock<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params,Is_Q_split_K>(const Params &, int, int, int) [with Kernel_traits=Flash_fwd_kernel_traits<256, 128, 64, 8, false, false, cutlass::bfloat16_t, Flash_kernel_traits<256, 128, 64, 8, cutlass::bfloat16_t>>, Is_dropout=true, Is_causal=true, Is_even_N=false, Is_even_K=false, Return_softmax=true, Params=Flash_fwd_params, Is_Q_split_K=false]" 
(589): here
            instantiation of "void flash::compute_attn<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &) [with Kernel_traits=Flash_fwd_kernel_traits<256, 128, 64, 8, false, false, cutlass::bfloat16_t, Flash_kernel_traits<256, 128, 64, 8, cutlass::bfloat16_t>>, Is_dropout=true, Is_causal=true, Is_even_N=false, Is_even_K=false, Return_softmax=true, Params=Flash_fwd_params]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(15): here
            instantiation of "void flash_fwd_kernel<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax>(Flash_fwd_params) [with Kernel_traits=Flash_fwd_kernel_traits<256, 128, 64, 8, false, false, cutlass::bfloat16_t, Flash_kernel_traits<256, 128, 64, 8, cutlass::bfloat16_t>>, Is_dropout=true, Is_causal=true, Is_even_N=false, Is_even_K=false, Return_softmax=true]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(34): here
            instantiation of "void run_flash_fwd<Kernel_traits,Is_dropout,Is_causal>(Flash_fwd_params &, cudaStream_t) [with Kernel_traits=Flash_fwd_kernel_traits<256, 128, 64, 8, false, false, cutlass::bfloat16_t, Flash_kernel_traits<256, 128, 64, 8, cutlass::bfloat16_t>>, Is_dropout=true, Is_causal=true]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(236): here
            instantiation of "void run_mha_fwd_hdim256<T>(Flash_fwd_params &, cudaStream_t) [with T=cutlass::bfloat16_t]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_hdim256_bf16_sm80.cu(8): here

/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_kernel.h(140): warning: variable "MMA_M" was declared but never referenced
          detected during:
            instantiation of "void flash::compute_attn_1rowblock<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params,Is_Q_split_K>(const Params &, int, int, int) [with Kernel_traits=Flash_fwd_kernel_traits<256, 128, 64, 8, false, false, cutlass::bfloat16_t, Flash_kernel_traits<256, 128, 64, 8, cutlass::bfloat16_t>>, Is_dropout=true, Is_causal=true, Is_even_N=false, Is_even_K=false, Return_softmax=false, Params=Flash_fwd_params, Is_Q_split_K=false]" 
(589): here
            instantiation of "void flash::compute_attn<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &) [with Kernel_traits=Flash_fwd_kernel_traits<256, 128, 64, 8, false, false, cutlass::bfloat16_t, Flash_kernel_traits<256, 128, 64, 8, cutlass::bfloat16_t>>, Is_dropout=true, Is_causal=true, Is_even_N=false, Is_even_K=false, Return_softmax=false, Params=Flash_fwd_params]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(15): here
            instantiation of "void flash_fwd_kernel<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax>(Flash_fwd_params) [with Kernel_traits=Flash_fwd_kernel_traits<256, 128, 64, 8, false, false, cutlass::bfloat16_t, Flash_kernel_traits<256, 128, 64, 8, cutlass::bfloat16_t>>, Is_dropout=true, Is_causal=true, Is_even_N=false, Is_even_K=false, Return_softmax=false]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(34): here
            instantiation of "void run_flash_fwd<Kernel_traits,Is_dropout,Is_causal>(Flash_fwd_params &, cudaStream_t) [with Kernel_traits=Flash_fwd_kernel_traits<256, 128, 64, 8, false, false, cutlass::bfloat16_t, Flash_kernel_traits<256, 128, 64, 8, cutlass::bfloat16_t>>, Is_dropout=true, Is_causal=true]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(236): here
            instantiation of "void run_mha_fwd_hdim256<T>(Flash_fwd_params &, cudaStream_t) [with T=cutlass::bfloat16_t]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_hdim256_bf16_sm80.cu(8): here

/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_kernel.h(276): warning: variable "tQrQ" was declared but never referenced
          detected during:
            instantiation of "void flash::compute_attn_1rowblock<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params,Is_Q_split_K>(const Params &, int, int, int) [with Kernel_traits=Flash_fwd_kernel_traits<256, 128, 64, 8, false, false, cutlass::bfloat16_t, Flash_kernel_traits<256, 128, 64, 8, cutlass::bfloat16_t>>, Is_dropout=true, Is_causal=true, Is_even_N=false, Is_even_K=false, Return_softmax=false, Params=Flash_fwd_params, Is_Q_split_K=false]" 
(589): here
            instantiation of "void flash::compute_attn<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &) [with Kernel_traits=Flash_fwd_kernel_traits<256, 128, 64, 8, false, false, cutlass::bfloat16_t, Flash_kernel_traits<256, 128, 64, 8, cutlass::bfloat16_t>>, Is_dropout=true, Is_causal=true, Is_even_N=false, Is_even_K=false, Return_softmax=false, Params=Flash_fwd_params]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(15): here
            instantiation of "void flash_fwd_kernel<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax>(Flash_fwd_params) [with Kernel_traits=Flash_fwd_kernel_traits<256, 128, 64, 8, false, false, cutlass::bfloat16_t, Flash_kernel_traits<256, 128, 64, 8, cutlass::bfloat16_t>>, Is_dropout=true, Is_causal=true, Is_even_N=false, Is_even_K=false, Return_softmax=false]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(34): here
            instantiation of "void run_flash_fwd<Kernel_traits,Is_dropout,Is_causal>(Flash_fwd_params &, cudaStream_t) [with Kernel_traits=Flash_fwd_kernel_traits<256, 128, 64, 8, false, false, cutlass::bfloat16_t, Flash_kernel_traits<256, 128, 64, 8, cutlass::bfloat16_t>>, Is_dropout=true, Is_causal=true]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(236): here
            instantiation of "void run_mha_fwd_hdim256<T>(Flash_fwd_params &, cudaStream_t) [with T=cutlass::bfloat16_t]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_hdim256_bf16_sm80.cu(8): here

/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_kernel.h(140): warning: variable "MMA_M" was declared but never referenced
          detected during:
            instantiation of "void flash::compute_attn_1rowblock<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params,Is_Q_split_K>(const Params &, int, int, int) [with Kernel_traits=Flash_fwd_kernel_traits<256, 64, 64, 4, false, false, cutlass::bfloat16_t, Flash_kernel_traits<256, 64, 64, 4, cutlass::bfloat16_t>>, Is_dropout=true, Is_causal=true, Is_even_N=true, Is_even_K=true, Return_softmax=true, Params=Flash_fwd_params, Is_Q_split_K=false]" 
(589): here
            instantiation of "void flash::compute_attn<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &) [with Kernel_traits=Flash_fwd_kernel_traits<256, 64, 64, 4, false, false, cutlass::bfloat16_t, Flash_kernel_traits<256, 64, 64, 4, cutlass::bfloat16_t>>, Is_dropout=true, Is_causal=true, Is_even_N=true, Is_even_K=true, Return_softmax=true, Params=Flash_fwd_params]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(15): here
            instantiation of "void flash_fwd_kernel<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax>(Flash_fwd_params) [with Kernel_traits=Flash_fwd_kernel_traits<256, 64, 64, 4, false, false, cutlass::bfloat16_t, Flash_kernel_traits<256, 64, 64, 4, cutlass::bfloat16_t>>, Is_dropout=true, Is_causal=true, Is_even_N=true, Is_even_K=true, Return_softmax=true]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(34): here
            instantiation of "void run_flash_fwd<Kernel_traits,Is_dropout,Is_causal>(Flash_fwd_params &, cudaStream_t) [with Kernel_traits=Flash_fwd_kernel_traits<256, 64, 64, 4, false, false, cutlass::bfloat16_t, Flash_kernel_traits<256, 64, 64, 4, cutlass::bfloat16_t>>, Is_dropout=true, Is_causal=true]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(236): here
            instantiation of "void run_mha_fwd_hdim256<T>(Flash_fwd_params &, cudaStream_t) [with T=cutlass::bfloat16_t]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_hdim256_bf16_sm80.cu(8): here

/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_kernel.h(276): warning: variable "tQrQ" was declared but never referenced
          detected during:
            instantiation of "void flash::compute_attn_1rowblock<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params,Is_Q_split_K>(const Params &, int, int, int) [with Kernel_traits=Flash_fwd_kernel_traits<256, 64, 64, 4, false, false, cutlass::bfloat16_t, Flash_kernel_traits<256, 64, 64, 4, cutlass::bfloat16_t>>, Is_dropout=true, Is_causal=true, Is_even_N=true, Is_even_K=true, Return_softmax=true, Params=Flash_fwd_params, Is_Q_split_K=false]" 
(589): here
            instantiation of "void flash::compute_attn<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &) [with Kernel_traits=Flash_fwd_kernel_traits<256, 64, 64, 4, false, false, cutlass::bfloat16_t, Flash_kernel_traits<256, 64, 64, 4, cutlass::bfloat16_t>>, Is_dropout=true, Is_causal=true, Is_even_N=true, Is_even_K=true, Return_softmax=true, Params=Flash_fwd_params]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(15): here
            instantiation of "void flash_fwd_kernel<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax>(Flash_fwd_params) [with Kernel_traits=Flash_fwd_kernel_traits<256, 64, 64, 4, false, false, cutlass::bfloat16_t, Flash_kernel_traits<256, 64, 64, 4, cutlass::bfloat16_t>>, Is_dropout=true, Is_causal=true, Is_even_N=true, Is_even_K=true, Return_softmax=true]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(34): here
            instantiation of "void run_flash_fwd<Kernel_traits,Is_dropout,Is_causal>(Flash_fwd_params &, cudaStream_t) [with Kernel_traits=Flash_fwd_kernel_traits<256, 64, 64, 4, false, false, cutlass::bfloat16_t, Flash_kernel_traits<256, 64, 64, 4, cutlass::bfloat16_t>>, Is_dropout=true, Is_causal=true]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(236): here
            instantiation of "void run_mha_fwd_hdim256<T>(Flash_fwd_params &, cudaStream_t) [with T=cutlass::bfloat16_t]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_hdim256_bf16_sm80.cu(8): here

/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_kernel.h(140): warning: variable "MMA_M" was declared but never referenced
          detected during:
            instantiation of "void flash::compute_attn_1rowblock<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params,Is_Q_split_K>(const Params &, int, int, int) [with Kernel_traits=Flash_fwd_kernel_traits<256, 64, 64, 4, false, false, cutlass::bfloat16_t, Flash_kernel_traits<256, 64, 64, 4, cutlass::bfloat16_t>>, Is_dropout=true, Is_causal=true, Is_even_N=true, Is_even_K=true, Return_softmax=false, Params=Flash_fwd_params, Is_Q_split_K=false]" 
(589): here
            instantiation of "void flash::compute_attn<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &) [with Kernel_traits=Flash_fwd_kernel_traits<256, 64, 64, 4, false, false, cutlass::bfloat16_t, Flash_kernel_traits<256, 64, 64, 4, cutlass::bfloat16_t>>, Is_dropout=true, Is_causal=true, Is_even_N=true, Is_even_K=true, Return_softmax=false, Params=Flash_fwd_params]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(15): here
            instantiation of "void flash_fwd_kernel<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax>(Flash_fwd_params) [with Kernel_traits=Flash_fwd_kernel_traits<256, 64, 64, 4, false, false, cutlass::bfloat16_t, Flash_kernel_traits<256, 64, 64, 4, cutlass::bfloat16_t>>, Is_dropout=true, Is_causal=true, Is_even_N=true, Is_even_K=true, Return_softmax=false]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(34): here
            instantiation of "void run_flash_fwd<Kernel_traits,Is_dropout,Is_causal>(Flash_fwd_params &, cudaStream_t) [with Kernel_traits=Flash_fwd_kernel_traits<256, 64, 64, 4, false, false, cutlass::bfloat16_t, Flash_kernel_traits<256, 64, 64, 4, cutlass::bfloat16_t>>, Is_dropout=true, Is_causal=true]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(236): here
            instantiation of "void run_mha_fwd_hdim256<T>(Flash_fwd_params &, cudaStream_t) [with T=cutlass::bfloat16_t]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_hdim256_bf16_sm80.cu(8): here

/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_kernel.h(276): warning: variable "tQrQ" was declared but never referenced
          detected during:
            instantiation of "void flash::compute_attn_1rowblock<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params,Is_Q_split_K>(const Params &, int, int, int) [with Kernel_traits=Flash_fwd_kernel_traits<256, 64, 64, 4, false, false, cutlass::bfloat16_t, Flash_kernel_traits<256, 64, 64, 4, cutlass::bfloat16_t>>, Is_dropout=true, Is_causal=true, Is_even_N=true, Is_even_K=true, Return_softmax=false, Params=Flash_fwd_params, Is_Q_split_K=false]" 
(589): here
            instantiation of "void flash::compute_attn<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &) [with Kernel_traits=Flash_fwd_kernel_traits<256, 64, 64, 4, false, false, cutlass::bfloat16_t, Flash_kernel_traits<256, 64, 64, 4, cutlass::bfloat16_t>>, Is_dropout=true, Is_causal=true, Is_even_N=true, Is_even_K=true, Return_softmax=false, Params=Flash_fwd_params]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(15): here
            instantiation of "void flash_fwd_kernel<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax>(Flash_fwd_params) [with Kernel_traits=Flash_fwd_kernel_traits<256, 64, 64, 4, false, false, cutlass::bfloat16_t, Flash_kernel_traits<256, 64, 64, 4, cutlass::bfloat16_t>>, Is_dropout=true, Is_causal=true, Is_even_N=true, Is_even_K=true, Return_softmax=false]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(34): here
            instantiation of "void run_flash_fwd<Kernel_traits,Is_dropout,Is_causal>(Flash_fwd_params &, cudaStream_t) [with Kernel_traits=Flash_fwd_kernel_traits<256, 64, 64, 4, false, false, cutlass::bfloat16_t, Flash_kernel_traits<256, 64, 64, 4, cutlass::bfloat16_t>>, Is_dropout=true, Is_causal=true]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(236): here
            instantiation of "void run_mha_fwd_hdim256<T>(Flash_fwd_params &, cudaStream_t) [with T=cutlass::bfloat16_t]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_hdim256_bf16_sm80.cu(8): here

/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_kernel.h(140): warning: variable "MMA_M" was declared but never referenced
          detected during:
            instantiation of "void flash::compute_attn_1rowblock<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params,Is_Q_split_K>(const Params &, int, int, int) [with Kernel_traits=Flash_fwd_kernel_traits<256, 64, 64, 4, false, false, cutlass::bfloat16_t, Flash_kernel_traits<256, 64, 64, 4, cutlass::bfloat16_t>>, Is_dropout=true, Is_causal=true, Is_even_N=true, Is_even_K=false, Return_softmax=true, Params=Flash_fwd_params, Is_Q_split_K=false]" 
(589): here
            instantiation of "void flash::compute_attn<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &) [with Kernel_traits=Flash_fwd_kernel_traits<256, 64, 64, 4, false, false, cutlass::bfloat16_t, Flash_kernel_traits<256, 64, 64, 4, cutlass::bfloat16_t>>, Is_dropout=true, Is_causal=true, Is_even_N=true, Is_even_K=false, Return_softmax=true, Params=Flash_fwd_params]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(15): here
            instantiation of "void flash_fwd_kernel<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax>(Flash_fwd_params) [with Kernel_traits=Flash_fwd_kernel_traits<256, 64, 64, 4, false, false, cutlass::bfloat16_t, Flash_kernel_traits<256, 64, 64, 4, cutlass::bfloat16_t>>, Is_dropout=true, Is_causal=true, Is_even_N=true, Is_even_K=false, Return_softmax=true]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(34): here
            instantiation of "void run_flash_fwd<Kernel_traits,Is_dropout,Is_causal>(Flash_fwd_params &, cudaStream_t) [with Kernel_traits=Flash_fwd_kernel_traits<256, 64, 64, 4, false, false, cutlass::bfloat16_t, Flash_kernel_traits<256, 64, 64, 4, cutlass::bfloat16_t>>, Is_dropout=true, Is_causal=true]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(236): here
            instantiation of "void run_mha_fwd_hdim256<T>(Flash_fwd_params &, cudaStream_t) [with T=cutlass::bfloat16_t]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_hdim256_bf16_sm80.cu(8): here

/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_kernel.h(276): warning: variable "tQrQ" was declared but never referenced
          detected during:
            instantiation of "void flash::compute_attn_1rowblock<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params,Is_Q_split_K>(const Params &, int, int, int) [with Kernel_traits=Flash_fwd_kernel_traits<256, 64, 64, 4, false, false, cutlass::bfloat16_t, Flash_kernel_traits<256, 64, 64, 4, cutlass::bfloat16_t>>, Is_dropout=true, Is_causal=true, Is_even_N=true, Is_even_K=false, Return_softmax=true, Params=Flash_fwd_params, Is_Q_split_K=false]" 
(589): here
            instantiation of "void flash::compute_attn<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &) [with Kernel_traits=Flash_fwd_kernel_traits<256, 64, 64, 4, false, false, cutlass::bfloat16_t, Flash_kernel_traits<256, 64, 64, 4, cutlass::bfloat16_t>>, Is_dropout=true, Is_causal=true, Is_even_N=true, Is_even_K=false, Return_softmax=true, Params=Flash_fwd_params]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(15): here
            instantiation of "void flash_fwd_kernel<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax>(Flash_fwd_params) [with Kernel_traits=Flash_fwd_kernel_traits<256, 64, 64, 4, false, false, cutlass::bfloat16_t, Flash_kernel_traits<256, 64, 64, 4, cutlass::bfloat16_t>>, Is_dropout=true, Is_causal=true, Is_even_N=true, Is_even_K=false, Return_softmax=true]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(34): here
            instantiation of "void run_flash_fwd<Kernel_traits,Is_dropout,Is_causal>(Flash_fwd_params &, cudaStream_t) [with Kernel_traits=Flash_fwd_kernel_traits<256, 64, 64, 4, false, false, cutlass::bfloat16_t, Flash_kernel_traits<256, 64, 64, 4, cutlass::bfloat16_t>>, Is_dropout=true, Is_causal=true]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(236): here
            instantiation of "void run_mha_fwd_hdim256<T>(Flash_fwd_params &, cudaStream_t) [with T=cutlass::bfloat16_t]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_hdim256_bf16_sm80.cu(8): here

/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_kernel.h(140): warning: variable "MMA_M" was declared but never referenced
          detected during:
            instantiation of "void flash::compute_attn_1rowblock<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params,Is_Q_split_K>(const Params &, int, int, int) [with Kernel_traits=Flash_fwd_kernel_traits<256, 64, 64, 4, false, false, cutlass::bfloat16_t, Flash_kernel_traits<256, 64, 64, 4, cutlass::bfloat16_t>>, Is_dropout=true, Is_causal=true, Is_even_N=true, Is_even_K=false, Return_softmax=false, Params=Flash_fwd_params, Is_Q_split_K=false]" 
(589): here
            instantiation of "void flash::compute_attn<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &) [with Kernel_traits=Flash_fwd_kernel_traits<256, 64, 64, 4, false, false, cutlass::bfloat16_t, Flash_kernel_traits<256, 64, 64, 4, cutlass::bfloat16_t>>, Is_dropout=true, Is_causal=true, Is_even_N=true, Is_even_K=false, Return_softmax=false, Params=Flash_fwd_params]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(15): here
            instantiation of "void flash_fwd_kernel<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax>(Flash_fwd_params) [with Kernel_traits=Flash_fwd_kernel_traits<256, 64, 64, 4, false, false, cutlass::bfloat16_t, Flash_kernel_traits<256, 64, 64, 4, cutlass::bfloat16_t>>, Is_dropout=true, Is_causal=true, Is_even_N=true, Is_even_K=false, Return_softmax=false]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(34): here
            instantiation of "void run_flash_fwd<Kernel_traits,Is_dropout,Is_causal>(Flash_fwd_params &, cudaStream_t) [with Kernel_traits=Flash_fwd_kernel_traits<256, 64, 64, 4, false, false, cutlass::bfloat16_t, Flash_kernel_traits<256, 64, 64, 4, cutlass::bfloat16_t>>, Is_dropout=true, Is_causal=true]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(236): here
            instantiation of "void run_mha_fwd_hdim256<T>(Flash_fwd_params &, cudaStream_t) [with T=cutlass::bfloat16_t]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_hdim256_bf16_sm80.cu(8): here

/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_kernel.h(276): warning: variable "tQrQ" was declared but never referenced
          detected during:
            instantiation of "void flash::compute_attn_1rowblock<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params,Is_Q_split_K>(const Params &, int, int, int) [with Kernel_traits=Flash_fwd_kernel_traits<256, 64, 64, 4, false, false, cutlass::bfloat16_t, Flash_kernel_traits<256, 64, 64, 4, cutlass::bfloat16_t>>, Is_dropout=true, Is_causal=true, Is_even_N=true, Is_even_K=false, Return_softmax=false, Params=Flash_fwd_params, Is_Q_split_K=false]" 
(589): here
            instantiation of "void flash::compute_attn<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &) [with Kernel_traits=Flash_fwd_kernel_traits<256, 64, 64, 4, false, false, cutlass::bfloat16_t, Flash_kernel_traits<256, 64, 64, 4, cutlass::bfloat16_t>>, Is_dropout=true, Is_causal=true, Is_even_N=true, Is_even_K=false, Return_softmax=false, Params=Flash_fwd_params]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(15): here
            instantiation of "void flash_fwd_kernel<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax>(Flash_fwd_params) [with Kernel_traits=Flash_fwd_kernel_traits<256, 64, 64, 4, false, false, cutlass::bfloat16_t, Flash_kernel_traits<256, 64, 64, 4, cutlass::bfloat16_t>>, Is_dropout=true, Is_causal=true, Is_even_N=true, Is_even_K=false, Return_softmax=false]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(34): here
            instantiation of "void run_flash_fwd<Kernel_traits,Is_dropout,Is_causal>(Flash_fwd_params &, cudaStream_t) [with Kernel_traits=Flash_fwd_kernel_traits<256, 64, 64, 4, false, false, cutlass::bfloat16_t, Flash_kernel_traits<256, 64, 64, 4, cutlass::bfloat16_t>>, Is_dropout=true, Is_causal=true]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(236): here
            instantiation of "void run_mha_fwd_hdim256<T>(Flash_fwd_params &, cudaStream_t) [with T=cutlass::bfloat16_t]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_hdim256_bf16_sm80.cu(8): here

/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_kernel.h(140): warning: variable "MMA_M" was declared but never referenced
          detected during:
            instantiation of "void flash::compute_attn_1rowblock<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params,Is_Q_split_K>(const Params &, int, int, int) [with Kernel_traits=Flash_fwd_kernel_traits<256, 64, 64, 4, false, false, cutlass::bfloat16_t, Flash_kernel_traits<256, 64, 64, 4, cutlass::bfloat16_t>>, Is_dropout=true, Is_causal=true, Is_even_N=false, Is_even_K=true, Return_softmax=true, Params=Flash_fwd_params, Is_Q_split_K=false]" 
(589): here
            instantiation of "void flash::compute_attn<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &) [with Kernel_traits=Flash_fwd_kernel_traits<256, 64, 64, 4, false, false, cutlass::bfloat16_t, Flash_kernel_traits<256, 64, 64, 4, cutlass::bfloat16_t>>, Is_dropout=true, Is_causal=true, Is_even_N=false, Is_even_K=true, Return_softmax=true, Params=Flash_fwd_params]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(15): here
            instantiation of "void flash_fwd_kernel<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax>(Flash_fwd_params) [with Kernel_traits=Flash_fwd_kernel_traits<256, 64, 64, 4, false, false, cutlass::bfloat16_t, Flash_kernel_traits<256, 64, 64, 4, cutlass::bfloat16_t>>, Is_dropout=true, Is_causal=true, Is_even_N=false, Is_even_K=true, Return_softmax=true]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(34): here
            instantiation of "void run_flash_fwd<Kernel_traits,Is_dropout,Is_causal>(Flash_fwd_params &, cudaStream_t) [with Kernel_traits=Flash_fwd_kernel_traits<256, 64, 64, 4, false, false, cutlass::bfloat16_t, Flash_kernel_traits<256, 64, 64, 4, cutlass::bfloat16_t>>, Is_dropout=true, Is_causal=true]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(236): here
            instantiation of "void run_mha_fwd_hdim256<T>(Flash_fwd_params &, cudaStream_t) [with T=cutlass::bfloat16_t]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_hdim256_bf16_sm80.cu(8): here

/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_kernel.h(276): warning: variable "tQrQ" was declared but never referenced
          detected during:
            instantiation of "void flash::compute_attn_1rowblock<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params,Is_Q_split_K>(const Params &, int, int, int) [with Kernel_traits=Flash_fwd_kernel_traits<256, 64, 64, 4, false, false, cutlass::bfloat16_t, Flash_kernel_traits<256, 64, 64, 4, cutlass::bfloat16_t>>, Is_dropout=true, Is_causal=true, Is_even_N=false, Is_even_K=true, Return_softmax=true, Params=Flash_fwd_params, Is_Q_split_K=false]" 
(589): here
            instantiation of "void flash::compute_attn<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &) [with Kernel_traits=Flash_fwd_kernel_traits<256, 64, 64, 4, false, false, cutlass::bfloat16_t, Flash_kernel_traits<256, 64, 64, 4, cutlass::bfloat16_t>>, Is_dropout=true, Is_causal=true, Is_even_N=false, Is_even_K=true, Return_softmax=true, Params=Flash_fwd_params]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(15): here
            instantiation of "void flash_fwd_kernel<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax>(Flash_fwd_params) [with Kernel_traits=Flash_fwd_kernel_traits<256, 64, 64, 4, false, false, cutlass::bfloat16_t, Flash_kernel_traits<256, 64, 64, 4, cutlass::bfloat16_t>>, Is_dropout=true, Is_causal=true, Is_even_N=false, Is_even_K=true, Return_softmax=true]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(34): here
            instantiation of "void run_flash_fwd<Kernel_traits,Is_dropout,Is_causal>(Flash_fwd_params &, cudaStream_t) [with Kernel_traits=Flash_fwd_kernel_traits<256, 64, 64, 4, false, false, cutlass::bfloat16_t, Flash_kernel_traits<256, 64, 64, 4, cutlass::bfloat16_t>>, Is_dropout=true, Is_causal=true]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(236): here
            instantiation of "void run_mha_fwd_hdim256<T>(Flash_fwd_params &, cudaStream_t) [with T=cutlass::bfloat16_t]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_hdim256_bf16_sm80.cu(8): here

/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_kernel.h(140): warning: variable "MMA_M" was declared but never referenced
          detected during:
            instantiation of "void flash::compute_attn_1rowblock<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params,Is_Q_split_K>(const Params &, int, int, int) [with Kernel_traits=Flash_fwd_kernel_traits<256, 64, 64, 4, false, false, cutlass::bfloat16_t, Flash_kernel_traits<256, 64, 64, 4, cutlass::bfloat16_t>>, Is_dropout=true, Is_causal=true, Is_even_N=false, Is_even_K=true, Return_softmax=false, Params=Flash_fwd_params, Is_Q_split_K=false]" 
(589): here
            instantiation of "void flash::compute_attn<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &) [with Kernel_traits=Flash_fwd_kernel_traits<256, 64, 64, 4, false, false, cutlass::bfloat16_t, Flash_kernel_traits<256, 64, 64, 4, cutlass::bfloat16_t>>, Is_dropout=true, Is_causal=true, Is_even_N=false, Is_even_K=true, Return_softmax=false, Params=Flash_fwd_params]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(15): here
            instantiation of "void flash_fwd_kernel<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax>(Flash_fwd_params) [with Kernel_traits=Flash_fwd_kernel_traits<256, 64, 64, 4, false, false, cutlass::bfloat16_t, Flash_kernel_traits<256, 64, 64, 4, cutlass::bfloat16_t>>, Is_dropout=true, Is_causal=true, Is_even_N=false, Is_even_K=true, Return_softmax=false]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(34): here
            instantiation of "void run_flash_fwd<Kernel_traits,Is_dropout,Is_causal>(Flash_fwd_params &, cudaStream_t) [with Kernel_traits=Flash_fwd_kernel_traits<256, 64, 64, 4, false, false, cutlass::bfloat16_t, Flash_kernel_traits<256, 64, 64, 4, cutlass::bfloat16_t>>, Is_dropout=true, Is_causal=true]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(236): here
            instantiation of "void run_mha_fwd_hdim256<T>(Flash_fwd_params &, cudaStream_t) [with T=cutlass::bfloat16_t]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_hdim256_bf16_sm80.cu(8): here

/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_kernel.h(276): warning: variable "tQrQ" was declared but never referenced
          detected during:
            instantiation of "void flash::compute_attn_1rowblock<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params,Is_Q_split_K>(const Params &, int, int, int) [with Kernel_traits=Flash_fwd_kernel_traits<256, 64, 64, 4, false, false, cutlass::bfloat16_t, Flash_kernel_traits<256, 64, 64, 4, cutlass::bfloat16_t>>, Is_dropout=true, Is_causal=true, Is_even_N=false, Is_even_K=true, Return_softmax=false, Params=Flash_fwd_params, Is_Q_split_K=false]" 
(589): here
            instantiation of "void flash::compute_attn<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &) [with Kernel_traits=Flash_fwd_kernel_traits<256, 64, 64, 4, false, false, cutlass::bfloat16_t, Flash_kernel_traits<256, 64, 64, 4, cutlass::bfloat16_t>>, Is_dropout=true, Is_causal=true, Is_even_N=false, Is_even_K=true, Return_softmax=false, Params=Flash_fwd_params]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(15): here
            instantiation of "void flash_fwd_kernel<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax>(Flash_fwd_params) [with Kernel_traits=Flash_fwd_kernel_traits<256, 64, 64, 4, false, false, cutlass::bfloat16_t, Flash_kernel_traits<256, 64, 64, 4, cutlass::bfloat16_t>>, Is_dropout=true, Is_causal=true, Is_even_N=false, Is_even_K=true, Return_softmax=false]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(34): here
            instantiation of "void run_flash_fwd<Kernel_traits,Is_dropout,Is_causal>(Flash_fwd_params &, cudaStream_t) [with Kernel_traits=Flash_fwd_kernel_traits<256, 64, 64, 4, false, false, cutlass::bfloat16_t, Flash_kernel_traits<256, 64, 64, 4, cutlass::bfloat16_t>>, Is_dropout=true, Is_causal=true]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(236): here
            instantiation of "void run_mha_fwd_hdim256<T>(Flash_fwd_params &, cudaStream_t) [with T=cutlass::bfloat16_t]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_hdim256_bf16_sm80.cu(8): here

/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_kernel.h(140): warning: variable "MMA_M" was declared but never referenced
          detected during:
            instantiation of "void flash::compute_attn_1rowblock<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params,Is_Q_split_K>(const Params &, int, int, int) [with Kernel_traits=Flash_fwd_kernel_traits<256, 64, 64, 4, false, false, cutlass::bfloat16_t, Flash_kernel_traits<256, 64, 64, 4, cutlass::bfloat16_t>>, Is_dropout=true, Is_causal=true, Is_even_N=false, Is_even_K=false, Return_softmax=true, Params=Flash_fwd_params, Is_Q_split_K=false]" 
(589): here
            instantiation of "void flash::compute_attn<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &) [with Kernel_traits=Flash_fwd_kernel_traits<256, 64, 64, 4, false, false, cutlass::bfloat16_t, Flash_kernel_traits<256, 64, 64, 4, cutlass::bfloat16_t>>, Is_dropout=true, Is_causal=true, Is_even_N=false, Is_even_K=false, Return_softmax=true, Params=Flash_fwd_params]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(15): here
            instantiation of "void flash_fwd_kernel<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax>(Flash_fwd_params) [with Kernel_traits=Flash_fwd_kernel_traits<256, 64, 64, 4, false, false, cutlass::bfloat16_t, Flash_kernel_traits<256, 64, 64, 4, cutlass::bfloat16_t>>, Is_dropout=true, Is_causal=true, Is_even_N=false, Is_even_K=false, Return_softmax=true]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(34): here
            instantiation of "void run_flash_fwd<Kernel_traits,Is_dropout,Is_causal>(Flash_fwd_params &, cudaStream_t) [with Kernel_traits=Flash_fwd_kernel_traits<256, 64, 64, 4, false, false, cutlass::bfloat16_t, Flash_kernel_traits<256, 64, 64, 4, cutlass::bfloat16_t>>, Is_dropout=true, Is_causal=true]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(236): here
            instantiation of "void run_mha_fwd_hdim256<T>(Flash_fwd_params &, cudaStream_t) [with T=cutlass::bfloat16_t]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_hdim256_bf16_sm80.cu(8): here

/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_kernel.h(276): warning: variable "tQrQ" was declared but never referenced
          detected during:
            instantiation of "void flash::compute_attn_1rowblock<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params,Is_Q_split_K>(const Params &, int, int, int) [with Kernel_traits=Flash_fwd_kernel_traits<256, 64, 64, 4, false, false, cutlass::bfloat16_t, Flash_kernel_traits<256, 64, 64, 4, cutlass::bfloat16_t>>, Is_dropout=true, Is_causal=true, Is_even_N=false, Is_even_K=false, Return_softmax=true, Params=Flash_fwd_params, Is_Q_split_K=false]" 
(589): here
            instantiation of "void flash::compute_attn<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &) [with Kernel_traits=Flash_fwd_kernel_traits<256, 64, 64, 4, false, false, cutlass::bfloat16_t, Flash_kernel_traits<256, 64, 64, 4, cutlass::bfloat16_t>>, Is_dropout=true, Is_causal=true, Is_even_N=false, Is_even_K=false, Return_softmax=true, Params=Flash_fwd_params]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(15): here
            instantiation of "void flash_fwd_kernel<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax>(Flash_fwd_params) [with Kernel_traits=Flash_fwd_kernel_traits<256, 64, 64, 4, false, false, cutlass::bfloat16_t, Flash_kernel_traits<256, 64, 64, 4, cutlass::bfloat16_t>>, Is_dropout=true, Is_causal=true, Is_even_N=false, Is_even_K=false, Return_softmax=true]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(34): here
            instantiation of "void run_flash_fwd<Kernel_traits,Is_dropout,Is_causal>(Flash_fwd_params &, cudaStream_t) [with Kernel_traits=Flash_fwd_kernel_traits<256, 64, 64, 4, false, false, cutlass::bfloat16_t, Flash_kernel_traits<256, 64, 64, 4, cutlass::bfloat16_t>>, Is_dropout=true, Is_causal=true]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(236): here
            instantiation of "void run_mha_fwd_hdim256<T>(Flash_fwd_params &, cudaStream_t) [with T=cutlass::bfloat16_t]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_hdim256_bf16_sm80.cu(8): here

/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_kernel.h(140): warning: variable "MMA_M" was declared but never referenced
          detected during:
            instantiation of "void flash::compute_attn_1rowblock<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params,Is_Q_split_K>(const Params &, int, int, int) [with Kernel_traits=Flash_fwd_kernel_traits<256, 64, 64, 4, false, false, cutlass::bfloat16_t, Flash_kernel_traits<256, 64, 64, 4, cutlass::bfloat16_t>>, Is_dropout=true, Is_causal=true, Is_even_N=false, Is_even_K=false, Return_softmax=false, Params=Flash_fwd_params, Is_Q_split_K=false]" 
(589): here
            instantiation of "void flash::compute_attn<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &) [with Kernel_traits=Flash_fwd_kernel_traits<256, 64, 64, 4, false, false, cutlass::bfloat16_t, Flash_kernel_traits<256, 64, 64, 4, cutlass::bfloat16_t>>, Is_dropout=true, Is_causal=true, Is_even_N=false, Is_even_K=false, Return_softmax=false, Params=Flash_fwd_params]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(15): here
            instantiation of "void flash_fwd_kernel<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax>(Flash_fwd_params) [with Kernel_traits=Flash_fwd_kernel_traits<256, 64, 64, 4, false, false, cutlass::bfloat16_t, Flash_kernel_traits<256, 64, 64, 4, cutlass::bfloat16_t>>, Is_dropout=true, Is_causal=true, Is_even_N=false, Is_even_K=false, Return_softmax=false]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(34): here
            instantiation of "void run_flash_fwd<Kernel_traits,Is_dropout,Is_causal>(Flash_fwd_params &, cudaStream_t) [with Kernel_traits=Flash_fwd_kernel_traits<256, 64, 64, 4, false, false, cutlass::bfloat16_t, Flash_kernel_traits<256, 64, 64, 4, cutlass::bfloat16_t>>, Is_dropout=true, Is_causal=true]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(236): here
            instantiation of "void run_mha_fwd_hdim256<T>(Flash_fwd_params &, cudaStream_t) [with T=cutlass::bfloat16_t]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_hdim256_bf16_sm80.cu(8): here

/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_kernel.h(276): warning: variable "tQrQ" was declared but never referenced
          detected during:
            instantiation of "void flash::compute_attn_1rowblock<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params,Is_Q_split_K>(const Params &, int, int, int) [with Kernel_traits=Flash_fwd_kernel_traits<256, 64, 64, 4, false, false, cutlass::bfloat16_t, Flash_kernel_traits<256, 64, 64, 4, cutlass::bfloat16_t>>, Is_dropout=true, Is_causal=true, Is_even_N=false, Is_even_K=false, Return_softmax=false, Params=Flash_fwd_params, Is_Q_split_K=false]" 
(589): here
            instantiation of "void flash::compute_attn<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &) [with Kernel_traits=Flash_fwd_kernel_traits<256, 64, 64, 4, false, false, cutlass::bfloat16_t, Flash_kernel_traits<256, 64, 64, 4, cutlass::bfloat16_t>>, Is_dropout=true, Is_causal=true, Is_even_N=false, Is_even_K=false, Return_softmax=false, Params=Flash_fwd_params]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(15): here
            instantiation of "void flash_fwd_kernel<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax>(Flash_fwd_params) [with Kernel_traits=Flash_fwd_kernel_traits<256, 64, 64, 4, false, false, cutlass::bfloat16_t, Flash_kernel_traits<256, 64, 64, 4, cutlass::bfloat16_t>>, Is_dropout=true, Is_causal=true, Is_even_N=false, Is_even_K=false, Return_softmax=false]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(34): here
            instantiation of "void run_flash_fwd<Kernel_traits,Is_dropout,Is_causal>(Flash_fwd_params &, cudaStream_t) [with Kernel_traits=Flash_fwd_kernel_traits<256, 64, 64, 4, false, false, cutlass::bfloat16_t, Flash_kernel_traits<256, 64, 64, 4, cutlass::bfloat16_t>>, Is_dropout=true, Is_causal=true]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(236): here
            instantiation of "void run_mha_fwd_hdim256<T>(Flash_fwd_params &, cudaStream_t) [with T=cutlass::bfloat16_t]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_hdim256_bf16_sm80.cu(8): here

/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_kernel.h(140): warning: variable "MMA_M" was declared but never referenced
          detected during:
            instantiation of "void flash::compute_attn_1rowblock<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params,Is_Q_split_K>(const Params &, int, int, int) [with Kernel_traits=Flash_fwd_kernel_traits<256, 128, 64, 8, false, false, cutlass::bfloat16_t, Flash_kernel_traits<256, 128, 64, 8, cutlass::bfloat16_t>>, Is_dropout=true, Is_causal=false, Is_even_N=true, Is_even_K=true, Return_softmax=true, Params=Flash_fwd_params, Is_Q_split_K=false]" 
(589): here
            instantiation of "void flash::compute_attn<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &) [with Kernel_traits=Flash_fwd_kernel_traits<256, 128, 64, 8, false, false, cutlass::bfloat16_t, Flash_kernel_traits<256, 128, 64, 8, cutlass::bfloat16_t>>, Is_dropout=true, Is_causal=false, Is_even_N=true, Is_even_K=true, Return_softmax=true, Params=Flash_fwd_params]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(15): here
            instantiation of "void flash_fwd_kernel<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax>(Flash_fwd_params) [with Kernel_traits=Flash_fwd_kernel_traits<256, 128, 64, 8, false, false, cutlass::bfloat16_t, Flash_kernel_traits<256, 128, 64, 8, cutlass::bfloat16_t>>, Is_dropout=true, Is_causal=false, Is_even_N=true, Is_even_K=true, Return_softmax=true]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(34): here
            instantiation of "void run_flash_fwd<Kernel_traits,Is_dropout,Is_causal>(Flash_fwd_params &, cudaStream_t) [with Kernel_traits=Flash_fwd_kernel_traits<256, 128, 64, 8, false, false, cutlass::bfloat16_t, Flash_kernel_traits<256, 128, 64, 8, cutlass::bfloat16_t>>, Is_dropout=true, Is_causal=false]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(236): here
            instantiation of "void run_mha_fwd_hdim256<T>(Flash_fwd_params &, cudaStream_t) [with T=cutlass::bfloat16_t]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_hdim256_bf16_sm80.cu(8): here

/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_kernel.h(276): warning: variable "tQrQ" was declared but never referenced
          detected during:
            instantiation of "void flash::compute_attn_1rowblock<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params,Is_Q_split_K>(const Params &, int, int, int) [with Kernel_traits=Flash_fwd_kernel_traits<256, 128, 64, 8, false, false, cutlass::bfloat16_t, Flash_kernel_traits<256, 128, 64, 8, cutlass::bfloat16_t>>, Is_dropout=true, Is_causal=false, Is_even_N=true, Is_even_K=true, Return_softmax=true, Params=Flash_fwd_params, Is_Q_split_K=false]" 
(589): here
            instantiation of "void flash::compute_attn<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &) [with Kernel_traits=Flash_fwd_kernel_traits<256, 128, 64, 8, false, false, cutlass::bfloat16_t, Flash_kernel_traits<256, 128, 64, 8, cutlass::bfloat16_t>>, Is_dropout=true, Is_causal=false, Is_even_N=true, Is_even_K=true, Return_softmax=true, Params=Flash_fwd_params]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(15): here
            instantiation of "void flash_fwd_kernel<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax>(Flash_fwd_params) [with Kernel_traits=Flash_fwd_kernel_traits<256, 128, 64, 8, false, false, cutlass::bfloat16_t, Flash_kernel_traits<256, 128, 64, 8, cutlass::bfloat16_t>>, Is_dropout=true, Is_causal=false, Is_even_N=true, Is_even_K=true, Return_softmax=true]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(34): here
            instantiation of "void run_flash_fwd<Kernel_traits,Is_dropout,Is_causal>(Flash_fwd_params &, cudaStream_t) [with Kernel_traits=Flash_fwd_kernel_traits<256, 128, 64, 8, false, false, cutlass::bfloat16_t, Flash_kernel_traits<256, 128, 64, 8, cutlass::bfloat16_t>>, Is_dropout=true, Is_causal=false]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(236): here
            instantiation of "void run_mha_fwd_hdim256<T>(Flash_fwd_params &, cudaStream_t) [with T=cutlass::bfloat16_t]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_hdim256_bf16_sm80.cu(8): here

/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_kernel.h(140): warning: variable "MMA_M" was declared but never referenced
          detected during:
            instantiation of "void flash::compute_attn_1rowblock<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params,Is_Q_split_K>(const Params &, int, int, int) [with Kernel_traits=Flash_fwd_kernel_traits<256, 128, 64, 8, false, false, cutlass::bfloat16_t, Flash_kernel_traits<256, 128, 64, 8, cutlass::bfloat16_t>>, Is_dropout=true, Is_causal=false, Is_even_N=true, Is_even_K=true, Return_softmax=false, Params=Flash_fwd_params, Is_Q_split_K=false]" 
(589): here
            instantiation of "void flash::compute_attn<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &) [with Kernel_traits=Flash_fwd_kernel_traits<256, 128, 64, 8, false, false, cutlass::bfloat16_t, Flash_kernel_traits<256, 128, 64, 8, cutlass::bfloat16_t>>, Is_dropout=true, Is_causal=false, Is_even_N=true, Is_even_K=true, Return_softmax=false, Params=Flash_fwd_params]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(15): here
            instantiation of "void flash_fwd_kernel<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax>(Flash_fwd_params) [with Kernel_traits=Flash_fwd_kernel_traits<256, 128, 64, 8, false, false, cutlass::bfloat16_t, Flash_kernel_traits<256, 128, 64, 8, cutlass::bfloat16_t>>, Is_dropout=true, Is_causal=false, Is_even_N=true, Is_even_K=true, Return_softmax=false]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(34): here
            instantiation of "void run_flash_fwd<Kernel_traits,Is_dropout,Is_causal>(Flash_fwd_params &, cudaStream_t) [with Kernel_traits=Flash_fwd_kernel_traits<256, 128, 64, 8, false, false, cutlass::bfloat16_t, Flash_kernel_traits<256, 128, 64, 8, cutlass::bfloat16_t>>, Is_dropout=true, Is_causal=false]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(236): here
            instantiation of "void run_mha_fwd_hdim256<T>(Flash_fwd_params &, cudaStream_t) [with T=cutlass::bfloat16_t]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_hdim256_bf16_sm80.cu(8): here

/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_kernel.h(276): warning: variable "tQrQ" was declared but never referenced
          detected during:
            instantiation of "void flash::compute_attn_1rowblock<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params,Is_Q_split_K>(const Params &, int, int, int) [with Kernel_traits=Flash_fwd_kernel_traits<256, 128, 64, 8, false, false, cutlass::bfloat16_t, Flash_kernel_traits<256, 128, 64, 8, cutlass::bfloat16_t>>, Is_dropout=true, Is_causal=false, Is_even_N=true, Is_even_K=true, Return_softmax=false, Params=Flash_fwd_params, Is_Q_split_K=false]" 
(589): here
            instantiation of "void flash::compute_attn<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &) [with Kernel_traits=Flash_fwd_kernel_traits<256, 128, 64, 8, false, false, cutlass::bfloat16_t, Flash_kernel_traits<256, 128, 64, 8, cutlass::bfloat16_t>>, Is_dropout=true, Is_causal=false, Is_even_N=true, Is_even_K=true, Return_softmax=false, Params=Flash_fwd_params]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(15): here
            instantiation of "void flash_fwd_kernel<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax>(Flash_fwd_params) [with Kernel_traits=Flash_fwd_kernel_traits<256, 128, 64, 8, false, false, cutlass::bfloat16_t, Flash_kernel_traits<256, 128, 64, 8, cutlass::bfloat16_t>>, Is_dropout=true, Is_causal=false, Is_even_N=true, Is_even_K=true, Return_softmax=false]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(34): here
            instantiation of "void run_flash_fwd<Kernel_traits,Is_dropout,Is_causal>(Flash_fwd_params &, cudaStream_t) [with Kernel_traits=Flash_fwd_kernel_traits<256, 128, 64, 8, false, false, cutlass::bfloat16_t, Flash_kernel_traits<256, 128, 64, 8, cutlass::bfloat16_t>>, Is_dropout=true, Is_causal=false]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(236): here
            instantiation of "void run_mha_fwd_hdim256<T>(Flash_fwd_params &, cudaStream_t) [with T=cutlass::bfloat16_t]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_hdim256_bf16_sm80.cu(8): here

/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_kernel.h(140): warning: variable "MMA_M" was declared but never referenced
          detected during:
            instantiation of "void flash::compute_attn_1rowblock<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params,Is_Q_split_K>(const Params &, int, int, int) [with Kernel_traits=Flash_fwd_kernel_traits<256, 128, 64, 8, false, false, cutlass::bfloat16_t, Flash_kernel_traits<256, 128, 64, 8, cutlass::bfloat16_t>>, Is_dropout=true, Is_causal=false, Is_even_N=true, Is_even_K=false, Return_softmax=true, Params=Flash_fwd_params, Is_Q_split_K=false]" 
(589): here
            instantiation of "void flash::compute_attn<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &) [with Kernel_traits=Flash_fwd_kernel_traits<256, 128, 64, 8, false, false, cutlass::bfloat16_t, Flash_kernel_traits<256, 128, 64, 8, cutlass::bfloat16_t>>, Is_dropout=true, Is_causal=false, Is_even_N=true, Is_even_K=false, Return_softmax=true, Params=Flash_fwd_params]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(15): here
            instantiation of "void flash_fwd_kernel<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax>(Flash_fwd_params) [with Kernel_traits=Flash_fwd_kernel_traits<256, 128, 64, 8, false, false, cutlass::bfloat16_t, Flash_kernel_traits<256, 128, 64, 8, cutlass::bfloat16_t>>, Is_dropout=true, Is_causal=false, Is_even_N=true, Is_even_K=false, Return_softmax=true]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(34): here
            instantiation of "void run_flash_fwd<Kernel_traits,Is_dropout,Is_causal>(Flash_fwd_params &, cudaStream_t) [with Kernel_traits=Flash_fwd_kernel_traits<256, 128, 64, 8, false, false, cutlass::bfloat16_t, Flash_kernel_traits<256, 128, 64, 8, cutlass::bfloat16_t>>, Is_dropout=true, Is_causal=false]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(236): here
            instantiation of "void run_mha_fwd_hdim256<T>(Flash_fwd_params &, cudaStream_t) [with T=cutlass::bfloat16_t]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_hdim256_bf16_sm80.cu(8): here

/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_kernel.h(276): warning: variable "tQrQ" was declared but never referenced
          detected during:
            instantiation of "void flash::compute_attn_1rowblock<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params,Is_Q_split_K>(const Params &, int, int, int) [with Kernel_traits=Flash_fwd_kernel_traits<256, 128, 64, 8, false, false, cutlass::bfloat16_t, Flash_kernel_traits<256, 128, 64, 8, cutlass::bfloat16_t>>, Is_dropout=true, Is_causal=false, Is_even_N=true, Is_even_K=false, Return_softmax=true, Params=Flash_fwd_params, Is_Q_split_K=false]" 
(589): here
            instantiation of "void flash::compute_attn<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &) [with Kernel_traits=Flash_fwd_kernel_traits<256, 128, 64, 8, false, false, cutlass::bfloat16_t, Flash_kernel_traits<256, 128, 64, 8, cutlass::bfloat16_t>>, Is_dropout=true, Is_causal=false, Is_even_N=true, Is_even_K=false, Return_softmax=true, Params=Flash_fwd_params]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(15): here
            instantiation of "void flash_fwd_kernel<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax>(Flash_fwd_params) [with Kernel_traits=Flash_fwd_kernel_traits<256, 128, 64, 8, false, false, cutlass::bfloat16_t, Flash_kernel_traits<256, 128, 64, 8, cutlass::bfloat16_t>>, Is_dropout=true, Is_causal=false, Is_even_N=true, Is_even_K=false, Return_softmax=true]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(34): here
            instantiation of "void run_flash_fwd<Kernel_traits,Is_dropout,Is_causal>(Flash_fwd_params &, cudaStream_t) [with Kernel_traits=Flash_fwd_kernel_traits<256, 128, 64, 8, false, false, cutlass::bfloat16_t, Flash_kernel_traits<256, 128, 64, 8, cutlass::bfloat16_t>>, Is_dropout=true, Is_causal=false]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(236): here
            instantiation of "void run_mha_fwd_hdim256<T>(Flash_fwd_params &, cudaStream_t) [with T=cutlass::bfloat16_t]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_hdim256_bf16_sm80.cu(8): here

/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_kernel.h(140): warning: variable "MMA_M" was declared but never referenced
          detected during:
            instantiation of "void flash::compute_attn_1rowblock<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params,Is_Q_split_K>(const Params &, int, int, int) [with Kernel_traits=Flash_fwd_kernel_traits<256, 128, 64, 8, false, false, cutlass::bfloat16_t, Flash_kernel_traits<256, 128, 64, 8, cutlass::bfloat16_t>>, Is_dropout=true, Is_causal=false, Is_even_N=true, Is_even_K=false, Return_softmax=false, Params=Flash_fwd_params, Is_Q_split_K=false]" 
(589): here
            instantiation of "void flash::compute_attn<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &) [with Kernel_traits=Flash_fwd_kernel_traits<256, 128, 64, 8, false, false, cutlass::bfloat16_t, Flash_kernel_traits<256, 128, 64, 8, cutlass::bfloat16_t>>, Is_dropout=true, Is_causal=false, Is_even_N=true, Is_even_K=false, Return_softmax=false, Params=Flash_fwd_params]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(15): here
            instantiation of "void flash_fwd_kernel<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax>(Flash_fwd_params) [with Kernel_traits=Flash_fwd_kernel_traits<256, 128, 64, 8, false, false, cutlass::bfloat16_t, Flash_kernel_traits<256, 128, 64, 8, cutlass::bfloat16_t>>, Is_dropout=true, Is_causal=false, Is_even_N=true, Is_even_K=false, Return_softmax=false]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(34): here
            instantiation of "void run_flash_fwd<Kernel_traits,Is_dropout,Is_causal>(Flash_fwd_params &, cudaStream_t) [with Kernel_traits=Flash_fwd_kernel_traits<256, 128, 64, 8, false, false, cutlass::bfloat16_t, Flash_kernel_traits<256, 128, 64, 8, cutlass::bfloat16_t>>, Is_dropout=true, Is_causal=false]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(236): here
            instantiation of "void run_mha_fwd_hdim256<T>(Flash_fwd_params &, cudaStream_t) [with T=cutlass::bfloat16_t]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_hdim256_bf16_sm80.cu(8): here

/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_kernel.h(276): warning: variable "tQrQ" was declared but never referenced
          detected during:
            instantiation of "void flash::compute_attn_1rowblock<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params,Is_Q_split_K>(const Params &, int, int, int) [with Kernel_traits=Flash_fwd_kernel_traits<256, 128, 64, 8, false, false, cutlass::bfloat16_t, Flash_kernel_traits<256, 128, 64, 8, cutlass::bfloat16_t>>, Is_dropout=true, Is_causal=false, Is_even_N=true, Is_even_K=false, Return_softmax=false, Params=Flash_fwd_params, Is_Q_split_K=false]" 
(589): here
            instantiation of "void flash::compute_attn<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &) [with Kernel_traits=Flash_fwd_kernel_traits<256, 128, 64, 8, false, false, cutlass::bfloat16_t, Flash_kernel_traits<256, 128, 64, 8, cutlass::bfloat16_t>>, Is_dropout=true, Is_causal=false, Is_even_N=true, Is_even_K=false, Return_softmax=false, Params=Flash_fwd_params]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(15): here
            instantiation of "void flash_fwd_kernel<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax>(Flash_fwd_params) [with Kernel_traits=Flash_fwd_kernel_traits<256, 128, 64, 8, false, false, cutlass::bfloat16_t, Flash_kernel_traits<256, 128, 64, 8, cutlass::bfloat16_t>>, Is_dropout=true, Is_causal=false, Is_even_N=true, Is_even_K=false, Return_softmax=false]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(34): here
            instantiation of "void run_flash_fwd<Kernel_traits,Is_dropout,Is_causal>(Flash_fwd_params &, cudaStream_t) [with Kernel_traits=Flash_fwd_kernel_traits<256, 128, 64, 8, false, false, cutlass::bfloat16_t, Flash_kernel_traits<256, 128, 64, 8, cutlass::bfloat16_t>>, Is_dropout=true, Is_causal=false]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(236): here
            instantiation of "void run_mha_fwd_hdim256<T>(Flash_fwd_params &, cudaStream_t) [with T=cutlass::bfloat16_t]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_hdim256_bf16_sm80.cu(8): here

/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_kernel.h(140): warning: variable "MMA_M" was declared but never referenced
          detected during:
            instantiation of "void flash::compute_attn_1rowblock<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params,Is_Q_split_K>(const Params &, int, int, int) [with Kernel_traits=Flash_fwd_kernel_traits<256, 128, 64, 8, false, false, cutlass::bfloat16_t, Flash_kernel_traits<256, 128, 64, 8, cutlass::bfloat16_t>>, Is_dropout=true, Is_causal=false, Is_even_N=false, Is_even_K=true, Return_softmax=true, Params=Flash_fwd_params, Is_Q_split_K=false]" 
(589): here
            instantiation of "void flash::compute_attn<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &) [with Kernel_traits=Flash_fwd_kernel_traits<256, 128, 64, 8, false, false, cutlass::bfloat16_t, Flash_kernel_traits<256, 128, 64, 8, cutlass::bfloat16_t>>, Is_dropout=true, Is_causal=false, Is_even_N=false, Is_even_K=true, Return_softmax=true, Params=Flash_fwd_params]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(15): here
            instantiation of "void flash_fwd_kernel<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax>(Flash_fwd_params) [with Kernel_traits=Flash_fwd_kernel_traits<256, 128, 64, 8, false, false, cutlass::bfloat16_t, Flash_kernel_traits<256, 128, 64, 8, cutlass::bfloat16_t>>, Is_dropout=true, Is_causal=false, Is_even_N=false, Is_even_K=true, Return_softmax=true]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(34): here
            instantiation of "void run_flash_fwd<Kernel_traits,Is_dropout,Is_causal>(Flash_fwd_params &, cudaStream_t) [with Kernel_traits=Flash_fwd_kernel_traits<256, 128, 64, 8, false, false, cutlass::bfloat16_t, Flash_kernel_traits<256, 128, 64, 8, cutlass::bfloat16_t>>, Is_dropout=true, Is_causal=false]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(236): here
            instantiation of "void run_mha_fwd_hdim256<T>(Flash_fwd_params &, cudaStream_t) [with T=cutlass::bfloat16_t]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_hdim256_bf16_sm80.cu(8): here

/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_kernel.h(276): warning: variable "tQrQ" was declared but never referenced
          detected during:
            instantiation of "void flash::compute_attn_1rowblock<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params,Is_Q_split_K>(const Params &, int, int, int) [with Kernel_traits=Flash_fwd_kernel_traits<256, 128, 64, 8, false, false, cutlass::bfloat16_t, Flash_kernel_traits<256, 128, 64, 8, cutlass::bfloat16_t>>, Is_dropout=true, Is_causal=false, Is_even_N=false, Is_even_K=true, Return_softmax=true, Params=Flash_fwd_params, Is_Q_split_K=false]" 
(589): here
            instantiation of "void flash::compute_attn<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &) [with Kernel_traits=Flash_fwd_kernel_traits<256, 128, 64, 8, false, false, cutlass::bfloat16_t, Flash_kernel_traits<256, 128, 64, 8, cutlass::bfloat16_t>>, Is_dropout=true, Is_causal=false, Is_even_N=false, Is_even_K=true, Return_softmax=true, Params=Flash_fwd_params]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(15): here
            instantiation of "void flash_fwd_kernel<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax>(Flash_fwd_params) [with Kernel_traits=Flash_fwd_kernel_traits<256, 128, 64, 8, false, false, cutlass::bfloat16_t, Flash_kernel_traits<256, 128, 64, 8, cutlass::bfloat16_t>>, Is_dropout=true, Is_causal=false, Is_even_N=false, Is_even_K=true, Return_softmax=true]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(34): here
            instantiation of "void run_flash_fwd<Kernel_traits,Is_dropout,Is_causal>(Flash_fwd_params &, cudaStream_t) [with Kernel_traits=Flash_fwd_kernel_traits<256, 128, 64, 8, false, false, cutlass::bfloat16_t, Flash_kernel_traits<256, 128, 64, 8, cutlass::bfloat16_t>>, Is_dropout=true, Is_causal=false]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(236): here
            instantiation of "void run_mha_fwd_hdim256<T>(Flash_fwd_params &, cudaStream_t) [with T=cutlass::bfloat16_t]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_hdim256_bf16_sm80.cu(8): here

/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_kernel.h(140): warning: variable "MMA_M" was declared but never referenced
          detected during:
            instantiation of "void flash::compute_attn_1rowblock<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params,Is_Q_split_K>(const Params &, int, int, int) [with Kernel_traits=Flash_fwd_kernel_traits<256, 128, 64, 8, false, false, cutlass::bfloat16_t, Flash_kernel_traits<256, 128, 64, 8, cutlass::bfloat16_t>>, Is_dropout=true, Is_causal=false, Is_even_N=false, Is_even_K=true, Return_softmax=false, Params=Flash_fwd_params, Is_Q_split_K=false]" 
(589): here
            instantiation of "void flash::compute_attn<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &) [with Kernel_traits=Flash_fwd_kernel_traits<256, 128, 64, 8, false, false, cutlass::bfloat16_t, Flash_kernel_traits<256, 128, 64, 8, cutlass::bfloat16_t>>, Is_dropout=true, Is_causal=false, Is_even_N=false, Is_even_K=true, Return_softmax=false, Params=Flash_fwd_params]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(15): here
            instantiation of "void flash_fwd_kernel<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax>(Flash_fwd_params) [with Kernel_traits=Flash_fwd_kernel_traits<256, 128, 64, 8, false, false, cutlass::bfloat16_t, Flash_kernel_traits<256, 128, 64, 8, cutlass::bfloat16_t>>, Is_dropout=true, Is_causal=false, Is_even_N=false, Is_even_K=true, Return_softmax=false]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(34): here
            instantiation of "void run_flash_fwd<Kernel_traits,Is_dropout,Is_causal>(Flash_fwd_params &, cudaStream_t) [with Kernel_traits=Flash_fwd_kernel_traits<256, 128, 64, 8, false, false, cutlass::bfloat16_t, Flash_kernel_traits<256, 128, 64, 8, cutlass::bfloat16_t>>, Is_dropout=true, Is_causal=false]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(236): here
            instantiation of "void run_mha_fwd_hdim256<T>(Flash_fwd_params &, cudaStream_t) [with T=cutlass::bfloat16_t]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_hdim256_bf16_sm80.cu(8): here

/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_kernel.h(276): warning: variable "tQrQ" was declared but never referenced
          detected during:
            instantiation of "void flash::compute_attn_1rowblock<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params,Is_Q_split_K>(const Params &, int, int, int) [with Kernel_traits=Flash_fwd_kernel_traits<256, 128, 64, 8, false, false, cutlass::bfloat16_t, Flash_kernel_traits<256, 128, 64, 8, cutlass::bfloat16_t>>, Is_dropout=true, Is_causal=false, Is_even_N=false, Is_even_K=true, Return_softmax=false, Params=Flash_fwd_params, Is_Q_split_K=false]" 
(589): here
            instantiation of "void flash::compute_attn<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &) [with Kernel_traits=Flash_fwd_kernel_traits<256, 128, 64, 8, false, false, cutlass::bfloat16_t, Flash_kernel_traits<256, 128, 64, 8, cutlass::bfloat16_t>>, Is_dropout=true, Is_causal=false, Is_even_N=false, Is_even_K=true, Return_softmax=false, Params=Flash_fwd_params]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(15): here
            instantiation of "void flash_fwd_kernel<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax>(Flash_fwd_params) [with Kernel_traits=Flash_fwd_kernel_traits<256, 128, 64, 8, false, false, cutlass::bfloat16_t, Flash_kernel_traits<256, 128, 64, 8, cutlass::bfloat16_t>>, Is_dropout=true, Is_causal=false, Is_even_N=false, Is_even_K=true, Return_softmax=false]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(34): here
            instantiation of "void run_flash_fwd<Kernel_traits,Is_dropout,Is_causal>(Flash_fwd_params &, cudaStream_t) [with Kernel_traits=Flash_fwd_kernel_traits<256, 128, 64, 8, false, false, cutlass::bfloat16_t, Flash_kernel_traits<256, 128, 64, 8, cutlass::bfloat16_t>>, Is_dropout=true, Is_causal=false]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(236): here
            instantiation of "void run_mha_fwd_hdim256<T>(Flash_fwd_params &, cudaStream_t) [with T=cutlass::bfloat16_t]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_hdim256_bf16_sm80.cu(8): here

/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_kernel.h(140): warning: variable "MMA_M" was declared but never referenced
          detected during:
            instantiation of "void flash::compute_attn_1rowblock<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params,Is_Q_split_K>(const Params &, int, int, int) [with Kernel_traits=Flash_fwd_kernel_traits<256, 128, 64, 8, false, false, cutlass::bfloat16_t, Flash_kernel_traits<256, 128, 64, 8, cutlass::bfloat16_t>>, Is_dropout=true, Is_causal=false, Is_even_N=false, Is_even_K=false, Return_softmax=true, Params=Flash_fwd_params, Is_Q_split_K=false]" 
(589): here
            instantiation of "void flash::compute_attn<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &) [with Kernel_traits=Flash_fwd_kernel_traits<256, 128, 64, 8, false, false, cutlass::bfloat16_t, Flash_kernel_traits<256, 128, 64, 8, cutlass::bfloat16_t>>, Is_dropout=true, Is_causal=false, Is_even_N=false, Is_even_K=false, Return_softmax=true, Params=Flash_fwd_params]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(15): here
            instantiation of "void flash_fwd_kernel<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax>(Flash_fwd_params) [with Kernel_traits=Flash_fwd_kernel_traits<256, 128, 64, 8, false, false, cutlass::bfloat16_t, Flash_kernel_traits<256, 128, 64, 8, cutlass::bfloat16_t>>, Is_dropout=true, Is_causal=false, Is_even_N=false, Is_even_K=false, Return_softmax=true]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(34): here
            instantiation of "void run_flash_fwd<Kernel_traits,Is_dropout,Is_causal>(Flash_fwd_params &, cudaStream_t) [with Kernel_traits=Flash_fwd_kernel_traits<256, 128, 64, 8, false, false, cutlass::bfloat16_t, Flash_kernel_traits<256, 128, 64, 8, cutlass::bfloat16_t>>, Is_dropout=true, Is_causal=false]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(236): here
            instantiation of "void run_mha_fwd_hdim256<T>(Flash_fwd_params &, cudaStream_t) [with T=cutlass::bfloat16_t]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_hdim256_bf16_sm80.cu(8): here

/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_kernel.h(276): warning: variable "tQrQ" was declared but never referenced
          detected during:
            instantiation of "void flash::compute_attn_1rowblock<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params,Is_Q_split_K>(const Params &, int, int, int) [with Kernel_traits=Flash_fwd_kernel_traits<256, 128, 64, 8, false, false, cutlass::bfloat16_t, Flash_kernel_traits<256, 128, 64, 8, cutlass::bfloat16_t>>, Is_dropout=true, Is_causal=false, Is_even_N=false, Is_even_K=false, Return_softmax=true, Params=Flash_fwd_params, Is_Q_split_K=false]" 
(589): here
            instantiation of "void flash::compute_attn<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &) [with Kernel_traits=Flash_fwd_kernel_traits<256, 128, 64, 8, false, false, cutlass::bfloat16_t, Flash_kernel_traits<256, 128, 64, 8, cutlass::bfloat16_t>>, Is_dropout=true, Is_causal=false, Is_even_N=false, Is_even_K=false, Return_softmax=true, Params=Flash_fwd_params]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(15): here
            instantiation of "void flash_fwd_kernel<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax>(Flash_fwd_params) [with Kernel_traits=Flash_fwd_kernel_traits<256, 128, 64, 8, false, false, cutlass::bfloat16_t, Flash_kernel_traits<256, 128, 64, 8, cutlass::bfloat16_t>>, Is_dropout=true, Is_causal=false, Is_even_N=false, Is_even_K=false, Return_softmax=true]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(34): here
            instantiation of "void run_flash_fwd<Kernel_traits,Is_dropout,Is_causal>(Flash_fwd_params &, cudaStream_t) [with Kernel_traits=Flash_fwd_kernel_traits<256, 128, 64, 8, false, false, cutlass::bfloat16_t, Flash_kernel_traits<256, 128, 64, 8, cutlass::bfloat16_t>>, Is_dropout=true, Is_causal=false]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(236): here
            instantiation of "void run_mha_fwd_hdim256<T>(Flash_fwd_params &, cudaStream_t) [with T=cutlass::bfloat16_t]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_hdim256_bf16_sm80.cu(8): here

/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_kernel.h(140): warning: variable "MMA_M" was declared but never referenced
          detected during:
            instantiation of "void flash::compute_attn_1rowblock<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params,Is_Q_split_K>(const Params &, int, int, int) [with Kernel_traits=Flash_fwd_kernel_traits<256, 128, 64, 8, false, false, cutlass::bfloat16_t, Flash_kernel_traits<256, 128, 64, 8, cutlass::bfloat16_t>>, Is_dropout=true, Is_causal=false, Is_even_N=false, Is_even_K=false, Return_softmax=false, Params=Flash_fwd_params, Is_Q_split_K=false]" 
(589): here
            instantiation of "void flash::compute_attn<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &) [with Kernel_traits=Flash_fwd_kernel_traits<256, 128, 64, 8, false, false, cutlass::bfloat16_t, Flash_kernel_traits<256, 128, 64, 8, cutlass::bfloat16_t>>, Is_dropout=true, Is_causal=false, Is_even_N=false, Is_even_K=false, Return_softmax=false, Params=Flash_fwd_params]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(15): here
            instantiation of "void flash_fwd_kernel<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax>(Flash_fwd_params) [with Kernel_traits=Flash_fwd_kernel_traits<256, 128, 64, 8, false, false, cutlass::bfloat16_t, Flash_kernel_traits<256, 128, 64, 8, cutlass::bfloat16_t>>, Is_dropout=true, Is_causal=false, Is_even_N=false, Is_even_K=false, Return_softmax=false]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(34): here
            instantiation of "void run_flash_fwd<Kernel_traits,Is_dropout,Is_causal>(Flash_fwd_params &, cudaStream_t) [with Kernel_traits=Flash_fwd_kernel_traits<256, 128, 64, 8, false, false, cutlass::bfloat16_t, Flash_kernel_traits<256, 128, 64, 8, cutlass::bfloat16_t>>, Is_dropout=true, Is_causal=false]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(236): here
            instantiation of "void run_mha_fwd_hdim256<T>(Flash_fwd_params &, cudaStream_t) [with T=cutlass::bfloat16_t]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_hdim256_bf16_sm80.cu(8): here

/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_kernel.h(276): warning: variable "tQrQ" was declared but never referenced
          detected during:
            instantiation of "void flash::compute_attn_1rowblock<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params,Is_Q_split_K>(const Params &, int, int, int) [with Kernel_traits=Flash_fwd_kernel_traits<256, 128, 64, 8, false, false, cutlass::bfloat16_t, Flash_kernel_traits<256, 128, 64, 8, cutlass::bfloat16_t>>, Is_dropout=true, Is_causal=false, Is_even_N=false, Is_even_K=false, Return_softmax=false, Params=Flash_fwd_params, Is_Q_split_K=false]" 
(589): here
            instantiation of "void flash::compute_attn<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &) [with Kernel_traits=Flash_fwd_kernel_traits<256, 128, 64, 8, false, false, cutlass::bfloat16_t, Flash_kernel_traits<256, 128, 64, 8, cutlass::bfloat16_t>>, Is_dropout=true, Is_causal=false, Is_even_N=false, Is_even_K=false, Return_softmax=false, Params=Flash_fwd_params]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(15): here
            instantiation of "void flash_fwd_kernel<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax>(Flash_fwd_params) [with Kernel_traits=Flash_fwd_kernel_traits<256, 128, 64, 8, false, false, cutlass::bfloat16_t, Flash_kernel_traits<256, 128, 64, 8, cutlass::bfloat16_t>>, Is_dropout=true, Is_causal=false, Is_even_N=false, Is_even_K=false, Return_softmax=false]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(34): here
            instantiation of "void run_flash_fwd<Kernel_traits,Is_dropout,Is_causal>(Flash_fwd_params &, cudaStream_t) [with Kernel_traits=Flash_fwd_kernel_traits<256, 128, 64, 8, false, false, cutlass::bfloat16_t, Flash_kernel_traits<256, 128, 64, 8, cutlass::bfloat16_t>>, Is_dropout=true, Is_causal=false]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(236): here
            instantiation of "void run_mha_fwd_hdim256<T>(Flash_fwd_params &, cudaStream_t) [with T=cutlass::bfloat16_t]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_hdim256_bf16_sm80.cu(8): here

/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_kernel.h(140): warning: variable "MMA_M" was declared but never referenced
          detected during:
            instantiation of "void flash::compute_attn_1rowblock<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params,Is_Q_split_K>(const Params &, int, int, int) [with Kernel_traits=Flash_fwd_kernel_traits<256, 64, 64, 4, false, false, cutlass::bfloat16_t, Flash_kernel_traits<256, 64, 64, 4, cutlass::bfloat16_t>>, Is_dropout=true, Is_causal=false, Is_even_N=true, Is_even_K=true, Return_softmax=true, Params=Flash_fwd_params, Is_Q_split_K=false]" 
(589): here
            instantiation of "void flash::compute_attn<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &) [with Kernel_traits=Flash_fwd_kernel_traits<256, 64, 64, 4, false, false, cutlass::bfloat16_t, Flash_kernel_traits<256, 64, 64, 4, cutlass::bfloat16_t>>, Is_dropout=true, Is_causal=false, Is_even_N=true, Is_even_K=true, Return_softmax=true, Params=Flash_fwd_params]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(15): here
            instantiation of "void flash_fwd_kernel<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax>(Flash_fwd_params) [with Kernel_traits=Flash_fwd_kernel_traits<256, 64, 64, 4, false, false, cutlass::bfloat16_t, Flash_kernel_traits<256, 64, 64, 4, cutlass::bfloat16_t>>, Is_dropout=true, Is_causal=false, Is_even_N=true, Is_even_K=true, Return_softmax=true]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(34): here
            instantiation of "void run_flash_fwd<Kernel_traits,Is_dropout,Is_causal>(Flash_fwd_params &, cudaStream_t) [with Kernel_traits=Flash_fwd_kernel_traits<256, 64, 64, 4, false, false, cutlass::bfloat16_t, Flash_kernel_traits<256, 64, 64, 4, cutlass::bfloat16_t>>, Is_dropout=true, Is_causal=false]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(236): here
            instantiation of "void run_mha_fwd_hdim256<T>(Flash_fwd_params &, cudaStream_t) [with T=cutlass::bfloat16_t]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_hdim256_bf16_sm80.cu(8): here

/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_kernel.h(276): warning: variable "tQrQ" was declared but never referenced
          detected during:
            instantiation of "void flash::compute_attn_1rowblock<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params,Is_Q_split_K>(const Params &, int, int, int) [with Kernel_traits=Flash_fwd_kernel_traits<256, 64, 64, 4, false, false, cutlass::bfloat16_t, Flash_kernel_traits<256, 64, 64, 4, cutlass::bfloat16_t>>, Is_dropout=true, Is_causal=false, Is_even_N=true, Is_even_K=true, Return_softmax=true, Params=Flash_fwd_params, Is_Q_split_K=false]" 
(589): here
            instantiation of "void flash::compute_attn<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &) [with Kernel_traits=Flash_fwd_kernel_traits<256, 64, 64, 4, false, false, cutlass::bfloat16_t, Flash_kernel_traits<256, 64, 64, 4, cutlass::bfloat16_t>>, Is_dropout=true, Is_causal=false, Is_even_N=true, Is_even_K=true, Return_softmax=true, Params=Flash_fwd_params]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(15): here
            instantiation of "void flash_fwd_kernel<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax>(Flash_fwd_params) [with Kernel_traits=Flash_fwd_kernel_traits<256, 64, 64, 4, false, false, cutlass::bfloat16_t, Flash_kernel_traits<256, 64, 64, 4, cutlass::bfloat16_t>>, Is_dropout=true, Is_causal=false, Is_even_N=true, Is_even_K=true, Return_softmax=true]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(34): here
            instantiation of "void run_flash_fwd<Kernel_traits,Is_dropout,Is_causal>(Flash_fwd_params &, cudaStream_t) [with Kernel_traits=Flash_fwd_kernel_traits<256, 64, 64, 4, false, false, cutlass::bfloat16_t, Flash_kernel_traits<256, 64, 64, 4, cutlass::bfloat16_t>>, Is_dropout=true, Is_causal=false]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(236): here
            instantiation of "void run_mha_fwd_hdim256<T>(Flash_fwd_params &, cudaStream_t) [with T=cutlass::bfloat16_t]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_hdim256_bf16_sm80.cu(8): here

/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_kernel.h(140): warning: variable "MMA_M" was declared but never referenced
          detected during:
            instantiation of "void flash::compute_attn_1rowblock<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params,Is_Q_split_K>(const Params &, int, int, int) [with Kernel_traits=Flash_fwd_kernel_traits<256, 64, 64, 4, false, false, cutlass::bfloat16_t, Flash_kernel_traits<256, 64, 64, 4, cutlass::bfloat16_t>>, Is_dropout=true, Is_causal=false, Is_even_N=true, Is_even_K=true, Return_softmax=false, Params=Flash_fwd_params, Is_Q_split_K=false]" 
(589): here
            instantiation of "void flash::compute_attn<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &) [with Kernel_traits=Flash_fwd_kernel_traits<256, 64, 64, 4, false, false, cutlass::bfloat16_t, Flash_kernel_traits<256, 64, 64, 4, cutlass::bfloat16_t>>, Is_dropout=true, Is_causal=false, Is_even_N=true, Is_even_K=true, Return_softmax=false, Params=Flash_fwd_params]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(15): here
            instantiation of "void flash_fwd_kernel<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax>(Flash_fwd_params) [with Kernel_traits=Flash_fwd_kernel_traits<256, 64, 64, 4, false, false, cutlass::bfloat16_t, Flash_kernel_traits<256, 64, 64, 4, cutlass::bfloat16_t>>, Is_dropout=true, Is_causal=false, Is_even_N=true, Is_even_K=true, Return_softmax=false]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(34): here
            instantiation of "void run_flash_fwd<Kernel_traits,Is_dropout,Is_causal>(Flash_fwd_params &, cudaStream_t) [with Kernel_traits=Flash_fwd_kernel_traits<256, 64, 64, 4, false, false, cutlass::bfloat16_t, Flash_kernel_traits<256, 64, 64, 4, cutlass::bfloat16_t>>, Is_dropout=true, Is_causal=false]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(236): here
            instantiation of "void run_mha_fwd_hdim256<T>(Flash_fwd_params &, cudaStream_t) [with T=cutlass::bfloat16_t]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_hdim256_bf16_sm80.cu(8): here

/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_kernel.h(276): warning: variable "tQrQ" was declared but never referenced
          detected during:
            instantiation of "void flash::compute_attn_1rowblock<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params,Is_Q_split_K>(const Params &, int, int, int) [with Kernel_traits=Flash_fwd_kernel_traits<256, 64, 64, 4, false, false, cutlass::bfloat16_t, Flash_kernel_traits<256, 64, 64, 4, cutlass::bfloat16_t>>, Is_dropout=true, Is_causal=false, Is_even_N=true, Is_even_K=true, Return_softmax=false, Params=Flash_fwd_params, Is_Q_split_K=false]" 
(589): here
            instantiation of "void flash::compute_attn<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &) [with Kernel_traits=Flash_fwd_kernel_traits<256, 64, 64, 4, false, false, cutlass::bfloat16_t, Flash_kernel_traits<256, 64, 64, 4, cutlass::bfloat16_t>>, Is_dropout=true, Is_causal=false, Is_even_N=true, Is_even_K=true, Return_softmax=false, Params=Flash_fwd_params]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(15): here
            instantiation of "void flash_fwd_kernel<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax>(Flash_fwd_params) [with Kernel_traits=Flash_fwd_kernel_traits<256, 64, 64, 4, false, false, cutlass::bfloat16_t, Flash_kernel_traits<256, 64, 64, 4, cutlass::bfloat16_t>>, Is_dropout=true, Is_causal=false, Is_even_N=true, Is_even_K=true, Return_softmax=false]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(34): here
            instantiation of "void run_flash_fwd<Kernel_traits,Is_dropout,Is_causal>(Flash_fwd_params &, cudaStream_t) [with Kernel_traits=Flash_fwd_kernel_traits<256, 64, 64, 4, false, false, cutlass::bfloat16_t, Flash_kernel_traits<256, 64, 64, 4, cutlass::bfloat16_t>>, Is_dropout=true, Is_causal=false]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(236): here
            instantiation of "void run_mha_fwd_hdim256<T>(Flash_fwd_params &, cudaStream_t) [with T=cutlass::bfloat16_t]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_hdim256_bf16_sm80.cu(8): here

/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_kernel.h(140): warning: variable "MMA_M" was declared but never referenced
          detected during:
            instantiation of "void flash::compute_attn_1rowblock<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params,Is_Q_split_K>(const Params &, int, int, int) [with Kernel_traits=Flash_fwd_kernel_traits<256, 64, 64, 4, false, false, cutlass::bfloat16_t, Flash_kernel_traits<256, 64, 64, 4, cutlass::bfloat16_t>>, Is_dropout=true, Is_causal=false, Is_even_N=true, Is_even_K=false, Return_softmax=true, Params=Flash_fwd_params, Is_Q_split_K=false]" 
(589): here
            instantiation of "void flash::compute_attn<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &) [with Kernel_traits=Flash_fwd_kernel_traits<256, 64, 64, 4, false, false, cutlass::bfloat16_t, Flash_kernel_traits<256, 64, 64, 4, cutlass::bfloat16_t>>, Is_dropout=true, Is_causal=false, Is_even_N=true, Is_even_K=false, Return_softmax=true, Params=Flash_fwd_params]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(15): here
            instantiation of "void flash_fwd_kernel<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax>(Flash_fwd_params) [with Kernel_traits=Flash_fwd_kernel_traits<256, 64, 64, 4, false, false, cutlass::bfloat16_t, Flash_kernel_traits<256, 64, 64, 4, cutlass::bfloat16_t>>, Is_dropout=true, Is_causal=false, Is_even_N=true, Is_even_K=false, Return_softmax=true]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(34): here
            instantiation of "void run_flash_fwd<Kernel_traits,Is_dropout,Is_causal>(Flash_fwd_params &, cudaStream_t) [with Kernel_traits=Flash_fwd_kernel_traits<256, 64, 64, 4, false, false, cutlass::bfloat16_t, Flash_kernel_traits<256, 64, 64, 4, cutlass::bfloat16_t>>, Is_dropout=true, Is_causal=false]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(236): here
            instantiation of "void run_mha_fwd_hdim256<T>(Flash_fwd_params &, cudaStream_t) [with T=cutlass::bfloat16_t]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_hdim256_bf16_sm80.cu(8): here

/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_kernel.h(276): warning: variable "tQrQ" was declared but never referenced
          detected during:
            instantiation of "void flash::compute_attn_1rowblock<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params,Is_Q_split_K>(const Params &, int, int, int) [with Kernel_traits=Flash_fwd_kernel_traits<256, 64, 64, 4, false, false, cutlass::bfloat16_t, Flash_kernel_traits<256, 64, 64, 4, cutlass::bfloat16_t>>, Is_dropout=true, Is_causal=false, Is_even_N=true, Is_even_K=false, Return_softmax=true, Params=Flash_fwd_params, Is_Q_split_K=false]" 
(589): here
            instantiation of "void flash::compute_attn<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &) [with Kernel_traits=Flash_fwd_kernel_traits<256, 64, 64, 4, false, false, cutlass::bfloat16_t, Flash_kernel_traits<256, 64, 64, 4, cutlass::bfloat16_t>>, Is_dropout=true, Is_causal=false, Is_even_N=true, Is_even_K=false, Return_softmax=true, Params=Flash_fwd_params]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(15): here
            instantiation of "void flash_fwd_kernel<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax>(Flash_fwd_params) [with Kernel_traits=Flash_fwd_kernel_traits<256, 64, 64, 4, false, false, cutlass::bfloat16_t, Flash_kernel_traits<256, 64, 64, 4, cutlass::bfloat16_t>>, Is_dropout=true, Is_causal=false, Is_even_N=true, Is_even_K=false, Return_softmax=true]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(34): here
            instantiation of "void run_flash_fwd<Kernel_traits,Is_dropout,Is_causal>(Flash_fwd_params &, cudaStream_t) [with Kernel_traits=Flash_fwd_kernel_traits<256, 64, 64, 4, false, false, cutlass::bfloat16_t, Flash_kernel_traits<256, 64, 64, 4, cutlass::bfloat16_t>>, Is_dropout=true, Is_causal=false]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(236): here
            instantiation of "void run_mha_fwd_hdim256<T>(Flash_fwd_params &, cudaStream_t) [with T=cutlass::bfloat16_t]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_hdim256_bf16_sm80.cu(8): here

/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_kernel.h(140): warning: variable "MMA_M" was declared but never referenced
          detected during:
            instantiation of "void flash::compute_attn_1rowblock<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params,Is_Q_split_K>(const Params &, int, int, int) [with Kernel_traits=Flash_fwd_kernel_traits<256, 64, 64, 4, false, false, cutlass::bfloat16_t, Flash_kernel_traits<256, 64, 64, 4, cutlass::bfloat16_t>>, Is_dropout=true, Is_causal=false, Is_even_N=true, Is_even_K=false, Return_softmax=false, Params=Flash_fwd_params, Is_Q_split_K=false]" 
(589): here
            instantiation of "void flash::compute_attn<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &) [with Kernel_traits=Flash_fwd_kernel_traits<256, 64, 64, 4, false, false, cutlass::bfloat16_t, Flash_kernel_traits<256, 64, 64, 4, cutlass::bfloat16_t>>, Is_dropout=true, Is_causal=false, Is_even_N=true, Is_even_K=false, Return_softmax=false, Params=Flash_fwd_params]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(15): here
            instantiation of "void flash_fwd_kernel<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax>(Flash_fwd_params) [with Kernel_traits=Flash_fwd_kernel_traits<256, 64, 64, 4, false, false, cutlass::bfloat16_t, Flash_kernel_traits<256, 64, 64, 4, cutlass::bfloat16_t>>, Is_dropout=true, Is_causal=false, Is_even_N=true, Is_even_K=false, Return_softmax=false]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(34): here
            instantiation of "void run_flash_fwd<Kernel_traits,Is_dropout,Is_causal>(Flash_fwd_params &, cudaStream_t) [with Kernel_traits=Flash_fwd_kernel_traits<256, 64, 64, 4, false, false, cutlass::bfloat16_t, Flash_kernel_traits<256, 64, 64, 4, cutlass::bfloat16_t>>, Is_dropout=true, Is_causal=false]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(236): here
            instantiation of "void run_mha_fwd_hdim256<T>(Flash_fwd_params &, cudaStream_t) [with T=cutlass::bfloat16_t]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_hdim256_bf16_sm80.cu(8): here

/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_kernel.h(276): warning: variable "tQrQ" was declared but never referenced
          detected during:
            instantiation of "void flash::compute_attn_1rowblock<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params,Is_Q_split_K>(const Params &, int, int, int) [with Kernel_traits=Flash_fwd_kernel_traits<256, 64, 64, 4, false, false, cutlass::bfloat16_t, Flash_kernel_traits<256, 64, 64, 4, cutlass::bfloat16_t>>, Is_dropout=true, Is_causal=false, Is_even_N=true, Is_even_K=false, Return_softmax=false, Params=Flash_fwd_params, Is_Q_split_K=false]" 
(589): here
            instantiation of "void flash::compute_attn<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &) [with Kernel_traits=Flash_fwd_kernel_traits<256, 64, 64, 4, false, false, cutlass::bfloat16_t, Flash_kernel_traits<256, 64, 64, 4, cutlass::bfloat16_t>>, Is_dropout=true, Is_causal=false, Is_even_N=true, Is_even_K=false, Return_softmax=false, Params=Flash_fwd_params]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(15): here
            instantiation of "void flash_fwd_kernel<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax>(Flash_fwd_params) [with Kernel_traits=Flash_fwd_kernel_traits<256, 64, 64, 4, false, false, cutlass::bfloat16_t, Flash_kernel_traits<256, 64, 64, 4, cutlass::bfloat16_t>>, Is_dropout=true, Is_causal=false, Is_even_N=true, Is_even_K=false, Return_softmax=false]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(34): here
            instantiation of "void run_flash_fwd<Kernel_traits,Is_dropout,Is_causal>(Flash_fwd_params &, cudaStream_t) [with Kernel_traits=Flash_fwd_kernel_traits<256, 64, 64, 4, false, false, cutlass::bfloat16_t, Flash_kernel_traits<256, 64, 64, 4, cutlass::bfloat16_t>>, Is_dropout=true, Is_causal=false]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(236): here
            instantiation of "void run_mha_fwd_hdim256<T>(Flash_fwd_params &, cudaStream_t) [with T=cutlass::bfloat16_t]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_hdim256_bf16_sm80.cu(8): here

/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_kernel.h(140): warning: variable "MMA_M" was declared but never referenced
          detected during:
            instantiation of "void flash::compute_attn_1rowblock<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params,Is_Q_split_K>(const Params &, int, int, int) [with Kernel_traits=Flash_fwd_kernel_traits<256, 64, 64, 4, false, false, cutlass::bfloat16_t, Flash_kernel_traits<256, 64, 64, 4, cutlass::bfloat16_t>>, Is_dropout=true, Is_causal=false, Is_even_N=false, Is_even_K=true, Return_softmax=true, Params=Flash_fwd_params, Is_Q_split_K=false]" 
(589): here
            instantiation of "void flash::compute_attn<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &) [with Kernel_traits=Flash_fwd_kernel_traits<256, 64, 64, 4, false, false, cutlass::bfloat16_t, Flash_kernel_traits<256, 64, 64, 4, cutlass::bfloat16_t>>, Is_dropout=true, Is_causal=false, Is_even_N=false, Is_even_K=true, Return_softmax=true, Params=Flash_fwd_params]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(15): here
            instantiation of "void flash_fwd_kernel<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax>(Flash_fwd_params) [with Kernel_traits=Flash_fwd_kernel_traits<256, 64, 64, 4, false, false, cutlass::bfloat16_t, Flash_kernel_traits<256, 64, 64, 4, cutlass::bfloat16_t>>, Is_dropout=true, Is_causal=false, Is_even_N=false, Is_even_K=true, Return_softmax=true]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(34): here
            instantiation of "void run_flash_fwd<Kernel_traits,Is_dropout,Is_causal>(Flash_fwd_params &, cudaStream_t) [with Kernel_traits=Flash_fwd_kernel_traits<256, 64, 64, 4, false, false, cutlass::bfloat16_t, Flash_kernel_traits<256, 64, 64, 4, cutlass::bfloat16_t>>, Is_dropout=true, Is_causal=false]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(236): here
            instantiation of "void run_mha_fwd_hdim256<T>(Flash_fwd_params &, cudaStream_t) [with T=cutlass::bfloat16_t]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_hdim256_bf16_sm80.cu(8): here

/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_kernel.h(276): warning: variable "tQrQ" was declared but never referenced
          detected during:
            instantiation of "void flash::compute_attn_1rowblock<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params,Is_Q_split_K>(const Params &, int, int, int) [with Kernel_traits=Flash_fwd_kernel_traits<256, 64, 64, 4, false, false, cutlass::bfloat16_t, Flash_kernel_traits<256, 64, 64, 4, cutlass::bfloat16_t>>, Is_dropout=true, Is_causal=false, Is_even_N=false, Is_even_K=true, Return_softmax=true, Params=Flash_fwd_params, Is_Q_split_K=false]" 
(589): here
            instantiation of "void flash::compute_attn<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &) [with Kernel_traits=Flash_fwd_kernel_traits<256, 64, 64, 4, false, false, cutlass::bfloat16_t, Flash_kernel_traits<256, 64, 64, 4, cutlass::bfloat16_t>>, Is_dropout=true, Is_causal=false, Is_even_N=false, Is_even_K=true, Return_softmax=true, Params=Flash_fwd_params]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(15): here
            instantiation of "void flash_fwd_kernel<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax>(Flash_fwd_params) [with Kernel_traits=Flash_fwd_kernel_traits<256, 64, 64, 4, false, false, cutlass::bfloat16_t, Flash_kernel_traits<256, 64, 64, 4, cutlass::bfloat16_t>>, Is_dropout=true, Is_causal=false, Is_even_N=false, Is_even_K=true, Return_softmax=true]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(34): here
            instantiation of "void run_flash_fwd<Kernel_traits,Is_dropout,Is_causal>(Flash_fwd_params &, cudaStream_t) [with Kernel_traits=Flash_fwd_kernel_traits<256, 64, 64, 4, false, false, cutlass::bfloat16_t, Flash_kernel_traits<256, 64, 64, 4, cutlass::bfloat16_t>>, Is_dropout=true, Is_causal=false]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(236): here
            instantiation of "void run_mha_fwd_hdim256<T>(Flash_fwd_params &, cudaStream_t) [with T=cutlass::bfloat16_t]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_hdim256_bf16_sm80.cu(8): here

/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_kernel.h(140): warning: variable "MMA_M" was declared but never referenced
          detected during:
            instantiation of "void flash::compute_attn_1rowblock<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params,Is_Q_split_K>(const Params &, int, int, int) [with Kernel_traits=Flash_fwd_kernel_traits<256, 64, 64, 4, false, false, cutlass::bfloat16_t, Flash_kernel_traits<256, 64, 64, 4, cutlass::bfloat16_t>>, Is_dropout=true, Is_causal=false, Is_even_N=false, Is_even_K=true, Return_softmax=false, Params=Flash_fwd_params, Is_Q_split_K=false]" 
(589): here
            instantiation of "void flash::compute_attn<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &) [with Kernel_traits=Flash_fwd_kernel_traits<256, 64, 64, 4, false, false, cutlass::bfloat16_t, Flash_kernel_traits<256, 64, 64, 4, cutlass::bfloat16_t>>, Is_dropout=true, Is_causal=false, Is_even_N=false, Is_even_K=true, Return_softmax=false, Params=Flash_fwd_params]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(15): here
            instantiation of "void flash_fwd_kernel<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax>(Flash_fwd_params) [with Kernel_traits=Flash_fwd_kernel_traits<256, 64, 64, 4, false, false, cutlass::bfloat16_t, Flash_kernel_traits<256, 64, 64, 4, cutlass::bfloat16_t>>, Is_dropout=true, Is_causal=false, Is_even_N=false, Is_even_K=true, Return_softmax=false]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(34): here
            instantiation of "void run_flash_fwd<Kernel_traits,Is_dropout,Is_causal>(Flash_fwd_params &, cudaStream_t) [with Kernel_traits=Flash_fwd_kernel_traits<256, 64, 64, 4, false, false, cutlass::bfloat16_t, Flash_kernel_traits<256, 64, 64, 4, cutlass::bfloat16_t>>, Is_dropout=true, Is_causal=false]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(236): here
            instantiation of "void run_mha_fwd_hdim256<T>(Flash_fwd_params &, cudaStream_t) [with T=cutlass::bfloat16_t]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_hdim256_bf16_sm80.cu(8): here

/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_kernel.h(276): warning: variable "tQrQ" was declared but never referenced
          detected during:
            instantiation of "void flash::compute_attn_1rowblock<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params,Is_Q_split_K>(const Params &, int, int, int) [with Kernel_traits=Flash_fwd_kernel_traits<256, 64, 64, 4, false, false, cutlass::bfloat16_t, Flash_kernel_traits<256, 64, 64, 4, cutlass::bfloat16_t>>, Is_dropout=true, Is_causal=false, Is_even_N=false, Is_even_K=true, Return_softmax=false, Params=Flash_fwd_params, Is_Q_split_K=false]" 
(589): here
            instantiation of "void flash::compute_attn<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &) [with Kernel_traits=Flash_fwd_kernel_traits<256, 64, 64, 4, false, false, cutlass::bfloat16_t, Flash_kernel_traits<256, 64, 64, 4, cutlass::bfloat16_t>>, Is_dropout=true, Is_causal=false, Is_even_N=false, Is_even_K=true, Return_softmax=false, Params=Flash_fwd_params]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(15): here
            instantiation of "void flash_fwd_kernel<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax>(Flash_fwd_params) [with Kernel_traits=Flash_fwd_kernel_traits<256, 64, 64, 4, false, false, cutlass::bfloat16_t, Flash_kernel_traits<256, 64, 64, 4, cutlass::bfloat16_t>>, Is_dropout=true, Is_causal=false, Is_even_N=false, Is_even_K=true, Return_softmax=false]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(34): here
            instantiation of "void run_flash_fwd<Kernel_traits,Is_dropout,Is_causal>(Flash_fwd_params &, cudaStream_t) [with Kernel_traits=Flash_fwd_kernel_traits<256, 64, 64, 4, false, false, cutlass::bfloat16_t, Flash_kernel_traits<256, 64, 64, 4, cutlass::bfloat16_t>>, Is_dropout=true, Is_causal=false]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(236): here
            instantiation of "void run_mha_fwd_hdim256<T>(Flash_fwd_params &, cudaStream_t) [with T=cutlass::bfloat16_t]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_hdim256_bf16_sm80.cu(8): here

/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_kernel.h(140): warning: variable "MMA_M" was declared but never referenced
          detected during:
            instantiation of "void flash::compute_attn_1rowblock<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params,Is_Q_split_K>(const Params &, int, int, int) [with Kernel_traits=Flash_fwd_kernel_traits<256, 64, 64, 4, false, false, cutlass::bfloat16_t, Flash_kernel_traits<256, 64, 64, 4, cutlass::bfloat16_t>>, Is_dropout=true, Is_causal=false, Is_even_N=false, Is_even_K=false, Return_softmax=true, Params=Flash_fwd_params, Is_Q_split_K=false]" 
(589): here
            instantiation of "void flash::compute_attn<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &) [with Kernel_traits=Flash_fwd_kernel_traits<256, 64, 64, 4, false, false, cutlass::bfloat16_t, Flash_kernel_traits<256, 64, 64, 4, cutlass::bfloat16_t>>, Is_dropout=true, Is_causal=false, Is_even_N=false, Is_even_K=false, Return_softmax=true, Params=Flash_fwd_params]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(15): here
            instantiation of "void flash_fwd_kernel<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax>(Flash_fwd_params) [with Kernel_traits=Flash_fwd_kernel_traits<256, 64, 64, 4, false, false, cutlass::bfloat16_t, Flash_kernel_traits<256, 64, 64, 4, cutlass::bfloat16_t>>, Is_dropout=true, Is_causal=false, Is_even_N=false, Is_even_K=false, Return_softmax=true]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(34): here
            instantiation of "void run_flash_fwd<Kernel_traits,Is_dropout,Is_causal>(Flash_fwd_params &, cudaStream_t) [with Kernel_traits=Flash_fwd_kernel_traits<256, 64, 64, 4, false, false, cutlass::bfloat16_t, Flash_kernel_traits<256, 64, 64, 4, cutlass::bfloat16_t>>, Is_dropout=true, Is_causal=false]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(236): here
            instantiation of "void run_mha_fwd_hdim256<T>(Flash_fwd_params &, cudaStream_t) [with T=cutlass::bfloat16_t]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_hdim256_bf16_sm80.cu(8): here

/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_kernel.h(276): warning: variable "tQrQ" was declared but never referenced
          detected during:
            instantiation of "void flash::compute_attn_1rowblock<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params,Is_Q_split_K>(const Params &, int, int, int) [with Kernel_traits=Flash_fwd_kernel_traits<256, 64, 64, 4, false, false, cutlass::bfloat16_t, Flash_kernel_traits<256, 64, 64, 4, cutlass::bfloat16_t>>, Is_dropout=true, Is_causal=false, Is_even_N=false, Is_even_K=false, Return_softmax=true, Params=Flash_fwd_params, Is_Q_split_K=false]" 
(589): here
            instantiation of "void flash::compute_attn<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &) [with Kernel_traits=Flash_fwd_kernel_traits<256, 64, 64, 4, false, false, cutlass::bfloat16_t, Flash_kernel_traits<256, 64, 64, 4, cutlass::bfloat16_t>>, Is_dropout=true, Is_causal=false, Is_even_N=false, Is_even_K=false, Return_softmax=true, Params=Flash_fwd_params]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(15): here
            instantiation of "void flash_fwd_kernel<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax>(Flash_fwd_params) [with Kernel_traits=Flash_fwd_kernel_traits<256, 64, 64, 4, false, false, cutlass::bfloat16_t, Flash_kernel_traits<256, 64, 64, 4, cutlass::bfloat16_t>>, Is_dropout=true, Is_causal=false, Is_even_N=false, Is_even_K=false, Return_softmax=true]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(34): here
            instantiation of "void run_flash_fwd<Kernel_traits,Is_dropout,Is_causal>(Flash_fwd_params &, cudaStream_t) [with Kernel_traits=Flash_fwd_kernel_traits<256, 64, 64, 4, false, false, cutlass::bfloat16_t, Flash_kernel_traits<256, 64, 64, 4, cutlass::bfloat16_t>>, Is_dropout=true, Is_causal=false]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(236): here
            instantiation of "void run_mha_fwd_hdim256<T>(Flash_fwd_params &, cudaStream_t) [with T=cutlass::bfloat16_t]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_hdim256_bf16_sm80.cu(8): here

/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_kernel.h(140): warning: variable "MMA_M" was declared but never referenced
          detected during:
            instantiation of "void flash::compute_attn_1rowblock<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params,Is_Q_split_K>(const Params &, int, int, int) [with Kernel_traits=Flash_fwd_kernel_traits<256, 64, 64, 4, false, false, cutlass::bfloat16_t, Flash_kernel_traits<256, 64, 64, 4, cutlass::bfloat16_t>>, Is_dropout=true, Is_causal=false, Is_even_N=false, Is_even_K=false, Return_softmax=false, Params=Flash_fwd_params, Is_Q_split_K=false]" 
(589): here
            instantiation of "void flash::compute_attn<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &) [with Kernel_traits=Flash_fwd_kernel_traits<256, 64, 64, 4, false, false, cutlass::bfloat16_t, Flash_kernel_traits<256, 64, 64, 4, cutlass::bfloat16_t>>, Is_dropout=true, Is_causal=false, Is_even_N=false, Is_even_K=false, Return_softmax=false, Params=Flash_fwd_params]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(15): here
            instantiation of "void flash_fwd_kernel<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax>(Flash_fwd_params) [with Kernel_traits=Flash_fwd_kernel_traits<256, 64, 64, 4, false, false, cutlass::bfloat16_t, Flash_kernel_traits<256, 64, 64, 4, cutlass::bfloat16_t>>, Is_dropout=true, Is_causal=false, Is_even_N=false, Is_even_K=false, Return_softmax=false]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(34): here
            instantiation of "void run_flash_fwd<Kernel_traits,Is_dropout,Is_causal>(Flash_fwd_params &, cudaStream_t) [with Kernel_traits=Flash_fwd_kernel_traits<256, 64, 64, 4, false, false, cutlass::bfloat16_t, Flash_kernel_traits<256, 64, 64, 4, cutlass::bfloat16_t>>, Is_dropout=true, Is_causal=false]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(236): here
            instantiation of "void run_mha_fwd_hdim256<T>(Flash_fwd_params &, cudaStream_t) [with T=cutlass::bfloat16_t]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_hdim256_bf16_sm80.cu(8): here

/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_kernel.h(276): warning: variable "tQrQ" was declared but never referenced
          detected during:
            instantiation of "void flash::compute_attn_1rowblock<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params,Is_Q_split_K>(const Params &, int, int, int) [with Kernel_traits=Flash_fwd_kernel_traits<256, 64, 64, 4, false, false, cutlass::bfloat16_t, Flash_kernel_traits<256, 64, 64, 4, cutlass::bfloat16_t>>, Is_dropout=true, Is_causal=false, Is_even_N=false, Is_even_K=false, Return_softmax=false, Params=Flash_fwd_params, Is_Q_split_K=false]" 
(589): here
            instantiation of "void flash::compute_attn<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &) [with Kernel_traits=Flash_fwd_kernel_traits<256, 64, 64, 4, false, false, cutlass::bfloat16_t, Flash_kernel_traits<256, 64, 64, 4, cutlass::bfloat16_t>>, Is_dropout=true, Is_causal=false, Is_even_N=false, Is_even_K=false, Return_softmax=false, Params=Flash_fwd_params]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(15): here
            instantiation of "void flash_fwd_kernel<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax>(Flash_fwd_params) [with Kernel_traits=Flash_fwd_kernel_traits<256, 64, 64, 4, false, false, cutlass::bfloat16_t, Flash_kernel_traits<256, 64, 64, 4, cutlass::bfloat16_t>>, Is_dropout=true, Is_causal=false, Is_even_N=false, Is_even_K=false, Return_softmax=false]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(34): here
            instantiation of "void run_flash_fwd<Kernel_traits,Is_dropout,Is_causal>(Flash_fwd_params &, cudaStream_t) [with Kernel_traits=Flash_fwd_kernel_traits<256, 64, 64, 4, false, false, cutlass::bfloat16_t, Flash_kernel_traits<256, 64, 64, 4, cutlass::bfloat16_t>>, Is_dropout=true, Is_causal=false]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(236): here
            instantiation of "void run_mha_fwd_hdim256<T>(Flash_fwd_params &, cudaStream_t) [with T=cutlass::bfloat16_t]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_hdim256_bf16_sm80.cu(8): here

/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_kernel.h(140): warning: variable "MMA_M" was declared but never referenced
          detected during:
            instantiation of "void flash::compute_attn_1rowblock<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params,Is_Q_split_K>(const Params &, int, int, int) [with Kernel_traits=Flash_fwd_kernel_traits<256, 128, 64, 8, false, false, cutlass::bfloat16_t, Flash_kernel_traits<256, 128, 64, 8, cutlass::bfloat16_t>>, Is_dropout=false, Is_causal=true, Is_even_N=true, Is_even_K=true, Return_softmax=false, Params=Flash_fwd_params, Is_Q_split_K=false]" 
(589): here
            instantiation of "void flash::compute_attn<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &) [with Kernel_traits=Flash_fwd_kernel_traits<256, 128, 64, 8, false, false, cutlass::bfloat16_t, Flash_kernel_traits<256, 128, 64, 8, cutlass::bfloat16_t>>, Is_dropout=false, Is_causal=true, Is_even_N=true, Is_even_K=true, Return_softmax=false, Params=Flash_fwd_params]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(15): here
            instantiation of "void flash_fwd_kernel<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax>(Flash_fwd_params) [with Kernel_traits=Flash_fwd_kernel_traits<256, 128, 64, 8, false, false, cutlass::bfloat16_t, Flash_kernel_traits<256, 128, 64, 8, cutlass::bfloat16_t>>, Is_dropout=false, Is_causal=true, Is_even_N=true, Is_even_K=true, Return_softmax=false]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(34): here
            instantiation of "void run_flash_fwd<Kernel_traits,Is_dropout,Is_causal>(Flash_fwd_params &, cudaStream_t) [with Kernel_traits=Flash_fwd_kernel_traits<256, 128, 64, 8, false, false, cutlass::bfloat16_t, Flash_kernel_traits<256, 128, 64, 8, cutlass::bfloat16_t>>, Is_dropout=false, Is_causal=true]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(236): here
            instantiation of "void run_mha_fwd_hdim256<T>(Flash_fwd_params &, cudaStream_t) [with T=cutlass::bfloat16_t]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_hdim256_bf16_sm80.cu(8): here

/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_kernel.h(276): warning: variable "tQrQ" was declared but never referenced
          detected during:
            instantiation of "void flash::compute_attn_1rowblock<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params,Is_Q_split_K>(const Params &, int, int, int) [with Kernel_traits=Flash_fwd_kernel_traits<256, 128, 64, 8, false, false, cutlass::bfloat16_t, Flash_kernel_traits<256, 128, 64, 8, cutlass::bfloat16_t>>, Is_dropout=false, Is_causal=true, Is_even_N=true, Is_even_K=true, Return_softmax=false, Params=Flash_fwd_params, Is_Q_split_K=false]" 
(589): here
            instantiation of "void flash::compute_attn<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &) [with Kernel_traits=Flash_fwd_kernel_traits<256, 128, 64, 8, false, false, cutlass::bfloat16_t, Flash_kernel_traits<256, 128, 64, 8, cutlass::bfloat16_t>>, Is_dropout=false, Is_causal=true, Is_even_N=true, Is_even_K=true, Return_softmax=false, Params=Flash_fwd_params]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(15): here
            instantiation of "void flash_fwd_kernel<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax>(Flash_fwd_params) [with Kernel_traits=Flash_fwd_kernel_traits<256, 128, 64, 8, false, false, cutlass::bfloat16_t, Flash_kernel_traits<256, 128, 64, 8, cutlass::bfloat16_t>>, Is_dropout=false, Is_causal=true, Is_even_N=true, Is_even_K=true, Return_softmax=false]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(34): here
            instantiation of "void run_flash_fwd<Kernel_traits,Is_dropout,Is_causal>(Flash_fwd_params &, cudaStream_t) [with Kernel_traits=Flash_fwd_kernel_traits<256, 128, 64, 8, false, false, cutlass::bfloat16_t, Flash_kernel_traits<256, 128, 64, 8, cutlass::bfloat16_t>>, Is_dropout=false, Is_causal=true]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(236): here
            instantiation of "void run_mha_fwd_hdim256<T>(Flash_fwd_params &, cudaStream_t) [with T=cutlass::bfloat16_t]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_hdim256_bf16_sm80.cu(8): here

/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_kernel.h(140): warning: variable "MMA_M" was declared but never referenced
          detected during:
            instantiation of "void flash::compute_attn_1rowblock<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params,Is_Q_split_K>(const Params &, int, int, int) [with Kernel_traits=Flash_fwd_kernel_traits<256, 128, 64, 8, false, false, cutlass::bfloat16_t, Flash_kernel_traits<256, 128, 64, 8, cutlass::bfloat16_t>>, Is_dropout=false, Is_causal=true, Is_even_N=true, Is_even_K=false, Return_softmax=false, Params=Flash_fwd_params, Is_Q_split_K=false]" 
(589): here
            instantiation of "void flash::compute_attn<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &) [with Kernel_traits=Flash_fwd_kernel_traits<256, 128, 64, 8, false, false, cutlass::bfloat16_t, Flash_kernel_traits<256, 128, 64, 8, cutlass::bfloat16_t>>, Is_dropout=false, Is_causal=true, Is_even_N=true, Is_even_K=false, Return_softmax=false, Params=Flash_fwd_params]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(15): here
            instantiation of "void flash_fwd_kernel<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax>(Flash_fwd_params) [with Kernel_traits=Flash_fwd_kernel_traits<256, 128, 64, 8, false, false, cutlass::bfloat16_t, Flash_kernel_traits<256, 128, 64, 8, cutlass::bfloat16_t>>, Is_dropout=false, Is_causal=true, Is_even_N=true, Is_even_K=false, Return_softmax=false]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(34): here
            instantiation of "void run_flash_fwd<Kernel_traits,Is_dropout,Is_causal>(Flash_fwd_params &, cudaStream_t) [with Kernel_traits=Flash_fwd_kernel_traits<256, 128, 64, 8, false, false, cutlass::bfloat16_t, Flash_kernel_traits<256, 128, 64, 8, cutlass::bfloat16_t>>, Is_dropout=false, Is_causal=true]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(236): here
            instantiation of "void run_mha_fwd_hdim256<T>(Flash_fwd_params &, cudaStream_t) [with T=cutlass::bfloat16_t]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_hdim256_bf16_sm80.cu(8): here

/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_kernel.h(276): warning: variable "tQrQ" was declared but never referenced
          detected during:
            instantiation of "void flash::compute_attn_1rowblock<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params,Is_Q_split_K>(const Params &, int, int, int) [with Kernel_traits=Flash_fwd_kernel_traits<256, 128, 64, 8, false, false, cutlass::bfloat16_t, Flash_kernel_traits<256, 128, 64, 8, cutlass::bfloat16_t>>, Is_dropout=false, Is_causal=true, Is_even_N=true, Is_even_K=false, Return_softmax=false, Params=Flash_fwd_params, Is_Q_split_K=false]" 
(589): here
            instantiation of "void flash::compute_attn<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &) [with Kernel_traits=Flash_fwd_kernel_traits<256, 128, 64, 8, false, false, cutlass::bfloat16_t, Flash_kernel_traits<256, 128, 64, 8, cutlass::bfloat16_t>>, Is_dropout=false, Is_causal=true, Is_even_N=true, Is_even_K=false, Return_softmax=false, Params=Flash_fwd_params]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(15): here
            instantiation of "void flash_fwd_kernel<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax>(Flash_fwd_params) [with Kernel_traits=Flash_fwd_kernel_traits<256, 128, 64, 8, false, false, cutlass::bfloat16_t, Flash_kernel_traits<256, 128, 64, 8, cutlass::bfloat16_t>>, Is_dropout=false, Is_causal=true, Is_even_N=true, Is_even_K=false, Return_softmax=false]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(34): here
            instantiation of "void run_flash_fwd<Kernel_traits,Is_dropout,Is_causal>(Flash_fwd_params &, cudaStream_t) [with Kernel_traits=Flash_fwd_kernel_traits<256, 128, 64, 8, false, false, cutlass::bfloat16_t, Flash_kernel_traits<256, 128, 64, 8, cutlass::bfloat16_t>>, Is_dropout=false, Is_causal=true]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(236): here
            instantiation of "void run_mha_fwd_hdim256<T>(Flash_fwd_params &, cudaStream_t) [with T=cutlass::bfloat16_t]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_hdim256_bf16_sm80.cu(8): here

/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_kernel.h(140): warning: variable "MMA_M" was declared but never referenced
          detected during:
            instantiation of "void flash::compute_attn_1rowblock<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params,Is_Q_split_K>(const Params &, int, int, int) [with Kernel_traits=Flash_fwd_kernel_traits<256, 128, 64, 8, false, false, cutlass::bfloat16_t, Flash_kernel_traits<256, 128, 64, 8, cutlass::bfloat16_t>>, Is_dropout=false, Is_causal=true, Is_even_N=false, Is_even_K=true, Return_softmax=false, Params=Flash_fwd_params, Is_Q_split_K=false]" 
(589): here
            instantiation of "void flash::compute_attn<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &) [with Kernel_traits=Flash_fwd_kernel_traits<256, 128, 64, 8, false, false, cutlass::bfloat16_t, Flash_kernel_traits<256, 128, 64, 8, cutlass::bfloat16_t>>, Is_dropout=false, Is_causal=true, Is_even_N=false, Is_even_K=true, Return_softmax=false, Params=Flash_fwd_params]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(15): here
            instantiation of "void flash_fwd_kernel<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax>(Flash_fwd_params) [with Kernel_traits=Flash_fwd_kernel_traits<256, 128, 64, 8, false, false, cutlass::bfloat16_t, Flash_kernel_traits<256, 128, 64, 8, cutlass::bfloat16_t>>, Is_dropout=false, Is_causal=true, Is_even_N=false, Is_even_K=true, Return_softmax=false]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(34): here
            instantiation of "void run_flash_fwd<Kernel_traits,Is_dropout,Is_causal>(Flash_fwd_params &, cudaStream_t) [with Kernel_traits=Flash_fwd_kernel_traits<256, 128, 64, 8, false, false, cutlass::bfloat16_t, Flash_kernel_traits<256, 128, 64, 8, cutlass::bfloat16_t>>, Is_dropout=false, Is_causal=true]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(236): here
            instantiation of "void run_mha_fwd_hdim256<T>(Flash_fwd_params &, cudaStream_t) [with T=cutlass::bfloat16_t]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_hdim256_bf16_sm80.cu(8): here

/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_kernel.h(276): warning: variable "tQrQ" was declared but never referenced
          detected during:
            instantiation of "void flash::compute_attn_1rowblock<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params,Is_Q_split_K>(const Params &, int, int, int) [with Kernel_traits=Flash_fwd_kernel_traits<256, 128, 64, 8, false, false, cutlass::bfloat16_t, Flash_kernel_traits<256, 128, 64, 8, cutlass::bfloat16_t>>, Is_dropout=false, Is_causal=true, Is_even_N=false, Is_even_K=true, Return_softmax=false, Params=Flash_fwd_params, Is_Q_split_K=false]" 
(589): here
            instantiation of "void flash::compute_attn<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &) [with Kernel_traits=Flash_fwd_kernel_traits<256, 128, 64, 8, false, false, cutlass::bfloat16_t, Flash_kernel_traits<256, 128, 64, 8, cutlass::bfloat16_t>>, Is_dropout=false, Is_causal=true, Is_even_N=false, Is_even_K=true, Return_softmax=false, Params=Flash_fwd_params]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(15): here
            instantiation of "void flash_fwd_kernel<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax>(Flash_fwd_params) [with Kernel_traits=Flash_fwd_kernel_traits<256, 128, 64, 8, false, false, cutlass::bfloat16_t, Flash_kernel_traits<256, 128, 64, 8, cutlass::bfloat16_t>>, Is_dropout=false, Is_causal=true, Is_even_N=false, Is_even_K=true, Return_softmax=false]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(34): here
            instantiation of "void run_flash_fwd<Kernel_traits,Is_dropout,Is_causal>(Flash_fwd_params &, cudaStream_t) [with Kernel_traits=Flash_fwd_kernel_traits<256, 128, 64, 8, false, false, cutlass::bfloat16_t, Flash_kernel_traits<256, 128, 64, 8, cutlass::bfloat16_t>>, Is_dropout=false, Is_causal=true]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(236): here
            instantiation of "void run_mha_fwd_hdim256<T>(Flash_fwd_params &, cudaStream_t) [with T=cutlass::bfloat16_t]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_hdim256_bf16_sm80.cu(8): here

/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_kernel.h(140): warning: variable "MMA_M" was declared but never referenced
          detected during:
            instantiation of "void flash::compute_attn_1rowblock<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params,Is_Q_split_K>(const Params &, int, int, int) [with Kernel_traits=Flash_fwd_kernel_traits<256, 128, 64, 8, false, false, cutlass::bfloat16_t, Flash_kernel_traits<256, 128, 64, 8, cutlass::bfloat16_t>>, Is_dropout=false, Is_causal=true, Is_even_N=false, Is_even_K=false, Return_softmax=false, Params=Flash_fwd_params, Is_Q_split_K=false]" 
(589): here
            instantiation of "void flash::compute_attn<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &) [with Kernel_traits=Flash_fwd_kernel_traits<256, 128, 64, 8, false, false, cutlass::bfloat16_t, Flash_kernel_traits<256, 128, 64, 8, cutlass::bfloat16_t>>, Is_dropout=false, Is_causal=true, Is_even_N=false, Is_even_K=false, Return_softmax=false, Params=Flash_fwd_params]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(15): here
            instantiation of "void flash_fwd_kernel<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax>(Flash_fwd_params) [with Kernel_traits=Flash_fwd_kernel_traits<256, 128, 64, 8, false, false, cutlass::bfloat16_t, Flash_kernel_traits<256, 128, 64, 8, cutlass::bfloat16_t>>, Is_dropout=false, Is_causal=true, Is_even_N=false, Is_even_K=false, Return_softmax=false]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(34): here
            instantiation of "void run_flash_fwd<Kernel_traits,Is_dropout,Is_causal>(Flash_fwd_params &, cudaStream_t) [with Kernel_traits=Flash_fwd_kernel_traits<256, 128, 64, 8, false, false, cutlass::bfloat16_t, Flash_kernel_traits<256, 128, 64, 8, cutlass::bfloat16_t>>, Is_dropout=false, Is_causal=true]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(236): here
            instantiation of "void run_mha_fwd_hdim256<T>(Flash_fwd_params &, cudaStream_t) [with T=cutlass::bfloat16_t]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_hdim256_bf16_sm80.cu(8): here

/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_kernel.h(276): warning: variable "tQrQ" was declared but never referenced
          detected during:
            instantiation of "void flash::compute_attn_1rowblock<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params,Is_Q_split_K>(const Params &, int, int, int) [with Kernel_traits=Flash_fwd_kernel_traits<256, 128, 64, 8, false, false, cutlass::bfloat16_t, Flash_kernel_traits<256, 128, 64, 8, cutlass::bfloat16_t>>, Is_dropout=false, Is_causal=true, Is_even_N=false, Is_even_K=false, Return_softmax=false, Params=Flash_fwd_params, Is_Q_split_K=false]" 
(589): here
            instantiation of "void flash::compute_attn<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &) [with Kernel_traits=Flash_fwd_kernel_traits<256, 128, 64, 8, false, false, cutlass::bfloat16_t, Flash_kernel_traits<256, 128, 64, 8, cutlass::bfloat16_t>>, Is_dropout=false, Is_causal=true, Is_even_N=false, Is_even_K=false, Return_softmax=false, Params=Flash_fwd_params]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(15): here
            instantiation of "void flash_fwd_kernel<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax>(Flash_fwd_params) [with Kernel_traits=Flash_fwd_kernel_traits<256, 128, 64, 8, false, false, cutlass::bfloat16_t, Flash_kernel_traits<256, 128, 64, 8, cutlass::bfloat16_t>>, Is_dropout=false, Is_causal=true, Is_even_N=false, Is_even_K=false, Return_softmax=false]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(34): here
            instantiation of "void run_flash_fwd<Kernel_traits,Is_dropout,Is_causal>(Flash_fwd_params &, cudaStream_t) [with Kernel_traits=Flash_fwd_kernel_traits<256, 128, 64, 8, false, false, cutlass::bfloat16_t, Flash_kernel_traits<256, 128, 64, 8, cutlass::bfloat16_t>>, Is_dropout=false, Is_causal=true]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(236): here
            instantiation of "void run_mha_fwd_hdim256<T>(Flash_fwd_params &, cudaStream_t) [with T=cutlass::bfloat16_t]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_hdim256_bf16_sm80.cu(8): here

/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_kernel.h(140): warning: variable "MMA_M" was declared but never referenced
          detected during:
            instantiation of "void flash::compute_attn_1rowblock<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params,Is_Q_split_K>(const Params &, int, int, int) [with Kernel_traits=Flash_fwd_kernel_traits<256, 64, 64, 4, false, false, cutlass::bfloat16_t, Flash_kernel_traits<256, 64, 64, 4, cutlass::bfloat16_t>>, Is_dropout=false, Is_causal=true, Is_even_N=true, Is_even_K=true, Return_softmax=false, Params=Flash_fwd_params, Is_Q_split_K=false]" 
(589): here
            instantiation of "void flash::compute_attn<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &) [with Kernel_traits=Flash_fwd_kernel_traits<256, 64, 64, 4, false, false, cutlass::bfloat16_t, Flash_kernel_traits<256, 64, 64, 4, cutlass::bfloat16_t>>, Is_dropout=false, Is_causal=true, Is_even_N=true, Is_even_K=true, Return_softmax=false, Params=Flash_fwd_params]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(15): here
            instantiation of "void flash_fwd_kernel<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax>(Flash_fwd_params) [with Kernel_traits=Flash_fwd_kernel_traits<256, 64, 64, 4, false, false, cutlass::bfloat16_t, Flash_kernel_traits<256, 64, 64, 4, cutlass::bfloat16_t>>, Is_dropout=false, Is_causal=true, Is_even_N=true, Is_even_K=true, Return_softmax=false]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(34): here
            instantiation of "void run_flash_fwd<Kernel_traits,Is_dropout,Is_causal>(Flash_fwd_params &, cudaStream_t) [with Kernel_traits=Flash_fwd_kernel_traits<256, 64, 64, 4, false, false, cutlass::bfloat16_t, Flash_kernel_traits<256, 64, 64, 4, cutlass::bfloat16_t>>, Is_dropout=false, Is_causal=true]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(236): here
            instantiation of "void run_mha_fwd_hdim256<T>(Flash_fwd_params &, cudaStream_t) [with T=cutlass::bfloat16_t]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_hdim256_bf16_sm80.cu(8): here

/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_kernel.h(276): warning: variable "tQrQ" was declared but never referenced
          detected during:
            instantiation of "void flash::compute_attn_1rowblock<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params,Is_Q_split_K>(const Params &, int, int, int) [with Kernel_traits=Flash_fwd_kernel_traits<256, 64, 64, 4, false, false, cutlass::bfloat16_t, Flash_kernel_traits<256, 64, 64, 4, cutlass::bfloat16_t>>, Is_dropout=false, Is_causal=true, Is_even_N=true, Is_even_K=true, Return_softmax=false, Params=Flash_fwd_params, Is_Q_split_K=false]" 
(589): here
            instantiation of "void flash::compute_attn<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &) [with Kernel_traits=Flash_fwd_kernel_traits<256, 64, 64, 4, false, false, cutlass::bfloat16_t, Flash_kernel_traits<256, 64, 64, 4, cutlass::bfloat16_t>>, Is_dropout=false, Is_causal=true, Is_even_N=true, Is_even_K=true, Return_softmax=false, Params=Flash_fwd_params]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(15): here
            instantiation of "void flash_fwd_kernel<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax>(Flash_fwd_params) [with Kernel_traits=Flash_fwd_kernel_traits<256, 64, 64, 4, false, false, cutlass::bfloat16_t, Flash_kernel_traits<256, 64, 64, 4, cutlass::bfloat16_t>>, Is_dropout=false, Is_causal=true, Is_even_N=true, Is_even_K=true, Return_softmax=false]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(34): here
            instantiation of "void run_flash_fwd<Kernel_traits,Is_dropout,Is_causal>(Flash_fwd_params &, cudaStream_t) [with Kernel_traits=Flash_fwd_kernel_traits<256, 64, 64, 4, false, false, cutlass::bfloat16_t, Flash_kernel_traits<256, 64, 64, 4, cutlass::bfloat16_t>>, Is_dropout=false, Is_causal=true]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(236): here
            instantiation of "void run_mha_fwd_hdim256<T>(Flash_fwd_params &, cudaStream_t) [with T=cutlass::bfloat16_t]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_hdim256_bf16_sm80.cu(8): here

/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_kernel.h(140): warning: variable "MMA_M" was declared but never referenced
          detected during:
            instantiation of "void flash::compute_attn_1rowblock<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params,Is_Q_split_K>(const Params &, int, int, int) [with Kernel_traits=Flash_fwd_kernel_traits<256, 64, 64, 4, false, false, cutlass::bfloat16_t, Flash_kernel_traits<256, 64, 64, 4, cutlass::bfloat16_t>>, Is_dropout=false, Is_causal=true, Is_even_N=true, Is_even_K=false, Return_softmax=false, Params=Flash_fwd_params, Is_Q_split_K=false]" 
(589): here
            instantiation of "void flash::compute_attn<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &) [with Kernel_traits=Flash_fwd_kernel_traits<256, 64, 64, 4, false, false, cutlass::bfloat16_t, Flash_kernel_traits<256, 64, 64, 4, cutlass::bfloat16_t>>, Is_dropout=false, Is_causal=true, Is_even_N=true, Is_even_K=false, Return_softmax=false, Params=Flash_fwd_params]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(15): here
            instantiation of "void flash_fwd_kernel<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax>(Flash_fwd_params) [with Kernel_traits=Flash_fwd_kernel_traits<256, 64, 64, 4, false, false, cutlass::bfloat16_t, Flash_kernel_traits<256, 64, 64, 4, cutlass::bfloat16_t>>, Is_dropout=false, Is_causal=true, Is_even_N=true, Is_even_K=false, Return_softmax=false]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(34): here
            instantiation of "void run_flash_fwd<Kernel_traits,Is_dropout,Is_causal>(Flash_fwd_params &, cudaStream_t) [with Kernel_traits=Flash_fwd_kernel_traits<256, 64, 64, 4, false, false, cutlass::bfloat16_t, Flash_kernel_traits<256, 64, 64, 4, cutlass::bfloat16_t>>, Is_dropout=false, Is_causal=true]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(236): here
            instantiation of "void run_mha_fwd_hdim256<T>(Flash_fwd_params &, cudaStream_t) [with T=cutlass::bfloat16_t]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_hdim256_bf16_sm80.cu(8): here

/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_kernel.h(276): warning: variable "tQrQ" was declared but never referenced
          detected during:
            instantiation of "void flash::compute_attn_1rowblock<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params,Is_Q_split_K>(const Params &, int, int, int) [with Kernel_traits=Flash_fwd_kernel_traits<256, 64, 64, 4, false, false, cutlass::bfloat16_t, Flash_kernel_traits<256, 64, 64, 4, cutlass::bfloat16_t>>, Is_dropout=false, Is_causal=true, Is_even_N=true, Is_even_K=false, Return_softmax=false, Params=Flash_fwd_params, Is_Q_split_K=false]" 
(589): here
            instantiation of "void flash::compute_attn<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &) [with Kernel_traits=Flash_fwd_kernel_traits<256, 64, 64, 4, false, false, cutlass::bfloat16_t, Flash_kernel_traits<256, 64, 64, 4, cutlass::bfloat16_t>>, Is_dropout=false, Is_causal=true, Is_even_N=true, Is_even_K=false, Return_softmax=false, Params=Flash_fwd_params]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(15): here
            instantiation of "void flash_fwd_kernel<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax>(Flash_fwd_params) [with Kernel_traits=Flash_fwd_kernel_traits<256, 64, 64, 4, false, false, cutlass::bfloat16_t, Flash_kernel_traits<256, 64, 64, 4, cutlass::bfloat16_t>>, Is_dropout=false, Is_causal=true, Is_even_N=true, Is_even_K=false, Return_softmax=false]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(34): here
            instantiation of "void run_flash_fwd<Kernel_traits,Is_dropout,Is_causal>(Flash_fwd_params &, cudaStream_t) [with Kernel_traits=Flash_fwd_kernel_traits<256, 64, 64, 4, false, false, cutlass::bfloat16_t, Flash_kernel_traits<256, 64, 64, 4, cutlass::bfloat16_t>>, Is_dropout=false, Is_causal=true]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(236): here
            instantiation of "void run_mha_fwd_hdim256<T>(Flash_fwd_params &, cudaStream_t) [with T=cutlass::bfloat16_t]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_hdim256_bf16_sm80.cu(8): here

/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_kernel.h(140): warning: variable "MMA_M" was declared but never referenced
          detected during:
            instantiation of "void flash::compute_attn_1rowblock<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params,Is_Q_split_K>(const Params &, int, int, int) [with Kernel_traits=Flash_fwd_kernel_traits<256, 64, 64, 4, false, false, cutlass::bfloat16_t, Flash_kernel_traits<256, 64, 64, 4, cutlass::bfloat16_t>>, Is_dropout=false, Is_causal=true, Is_even_N=false, Is_even_K=true, Return_softmax=false, Params=Flash_fwd_params, Is_Q_split_K=false]" 
(589): here
            instantiation of "void flash::compute_attn<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &) [with Kernel_traits=Flash_fwd_kernel_traits<256, 64, 64, 4, false, false, cutlass::bfloat16_t, Flash_kernel_traits<256, 64, 64, 4, cutlass::bfloat16_t>>, Is_dropout=false, Is_causal=true, Is_even_N=false, Is_even_K=true, Return_softmax=false, Params=Flash_fwd_params]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(15): here
            instantiation of "void flash_fwd_kernel<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax>(Flash_fwd_params) [with Kernel_traits=Flash_fwd_kernel_traits<256, 64, 64, 4, false, false, cutlass::bfloat16_t, Flash_kernel_traits<256, 64, 64, 4, cutlass::bfloat16_t>>, Is_dropout=false, Is_causal=true, Is_even_N=false, Is_even_K=true, Return_softmax=false]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(34): here
            instantiation of "void run_flash_fwd<Kernel_traits,Is_dropout,Is_causal>(Flash_fwd_params &, cudaStream_t) [with Kernel_traits=Flash_fwd_kernel_traits<256, 64, 64, 4, false, false, cutlass::bfloat16_t, Flash_kernel_traits<256, 64, 64, 4, cutlass::bfloat16_t>>, Is_dropout=false, Is_causal=true]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(236): here
            instantiation of "void run_mha_fwd_hdim256<T>(Flash_fwd_params &, cudaStream_t) [with T=cutlass::bfloat16_t]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_hdim256_bf16_sm80.cu(8): here

/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_kernel.h(276): warning: variable "tQrQ" was declared but never referenced
          detected during:
            instantiation of "void flash::compute_attn_1rowblock<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params,Is_Q_split_K>(const Params &, int, int, int) [with Kernel_traits=Flash_fwd_kernel_traits<256, 64, 64, 4, false, false, cutlass::bfloat16_t, Flash_kernel_traits<256, 64, 64, 4, cutlass::bfloat16_t>>, Is_dropout=false, Is_causal=true, Is_even_N=false, Is_even_K=true, Return_softmax=false, Params=Flash_fwd_params, Is_Q_split_K=false]" 
(589): here
            instantiation of "void flash::compute_attn<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &) [with Kernel_traits=Flash_fwd_kernel_traits<256, 64, 64, 4, false, false, cutlass::bfloat16_t, Flash_kernel_traits<256, 64, 64, 4, cutlass::bfloat16_t>>, Is_dropout=false, Is_causal=true, Is_even_N=false, Is_even_K=true, Return_softmax=false, Params=Flash_fwd_params]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(15): here
            instantiation of "void flash_fwd_kernel<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax>(Flash_fwd_params) [with Kernel_traits=Flash_fwd_kernel_traits<256, 64, 64, 4, false, false, cutlass::bfloat16_t, Flash_kernel_traits<256, 64, 64, 4, cutlass::bfloat16_t>>, Is_dropout=false, Is_causal=true, Is_even_N=false, Is_even_K=true, Return_softmax=false]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(34): here
            instantiation of "void run_flash_fwd<Kernel_traits,Is_dropout,Is_causal>(Flash_fwd_params &, cudaStream_t) [with Kernel_traits=Flash_fwd_kernel_traits<256, 64, 64, 4, false, false, cutlass::bfloat16_t, Flash_kernel_traits<256, 64, 64, 4, cutlass::bfloat16_t>>, Is_dropout=false, Is_causal=true]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(236): here
            instantiation of "void run_mha_fwd_hdim256<T>(Flash_fwd_params &, cudaStream_t) [with T=cutlass::bfloat16_t]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_hdim256_bf16_sm80.cu(8): here

/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_kernel.h(140): warning: variable "MMA_M" was declared but never referenced
          detected during:
            instantiation of "void flash::compute_attn_1rowblock<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params,Is_Q_split_K>(const Params &, int, int, int) [with Kernel_traits=Flash_fwd_kernel_traits<256, 64, 64, 4, false, false, cutlass::bfloat16_t, Flash_kernel_traits<256, 64, 64, 4, cutlass::bfloat16_t>>, Is_dropout=false, Is_causal=true, Is_even_N=false, Is_even_K=false, Return_softmax=false, Params=Flash_fwd_params, Is_Q_split_K=false]" 
(589): here
            instantiation of "void flash::compute_attn<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &) [with Kernel_traits=Flash_fwd_kernel_traits<256, 64, 64, 4, false, false, cutlass::bfloat16_t, Flash_kernel_traits<256, 64, 64, 4, cutlass::bfloat16_t>>, Is_dropout=false, Is_causal=true, Is_even_N=false, Is_even_K=false, Return_softmax=false, Params=Flash_fwd_params]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(15): here
            instantiation of "void flash_fwd_kernel<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax>(Flash_fwd_params) [with Kernel_traits=Flash_fwd_kernel_traits<256, 64, 64, 4, false, false, cutlass::bfloat16_t, Flash_kernel_traits<256, 64, 64, 4, cutlass::bfloat16_t>>, Is_dropout=false, Is_causal=true, Is_even_N=false, Is_even_K=false, Return_softmax=false]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(34): here
            instantiation of "void run_flash_fwd<Kernel_traits,Is_dropout,Is_causal>(Flash_fwd_params &, cudaStream_t) [with Kernel_traits=Flash_fwd_kernel_traits<256, 64, 64, 4, false, false, cutlass::bfloat16_t, Flash_kernel_traits<256, 64, 64, 4, cutlass::bfloat16_t>>, Is_dropout=false, Is_causal=true]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(236): here
            instantiation of "void run_mha_fwd_hdim256<T>(Flash_fwd_params &, cudaStream_t) [with T=cutlass::bfloat16_t]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_hdim256_bf16_sm80.cu(8): here

/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_kernel.h(276): warning: variable "tQrQ" was declared but never referenced
          detected during:
            instantiation of "void flash::compute_attn_1rowblock<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params,Is_Q_split_K>(const Params &, int, int, int) [with Kernel_traits=Flash_fwd_kernel_traits<256, 64, 64, 4, false, false, cutlass::bfloat16_t, Flash_kernel_traits<256, 64, 64, 4, cutlass::bfloat16_t>>, Is_dropout=false, Is_causal=true, Is_even_N=false, Is_even_K=false, Return_softmax=false, Params=Flash_fwd_params, Is_Q_split_K=false]" 
(589): here
            instantiation of "void flash::compute_attn<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &) [with Kernel_traits=Flash_fwd_kernel_traits<256, 64, 64, 4, false, false, cutlass::bfloat16_t, Flash_kernel_traits<256, 64, 64, 4, cutlass::bfloat16_t>>, Is_dropout=false, Is_causal=true, Is_even_N=false, Is_even_K=false, Return_softmax=false, Params=Flash_fwd_params]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(15): here
            instantiation of "void flash_fwd_kernel<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax>(Flash_fwd_params) [with Kernel_traits=Flash_fwd_kernel_traits<256, 64, 64, 4, false, false, cutlass::bfloat16_t, Flash_kernel_traits<256, 64, 64, 4, cutlass::bfloat16_t>>, Is_dropout=false, Is_causal=true, Is_even_N=false, Is_even_K=false, Return_softmax=false]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(34): here
            instantiation of "void run_flash_fwd<Kernel_traits,Is_dropout,Is_causal>(Flash_fwd_params &, cudaStream_t) [with Kernel_traits=Flash_fwd_kernel_traits<256, 64, 64, 4, false, false, cutlass::bfloat16_t, Flash_kernel_traits<256, 64, 64, 4, cutlass::bfloat16_t>>, Is_dropout=false, Is_causal=true]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(236): here
            instantiation of "void run_mha_fwd_hdim256<T>(Flash_fwd_params &, cudaStream_t) [with T=cutlass::bfloat16_t]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_hdim256_bf16_sm80.cu(8): here

/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_kernel.h(140): warning: variable "MMA_M" was declared but never referenced
          detected during:
            instantiation of "void flash::compute_attn_1rowblock<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params,Is_Q_split_K>(const Params &, int, int, int) [with Kernel_traits=Flash_fwd_kernel_traits<256, 128, 64, 8, false, false, cutlass::bfloat16_t, Flash_kernel_traits<256, 128, 64, 8, cutlass::bfloat16_t>>, Is_dropout=false, Is_causal=false, Is_even_N=true, Is_even_K=true, Return_softmax=false, Params=Flash_fwd_params, Is_Q_split_K=false]" 
(589): here
            instantiation of "void flash::compute_attn<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &) [with Kernel_traits=Flash_fwd_kernel_traits<256, 128, 64, 8, false, false, cutlass::bfloat16_t, Flash_kernel_traits<256, 128, 64, 8, cutlass::bfloat16_t>>, Is_dropout=false, Is_causal=false, Is_even_N=true, Is_even_K=true, Return_softmax=false, Params=Flash_fwd_params]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(15): here
            instantiation of "void flash_fwd_kernel<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax>(Flash_fwd_params) [with Kernel_traits=Flash_fwd_kernel_traits<256, 128, 64, 8, false, false, cutlass::bfloat16_t, Flash_kernel_traits<256, 128, 64, 8, cutlass::bfloat16_t>>, Is_dropout=false, Is_causal=false, Is_even_N=true, Is_even_K=true, Return_softmax=false]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(34): here
            instantiation of "void run_flash_fwd<Kernel_traits,Is_dropout,Is_causal>(Flash_fwd_params &, cudaStream_t) [with Kernel_traits=Flash_fwd_kernel_traits<256, 128, 64, 8, false, false, cutlass::bfloat16_t, Flash_kernel_traits<256, 128, 64, 8, cutlass::bfloat16_t>>, Is_dropout=false, Is_causal=false]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(236): here
            instantiation of "void run_mha_fwd_hdim256<T>(Flash_fwd_params &, cudaStream_t) [with T=cutlass::bfloat16_t]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_hdim256_bf16_sm80.cu(8): here

/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_kernel.h(276): warning: variable "tQrQ" was declared but never referenced
          detected during:
            instantiation of "void flash::compute_attn_1rowblock<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params,Is_Q_split_K>(const Params &, int, int, int) [with Kernel_traits=Flash_fwd_kernel_traits<256, 128, 64, 8, false, false, cutlass::bfloat16_t, Flash_kernel_traits<256, 128, 64, 8, cutlass::bfloat16_t>>, Is_dropout=false, Is_causal=false, Is_even_N=true, Is_even_K=true, Return_softmax=false, Params=Flash_fwd_params, Is_Q_split_K=false]" 
(589): here
            instantiation of "void flash::compute_attn<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &) [with Kernel_traits=Flash_fwd_kernel_traits<256, 128, 64, 8, false, false, cutlass::bfloat16_t, Flash_kernel_traits<256, 128, 64, 8, cutlass::bfloat16_t>>, Is_dropout=false, Is_causal=false, Is_even_N=true, Is_even_K=true, Return_softmax=false, Params=Flash_fwd_params]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(15): here
            instantiation of "void flash_fwd_kernel<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax>(Flash_fwd_params) [with Kernel_traits=Flash_fwd_kernel_traits<256, 128, 64, 8, false, false, cutlass::bfloat16_t, Flash_kernel_traits<256, 128, 64, 8, cutlass::bfloat16_t>>, Is_dropout=false, Is_causal=false, Is_even_N=true, Is_even_K=true, Return_softmax=false]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(34): here
            instantiation of "void run_flash_fwd<Kernel_traits,Is_dropout,Is_causal>(Flash_fwd_params &, cudaStream_t) [with Kernel_traits=Flash_fwd_kernel_traits<256, 128, 64, 8, false, false, cutlass::bfloat16_t, Flash_kernel_traits<256, 128, 64, 8, cutlass::bfloat16_t>>, Is_dropout=false, Is_causal=false]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(236): here
            instantiation of "void run_mha_fwd_hdim256<T>(Flash_fwd_params &, cudaStream_t) [with T=cutlass::bfloat16_t]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_hdim256_bf16_sm80.cu(8): here

/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_kernel.h(140): warning: variable "MMA_M" was declared but never referenced
          detected during:
            instantiation of "void flash::compute_attn_1rowblock<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params,Is_Q_split_K>(const Params &, int, int, int) [with Kernel_traits=Flash_fwd_kernel_traits<256, 128, 64, 8, false, false, cutlass::bfloat16_t, Flash_kernel_traits<256, 128, 64, 8, cutlass::bfloat16_t>>, Is_dropout=false, Is_causal=false, Is_even_N=true, Is_even_K=false, Return_softmax=false, Params=Flash_fwd_params, Is_Q_split_K=false]" 
(589): here
            instantiation of "void flash::compute_attn<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &) [with Kernel_traits=Flash_fwd_kernel_traits<256, 128, 64, 8, false, false, cutlass::bfloat16_t, Flash_kernel_traits<256, 128, 64, 8, cutlass::bfloat16_t>>, Is_dropout=false, Is_causal=false, Is_even_N=true, Is_even_K=false, Return_softmax=false, Params=Flash_fwd_params]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(15): here
            instantiation of "void flash_fwd_kernel<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax>(Flash_fwd_params) [with Kernel_traits=Flash_fwd_kernel_traits<256, 128, 64, 8, false, false, cutlass::bfloat16_t, Flash_kernel_traits<256, 128, 64, 8, cutlass::bfloat16_t>>, Is_dropout=false, Is_causal=false, Is_even_N=true, Is_even_K=false, Return_softmax=false]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(34): here
            instantiation of "void run_flash_fwd<Kernel_traits,Is_dropout,Is_causal>(Flash_fwd_params &, cudaStream_t) [with Kernel_traits=Flash_fwd_kernel_traits<256, 128, 64, 8, false, false, cutlass::bfloat16_t, Flash_kernel_traits<256, 128, 64, 8, cutlass::bfloat16_t>>, Is_dropout=false, Is_causal=false]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(236): here
            instantiation of "void run_mha_fwd_hdim256<T>(Flash_fwd_params &, cudaStream_t) [with T=cutlass::bfloat16_t]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_hdim256_bf16_sm80.cu(8): here

/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_kernel.h(276): warning: variable "tQrQ" was declared but never referenced
          detected during:
            instantiation of "void flash::compute_attn_1rowblock<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params,Is_Q_split_K>(const Params &, int, int, int) [with Kernel_traits=Flash_fwd_kernel_traits<256, 128, 64, 8, false, false, cutlass::bfloat16_t, Flash_kernel_traits<256, 128, 64, 8, cutlass::bfloat16_t>>, Is_dropout=false, Is_causal=false, Is_even_N=true, Is_even_K=false, Return_softmax=false, Params=Flash_fwd_params, Is_Q_split_K=false]" 
(589): here
            instantiation of "void flash::compute_attn<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &) [with Kernel_traits=Flash_fwd_kernel_traits<256, 128, 64, 8, false, false, cutlass::bfloat16_t, Flash_kernel_traits<256, 128, 64, 8, cutlass::bfloat16_t>>, Is_dropout=false, Is_causal=false, Is_even_N=true, Is_even_K=false, Return_softmax=false, Params=Flash_fwd_params]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(15): here
            instantiation of "void flash_fwd_kernel<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax>(Flash_fwd_params) [with Kernel_traits=Flash_fwd_kernel_traits<256, 128, 64, 8, false, false, cutlass::bfloat16_t, Flash_kernel_traits<256, 128, 64, 8, cutlass::bfloat16_t>>, Is_dropout=false, Is_causal=false, Is_even_N=true, Is_even_K=false, Return_softmax=false]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(34): here
            instantiation of "void run_flash_fwd<Kernel_traits,Is_dropout,Is_causal>(Flash_fwd_params &, cudaStream_t) [with Kernel_traits=Flash_fwd_kernel_traits<256, 128, 64, 8, false, false, cutlass::bfloat16_t, Flash_kernel_traits<256, 128, 64, 8, cutlass::bfloat16_t>>, Is_dropout=false, Is_causal=false]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(236): here
            instantiation of "void run_mha_fwd_hdim256<T>(Flash_fwd_params &, cudaStream_t) [with T=cutlass::bfloat16_t]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_hdim256_bf16_sm80.cu(8): here

/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_kernel.h(140): warning: variable "MMA_M" was declared but never referenced
          detected during:
            instantiation of "void flash::compute_attn_1rowblock<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params,Is_Q_split_K>(const Params &, int, int, int) [with Kernel_traits=Flash_fwd_kernel_traits<256, 128, 64, 8, false, false, cutlass::bfloat16_t, Flash_kernel_traits<256, 128, 64, 8, cutlass::bfloat16_t>>, Is_dropout=false, Is_causal=false, Is_even_N=false, Is_even_K=true, Return_softmax=false, Params=Flash_fwd_params, Is_Q_split_K=false]" 
(589): here
            instantiation of "void flash::compute_attn<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &) [with Kernel_traits=Flash_fwd_kernel_traits<256, 128, 64, 8, false, false, cutlass::bfloat16_t, Flash_kernel_traits<256, 128, 64, 8, cutlass::bfloat16_t>>, Is_dropout=false, Is_causal=false, Is_even_N=false, Is_even_K=true, Return_softmax=false, Params=Flash_fwd_params]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(15): here
            instantiation of "void flash_fwd_kernel<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax>(Flash_fwd_params) [with Kernel_traits=Flash_fwd_kernel_traits<256, 128, 64, 8, false, false, cutlass::bfloat16_t, Flash_kernel_traits<256, 128, 64, 8, cutlass::bfloat16_t>>, Is_dropout=false, Is_causal=false, Is_even_N=false, Is_even_K=true, Return_softmax=false]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(34): here
            instantiation of "void run_flash_fwd<Kernel_traits,Is_dropout,Is_causal>(Flash_fwd_params &, cudaStream_t) [with Kernel_traits=Flash_fwd_kernel_traits<256, 128, 64, 8, false, false, cutlass::bfloat16_t, Flash_kernel_traits<256, 128, 64, 8, cutlass::bfloat16_t>>, Is_dropout=false, Is_causal=false]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(236): here
            instantiation of "void run_mha_fwd_hdim256<T>(Flash_fwd_params &, cudaStream_t) [with T=cutlass::bfloat16_t]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_hdim256_bf16_sm80.cu(8): here

/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_kernel.h(276): warning: variable "tQrQ" was declared but never referenced
          detected during:
            instantiation of "void flash::compute_attn_1rowblock<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params,Is_Q_split_K>(const Params &, int, int, int) [with Kernel_traits=Flash_fwd_kernel_traits<256, 128, 64, 8, false, false, cutlass::bfloat16_t, Flash_kernel_traits<256, 128, 64, 8, cutlass::bfloat16_t>>, Is_dropout=false, Is_causal=false, Is_even_N=false, Is_even_K=true, Return_softmax=false, Params=Flash_fwd_params, Is_Q_split_K=false]" 
(589): here
            instantiation of "void flash::compute_attn<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &) [with Kernel_traits=Flash_fwd_kernel_traits<256, 128, 64, 8, false, false, cutlass::bfloat16_t, Flash_kernel_traits<256, 128, 64, 8, cutlass::bfloat16_t>>, Is_dropout=false, Is_causal=false, Is_even_N=false, Is_even_K=true, Return_softmax=false, Params=Flash_fwd_params]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(15): here
            instantiation of "void flash_fwd_kernel<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax>(Flash_fwd_params) [with Kernel_traits=Flash_fwd_kernel_traits<256, 128, 64, 8, false, false, cutlass::bfloat16_t, Flash_kernel_traits<256, 128, 64, 8, cutlass::bfloat16_t>>, Is_dropout=false, Is_causal=false, Is_even_N=false, Is_even_K=true, Return_softmax=false]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(34): here
            instantiation of "void run_flash_fwd<Kernel_traits,Is_dropout,Is_causal>(Flash_fwd_params &, cudaStream_t) [with Kernel_traits=Flash_fwd_kernel_traits<256, 128, 64, 8, false, false, cutlass::bfloat16_t, Flash_kernel_traits<256, 128, 64, 8, cutlass::bfloat16_t>>, Is_dropout=false, Is_causal=false]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(236): here
            instantiation of "void run_mha_fwd_hdim256<T>(Flash_fwd_params &, cudaStream_t) [with T=cutlass::bfloat16_t]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_hdim256_bf16_sm80.cu(8): here

/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_kernel.h(140): warning: variable "MMA_M" was declared but never referenced
          detected during:
            instantiation of "void flash::compute_attn_1rowblock<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params,Is_Q_split_K>(const Params &, int, int, int) [with Kernel_traits=Flash_fwd_kernel_traits<256, 128, 64, 8, false, false, cutlass::bfloat16_t, Flash_kernel_traits<256, 128, 64, 8, cutlass::bfloat16_t>>, Is_dropout=false, Is_causal=false, Is_even_N=false, Is_even_K=false, Return_softmax=false, Params=Flash_fwd_params, Is_Q_split_K=false]" 
(589): here
            instantiation of "void flash::compute_attn<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &) [with Kernel_traits=Flash_fwd_kernel_traits<256, 128, 64, 8, false, false, cutlass::bfloat16_t, Flash_kernel_traits<256, 128, 64, 8, cutlass::bfloat16_t>>, Is_dropout=false, Is_causal=false, Is_even_N=false, Is_even_K=false, Return_softmax=false, Params=Flash_fwd_params]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(15): here
            instantiation of "void flash_fwd_kernel<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax>(Flash_fwd_params) [with Kernel_traits=Flash_fwd_kernel_traits<256, 128, 64, 8, false, false, cutlass::bfloat16_t, Flash_kernel_traits<256, 128, 64, 8, cutlass::bfloat16_t>>, Is_dropout=false, Is_causal=false, Is_even_N=false, Is_even_K=false, Return_softmax=false]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(34): here
            instantiation of "void run_flash_fwd<Kernel_traits,Is_dropout,Is_causal>(Flash_fwd_params &, cudaStream_t) [with Kernel_traits=Flash_fwd_kernel_traits<256, 128, 64, 8, false, false, cutlass::bfloat16_t, Flash_kernel_traits<256, 128, 64, 8, cutlass::bfloat16_t>>, Is_dropout=false, Is_causal=false]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(236): here
            instantiation of "void run_mha_fwd_hdim256<T>(Flash_fwd_params &, cudaStream_t) [with T=cutlass::bfloat16_t]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_hdim256_bf16_sm80.cu(8): here

/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_kernel.h(276): warning: variable "tQrQ" was declared but never referenced
          detected during:
            instantiation of "void flash::compute_attn_1rowblock<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params,Is_Q_split_K>(const Params &, int, int, int) [with Kernel_traits=Flash_fwd_kernel_traits<256, 128, 64, 8, false, false, cutlass::bfloat16_t, Flash_kernel_traits<256, 128, 64, 8, cutlass::bfloat16_t>>, Is_dropout=false, Is_causal=false, Is_even_N=false, Is_even_K=false, Return_softmax=false, Params=Flash_fwd_params, Is_Q_split_K=false]" 
(589): here
            instantiation of "void flash::compute_attn<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &) [with Kernel_traits=Flash_fwd_kernel_traits<256, 128, 64, 8, false, false, cutlass::bfloat16_t, Flash_kernel_traits<256, 128, 64, 8, cutlass::bfloat16_t>>, Is_dropout=false, Is_causal=false, Is_even_N=false, Is_even_K=false, Return_softmax=false, Params=Flash_fwd_params]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(15): here
            instantiation of "void flash_fwd_kernel<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax>(Flash_fwd_params) [with Kernel_traits=Flash_fwd_kernel_traits<256, 128, 64, 8, false, false, cutlass::bfloat16_t, Flash_kernel_traits<256, 128, 64, 8, cutlass::bfloat16_t>>, Is_dropout=false, Is_causal=false, Is_even_N=false, Is_even_K=false, Return_softmax=false]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(34): here
            instantiation of "void run_flash_fwd<Kernel_traits,Is_dropout,Is_causal>(Flash_fwd_params &, cudaStream_t) [with Kernel_traits=Flash_fwd_kernel_traits<256, 128, 64, 8, false, false, cutlass::bfloat16_t, Flash_kernel_traits<256, 128, 64, 8, cutlass::bfloat16_t>>, Is_dropout=false, Is_causal=false]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(236): here
            instantiation of "void run_mha_fwd_hdim256<T>(Flash_fwd_params &, cudaStream_t) [with T=cutlass::bfloat16_t]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_hdim256_bf16_sm80.cu(8): here

/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_kernel.h(140): warning: variable "MMA_M" was declared but never referenced
          detected during:
            instantiation of "void flash::compute_attn_1rowblock<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params,Is_Q_split_K>(const Params &, int, int, int) [with Kernel_traits=Flash_fwd_kernel_traits<256, 64, 64, 4, false, false, cutlass::bfloat16_t, Flash_kernel_traits<256, 64, 64, 4, cutlass::bfloat16_t>>, Is_dropout=false, Is_causal=false, Is_even_N=true, Is_even_K=true, Return_softmax=false, Params=Flash_fwd_params, Is_Q_split_K=false]" 
(589): here
            instantiation of "void flash::compute_attn<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &) [with Kernel_traits=Flash_fwd_kernel_traits<256, 64, 64, 4, false, false, cutlass::bfloat16_t, Flash_kernel_traits<256, 64, 64, 4, cutlass::bfloat16_t>>, Is_dropout=false, Is_causal=false, Is_even_N=true, Is_even_K=true, Return_softmax=false, Params=Flash_fwd_params]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(15): here
            instantiation of "void flash_fwd_kernel<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax>(Flash_fwd_params) [with Kernel_traits=Flash_fwd_kernel_traits<256, 64, 64, 4, false, false, cutlass::bfloat16_t, Flash_kernel_traits<256, 64, 64, 4, cutlass::bfloat16_t>>, Is_dropout=false, Is_causal=false, Is_even_N=true, Is_even_K=true, Return_softmax=false]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(34): here
            instantiation of "void run_flash_fwd<Kernel_traits,Is_dropout,Is_causal>(Flash_fwd_params &, cudaStream_t) [with Kernel_traits=Flash_fwd_kernel_traits<256, 64, 64, 4, false, false, cutlass::bfloat16_t, Flash_kernel_traits<256, 64, 64, 4, cutlass::bfloat16_t>>, Is_dropout=false, Is_causal=false]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(236): here
            instantiation of "void run_mha_fwd_hdim256<T>(Flash_fwd_params &, cudaStream_t) [with T=cutlass::bfloat16_t]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_hdim256_bf16_sm80.cu(8): here

/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_kernel.h(276): warning: variable "tQrQ" was declared but never referenced
          detected during:
            instantiation of "void flash::compute_attn_1rowblock<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params,Is_Q_split_K>(const Params &, int, int, int) [with Kernel_traits=Flash_fwd_kernel_traits<256, 64, 64, 4, false, false, cutlass::bfloat16_t, Flash_kernel_traits<256, 64, 64, 4, cutlass::bfloat16_t>>, Is_dropout=false, Is_causal=false, Is_even_N=true, Is_even_K=true, Return_softmax=false, Params=Flash_fwd_params, Is_Q_split_K=false]" 
(589): here
            instantiation of "void flash::compute_attn<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &) [with Kernel_traits=Flash_fwd_kernel_traits<256, 64, 64, 4, false, false, cutlass::bfloat16_t, Flash_kernel_traits<256, 64, 64, 4, cutlass::bfloat16_t>>, Is_dropout=false, Is_causal=false, Is_even_N=true, Is_even_K=true, Return_softmax=false, Params=Flash_fwd_params]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(15): here
            instantiation of "void flash_fwd_kernel<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax>(Flash_fwd_params) [with Kernel_traits=Flash_fwd_kernel_traits<256, 64, 64, 4, false, false, cutlass::bfloat16_t, Flash_kernel_traits<256, 64, 64, 4, cutlass::bfloat16_t>>, Is_dropout=false, Is_causal=false, Is_even_N=true, Is_even_K=true, Return_softmax=false]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(34): here
            instantiation of "void run_flash_fwd<Kernel_traits,Is_dropout,Is_causal>(Flash_fwd_params &, cudaStream_t) [with Kernel_traits=Flash_fwd_kernel_traits<256, 64, 64, 4, false, false, cutlass::bfloat16_t, Flash_kernel_traits<256, 64, 64, 4, cutlass::bfloat16_t>>, Is_dropout=false, Is_causal=false]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(236): here
            instantiation of "void run_mha_fwd_hdim256<T>(Flash_fwd_params &, cudaStream_t) [with T=cutlass::bfloat16_t]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_hdim256_bf16_sm80.cu(8): here

/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_kernel.h(140): warning: variable "MMA_M" was declared but never referenced
          detected during:
            instantiation of "void flash::compute_attn_1rowblock<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params,Is_Q_split_K>(const Params &, int, int, int) [with Kernel_traits=Flash_fwd_kernel_traits<256, 64, 64, 4, false, false, cutlass::bfloat16_t, Flash_kernel_traits<256, 64, 64, 4, cutlass::bfloat16_t>>, Is_dropout=false, Is_causal=false, Is_even_N=true, Is_even_K=false, Return_softmax=false, Params=Flash_fwd_params, Is_Q_split_K=false]" 
(589): here
            instantiation of "void flash::compute_attn<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &) [with Kernel_traits=Flash_fwd_kernel_traits<256, 64, 64, 4, false, false, cutlass::bfloat16_t, Flash_kernel_traits<256, 64, 64, 4, cutlass::bfloat16_t>>, Is_dropout=false, Is_causal=false, Is_even_N=true, Is_even_K=false, Return_softmax=false, Params=Flash_fwd_params]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(15): here
            instantiation of "void flash_fwd_kernel<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax>(Flash_fwd_params) [with Kernel_traits=Flash_fwd_kernel_traits<256, 64, 64, 4, false, false, cutlass::bfloat16_t, Flash_kernel_traits<256, 64, 64, 4, cutlass::bfloat16_t>>, Is_dropout=false, Is_causal=false, Is_even_N=true, Is_even_K=false, Return_softmax=false]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(34): here
            instantiation of "void run_flash_fwd<Kernel_traits,Is_dropout,Is_causal>(Flash_fwd_params &, cudaStream_t) [with Kernel_traits=Flash_fwd_kernel_traits<256, 64, 64, 4, false, false, cutlass::bfloat16_t, Flash_kernel_traits<256, 64, 64, 4, cutlass::bfloat16_t>>, Is_dropout=false, Is_causal=false]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(236): here
            instantiation of "void run_mha_fwd_hdim256<T>(Flash_fwd_params &, cudaStream_t) [with T=cutlass::bfloat16_t]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_hdim256_bf16_sm80.cu(8): here

/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_kernel.h(276): warning: variable "tQrQ" was declared but never referenced
          detected during:
            instantiation of "void flash::compute_attn_1rowblock<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params,Is_Q_split_K>(const Params &, int, int, int) [with Kernel_traits=Flash_fwd_kernel_traits<256, 64, 64, 4, false, false, cutlass::bfloat16_t, Flash_kernel_traits<256, 64, 64, 4, cutlass::bfloat16_t>>, Is_dropout=false, Is_causal=false, Is_even_N=true, Is_even_K=false, Return_softmax=false, Params=Flash_fwd_params, Is_Q_split_K=false]" 
(589): here
            instantiation of "void flash::compute_attn<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &) [with Kernel_traits=Flash_fwd_kernel_traits<256, 64, 64, 4, false, false, cutlass::bfloat16_t, Flash_kernel_traits<256, 64, 64, 4, cutlass::bfloat16_t>>, Is_dropout=false, Is_causal=false, Is_even_N=true, Is_even_K=false, Return_softmax=false, Params=Flash_fwd_params]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(15): here
            instantiation of "void flash_fwd_kernel<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax>(Flash_fwd_params) [with Kernel_traits=Flash_fwd_kernel_traits<256, 64, 64, 4, false, false, cutlass::bfloat16_t, Flash_kernel_traits<256, 64, 64, 4, cutlass::bfloat16_t>>, Is_dropout=false, Is_causal=false, Is_even_N=true, Is_even_K=false, Return_softmax=false]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(34): here
            instantiation of "void run_flash_fwd<Kernel_traits,Is_dropout,Is_causal>(Flash_fwd_params &, cudaStream_t) [with Kernel_traits=Flash_fwd_kernel_traits<256, 64, 64, 4, false, false, cutlass::bfloat16_t, Flash_kernel_traits<256, 64, 64, 4, cutlass::bfloat16_t>>, Is_dropout=false, Is_causal=false]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(236): here
            instantiation of "void run_mha_fwd_hdim256<T>(Flash_fwd_params &, cudaStream_t) [with T=cutlass::bfloat16_t]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_hdim256_bf16_sm80.cu(8): here

/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_kernel.h(140): warning: variable "MMA_M" was declared but never referenced
          detected during:
            instantiation of "void flash::compute_attn_1rowblock<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params,Is_Q_split_K>(const Params &, int, int, int) [with Kernel_traits=Flash_fwd_kernel_traits<256, 64, 64, 4, false, false, cutlass::bfloat16_t, Flash_kernel_traits<256, 64, 64, 4, cutlass::bfloat16_t>>, Is_dropout=false, Is_causal=false, Is_even_N=false, Is_even_K=true, Return_softmax=false, Params=Flash_fwd_params, Is_Q_split_K=false]" 
(589): here
            instantiation of "void flash::compute_attn<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &) [with Kernel_traits=Flash_fwd_kernel_traits<256, 64, 64, 4, false, false, cutlass::bfloat16_t, Flash_kernel_traits<256, 64, 64, 4, cutlass::bfloat16_t>>, Is_dropout=false, Is_causal=false, Is_even_N=false, Is_even_K=true, Return_softmax=false, Params=Flash_fwd_params]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(15): here
            instantiation of "void flash_fwd_kernel<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax>(Flash_fwd_params) [with Kernel_traits=Flash_fwd_kernel_traits<256, 64, 64, 4, false, false, cutlass::bfloat16_t, Flash_kernel_traits<256, 64, 64, 4, cutlass::bfloat16_t>>, Is_dropout=false, Is_causal=false, Is_even_N=false, Is_even_K=true, Return_softmax=false]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(34): here
            instantiation of "void run_flash_fwd<Kernel_traits,Is_dropout,Is_causal>(Flash_fwd_params &, cudaStream_t) [with Kernel_traits=Flash_fwd_kernel_traits<256, 64, 64, 4, false, false, cutlass::bfloat16_t, Flash_kernel_traits<256, 64, 64, 4, cutlass::bfloat16_t>>, Is_dropout=false, Is_causal=false]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(236): here
            instantiation of "void run_mha_fwd_hdim256<T>(Flash_fwd_params &, cudaStream_t) [with T=cutlass::bfloat16_t]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_hdim256_bf16_sm80.cu(8): here

/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_kernel.h(276): warning: variable "tQrQ" was declared but never referenced
          detected during:
            instantiation of "void flash::compute_attn_1rowblock<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params,Is_Q_split_K>(const Params &, int, int, int) [with Kernel_traits=Flash_fwd_kernel_traits<256, 64, 64, 4, false, false, cutlass::bfloat16_t, Flash_kernel_traits<256, 64, 64, 4, cutlass::bfloat16_t>>, Is_dropout=false, Is_causal=false, Is_even_N=false, Is_even_K=true, Return_softmax=false, Params=Flash_fwd_params, Is_Q_split_K=false]" 
(589): here
            instantiation of "void flash::compute_attn<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &) [with Kernel_traits=Flash_fwd_kernel_traits<256, 64, 64, 4, false, false, cutlass::bfloat16_t, Flash_kernel_traits<256, 64, 64, 4, cutlass::bfloat16_t>>, Is_dropout=false, Is_causal=false, Is_even_N=false, Is_even_K=true, Return_softmax=false, Params=Flash_fwd_params]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(15): here
            instantiation of "void flash_fwd_kernel<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax>(Flash_fwd_params) [with Kernel_traits=Flash_fwd_kernel_traits<256, 64, 64, 4, false, false, cutlass::bfloat16_t, Flash_kernel_traits<256, 64, 64, 4, cutlass::bfloat16_t>>, Is_dropout=false, Is_causal=false, Is_even_N=false, Is_even_K=true, Return_softmax=false]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(34): here
            instantiation of "void run_flash_fwd<Kernel_traits,Is_dropout,Is_causal>(Flash_fwd_params &, cudaStream_t) [with Kernel_traits=Flash_fwd_kernel_traits<256, 64, 64, 4, false, false, cutlass::bfloat16_t, Flash_kernel_traits<256, 64, 64, 4, cutlass::bfloat16_t>>, Is_dropout=false, Is_causal=false]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(236): here
            instantiation of "void run_mha_fwd_hdim256<T>(Flash_fwd_params &, cudaStream_t) [with T=cutlass::bfloat16_t]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_hdim256_bf16_sm80.cu(8): here

/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_kernel.h(140): warning: variable "MMA_M" was declared but never referenced
          detected during:
            instantiation of "void flash::compute_attn_1rowblock<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params,Is_Q_split_K>(const Params &, int, int, int) [with Kernel_traits=Flash_fwd_kernel_traits<256, 64, 64, 4, false, false, cutlass::bfloat16_t, Flash_kernel_traits<256, 64, 64, 4, cutlass::bfloat16_t>>, Is_dropout=false, Is_causal=false, Is_even_N=false, Is_even_K=false, Return_softmax=false, Params=Flash_fwd_params, Is_Q_split_K=false]" 
(589): here
            instantiation of "void flash::compute_attn<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &) [with Kernel_traits=Flash_fwd_kernel_traits<256, 64, 64, 4, false, false, cutlass::bfloat16_t, Flash_kernel_traits<256, 64, 64, 4, cutlass::bfloat16_t>>, Is_dropout=false, Is_causal=false, Is_even_N=false, Is_even_K=false, Return_softmax=false, Params=Flash_fwd_params]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(15): here
            instantiation of "void flash_fwd_kernel<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax>(Flash_fwd_params) [with Kernel_traits=Flash_fwd_kernel_traits<256, 64, 64, 4, false, false, cutlass::bfloat16_t, Flash_kernel_traits<256, 64, 64, 4, cutlass::bfloat16_t>>, Is_dropout=false, Is_causal=false, Is_even_N=false, Is_even_K=false, Return_softmax=false]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(34): here
            instantiation of "void run_flash_fwd<Kernel_traits,Is_dropout,Is_causal>(Flash_fwd_params &, cudaStream_t) [with Kernel_traits=Flash_fwd_kernel_traits<256, 64, 64, 4, false, false, cutlass::bfloat16_t, Flash_kernel_traits<256, 64, 64, 4, cutlass::bfloat16_t>>, Is_dropout=false, Is_causal=false]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(236): here
            instantiation of "void run_mha_fwd_hdim256<T>(Flash_fwd_params &, cudaStream_t) [with T=cutlass::bfloat16_t]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_hdim256_bf16_sm80.cu(8): here

/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_kernel.h(276): warning: variable "tQrQ" was declared but never referenced
          detected during:
            instantiation of "void flash::compute_attn_1rowblock<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params,Is_Q_split_K>(const Params &, int, int, int) [with Kernel_traits=Flash_fwd_kernel_traits<256, 64, 64, 4, false, false, cutlass::bfloat16_t, Flash_kernel_traits<256, 64, 64, 4, cutlass::bfloat16_t>>, Is_dropout=false, Is_causal=false, Is_even_N=false, Is_even_K=false, Return_softmax=false, Params=Flash_fwd_params, Is_Q_split_K=false]" 
(589): here
            instantiation of "void flash::compute_attn<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &) [with Kernel_traits=Flash_fwd_kernel_traits<256, 64, 64, 4, false, false, cutlass::bfloat16_t, Flash_kernel_traits<256, 64, 64, 4, cutlass::bfloat16_t>>, Is_dropout=false, Is_causal=false, Is_even_N=false, Is_even_K=false, Return_softmax=false, Params=Flash_fwd_params]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(15): here
            instantiation of "void flash_fwd_kernel<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax>(Flash_fwd_params) [with Kernel_traits=Flash_fwd_kernel_traits<256, 64, 64, 4, false, false, cutlass::bfloat16_t, Flash_kernel_traits<256, 64, 64, 4, cutlass::bfloat16_t>>, Is_dropout=false, Is_causal=false, Is_even_N=false, Is_even_K=false, Return_softmax=false]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(34): here
            instantiation of "void run_flash_fwd<Kernel_traits,Is_dropout,Is_causal>(Flash_fwd_params &, cudaStream_t) [with Kernel_traits=Flash_fwd_kernel_traits<256, 64, 64, 4, false, false, cutlass::bfloat16_t, Flash_kernel_traits<256, 64, 64, 4, cutlass::bfloat16_t>>, Is_dropout=false, Is_causal=false]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(236): here
            instantiation of "void run_mha_fwd_hdim256<T>(Flash_fwd_params &, cudaStream_t) [with T=cutlass::bfloat16_t]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_hdim256_bf16_sm80.cu(8): here

ptxas info    : 2 bytes gmem
ptxas info    : Compiling entry function '_Z16flash_fwd_kernelI23Flash_fwd_kernel_traitsILi256ELi64ELi64ELi4ELb0ELb0EN7cutlass10bfloat16_tE19Flash_kernel_traitsILi256ELi64ELi64ELi4ES2_EELb0ELb0ELb0ELb0ELb0EEv16Flash_fwd_params' for 'sm_80'
ptxas info    : Function properties for _Z16flash_fwd_kernelI23Flash_fwd_kernel_traitsILi256ELi64ELi64ELi4ELb0ELb0EN7cutlass10bfloat16_tE19Flash_kernel_traitsILi256ELi64ELi64ELi4ES2_EELb0ELb0ELb0ELb0ELb0EEv16Flash_fwd_params
    0 bytes stack frame, 0 bytes spill stores, 0 bytes spill loads
ptxas info    : Used 241 registers, 584 bytes cmem[0]
ptxas info    : Compiling entry function '_Z16flash_fwd_kernelI23Flash_fwd_kernel_traitsILi256ELi64ELi64ELi4ELb0ELb0EN7cutlass10bfloat16_tE19Flash_kernel_traitsILi256ELi64ELi64ELi4ES2_EELb0ELb0ELb0ELb1ELb0EEv16Flash_fwd_params' for 'sm_80'
ptxas info    : Function properties for _Z16flash_fwd_kernelI23Flash_fwd_kernel_traitsILi256ELi64ELi64ELi4ELb0ELb0EN7cutlass10bfloat16_tE19Flash_kernel_traitsILi256ELi64ELi64ELi4ES2_EELb0ELb0ELb0ELb1ELb0EEv16Flash_fwd_params
    0 bytes stack frame, 0 bytes spill stores, 0 bytes spill loads
ptxas info    : Used 237 registers, 584 bytes cmem[0]
ptxas info    : Compiling entry function '_Z16flash_fwd_kernelI23Flash_fwd_kernel_traitsILi256ELi64ELi64ELi4ELb0ELb0EN7cutlass10bfloat16_tE19Flash_kernel_traitsILi256ELi64ELi64ELi4ES2_EELb0ELb0ELb1ELb0ELb0EEv16Flash_fwd_params' for 'sm_80'
ptxas info    : Function properties for _Z16flash_fwd_kernelI23Flash_fwd_kernel_traitsILi256ELi64ELi64ELi4ELb0ELb0EN7cutlass10bfloat16_tE19Flash_kernel_traitsILi256ELi64ELi64ELi4ES2_EELb0ELb0ELb1ELb0ELb0EEv16Flash_fwd_params
    0 bytes stack frame, 0 bytes spill stores, 0 bytes spill loads
ptxas info    : Used 233 registers, 584 bytes cmem[0]
ptxas info    : Compiling entry function '_Z16flash_fwd_kernelI23Flash_fwd_kernel_traitsILi256ELi64ELi64ELi4ELb0ELb0EN7cutlass10bfloat16_tE19Flash_kernel_traitsILi256ELi64ELi64ELi4ES2_EELb0ELb0ELb1ELb1ELb0EEv16Flash_fwd_params' for 'sm_80'
ptxas info    : Function properties for _Z16flash_fwd_kernelI23Flash_fwd_kernel_traitsILi256ELi64ELi64ELi4ELb0ELb0EN7cutlass10bfloat16_tE19Flash_kernel_traitsILi256ELi64ELi64ELi4ES2_EELb0ELb0ELb1ELb1ELb0EEv16Flash_fwd_params
    0 bytes stack frame, 0 bytes spill stores, 0 bytes spill loads
ptxas info    : Used 237 registers, 584 bytes cmem[0]
ptxas info    : Compiling entry function '_Z16flash_fwd_kernelI23Flash_fwd_kernel_traitsILi256ELi128ELi64ELi8ELb0ELb0EN7cutlass10bfloat16_tE19Flash_kernel_traitsILi256ELi128ELi64ELi8ES2_EELb0ELb0ELb0ELb0ELb0EEv16Flash_fwd_params' for 'sm_80'
ptxas info    : Function properties for _Z16flash_fwd_kernelI23Flash_fwd_kernel_traitsILi256ELi128ELi64ELi8ELb0ELb0EN7cutlass10bfloat16_tE19Flash_kernel_traitsILi256ELi128ELi64ELi8ES2_EELb0ELb0ELb0ELb0ELb0EEv16Flash_fwd_params
    0 bytes stack frame, 0 bytes spill stores, 0 bytes spill loads
ptxas info    : Used 241 registers, 584 bytes cmem[0]
ptxas info    : Compiling entry function '_Z16flash_fwd_kernelI23Flash_fwd_kernel_traitsILi256ELi128ELi64ELi8ELb0ELb0EN7cutlass10bfloat16_tE19Flash_kernel_traitsILi256ELi128ELi64ELi8ES2_EELb0ELb0ELb0ELb1ELb0EEv16Flash_fwd_params' for 'sm_80'
ptxas info    : Function properties for _Z16flash_fwd_kernelI23Flash_fwd_kernel_traitsILi256ELi128ELi64ELi8ELb0ELb0EN7cutlass10bfloat16_tE19Flash_kernel_traitsILi256ELi128ELi64ELi8ES2_EELb0ELb0ELb0ELb1ELb0EEv16Flash_fwd_params
    0 bytes stack frame, 0 bytes spill stores, 0 bytes spill loads
ptxas info    : Used 237 registers, 584 bytes cmem[0]
ptxas info    : Compiling entry function '_Z16flash_fwd_kernelI23Flash_fwd_kernel_traitsILi256ELi128ELi64ELi8ELb0ELb0EN7cutlass10bfloat16_tE19Flash_kernel_traitsILi256ELi128ELi64ELi8ES2_EELb0ELb0ELb1ELb0ELb0EEv16Flash_fwd_params' for 'sm_80'
ptxas info    : Function properties for _Z16flash_fwd_kernelI23Flash_fwd_kernel_traitsILi256ELi128ELi64ELi8ELb0ELb0EN7cutlass10bfloat16_tE19Flash_kernel_traitsILi256ELi128ELi64ELi8ES2_EELb0ELb0ELb1ELb0ELb0EEv16Flash_fwd_params
    0 bytes stack frame, 0 bytes spill stores, 0 bytes spill loads
ptxas info    : Used 240 registers, 584 bytes cmem[0]
ptxas info    : Compiling entry function '_Z16flash_fwd_kernelI23Flash_fwd_kernel_traitsILi256ELi128ELi64ELi8ELb0ELb0EN7cutlass10bfloat16_tE19Flash_kernel_traitsILi256ELi128ELi64ELi8ES2_EELb0ELb0ELb1ELb1ELb0EEv16Flash_fwd_params' for 'sm_80'
ptxas info    : Function properties for _Z16flash_fwd_kernelI23Flash_fwd_kernel_traitsILi256ELi128ELi64ELi8ELb0ELb0EN7cutlass10bfloat16_tE19Flash_kernel_traitsILi256ELi128ELi64ELi8ES2_EELb0ELb0ELb1ELb1ELb0EEv16Flash_fwd_params
    0 bytes stack frame, 0 bytes spill stores, 0 bytes spill loads
ptxas info    : Used 237 registers, 584 bytes cmem[0]
ptxas info    : Compiling entry function '_Z16flash_fwd_kernelI23Flash_fwd_kernel_traitsILi256ELi64ELi64ELi4ELb0ELb0EN7cutlass10bfloat16_tE19Flash_kernel_traitsILi256ELi64ELi64ELi4ES2_EELb0ELb1ELb0ELb0ELb0EEv16Flash_fwd_params' for 'sm_80'
ptxas info    : Function properties for _Z16flash_fwd_kernelI23Flash_fwd_kernel_traitsILi256ELi64ELi64ELi4ELb0ELb0EN7cutlass10bfloat16_tE19Flash_kernel_traitsILi256ELi64ELi64ELi4ES2_EELb0ELb1ELb0ELb0ELb0EEv16Flash_fwd_params
    0 bytes stack frame, 0 bytes spill stores, 0 bytes spill loads
ptxas info    : Used 241 registers, 584 bytes cmem[0]
ptxas info    : Compiling entry function '_Z16flash_fwd_kernelI23Flash_fwd_kernel_traitsILi256ELi64ELi64ELi4ELb0ELb0EN7cutlass10bfloat16_tE19Flash_kernel_traitsILi256ELi64ELi64ELi4ES2_EELb0ELb1ELb0ELb1ELb0EEv16Flash_fwd_params' for 'sm_80'
ptxas info    : Function properties for _Z16flash_fwd_kernelI23Flash_fwd_kernel_traitsILi256ELi64ELi64ELi4ELb0ELb0EN7cutlass10bfloat16_tE19Flash_kernel_traitsILi256ELi64ELi64ELi4ES2_EELb0ELb1ELb0ELb1ELb0EEv16Flash_fwd_params
    0 bytes stack frame, 0 bytes spill stores, 0 bytes spill loads
ptxas info    : Used 238 registers, 584 bytes cmem[0]
ptxas info    : Compiling entry function '_Z16flash_fwd_kernelI23Flash_fwd_kernel_traitsILi256ELi64ELi64ELi4ELb0ELb0EN7cutlass10bfloat16_tE19Flash_kernel_traitsILi256ELi64ELi64ELi4ES2_EELb0ELb1ELb1ELb0ELb0EEv16Flash_fwd_params' for 'sm_80'
ptxas info    : Function properties for _Z16flash_fwd_kernelI23Flash_fwd_kernel_traitsILi256ELi64ELi64ELi4ELb0ELb0EN7cutlass10bfloat16_tE19Flash_kernel_traitsILi256ELi64ELi64ELi4ES2_EELb0ELb1ELb1ELb0ELb0EEv16Flash_fwd_params
    0 bytes stack frame, 0 bytes spill stores, 0 bytes spill loads
ptxas info    : Used 245 registers, 584 bytes cmem[0]
ptxas info    : Compiling entry function '_Z16flash_fwd_kernelI23Flash_fwd_kernel_traitsILi256ELi64ELi64ELi4ELb0ELb0EN7cutlass10bfloat16_tE19Flash_kernel_traitsILi256ELi64ELi64ELi4ES2_EELb0ELb1ELb1ELb1ELb0EEv16Flash_fwd_params' for 'sm_80'
ptxas info    : Function properties for _Z16flash_fwd_kernelI23Flash_fwd_kernel_traitsILi256ELi64ELi64ELi4ELb0ELb0EN7cutlass10bfloat16_tE19Flash_kernel_traitsILi256ELi64ELi64ELi4ES2_EELb0ELb1ELb1ELb1ELb0EEv16Flash_fwd_params
    0 bytes stack frame, 0 bytes spill stores, 0 bytes spill loads
ptxas info    : Used 241 registers, 584 bytes cmem[0]
ptxas info    : Compiling entry function '_Z16flash_fwd_kernelI23Flash_fwd_kernel_traitsILi256ELi128ELi64ELi8ELb0ELb0EN7cutlass10bfloat16_tE19Flash_kernel_traitsILi256ELi128ELi64ELi8ES2_EELb0ELb1ELb0ELb0ELb0EEv16Flash_fwd_params' for 'sm_80'
ptxas info    : Function properties for _Z16flash_fwd_kernelI23Flash_fwd_kernel_traitsILi256ELi128ELi64ELi8ELb0ELb0EN7cutlass10bfloat16_tE19Flash_kernel_traitsILi256ELi128ELi64ELi8ES2_EELb0ELb1ELb0ELb0ELb0EEv16Flash_fwd_params
    0 bytes stack frame, 0 bytes spill stores, 0 bytes spill loads
ptxas info    : Used 248 registers, 584 bytes cmem[0]
ptxas info    : Compiling entry function '_Z16flash_fwd_kernelI23Flash_fwd_kernel_traitsILi256ELi128ELi64ELi8ELb0ELb0EN7cutlass10bfloat16_tE19Flash_kernel_traitsILi256ELi128ELi64ELi8ES2_EELb0ELb1ELb0ELb1ELb0EEv16Flash_fwd_params' for 'sm_80'
ptxas info    : Function properties for _Z16flash_fwd_kernelI23Flash_fwd_kernel_traitsILi256ELi128ELi64ELi8ELb0ELb0EN7cutlass10bfloat16_tE19Flash_kernel_traitsILi256ELi128ELi64ELi8ES2_EELb0ELb1ELb0ELb1ELb0EEv16Flash_fwd_params
    0 bytes stack frame, 0 bytes spill stores, 0 bytes spill loads
ptxas info    : Used 245 registers, 584 bytes cmem[0]
ptxas info    : Compiling entry function '_Z16flash_fwd_kernelI23Flash_fwd_kernel_traitsILi256ELi128ELi64ELi8ELb0ELb0EN7cutlass10bfloat16_tE19Flash_kernel_traitsILi256ELi128ELi64ELi8ES2_EELb0ELb1ELb1ELb0ELb0EEv16Flash_fwd_params' for 'sm_80'
ptxas info    : Function properties for _Z16flash_fwd_kernelI23Flash_fwd_kernel_traitsILi256ELi128ELi64ELi8ELb0ELb0EN7cutlass10bfloat16_tE19Flash_kernel_traitsILi256ELi128ELi64ELi8ES2_EELb0ELb1ELb1ELb0ELb0EEv16Flash_fwd_params
    0 bytes stack frame, 0 bytes spill stores, 0 bytes spill loads
ptxas info    : Used 248 registers, 584 bytes cmem[0]
ptxas info    : Compiling entry function '_Z16flash_fwd_kernelI23Flash_fwd_kernel_traitsILi256ELi128ELi64ELi8ELb0ELb0EN7cutlass10bfloat16_tE19Flash_kernel_traitsILi256ELi128ELi64ELi8ES2_EELb0ELb1ELb1ELb1ELb0EEv16Flash_fwd_params' for 'sm_80'
ptxas info    : Function properties for _Z16flash_fwd_kernelI23Flash_fwd_kernel_traitsILi256ELi128ELi64ELi8ELb0ELb0EN7cutlass10bfloat16_tE19Flash_kernel_traitsILi256ELi128ELi64ELi8ES2_EELb0ELb1ELb1ELb1ELb0EEv16Flash_fwd_params
    0 bytes stack frame, 0 bytes spill stores, 0 bytes spill loads
ptxas info    : Used 250 registers, 584 bytes cmem[0]
ptxas info    : Compiling entry function '_Z16flash_fwd_kernelI23Flash_fwd_kernel_traitsILi256ELi64ELi64ELi4ELb0ELb0EN7cutlass10bfloat16_tE19Flash_kernel_traitsILi256ELi64ELi64ELi4ES2_EELb1ELb0ELb0ELb0ELb0EEv16Flash_fwd_params' for 'sm_80'
ptxas info    : Function properties for _Z16flash_fwd_kernelI23Flash_fwd_kernel_traitsILi256ELi64ELi64ELi4ELb0ELb0EN7cutlass10bfloat16_tE19Flash_kernel_traitsILi256ELi64ELi64ELi4ES2_EELb1ELb0ELb0ELb0ELb0EEv16Flash_fwd_params
    0 bytes stack frame, 0 bytes spill stores, 0 bytes spill loads
ptxas info    : Used 246 registers, 584 bytes cmem[0]
ptxas info    : Compiling entry function '_Z16flash_fwd_kernelI23Flash_fwd_kernel_traitsILi256ELi64ELi64ELi4ELb0ELb0EN7cutlass10bfloat16_tE19Flash_kernel_traitsILi256ELi64ELi64ELi4ES2_EELb1ELb0ELb0ELb0ELb1EEv16Flash_fwd_params' for 'sm_80'
ptxas info    : Function properties for _Z16flash_fwd_kernelI23Flash_fwd_kernel_traitsILi256ELi64ELi64ELi4ELb0ELb0EN7cutlass10bfloat16_tE19Flash_kernel_traitsILi256ELi64ELi64ELi4ES2_EELb1ELb0ELb0ELb0ELb1EEv16Flash_fwd_params
    24 bytes stack frame, 28 bytes spill stores, 28 bytes spill loads
ptxas info    : Used 255 registers, 584 bytes cmem[0]
ptxas info    : Compiling entry function '_Z16flash_fwd_kernelI23Flash_fwd_kernel_traitsILi256ELi64ELi64ELi4ELb0ELb0EN7cutlass10bfloat16_tE19Flash_kernel_traitsILi256ELi64ELi64ELi4ES2_EELb1ELb0ELb0ELb1ELb0EEv16Flash_fwd_params' for 'sm_80'
ptxas info    : Function properties for _Z16flash_fwd_kernelI23Flash_fwd_kernel_traitsILi256ELi64ELi64ELi4ELb0ELb0EN7cutlass10bfloat16_tE19Flash_kernel_traitsILi256ELi64ELi64ELi4ES2_EELb1ELb0ELb0ELb1ELb0EEv16Flash_fwd_params
    0 bytes stack frame, 0 bytes spill stores, 0 bytes spill loads
ptxas info    : Used 242 registers, 584 bytes cmem[0]
ptxas info    : Compiling entry function '_Z16flash_fwd_kernelI23Flash_fwd_kernel_traitsILi256ELi64ELi64ELi4ELb0ELb0EN7cutlass10bfloat16_tE19Flash_kernel_traitsILi256ELi64ELi64ELi4ES2_EELb1ELb0ELb0ELb1ELb1EEv16Flash_fwd_params' for 'sm_80'
ptxas info    : Function properties for _Z16flash_fwd_kernelI23Flash_fwd_kernel_traitsILi256ELi64ELi64ELi4ELb0ELb0EN7cutlass10bfloat16_tE19Flash_kernel_traitsILi256ELi64ELi64ELi4ES2_EELb1ELb0ELb0ELb1ELb1EEv16Flash_fwd_params
    16 bytes stack frame, 12 bytes spill stores, 12 bytes spill loads
ptxas info    : Used 255 registers, 584 bytes cmem[0]
ptxas info    : Compiling entry function '_Z16flash_fwd_kernelI23Flash_fwd_kernel_traitsILi256ELi64ELi64ELi4ELb0ELb0EN7cutlass10bfloat16_tE19Flash_kernel_traitsILi256ELi64ELi64ELi4ES2_EELb1ELb0ELb1ELb0ELb0EEv16Flash_fwd_params' for 'sm_80'
ptxas info    : Function properties for _Z16flash_fwd_kernelI23Flash_fwd_kernel_traitsILi256ELi64ELi64ELi4ELb0ELb0EN7cutlass10bfloat16_tE19Flash_kernel_traitsILi256ELi64ELi64ELi4ES2_EELb1ELb0ELb1ELb0ELb0EEv16Flash_fwd_params
    0 bytes stack frame, 0 bytes spill stores, 0 bytes spill loads
ptxas info    : Used 246 registers, 584 bytes cmem[0]
ptxas info    : Compiling entry function '_Z16flash_fwd_kernelI23Flash_fwd_kernel_traitsILi256ELi64ELi64ELi4ELb0ELb0EN7cutlass10bfloat16_tE19Flash_kernel_traitsILi256ELi64ELi64ELi4ES2_EELb1ELb0ELb1ELb0ELb1EEv16Flash_fwd_params' for 'sm_80'
ptxas info    : Function properties for _Z16flash_fwd_kernelI23Flash_fwd_kernel_traitsILi256ELi64ELi64ELi4ELb0ELb0EN7cutlass10bfloat16_tE19Flash_kernel_traitsILi256ELi64ELi64ELi4ES2_EELb1ELb0ELb1ELb0ELb1EEv16Flash_fwd_params
    16 bytes stack frame, 20 bytes spill stores, 20 bytes spill loads
ptxas info    : Used 255 registers, 584 bytes cmem[0]
ptxas info    : Compiling entry function '_Z16flash_fwd_kernelI23Flash_fwd_kernel_traitsILi256ELi64ELi64ELi4ELb0ELb0EN7cutlass10bfloat16_tE19Flash_kernel_traitsILi256ELi64ELi64ELi4ES2_EELb1ELb0ELb1ELb1ELb0EEv16Flash_fwd_params' for 'sm_80'
ptxas info    : Function properties for _Z16flash_fwd_kernelI23Flash_fwd_kernel_traitsILi256ELi64ELi64ELi4ELb0ELb0EN7cutlass10bfloat16_tE19Flash_kernel_traitsILi256ELi64ELi64ELi4ES2_EELb1ELb0ELb1ELb1ELb0EEv16Flash_fwd_params
    0 bytes stack frame, 0 bytes spill stores, 0 bytes spill loads
ptxas info    : Used 242 registers, 584 bytes cmem[0]
ptxas info    : Compiling entry function '_Z16flash_fwd_kernelI23Flash_fwd_kernel_traitsILi256ELi64ELi64ELi4ELb0ELb0EN7cutlass10bfloat16_tE19Flash_kernel_traitsILi256ELi64ELi64ELi4ES2_EELb1ELb0ELb1ELb1ELb1EEv16Flash_fwd_params' for 'sm_80'
ptxas info    : Function properties for _Z16flash_fwd_kernelI23Flash_fwd_kernel_traitsILi256ELi64ELi64ELi4ELb0ELb0EN7cutlass10bfloat16_tE19Flash_kernel_traitsILi256ELi64ELi64ELi4ES2_EELb1ELb0ELb1ELb1ELb1EEv16Flash_fwd_params
    0 bytes stack frame, 0 bytes spill stores, 0 bytes spill loads
ptxas info    : Used 254 registers, 584 bytes cmem[0]
ptxas info    : Compiling entry function '_Z16flash_fwd_kernelI23Flash_fwd_kernel_traitsILi256ELi128ELi64ELi8ELb0ELb0EN7cutlass10bfloat16_tE19Flash_kernel_traitsILi256ELi128ELi64ELi8ES2_EELb1ELb0ELb0ELb0ELb0EEv16Flash_fwd_params' for 'sm_80'
ptxas info    : Function properties for _Z16flash_fwd_kernelI23Flash_fwd_kernel_traitsILi256ELi128ELi64ELi8ELb0ELb0EN7cutlass10bfloat16_tE19Flash_kernel_traitsILi256ELi128ELi64ELi8ES2_EELb1ELb0ELb0ELb0ELb0EEv16Flash_fwd_params
    0 bytes stack frame, 0 bytes spill stores, 0 bytes spill loads
ptxas info    : Used 246 registers, 584 bytes cmem[0]
ptxas info    : Compiling entry function '_Z16flash_fwd_kernelI23Flash_fwd_kernel_traitsILi256ELi128ELi64ELi8ELb0ELb0EN7cutlass10bfloat16_tE19Flash_kernel_traitsILi256ELi128ELi64ELi8ES2_EELb1ELb0ELb0ELb0ELb1EEv16Flash_fwd_params' for 'sm_80'
ptxas info    : Function properties for _Z16flash_fwd_kernelI23Flash_fwd_kernel_traitsILi256ELi128ELi64ELi8ELb0ELb0EN7cutlass10bfloat16_tE19Flash_kernel_traitsILi256ELi128ELi64ELi8ES2_EELb1ELb0ELb0ELb0ELb1EEv16Flash_fwd_params
    8 bytes stack frame, 8 bytes spill stores, 8 bytes spill loads
ptxas info    : Used 255 registers, 584 bytes cmem[0]
ptxas info    : Compiling entry function '_Z16flash_fwd_kernelI23Flash_fwd_kernel_traitsILi256ELi128ELi64ELi8ELb0ELb0EN7cutlass10bfloat16_tE19Flash_kernel_traitsILi256ELi128ELi64ELi8ES2_EELb1ELb0ELb0ELb1ELb0EEv16Flash_fwd_params' for 'sm_80'
ptxas info    : Function properties for _Z16flash_fwd_kernelI23Flash_fwd_kernel_traitsILi256ELi128ELi64ELi8ELb0ELb0EN7cutlass10bfloat16_tE19Flash_kernel_traitsILi256ELi128ELi64ELi8ES2_EELb1ELb0ELb0ELb1ELb0EEv16Flash_fwd_params
    0 bytes stack frame, 0 bytes spill stores, 0 bytes spill loads
ptxas info    : Used 242 registers, 584 bytes cmem[0]
ptxas info    : Compiling entry function '_Z16flash_fwd_kernelI23Flash_fwd_kernel_traitsILi256ELi128ELi64ELi8ELb0ELb0EN7cutlass10bfloat16_tE19Flash_kernel_traitsILi256ELi128ELi64ELi8ES2_EELb1ELb0ELb0ELb1ELb1EEv16Flash_fwd_params' for 'sm_80'
ptxas info    : Function properties for _Z16flash_fwd_kernelI23Flash_fwd_kernel_traitsILi256ELi128ELi64ELi8ELb0ELb0EN7cutlass10bfloat16_tE19Flash_kernel_traitsILi256ELi128ELi64ELi8ES2_EELb1ELb0ELb0ELb1ELb1EEv16Flash_fwd_params
    8 bytes stack frame, 4 bytes spill stores, 4 bytes spill loads
ptxas info    : Used 255 registers, 584 bytes cmem[0]
ptxas info    : Compiling entry function '_Z16flash_fwd_kernelI23Flash_fwd_kernel_traitsILi256ELi128ELi64ELi8ELb0ELb0EN7cutlass10bfloat16_tE19Flash_kernel_traitsILi256ELi128ELi64ELi8ES2_EELb1ELb0ELb1ELb0ELb0EEv16Flash_fwd_params' for 'sm_80'
ptxas info    : Function properties for _Z16flash_fwd_kernelI23Flash_fwd_kernel_traitsILi256ELi128ELi64ELi8ELb0ELb0EN7cutlass10bfloat16_tE19Flash_kernel_traitsILi256ELi128ELi64ELi8ES2_EELb1ELb0ELb1ELb0ELb0EEv16Flash_fwd_params
    0 bytes stack frame, 0 bytes spill stores, 0 bytes spill loads
ptxas info    : Used 244 registers, 584 bytes cmem[0]
ptxas info    : Compiling entry function '_Z16flash_fwd_kernelI23Flash_fwd_kernel_traitsILi256ELi128ELi64ELi8ELb0ELb0EN7cutlass10bfloat16_tE19Flash_kernel_traitsILi256ELi128ELi64ELi8ES2_EELb1ELb0ELb1ELb0ELb1EEv16Flash_fwd_params' for 'sm_80'
ptxas info    : Function properties for _Z16flash_fwd_kernelI23Flash_fwd_kernel_traitsILi256ELi128ELi64ELi8ELb0ELb0EN7cutlass10bfloat16_tE19Flash_kernel_traitsILi256ELi128ELi64ELi8ES2_EELb1ELb0ELb1ELb0ELb1EEv16Flash_fwd_params
    0 bytes stack frame, 0 bytes spill stores, 0 bytes spill loads
ptxas info    : Used 255 registers, 584 bytes cmem[0]
ptxas info    : Compiling entry function '_Z16flash_fwd_kernelI23Flash_fwd_kernel_traitsILi256ELi128ELi64ELi8ELb0ELb0EN7cutlass10bfloat16_tE19Flash_kernel_traitsILi256ELi128ELi64ELi8ES2_EELb1ELb0ELb1ELb1ELb0EEv16Flash_fwd_params' for 'sm_80'
ptxas info    : Function properties for _Z16flash_fwd_kernelI23Flash_fwd_kernel_traitsILi256ELi128ELi64ELi8ELb0ELb0EN7cutlass10bfloat16_tE19Flash_kernel_traitsILi256ELi128ELi64ELi8ES2_EELb1ELb0ELb1ELb1ELb0EEv16Flash_fwd_params
    0 bytes stack frame, 0 bytes spill stores, 0 bytes spill loads
ptxas info    : Used 242 registers, 584 bytes cmem[0]
ptxas info    : Compiling entry function '_Z16flash_fwd_kernelI23Flash_fwd_kernel_traitsILi256ELi128ELi64ELi8ELb0ELb0EN7cutlass10bfloat16_tE19Flash_kernel_traitsILi256ELi128ELi64ELi8ES2_EELb1ELb0ELb1ELb1ELb1EEv16Flash_fwd_params' for 'sm_80'
ptxas info    : Function properties for _Z16flash_fwd_kernelI23Flash_fwd_kernel_traitsILi256ELi128ELi64ELi8ELb0ELb0EN7cutlass10bfloat16_tE19Flash_kernel_traitsILi256ELi128ELi64ELi8ES2_EELb1ELb0ELb1ELb1ELb1EEv16Flash_fwd_params
    0 bytes stack frame, 0 bytes spill stores, 0 bytes spill loads
ptxas info    : Used 254 registers, 584 bytes cmem[0]
ptxas info    : Compiling entry function '_Z16flash_fwd_kernelI23Flash_fwd_kernel_traitsILi256ELi64ELi64ELi4ELb0ELb0EN7cutlass10bfloat16_tE19Flash_kernel_traitsILi256ELi64ELi64ELi4ES2_EELb1ELb1ELb0ELb0ELb0EEv16Flash_fwd_params' for 'sm_80'
ptxas info    : Function properties for _Z16flash_fwd_kernelI23Flash_fwd_kernel_traitsILi256ELi64ELi64ELi4ELb0ELb0EN7cutlass10bfloat16_tE19Flash_kernel_traitsILi256ELi64ELi64ELi4ES2_EELb1ELb1ELb0ELb0ELb0EEv16Flash_fwd_params
    0 bytes stack frame, 0 bytes spill stores, 0 bytes spill loads
ptxas info    : Used 246 registers, 584 bytes cmem[0]
ptxas info    : Compiling entry function '_Z16flash_fwd_kernelI23Flash_fwd_kernel_traitsILi256ELi64ELi64ELi4ELb0ELb0EN7cutlass10bfloat16_tE19Flash_kernel_traitsILi256ELi64ELi64ELi4ES2_EELb1ELb1ELb0ELb0ELb1EEv16Flash_fwd_params' for 'sm_80'
ptxas info    : Function properties for _Z16flash_fwd_kernelI23Flash_fwd_kernel_traitsILi256ELi64ELi64ELi4ELb0ELb0EN7cutlass10bfloat16_tE19Flash_kernel_traitsILi256ELi64ELi64ELi4ES2_EELb1ELb1ELb0ELb0ELb1EEv16Flash_fwd_params
    16 bytes stack frame, 16 bytes spill stores, 16 bytes spill loads
ptxas info    : Used 254 registers, 584 bytes cmem[0]
ptxas info    : Compiling entry function '_Z16flash_fwd_kernelI23Flash_fwd_kernel_traitsILi256ELi64ELi64ELi4ELb0ELb0EN7cutlass10bfloat16_tE19Flash_kernel_traitsILi256ELi64ELi64ELi4ES2_EELb1ELb1ELb0ELb1ELb0EEv16Flash_fwd_params' for 'sm_80'
ptxas info    : Function properties for _Z16flash_fwd_kernelI23Flash_fwd_kernel_traitsILi256ELi64ELi64ELi4ELb0ELb0EN7cutlass10bfloat16_tE19Flash_kernel_traitsILi256ELi64ELi64ELi4ES2_EELb1ELb1ELb0ELb1ELb0EEv16Flash_fwd_params
    0 bytes stack frame, 0 bytes spill stores, 0 bytes spill loads
ptxas info    : Used 248 registers, 584 bytes cmem[0]
ptxas info    : Compiling entry function '_Z16flash_fwd_kernelI23Flash_fwd_kernel_traitsILi256ELi64ELi64ELi4ELb0ELb0EN7cutlass10bfloat16_tE19Flash_kernel_traitsILi256ELi64ELi64ELi4ES2_EELb1ELb1ELb0ELb1ELb1EEv16Flash_fwd_params' for 'sm_80'
ptxas info    : Function properties for _Z16flash_fwd_kernelI23Flash_fwd_kernel_traitsILi256ELi64ELi64ELi4ELb0ELb0EN7cutlass10bfloat16_tE19Flash_kernel_traitsILi256ELi64ELi64ELi4ES2_EELb1ELb1ELb0ELb1ELb1EEv16Flash_fwd_params
    16 bytes stack frame, 12 bytes spill stores, 12 bytes spill loads
ptxas info    : Used 255 registers, 584 bytes cmem[0]
ptxas info    : Compiling entry function '_Z16flash_fwd_kernelI23Flash_fwd_kernel_traitsILi256ELi64ELi64ELi4ELb0ELb0EN7cutlass10bfloat16_tE19Flash_kernel_traitsILi256ELi64ELi64ELi4ES2_EELb1ELb1ELb1ELb0ELb0EEv16Flash_fwd_params' for 'sm_80'
ptxas info    : Function properties for _Z16flash_fwd_kernelI23Flash_fwd_kernel_traitsILi256ELi64ELi64ELi4ELb0ELb0EN7cutlass10bfloat16_tE19Flash_kernel_traitsILi256ELi64ELi64ELi4ES2_EELb1ELb1ELb1ELb0ELb0EEv16Flash_fwd_params
    0 bytes stack frame, 0 bytes spill stores, 0 bytes spill loads
ptxas info    : Used 254 registers, 584 bytes cmem[0]
ptxas info    : Compiling entry function '_Z16flash_fwd_kernelI23Flash_fwd_kernel_traitsILi256ELi64ELi64ELi4ELb0ELb0EN7cutlass10bfloat16_tE19Flash_kernel_traitsILi256ELi64ELi64ELi4ES2_EELb1ELb1ELb1ELb0ELb1EEv16Flash_fwd_params' for 'sm_80'
ptxas info    : Function properties for _Z16flash_fwd_kernelI23Flash_fwd_kernel_traitsILi256ELi64ELi64ELi4ELb0ELb0EN7cutlass10bfloat16_tE19Flash_kernel_traitsILi256ELi64ELi64ELi4ES2_EELb1ELb1ELb1ELb0ELb1EEv16Flash_fwd_params
    16 bytes stack frame, 16 bytes spill stores, 16 bytes spill loads
ptxas info    : Used 255 registers, 584 bytes cmem[0]
ptxas info    : Compiling entry function '_Z16flash_fwd_kernelI23Flash_fwd_kernel_traitsILi256ELi64ELi64ELi4ELb0ELb0EN7cutlass10bfloat16_tE19Flash_kernel_traitsILi256ELi64ELi64ELi4ES2_EELb1ELb1ELb1ELb1ELb0EEv16Flash_fwd_params' for 'sm_80'
ptxas info    : Function properties for _Z16flash_fwd_kernelI23Flash_fwd_kernel_traitsILi256ELi64ELi64ELi4ELb0ELb0EN7cutlass10bfloat16_tE19Flash_kernel_traitsILi256ELi64ELi64ELi4ES2_EELb1ELb1ELb1ELb1ELb0EEv16Flash_fwd_params
    0 bytes stack frame, 0 bytes spill stores, 0 bytes spill loads
ptxas info    : Used 254 registers, 584 bytes cmem[0]
ptxas info    : Compiling entry function '_Z16flash_fwd_kernelI23Flash_fwd_kernel_traitsILi256ELi64ELi64ELi4ELb0ELb0EN7cutlass10bfloat16_tE19Flash_kernel_traitsILi256ELi64ELi64ELi4ES2_EELb1ELb1ELb1ELb1ELb1EEv16Flash_fwd_params' for 'sm_80'
ptxas info    : Function properties for _Z16flash_fwd_kernelI23Flash_fwd_kernel_traitsILi256ELi64ELi64ELi4ELb0ELb0EN7cutlass10bfloat16_tE19Flash_kernel_traitsILi256ELi64ELi64ELi4ES2_EELb1ELb1ELb1ELb1ELb1EEv16Flash_fwd_params
    8 bytes stack frame, 4 bytes spill stores, 4 bytes spill loads
ptxas info    : Used 255 registers, 584 bytes cmem[0]
ptxas info    : Compiling entry function '_Z16flash_fwd_kernelI23Flash_fwd_kernel_traitsILi256ELi128ELi64ELi8ELb0ELb0EN7cutlass10bfloat16_tE19Flash_kernel_traitsILi256ELi128ELi64ELi8ES2_EELb1ELb1ELb0ELb0ELb0EEv16Flash_fwd_params' for 'sm_80'
ptxas info    : Function properties for _Z16flash_fwd_kernelI23Flash_fwd_kernel_traitsILi256ELi128ELi64ELi8ELb0ELb0EN7cutlass10bfloat16_tE19Flash_kernel_traitsILi256ELi128ELi64ELi8ES2_EELb1ELb1ELb0ELb0ELb0EEv16Flash_fwd_params
    0 bytes stack frame, 0 bytes spill stores, 0 bytes spill loads
ptxas info    : Used 255 registers, 584 bytes cmem[0]
ptxas info    : Compiling entry function '_Z16flash_fwd_kernelI23Flash_fwd_kernel_traitsILi256ELi128ELi64ELi8ELb0ELb0EN7cutlass10bfloat16_tE19Flash_kernel_traitsILi256ELi128ELi64ELi8ES2_EELb1ELb1ELb0ELb0ELb1EEv16Flash_fwd_params' for 'sm_80'
ptxas info    : Function properties for _Z16flash_fwd_kernelI23Flash_fwd_kernel_traitsILi256ELi128ELi64ELi8ELb0ELb0EN7cutlass10bfloat16_tE19Flash_kernel_traitsILi256ELi128ELi64ELi8ES2_EELb1ELb1ELb0ELb0ELb1EEv16Flash_fwd_params
    144 bytes stack frame, 184 bytes spill stores, 188 bytes spill loads
ptxas info    : Used 255 registers, 584 bytes cmem[0]
ptxas info    : Compiling entry function '_Z16flash_fwd_kernelI23Flash_fwd_kernel_traitsILi256ELi128ELi64ELi8ELb0ELb0EN7cutlass10bfloat16_tE19Flash_kernel_traitsILi256ELi128ELi64ELi8ES2_EELb1ELb1ELb0ELb1ELb0EEv16Flash_fwd_params' for 'sm_80'
ptxas info    : Function properties for _Z16flash_fwd_kernelI23Flash_fwd_kernel_traitsILi256ELi128ELi64ELi8ELb0ELb0EN7cutlass10bfloat16_tE19Flash_kernel_traitsILi256ELi128ELi64ELi8ES2_EELb1ELb1ELb0ELb1ELb0EEv16Flash_fwd_params
    8 bytes stack frame, 4 bytes spill stores, 4 bytes spill loads
ptxas info    : Used 255 registers, 584 bytes cmem[0]
ptxas info    : Compiling entry function '_Z16flash_fwd_kernelI23Flash_fwd_kernel_traitsILi256ELi128ELi64ELi8ELb0ELb0EN7cutlass10bfloat16_tE19Flash_kernel_traitsILi256ELi128ELi64ELi8ES2_EELb1ELb1ELb0ELb1ELb1EEv16Flash_fwd_params' for 'sm_80'
ptxas info    : Function properties for _Z16flash_fwd_kernelI23Flash_fwd_kernel_traitsILi256ELi128ELi64ELi8ELb0ELb0EN7cutlass10bfloat16_tE19Flash_kernel_traitsILi256ELi128ELi64ELi8ES2_EELb1ELb1ELb0ELb1ELb1EEv16Flash_fwd_params
    8 bytes stack frame, 8 bytes spill stores, 8 bytes spill loads
ptxas info    : Used 255 registers, 584 bytes cmem[0]
ptxas info    : Compiling entry function '_Z16flash_fwd_kernelI23Flash_fwd_kernel_traitsILi256ELi128ELi64ELi8ELb0ELb0EN7cutlass10bfloat16_tE19Flash_kernel_traitsILi256ELi128ELi64ELi8ES2_EELb1ELb1ELb1ELb0ELb0EEv16Flash_fwd_params' for 'sm_80'
ptxas info    : Function properties for _Z16flash_fwd_kernelI23Flash_fwd_kernel_traitsILi256ELi128ELi64ELi8ELb0ELb0EN7cutlass10bfloat16_tE19Flash_kernel_traitsILi256ELi128ELi64ELi8ES2_EELb1ELb1ELb1ELb0ELb0EEv16Flash_fwd_params
    0 bytes stack frame, 0 bytes spill stores, 0 bytes spill loads
ptxas info    : Used 253 registers, 584 bytes cmem[0]
ptxas info    : Compiling entry function '_Z16flash_fwd_kernelI23Flash_fwd_kernel_traitsILi256ELi128ELi64ELi8ELb0ELb0EN7cutlass10bfloat16_tE19Flash_kernel_traitsILi256ELi128ELi64ELi8ES2_EELb1ELb1ELb1ELb0ELb1EEv16Flash_fwd_params' for 'sm_80'
ptxas info    : Function properties for _Z16flash_fwd_kernelI23Flash_fwd_kernel_traitsILi256ELi128ELi64ELi8ELb0ELb0EN7cutlass10bfloat16_tE19Flash_kernel_traitsILi256ELi128ELi64ELi8ES2_EELb1ELb1ELb1ELb0ELb1EEv16Flash_fwd_params
    48 bytes stack frame, 72 bytes spill stores, 72 bytes spill loads
ptxas info    : Used 255 registers, 584 bytes cmem[0]
ptxas info    : Compiling entry function '_Z16flash_fwd_kernelI23Flash_fwd_kernel_traitsILi256ELi128ELi64ELi8ELb0ELb0EN7cutlass10bfloat16_tE19Flash_kernel_traitsILi256ELi128ELi64ELi8ES2_EELb1ELb1ELb1ELb1ELb0EEv16Flash_fwd_params' for 'sm_80'
ptxas info    : Function properties for _Z16flash_fwd_kernelI23Flash_fwd_kernel_traitsILi256ELi128ELi64ELi8ELb0ELb0EN7cutlass10bfloat16_tE19Flash_kernel_traitsILi256ELi128ELi64ELi8ES2_EELb1ELb1ELb1ELb1ELb0EEv16Flash_fwd_params
    8 bytes stack frame, 4 bytes spill stores, 4 bytes spill loads
ptxas info    : Used 255 registers, 584 bytes cmem[0]
ptxas info    : Compiling entry function '_Z16flash_fwd_kernelI23Flash_fwd_kernel_traitsILi256ELi128ELi64ELi8ELb0ELb0EN7cutlass10bfloat16_tE19Flash_kernel_traitsILi256ELi128ELi64ELi8ES2_EELb1ELb1ELb1ELb1ELb1EEv16Flash_fwd_params' for 'sm_80'
ptxas info    : Function properties for _Z16flash_fwd_kernelI23Flash_fwd_kernel_traitsILi256ELi128ELi64ELi8ELb0ELb0EN7cutlass10bfloat16_tE19Flash_kernel_traitsILi256ELi128ELi64ELi8ES2_EELb1ELb1ELb1ELb1ELb1EEv16Flash_fwd_params
    24 bytes stack frame, 24 bytes spill stores, 20 bytes spill loads
ptxas info    : Used 255 registers, 584 bytes cmem[0]
/anaconda/envs/flashattention/lib/python3.9/site-packages/torch/include/c10/core/SymInt.h(84): warning: integer conversion resulted in a change of sign

[35/37] /usr/local/cuda-11.4/bin/nvcc  -I/home/v-feiychen/flash-attention/csrc/flash_attn -I/home/v-feiychen/flash-attention/csrc/flash_attn/src -I/home/v-feiychen/flash-attention/csrc/cutlass/include -I/anaconda/envs/flashattention/lib/python3.9/site-packages/torch/include -I/anaconda/envs/flashattention/lib/python3.9/site-packages/torch/include/torch/csrc/api/include -I/anaconda/envs/flashattention/lib/python3.9/site-packages/torch/include/TH -I/anaconda/envs/flashattention/lib/python3.9/site-packages/torch/include/THC -I/usr/local/cuda-11.4/include -I/anaconda/envs/flashattention/include/python3.9 -c -c /home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_hdim256_fp16_sm80.cu -o /home/v-feiychen/flash-attention/build/temp.linux-x86_64-cpython-39/csrc/flash_attn/src/flash_fwd_hdim256_fp16_sm80.o -D__CUDA_NO_HALF_OPERATORS__ -D__CUDA_NO_HALF_CONVERSIONS__ -D__CUDA_NO_BFLOAT16_CONVERSIONS__ -D__CUDA_NO_HALF2_OPERATORS__ --expt-relaxed-constexpr --compiler-options ''"'"'-fPIC'"'"'' -O3 -std=c++17 -U__CUDA_NO_HALF_OPERATORS__ -U__CUDA_NO_HALF_CONVERSIONS__ -U__CUDA_NO_HALF2_OPERATORS__ -U__CUDA_NO_BFLOAT16_CONVERSIONS__ --expt-relaxed-constexpr --expt-extended-lambda --use_fast_math --ptxas-options=-v -lineinfo -gencode arch=compute_80,code=sm_80 --threads 4 -DTORCH_API_INCLUDE_EXTENSION_H '-DPYBIND11_COMPILER_TYPE="_gcc"' '-DPYBIND11_STDLIB="_libstdcpp"' '-DPYBIND11_BUILD_ABI="_cxxabi1011"' -DTORCH_EXTENSION_NAME=flash_attn_2_cuda -D_GLIBCXX_USE_CXX11_ABI=0
/anaconda/envs/flashattention/lib/python3.9/site-packages/torch/include/c10/core/SymInt.h(84): warning: integer conversion resulted in a change of sign

/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_kernel.h(140): warning: variable "MMA_M" was declared but never referenced
          detected during:
            instantiation of "void flash::compute_attn_1rowblock<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params,Is_Q_split_K>(const Params &, int, int, int) [with Kernel_traits=Flash_fwd_kernel_traits<256, 128, 64, 8, false, false, cutlass::half_t, Flash_kernel_traits<256, 128, 64, 8, cutlass::half_t>>, Is_dropout=true, Is_causal=true, Is_even_N=true, Is_even_K=true, Return_softmax=true, Params=Flash_fwd_params, Is_Q_split_K=false]" 
(589): here
            instantiation of "void flash::compute_attn<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &) [with Kernel_traits=Flash_fwd_kernel_traits<256, 128, 64, 8, false, false, cutlass::half_t, Flash_kernel_traits<256, 128, 64, 8, cutlass::half_t>>, Is_dropout=true, Is_causal=true, Is_even_N=true, Is_even_K=true, Return_softmax=true, Params=Flash_fwd_params]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(15): here
            instantiation of "void flash_fwd_kernel<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax>(Flash_fwd_params) [with Kernel_traits=Flash_fwd_kernel_traits<256, 128, 64, 8, false, false, cutlass::half_t, Flash_kernel_traits<256, 128, 64, 8, cutlass::half_t>>, Is_dropout=true, Is_causal=true, Is_even_N=true, Is_even_K=true, Return_softmax=true]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(34): here
            instantiation of "void run_flash_fwd<Kernel_traits,Is_dropout,Is_causal>(Flash_fwd_params &, cudaStream_t) [with Kernel_traits=Flash_fwd_kernel_traits<256, 128, 64, 8, false, false, cutlass::half_t, Flash_kernel_traits<256, 128, 64, 8, cutlass::half_t>>, Is_dropout=true, Is_causal=true]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(236): here
            instantiation of "void run_mha_fwd_hdim256<T>(Flash_fwd_params &, cudaStream_t) [with T=cutlass::half_t]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_hdim256_fp16_sm80.cu(8): here

/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_kernel.h(276): warning: variable "tQrQ" was declared but never referenced
          detected during:
            instantiation of "void flash::compute_attn_1rowblock<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params,Is_Q_split_K>(const Params &, int, int, int) [with Kernel_traits=Flash_fwd_kernel_traits<256, 128, 64, 8, false, false, cutlass::half_t, Flash_kernel_traits<256, 128, 64, 8, cutlass::half_t>>, Is_dropout=true, Is_causal=true, Is_even_N=true, Is_even_K=true, Return_softmax=true, Params=Flash_fwd_params, Is_Q_split_K=false]" 
(589): here
            instantiation of "void flash::compute_attn<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &) [with Kernel_traits=Flash_fwd_kernel_traits<256, 128, 64, 8, false, false, cutlass::half_t, Flash_kernel_traits<256, 128, 64, 8, cutlass::half_t>>, Is_dropout=true, Is_causal=true, Is_even_N=true, Is_even_K=true, Return_softmax=true, Params=Flash_fwd_params]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(15): here
            instantiation of "void flash_fwd_kernel<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax>(Flash_fwd_params) [with Kernel_traits=Flash_fwd_kernel_traits<256, 128, 64, 8, false, false, cutlass::half_t, Flash_kernel_traits<256, 128, 64, 8, cutlass::half_t>>, Is_dropout=true, Is_causal=true, Is_even_N=true, Is_even_K=true, Return_softmax=true]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(34): here
            instantiation of "void run_flash_fwd<Kernel_traits,Is_dropout,Is_causal>(Flash_fwd_params &, cudaStream_t) [with Kernel_traits=Flash_fwd_kernel_traits<256, 128, 64, 8, false, false, cutlass::half_t, Flash_kernel_traits<256, 128, 64, 8, cutlass::half_t>>, Is_dropout=true, Is_causal=true]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(236): here
            instantiation of "void run_mha_fwd_hdim256<T>(Flash_fwd_params &, cudaStream_t) [with T=cutlass::half_t]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_hdim256_fp16_sm80.cu(8): here

/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_kernel.h(140): warning: variable "MMA_M" was declared but never referenced
          detected during:
            instantiation of "void flash::compute_attn_1rowblock<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params,Is_Q_split_K>(const Params &, int, int, int) [with Kernel_traits=Flash_fwd_kernel_traits<256, 128, 64, 8, false, false, cutlass::half_t, Flash_kernel_traits<256, 128, 64, 8, cutlass::half_t>>, Is_dropout=true, Is_causal=true, Is_even_N=true, Is_even_K=true, Return_softmax=false, Params=Flash_fwd_params, Is_Q_split_K=false]" 
(589): here
            instantiation of "void flash::compute_attn<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &) [with Kernel_traits=Flash_fwd_kernel_traits<256, 128, 64, 8, false, false, cutlass::half_t, Flash_kernel_traits<256, 128, 64, 8, cutlass::half_t>>, Is_dropout=true, Is_causal=true, Is_even_N=true, Is_even_K=true, Return_softmax=false, Params=Flash_fwd_params]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(15): here
            instantiation of "void flash_fwd_kernel<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax>(Flash_fwd_params) [with Kernel_traits=Flash_fwd_kernel_traits<256, 128, 64, 8, false, false, cutlass::half_t, Flash_kernel_traits<256, 128, 64, 8, cutlass::half_t>>, Is_dropout=true, Is_causal=true, Is_even_N=true, Is_even_K=true, Return_softmax=false]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(34): here
            instantiation of "void run_flash_fwd<Kernel_traits,Is_dropout,Is_causal>(Flash_fwd_params &, cudaStream_t) [with Kernel_traits=Flash_fwd_kernel_traits<256, 128, 64, 8, false, false, cutlass::half_t, Flash_kernel_traits<256, 128, 64, 8, cutlass::half_t>>, Is_dropout=true, Is_causal=true]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(236): here
            instantiation of "void run_mha_fwd_hdim256<T>(Flash_fwd_params &, cudaStream_t) [with T=cutlass::half_t]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_hdim256_fp16_sm80.cu(8): here

/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_kernel.h(276): warning: variable "tQrQ" was declared but never referenced
          detected during:
            instantiation of "void flash::compute_attn_1rowblock<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params,Is_Q_split_K>(const Params &, int, int, int) [with Kernel_traits=Flash_fwd_kernel_traits<256, 128, 64, 8, false, false, cutlass::half_t, Flash_kernel_traits<256, 128, 64, 8, cutlass::half_t>>, Is_dropout=true, Is_causal=true, Is_even_N=true, Is_even_K=true, Return_softmax=false, Params=Flash_fwd_params, Is_Q_split_K=false]" 
(589): here
            instantiation of "void flash::compute_attn<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &) [with Kernel_traits=Flash_fwd_kernel_traits<256, 128, 64, 8, false, false, cutlass::half_t, Flash_kernel_traits<256, 128, 64, 8, cutlass::half_t>>, Is_dropout=true, Is_causal=true, Is_even_N=true, Is_even_K=true, Return_softmax=false, Params=Flash_fwd_params]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(15): here
            instantiation of "void flash_fwd_kernel<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax>(Flash_fwd_params) [with Kernel_traits=Flash_fwd_kernel_traits<256, 128, 64, 8, false, false, cutlass::half_t, Flash_kernel_traits<256, 128, 64, 8, cutlass::half_t>>, Is_dropout=true, Is_causal=true, Is_even_N=true, Is_even_K=true, Return_softmax=false]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(34): here
            instantiation of "void run_flash_fwd<Kernel_traits,Is_dropout,Is_causal>(Flash_fwd_params &, cudaStream_t) [with Kernel_traits=Flash_fwd_kernel_traits<256, 128, 64, 8, false, false, cutlass::half_t, Flash_kernel_traits<256, 128, 64, 8, cutlass::half_t>>, Is_dropout=true, Is_causal=true]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(236): here
            instantiation of "void run_mha_fwd_hdim256<T>(Flash_fwd_params &, cudaStream_t) [with T=cutlass::half_t]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_hdim256_fp16_sm80.cu(8): here

/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_kernel.h(140): warning: variable "MMA_M" was declared but never referenced
          detected during:
            instantiation of "void flash::compute_attn_1rowblock<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params,Is_Q_split_K>(const Params &, int, int, int) [with Kernel_traits=Flash_fwd_kernel_traits<256, 128, 64, 8, false, false, cutlass::half_t, Flash_kernel_traits<256, 128, 64, 8, cutlass::half_t>>, Is_dropout=true, Is_causal=true, Is_even_N=true, Is_even_K=false, Return_softmax=true, Params=Flash_fwd_params, Is_Q_split_K=false]" 
(589): here
            instantiation of "void flash::compute_attn<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &) [with Kernel_traits=Flash_fwd_kernel_traits<256, 128, 64, 8, false, false, cutlass::half_t, Flash_kernel_traits<256, 128, 64, 8, cutlass::half_t>>, Is_dropout=true, Is_causal=true, Is_even_N=true, Is_even_K=false, Return_softmax=true, Params=Flash_fwd_params]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(15): here
            instantiation of "void flash_fwd_kernel<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax>(Flash_fwd_params) [with Kernel_traits=Flash_fwd_kernel_traits<256, 128, 64, 8, false, false, cutlass::half_t, Flash_kernel_traits<256, 128, 64, 8, cutlass::half_t>>, Is_dropout=true, Is_causal=true, Is_even_N=true, Is_even_K=false, Return_softmax=true]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(34): here
            instantiation of "void run_flash_fwd<Kernel_traits,Is_dropout,Is_causal>(Flash_fwd_params &, cudaStream_t) [with Kernel_traits=Flash_fwd_kernel_traits<256, 128, 64, 8, false, false, cutlass::half_t, Flash_kernel_traits<256, 128, 64, 8, cutlass::half_t>>, Is_dropout=true, Is_causal=true]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(236): here
            instantiation of "void run_mha_fwd_hdim256<T>(Flash_fwd_params &, cudaStream_t) [with T=cutlass::half_t]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_hdim256_fp16_sm80.cu(8): here

/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_kernel.h(276): warning: variable "tQrQ" was declared but never referenced
          detected during:
            instantiation of "void flash::compute_attn_1rowblock<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params,Is_Q_split_K>(const Params &, int, int, int) [with Kernel_traits=Flash_fwd_kernel_traits<256, 128, 64, 8, false, false, cutlass::half_t, Flash_kernel_traits<256, 128, 64, 8, cutlass::half_t>>, Is_dropout=true, Is_causal=true, Is_even_N=true, Is_even_K=false, Return_softmax=true, Params=Flash_fwd_params, Is_Q_split_K=false]" 
(589): here
            instantiation of "void flash::compute_attn<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &) [with Kernel_traits=Flash_fwd_kernel_traits<256, 128, 64, 8, false, false, cutlass::half_t, Flash_kernel_traits<256, 128, 64, 8, cutlass::half_t>>, Is_dropout=true, Is_causal=true, Is_even_N=true, Is_even_K=false, Return_softmax=true, Params=Flash_fwd_params]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(15): here
            instantiation of "void flash_fwd_kernel<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax>(Flash_fwd_params) [with Kernel_traits=Flash_fwd_kernel_traits<256, 128, 64, 8, false, false, cutlass::half_t, Flash_kernel_traits<256, 128, 64, 8, cutlass::half_t>>, Is_dropout=true, Is_causal=true, Is_even_N=true, Is_even_K=false, Return_softmax=true]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(34): here
            instantiation of "void run_flash_fwd<Kernel_traits,Is_dropout,Is_causal>(Flash_fwd_params &, cudaStream_t) [with Kernel_traits=Flash_fwd_kernel_traits<256, 128, 64, 8, false, false, cutlass::half_t, Flash_kernel_traits<256, 128, 64, 8, cutlass::half_t>>, Is_dropout=true, Is_causal=true]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(236): here
            instantiation of "void run_mha_fwd_hdim256<T>(Flash_fwd_params &, cudaStream_t) [with T=cutlass::half_t]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_hdim256_fp16_sm80.cu(8): here

/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_kernel.h(140): warning: variable "MMA_M" was declared but never referenced
          detected during:
            instantiation of "void flash::compute_attn_1rowblock<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params,Is_Q_split_K>(const Params &, int, int, int) [with Kernel_traits=Flash_fwd_kernel_traits<256, 128, 64, 8, false, false, cutlass::half_t, Flash_kernel_traits<256, 128, 64, 8, cutlass::half_t>>, Is_dropout=true, Is_causal=true, Is_even_N=true, Is_even_K=false, Return_softmax=false, Params=Flash_fwd_params, Is_Q_split_K=false]" 
(589): here
            instantiation of "void flash::compute_attn<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &) [with Kernel_traits=Flash_fwd_kernel_traits<256, 128, 64, 8, false, false, cutlass::half_t, Flash_kernel_traits<256, 128, 64, 8, cutlass::half_t>>, Is_dropout=true, Is_causal=true, Is_even_N=true, Is_even_K=false, Return_softmax=false, Params=Flash_fwd_params]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(15): here
            instantiation of "void flash_fwd_kernel<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax>(Flash_fwd_params) [with Kernel_traits=Flash_fwd_kernel_traits<256, 128, 64, 8, false, false, cutlass::half_t, Flash_kernel_traits<256, 128, 64, 8, cutlass::half_t>>, Is_dropout=true, Is_causal=true, Is_even_N=true, Is_even_K=false, Return_softmax=false]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(34): here
            instantiation of "void run_flash_fwd<Kernel_traits,Is_dropout,Is_causal>(Flash_fwd_params &, cudaStream_t) [with Kernel_traits=Flash_fwd_kernel_traits<256, 128, 64, 8, false, false, cutlass::half_t, Flash_kernel_traits<256, 128, 64, 8, cutlass::half_t>>, Is_dropout=true, Is_causal=true]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(236): here
            instantiation of "void run_mha_fwd_hdim256<T>(Flash_fwd_params &, cudaStream_t) [with T=cutlass::half_t]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_hdim256_fp16_sm80.cu(8): here

/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_kernel.h(276): warning: variable "tQrQ" was declared but never referenced
          detected during:
            instantiation of "void flash::compute_attn_1rowblock<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params,Is_Q_split_K>(const Params &, int, int, int) [with Kernel_traits=Flash_fwd_kernel_traits<256, 128, 64, 8, false, false, cutlass::half_t, Flash_kernel_traits<256, 128, 64, 8, cutlass::half_t>>, Is_dropout=true, Is_causal=true, Is_even_N=true, Is_even_K=false, Return_softmax=false, Params=Flash_fwd_params, Is_Q_split_K=false]" 
(589): here
            instantiation of "void flash::compute_attn<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &) [with Kernel_traits=Flash_fwd_kernel_traits<256, 128, 64, 8, false, false, cutlass::half_t, Flash_kernel_traits<256, 128, 64, 8, cutlass::half_t>>, Is_dropout=true, Is_causal=true, Is_even_N=true, Is_even_K=false, Return_softmax=false, Params=Flash_fwd_params]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(15): here
            instantiation of "void flash_fwd_kernel<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax>(Flash_fwd_params) [with Kernel_traits=Flash_fwd_kernel_traits<256, 128, 64, 8, false, false, cutlass::half_t, Flash_kernel_traits<256, 128, 64, 8, cutlass::half_t>>, Is_dropout=true, Is_causal=true, Is_even_N=true, Is_even_K=false, Return_softmax=false]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(34): here
            instantiation of "void run_flash_fwd<Kernel_traits,Is_dropout,Is_causal>(Flash_fwd_params &, cudaStream_t) [with Kernel_traits=Flash_fwd_kernel_traits<256, 128, 64, 8, false, false, cutlass::half_t, Flash_kernel_traits<256, 128, 64, 8, cutlass::half_t>>, Is_dropout=true, Is_causal=true]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(236): here
            instantiation of "void run_mha_fwd_hdim256<T>(Flash_fwd_params &, cudaStream_t) [with T=cutlass::half_t]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_hdim256_fp16_sm80.cu(8): here

/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_kernel.h(140): warning: variable "MMA_M" was declared but never referenced
          detected during:
            instantiation of "void flash::compute_attn_1rowblock<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params,Is_Q_split_K>(const Params &, int, int, int) [with Kernel_traits=Flash_fwd_kernel_traits<256, 128, 64, 8, false, false, cutlass::half_t, Flash_kernel_traits<256, 128, 64, 8, cutlass::half_t>>, Is_dropout=true, Is_causal=true, Is_even_N=false, Is_even_K=true, Return_softmax=true, Params=Flash_fwd_params, Is_Q_split_K=false]" 
(589): here
            instantiation of "void flash::compute_attn<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &) [with Kernel_traits=Flash_fwd_kernel_traits<256, 128, 64, 8, false, false, cutlass::half_t, Flash_kernel_traits<256, 128, 64, 8, cutlass::half_t>>, Is_dropout=true, Is_causal=true, Is_even_N=false, Is_even_K=true, Return_softmax=true, Params=Flash_fwd_params]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(15): here
            instantiation of "void flash_fwd_kernel<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax>(Flash_fwd_params) [with Kernel_traits=Flash_fwd_kernel_traits<256, 128, 64, 8, false, false, cutlass::half_t, Flash_kernel_traits<256, 128, 64, 8, cutlass::half_t>>, Is_dropout=true, Is_causal=true, Is_even_N=false, Is_even_K=true, Return_softmax=true]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(34): here
            instantiation of "void run_flash_fwd<Kernel_traits,Is_dropout,Is_causal>(Flash_fwd_params &, cudaStream_t) [with Kernel_traits=Flash_fwd_kernel_traits<256, 128, 64, 8, false, false, cutlass::half_t, Flash_kernel_traits<256, 128, 64, 8, cutlass::half_t>>, Is_dropout=true, Is_causal=true]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(236): here
            instantiation of "void run_mha_fwd_hdim256<T>(Flash_fwd_params &, cudaStream_t) [with T=cutlass::half_t]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_hdim256_fp16_sm80.cu(8): here

/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_kernel.h(276): warning: variable "tQrQ" was declared but never referenced
          detected during:
            instantiation of "void flash::compute_attn_1rowblock<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params,Is_Q_split_K>(const Params &, int, int, int) [with Kernel_traits=Flash_fwd_kernel_traits<256, 128, 64, 8, false, false, cutlass::half_t, Flash_kernel_traits<256, 128, 64, 8, cutlass::half_t>>, Is_dropout=true, Is_causal=true, Is_even_N=false, Is_even_K=true, Return_softmax=true, Params=Flash_fwd_params, Is_Q_split_K=false]" 
(589): here
            instantiation of "void flash::compute_attn<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &) [with Kernel_traits=Flash_fwd_kernel_traits<256, 128, 64, 8, false, false, cutlass::half_t, Flash_kernel_traits<256, 128, 64, 8, cutlass::half_t>>, Is_dropout=true, Is_causal=true, Is_even_N=false, Is_even_K=true, Return_softmax=true, Params=Flash_fwd_params]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(15): here
            instantiation of "void flash_fwd_kernel<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax>(Flash_fwd_params) [with Kernel_traits=Flash_fwd_kernel_traits<256, 128, 64, 8, false, false, cutlass::half_t, Flash_kernel_traits<256, 128, 64, 8, cutlass::half_t>>, Is_dropout=true, Is_causal=true, Is_even_N=false, Is_even_K=true, Return_softmax=true]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(34): here
            instantiation of "void run_flash_fwd<Kernel_traits,Is_dropout,Is_causal>(Flash_fwd_params &, cudaStream_t) [with Kernel_traits=Flash_fwd_kernel_traits<256, 128, 64, 8, false, false, cutlass::half_t, Flash_kernel_traits<256, 128, 64, 8, cutlass::half_t>>, Is_dropout=true, Is_causal=true]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(236): here
            instantiation of "void run_mha_fwd_hdim256<T>(Flash_fwd_params &, cudaStream_t) [with T=cutlass::half_t]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_hdim256_fp16_sm80.cu(8): here

/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_kernel.h(140): warning: variable "MMA_M" was declared but never referenced
          detected during:
            instantiation of "void flash::compute_attn_1rowblock<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params,Is_Q_split_K>(const Params &, int, int, int) [with Kernel_traits=Flash_fwd_kernel_traits<256, 128, 64, 8, false, false, cutlass::half_t, Flash_kernel_traits<256, 128, 64, 8, cutlass::half_t>>, Is_dropout=true, Is_causal=true, Is_even_N=false, Is_even_K=true, Return_softmax=false, Params=Flash_fwd_params, Is_Q_split_K=false]" 
(589): here
            instantiation of "void flash::compute_attn<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &) [with Kernel_traits=Flash_fwd_kernel_traits<256, 128, 64, 8, false, false, cutlass::half_t, Flash_kernel_traits<256, 128, 64, 8, cutlass::half_t>>, Is_dropout=true, Is_causal=true, Is_even_N=false, Is_even_K=true, Return_softmax=false, Params=Flash_fwd_params]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(15): here
            instantiation of "void flash_fwd_kernel<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax>(Flash_fwd_params) [with Kernel_traits=Flash_fwd_kernel_traits<256, 128, 64, 8, false, false, cutlass::half_t, Flash_kernel_traits<256, 128, 64, 8, cutlass::half_t>>, Is_dropout=true, Is_causal=true, Is_even_N=false, Is_even_K=true, Return_softmax=false]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(34): here
            instantiation of "void run_flash_fwd<Kernel_traits,Is_dropout,Is_causal>(Flash_fwd_params &, cudaStream_t) [with Kernel_traits=Flash_fwd_kernel_traits<256, 128, 64, 8, false, false, cutlass::half_t, Flash_kernel_traits<256, 128, 64, 8, cutlass::half_t>>, Is_dropout=true, Is_causal=true]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(236): here
            instantiation of "void run_mha_fwd_hdim256<T>(Flash_fwd_params &, cudaStream_t) [with T=cutlass::half_t]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_hdim256_fp16_sm80.cu(8): here

/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_kernel.h(276): warning: variable "tQrQ" was declared but never referenced
          detected during:
            instantiation of "void flash::compute_attn_1rowblock<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params,Is_Q_split_K>(const Params &, int, int, int) [with Kernel_traits=Flash_fwd_kernel_traits<256, 128, 64, 8, false, false, cutlass::half_t, Flash_kernel_traits<256, 128, 64, 8, cutlass::half_t>>, Is_dropout=true, Is_causal=true, Is_even_N=false, Is_even_K=true, Return_softmax=false, Params=Flash_fwd_params, Is_Q_split_K=false]" 
(589): here
            instantiation of "void flash::compute_attn<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &) [with Kernel_traits=Flash_fwd_kernel_traits<256, 128, 64, 8, false, false, cutlass::half_t, Flash_kernel_traits<256, 128, 64, 8, cutlass::half_t>>, Is_dropout=true, Is_causal=true, Is_even_N=false, Is_even_K=true, Return_softmax=false, Params=Flash_fwd_params]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(15): here
            instantiation of "void flash_fwd_kernel<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax>(Flash_fwd_params) [with Kernel_traits=Flash_fwd_kernel_traits<256, 128, 64, 8, false, false, cutlass::half_t, Flash_kernel_traits<256, 128, 64, 8, cutlass::half_t>>, Is_dropout=true, Is_causal=true, Is_even_N=false, Is_even_K=true, Return_softmax=false]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(34): here
            instantiation of "void run_flash_fwd<Kernel_traits,Is_dropout,Is_causal>(Flash_fwd_params &, cudaStream_t) [with Kernel_traits=Flash_fwd_kernel_traits<256, 128, 64, 8, false, false, cutlass::half_t, Flash_kernel_traits<256, 128, 64, 8, cutlass::half_t>>, Is_dropout=true, Is_causal=true]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(236): here
            instantiation of "void run_mha_fwd_hdim256<T>(Flash_fwd_params &, cudaStream_t) [with T=cutlass::half_t]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_hdim256_fp16_sm80.cu(8): here

/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_kernel.h(140): warning: variable "MMA_M" was declared but never referenced
          detected during:
            instantiation of "void flash::compute_attn_1rowblock<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params,Is_Q_split_K>(const Params &, int, int, int) [with Kernel_traits=Flash_fwd_kernel_traits<256, 128, 64, 8, false, false, cutlass::half_t, Flash_kernel_traits<256, 128, 64, 8, cutlass::half_t>>, Is_dropout=true, Is_causal=true, Is_even_N=false, Is_even_K=false, Return_softmax=true, Params=Flash_fwd_params, Is_Q_split_K=false]" 
(589): here
            instantiation of "void flash::compute_attn<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &) [with Kernel_traits=Flash_fwd_kernel_traits<256, 128, 64, 8, false, false, cutlass::half_t, Flash_kernel_traits<256, 128, 64, 8, cutlass::half_t>>, Is_dropout=true, Is_causal=true, Is_even_N=false, Is_even_K=false, Return_softmax=true, Params=Flash_fwd_params]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(15): here
            instantiation of "void flash_fwd_kernel<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax>(Flash_fwd_params) [with Kernel_traits=Flash_fwd_kernel_traits<256, 128, 64, 8, false, false, cutlass::half_t, Flash_kernel_traits<256, 128, 64, 8, cutlass::half_t>>, Is_dropout=true, Is_causal=true, Is_even_N=false, Is_even_K=false, Return_softmax=true]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(34): here
            instantiation of "void run_flash_fwd<Kernel_traits,Is_dropout,Is_causal>(Flash_fwd_params &, cudaStream_t) [with Kernel_traits=Flash_fwd_kernel_traits<256, 128, 64, 8, false, false, cutlass::half_t, Flash_kernel_traits<256, 128, 64, 8, cutlass::half_t>>, Is_dropout=true, Is_causal=true]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(236): here
            instantiation of "void run_mha_fwd_hdim256<T>(Flash_fwd_params &, cudaStream_t) [with T=cutlass::half_t]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_hdim256_fp16_sm80.cu(8): here

/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_kernel.h(276): warning: variable "tQrQ" was declared but never referenced
          detected during:
            instantiation of "void flash::compute_attn_1rowblock<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params,Is_Q_split_K>(const Params &, int, int, int) [with Kernel_traits=Flash_fwd_kernel_traits<256, 128, 64, 8, false, false, cutlass::half_t, Flash_kernel_traits<256, 128, 64, 8, cutlass::half_t>>, Is_dropout=true, Is_causal=true, Is_even_N=false, Is_even_K=false, Return_softmax=true, Params=Flash_fwd_params, Is_Q_split_K=false]" 
(589): here
            instantiation of "void flash::compute_attn<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &) [with Kernel_traits=Flash_fwd_kernel_traits<256, 128, 64, 8, false, false, cutlass::half_t, Flash_kernel_traits<256, 128, 64, 8, cutlass::half_t>>, Is_dropout=true, Is_causal=true, Is_even_N=false, Is_even_K=false, Return_softmax=true, Params=Flash_fwd_params]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(15): here
            instantiation of "void flash_fwd_kernel<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax>(Flash_fwd_params) [with Kernel_traits=Flash_fwd_kernel_traits<256, 128, 64, 8, false, false, cutlass::half_t, Flash_kernel_traits<256, 128, 64, 8, cutlass::half_t>>, Is_dropout=true, Is_causal=true, Is_even_N=false, Is_even_K=false, Return_softmax=true]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(34): here
            instantiation of "void run_flash_fwd<Kernel_traits,Is_dropout,Is_causal>(Flash_fwd_params &, cudaStream_t) [with Kernel_traits=Flash_fwd_kernel_traits<256, 128, 64, 8, false, false, cutlass::half_t, Flash_kernel_traits<256, 128, 64, 8, cutlass::half_t>>, Is_dropout=true, Is_causal=true]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(236): here
            instantiation of "void run_mha_fwd_hdim256<T>(Flash_fwd_params &, cudaStream_t) [with T=cutlass::half_t]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_hdim256_fp16_sm80.cu(8): here

/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_kernel.h(140): warning: variable "MMA_M" was declared but never referenced
          detected during:
            instantiation of "void flash::compute_attn_1rowblock<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params,Is_Q_split_K>(const Params &, int, int, int) [with Kernel_traits=Flash_fwd_kernel_traits<256, 128, 64, 8, false, false, cutlass::half_t, Flash_kernel_traits<256, 128, 64, 8, cutlass::half_t>>, Is_dropout=true, Is_causal=true, Is_even_N=false, Is_even_K=false, Return_softmax=false, Params=Flash_fwd_params, Is_Q_split_K=false]" 
(589): here
            instantiation of "void flash::compute_attn<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &) [with Kernel_traits=Flash_fwd_kernel_traits<256, 128, 64, 8, false, false, cutlass::half_t, Flash_kernel_traits<256, 128, 64, 8, cutlass::half_t>>, Is_dropout=true, Is_causal=true, Is_even_N=false, Is_even_K=false, Return_softmax=false, Params=Flash_fwd_params]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(15): here
            instantiation of "void flash_fwd_kernel<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax>(Flash_fwd_params) [with Kernel_traits=Flash_fwd_kernel_traits<256, 128, 64, 8, false, false, cutlass::half_t, Flash_kernel_traits<256, 128, 64, 8, cutlass::half_t>>, Is_dropout=true, Is_causal=true, Is_even_N=false, Is_even_K=false, Return_softmax=false]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(34): here
            instantiation of "void run_flash_fwd<Kernel_traits,Is_dropout,Is_causal>(Flash_fwd_params &, cudaStream_t) [with Kernel_traits=Flash_fwd_kernel_traits<256, 128, 64, 8, false, false, cutlass::half_t, Flash_kernel_traits<256, 128, 64, 8, cutlass::half_t>>, Is_dropout=true, Is_causal=true]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(236): here
            instantiation of "void run_mha_fwd_hdim256<T>(Flash_fwd_params &, cudaStream_t) [with T=cutlass::half_t]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_hdim256_fp16_sm80.cu(8): here

/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_kernel.h(276): warning: variable "tQrQ" was declared but never referenced
          detected during:
            instantiation of "void flash::compute_attn_1rowblock<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params,Is_Q_split_K>(const Params &, int, int, int) [with Kernel_traits=Flash_fwd_kernel_traits<256, 128, 64, 8, false, false, cutlass::half_t, Flash_kernel_traits<256, 128, 64, 8, cutlass::half_t>>, Is_dropout=true, Is_causal=true, Is_even_N=false, Is_even_K=false, Return_softmax=false, Params=Flash_fwd_params, Is_Q_split_K=false]" 
(589): here
            instantiation of "void flash::compute_attn<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &) [with Kernel_traits=Flash_fwd_kernel_traits<256, 128, 64, 8, false, false, cutlass::half_t, Flash_kernel_traits<256, 128, 64, 8, cutlass::half_t>>, Is_dropout=true, Is_causal=true, Is_even_N=false, Is_even_K=false, Return_softmax=false, Params=Flash_fwd_params]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(15): here
            instantiation of "void flash_fwd_kernel<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax>(Flash_fwd_params) [with Kernel_traits=Flash_fwd_kernel_traits<256, 128, 64, 8, false, false, cutlass::half_t, Flash_kernel_traits<256, 128, 64, 8, cutlass::half_t>>, Is_dropout=true, Is_causal=true, Is_even_N=false, Is_even_K=false, Return_softmax=false]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(34): here
            instantiation of "void run_flash_fwd<Kernel_traits,Is_dropout,Is_causal>(Flash_fwd_params &, cudaStream_t) [with Kernel_traits=Flash_fwd_kernel_traits<256, 128, 64, 8, false, false, cutlass::half_t, Flash_kernel_traits<256, 128, 64, 8, cutlass::half_t>>, Is_dropout=true, Is_causal=true]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(236): here
            instantiation of "void run_mha_fwd_hdim256<T>(Flash_fwd_params &, cudaStream_t) [with T=cutlass::half_t]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_hdim256_fp16_sm80.cu(8): here

/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_kernel.h(140): warning: variable "MMA_M" was declared but never referenced
          detected during:
            instantiation of "void flash::compute_attn_1rowblock<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params,Is_Q_split_K>(const Params &, int, int, int) [with Kernel_traits=Flash_fwd_kernel_traits<256, 64, 64, 4, false, false, cutlass::half_t, Flash_kernel_traits<256, 64, 64, 4, cutlass::half_t>>, Is_dropout=true, Is_causal=true, Is_even_N=true, Is_even_K=true, Return_softmax=true, Params=Flash_fwd_params, Is_Q_split_K=false]" 
(589): here
            instantiation of "void flash::compute_attn<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &) [with Kernel_traits=Flash_fwd_kernel_traits<256, 64, 64, 4, false, false, cutlass::half_t, Flash_kernel_traits<256, 64, 64, 4, cutlass::half_t>>, Is_dropout=true, Is_causal=true, Is_even_N=true, Is_even_K=true, Return_softmax=true, Params=Flash_fwd_params]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(15): here
            instantiation of "void flash_fwd_kernel<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax>(Flash_fwd_params) [with Kernel_traits=Flash_fwd_kernel_traits<256, 64, 64, 4, false, false, cutlass::half_t, Flash_kernel_traits<256, 64, 64, 4, cutlass::half_t>>, Is_dropout=true, Is_causal=true, Is_even_N=true, Is_even_K=true, Return_softmax=true]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(34): here
            instantiation of "void run_flash_fwd<Kernel_traits,Is_dropout,Is_causal>(Flash_fwd_params &, cudaStream_t) [with Kernel_traits=Flash_fwd_kernel_traits<256, 64, 64, 4, false, false, cutlass::half_t, Flash_kernel_traits<256, 64, 64, 4, cutlass::half_t>>, Is_dropout=true, Is_causal=true]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(236): here
            instantiation of "void run_mha_fwd_hdim256<T>(Flash_fwd_params &, cudaStream_t) [with T=cutlass::half_t]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_hdim256_fp16_sm80.cu(8): here

/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_kernel.h(276): warning: variable "tQrQ" was declared but never referenced
          detected during:
            instantiation of "void flash::compute_attn_1rowblock<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params,Is_Q_split_K>(const Params &, int, int, int) [with Kernel_traits=Flash_fwd_kernel_traits<256, 64, 64, 4, false, false, cutlass::half_t, Flash_kernel_traits<256, 64, 64, 4, cutlass::half_t>>, Is_dropout=true, Is_causal=true, Is_even_N=true, Is_even_K=true, Return_softmax=true, Params=Flash_fwd_params, Is_Q_split_K=false]" 
(589): here
            instantiation of "void flash::compute_attn<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &) [with Kernel_traits=Flash_fwd_kernel_traits<256, 64, 64, 4, false, false, cutlass::half_t, Flash_kernel_traits<256, 64, 64, 4, cutlass::half_t>>, Is_dropout=true, Is_causal=true, Is_even_N=true, Is_even_K=true, Return_softmax=true, Params=Flash_fwd_params]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(15): here
            instantiation of "void flash_fwd_kernel<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax>(Flash_fwd_params) [with Kernel_traits=Flash_fwd_kernel_traits<256, 64, 64, 4, false, false, cutlass::half_t, Flash_kernel_traits<256, 64, 64, 4, cutlass::half_t>>, Is_dropout=true, Is_causal=true, Is_even_N=true, Is_even_K=true, Return_softmax=true]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(34): here
            instantiation of "void run_flash_fwd<Kernel_traits,Is_dropout,Is_causal>(Flash_fwd_params &, cudaStream_t) [with Kernel_traits=Flash_fwd_kernel_traits<256, 64, 64, 4, false, false, cutlass::half_t, Flash_kernel_traits<256, 64, 64, 4, cutlass::half_t>>, Is_dropout=true, Is_causal=true]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(236): here
            instantiation of "void run_mha_fwd_hdim256<T>(Flash_fwd_params &, cudaStream_t) [with T=cutlass::half_t]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_hdim256_fp16_sm80.cu(8): here

/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_kernel.h(140): warning: variable "MMA_M" was declared but never referenced
          detected during:
            instantiation of "void flash::compute_attn_1rowblock<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params,Is_Q_split_K>(const Params &, int, int, int) [with Kernel_traits=Flash_fwd_kernel_traits<256, 64, 64, 4, false, false, cutlass::half_t, Flash_kernel_traits<256, 64, 64, 4, cutlass::half_t>>, Is_dropout=true, Is_causal=true, Is_even_N=true, Is_even_K=true, Return_softmax=false, Params=Flash_fwd_params, Is_Q_split_K=false]" 
(589): here
            instantiation of "void flash::compute_attn<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &) [with Kernel_traits=Flash_fwd_kernel_traits<256, 64, 64, 4, false, false, cutlass::half_t, Flash_kernel_traits<256, 64, 64, 4, cutlass::half_t>>, Is_dropout=true, Is_causal=true, Is_even_N=true, Is_even_K=true, Return_softmax=false, Params=Flash_fwd_params]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(15): here
            instantiation of "void flash_fwd_kernel<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax>(Flash_fwd_params) [with Kernel_traits=Flash_fwd_kernel_traits<256, 64, 64, 4, false, false, cutlass::half_t, Flash_kernel_traits<256, 64, 64, 4, cutlass::half_t>>, Is_dropout=true, Is_causal=true, Is_even_N=true, Is_even_K=true, Return_softmax=false]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(34): here
            instantiation of "void run_flash_fwd<Kernel_traits,Is_dropout,Is_causal>(Flash_fwd_params &, cudaStream_t) [with Kernel_traits=Flash_fwd_kernel_traits<256, 64, 64, 4, false, false, cutlass::half_t, Flash_kernel_traits<256, 64, 64, 4, cutlass::half_t>>, Is_dropout=true, Is_causal=true]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(236): here
            instantiation of "void run_mha_fwd_hdim256<T>(Flash_fwd_params &, cudaStream_t) [with T=cutlass::half_t]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_hdim256_fp16_sm80.cu(8): here

/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_kernel.h(276): warning: variable "tQrQ" was declared but never referenced
          detected during:
            instantiation of "void flash::compute_attn_1rowblock<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params,Is_Q_split_K>(const Params &, int, int, int) [with Kernel_traits=Flash_fwd_kernel_traits<256, 64, 64, 4, false, false, cutlass::half_t, Flash_kernel_traits<256, 64, 64, 4, cutlass::half_t>>, Is_dropout=true, Is_causal=true, Is_even_N=true, Is_even_K=true, Return_softmax=false, Params=Flash_fwd_params, Is_Q_split_K=false]" 
(589): here
            instantiation of "void flash::compute_attn<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &) [with Kernel_traits=Flash_fwd_kernel_traits<256, 64, 64, 4, false, false, cutlass::half_t, Flash_kernel_traits<256, 64, 64, 4, cutlass::half_t>>, Is_dropout=true, Is_causal=true, Is_even_N=true, Is_even_K=true, Return_softmax=false, Params=Flash_fwd_params]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(15): here
            instantiation of "void flash_fwd_kernel<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax>(Flash_fwd_params) [with Kernel_traits=Flash_fwd_kernel_traits<256, 64, 64, 4, false, false, cutlass::half_t, Flash_kernel_traits<256, 64, 64, 4, cutlass::half_t>>, Is_dropout=true, Is_causal=true, Is_even_N=true, Is_even_K=true, Return_softmax=false]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(34): here
            instantiation of "void run_flash_fwd<Kernel_traits,Is_dropout,Is_causal>(Flash_fwd_params &, cudaStream_t) [with Kernel_traits=Flash_fwd_kernel_traits<256, 64, 64, 4, false, false, cutlass::half_t, Flash_kernel_traits<256, 64, 64, 4, cutlass::half_t>>, Is_dropout=true, Is_causal=true]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(236): here
            instantiation of "void run_mha_fwd_hdim256<T>(Flash_fwd_params &, cudaStream_t) [with T=cutlass::half_t]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_hdim256_fp16_sm80.cu(8): here

/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_kernel.h(140): warning: variable "MMA_M" was declared but never referenced
          detected during:
            instantiation of "void flash::compute_attn_1rowblock<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params,Is_Q_split_K>(const Params &, int, int, int) [with Kernel_traits=Flash_fwd_kernel_traits<256, 64, 64, 4, false, false, cutlass::half_t, Flash_kernel_traits<256, 64, 64, 4, cutlass::half_t>>, Is_dropout=true, Is_causal=true, Is_even_N=true, Is_even_K=false, Return_softmax=true, Params=Flash_fwd_params, Is_Q_split_K=false]" 
(589): here
            instantiation of "void flash::compute_attn<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &) [with Kernel_traits=Flash_fwd_kernel_traits<256, 64, 64, 4, false, false, cutlass::half_t, Flash_kernel_traits<256, 64, 64, 4, cutlass::half_t>>, Is_dropout=true, Is_causal=true, Is_even_N=true, Is_even_K=false, Return_softmax=true, Params=Flash_fwd_params]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(15): here
            instantiation of "void flash_fwd_kernel<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax>(Flash_fwd_params) [with Kernel_traits=Flash_fwd_kernel_traits<256, 64, 64, 4, false, false, cutlass::half_t, Flash_kernel_traits<256, 64, 64, 4, cutlass::half_t>>, Is_dropout=true, Is_causal=true, Is_even_N=true, Is_even_K=false, Return_softmax=true]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(34): here
            instantiation of "void run_flash_fwd<Kernel_traits,Is_dropout,Is_causal>(Flash_fwd_params &, cudaStream_t) [with Kernel_traits=Flash_fwd_kernel_traits<256, 64, 64, 4, false, false, cutlass::half_t, Flash_kernel_traits<256, 64, 64, 4, cutlass::half_t>>, Is_dropout=true, Is_causal=true]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(236): here
            instantiation of "void run_mha_fwd_hdim256<T>(Flash_fwd_params &, cudaStream_t) [with T=cutlass::half_t]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_hdim256_fp16_sm80.cu(8): here

/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_kernel.h(276): warning: variable "tQrQ" was declared but never referenced
          detected during:
            instantiation of "void flash::compute_attn_1rowblock<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params,Is_Q_split_K>(const Params &, int, int, int) [with Kernel_traits=Flash_fwd_kernel_traits<256, 64, 64, 4, false, false, cutlass::half_t, Flash_kernel_traits<256, 64, 64, 4, cutlass::half_t>>, Is_dropout=true, Is_causal=true, Is_even_N=true, Is_even_K=false, Return_softmax=true, Params=Flash_fwd_params, Is_Q_split_K=false]" 
(589): here
            instantiation of "void flash::compute_attn<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &) [with Kernel_traits=Flash_fwd_kernel_traits<256, 64, 64, 4, false, false, cutlass::half_t, Flash_kernel_traits<256, 64, 64, 4, cutlass::half_t>>, Is_dropout=true, Is_causal=true, Is_even_N=true, Is_even_K=false, Return_softmax=true, Params=Flash_fwd_params]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(15): here
            instantiation of "void flash_fwd_kernel<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax>(Flash_fwd_params) [with Kernel_traits=Flash_fwd_kernel_traits<256, 64, 64, 4, false, false, cutlass::half_t, Flash_kernel_traits<256, 64, 64, 4, cutlass::half_t>>, Is_dropout=true, Is_causal=true, Is_even_N=true, Is_even_K=false, Return_softmax=true]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(34): here
            instantiation of "void run_flash_fwd<Kernel_traits,Is_dropout,Is_causal>(Flash_fwd_params &, cudaStream_t) [with Kernel_traits=Flash_fwd_kernel_traits<256, 64, 64, 4, false, false, cutlass::half_t, Flash_kernel_traits<256, 64, 64, 4, cutlass::half_t>>, Is_dropout=true, Is_causal=true]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(236): here
            instantiation of "void run_mha_fwd_hdim256<T>(Flash_fwd_params &, cudaStream_t) [with T=cutlass::half_t]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_hdim256_fp16_sm80.cu(8): here

/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_kernel.h(140): warning: variable "MMA_M" was declared but never referenced
          detected during:
            instantiation of "void flash::compute_attn_1rowblock<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params,Is_Q_split_K>(const Params &, int, int, int) [with Kernel_traits=Flash_fwd_kernel_traits<256, 64, 64, 4, false, false, cutlass::half_t, Flash_kernel_traits<256, 64, 64, 4, cutlass::half_t>>, Is_dropout=true, Is_causal=true, Is_even_N=true, Is_even_K=false, Return_softmax=false, Params=Flash_fwd_params, Is_Q_split_K=false]" 
(589): here
            instantiation of "void flash::compute_attn<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &) [with Kernel_traits=Flash_fwd_kernel_traits<256, 64, 64, 4, false, false, cutlass::half_t, Flash_kernel_traits<256, 64, 64, 4, cutlass::half_t>>, Is_dropout=true, Is_causal=true, Is_even_N=true, Is_even_K=false, Return_softmax=false, Params=Flash_fwd_params]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(15): here
            instantiation of "void flash_fwd_kernel<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax>(Flash_fwd_params) [with Kernel_traits=Flash_fwd_kernel_traits<256, 64, 64, 4, false, false, cutlass::half_t, Flash_kernel_traits<256, 64, 64, 4, cutlass::half_t>>, Is_dropout=true, Is_causal=true, Is_even_N=true, Is_even_K=false, Return_softmax=false]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(34): here
            instantiation of "void run_flash_fwd<Kernel_traits,Is_dropout,Is_causal>(Flash_fwd_params &, cudaStream_t) [with Kernel_traits=Flash_fwd_kernel_traits<256, 64, 64, 4, false, false, cutlass::half_t, Flash_kernel_traits<256, 64, 64, 4, cutlass::half_t>>, Is_dropout=true, Is_causal=true]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(236): here
            instantiation of "void run_mha_fwd_hdim256<T>(Flash_fwd_params &, cudaStream_t) [with T=cutlass::half_t]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_hdim256_fp16_sm80.cu(8): here

/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_kernel.h(276): warning: variable "tQrQ" was declared but never referenced
          detected during:
            instantiation of "void flash::compute_attn_1rowblock<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params,Is_Q_split_K>(const Params &, int, int, int) [with Kernel_traits=Flash_fwd_kernel_traits<256, 64, 64, 4, false, false, cutlass::half_t, Flash_kernel_traits<256, 64, 64, 4, cutlass::half_t>>, Is_dropout=true, Is_causal=true, Is_even_N=true, Is_even_K=false, Return_softmax=false, Params=Flash_fwd_params, Is_Q_split_K=false]" 
(589): here
            instantiation of "void flash::compute_attn<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &) [with Kernel_traits=Flash_fwd_kernel_traits<256, 64, 64, 4, false, false, cutlass::half_t, Flash_kernel_traits<256, 64, 64, 4, cutlass::half_t>>, Is_dropout=true, Is_causal=true, Is_even_N=true, Is_even_K=false, Return_softmax=false, Params=Flash_fwd_params]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(15): here
            instantiation of "void flash_fwd_kernel<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax>(Flash_fwd_params) [with Kernel_traits=Flash_fwd_kernel_traits<256, 64, 64, 4, false, false, cutlass::half_t, Flash_kernel_traits<256, 64, 64, 4, cutlass::half_t>>, Is_dropout=true, Is_causal=true, Is_even_N=true, Is_even_K=false, Return_softmax=false]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(34): here
            instantiation of "void run_flash_fwd<Kernel_traits,Is_dropout,Is_causal>(Flash_fwd_params &, cudaStream_t) [with Kernel_traits=Flash_fwd_kernel_traits<256, 64, 64, 4, false, false, cutlass::half_t, Flash_kernel_traits<256, 64, 64, 4, cutlass::half_t>>, Is_dropout=true, Is_causal=true]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(236): here
            instantiation of "void run_mha_fwd_hdim256<T>(Flash_fwd_params &, cudaStream_t) [with T=cutlass::half_t]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_hdim256_fp16_sm80.cu(8): here

/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_kernel.h(140): warning: variable "MMA_M" was declared but never referenced
          detected during:
            instantiation of "void flash::compute_attn_1rowblock<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params,Is_Q_split_K>(const Params &, int, int, int) [with Kernel_traits=Flash_fwd_kernel_traits<256, 64, 64, 4, false, false, cutlass::half_t, Flash_kernel_traits<256, 64, 64, 4, cutlass::half_t>>, Is_dropout=true, Is_causal=true, Is_even_N=false, Is_even_K=true, Return_softmax=true, Params=Flash_fwd_params, Is_Q_split_K=false]" 
(589): here
            instantiation of "void flash::compute_attn<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &) [with Kernel_traits=Flash_fwd_kernel_traits<256, 64, 64, 4, false, false, cutlass::half_t, Flash_kernel_traits<256, 64, 64, 4, cutlass::half_t>>, Is_dropout=true, Is_causal=true, Is_even_N=false, Is_even_K=true, Return_softmax=true, Params=Flash_fwd_params]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(15): here
            instantiation of "void flash_fwd_kernel<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax>(Flash_fwd_params) [with Kernel_traits=Flash_fwd_kernel_traits<256, 64, 64, 4, false, false, cutlass::half_t, Flash_kernel_traits<256, 64, 64, 4, cutlass::half_t>>, Is_dropout=true, Is_causal=true, Is_even_N=false, Is_even_K=true, Return_softmax=true]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(34): here
            instantiation of "void run_flash_fwd<Kernel_traits,Is_dropout,Is_causal>(Flash_fwd_params &, cudaStream_t) [with Kernel_traits=Flash_fwd_kernel_traits<256, 64, 64, 4, false, false, cutlass::half_t, Flash_kernel_traits<256, 64, 64, 4, cutlass::half_t>>, Is_dropout=true, Is_causal=true]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(236): here
            instantiation of "void run_mha_fwd_hdim256<T>(Flash_fwd_params &, cudaStream_t) [with T=cutlass::half_t]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_hdim256_fp16_sm80.cu(8): here

/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_kernel.h(276): warning: variable "tQrQ" was declared but never referenced
          detected during:
            instantiation of "void flash::compute_attn_1rowblock<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params,Is_Q_split_K>(const Params &, int, int, int) [with Kernel_traits=Flash_fwd_kernel_traits<256, 64, 64, 4, false, false, cutlass::half_t, Flash_kernel_traits<256, 64, 64, 4, cutlass::half_t>>, Is_dropout=true, Is_causal=true, Is_even_N=false, Is_even_K=true, Return_softmax=true, Params=Flash_fwd_params, Is_Q_split_K=false]" 
(589): here
            instantiation of "void flash::compute_attn<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &) [with Kernel_traits=Flash_fwd_kernel_traits<256, 64, 64, 4, false, false, cutlass::half_t, Flash_kernel_traits<256, 64, 64, 4, cutlass::half_t>>, Is_dropout=true, Is_causal=true, Is_even_N=false, Is_even_K=true, Return_softmax=true, Params=Flash_fwd_params]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(15): here
            instantiation of "void flash_fwd_kernel<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax>(Flash_fwd_params) [with Kernel_traits=Flash_fwd_kernel_traits<256, 64, 64, 4, false, false, cutlass::half_t, Flash_kernel_traits<256, 64, 64, 4, cutlass::half_t>>, Is_dropout=true, Is_causal=true, Is_even_N=false, Is_even_K=true, Return_softmax=true]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(34): here
            instantiation of "void run_flash_fwd<Kernel_traits,Is_dropout,Is_causal>(Flash_fwd_params &, cudaStream_t) [with Kernel_traits=Flash_fwd_kernel_traits<256, 64, 64, 4, false, false, cutlass::half_t, Flash_kernel_traits<256, 64, 64, 4, cutlass::half_t>>, Is_dropout=true, Is_causal=true]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(236): here
            instantiation of "void run_mha_fwd_hdim256<T>(Flash_fwd_params &, cudaStream_t) [with T=cutlass::half_t]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_hdim256_fp16_sm80.cu(8): here

/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_kernel.h(140): warning: variable "MMA_M" was declared but never referenced
          detected during:
            instantiation of "void flash::compute_attn_1rowblock<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params,Is_Q_split_K>(const Params &, int, int, int) [with Kernel_traits=Flash_fwd_kernel_traits<256, 64, 64, 4, false, false, cutlass::half_t, Flash_kernel_traits<256, 64, 64, 4, cutlass::half_t>>, Is_dropout=true, Is_causal=true, Is_even_N=false, Is_even_K=true, Return_softmax=false, Params=Flash_fwd_params, Is_Q_split_K=false]" 
(589): here
            instantiation of "void flash::compute_attn<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &) [with Kernel_traits=Flash_fwd_kernel_traits<256, 64, 64, 4, false, false, cutlass::half_t, Flash_kernel_traits<256, 64, 64, 4, cutlass::half_t>>, Is_dropout=true, Is_causal=true, Is_even_N=false, Is_even_K=true, Return_softmax=false, Params=Flash_fwd_params]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(15): here
            instantiation of "void flash_fwd_kernel<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax>(Flash_fwd_params) [with Kernel_traits=Flash_fwd_kernel_traits<256, 64, 64, 4, false, false, cutlass::half_t, Flash_kernel_traits<256, 64, 64, 4, cutlass::half_t>>, Is_dropout=true, Is_causal=true, Is_even_N=false, Is_even_K=true, Return_softmax=false]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(34): here
            instantiation of "void run_flash_fwd<Kernel_traits,Is_dropout,Is_causal>(Flash_fwd_params &, cudaStream_t) [with Kernel_traits=Flash_fwd_kernel_traits<256, 64, 64, 4, false, false, cutlass::half_t, Flash_kernel_traits<256, 64, 64, 4, cutlass::half_t>>, Is_dropout=true, Is_causal=true]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(236): here
            instantiation of "void run_mha_fwd_hdim256<T>(Flash_fwd_params &, cudaStream_t) [with T=cutlass::half_t]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_hdim256_fp16_sm80.cu(8): here

/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_kernel.h(276): warning: variable "tQrQ" was declared but never referenced
          detected during:
            instantiation of "void flash::compute_attn_1rowblock<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params,Is_Q_split_K>(const Params &, int, int, int) [with Kernel_traits=Flash_fwd_kernel_traits<256, 64, 64, 4, false, false, cutlass::half_t, Flash_kernel_traits<256, 64, 64, 4, cutlass::half_t>>, Is_dropout=true, Is_causal=true, Is_even_N=false, Is_even_K=true, Return_softmax=false, Params=Flash_fwd_params, Is_Q_split_K=false]" 
(589): here
            instantiation of "void flash::compute_attn<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &) [with Kernel_traits=Flash_fwd_kernel_traits<256, 64, 64, 4, false, false, cutlass::half_t, Flash_kernel_traits<256, 64, 64, 4, cutlass::half_t>>, Is_dropout=true, Is_causal=true, Is_even_N=false, Is_even_K=true, Return_softmax=false, Params=Flash_fwd_params]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(15): here
            instantiation of "void flash_fwd_kernel<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax>(Flash_fwd_params) [with Kernel_traits=Flash_fwd_kernel_traits<256, 64, 64, 4, false, false, cutlass::half_t, Flash_kernel_traits<256, 64, 64, 4, cutlass::half_t>>, Is_dropout=true, Is_causal=true, Is_even_N=false, Is_even_K=true, Return_softmax=false]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(34): here
            instantiation of "void run_flash_fwd<Kernel_traits,Is_dropout,Is_causal>(Flash_fwd_params &, cudaStream_t) [with Kernel_traits=Flash_fwd_kernel_traits<256, 64, 64, 4, false, false, cutlass::half_t, Flash_kernel_traits<256, 64, 64, 4, cutlass::half_t>>, Is_dropout=true, Is_causal=true]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(236): here
            instantiation of "void run_mha_fwd_hdim256<T>(Flash_fwd_params &, cudaStream_t) [with T=cutlass::half_t]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_hdim256_fp16_sm80.cu(8): here

/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_kernel.h(140): warning: variable "MMA_M" was declared but never referenced
          detected during:
            instantiation of "void flash::compute_attn_1rowblock<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params,Is_Q_split_K>(const Params &, int, int, int) [with Kernel_traits=Flash_fwd_kernel_traits<256, 64, 64, 4, false, false, cutlass::half_t, Flash_kernel_traits<256, 64, 64, 4, cutlass::half_t>>, Is_dropout=true, Is_causal=true, Is_even_N=false, Is_even_K=false, Return_softmax=true, Params=Flash_fwd_params, Is_Q_split_K=false]" 
(589): here
            instantiation of "void flash::compute_attn<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &) [with Kernel_traits=Flash_fwd_kernel_traits<256, 64, 64, 4, false, false, cutlass::half_t, Flash_kernel_traits<256, 64, 64, 4, cutlass::half_t>>, Is_dropout=true, Is_causal=true, Is_even_N=false, Is_even_K=false, Return_softmax=true, Params=Flash_fwd_params]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(15): here
            instantiation of "void flash_fwd_kernel<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax>(Flash_fwd_params) [with Kernel_traits=Flash_fwd_kernel_traits<256, 64, 64, 4, false, false, cutlass::half_t, Flash_kernel_traits<256, 64, 64, 4, cutlass::half_t>>, Is_dropout=true, Is_causal=true, Is_even_N=false, Is_even_K=false, Return_softmax=true]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(34): here
            instantiation of "void run_flash_fwd<Kernel_traits,Is_dropout,Is_causal>(Flash_fwd_params &, cudaStream_t) [with Kernel_traits=Flash_fwd_kernel_traits<256, 64, 64, 4, false, false, cutlass::half_t, Flash_kernel_traits<256, 64, 64, 4, cutlass::half_t>>, Is_dropout=true, Is_causal=true]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(236): here
            instantiation of "void run_mha_fwd_hdim256<T>(Flash_fwd_params &, cudaStream_t) [with T=cutlass::half_t]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_hdim256_fp16_sm80.cu(8): here

/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_kernel.h(276): warning: variable "tQrQ" was declared but never referenced
          detected during:
            instantiation of "void flash::compute_attn_1rowblock<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params,Is_Q_split_K>(const Params &, int, int, int) [with Kernel_traits=Flash_fwd_kernel_traits<256, 64, 64, 4, false, false, cutlass::half_t, Flash_kernel_traits<256, 64, 64, 4, cutlass::half_t>>, Is_dropout=true, Is_causal=true, Is_even_N=false, Is_even_K=false, Return_softmax=true, Params=Flash_fwd_params, Is_Q_split_K=false]" 
(589): here
            instantiation of "void flash::compute_attn<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &) [with Kernel_traits=Flash_fwd_kernel_traits<256, 64, 64, 4, false, false, cutlass::half_t, Flash_kernel_traits<256, 64, 64, 4, cutlass::half_t>>, Is_dropout=true, Is_causal=true, Is_even_N=false, Is_even_K=false, Return_softmax=true, Params=Flash_fwd_params]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(15): here
            instantiation of "void flash_fwd_kernel<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax>(Flash_fwd_params) [with Kernel_traits=Flash_fwd_kernel_traits<256, 64, 64, 4, false, false, cutlass::half_t, Flash_kernel_traits<256, 64, 64, 4, cutlass::half_t>>, Is_dropout=true, Is_causal=true, Is_even_N=false, Is_even_K=false, Return_softmax=true]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(34): here
            instantiation of "void run_flash_fwd<Kernel_traits,Is_dropout,Is_causal>(Flash_fwd_params &, cudaStream_t) [with Kernel_traits=Flash_fwd_kernel_traits<256, 64, 64, 4, false, false, cutlass::half_t, Flash_kernel_traits<256, 64, 64, 4, cutlass::half_t>>, Is_dropout=true, Is_causal=true]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(236): here
            instantiation of "void run_mha_fwd_hdim256<T>(Flash_fwd_params &, cudaStream_t) [with T=cutlass::half_t]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_hdim256_fp16_sm80.cu(8): here

/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_kernel.h(140): warning: variable "MMA_M" was declared but never referenced
          detected during:
            instantiation of "void flash::compute_attn_1rowblock<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params,Is_Q_split_K>(const Params &, int, int, int) [with Kernel_traits=Flash_fwd_kernel_traits<256, 64, 64, 4, false, false, cutlass::half_t, Flash_kernel_traits<256, 64, 64, 4, cutlass::half_t>>, Is_dropout=true, Is_causal=true, Is_even_N=false, Is_even_K=false, Return_softmax=false, Params=Flash_fwd_params, Is_Q_split_K=false]" 
(589): here
            instantiation of "void flash::compute_attn<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &) [with Kernel_traits=Flash_fwd_kernel_traits<256, 64, 64, 4, false, false, cutlass::half_t, Flash_kernel_traits<256, 64, 64, 4, cutlass::half_t>>, Is_dropout=true, Is_causal=true, Is_even_N=false, Is_even_K=false, Return_softmax=false, Params=Flash_fwd_params]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(15): here
            instantiation of "void flash_fwd_kernel<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax>(Flash_fwd_params) [with Kernel_traits=Flash_fwd_kernel_traits<256, 64, 64, 4, false, false, cutlass::half_t, Flash_kernel_traits<256, 64, 64, 4, cutlass::half_t>>, Is_dropout=true, Is_causal=true, Is_even_N=false, Is_even_K=false, Return_softmax=false]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(34): here
            instantiation of "void run_flash_fwd<Kernel_traits,Is_dropout,Is_causal>(Flash_fwd_params &, cudaStream_t) [with Kernel_traits=Flash_fwd_kernel_traits<256, 64, 64, 4, false, false, cutlass::half_t, Flash_kernel_traits<256, 64, 64, 4, cutlass::half_t>>, Is_dropout=true, Is_causal=true]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(236): here
            instantiation of "void run_mha_fwd_hdim256<T>(Flash_fwd_params &, cudaStream_t) [with T=cutlass::half_t]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_hdim256_fp16_sm80.cu(8): here

/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_kernel.h(276): warning: variable "tQrQ" was declared but never referenced
          detected during:
            instantiation of "void flash::compute_attn_1rowblock<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params,Is_Q_split_K>(const Params &, int, int, int) [with Kernel_traits=Flash_fwd_kernel_traits<256, 64, 64, 4, false, false, cutlass::half_t, Flash_kernel_traits<256, 64, 64, 4, cutlass::half_t>>, Is_dropout=true, Is_causal=true, Is_even_N=false, Is_even_K=false, Return_softmax=false, Params=Flash_fwd_params, Is_Q_split_K=false]" 
(589): here
            instantiation of "void flash::compute_attn<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &) [with Kernel_traits=Flash_fwd_kernel_traits<256, 64, 64, 4, false, false, cutlass::half_t, Flash_kernel_traits<256, 64, 64, 4, cutlass::half_t>>, Is_dropout=true, Is_causal=true, Is_even_N=false, Is_even_K=false, Return_softmax=false, Params=Flash_fwd_params]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(15): here
            instantiation of "void flash_fwd_kernel<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax>(Flash_fwd_params) [with Kernel_traits=Flash_fwd_kernel_traits<256, 64, 64, 4, false, false, cutlass::half_t, Flash_kernel_traits<256, 64, 64, 4, cutlass::half_t>>, Is_dropout=true, Is_causal=true, Is_even_N=false, Is_even_K=false, Return_softmax=false]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(34): here
            instantiation of "void run_flash_fwd<Kernel_traits,Is_dropout,Is_causal>(Flash_fwd_params &, cudaStream_t) [with Kernel_traits=Flash_fwd_kernel_traits<256, 64, 64, 4, false, false, cutlass::half_t, Flash_kernel_traits<256, 64, 64, 4, cutlass::half_t>>, Is_dropout=true, Is_causal=true]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(236): here
            instantiation of "void run_mha_fwd_hdim256<T>(Flash_fwd_params &, cudaStream_t) [with T=cutlass::half_t]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_hdim256_fp16_sm80.cu(8): here

/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_kernel.h(140): warning: variable "MMA_M" was declared but never referenced
          detected during:
            instantiation of "void flash::compute_attn_1rowblock<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params,Is_Q_split_K>(const Params &, int, int, int) [with Kernel_traits=Flash_fwd_kernel_traits<256, 128, 64, 8, false, false, cutlass::half_t, Flash_kernel_traits<256, 128, 64, 8, cutlass::half_t>>, Is_dropout=true, Is_causal=false, Is_even_N=true, Is_even_K=true, Return_softmax=true, Params=Flash_fwd_params, Is_Q_split_K=false]" 
(589): here
            instantiation of "void flash::compute_attn<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &) [with Kernel_traits=Flash_fwd_kernel_traits<256, 128, 64, 8, false, false, cutlass::half_t, Flash_kernel_traits<256, 128, 64, 8, cutlass::half_t>>, Is_dropout=true, Is_causal=false, Is_even_N=true, Is_even_K=true, Return_softmax=true, Params=Flash_fwd_params]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(15): here
            instantiation of "void flash_fwd_kernel<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax>(Flash_fwd_params) [with Kernel_traits=Flash_fwd_kernel_traits<256, 128, 64, 8, false, false, cutlass::half_t, Flash_kernel_traits<256, 128, 64, 8, cutlass::half_t>>, Is_dropout=true, Is_causal=false, Is_even_N=true, Is_even_K=true, Return_softmax=true]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(34): here
            instantiation of "void run_flash_fwd<Kernel_traits,Is_dropout,Is_causal>(Flash_fwd_params &, cudaStream_t) [with Kernel_traits=Flash_fwd_kernel_traits<256, 128, 64, 8, false, false, cutlass::half_t, Flash_kernel_traits<256, 128, 64, 8, cutlass::half_t>>, Is_dropout=true, Is_causal=false]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(236): here
            instantiation of "void run_mha_fwd_hdim256<T>(Flash_fwd_params &, cudaStream_t) [with T=cutlass::half_t]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_hdim256_fp16_sm80.cu(8): here

/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_kernel.h(276): warning: variable "tQrQ" was declared but never referenced
          detected during:
            instantiation of "void flash::compute_attn_1rowblock<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params,Is_Q_split_K>(const Params &, int, int, int) [with Kernel_traits=Flash_fwd_kernel_traits<256, 128, 64, 8, false, false, cutlass::half_t, Flash_kernel_traits<256, 128, 64, 8, cutlass::half_t>>, Is_dropout=true, Is_causal=false, Is_even_N=true, Is_even_K=true, Return_softmax=true, Params=Flash_fwd_params, Is_Q_split_K=false]" 
(589): here
            instantiation of "void flash::compute_attn<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &) [with Kernel_traits=Flash_fwd_kernel_traits<256, 128, 64, 8, false, false, cutlass::half_t, Flash_kernel_traits<256, 128, 64, 8, cutlass::half_t>>, Is_dropout=true, Is_causal=false, Is_even_N=true, Is_even_K=true, Return_softmax=true, Params=Flash_fwd_params]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(15): here
            instantiation of "void flash_fwd_kernel<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax>(Flash_fwd_params) [with Kernel_traits=Flash_fwd_kernel_traits<256, 128, 64, 8, false, false, cutlass::half_t, Flash_kernel_traits<256, 128, 64, 8, cutlass::half_t>>, Is_dropout=true, Is_causal=false, Is_even_N=true, Is_even_K=true, Return_softmax=true]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(34): here
            instantiation of "void run_flash_fwd<Kernel_traits,Is_dropout,Is_causal>(Flash_fwd_params &, cudaStream_t) [with Kernel_traits=Flash_fwd_kernel_traits<256, 128, 64, 8, false, false, cutlass::half_t, Flash_kernel_traits<256, 128, 64, 8, cutlass::half_t>>, Is_dropout=true, Is_causal=false]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(236): here
            instantiation of "void run_mha_fwd_hdim256<T>(Flash_fwd_params &, cudaStream_t) [with T=cutlass::half_t]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_hdim256_fp16_sm80.cu(8): here

/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_kernel.h(140): warning: variable "MMA_M" was declared but never referenced
          detected during:
            instantiation of "void flash::compute_attn_1rowblock<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params,Is_Q_split_K>(const Params &, int, int, int) [with Kernel_traits=Flash_fwd_kernel_traits<256, 128, 64, 8, false, false, cutlass::half_t, Flash_kernel_traits<256, 128, 64, 8, cutlass::half_t>>, Is_dropout=true, Is_causal=false, Is_even_N=true, Is_even_K=true, Return_softmax=false, Params=Flash_fwd_params, Is_Q_split_K=false]" 
(589): here
            instantiation of "void flash::compute_attn<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &) [with Kernel_traits=Flash_fwd_kernel_traits<256, 128, 64, 8, false, false, cutlass::half_t, Flash_kernel_traits<256, 128, 64, 8, cutlass::half_t>>, Is_dropout=true, Is_causal=false, Is_even_N=true, Is_even_K=true, Return_softmax=false, Params=Flash_fwd_params]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(15): here
            instantiation of "void flash_fwd_kernel<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax>(Flash_fwd_params) [with Kernel_traits=Flash_fwd_kernel_traits<256, 128, 64, 8, false, false, cutlass::half_t, Flash_kernel_traits<256, 128, 64, 8, cutlass::half_t>>, Is_dropout=true, Is_causal=false, Is_even_N=true, Is_even_K=true, Return_softmax=false]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(34): here
            instantiation of "void run_flash_fwd<Kernel_traits,Is_dropout,Is_causal>(Flash_fwd_params &, cudaStream_t) [with Kernel_traits=Flash_fwd_kernel_traits<256, 128, 64, 8, false, false, cutlass::half_t, Flash_kernel_traits<256, 128, 64, 8, cutlass::half_t>>, Is_dropout=true, Is_causal=false]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(236): here
            instantiation of "void run_mha_fwd_hdim256<T>(Flash_fwd_params &, cudaStream_t) [with T=cutlass::half_t]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_hdim256_fp16_sm80.cu(8): here

/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_kernel.h(276): warning: variable "tQrQ" was declared but never referenced
          detected during:
            instantiation of "void flash::compute_attn_1rowblock<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params,Is_Q_split_K>(const Params &, int, int, int) [with Kernel_traits=Flash_fwd_kernel_traits<256, 128, 64, 8, false, false, cutlass::half_t, Flash_kernel_traits<256, 128, 64, 8, cutlass::half_t>>, Is_dropout=true, Is_causal=false, Is_even_N=true, Is_even_K=true, Return_softmax=false, Params=Flash_fwd_params, Is_Q_split_K=false]" 
(589): here
            instantiation of "void flash::compute_attn<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &) [with Kernel_traits=Flash_fwd_kernel_traits<256, 128, 64, 8, false, false, cutlass::half_t, Flash_kernel_traits<256, 128, 64, 8, cutlass::half_t>>, Is_dropout=true, Is_causal=false, Is_even_N=true, Is_even_K=true, Return_softmax=false, Params=Flash_fwd_params]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(15): here
            instantiation of "void flash_fwd_kernel<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax>(Flash_fwd_params) [with Kernel_traits=Flash_fwd_kernel_traits<256, 128, 64, 8, false, false, cutlass::half_t, Flash_kernel_traits<256, 128, 64, 8, cutlass::half_t>>, Is_dropout=true, Is_causal=false, Is_even_N=true, Is_even_K=true, Return_softmax=false]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(34): here
            instantiation of "void run_flash_fwd<Kernel_traits,Is_dropout,Is_causal>(Flash_fwd_params &, cudaStream_t) [with Kernel_traits=Flash_fwd_kernel_traits<256, 128, 64, 8, false, false, cutlass::half_t, Flash_kernel_traits<256, 128, 64, 8, cutlass::half_t>>, Is_dropout=true, Is_causal=false]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(236): here
            instantiation of "void run_mha_fwd_hdim256<T>(Flash_fwd_params &, cudaStream_t) [with T=cutlass::half_t]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_hdim256_fp16_sm80.cu(8): here

/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_kernel.h(140): warning: variable "MMA_M" was declared but never referenced
          detected during:
            instantiation of "void flash::compute_attn_1rowblock<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params,Is_Q_split_K>(const Params &, int, int, int) [with Kernel_traits=Flash_fwd_kernel_traits<256, 128, 64, 8, false, false, cutlass::half_t, Flash_kernel_traits<256, 128, 64, 8, cutlass::half_t>>, Is_dropout=true, Is_causal=false, Is_even_N=true, Is_even_K=false, Return_softmax=true, Params=Flash_fwd_params, Is_Q_split_K=false]" 
(589): here
            instantiation of "void flash::compute_attn<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &) [with Kernel_traits=Flash_fwd_kernel_traits<256, 128, 64, 8, false, false, cutlass::half_t, Flash_kernel_traits<256, 128, 64, 8, cutlass::half_t>>, Is_dropout=true, Is_causal=false, Is_even_N=true, Is_even_K=false, Return_softmax=true, Params=Flash_fwd_params]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(15): here
            instantiation of "void flash_fwd_kernel<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax>(Flash_fwd_params) [with Kernel_traits=Flash_fwd_kernel_traits<256, 128, 64, 8, false, false, cutlass::half_t, Flash_kernel_traits<256, 128, 64, 8, cutlass::half_t>>, Is_dropout=true, Is_causal=false, Is_even_N=true, Is_even_K=false, Return_softmax=true]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(34): here
            instantiation of "void run_flash_fwd<Kernel_traits,Is_dropout,Is_causal>(Flash_fwd_params &, cudaStream_t) [with Kernel_traits=Flash_fwd_kernel_traits<256, 128, 64, 8, false, false, cutlass::half_t, Flash_kernel_traits<256, 128, 64, 8, cutlass::half_t>>, Is_dropout=true, Is_causal=false]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(236): here
            instantiation of "void run_mha_fwd_hdim256<T>(Flash_fwd_params &, cudaStream_t) [with T=cutlass::half_t]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_hdim256_fp16_sm80.cu(8): here

/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_kernel.h(276): warning: variable "tQrQ" was declared but never referenced
          detected during:
            instantiation of "void flash::compute_attn_1rowblock<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params,Is_Q_split_K>(const Params &, int, int, int) [with Kernel_traits=Flash_fwd_kernel_traits<256, 128, 64, 8, false, false, cutlass::half_t, Flash_kernel_traits<256, 128, 64, 8, cutlass::half_t>>, Is_dropout=true, Is_causal=false, Is_even_N=true, Is_even_K=false, Return_softmax=true, Params=Flash_fwd_params, Is_Q_split_K=false]" 
(589): here
            instantiation of "void flash::compute_attn<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &) [with Kernel_traits=Flash_fwd_kernel_traits<256, 128, 64, 8, false, false, cutlass::half_t, Flash_kernel_traits<256, 128, 64, 8, cutlass::half_t>>, Is_dropout=true, Is_causal=false, Is_even_N=true, Is_even_K=false, Return_softmax=true, Params=Flash_fwd_params]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(15): here
            instantiation of "void flash_fwd_kernel<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax>(Flash_fwd_params) [with Kernel_traits=Flash_fwd_kernel_traits<256, 128, 64, 8, false, false, cutlass::half_t, Flash_kernel_traits<256, 128, 64, 8, cutlass::half_t>>, Is_dropout=true, Is_causal=false, Is_even_N=true, Is_even_K=false, Return_softmax=true]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(34): here
            instantiation of "void run_flash_fwd<Kernel_traits,Is_dropout,Is_causal>(Flash_fwd_params &, cudaStream_t) [with Kernel_traits=Flash_fwd_kernel_traits<256, 128, 64, 8, false, false, cutlass::half_t, Flash_kernel_traits<256, 128, 64, 8, cutlass::half_t>>, Is_dropout=true, Is_causal=false]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(236): here
            instantiation of "void run_mha_fwd_hdim256<T>(Flash_fwd_params &, cudaStream_t) [with T=cutlass::half_t]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_hdim256_fp16_sm80.cu(8): here

/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_kernel.h(140): warning: variable "MMA_M" was declared but never referenced
          detected during:
            instantiation of "void flash::compute_attn_1rowblock<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params,Is_Q_split_K>(const Params &, int, int, int) [with Kernel_traits=Flash_fwd_kernel_traits<256, 128, 64, 8, false, false, cutlass::half_t, Flash_kernel_traits<256, 128, 64, 8, cutlass::half_t>>, Is_dropout=true, Is_causal=false, Is_even_N=true, Is_even_K=false, Return_softmax=false, Params=Flash_fwd_params, Is_Q_split_K=false]" 
(589): here
            instantiation of "void flash::compute_attn<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &) [with Kernel_traits=Flash_fwd_kernel_traits<256, 128, 64, 8, false, false, cutlass::half_t, Flash_kernel_traits<256, 128, 64, 8, cutlass::half_t>>, Is_dropout=true, Is_causal=false, Is_even_N=true, Is_even_K=false, Return_softmax=false, Params=Flash_fwd_params]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(15): here
            instantiation of "void flash_fwd_kernel<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax>(Flash_fwd_params) [with Kernel_traits=Flash_fwd_kernel_traits<256, 128, 64, 8, false, false, cutlass::half_t, Flash_kernel_traits<256, 128, 64, 8, cutlass::half_t>>, Is_dropout=true, Is_causal=false, Is_even_N=true, Is_even_K=false, Return_softmax=false]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(34): here
            instantiation of "void run_flash_fwd<Kernel_traits,Is_dropout,Is_causal>(Flash_fwd_params &, cudaStream_t) [with Kernel_traits=Flash_fwd_kernel_traits<256, 128, 64, 8, false, false, cutlass::half_t, Flash_kernel_traits<256, 128, 64, 8, cutlass::half_t>>, Is_dropout=true, Is_causal=false]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(236): here
            instantiation of "void run_mha_fwd_hdim256<T>(Flash_fwd_params &, cudaStream_t) [with T=cutlass::half_t]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_hdim256_fp16_sm80.cu(8): here

/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_kernel.h(276): warning: variable "tQrQ" was declared but never referenced
          detected during:
            instantiation of "void flash::compute_attn_1rowblock<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params,Is_Q_split_K>(const Params &, int, int, int) [with Kernel_traits=Flash_fwd_kernel_traits<256, 128, 64, 8, false, false, cutlass::half_t, Flash_kernel_traits<256, 128, 64, 8, cutlass::half_t>>, Is_dropout=true, Is_causal=false, Is_even_N=true, Is_even_K=false, Return_softmax=false, Params=Flash_fwd_params, Is_Q_split_K=false]" 
(589): here
            instantiation of "void flash::compute_attn<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &) [with Kernel_traits=Flash_fwd_kernel_traits<256, 128, 64, 8, false, false, cutlass::half_t, Flash_kernel_traits<256, 128, 64, 8, cutlass::half_t>>, Is_dropout=true, Is_causal=false, Is_even_N=true, Is_even_K=false, Return_softmax=false, Params=Flash_fwd_params]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(15): here
            instantiation of "void flash_fwd_kernel<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax>(Flash_fwd_params) [with Kernel_traits=Flash_fwd_kernel_traits<256, 128, 64, 8, false, false, cutlass::half_t, Flash_kernel_traits<256, 128, 64, 8, cutlass::half_t>>, Is_dropout=true, Is_causal=false, Is_even_N=true, Is_even_K=false, Return_softmax=false]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(34): here
            instantiation of "void run_flash_fwd<Kernel_traits,Is_dropout,Is_causal>(Flash_fwd_params &, cudaStream_t) [with Kernel_traits=Flash_fwd_kernel_traits<256, 128, 64, 8, false, false, cutlass::half_t, Flash_kernel_traits<256, 128, 64, 8, cutlass::half_t>>, Is_dropout=true, Is_causal=false]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(236): here
            instantiation of "void run_mha_fwd_hdim256<T>(Flash_fwd_params &, cudaStream_t) [with T=cutlass::half_t]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_hdim256_fp16_sm80.cu(8): here

/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_kernel.h(140): warning: variable "MMA_M" was declared but never referenced
          detected during:
            instantiation of "void flash::compute_attn_1rowblock<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params,Is_Q_split_K>(const Params &, int, int, int) [with Kernel_traits=Flash_fwd_kernel_traits<256, 128, 64, 8, false, false, cutlass::half_t, Flash_kernel_traits<256, 128, 64, 8, cutlass::half_t>>, Is_dropout=true, Is_causal=false, Is_even_N=false, Is_even_K=true, Return_softmax=true, Params=Flash_fwd_params, Is_Q_split_K=false]" 
(589): here
            instantiation of "void flash::compute_attn<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &) [with Kernel_traits=Flash_fwd_kernel_traits<256, 128, 64, 8, false, false, cutlass::half_t, Flash_kernel_traits<256, 128, 64, 8, cutlass::half_t>>, Is_dropout=true, Is_causal=false, Is_even_N=false, Is_even_K=true, Return_softmax=true, Params=Flash_fwd_params]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(15): here
            instantiation of "void flash_fwd_kernel<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax>(Flash_fwd_params) [with Kernel_traits=Flash_fwd_kernel_traits<256, 128, 64, 8, false, false, cutlass::half_t, Flash_kernel_traits<256, 128, 64, 8, cutlass::half_t>>, Is_dropout=true, Is_causal=false, Is_even_N=false, Is_even_K=true, Return_softmax=true]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(34): here
            instantiation of "void run_flash_fwd<Kernel_traits,Is_dropout,Is_causal>(Flash_fwd_params &, cudaStream_t) [with Kernel_traits=Flash_fwd_kernel_traits<256, 128, 64, 8, false, false, cutlass::half_t, Flash_kernel_traits<256, 128, 64, 8, cutlass::half_t>>, Is_dropout=true, Is_causal=false]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(236): here
            instantiation of "void run_mha_fwd_hdim256<T>(Flash_fwd_params &, cudaStream_t) [with T=cutlass::half_t]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_hdim256_fp16_sm80.cu(8): here

/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_kernel.h(276): warning: variable "tQrQ" was declared but never referenced
          detected during:
            instantiation of "void flash::compute_attn_1rowblock<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params,Is_Q_split_K>(const Params &, int, int, int) [with Kernel_traits=Flash_fwd_kernel_traits<256, 128, 64, 8, false, false, cutlass::half_t, Flash_kernel_traits<256, 128, 64, 8, cutlass::half_t>>, Is_dropout=true, Is_causal=false, Is_even_N=false, Is_even_K=true, Return_softmax=true, Params=Flash_fwd_params, Is_Q_split_K=false]" 
(589): here
            instantiation of "void flash::compute_attn<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &) [with Kernel_traits=Flash_fwd_kernel_traits<256, 128, 64, 8, false, false, cutlass::half_t, Flash_kernel_traits<256, 128, 64, 8, cutlass::half_t>>, Is_dropout=true, Is_causal=false, Is_even_N=false, Is_even_K=true, Return_softmax=true, Params=Flash_fwd_params]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(15): here
            instantiation of "void flash_fwd_kernel<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax>(Flash_fwd_params) [with Kernel_traits=Flash_fwd_kernel_traits<256, 128, 64, 8, false, false, cutlass::half_t, Flash_kernel_traits<256, 128, 64, 8, cutlass::half_t>>, Is_dropout=true, Is_causal=false, Is_even_N=false, Is_even_K=true, Return_softmax=true]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(34): here
            instantiation of "void run_flash_fwd<Kernel_traits,Is_dropout,Is_causal>(Flash_fwd_params &, cudaStream_t) [with Kernel_traits=Flash_fwd_kernel_traits<256, 128, 64, 8, false, false, cutlass::half_t, Flash_kernel_traits<256, 128, 64, 8, cutlass::half_t>>, Is_dropout=true, Is_causal=false]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(236): here
            instantiation of "void run_mha_fwd_hdim256<T>(Flash_fwd_params &, cudaStream_t) [with T=cutlass::half_t]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_hdim256_fp16_sm80.cu(8): here

/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_kernel.h(140): warning: variable "MMA_M" was declared but never referenced
          detected during:
            instantiation of "void flash::compute_attn_1rowblock<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params,Is_Q_split_K>(const Params &, int, int, int) [with Kernel_traits=Flash_fwd_kernel_traits<256, 128, 64, 8, false, false, cutlass::half_t, Flash_kernel_traits<256, 128, 64, 8, cutlass::half_t>>, Is_dropout=true, Is_causal=false, Is_even_N=false, Is_even_K=true, Return_softmax=false, Params=Flash_fwd_params, Is_Q_split_K=false]" 
(589): here
            instantiation of "void flash::compute_attn<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &) [with Kernel_traits=Flash_fwd_kernel_traits<256, 128, 64, 8, false, false, cutlass::half_t, Flash_kernel_traits<256, 128, 64, 8, cutlass::half_t>>, Is_dropout=true, Is_causal=false, Is_even_N=false, Is_even_K=true, Return_softmax=false, Params=Flash_fwd_params]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(15): here
            instantiation of "void flash_fwd_kernel<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax>(Flash_fwd_params) [with Kernel_traits=Flash_fwd_kernel_traits<256, 128, 64, 8, false, false, cutlass::half_t, Flash_kernel_traits<256, 128, 64, 8, cutlass::half_t>>, Is_dropout=true, Is_causal=false, Is_even_N=false, Is_even_K=true, Return_softmax=false]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(34): here
            instantiation of "void run_flash_fwd<Kernel_traits,Is_dropout,Is_causal>(Flash_fwd_params &, cudaStream_t) [with Kernel_traits=Flash_fwd_kernel_traits<256, 128, 64, 8, false, false, cutlass::half_t, Flash_kernel_traits<256, 128, 64, 8, cutlass::half_t>>, Is_dropout=true, Is_causal=false]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(236): here
            instantiation of "void run_mha_fwd_hdim256<T>(Flash_fwd_params &, cudaStream_t) [with T=cutlass::half_t]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_hdim256_fp16_sm80.cu(8): here

/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_kernel.h(276): warning: variable "tQrQ" was declared but never referenced
          detected during:
            instantiation of "void flash::compute_attn_1rowblock<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params,Is_Q_split_K>(const Params &, int, int, int) [with Kernel_traits=Flash_fwd_kernel_traits<256, 128, 64, 8, false, false, cutlass::half_t, Flash_kernel_traits<256, 128, 64, 8, cutlass::half_t>>, Is_dropout=true, Is_causal=false, Is_even_N=false, Is_even_K=true, Return_softmax=false, Params=Flash_fwd_params, Is_Q_split_K=false]" 
(589): here
            instantiation of "void flash::compute_attn<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &) [with Kernel_traits=Flash_fwd_kernel_traits<256, 128, 64, 8, false, false, cutlass::half_t, Flash_kernel_traits<256, 128, 64, 8, cutlass::half_t>>, Is_dropout=true, Is_causal=false, Is_even_N=false, Is_even_K=true, Return_softmax=false, Params=Flash_fwd_params]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(15): here
            instantiation of "void flash_fwd_kernel<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax>(Flash_fwd_params) [with Kernel_traits=Flash_fwd_kernel_traits<256, 128, 64, 8, false, false, cutlass::half_t, Flash_kernel_traits<256, 128, 64, 8, cutlass::half_t>>, Is_dropout=true, Is_causal=false, Is_even_N=false, Is_even_K=true, Return_softmax=false]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(34): here
            instantiation of "void run_flash_fwd<Kernel_traits,Is_dropout,Is_causal>(Flash_fwd_params &, cudaStream_t) [with Kernel_traits=Flash_fwd_kernel_traits<256, 128, 64, 8, false, false, cutlass::half_t, Flash_kernel_traits<256, 128, 64, 8, cutlass::half_t>>, Is_dropout=true, Is_causal=false]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(236): here
            instantiation of "void run_mha_fwd_hdim256<T>(Flash_fwd_params &, cudaStream_t) [with T=cutlass::half_t]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_hdim256_fp16_sm80.cu(8): here

/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_kernel.h(140): warning: variable "MMA_M" was declared but never referenced
          detected during:
            instantiation of "void flash::compute_attn_1rowblock<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params,Is_Q_split_K>(const Params &, int, int, int) [with Kernel_traits=Flash_fwd_kernel_traits<256, 128, 64, 8, false, false, cutlass::half_t, Flash_kernel_traits<256, 128, 64, 8, cutlass::half_t>>, Is_dropout=true, Is_causal=false, Is_even_N=false, Is_even_K=false, Return_softmax=true, Params=Flash_fwd_params, Is_Q_split_K=false]" 
(589): here
            instantiation of "void flash::compute_attn<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &) [with Kernel_traits=Flash_fwd_kernel_traits<256, 128, 64, 8, false, false, cutlass::half_t, Flash_kernel_traits<256, 128, 64, 8, cutlass::half_t>>, Is_dropout=true, Is_causal=false, Is_even_N=false, Is_even_K=false, Return_softmax=true, Params=Flash_fwd_params]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(15): here
            instantiation of "void flash_fwd_kernel<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax>(Flash_fwd_params) [with Kernel_traits=Flash_fwd_kernel_traits<256, 128, 64, 8, false, false, cutlass::half_t, Flash_kernel_traits<256, 128, 64, 8, cutlass::half_t>>, Is_dropout=true, Is_causal=false, Is_even_N=false, Is_even_K=false, Return_softmax=true]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(34): here
            instantiation of "void run_flash_fwd<Kernel_traits,Is_dropout,Is_causal>(Flash_fwd_params &, cudaStream_t) [with Kernel_traits=Flash_fwd_kernel_traits<256, 128, 64, 8, false, false, cutlass::half_t, Flash_kernel_traits<256, 128, 64, 8, cutlass::half_t>>, Is_dropout=true, Is_causal=false]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(236): here
            instantiation of "void run_mha_fwd_hdim256<T>(Flash_fwd_params &, cudaStream_t) [with T=cutlass::half_t]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_hdim256_fp16_sm80.cu(8): here

/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_kernel.h(276): warning: variable "tQrQ" was declared but never referenced
          detected during:
            instantiation of "void flash::compute_attn_1rowblock<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params,Is_Q_split_K>(const Params &, int, int, int) [with Kernel_traits=Flash_fwd_kernel_traits<256, 128, 64, 8, false, false, cutlass::half_t, Flash_kernel_traits<256, 128, 64, 8, cutlass::half_t>>, Is_dropout=true, Is_causal=false, Is_even_N=false, Is_even_K=false, Return_softmax=true, Params=Flash_fwd_params, Is_Q_split_K=false]" 
(589): here
            instantiation of "void flash::compute_attn<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &) [with Kernel_traits=Flash_fwd_kernel_traits<256, 128, 64, 8, false, false, cutlass::half_t, Flash_kernel_traits<256, 128, 64, 8, cutlass::half_t>>, Is_dropout=true, Is_causal=false, Is_even_N=false, Is_even_K=false, Return_softmax=true, Params=Flash_fwd_params]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(15): here
            instantiation of "void flash_fwd_kernel<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax>(Flash_fwd_params) [with Kernel_traits=Flash_fwd_kernel_traits<256, 128, 64, 8, false, false, cutlass::half_t, Flash_kernel_traits<256, 128, 64, 8, cutlass::half_t>>, Is_dropout=true, Is_causal=false, Is_even_N=false, Is_even_K=false, Return_softmax=true]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(34): here
            instantiation of "void run_flash_fwd<Kernel_traits,Is_dropout,Is_causal>(Flash_fwd_params &, cudaStream_t) [with Kernel_traits=Flash_fwd_kernel_traits<256, 128, 64, 8, false, false, cutlass::half_t, Flash_kernel_traits<256, 128, 64, 8, cutlass::half_t>>, Is_dropout=true, Is_causal=false]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(236): here
            instantiation of "void run_mha_fwd_hdim256<T>(Flash_fwd_params &, cudaStream_t) [with T=cutlass::half_t]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_hdim256_fp16_sm80.cu(8): here

/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_kernel.h(140): warning: variable "MMA_M" was declared but never referenced
          detected during:
            instantiation of "void flash::compute_attn_1rowblock<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params,Is_Q_split_K>(const Params &, int, int, int) [with Kernel_traits=Flash_fwd_kernel_traits<256, 128, 64, 8, false, false, cutlass::half_t, Flash_kernel_traits<256, 128, 64, 8, cutlass::half_t>>, Is_dropout=true, Is_causal=false, Is_even_N=false, Is_even_K=false, Return_softmax=false, Params=Flash_fwd_params, Is_Q_split_K=false]" 
(589): here
            instantiation of "void flash::compute_attn<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &) [with Kernel_traits=Flash_fwd_kernel_traits<256, 128, 64, 8, false, false, cutlass::half_t, Flash_kernel_traits<256, 128, 64, 8, cutlass::half_t>>, Is_dropout=true, Is_causal=false, Is_even_N=false, Is_even_K=false, Return_softmax=false, Params=Flash_fwd_params]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(15): here
            instantiation of "void flash_fwd_kernel<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax>(Flash_fwd_params) [with Kernel_traits=Flash_fwd_kernel_traits<256, 128, 64, 8, false, false, cutlass::half_t, Flash_kernel_traits<256, 128, 64, 8, cutlass::half_t>>, Is_dropout=true, Is_causal=false, Is_even_N=false, Is_even_K=false, Return_softmax=false]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(34): here
            instantiation of "void run_flash_fwd<Kernel_traits,Is_dropout,Is_causal>(Flash_fwd_params &, cudaStream_t) [with Kernel_traits=Flash_fwd_kernel_traits<256, 128, 64, 8, false, false, cutlass::half_t, Flash_kernel_traits<256, 128, 64, 8, cutlass::half_t>>, Is_dropout=true, Is_causal=false]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(236): here
            instantiation of "void run_mha_fwd_hdim256<T>(Flash_fwd_params &, cudaStream_t) [with T=cutlass::half_t]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_hdim256_fp16_sm80.cu(8): here

/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_kernel.h(276): warning: variable "tQrQ" was declared but never referenced
          detected during:
            instantiation of "void flash::compute_attn_1rowblock<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params,Is_Q_split_K>(const Params &, int, int, int) [with Kernel_traits=Flash_fwd_kernel_traits<256, 128, 64, 8, false, false, cutlass::half_t, Flash_kernel_traits<256, 128, 64, 8, cutlass::half_t>>, Is_dropout=true, Is_causal=false, Is_even_N=false, Is_even_K=false, Return_softmax=false, Params=Flash_fwd_params, Is_Q_split_K=false]" 
(589): here
            instantiation of "void flash::compute_attn<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &) [with Kernel_traits=Flash_fwd_kernel_traits<256, 128, 64, 8, false, false, cutlass::half_t, Flash_kernel_traits<256, 128, 64, 8, cutlass::half_t>>, Is_dropout=true, Is_causal=false, Is_even_N=false, Is_even_K=false, Return_softmax=false, Params=Flash_fwd_params]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(15): here
            instantiation of "void flash_fwd_kernel<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax>(Flash_fwd_params) [with Kernel_traits=Flash_fwd_kernel_traits<256, 128, 64, 8, false, false, cutlass::half_t, Flash_kernel_traits<256, 128, 64, 8, cutlass::half_t>>, Is_dropout=true, Is_causal=false, Is_even_N=false, Is_even_K=false, Return_softmax=false]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(34): here
            instantiation of "void run_flash_fwd<Kernel_traits,Is_dropout,Is_causal>(Flash_fwd_params &, cudaStream_t) [with Kernel_traits=Flash_fwd_kernel_traits<256, 128, 64, 8, false, false, cutlass::half_t, Flash_kernel_traits<256, 128, 64, 8, cutlass::half_t>>, Is_dropout=true, Is_causal=false]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(236): here
            instantiation of "void run_mha_fwd_hdim256<T>(Flash_fwd_params &, cudaStream_t) [with T=cutlass::half_t]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_hdim256_fp16_sm80.cu(8): here

/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_kernel.h(140): warning: variable "MMA_M" was declared but never referenced
          detected during:
            instantiation of "void flash::compute_attn_1rowblock<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params,Is_Q_split_K>(const Params &, int, int, int) [with Kernel_traits=Flash_fwd_kernel_traits<256, 64, 64, 4, false, false, cutlass::half_t, Flash_kernel_traits<256, 64, 64, 4, cutlass::half_t>>, Is_dropout=true, Is_causal=false, Is_even_N=true, Is_even_K=true, Return_softmax=true, Params=Flash_fwd_params, Is_Q_split_K=false]" 
(589): here
            instantiation of "void flash::compute_attn<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &) [with Kernel_traits=Flash_fwd_kernel_traits<256, 64, 64, 4, false, false, cutlass::half_t, Flash_kernel_traits<256, 64, 64, 4, cutlass::half_t>>, Is_dropout=true, Is_causal=false, Is_even_N=true, Is_even_K=true, Return_softmax=true, Params=Flash_fwd_params]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(15): here
            instantiation of "void flash_fwd_kernel<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax>(Flash_fwd_params) [with Kernel_traits=Flash_fwd_kernel_traits<256, 64, 64, 4, false, false, cutlass::half_t, Flash_kernel_traits<256, 64, 64, 4, cutlass::half_t>>, Is_dropout=true, Is_causal=false, Is_even_N=true, Is_even_K=true, Return_softmax=true]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(34): here
            instantiation of "void run_flash_fwd<Kernel_traits,Is_dropout,Is_causal>(Flash_fwd_params &, cudaStream_t) [with Kernel_traits=Flash_fwd_kernel_traits<256, 64, 64, 4, false, false, cutlass::half_t, Flash_kernel_traits<256, 64, 64, 4, cutlass::half_t>>, Is_dropout=true, Is_causal=false]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(236): here
            instantiation of "void run_mha_fwd_hdim256<T>(Flash_fwd_params &, cudaStream_t) [with T=cutlass::half_t]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_hdim256_fp16_sm80.cu(8): here

/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_kernel.h(276): warning: variable "tQrQ" was declared but never referenced
          detected during:
            instantiation of "void flash::compute_attn_1rowblock<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params,Is_Q_split_K>(const Params &, int, int, int) [with Kernel_traits=Flash_fwd_kernel_traits<256, 64, 64, 4, false, false, cutlass::half_t, Flash_kernel_traits<256, 64, 64, 4, cutlass::half_t>>, Is_dropout=true, Is_causal=false, Is_even_N=true, Is_even_K=true, Return_softmax=true, Params=Flash_fwd_params, Is_Q_split_K=false]" 
(589): here
            instantiation of "void flash::compute_attn<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &) [with Kernel_traits=Flash_fwd_kernel_traits<256, 64, 64, 4, false, false, cutlass::half_t, Flash_kernel_traits<256, 64, 64, 4, cutlass::half_t>>, Is_dropout=true, Is_causal=false, Is_even_N=true, Is_even_K=true, Return_softmax=true, Params=Flash_fwd_params]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(15): here
            instantiation of "void flash_fwd_kernel<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax>(Flash_fwd_params) [with Kernel_traits=Flash_fwd_kernel_traits<256, 64, 64, 4, false, false, cutlass::half_t, Flash_kernel_traits<256, 64, 64, 4, cutlass::half_t>>, Is_dropout=true, Is_causal=false, Is_even_N=true, Is_even_K=true, Return_softmax=true]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(34): here
            instantiation of "void run_flash_fwd<Kernel_traits,Is_dropout,Is_causal>(Flash_fwd_params &, cudaStream_t) [with Kernel_traits=Flash_fwd_kernel_traits<256, 64, 64, 4, false, false, cutlass::half_t, Flash_kernel_traits<256, 64, 64, 4, cutlass::half_t>>, Is_dropout=true, Is_causal=false]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(236): here
            instantiation of "void run_mha_fwd_hdim256<T>(Flash_fwd_params &, cudaStream_t) [with T=cutlass::half_t]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_hdim256_fp16_sm80.cu(8): here

/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_kernel.h(140): warning: variable "MMA_M" was declared but never referenced
          detected during:
            instantiation of "void flash::compute_attn_1rowblock<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params,Is_Q_split_K>(const Params &, int, int, int) [with Kernel_traits=Flash_fwd_kernel_traits<256, 64, 64, 4, false, false, cutlass::half_t, Flash_kernel_traits<256, 64, 64, 4, cutlass::half_t>>, Is_dropout=true, Is_causal=false, Is_even_N=true, Is_even_K=true, Return_softmax=false, Params=Flash_fwd_params, Is_Q_split_K=false]" 
(589): here
            instantiation of "void flash::compute_attn<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &) [with Kernel_traits=Flash_fwd_kernel_traits<256, 64, 64, 4, false, false, cutlass::half_t, Flash_kernel_traits<256, 64, 64, 4, cutlass::half_t>>, Is_dropout=true, Is_causal=false, Is_even_N=true, Is_even_K=true, Return_softmax=false, Params=Flash_fwd_params]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(15): here
            instantiation of "void flash_fwd_kernel<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax>(Flash_fwd_params) [with Kernel_traits=Flash_fwd_kernel_traits<256, 64, 64, 4, false, false, cutlass::half_t, Flash_kernel_traits<256, 64, 64, 4, cutlass::half_t>>, Is_dropout=true, Is_causal=false, Is_even_N=true, Is_even_K=true, Return_softmax=false]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(34): here
            instantiation of "void run_flash_fwd<Kernel_traits,Is_dropout,Is_causal>(Flash_fwd_params &, cudaStream_t) [with Kernel_traits=Flash_fwd_kernel_traits<256, 64, 64, 4, false, false, cutlass::half_t, Flash_kernel_traits<256, 64, 64, 4, cutlass::half_t>>, Is_dropout=true, Is_causal=false]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(236): here
            instantiation of "void run_mha_fwd_hdim256<T>(Flash_fwd_params &, cudaStream_t) [with T=cutlass::half_t]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_hdim256_fp16_sm80.cu(8): here

/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_kernel.h(276): warning: variable "tQrQ" was declared but never referenced
          detected during:
            instantiation of "void flash::compute_attn_1rowblock<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params,Is_Q_split_K>(const Params &, int, int, int) [with Kernel_traits=Flash_fwd_kernel_traits<256, 64, 64, 4, false, false, cutlass::half_t, Flash_kernel_traits<256, 64, 64, 4, cutlass::half_t>>, Is_dropout=true, Is_causal=false, Is_even_N=true, Is_even_K=true, Return_softmax=false, Params=Flash_fwd_params, Is_Q_split_K=false]" 
(589): here
            instantiation of "void flash::compute_attn<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &) [with Kernel_traits=Flash_fwd_kernel_traits<256, 64, 64, 4, false, false, cutlass::half_t, Flash_kernel_traits<256, 64, 64, 4, cutlass::half_t>>, Is_dropout=true, Is_causal=false, Is_even_N=true, Is_even_K=true, Return_softmax=false, Params=Flash_fwd_params]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(15): here
            instantiation of "void flash_fwd_kernel<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax>(Flash_fwd_params) [with Kernel_traits=Flash_fwd_kernel_traits<256, 64, 64, 4, false, false, cutlass::half_t, Flash_kernel_traits<256, 64, 64, 4, cutlass::half_t>>, Is_dropout=true, Is_causal=false, Is_even_N=true, Is_even_K=true, Return_softmax=false]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(34): here
            instantiation of "void run_flash_fwd<Kernel_traits,Is_dropout,Is_causal>(Flash_fwd_params &, cudaStream_t) [with Kernel_traits=Flash_fwd_kernel_traits<256, 64, 64, 4, false, false, cutlass::half_t, Flash_kernel_traits<256, 64, 64, 4, cutlass::half_t>>, Is_dropout=true, Is_causal=false]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(236): here
            instantiation of "void run_mha_fwd_hdim256<T>(Flash_fwd_params &, cudaStream_t) [with T=cutlass::half_t]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_hdim256_fp16_sm80.cu(8): here

/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_kernel.h(140): warning: variable "MMA_M" was declared but never referenced
          detected during:
            instantiation of "void flash::compute_attn_1rowblock<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params,Is_Q_split_K>(const Params &, int, int, int) [with Kernel_traits=Flash_fwd_kernel_traits<256, 64, 64, 4, false, false, cutlass::half_t, Flash_kernel_traits<256, 64, 64, 4, cutlass::half_t>>, Is_dropout=true, Is_causal=false, Is_even_N=true, Is_even_K=false, Return_softmax=true, Params=Flash_fwd_params, Is_Q_split_K=false]" 
(589): here
            instantiation of "void flash::compute_attn<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &) [with Kernel_traits=Flash_fwd_kernel_traits<256, 64, 64, 4, false, false, cutlass::half_t, Flash_kernel_traits<256, 64, 64, 4, cutlass::half_t>>, Is_dropout=true, Is_causal=false, Is_even_N=true, Is_even_K=false, Return_softmax=true, Params=Flash_fwd_params]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(15): here
            instantiation of "void flash_fwd_kernel<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax>(Flash_fwd_params) [with Kernel_traits=Flash_fwd_kernel_traits<256, 64, 64, 4, false, false, cutlass::half_t, Flash_kernel_traits<256, 64, 64, 4, cutlass::half_t>>, Is_dropout=true, Is_causal=false, Is_even_N=true, Is_even_K=false, Return_softmax=true]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(34): here
            instantiation of "void run_flash_fwd<Kernel_traits,Is_dropout,Is_causal>(Flash_fwd_params &, cudaStream_t) [with Kernel_traits=Flash_fwd_kernel_traits<256, 64, 64, 4, false, false, cutlass::half_t, Flash_kernel_traits<256, 64, 64, 4, cutlass::half_t>>, Is_dropout=true, Is_causal=false]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(236): here
            instantiation of "void run_mha_fwd_hdim256<T>(Flash_fwd_params &, cudaStream_t) [with T=cutlass::half_t]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_hdim256_fp16_sm80.cu(8): here

/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_kernel.h(276): warning: variable "tQrQ" was declared but never referenced
          detected during:
            instantiation of "void flash::compute_attn_1rowblock<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params,Is_Q_split_K>(const Params &, int, int, int) [with Kernel_traits=Flash_fwd_kernel_traits<256, 64, 64, 4, false, false, cutlass::half_t, Flash_kernel_traits<256, 64, 64, 4, cutlass::half_t>>, Is_dropout=true, Is_causal=false, Is_even_N=true, Is_even_K=false, Return_softmax=true, Params=Flash_fwd_params, Is_Q_split_K=false]" 
(589): here
            instantiation of "void flash::compute_attn<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &) [with Kernel_traits=Flash_fwd_kernel_traits<256, 64, 64, 4, false, false, cutlass::half_t, Flash_kernel_traits<256, 64, 64, 4, cutlass::half_t>>, Is_dropout=true, Is_causal=false, Is_even_N=true, Is_even_K=false, Return_softmax=true, Params=Flash_fwd_params]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(15): here
            instantiation of "void flash_fwd_kernel<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax>(Flash_fwd_params) [with Kernel_traits=Flash_fwd_kernel_traits<256, 64, 64, 4, false, false, cutlass::half_t, Flash_kernel_traits<256, 64, 64, 4, cutlass::half_t>>, Is_dropout=true, Is_causal=false, Is_even_N=true, Is_even_K=false, Return_softmax=true]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(34): here
            instantiation of "void run_flash_fwd<Kernel_traits,Is_dropout,Is_causal>(Flash_fwd_params &, cudaStream_t) [with Kernel_traits=Flash_fwd_kernel_traits<256, 64, 64, 4, false, false, cutlass::half_t, Flash_kernel_traits<256, 64, 64, 4, cutlass::half_t>>, Is_dropout=true, Is_causal=false]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(236): here
            instantiation of "void run_mha_fwd_hdim256<T>(Flash_fwd_params &, cudaStream_t) [with T=cutlass::half_t]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_hdim256_fp16_sm80.cu(8): here

/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_kernel.h(140): warning: variable "MMA_M" was declared but never referenced
          detected during:
            instantiation of "void flash::compute_attn_1rowblock<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params,Is_Q_split_K>(const Params &, int, int, int) [with Kernel_traits=Flash_fwd_kernel_traits<256, 64, 64, 4, false, false, cutlass::half_t, Flash_kernel_traits<256, 64, 64, 4, cutlass::half_t>>, Is_dropout=true, Is_causal=false, Is_even_N=true, Is_even_K=false, Return_softmax=false, Params=Flash_fwd_params, Is_Q_split_K=false]" 
(589): here
            instantiation of "void flash::compute_attn<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &) [with Kernel_traits=Flash_fwd_kernel_traits<256, 64, 64, 4, false, false, cutlass::half_t, Flash_kernel_traits<256, 64, 64, 4, cutlass::half_t>>, Is_dropout=true, Is_causal=false, Is_even_N=true, Is_even_K=false, Return_softmax=false, Params=Flash_fwd_params]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(15): here
            instantiation of "void flash_fwd_kernel<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax>(Flash_fwd_params) [with Kernel_traits=Flash_fwd_kernel_traits<256, 64, 64, 4, false, false, cutlass::half_t, Flash_kernel_traits<256, 64, 64, 4, cutlass::half_t>>, Is_dropout=true, Is_causal=false, Is_even_N=true, Is_even_K=false, Return_softmax=false]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(34): here
            instantiation of "void run_flash_fwd<Kernel_traits,Is_dropout,Is_causal>(Flash_fwd_params &, cudaStream_t) [with Kernel_traits=Flash_fwd_kernel_traits<256, 64, 64, 4, false, false, cutlass::half_t, Flash_kernel_traits<256, 64, 64, 4, cutlass::half_t>>, Is_dropout=true, Is_causal=false]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(236): here
            instantiation of "void run_mha_fwd_hdim256<T>(Flash_fwd_params &, cudaStream_t) [with T=cutlass::half_t]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_hdim256_fp16_sm80.cu(8): here

/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_kernel.h(276): warning: variable "tQrQ" was declared but never referenced
          detected during:
            instantiation of "void flash::compute_attn_1rowblock<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params,Is_Q_split_K>(const Params &, int, int, int) [with Kernel_traits=Flash_fwd_kernel_traits<256, 64, 64, 4, false, false, cutlass::half_t, Flash_kernel_traits<256, 64, 64, 4, cutlass::half_t>>, Is_dropout=true, Is_causal=false, Is_even_N=true, Is_even_K=false, Return_softmax=false, Params=Flash_fwd_params, Is_Q_split_K=false]" 
(589): here
            instantiation of "void flash::compute_attn<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &) [with Kernel_traits=Flash_fwd_kernel_traits<256, 64, 64, 4, false, false, cutlass::half_t, Flash_kernel_traits<256, 64, 64, 4, cutlass::half_t>>, Is_dropout=true, Is_causal=false, Is_even_N=true, Is_even_K=false, Return_softmax=false, Params=Flash_fwd_params]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(15): here
            instantiation of "void flash_fwd_kernel<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax>(Flash_fwd_params) [with Kernel_traits=Flash_fwd_kernel_traits<256, 64, 64, 4, false, false, cutlass::half_t, Flash_kernel_traits<256, 64, 64, 4, cutlass::half_t>>, Is_dropout=true, Is_causal=false, Is_even_N=true, Is_even_K=false, Return_softmax=false]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(34): here
            instantiation of "void run_flash_fwd<Kernel_traits,Is_dropout,Is_causal>(Flash_fwd_params &, cudaStream_t) [with Kernel_traits=Flash_fwd_kernel_traits<256, 64, 64, 4, false, false, cutlass::half_t, Flash_kernel_traits<256, 64, 64, 4, cutlass::half_t>>, Is_dropout=true, Is_causal=false]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(236): here
            instantiation of "void run_mha_fwd_hdim256<T>(Flash_fwd_params &, cudaStream_t) [with T=cutlass::half_t]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_hdim256_fp16_sm80.cu(8): here

/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_kernel.h(140): warning: variable "MMA_M" was declared but never referenced
          detected during:
            instantiation of "void flash::compute_attn_1rowblock<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params,Is_Q_split_K>(const Params &, int, int, int) [with Kernel_traits=Flash_fwd_kernel_traits<256, 64, 64, 4, false, false, cutlass::half_t, Flash_kernel_traits<256, 64, 64, 4, cutlass::half_t>>, Is_dropout=true, Is_causal=false, Is_even_N=false, Is_even_K=true, Return_softmax=true, Params=Flash_fwd_params, Is_Q_split_K=false]" 
(589): here
            instantiation of "void flash::compute_attn<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &) [with Kernel_traits=Flash_fwd_kernel_traits<256, 64, 64, 4, false, false, cutlass::half_t, Flash_kernel_traits<256, 64, 64, 4, cutlass::half_t>>, Is_dropout=true, Is_causal=false, Is_even_N=false, Is_even_K=true, Return_softmax=true, Params=Flash_fwd_params]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(15): here
            instantiation of "void flash_fwd_kernel<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax>(Flash_fwd_params) [with Kernel_traits=Flash_fwd_kernel_traits<256, 64, 64, 4, false, false, cutlass::half_t, Flash_kernel_traits<256, 64, 64, 4, cutlass::half_t>>, Is_dropout=true, Is_causal=false, Is_even_N=false, Is_even_K=true, Return_softmax=true]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(34): here
            instantiation of "void run_flash_fwd<Kernel_traits,Is_dropout,Is_causal>(Flash_fwd_params &, cudaStream_t) [with Kernel_traits=Flash_fwd_kernel_traits<256, 64, 64, 4, false, false, cutlass::half_t, Flash_kernel_traits<256, 64, 64, 4, cutlass::half_t>>, Is_dropout=true, Is_causal=false]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(236): here
            instantiation of "void run_mha_fwd_hdim256<T>(Flash_fwd_params &, cudaStream_t) [with T=cutlass::half_t]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_hdim256_fp16_sm80.cu(8): here

/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_kernel.h(276): warning: variable "tQrQ" was declared but never referenced
          detected during:
            instantiation of "void flash::compute_attn_1rowblock<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params,Is_Q_split_K>(const Params &, int, int, int) [with Kernel_traits=Flash_fwd_kernel_traits<256, 64, 64, 4, false, false, cutlass::half_t, Flash_kernel_traits<256, 64, 64, 4, cutlass::half_t>>, Is_dropout=true, Is_causal=false, Is_even_N=false, Is_even_K=true, Return_softmax=true, Params=Flash_fwd_params, Is_Q_split_K=false]" 
(589): here
            instantiation of "void flash::compute_attn<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &) [with Kernel_traits=Flash_fwd_kernel_traits<256, 64, 64, 4, false, false, cutlass::half_t, Flash_kernel_traits<256, 64, 64, 4, cutlass::half_t>>, Is_dropout=true, Is_causal=false, Is_even_N=false, Is_even_K=true, Return_softmax=true, Params=Flash_fwd_params]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(15): here
            instantiation of "void flash_fwd_kernel<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax>(Flash_fwd_params) [with Kernel_traits=Flash_fwd_kernel_traits<256, 64, 64, 4, false, false, cutlass::half_t, Flash_kernel_traits<256, 64, 64, 4, cutlass::half_t>>, Is_dropout=true, Is_causal=false, Is_even_N=false, Is_even_K=true, Return_softmax=true]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(34): here
            instantiation of "void run_flash_fwd<Kernel_traits,Is_dropout,Is_causal>(Flash_fwd_params &, cudaStream_t) [with Kernel_traits=Flash_fwd_kernel_traits<256, 64, 64, 4, false, false, cutlass::half_t, Flash_kernel_traits<256, 64, 64, 4, cutlass::half_t>>, Is_dropout=true, Is_causal=false]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(236): here
            instantiation of "void run_mha_fwd_hdim256<T>(Flash_fwd_params &, cudaStream_t) [with T=cutlass::half_t]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_hdim256_fp16_sm80.cu(8): here

/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_kernel.h(140): warning: variable "MMA_M" was declared but never referenced
          detected during:
            instantiation of "void flash::compute_attn_1rowblock<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params,Is_Q_split_K>(const Params &, int, int, int) [with Kernel_traits=Flash_fwd_kernel_traits<256, 64, 64, 4, false, false, cutlass::half_t, Flash_kernel_traits<256, 64, 64, 4, cutlass::half_t>>, Is_dropout=true, Is_causal=false, Is_even_N=false, Is_even_K=true, Return_softmax=false, Params=Flash_fwd_params, Is_Q_split_K=false]" 
(589): here
            instantiation of "void flash::compute_attn<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &) [with Kernel_traits=Flash_fwd_kernel_traits<256, 64, 64, 4, false, false, cutlass::half_t, Flash_kernel_traits<256, 64, 64, 4, cutlass::half_t>>, Is_dropout=true, Is_causal=false, Is_even_N=false, Is_even_K=true, Return_softmax=false, Params=Flash_fwd_params]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(15): here
            instantiation of "void flash_fwd_kernel<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax>(Flash_fwd_params) [with Kernel_traits=Flash_fwd_kernel_traits<256, 64, 64, 4, false, false, cutlass::half_t, Flash_kernel_traits<256, 64, 64, 4, cutlass::half_t>>, Is_dropout=true, Is_causal=false, Is_even_N=false, Is_even_K=true, Return_softmax=false]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(34): here
            instantiation of "void run_flash_fwd<Kernel_traits,Is_dropout,Is_causal>(Flash_fwd_params &, cudaStream_t) [with Kernel_traits=Flash_fwd_kernel_traits<256, 64, 64, 4, false, false, cutlass::half_t, Flash_kernel_traits<256, 64, 64, 4, cutlass::half_t>>, Is_dropout=true, Is_causal=false]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(236): here
            instantiation of "void run_mha_fwd_hdim256<T>(Flash_fwd_params &, cudaStream_t) [with T=cutlass::half_t]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_hdim256_fp16_sm80.cu(8): here

/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_kernel.h(276): warning: variable "tQrQ" was declared but never referenced
          detected during:
            instantiation of "void flash::compute_attn_1rowblock<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params,Is_Q_split_K>(const Params &, int, int, int) [with Kernel_traits=Flash_fwd_kernel_traits<256, 64, 64, 4, false, false, cutlass::half_t, Flash_kernel_traits<256, 64, 64, 4, cutlass::half_t>>, Is_dropout=true, Is_causal=false, Is_even_N=false, Is_even_K=true, Return_softmax=false, Params=Flash_fwd_params, Is_Q_split_K=false]" 
(589): here
            instantiation of "void flash::compute_attn<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &) [with Kernel_traits=Flash_fwd_kernel_traits<256, 64, 64, 4, false, false, cutlass::half_t, Flash_kernel_traits<256, 64, 64, 4, cutlass::half_t>>, Is_dropout=true, Is_causal=false, Is_even_N=false, Is_even_K=true, Return_softmax=false, Params=Flash_fwd_params]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(15): here
            instantiation of "void flash_fwd_kernel<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax>(Flash_fwd_params) [with Kernel_traits=Flash_fwd_kernel_traits<256, 64, 64, 4, false, false, cutlass::half_t, Flash_kernel_traits<256, 64, 64, 4, cutlass::half_t>>, Is_dropout=true, Is_causal=false, Is_even_N=false, Is_even_K=true, Return_softmax=false]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(34): here
            instantiation of "void run_flash_fwd<Kernel_traits,Is_dropout,Is_causal>(Flash_fwd_params &, cudaStream_t) [with Kernel_traits=Flash_fwd_kernel_traits<256, 64, 64, 4, false, false, cutlass::half_t, Flash_kernel_traits<256, 64, 64, 4, cutlass::half_t>>, Is_dropout=true, Is_causal=false]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(236): here
            instantiation of "void run_mha_fwd_hdim256<T>(Flash_fwd_params &, cudaStream_t) [with T=cutlass::half_t]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_hdim256_fp16_sm80.cu(8): here

/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_kernel.h(140): warning: variable "MMA_M" was declared but never referenced
          detected during:
            instantiation of "void flash::compute_attn_1rowblock<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params,Is_Q_split_K>(const Params &, int, int, int) [with Kernel_traits=Flash_fwd_kernel_traits<256, 64, 64, 4, false, false, cutlass::half_t, Flash_kernel_traits<256, 64, 64, 4, cutlass::half_t>>, Is_dropout=true, Is_causal=false, Is_even_N=false, Is_even_K=false, Return_softmax=true, Params=Flash_fwd_params, Is_Q_split_K=false]" 
(589): here
            instantiation of "void flash::compute_attn<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &) [with Kernel_traits=Flash_fwd_kernel_traits<256, 64, 64, 4, false, false, cutlass::half_t, Flash_kernel_traits<256, 64, 64, 4, cutlass::half_t>>, Is_dropout=true, Is_causal=false, Is_even_N=false, Is_even_K=false, Return_softmax=true, Params=Flash_fwd_params]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(15): here
            instantiation of "void flash_fwd_kernel<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax>(Flash_fwd_params) [with Kernel_traits=Flash_fwd_kernel_traits<256, 64, 64, 4, false, false, cutlass::half_t, Flash_kernel_traits<256, 64, 64, 4, cutlass::half_t>>, Is_dropout=true, Is_causal=false, Is_even_N=false, Is_even_K=false, Return_softmax=true]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(34): here
            instantiation of "void run_flash_fwd<Kernel_traits,Is_dropout,Is_causal>(Flash_fwd_params &, cudaStream_t) [with Kernel_traits=Flash_fwd_kernel_traits<256, 64, 64, 4, false, false, cutlass::half_t, Flash_kernel_traits<256, 64, 64, 4, cutlass::half_t>>, Is_dropout=true, Is_causal=false]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(236): here
            instantiation of "void run_mha_fwd_hdim256<T>(Flash_fwd_params &, cudaStream_t) [with T=cutlass::half_t]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_hdim256_fp16_sm80.cu(8): here

/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_kernel.h(276): warning: variable "tQrQ" was declared but never referenced
          detected during:
            instantiation of "void flash::compute_attn_1rowblock<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params,Is_Q_split_K>(const Params &, int, int, int) [with Kernel_traits=Flash_fwd_kernel_traits<256, 64, 64, 4, false, false, cutlass::half_t, Flash_kernel_traits<256, 64, 64, 4, cutlass::half_t>>, Is_dropout=true, Is_causal=false, Is_even_N=false, Is_even_K=false, Return_softmax=true, Params=Flash_fwd_params, Is_Q_split_K=false]" 
(589): here
            instantiation of "void flash::compute_attn<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &) [with Kernel_traits=Flash_fwd_kernel_traits<256, 64, 64, 4, false, false, cutlass::half_t, Flash_kernel_traits<256, 64, 64, 4, cutlass::half_t>>, Is_dropout=true, Is_causal=false, Is_even_N=false, Is_even_K=false, Return_softmax=true, Params=Flash_fwd_params]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(15): here
            instantiation of "void flash_fwd_kernel<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax>(Flash_fwd_params) [with Kernel_traits=Flash_fwd_kernel_traits<256, 64, 64, 4, false, false, cutlass::half_t, Flash_kernel_traits<256, 64, 64, 4, cutlass::half_t>>, Is_dropout=true, Is_causal=false, Is_even_N=false, Is_even_K=false, Return_softmax=true]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(34): here
            instantiation of "void run_flash_fwd<Kernel_traits,Is_dropout,Is_causal>(Flash_fwd_params &, cudaStream_t) [with Kernel_traits=Flash_fwd_kernel_traits<256, 64, 64, 4, false, false, cutlass::half_t, Flash_kernel_traits<256, 64, 64, 4, cutlass::half_t>>, Is_dropout=true, Is_causal=false]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(236): here
            instantiation of "void run_mha_fwd_hdim256<T>(Flash_fwd_params &, cudaStream_t) [with T=cutlass::half_t]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_hdim256_fp16_sm80.cu(8): here

/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_kernel.h(140): warning: variable "MMA_M" was declared but never referenced
          detected during:
            instantiation of "void flash::compute_attn_1rowblock<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params,Is_Q_split_K>(const Params &, int, int, int) [with Kernel_traits=Flash_fwd_kernel_traits<256, 64, 64, 4, false, false, cutlass::half_t, Flash_kernel_traits<256, 64, 64, 4, cutlass::half_t>>, Is_dropout=true, Is_causal=false, Is_even_N=false, Is_even_K=false, Return_softmax=false, Params=Flash_fwd_params, Is_Q_split_K=false]" 
(589): here
            instantiation of "void flash::compute_attn<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &) [with Kernel_traits=Flash_fwd_kernel_traits<256, 64, 64, 4, false, false, cutlass::half_t, Flash_kernel_traits<256, 64, 64, 4, cutlass::half_t>>, Is_dropout=true, Is_causal=false, Is_even_N=false, Is_even_K=false, Return_softmax=false, Params=Flash_fwd_params]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(15): here
            instantiation of "void flash_fwd_kernel<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax>(Flash_fwd_params) [with Kernel_traits=Flash_fwd_kernel_traits<256, 64, 64, 4, false, false, cutlass::half_t, Flash_kernel_traits<256, 64, 64, 4, cutlass::half_t>>, Is_dropout=true, Is_causal=false, Is_even_N=false, Is_even_K=false, Return_softmax=false]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(34): here
            instantiation of "void run_flash_fwd<Kernel_traits,Is_dropout,Is_causal>(Flash_fwd_params &, cudaStream_t) [with Kernel_traits=Flash_fwd_kernel_traits<256, 64, 64, 4, false, false, cutlass::half_t, Flash_kernel_traits<256, 64, 64, 4, cutlass::half_t>>, Is_dropout=true, Is_causal=false]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(236): here
            instantiation of "void run_mha_fwd_hdim256<T>(Flash_fwd_params &, cudaStream_t) [with T=cutlass::half_t]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_hdim256_fp16_sm80.cu(8): here

/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_kernel.h(276): warning: variable "tQrQ" was declared but never referenced
          detected during:
            instantiation of "void flash::compute_attn_1rowblock<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params,Is_Q_split_K>(const Params &, int, int, int) [with Kernel_traits=Flash_fwd_kernel_traits<256, 64, 64, 4, false, false, cutlass::half_t, Flash_kernel_traits<256, 64, 64, 4, cutlass::half_t>>, Is_dropout=true, Is_causal=false, Is_even_N=false, Is_even_K=false, Return_softmax=false, Params=Flash_fwd_params, Is_Q_split_K=false]" 
(589): here
            instantiation of "void flash::compute_attn<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &) [with Kernel_traits=Flash_fwd_kernel_traits<256, 64, 64, 4, false, false, cutlass::half_t, Flash_kernel_traits<256, 64, 64, 4, cutlass::half_t>>, Is_dropout=true, Is_causal=false, Is_even_N=false, Is_even_K=false, Return_softmax=false, Params=Flash_fwd_params]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(15): here
            instantiation of "void flash_fwd_kernel<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax>(Flash_fwd_params) [with Kernel_traits=Flash_fwd_kernel_traits<256, 64, 64, 4, false, false, cutlass::half_t, Flash_kernel_traits<256, 64, 64, 4, cutlass::half_t>>, Is_dropout=true, Is_causal=false, Is_even_N=false, Is_even_K=false, Return_softmax=false]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(34): here
            instantiation of "void run_flash_fwd<Kernel_traits,Is_dropout,Is_causal>(Flash_fwd_params &, cudaStream_t) [with Kernel_traits=Flash_fwd_kernel_traits<256, 64, 64, 4, false, false, cutlass::half_t, Flash_kernel_traits<256, 64, 64, 4, cutlass::half_t>>, Is_dropout=true, Is_causal=false]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(236): here
            instantiation of "void run_mha_fwd_hdim256<T>(Flash_fwd_params &, cudaStream_t) [with T=cutlass::half_t]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_hdim256_fp16_sm80.cu(8): here

/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_kernel.h(140): warning: variable "MMA_M" was declared but never referenced
          detected during:
            instantiation of "void flash::compute_attn_1rowblock<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params,Is_Q_split_K>(const Params &, int, int, int) [with Kernel_traits=Flash_fwd_kernel_traits<256, 128, 64, 8, false, false, cutlass::half_t, Flash_kernel_traits<256, 128, 64, 8, cutlass::half_t>>, Is_dropout=false, Is_causal=true, Is_even_N=true, Is_even_K=true, Return_softmax=false, Params=Flash_fwd_params, Is_Q_split_K=false]" 
(589): here
            instantiation of "void flash::compute_attn<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &) [with Kernel_traits=Flash_fwd_kernel_traits<256, 128, 64, 8, false, false, cutlass::half_t, Flash_kernel_traits<256, 128, 64, 8, cutlass::half_t>>, Is_dropout=false, Is_causal=true, Is_even_N=true, Is_even_K=true, Return_softmax=false, Params=Flash_fwd_params]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(15): here
            instantiation of "void flash_fwd_kernel<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax>(Flash_fwd_params) [with Kernel_traits=Flash_fwd_kernel_traits<256, 128, 64, 8, false, false, cutlass::half_t, Flash_kernel_traits<256, 128, 64, 8, cutlass::half_t>>, Is_dropout=false, Is_causal=true, Is_even_N=true, Is_even_K=true, Return_softmax=false]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(34): here
            instantiation of "void run_flash_fwd<Kernel_traits,Is_dropout,Is_causal>(Flash_fwd_params &, cudaStream_t) [with Kernel_traits=Flash_fwd_kernel_traits<256, 128, 64, 8, false, false, cutlass::half_t, Flash_kernel_traits<256, 128, 64, 8, cutlass::half_t>>, Is_dropout=false, Is_causal=true]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(236): here
            instantiation of "void run_mha_fwd_hdim256<T>(Flash_fwd_params &, cudaStream_t) [with T=cutlass::half_t]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_hdim256_fp16_sm80.cu(8): here

/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_kernel.h(276): warning: variable "tQrQ" was declared but never referenced
          detected during:
            instantiation of "void flash::compute_attn_1rowblock<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params,Is_Q_split_K>(const Params &, int, int, int) [with Kernel_traits=Flash_fwd_kernel_traits<256, 128, 64, 8, false, false, cutlass::half_t, Flash_kernel_traits<256, 128, 64, 8, cutlass::half_t>>, Is_dropout=false, Is_causal=true, Is_even_N=true, Is_even_K=true, Return_softmax=false, Params=Flash_fwd_params, Is_Q_split_K=false]" 
(589): here
            instantiation of "void flash::compute_attn<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &) [with Kernel_traits=Flash_fwd_kernel_traits<256, 128, 64, 8, false, false, cutlass::half_t, Flash_kernel_traits<256, 128, 64, 8, cutlass::half_t>>, Is_dropout=false, Is_causal=true, Is_even_N=true, Is_even_K=true, Return_softmax=false, Params=Flash_fwd_params]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(15): here
            instantiation of "void flash_fwd_kernel<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax>(Flash_fwd_params) [with Kernel_traits=Flash_fwd_kernel_traits<256, 128, 64, 8, false, false, cutlass::half_t, Flash_kernel_traits<256, 128, 64, 8, cutlass::half_t>>, Is_dropout=false, Is_causal=true, Is_even_N=true, Is_even_K=true, Return_softmax=false]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(34): here
            instantiation of "void run_flash_fwd<Kernel_traits,Is_dropout,Is_causal>(Flash_fwd_params &, cudaStream_t) [with Kernel_traits=Flash_fwd_kernel_traits<256, 128, 64, 8, false, false, cutlass::half_t, Flash_kernel_traits<256, 128, 64, 8, cutlass::half_t>>, Is_dropout=false, Is_causal=true]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(236): here
            instantiation of "void run_mha_fwd_hdim256<T>(Flash_fwd_params &, cudaStream_t) [with T=cutlass::half_t]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_hdim256_fp16_sm80.cu(8): here

/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_kernel.h(140): warning: variable "MMA_M" was declared but never referenced
          detected during:
            instantiation of "void flash::compute_attn_1rowblock<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params,Is_Q_split_K>(const Params &, int, int, int) [with Kernel_traits=Flash_fwd_kernel_traits<256, 128, 64, 8, false, false, cutlass::half_t, Flash_kernel_traits<256, 128, 64, 8, cutlass::half_t>>, Is_dropout=false, Is_causal=true, Is_even_N=true, Is_even_K=false, Return_softmax=false, Params=Flash_fwd_params, Is_Q_split_K=false]" 
(589): here
            instantiation of "void flash::compute_attn<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &) [with Kernel_traits=Flash_fwd_kernel_traits<256, 128, 64, 8, false, false, cutlass::half_t, Flash_kernel_traits<256, 128, 64, 8, cutlass::half_t>>, Is_dropout=false, Is_causal=true, Is_even_N=true, Is_even_K=false, Return_softmax=false, Params=Flash_fwd_params]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(15): here
            instantiation of "void flash_fwd_kernel<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax>(Flash_fwd_params) [with Kernel_traits=Flash_fwd_kernel_traits<256, 128, 64, 8, false, false, cutlass::half_t, Flash_kernel_traits<256, 128, 64, 8, cutlass::half_t>>, Is_dropout=false, Is_causal=true, Is_even_N=true, Is_even_K=false, Return_softmax=false]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(34): here
            instantiation of "void run_flash_fwd<Kernel_traits,Is_dropout,Is_causal>(Flash_fwd_params &, cudaStream_t) [with Kernel_traits=Flash_fwd_kernel_traits<256, 128, 64, 8, false, false, cutlass::half_t, Flash_kernel_traits<256, 128, 64, 8, cutlass::half_t>>, Is_dropout=false, Is_causal=true]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(236): here
            instantiation of "void run_mha_fwd_hdim256<T>(Flash_fwd_params &, cudaStream_t) [with T=cutlass::half_t]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_hdim256_fp16_sm80.cu(8): here

/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_kernel.h(276): warning: variable "tQrQ" was declared but never referenced
          detected during:
            instantiation of "void flash::compute_attn_1rowblock<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params,Is_Q_split_K>(const Params &, int, int, int) [with Kernel_traits=Flash_fwd_kernel_traits<256, 128, 64, 8, false, false, cutlass::half_t, Flash_kernel_traits<256, 128, 64, 8, cutlass::half_t>>, Is_dropout=false, Is_causal=true, Is_even_N=true, Is_even_K=false, Return_softmax=false, Params=Flash_fwd_params, Is_Q_split_K=false]" 
(589): here
            instantiation of "void flash::compute_attn<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &) [with Kernel_traits=Flash_fwd_kernel_traits<256, 128, 64, 8, false, false, cutlass::half_t, Flash_kernel_traits<256, 128, 64, 8, cutlass::half_t>>, Is_dropout=false, Is_causal=true, Is_even_N=true, Is_even_K=false, Return_softmax=false, Params=Flash_fwd_params]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(15): here
            instantiation of "void flash_fwd_kernel<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax>(Flash_fwd_params) [with Kernel_traits=Flash_fwd_kernel_traits<256, 128, 64, 8, false, false, cutlass::half_t, Flash_kernel_traits<256, 128, 64, 8, cutlass::half_t>>, Is_dropout=false, Is_causal=true, Is_even_N=true, Is_even_K=false, Return_softmax=false]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(34): here
            instantiation of "void run_flash_fwd<Kernel_traits,Is_dropout,Is_causal>(Flash_fwd_params &, cudaStream_t) [with Kernel_traits=Flash_fwd_kernel_traits<256, 128, 64, 8, false, false, cutlass::half_t, Flash_kernel_traits<256, 128, 64, 8, cutlass::half_t>>, Is_dropout=false, Is_causal=true]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(236): here
            instantiation of "void run_mha_fwd_hdim256<T>(Flash_fwd_params &, cudaStream_t) [with T=cutlass::half_t]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_hdim256_fp16_sm80.cu(8): here

/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_kernel.h(140): warning: variable "MMA_M" was declared but never referenced
          detected during:
            instantiation of "void flash::compute_attn_1rowblock<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params,Is_Q_split_K>(const Params &, int, int, int) [with Kernel_traits=Flash_fwd_kernel_traits<256, 128, 64, 8, false, false, cutlass::half_t, Flash_kernel_traits<256, 128, 64, 8, cutlass::half_t>>, Is_dropout=false, Is_causal=true, Is_even_N=false, Is_even_K=true, Return_softmax=false, Params=Flash_fwd_params, Is_Q_split_K=false]" 
(589): here
            instantiation of "void flash::compute_attn<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &) [with Kernel_traits=Flash_fwd_kernel_traits<256, 128, 64, 8, false, false, cutlass::half_t, Flash_kernel_traits<256, 128, 64, 8, cutlass::half_t>>, Is_dropout=false, Is_causal=true, Is_even_N=false, Is_even_K=true, Return_softmax=false, Params=Flash_fwd_params]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(15): here
            instantiation of "void flash_fwd_kernel<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax>(Flash_fwd_params) [with Kernel_traits=Flash_fwd_kernel_traits<256, 128, 64, 8, false, false, cutlass::half_t, Flash_kernel_traits<256, 128, 64, 8, cutlass::half_t>>, Is_dropout=false, Is_causal=true, Is_even_N=false, Is_even_K=true, Return_softmax=false]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(34): here
            instantiation of "void run_flash_fwd<Kernel_traits,Is_dropout,Is_causal>(Flash_fwd_params &, cudaStream_t) [with Kernel_traits=Flash_fwd_kernel_traits<256, 128, 64, 8, false, false, cutlass::half_t, Flash_kernel_traits<256, 128, 64, 8, cutlass::half_t>>, Is_dropout=false, Is_causal=true]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(236): here
            instantiation of "void run_mha_fwd_hdim256<T>(Flash_fwd_params &, cudaStream_t) [with T=cutlass::half_t]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_hdim256_fp16_sm80.cu(8): here

/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_kernel.h(276): warning: variable "tQrQ" was declared but never referenced
          detected during:
            instantiation of "void flash::compute_attn_1rowblock<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params,Is_Q_split_K>(const Params &, int, int, int) [with Kernel_traits=Flash_fwd_kernel_traits<256, 128, 64, 8, false, false, cutlass::half_t, Flash_kernel_traits<256, 128, 64, 8, cutlass::half_t>>, Is_dropout=false, Is_causal=true, Is_even_N=false, Is_even_K=true, Return_softmax=false, Params=Flash_fwd_params, Is_Q_split_K=false]" 
(589): here
            instantiation of "void flash::compute_attn<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &) [with Kernel_traits=Flash_fwd_kernel_traits<256, 128, 64, 8, false, false, cutlass::half_t, Flash_kernel_traits<256, 128, 64, 8, cutlass::half_t>>, Is_dropout=false, Is_causal=true, Is_even_N=false, Is_even_K=true, Return_softmax=false, Params=Flash_fwd_params]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(15): here
            instantiation of "void flash_fwd_kernel<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax>(Flash_fwd_params) [with Kernel_traits=Flash_fwd_kernel_traits<256, 128, 64, 8, false, false, cutlass::half_t, Flash_kernel_traits<256, 128, 64, 8, cutlass::half_t>>, Is_dropout=false, Is_causal=true, Is_even_N=false, Is_even_K=true, Return_softmax=false]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(34): here
            instantiation of "void run_flash_fwd<Kernel_traits,Is_dropout,Is_causal>(Flash_fwd_params &, cudaStream_t) [with Kernel_traits=Flash_fwd_kernel_traits<256, 128, 64, 8, false, false, cutlass::half_t, Flash_kernel_traits<256, 128, 64, 8, cutlass::half_t>>, Is_dropout=false, Is_causal=true]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(236): here
            instantiation of "void run_mha_fwd_hdim256<T>(Flash_fwd_params &, cudaStream_t) [with T=cutlass::half_t]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_hdim256_fp16_sm80.cu(8): here

/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_kernel.h(140): warning: variable "MMA_M" was declared but never referenced
          detected during:
            instantiation of "void flash::compute_attn_1rowblock<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params,Is_Q_split_K>(const Params &, int, int, int) [with Kernel_traits=Flash_fwd_kernel_traits<256, 128, 64, 8, false, false, cutlass::half_t, Flash_kernel_traits<256, 128, 64, 8, cutlass::half_t>>, Is_dropout=false, Is_causal=true, Is_even_N=false, Is_even_K=false, Return_softmax=false, Params=Flash_fwd_params, Is_Q_split_K=false]" 
(589): here
            instantiation of "void flash::compute_attn<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &) [with Kernel_traits=Flash_fwd_kernel_traits<256, 128, 64, 8, false, false, cutlass::half_t, Flash_kernel_traits<256, 128, 64, 8, cutlass::half_t>>, Is_dropout=false, Is_causal=true, Is_even_N=false, Is_even_K=false, Return_softmax=false, Params=Flash_fwd_params]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(15): here
            instantiation of "void flash_fwd_kernel<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax>(Flash_fwd_params) [with Kernel_traits=Flash_fwd_kernel_traits<256, 128, 64, 8, false, false, cutlass::half_t, Flash_kernel_traits<256, 128, 64, 8, cutlass::half_t>>, Is_dropout=false, Is_causal=true, Is_even_N=false, Is_even_K=false, Return_softmax=false]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(34): here
            instantiation of "void run_flash_fwd<Kernel_traits,Is_dropout,Is_causal>(Flash_fwd_params &, cudaStream_t) [with Kernel_traits=Flash_fwd_kernel_traits<256, 128, 64, 8, false, false, cutlass::half_t, Flash_kernel_traits<256, 128, 64, 8, cutlass::half_t>>, Is_dropout=false, Is_causal=true]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(236): here
            instantiation of "void run_mha_fwd_hdim256<T>(Flash_fwd_params &, cudaStream_t) [with T=cutlass::half_t]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_hdim256_fp16_sm80.cu(8): here

/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_kernel.h(276): warning: variable "tQrQ" was declared but never referenced
          detected during:
            instantiation of "void flash::compute_attn_1rowblock<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params,Is_Q_split_K>(const Params &, int, int, int) [with Kernel_traits=Flash_fwd_kernel_traits<256, 128, 64, 8, false, false, cutlass::half_t, Flash_kernel_traits<256, 128, 64, 8, cutlass::half_t>>, Is_dropout=false, Is_causal=true, Is_even_N=false, Is_even_K=false, Return_softmax=false, Params=Flash_fwd_params, Is_Q_split_K=false]" 
(589): here
            instantiation of "void flash::compute_attn<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &) [with Kernel_traits=Flash_fwd_kernel_traits<256, 128, 64, 8, false, false, cutlass::half_t, Flash_kernel_traits<256, 128, 64, 8, cutlass::half_t>>, Is_dropout=false, Is_causal=true, Is_even_N=false, Is_even_K=false, Return_softmax=false, Params=Flash_fwd_params]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(15): here
            instantiation of "void flash_fwd_kernel<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax>(Flash_fwd_params) [with Kernel_traits=Flash_fwd_kernel_traits<256, 128, 64, 8, false, false, cutlass::half_t, Flash_kernel_traits<256, 128, 64, 8, cutlass::half_t>>, Is_dropout=false, Is_causal=true, Is_even_N=false, Is_even_K=false, Return_softmax=false]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(34): here
            instantiation of "void run_flash_fwd<Kernel_traits,Is_dropout,Is_causal>(Flash_fwd_params &, cudaStream_t) [with Kernel_traits=Flash_fwd_kernel_traits<256, 128, 64, 8, false, false, cutlass::half_t, Flash_kernel_traits<256, 128, 64, 8, cutlass::half_t>>, Is_dropout=false, Is_causal=true]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(236): here
            instantiation of "void run_mha_fwd_hdim256<T>(Flash_fwd_params &, cudaStream_t) [with T=cutlass::half_t]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_hdim256_fp16_sm80.cu(8): here

/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_kernel.h(140): warning: variable "MMA_M" was declared but never referenced
          detected during:
            instantiation of "void flash::compute_attn_1rowblock<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params,Is_Q_split_K>(const Params &, int, int, int) [with Kernel_traits=Flash_fwd_kernel_traits<256, 64, 64, 4, false, false, cutlass::half_t, Flash_kernel_traits<256, 64, 64, 4, cutlass::half_t>>, Is_dropout=false, Is_causal=true, Is_even_N=true, Is_even_K=true, Return_softmax=false, Params=Flash_fwd_params, Is_Q_split_K=false]" 
(589): here
            instantiation of "void flash::compute_attn<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &) [with Kernel_traits=Flash_fwd_kernel_traits<256, 64, 64, 4, false, false, cutlass::half_t, Flash_kernel_traits<256, 64, 64, 4, cutlass::half_t>>, Is_dropout=false, Is_causal=true, Is_even_N=true, Is_even_K=true, Return_softmax=false, Params=Flash_fwd_params]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(15): here
            instantiation of "void flash_fwd_kernel<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax>(Flash_fwd_params) [with Kernel_traits=Flash_fwd_kernel_traits<256, 64, 64, 4, false, false, cutlass::half_t, Flash_kernel_traits<256, 64, 64, 4, cutlass::half_t>>, Is_dropout=false, Is_causal=true, Is_even_N=true, Is_even_K=true, Return_softmax=false]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(34): here
            instantiation of "void run_flash_fwd<Kernel_traits,Is_dropout,Is_causal>(Flash_fwd_params &, cudaStream_t) [with Kernel_traits=Flash_fwd_kernel_traits<256, 64, 64, 4, false, false, cutlass::half_t, Flash_kernel_traits<256, 64, 64, 4, cutlass::half_t>>, Is_dropout=false, Is_causal=true]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(236): here
            instantiation of "void run_mha_fwd_hdim256<T>(Flash_fwd_params &, cudaStream_t) [with T=cutlass::half_t]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_hdim256_fp16_sm80.cu(8): here

/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_kernel.h(276): warning: variable "tQrQ" was declared but never referenced
          detected during:
            instantiation of "void flash::compute_attn_1rowblock<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params,Is_Q_split_K>(const Params &, int, int, int) [with Kernel_traits=Flash_fwd_kernel_traits<256, 64, 64, 4, false, false, cutlass::half_t, Flash_kernel_traits<256, 64, 64, 4, cutlass::half_t>>, Is_dropout=false, Is_causal=true, Is_even_N=true, Is_even_K=true, Return_softmax=false, Params=Flash_fwd_params, Is_Q_split_K=false]" 
(589): here
            instantiation of "void flash::compute_attn<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &) [with Kernel_traits=Flash_fwd_kernel_traits<256, 64, 64, 4, false, false, cutlass::half_t, Flash_kernel_traits<256, 64, 64, 4, cutlass::half_t>>, Is_dropout=false, Is_causal=true, Is_even_N=true, Is_even_K=true, Return_softmax=false, Params=Flash_fwd_params]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(15): here
            instantiation of "void flash_fwd_kernel<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax>(Flash_fwd_params) [with Kernel_traits=Flash_fwd_kernel_traits<256, 64, 64, 4, false, false, cutlass::half_t, Flash_kernel_traits<256, 64, 64, 4, cutlass::half_t>>, Is_dropout=false, Is_causal=true, Is_even_N=true, Is_even_K=true, Return_softmax=false]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(34): here
            instantiation of "void run_flash_fwd<Kernel_traits,Is_dropout,Is_causal>(Flash_fwd_params &, cudaStream_t) [with Kernel_traits=Flash_fwd_kernel_traits<256, 64, 64, 4, false, false, cutlass::half_t, Flash_kernel_traits<256, 64, 64, 4, cutlass::half_t>>, Is_dropout=false, Is_causal=true]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(236): here
            instantiation of "void run_mha_fwd_hdim256<T>(Flash_fwd_params &, cudaStream_t) [with T=cutlass::half_t]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_hdim256_fp16_sm80.cu(8): here

/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_kernel.h(140): warning: variable "MMA_M" was declared but never referenced
          detected during:
            instantiation of "void flash::compute_attn_1rowblock<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params,Is_Q_split_K>(const Params &, int, int, int) [with Kernel_traits=Flash_fwd_kernel_traits<256, 64, 64, 4, false, false, cutlass::half_t, Flash_kernel_traits<256, 64, 64, 4, cutlass::half_t>>, Is_dropout=false, Is_causal=true, Is_even_N=true, Is_even_K=false, Return_softmax=false, Params=Flash_fwd_params, Is_Q_split_K=false]" 
(589): here
            instantiation of "void flash::compute_attn<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &) [with Kernel_traits=Flash_fwd_kernel_traits<256, 64, 64, 4, false, false, cutlass::half_t, Flash_kernel_traits<256, 64, 64, 4, cutlass::half_t>>, Is_dropout=false, Is_causal=true, Is_even_N=true, Is_even_K=false, Return_softmax=false, Params=Flash_fwd_params]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(15): here
            instantiation of "void flash_fwd_kernel<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax>(Flash_fwd_params) [with Kernel_traits=Flash_fwd_kernel_traits<256, 64, 64, 4, false, false, cutlass::half_t, Flash_kernel_traits<256, 64, 64, 4, cutlass::half_t>>, Is_dropout=false, Is_causal=true, Is_even_N=true, Is_even_K=false, Return_softmax=false]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(34): here
            instantiation of "void run_flash_fwd<Kernel_traits,Is_dropout,Is_causal>(Flash_fwd_params &, cudaStream_t) [with Kernel_traits=Flash_fwd_kernel_traits<256, 64, 64, 4, false, false, cutlass::half_t, Flash_kernel_traits<256, 64, 64, 4, cutlass::half_t>>, Is_dropout=false, Is_causal=true]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(236): here
            instantiation of "void run_mha_fwd_hdim256<T>(Flash_fwd_params &, cudaStream_t) [with T=cutlass::half_t]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_hdim256_fp16_sm80.cu(8): here

/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_kernel.h(276): warning: variable "tQrQ" was declared but never referenced
          detected during:
            instantiation of "void flash::compute_attn_1rowblock<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params,Is_Q_split_K>(const Params &, int, int, int) [with Kernel_traits=Flash_fwd_kernel_traits<256, 64, 64, 4, false, false, cutlass::half_t, Flash_kernel_traits<256, 64, 64, 4, cutlass::half_t>>, Is_dropout=false, Is_causal=true, Is_even_N=true, Is_even_K=false, Return_softmax=false, Params=Flash_fwd_params, Is_Q_split_K=false]" 
(589): here
            instantiation of "void flash::compute_attn<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &) [with Kernel_traits=Flash_fwd_kernel_traits<256, 64, 64, 4, false, false, cutlass::half_t, Flash_kernel_traits<256, 64, 64, 4, cutlass::half_t>>, Is_dropout=false, Is_causal=true, Is_even_N=true, Is_even_K=false, Return_softmax=false, Params=Flash_fwd_params]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(15): here
            instantiation of "void flash_fwd_kernel<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax>(Flash_fwd_params) [with Kernel_traits=Flash_fwd_kernel_traits<256, 64, 64, 4, false, false, cutlass::half_t, Flash_kernel_traits<256, 64, 64, 4, cutlass::half_t>>, Is_dropout=false, Is_causal=true, Is_even_N=true, Is_even_K=false, Return_softmax=false]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(34): here
            instantiation of "void run_flash_fwd<Kernel_traits,Is_dropout,Is_causal>(Flash_fwd_params &, cudaStream_t) [with Kernel_traits=Flash_fwd_kernel_traits<256, 64, 64, 4, false, false, cutlass::half_t, Flash_kernel_traits<256, 64, 64, 4, cutlass::half_t>>, Is_dropout=false, Is_causal=true]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(236): here
            instantiation of "void run_mha_fwd_hdim256<T>(Flash_fwd_params &, cudaStream_t) [with T=cutlass::half_t]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_hdim256_fp16_sm80.cu(8): here

/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_kernel.h(140): warning: variable "MMA_M" was declared but never referenced
          detected during:
            instantiation of "void flash::compute_attn_1rowblock<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params,Is_Q_split_K>(const Params &, int, int, int) [with Kernel_traits=Flash_fwd_kernel_traits<256, 64, 64, 4, false, false, cutlass::half_t, Flash_kernel_traits<256, 64, 64, 4, cutlass::half_t>>, Is_dropout=false, Is_causal=true, Is_even_N=false, Is_even_K=true, Return_softmax=false, Params=Flash_fwd_params, Is_Q_split_K=false]" 
(589): here
            instantiation of "void flash::compute_attn<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &) [with Kernel_traits=Flash_fwd_kernel_traits<256, 64, 64, 4, false, false, cutlass::half_t, Flash_kernel_traits<256, 64, 64, 4, cutlass::half_t>>, Is_dropout=false, Is_causal=true, Is_even_N=false, Is_even_K=true, Return_softmax=false, Params=Flash_fwd_params]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(15): here
            instantiation of "void flash_fwd_kernel<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax>(Flash_fwd_params) [with Kernel_traits=Flash_fwd_kernel_traits<256, 64, 64, 4, false, false, cutlass::half_t, Flash_kernel_traits<256, 64, 64, 4, cutlass::half_t>>, Is_dropout=false, Is_causal=true, Is_even_N=false, Is_even_K=true, Return_softmax=false]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(34): here
            instantiation of "void run_flash_fwd<Kernel_traits,Is_dropout,Is_causal>(Flash_fwd_params &, cudaStream_t) [with Kernel_traits=Flash_fwd_kernel_traits<256, 64, 64, 4, false, false, cutlass::half_t, Flash_kernel_traits<256, 64, 64, 4, cutlass::half_t>>, Is_dropout=false, Is_causal=true]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(236): here
            instantiation of "void run_mha_fwd_hdim256<T>(Flash_fwd_params &, cudaStream_t) [with T=cutlass::half_t]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_hdim256_fp16_sm80.cu(8): here

/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_kernel.h(276): warning: variable "tQrQ" was declared but never referenced
          detected during:
            instantiation of "void flash::compute_attn_1rowblock<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params,Is_Q_split_K>(const Params &, int, int, int) [with Kernel_traits=Flash_fwd_kernel_traits<256, 64, 64, 4, false, false, cutlass::half_t, Flash_kernel_traits<256, 64, 64, 4, cutlass::half_t>>, Is_dropout=false, Is_causal=true, Is_even_N=false, Is_even_K=true, Return_softmax=false, Params=Flash_fwd_params, Is_Q_split_K=false]" 
(589): here
            instantiation of "void flash::compute_attn<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &) [with Kernel_traits=Flash_fwd_kernel_traits<256, 64, 64, 4, false, false, cutlass::half_t, Flash_kernel_traits<256, 64, 64, 4, cutlass::half_t>>, Is_dropout=false, Is_causal=true, Is_even_N=false, Is_even_K=true, Return_softmax=false, Params=Flash_fwd_params]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(15): here
            instantiation of "void flash_fwd_kernel<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax>(Flash_fwd_params) [with Kernel_traits=Flash_fwd_kernel_traits<256, 64, 64, 4, false, false, cutlass::half_t, Flash_kernel_traits<256, 64, 64, 4, cutlass::half_t>>, Is_dropout=false, Is_causal=true, Is_even_N=false, Is_even_K=true, Return_softmax=false]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(34): here
            instantiation of "void run_flash_fwd<Kernel_traits,Is_dropout,Is_causal>(Flash_fwd_params &, cudaStream_t) [with Kernel_traits=Flash_fwd_kernel_traits<256, 64, 64, 4, false, false, cutlass::half_t, Flash_kernel_traits<256, 64, 64, 4, cutlass::half_t>>, Is_dropout=false, Is_causal=true]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(236): here
            instantiation of "void run_mha_fwd_hdim256<T>(Flash_fwd_params &, cudaStream_t) [with T=cutlass::half_t]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_hdim256_fp16_sm80.cu(8): here

/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_kernel.h(140): warning: variable "MMA_M" was declared but never referenced
          detected during:
            instantiation of "void flash::compute_attn_1rowblock<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params,Is_Q_split_K>(const Params &, int, int, int) [with Kernel_traits=Flash_fwd_kernel_traits<256, 64, 64, 4, false, false, cutlass::half_t, Flash_kernel_traits<256, 64, 64, 4, cutlass::half_t>>, Is_dropout=false, Is_causal=true, Is_even_N=false, Is_even_K=false, Return_softmax=false, Params=Flash_fwd_params, Is_Q_split_K=false]" 
(589): here
            instantiation of "void flash::compute_attn<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &) [with Kernel_traits=Flash_fwd_kernel_traits<256, 64, 64, 4, false, false, cutlass::half_t, Flash_kernel_traits<256, 64, 64, 4, cutlass::half_t>>, Is_dropout=false, Is_causal=true, Is_even_N=false, Is_even_K=false, Return_softmax=false, Params=Flash_fwd_params]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(15): here
            instantiation of "void flash_fwd_kernel<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax>(Flash_fwd_params) [with Kernel_traits=Flash_fwd_kernel_traits<256, 64, 64, 4, false, false, cutlass::half_t, Flash_kernel_traits<256, 64, 64, 4, cutlass::half_t>>, Is_dropout=false, Is_causal=true, Is_even_N=false, Is_even_K=false, Return_softmax=false]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(34): here
            instantiation of "void run_flash_fwd<Kernel_traits,Is_dropout,Is_causal>(Flash_fwd_params &, cudaStream_t) [with Kernel_traits=Flash_fwd_kernel_traits<256, 64, 64, 4, false, false, cutlass::half_t, Flash_kernel_traits<256, 64, 64, 4, cutlass::half_t>>, Is_dropout=false, Is_causal=true]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(236): here
            instantiation of "void run_mha_fwd_hdim256<T>(Flash_fwd_params &, cudaStream_t) [with T=cutlass::half_t]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_hdim256_fp16_sm80.cu(8): here

/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_kernel.h(276): warning: variable "tQrQ" was declared but never referenced
          detected during:
            instantiation of "void flash::compute_attn_1rowblock<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params,Is_Q_split_K>(const Params &, int, int, int) [with Kernel_traits=Flash_fwd_kernel_traits<256, 64, 64, 4, false, false, cutlass::half_t, Flash_kernel_traits<256, 64, 64, 4, cutlass::half_t>>, Is_dropout=false, Is_causal=true, Is_even_N=false, Is_even_K=false, Return_softmax=false, Params=Flash_fwd_params, Is_Q_split_K=false]" 
(589): here
            instantiation of "void flash::compute_attn<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &) [with Kernel_traits=Flash_fwd_kernel_traits<256, 64, 64, 4, false, false, cutlass::half_t, Flash_kernel_traits<256, 64, 64, 4, cutlass::half_t>>, Is_dropout=false, Is_causal=true, Is_even_N=false, Is_even_K=false, Return_softmax=false, Params=Flash_fwd_params]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(15): here
            instantiation of "void flash_fwd_kernel<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax>(Flash_fwd_params) [with Kernel_traits=Flash_fwd_kernel_traits<256, 64, 64, 4, false, false, cutlass::half_t, Flash_kernel_traits<256, 64, 64, 4, cutlass::half_t>>, Is_dropout=false, Is_causal=true, Is_even_N=false, Is_even_K=false, Return_softmax=false]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(34): here
            instantiation of "void run_flash_fwd<Kernel_traits,Is_dropout,Is_causal>(Flash_fwd_params &, cudaStream_t) [with Kernel_traits=Flash_fwd_kernel_traits<256, 64, 64, 4, false, false, cutlass::half_t, Flash_kernel_traits<256, 64, 64, 4, cutlass::half_t>>, Is_dropout=false, Is_causal=true]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(236): here
            instantiation of "void run_mha_fwd_hdim256<T>(Flash_fwd_params &, cudaStream_t) [with T=cutlass::half_t]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_hdim256_fp16_sm80.cu(8): here

/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_kernel.h(140): warning: variable "MMA_M" was declared but never referenced
          detected during:
            instantiation of "void flash::compute_attn_1rowblock<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params,Is_Q_split_K>(const Params &, int, int, int) [with Kernel_traits=Flash_fwd_kernel_traits<256, 128, 64, 8, false, false, cutlass::half_t, Flash_kernel_traits<256, 128, 64, 8, cutlass::half_t>>, Is_dropout=false, Is_causal=false, Is_even_N=true, Is_even_K=true, Return_softmax=false, Params=Flash_fwd_params, Is_Q_split_K=false]" 
(589): here
            instantiation of "void flash::compute_attn<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &) [with Kernel_traits=Flash_fwd_kernel_traits<256, 128, 64, 8, false, false, cutlass::half_t, Flash_kernel_traits<256, 128, 64, 8, cutlass::half_t>>, Is_dropout=false, Is_causal=false, Is_even_N=true, Is_even_K=true, Return_softmax=false, Params=Flash_fwd_params]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(15): here
            instantiation of "void flash_fwd_kernel<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax>(Flash_fwd_params) [with Kernel_traits=Flash_fwd_kernel_traits<256, 128, 64, 8, false, false, cutlass::half_t, Flash_kernel_traits<256, 128, 64, 8, cutlass::half_t>>, Is_dropout=false, Is_causal=false, Is_even_N=true, Is_even_K=true, Return_softmax=false]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(34): here
            instantiation of "void run_flash_fwd<Kernel_traits,Is_dropout,Is_causal>(Flash_fwd_params &, cudaStream_t) [with Kernel_traits=Flash_fwd_kernel_traits<256, 128, 64, 8, false, false, cutlass::half_t, Flash_kernel_traits<256, 128, 64, 8, cutlass::half_t>>, Is_dropout=false, Is_causal=false]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(236): here
            instantiation of "void run_mha_fwd_hdim256<T>(Flash_fwd_params &, cudaStream_t) [with T=cutlass::half_t]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_hdim256_fp16_sm80.cu(8): here

/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_kernel.h(276): warning: variable "tQrQ" was declared but never referenced
          detected during:
            instantiation of "void flash::compute_attn_1rowblock<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params,Is_Q_split_K>(const Params &, int, int, int) [with Kernel_traits=Flash_fwd_kernel_traits<256, 128, 64, 8, false, false, cutlass::half_t, Flash_kernel_traits<256, 128, 64, 8, cutlass::half_t>>, Is_dropout=false, Is_causal=false, Is_even_N=true, Is_even_K=true, Return_softmax=false, Params=Flash_fwd_params, Is_Q_split_K=false]" 
(589): here
            instantiation of "void flash::compute_attn<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &) [with Kernel_traits=Flash_fwd_kernel_traits<256, 128, 64, 8, false, false, cutlass::half_t, Flash_kernel_traits<256, 128, 64, 8, cutlass::half_t>>, Is_dropout=false, Is_causal=false, Is_even_N=true, Is_even_K=true, Return_softmax=false, Params=Flash_fwd_params]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(15): here
            instantiation of "void flash_fwd_kernel<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax>(Flash_fwd_params) [with Kernel_traits=Flash_fwd_kernel_traits<256, 128, 64, 8, false, false, cutlass::half_t, Flash_kernel_traits<256, 128, 64, 8, cutlass::half_t>>, Is_dropout=false, Is_causal=false, Is_even_N=true, Is_even_K=true, Return_softmax=false]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(34): here
            instantiation of "void run_flash_fwd<Kernel_traits,Is_dropout,Is_causal>(Flash_fwd_params &, cudaStream_t) [with Kernel_traits=Flash_fwd_kernel_traits<256, 128, 64, 8, false, false, cutlass::half_t, Flash_kernel_traits<256, 128, 64, 8, cutlass::half_t>>, Is_dropout=false, Is_causal=false]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(236): here
            instantiation of "void run_mha_fwd_hdim256<T>(Flash_fwd_params &, cudaStream_t) [with T=cutlass::half_t]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_hdim256_fp16_sm80.cu(8): here

/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_kernel.h(140): warning: variable "MMA_M" was declared but never referenced
          detected during:
            instantiation of "void flash::compute_attn_1rowblock<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params,Is_Q_split_K>(const Params &, int, int, int) [with Kernel_traits=Flash_fwd_kernel_traits<256, 128, 64, 8, false, false, cutlass::half_t, Flash_kernel_traits<256, 128, 64, 8, cutlass::half_t>>, Is_dropout=false, Is_causal=false, Is_even_N=true, Is_even_K=false, Return_softmax=false, Params=Flash_fwd_params, Is_Q_split_K=false]" 
(589): here
            instantiation of "void flash::compute_attn<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &) [with Kernel_traits=Flash_fwd_kernel_traits<256, 128, 64, 8, false, false, cutlass::half_t, Flash_kernel_traits<256, 128, 64, 8, cutlass::half_t>>, Is_dropout=false, Is_causal=false, Is_even_N=true, Is_even_K=false, Return_softmax=false, Params=Flash_fwd_params]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(15): here
            instantiation of "void flash_fwd_kernel<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax>(Flash_fwd_params) [with Kernel_traits=Flash_fwd_kernel_traits<256, 128, 64, 8, false, false, cutlass::half_t, Flash_kernel_traits<256, 128, 64, 8, cutlass::half_t>>, Is_dropout=false, Is_causal=false, Is_even_N=true, Is_even_K=false, Return_softmax=false]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(34): here
            instantiation of "void run_flash_fwd<Kernel_traits,Is_dropout,Is_causal>(Flash_fwd_params &, cudaStream_t) [with Kernel_traits=Flash_fwd_kernel_traits<256, 128, 64, 8, false, false, cutlass::half_t, Flash_kernel_traits<256, 128, 64, 8, cutlass::half_t>>, Is_dropout=false, Is_causal=false]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(236): here
            instantiation of "void run_mha_fwd_hdim256<T>(Flash_fwd_params &, cudaStream_t) [with T=cutlass::half_t]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_hdim256_fp16_sm80.cu(8): here

/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_kernel.h(276): warning: variable "tQrQ" was declared but never referenced
          detected during:
            instantiation of "void flash::compute_attn_1rowblock<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params,Is_Q_split_K>(const Params &, int, int, int) [with Kernel_traits=Flash_fwd_kernel_traits<256, 128, 64, 8, false, false, cutlass::half_t, Flash_kernel_traits<256, 128, 64, 8, cutlass::half_t>>, Is_dropout=false, Is_causal=false, Is_even_N=true, Is_even_K=false, Return_softmax=false, Params=Flash_fwd_params, Is_Q_split_K=false]" 
(589): here
            instantiation of "void flash::compute_attn<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &) [with Kernel_traits=Flash_fwd_kernel_traits<256, 128, 64, 8, false, false, cutlass::half_t, Flash_kernel_traits<256, 128, 64, 8, cutlass::half_t>>, Is_dropout=false, Is_causal=false, Is_even_N=true, Is_even_K=false, Return_softmax=false, Params=Flash_fwd_params]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(15): here
            instantiation of "void flash_fwd_kernel<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax>(Flash_fwd_params) [with Kernel_traits=Flash_fwd_kernel_traits<256, 128, 64, 8, false, false, cutlass::half_t, Flash_kernel_traits<256, 128, 64, 8, cutlass::half_t>>, Is_dropout=false, Is_causal=false, Is_even_N=true, Is_even_K=false, Return_softmax=false]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(34): here
            instantiation of "void run_flash_fwd<Kernel_traits,Is_dropout,Is_causal>(Flash_fwd_params &, cudaStream_t) [with Kernel_traits=Flash_fwd_kernel_traits<256, 128, 64, 8, false, false, cutlass::half_t, Flash_kernel_traits<256, 128, 64, 8, cutlass::half_t>>, Is_dropout=false, Is_causal=false]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(236): here
            instantiation of "void run_mha_fwd_hdim256<T>(Flash_fwd_params &, cudaStream_t) [with T=cutlass::half_t]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_hdim256_fp16_sm80.cu(8): here

/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_kernel.h(140): warning: variable "MMA_M" was declared but never referenced
          detected during:
            instantiation of "void flash::compute_attn_1rowblock<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params,Is_Q_split_K>(const Params &, int, int, int) [with Kernel_traits=Flash_fwd_kernel_traits<256, 128, 64, 8, false, false, cutlass::half_t, Flash_kernel_traits<256, 128, 64, 8, cutlass::half_t>>, Is_dropout=false, Is_causal=false, Is_even_N=false, Is_even_K=true, Return_softmax=false, Params=Flash_fwd_params, Is_Q_split_K=false]" 
(589): here
            instantiation of "void flash::compute_attn<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &) [with Kernel_traits=Flash_fwd_kernel_traits<256, 128, 64, 8, false, false, cutlass::half_t, Flash_kernel_traits<256, 128, 64, 8, cutlass::half_t>>, Is_dropout=false, Is_causal=false, Is_even_N=false, Is_even_K=true, Return_softmax=false, Params=Flash_fwd_params]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(15): here
            instantiation of "void flash_fwd_kernel<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax>(Flash_fwd_params) [with Kernel_traits=Flash_fwd_kernel_traits<256, 128, 64, 8, false, false, cutlass::half_t, Flash_kernel_traits<256, 128, 64, 8, cutlass::half_t>>, Is_dropout=false, Is_causal=false, Is_even_N=false, Is_even_K=true, Return_softmax=false]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(34): here
            instantiation of "void run_flash_fwd<Kernel_traits,Is_dropout,Is_causal>(Flash_fwd_params &, cudaStream_t) [with Kernel_traits=Flash_fwd_kernel_traits<256, 128, 64, 8, false, false, cutlass::half_t, Flash_kernel_traits<256, 128, 64, 8, cutlass::half_t>>, Is_dropout=false, Is_causal=false]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(236): here
            instantiation of "void run_mha_fwd_hdim256<T>(Flash_fwd_params &, cudaStream_t) [with T=cutlass::half_t]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_hdim256_fp16_sm80.cu(8): here

/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_kernel.h(276): warning: variable "tQrQ" was declared but never referenced
          detected during:
            instantiation of "void flash::compute_attn_1rowblock<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params,Is_Q_split_K>(const Params &, int, int, int) [with Kernel_traits=Flash_fwd_kernel_traits<256, 128, 64, 8, false, false, cutlass::half_t, Flash_kernel_traits<256, 128, 64, 8, cutlass::half_t>>, Is_dropout=false, Is_causal=false, Is_even_N=false, Is_even_K=true, Return_softmax=false, Params=Flash_fwd_params, Is_Q_split_K=false]" 
(589): here
            instantiation of "void flash::compute_attn<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &) [with Kernel_traits=Flash_fwd_kernel_traits<256, 128, 64, 8, false, false, cutlass::half_t, Flash_kernel_traits<256, 128, 64, 8, cutlass::half_t>>, Is_dropout=false, Is_causal=false, Is_even_N=false, Is_even_K=true, Return_softmax=false, Params=Flash_fwd_params]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(15): here
            instantiation of "void flash_fwd_kernel<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax>(Flash_fwd_params) [with Kernel_traits=Flash_fwd_kernel_traits<256, 128, 64, 8, false, false, cutlass::half_t, Flash_kernel_traits<256, 128, 64, 8, cutlass::half_t>>, Is_dropout=false, Is_causal=false, Is_even_N=false, Is_even_K=true, Return_softmax=false]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(34): here
            instantiation of "void run_flash_fwd<Kernel_traits,Is_dropout,Is_causal>(Flash_fwd_params &, cudaStream_t) [with Kernel_traits=Flash_fwd_kernel_traits<256, 128, 64, 8, false, false, cutlass::half_t, Flash_kernel_traits<256, 128, 64, 8, cutlass::half_t>>, Is_dropout=false, Is_causal=false]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(236): here
            instantiation of "void run_mha_fwd_hdim256<T>(Flash_fwd_params &, cudaStream_t) [with T=cutlass::half_t]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_hdim256_fp16_sm80.cu(8): here

/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_kernel.h(140): warning: variable "MMA_M" was declared but never referenced
          detected during:
            instantiation of "void flash::compute_attn_1rowblock<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params,Is_Q_split_K>(const Params &, int, int, int) [with Kernel_traits=Flash_fwd_kernel_traits<256, 128, 64, 8, false, false, cutlass::half_t, Flash_kernel_traits<256, 128, 64, 8, cutlass::half_t>>, Is_dropout=false, Is_causal=false, Is_even_N=false, Is_even_K=false, Return_softmax=false, Params=Flash_fwd_params, Is_Q_split_K=false]" 
(589): here
            instantiation of "void flash::compute_attn<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &) [with Kernel_traits=Flash_fwd_kernel_traits<256, 128, 64, 8, false, false, cutlass::half_t, Flash_kernel_traits<256, 128, 64, 8, cutlass::half_t>>, Is_dropout=false, Is_causal=false, Is_even_N=false, Is_even_K=false, Return_softmax=false, Params=Flash_fwd_params]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(15): here
            instantiation of "void flash_fwd_kernel<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax>(Flash_fwd_params) [with Kernel_traits=Flash_fwd_kernel_traits<256, 128, 64, 8, false, false, cutlass::half_t, Flash_kernel_traits<256, 128, 64, 8, cutlass::half_t>>, Is_dropout=false, Is_causal=false, Is_even_N=false, Is_even_K=false, Return_softmax=false]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(34): here
            instantiation of "void run_flash_fwd<Kernel_traits,Is_dropout,Is_causal>(Flash_fwd_params &, cudaStream_t) [with Kernel_traits=Flash_fwd_kernel_traits<256, 128, 64, 8, false, false, cutlass::half_t, Flash_kernel_traits<256, 128, 64, 8, cutlass::half_t>>, Is_dropout=false, Is_causal=false]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(236): here
            instantiation of "void run_mha_fwd_hdim256<T>(Flash_fwd_params &, cudaStream_t) [with T=cutlass::half_t]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_hdim256_fp16_sm80.cu(8): here

/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_kernel.h(276): warning: variable "tQrQ" was declared but never referenced
          detected during:
            instantiation of "void flash::compute_attn_1rowblock<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params,Is_Q_split_K>(const Params &, int, int, int) [with Kernel_traits=Flash_fwd_kernel_traits<256, 128, 64, 8, false, false, cutlass::half_t, Flash_kernel_traits<256, 128, 64, 8, cutlass::half_t>>, Is_dropout=false, Is_causal=false, Is_even_N=false, Is_even_K=false, Return_softmax=false, Params=Flash_fwd_params, Is_Q_split_K=false]" 
(589): here
            instantiation of "void flash::compute_attn<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &) [with Kernel_traits=Flash_fwd_kernel_traits<256, 128, 64, 8, false, false, cutlass::half_t, Flash_kernel_traits<256, 128, 64, 8, cutlass::half_t>>, Is_dropout=false, Is_causal=false, Is_even_N=false, Is_even_K=false, Return_softmax=false, Params=Flash_fwd_params]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(15): here
            instantiation of "void flash_fwd_kernel<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax>(Flash_fwd_params) [with Kernel_traits=Flash_fwd_kernel_traits<256, 128, 64, 8, false, false, cutlass::half_t, Flash_kernel_traits<256, 128, 64, 8, cutlass::half_t>>, Is_dropout=false, Is_causal=false, Is_even_N=false, Is_even_K=false, Return_softmax=false]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(34): here
            instantiation of "void run_flash_fwd<Kernel_traits,Is_dropout,Is_causal>(Flash_fwd_params &, cudaStream_t) [with Kernel_traits=Flash_fwd_kernel_traits<256, 128, 64, 8, false, false, cutlass::half_t, Flash_kernel_traits<256, 128, 64, 8, cutlass::half_t>>, Is_dropout=false, Is_causal=false]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(236): here
            instantiation of "void run_mha_fwd_hdim256<T>(Flash_fwd_params &, cudaStream_t) [with T=cutlass::half_t]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_hdim256_fp16_sm80.cu(8): here

/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_kernel.h(140): warning: variable "MMA_M" was declared but never referenced
          detected during:
            instantiation of "void flash::compute_attn_1rowblock<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params,Is_Q_split_K>(const Params &, int, int, int) [with Kernel_traits=Flash_fwd_kernel_traits<256, 64, 64, 4, false, false, cutlass::half_t, Flash_kernel_traits<256, 64, 64, 4, cutlass::half_t>>, Is_dropout=false, Is_causal=false, Is_even_N=true, Is_even_K=true, Return_softmax=false, Params=Flash_fwd_params, Is_Q_split_K=false]" 
(589): here
            instantiation of "void flash::compute_attn<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &) [with Kernel_traits=Flash_fwd_kernel_traits<256, 64, 64, 4, false, false, cutlass::half_t, Flash_kernel_traits<256, 64, 64, 4, cutlass::half_t>>, Is_dropout=false, Is_causal=false, Is_even_N=true, Is_even_K=true, Return_softmax=false, Params=Flash_fwd_params]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(15): here
            instantiation of "void flash_fwd_kernel<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax>(Flash_fwd_params) [with Kernel_traits=Flash_fwd_kernel_traits<256, 64, 64, 4, false, false, cutlass::half_t, Flash_kernel_traits<256, 64, 64, 4, cutlass::half_t>>, Is_dropout=false, Is_causal=false, Is_even_N=true, Is_even_K=true, Return_softmax=false]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(34): here
            instantiation of "void run_flash_fwd<Kernel_traits,Is_dropout,Is_causal>(Flash_fwd_params &, cudaStream_t) [with Kernel_traits=Flash_fwd_kernel_traits<256, 64, 64, 4, false, false, cutlass::half_t, Flash_kernel_traits<256, 64, 64, 4, cutlass::half_t>>, Is_dropout=false, Is_causal=false]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(236): here
            instantiation of "void run_mha_fwd_hdim256<T>(Flash_fwd_params &, cudaStream_t) [with T=cutlass::half_t]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_hdim256_fp16_sm80.cu(8): here

/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_kernel.h(276): warning: variable "tQrQ" was declared but never referenced
          detected during:
            instantiation of "void flash::compute_attn_1rowblock<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params,Is_Q_split_K>(const Params &, int, int, int) [with Kernel_traits=Flash_fwd_kernel_traits<256, 64, 64, 4, false, false, cutlass::half_t, Flash_kernel_traits<256, 64, 64, 4, cutlass::half_t>>, Is_dropout=false, Is_causal=false, Is_even_N=true, Is_even_K=true, Return_softmax=false, Params=Flash_fwd_params, Is_Q_split_K=false]" 
(589): here
            instantiation of "void flash::compute_attn<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &) [with Kernel_traits=Flash_fwd_kernel_traits<256, 64, 64, 4, false, false, cutlass::half_t, Flash_kernel_traits<256, 64, 64, 4, cutlass::half_t>>, Is_dropout=false, Is_causal=false, Is_even_N=true, Is_even_K=true, Return_softmax=false, Params=Flash_fwd_params]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(15): here
            instantiation of "void flash_fwd_kernel<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax>(Flash_fwd_params) [with Kernel_traits=Flash_fwd_kernel_traits<256, 64, 64, 4, false, false, cutlass::half_t, Flash_kernel_traits<256, 64, 64, 4, cutlass::half_t>>, Is_dropout=false, Is_causal=false, Is_even_N=true, Is_even_K=true, Return_softmax=false]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(34): here
            instantiation of "void run_flash_fwd<Kernel_traits,Is_dropout,Is_causal>(Flash_fwd_params &, cudaStream_t) [with Kernel_traits=Flash_fwd_kernel_traits<256, 64, 64, 4, false, false, cutlass::half_t, Flash_kernel_traits<256, 64, 64, 4, cutlass::half_t>>, Is_dropout=false, Is_causal=false]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(236): here
            instantiation of "void run_mha_fwd_hdim256<T>(Flash_fwd_params &, cudaStream_t) [with T=cutlass::half_t]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_hdim256_fp16_sm80.cu(8): here

/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_kernel.h(140): warning: variable "MMA_M" was declared but never referenced
          detected during:
            instantiation of "void flash::compute_attn_1rowblock<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params,Is_Q_split_K>(const Params &, int, int, int) [with Kernel_traits=Flash_fwd_kernel_traits<256, 64, 64, 4, false, false, cutlass::half_t, Flash_kernel_traits<256, 64, 64, 4, cutlass::half_t>>, Is_dropout=false, Is_causal=false, Is_even_N=true, Is_even_K=false, Return_softmax=false, Params=Flash_fwd_params, Is_Q_split_K=false]" 
(589): here
            instantiation of "void flash::compute_attn<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &) [with Kernel_traits=Flash_fwd_kernel_traits<256, 64, 64, 4, false, false, cutlass::half_t, Flash_kernel_traits<256, 64, 64, 4, cutlass::half_t>>, Is_dropout=false, Is_causal=false, Is_even_N=true, Is_even_K=false, Return_softmax=false, Params=Flash_fwd_params]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(15): here
            instantiation of "void flash_fwd_kernel<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax>(Flash_fwd_params) [with Kernel_traits=Flash_fwd_kernel_traits<256, 64, 64, 4, false, false, cutlass::half_t, Flash_kernel_traits<256, 64, 64, 4, cutlass::half_t>>, Is_dropout=false, Is_causal=false, Is_even_N=true, Is_even_K=false, Return_softmax=false]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(34): here
            instantiation of "void run_flash_fwd<Kernel_traits,Is_dropout,Is_causal>(Flash_fwd_params &, cudaStream_t) [with Kernel_traits=Flash_fwd_kernel_traits<256, 64, 64, 4, false, false, cutlass::half_t, Flash_kernel_traits<256, 64, 64, 4, cutlass::half_t>>, Is_dropout=false, Is_causal=false]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(236): here
            instantiation of "void run_mha_fwd_hdim256<T>(Flash_fwd_params &, cudaStream_t) [with T=cutlass::half_t]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_hdim256_fp16_sm80.cu(8): here

/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_kernel.h(276): warning: variable "tQrQ" was declared but never referenced
          detected during:
            instantiation of "void flash::compute_attn_1rowblock<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params,Is_Q_split_K>(const Params &, int, int, int) [with Kernel_traits=Flash_fwd_kernel_traits<256, 64, 64, 4, false, false, cutlass::half_t, Flash_kernel_traits<256, 64, 64, 4, cutlass::half_t>>, Is_dropout=false, Is_causal=false, Is_even_N=true, Is_even_K=false, Return_softmax=false, Params=Flash_fwd_params, Is_Q_split_K=false]" 
(589): here
            instantiation of "void flash::compute_attn<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &) [with Kernel_traits=Flash_fwd_kernel_traits<256, 64, 64, 4, false, false, cutlass::half_t, Flash_kernel_traits<256, 64, 64, 4, cutlass::half_t>>, Is_dropout=false, Is_causal=false, Is_even_N=true, Is_even_K=false, Return_softmax=false, Params=Flash_fwd_params]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(15): here
            instantiation of "void flash_fwd_kernel<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax>(Flash_fwd_params) [with Kernel_traits=Flash_fwd_kernel_traits<256, 64, 64, 4, false, false, cutlass::half_t, Flash_kernel_traits<256, 64, 64, 4, cutlass::half_t>>, Is_dropout=false, Is_causal=false, Is_even_N=true, Is_even_K=false, Return_softmax=false]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(34): here
            instantiation of "void run_flash_fwd<Kernel_traits,Is_dropout,Is_causal>(Flash_fwd_params &, cudaStream_t) [with Kernel_traits=Flash_fwd_kernel_traits<256, 64, 64, 4, false, false, cutlass::half_t, Flash_kernel_traits<256, 64, 64, 4, cutlass::half_t>>, Is_dropout=false, Is_causal=false]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(236): here
            instantiation of "void run_mha_fwd_hdim256<T>(Flash_fwd_params &, cudaStream_t) [with T=cutlass::half_t]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_hdim256_fp16_sm80.cu(8): here

/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_kernel.h(140): warning: variable "MMA_M" was declared but never referenced
          detected during:
            instantiation of "void flash::compute_attn_1rowblock<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params,Is_Q_split_K>(const Params &, int, int, int) [with Kernel_traits=Flash_fwd_kernel_traits<256, 64, 64, 4, false, false, cutlass::half_t, Flash_kernel_traits<256, 64, 64, 4, cutlass::half_t>>, Is_dropout=false, Is_causal=false, Is_even_N=false, Is_even_K=true, Return_softmax=false, Params=Flash_fwd_params, Is_Q_split_K=false]" 
(589): here
            instantiation of "void flash::compute_attn<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &) [with Kernel_traits=Flash_fwd_kernel_traits<256, 64, 64, 4, false, false, cutlass::half_t, Flash_kernel_traits<256, 64, 64, 4, cutlass::half_t>>, Is_dropout=false, Is_causal=false, Is_even_N=false, Is_even_K=true, Return_softmax=false, Params=Flash_fwd_params]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(15): here
            instantiation of "void flash_fwd_kernel<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax>(Flash_fwd_params) [with Kernel_traits=Flash_fwd_kernel_traits<256, 64, 64, 4, false, false, cutlass::half_t, Flash_kernel_traits<256, 64, 64, 4, cutlass::half_t>>, Is_dropout=false, Is_causal=false, Is_even_N=false, Is_even_K=true, Return_softmax=false]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(34): here
            instantiation of "void run_flash_fwd<Kernel_traits,Is_dropout,Is_causal>(Flash_fwd_params &, cudaStream_t) [with Kernel_traits=Flash_fwd_kernel_traits<256, 64, 64, 4, false, false, cutlass::half_t, Flash_kernel_traits<256, 64, 64, 4, cutlass::half_t>>, Is_dropout=false, Is_causal=false]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(236): here
            instantiation of "void run_mha_fwd_hdim256<T>(Flash_fwd_params &, cudaStream_t) [with T=cutlass::half_t]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_hdim256_fp16_sm80.cu(8): here

/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_kernel.h(276): warning: variable "tQrQ" was declared but never referenced
          detected during:
            instantiation of "void flash::compute_attn_1rowblock<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params,Is_Q_split_K>(const Params &, int, int, int) [with Kernel_traits=Flash_fwd_kernel_traits<256, 64, 64, 4, false, false, cutlass::half_t, Flash_kernel_traits<256, 64, 64, 4, cutlass::half_t>>, Is_dropout=false, Is_causal=false, Is_even_N=false, Is_even_K=true, Return_softmax=false, Params=Flash_fwd_params, Is_Q_split_K=false]" 
(589): here
            instantiation of "void flash::compute_attn<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &) [with Kernel_traits=Flash_fwd_kernel_traits<256, 64, 64, 4, false, false, cutlass::half_t, Flash_kernel_traits<256, 64, 64, 4, cutlass::half_t>>, Is_dropout=false, Is_causal=false, Is_even_N=false, Is_even_K=true, Return_softmax=false, Params=Flash_fwd_params]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(15): here
            instantiation of "void flash_fwd_kernel<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax>(Flash_fwd_params) [with Kernel_traits=Flash_fwd_kernel_traits<256, 64, 64, 4, false, false, cutlass::half_t, Flash_kernel_traits<256, 64, 64, 4, cutlass::half_t>>, Is_dropout=false, Is_causal=false, Is_even_N=false, Is_even_K=true, Return_softmax=false]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(34): here
            instantiation of "void run_flash_fwd<Kernel_traits,Is_dropout,Is_causal>(Flash_fwd_params &, cudaStream_t) [with Kernel_traits=Flash_fwd_kernel_traits<256, 64, 64, 4, false, false, cutlass::half_t, Flash_kernel_traits<256, 64, 64, 4, cutlass::half_t>>, Is_dropout=false, Is_causal=false]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(236): here
            instantiation of "void run_mha_fwd_hdim256<T>(Flash_fwd_params &, cudaStream_t) [with T=cutlass::half_t]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_hdim256_fp16_sm80.cu(8): here

/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_kernel.h(140): warning: variable "MMA_M" was declared but never referenced
          detected during:
            instantiation of "void flash::compute_attn_1rowblock<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params,Is_Q_split_K>(const Params &, int, int, int) [with Kernel_traits=Flash_fwd_kernel_traits<256, 64, 64, 4, false, false, cutlass::half_t, Flash_kernel_traits<256, 64, 64, 4, cutlass::half_t>>, Is_dropout=false, Is_causal=false, Is_even_N=false, Is_even_K=false, Return_softmax=false, Params=Flash_fwd_params, Is_Q_split_K=false]" 
(589): here
            instantiation of "void flash::compute_attn<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &) [with Kernel_traits=Flash_fwd_kernel_traits<256, 64, 64, 4, false, false, cutlass::half_t, Flash_kernel_traits<256, 64, 64, 4, cutlass::half_t>>, Is_dropout=false, Is_causal=false, Is_even_N=false, Is_even_K=false, Return_softmax=false, Params=Flash_fwd_params]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(15): here
            instantiation of "void flash_fwd_kernel<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax>(Flash_fwd_params) [with Kernel_traits=Flash_fwd_kernel_traits<256, 64, 64, 4, false, false, cutlass::half_t, Flash_kernel_traits<256, 64, 64, 4, cutlass::half_t>>, Is_dropout=false, Is_causal=false, Is_even_N=false, Is_even_K=false, Return_softmax=false]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(34): here
            instantiation of "void run_flash_fwd<Kernel_traits,Is_dropout,Is_causal>(Flash_fwd_params &, cudaStream_t) [with Kernel_traits=Flash_fwd_kernel_traits<256, 64, 64, 4, false, false, cutlass::half_t, Flash_kernel_traits<256, 64, 64, 4, cutlass::half_t>>, Is_dropout=false, Is_causal=false]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(236): here
            instantiation of "void run_mha_fwd_hdim256<T>(Flash_fwd_params &, cudaStream_t) [with T=cutlass::half_t]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_hdim256_fp16_sm80.cu(8): here

/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_kernel.h(276): warning: variable "tQrQ" was declared but never referenced
          detected during:
            instantiation of "void flash::compute_attn_1rowblock<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params,Is_Q_split_K>(const Params &, int, int, int) [with Kernel_traits=Flash_fwd_kernel_traits<256, 64, 64, 4, false, false, cutlass::half_t, Flash_kernel_traits<256, 64, 64, 4, cutlass::half_t>>, Is_dropout=false, Is_causal=false, Is_even_N=false, Is_even_K=false, Return_softmax=false, Params=Flash_fwd_params, Is_Q_split_K=false]" 
(589): here
            instantiation of "void flash::compute_attn<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &) [with Kernel_traits=Flash_fwd_kernel_traits<256, 64, 64, 4, false, false, cutlass::half_t, Flash_kernel_traits<256, 64, 64, 4, cutlass::half_t>>, Is_dropout=false, Is_causal=false, Is_even_N=false, Is_even_K=false, Return_softmax=false, Params=Flash_fwd_params]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(15): here
            instantiation of "void flash_fwd_kernel<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax>(Flash_fwd_params) [with Kernel_traits=Flash_fwd_kernel_traits<256, 64, 64, 4, false, false, cutlass::half_t, Flash_kernel_traits<256, 64, 64, 4, cutlass::half_t>>, Is_dropout=false, Is_causal=false, Is_even_N=false, Is_even_K=false, Return_softmax=false]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(34): here
            instantiation of "void run_flash_fwd<Kernel_traits,Is_dropout,Is_causal>(Flash_fwd_params &, cudaStream_t) [with Kernel_traits=Flash_fwd_kernel_traits<256, 64, 64, 4, false, false, cutlass::half_t, Flash_kernel_traits<256, 64, 64, 4, cutlass::half_t>>, Is_dropout=false, Is_causal=false]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(236): here
            instantiation of "void run_mha_fwd_hdim256<T>(Flash_fwd_params &, cudaStream_t) [with T=cutlass::half_t]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_hdim256_fp16_sm80.cu(8): here

ptxas info    : 2 bytes gmem
ptxas info    : Compiling entry function '_Z16flash_fwd_kernelI23Flash_fwd_kernel_traitsILi256ELi64ELi64ELi4ELb0ELb0EN7cutlass6half_tE19Flash_kernel_traitsILi256ELi64ELi64ELi4ES2_EELb0ELb0ELb0ELb0ELb0EEv16Flash_fwd_params' for 'sm_80'
ptxas info    : Function properties for _Z16flash_fwd_kernelI23Flash_fwd_kernel_traitsILi256ELi64ELi64ELi4ELb0ELb0EN7cutlass6half_tE19Flash_kernel_traitsILi256ELi64ELi64ELi4ES2_EELb0ELb0ELb0ELb0ELb0EEv16Flash_fwd_params
    0 bytes stack frame, 0 bytes spill stores, 0 bytes spill loads
ptxas info    : Used 241 registers, 584 bytes cmem[0]
ptxas info    : Compiling entry function '_Z16flash_fwd_kernelI23Flash_fwd_kernel_traitsILi256ELi64ELi64ELi4ELb0ELb0EN7cutlass6half_tE19Flash_kernel_traitsILi256ELi64ELi64ELi4ES2_EELb0ELb0ELb0ELb1ELb0EEv16Flash_fwd_params' for 'sm_80'
ptxas info    : Function properties for _Z16flash_fwd_kernelI23Flash_fwd_kernel_traitsILi256ELi64ELi64ELi4ELb0ELb0EN7cutlass6half_tE19Flash_kernel_traitsILi256ELi64ELi64ELi4ES2_EELb0ELb0ELb0ELb1ELb0EEv16Flash_fwd_params
    0 bytes stack frame, 0 bytes spill stores, 0 bytes spill loads
ptxas info    : Used 237 registers, 584 bytes cmem[0]
ptxas info    : Compiling entry function '_Z16flash_fwd_kernelI23Flash_fwd_kernel_traitsILi256ELi64ELi64ELi4ELb0ELb0EN7cutlass6half_tE19Flash_kernel_traitsILi256ELi64ELi64ELi4ES2_EELb0ELb0ELb1ELb0ELb0EEv16Flash_fwd_params' for 'sm_80'
ptxas info    : Function properties for _Z16flash_fwd_kernelI23Flash_fwd_kernel_traitsILi256ELi64ELi64ELi4ELb0ELb0EN7cutlass6half_tE19Flash_kernel_traitsILi256ELi64ELi64ELi4ES2_EELb0ELb0ELb1ELb0ELb0EEv16Flash_fwd_params
    0 bytes stack frame, 0 bytes spill stores, 0 bytes spill loads
ptxas info    : Used 233 registers, 584 bytes cmem[0]
ptxas info    : Compiling entry function '_Z16flash_fwd_kernelI23Flash_fwd_kernel_traitsILi256ELi64ELi64ELi4ELb0ELb0EN7cutlass6half_tE19Flash_kernel_traitsILi256ELi64ELi64ELi4ES2_EELb0ELb0ELb1ELb1ELb0EEv16Flash_fwd_params' for 'sm_80'
ptxas info    : Function properties for _Z16flash_fwd_kernelI23Flash_fwd_kernel_traitsILi256ELi64ELi64ELi4ELb0ELb0EN7cutlass6half_tE19Flash_kernel_traitsILi256ELi64ELi64ELi4ES2_EELb0ELb0ELb1ELb1ELb0EEv16Flash_fwd_params
    0 bytes stack frame, 0 bytes spill stores, 0 bytes spill loads
ptxas info    : Used 237 registers, 584 bytes cmem[0]
ptxas info    : Compiling entry function '_Z16flash_fwd_kernelI23Flash_fwd_kernel_traitsILi256ELi128ELi64ELi8ELb0ELb0EN7cutlass6half_tE19Flash_kernel_traitsILi256ELi128ELi64ELi8ES2_EELb0ELb0ELb0ELb0ELb0EEv16Flash_fwd_params' for 'sm_80'
ptxas info    : Function properties for _Z16flash_fwd_kernelI23Flash_fwd_kernel_traitsILi256ELi128ELi64ELi8ELb0ELb0EN7cutlass6half_tE19Flash_kernel_traitsILi256ELi128ELi64ELi8ES2_EELb0ELb0ELb0ELb0ELb0EEv16Flash_fwd_params
    0 bytes stack frame, 0 bytes spill stores, 0 bytes spill loads
ptxas info    : Used 241 registers, 584 bytes cmem[0]
ptxas info    : Compiling entry function '_Z16flash_fwd_kernelI23Flash_fwd_kernel_traitsILi256ELi128ELi64ELi8ELb0ELb0EN7cutlass6half_tE19Flash_kernel_traitsILi256ELi128ELi64ELi8ES2_EELb0ELb0ELb0ELb1ELb0EEv16Flash_fwd_params' for 'sm_80'
ptxas info    : Function properties for _Z16flash_fwd_kernelI23Flash_fwd_kernel_traitsILi256ELi128ELi64ELi8ELb0ELb0EN7cutlass6half_tE19Flash_kernel_traitsILi256ELi128ELi64ELi8ES2_EELb0ELb0ELb0ELb1ELb0EEv16Flash_fwd_params
    0 bytes stack frame, 0 bytes spill stores, 0 bytes spill loads
ptxas info    : Used 237 registers, 584 bytes cmem[0]
ptxas info    : Compiling entry function '_Z16flash_fwd_kernelI23Flash_fwd_kernel_traitsILi256ELi128ELi64ELi8ELb0ELb0EN7cutlass6half_tE19Flash_kernel_traitsILi256ELi128ELi64ELi8ES2_EELb0ELb0ELb1ELb0ELb0EEv16Flash_fwd_params' for 'sm_80'
ptxas info    : Function properties for _Z16flash_fwd_kernelI23Flash_fwd_kernel_traitsILi256ELi128ELi64ELi8ELb0ELb0EN7cutlass6half_tE19Flash_kernel_traitsILi256ELi128ELi64ELi8ES2_EELb0ELb0ELb1ELb0ELb0EEv16Flash_fwd_params
    0 bytes stack frame, 0 bytes spill stores, 0 bytes spill loads
ptxas info    : Used 240 registers, 584 bytes cmem[0]
ptxas info    : Compiling entry function '_Z16flash_fwd_kernelI23Flash_fwd_kernel_traitsILi256ELi128ELi64ELi8ELb0ELb0EN7cutlass6half_tE19Flash_kernel_traitsILi256ELi128ELi64ELi8ES2_EELb0ELb0ELb1ELb1ELb0EEv16Flash_fwd_params' for 'sm_80'
ptxas info    : Function properties for _Z16flash_fwd_kernelI23Flash_fwd_kernel_traitsILi256ELi128ELi64ELi8ELb0ELb0EN7cutlass6half_tE19Flash_kernel_traitsILi256ELi128ELi64ELi8ES2_EELb0ELb0ELb1ELb1ELb0EEv16Flash_fwd_params
    0 bytes stack frame, 0 bytes spill stores, 0 bytes spill loads
ptxas info    : Used 237 registers, 584 bytes cmem[0]
ptxas info    : Compiling entry function '_Z16flash_fwd_kernelI23Flash_fwd_kernel_traitsILi256ELi64ELi64ELi4ELb0ELb0EN7cutlass6half_tE19Flash_kernel_traitsILi256ELi64ELi64ELi4ES2_EELb0ELb1ELb0ELb0ELb0EEv16Flash_fwd_params' for 'sm_80'
ptxas info    : Function properties for _Z16flash_fwd_kernelI23Flash_fwd_kernel_traitsILi256ELi64ELi64ELi4ELb0ELb0EN7cutlass6half_tE19Flash_kernel_traitsILi256ELi64ELi64ELi4ES2_EELb0ELb1ELb0ELb0ELb0EEv16Flash_fwd_params
    0 bytes stack frame, 0 bytes spill stores, 0 bytes spill loads
ptxas info    : Used 241 registers, 584 bytes cmem[0]
ptxas info    : Compiling entry function '_Z16flash_fwd_kernelI23Flash_fwd_kernel_traitsILi256ELi64ELi64ELi4ELb0ELb0EN7cutlass6half_tE19Flash_kernel_traitsILi256ELi64ELi64ELi4ES2_EELb0ELb1ELb0ELb1ELb0EEv16Flash_fwd_params' for 'sm_80'
ptxas info    : Function properties for _Z16flash_fwd_kernelI23Flash_fwd_kernel_traitsILi256ELi64ELi64ELi4ELb0ELb0EN7cutlass6half_tE19Flash_kernel_traitsILi256ELi64ELi64ELi4ES2_EELb0ELb1ELb0ELb1ELb0EEv16Flash_fwd_params
    0 bytes stack frame, 0 bytes spill stores, 0 bytes spill loads
ptxas info    : Used 238 registers, 584 bytes cmem[0]
ptxas info    : Compiling entry function '_Z16flash_fwd_kernelI23Flash_fwd_kernel_traitsILi256ELi64ELi64ELi4ELb0ELb0EN7cutlass6half_tE19Flash_kernel_traitsILi256ELi64ELi64ELi4ES2_EELb0ELb1ELb1ELb0ELb0EEv16Flash_fwd_params' for 'sm_80'
ptxas info    : Function properties for _Z16flash_fwd_kernelI23Flash_fwd_kernel_traitsILi256ELi64ELi64ELi4ELb0ELb0EN7cutlass6half_tE19Flash_kernel_traitsILi256ELi64ELi64ELi4ES2_EELb0ELb1ELb1ELb0ELb0EEv16Flash_fwd_params
    0 bytes stack frame, 0 bytes spill stores, 0 bytes spill loads
ptxas info    : Used 245 registers, 584 bytes cmem[0]
ptxas info    : Compiling entry function '_Z16flash_fwd_kernelI23Flash_fwd_kernel_traitsILi256ELi64ELi64ELi4ELb0ELb0EN7cutlass6half_tE19Flash_kernel_traitsILi256ELi64ELi64ELi4ES2_EELb0ELb1ELb1ELb1ELb0EEv16Flash_fwd_params' for 'sm_80'
ptxas info    : Function properties for _Z16flash_fwd_kernelI23Flash_fwd_kernel_traitsILi256ELi64ELi64ELi4ELb0ELb0EN7cutlass6half_tE19Flash_kernel_traitsILi256ELi64ELi64ELi4ES2_EELb0ELb1ELb1ELb1ELb0EEv16Flash_fwd_params
    0 bytes stack frame, 0 bytes spill stores, 0 bytes spill loads
ptxas info    : Used 241 registers, 584 bytes cmem[0]
ptxas info    : Compiling entry function '_Z16flash_fwd_kernelI23Flash_fwd_kernel_traitsILi256ELi128ELi64ELi8ELb0ELb0EN7cutlass6half_tE19Flash_kernel_traitsILi256ELi128ELi64ELi8ES2_EELb0ELb1ELb0ELb0ELb0EEv16Flash_fwd_params' for 'sm_80'
ptxas info    : Function properties for _Z16flash_fwd_kernelI23Flash_fwd_kernel_traitsILi256ELi128ELi64ELi8ELb0ELb0EN7cutlass6half_tE19Flash_kernel_traitsILi256ELi128ELi64ELi8ES2_EELb0ELb1ELb0ELb0ELb0EEv16Flash_fwd_params
    0 bytes stack frame, 0 bytes spill stores, 0 bytes spill loads
ptxas info    : Used 248 registers, 584 bytes cmem[0]
ptxas info    : Compiling entry function '_Z16flash_fwd_kernelI23Flash_fwd_kernel_traitsILi256ELi128ELi64ELi8ELb0ELb0EN7cutlass6half_tE19Flash_kernel_traitsILi256ELi128ELi64ELi8ES2_EELb0ELb1ELb0ELb1ELb0EEv16Flash_fwd_params' for 'sm_80'
ptxas info    : Function properties for _Z16flash_fwd_kernelI23Flash_fwd_kernel_traitsILi256ELi128ELi64ELi8ELb0ELb0EN7cutlass6half_tE19Flash_kernel_traitsILi256ELi128ELi64ELi8ES2_EELb0ELb1ELb0ELb1ELb0EEv16Flash_fwd_params
    0 bytes stack frame, 0 bytes spill stores, 0 bytes spill loads
ptxas info    : Used 245 registers, 584 bytes cmem[0]
ptxas info    : Compiling entry function '_Z16flash_fwd_kernelI23Flash_fwd_kernel_traitsILi256ELi128ELi64ELi8ELb0ELb0EN7cutlass6half_tE19Flash_kernel_traitsILi256ELi128ELi64ELi8ES2_EELb0ELb1ELb1ELb0ELb0EEv16Flash_fwd_params' for 'sm_80'
ptxas info    : Function properties for _Z16flash_fwd_kernelI23Flash_fwd_kernel_traitsILi256ELi128ELi64ELi8ELb0ELb0EN7cutlass6half_tE19Flash_kernel_traitsILi256ELi128ELi64ELi8ES2_EELb0ELb1ELb1ELb0ELb0EEv16Flash_fwd_params
    0 bytes stack frame, 0 bytes spill stores, 0 bytes spill loads
ptxas info    : Used 248 registers, 584 bytes cmem[0]
ptxas info    : Compiling entry function '_Z16flash_fwd_kernelI23Flash_fwd_kernel_traitsILi256ELi128ELi64ELi8ELb0ELb0EN7cutlass6half_tE19Flash_kernel_traitsILi256ELi128ELi64ELi8ES2_EELb0ELb1ELb1ELb1ELb0EEv16Flash_fwd_params' for 'sm_80'
ptxas info    : Function properties for _Z16flash_fwd_kernelI23Flash_fwd_kernel_traitsILi256ELi128ELi64ELi8ELb0ELb0EN7cutlass6half_tE19Flash_kernel_traitsILi256ELi128ELi64ELi8ES2_EELb0ELb1ELb1ELb1ELb0EEv16Flash_fwd_params
    0 bytes stack frame, 0 bytes spill stores, 0 bytes spill loads
ptxas info    : Used 250 registers, 584 bytes cmem[0]
ptxas info    : Compiling entry function '_Z16flash_fwd_kernelI23Flash_fwd_kernel_traitsILi256ELi64ELi64ELi4ELb0ELb0EN7cutlass6half_tE19Flash_kernel_traitsILi256ELi64ELi64ELi4ES2_EELb1ELb0ELb0ELb0ELb0EEv16Flash_fwd_params' for 'sm_80'
ptxas info    : Function properties for _Z16flash_fwd_kernelI23Flash_fwd_kernel_traitsILi256ELi64ELi64ELi4ELb0ELb0EN7cutlass6half_tE19Flash_kernel_traitsILi256ELi64ELi64ELi4ES2_EELb1ELb0ELb0ELb0ELb0EEv16Flash_fwd_params
    0 bytes stack frame, 0 bytes spill stores, 0 bytes spill loads
ptxas info    : Used 246 registers, 584 bytes cmem[0]
ptxas info    : Compiling entry function '_Z16flash_fwd_kernelI23Flash_fwd_kernel_traitsILi256ELi64ELi64ELi4ELb0ELb0EN7cutlass6half_tE19Flash_kernel_traitsILi256ELi64ELi64ELi4ES2_EELb1ELb0ELb0ELb0ELb1EEv16Flash_fwd_params' for 'sm_80'
ptxas info    : Function properties for _Z16flash_fwd_kernelI23Flash_fwd_kernel_traitsILi256ELi64ELi64ELi4ELb0ELb0EN7cutlass6half_tE19Flash_kernel_traitsILi256ELi64ELi64ELi4ES2_EELb1ELb0ELb0ELb0ELb1EEv16Flash_fwd_params
    112 bytes stack frame, 136 bytes spill stores, 128 bytes spill loads
ptxas info    : Used 255 registers, 584 bytes cmem[0]
ptxas info    : Compiling entry function '_Z16flash_fwd_kernelI23Flash_fwd_kernel_traitsILi256ELi64ELi64ELi4ELb0ELb0EN7cutlass6half_tE19Flash_kernel_traitsILi256ELi64ELi64ELi4ES2_EELb1ELb0ELb0ELb1ELb0EEv16Flash_fwd_params' for 'sm_80'
ptxas info    : Function properties for _Z16flash_fwd_kernelI23Flash_fwd_kernel_traitsILi256ELi64ELi64ELi4ELb0ELb0EN7cutlass6half_tE19Flash_kernel_traitsILi256ELi64ELi64ELi4ES2_EELb1ELb0ELb0ELb1ELb0EEv16Flash_fwd_params
    0 bytes stack frame, 0 bytes spill stores, 0 bytes spill loads
ptxas info    : Used 242 registers, 584 bytes cmem[0]
ptxas info    : Compiling entry function '_Z16flash_fwd_kernelI23Flash_fwd_kernel_traitsILi256ELi64ELi64ELi4ELb0ELb0EN7cutlass6half_tE19Flash_kernel_traitsILi256ELi64ELi64ELi4ES2_EELb1ELb0ELb0ELb1ELb1EEv16Flash_fwd_params' for 'sm_80'
ptxas info    : Function properties for _Z16flash_fwd_kernelI23Flash_fwd_kernel_traitsILi256ELi64ELi64ELi4ELb0ELb0EN7cutlass6half_tE19Flash_kernel_traitsILi256ELi64ELi64ELi4ES2_EELb1ELb0ELb0ELb1ELb1EEv16Flash_fwd_params
    104 bytes stack frame, 132 bytes spill stores, 132 bytes spill loads
ptxas info    : Used 255 registers, 584 bytes cmem[0]
ptxas info    : Compiling entry function '_Z16flash_fwd_kernelI23Flash_fwd_kernel_traitsILi256ELi64ELi64ELi4ELb0ELb0EN7cutlass6half_tE19Flash_kernel_traitsILi256ELi64ELi64ELi4ES2_EELb1ELb0ELb1ELb0ELb0EEv16Flash_fwd_params' for 'sm_80'
ptxas info    : Function properties for _Z16flash_fwd_kernelI23Flash_fwd_kernel_traitsILi256ELi64ELi64ELi4ELb0ELb0EN7cutlass6half_tE19Flash_kernel_traitsILi256ELi64ELi64ELi4ES2_EELb1ELb0ELb1ELb0ELb0EEv16Flash_fwd_params
    0 bytes stack frame, 0 bytes spill stores, 0 bytes spill loads
ptxas info    : Used 246 registers, 584 bytes cmem[0]
ptxas info    : Compiling entry function '_Z16flash_fwd_kernelI23Flash_fwd_kernel_traitsILi256ELi64ELi64ELi4ELb0ELb0EN7cutlass6half_tE19Flash_kernel_traitsILi256ELi64ELi64ELi4ES2_EELb1ELb0ELb1ELb0ELb1EEv16Flash_fwd_params' for 'sm_80'
ptxas info    : Function properties for _Z16flash_fwd_kernelI23Flash_fwd_kernel_traitsILi256ELi64ELi64ELi4ELb0ELb0EN7cutlass6half_tE19Flash_kernel_traitsILi256ELi64ELi64ELi4ES2_EELb1ELb0ELb1ELb0ELb1EEv16Flash_fwd_params
    104 bytes stack frame, 140 bytes spill stores, 132 bytes spill loads
ptxas info    : Used 255 registers, 584 bytes cmem[0]
ptxas info    : Compiling entry function '_Z16flash_fwd_kernelI23Flash_fwd_kernel_traitsILi256ELi64ELi64ELi4ELb0ELb0EN7cutlass6half_tE19Flash_kernel_traitsILi256ELi64ELi64ELi4ES2_EELb1ELb0ELb1ELb1ELb0EEv16Flash_fwd_params' for 'sm_80'
ptxas info    : Function properties for _Z16flash_fwd_kernelI23Flash_fwd_kernel_traitsILi256ELi64ELi64ELi4ELb0ELb0EN7cutlass6half_tE19Flash_kernel_traitsILi256ELi64ELi64ELi4ES2_EELb1ELb0ELb1ELb1ELb0EEv16Flash_fwd_params
    0 bytes stack frame, 0 bytes spill stores, 0 bytes spill loads
ptxas info    : Used 242 registers, 584 bytes cmem[0]
ptxas info    : Compiling entry function '_Z16flash_fwd_kernelI23Flash_fwd_kernel_traitsILi256ELi64ELi64ELi4ELb0ELb0EN7cutlass6half_tE19Flash_kernel_traitsILi256ELi64ELi64ELi4ES2_EELb1ELb0ELb1ELb1ELb1EEv16Flash_fwd_params' for 'sm_80'
ptxas info    : Function properties for _Z16flash_fwd_kernelI23Flash_fwd_kernel_traitsILi256ELi64ELi64ELi4ELb0ELb0EN7cutlass6half_tE19Flash_kernel_traitsILi256ELi64ELi64ELi4ES2_EELb1ELb0ELb1ELb1ELb1EEv16Flash_fwd_params
    104 bytes stack frame, 116 bytes spill stores, 116 bytes spill loads
ptxas info    : Used 255 registers, 584 bytes cmem[0]
ptxas info    : Compiling entry function '_Z16flash_fwd_kernelI23Flash_fwd_kernel_traitsILi256ELi128ELi64ELi8ELb0ELb0EN7cutlass6half_tE19Flash_kernel_traitsILi256ELi128ELi64ELi8ES2_EELb1ELb0ELb0ELb0ELb0EEv16Flash_fwd_params' for 'sm_80'
ptxas info    : Function properties for _Z16flash_fwd_kernelI23Flash_fwd_kernel_traitsILi256ELi128ELi64ELi8ELb0ELb0EN7cutlass6half_tE19Flash_kernel_traitsILi256ELi128ELi64ELi8ES2_EELb1ELb0ELb0ELb0ELb0EEv16Flash_fwd_params
    0 bytes stack frame, 0 bytes spill stores, 0 bytes spill loads
ptxas info    : Used 246 registers, 584 bytes cmem[0]
ptxas info    : Compiling entry function '_Z16flash_fwd_kernelI23Flash_fwd_kernel_traitsILi256ELi128ELi64ELi8ELb0ELb0EN7cutlass6half_tE19Flash_kernel_traitsILi256ELi128ELi64ELi8ES2_EELb1ELb0ELb0ELb0ELb1EEv16Flash_fwd_params' for 'sm_80'
ptxas info    : Function properties for _Z16flash_fwd_kernelI23Flash_fwd_kernel_traitsILi256ELi128ELi64ELi8ELb0ELb0EN7cutlass6half_tE19Flash_kernel_traitsILi256ELi128ELi64ELi8ES2_EELb1ELb0ELb0ELb0ELb1EEv16Flash_fwd_params
    112 bytes stack frame, 94 bytes spill stores, 94 bytes spill loads
ptxas info    : Used 255 registers, 584 bytes cmem[0]
ptxas info    : Compiling entry function '_Z16flash_fwd_kernelI23Flash_fwd_kernel_traitsILi256ELi128ELi64ELi8ELb0ELb0EN7cutlass6half_tE19Flash_kernel_traitsILi256ELi128ELi64ELi8ES2_EELb1ELb0ELb0ELb1ELb0EEv16Flash_fwd_params' for 'sm_80'
ptxas info    : Function properties for _Z16flash_fwd_kernelI23Flash_fwd_kernel_traitsILi256ELi128ELi64ELi8ELb0ELb0EN7cutlass6half_tE19Flash_kernel_traitsILi256ELi128ELi64ELi8ES2_EELb1ELb0ELb0ELb1ELb0EEv16Flash_fwd_params
    0 bytes stack frame, 0 bytes spill stores, 0 bytes spill loads
ptxas info    : Used 242 registers, 584 bytes cmem[0]
ptxas info    : Compiling entry function '_Z16flash_fwd_kernelI23Flash_fwd_kernel_traitsILi256ELi128ELi64ELi8ELb0ELb0EN7cutlass6half_tE19Flash_kernel_traitsILi256ELi128ELi64ELi8ES2_EELb1ELb0ELb0ELb1ELb1EEv16Flash_fwd_params' for 'sm_80'
ptxas info    : Function properties for _Z16flash_fwd_kernelI23Flash_fwd_kernel_traitsILi256ELi128ELi64ELi8ELb0ELb0EN7cutlass6half_tE19Flash_kernel_traitsILi256ELi128ELi64ELi8ES2_EELb1ELb0ELb0ELb1ELb1EEv16Flash_fwd_params
    80 bytes stack frame, 106 bytes spill stores, 82 bytes spill loads
ptxas info    : Used 255 registers, 584 bytes cmem[0]
ptxas info    : Compiling entry function '_Z16flash_fwd_kernelI23Flash_fwd_kernel_traitsILi256ELi128ELi64ELi8ELb0ELb0EN7cutlass6half_tE19Flash_kernel_traitsILi256ELi128ELi64ELi8ES2_EELb1ELb0ELb1ELb0ELb0EEv16Flash_fwd_params' for 'sm_80'
ptxas info    : Function properties for _Z16flash_fwd_kernelI23Flash_fwd_kernel_traitsILi256ELi128ELi64ELi8ELb0ELb0EN7cutlass6half_tE19Flash_kernel_traitsILi256ELi128ELi64ELi8ES2_EELb1ELb0ELb1ELb0ELb0EEv16Flash_fwd_params
    0 bytes stack frame, 0 bytes spill stores, 0 bytes spill loads
ptxas info    : Used 244 registers, 584 bytes cmem[0]
ptxas info    : Compiling entry function '_Z16flash_fwd_kernelI23Flash_fwd_kernel_traitsILi256ELi128ELi64ELi8ELb0ELb0EN7cutlass6half_tE19Flash_kernel_traitsILi256ELi128ELi64ELi8ES2_EELb1ELb0ELb1ELb0ELb1EEv16Flash_fwd_params' for 'sm_80'
ptxas info    : Function properties for _Z16flash_fwd_kernelI23Flash_fwd_kernel_traitsILi256ELi128ELi64ELi8ELb0ELb0EN7cutlass6half_tE19Flash_kernel_traitsILi256ELi128ELi64ELi8ES2_EELb1ELb0ELb1ELb0ELb1EEv16Flash_fwd_params
    56 bytes stack frame, 44 bytes spill stores, 44 bytes spill loads
ptxas info    : Used 255 registers, 584 bytes cmem[0]
ptxas info    : Compiling entry function '_Z16flash_fwd_kernelI23Flash_fwd_kernel_traitsILi256ELi128ELi64ELi8ELb0ELb0EN7cutlass6half_tE19Flash_kernel_traitsILi256ELi128ELi64ELi8ES2_EELb1ELb0ELb1ELb1ELb0EEv16Flash_fwd_params' for 'sm_80'
ptxas info    : Function properties for _Z16flash_fwd_kernelI23Flash_fwd_kernel_traitsILi256ELi128ELi64ELi8ELb0ELb0EN7cutlass6half_tE19Flash_kernel_traitsILi256ELi128ELi64ELi8ES2_EELb1ELb0ELb1ELb1ELb0EEv16Flash_fwd_params
    0 bytes stack frame, 0 bytes spill stores, 0 bytes spill loads
ptxas info    : Used 242 registers, 584 bytes cmem[0]
ptxas info    : Compiling entry function '_Z16flash_fwd_kernelI23Flash_fwd_kernel_traitsILi256ELi128ELi64ELi8ELb0ELb0EN7cutlass6half_tE19Flash_kernel_traitsILi256ELi128ELi64ELi8ES2_EELb1ELb0ELb1ELb1ELb1EEv16Flash_fwd_params' for 'sm_80'
ptxas info    : Function properties for _Z16flash_fwd_kernelI23Flash_fwd_kernel_traitsILi256ELi128ELi64ELi8ELb0ELb0EN7cutlass6half_tE19Flash_kernel_traitsILi256ELi128ELi64ELi8ES2_EELb1ELb0ELb1ELb1ELb1EEv16Flash_fwd_params
    80 bytes stack frame, 78 bytes spill stores, 70 bytes spill loads
ptxas info    : Used 255 registers, 584 bytes cmem[0]
ptxas info    : Compiling entry function '_Z16flash_fwd_kernelI23Flash_fwd_kernel_traitsILi256ELi64ELi64ELi4ELb0ELb0EN7cutlass6half_tE19Flash_kernel_traitsILi256ELi64ELi64ELi4ES2_EELb1ELb1ELb0ELb0ELb0EEv16Flash_fwd_params' for 'sm_80'
ptxas info    : Function properties for _Z16flash_fwd_kernelI23Flash_fwd_kernel_traitsILi256ELi64ELi64ELi4ELb0ELb0EN7cutlass6half_tE19Flash_kernel_traitsILi256ELi64ELi64ELi4ES2_EELb1ELb1ELb0ELb0ELb0EEv16Flash_fwd_params
    0 bytes stack frame, 0 bytes spill stores, 0 bytes spill loads
ptxas info    : Used 246 registers, 584 bytes cmem[0]
ptxas info    : Compiling entry function '_Z16flash_fwd_kernelI23Flash_fwd_kernel_traitsILi256ELi64ELi64ELi4ELb0ELb0EN7cutlass6half_tE19Flash_kernel_traitsILi256ELi64ELi64ELi4ES2_EELb1ELb1ELb0ELb0ELb1EEv16Flash_fwd_params' for 'sm_80'
ptxas info    : Function properties for _Z16flash_fwd_kernelI23Flash_fwd_kernel_traitsILi256ELi64ELi64ELi4ELb0ELb0EN7cutlass6half_tE19Flash_kernel_traitsILi256ELi64ELi64ELi4ES2_EELb1ELb1ELb0ELb0ELb1EEv16Flash_fwd_params
    104 bytes stack frame, 132 bytes spill stores, 124 bytes spill loads
ptxas info    : Used 255 registers, 584 bytes cmem[0]
ptxas info    : Compiling entry function '_Z16flash_fwd_kernelI23Flash_fwd_kernel_traitsILi256ELi64ELi64ELi4ELb0ELb0EN7cutlass6half_tE19Flash_kernel_traitsILi256ELi64ELi64ELi4ES2_EELb1ELb1ELb0ELb1ELb0EEv16Flash_fwd_params' for 'sm_80'
ptxas info    : Function properties for _Z16flash_fwd_kernelI23Flash_fwd_kernel_traitsILi256ELi64ELi64ELi4ELb0ELb0EN7cutlass6half_tE19Flash_kernel_traitsILi256ELi64ELi64ELi4ES2_EELb1ELb1ELb0ELb1ELb0EEv16Flash_fwd_params
    0 bytes stack frame, 0 bytes spill stores, 0 bytes spill loads
ptxas info    : Used 248 registers, 584 bytes cmem[0]
ptxas info    : Compiling entry function '_Z16flash_fwd_kernelI23Flash_fwd_kernel_traitsILi256ELi64ELi64ELi4ELb0ELb0EN7cutlass6half_tE19Flash_kernel_traitsILi256ELi64ELi64ELi4ES2_EELb1ELb1ELb0ELb1ELb1EEv16Flash_fwd_params' for 'sm_80'
ptxas info    : Function properties for _Z16flash_fwd_kernelI23Flash_fwd_kernel_traitsILi256ELi64ELi64ELi4ELb0ELb0EN7cutlass6half_tE19Flash_kernel_traitsILi256ELi64ELi64ELi4ES2_EELb1ELb1ELb0ELb1ELb1EEv16Flash_fwd_params
    104 bytes stack frame, 132 bytes spill stores, 132 bytes spill loads
ptxas info    : Used 255 registers, 584 bytes cmem[0]
ptxas info    : Compiling entry function '_Z16flash_fwd_kernelI23Flash_fwd_kernel_traitsILi256ELi64ELi64ELi4ELb0ELb0EN7cutlass6half_tE19Flash_kernel_traitsILi256ELi64ELi64ELi4ES2_EELb1ELb1ELb1ELb0ELb0EEv16Flash_fwd_params' for 'sm_80'
ptxas info    : Function properties for _Z16flash_fwd_kernelI23Flash_fwd_kernel_traitsILi256ELi64ELi64ELi4ELb0ELb0EN7cutlass6half_tE19Flash_kernel_traitsILi256ELi64ELi64ELi4ES2_EELb1ELb1ELb1ELb0ELb0EEv16Flash_fwd_params
    0 bytes stack frame, 0 bytes spill stores, 0 bytes spill loads
ptxas info    : Used 254 registers, 584 bytes cmem[0]
ptxas info    : Compiling entry function '_Z16flash_fwd_kernelI23Flash_fwd_kernel_traitsILi256ELi64ELi64ELi4ELb0ELb0EN7cutlass6half_tE19Flash_kernel_traitsILi256ELi64ELi64ELi4ES2_EELb1ELb1ELb1ELb0ELb1EEv16Flash_fwd_params' for 'sm_80'
ptxas info    : Function properties for _Z16flash_fwd_kernelI23Flash_fwd_kernel_traitsILi256ELi64ELi64ELi4ELb0ELb0EN7cutlass6half_tE19Flash_kernel_traitsILi256ELi64ELi64ELi4ES2_EELb1ELb1ELb1ELb0ELb1EEv16Flash_fwd_params
    112 bytes stack frame, 132 bytes spill stores, 124 bytes spill loads
ptxas info    : Used 255 registers, 584 bytes cmem[0]
ptxas info    : Compiling entry function '_Z16flash_fwd_kernelI23Flash_fwd_kernel_traitsILi256ELi64ELi64ELi4ELb0ELb0EN7cutlass6half_tE19Flash_kernel_traitsILi256ELi64ELi64ELi4ES2_EELb1ELb1ELb1ELb1ELb0EEv16Flash_fwd_params' for 'sm_80'
ptxas info    : Function properties for _Z16flash_fwd_kernelI23Flash_fwd_kernel_traitsILi256ELi64ELi64ELi4ELb0ELb0EN7cutlass6half_tE19Flash_kernel_traitsILi256ELi64ELi64ELi4ES2_EELb1ELb1ELb1ELb1ELb0EEv16Flash_fwd_params
    0 bytes stack frame, 0 bytes spill stores, 0 bytes spill loads
ptxas info    : Used 254 registers, 584 bytes cmem[0]
ptxas info    : Compiling entry function '_Z16flash_fwd_kernelI23Flash_fwd_kernel_traitsILi256ELi64ELi64ELi4ELb0ELb0EN7cutlass6half_tE19Flash_kernel_traitsILi256ELi64ELi64ELi4ES2_EELb1ELb1ELb1ELb1ELb1EEv16Flash_fwd_params' for 'sm_80'
ptxas info    : Function properties for _Z16flash_fwd_kernelI23Flash_fwd_kernel_traitsILi256ELi64ELi64ELi4ELb0ELb0EN7cutlass6half_tE19Flash_kernel_traitsILi256ELi64ELi64ELi4ES2_EELb1ELb1ELb1ELb1ELb1EEv16Flash_fwd_params
    96 bytes stack frame, 116 bytes spill stores, 116 bytes spill loads
ptxas info    : Used 255 registers, 584 bytes cmem[0]
ptxas info    : Compiling entry function '_Z16flash_fwd_kernelI23Flash_fwd_kernel_traitsILi256ELi128ELi64ELi8ELb0ELb0EN7cutlass6half_tE19Flash_kernel_traitsILi256ELi128ELi64ELi8ES2_EELb1ELb1ELb0ELb0ELb0EEv16Flash_fwd_params' for 'sm_80'
ptxas info    : Function properties for _Z16flash_fwd_kernelI23Flash_fwd_kernel_traitsILi256ELi128ELi64ELi8ELb0ELb0EN7cutlass6half_tE19Flash_kernel_traitsILi256ELi128ELi64ELi8ES2_EELb1ELb1ELb0ELb0ELb0EEv16Flash_fwd_params
    0 bytes stack frame, 0 bytes spill stores, 0 bytes spill loads
ptxas info    : Used 255 registers, 584 bytes cmem[0]
ptxas info    : Compiling entry function '_Z16flash_fwd_kernelI23Flash_fwd_kernel_traitsILi256ELi128ELi64ELi8ELb0ELb0EN7cutlass6half_tE19Flash_kernel_traitsILi256ELi128ELi64ELi8ES2_EELb1ELb1ELb0ELb0ELb1EEv16Flash_fwd_params' for 'sm_80'
ptxas info    : Function properties for _Z16flash_fwd_kernelI23Flash_fwd_kernel_traitsILi256ELi128ELi64ELi8ELb0ELb0EN7cutlass6half_tE19Flash_kernel_traitsILi256ELi128ELi64ELi8ES2_EELb1ELb1ELb0ELb0ELb1EEv16Flash_fwd_params
    104 bytes stack frame, 200 bytes spill stores, 176 bytes spill loads
ptxas info    : Used 255 registers, 584 bytes cmem[0]
ptxas info    : Compiling entry function '_Z16flash_fwd_kernelI23Flash_fwd_kernel_traitsILi256ELi128ELi64ELi8ELb0ELb0EN7cutlass6half_tE19Flash_kernel_traitsILi256ELi128ELi64ELi8ES2_EELb1ELb1ELb0ELb1ELb0EEv16Flash_fwd_params' for 'sm_80'
ptxas info    : Function properties for _Z16flash_fwd_kernelI23Flash_fwd_kernel_traitsILi256ELi128ELi64ELi8ELb0ELb0EN7cutlass6half_tE19Flash_kernel_traitsILi256ELi128ELi64ELi8ES2_EELb1ELb1ELb0ELb1ELb0EEv16Flash_fwd_params
    8 bytes stack frame, 4 bytes spill stores, 4 bytes spill loads
ptxas info    : Used 255 registers, 584 bytes cmem[0]
ptxas info    : Compiling entry function '_Z16flash_fwd_kernelI23Flash_fwd_kernel_traitsILi256ELi128ELi64ELi8ELb0ELb0EN7cutlass6half_tE19Flash_kernel_traitsILi256ELi128ELi64ELi8ES2_EELb1ELb1ELb0ELb1ELb1EEv16Flash_fwd_params' for 'sm_80'
ptxas info    : Function properties for _Z16flash_fwd_kernelI23Flash_fwd_kernel_traitsILi256ELi128ELi64ELi8ELb0ELb0EN7cutlass6half_tE19Flash_kernel_traitsILi256ELi128ELi64ELi8ES2_EELb1ELb1ELb0ELb1ELb1EEv16Flash_fwd_params
    152 bytes stack frame, 204 bytes spill stores, 204 bytes spill loads
ptxas info    : Used 255 registers, 584 bytes cmem[0]
ptxas info    : Compiling entry function '_Z16flash_fwd_kernelI23Flash_fwd_kernel_traitsILi256ELi128ELi64ELi8ELb0ELb0EN7cutlass6half_tE19Flash_kernel_traitsILi256ELi128ELi64ELi8ES2_EELb1ELb1ELb1ELb0ELb0EEv16Flash_fwd_params' for 'sm_80'
ptxas info    : Function properties for _Z16flash_fwd_kernelI23Flash_fwd_kernel_traitsILi256ELi128ELi64ELi8ELb0ELb0EN7cutlass6half_tE19Flash_kernel_traitsILi256ELi128ELi64ELi8ES2_EELb1ELb1ELb1ELb0ELb0EEv16Flash_fwd_params
    0 bytes stack frame, 0 bytes spill stores, 0 bytes spill loads
ptxas info    : Used 254 registers, 584 bytes cmem[0]
ptxas info    : Compiling entry function '_Z16flash_fwd_kernelI23Flash_fwd_kernel_traitsILi256ELi128ELi64ELi8ELb0ELb0EN7cutlass6half_tE19Flash_kernel_traitsILi256ELi128ELi64ELi8ES2_EELb1ELb1ELb1ELb0ELb1EEv16Flash_fwd_params' for 'sm_80'
ptxas info    : Function properties for _Z16flash_fwd_kernelI23Flash_fwd_kernel_traitsILi256ELi128ELi64ELi8ELb0ELb0EN7cutlass6half_tE19Flash_kernel_traitsILi256ELi128ELi64ELi8ES2_EELb1ELb1ELb1ELb0ELb1EEv16Flash_fwd_params
    144 bytes stack frame, 192 bytes spill stores, 168 bytes spill loads
ptxas info    : Used 255 registers, 584 bytes cmem[0]
ptxas info    : Compiling entry function '_Z16flash_fwd_kernelI23Flash_fwd_kernel_traitsILi256ELi128ELi64ELi8ELb0ELb0EN7cutlass6half_tE19Flash_kernel_traitsILi256ELi128ELi64ELi8ES2_EELb1ELb1ELb1ELb1ELb0EEv16Flash_fwd_params' for 'sm_80'
ptxas info    : Function properties for _Z16flash_fwd_kernelI23Flash_fwd_kernel_traitsILi256ELi128ELi64ELi8ELb0ELb0EN7cutlass6half_tE19Flash_kernel_traitsILi256ELi128ELi64ELi8ES2_EELb1ELb1ELb1ELb1ELb0EEv16Flash_fwd_params
    8 bytes stack frame, 4 bytes spill stores, 4 bytes spill loads
ptxas info    : Used 255 registers, 584 bytes cmem[0]
ptxas info    : Compiling entry function '_Z16flash_fwd_kernelI23Flash_fwd_kernel_traitsILi256ELi128ELi64ELi8ELb0ELb0EN7cutlass6half_tE19Flash_kernel_traitsILi256ELi128ELi64ELi8ES2_EELb1ELb1ELb1ELb1ELb1EEv16Flash_fwd_params' for 'sm_80'
ptxas info    : Function properties for _Z16flash_fwd_kernelI23Flash_fwd_kernel_traitsILi256ELi128ELi64ELi8ELb0ELb0EN7cutlass6half_tE19Flash_kernel_traitsILi256ELi128ELi64ELi8ES2_EELb1ELb1ELb1ELb1ELb1EEv16Flash_fwd_params
    208 bytes stack frame, 220 bytes spill stores, 224 bytes spill loads
ptxas info    : Used 255 registers, 584 bytes cmem[0]
/anaconda/envs/flashattention/lib/python3.9/site-packages/torch/include/c10/core/SymInt.h(84): warning: integer conversion resulted in a change of sign

[36/37] /usr/local/cuda-11.4/bin/nvcc  -I/home/v-feiychen/flash-attention/csrc/flash_attn -I/home/v-feiychen/flash-attention/csrc/flash_attn/src -I/home/v-feiychen/flash-attention/csrc/cutlass/include -I/anaconda/envs/flashattention/lib/python3.9/site-packages/torch/include -I/anaconda/envs/flashattention/lib/python3.9/site-packages/torch/include/torch/csrc/api/include -I/anaconda/envs/flashattention/lib/python3.9/site-packages/torch/include/TH -I/anaconda/envs/flashattention/lib/python3.9/site-packages/torch/include/THC -I/usr/local/cuda-11.4/include -I/anaconda/envs/flashattention/include/python3.9 -c -c /home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_hdim512_bf16_sm80.cu -o /home/v-feiychen/flash-attention/build/temp.linux-x86_64-cpython-39/csrc/flash_attn/src/flash_fwd_hdim512_bf16_sm80.o -D__CUDA_NO_HALF_OPERATORS__ -D__CUDA_NO_HALF_CONVERSIONS__ -D__CUDA_NO_BFLOAT16_CONVERSIONS__ -D__CUDA_NO_HALF2_OPERATORS__ --expt-relaxed-constexpr --compiler-options ''"'"'-fPIC'"'"'' -O3 -std=c++17 -U__CUDA_NO_HALF_OPERATORS__ -U__CUDA_NO_HALF_CONVERSIONS__ -U__CUDA_NO_HALF2_OPERATORS__ -U__CUDA_NO_BFLOAT16_CONVERSIONS__ --expt-relaxed-constexpr --expt-extended-lambda --use_fast_math --ptxas-options=-v -lineinfo -gencode arch=compute_80,code=sm_80 --threads 4 -DTORCH_API_INCLUDE_EXTENSION_H '-DPYBIND11_COMPILER_TYPE="_gcc"' '-DPYBIND11_STDLIB="_libstdcpp"' '-DPYBIND11_BUILD_ABI="_cxxabi1011"' -DTORCH_EXTENSION_NAME=flash_attn_2_cuda -D_GLIBCXX_USE_CXX11_ABI=0
/anaconda/envs/flashattention/lib/python3.9/site-packages/torch/include/c10/core/SymInt.h(84): warning: integer conversion resulted in a change of sign

/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_kernel.h(140): warning: variable "MMA_M" was declared but never referenced
          detected during:
            instantiation of "void flash::compute_attn_1rowblock<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params,Is_Q_split_K>(const Params &, int, int, int) [with Kernel_traits=Flash_fwd_kernel_traits<512, 128, 32, 8, false, true, cutlass::bfloat16_t, Flash_kernel_traits<512, 128, 32, 8, cutlass::bfloat16_t>>, Is_dropout=true, Is_causal=true, Is_even_N=true, Is_even_K=true, Return_softmax=true, Params=Flash_fwd_params, Is_Q_split_K=false]" 
(589): here
            instantiation of "void flash::compute_attn<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &) [with Kernel_traits=Flash_fwd_kernel_traits<512, 128, 32, 8, false, true, cutlass::bfloat16_t, Flash_kernel_traits<512, 128, 32, 8, cutlass::bfloat16_t>>, Is_dropout=true, Is_causal=true, Is_even_N=true, Is_even_K=true, Return_softmax=true, Params=Flash_fwd_params]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(15): here
            instantiation of "void flash_fwd_kernel<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax>(Flash_fwd_params) [with Kernel_traits=Flash_fwd_kernel_traits<512, 128, 32, 8, false, true, cutlass::bfloat16_t, Flash_kernel_traits<512, 128, 32, 8, cutlass::bfloat16_t>>, Is_dropout=true, Is_causal=true, Is_even_N=true, Is_even_K=true, Return_softmax=true]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(34): here
            instantiation of "void run_flash_fwd<Kernel_traits,Is_dropout,Is_causal>(Flash_fwd_params &, cudaStream_t) [with Kernel_traits=Flash_fwd_kernel_traits<512, 128, 32, 8, false, true, cutlass::bfloat16_t, Flash_kernel_traits<512, 128, 32, 8, cutlass::bfloat16_t>>, Is_dropout=true, Is_causal=true]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(264): here
            instantiation of "void run_mha_fwd_hdim512<T>(Flash_fwd_params &, cudaStream_t) [with T=cutlass::bfloat16_t]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_hdim512_bf16_sm80.cu(8): here

/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_kernel.h(276): warning: variable "tQrQ" was declared but never referenced
          detected during:
            instantiation of "void flash::compute_attn_1rowblock<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params,Is_Q_split_K>(const Params &, int, int, int) [with Kernel_traits=Flash_fwd_kernel_traits<512, 128, 32, 8, false, true, cutlass::bfloat16_t, Flash_kernel_traits<512, 128, 32, 8, cutlass::bfloat16_t>>, Is_dropout=true, Is_causal=true, Is_even_N=true, Is_even_K=true, Return_softmax=true, Params=Flash_fwd_params, Is_Q_split_K=false]" 
(589): here
            instantiation of "void flash::compute_attn<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &) [with Kernel_traits=Flash_fwd_kernel_traits<512, 128, 32, 8, false, true, cutlass::bfloat16_t, Flash_kernel_traits<512, 128, 32, 8, cutlass::bfloat16_t>>, Is_dropout=true, Is_causal=true, Is_even_N=true, Is_even_K=true, Return_softmax=true, Params=Flash_fwd_params]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(15): here
            instantiation of "void flash_fwd_kernel<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax>(Flash_fwd_params) [with Kernel_traits=Flash_fwd_kernel_traits<512, 128, 32, 8, false, true, cutlass::bfloat16_t, Flash_kernel_traits<512, 128, 32, 8, cutlass::bfloat16_t>>, Is_dropout=true, Is_causal=true, Is_even_N=true, Is_even_K=true, Return_softmax=true]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(34): here
            instantiation of "void run_flash_fwd<Kernel_traits,Is_dropout,Is_causal>(Flash_fwd_params &, cudaStream_t) [with Kernel_traits=Flash_fwd_kernel_traits<512, 128, 32, 8, false, true, cutlass::bfloat16_t, Flash_kernel_traits<512, 128, 32, 8, cutlass::bfloat16_t>>, Is_dropout=true, Is_causal=true]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(264): here
            instantiation of "void run_mha_fwd_hdim512<T>(Flash_fwd_params &, cudaStream_t) [with T=cutlass::bfloat16_t]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_hdim512_bf16_sm80.cu(8): here

/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_kernel.h(140): warning: variable "MMA_M" was declared but never referenced
          detected during:
            instantiation of "void flash::compute_attn_1rowblock<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params,Is_Q_split_K>(const Params &, int, int, int) [with Kernel_traits=Flash_fwd_kernel_traits<512, 128, 32, 8, false, true, cutlass::bfloat16_t, Flash_kernel_traits<512, 128, 32, 8, cutlass::bfloat16_t>>, Is_dropout=true, Is_causal=true, Is_even_N=true, Is_even_K=true, Return_softmax=false, Params=Flash_fwd_params, Is_Q_split_K=false]" 
(589): here
            instantiation of "void flash::compute_attn<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &) [with Kernel_traits=Flash_fwd_kernel_traits<512, 128, 32, 8, false, true, cutlass::bfloat16_t, Flash_kernel_traits<512, 128, 32, 8, cutlass::bfloat16_t>>, Is_dropout=true, Is_causal=true, Is_even_N=true, Is_even_K=true, Return_softmax=false, Params=Flash_fwd_params]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(15): here
            instantiation of "void flash_fwd_kernel<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax>(Flash_fwd_params) [with Kernel_traits=Flash_fwd_kernel_traits<512, 128, 32, 8, false, true, cutlass::bfloat16_t, Flash_kernel_traits<512, 128, 32, 8, cutlass::bfloat16_t>>, Is_dropout=true, Is_causal=true, Is_even_N=true, Is_even_K=true, Return_softmax=false]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(34): here
            instantiation of "void run_flash_fwd<Kernel_traits,Is_dropout,Is_causal>(Flash_fwd_params &, cudaStream_t) [with Kernel_traits=Flash_fwd_kernel_traits<512, 128, 32, 8, false, true, cutlass::bfloat16_t, Flash_kernel_traits<512, 128, 32, 8, cutlass::bfloat16_t>>, Is_dropout=true, Is_causal=true]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(264): here
            instantiation of "void run_mha_fwd_hdim512<T>(Flash_fwd_params &, cudaStream_t) [with T=cutlass::bfloat16_t]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_hdim512_bf16_sm80.cu(8): here

/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_kernel.h(276): warning: variable "tQrQ" was declared but never referenced
          detected during:
            instantiation of "void flash::compute_attn_1rowblock<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params,Is_Q_split_K>(const Params &, int, int, int) [with Kernel_traits=Flash_fwd_kernel_traits<512, 128, 32, 8, false, true, cutlass::bfloat16_t, Flash_kernel_traits<512, 128, 32, 8, cutlass::bfloat16_t>>, Is_dropout=true, Is_causal=true, Is_even_N=true, Is_even_K=true, Return_softmax=false, Params=Flash_fwd_params, Is_Q_split_K=false]" 
(589): here
            instantiation of "void flash::compute_attn<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &) [with Kernel_traits=Flash_fwd_kernel_traits<512, 128, 32, 8, false, true, cutlass::bfloat16_t, Flash_kernel_traits<512, 128, 32, 8, cutlass::bfloat16_t>>, Is_dropout=true, Is_causal=true, Is_even_N=true, Is_even_K=true, Return_softmax=false, Params=Flash_fwd_params]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(15): here
            instantiation of "void flash_fwd_kernel<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax>(Flash_fwd_params) [with Kernel_traits=Flash_fwd_kernel_traits<512, 128, 32, 8, false, true, cutlass::bfloat16_t, Flash_kernel_traits<512, 128, 32, 8, cutlass::bfloat16_t>>, Is_dropout=true, Is_causal=true, Is_even_N=true, Is_even_K=true, Return_softmax=false]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(34): here
            instantiation of "void run_flash_fwd<Kernel_traits,Is_dropout,Is_causal>(Flash_fwd_params &, cudaStream_t) [with Kernel_traits=Flash_fwd_kernel_traits<512, 128, 32, 8, false, true, cutlass::bfloat16_t, Flash_kernel_traits<512, 128, 32, 8, cutlass::bfloat16_t>>, Is_dropout=true, Is_causal=true]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(264): here
            instantiation of "void run_mha_fwd_hdim512<T>(Flash_fwd_params &, cudaStream_t) [with T=cutlass::bfloat16_t]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_hdim512_bf16_sm80.cu(8): here

/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_kernel.h(140): warning: variable "MMA_M" was declared but never referenced
          detected during:
            instantiation of "void flash::compute_attn_1rowblock<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params,Is_Q_split_K>(const Params &, int, int, int) [with Kernel_traits=Flash_fwd_kernel_traits<512, 128, 32, 8, false, true, cutlass::bfloat16_t, Flash_kernel_traits<512, 128, 32, 8, cutlass::bfloat16_t>>, Is_dropout=true, Is_causal=true, Is_even_N=true, Is_even_K=false, Return_softmax=true, Params=Flash_fwd_params, Is_Q_split_K=false]" 
(589): here
            instantiation of "void flash::compute_attn<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &) [with Kernel_traits=Flash_fwd_kernel_traits<512, 128, 32, 8, false, true, cutlass::bfloat16_t, Flash_kernel_traits<512, 128, 32, 8, cutlass::bfloat16_t>>, Is_dropout=true, Is_causal=true, Is_even_N=true, Is_even_K=false, Return_softmax=true, Params=Flash_fwd_params]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(15): here
            instantiation of "void flash_fwd_kernel<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax>(Flash_fwd_params) [with Kernel_traits=Flash_fwd_kernel_traits<512, 128, 32, 8, false, true, cutlass::bfloat16_t, Flash_kernel_traits<512, 128, 32, 8, cutlass::bfloat16_t>>, Is_dropout=true, Is_causal=true, Is_even_N=true, Is_even_K=false, Return_softmax=true]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(34): here
            instantiation of "void run_flash_fwd<Kernel_traits,Is_dropout,Is_causal>(Flash_fwd_params &, cudaStream_t) [with Kernel_traits=Flash_fwd_kernel_traits<512, 128, 32, 8, false, true, cutlass::bfloat16_t, Flash_kernel_traits<512, 128, 32, 8, cutlass::bfloat16_t>>, Is_dropout=true, Is_causal=true]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(264): here
            instantiation of "void run_mha_fwd_hdim512<T>(Flash_fwd_params &, cudaStream_t) [with T=cutlass::bfloat16_t]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_hdim512_bf16_sm80.cu(8): here

/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_kernel.h(276): warning: variable "tQrQ" was declared but never referenced
          detected during:
            instantiation of "void flash::compute_attn_1rowblock<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params,Is_Q_split_K>(const Params &, int, int, int) [with Kernel_traits=Flash_fwd_kernel_traits<512, 128, 32, 8, false, true, cutlass::bfloat16_t, Flash_kernel_traits<512, 128, 32, 8, cutlass::bfloat16_t>>, Is_dropout=true, Is_causal=true, Is_even_N=true, Is_even_K=false, Return_softmax=true, Params=Flash_fwd_params, Is_Q_split_K=false]" 
(589): here
            instantiation of "void flash::compute_attn<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &) [with Kernel_traits=Flash_fwd_kernel_traits<512, 128, 32, 8, false, true, cutlass::bfloat16_t, Flash_kernel_traits<512, 128, 32, 8, cutlass::bfloat16_t>>, Is_dropout=true, Is_causal=true, Is_even_N=true, Is_even_K=false, Return_softmax=true, Params=Flash_fwd_params]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(15): here
            instantiation of "void flash_fwd_kernel<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax>(Flash_fwd_params) [with Kernel_traits=Flash_fwd_kernel_traits<512, 128, 32, 8, false, true, cutlass::bfloat16_t, Flash_kernel_traits<512, 128, 32, 8, cutlass::bfloat16_t>>, Is_dropout=true, Is_causal=true, Is_even_N=true, Is_even_K=false, Return_softmax=true]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(34): here
            instantiation of "void run_flash_fwd<Kernel_traits,Is_dropout,Is_causal>(Flash_fwd_params &, cudaStream_t) [with Kernel_traits=Flash_fwd_kernel_traits<512, 128, 32, 8, false, true, cutlass::bfloat16_t, Flash_kernel_traits<512, 128, 32, 8, cutlass::bfloat16_t>>, Is_dropout=true, Is_causal=true]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(264): here
            instantiation of "void run_mha_fwd_hdim512<T>(Flash_fwd_params &, cudaStream_t) [with T=cutlass::bfloat16_t]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_hdim512_bf16_sm80.cu(8): here

/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_kernel.h(140): warning: variable "MMA_M" was declared but never referenced
          detected during:
            instantiation of "void flash::compute_attn_1rowblock<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params,Is_Q_split_K>(const Params &, int, int, int) [with Kernel_traits=Flash_fwd_kernel_traits<512, 128, 32, 8, false, true, cutlass::bfloat16_t, Flash_kernel_traits<512, 128, 32, 8, cutlass::bfloat16_t>>, Is_dropout=true, Is_causal=true, Is_even_N=true, Is_even_K=false, Return_softmax=false, Params=Flash_fwd_params, Is_Q_split_K=false]" 
(589): here
            instantiation of "void flash::compute_attn<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &) [with Kernel_traits=Flash_fwd_kernel_traits<512, 128, 32, 8, false, true, cutlass::bfloat16_t, Flash_kernel_traits<512, 128, 32, 8, cutlass::bfloat16_t>>, Is_dropout=true, Is_causal=true, Is_even_N=true, Is_even_K=false, Return_softmax=false, Params=Flash_fwd_params]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(15): here
            instantiation of "void flash_fwd_kernel<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax>(Flash_fwd_params) [with Kernel_traits=Flash_fwd_kernel_traits<512, 128, 32, 8, false, true, cutlass::bfloat16_t, Flash_kernel_traits<512, 128, 32, 8, cutlass::bfloat16_t>>, Is_dropout=true, Is_causal=true, Is_even_N=true, Is_even_K=false, Return_softmax=false]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(34): here
            instantiation of "void run_flash_fwd<Kernel_traits,Is_dropout,Is_causal>(Flash_fwd_params &, cudaStream_t) [with Kernel_traits=Flash_fwd_kernel_traits<512, 128, 32, 8, false, true, cutlass::bfloat16_t, Flash_kernel_traits<512, 128, 32, 8, cutlass::bfloat16_t>>, Is_dropout=true, Is_causal=true]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(264): here
            instantiation of "void run_mha_fwd_hdim512<T>(Flash_fwd_params &, cudaStream_t) [with T=cutlass::bfloat16_t]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_hdim512_bf16_sm80.cu(8): here

/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_kernel.h(276): warning: variable "tQrQ" was declared but never referenced
          detected during:
            instantiation of "void flash::compute_attn_1rowblock<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params,Is_Q_split_K>(const Params &, int, int, int) [with Kernel_traits=Flash_fwd_kernel_traits<512, 128, 32, 8, false, true, cutlass::bfloat16_t, Flash_kernel_traits<512, 128, 32, 8, cutlass::bfloat16_t>>, Is_dropout=true, Is_causal=true, Is_even_N=true, Is_even_K=false, Return_softmax=false, Params=Flash_fwd_params, Is_Q_split_K=false]" 
(589): here
            instantiation of "void flash::compute_attn<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &) [with Kernel_traits=Flash_fwd_kernel_traits<512, 128, 32, 8, false, true, cutlass::bfloat16_t, Flash_kernel_traits<512, 128, 32, 8, cutlass::bfloat16_t>>, Is_dropout=true, Is_causal=true, Is_even_N=true, Is_even_K=false, Return_softmax=false, Params=Flash_fwd_params]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(15): here
            instantiation of "void flash_fwd_kernel<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax>(Flash_fwd_params) [with Kernel_traits=Flash_fwd_kernel_traits<512, 128, 32, 8, false, true, cutlass::bfloat16_t, Flash_kernel_traits<512, 128, 32, 8, cutlass::bfloat16_t>>, Is_dropout=true, Is_causal=true, Is_even_N=true, Is_even_K=false, Return_softmax=false]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(34): here
            instantiation of "void run_flash_fwd<Kernel_traits,Is_dropout,Is_causal>(Flash_fwd_params &, cudaStream_t) [with Kernel_traits=Flash_fwd_kernel_traits<512, 128, 32, 8, false, true, cutlass::bfloat16_t, Flash_kernel_traits<512, 128, 32, 8, cutlass::bfloat16_t>>, Is_dropout=true, Is_causal=true]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(264): here
            instantiation of "void run_mha_fwd_hdim512<T>(Flash_fwd_params &, cudaStream_t) [with T=cutlass::bfloat16_t]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_hdim512_bf16_sm80.cu(8): here

/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_kernel.h(140): warning: variable "MMA_M" was declared but never referenced
          detected during:
            instantiation of "void flash::compute_attn_1rowblock<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params,Is_Q_split_K>(const Params &, int, int, int) [with Kernel_traits=Flash_fwd_kernel_traits<512, 128, 32, 8, false, true, cutlass::bfloat16_t, Flash_kernel_traits<512, 128, 32, 8, cutlass::bfloat16_t>>, Is_dropout=true, Is_causal=true, Is_even_N=false, Is_even_K=true, Return_softmax=true, Params=Flash_fwd_params, Is_Q_split_K=false]" 
(589): here
            instantiation of "void flash::compute_attn<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &) [with Kernel_traits=Flash_fwd_kernel_traits<512, 128, 32, 8, false, true, cutlass::bfloat16_t, Flash_kernel_traits<512, 128, 32, 8, cutlass::bfloat16_t>>, Is_dropout=true, Is_causal=true, Is_even_N=false, Is_even_K=true, Return_softmax=true, Params=Flash_fwd_params]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(15): here
            instantiation of "void flash_fwd_kernel<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax>(Flash_fwd_params) [with Kernel_traits=Flash_fwd_kernel_traits<512, 128, 32, 8, false, true, cutlass::bfloat16_t, Flash_kernel_traits<512, 128, 32, 8, cutlass::bfloat16_t>>, Is_dropout=true, Is_causal=true, Is_even_N=false, Is_even_K=true, Return_softmax=true]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(34): here
            instantiation of "void run_flash_fwd<Kernel_traits,Is_dropout,Is_causal>(Flash_fwd_params &, cudaStream_t) [with Kernel_traits=Flash_fwd_kernel_traits<512, 128, 32, 8, false, true, cutlass::bfloat16_t, Flash_kernel_traits<512, 128, 32, 8, cutlass::bfloat16_t>>, Is_dropout=true, Is_causal=true]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(264): here
            instantiation of "void run_mha_fwd_hdim512<T>(Flash_fwd_params &, cudaStream_t) [with T=cutlass::bfloat16_t]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_hdim512_bf16_sm80.cu(8): here

/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_kernel.h(276): warning: variable "tQrQ" was declared but never referenced
          detected during:
            instantiation of "void flash::compute_attn_1rowblock<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params,Is_Q_split_K>(const Params &, int, int, int) [with Kernel_traits=Flash_fwd_kernel_traits<512, 128, 32, 8, false, true, cutlass::bfloat16_t, Flash_kernel_traits<512, 128, 32, 8, cutlass::bfloat16_t>>, Is_dropout=true, Is_causal=true, Is_even_N=false, Is_even_K=true, Return_softmax=true, Params=Flash_fwd_params, Is_Q_split_K=false]" 
(589): here
            instantiation of "void flash::compute_attn<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &) [with Kernel_traits=Flash_fwd_kernel_traits<512, 128, 32, 8, false, true, cutlass::bfloat16_t, Flash_kernel_traits<512, 128, 32, 8, cutlass::bfloat16_t>>, Is_dropout=true, Is_causal=true, Is_even_N=false, Is_even_K=true, Return_softmax=true, Params=Flash_fwd_params]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(15): here
            instantiation of "void flash_fwd_kernel<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax>(Flash_fwd_params) [with Kernel_traits=Flash_fwd_kernel_traits<512, 128, 32, 8, false, true, cutlass::bfloat16_t, Flash_kernel_traits<512, 128, 32, 8, cutlass::bfloat16_t>>, Is_dropout=true, Is_causal=true, Is_even_N=false, Is_even_K=true, Return_softmax=true]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(34): here
            instantiation of "void run_flash_fwd<Kernel_traits,Is_dropout,Is_causal>(Flash_fwd_params &, cudaStream_t) [with Kernel_traits=Flash_fwd_kernel_traits<512, 128, 32, 8, false, true, cutlass::bfloat16_t, Flash_kernel_traits<512, 128, 32, 8, cutlass::bfloat16_t>>, Is_dropout=true, Is_causal=true]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(264): here
            instantiation of "void run_mha_fwd_hdim512<T>(Flash_fwd_params &, cudaStream_t) [with T=cutlass::bfloat16_t]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_hdim512_bf16_sm80.cu(8): here

/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_kernel.h(140): warning: variable "MMA_M" was declared but never referenced
          detected during:
            instantiation of "void flash::compute_attn_1rowblock<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params,Is_Q_split_K>(const Params &, int, int, int) [with Kernel_traits=Flash_fwd_kernel_traits<512, 128, 32, 8, false, true, cutlass::bfloat16_t, Flash_kernel_traits<512, 128, 32, 8, cutlass::bfloat16_t>>, Is_dropout=true, Is_causal=true, Is_even_N=false, Is_even_K=true, Return_softmax=false, Params=Flash_fwd_params, Is_Q_split_K=false]" 
(589): here
            instantiation of "void flash::compute_attn<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &) [with Kernel_traits=Flash_fwd_kernel_traits<512, 128, 32, 8, false, true, cutlass::bfloat16_t, Flash_kernel_traits<512, 128, 32, 8, cutlass::bfloat16_t>>, Is_dropout=true, Is_causal=true, Is_even_N=false, Is_even_K=true, Return_softmax=false, Params=Flash_fwd_params]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(15): here
            instantiation of "void flash_fwd_kernel<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax>(Flash_fwd_params) [with Kernel_traits=Flash_fwd_kernel_traits<512, 128, 32, 8, false, true, cutlass::bfloat16_t, Flash_kernel_traits<512, 128, 32, 8, cutlass::bfloat16_t>>, Is_dropout=true, Is_causal=true, Is_even_N=false, Is_even_K=true, Return_softmax=false]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(34): here
            instantiation of "void run_flash_fwd<Kernel_traits,Is_dropout,Is_causal>(Flash_fwd_params &, cudaStream_t) [with Kernel_traits=Flash_fwd_kernel_traits<512, 128, 32, 8, false, true, cutlass::bfloat16_t, Flash_kernel_traits<512, 128, 32, 8, cutlass::bfloat16_t>>, Is_dropout=true, Is_causal=true]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(264): here
            instantiation of "void run_mha_fwd_hdim512<T>(Flash_fwd_params &, cudaStream_t) [with T=cutlass::bfloat16_t]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_hdim512_bf16_sm80.cu(8): here

/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_kernel.h(276): warning: variable "tQrQ" was declared but never referenced
          detected during:
            instantiation of "void flash::compute_attn_1rowblock<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params,Is_Q_split_K>(const Params &, int, int, int) [with Kernel_traits=Flash_fwd_kernel_traits<512, 128, 32, 8, false, true, cutlass::bfloat16_t, Flash_kernel_traits<512, 128, 32, 8, cutlass::bfloat16_t>>, Is_dropout=true, Is_causal=true, Is_even_N=false, Is_even_K=true, Return_softmax=false, Params=Flash_fwd_params, Is_Q_split_K=false]" 
(589): here
            instantiation of "void flash::compute_attn<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &) [with Kernel_traits=Flash_fwd_kernel_traits<512, 128, 32, 8, false, true, cutlass::bfloat16_t, Flash_kernel_traits<512, 128, 32, 8, cutlass::bfloat16_t>>, Is_dropout=true, Is_causal=true, Is_even_N=false, Is_even_K=true, Return_softmax=false, Params=Flash_fwd_params]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(15): here
            instantiation of "void flash_fwd_kernel<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax>(Flash_fwd_params) [with Kernel_traits=Flash_fwd_kernel_traits<512, 128, 32, 8, false, true, cutlass::bfloat16_t, Flash_kernel_traits<512, 128, 32, 8, cutlass::bfloat16_t>>, Is_dropout=true, Is_causal=true, Is_even_N=false, Is_even_K=true, Return_softmax=false]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(34): here
            instantiation of "void run_flash_fwd<Kernel_traits,Is_dropout,Is_causal>(Flash_fwd_params &, cudaStream_t) [with Kernel_traits=Flash_fwd_kernel_traits<512, 128, 32, 8, false, true, cutlass::bfloat16_t, Flash_kernel_traits<512, 128, 32, 8, cutlass::bfloat16_t>>, Is_dropout=true, Is_causal=true]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(264): here
            instantiation of "void run_mha_fwd_hdim512<T>(Flash_fwd_params &, cudaStream_t) [with T=cutlass::bfloat16_t]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_hdim512_bf16_sm80.cu(8): here

/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_kernel.h(140): warning: variable "MMA_M" was declared but never referenced
          detected during:
            instantiation of "void flash::compute_attn_1rowblock<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params,Is_Q_split_K>(const Params &, int, int, int) [with Kernel_traits=Flash_fwd_kernel_traits<512, 128, 32, 8, false, true, cutlass::bfloat16_t, Flash_kernel_traits<512, 128, 32, 8, cutlass::bfloat16_t>>, Is_dropout=true, Is_causal=true, Is_even_N=false, Is_even_K=false, Return_softmax=true, Params=Flash_fwd_params, Is_Q_split_K=false]" 
(589): here
            instantiation of "void flash::compute_attn<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &) [with Kernel_traits=Flash_fwd_kernel_traits<512, 128, 32, 8, false, true, cutlass::bfloat16_t, Flash_kernel_traits<512, 128, 32, 8, cutlass::bfloat16_t>>, Is_dropout=true, Is_causal=true, Is_even_N=false, Is_even_K=false, Return_softmax=true, Params=Flash_fwd_params]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(15): here
            instantiation of "void flash_fwd_kernel<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax>(Flash_fwd_params) [with Kernel_traits=Flash_fwd_kernel_traits<512, 128, 32, 8, false, true, cutlass::bfloat16_t, Flash_kernel_traits<512, 128, 32, 8, cutlass::bfloat16_t>>, Is_dropout=true, Is_causal=true, Is_even_N=false, Is_even_K=false, Return_softmax=true]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(34): here
            instantiation of "void run_flash_fwd<Kernel_traits,Is_dropout,Is_causal>(Flash_fwd_params &, cudaStream_t) [with Kernel_traits=Flash_fwd_kernel_traits<512, 128, 32, 8, false, true, cutlass::bfloat16_t, Flash_kernel_traits<512, 128, 32, 8, cutlass::bfloat16_t>>, Is_dropout=true, Is_causal=true]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(264): here
            instantiation of "void run_mha_fwd_hdim512<T>(Flash_fwd_params &, cudaStream_t) [with T=cutlass::bfloat16_t]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_hdim512_bf16_sm80.cu(8): here

/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_kernel.h(276): warning: variable "tQrQ" was declared but never referenced
          detected during:
            instantiation of "void flash::compute_attn_1rowblock<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params,Is_Q_split_K>(const Params &, int, int, int) [with Kernel_traits=Flash_fwd_kernel_traits<512, 128, 32, 8, false, true, cutlass::bfloat16_t, Flash_kernel_traits<512, 128, 32, 8, cutlass::bfloat16_t>>, Is_dropout=true, Is_causal=true, Is_even_N=false, Is_even_K=false, Return_softmax=true, Params=Flash_fwd_params, Is_Q_split_K=false]" 
(589): here
            instantiation of "void flash::compute_attn<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &) [with Kernel_traits=Flash_fwd_kernel_traits<512, 128, 32, 8, false, true, cutlass::bfloat16_t, Flash_kernel_traits<512, 128, 32, 8, cutlass::bfloat16_t>>, Is_dropout=true, Is_causal=true, Is_even_N=false, Is_even_K=false, Return_softmax=true, Params=Flash_fwd_params]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(15): here
            instantiation of "void flash_fwd_kernel<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax>(Flash_fwd_params) [with Kernel_traits=Flash_fwd_kernel_traits<512, 128, 32, 8, false, true, cutlass::bfloat16_t, Flash_kernel_traits<512, 128, 32, 8, cutlass::bfloat16_t>>, Is_dropout=true, Is_causal=true, Is_even_N=false, Is_even_K=false, Return_softmax=true]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(34): here
            instantiation of "void run_flash_fwd<Kernel_traits,Is_dropout,Is_causal>(Flash_fwd_params &, cudaStream_t) [with Kernel_traits=Flash_fwd_kernel_traits<512, 128, 32, 8, false, true, cutlass::bfloat16_t, Flash_kernel_traits<512, 128, 32, 8, cutlass::bfloat16_t>>, Is_dropout=true, Is_causal=true]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(264): here
            instantiation of "void run_mha_fwd_hdim512<T>(Flash_fwd_params &, cudaStream_t) [with T=cutlass::bfloat16_t]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_hdim512_bf16_sm80.cu(8): here

/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_kernel.h(140): warning: variable "MMA_M" was declared but never referenced
          detected during:
            instantiation of "void flash::compute_attn_1rowblock<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params,Is_Q_split_K>(const Params &, int, int, int) [with Kernel_traits=Flash_fwd_kernel_traits<512, 128, 32, 8, false, true, cutlass::bfloat16_t, Flash_kernel_traits<512, 128, 32, 8, cutlass::bfloat16_t>>, Is_dropout=true, Is_causal=true, Is_even_N=false, Is_even_K=false, Return_softmax=false, Params=Flash_fwd_params, Is_Q_split_K=false]" 
(589): here
            instantiation of "void flash::compute_attn<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &) [with Kernel_traits=Flash_fwd_kernel_traits<512, 128, 32, 8, false, true, cutlass::bfloat16_t, Flash_kernel_traits<512, 128, 32, 8, cutlass::bfloat16_t>>, Is_dropout=true, Is_causal=true, Is_even_N=false, Is_even_K=false, Return_softmax=false, Params=Flash_fwd_params]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(15): here
            instantiation of "void flash_fwd_kernel<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax>(Flash_fwd_params) [with Kernel_traits=Flash_fwd_kernel_traits<512, 128, 32, 8, false, true, cutlass::bfloat16_t, Flash_kernel_traits<512, 128, 32, 8, cutlass::bfloat16_t>>, Is_dropout=true, Is_causal=true, Is_even_N=false, Is_even_K=false, Return_softmax=false]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(34): here
            instantiation of "void run_flash_fwd<Kernel_traits,Is_dropout,Is_causal>(Flash_fwd_params &, cudaStream_t) [with Kernel_traits=Flash_fwd_kernel_traits<512, 128, 32, 8, false, true, cutlass::bfloat16_t, Flash_kernel_traits<512, 128, 32, 8, cutlass::bfloat16_t>>, Is_dropout=true, Is_causal=true]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(264): here
            instantiation of "void run_mha_fwd_hdim512<T>(Flash_fwd_params &, cudaStream_t) [with T=cutlass::bfloat16_t]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_hdim512_bf16_sm80.cu(8): here

/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_kernel.h(276): warning: variable "tQrQ" was declared but never referenced
          detected during:
            instantiation of "void flash::compute_attn_1rowblock<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params,Is_Q_split_K>(const Params &, int, int, int) [with Kernel_traits=Flash_fwd_kernel_traits<512, 128, 32, 8, false, true, cutlass::bfloat16_t, Flash_kernel_traits<512, 128, 32, 8, cutlass::bfloat16_t>>, Is_dropout=true, Is_causal=true, Is_even_N=false, Is_even_K=false, Return_softmax=false, Params=Flash_fwd_params, Is_Q_split_K=false]" 
(589): here
            instantiation of "void flash::compute_attn<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &) [with Kernel_traits=Flash_fwd_kernel_traits<512, 128, 32, 8, false, true, cutlass::bfloat16_t, Flash_kernel_traits<512, 128, 32, 8, cutlass::bfloat16_t>>, Is_dropout=true, Is_causal=true, Is_even_N=false, Is_even_K=false, Return_softmax=false, Params=Flash_fwd_params]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(15): here
            instantiation of "void flash_fwd_kernel<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax>(Flash_fwd_params) [with Kernel_traits=Flash_fwd_kernel_traits<512, 128, 32, 8, false, true, cutlass::bfloat16_t, Flash_kernel_traits<512, 128, 32, 8, cutlass::bfloat16_t>>, Is_dropout=true, Is_causal=true, Is_even_N=false, Is_even_K=false, Return_softmax=false]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(34): here
            instantiation of "void run_flash_fwd<Kernel_traits,Is_dropout,Is_causal>(Flash_fwd_params &, cudaStream_t) [with Kernel_traits=Flash_fwd_kernel_traits<512, 128, 32, 8, false, true, cutlass::bfloat16_t, Flash_kernel_traits<512, 128, 32, 8, cutlass::bfloat16_t>>, Is_dropout=true, Is_causal=true]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(264): here
            instantiation of "void run_mha_fwd_hdim512<T>(Flash_fwd_params &, cudaStream_t) [with T=cutlass::bfloat16_t]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_hdim512_bf16_sm80.cu(8): here

/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_kernel.h(140): warning: variable "MMA_M" was declared but never referenced
          detected during:
            instantiation of "void flash::compute_attn_1rowblock<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params,Is_Q_split_K>(const Params &, int, int, int) [with Kernel_traits=Flash_fwd_kernel_traits<512, 32, 32, 2, false, false, cutlass::bfloat16_t, Flash_kernel_traits<512, 32, 32, 2, cutlass::bfloat16_t>>, Is_dropout=true, Is_causal=true, Is_even_N=true, Is_even_K=true, Return_softmax=true, Params=Flash_fwd_params, Is_Q_split_K=false]" 
(589): here
            instantiation of "void flash::compute_attn<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &) [with Kernel_traits=Flash_fwd_kernel_traits<512, 32, 32, 2, false, false, cutlass::bfloat16_t, Flash_kernel_traits<512, 32, 32, 2, cutlass::bfloat16_t>>, Is_dropout=true, Is_causal=true, Is_even_N=true, Is_even_K=true, Return_softmax=true, Params=Flash_fwd_params]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(15): here
            instantiation of "void flash_fwd_kernel<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax>(Flash_fwd_params) [with Kernel_traits=Flash_fwd_kernel_traits<512, 32, 32, 2, false, false, cutlass::bfloat16_t, Flash_kernel_traits<512, 32, 32, 2, cutlass::bfloat16_t>>, Is_dropout=true, Is_causal=true, Is_even_N=true, Is_even_K=true, Return_softmax=true]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(34): here
            instantiation of "void run_flash_fwd<Kernel_traits,Is_dropout,Is_causal>(Flash_fwd_params &, cudaStream_t) [with Kernel_traits=Flash_fwd_kernel_traits<512, 32, 32, 2, false, false, cutlass::bfloat16_t, Flash_kernel_traits<512, 32, 32, 2, cutlass::bfloat16_t>>, Is_dropout=true, Is_causal=true]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(264): here
            instantiation of "void run_mha_fwd_hdim512<T>(Flash_fwd_params &, cudaStream_t) [with T=cutlass::bfloat16_t]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_hdim512_bf16_sm80.cu(8): here

/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_kernel.h(276): warning: variable "tQrQ" was declared but never referenced
          detected during:
            instantiation of "void flash::compute_attn_1rowblock<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params,Is_Q_split_K>(const Params &, int, int, int) [with Kernel_traits=Flash_fwd_kernel_traits<512, 32, 32, 2, false, false, cutlass::bfloat16_t, Flash_kernel_traits<512, 32, 32, 2, cutlass::bfloat16_t>>, Is_dropout=true, Is_causal=true, Is_even_N=true, Is_even_K=true, Return_softmax=true, Params=Flash_fwd_params, Is_Q_split_K=false]" 
(589): here
            instantiation of "void flash::compute_attn<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &) [with Kernel_traits=Flash_fwd_kernel_traits<512, 32, 32, 2, false, false, cutlass::bfloat16_t, Flash_kernel_traits<512, 32, 32, 2, cutlass::bfloat16_t>>, Is_dropout=true, Is_causal=true, Is_even_N=true, Is_even_K=true, Return_softmax=true, Params=Flash_fwd_params]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(15): here
            instantiation of "void flash_fwd_kernel<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax>(Flash_fwd_params) [with Kernel_traits=Flash_fwd_kernel_traits<512, 32, 32, 2, false, false, cutlass::bfloat16_t, Flash_kernel_traits<512, 32, 32, 2, cutlass::bfloat16_t>>, Is_dropout=true, Is_causal=true, Is_even_N=true, Is_even_K=true, Return_softmax=true]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(34): here
            instantiation of "void run_flash_fwd<Kernel_traits,Is_dropout,Is_causal>(Flash_fwd_params &, cudaStream_t) [with Kernel_traits=Flash_fwd_kernel_traits<512, 32, 32, 2, false, false, cutlass::bfloat16_t, Flash_kernel_traits<512, 32, 32, 2, cutlass::bfloat16_t>>, Is_dropout=true, Is_causal=true]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(264): here
            instantiation of "void run_mha_fwd_hdim512<T>(Flash_fwd_params &, cudaStream_t) [with T=cutlass::bfloat16_t]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_hdim512_bf16_sm80.cu(8): here

/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_kernel.h(140): warning: variable "MMA_M" was declared but never referenced
          detected during:
            instantiation of "void flash::compute_attn_1rowblock<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params,Is_Q_split_K>(const Params &, int, int, int) [with Kernel_traits=Flash_fwd_kernel_traits<512, 32, 32, 2, false, false, cutlass::bfloat16_t, Flash_kernel_traits<512, 32, 32, 2, cutlass::bfloat16_t>>, Is_dropout=true, Is_causal=true, Is_even_N=true, Is_even_K=true, Return_softmax=false, Params=Flash_fwd_params, Is_Q_split_K=false]" 
(589): here
            instantiation of "void flash::compute_attn<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &) [with Kernel_traits=Flash_fwd_kernel_traits<512, 32, 32, 2, false, false, cutlass::bfloat16_t, Flash_kernel_traits<512, 32, 32, 2, cutlass::bfloat16_t>>, Is_dropout=true, Is_causal=true, Is_even_N=true, Is_even_K=true, Return_softmax=false, Params=Flash_fwd_params]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(15): here
            instantiation of "void flash_fwd_kernel<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax>(Flash_fwd_params) [with Kernel_traits=Flash_fwd_kernel_traits<512, 32, 32, 2, false, false, cutlass::bfloat16_t, Flash_kernel_traits<512, 32, 32, 2, cutlass::bfloat16_t>>, Is_dropout=true, Is_causal=true, Is_even_N=true, Is_even_K=true, Return_softmax=false]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(34): here
            instantiation of "void run_flash_fwd<Kernel_traits,Is_dropout,Is_causal>(Flash_fwd_params &, cudaStream_t) [with Kernel_traits=Flash_fwd_kernel_traits<512, 32, 32, 2, false, false, cutlass::bfloat16_t, Flash_kernel_traits<512, 32, 32, 2, cutlass::bfloat16_t>>, Is_dropout=true, Is_causal=true]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(264): here
            instantiation of "void run_mha_fwd_hdim512<T>(Flash_fwd_params &, cudaStream_t) [with T=cutlass::bfloat16_t]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_hdim512_bf16_sm80.cu(8): here

/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_kernel.h(276): warning: variable "tQrQ" was declared but never referenced
          detected during:
            instantiation of "void flash::compute_attn_1rowblock<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params,Is_Q_split_K>(const Params &, int, int, int) [with Kernel_traits=Flash_fwd_kernel_traits<512, 32, 32, 2, false, false, cutlass::bfloat16_t, Flash_kernel_traits<512, 32, 32, 2, cutlass::bfloat16_t>>, Is_dropout=true, Is_causal=true, Is_even_N=true, Is_even_K=true, Return_softmax=false, Params=Flash_fwd_params, Is_Q_split_K=false]" 
(589): here
            instantiation of "void flash::compute_attn<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &) [with Kernel_traits=Flash_fwd_kernel_traits<512, 32, 32, 2, false, false, cutlass::bfloat16_t, Flash_kernel_traits<512, 32, 32, 2, cutlass::bfloat16_t>>, Is_dropout=true, Is_causal=true, Is_even_N=true, Is_even_K=true, Return_softmax=false, Params=Flash_fwd_params]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(15): here
            instantiation of "void flash_fwd_kernel<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax>(Flash_fwd_params) [with Kernel_traits=Flash_fwd_kernel_traits<512, 32, 32, 2, false, false, cutlass::bfloat16_t, Flash_kernel_traits<512, 32, 32, 2, cutlass::bfloat16_t>>, Is_dropout=true, Is_causal=true, Is_even_N=true, Is_even_K=true, Return_softmax=false]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(34): here
            instantiation of "void run_flash_fwd<Kernel_traits,Is_dropout,Is_causal>(Flash_fwd_params &, cudaStream_t) [with Kernel_traits=Flash_fwd_kernel_traits<512, 32, 32, 2, false, false, cutlass::bfloat16_t, Flash_kernel_traits<512, 32, 32, 2, cutlass::bfloat16_t>>, Is_dropout=true, Is_causal=true]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(264): here
            instantiation of "void run_mha_fwd_hdim512<T>(Flash_fwd_params &, cudaStream_t) [with T=cutlass::bfloat16_t]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_hdim512_bf16_sm80.cu(8): here

/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_kernel.h(140): warning: variable "MMA_M" was declared but never referenced
          detected during:
            instantiation of "void flash::compute_attn_1rowblock<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params,Is_Q_split_K>(const Params &, int, int, int) [with Kernel_traits=Flash_fwd_kernel_traits<512, 32, 32, 2, false, false, cutlass::bfloat16_t, Flash_kernel_traits<512, 32, 32, 2, cutlass::bfloat16_t>>, Is_dropout=true, Is_causal=true, Is_even_N=true, Is_even_K=false, Return_softmax=true, Params=Flash_fwd_params, Is_Q_split_K=false]" 
(589): here
            instantiation of "void flash::compute_attn<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &) [with Kernel_traits=Flash_fwd_kernel_traits<512, 32, 32, 2, false, false, cutlass::bfloat16_t, Flash_kernel_traits<512, 32, 32, 2, cutlass::bfloat16_t>>, Is_dropout=true, Is_causal=true, Is_even_N=true, Is_even_K=false, Return_softmax=true, Params=Flash_fwd_params]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(15): here
            instantiation of "void flash_fwd_kernel<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax>(Flash_fwd_params) [with Kernel_traits=Flash_fwd_kernel_traits<512, 32, 32, 2, false, false, cutlass::bfloat16_t, Flash_kernel_traits<512, 32, 32, 2, cutlass::bfloat16_t>>, Is_dropout=true, Is_causal=true, Is_even_N=true, Is_even_K=false, Return_softmax=true]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(34): here
            instantiation of "void run_flash_fwd<Kernel_traits,Is_dropout,Is_causal>(Flash_fwd_params &, cudaStream_t) [with Kernel_traits=Flash_fwd_kernel_traits<512, 32, 32, 2, false, false, cutlass::bfloat16_t, Flash_kernel_traits<512, 32, 32, 2, cutlass::bfloat16_t>>, Is_dropout=true, Is_causal=true]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(264): here
            instantiation of "void run_mha_fwd_hdim512<T>(Flash_fwd_params &, cudaStream_t) [with T=cutlass::bfloat16_t]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_hdim512_bf16_sm80.cu(8): here

/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_kernel.h(276): warning: variable "tQrQ" was declared but never referenced
          detected during:
            instantiation of "void flash::compute_attn_1rowblock<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params,Is_Q_split_K>(const Params &, int, int, int) [with Kernel_traits=Flash_fwd_kernel_traits<512, 32, 32, 2, false, false, cutlass::bfloat16_t, Flash_kernel_traits<512, 32, 32, 2, cutlass::bfloat16_t>>, Is_dropout=true, Is_causal=true, Is_even_N=true, Is_even_K=false, Return_softmax=true, Params=Flash_fwd_params, Is_Q_split_K=false]" 
(589): here
            instantiation of "void flash::compute_attn<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &) [with Kernel_traits=Flash_fwd_kernel_traits<512, 32, 32, 2, false, false, cutlass::bfloat16_t, Flash_kernel_traits<512, 32, 32, 2, cutlass::bfloat16_t>>, Is_dropout=true, Is_causal=true, Is_even_N=true, Is_even_K=false, Return_softmax=true, Params=Flash_fwd_params]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(15): here
            instantiation of "void flash_fwd_kernel<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax>(Flash_fwd_params) [with Kernel_traits=Flash_fwd_kernel_traits<512, 32, 32, 2, false, false, cutlass::bfloat16_t, Flash_kernel_traits<512, 32, 32, 2, cutlass::bfloat16_t>>, Is_dropout=true, Is_causal=true, Is_even_N=true, Is_even_K=false, Return_softmax=true]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(34): here
            instantiation of "void run_flash_fwd<Kernel_traits,Is_dropout,Is_causal>(Flash_fwd_params &, cudaStream_t) [with Kernel_traits=Flash_fwd_kernel_traits<512, 32, 32, 2, false, false, cutlass::bfloat16_t, Flash_kernel_traits<512, 32, 32, 2, cutlass::bfloat16_t>>, Is_dropout=true, Is_causal=true]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(264): here
            instantiation of "void run_mha_fwd_hdim512<T>(Flash_fwd_params &, cudaStream_t) [with T=cutlass::bfloat16_t]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_hdim512_bf16_sm80.cu(8): here

/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_kernel.h(140): warning: variable "MMA_M" was declared but never referenced
          detected during:
            instantiation of "void flash::compute_attn_1rowblock<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params,Is_Q_split_K>(const Params &, int, int, int) [with Kernel_traits=Flash_fwd_kernel_traits<512, 32, 32, 2, false, false, cutlass::bfloat16_t, Flash_kernel_traits<512, 32, 32, 2, cutlass::bfloat16_t>>, Is_dropout=true, Is_causal=true, Is_even_N=true, Is_even_K=false, Return_softmax=false, Params=Flash_fwd_params, Is_Q_split_K=false]" 
(589): here
            instantiation of "void flash::compute_attn<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &) [with Kernel_traits=Flash_fwd_kernel_traits<512, 32, 32, 2, false, false, cutlass::bfloat16_t, Flash_kernel_traits<512, 32, 32, 2, cutlass::bfloat16_t>>, Is_dropout=true, Is_causal=true, Is_even_N=true, Is_even_K=false, Return_softmax=false, Params=Flash_fwd_params]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(15): here
            instantiation of "void flash_fwd_kernel<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax>(Flash_fwd_params) [with Kernel_traits=Flash_fwd_kernel_traits<512, 32, 32, 2, false, false, cutlass::bfloat16_t, Flash_kernel_traits<512, 32, 32, 2, cutlass::bfloat16_t>>, Is_dropout=true, Is_causal=true, Is_even_N=true, Is_even_K=false, Return_softmax=false]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(34): here
            instantiation of "void run_flash_fwd<Kernel_traits,Is_dropout,Is_causal>(Flash_fwd_params &, cudaStream_t) [with Kernel_traits=Flash_fwd_kernel_traits<512, 32, 32, 2, false, false, cutlass::bfloat16_t, Flash_kernel_traits<512, 32, 32, 2, cutlass::bfloat16_t>>, Is_dropout=true, Is_causal=true]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(264): here
            instantiation of "void run_mha_fwd_hdim512<T>(Flash_fwd_params &, cudaStream_t) [with T=cutlass::bfloat16_t]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_hdim512_bf16_sm80.cu(8): here

/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_kernel.h(276): warning: variable "tQrQ" was declared but never referenced
          detected during:
            instantiation of "void flash::compute_attn_1rowblock<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params,Is_Q_split_K>(const Params &, int, int, int) [with Kernel_traits=Flash_fwd_kernel_traits<512, 32, 32, 2, false, false, cutlass::bfloat16_t, Flash_kernel_traits<512, 32, 32, 2, cutlass::bfloat16_t>>, Is_dropout=true, Is_causal=true, Is_even_N=true, Is_even_K=false, Return_softmax=false, Params=Flash_fwd_params, Is_Q_split_K=false]" 
(589): here
            instantiation of "void flash::compute_attn<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &) [with Kernel_traits=Flash_fwd_kernel_traits<512, 32, 32, 2, false, false, cutlass::bfloat16_t, Flash_kernel_traits<512, 32, 32, 2, cutlass::bfloat16_t>>, Is_dropout=true, Is_causal=true, Is_even_N=true, Is_even_K=false, Return_softmax=false, Params=Flash_fwd_params]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(15): here
            instantiation of "void flash_fwd_kernel<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax>(Flash_fwd_params) [with Kernel_traits=Flash_fwd_kernel_traits<512, 32, 32, 2, false, false, cutlass::bfloat16_t, Flash_kernel_traits<512, 32, 32, 2, cutlass::bfloat16_t>>, Is_dropout=true, Is_causal=true, Is_even_N=true, Is_even_K=false, Return_softmax=false]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(34): here
            instantiation of "void run_flash_fwd<Kernel_traits,Is_dropout,Is_causal>(Flash_fwd_params &, cudaStream_t) [with Kernel_traits=Flash_fwd_kernel_traits<512, 32, 32, 2, false, false, cutlass::bfloat16_t, Flash_kernel_traits<512, 32, 32, 2, cutlass::bfloat16_t>>, Is_dropout=true, Is_causal=true]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(264): here
            instantiation of "void run_mha_fwd_hdim512<T>(Flash_fwd_params &, cudaStream_t) [with T=cutlass::bfloat16_t]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_hdim512_bf16_sm80.cu(8): here

/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_kernel.h(140): warning: variable "MMA_M" was declared but never referenced
          detected during:
            instantiation of "void flash::compute_attn_1rowblock<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params,Is_Q_split_K>(const Params &, int, int, int) [with Kernel_traits=Flash_fwd_kernel_traits<512, 32, 32, 2, false, false, cutlass::bfloat16_t, Flash_kernel_traits<512, 32, 32, 2, cutlass::bfloat16_t>>, Is_dropout=true, Is_causal=true, Is_even_N=false, Is_even_K=true, Return_softmax=true, Params=Flash_fwd_params, Is_Q_split_K=false]" 
(589): here
            instantiation of "void flash::compute_attn<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &) [with Kernel_traits=Flash_fwd_kernel_traits<512, 32, 32, 2, false, false, cutlass::bfloat16_t, Flash_kernel_traits<512, 32, 32, 2, cutlass::bfloat16_t>>, Is_dropout=true, Is_causal=true, Is_even_N=false, Is_even_K=true, Return_softmax=true, Params=Flash_fwd_params]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(15): here
            instantiation of "void flash_fwd_kernel<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax>(Flash_fwd_params) [with Kernel_traits=Flash_fwd_kernel_traits<512, 32, 32, 2, false, false, cutlass::bfloat16_t, Flash_kernel_traits<512, 32, 32, 2, cutlass::bfloat16_t>>, Is_dropout=true, Is_causal=true, Is_even_N=false, Is_even_K=true, Return_softmax=true]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(34): here
            instantiation of "void run_flash_fwd<Kernel_traits,Is_dropout,Is_causal>(Flash_fwd_params &, cudaStream_t) [with Kernel_traits=Flash_fwd_kernel_traits<512, 32, 32, 2, false, false, cutlass::bfloat16_t, Flash_kernel_traits<512, 32, 32, 2, cutlass::bfloat16_t>>, Is_dropout=true, Is_causal=true]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(264): here
            instantiation of "void run_mha_fwd_hdim512<T>(Flash_fwd_params &, cudaStream_t) [with T=cutlass::bfloat16_t]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_hdim512_bf16_sm80.cu(8): here

/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_kernel.h(276): warning: variable "tQrQ" was declared but never referenced
          detected during:
            instantiation of "void flash::compute_attn_1rowblock<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params,Is_Q_split_K>(const Params &, int, int, int) [with Kernel_traits=Flash_fwd_kernel_traits<512, 32, 32, 2, false, false, cutlass::bfloat16_t, Flash_kernel_traits<512, 32, 32, 2, cutlass::bfloat16_t>>, Is_dropout=true, Is_causal=true, Is_even_N=false, Is_even_K=true, Return_softmax=true, Params=Flash_fwd_params, Is_Q_split_K=false]" 
(589): here
            instantiation of "void flash::compute_attn<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &) [with Kernel_traits=Flash_fwd_kernel_traits<512, 32, 32, 2, false, false, cutlass::bfloat16_t, Flash_kernel_traits<512, 32, 32, 2, cutlass::bfloat16_t>>, Is_dropout=true, Is_causal=true, Is_even_N=false, Is_even_K=true, Return_softmax=true, Params=Flash_fwd_params]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(15): here
            instantiation of "void flash_fwd_kernel<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax>(Flash_fwd_params) [with Kernel_traits=Flash_fwd_kernel_traits<512, 32, 32, 2, false, false, cutlass::bfloat16_t, Flash_kernel_traits<512, 32, 32, 2, cutlass::bfloat16_t>>, Is_dropout=true, Is_causal=true, Is_even_N=false, Is_even_K=true, Return_softmax=true]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(34): here
            instantiation of "void run_flash_fwd<Kernel_traits,Is_dropout,Is_causal>(Flash_fwd_params &, cudaStream_t) [with Kernel_traits=Flash_fwd_kernel_traits<512, 32, 32, 2, false, false, cutlass::bfloat16_t, Flash_kernel_traits<512, 32, 32, 2, cutlass::bfloat16_t>>, Is_dropout=true, Is_causal=true]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(264): here
            instantiation of "void run_mha_fwd_hdim512<T>(Flash_fwd_params &, cudaStream_t) [with T=cutlass::bfloat16_t]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_hdim512_bf16_sm80.cu(8): here

/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_kernel.h(140): warning: variable "MMA_M" was declared but never referenced
          detected during:
            instantiation of "void flash::compute_attn_1rowblock<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params,Is_Q_split_K>(const Params &, int, int, int) [with Kernel_traits=Flash_fwd_kernel_traits<512, 32, 32, 2, false, false, cutlass::bfloat16_t, Flash_kernel_traits<512, 32, 32, 2, cutlass::bfloat16_t>>, Is_dropout=true, Is_causal=true, Is_even_N=false, Is_even_K=true, Return_softmax=false, Params=Flash_fwd_params, Is_Q_split_K=false]" 
(589): here
            instantiation of "void flash::compute_attn<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &) [with Kernel_traits=Flash_fwd_kernel_traits<512, 32, 32, 2, false, false, cutlass::bfloat16_t, Flash_kernel_traits<512, 32, 32, 2, cutlass::bfloat16_t>>, Is_dropout=true, Is_causal=true, Is_even_N=false, Is_even_K=true, Return_softmax=false, Params=Flash_fwd_params]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(15): here
            instantiation of "void flash_fwd_kernel<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax>(Flash_fwd_params) [with Kernel_traits=Flash_fwd_kernel_traits<512, 32, 32, 2, false, false, cutlass::bfloat16_t, Flash_kernel_traits<512, 32, 32, 2, cutlass::bfloat16_t>>, Is_dropout=true, Is_causal=true, Is_even_N=false, Is_even_K=true, Return_softmax=false]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(34): here
            instantiation of "void run_flash_fwd<Kernel_traits,Is_dropout,Is_causal>(Flash_fwd_params &, cudaStream_t) [with Kernel_traits=Flash_fwd_kernel_traits<512, 32, 32, 2, false, false, cutlass::bfloat16_t, Flash_kernel_traits<512, 32, 32, 2, cutlass::bfloat16_t>>, Is_dropout=true, Is_causal=true]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(264): here
            instantiation of "void run_mha_fwd_hdim512<T>(Flash_fwd_params &, cudaStream_t) [with T=cutlass::bfloat16_t]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_hdim512_bf16_sm80.cu(8): here

/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_kernel.h(276): warning: variable "tQrQ" was declared but never referenced
          detected during:
            instantiation of "void flash::compute_attn_1rowblock<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params,Is_Q_split_K>(const Params &, int, int, int) [with Kernel_traits=Flash_fwd_kernel_traits<512, 32, 32, 2, false, false, cutlass::bfloat16_t, Flash_kernel_traits<512, 32, 32, 2, cutlass::bfloat16_t>>, Is_dropout=true, Is_causal=true, Is_even_N=false, Is_even_K=true, Return_softmax=false, Params=Flash_fwd_params, Is_Q_split_K=false]" 
(589): here
            instantiation of "void flash::compute_attn<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &) [with Kernel_traits=Flash_fwd_kernel_traits<512, 32, 32, 2, false, false, cutlass::bfloat16_t, Flash_kernel_traits<512, 32, 32, 2, cutlass::bfloat16_t>>, Is_dropout=true, Is_causal=true, Is_even_N=false, Is_even_K=true, Return_softmax=false, Params=Flash_fwd_params]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(15): here
            instantiation of "void flash_fwd_kernel<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax>(Flash_fwd_params) [with Kernel_traits=Flash_fwd_kernel_traits<512, 32, 32, 2, false, false, cutlass::bfloat16_t, Flash_kernel_traits<512, 32, 32, 2, cutlass::bfloat16_t>>, Is_dropout=true, Is_causal=true, Is_even_N=false, Is_even_K=true, Return_softmax=false]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(34): here
            instantiation of "void run_flash_fwd<Kernel_traits,Is_dropout,Is_causal>(Flash_fwd_params &, cudaStream_t) [with Kernel_traits=Flash_fwd_kernel_traits<512, 32, 32, 2, false, false, cutlass::bfloat16_t, Flash_kernel_traits<512, 32, 32, 2, cutlass::bfloat16_t>>, Is_dropout=true, Is_causal=true]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(264): here
            instantiation of "void run_mha_fwd_hdim512<T>(Flash_fwd_params &, cudaStream_t) [with T=cutlass::bfloat16_t]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_hdim512_bf16_sm80.cu(8): here

/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_kernel.h(140): warning: variable "MMA_M" was declared but never referenced
          detected during:
            instantiation of "void flash::compute_attn_1rowblock<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params,Is_Q_split_K>(const Params &, int, int, int) [with Kernel_traits=Flash_fwd_kernel_traits<512, 32, 32, 2, false, false, cutlass::bfloat16_t, Flash_kernel_traits<512, 32, 32, 2, cutlass::bfloat16_t>>, Is_dropout=true, Is_causal=true, Is_even_N=false, Is_even_K=false, Return_softmax=true, Params=Flash_fwd_params, Is_Q_split_K=false]" 
(589): here
            instantiation of "void flash::compute_attn<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &) [with Kernel_traits=Flash_fwd_kernel_traits<512, 32, 32, 2, false, false, cutlass::bfloat16_t, Flash_kernel_traits<512, 32, 32, 2, cutlass::bfloat16_t>>, Is_dropout=true, Is_causal=true, Is_even_N=false, Is_even_K=false, Return_softmax=true, Params=Flash_fwd_params]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(15): here
            instantiation of "void flash_fwd_kernel<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax>(Flash_fwd_params) [with Kernel_traits=Flash_fwd_kernel_traits<512, 32, 32, 2, false, false, cutlass::bfloat16_t, Flash_kernel_traits<512, 32, 32, 2, cutlass::bfloat16_t>>, Is_dropout=true, Is_causal=true, Is_even_N=false, Is_even_K=false, Return_softmax=true]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(34): here
            instantiation of "void run_flash_fwd<Kernel_traits,Is_dropout,Is_causal>(Flash_fwd_params &, cudaStream_t) [with Kernel_traits=Flash_fwd_kernel_traits<512, 32, 32, 2, false, false, cutlass::bfloat16_t, Flash_kernel_traits<512, 32, 32, 2, cutlass::bfloat16_t>>, Is_dropout=true, Is_causal=true]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(264): here
            instantiation of "void run_mha_fwd_hdim512<T>(Flash_fwd_params &, cudaStream_t) [with T=cutlass::bfloat16_t]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_hdim512_bf16_sm80.cu(8): here

/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_kernel.h(276): warning: variable "tQrQ" was declared but never referenced
          detected during:
            instantiation of "void flash::compute_attn_1rowblock<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params,Is_Q_split_K>(const Params &, int, int, int) [with Kernel_traits=Flash_fwd_kernel_traits<512, 32, 32, 2, false, false, cutlass::bfloat16_t, Flash_kernel_traits<512, 32, 32, 2, cutlass::bfloat16_t>>, Is_dropout=true, Is_causal=true, Is_even_N=false, Is_even_K=false, Return_softmax=true, Params=Flash_fwd_params, Is_Q_split_K=false]" 
(589): here
            instantiation of "void flash::compute_attn<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &) [with Kernel_traits=Flash_fwd_kernel_traits<512, 32, 32, 2, false, false, cutlass::bfloat16_t, Flash_kernel_traits<512, 32, 32, 2, cutlass::bfloat16_t>>, Is_dropout=true, Is_causal=true, Is_even_N=false, Is_even_K=false, Return_softmax=true, Params=Flash_fwd_params]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(15): here
            instantiation of "void flash_fwd_kernel<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax>(Flash_fwd_params) [with Kernel_traits=Flash_fwd_kernel_traits<512, 32, 32, 2, false, false, cutlass::bfloat16_t, Flash_kernel_traits<512, 32, 32, 2, cutlass::bfloat16_t>>, Is_dropout=true, Is_causal=true, Is_even_N=false, Is_even_K=false, Return_softmax=true]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(34): here
            instantiation of "void run_flash_fwd<Kernel_traits,Is_dropout,Is_causal>(Flash_fwd_params &, cudaStream_t) [with Kernel_traits=Flash_fwd_kernel_traits<512, 32, 32, 2, false, false, cutlass::bfloat16_t, Flash_kernel_traits<512, 32, 32, 2, cutlass::bfloat16_t>>, Is_dropout=true, Is_causal=true]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(264): here
            instantiation of "void run_mha_fwd_hdim512<T>(Flash_fwd_params &, cudaStream_t) [with T=cutlass::bfloat16_t]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_hdim512_bf16_sm80.cu(8): here

/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_kernel.h(140): warning: variable "MMA_M" was declared but never referenced
          detected during:
            instantiation of "void flash::compute_attn_1rowblock<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params,Is_Q_split_K>(const Params &, int, int, int) [with Kernel_traits=Flash_fwd_kernel_traits<512, 32, 32, 2, false, false, cutlass::bfloat16_t, Flash_kernel_traits<512, 32, 32, 2, cutlass::bfloat16_t>>, Is_dropout=true, Is_causal=true, Is_even_N=false, Is_even_K=false, Return_softmax=false, Params=Flash_fwd_params, Is_Q_split_K=false]" 
(589): here
            instantiation of "void flash::compute_attn<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &) [with Kernel_traits=Flash_fwd_kernel_traits<512, 32, 32, 2, false, false, cutlass::bfloat16_t, Flash_kernel_traits<512, 32, 32, 2, cutlass::bfloat16_t>>, Is_dropout=true, Is_causal=true, Is_even_N=false, Is_even_K=false, Return_softmax=false, Params=Flash_fwd_params]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(15): here
            instantiation of "void flash_fwd_kernel<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax>(Flash_fwd_params) [with Kernel_traits=Flash_fwd_kernel_traits<512, 32, 32, 2, false, false, cutlass::bfloat16_t, Flash_kernel_traits<512, 32, 32, 2, cutlass::bfloat16_t>>, Is_dropout=true, Is_causal=true, Is_even_N=false, Is_even_K=false, Return_softmax=false]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(34): here
            instantiation of "void run_flash_fwd<Kernel_traits,Is_dropout,Is_causal>(Flash_fwd_params &, cudaStream_t) [with Kernel_traits=Flash_fwd_kernel_traits<512, 32, 32, 2, false, false, cutlass::bfloat16_t, Flash_kernel_traits<512, 32, 32, 2, cutlass::bfloat16_t>>, Is_dropout=true, Is_causal=true]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(264): here
            instantiation of "void run_mha_fwd_hdim512<T>(Flash_fwd_params &, cudaStream_t) [with T=cutlass::bfloat16_t]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_hdim512_bf16_sm80.cu(8): here

/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_kernel.h(276): warning: variable "tQrQ" was declared but never referenced
          detected during:
            instantiation of "void flash::compute_attn_1rowblock<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params,Is_Q_split_K>(const Params &, int, int, int) [with Kernel_traits=Flash_fwd_kernel_traits<512, 32, 32, 2, false, false, cutlass::bfloat16_t, Flash_kernel_traits<512, 32, 32, 2, cutlass::bfloat16_t>>, Is_dropout=true, Is_causal=true, Is_even_N=false, Is_even_K=false, Return_softmax=false, Params=Flash_fwd_params, Is_Q_split_K=false]" 
(589): here
            instantiation of "void flash::compute_attn<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &) [with Kernel_traits=Flash_fwd_kernel_traits<512, 32, 32, 2, false, false, cutlass::bfloat16_t, Flash_kernel_traits<512, 32, 32, 2, cutlass::bfloat16_t>>, Is_dropout=true, Is_causal=true, Is_even_N=false, Is_even_K=false, Return_softmax=false, Params=Flash_fwd_params]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(15): here
            instantiation of "void flash_fwd_kernel<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax>(Flash_fwd_params) [with Kernel_traits=Flash_fwd_kernel_traits<512, 32, 32, 2, false, false, cutlass::bfloat16_t, Flash_kernel_traits<512, 32, 32, 2, cutlass::bfloat16_t>>, Is_dropout=true, Is_causal=true, Is_even_N=false, Is_even_K=false, Return_softmax=false]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(34): here
            instantiation of "void run_flash_fwd<Kernel_traits,Is_dropout,Is_causal>(Flash_fwd_params &, cudaStream_t) [with Kernel_traits=Flash_fwd_kernel_traits<512, 32, 32, 2, false, false, cutlass::bfloat16_t, Flash_kernel_traits<512, 32, 32, 2, cutlass::bfloat16_t>>, Is_dropout=true, Is_causal=true]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(264): here
            instantiation of "void run_mha_fwd_hdim512<T>(Flash_fwd_params &, cudaStream_t) [with T=cutlass::bfloat16_t]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_hdim512_bf16_sm80.cu(8): here

/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_kernel.h(140): warning: variable "MMA_M" was declared but never referenced
          detected during:
            instantiation of "void flash::compute_attn_1rowblock<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params,Is_Q_split_K>(const Params &, int, int, int) [with Kernel_traits=Flash_fwd_kernel_traits<512, 128, 32, 8, false, true, cutlass::bfloat16_t, Flash_kernel_traits<512, 128, 32, 8, cutlass::bfloat16_t>>, Is_dropout=true, Is_causal=false, Is_even_N=true, Is_even_K=true, Return_softmax=true, Params=Flash_fwd_params, Is_Q_split_K=false]" 
(589): here
            instantiation of "void flash::compute_attn<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &) [with Kernel_traits=Flash_fwd_kernel_traits<512, 128, 32, 8, false, true, cutlass::bfloat16_t, Flash_kernel_traits<512, 128, 32, 8, cutlass::bfloat16_t>>, Is_dropout=true, Is_causal=false, Is_even_N=true, Is_even_K=true, Return_softmax=true, Params=Flash_fwd_params]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(15): here
            instantiation of "void flash_fwd_kernel<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax>(Flash_fwd_params) [with Kernel_traits=Flash_fwd_kernel_traits<512, 128, 32, 8, false, true, cutlass::bfloat16_t, Flash_kernel_traits<512, 128, 32, 8, cutlass::bfloat16_t>>, Is_dropout=true, Is_causal=false, Is_even_N=true, Is_even_K=true, Return_softmax=true]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(34): here
            instantiation of "void run_flash_fwd<Kernel_traits,Is_dropout,Is_causal>(Flash_fwd_params &, cudaStream_t) [with Kernel_traits=Flash_fwd_kernel_traits<512, 128, 32, 8, false, true, cutlass::bfloat16_t, Flash_kernel_traits<512, 128, 32, 8, cutlass::bfloat16_t>>, Is_dropout=true, Is_causal=false]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(264): here
            instantiation of "void run_mha_fwd_hdim512<T>(Flash_fwd_params &, cudaStream_t) [with T=cutlass::bfloat16_t]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_hdim512_bf16_sm80.cu(8): here

/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_kernel.h(276): warning: variable "tQrQ" was declared but never referenced
          detected during:
            instantiation of "void flash::compute_attn_1rowblock<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params,Is_Q_split_K>(const Params &, int, int, int) [with Kernel_traits=Flash_fwd_kernel_traits<512, 128, 32, 8, false, true, cutlass::bfloat16_t, Flash_kernel_traits<512, 128, 32, 8, cutlass::bfloat16_t>>, Is_dropout=true, Is_causal=false, Is_even_N=true, Is_even_K=true, Return_softmax=true, Params=Flash_fwd_params, Is_Q_split_K=false]" 
(589): here
            instantiation of "void flash::compute_attn<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &) [with Kernel_traits=Flash_fwd_kernel_traits<512, 128, 32, 8, false, true, cutlass::bfloat16_t, Flash_kernel_traits<512, 128, 32, 8, cutlass::bfloat16_t>>, Is_dropout=true, Is_causal=false, Is_even_N=true, Is_even_K=true, Return_softmax=true, Params=Flash_fwd_params]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(15): here
            instantiation of "void flash_fwd_kernel<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax>(Flash_fwd_params) [with Kernel_traits=Flash_fwd_kernel_traits<512, 128, 32, 8, false, true, cutlass::bfloat16_t, Flash_kernel_traits<512, 128, 32, 8, cutlass::bfloat16_t>>, Is_dropout=true, Is_causal=false, Is_even_N=true, Is_even_K=true, Return_softmax=true]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(34): here
            instantiation of "void run_flash_fwd<Kernel_traits,Is_dropout,Is_causal>(Flash_fwd_params &, cudaStream_t) [with Kernel_traits=Flash_fwd_kernel_traits<512, 128, 32, 8, false, true, cutlass::bfloat16_t, Flash_kernel_traits<512, 128, 32, 8, cutlass::bfloat16_t>>, Is_dropout=true, Is_causal=false]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(264): here
            instantiation of "void run_mha_fwd_hdim512<T>(Flash_fwd_params &, cudaStream_t) [with T=cutlass::bfloat16_t]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_hdim512_bf16_sm80.cu(8): here

/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_kernel.h(140): warning: variable "MMA_M" was declared but never referenced
          detected during:
            instantiation of "void flash::compute_attn_1rowblock<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params,Is_Q_split_K>(const Params &, int, int, int) [with Kernel_traits=Flash_fwd_kernel_traits<512, 128, 32, 8, false, true, cutlass::bfloat16_t, Flash_kernel_traits<512, 128, 32, 8, cutlass::bfloat16_t>>, Is_dropout=true, Is_causal=false, Is_even_N=true, Is_even_K=true, Return_softmax=false, Params=Flash_fwd_params, Is_Q_split_K=false]" 
(589): here
            instantiation of "void flash::compute_attn<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &) [with Kernel_traits=Flash_fwd_kernel_traits<512, 128, 32, 8, false, true, cutlass::bfloat16_t, Flash_kernel_traits<512, 128, 32, 8, cutlass::bfloat16_t>>, Is_dropout=true, Is_causal=false, Is_even_N=true, Is_even_K=true, Return_softmax=false, Params=Flash_fwd_params]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(15): here
            instantiation of "void flash_fwd_kernel<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax>(Flash_fwd_params) [with Kernel_traits=Flash_fwd_kernel_traits<512, 128, 32, 8, false, true, cutlass::bfloat16_t, Flash_kernel_traits<512, 128, 32, 8, cutlass::bfloat16_t>>, Is_dropout=true, Is_causal=false, Is_even_N=true, Is_even_K=true, Return_softmax=false]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(34): here
            instantiation of "void run_flash_fwd<Kernel_traits,Is_dropout,Is_causal>(Flash_fwd_params &, cudaStream_t) [with Kernel_traits=Flash_fwd_kernel_traits<512, 128, 32, 8, false, true, cutlass::bfloat16_t, Flash_kernel_traits<512, 128, 32, 8, cutlass::bfloat16_t>>, Is_dropout=true, Is_causal=false]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(264): here
            instantiation of "void run_mha_fwd_hdim512<T>(Flash_fwd_params &, cudaStream_t) [with T=cutlass::bfloat16_t]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_hdim512_bf16_sm80.cu(8): here

/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_kernel.h(276): warning: variable "tQrQ" was declared but never referenced
          detected during:
            instantiation of "void flash::compute_attn_1rowblock<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params,Is_Q_split_K>(const Params &, int, int, int) [with Kernel_traits=Flash_fwd_kernel_traits<512, 128, 32, 8, false, true, cutlass::bfloat16_t, Flash_kernel_traits<512, 128, 32, 8, cutlass::bfloat16_t>>, Is_dropout=true, Is_causal=false, Is_even_N=true, Is_even_K=true, Return_softmax=false, Params=Flash_fwd_params, Is_Q_split_K=false]" 
(589): here
            instantiation of "void flash::compute_attn<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &) [with Kernel_traits=Flash_fwd_kernel_traits<512, 128, 32, 8, false, true, cutlass::bfloat16_t, Flash_kernel_traits<512, 128, 32, 8, cutlass::bfloat16_t>>, Is_dropout=true, Is_causal=false, Is_even_N=true, Is_even_K=true, Return_softmax=false, Params=Flash_fwd_params]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(15): here
            instantiation of "void flash_fwd_kernel<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax>(Flash_fwd_params) [with Kernel_traits=Flash_fwd_kernel_traits<512, 128, 32, 8, false, true, cutlass::bfloat16_t, Flash_kernel_traits<512, 128, 32, 8, cutlass::bfloat16_t>>, Is_dropout=true, Is_causal=false, Is_even_N=true, Is_even_K=true, Return_softmax=false]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(34): here
            instantiation of "void run_flash_fwd<Kernel_traits,Is_dropout,Is_causal>(Flash_fwd_params &, cudaStream_t) [with Kernel_traits=Flash_fwd_kernel_traits<512, 128, 32, 8, false, true, cutlass::bfloat16_t, Flash_kernel_traits<512, 128, 32, 8, cutlass::bfloat16_t>>, Is_dropout=true, Is_causal=false]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(264): here
            instantiation of "void run_mha_fwd_hdim512<T>(Flash_fwd_params &, cudaStream_t) [with T=cutlass::bfloat16_t]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_hdim512_bf16_sm80.cu(8): here

/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_kernel.h(140): warning: variable "MMA_M" was declared but never referenced
          detected during:
            instantiation of "void flash::compute_attn_1rowblock<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params,Is_Q_split_K>(const Params &, int, int, int) [with Kernel_traits=Flash_fwd_kernel_traits<512, 128, 32, 8, false, true, cutlass::bfloat16_t, Flash_kernel_traits<512, 128, 32, 8, cutlass::bfloat16_t>>, Is_dropout=true, Is_causal=false, Is_even_N=true, Is_even_K=false, Return_softmax=true, Params=Flash_fwd_params, Is_Q_split_K=false]" 
(589): here
            instantiation of "void flash::compute_attn<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &) [with Kernel_traits=Flash_fwd_kernel_traits<512, 128, 32, 8, false, true, cutlass::bfloat16_t, Flash_kernel_traits<512, 128, 32, 8, cutlass::bfloat16_t>>, Is_dropout=true, Is_causal=false, Is_even_N=true, Is_even_K=false, Return_softmax=true, Params=Flash_fwd_params]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(15): here
            instantiation of "void flash_fwd_kernel<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax>(Flash_fwd_params) [with Kernel_traits=Flash_fwd_kernel_traits<512, 128, 32, 8, false, true, cutlass::bfloat16_t, Flash_kernel_traits<512, 128, 32, 8, cutlass::bfloat16_t>>, Is_dropout=true, Is_causal=false, Is_even_N=true, Is_even_K=false, Return_softmax=true]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(34): here
            instantiation of "void run_flash_fwd<Kernel_traits,Is_dropout,Is_causal>(Flash_fwd_params &, cudaStream_t) [with Kernel_traits=Flash_fwd_kernel_traits<512, 128, 32, 8, false, true, cutlass::bfloat16_t, Flash_kernel_traits<512, 128, 32, 8, cutlass::bfloat16_t>>, Is_dropout=true, Is_causal=false]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(264): here
            instantiation of "void run_mha_fwd_hdim512<T>(Flash_fwd_params &, cudaStream_t) [with T=cutlass::bfloat16_t]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_hdim512_bf16_sm80.cu(8): here

/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_kernel.h(276): warning: variable "tQrQ" was declared but never referenced
          detected during:
            instantiation of "void flash::compute_attn_1rowblock<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params,Is_Q_split_K>(const Params &, int, int, int) [with Kernel_traits=Flash_fwd_kernel_traits<512, 128, 32, 8, false, true, cutlass::bfloat16_t, Flash_kernel_traits<512, 128, 32, 8, cutlass::bfloat16_t>>, Is_dropout=true, Is_causal=false, Is_even_N=true, Is_even_K=false, Return_softmax=true, Params=Flash_fwd_params, Is_Q_split_K=false]" 
(589): here
            instantiation of "void flash::compute_attn<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &) [with Kernel_traits=Flash_fwd_kernel_traits<512, 128, 32, 8, false, true, cutlass::bfloat16_t, Flash_kernel_traits<512, 128, 32, 8, cutlass::bfloat16_t>>, Is_dropout=true, Is_causal=false, Is_even_N=true, Is_even_K=false, Return_softmax=true, Params=Flash_fwd_params]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(15): here
            instantiation of "void flash_fwd_kernel<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax>(Flash_fwd_params) [with Kernel_traits=Flash_fwd_kernel_traits<512, 128, 32, 8, false, true, cutlass::bfloat16_t, Flash_kernel_traits<512, 128, 32, 8, cutlass::bfloat16_t>>, Is_dropout=true, Is_causal=false, Is_even_N=true, Is_even_K=false, Return_softmax=true]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(34): here
            instantiation of "void run_flash_fwd<Kernel_traits,Is_dropout,Is_causal>(Flash_fwd_params &, cudaStream_t) [with Kernel_traits=Flash_fwd_kernel_traits<512, 128, 32, 8, false, true, cutlass::bfloat16_t, Flash_kernel_traits<512, 128, 32, 8, cutlass::bfloat16_t>>, Is_dropout=true, Is_causal=false]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(264): here
            instantiation of "void run_mha_fwd_hdim512<T>(Flash_fwd_params &, cudaStream_t) [with T=cutlass::bfloat16_t]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_hdim512_bf16_sm80.cu(8): here

/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_kernel.h(140): warning: variable "MMA_M" was declared but never referenced
          detected during:
            instantiation of "void flash::compute_attn_1rowblock<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params,Is_Q_split_K>(const Params &, int, int, int) [with Kernel_traits=Flash_fwd_kernel_traits<512, 128, 32, 8, false, true, cutlass::bfloat16_t, Flash_kernel_traits<512, 128, 32, 8, cutlass::bfloat16_t>>, Is_dropout=true, Is_causal=false, Is_even_N=true, Is_even_K=false, Return_softmax=false, Params=Flash_fwd_params, Is_Q_split_K=false]" 
(589): here
            instantiation of "void flash::compute_attn<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &) [with Kernel_traits=Flash_fwd_kernel_traits<512, 128, 32, 8, false, true, cutlass::bfloat16_t, Flash_kernel_traits<512, 128, 32, 8, cutlass::bfloat16_t>>, Is_dropout=true, Is_causal=false, Is_even_N=true, Is_even_K=false, Return_softmax=false, Params=Flash_fwd_params]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(15): here
            instantiation of "void flash_fwd_kernel<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax>(Flash_fwd_params) [with Kernel_traits=Flash_fwd_kernel_traits<512, 128, 32, 8, false, true, cutlass::bfloat16_t, Flash_kernel_traits<512, 128, 32, 8, cutlass::bfloat16_t>>, Is_dropout=true, Is_causal=false, Is_even_N=true, Is_even_K=false, Return_softmax=false]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(34): here
            instantiation of "void run_flash_fwd<Kernel_traits,Is_dropout,Is_causal>(Flash_fwd_params &, cudaStream_t) [with Kernel_traits=Flash_fwd_kernel_traits<512, 128, 32, 8, false, true, cutlass::bfloat16_t, Flash_kernel_traits<512, 128, 32, 8, cutlass::bfloat16_t>>, Is_dropout=true, Is_causal=false]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(264): here
            instantiation of "void run_mha_fwd_hdim512<T>(Flash_fwd_params &, cudaStream_t) [with T=cutlass::bfloat16_t]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_hdim512_bf16_sm80.cu(8): here

/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_kernel.h(276): warning: variable "tQrQ" was declared but never referenced
          detected during:
            instantiation of "void flash::compute_attn_1rowblock<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params,Is_Q_split_K>(const Params &, int, int, int) [with Kernel_traits=Flash_fwd_kernel_traits<512, 128, 32, 8, false, true, cutlass::bfloat16_t, Flash_kernel_traits<512, 128, 32, 8, cutlass::bfloat16_t>>, Is_dropout=true, Is_causal=false, Is_even_N=true, Is_even_K=false, Return_softmax=false, Params=Flash_fwd_params, Is_Q_split_K=false]" 
(589): here
            instantiation of "void flash::compute_attn<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &) [with Kernel_traits=Flash_fwd_kernel_traits<512, 128, 32, 8, false, true, cutlass::bfloat16_t, Flash_kernel_traits<512, 128, 32, 8, cutlass::bfloat16_t>>, Is_dropout=true, Is_causal=false, Is_even_N=true, Is_even_K=false, Return_softmax=false, Params=Flash_fwd_params]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(15): here
            instantiation of "void flash_fwd_kernel<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax>(Flash_fwd_params) [with Kernel_traits=Flash_fwd_kernel_traits<512, 128, 32, 8, false, true, cutlass::bfloat16_t, Flash_kernel_traits<512, 128, 32, 8, cutlass::bfloat16_t>>, Is_dropout=true, Is_causal=false, Is_even_N=true, Is_even_K=false, Return_softmax=false]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(34): here
            instantiation of "void run_flash_fwd<Kernel_traits,Is_dropout,Is_causal>(Flash_fwd_params &, cudaStream_t) [with Kernel_traits=Flash_fwd_kernel_traits<512, 128, 32, 8, false, true, cutlass::bfloat16_t, Flash_kernel_traits<512, 128, 32, 8, cutlass::bfloat16_t>>, Is_dropout=true, Is_causal=false]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(264): here
            instantiation of "void run_mha_fwd_hdim512<T>(Flash_fwd_params &, cudaStream_t) [with T=cutlass::bfloat16_t]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_hdim512_bf16_sm80.cu(8): here

/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_kernel.h(140): warning: variable "MMA_M" was declared but never referenced
          detected during:
            instantiation of "void flash::compute_attn_1rowblock<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params,Is_Q_split_K>(const Params &, int, int, int) [with Kernel_traits=Flash_fwd_kernel_traits<512, 128, 32, 8, false, true, cutlass::bfloat16_t, Flash_kernel_traits<512, 128, 32, 8, cutlass::bfloat16_t>>, Is_dropout=true, Is_causal=false, Is_even_N=false, Is_even_K=true, Return_softmax=true, Params=Flash_fwd_params, Is_Q_split_K=false]" 
(589): here
            instantiation of "void flash::compute_attn<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &) [with Kernel_traits=Flash_fwd_kernel_traits<512, 128, 32, 8, false, true, cutlass::bfloat16_t, Flash_kernel_traits<512, 128, 32, 8, cutlass::bfloat16_t>>, Is_dropout=true, Is_causal=false, Is_even_N=false, Is_even_K=true, Return_softmax=true, Params=Flash_fwd_params]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(15): here
            instantiation of "void flash_fwd_kernel<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax>(Flash_fwd_params) [with Kernel_traits=Flash_fwd_kernel_traits<512, 128, 32, 8, false, true, cutlass::bfloat16_t, Flash_kernel_traits<512, 128, 32, 8, cutlass::bfloat16_t>>, Is_dropout=true, Is_causal=false, Is_even_N=false, Is_even_K=true, Return_softmax=true]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(34): here
            instantiation of "void run_flash_fwd<Kernel_traits,Is_dropout,Is_causal>(Flash_fwd_params &, cudaStream_t) [with Kernel_traits=Flash_fwd_kernel_traits<512, 128, 32, 8, false, true, cutlass::bfloat16_t, Flash_kernel_traits<512, 128, 32, 8, cutlass::bfloat16_t>>, Is_dropout=true, Is_causal=false]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(264): here
            instantiation of "void run_mha_fwd_hdim512<T>(Flash_fwd_params &, cudaStream_t) [with T=cutlass::bfloat16_t]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_hdim512_bf16_sm80.cu(8): here

/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_kernel.h(276): warning: variable "tQrQ" was declared but never referenced
          detected during:
            instantiation of "void flash::compute_attn_1rowblock<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params,Is_Q_split_K>(const Params &, int, int, int) [with Kernel_traits=Flash_fwd_kernel_traits<512, 128, 32, 8, false, true, cutlass::bfloat16_t, Flash_kernel_traits<512, 128, 32, 8, cutlass::bfloat16_t>>, Is_dropout=true, Is_causal=false, Is_even_N=false, Is_even_K=true, Return_softmax=true, Params=Flash_fwd_params, Is_Q_split_K=false]" 
(589): here
            instantiation of "void flash::compute_attn<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &) [with Kernel_traits=Flash_fwd_kernel_traits<512, 128, 32, 8, false, true, cutlass::bfloat16_t, Flash_kernel_traits<512, 128, 32, 8, cutlass::bfloat16_t>>, Is_dropout=true, Is_causal=false, Is_even_N=false, Is_even_K=true, Return_softmax=true, Params=Flash_fwd_params]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(15): here
            instantiation of "void flash_fwd_kernel<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax>(Flash_fwd_params) [with Kernel_traits=Flash_fwd_kernel_traits<512, 128, 32, 8, false, true, cutlass::bfloat16_t, Flash_kernel_traits<512, 128, 32, 8, cutlass::bfloat16_t>>, Is_dropout=true, Is_causal=false, Is_even_N=false, Is_even_K=true, Return_softmax=true]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(34): here
            instantiation of "void run_flash_fwd<Kernel_traits,Is_dropout,Is_causal>(Flash_fwd_params &, cudaStream_t) [with Kernel_traits=Flash_fwd_kernel_traits<512, 128, 32, 8, false, true, cutlass::bfloat16_t, Flash_kernel_traits<512, 128, 32, 8, cutlass::bfloat16_t>>, Is_dropout=true, Is_causal=false]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(264): here
            instantiation of "void run_mha_fwd_hdim512<T>(Flash_fwd_params &, cudaStream_t) [with T=cutlass::bfloat16_t]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_hdim512_bf16_sm80.cu(8): here

/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_kernel.h(140): warning: variable "MMA_M" was declared but never referenced
          detected during:
            instantiation of "void flash::compute_attn_1rowblock<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params,Is_Q_split_K>(const Params &, int, int, int) [with Kernel_traits=Flash_fwd_kernel_traits<512, 128, 32, 8, false, true, cutlass::bfloat16_t, Flash_kernel_traits<512, 128, 32, 8, cutlass::bfloat16_t>>, Is_dropout=true, Is_causal=false, Is_even_N=false, Is_even_K=true, Return_softmax=false, Params=Flash_fwd_params, Is_Q_split_K=false]" 
(589): here
            instantiation of "void flash::compute_attn<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &) [with Kernel_traits=Flash_fwd_kernel_traits<512, 128, 32, 8, false, true, cutlass::bfloat16_t, Flash_kernel_traits<512, 128, 32, 8, cutlass::bfloat16_t>>, Is_dropout=true, Is_causal=false, Is_even_N=false, Is_even_K=true, Return_softmax=false, Params=Flash_fwd_params]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(15): here
            instantiation of "void flash_fwd_kernel<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax>(Flash_fwd_params) [with Kernel_traits=Flash_fwd_kernel_traits<512, 128, 32, 8, false, true, cutlass::bfloat16_t, Flash_kernel_traits<512, 128, 32, 8, cutlass::bfloat16_t>>, Is_dropout=true, Is_causal=false, Is_even_N=false, Is_even_K=true, Return_softmax=false]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(34): here
            instantiation of "void run_flash_fwd<Kernel_traits,Is_dropout,Is_causal>(Flash_fwd_params &, cudaStream_t) [with Kernel_traits=Flash_fwd_kernel_traits<512, 128, 32, 8, false, true, cutlass::bfloat16_t, Flash_kernel_traits<512, 128, 32, 8, cutlass::bfloat16_t>>, Is_dropout=true, Is_causal=false]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(264): here
            instantiation of "void run_mha_fwd_hdim512<T>(Flash_fwd_params &, cudaStream_t) [with T=cutlass::bfloat16_t]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_hdim512_bf16_sm80.cu(8): here

/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_kernel.h(276): warning: variable "tQrQ" was declared but never referenced
          detected during:
            instantiation of "void flash::compute_attn_1rowblock<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params,Is_Q_split_K>(const Params &, int, int, int) [with Kernel_traits=Flash_fwd_kernel_traits<512, 128, 32, 8, false, true, cutlass::bfloat16_t, Flash_kernel_traits<512, 128, 32, 8, cutlass::bfloat16_t>>, Is_dropout=true, Is_causal=false, Is_even_N=false, Is_even_K=true, Return_softmax=false, Params=Flash_fwd_params, Is_Q_split_K=false]" 
(589): here
            instantiation of "void flash::compute_attn<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &) [with Kernel_traits=Flash_fwd_kernel_traits<512, 128, 32, 8, false, true, cutlass::bfloat16_t, Flash_kernel_traits<512, 128, 32, 8, cutlass::bfloat16_t>>, Is_dropout=true, Is_causal=false, Is_even_N=false, Is_even_K=true, Return_softmax=false, Params=Flash_fwd_params]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(15): here
            instantiation of "void flash_fwd_kernel<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax>(Flash_fwd_params) [with Kernel_traits=Flash_fwd_kernel_traits<512, 128, 32, 8, false, true, cutlass::bfloat16_t, Flash_kernel_traits<512, 128, 32, 8, cutlass::bfloat16_t>>, Is_dropout=true, Is_causal=false, Is_even_N=false, Is_even_K=true, Return_softmax=false]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(34): here
            instantiation of "void run_flash_fwd<Kernel_traits,Is_dropout,Is_causal>(Flash_fwd_params &, cudaStream_t) [with Kernel_traits=Flash_fwd_kernel_traits<512, 128, 32, 8, false, true, cutlass::bfloat16_t, Flash_kernel_traits<512, 128, 32, 8, cutlass::bfloat16_t>>, Is_dropout=true, Is_causal=false]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(264): here
            instantiation of "void run_mha_fwd_hdim512<T>(Flash_fwd_params &, cudaStream_t) [with T=cutlass::bfloat16_t]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_hdim512_bf16_sm80.cu(8): here

/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_kernel.h(140): warning: variable "MMA_M" was declared but never referenced
          detected during:
            instantiation of "void flash::compute_attn_1rowblock<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params,Is_Q_split_K>(const Params &, int, int, int) [with Kernel_traits=Flash_fwd_kernel_traits<512, 128, 32, 8, false, true, cutlass::bfloat16_t, Flash_kernel_traits<512, 128, 32, 8, cutlass::bfloat16_t>>, Is_dropout=true, Is_causal=false, Is_even_N=false, Is_even_K=false, Return_softmax=true, Params=Flash_fwd_params, Is_Q_split_K=false]" 
(589): here
            instantiation of "void flash::compute_attn<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &) [with Kernel_traits=Flash_fwd_kernel_traits<512, 128, 32, 8, false, true, cutlass::bfloat16_t, Flash_kernel_traits<512, 128, 32, 8, cutlass::bfloat16_t>>, Is_dropout=true, Is_causal=false, Is_even_N=false, Is_even_K=false, Return_softmax=true, Params=Flash_fwd_params]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(15): here
            instantiation of "void flash_fwd_kernel<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax>(Flash_fwd_params) [with Kernel_traits=Flash_fwd_kernel_traits<512, 128, 32, 8, false, true, cutlass::bfloat16_t, Flash_kernel_traits<512, 128, 32, 8, cutlass::bfloat16_t>>, Is_dropout=true, Is_causal=false, Is_even_N=false, Is_even_K=false, Return_softmax=true]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(34): here
            instantiation of "void run_flash_fwd<Kernel_traits,Is_dropout,Is_causal>(Flash_fwd_params &, cudaStream_t) [with Kernel_traits=Flash_fwd_kernel_traits<512, 128, 32, 8, false, true, cutlass::bfloat16_t, Flash_kernel_traits<512, 128, 32, 8, cutlass::bfloat16_t>>, Is_dropout=true, Is_causal=false]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(264): here
            instantiation of "void run_mha_fwd_hdim512<T>(Flash_fwd_params &, cudaStream_t) [with T=cutlass::bfloat16_t]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_hdim512_bf16_sm80.cu(8): here

/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_kernel.h(276): warning: variable "tQrQ" was declared but never referenced
          detected during:
            instantiation of "void flash::compute_attn_1rowblock<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params,Is_Q_split_K>(const Params &, int, int, int) [with Kernel_traits=Flash_fwd_kernel_traits<512, 128, 32, 8, false, true, cutlass::bfloat16_t, Flash_kernel_traits<512, 128, 32, 8, cutlass::bfloat16_t>>, Is_dropout=true, Is_causal=false, Is_even_N=false, Is_even_K=false, Return_softmax=true, Params=Flash_fwd_params, Is_Q_split_K=false]" 
(589): here
            instantiation of "void flash::compute_attn<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &) [with Kernel_traits=Flash_fwd_kernel_traits<512, 128, 32, 8, false, true, cutlass::bfloat16_t, Flash_kernel_traits<512, 128, 32, 8, cutlass::bfloat16_t>>, Is_dropout=true, Is_causal=false, Is_even_N=false, Is_even_K=false, Return_softmax=true, Params=Flash_fwd_params]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(15): here
            instantiation of "void flash_fwd_kernel<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax>(Flash_fwd_params) [with Kernel_traits=Flash_fwd_kernel_traits<512, 128, 32, 8, false, true, cutlass::bfloat16_t, Flash_kernel_traits<512, 128, 32, 8, cutlass::bfloat16_t>>, Is_dropout=true, Is_causal=false, Is_even_N=false, Is_even_K=false, Return_softmax=true]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(34): here
            instantiation of "void run_flash_fwd<Kernel_traits,Is_dropout,Is_causal>(Flash_fwd_params &, cudaStream_t) [with Kernel_traits=Flash_fwd_kernel_traits<512, 128, 32, 8, false, true, cutlass::bfloat16_t, Flash_kernel_traits<512, 128, 32, 8, cutlass::bfloat16_t>>, Is_dropout=true, Is_causal=false]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(264): here
            instantiation of "void run_mha_fwd_hdim512<T>(Flash_fwd_params &, cudaStream_t) [with T=cutlass::bfloat16_t]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_hdim512_bf16_sm80.cu(8): here

/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_kernel.h(140): warning: variable "MMA_M" was declared but never referenced
          detected during:
            instantiation of "void flash::compute_attn_1rowblock<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params,Is_Q_split_K>(const Params &, int, int, int) [with Kernel_traits=Flash_fwd_kernel_traits<512, 128, 32, 8, false, true, cutlass::bfloat16_t, Flash_kernel_traits<512, 128, 32, 8, cutlass::bfloat16_t>>, Is_dropout=true, Is_causal=false, Is_even_N=false, Is_even_K=false, Return_softmax=false, Params=Flash_fwd_params, Is_Q_split_K=false]" 
(589): here
            instantiation of "void flash::compute_attn<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &) [with Kernel_traits=Flash_fwd_kernel_traits<512, 128, 32, 8, false, true, cutlass::bfloat16_t, Flash_kernel_traits<512, 128, 32, 8, cutlass::bfloat16_t>>, Is_dropout=true, Is_causal=false, Is_even_N=false, Is_even_K=false, Return_softmax=false, Params=Flash_fwd_params]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(15): here
            instantiation of "void flash_fwd_kernel<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax>(Flash_fwd_params) [with Kernel_traits=Flash_fwd_kernel_traits<512, 128, 32, 8, false, true, cutlass::bfloat16_t, Flash_kernel_traits<512, 128, 32, 8, cutlass::bfloat16_t>>, Is_dropout=true, Is_causal=false, Is_even_N=false, Is_even_K=false, Return_softmax=false]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(34): here
            instantiation of "void run_flash_fwd<Kernel_traits,Is_dropout,Is_causal>(Flash_fwd_params &, cudaStream_t) [with Kernel_traits=Flash_fwd_kernel_traits<512, 128, 32, 8, false, true, cutlass::bfloat16_t, Flash_kernel_traits<512, 128, 32, 8, cutlass::bfloat16_t>>, Is_dropout=true, Is_causal=false]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(264): here
            instantiation of "void run_mha_fwd_hdim512<T>(Flash_fwd_params &, cudaStream_t) [with T=cutlass::bfloat16_t]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_hdim512_bf16_sm80.cu(8): here

/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_kernel.h(276): warning: variable "tQrQ" was declared but never referenced
          detected during:
            instantiation of "void flash::compute_attn_1rowblock<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params,Is_Q_split_K>(const Params &, int, int, int) [with Kernel_traits=Flash_fwd_kernel_traits<512, 128, 32, 8, false, true, cutlass::bfloat16_t, Flash_kernel_traits<512, 128, 32, 8, cutlass::bfloat16_t>>, Is_dropout=true, Is_causal=false, Is_even_N=false, Is_even_K=false, Return_softmax=false, Params=Flash_fwd_params, Is_Q_split_K=false]" 
(589): here
            instantiation of "void flash::compute_attn<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &) [with Kernel_traits=Flash_fwd_kernel_traits<512, 128, 32, 8, false, true, cutlass::bfloat16_t, Flash_kernel_traits<512, 128, 32, 8, cutlass::bfloat16_t>>, Is_dropout=true, Is_causal=false, Is_even_N=false, Is_even_K=false, Return_softmax=false, Params=Flash_fwd_params]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(15): here
            instantiation of "void flash_fwd_kernel<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax>(Flash_fwd_params) [with Kernel_traits=Flash_fwd_kernel_traits<512, 128, 32, 8, false, true, cutlass::bfloat16_t, Flash_kernel_traits<512, 128, 32, 8, cutlass::bfloat16_t>>, Is_dropout=true, Is_causal=false, Is_even_N=false, Is_even_K=false, Return_softmax=false]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(34): here
            instantiation of "void run_flash_fwd<Kernel_traits,Is_dropout,Is_causal>(Flash_fwd_params &, cudaStream_t) [with Kernel_traits=Flash_fwd_kernel_traits<512, 128, 32, 8, false, true, cutlass::bfloat16_t, Flash_kernel_traits<512, 128, 32, 8, cutlass::bfloat16_t>>, Is_dropout=true, Is_causal=false]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(264): here
            instantiation of "void run_mha_fwd_hdim512<T>(Flash_fwd_params &, cudaStream_t) [with T=cutlass::bfloat16_t]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_hdim512_bf16_sm80.cu(8): here

/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_kernel.h(140): warning: variable "MMA_M" was declared but never referenced
          detected during:
            instantiation of "void flash::compute_attn_1rowblock<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params,Is_Q_split_K>(const Params &, int, int, int) [with Kernel_traits=Flash_fwd_kernel_traits<512, 32, 32, 2, false, false, cutlass::bfloat16_t, Flash_kernel_traits<512, 32, 32, 2, cutlass::bfloat16_t>>, Is_dropout=true, Is_causal=false, Is_even_N=true, Is_even_K=true, Return_softmax=true, Params=Flash_fwd_params, Is_Q_split_K=false]" 
(589): here
            instantiation of "void flash::compute_attn<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &) [with Kernel_traits=Flash_fwd_kernel_traits<512, 32, 32, 2, false, false, cutlass::bfloat16_t, Flash_kernel_traits<512, 32, 32, 2, cutlass::bfloat16_t>>, Is_dropout=true, Is_causal=false, Is_even_N=true, Is_even_K=true, Return_softmax=true, Params=Flash_fwd_params]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(15): here
            instantiation of "void flash_fwd_kernel<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax>(Flash_fwd_params) [with Kernel_traits=Flash_fwd_kernel_traits<512, 32, 32, 2, false, false, cutlass::bfloat16_t, Flash_kernel_traits<512, 32, 32, 2, cutlass::bfloat16_t>>, Is_dropout=true, Is_causal=false, Is_even_N=true, Is_even_K=true, Return_softmax=true]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(34): here
            instantiation of "void run_flash_fwd<Kernel_traits,Is_dropout,Is_causal>(Flash_fwd_params &, cudaStream_t) [with Kernel_traits=Flash_fwd_kernel_traits<512, 32, 32, 2, false, false, cutlass::bfloat16_t, Flash_kernel_traits<512, 32, 32, 2, cutlass::bfloat16_t>>, Is_dropout=true, Is_causal=false]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(264): here
            instantiation of "void run_mha_fwd_hdim512<T>(Flash_fwd_params &, cudaStream_t) [with T=cutlass::bfloat16_t]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_hdim512_bf16_sm80.cu(8): here

/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_kernel.h(276): warning: variable "tQrQ" was declared but never referenced
          detected during:
            instantiation of "void flash::compute_attn_1rowblock<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params,Is_Q_split_K>(const Params &, int, int, int) [with Kernel_traits=Flash_fwd_kernel_traits<512, 32, 32, 2, false, false, cutlass::bfloat16_t, Flash_kernel_traits<512, 32, 32, 2, cutlass::bfloat16_t>>, Is_dropout=true, Is_causal=false, Is_even_N=true, Is_even_K=true, Return_softmax=true, Params=Flash_fwd_params, Is_Q_split_K=false]" 
(589): here
            instantiation of "void flash::compute_attn<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &) [with Kernel_traits=Flash_fwd_kernel_traits<512, 32, 32, 2, false, false, cutlass::bfloat16_t, Flash_kernel_traits<512, 32, 32, 2, cutlass::bfloat16_t>>, Is_dropout=true, Is_causal=false, Is_even_N=true, Is_even_K=true, Return_softmax=true, Params=Flash_fwd_params]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(15): here
            instantiation of "void flash_fwd_kernel<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax>(Flash_fwd_params) [with Kernel_traits=Flash_fwd_kernel_traits<512, 32, 32, 2, false, false, cutlass::bfloat16_t, Flash_kernel_traits<512, 32, 32, 2, cutlass::bfloat16_t>>, Is_dropout=true, Is_causal=false, Is_even_N=true, Is_even_K=true, Return_softmax=true]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(34): here
            instantiation of "void run_flash_fwd<Kernel_traits,Is_dropout,Is_causal>(Flash_fwd_params &, cudaStream_t) [with Kernel_traits=Flash_fwd_kernel_traits<512, 32, 32, 2, false, false, cutlass::bfloat16_t, Flash_kernel_traits<512, 32, 32, 2, cutlass::bfloat16_t>>, Is_dropout=true, Is_causal=false]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(264): here
            instantiation of "void run_mha_fwd_hdim512<T>(Flash_fwd_params &, cudaStream_t) [with T=cutlass::bfloat16_t]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_hdim512_bf16_sm80.cu(8): here

/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_kernel.h(140): warning: variable "MMA_M" was declared but never referenced
          detected during:
            instantiation of "void flash::compute_attn_1rowblock<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params,Is_Q_split_K>(const Params &, int, int, int) [with Kernel_traits=Flash_fwd_kernel_traits<512, 32, 32, 2, false, false, cutlass::bfloat16_t, Flash_kernel_traits<512, 32, 32, 2, cutlass::bfloat16_t>>, Is_dropout=true, Is_causal=false, Is_even_N=true, Is_even_K=true, Return_softmax=false, Params=Flash_fwd_params, Is_Q_split_K=false]" 
(589): here
            instantiation of "void flash::compute_attn<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &) [with Kernel_traits=Flash_fwd_kernel_traits<512, 32, 32, 2, false, false, cutlass::bfloat16_t, Flash_kernel_traits<512, 32, 32, 2, cutlass::bfloat16_t>>, Is_dropout=true, Is_causal=false, Is_even_N=true, Is_even_K=true, Return_softmax=false, Params=Flash_fwd_params]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(15): here
            instantiation of "void flash_fwd_kernel<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax>(Flash_fwd_params) [with Kernel_traits=Flash_fwd_kernel_traits<512, 32, 32, 2, false, false, cutlass::bfloat16_t, Flash_kernel_traits<512, 32, 32, 2, cutlass::bfloat16_t>>, Is_dropout=true, Is_causal=false, Is_even_N=true, Is_even_K=true, Return_softmax=false]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(34): here
            instantiation of "void run_flash_fwd<Kernel_traits,Is_dropout,Is_causal>(Flash_fwd_params &, cudaStream_t) [with Kernel_traits=Flash_fwd_kernel_traits<512, 32, 32, 2, false, false, cutlass::bfloat16_t, Flash_kernel_traits<512, 32, 32, 2, cutlass::bfloat16_t>>, Is_dropout=true, Is_causal=false]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(264): here
            instantiation of "void run_mha_fwd_hdim512<T>(Flash_fwd_params &, cudaStream_t) [with T=cutlass::bfloat16_t]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_hdim512_bf16_sm80.cu(8): here

/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_kernel.h(276): warning: variable "tQrQ" was declared but never referenced
          detected during:
            instantiation of "void flash::compute_attn_1rowblock<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params,Is_Q_split_K>(const Params &, int, int, int) [with Kernel_traits=Flash_fwd_kernel_traits<512, 32, 32, 2, false, false, cutlass::bfloat16_t, Flash_kernel_traits<512, 32, 32, 2, cutlass::bfloat16_t>>, Is_dropout=true, Is_causal=false, Is_even_N=true, Is_even_K=true, Return_softmax=false, Params=Flash_fwd_params, Is_Q_split_K=false]" 
(589): here
            instantiation of "void flash::compute_attn<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &) [with Kernel_traits=Flash_fwd_kernel_traits<512, 32, 32, 2, false, false, cutlass::bfloat16_t, Flash_kernel_traits<512, 32, 32, 2, cutlass::bfloat16_t>>, Is_dropout=true, Is_causal=false, Is_even_N=true, Is_even_K=true, Return_softmax=false, Params=Flash_fwd_params]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(15): here
            instantiation of "void flash_fwd_kernel<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax>(Flash_fwd_params) [with Kernel_traits=Flash_fwd_kernel_traits<512, 32, 32, 2, false, false, cutlass::bfloat16_t, Flash_kernel_traits<512, 32, 32, 2, cutlass::bfloat16_t>>, Is_dropout=true, Is_causal=false, Is_even_N=true, Is_even_K=true, Return_softmax=false]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(34): here
            instantiation of "void run_flash_fwd<Kernel_traits,Is_dropout,Is_causal>(Flash_fwd_params &, cudaStream_t) [with Kernel_traits=Flash_fwd_kernel_traits<512, 32, 32, 2, false, false, cutlass::bfloat16_t, Flash_kernel_traits<512, 32, 32, 2, cutlass::bfloat16_t>>, Is_dropout=true, Is_causal=false]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(264): here
            instantiation of "void run_mha_fwd_hdim512<T>(Flash_fwd_params &, cudaStream_t) [with T=cutlass::bfloat16_t]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_hdim512_bf16_sm80.cu(8): here

/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_kernel.h(140): warning: variable "MMA_M" was declared but never referenced
          detected during:
            instantiation of "void flash::compute_attn_1rowblock<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params,Is_Q_split_K>(const Params &, int, int, int) [with Kernel_traits=Flash_fwd_kernel_traits<512, 32, 32, 2, false, false, cutlass::bfloat16_t, Flash_kernel_traits<512, 32, 32, 2, cutlass::bfloat16_t>>, Is_dropout=true, Is_causal=false, Is_even_N=true, Is_even_K=false, Return_softmax=true, Params=Flash_fwd_params, Is_Q_split_K=false]" 
(589): here
            instantiation of "void flash::compute_attn<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &) [with Kernel_traits=Flash_fwd_kernel_traits<512, 32, 32, 2, false, false, cutlass::bfloat16_t, Flash_kernel_traits<512, 32, 32, 2, cutlass::bfloat16_t>>, Is_dropout=true, Is_causal=false, Is_even_N=true, Is_even_K=false, Return_softmax=true, Params=Flash_fwd_params]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(15): here
            instantiation of "void flash_fwd_kernel<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax>(Flash_fwd_params) [with Kernel_traits=Flash_fwd_kernel_traits<512, 32, 32, 2, false, false, cutlass::bfloat16_t, Flash_kernel_traits<512, 32, 32, 2, cutlass::bfloat16_t>>, Is_dropout=true, Is_causal=false, Is_even_N=true, Is_even_K=false, Return_softmax=true]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(34): here
            instantiation of "void run_flash_fwd<Kernel_traits,Is_dropout,Is_causal>(Flash_fwd_params &, cudaStream_t) [with Kernel_traits=Flash_fwd_kernel_traits<512, 32, 32, 2, false, false, cutlass::bfloat16_t, Flash_kernel_traits<512, 32, 32, 2, cutlass::bfloat16_t>>, Is_dropout=true, Is_causal=false]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(264): here
            instantiation of "void run_mha_fwd_hdim512<T>(Flash_fwd_params &, cudaStream_t) [with T=cutlass::bfloat16_t]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_hdim512_bf16_sm80.cu(8): here

/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_kernel.h(276): warning: variable "tQrQ" was declared but never referenced
          detected during:
            instantiation of "void flash::compute_attn_1rowblock<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params,Is_Q_split_K>(const Params &, int, int, int) [with Kernel_traits=Flash_fwd_kernel_traits<512, 32, 32, 2, false, false, cutlass::bfloat16_t, Flash_kernel_traits<512, 32, 32, 2, cutlass::bfloat16_t>>, Is_dropout=true, Is_causal=false, Is_even_N=true, Is_even_K=false, Return_softmax=true, Params=Flash_fwd_params, Is_Q_split_K=false]" 
(589): here
            instantiation of "void flash::compute_attn<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &) [with Kernel_traits=Flash_fwd_kernel_traits<512, 32, 32, 2, false, false, cutlass::bfloat16_t, Flash_kernel_traits<512, 32, 32, 2, cutlass::bfloat16_t>>, Is_dropout=true, Is_causal=false, Is_even_N=true, Is_even_K=false, Return_softmax=true, Params=Flash_fwd_params]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(15): here
            instantiation of "void flash_fwd_kernel<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax>(Flash_fwd_params) [with Kernel_traits=Flash_fwd_kernel_traits<512, 32, 32, 2, false, false, cutlass::bfloat16_t, Flash_kernel_traits<512, 32, 32, 2, cutlass::bfloat16_t>>, Is_dropout=true, Is_causal=false, Is_even_N=true, Is_even_K=false, Return_softmax=true]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(34): here
            instantiation of "void run_flash_fwd<Kernel_traits,Is_dropout,Is_causal>(Flash_fwd_params &, cudaStream_t) [with Kernel_traits=Flash_fwd_kernel_traits<512, 32, 32, 2, false, false, cutlass::bfloat16_t, Flash_kernel_traits<512, 32, 32, 2, cutlass::bfloat16_t>>, Is_dropout=true, Is_causal=false]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(264): here
            instantiation of "void run_mha_fwd_hdim512<T>(Flash_fwd_params &, cudaStream_t) [with T=cutlass::bfloat16_t]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_hdim512_bf16_sm80.cu(8): here

/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_kernel.h(140): warning: variable "MMA_M" was declared but never referenced
          detected during:
            instantiation of "void flash::compute_attn_1rowblock<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params,Is_Q_split_K>(const Params &, int, int, int) [with Kernel_traits=Flash_fwd_kernel_traits<512, 32, 32, 2, false, false, cutlass::bfloat16_t, Flash_kernel_traits<512, 32, 32, 2, cutlass::bfloat16_t>>, Is_dropout=true, Is_causal=false, Is_even_N=true, Is_even_K=false, Return_softmax=false, Params=Flash_fwd_params, Is_Q_split_K=false]" 
(589): here
            instantiation of "void flash::compute_attn<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &) [with Kernel_traits=Flash_fwd_kernel_traits<512, 32, 32, 2, false, false, cutlass::bfloat16_t, Flash_kernel_traits<512, 32, 32, 2, cutlass::bfloat16_t>>, Is_dropout=true, Is_causal=false, Is_even_N=true, Is_even_K=false, Return_softmax=false, Params=Flash_fwd_params]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(15): here
            instantiation of "void flash_fwd_kernel<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax>(Flash_fwd_params) [with Kernel_traits=Flash_fwd_kernel_traits<512, 32, 32, 2, false, false, cutlass::bfloat16_t, Flash_kernel_traits<512, 32, 32, 2, cutlass::bfloat16_t>>, Is_dropout=true, Is_causal=false, Is_even_N=true, Is_even_K=false, Return_softmax=false]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(34): here
            instantiation of "void run_flash_fwd<Kernel_traits,Is_dropout,Is_causal>(Flash_fwd_params &, cudaStream_t) [with Kernel_traits=Flash_fwd_kernel_traits<512, 32, 32, 2, false, false, cutlass::bfloat16_t, Flash_kernel_traits<512, 32, 32, 2, cutlass::bfloat16_t>>, Is_dropout=true, Is_causal=false]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(264): here
            instantiation of "void run_mha_fwd_hdim512<T>(Flash_fwd_params &, cudaStream_t) [with T=cutlass::bfloat16_t]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_hdim512_bf16_sm80.cu(8): here

/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_kernel.h(276): warning: variable "tQrQ" was declared but never referenced
          detected during:
            instantiation of "void flash::compute_attn_1rowblock<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params,Is_Q_split_K>(const Params &, int, int, int) [with Kernel_traits=Flash_fwd_kernel_traits<512, 32, 32, 2, false, false, cutlass::bfloat16_t, Flash_kernel_traits<512, 32, 32, 2, cutlass::bfloat16_t>>, Is_dropout=true, Is_causal=false, Is_even_N=true, Is_even_K=false, Return_softmax=false, Params=Flash_fwd_params, Is_Q_split_K=false]" 
(589): here
            instantiation of "void flash::compute_attn<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &) [with Kernel_traits=Flash_fwd_kernel_traits<512, 32, 32, 2, false, false, cutlass::bfloat16_t, Flash_kernel_traits<512, 32, 32, 2, cutlass::bfloat16_t>>, Is_dropout=true, Is_causal=false, Is_even_N=true, Is_even_K=false, Return_softmax=false, Params=Flash_fwd_params]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(15): here
            instantiation of "void flash_fwd_kernel<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax>(Flash_fwd_params) [with Kernel_traits=Flash_fwd_kernel_traits<512, 32, 32, 2, false, false, cutlass::bfloat16_t, Flash_kernel_traits<512, 32, 32, 2, cutlass::bfloat16_t>>, Is_dropout=true, Is_causal=false, Is_even_N=true, Is_even_K=false, Return_softmax=false]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(34): here
            instantiation of "void run_flash_fwd<Kernel_traits,Is_dropout,Is_causal>(Flash_fwd_params &, cudaStream_t) [with Kernel_traits=Flash_fwd_kernel_traits<512, 32, 32, 2, false, false, cutlass::bfloat16_t, Flash_kernel_traits<512, 32, 32, 2, cutlass::bfloat16_t>>, Is_dropout=true, Is_causal=false]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(264): here
            instantiation of "void run_mha_fwd_hdim512<T>(Flash_fwd_params &, cudaStream_t) [with T=cutlass::bfloat16_t]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_hdim512_bf16_sm80.cu(8): here

/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_kernel.h(140): warning: variable "MMA_M" was declared but never referenced
          detected during:
            instantiation of "void flash::compute_attn_1rowblock<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params,Is_Q_split_K>(const Params &, int, int, int) [with Kernel_traits=Flash_fwd_kernel_traits<512, 32, 32, 2, false, false, cutlass::bfloat16_t, Flash_kernel_traits<512, 32, 32, 2, cutlass::bfloat16_t>>, Is_dropout=true, Is_causal=false, Is_even_N=false, Is_even_K=true, Return_softmax=true, Params=Flash_fwd_params, Is_Q_split_K=false]" 
(589): here
            instantiation of "void flash::compute_attn<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &) [with Kernel_traits=Flash_fwd_kernel_traits<512, 32, 32, 2, false, false, cutlass::bfloat16_t, Flash_kernel_traits<512, 32, 32, 2, cutlass::bfloat16_t>>, Is_dropout=true, Is_causal=false, Is_even_N=false, Is_even_K=true, Return_softmax=true, Params=Flash_fwd_params]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(15): here
            instantiation of "void flash_fwd_kernel<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax>(Flash_fwd_params) [with Kernel_traits=Flash_fwd_kernel_traits<512, 32, 32, 2, false, false, cutlass::bfloat16_t, Flash_kernel_traits<512, 32, 32, 2, cutlass::bfloat16_t>>, Is_dropout=true, Is_causal=false, Is_even_N=false, Is_even_K=true, Return_softmax=true]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(34): here
            instantiation of "void run_flash_fwd<Kernel_traits,Is_dropout,Is_causal>(Flash_fwd_params &, cudaStream_t) [with Kernel_traits=Flash_fwd_kernel_traits<512, 32, 32, 2, false, false, cutlass::bfloat16_t, Flash_kernel_traits<512, 32, 32, 2, cutlass::bfloat16_t>>, Is_dropout=true, Is_causal=false]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(264): here
            instantiation of "void run_mha_fwd_hdim512<T>(Flash_fwd_params &, cudaStream_t) [with T=cutlass::bfloat16_t]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_hdim512_bf16_sm80.cu(8): here

/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_kernel.h(276): warning: variable "tQrQ" was declared but never referenced
          detected during:
            instantiation of "void flash::compute_attn_1rowblock<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params,Is_Q_split_K>(const Params &, int, int, int) [with Kernel_traits=Flash_fwd_kernel_traits<512, 32, 32, 2, false, false, cutlass::bfloat16_t, Flash_kernel_traits<512, 32, 32, 2, cutlass::bfloat16_t>>, Is_dropout=true, Is_causal=false, Is_even_N=false, Is_even_K=true, Return_softmax=true, Params=Flash_fwd_params, Is_Q_split_K=false]" 
(589): here
            instantiation of "void flash::compute_attn<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &) [with Kernel_traits=Flash_fwd_kernel_traits<512, 32, 32, 2, false, false, cutlass::bfloat16_t, Flash_kernel_traits<512, 32, 32, 2, cutlass::bfloat16_t>>, Is_dropout=true, Is_causal=false, Is_even_N=false, Is_even_K=true, Return_softmax=true, Params=Flash_fwd_params]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(15): here
            instantiation of "void flash_fwd_kernel<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax>(Flash_fwd_params) [with Kernel_traits=Flash_fwd_kernel_traits<512, 32, 32, 2, false, false, cutlass::bfloat16_t, Flash_kernel_traits<512, 32, 32, 2, cutlass::bfloat16_t>>, Is_dropout=true, Is_causal=false, Is_even_N=false, Is_even_K=true, Return_softmax=true]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(34): here
            instantiation of "void run_flash_fwd<Kernel_traits,Is_dropout,Is_causal>(Flash_fwd_params &, cudaStream_t) [with Kernel_traits=Flash_fwd_kernel_traits<512, 32, 32, 2, false, false, cutlass::bfloat16_t, Flash_kernel_traits<512, 32, 32, 2, cutlass::bfloat16_t>>, Is_dropout=true, Is_causal=false]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(264): here
            instantiation of "void run_mha_fwd_hdim512<T>(Flash_fwd_params &, cudaStream_t) [with T=cutlass::bfloat16_t]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_hdim512_bf16_sm80.cu(8): here

/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_kernel.h(140): warning: variable "MMA_M" was declared but never referenced
          detected during:
            instantiation of "void flash::compute_attn_1rowblock<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params,Is_Q_split_K>(const Params &, int, int, int) [with Kernel_traits=Flash_fwd_kernel_traits<512, 32, 32, 2, false, false, cutlass::bfloat16_t, Flash_kernel_traits<512, 32, 32, 2, cutlass::bfloat16_t>>, Is_dropout=true, Is_causal=false, Is_even_N=false, Is_even_K=true, Return_softmax=false, Params=Flash_fwd_params, Is_Q_split_K=false]" 
(589): here
            instantiation of "void flash::compute_attn<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &) [with Kernel_traits=Flash_fwd_kernel_traits<512, 32, 32, 2, false, false, cutlass::bfloat16_t, Flash_kernel_traits<512, 32, 32, 2, cutlass::bfloat16_t>>, Is_dropout=true, Is_causal=false, Is_even_N=false, Is_even_K=true, Return_softmax=false, Params=Flash_fwd_params]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(15): here
            instantiation of "void flash_fwd_kernel<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax>(Flash_fwd_params) [with Kernel_traits=Flash_fwd_kernel_traits<512, 32, 32, 2, false, false, cutlass::bfloat16_t, Flash_kernel_traits<512, 32, 32, 2, cutlass::bfloat16_t>>, Is_dropout=true, Is_causal=false, Is_even_N=false, Is_even_K=true, Return_softmax=false]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(34): here
            instantiation of "void run_flash_fwd<Kernel_traits,Is_dropout,Is_causal>(Flash_fwd_params &, cudaStream_t) [with Kernel_traits=Flash_fwd_kernel_traits<512, 32, 32, 2, false, false, cutlass::bfloat16_t, Flash_kernel_traits<512, 32, 32, 2, cutlass::bfloat16_t>>, Is_dropout=true, Is_causal=false]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(264): here
            instantiation of "void run_mha_fwd_hdim512<T>(Flash_fwd_params &, cudaStream_t) [with T=cutlass::bfloat16_t]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_hdim512_bf16_sm80.cu(8): here

/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_kernel.h(276): warning: variable "tQrQ" was declared but never referenced
          detected during:
            instantiation of "void flash::compute_attn_1rowblock<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params,Is_Q_split_K>(const Params &, int, int, int) [with Kernel_traits=Flash_fwd_kernel_traits<512, 32, 32, 2, false, false, cutlass::bfloat16_t, Flash_kernel_traits<512, 32, 32, 2, cutlass::bfloat16_t>>, Is_dropout=true, Is_causal=false, Is_even_N=false, Is_even_K=true, Return_softmax=false, Params=Flash_fwd_params, Is_Q_split_K=false]" 
(589): here
            instantiation of "void flash::compute_attn<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &) [with Kernel_traits=Flash_fwd_kernel_traits<512, 32, 32, 2, false, false, cutlass::bfloat16_t, Flash_kernel_traits<512, 32, 32, 2, cutlass::bfloat16_t>>, Is_dropout=true, Is_causal=false, Is_even_N=false, Is_even_K=true, Return_softmax=false, Params=Flash_fwd_params]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(15): here
            instantiation of "void flash_fwd_kernel<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax>(Flash_fwd_params) [with Kernel_traits=Flash_fwd_kernel_traits<512, 32, 32, 2, false, false, cutlass::bfloat16_t, Flash_kernel_traits<512, 32, 32, 2, cutlass::bfloat16_t>>, Is_dropout=true, Is_causal=false, Is_even_N=false, Is_even_K=true, Return_softmax=false]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(34): here
            instantiation of "void run_flash_fwd<Kernel_traits,Is_dropout,Is_causal>(Flash_fwd_params &, cudaStream_t) [with Kernel_traits=Flash_fwd_kernel_traits<512, 32, 32, 2, false, false, cutlass::bfloat16_t, Flash_kernel_traits<512, 32, 32, 2, cutlass::bfloat16_t>>, Is_dropout=true, Is_causal=false]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(264): here
            instantiation of "void run_mha_fwd_hdim512<T>(Flash_fwd_params &, cudaStream_t) [with T=cutlass::bfloat16_t]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_hdim512_bf16_sm80.cu(8): here

/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_kernel.h(140): warning: variable "MMA_M" was declared but never referenced
          detected during:
            instantiation of "void flash::compute_attn_1rowblock<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params,Is_Q_split_K>(const Params &, int, int, int) [with Kernel_traits=Flash_fwd_kernel_traits<512, 32, 32, 2, false, false, cutlass::bfloat16_t, Flash_kernel_traits<512, 32, 32, 2, cutlass::bfloat16_t>>, Is_dropout=true, Is_causal=false, Is_even_N=false, Is_even_K=false, Return_softmax=true, Params=Flash_fwd_params, Is_Q_split_K=false]" 
(589): here
            instantiation of "void flash::compute_attn<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &) [with Kernel_traits=Flash_fwd_kernel_traits<512, 32, 32, 2, false, false, cutlass::bfloat16_t, Flash_kernel_traits<512, 32, 32, 2, cutlass::bfloat16_t>>, Is_dropout=true, Is_causal=false, Is_even_N=false, Is_even_K=false, Return_softmax=true, Params=Flash_fwd_params]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(15): here
            instantiation of "void flash_fwd_kernel<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax>(Flash_fwd_params) [with Kernel_traits=Flash_fwd_kernel_traits<512, 32, 32, 2, false, false, cutlass::bfloat16_t, Flash_kernel_traits<512, 32, 32, 2, cutlass::bfloat16_t>>, Is_dropout=true, Is_causal=false, Is_even_N=false, Is_even_K=false, Return_softmax=true]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(34): here
            instantiation of "void run_flash_fwd<Kernel_traits,Is_dropout,Is_causal>(Flash_fwd_params &, cudaStream_t) [with Kernel_traits=Flash_fwd_kernel_traits<512, 32, 32, 2, false, false, cutlass::bfloat16_t, Flash_kernel_traits<512, 32, 32, 2, cutlass::bfloat16_t>>, Is_dropout=true, Is_causal=false]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(264): here
            instantiation of "void run_mha_fwd_hdim512<T>(Flash_fwd_params &, cudaStream_t) [with T=cutlass::bfloat16_t]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_hdim512_bf16_sm80.cu(8): here

/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_kernel.h(276): warning: variable "tQrQ" was declared but never referenced
          detected during:
            instantiation of "void flash::compute_attn_1rowblock<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params,Is_Q_split_K>(const Params &, int, int, int) [with Kernel_traits=Flash_fwd_kernel_traits<512, 32, 32, 2, false, false, cutlass::bfloat16_t, Flash_kernel_traits<512, 32, 32, 2, cutlass::bfloat16_t>>, Is_dropout=true, Is_causal=false, Is_even_N=false, Is_even_K=false, Return_softmax=true, Params=Flash_fwd_params, Is_Q_split_K=false]" 
(589): here
            instantiation of "void flash::compute_attn<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &) [with Kernel_traits=Flash_fwd_kernel_traits<512, 32, 32, 2, false, false, cutlass::bfloat16_t, Flash_kernel_traits<512, 32, 32, 2, cutlass::bfloat16_t>>, Is_dropout=true, Is_causal=false, Is_even_N=false, Is_even_K=false, Return_softmax=true, Params=Flash_fwd_params]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(15): here
            instantiation of "void flash_fwd_kernel<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax>(Flash_fwd_params) [with Kernel_traits=Flash_fwd_kernel_traits<512, 32, 32, 2, false, false, cutlass::bfloat16_t, Flash_kernel_traits<512, 32, 32, 2, cutlass::bfloat16_t>>, Is_dropout=true, Is_causal=false, Is_even_N=false, Is_even_K=false, Return_softmax=true]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(34): here
            instantiation of "void run_flash_fwd<Kernel_traits,Is_dropout,Is_causal>(Flash_fwd_params &, cudaStream_t) [with Kernel_traits=Flash_fwd_kernel_traits<512, 32, 32, 2, false, false, cutlass::bfloat16_t, Flash_kernel_traits<512, 32, 32, 2, cutlass::bfloat16_t>>, Is_dropout=true, Is_causal=false]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(264): here
            instantiation of "void run_mha_fwd_hdim512<T>(Flash_fwd_params &, cudaStream_t) [with T=cutlass::bfloat16_t]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_hdim512_bf16_sm80.cu(8): here

/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_kernel.h(140): warning: variable "MMA_M" was declared but never referenced
          detected during:
            instantiation of "void flash::compute_attn_1rowblock<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params,Is_Q_split_K>(const Params &, int, int, int) [with Kernel_traits=Flash_fwd_kernel_traits<512, 32, 32, 2, false, false, cutlass::bfloat16_t, Flash_kernel_traits<512, 32, 32, 2, cutlass::bfloat16_t>>, Is_dropout=true, Is_causal=false, Is_even_N=false, Is_even_K=false, Return_softmax=false, Params=Flash_fwd_params, Is_Q_split_K=false]" 
(589): here
            instantiation of "void flash::compute_attn<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &) [with Kernel_traits=Flash_fwd_kernel_traits<512, 32, 32, 2, false, false, cutlass::bfloat16_t, Flash_kernel_traits<512, 32, 32, 2, cutlass::bfloat16_t>>, Is_dropout=true, Is_causal=false, Is_even_N=false, Is_even_K=false, Return_softmax=false, Params=Flash_fwd_params]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(15): here
            instantiation of "void flash_fwd_kernel<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax>(Flash_fwd_params) [with Kernel_traits=Flash_fwd_kernel_traits<512, 32, 32, 2, false, false, cutlass::bfloat16_t, Flash_kernel_traits<512, 32, 32, 2, cutlass::bfloat16_t>>, Is_dropout=true, Is_causal=false, Is_even_N=false, Is_even_K=false, Return_softmax=false]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(34): here
            instantiation of "void run_flash_fwd<Kernel_traits,Is_dropout,Is_causal>(Flash_fwd_params &, cudaStream_t) [with Kernel_traits=Flash_fwd_kernel_traits<512, 32, 32, 2, false, false, cutlass::bfloat16_t, Flash_kernel_traits<512, 32, 32, 2, cutlass::bfloat16_t>>, Is_dropout=true, Is_causal=false]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(264): here
            instantiation of "void run_mha_fwd_hdim512<T>(Flash_fwd_params &, cudaStream_t) [with T=cutlass::bfloat16_t]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_hdim512_bf16_sm80.cu(8): here

/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_kernel.h(276): warning: variable "tQrQ" was declared but never referenced
          detected during:
            instantiation of "void flash::compute_attn_1rowblock<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params,Is_Q_split_K>(const Params &, int, int, int) [with Kernel_traits=Flash_fwd_kernel_traits<512, 32, 32, 2, false, false, cutlass::bfloat16_t, Flash_kernel_traits<512, 32, 32, 2, cutlass::bfloat16_t>>, Is_dropout=true, Is_causal=false, Is_even_N=false, Is_even_K=false, Return_softmax=false, Params=Flash_fwd_params, Is_Q_split_K=false]" 
(589): here
            instantiation of "void flash::compute_attn<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &) [with Kernel_traits=Flash_fwd_kernel_traits<512, 32, 32, 2, false, false, cutlass::bfloat16_t, Flash_kernel_traits<512, 32, 32, 2, cutlass::bfloat16_t>>, Is_dropout=true, Is_causal=false, Is_even_N=false, Is_even_K=false, Return_softmax=false, Params=Flash_fwd_params]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(15): here
            instantiation of "void flash_fwd_kernel<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax>(Flash_fwd_params) [with Kernel_traits=Flash_fwd_kernel_traits<512, 32, 32, 2, false, false, cutlass::bfloat16_t, Flash_kernel_traits<512, 32, 32, 2, cutlass::bfloat16_t>>, Is_dropout=true, Is_causal=false, Is_even_N=false, Is_even_K=false, Return_softmax=false]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(34): here
            instantiation of "void run_flash_fwd<Kernel_traits,Is_dropout,Is_causal>(Flash_fwd_params &, cudaStream_t) [with Kernel_traits=Flash_fwd_kernel_traits<512, 32, 32, 2, false, false, cutlass::bfloat16_t, Flash_kernel_traits<512, 32, 32, 2, cutlass::bfloat16_t>>, Is_dropout=true, Is_causal=false]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(264): here
            instantiation of "void run_mha_fwd_hdim512<T>(Flash_fwd_params &, cudaStream_t) [with T=cutlass::bfloat16_t]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_hdim512_bf16_sm80.cu(8): here

/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_kernel.h(140): warning: variable "MMA_M" was declared but never referenced
          detected during:
            instantiation of "void flash::compute_attn_1rowblock<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params,Is_Q_split_K>(const Params &, int, int, int) [with Kernel_traits=Flash_fwd_kernel_traits<512, 128, 32, 8, false, true, cutlass::bfloat16_t, Flash_kernel_traits<512, 128, 32, 8, cutlass::bfloat16_t>>, Is_dropout=false, Is_causal=true, Is_even_N=true, Is_even_K=true, Return_softmax=false, Params=Flash_fwd_params, Is_Q_split_K=false]" 
(589): here
            instantiation of "void flash::compute_attn<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &) [with Kernel_traits=Flash_fwd_kernel_traits<512, 128, 32, 8, false, true, cutlass::bfloat16_t, Flash_kernel_traits<512, 128, 32, 8, cutlass::bfloat16_t>>, Is_dropout=false, Is_causal=true, Is_even_N=true, Is_even_K=true, Return_softmax=false, Params=Flash_fwd_params]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(15): here
            instantiation of "void flash_fwd_kernel<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax>(Flash_fwd_params) [with Kernel_traits=Flash_fwd_kernel_traits<512, 128, 32, 8, false, true, cutlass::bfloat16_t, Flash_kernel_traits<512, 128, 32, 8, cutlass::bfloat16_t>>, Is_dropout=false, Is_causal=true, Is_even_N=true, Is_even_K=true, Return_softmax=false]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(34): here
            instantiation of "void run_flash_fwd<Kernel_traits,Is_dropout,Is_causal>(Flash_fwd_params &, cudaStream_t) [with Kernel_traits=Flash_fwd_kernel_traits<512, 128, 32, 8, false, true, cutlass::bfloat16_t, Flash_kernel_traits<512, 128, 32, 8, cutlass::bfloat16_t>>, Is_dropout=false, Is_causal=true]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(264): here
            instantiation of "void run_mha_fwd_hdim512<T>(Flash_fwd_params &, cudaStream_t) [with T=cutlass::bfloat16_t]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_hdim512_bf16_sm80.cu(8): here

/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_kernel.h(276): warning: variable "tQrQ" was declared but never referenced
          detected during:
            instantiation of "void flash::compute_attn_1rowblock<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params,Is_Q_split_K>(const Params &, int, int, int) [with Kernel_traits=Flash_fwd_kernel_traits<512, 128, 32, 8, false, true, cutlass::bfloat16_t, Flash_kernel_traits<512, 128, 32, 8, cutlass::bfloat16_t>>, Is_dropout=false, Is_causal=true, Is_even_N=true, Is_even_K=true, Return_softmax=false, Params=Flash_fwd_params, Is_Q_split_K=false]" 
(589): here
            instantiation of "void flash::compute_attn<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &) [with Kernel_traits=Flash_fwd_kernel_traits<512, 128, 32, 8, false, true, cutlass::bfloat16_t, Flash_kernel_traits<512, 128, 32, 8, cutlass::bfloat16_t>>, Is_dropout=false, Is_causal=true, Is_even_N=true, Is_even_K=true, Return_softmax=false, Params=Flash_fwd_params]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(15): here
            instantiation of "void flash_fwd_kernel<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax>(Flash_fwd_params) [with Kernel_traits=Flash_fwd_kernel_traits<512, 128, 32, 8, false, true, cutlass::bfloat16_t, Flash_kernel_traits<512, 128, 32, 8, cutlass::bfloat16_t>>, Is_dropout=false, Is_causal=true, Is_even_N=true, Is_even_K=true, Return_softmax=false]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(34): here
            instantiation of "void run_flash_fwd<Kernel_traits,Is_dropout,Is_causal>(Flash_fwd_params &, cudaStream_t) [with Kernel_traits=Flash_fwd_kernel_traits<512, 128, 32, 8, false, true, cutlass::bfloat16_t, Flash_kernel_traits<512, 128, 32, 8, cutlass::bfloat16_t>>, Is_dropout=false, Is_causal=true]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(264): here
            instantiation of "void run_mha_fwd_hdim512<T>(Flash_fwd_params &, cudaStream_t) [with T=cutlass::bfloat16_t]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_hdim512_bf16_sm80.cu(8): here

/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_kernel.h(140): warning: variable "MMA_M" was declared but never referenced
          detected during:
            instantiation of "void flash::compute_attn_1rowblock<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params,Is_Q_split_K>(const Params &, int, int, int) [with Kernel_traits=Flash_fwd_kernel_traits<512, 128, 32, 8, false, true, cutlass::bfloat16_t, Flash_kernel_traits<512, 128, 32, 8, cutlass::bfloat16_t>>, Is_dropout=false, Is_causal=true, Is_even_N=true, Is_even_K=false, Return_softmax=false, Params=Flash_fwd_params, Is_Q_split_K=false]" 
(589): here
            instantiation of "void flash::compute_attn<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &) [with Kernel_traits=Flash_fwd_kernel_traits<512, 128, 32, 8, false, true, cutlass::bfloat16_t, Flash_kernel_traits<512, 128, 32, 8, cutlass::bfloat16_t>>, Is_dropout=false, Is_causal=true, Is_even_N=true, Is_even_K=false, Return_softmax=false, Params=Flash_fwd_params]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(15): here
            instantiation of "void flash_fwd_kernel<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax>(Flash_fwd_params) [with Kernel_traits=Flash_fwd_kernel_traits<512, 128, 32, 8, false, true, cutlass::bfloat16_t, Flash_kernel_traits<512, 128, 32, 8, cutlass::bfloat16_t>>, Is_dropout=false, Is_causal=true, Is_even_N=true, Is_even_K=false, Return_softmax=false]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(34): here
            instantiation of "void run_flash_fwd<Kernel_traits,Is_dropout,Is_causal>(Flash_fwd_params &, cudaStream_t) [with Kernel_traits=Flash_fwd_kernel_traits<512, 128, 32, 8, false, true, cutlass::bfloat16_t, Flash_kernel_traits<512, 128, 32, 8, cutlass::bfloat16_t>>, Is_dropout=false, Is_causal=true]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(264): here
            instantiation of "void run_mha_fwd_hdim512<T>(Flash_fwd_params &, cudaStream_t) [with T=cutlass::bfloat16_t]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_hdim512_bf16_sm80.cu(8): here

/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_kernel.h(276): warning: variable "tQrQ" was declared but never referenced
          detected during:
            instantiation of "void flash::compute_attn_1rowblock<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params,Is_Q_split_K>(const Params &, int, int, int) [with Kernel_traits=Flash_fwd_kernel_traits<512, 128, 32, 8, false, true, cutlass::bfloat16_t, Flash_kernel_traits<512, 128, 32, 8, cutlass::bfloat16_t>>, Is_dropout=false, Is_causal=true, Is_even_N=true, Is_even_K=false, Return_softmax=false, Params=Flash_fwd_params, Is_Q_split_K=false]" 
(589): here
            instantiation of "void flash::compute_attn<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &) [with Kernel_traits=Flash_fwd_kernel_traits<512, 128, 32, 8, false, true, cutlass::bfloat16_t, Flash_kernel_traits<512, 128, 32, 8, cutlass::bfloat16_t>>, Is_dropout=false, Is_causal=true, Is_even_N=true, Is_even_K=false, Return_softmax=false, Params=Flash_fwd_params]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(15): here
            instantiation of "void flash_fwd_kernel<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax>(Flash_fwd_params) [with Kernel_traits=Flash_fwd_kernel_traits<512, 128, 32, 8, false, true, cutlass::bfloat16_t, Flash_kernel_traits<512, 128, 32, 8, cutlass::bfloat16_t>>, Is_dropout=false, Is_causal=true, Is_even_N=true, Is_even_K=false, Return_softmax=false]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(34): here
            instantiation of "void run_flash_fwd<Kernel_traits,Is_dropout,Is_causal>(Flash_fwd_params &, cudaStream_t) [with Kernel_traits=Flash_fwd_kernel_traits<512, 128, 32, 8, false, true, cutlass::bfloat16_t, Flash_kernel_traits<512, 128, 32, 8, cutlass::bfloat16_t>>, Is_dropout=false, Is_causal=true]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(264): here
            instantiation of "void run_mha_fwd_hdim512<T>(Flash_fwd_params &, cudaStream_t) [with T=cutlass::bfloat16_t]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_hdim512_bf16_sm80.cu(8): here

/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_kernel.h(140): warning: variable "MMA_M" was declared but never referenced
          detected during:
            instantiation of "void flash::compute_attn_1rowblock<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params,Is_Q_split_K>(const Params &, int, int, int) [with Kernel_traits=Flash_fwd_kernel_traits<512, 128, 32, 8, false, true, cutlass::bfloat16_t, Flash_kernel_traits<512, 128, 32, 8, cutlass::bfloat16_t>>, Is_dropout=false, Is_causal=true, Is_even_N=false, Is_even_K=true, Return_softmax=false, Params=Flash_fwd_params, Is_Q_split_K=false]" 
(589): here
            instantiation of "void flash::compute_attn<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &) [with Kernel_traits=Flash_fwd_kernel_traits<512, 128, 32, 8, false, true, cutlass::bfloat16_t, Flash_kernel_traits<512, 128, 32, 8, cutlass::bfloat16_t>>, Is_dropout=false, Is_causal=true, Is_even_N=false, Is_even_K=true, Return_softmax=false, Params=Flash_fwd_params]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(15): here
            instantiation of "void flash_fwd_kernel<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax>(Flash_fwd_params) [with Kernel_traits=Flash_fwd_kernel_traits<512, 128, 32, 8, false, true, cutlass::bfloat16_t, Flash_kernel_traits<512, 128, 32, 8, cutlass::bfloat16_t>>, Is_dropout=false, Is_causal=true, Is_even_N=false, Is_even_K=true, Return_softmax=false]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(34): here
            instantiation of "void run_flash_fwd<Kernel_traits,Is_dropout,Is_causal>(Flash_fwd_params &, cudaStream_t) [with Kernel_traits=Flash_fwd_kernel_traits<512, 128, 32, 8, false, true, cutlass::bfloat16_t, Flash_kernel_traits<512, 128, 32, 8, cutlass::bfloat16_t>>, Is_dropout=false, Is_causal=true]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(264): here
            instantiation of "void run_mha_fwd_hdim512<T>(Flash_fwd_params &, cudaStream_t) [with T=cutlass::bfloat16_t]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_hdim512_bf16_sm80.cu(8): here

/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_kernel.h(276): warning: variable "tQrQ" was declared but never referenced
          detected during:
            instantiation of "void flash::compute_attn_1rowblock<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params,Is_Q_split_K>(const Params &, int, int, int) [with Kernel_traits=Flash_fwd_kernel_traits<512, 128, 32, 8, false, true, cutlass::bfloat16_t, Flash_kernel_traits<512, 128, 32, 8, cutlass::bfloat16_t>>, Is_dropout=false, Is_causal=true, Is_even_N=false, Is_even_K=true, Return_softmax=false, Params=Flash_fwd_params, Is_Q_split_K=false]" 
(589): here
            instantiation of "void flash::compute_attn<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &) [with Kernel_traits=Flash_fwd_kernel_traits<512, 128, 32, 8, false, true, cutlass::bfloat16_t, Flash_kernel_traits<512, 128, 32, 8, cutlass::bfloat16_t>>, Is_dropout=false, Is_causal=true, Is_even_N=false, Is_even_K=true, Return_softmax=false, Params=Flash_fwd_params]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(15): here
            instantiation of "void flash_fwd_kernel<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax>(Flash_fwd_params) [with Kernel_traits=Flash_fwd_kernel_traits<512, 128, 32, 8, false, true, cutlass::bfloat16_t, Flash_kernel_traits<512, 128, 32, 8, cutlass::bfloat16_t>>, Is_dropout=false, Is_causal=true, Is_even_N=false, Is_even_K=true, Return_softmax=false]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(34): here
            instantiation of "void run_flash_fwd<Kernel_traits,Is_dropout,Is_causal>(Flash_fwd_params &, cudaStream_t) [with Kernel_traits=Flash_fwd_kernel_traits<512, 128, 32, 8, false, true, cutlass::bfloat16_t, Flash_kernel_traits<512, 128, 32, 8, cutlass::bfloat16_t>>, Is_dropout=false, Is_causal=true]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(264): here
            instantiation of "void run_mha_fwd_hdim512<T>(Flash_fwd_params &, cudaStream_t) [with T=cutlass::bfloat16_t]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_hdim512_bf16_sm80.cu(8): here

/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_kernel.h(140): warning: variable "MMA_M" was declared but never referenced
          detected during:
            instantiation of "void flash::compute_attn_1rowblock<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params,Is_Q_split_K>(const Params &, int, int, int) [with Kernel_traits=Flash_fwd_kernel_traits<512, 128, 32, 8, false, true, cutlass::bfloat16_t, Flash_kernel_traits<512, 128, 32, 8, cutlass::bfloat16_t>>, Is_dropout=false, Is_causal=true, Is_even_N=false, Is_even_K=false, Return_softmax=false, Params=Flash_fwd_params, Is_Q_split_K=false]" 
(589): here
            instantiation of "void flash::compute_attn<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &) [with Kernel_traits=Flash_fwd_kernel_traits<512, 128, 32, 8, false, true, cutlass::bfloat16_t, Flash_kernel_traits<512, 128, 32, 8, cutlass::bfloat16_t>>, Is_dropout=false, Is_causal=true, Is_even_N=false, Is_even_K=false, Return_softmax=false, Params=Flash_fwd_params]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(15): here
            instantiation of "void flash_fwd_kernel<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax>(Flash_fwd_params) [with Kernel_traits=Flash_fwd_kernel_traits<512, 128, 32, 8, false, true, cutlass::bfloat16_t, Flash_kernel_traits<512, 128, 32, 8, cutlass::bfloat16_t>>, Is_dropout=false, Is_causal=true, Is_even_N=false, Is_even_K=false, Return_softmax=false]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(34): here
            instantiation of "void run_flash_fwd<Kernel_traits,Is_dropout,Is_causal>(Flash_fwd_params &, cudaStream_t) [with Kernel_traits=Flash_fwd_kernel_traits<512, 128, 32, 8, false, true, cutlass::bfloat16_t, Flash_kernel_traits<512, 128, 32, 8, cutlass::bfloat16_t>>, Is_dropout=false, Is_causal=true]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(264): here
            instantiation of "void run_mha_fwd_hdim512<T>(Flash_fwd_params &, cudaStream_t) [with T=cutlass::bfloat16_t]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_hdim512_bf16_sm80.cu(8): here

/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_kernel.h(276): warning: variable "tQrQ" was declared but never referenced
          detected during:
            instantiation of "void flash::compute_attn_1rowblock<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params,Is_Q_split_K>(const Params &, int, int, int) [with Kernel_traits=Flash_fwd_kernel_traits<512, 128, 32, 8, false, true, cutlass::bfloat16_t, Flash_kernel_traits<512, 128, 32, 8, cutlass::bfloat16_t>>, Is_dropout=false, Is_causal=true, Is_even_N=false, Is_even_K=false, Return_softmax=false, Params=Flash_fwd_params, Is_Q_split_K=false]" 
(589): here
            instantiation of "void flash::compute_attn<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &) [with Kernel_traits=Flash_fwd_kernel_traits<512, 128, 32, 8, false, true, cutlass::bfloat16_t, Flash_kernel_traits<512, 128, 32, 8, cutlass::bfloat16_t>>, Is_dropout=false, Is_causal=true, Is_even_N=false, Is_even_K=false, Return_softmax=false, Params=Flash_fwd_params]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(15): here
            instantiation of "void flash_fwd_kernel<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax>(Flash_fwd_params) [with Kernel_traits=Flash_fwd_kernel_traits<512, 128, 32, 8, false, true, cutlass::bfloat16_t, Flash_kernel_traits<512, 128, 32, 8, cutlass::bfloat16_t>>, Is_dropout=false, Is_causal=true, Is_even_N=false, Is_even_K=false, Return_softmax=false]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(34): here
            instantiation of "void run_flash_fwd<Kernel_traits,Is_dropout,Is_causal>(Flash_fwd_params &, cudaStream_t) [with Kernel_traits=Flash_fwd_kernel_traits<512, 128, 32, 8, false, true, cutlass::bfloat16_t, Flash_kernel_traits<512, 128, 32, 8, cutlass::bfloat16_t>>, Is_dropout=false, Is_causal=true]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(264): here
            instantiation of "void run_mha_fwd_hdim512<T>(Flash_fwd_params &, cudaStream_t) [with T=cutlass::bfloat16_t]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_hdim512_bf16_sm80.cu(8): here

/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_kernel.h(140): warning: variable "MMA_M" was declared but never referenced
          detected during:
            instantiation of "void flash::compute_attn_1rowblock<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params,Is_Q_split_K>(const Params &, int, int, int) [with Kernel_traits=Flash_fwd_kernel_traits<512, 32, 32, 2, false, false, cutlass::bfloat16_t, Flash_kernel_traits<512, 32, 32, 2, cutlass::bfloat16_t>>, Is_dropout=false, Is_causal=true, Is_even_N=true, Is_even_K=true, Return_softmax=false, Params=Flash_fwd_params, Is_Q_split_K=false]" 
(589): here
            instantiation of "void flash::compute_attn<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &) [with Kernel_traits=Flash_fwd_kernel_traits<512, 32, 32, 2, false, false, cutlass::bfloat16_t, Flash_kernel_traits<512, 32, 32, 2, cutlass::bfloat16_t>>, Is_dropout=false, Is_causal=true, Is_even_N=true, Is_even_K=true, Return_softmax=false, Params=Flash_fwd_params]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(15): here
            instantiation of "void flash_fwd_kernel<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax>(Flash_fwd_params) [with Kernel_traits=Flash_fwd_kernel_traits<512, 32, 32, 2, false, false, cutlass::bfloat16_t, Flash_kernel_traits<512, 32, 32, 2, cutlass::bfloat16_t>>, Is_dropout=false, Is_causal=true, Is_even_N=true, Is_even_K=true, Return_softmax=false]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(34): here
            instantiation of "void run_flash_fwd<Kernel_traits,Is_dropout,Is_causal>(Flash_fwd_params &, cudaStream_t) [with Kernel_traits=Flash_fwd_kernel_traits<512, 32, 32, 2, false, false, cutlass::bfloat16_t, Flash_kernel_traits<512, 32, 32, 2, cutlass::bfloat16_t>>, Is_dropout=false, Is_causal=true]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(264): here
            instantiation of "void run_mha_fwd_hdim512<T>(Flash_fwd_params &, cudaStream_t) [with T=cutlass::bfloat16_t]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_hdim512_bf16_sm80.cu(8): here

/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_kernel.h(276): warning: variable "tQrQ" was declared but never referenced
          detected during:
            instantiation of "void flash::compute_attn_1rowblock<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params,Is_Q_split_K>(const Params &, int, int, int) [with Kernel_traits=Flash_fwd_kernel_traits<512, 32, 32, 2, false, false, cutlass::bfloat16_t, Flash_kernel_traits<512, 32, 32, 2, cutlass::bfloat16_t>>, Is_dropout=false, Is_causal=true, Is_even_N=true, Is_even_K=true, Return_softmax=false, Params=Flash_fwd_params, Is_Q_split_K=false]" 
(589): here
            instantiation of "void flash::compute_attn<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &) [with Kernel_traits=Flash_fwd_kernel_traits<512, 32, 32, 2, false, false, cutlass::bfloat16_t, Flash_kernel_traits<512, 32, 32, 2, cutlass::bfloat16_t>>, Is_dropout=false, Is_causal=true, Is_even_N=true, Is_even_K=true, Return_softmax=false, Params=Flash_fwd_params]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(15): here
            instantiation of "void flash_fwd_kernel<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax>(Flash_fwd_params) [with Kernel_traits=Flash_fwd_kernel_traits<512, 32, 32, 2, false, false, cutlass::bfloat16_t, Flash_kernel_traits<512, 32, 32, 2, cutlass::bfloat16_t>>, Is_dropout=false, Is_causal=true, Is_even_N=true, Is_even_K=true, Return_softmax=false]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(34): here
            instantiation of "void run_flash_fwd<Kernel_traits,Is_dropout,Is_causal>(Flash_fwd_params &, cudaStream_t) [with Kernel_traits=Flash_fwd_kernel_traits<512, 32, 32, 2, false, false, cutlass::bfloat16_t, Flash_kernel_traits<512, 32, 32, 2, cutlass::bfloat16_t>>, Is_dropout=false, Is_causal=true]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(264): here
            instantiation of "void run_mha_fwd_hdim512<T>(Flash_fwd_params &, cudaStream_t) [with T=cutlass::bfloat16_t]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_hdim512_bf16_sm80.cu(8): here

/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_kernel.h(140): warning: variable "MMA_M" was declared but never referenced
          detected during:
            instantiation of "void flash::compute_attn_1rowblock<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params,Is_Q_split_K>(const Params &, int, int, int) [with Kernel_traits=Flash_fwd_kernel_traits<512, 32, 32, 2, false, false, cutlass::bfloat16_t, Flash_kernel_traits<512, 32, 32, 2, cutlass::bfloat16_t>>, Is_dropout=false, Is_causal=true, Is_even_N=true, Is_even_K=false, Return_softmax=false, Params=Flash_fwd_params, Is_Q_split_K=false]" 
(589): here
            instantiation of "void flash::compute_attn<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &) [with Kernel_traits=Flash_fwd_kernel_traits<512, 32, 32, 2, false, false, cutlass::bfloat16_t, Flash_kernel_traits<512, 32, 32, 2, cutlass::bfloat16_t>>, Is_dropout=false, Is_causal=true, Is_even_N=true, Is_even_K=false, Return_softmax=false, Params=Flash_fwd_params]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(15): here
            instantiation of "void flash_fwd_kernel<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax>(Flash_fwd_params) [with Kernel_traits=Flash_fwd_kernel_traits<512, 32, 32, 2, false, false, cutlass::bfloat16_t, Flash_kernel_traits<512, 32, 32, 2, cutlass::bfloat16_t>>, Is_dropout=false, Is_causal=true, Is_even_N=true, Is_even_K=false, Return_softmax=false]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(34): here
            instantiation of "void run_flash_fwd<Kernel_traits,Is_dropout,Is_causal>(Flash_fwd_params &, cudaStream_t) [with Kernel_traits=Flash_fwd_kernel_traits<512, 32, 32, 2, false, false, cutlass::bfloat16_t, Flash_kernel_traits<512, 32, 32, 2, cutlass::bfloat16_t>>, Is_dropout=false, Is_causal=true]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(264): here
            instantiation of "void run_mha_fwd_hdim512<T>(Flash_fwd_params &, cudaStream_t) [with T=cutlass::bfloat16_t]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_hdim512_bf16_sm80.cu(8): here

/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_kernel.h(276): warning: variable "tQrQ" was declared but never referenced
          detected during:
            instantiation of "void flash::compute_attn_1rowblock<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params,Is_Q_split_K>(const Params &, int, int, int) [with Kernel_traits=Flash_fwd_kernel_traits<512, 32, 32, 2, false, false, cutlass::bfloat16_t, Flash_kernel_traits<512, 32, 32, 2, cutlass::bfloat16_t>>, Is_dropout=false, Is_causal=true, Is_even_N=true, Is_even_K=false, Return_softmax=false, Params=Flash_fwd_params, Is_Q_split_K=false]" 
(589): here
            instantiation of "void flash::compute_attn<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &) [with Kernel_traits=Flash_fwd_kernel_traits<512, 32, 32, 2, false, false, cutlass::bfloat16_t, Flash_kernel_traits<512, 32, 32, 2, cutlass::bfloat16_t>>, Is_dropout=false, Is_causal=true, Is_even_N=true, Is_even_K=false, Return_softmax=false, Params=Flash_fwd_params]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(15): here
            instantiation of "void flash_fwd_kernel<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax>(Flash_fwd_params) [with Kernel_traits=Flash_fwd_kernel_traits<512, 32, 32, 2, false, false, cutlass::bfloat16_t, Flash_kernel_traits<512, 32, 32, 2, cutlass::bfloat16_t>>, Is_dropout=false, Is_causal=true, Is_even_N=true, Is_even_K=false, Return_softmax=false]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(34): here
            instantiation of "void run_flash_fwd<Kernel_traits,Is_dropout,Is_causal>(Flash_fwd_params &, cudaStream_t) [with Kernel_traits=Flash_fwd_kernel_traits<512, 32, 32, 2, false, false, cutlass::bfloat16_t, Flash_kernel_traits<512, 32, 32, 2, cutlass::bfloat16_t>>, Is_dropout=false, Is_causal=true]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(264): here
            instantiation of "void run_mha_fwd_hdim512<T>(Flash_fwd_params &, cudaStream_t) [with T=cutlass::bfloat16_t]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_hdim512_bf16_sm80.cu(8): here

/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_kernel.h(140): warning: variable "MMA_M" was declared but never referenced
          detected during:
            instantiation of "void flash::compute_attn_1rowblock<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params,Is_Q_split_K>(const Params &, int, int, int) [with Kernel_traits=Flash_fwd_kernel_traits<512, 32, 32, 2, false, false, cutlass::bfloat16_t, Flash_kernel_traits<512, 32, 32, 2, cutlass::bfloat16_t>>, Is_dropout=false, Is_causal=true, Is_even_N=false, Is_even_K=true, Return_softmax=false, Params=Flash_fwd_params, Is_Q_split_K=false]" 
(589): here
            instantiation of "void flash::compute_attn<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &) [with Kernel_traits=Flash_fwd_kernel_traits<512, 32, 32, 2, false, false, cutlass::bfloat16_t, Flash_kernel_traits<512, 32, 32, 2, cutlass::bfloat16_t>>, Is_dropout=false, Is_causal=true, Is_even_N=false, Is_even_K=true, Return_softmax=false, Params=Flash_fwd_params]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(15): here
            instantiation of "void flash_fwd_kernel<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax>(Flash_fwd_params) [with Kernel_traits=Flash_fwd_kernel_traits<512, 32, 32, 2, false, false, cutlass::bfloat16_t, Flash_kernel_traits<512, 32, 32, 2, cutlass::bfloat16_t>>, Is_dropout=false, Is_causal=true, Is_even_N=false, Is_even_K=true, Return_softmax=false]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(34): here
            instantiation of "void run_flash_fwd<Kernel_traits,Is_dropout,Is_causal>(Flash_fwd_params &, cudaStream_t) [with Kernel_traits=Flash_fwd_kernel_traits<512, 32, 32, 2, false, false, cutlass::bfloat16_t, Flash_kernel_traits<512, 32, 32, 2, cutlass::bfloat16_t>>, Is_dropout=false, Is_causal=true]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(264): here
            instantiation of "void run_mha_fwd_hdim512<T>(Flash_fwd_params &, cudaStream_t) [with T=cutlass::bfloat16_t]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_hdim512_bf16_sm80.cu(8): here

/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_kernel.h(276): warning: variable "tQrQ" was declared but never referenced
          detected during:
            instantiation of "void flash::compute_attn_1rowblock<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params,Is_Q_split_K>(const Params &, int, int, int) [with Kernel_traits=Flash_fwd_kernel_traits<512, 32, 32, 2, false, false, cutlass::bfloat16_t, Flash_kernel_traits<512, 32, 32, 2, cutlass::bfloat16_t>>, Is_dropout=false, Is_causal=true, Is_even_N=false, Is_even_K=true, Return_softmax=false, Params=Flash_fwd_params, Is_Q_split_K=false]" 
(589): here
            instantiation of "void flash::compute_attn<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &) [with Kernel_traits=Flash_fwd_kernel_traits<512, 32, 32, 2, false, false, cutlass::bfloat16_t, Flash_kernel_traits<512, 32, 32, 2, cutlass::bfloat16_t>>, Is_dropout=false, Is_causal=true, Is_even_N=false, Is_even_K=true, Return_softmax=false, Params=Flash_fwd_params]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(15): here
            instantiation of "void flash_fwd_kernel<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax>(Flash_fwd_params) [with Kernel_traits=Flash_fwd_kernel_traits<512, 32, 32, 2, false, false, cutlass::bfloat16_t, Flash_kernel_traits<512, 32, 32, 2, cutlass::bfloat16_t>>, Is_dropout=false, Is_causal=true, Is_even_N=false, Is_even_K=true, Return_softmax=false]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(34): here
            instantiation of "void run_flash_fwd<Kernel_traits,Is_dropout,Is_causal>(Flash_fwd_params &, cudaStream_t) [with Kernel_traits=Flash_fwd_kernel_traits<512, 32, 32, 2, false, false, cutlass::bfloat16_t, Flash_kernel_traits<512, 32, 32, 2, cutlass::bfloat16_t>>, Is_dropout=false, Is_causal=true]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(264): here
            instantiation of "void run_mha_fwd_hdim512<T>(Flash_fwd_params &, cudaStream_t) [with T=cutlass::bfloat16_t]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_hdim512_bf16_sm80.cu(8): here

/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_kernel.h(140): warning: variable "MMA_M" was declared but never referenced
          detected during:
            instantiation of "void flash::compute_attn_1rowblock<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params,Is_Q_split_K>(const Params &, int, int, int) [with Kernel_traits=Flash_fwd_kernel_traits<512, 32, 32, 2, false, false, cutlass::bfloat16_t, Flash_kernel_traits<512, 32, 32, 2, cutlass::bfloat16_t>>, Is_dropout=false, Is_causal=true, Is_even_N=false, Is_even_K=false, Return_softmax=false, Params=Flash_fwd_params, Is_Q_split_K=false]" 
(589): here
            instantiation of "void flash::compute_attn<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &) [with Kernel_traits=Flash_fwd_kernel_traits<512, 32, 32, 2, false, false, cutlass::bfloat16_t, Flash_kernel_traits<512, 32, 32, 2, cutlass::bfloat16_t>>, Is_dropout=false, Is_causal=true, Is_even_N=false, Is_even_K=false, Return_softmax=false, Params=Flash_fwd_params]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(15): here
            instantiation of "void flash_fwd_kernel<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax>(Flash_fwd_params) [with Kernel_traits=Flash_fwd_kernel_traits<512, 32, 32, 2, false, false, cutlass::bfloat16_t, Flash_kernel_traits<512, 32, 32, 2, cutlass::bfloat16_t>>, Is_dropout=false, Is_causal=true, Is_even_N=false, Is_even_K=false, Return_softmax=false]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(34): here
            instantiation of "void run_flash_fwd<Kernel_traits,Is_dropout,Is_causal>(Flash_fwd_params &, cudaStream_t) [with Kernel_traits=Flash_fwd_kernel_traits<512, 32, 32, 2, false, false, cutlass::bfloat16_t, Flash_kernel_traits<512, 32, 32, 2, cutlass::bfloat16_t>>, Is_dropout=false, Is_causal=true]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(264): here
            instantiation of "void run_mha_fwd_hdim512<T>(Flash_fwd_params &, cudaStream_t) [with T=cutlass::bfloat16_t]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_hdim512_bf16_sm80.cu(8): here

/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_kernel.h(276): warning: variable "tQrQ" was declared but never referenced
          detected during:
            instantiation of "void flash::compute_attn_1rowblock<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params,Is_Q_split_K>(const Params &, int, int, int) [with Kernel_traits=Flash_fwd_kernel_traits<512, 32, 32, 2, false, false, cutlass::bfloat16_t, Flash_kernel_traits<512, 32, 32, 2, cutlass::bfloat16_t>>, Is_dropout=false, Is_causal=true, Is_even_N=false, Is_even_K=false, Return_softmax=false, Params=Flash_fwd_params, Is_Q_split_K=false]" 
(589): here
            instantiation of "void flash::compute_attn<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &) [with Kernel_traits=Flash_fwd_kernel_traits<512, 32, 32, 2, false, false, cutlass::bfloat16_t, Flash_kernel_traits<512, 32, 32, 2, cutlass::bfloat16_t>>, Is_dropout=false, Is_causal=true, Is_even_N=false, Is_even_K=false, Return_softmax=false, Params=Flash_fwd_params]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(15): here
            instantiation of "void flash_fwd_kernel<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax>(Flash_fwd_params) [with Kernel_traits=Flash_fwd_kernel_traits<512, 32, 32, 2, false, false, cutlass::bfloat16_t, Flash_kernel_traits<512, 32, 32, 2, cutlass::bfloat16_t>>, Is_dropout=false, Is_causal=true, Is_even_N=false, Is_even_K=false, Return_softmax=false]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(34): here
            instantiation of "void run_flash_fwd<Kernel_traits,Is_dropout,Is_causal>(Flash_fwd_params &, cudaStream_t) [with Kernel_traits=Flash_fwd_kernel_traits<512, 32, 32, 2, false, false, cutlass::bfloat16_t, Flash_kernel_traits<512, 32, 32, 2, cutlass::bfloat16_t>>, Is_dropout=false, Is_causal=true]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(264): here
            instantiation of "void run_mha_fwd_hdim512<T>(Flash_fwd_params &, cudaStream_t) [with T=cutlass::bfloat16_t]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_hdim512_bf16_sm80.cu(8): here

/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_kernel.h(140): warning: variable "MMA_M" was declared but never referenced
          detected during:
            instantiation of "void flash::compute_attn_1rowblock<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params,Is_Q_split_K>(const Params &, int, int, int) [with Kernel_traits=Flash_fwd_kernel_traits<512, 128, 32, 8, false, true, cutlass::bfloat16_t, Flash_kernel_traits<512, 128, 32, 8, cutlass::bfloat16_t>>, Is_dropout=false, Is_causal=false, Is_even_N=true, Is_even_K=true, Return_softmax=false, Params=Flash_fwd_params, Is_Q_split_K=false]" 
(589): here
            instantiation of "void flash::compute_attn<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &) [with Kernel_traits=Flash_fwd_kernel_traits<512, 128, 32, 8, false, true, cutlass::bfloat16_t, Flash_kernel_traits<512, 128, 32, 8, cutlass::bfloat16_t>>, Is_dropout=false, Is_causal=false, Is_even_N=true, Is_even_K=true, Return_softmax=false, Params=Flash_fwd_params]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(15): here
            instantiation of "void flash_fwd_kernel<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax>(Flash_fwd_params) [with Kernel_traits=Flash_fwd_kernel_traits<512, 128, 32, 8, false, true, cutlass::bfloat16_t, Flash_kernel_traits<512, 128, 32, 8, cutlass::bfloat16_t>>, Is_dropout=false, Is_causal=false, Is_even_N=true, Is_even_K=true, Return_softmax=false]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(34): here
            instantiation of "void run_flash_fwd<Kernel_traits,Is_dropout,Is_causal>(Flash_fwd_params &, cudaStream_t) [with Kernel_traits=Flash_fwd_kernel_traits<512, 128, 32, 8, false, true, cutlass::bfloat16_t, Flash_kernel_traits<512, 128, 32, 8, cutlass::bfloat16_t>>, Is_dropout=false, Is_causal=false]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(264): here
            instantiation of "void run_mha_fwd_hdim512<T>(Flash_fwd_params &, cudaStream_t) [with T=cutlass::bfloat16_t]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_hdim512_bf16_sm80.cu(8): here

/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_kernel.h(276): warning: variable "tQrQ" was declared but never referenced
          detected during:
            instantiation of "void flash::compute_attn_1rowblock<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params,Is_Q_split_K>(const Params &, int, int, int) [with Kernel_traits=Flash_fwd_kernel_traits<512, 128, 32, 8, false, true, cutlass::bfloat16_t, Flash_kernel_traits<512, 128, 32, 8, cutlass::bfloat16_t>>, Is_dropout=false, Is_causal=false, Is_even_N=true, Is_even_K=true, Return_softmax=false, Params=Flash_fwd_params, Is_Q_split_K=false]" 
(589): here
            instantiation of "void flash::compute_attn<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &) [with Kernel_traits=Flash_fwd_kernel_traits<512, 128, 32, 8, false, true, cutlass::bfloat16_t, Flash_kernel_traits<512, 128, 32, 8, cutlass::bfloat16_t>>, Is_dropout=false, Is_causal=false, Is_even_N=true, Is_even_K=true, Return_softmax=false, Params=Flash_fwd_params]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(15): here
            instantiation of "void flash_fwd_kernel<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax>(Flash_fwd_params) [with Kernel_traits=Flash_fwd_kernel_traits<512, 128, 32, 8, false, true, cutlass::bfloat16_t, Flash_kernel_traits<512, 128, 32, 8, cutlass::bfloat16_t>>, Is_dropout=false, Is_causal=false, Is_even_N=true, Is_even_K=true, Return_softmax=false]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(34): here
            instantiation of "void run_flash_fwd<Kernel_traits,Is_dropout,Is_causal>(Flash_fwd_params &, cudaStream_t) [with Kernel_traits=Flash_fwd_kernel_traits<512, 128, 32, 8, false, true, cutlass::bfloat16_t, Flash_kernel_traits<512, 128, 32, 8, cutlass::bfloat16_t>>, Is_dropout=false, Is_causal=false]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(264): here
            instantiation of "void run_mha_fwd_hdim512<T>(Flash_fwd_params &, cudaStream_t) [with T=cutlass::bfloat16_t]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_hdim512_bf16_sm80.cu(8): here

/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_kernel.h(140): warning: variable "MMA_M" was declared but never referenced
          detected during:
            instantiation of "void flash::compute_attn_1rowblock<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params,Is_Q_split_K>(const Params &, int, int, int) [with Kernel_traits=Flash_fwd_kernel_traits<512, 128, 32, 8, false, true, cutlass::bfloat16_t, Flash_kernel_traits<512, 128, 32, 8, cutlass::bfloat16_t>>, Is_dropout=false, Is_causal=false, Is_even_N=true, Is_even_K=false, Return_softmax=false, Params=Flash_fwd_params, Is_Q_split_K=false]" 
(589): here
            instantiation of "void flash::compute_attn<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &) [with Kernel_traits=Flash_fwd_kernel_traits<512, 128, 32, 8, false, true, cutlass::bfloat16_t, Flash_kernel_traits<512, 128, 32, 8, cutlass::bfloat16_t>>, Is_dropout=false, Is_causal=false, Is_even_N=true, Is_even_K=false, Return_softmax=false, Params=Flash_fwd_params]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(15): here
            instantiation of "void flash_fwd_kernel<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax>(Flash_fwd_params) [with Kernel_traits=Flash_fwd_kernel_traits<512, 128, 32, 8, false, true, cutlass::bfloat16_t, Flash_kernel_traits<512, 128, 32, 8, cutlass::bfloat16_t>>, Is_dropout=false, Is_causal=false, Is_even_N=true, Is_even_K=false, Return_softmax=false]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(34): here
            instantiation of "void run_flash_fwd<Kernel_traits,Is_dropout,Is_causal>(Flash_fwd_params &, cudaStream_t) [with Kernel_traits=Flash_fwd_kernel_traits<512, 128, 32, 8, false, true, cutlass::bfloat16_t, Flash_kernel_traits<512, 128, 32, 8, cutlass::bfloat16_t>>, Is_dropout=false, Is_causal=false]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(264): here
            instantiation of "void run_mha_fwd_hdim512<T>(Flash_fwd_params &, cudaStream_t) [with T=cutlass::bfloat16_t]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_hdim512_bf16_sm80.cu(8): here

/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_kernel.h(276): warning: variable "tQrQ" was declared but never referenced
          detected during:
            instantiation of "void flash::compute_attn_1rowblock<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params,Is_Q_split_K>(const Params &, int, int, int) [with Kernel_traits=Flash_fwd_kernel_traits<512, 128, 32, 8, false, true, cutlass::bfloat16_t, Flash_kernel_traits<512, 128, 32, 8, cutlass::bfloat16_t>>, Is_dropout=false, Is_causal=false, Is_even_N=true, Is_even_K=false, Return_softmax=false, Params=Flash_fwd_params, Is_Q_split_K=false]" 
(589): here
            instantiation of "void flash::compute_attn<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &) [with Kernel_traits=Flash_fwd_kernel_traits<512, 128, 32, 8, false, true, cutlass::bfloat16_t, Flash_kernel_traits<512, 128, 32, 8, cutlass::bfloat16_t>>, Is_dropout=false, Is_causal=false, Is_even_N=true, Is_even_K=false, Return_softmax=false, Params=Flash_fwd_params]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(15): here
            instantiation of "void flash_fwd_kernel<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax>(Flash_fwd_params) [with Kernel_traits=Flash_fwd_kernel_traits<512, 128, 32, 8, false, true, cutlass::bfloat16_t, Flash_kernel_traits<512, 128, 32, 8, cutlass::bfloat16_t>>, Is_dropout=false, Is_causal=false, Is_even_N=true, Is_even_K=false, Return_softmax=false]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(34): here
            instantiation of "void run_flash_fwd<Kernel_traits,Is_dropout,Is_causal>(Flash_fwd_params &, cudaStream_t) [with Kernel_traits=Flash_fwd_kernel_traits<512, 128, 32, 8, false, true, cutlass::bfloat16_t, Flash_kernel_traits<512, 128, 32, 8, cutlass::bfloat16_t>>, Is_dropout=false, Is_causal=false]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(264): here
            instantiation of "void run_mha_fwd_hdim512<T>(Flash_fwd_params &, cudaStream_t) [with T=cutlass::bfloat16_t]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_hdim512_bf16_sm80.cu(8): here

/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_kernel.h(140): warning: variable "MMA_M" was declared but never referenced
          detected during:
            instantiation of "void flash::compute_attn_1rowblock<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params,Is_Q_split_K>(const Params &, int, int, int) [with Kernel_traits=Flash_fwd_kernel_traits<512, 128, 32, 8, false, true, cutlass::bfloat16_t, Flash_kernel_traits<512, 128, 32, 8, cutlass::bfloat16_t>>, Is_dropout=false, Is_causal=false, Is_even_N=false, Is_even_K=true, Return_softmax=false, Params=Flash_fwd_params, Is_Q_split_K=false]" 
(589): here
            instantiation of "void flash::compute_attn<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &) [with Kernel_traits=Flash_fwd_kernel_traits<512, 128, 32, 8, false, true, cutlass::bfloat16_t, Flash_kernel_traits<512, 128, 32, 8, cutlass::bfloat16_t>>, Is_dropout=false, Is_causal=false, Is_even_N=false, Is_even_K=true, Return_softmax=false, Params=Flash_fwd_params]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(15): here
            instantiation of "void flash_fwd_kernel<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax>(Flash_fwd_params) [with Kernel_traits=Flash_fwd_kernel_traits<512, 128, 32, 8, false, true, cutlass::bfloat16_t, Flash_kernel_traits<512, 128, 32, 8, cutlass::bfloat16_t>>, Is_dropout=false, Is_causal=false, Is_even_N=false, Is_even_K=true, Return_softmax=false]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(34): here
            instantiation of "void run_flash_fwd<Kernel_traits,Is_dropout,Is_causal>(Flash_fwd_params &, cudaStream_t) [with Kernel_traits=Flash_fwd_kernel_traits<512, 128, 32, 8, false, true, cutlass::bfloat16_t, Flash_kernel_traits<512, 128, 32, 8, cutlass::bfloat16_t>>, Is_dropout=false, Is_causal=false]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(264): here
            instantiation of "void run_mha_fwd_hdim512<T>(Flash_fwd_params &, cudaStream_t) [with T=cutlass::bfloat16_t]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_hdim512_bf16_sm80.cu(8): here

/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_kernel.h(276): warning: variable "tQrQ" was declared but never referenced
          detected during:
            instantiation of "void flash::compute_attn_1rowblock<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params,Is_Q_split_K>(const Params &, int, int, int) [with Kernel_traits=Flash_fwd_kernel_traits<512, 128, 32, 8, false, true, cutlass::bfloat16_t, Flash_kernel_traits<512, 128, 32, 8, cutlass::bfloat16_t>>, Is_dropout=false, Is_causal=false, Is_even_N=false, Is_even_K=true, Return_softmax=false, Params=Flash_fwd_params, Is_Q_split_K=false]" 
(589): here
            instantiation of "void flash::compute_attn<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &) [with Kernel_traits=Flash_fwd_kernel_traits<512, 128, 32, 8, false, true, cutlass::bfloat16_t, Flash_kernel_traits<512, 128, 32, 8, cutlass::bfloat16_t>>, Is_dropout=false, Is_causal=false, Is_even_N=false, Is_even_K=true, Return_softmax=false, Params=Flash_fwd_params]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(15): here
            instantiation of "void flash_fwd_kernel<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax>(Flash_fwd_params) [with Kernel_traits=Flash_fwd_kernel_traits<512, 128, 32, 8, false, true, cutlass::bfloat16_t, Flash_kernel_traits<512, 128, 32, 8, cutlass::bfloat16_t>>, Is_dropout=false, Is_causal=false, Is_even_N=false, Is_even_K=true, Return_softmax=false]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(34): here
            instantiation of "void run_flash_fwd<Kernel_traits,Is_dropout,Is_causal>(Flash_fwd_params &, cudaStream_t) [with Kernel_traits=Flash_fwd_kernel_traits<512, 128, 32, 8, false, true, cutlass::bfloat16_t, Flash_kernel_traits<512, 128, 32, 8, cutlass::bfloat16_t>>, Is_dropout=false, Is_causal=false]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(264): here
            instantiation of "void run_mha_fwd_hdim512<T>(Flash_fwd_params &, cudaStream_t) [with T=cutlass::bfloat16_t]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_hdim512_bf16_sm80.cu(8): here

/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_kernel.h(140): warning: variable "MMA_M" was declared but never referenced
          detected during:
            instantiation of "void flash::compute_attn_1rowblock<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params,Is_Q_split_K>(const Params &, int, int, int) [with Kernel_traits=Flash_fwd_kernel_traits<512, 128, 32, 8, false, true, cutlass::bfloat16_t, Flash_kernel_traits<512, 128, 32, 8, cutlass::bfloat16_t>>, Is_dropout=false, Is_causal=false, Is_even_N=false, Is_even_K=false, Return_softmax=false, Params=Flash_fwd_params, Is_Q_split_K=false]" 
(589): here
            instantiation of "void flash::compute_attn<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &) [with Kernel_traits=Flash_fwd_kernel_traits<512, 128, 32, 8, false, true, cutlass::bfloat16_t, Flash_kernel_traits<512, 128, 32, 8, cutlass::bfloat16_t>>, Is_dropout=false, Is_causal=false, Is_even_N=false, Is_even_K=false, Return_softmax=false, Params=Flash_fwd_params]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(15): here
            instantiation of "void flash_fwd_kernel<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax>(Flash_fwd_params) [with Kernel_traits=Flash_fwd_kernel_traits<512, 128, 32, 8, false, true, cutlass::bfloat16_t, Flash_kernel_traits<512, 128, 32, 8, cutlass::bfloat16_t>>, Is_dropout=false, Is_causal=false, Is_even_N=false, Is_even_K=false, Return_softmax=false]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(34): here
            instantiation of "void run_flash_fwd<Kernel_traits,Is_dropout,Is_causal>(Flash_fwd_params &, cudaStream_t) [with Kernel_traits=Flash_fwd_kernel_traits<512, 128, 32, 8, false, true, cutlass::bfloat16_t, Flash_kernel_traits<512, 128, 32, 8, cutlass::bfloat16_t>>, Is_dropout=false, Is_causal=false]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(264): here
            instantiation of "void run_mha_fwd_hdim512<T>(Flash_fwd_params &, cudaStream_t) [with T=cutlass::bfloat16_t]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_hdim512_bf16_sm80.cu(8): here

/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_kernel.h(276): warning: variable "tQrQ" was declared but never referenced
          detected during:
            instantiation of "void flash::compute_attn_1rowblock<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params,Is_Q_split_K>(const Params &, int, int, int) [with Kernel_traits=Flash_fwd_kernel_traits<512, 128, 32, 8, false, true, cutlass::bfloat16_t, Flash_kernel_traits<512, 128, 32, 8, cutlass::bfloat16_t>>, Is_dropout=false, Is_causal=false, Is_even_N=false, Is_even_K=false, Return_softmax=false, Params=Flash_fwd_params, Is_Q_split_K=false]" 
(589): here
            instantiation of "void flash::compute_attn<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &) [with Kernel_traits=Flash_fwd_kernel_traits<512, 128, 32, 8, false, true, cutlass::bfloat16_t, Flash_kernel_traits<512, 128, 32, 8, cutlass::bfloat16_t>>, Is_dropout=false, Is_causal=false, Is_even_N=false, Is_even_K=false, Return_softmax=false, Params=Flash_fwd_params]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(15): here
            instantiation of "void flash_fwd_kernel<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax>(Flash_fwd_params) [with Kernel_traits=Flash_fwd_kernel_traits<512, 128, 32, 8, false, true, cutlass::bfloat16_t, Flash_kernel_traits<512, 128, 32, 8, cutlass::bfloat16_t>>, Is_dropout=false, Is_causal=false, Is_even_N=false, Is_even_K=false, Return_softmax=false]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(34): here
            instantiation of "void run_flash_fwd<Kernel_traits,Is_dropout,Is_causal>(Flash_fwd_params &, cudaStream_t) [with Kernel_traits=Flash_fwd_kernel_traits<512, 128, 32, 8, false, true, cutlass::bfloat16_t, Flash_kernel_traits<512, 128, 32, 8, cutlass::bfloat16_t>>, Is_dropout=false, Is_causal=false]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(264): here
            instantiation of "void run_mha_fwd_hdim512<T>(Flash_fwd_params &, cudaStream_t) [with T=cutlass::bfloat16_t]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_hdim512_bf16_sm80.cu(8): here

/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_kernel.h(140): warning: variable "MMA_M" was declared but never referenced
          detected during:
            instantiation of "void flash::compute_attn_1rowblock<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params,Is_Q_split_K>(const Params &, int, int, int) [with Kernel_traits=Flash_fwd_kernel_traits<512, 32, 32, 2, false, false, cutlass::bfloat16_t, Flash_kernel_traits<512, 32, 32, 2, cutlass::bfloat16_t>>, Is_dropout=false, Is_causal=false, Is_even_N=true, Is_even_K=true, Return_softmax=false, Params=Flash_fwd_params, Is_Q_split_K=false]" 
(589): here
            instantiation of "void flash::compute_attn<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &) [with Kernel_traits=Flash_fwd_kernel_traits<512, 32, 32, 2, false, false, cutlass::bfloat16_t, Flash_kernel_traits<512, 32, 32, 2, cutlass::bfloat16_t>>, Is_dropout=false, Is_causal=false, Is_even_N=true, Is_even_K=true, Return_softmax=false, Params=Flash_fwd_params]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(15): here
            instantiation of "void flash_fwd_kernel<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax>(Flash_fwd_params) [with Kernel_traits=Flash_fwd_kernel_traits<512, 32, 32, 2, false, false, cutlass::bfloat16_t, Flash_kernel_traits<512, 32, 32, 2, cutlass::bfloat16_t>>, Is_dropout=false, Is_causal=false, Is_even_N=true, Is_even_K=true, Return_softmax=false]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(34): here
            instantiation of "void run_flash_fwd<Kernel_traits,Is_dropout,Is_causal>(Flash_fwd_params &, cudaStream_t) [with Kernel_traits=Flash_fwd_kernel_traits<512, 32, 32, 2, false, false, cutlass::bfloat16_t, Flash_kernel_traits<512, 32, 32, 2, cutlass::bfloat16_t>>, Is_dropout=false, Is_causal=false]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(264): here
            instantiation of "void run_mha_fwd_hdim512<T>(Flash_fwd_params &, cudaStream_t) [with T=cutlass::bfloat16_t]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_hdim512_bf16_sm80.cu(8): here

/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_kernel.h(276): warning: variable "tQrQ" was declared but never referenced
          detected during:
            instantiation of "void flash::compute_attn_1rowblock<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params,Is_Q_split_K>(const Params &, int, int, int) [with Kernel_traits=Flash_fwd_kernel_traits<512, 32, 32, 2, false, false, cutlass::bfloat16_t, Flash_kernel_traits<512, 32, 32, 2, cutlass::bfloat16_t>>, Is_dropout=false, Is_causal=false, Is_even_N=true, Is_even_K=true, Return_softmax=false, Params=Flash_fwd_params, Is_Q_split_K=false]" 
(589): here
            instantiation of "void flash::compute_attn<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &) [with Kernel_traits=Flash_fwd_kernel_traits<512, 32, 32, 2, false, false, cutlass::bfloat16_t, Flash_kernel_traits<512, 32, 32, 2, cutlass::bfloat16_t>>, Is_dropout=false, Is_causal=false, Is_even_N=true, Is_even_K=true, Return_softmax=false, Params=Flash_fwd_params]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(15): here
            instantiation of "void flash_fwd_kernel<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax>(Flash_fwd_params) [with Kernel_traits=Flash_fwd_kernel_traits<512, 32, 32, 2, false, false, cutlass::bfloat16_t, Flash_kernel_traits<512, 32, 32, 2, cutlass::bfloat16_t>>, Is_dropout=false, Is_causal=false, Is_even_N=true, Is_even_K=true, Return_softmax=false]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(34): here
            instantiation of "void run_flash_fwd<Kernel_traits,Is_dropout,Is_causal>(Flash_fwd_params &, cudaStream_t) [with Kernel_traits=Flash_fwd_kernel_traits<512, 32, 32, 2, false, false, cutlass::bfloat16_t, Flash_kernel_traits<512, 32, 32, 2, cutlass::bfloat16_t>>, Is_dropout=false, Is_causal=false]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(264): here
            instantiation of "void run_mha_fwd_hdim512<T>(Flash_fwd_params &, cudaStream_t) [with T=cutlass::bfloat16_t]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_hdim512_bf16_sm80.cu(8): here

/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_kernel.h(140): warning: variable "MMA_M" was declared but never referenced
          detected during:
            instantiation of "void flash::compute_attn_1rowblock<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params,Is_Q_split_K>(const Params &, int, int, int) [with Kernel_traits=Flash_fwd_kernel_traits<512, 32, 32, 2, false, false, cutlass::bfloat16_t, Flash_kernel_traits<512, 32, 32, 2, cutlass::bfloat16_t>>, Is_dropout=false, Is_causal=false, Is_even_N=true, Is_even_K=false, Return_softmax=false, Params=Flash_fwd_params, Is_Q_split_K=false]" 
(589): here
            instantiation of "void flash::compute_attn<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &) [with Kernel_traits=Flash_fwd_kernel_traits<512, 32, 32, 2, false, false, cutlass::bfloat16_t, Flash_kernel_traits<512, 32, 32, 2, cutlass::bfloat16_t>>, Is_dropout=false, Is_causal=false, Is_even_N=true, Is_even_K=false, Return_softmax=false, Params=Flash_fwd_params]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(15): here
            instantiation of "void flash_fwd_kernel<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax>(Flash_fwd_params) [with Kernel_traits=Flash_fwd_kernel_traits<512, 32, 32, 2, false, false, cutlass::bfloat16_t, Flash_kernel_traits<512, 32, 32, 2, cutlass::bfloat16_t>>, Is_dropout=false, Is_causal=false, Is_even_N=true, Is_even_K=false, Return_softmax=false]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(34): here
            instantiation of "void run_flash_fwd<Kernel_traits,Is_dropout,Is_causal>(Flash_fwd_params &, cudaStream_t) [with Kernel_traits=Flash_fwd_kernel_traits<512, 32, 32, 2, false, false, cutlass::bfloat16_t, Flash_kernel_traits<512, 32, 32, 2, cutlass::bfloat16_t>>, Is_dropout=false, Is_causal=false]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(264): here
            instantiation of "void run_mha_fwd_hdim512<T>(Flash_fwd_params &, cudaStream_t) [with T=cutlass::bfloat16_t]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_hdim512_bf16_sm80.cu(8): here

/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_kernel.h(276): warning: variable "tQrQ" was declared but never referenced
          detected during:
            instantiation of "void flash::compute_attn_1rowblock<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params,Is_Q_split_K>(const Params &, int, int, int) [with Kernel_traits=Flash_fwd_kernel_traits<512, 32, 32, 2, false, false, cutlass::bfloat16_t, Flash_kernel_traits<512, 32, 32, 2, cutlass::bfloat16_t>>, Is_dropout=false, Is_causal=false, Is_even_N=true, Is_even_K=false, Return_softmax=false, Params=Flash_fwd_params, Is_Q_split_K=false]" 
(589): here
            instantiation of "void flash::compute_attn<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &) [with Kernel_traits=Flash_fwd_kernel_traits<512, 32, 32, 2, false, false, cutlass::bfloat16_t, Flash_kernel_traits<512, 32, 32, 2, cutlass::bfloat16_t>>, Is_dropout=false, Is_causal=false, Is_even_N=true, Is_even_K=false, Return_softmax=false, Params=Flash_fwd_params]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(15): here
            instantiation of "void flash_fwd_kernel<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax>(Flash_fwd_params) [with Kernel_traits=Flash_fwd_kernel_traits<512, 32, 32, 2, false, false, cutlass::bfloat16_t, Flash_kernel_traits<512, 32, 32, 2, cutlass::bfloat16_t>>, Is_dropout=false, Is_causal=false, Is_even_N=true, Is_even_K=false, Return_softmax=false]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(34): here
            instantiation of "void run_flash_fwd<Kernel_traits,Is_dropout,Is_causal>(Flash_fwd_params &, cudaStream_t) [with Kernel_traits=Flash_fwd_kernel_traits<512, 32, 32, 2, false, false, cutlass::bfloat16_t, Flash_kernel_traits<512, 32, 32, 2, cutlass::bfloat16_t>>, Is_dropout=false, Is_causal=false]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(264): here
            instantiation of "void run_mha_fwd_hdim512<T>(Flash_fwd_params &, cudaStream_t) [with T=cutlass::bfloat16_t]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_hdim512_bf16_sm80.cu(8): here

/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_kernel.h(140): warning: variable "MMA_M" was declared but never referenced
          detected during:
            instantiation of "void flash::compute_attn_1rowblock<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params,Is_Q_split_K>(const Params &, int, int, int) [with Kernel_traits=Flash_fwd_kernel_traits<512, 32, 32, 2, false, false, cutlass::bfloat16_t, Flash_kernel_traits<512, 32, 32, 2, cutlass::bfloat16_t>>, Is_dropout=false, Is_causal=false, Is_even_N=false, Is_even_K=true, Return_softmax=false, Params=Flash_fwd_params, Is_Q_split_K=false]" 
(589): here
            instantiation of "void flash::compute_attn<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &) [with Kernel_traits=Flash_fwd_kernel_traits<512, 32, 32, 2, false, false, cutlass::bfloat16_t, Flash_kernel_traits<512, 32, 32, 2, cutlass::bfloat16_t>>, Is_dropout=false, Is_causal=false, Is_even_N=false, Is_even_K=true, Return_softmax=false, Params=Flash_fwd_params]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(15): here
            instantiation of "void flash_fwd_kernel<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax>(Flash_fwd_params) [with Kernel_traits=Flash_fwd_kernel_traits<512, 32, 32, 2, false, false, cutlass::bfloat16_t, Flash_kernel_traits<512, 32, 32, 2, cutlass::bfloat16_t>>, Is_dropout=false, Is_causal=false, Is_even_N=false, Is_even_K=true, Return_softmax=false]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(34): here
            instantiation of "void run_flash_fwd<Kernel_traits,Is_dropout,Is_causal>(Flash_fwd_params &, cudaStream_t) [with Kernel_traits=Flash_fwd_kernel_traits<512, 32, 32, 2, false, false, cutlass::bfloat16_t, Flash_kernel_traits<512, 32, 32, 2, cutlass::bfloat16_t>>, Is_dropout=false, Is_causal=false]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(264): here
            instantiation of "void run_mha_fwd_hdim512<T>(Flash_fwd_params &, cudaStream_t) [with T=cutlass::bfloat16_t]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_hdim512_bf16_sm80.cu(8): here

/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_kernel.h(276): warning: variable "tQrQ" was declared but never referenced
          detected during:
            instantiation of "void flash::compute_attn_1rowblock<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params,Is_Q_split_K>(const Params &, int, int, int) [with Kernel_traits=Flash_fwd_kernel_traits<512, 32, 32, 2, false, false, cutlass::bfloat16_t, Flash_kernel_traits<512, 32, 32, 2, cutlass::bfloat16_t>>, Is_dropout=false, Is_causal=false, Is_even_N=false, Is_even_K=true, Return_softmax=false, Params=Flash_fwd_params, Is_Q_split_K=false]" 
(589): here
            instantiation of "void flash::compute_attn<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &) [with Kernel_traits=Flash_fwd_kernel_traits<512, 32, 32, 2, false, false, cutlass::bfloat16_t, Flash_kernel_traits<512, 32, 32, 2, cutlass::bfloat16_t>>, Is_dropout=false, Is_causal=false, Is_even_N=false, Is_even_K=true, Return_softmax=false, Params=Flash_fwd_params]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(15): here
            instantiation of "void flash_fwd_kernel<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax>(Flash_fwd_params) [with Kernel_traits=Flash_fwd_kernel_traits<512, 32, 32, 2, false, false, cutlass::bfloat16_t, Flash_kernel_traits<512, 32, 32, 2, cutlass::bfloat16_t>>, Is_dropout=false, Is_causal=false, Is_even_N=false, Is_even_K=true, Return_softmax=false]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(34): here
            instantiation of "void run_flash_fwd<Kernel_traits,Is_dropout,Is_causal>(Flash_fwd_params &, cudaStream_t) [with Kernel_traits=Flash_fwd_kernel_traits<512, 32, 32, 2, false, false, cutlass::bfloat16_t, Flash_kernel_traits<512, 32, 32, 2, cutlass::bfloat16_t>>, Is_dropout=false, Is_causal=false]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(264): here
            instantiation of "void run_mha_fwd_hdim512<T>(Flash_fwd_params &, cudaStream_t) [with T=cutlass::bfloat16_t]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_hdim512_bf16_sm80.cu(8): here

/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_kernel.h(140): warning: variable "MMA_M" was declared but never referenced
          detected during:
            instantiation of "void flash::compute_attn_1rowblock<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params,Is_Q_split_K>(const Params &, int, int, int) [with Kernel_traits=Flash_fwd_kernel_traits<512, 32, 32, 2, false, false, cutlass::bfloat16_t, Flash_kernel_traits<512, 32, 32, 2, cutlass::bfloat16_t>>, Is_dropout=false, Is_causal=false, Is_even_N=false, Is_even_K=false, Return_softmax=false, Params=Flash_fwd_params, Is_Q_split_K=false]" 
(589): here
            instantiation of "void flash::compute_attn<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &) [with Kernel_traits=Flash_fwd_kernel_traits<512, 32, 32, 2, false, false, cutlass::bfloat16_t, Flash_kernel_traits<512, 32, 32, 2, cutlass::bfloat16_t>>, Is_dropout=false, Is_causal=false, Is_even_N=false, Is_even_K=false, Return_softmax=false, Params=Flash_fwd_params]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(15): here
            instantiation of "void flash_fwd_kernel<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax>(Flash_fwd_params) [with Kernel_traits=Flash_fwd_kernel_traits<512, 32, 32, 2, false, false, cutlass::bfloat16_t, Flash_kernel_traits<512, 32, 32, 2, cutlass::bfloat16_t>>, Is_dropout=false, Is_causal=false, Is_even_N=false, Is_even_K=false, Return_softmax=false]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(34): here
            instantiation of "void run_flash_fwd<Kernel_traits,Is_dropout,Is_causal>(Flash_fwd_params &, cudaStream_t) [with Kernel_traits=Flash_fwd_kernel_traits<512, 32, 32, 2, false, false, cutlass::bfloat16_t, Flash_kernel_traits<512, 32, 32, 2, cutlass::bfloat16_t>>, Is_dropout=false, Is_causal=false]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(264): here
            instantiation of "void run_mha_fwd_hdim512<T>(Flash_fwd_params &, cudaStream_t) [with T=cutlass::bfloat16_t]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_hdim512_bf16_sm80.cu(8): here

/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_kernel.h(276): warning: variable "tQrQ" was declared but never referenced
          detected during:
            instantiation of "void flash::compute_attn_1rowblock<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params,Is_Q_split_K>(const Params &, int, int, int) [with Kernel_traits=Flash_fwd_kernel_traits<512, 32, 32, 2, false, false, cutlass::bfloat16_t, Flash_kernel_traits<512, 32, 32, 2, cutlass::bfloat16_t>>, Is_dropout=false, Is_causal=false, Is_even_N=false, Is_even_K=false, Return_softmax=false, Params=Flash_fwd_params, Is_Q_split_K=false]" 
(589): here
            instantiation of "void flash::compute_attn<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &) [with Kernel_traits=Flash_fwd_kernel_traits<512, 32, 32, 2, false, false, cutlass::bfloat16_t, Flash_kernel_traits<512, 32, 32, 2, cutlass::bfloat16_t>>, Is_dropout=false, Is_causal=false, Is_even_N=false, Is_even_K=false, Return_softmax=false, Params=Flash_fwd_params]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(15): here
            instantiation of "void flash_fwd_kernel<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax>(Flash_fwd_params) [with Kernel_traits=Flash_fwd_kernel_traits<512, 32, 32, 2, false, false, cutlass::bfloat16_t, Flash_kernel_traits<512, 32, 32, 2, cutlass::bfloat16_t>>, Is_dropout=false, Is_causal=false, Is_even_N=false, Is_even_K=false, Return_softmax=false]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(34): here
            instantiation of "void run_flash_fwd<Kernel_traits,Is_dropout,Is_causal>(Flash_fwd_params &, cudaStream_t) [with Kernel_traits=Flash_fwd_kernel_traits<512, 32, 32, 2, false, false, cutlass::bfloat16_t, Flash_kernel_traits<512, 32, 32, 2, cutlass::bfloat16_t>>, Is_dropout=false, Is_causal=false]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(264): here
            instantiation of "void run_mha_fwd_hdim512<T>(Flash_fwd_params &, cudaStream_t) [with T=cutlass::bfloat16_t]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_hdim512_bf16_sm80.cu(8): here

ptxas info    : 2 bytes gmem
ptxas info    : Compiling entry function '_Z16flash_fwd_kernelI23Flash_fwd_kernel_traitsILi512ELi32ELi32ELi2ELb0ELb0EN7cutlass10bfloat16_tE19Flash_kernel_traitsILi512ELi32ELi32ELi2ES2_EELb0ELb0ELb0ELb0ELb0EEv16Flash_fwd_params' for 'sm_80'
ptxas info    : Function properties for _Z16flash_fwd_kernelI23Flash_fwd_kernel_traitsILi512ELi32ELi32ELi2ELb0ELb0EN7cutlass10bfloat16_tE19Flash_kernel_traitsILi512ELi32ELi32ELi2ES2_EELb0ELb0ELb0ELb0ELb0EEv16Flash_fwd_params
    832 bytes stack frame, 1188 bytes spill stores, 1208 bytes spill loads
ptxas info    : Used 255 registers, 584 bytes cmem[0]
ptxas info    : Compiling entry function '_Z16flash_fwd_kernelI23Flash_fwd_kernel_traitsILi512ELi32ELi32ELi2ELb0ELb0EN7cutlass10bfloat16_tE19Flash_kernel_traitsILi512ELi32ELi32ELi2ES2_EELb0ELb0ELb0ELb1ELb0EEv16Flash_fwd_params' for 'sm_80'
ptxas info    : Function properties for _Z16flash_fwd_kernelI23Flash_fwd_kernel_traitsILi512ELi32ELi32ELi2ELb0ELb0EN7cutlass10bfloat16_tE19Flash_kernel_traitsILi512ELi32ELi32ELi2ES2_EELb0ELb0ELb0ELb1ELb0EEv16Flash_fwd_params
    456 bytes stack frame, 792 bytes spill stores, 784 bytes spill loads
ptxas info    : Used 255 registers, 584 bytes cmem[0]
ptxas info    : Compiling entry function '_Z16flash_fwd_kernelI23Flash_fwd_kernel_traitsILi512ELi32ELi32ELi2ELb0ELb0EN7cutlass10bfloat16_tE19Flash_kernel_traitsILi512ELi32ELi32ELi2ES2_EELb0ELb0ELb1ELb0ELb0EEv16Flash_fwd_params' for 'sm_80'
ptxas info    : Function properties for _Z16flash_fwd_kernelI23Flash_fwd_kernel_traitsILi512ELi32ELi32ELi2ELb0ELb0EN7cutlass10bfloat16_tE19Flash_kernel_traitsILi512ELi32ELi32ELi2ES2_EELb0ELb0ELb1ELb0ELb0EEv16Flash_fwd_params
    744 bytes stack frame, 1068 bytes spill stores, 1072 bytes spill loads
ptxas info    : Used 255 registers, 584 bytes cmem[0]
ptxas info    : Compiling entry function '_Z16flash_fwd_kernelI23Flash_fwd_kernel_traitsILi512ELi32ELi32ELi2ELb0ELb0EN7cutlass10bfloat16_tE19Flash_kernel_traitsILi512ELi32ELi32ELi2ES2_EELb0ELb0ELb1ELb1ELb0EEv16Flash_fwd_params' for 'sm_80'
ptxas info    : Function properties for _Z16flash_fwd_kernelI23Flash_fwd_kernel_traitsILi512ELi32ELi32ELi2ELb0ELb0EN7cutlass10bfloat16_tE19Flash_kernel_traitsILi512ELi32ELi32ELi2ES2_EELb0ELb0ELb1ELb1ELb0EEv16Flash_fwd_params
    456 bytes stack frame, 752 bytes spill stores, 772 bytes spill loads
ptxas info    : Used 255 registers, 584 bytes cmem[0]
ptxas info    : Compiling entry function '_Z16flash_fwd_kernelI23Flash_fwd_kernel_traitsILi512ELi128ELi32ELi8ELb0ELb1EN7cutlass10bfloat16_tE19Flash_kernel_traitsILi512ELi128ELi32ELi8ES2_EELb0ELb0ELb0ELb0ELb0EEv16Flash_fwd_params' for 'sm_80'
ptxas info    : Function properties for _Z16flash_fwd_kernelI23Flash_fwd_kernel_traitsILi512ELi128ELi32ELi8ELb0ELb1EN7cutlass10bfloat16_tE19Flash_kernel_traitsILi512ELi128ELi32ELi8ES2_EELb0ELb0ELb0ELb0ELb0EEv16Flash_fwd_params
    1536 bytes stack frame, 2604 bytes spill stores, 2600 bytes spill loads
ptxas info    : Used 255 registers, 584 bytes cmem[0]
ptxas info    : Compiling entry function '_Z16flash_fwd_kernelI23Flash_fwd_kernel_traitsILi512ELi128ELi32ELi8ELb0ELb1EN7cutlass10bfloat16_tE19Flash_kernel_traitsILi512ELi128ELi32ELi8ES2_EELb0ELb0ELb0ELb1ELb0EEv16Flash_fwd_params' for 'sm_80'
ptxas info    : Function properties for _Z16flash_fwd_kernelI23Flash_fwd_kernel_traitsILi512ELi128ELi32ELi8ELb0ELb1EN7cutlass10bfloat16_tE19Flash_kernel_traitsILi512ELi128ELi32ELi8ES2_EELb0ELb0ELb0ELb1ELb0EEv16Flash_fwd_params
    1440 bytes stack frame, 2464 bytes spill stores, 2448 bytes spill loads
ptxas info    : Used 255 registers, 584 bytes cmem[0]
ptxas info    : Compiling entry function '_Z16flash_fwd_kernelI23Flash_fwd_kernel_traitsILi512ELi128ELi32ELi8ELb0ELb1EN7cutlass10bfloat16_tE19Flash_kernel_traitsILi512ELi128ELi32ELi8ES2_EELb0ELb0ELb1ELb0ELb0EEv16Flash_fwd_params' for 'sm_80'
ptxas info    : Function properties for _Z16flash_fwd_kernelI23Flash_fwd_kernel_traitsILi512ELi128ELi32ELi8ELb0ELb1EN7cutlass10bfloat16_tE19Flash_kernel_traitsILi512ELi128ELi32ELi8ES2_EELb0ELb0ELb1ELb0ELb0EEv16Flash_fwd_params
    1440 bytes stack frame, 2476 bytes spill stores, 2484 bytes spill loads
ptxas info    : Used 255 registers, 584 bytes cmem[0]
ptxas info    : Compiling entry function '_Z16flash_fwd_kernelI23Flash_fwd_kernel_traitsILi512ELi128ELi32ELi8ELb0ELb1EN7cutlass10bfloat16_tE19Flash_kernel_traitsILi512ELi128ELi32ELi8ES2_EELb0ELb0ELb1ELb1ELb0EEv16Flash_fwd_params' for 'sm_80'
ptxas info    : Function properties for _Z16flash_fwd_kernelI23Flash_fwd_kernel_traitsILi512ELi128ELi32ELi8ELb0ELb1EN7cutlass10bfloat16_tE19Flash_kernel_traitsILi512ELi128ELi32ELi8ES2_EELb0ELb0ELb1ELb1ELb0EEv16Flash_fwd_params
    1424 bytes stack frame, 2348 bytes spill stores, 2340 bytes spill loads
ptxas info    : Used 255 registers, 584 bytes cmem[0]
ptxas info    : Compiling entry function '_Z16flash_fwd_kernelI23Flash_fwd_kernel_traitsILi512ELi32ELi32ELi2ELb0ELb0EN7cutlass10bfloat16_tE19Flash_kernel_traitsILi512ELi32ELi32ELi2ES2_EELb0ELb1ELb0ELb0ELb0EEv16Flash_fwd_params' for 'sm_80'
ptxas info    : Function properties for _Z16flash_fwd_kernelI23Flash_fwd_kernel_traitsILi512ELi32ELi32ELi2ELb0ELb0EN7cutlass10bfloat16_tE19Flash_kernel_traitsILi512ELi32ELi32ELi2ES2_EELb0ELb1ELb0ELb0ELb0EEv16Flash_fwd_params
    832 bytes stack frame, 1192 bytes spill stores, 1212 bytes spill loads
ptxas info    : Used 255 registers, 584 bytes cmem[0]
ptxas info    : Compiling entry function '_Z16flash_fwd_kernelI23Flash_fwd_kernel_traitsILi512ELi32ELi32ELi2ELb0ELb0EN7cutlass10bfloat16_tE19Flash_kernel_traitsILi512ELi32ELi32ELi2ES2_EELb0ELb1ELb0ELb1ELb0EEv16Flash_fwd_params' for 'sm_80'
ptxas info    : Function properties for _Z16flash_fwd_kernelI23Flash_fwd_kernel_traitsILi512ELi32ELi32ELi2ELb0ELb0EN7cutlass10bfloat16_tE19Flash_kernel_traitsILi512ELi32ELi32ELi2ES2_EELb0ELb1ELb0ELb1ELb0EEv16Flash_fwd_params
    464 bytes stack frame, 796 bytes spill stores, 788 bytes spill loads
ptxas info    : Used 255 registers, 584 bytes cmem[0]
ptxas info    : Compiling entry function '_Z16flash_fwd_kernelI23Flash_fwd_kernel_traitsILi512ELi32ELi32ELi2ELb0ELb0EN7cutlass10bfloat16_tE19Flash_kernel_traitsILi512ELi32ELi32ELi2ES2_EELb0ELb1ELb1ELb0ELb0EEv16Flash_fwd_params' for 'sm_80'
ptxas info    : Function properties for _Z16flash_fwd_kernelI23Flash_fwd_kernel_traitsILi512ELi32ELi32ELi2ELb0ELb0EN7cutlass10bfloat16_tE19Flash_kernel_traitsILi512ELi32ELi32ELi2ES2_EELb0ELb1ELb1ELb0ELb0EEv16Flash_fwd_params
    760 bytes stack frame, 1120 bytes spill stores, 1124 bytes spill loads
ptxas info    : Used 255 registers, 584 bytes cmem[0]
ptxas info    : Compiling entry function '_Z16flash_fwd_kernelI23Flash_fwd_kernel_traitsILi512ELi32ELi32ELi2ELb0ELb0EN7cutlass10bfloat16_tE19Flash_kernel_traitsILi512ELi32ELi32ELi2ES2_EELb0ELb1ELb1ELb1ELb0EEv16Flash_fwd_params' for 'sm_80'
ptxas info    : Function properties for _Z16flash_fwd_kernelI23Flash_fwd_kernel_traitsILi512ELi32ELi32ELi2ELb0ELb0EN7cutlass10bfloat16_tE19Flash_kernel_traitsILi512ELi32ELi32ELi2ES2_EELb0ELb1ELb1ELb1ELb0EEv16Flash_fwd_params
    464 bytes stack frame, 756 bytes spill stores, 776 bytes spill loads
ptxas info    : Used 255 registers, 584 bytes cmem[0]
ptxas info    : Compiling entry function '_Z16flash_fwd_kernelI23Flash_fwd_kernel_traitsILi512ELi128ELi32ELi8ELb0ELb1EN7cutlass10bfloat16_tE19Flash_kernel_traitsILi512ELi128ELi32ELi8ES2_EELb0ELb1ELb0ELb0ELb0EEv16Flash_fwd_params' for 'sm_80'
ptxas info    : Function properties for _Z16flash_fwd_kernelI23Flash_fwd_kernel_traitsILi512ELi128ELi32ELi8ELb0ELb1EN7cutlass10bfloat16_tE19Flash_kernel_traitsILi512ELi128ELi32ELi8ES2_EELb0ELb1ELb0ELb0ELb0EEv16Flash_fwd_params
    1936 bytes stack frame, 0 bytes spill stores, 0 bytes spill loads
ptxas info    : Used 255 registers, 584 bytes cmem[0]
ptxas info    : Function properties for _ZN5flash22compute_attn_1rowblockI23Flash_fwd_kernel_traitsILi512ELi128ELi32ELi8ELb0ELb1EN7cutlass10bfloat16_tE19Flash_kernel_traitsILi512ELi128ELi32ELi8ES3_EELb0ELb1ELb0ELb0ELb0E16Flash_fwd_paramsLb0EEEvRKT5_iii
    0 bytes stack frame, 7220 bytes spill stores, 7296 bytes spill loads
ptxas info    : Compiling entry function '_Z16flash_fwd_kernelI23Flash_fwd_kernel_traitsILi512ELi128ELi32ELi8ELb0ELb1EN7cutlass10bfloat16_tE19Flash_kernel_traitsILi512ELi128ELi32ELi8ES2_EELb0ELb1ELb0ELb1ELb0EEv16Flash_fwd_params' for 'sm_80'
ptxas info    : Function properties for _Z16flash_fwd_kernelI23Flash_fwd_kernel_traitsILi512ELi128ELi32ELi8ELb0ELb1EN7cutlass10bfloat16_tE19Flash_kernel_traitsILi512ELi128ELi32ELi8ES2_EELb0ELb1ELb0ELb1ELb0EEv16Flash_fwd_params
    1664 bytes stack frame, 7044 bytes spill stores, 7128 bytes spill loads
ptxas info    : Used 255 registers, 584 bytes cmem[0]
ptxas info    : Compiling entry function '_Z16flash_fwd_kernelI23Flash_fwd_kernel_traitsILi512ELi128ELi32ELi8ELb0ELb1EN7cutlass10bfloat16_tE19Flash_kernel_traitsILi512ELi128ELi32ELi8ES2_EELb0ELb1ELb1ELb0ELb0EEv16Flash_fwd_params' for 'sm_80'
ptxas info    : Function properties for _Z16flash_fwd_kernelI23Flash_fwd_kernel_traitsILi512ELi128ELi32ELi8ELb0ELb1EN7cutlass10bfloat16_tE19Flash_kernel_traitsILi512ELi128ELi32ELi8ES2_EELb0ELb1ELb1ELb0ELb0EEv16Flash_fwd_params
    1840 bytes stack frame, 0 bytes spill stores, 0 bytes spill loads
ptxas info    : Used 255 registers, 584 bytes cmem[0]
ptxas info    : Function properties for _ZN5flash22compute_attn_1rowblockI23Flash_fwd_kernel_traitsILi512ELi128ELi32ELi8ELb0ELb1EN7cutlass10bfloat16_tE19Flash_kernel_traitsILi512ELi128ELi32ELi8ES3_EELb0ELb1ELb1ELb0ELb0E16Flash_fwd_paramsLb0EEEvRKT5_iii
    0 bytes stack frame, 7108 bytes spill stores, 7212 bytes spill loads
ptxas info    : Compiling entry function '_Z16flash_fwd_kernelI23Flash_fwd_kernel_traitsILi512ELi128ELi32ELi8ELb0ELb1EN7cutlass10bfloat16_tE19Flash_kernel_traitsILi512ELi128ELi32ELi8ES2_EELb0ELb1ELb1ELb1ELb0EEv16Flash_fwd_params' for 'sm_80'
ptxas info    : Function properties for _Z16flash_fwd_kernelI23Flash_fwd_kernel_traitsILi512ELi128ELi32ELi8ELb0ELb1EN7cutlass10bfloat16_tE19Flash_kernel_traitsILi512ELi128ELi32ELi8ES2_EELb0ELb1ELb1ELb1ELb0EEv16Flash_fwd_params
    1728 bytes stack frame, 7116 bytes spill stores, 7248 bytes spill loads
ptxas info    : Used 255 registers, 584 bytes cmem[0]
ptxas info    : Compiling entry function '_Z16flash_fwd_kernelI23Flash_fwd_kernel_traitsILi512ELi32ELi32ELi2ELb0ELb0EN7cutlass10bfloat16_tE19Flash_kernel_traitsILi512ELi32ELi32ELi2ES2_EELb1ELb0ELb0ELb0ELb0EEv16Flash_fwd_params' for 'sm_80'
ptxas info    : Function properties for _Z16flash_fwd_kernelI23Flash_fwd_kernel_traitsILi512ELi32ELi32ELi2ELb0ELb0EN7cutlass10bfloat16_tE19Flash_kernel_traitsILi512ELi32ELi32ELi2ES2_EELb1ELb0ELb0ELb0ELb0EEv16Flash_fwd_params
    736 bytes stack frame, 1088 bytes spill stores, 1116 bytes spill loads
ptxas info    : Used 255 registers, 584 bytes cmem[0]
ptxas info    : Compiling entry function '_Z16flash_fwd_kernelI23Flash_fwd_kernel_traitsILi512ELi32ELi32ELi2ELb0ELb0EN7cutlass10bfloat16_tE19Flash_kernel_traitsILi512ELi32ELi32ELi2ES2_EELb1ELb0ELb0ELb0ELb1EEv16Flash_fwd_params' for 'sm_80'
ptxas info    : Function properties for _Z16flash_fwd_kernelI23Flash_fwd_kernel_traitsILi512ELi32ELi32ELi2ELb0ELb0EN7cutlass10bfloat16_tE19Flash_kernel_traitsILi512ELi32ELi32ELi2ES2_EELb1ELb0ELb0ELb0ELb1EEv16Flash_fwd_params
    728 bytes stack frame, 1086 bytes spill stores, 1100 bytes spill loads
ptxas info    : Used 255 registers, 584 bytes cmem[0]
ptxas info    : Compiling entry function '_Z16flash_fwd_kernelI23Flash_fwd_kernel_traitsILi512ELi32ELi32ELi2ELb0ELb0EN7cutlass10bfloat16_tE19Flash_kernel_traitsILi512ELi32ELi32ELi2ES2_EELb1ELb0ELb0ELb1ELb0EEv16Flash_fwd_params' for 'sm_80'
ptxas info    : Function properties for _Z16flash_fwd_kernelI23Flash_fwd_kernel_traitsILi512ELi32ELi32ELi2ELb0ELb0EN7cutlass10bfloat16_tE19Flash_kernel_traitsILi512ELi32ELi32ELi2ES2_EELb1ELb0ELb0ELb1ELb0EEv16Flash_fwd_params
    496 bytes stack frame, 792 bytes spill stores, 788 bytes spill loads
ptxas info    : Used 255 registers, 584 bytes cmem[0]
ptxas info    : Compiling entry function '_Z16flash_fwd_kernelI23Flash_fwd_kernel_traitsILi512ELi32ELi32ELi2ELb0ELb0EN7cutlass10bfloat16_tE19Flash_kernel_traitsILi512ELi32ELi32ELi2ES2_EELb1ELb0ELb0ELb1ELb1EEv16Flash_fwd_params' for 'sm_80'
ptxas info    : Function properties for _Z16flash_fwd_kernelI23Flash_fwd_kernel_traitsILi512ELi32ELi32ELi2ELb0ELb0EN7cutlass10bfloat16_tE19Flash_kernel_traitsILi512ELi32ELi32ELi2ES2_EELb1ELb0ELb0ELb1ELb1EEv16Flash_fwd_params
    680 bytes stack frame, 1020 bytes spill stores, 994 bytes spill loads
ptxas info    : Used 255 registers, 584 bytes cmem[0]
ptxas info    : Compiling entry function '_Z16flash_fwd_kernelI23Flash_fwd_kernel_traitsILi512ELi32ELi32ELi2ELb0ELb0EN7cutlass10bfloat16_tE19Flash_kernel_traitsILi512ELi32ELi32ELi2ES2_EELb1ELb0ELb1ELb0ELb0EEv16Flash_fwd_params' for 'sm_80'
ptxas info    : Function properties for _Z16flash_fwd_kernelI23Flash_fwd_kernel_traitsILi512ELi32ELi32ELi2ELb0ELb0EN7cutlass10bfloat16_tE19Flash_kernel_traitsILi512ELi32ELi32ELi2ES2_EELb1ELb0ELb1ELb0ELb0EEv16Flash_fwd_params
    704 bytes stack frame, 1004 bytes spill stores, 1008 bytes spill loads
ptxas info    : Used 255 registers, 584 bytes cmem[0]
ptxas info    : Compiling entry function '_Z16flash_fwd_kernelI23Flash_fwd_kernel_traitsILi512ELi32ELi32ELi2ELb0ELb0EN7cutlass10bfloat16_tE19Flash_kernel_traitsILi512ELi32ELi32ELi2ES2_EELb1ELb0ELb1ELb0ELb1EEv16Flash_fwd_params' for 'sm_80'
ptxas info    : Function properties for _Z16flash_fwd_kernelI23Flash_fwd_kernel_traitsILi512ELi32ELi32ELi2ELb0ELb0EN7cutlass10bfloat16_tE19Flash_kernel_traitsILi512ELi32ELi32ELi2ES2_EELb1ELb0ELb1ELb0ELb1EEv16Flash_fwd_params
    728 bytes stack frame, 1042 bytes spill stores, 1032 bytes spill loads
ptxas info    : Used 255 registers, 584 bytes cmem[0]
ptxas info    : Compiling entry function '_Z16flash_fwd_kernelI23Flash_fwd_kernel_traitsILi512ELi32ELi32ELi2ELb0ELb0EN7cutlass10bfloat16_tE19Flash_kernel_traitsILi512ELi32ELi32ELi2ES2_EELb1ELb0ELb1ELb1ELb0EEv16Flash_fwd_params' for 'sm_80'
ptxas info    : Function properties for _Z16flash_fwd_kernelI23Flash_fwd_kernel_traitsILi512ELi32ELi32ELi2ELb0ELb0EN7cutlass10bfloat16_tE19Flash_kernel_traitsILi512ELi32ELi32ELi2ES2_EELb1ELb0ELb1ELb1ELb0EEv16Flash_fwd_params
    448 bytes stack frame, 720 bytes spill stores, 736 bytes spill loads
ptxas info    : Used 255 registers, 584 bytes cmem[0]
ptxas info    : Compiling entry function '_Z16flash_fwd_kernelI23Flash_fwd_kernel_traitsILi512ELi32ELi32ELi2ELb0ELb0EN7cutlass10bfloat16_tE19Flash_kernel_traitsILi512ELi32ELi32ELi2ES2_EELb1ELb0ELb1ELb1ELb1EEv16Flash_fwd_params' for 'sm_80'
ptxas info    : Function properties for _Z16flash_fwd_kernelI23Flash_fwd_kernel_traitsILi512ELi32ELi32ELi2ELb0ELb0EN7cutlass10bfloat16_tE19Flash_kernel_traitsILi512ELi32ELi32ELi2ES2_EELb1ELb0ELb1ELb1ELb1EEv16Flash_fwd_params
    744 bytes stack frame, 1070 bytes spill stores, 1074 bytes spill loads
ptxas info    : Used 255 registers, 584 bytes cmem[0]
ptxas info    : Compiling entry function '_Z16flash_fwd_kernelI23Flash_fwd_kernel_traitsILi512ELi128ELi32ELi8ELb0ELb1EN7cutlass10bfloat16_tE19Flash_kernel_traitsILi512ELi128ELi32ELi8ES2_EELb1ELb0ELb0ELb0ELb0EEv16Flash_fwd_params' for 'sm_80'
ptxas info    : Function properties for _Z16flash_fwd_kernelI23Flash_fwd_kernel_traitsILi512ELi128ELi32ELi8ELb0ELb1EN7cutlass10bfloat16_tE19Flash_kernel_traitsILi512ELi128ELi32ELi8ES2_EELb1ELb0ELb0ELb0ELb0EEv16Flash_fwd_params
    1976 bytes stack frame, 3124 bytes spill stores, 3120 bytes spill loads
ptxas info    : Used 255 registers, 584 bytes cmem[0]
ptxas info    : Compiling entry function '_Z16flash_fwd_kernelI23Flash_fwd_kernel_traitsILi512ELi128ELi32ELi8ELb0ELb1EN7cutlass10bfloat16_tE19Flash_kernel_traitsILi512ELi128ELi32ELi8ES2_EELb1ELb0ELb0ELb0ELb1EEv16Flash_fwd_params' for 'sm_80'
ptxas info    : Function properties for _Z16flash_fwd_kernelI23Flash_fwd_kernel_traitsILi512ELi128ELi32ELi8ELb0ELb1EN7cutlass10bfloat16_tE19Flash_kernel_traitsILi512ELi128ELi32ELi8ES2_EELb1ELb0ELb0ELb0ELb1EEv16Flash_fwd_params
    1904 bytes stack frame, 2728 bytes spill stores, 2736 bytes spill loads
ptxas info    : Used 255 registers, 584 bytes cmem[0]
ptxas info    : Compiling entry function '_Z16flash_fwd_kernelI23Flash_fwd_kernel_traitsILi512ELi128ELi32ELi8ELb0ELb1EN7cutlass10bfloat16_tE19Flash_kernel_traitsILi512ELi128ELi32ELi8ES2_EELb1ELb0ELb0ELb1ELb0EEv16Flash_fwd_params' for 'sm_80'
ptxas info    : Function properties for _Z16flash_fwd_kernelI23Flash_fwd_kernel_traitsILi512ELi128ELi32ELi8ELb0ELb1EN7cutlass10bfloat16_tE19Flash_kernel_traitsILi512ELi128ELi32ELi8ES2_EELb1ELb0ELb0ELb1ELb0EEv16Flash_fwd_params
    1928 bytes stack frame, 3032 bytes spill stores, 3008 bytes spill loads
ptxas info    : Used 255 registers, 584 bytes cmem[0]
ptxas info    : Compiling entry function '_Z16flash_fwd_kernelI23Flash_fwd_kernel_traitsILi512ELi128ELi32ELi8ELb0ELb1EN7cutlass10bfloat16_tE19Flash_kernel_traitsILi512ELi128ELi32ELi8ES2_EELb1ELb0ELb0ELb1ELb1EEv16Flash_fwd_params' for 'sm_80'
ptxas info    : Function properties for _Z16flash_fwd_kernelI23Flash_fwd_kernel_traitsILi512ELi128ELi32ELi8ELb0ELb1EN7cutlass10bfloat16_tE19Flash_kernel_traitsILi512ELi128ELi32ELi8ES2_EELb1ELb0ELb0ELb1ELb1EEv16Flash_fwd_params
    1976 bytes stack frame, 2716 bytes spill stores, 2674 bytes spill loads
ptxas info    : Used 255 registers, 584 bytes cmem[0]
ptxas info    : Compiling entry function '_Z16flash_fwd_kernelI23Flash_fwd_kernel_traitsILi512ELi128ELi32ELi8ELb0ELb1EN7cutlass10bfloat16_tE19Flash_kernel_traitsILi512ELi128ELi32ELi8ES2_EELb1ELb0ELb1ELb0ELb0EEv16Flash_fwd_params' for 'sm_80'
ptxas info    : Function properties for _Z16flash_fwd_kernelI23Flash_fwd_kernel_traitsILi512ELi128ELi32ELi8ELb0ELb1EN7cutlass10bfloat16_tE19Flash_kernel_traitsILi512ELi128ELi32ELi8ES2_EELb1ELb0ELb1ELb0ELb0EEv16Flash_fwd_params
    1736 bytes stack frame, 2436 bytes spill stores, 2428 bytes spill loads
ptxas info    : Used 255 registers, 584 bytes cmem[0]
ptxas info    : Compiling entry function '_Z16flash_fwd_kernelI23Flash_fwd_kernel_traitsILi512ELi128ELi32ELi8ELb0ELb1EN7cutlass10bfloat16_tE19Flash_kernel_traitsILi512ELi128ELi32ELi8ES2_EELb1ELb0ELb1ELb0ELb1EEv16Flash_fwd_params' for 'sm_80'
ptxas info    : Function properties for _Z16flash_fwd_kernelI23Flash_fwd_kernel_traitsILi512ELi128ELi32ELi8ELb0ELb1EN7cutlass10bfloat16_tE19Flash_kernel_traitsILi512ELi128ELi32ELi8ES2_EELb1ELb0ELb1ELb0ELb1EEv16Flash_fwd_params
    1856 bytes stack frame, 2520 bytes spill stores, 2492 bytes spill loads
ptxas info    : Used 255 registers, 584 bytes cmem[0]
ptxas info    : Compiling entry function '_Z16flash_fwd_kernelI23Flash_fwd_kernel_traitsILi512ELi128ELi32ELi8ELb0ELb1EN7cutlass10bfloat16_tE19Flash_kernel_traitsILi512ELi128ELi32ELi8ES2_EELb1ELb0ELb1ELb1ELb0EEv16Flash_fwd_params' for 'sm_80'
ptxas info    : Function properties for _Z16flash_fwd_kernelI23Flash_fwd_kernel_traitsILi512ELi128ELi32ELi8ELb0ELb1EN7cutlass10bfloat16_tE19Flash_kernel_traitsILi512ELi128ELi32ELi8ES2_EELb1ELb0ELb1ELb1ELb0EEv16Flash_fwd_params
    1480 bytes stack frame, 2392 bytes spill stores, 2376 bytes spill loads
ptxas info    : Used 255 registers, 584 bytes cmem[0]
ptxas info    : Compiling entry function '_Z16flash_fwd_kernelI23Flash_fwd_kernel_traitsILi512ELi128ELi32ELi8ELb0ELb1EN7cutlass10bfloat16_tE19Flash_kernel_traitsILi512ELi128ELi32ELi8ES2_EELb1ELb0ELb1ELb1ELb1EEv16Flash_fwd_params' for 'sm_80'
ptxas info    : Function properties for _Z16flash_fwd_kernelI23Flash_fwd_kernel_traitsILi512ELi128ELi32ELi8ELb0ELb1EN7cutlass10bfloat16_tE19Flash_kernel_traitsILi512ELi128ELi32ELi8ES2_EELb1ELb0ELb1ELb1ELb1EEv16Flash_fwd_params
    2008 bytes stack frame, 2780 bytes spill stores, 2742 bytes spill loads
ptxas info    : Used 255 registers, 584 bytes cmem[0]
ptxas info    : Compiling entry function '_Z16flash_fwd_kernelI23Flash_fwd_kernel_traitsILi512ELi32ELi32ELi2ELb0ELb0EN7cutlass10bfloat16_tE19Flash_kernel_traitsILi512ELi32ELi32ELi2ES2_EELb1ELb1ELb0ELb0ELb0EEv16Flash_fwd_params' for 'sm_80'
ptxas info    : Function properties for _Z16flash_fwd_kernelI23Flash_fwd_kernel_traitsILi512ELi32ELi32ELi2ELb0ELb0EN7cutlass10bfloat16_tE19Flash_kernel_traitsILi512ELi32ELi32ELi2ES2_EELb1ELb1ELb0ELb0ELb0EEv16Flash_fwd_params
    736 bytes stack frame, 1092 bytes spill stores, 1120 bytes spill loads
ptxas info    : Used 255 registers, 584 bytes cmem[0]
ptxas info    : Compiling entry function '_Z16flash_fwd_kernelI23Flash_fwd_kernel_traitsILi512ELi32ELi32ELi2ELb0ELb0EN7cutlass10bfloat16_tE19Flash_kernel_traitsILi512ELi32ELi32ELi2ES2_EELb1ELb1ELb0ELb0ELb1EEv16Flash_fwd_params' for 'sm_80'
ptxas info    : Function properties for _Z16flash_fwd_kernelI23Flash_fwd_kernel_traitsILi512ELi32ELi32ELi2ELb0ELb0EN7cutlass10bfloat16_tE19Flash_kernel_traitsILi512ELi32ELi32ELi2ES2_EELb1ELb1ELb0ELb0ELb1EEv16Flash_fwd_params
    744 bytes stack frame, 1122 bytes spill stores, 1124 bytes spill loads
ptxas info    : Used 255 registers, 584 bytes cmem[0]
ptxas info    : Compiling entry function '_Z16flash_fwd_kernelI23Flash_fwd_kernel_traitsILi512ELi32ELi32ELi2ELb0ELb0EN7cutlass10bfloat16_tE19Flash_kernel_traitsILi512ELi32ELi32ELi2ES2_EELb1ELb1ELb0ELb1ELb0EEv16Flash_fwd_params' for 'sm_80'
ptxas info    : Function properties for _Z16flash_fwd_kernelI23Flash_fwd_kernel_traitsILi512ELi32ELi32ELi2ELb0ELb0EN7cutlass10bfloat16_tE19Flash_kernel_traitsILi512ELi32ELi32ELi2ES2_EELb1ELb1ELb0ELb1ELb0EEv16Flash_fwd_params
    504 bytes stack frame, 796 bytes spill stores, 788 bytes spill loads
ptxas info    : Used 255 registers, 584 bytes cmem[0]
ptxas info    : Compiling entry function '_Z16flash_fwd_kernelI23Flash_fwd_kernel_traitsILi512ELi32ELi32ELi2ELb0ELb0EN7cutlass10bfloat16_tE19Flash_kernel_traitsILi512ELi32ELi32ELi2ES2_EELb1ELb1ELb0ELb1ELb1EEv16Flash_fwd_params' for 'sm_80'
ptxas info    : Function properties for _Z16flash_fwd_kernelI23Flash_fwd_kernel_traitsILi512ELi32ELi32ELi2ELb0ELb0EN7cutlass10bfloat16_tE19Flash_kernel_traitsILi512ELi32ELi32ELi2ES2_EELb1ELb1ELb0ELb1ELb1EEv16Flash_fwd_params
    688 bytes stack frame, 1024 bytes spill stores, 998 bytes spill loads
ptxas info    : Used 255 registers, 584 bytes cmem[0]
ptxas info    : Compiling entry function '_Z16flash_fwd_kernelI23Flash_fwd_kernel_traitsILi512ELi32ELi32ELi2ELb0ELb0EN7cutlass10bfloat16_tE19Flash_kernel_traitsILi512ELi32ELi32ELi2ES2_EELb1ELb1ELb1ELb0ELb0EEv16Flash_fwd_params' for 'sm_80'
ptxas info    : Function properties for _Z16flash_fwd_kernelI23Flash_fwd_kernel_traitsILi512ELi32ELi32ELi2ELb0ELb0EN7cutlass10bfloat16_tE19Flash_kernel_traitsILi512ELi32ELi32ELi2ES2_EELb1ELb1ELb1ELb0ELb0EEv16Flash_fwd_params
    712 bytes stack frame, 1060 bytes spill stores, 1072 bytes spill loads
ptxas info    : Used 255 registers, 584 bytes cmem[0]
ptxas info    : Compiling entry function '_Z16flash_fwd_kernelI23Flash_fwd_kernel_traitsILi512ELi32ELi32ELi2ELb0ELb0EN7cutlass10bfloat16_tE19Flash_kernel_traitsILi512ELi32ELi32ELi2ES2_EELb1ELb1ELb1ELb0ELb1EEv16Flash_fwd_params' for 'sm_80'
ptxas info    : Function properties for _Z16flash_fwd_kernelI23Flash_fwd_kernel_traitsILi512ELi32ELi32ELi2ELb0ELb0EN7cutlass10bfloat16_tE19Flash_kernel_traitsILi512ELi32ELi32ELi2ES2_EELb1ELb1ELb1ELb0ELb1EEv16Flash_fwd_params
    736 bytes stack frame, 1106 bytes spill stores, 1104 bytes spill loads
ptxas info    : Used 255 registers, 584 bytes cmem[0]
ptxas info    : Compiling entry function '_Z16flash_fwd_kernelI23Flash_fwd_kernel_traitsILi512ELi32ELi32ELi2ELb0ELb0EN7cutlass10bfloat16_tE19Flash_kernel_traitsILi512ELi32ELi32ELi2ES2_EELb1ELb1ELb1ELb1ELb0EEv16Flash_fwd_params' for 'sm_80'
ptxas info    : Function properties for _Z16flash_fwd_kernelI23Flash_fwd_kernel_traitsILi512ELi32ELi32ELi2ELb0ELb0EN7cutlass10bfloat16_tE19Flash_kernel_traitsILi512ELi32ELi32ELi2ES2_EELb1ELb1ELb1ELb1ELb0EEv16Flash_fwd_params
    456 bytes stack frame, 728 bytes spill stores, 744 bytes spill loads
ptxas info    : Used 255 registers, 584 bytes cmem[0]
ptxas info    : Compiling entry function '_Z16flash_fwd_kernelI23Flash_fwd_kernel_traitsILi512ELi32ELi32ELi2ELb0ELb0EN7cutlass10bfloat16_tE19Flash_kernel_traitsILi512ELi32ELi32ELi2ES2_EELb1ELb1ELb1ELb1ELb1EEv16Flash_fwd_params' for 'sm_80'
ptxas info    : Function properties for _Z16flash_fwd_kernelI23Flash_fwd_kernel_traitsILi512ELi32ELi32ELi2ELb0ELb0EN7cutlass10bfloat16_tE19Flash_kernel_traitsILi512ELi32ELi32ELi2ES2_EELb1ELb1ELb1ELb1ELb1EEv16Flash_fwd_params
    744 bytes stack frame, 1074 bytes spill stores, 1078 bytes spill loads
ptxas info    : Used 255 registers, 584 bytes cmem[0]
ptxas info    : Compiling entry function '_Z16flash_fwd_kernelI23Flash_fwd_kernel_traitsILi512ELi128ELi32ELi8ELb0ELb1EN7cutlass10bfloat16_tE19Flash_kernel_traitsILi512ELi128ELi32ELi8ES2_EELb1ELb1ELb0ELb0ELb0EEv16Flash_fwd_params' for 'sm_80'
ptxas info    : Function properties for _Z16flash_fwd_kernelI23Flash_fwd_kernel_traitsILi512ELi128ELi32ELi8ELb0ELb1EN7cutlass10bfloat16_tE19Flash_kernel_traitsILi512ELi128ELi32ELi8ES2_EELb1ELb1ELb0ELb0ELb0EEv16Flash_fwd_params
    2368 bytes stack frame, 0 bytes spill stores, 0 bytes spill loads
ptxas info    : Used 255 registers, 584 bytes cmem[0]
ptxas info    : Function properties for _ZN5flash22compute_attn_1rowblockI23Flash_fwd_kernel_traitsILi512ELi128ELi32ELi8ELb0ELb1EN7cutlass10bfloat16_tE19Flash_kernel_traitsILi512ELi128ELi32ELi8ES3_EELb1ELb1ELb0ELb0ELb0E16Flash_fwd_paramsLb0EEEvRKT5_iii
    0 bytes stack frame, 7872 bytes spill stores, 8204 bytes spill loads
ptxas info    : Compiling entry function '_Z16flash_fwd_kernelI23Flash_fwd_kernel_traitsILi512ELi128ELi32ELi8ELb0ELb1EN7cutlass10bfloat16_tE19Flash_kernel_traitsILi512ELi128ELi32ELi8ES2_EELb1ELb1ELb0ELb0ELb1EEv16Flash_fwd_params' for 'sm_80'
ptxas info    : Function properties for _Z16flash_fwd_kernelI23Flash_fwd_kernel_traitsILi512ELi128ELi32ELi8ELb0ELb1EN7cutlass10bfloat16_tE19Flash_kernel_traitsILi512ELi128ELi32ELi8ES2_EELb1ELb1ELb0ELb0ELb1EEv16Flash_fwd_params
    2408 bytes stack frame, 0 bytes spill stores, 0 bytes spill loads
ptxas info    : Used 255 registers, 584 bytes cmem[0]
ptxas info    : Function properties for _ZN5flash22compute_attn_1rowblockI23Flash_fwd_kernel_traitsILi512ELi128ELi32ELi8ELb0ELb1EN7cutlass10bfloat16_tE19Flash_kernel_traitsILi512ELi128ELi32ELi8ES3_EELb1ELb1ELb0ELb0ELb1E16Flash_fwd_paramsLb0EEEvRKT5_iii
    0 bytes stack frame, 7688 bytes spill stores, 8112 bytes spill loads
ptxas info    : Compiling entry function '_Z16flash_fwd_kernelI23Flash_fwd_kernel_traitsILi512ELi128ELi32ELi8ELb0ELb1EN7cutlass10bfloat16_tE19Flash_kernel_traitsILi512ELi128ELi32ELi8ES2_EELb1ELb1ELb0ELb1ELb0EEv16Flash_fwd_params' for 'sm_80'
ptxas info    : Function properties for _Z16flash_fwd_kernelI23Flash_fwd_kernel_traitsILi512ELi128ELi32ELi8ELb0ELb1EN7cutlass10bfloat16_tE19Flash_kernel_traitsILi512ELi128ELi32ELi8ES2_EELb1ELb1ELb0ELb1ELb0EEv16Flash_fwd_params
    2008 bytes stack frame, 7580 bytes spill stores, 7716 bytes spill loads
ptxas info    : Used 255 registers, 584 bytes cmem[0]
ptxas info    : Compiling entry function '_Z16flash_fwd_kernelI23Flash_fwd_kernel_traitsILi512ELi128ELi32ELi8ELb0ELb1EN7cutlass10bfloat16_tE19Flash_kernel_traitsILi512ELi128ELi32ELi8ES2_EELb1ELb1ELb0ELb1ELb1EEv16Flash_fwd_params' for 'sm_80'
ptxas info    : Function properties for _Z16flash_fwd_kernelI23Flash_fwd_kernel_traitsILi512ELi128ELi32ELi8ELb0ELb1EN7cutlass10bfloat16_tE19Flash_kernel_traitsILi512ELi128ELi32ELi8ES2_EELb1ELb1ELb0ELb1ELb1EEv16Flash_fwd_params
    2264 bytes stack frame, 0 bytes spill stores, 0 bytes spill loads
ptxas info    : Used 255 registers, 584 bytes cmem[0]
ptxas info    : Function properties for _ZN5flash22compute_attn_1rowblockI23Flash_fwd_kernel_traitsILi512ELi128ELi32ELi8ELb0ELb1EN7cutlass10bfloat16_tE19Flash_kernel_traitsILi512ELi128ELi32ELi8ES3_EELb1ELb1ELb0ELb1ELb1E16Flash_fwd_paramsLb0EEEvRKT5_iii
    0 bytes stack frame, 7472 bytes spill stores, 7772 bytes spill loads
ptxas info    : Compiling entry function '_Z16flash_fwd_kernelI23Flash_fwd_kernel_traitsILi512ELi128ELi32ELi8ELb0ELb1EN7cutlass10bfloat16_tE19Flash_kernel_traitsILi512ELi128ELi32ELi8ES2_EELb1ELb1ELb1ELb0ELb0EEv16Flash_fwd_params' for 'sm_80'
ptxas info    : Function properties for _Z16flash_fwd_kernelI23Flash_fwd_kernel_traitsILi512ELi128ELi32ELi8ELb0ELb1EN7cutlass10bfloat16_tE19Flash_kernel_traitsILi512ELi128ELi32ELi8ES2_EELb1ELb1ELb1ELb0ELb0EEv16Flash_fwd_params
    2368 bytes stack frame, 0 bytes spill stores, 0 bytes spill loads
ptxas info    : Used 255 registers, 584 bytes cmem[0]
ptxas info    : Function properties for _ZN5flash22compute_attn_1rowblockI23Flash_fwd_kernel_traitsILi512ELi128ELi32ELi8ELb0ELb1EN7cutlass10bfloat16_tE19Flash_kernel_traitsILi512ELi128ELi32ELi8ES3_EELb1ELb1ELb1ELb0ELb0E16Flash_fwd_paramsLb0EEEvRKT5_iii
    0 bytes stack frame, 7968 bytes spill stores, 8284 bytes spill loads
ptxas info    : Compiling entry function '_Z16flash_fwd_kernelI23Flash_fwd_kernel_traitsILi512ELi128ELi32ELi8ELb0ELb1EN7cutlass10bfloat16_tE19Flash_kernel_traitsILi512ELi128ELi32ELi8ES2_EELb1ELb1ELb1ELb0ELb1EEv16Flash_fwd_params' for 'sm_80'
ptxas info    : Function properties for _Z16flash_fwd_kernelI23Flash_fwd_kernel_traitsILi512ELi128ELi32ELi8ELb0ELb1EN7cutlass10bfloat16_tE19Flash_kernel_traitsILi512ELi128ELi32ELi8ES2_EELb1ELb1ELb1ELb0ELb1EEv16Flash_fwd_params
    2296 bytes stack frame, 0 bytes spill stores, 0 bytes spill loads
ptxas info    : Used 255 registers, 584 bytes cmem[0]
ptxas info    : Function properties for _ZN5flash22compute_attn_1rowblockI23Flash_fwd_kernel_traitsILi512ELi128ELi32ELi8ELb0ELb1EN7cutlass10bfloat16_tE19Flash_kernel_traitsILi512ELi128ELi32ELi8ES3_EELb1ELb1ELb1ELb0ELb1E16Flash_fwd_paramsLb0EEEvRKT5_iii
    0 bytes stack frame, 7670 bytes spill stores, 8022 bytes spill loads
ptxas info    : Compiling entry function '_Z16flash_fwd_kernelI23Flash_fwd_kernel_traitsILi512ELi128ELi32ELi8ELb0ELb1EN7cutlass10bfloat16_tE19Flash_kernel_traitsILi512ELi128ELi32ELi8ES2_EELb1ELb1ELb1ELb1ELb0EEv16Flash_fwd_params' for 'sm_80'
ptxas info    : Function properties for _Z16flash_fwd_kernelI23Flash_fwd_kernel_traitsILi512ELi128ELi32ELi8ELb0ELb1EN7cutlass10bfloat16_tE19Flash_kernel_traitsILi512ELi128ELi32ELi8ES2_EELb1ELb1ELb1ELb1ELb0EEv16Flash_fwd_params
    2072 bytes stack frame, 7792 bytes spill stores, 7972 bytes spill loads
ptxas info    : Used 255 registers, 584 bytes cmem[0]
ptxas info    : Compiling entry function '_Z16flash_fwd_kernelI23Flash_fwd_kernel_traitsILi512ELi128ELi32ELi8ELb0ELb1EN7cutlass10bfloat16_tE19Flash_kernel_traitsILi512ELi128ELi32ELi8ES2_EELb1ELb1ELb1ELb1ELb1EEv16Flash_fwd_params' for 'sm_80'
ptxas info    : Function properties for _Z16flash_fwd_kernelI23Flash_fwd_kernel_traitsILi512ELi128ELi32ELi8ELb0ELb1EN7cutlass10bfloat16_tE19Flash_kernel_traitsILi512ELi128ELi32ELi8ES2_EELb1ELb1ELb1ELb1ELb1EEv16Flash_fwd_params
    1840 bytes stack frame, 7108 bytes spill stores, 7224 bytes spill loads
ptxas info    : Used 255 registers, 584 bytes cmem[0]
/anaconda/envs/flashattention/lib/python3.9/site-packages/torch/include/c10/core/SymInt.h(84): warning: integer conversion resulted in a change of sign

[37/37] /usr/local/cuda-11.4/bin/nvcc  -I/home/v-feiychen/flash-attention/csrc/flash_attn -I/home/v-feiychen/flash-attention/csrc/flash_attn/src -I/home/v-feiychen/flash-attention/csrc/cutlass/include -I/anaconda/envs/flashattention/lib/python3.9/site-packages/torch/include -I/anaconda/envs/flashattention/lib/python3.9/site-packages/torch/include/torch/csrc/api/include -I/anaconda/envs/flashattention/lib/python3.9/site-packages/torch/include/TH -I/anaconda/envs/flashattention/lib/python3.9/site-packages/torch/include/THC -I/usr/local/cuda-11.4/include -I/anaconda/envs/flashattention/include/python3.9 -c -c /home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_hdim512_fp16_sm80.cu -o /home/v-feiychen/flash-attention/build/temp.linux-x86_64-cpython-39/csrc/flash_attn/src/flash_fwd_hdim512_fp16_sm80.o -D__CUDA_NO_HALF_OPERATORS__ -D__CUDA_NO_HALF_CONVERSIONS__ -D__CUDA_NO_BFLOAT16_CONVERSIONS__ -D__CUDA_NO_HALF2_OPERATORS__ --expt-relaxed-constexpr --compiler-options ''"'"'-fPIC'"'"'' -O3 -std=c++17 -U__CUDA_NO_HALF_OPERATORS__ -U__CUDA_NO_HALF_CONVERSIONS__ -U__CUDA_NO_HALF2_OPERATORS__ -U__CUDA_NO_BFLOAT16_CONVERSIONS__ --expt-relaxed-constexpr --expt-extended-lambda --use_fast_math --ptxas-options=-v -lineinfo -gencode arch=compute_80,code=sm_80 --threads 4 -DTORCH_API_INCLUDE_EXTENSION_H '-DPYBIND11_COMPILER_TYPE="_gcc"' '-DPYBIND11_STDLIB="_libstdcpp"' '-DPYBIND11_BUILD_ABI="_cxxabi1011"' -DTORCH_EXTENSION_NAME=flash_attn_2_cuda -D_GLIBCXX_USE_CXX11_ABI=0
/anaconda/envs/flashattention/lib/python3.9/site-packages/torch/include/c10/core/SymInt.h(84): warning: integer conversion resulted in a change of sign

/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_kernel.h(140): warning: variable "MMA_M" was declared but never referenced
          detected during:
            instantiation of "void flash::compute_attn_1rowblock<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params,Is_Q_split_K>(const Params &, int, int, int) [with Kernel_traits=Flash_fwd_kernel_traits<512, 128, 32, 8, false, true, cutlass::half_t, Flash_kernel_traits<512, 128, 32, 8, cutlass::half_t>>, Is_dropout=true, Is_causal=true, Is_even_N=true, Is_even_K=true, Return_softmax=true, Params=Flash_fwd_params, Is_Q_split_K=false]" 
(589): here
            instantiation of "void flash::compute_attn<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &) [with Kernel_traits=Flash_fwd_kernel_traits<512, 128, 32, 8, false, true, cutlass::half_t, Flash_kernel_traits<512, 128, 32, 8, cutlass::half_t>>, Is_dropout=true, Is_causal=true, Is_even_N=true, Is_even_K=true, Return_softmax=true, Params=Flash_fwd_params]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(15): here
            instantiation of "void flash_fwd_kernel<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax>(Flash_fwd_params) [with Kernel_traits=Flash_fwd_kernel_traits<512, 128, 32, 8, false, true, cutlass::half_t, Flash_kernel_traits<512, 128, 32, 8, cutlass::half_t>>, Is_dropout=true, Is_causal=true, Is_even_N=true, Is_even_K=true, Return_softmax=true]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(34): here
            instantiation of "void run_flash_fwd<Kernel_traits,Is_dropout,Is_causal>(Flash_fwd_params &, cudaStream_t) [with Kernel_traits=Flash_fwd_kernel_traits<512, 128, 32, 8, false, true, cutlass::half_t, Flash_kernel_traits<512, 128, 32, 8, cutlass::half_t>>, Is_dropout=true, Is_causal=true]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(264): here
            instantiation of "void run_mha_fwd_hdim512<T>(Flash_fwd_params &, cudaStream_t) [with T=cutlass::half_t]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_hdim512_fp16_sm80.cu(8): here

/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_kernel.h(276): warning: variable "tQrQ" was declared but never referenced
          detected during:
            instantiation of "void flash::compute_attn_1rowblock<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params,Is_Q_split_K>(const Params &, int, int, int) [with Kernel_traits=Flash_fwd_kernel_traits<512, 128, 32, 8, false, true, cutlass::half_t, Flash_kernel_traits<512, 128, 32, 8, cutlass::half_t>>, Is_dropout=true, Is_causal=true, Is_even_N=true, Is_even_K=true, Return_softmax=true, Params=Flash_fwd_params, Is_Q_split_K=false]" 
(589): here
            instantiation of "void flash::compute_attn<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &) [with Kernel_traits=Flash_fwd_kernel_traits<512, 128, 32, 8, false, true, cutlass::half_t, Flash_kernel_traits<512, 128, 32, 8, cutlass::half_t>>, Is_dropout=true, Is_causal=true, Is_even_N=true, Is_even_K=true, Return_softmax=true, Params=Flash_fwd_params]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(15): here
            instantiation of "void flash_fwd_kernel<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax>(Flash_fwd_params) [with Kernel_traits=Flash_fwd_kernel_traits<512, 128, 32, 8, false, true, cutlass::half_t, Flash_kernel_traits<512, 128, 32, 8, cutlass::half_t>>, Is_dropout=true, Is_causal=true, Is_even_N=true, Is_even_K=true, Return_softmax=true]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(34): here
            instantiation of "void run_flash_fwd<Kernel_traits,Is_dropout,Is_causal>(Flash_fwd_params &, cudaStream_t) [with Kernel_traits=Flash_fwd_kernel_traits<512, 128, 32, 8, false, true, cutlass::half_t, Flash_kernel_traits<512, 128, 32, 8, cutlass::half_t>>, Is_dropout=true, Is_causal=true]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(264): here
            instantiation of "void run_mha_fwd_hdim512<T>(Flash_fwd_params &, cudaStream_t) [with T=cutlass::half_t]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_hdim512_fp16_sm80.cu(8): here

/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_kernel.h(140): warning: variable "MMA_M" was declared but never referenced
          detected during:
            instantiation of "void flash::compute_attn_1rowblock<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params,Is_Q_split_K>(const Params &, int, int, int) [with Kernel_traits=Flash_fwd_kernel_traits<512, 128, 32, 8, false, true, cutlass::half_t, Flash_kernel_traits<512, 128, 32, 8, cutlass::half_t>>, Is_dropout=true, Is_causal=true, Is_even_N=true, Is_even_K=true, Return_softmax=false, Params=Flash_fwd_params, Is_Q_split_K=false]" 
(589): here
            instantiation of "void flash::compute_attn<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &) [with Kernel_traits=Flash_fwd_kernel_traits<512, 128, 32, 8, false, true, cutlass::half_t, Flash_kernel_traits<512, 128, 32, 8, cutlass::half_t>>, Is_dropout=true, Is_causal=true, Is_even_N=true, Is_even_K=true, Return_softmax=false, Params=Flash_fwd_params]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(15): here
            instantiation of "void flash_fwd_kernel<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax>(Flash_fwd_params) [with Kernel_traits=Flash_fwd_kernel_traits<512, 128, 32, 8, false, true, cutlass::half_t, Flash_kernel_traits<512, 128, 32, 8, cutlass::half_t>>, Is_dropout=true, Is_causal=true, Is_even_N=true, Is_even_K=true, Return_softmax=false]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(34): here
            instantiation of "void run_flash_fwd<Kernel_traits,Is_dropout,Is_causal>(Flash_fwd_params &, cudaStream_t) [with Kernel_traits=Flash_fwd_kernel_traits<512, 128, 32, 8, false, true, cutlass::half_t, Flash_kernel_traits<512, 128, 32, 8, cutlass::half_t>>, Is_dropout=true, Is_causal=true]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(264): here
            instantiation of "void run_mha_fwd_hdim512<T>(Flash_fwd_params &, cudaStream_t) [with T=cutlass::half_t]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_hdim512_fp16_sm80.cu(8): here

/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_kernel.h(276): warning: variable "tQrQ" was declared but never referenced
          detected during:
            instantiation of "void flash::compute_attn_1rowblock<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params,Is_Q_split_K>(const Params &, int, int, int) [with Kernel_traits=Flash_fwd_kernel_traits<512, 128, 32, 8, false, true, cutlass::half_t, Flash_kernel_traits<512, 128, 32, 8, cutlass::half_t>>, Is_dropout=true, Is_causal=true, Is_even_N=true, Is_even_K=true, Return_softmax=false, Params=Flash_fwd_params, Is_Q_split_K=false]" 
(589): here
            instantiation of "void flash::compute_attn<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &) [with Kernel_traits=Flash_fwd_kernel_traits<512, 128, 32, 8, false, true, cutlass::half_t, Flash_kernel_traits<512, 128, 32, 8, cutlass::half_t>>, Is_dropout=true, Is_causal=true, Is_even_N=true, Is_even_K=true, Return_softmax=false, Params=Flash_fwd_params]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(15): here
            instantiation of "void flash_fwd_kernel<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax>(Flash_fwd_params) [with Kernel_traits=Flash_fwd_kernel_traits<512, 128, 32, 8, false, true, cutlass::half_t, Flash_kernel_traits<512, 128, 32, 8, cutlass::half_t>>, Is_dropout=true, Is_causal=true, Is_even_N=true, Is_even_K=true, Return_softmax=false]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(34): here
            instantiation of "void run_flash_fwd<Kernel_traits,Is_dropout,Is_causal>(Flash_fwd_params &, cudaStream_t) [with Kernel_traits=Flash_fwd_kernel_traits<512, 128, 32, 8, false, true, cutlass::half_t, Flash_kernel_traits<512, 128, 32, 8, cutlass::half_t>>, Is_dropout=true, Is_causal=true]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(264): here
            instantiation of "void run_mha_fwd_hdim512<T>(Flash_fwd_params &, cudaStream_t) [with T=cutlass::half_t]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_hdim512_fp16_sm80.cu(8): here

/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_kernel.h(140): warning: variable "MMA_M" was declared but never referenced
          detected during:
            instantiation of "void flash::compute_attn_1rowblock<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params,Is_Q_split_K>(const Params &, int, int, int) [with Kernel_traits=Flash_fwd_kernel_traits<512, 128, 32, 8, false, true, cutlass::half_t, Flash_kernel_traits<512, 128, 32, 8, cutlass::half_t>>, Is_dropout=true, Is_causal=true, Is_even_N=true, Is_even_K=false, Return_softmax=true, Params=Flash_fwd_params, Is_Q_split_K=false]" 
(589): here
            instantiation of "void flash::compute_attn<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &) [with Kernel_traits=Flash_fwd_kernel_traits<512, 128, 32, 8, false, true, cutlass::half_t, Flash_kernel_traits<512, 128, 32, 8, cutlass::half_t>>, Is_dropout=true, Is_causal=true, Is_even_N=true, Is_even_K=false, Return_softmax=true, Params=Flash_fwd_params]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(15): here
            instantiation of "void flash_fwd_kernel<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax>(Flash_fwd_params) [with Kernel_traits=Flash_fwd_kernel_traits<512, 128, 32, 8, false, true, cutlass::half_t, Flash_kernel_traits<512, 128, 32, 8, cutlass::half_t>>, Is_dropout=true, Is_causal=true, Is_even_N=true, Is_even_K=false, Return_softmax=true]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(34): here
            instantiation of "void run_flash_fwd<Kernel_traits,Is_dropout,Is_causal>(Flash_fwd_params &, cudaStream_t) [with Kernel_traits=Flash_fwd_kernel_traits<512, 128, 32, 8, false, true, cutlass::half_t, Flash_kernel_traits<512, 128, 32, 8, cutlass::half_t>>, Is_dropout=true, Is_causal=true]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(264): here
            instantiation of "void run_mha_fwd_hdim512<T>(Flash_fwd_params &, cudaStream_t) [with T=cutlass::half_t]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_hdim512_fp16_sm80.cu(8): here

/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_kernel.h(276): warning: variable "tQrQ" was declared but never referenced
          detected during:
            instantiation of "void flash::compute_attn_1rowblock<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params,Is_Q_split_K>(const Params &, int, int, int) [with Kernel_traits=Flash_fwd_kernel_traits<512, 128, 32, 8, false, true, cutlass::half_t, Flash_kernel_traits<512, 128, 32, 8, cutlass::half_t>>, Is_dropout=true, Is_causal=true, Is_even_N=true, Is_even_K=false, Return_softmax=true, Params=Flash_fwd_params, Is_Q_split_K=false]" 
(589): here
            instantiation of "void flash::compute_attn<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &) [with Kernel_traits=Flash_fwd_kernel_traits<512, 128, 32, 8, false, true, cutlass::half_t, Flash_kernel_traits<512, 128, 32, 8, cutlass::half_t>>, Is_dropout=true, Is_causal=true, Is_even_N=true, Is_even_K=false, Return_softmax=true, Params=Flash_fwd_params]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(15): here
            instantiation of "void flash_fwd_kernel<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax>(Flash_fwd_params) [with Kernel_traits=Flash_fwd_kernel_traits<512, 128, 32, 8, false, true, cutlass::half_t, Flash_kernel_traits<512, 128, 32, 8, cutlass::half_t>>, Is_dropout=true, Is_causal=true, Is_even_N=true, Is_even_K=false, Return_softmax=true]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(34): here
            instantiation of "void run_flash_fwd<Kernel_traits,Is_dropout,Is_causal>(Flash_fwd_params &, cudaStream_t) [with Kernel_traits=Flash_fwd_kernel_traits<512, 128, 32, 8, false, true, cutlass::half_t, Flash_kernel_traits<512, 128, 32, 8, cutlass::half_t>>, Is_dropout=true, Is_causal=true]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(264): here
            instantiation of "void run_mha_fwd_hdim512<T>(Flash_fwd_params &, cudaStream_t) [with T=cutlass::half_t]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_hdim512_fp16_sm80.cu(8): here

/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_kernel.h(140): warning: variable "MMA_M" was declared but never referenced
          detected during:
            instantiation of "void flash::compute_attn_1rowblock<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params,Is_Q_split_K>(const Params &, int, int, int) [with Kernel_traits=Flash_fwd_kernel_traits<512, 128, 32, 8, false, true, cutlass::half_t, Flash_kernel_traits<512, 128, 32, 8, cutlass::half_t>>, Is_dropout=true, Is_causal=true, Is_even_N=true, Is_even_K=false, Return_softmax=false, Params=Flash_fwd_params, Is_Q_split_K=false]" 
(589): here
            instantiation of "void flash::compute_attn<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &) [with Kernel_traits=Flash_fwd_kernel_traits<512, 128, 32, 8, false, true, cutlass::half_t, Flash_kernel_traits<512, 128, 32, 8, cutlass::half_t>>, Is_dropout=true, Is_causal=true, Is_even_N=true, Is_even_K=false, Return_softmax=false, Params=Flash_fwd_params]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(15): here
            instantiation of "void flash_fwd_kernel<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax>(Flash_fwd_params) [with Kernel_traits=Flash_fwd_kernel_traits<512, 128, 32, 8, false, true, cutlass::half_t, Flash_kernel_traits<512, 128, 32, 8, cutlass::half_t>>, Is_dropout=true, Is_causal=true, Is_even_N=true, Is_even_K=false, Return_softmax=false]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(34): here
            instantiation of "void run_flash_fwd<Kernel_traits,Is_dropout,Is_causal>(Flash_fwd_params &, cudaStream_t) [with Kernel_traits=Flash_fwd_kernel_traits<512, 128, 32, 8, false, true, cutlass::half_t, Flash_kernel_traits<512, 128, 32, 8, cutlass::half_t>>, Is_dropout=true, Is_causal=true]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(264): here
            instantiation of "void run_mha_fwd_hdim512<T>(Flash_fwd_params &, cudaStream_t) [with T=cutlass::half_t]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_hdim512_fp16_sm80.cu(8): here

/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_kernel.h(276): warning: variable "tQrQ" was declared but never referenced
          detected during:
            instantiation of "void flash::compute_attn_1rowblock<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params,Is_Q_split_K>(const Params &, int, int, int) [with Kernel_traits=Flash_fwd_kernel_traits<512, 128, 32, 8, false, true, cutlass::half_t, Flash_kernel_traits<512, 128, 32, 8, cutlass::half_t>>, Is_dropout=true, Is_causal=true, Is_even_N=true, Is_even_K=false, Return_softmax=false, Params=Flash_fwd_params, Is_Q_split_K=false]" 
(589): here
            instantiation of "void flash::compute_attn<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &) [with Kernel_traits=Flash_fwd_kernel_traits<512, 128, 32, 8, false, true, cutlass::half_t, Flash_kernel_traits<512, 128, 32, 8, cutlass::half_t>>, Is_dropout=true, Is_causal=true, Is_even_N=true, Is_even_K=false, Return_softmax=false, Params=Flash_fwd_params]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(15): here
            instantiation of "void flash_fwd_kernel<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax>(Flash_fwd_params) [with Kernel_traits=Flash_fwd_kernel_traits<512, 128, 32, 8, false, true, cutlass::half_t, Flash_kernel_traits<512, 128, 32, 8, cutlass::half_t>>, Is_dropout=true, Is_causal=true, Is_even_N=true, Is_even_K=false, Return_softmax=false]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(34): here
            instantiation of "void run_flash_fwd<Kernel_traits,Is_dropout,Is_causal>(Flash_fwd_params &, cudaStream_t) [with Kernel_traits=Flash_fwd_kernel_traits<512, 128, 32, 8, false, true, cutlass::half_t, Flash_kernel_traits<512, 128, 32, 8, cutlass::half_t>>, Is_dropout=true, Is_causal=true]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(264): here
            instantiation of "void run_mha_fwd_hdim512<T>(Flash_fwd_params &, cudaStream_t) [with T=cutlass::half_t]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_hdim512_fp16_sm80.cu(8): here

/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_kernel.h(140): warning: variable "MMA_M" was declared but never referenced
          detected during:
            instantiation of "void flash::compute_attn_1rowblock<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params,Is_Q_split_K>(const Params &, int, int, int) [with Kernel_traits=Flash_fwd_kernel_traits<512, 128, 32, 8, false, true, cutlass::half_t, Flash_kernel_traits<512, 128, 32, 8, cutlass::half_t>>, Is_dropout=true, Is_causal=true, Is_even_N=false, Is_even_K=true, Return_softmax=true, Params=Flash_fwd_params, Is_Q_split_K=false]" 
(589): here
            instantiation of "void flash::compute_attn<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &) [with Kernel_traits=Flash_fwd_kernel_traits<512, 128, 32, 8, false, true, cutlass::half_t, Flash_kernel_traits<512, 128, 32, 8, cutlass::half_t>>, Is_dropout=true, Is_causal=true, Is_even_N=false, Is_even_K=true, Return_softmax=true, Params=Flash_fwd_params]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(15): here
            instantiation of "void flash_fwd_kernel<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax>(Flash_fwd_params) [with Kernel_traits=Flash_fwd_kernel_traits<512, 128, 32, 8, false, true, cutlass::half_t, Flash_kernel_traits<512, 128, 32, 8, cutlass::half_t>>, Is_dropout=true, Is_causal=true, Is_even_N=false, Is_even_K=true, Return_softmax=true]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(34): here
            instantiation of "void run_flash_fwd<Kernel_traits,Is_dropout,Is_causal>(Flash_fwd_params &, cudaStream_t) [with Kernel_traits=Flash_fwd_kernel_traits<512, 128, 32, 8, false, true, cutlass::half_t, Flash_kernel_traits<512, 128, 32, 8, cutlass::half_t>>, Is_dropout=true, Is_causal=true]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(264): here
            instantiation of "void run_mha_fwd_hdim512<T>(Flash_fwd_params &, cudaStream_t) [with T=cutlass::half_t]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_hdim512_fp16_sm80.cu(8): here

/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_kernel.h(276): warning: variable "tQrQ" was declared but never referenced
          detected during:
            instantiation of "void flash::compute_attn_1rowblock<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params,Is_Q_split_K>(const Params &, int, int, int) [with Kernel_traits=Flash_fwd_kernel_traits<512, 128, 32, 8, false, true, cutlass::half_t, Flash_kernel_traits<512, 128, 32, 8, cutlass::half_t>>, Is_dropout=true, Is_causal=true, Is_even_N=false, Is_even_K=true, Return_softmax=true, Params=Flash_fwd_params, Is_Q_split_K=false]" 
(589): here
            instantiation of "void flash::compute_attn<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &) [with Kernel_traits=Flash_fwd_kernel_traits<512, 128, 32, 8, false, true, cutlass::half_t, Flash_kernel_traits<512, 128, 32, 8, cutlass::half_t>>, Is_dropout=true, Is_causal=true, Is_even_N=false, Is_even_K=true, Return_softmax=true, Params=Flash_fwd_params]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(15): here
            instantiation of "void flash_fwd_kernel<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax>(Flash_fwd_params) [with Kernel_traits=Flash_fwd_kernel_traits<512, 128, 32, 8, false, true, cutlass::half_t, Flash_kernel_traits<512, 128, 32, 8, cutlass::half_t>>, Is_dropout=true, Is_causal=true, Is_even_N=false, Is_even_K=true, Return_softmax=true]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(34): here
            instantiation of "void run_flash_fwd<Kernel_traits,Is_dropout,Is_causal>(Flash_fwd_params &, cudaStream_t) [with Kernel_traits=Flash_fwd_kernel_traits<512, 128, 32, 8, false, true, cutlass::half_t, Flash_kernel_traits<512, 128, 32, 8, cutlass::half_t>>, Is_dropout=true, Is_causal=true]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(264): here
            instantiation of "void run_mha_fwd_hdim512<T>(Flash_fwd_params &, cudaStream_t) [with T=cutlass::half_t]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_hdim512_fp16_sm80.cu(8): here

/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_kernel.h(140): warning: variable "MMA_M" was declared but never referenced
          detected during:
            instantiation of "void flash::compute_attn_1rowblock<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params,Is_Q_split_K>(const Params &, int, int, int) [with Kernel_traits=Flash_fwd_kernel_traits<512, 128, 32, 8, false, true, cutlass::half_t, Flash_kernel_traits<512, 128, 32, 8, cutlass::half_t>>, Is_dropout=true, Is_causal=true, Is_even_N=false, Is_even_K=true, Return_softmax=false, Params=Flash_fwd_params, Is_Q_split_K=false]" 
(589): here
            instantiation of "void flash::compute_attn<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &) [with Kernel_traits=Flash_fwd_kernel_traits<512, 128, 32, 8, false, true, cutlass::half_t, Flash_kernel_traits<512, 128, 32, 8, cutlass::half_t>>, Is_dropout=true, Is_causal=true, Is_even_N=false, Is_even_K=true, Return_softmax=false, Params=Flash_fwd_params]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(15): here
            instantiation of "void flash_fwd_kernel<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax>(Flash_fwd_params) [with Kernel_traits=Flash_fwd_kernel_traits<512, 128, 32, 8, false, true, cutlass::half_t, Flash_kernel_traits<512, 128, 32, 8, cutlass::half_t>>, Is_dropout=true, Is_causal=true, Is_even_N=false, Is_even_K=true, Return_softmax=false]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(34): here
            instantiation of "void run_flash_fwd<Kernel_traits,Is_dropout,Is_causal>(Flash_fwd_params &, cudaStream_t) [with Kernel_traits=Flash_fwd_kernel_traits<512, 128, 32, 8, false, true, cutlass::half_t, Flash_kernel_traits<512, 128, 32, 8, cutlass::half_t>>, Is_dropout=true, Is_causal=true]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(264): here
            instantiation of "void run_mha_fwd_hdim512<T>(Flash_fwd_params &, cudaStream_t) [with T=cutlass::half_t]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_hdim512_fp16_sm80.cu(8): here

/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_kernel.h(276): warning: variable "tQrQ" was declared but never referenced
          detected during:
            instantiation of "void flash::compute_attn_1rowblock<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params,Is_Q_split_K>(const Params &, int, int, int) [with Kernel_traits=Flash_fwd_kernel_traits<512, 128, 32, 8, false, true, cutlass::half_t, Flash_kernel_traits<512, 128, 32, 8, cutlass::half_t>>, Is_dropout=true, Is_causal=true, Is_even_N=false, Is_even_K=true, Return_softmax=false, Params=Flash_fwd_params, Is_Q_split_K=false]" 
(589): here
            instantiation of "void flash::compute_attn<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &) [with Kernel_traits=Flash_fwd_kernel_traits<512, 128, 32, 8, false, true, cutlass::half_t, Flash_kernel_traits<512, 128, 32, 8, cutlass::half_t>>, Is_dropout=true, Is_causal=true, Is_even_N=false, Is_even_K=true, Return_softmax=false, Params=Flash_fwd_params]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(15): here
            instantiation of "void flash_fwd_kernel<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax>(Flash_fwd_params) [with Kernel_traits=Flash_fwd_kernel_traits<512, 128, 32, 8, false, true, cutlass::half_t, Flash_kernel_traits<512, 128, 32, 8, cutlass::half_t>>, Is_dropout=true, Is_causal=true, Is_even_N=false, Is_even_K=true, Return_softmax=false]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(34): here
            instantiation of "void run_flash_fwd<Kernel_traits,Is_dropout,Is_causal>(Flash_fwd_params &, cudaStream_t) [with Kernel_traits=Flash_fwd_kernel_traits<512, 128, 32, 8, false, true, cutlass::half_t, Flash_kernel_traits<512, 128, 32, 8, cutlass::half_t>>, Is_dropout=true, Is_causal=true]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(264): here
            instantiation of "void run_mha_fwd_hdim512<T>(Flash_fwd_params &, cudaStream_t) [with T=cutlass::half_t]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_hdim512_fp16_sm80.cu(8): here

/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_kernel.h(140): warning: variable "MMA_M" was declared but never referenced
          detected during:
            instantiation of "void flash::compute_attn_1rowblock<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params,Is_Q_split_K>(const Params &, int, int, int) [with Kernel_traits=Flash_fwd_kernel_traits<512, 128, 32, 8, false, true, cutlass::half_t, Flash_kernel_traits<512, 128, 32, 8, cutlass::half_t>>, Is_dropout=true, Is_causal=true, Is_even_N=false, Is_even_K=false, Return_softmax=true, Params=Flash_fwd_params, Is_Q_split_K=false]" 
(589): here
            instantiation of "void flash::compute_attn<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &) [with Kernel_traits=Flash_fwd_kernel_traits<512, 128, 32, 8, false, true, cutlass::half_t, Flash_kernel_traits<512, 128, 32, 8, cutlass::half_t>>, Is_dropout=true, Is_causal=true, Is_even_N=false, Is_even_K=false, Return_softmax=true, Params=Flash_fwd_params]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(15): here
            instantiation of "void flash_fwd_kernel<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax>(Flash_fwd_params) [with Kernel_traits=Flash_fwd_kernel_traits<512, 128, 32, 8, false, true, cutlass::half_t, Flash_kernel_traits<512, 128, 32, 8, cutlass::half_t>>, Is_dropout=true, Is_causal=true, Is_even_N=false, Is_even_K=false, Return_softmax=true]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(34): here
            instantiation of "void run_flash_fwd<Kernel_traits,Is_dropout,Is_causal>(Flash_fwd_params &, cudaStream_t) [with Kernel_traits=Flash_fwd_kernel_traits<512, 128, 32, 8, false, true, cutlass::half_t, Flash_kernel_traits<512, 128, 32, 8, cutlass::half_t>>, Is_dropout=true, Is_causal=true]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(264): here
            instantiation of "void run_mha_fwd_hdim512<T>(Flash_fwd_params &, cudaStream_t) [with T=cutlass::half_t]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_hdim512_fp16_sm80.cu(8): here

/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_kernel.h(276): warning: variable "tQrQ" was declared but never referenced
          detected during:
            instantiation of "void flash::compute_attn_1rowblock<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params,Is_Q_split_K>(const Params &, int, int, int) [with Kernel_traits=Flash_fwd_kernel_traits<512, 128, 32, 8, false, true, cutlass::half_t, Flash_kernel_traits<512, 128, 32, 8, cutlass::half_t>>, Is_dropout=true, Is_causal=true, Is_even_N=false, Is_even_K=false, Return_softmax=true, Params=Flash_fwd_params, Is_Q_split_K=false]" 
(589): here
            instantiation of "void flash::compute_attn<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &) [with Kernel_traits=Flash_fwd_kernel_traits<512, 128, 32, 8, false, true, cutlass::half_t, Flash_kernel_traits<512, 128, 32, 8, cutlass::half_t>>, Is_dropout=true, Is_causal=true, Is_even_N=false, Is_even_K=false, Return_softmax=true, Params=Flash_fwd_params]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(15): here
            instantiation of "void flash_fwd_kernel<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax>(Flash_fwd_params) [with Kernel_traits=Flash_fwd_kernel_traits<512, 128, 32, 8, false, true, cutlass::half_t, Flash_kernel_traits<512, 128, 32, 8, cutlass::half_t>>, Is_dropout=true, Is_causal=true, Is_even_N=false, Is_even_K=false, Return_softmax=true]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(34): here
            instantiation of "void run_flash_fwd<Kernel_traits,Is_dropout,Is_causal>(Flash_fwd_params &, cudaStream_t) [with Kernel_traits=Flash_fwd_kernel_traits<512, 128, 32, 8, false, true, cutlass::half_t, Flash_kernel_traits<512, 128, 32, 8, cutlass::half_t>>, Is_dropout=true, Is_causal=true]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(264): here
            instantiation of "void run_mha_fwd_hdim512<T>(Flash_fwd_params &, cudaStream_t) [with T=cutlass::half_t]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_hdim512_fp16_sm80.cu(8): here

/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_kernel.h(140): warning: variable "MMA_M" was declared but never referenced
          detected during:
            instantiation of "void flash::compute_attn_1rowblock<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params,Is_Q_split_K>(const Params &, int, int, int) [with Kernel_traits=Flash_fwd_kernel_traits<512, 128, 32, 8, false, true, cutlass::half_t, Flash_kernel_traits<512, 128, 32, 8, cutlass::half_t>>, Is_dropout=true, Is_causal=true, Is_even_N=false, Is_even_K=false, Return_softmax=false, Params=Flash_fwd_params, Is_Q_split_K=false]" 
(589): here
            instantiation of "void flash::compute_attn<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &) [with Kernel_traits=Flash_fwd_kernel_traits<512, 128, 32, 8, false, true, cutlass::half_t, Flash_kernel_traits<512, 128, 32, 8, cutlass::half_t>>, Is_dropout=true, Is_causal=true, Is_even_N=false, Is_even_K=false, Return_softmax=false, Params=Flash_fwd_params]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(15): here
            instantiation of "void flash_fwd_kernel<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax>(Flash_fwd_params) [with Kernel_traits=Flash_fwd_kernel_traits<512, 128, 32, 8, false, true, cutlass::half_t, Flash_kernel_traits<512, 128, 32, 8, cutlass::half_t>>, Is_dropout=true, Is_causal=true, Is_even_N=false, Is_even_K=false, Return_softmax=false]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(34): here
            instantiation of "void run_flash_fwd<Kernel_traits,Is_dropout,Is_causal>(Flash_fwd_params &, cudaStream_t) [with Kernel_traits=Flash_fwd_kernel_traits<512, 128, 32, 8, false, true, cutlass::half_t, Flash_kernel_traits<512, 128, 32, 8, cutlass::half_t>>, Is_dropout=true, Is_causal=true]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(264): here
            instantiation of "void run_mha_fwd_hdim512<T>(Flash_fwd_params &, cudaStream_t) [with T=cutlass::half_t]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_hdim512_fp16_sm80.cu(8): here

/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_kernel.h(276): warning: variable "tQrQ" was declared but never referenced
          detected during:
            instantiation of "void flash::compute_attn_1rowblock<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params,Is_Q_split_K>(const Params &, int, int, int) [with Kernel_traits=Flash_fwd_kernel_traits<512, 128, 32, 8, false, true, cutlass::half_t, Flash_kernel_traits<512, 128, 32, 8, cutlass::half_t>>, Is_dropout=true, Is_causal=true, Is_even_N=false, Is_even_K=false, Return_softmax=false, Params=Flash_fwd_params, Is_Q_split_K=false]" 
(589): here
            instantiation of "void flash::compute_attn<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &) [with Kernel_traits=Flash_fwd_kernel_traits<512, 128, 32, 8, false, true, cutlass::half_t, Flash_kernel_traits<512, 128, 32, 8, cutlass::half_t>>, Is_dropout=true, Is_causal=true, Is_even_N=false, Is_even_K=false, Return_softmax=false, Params=Flash_fwd_params]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(15): here
            instantiation of "void flash_fwd_kernel<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax>(Flash_fwd_params) [with Kernel_traits=Flash_fwd_kernel_traits<512, 128, 32, 8, false, true, cutlass::half_t, Flash_kernel_traits<512, 128, 32, 8, cutlass::half_t>>, Is_dropout=true, Is_causal=true, Is_even_N=false, Is_even_K=false, Return_softmax=false]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(34): here
            instantiation of "void run_flash_fwd<Kernel_traits,Is_dropout,Is_causal>(Flash_fwd_params &, cudaStream_t) [with Kernel_traits=Flash_fwd_kernel_traits<512, 128, 32, 8, false, true, cutlass::half_t, Flash_kernel_traits<512, 128, 32, 8, cutlass::half_t>>, Is_dropout=true, Is_causal=true]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(264): here
            instantiation of "void run_mha_fwd_hdim512<T>(Flash_fwd_params &, cudaStream_t) [with T=cutlass::half_t]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_hdim512_fp16_sm80.cu(8): here

/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_kernel.h(140): warning: variable "MMA_M" was declared but never referenced
          detected during:
            instantiation of "void flash::compute_attn_1rowblock<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params,Is_Q_split_K>(const Params &, int, int, int) [with Kernel_traits=Flash_fwd_kernel_traits<512, 32, 32, 2, false, false, cutlass::half_t, Flash_kernel_traits<512, 32, 32, 2, cutlass::half_t>>, Is_dropout=true, Is_causal=true, Is_even_N=true, Is_even_K=true, Return_softmax=true, Params=Flash_fwd_params, Is_Q_split_K=false]" 
(589): here
            instantiation of "void flash::compute_attn<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &) [with Kernel_traits=Flash_fwd_kernel_traits<512, 32, 32, 2, false, false, cutlass::half_t, Flash_kernel_traits<512, 32, 32, 2, cutlass::half_t>>, Is_dropout=true, Is_causal=true, Is_even_N=true, Is_even_K=true, Return_softmax=true, Params=Flash_fwd_params]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(15): here
            instantiation of "void flash_fwd_kernel<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax>(Flash_fwd_params) [with Kernel_traits=Flash_fwd_kernel_traits<512, 32, 32, 2, false, false, cutlass::half_t, Flash_kernel_traits<512, 32, 32, 2, cutlass::half_t>>, Is_dropout=true, Is_causal=true, Is_even_N=true, Is_even_K=true, Return_softmax=true]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(34): here
            instantiation of "void run_flash_fwd<Kernel_traits,Is_dropout,Is_causal>(Flash_fwd_params &, cudaStream_t) [with Kernel_traits=Flash_fwd_kernel_traits<512, 32, 32, 2, false, false, cutlass::half_t, Flash_kernel_traits<512, 32, 32, 2, cutlass::half_t>>, Is_dropout=true, Is_causal=true]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(264): here
            instantiation of "void run_mha_fwd_hdim512<T>(Flash_fwd_params &, cudaStream_t) [with T=cutlass::half_t]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_hdim512_fp16_sm80.cu(8): here

/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_kernel.h(276): warning: variable "tQrQ" was declared but never referenced
          detected during:
            instantiation of "void flash::compute_attn_1rowblock<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params,Is_Q_split_K>(const Params &, int, int, int) [with Kernel_traits=Flash_fwd_kernel_traits<512, 32, 32, 2, false, false, cutlass::half_t, Flash_kernel_traits<512, 32, 32, 2, cutlass::half_t>>, Is_dropout=true, Is_causal=true, Is_even_N=true, Is_even_K=true, Return_softmax=true, Params=Flash_fwd_params, Is_Q_split_K=false]" 
(589): here
            instantiation of "void flash::compute_attn<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &) [with Kernel_traits=Flash_fwd_kernel_traits<512, 32, 32, 2, false, false, cutlass::half_t, Flash_kernel_traits<512, 32, 32, 2, cutlass::half_t>>, Is_dropout=true, Is_causal=true, Is_even_N=true, Is_even_K=true, Return_softmax=true, Params=Flash_fwd_params]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(15): here
            instantiation of "void flash_fwd_kernel<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax>(Flash_fwd_params) [with Kernel_traits=Flash_fwd_kernel_traits<512, 32, 32, 2, false, false, cutlass::half_t, Flash_kernel_traits<512, 32, 32, 2, cutlass::half_t>>, Is_dropout=true, Is_causal=true, Is_even_N=true, Is_even_K=true, Return_softmax=true]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(34): here
            instantiation of "void run_flash_fwd<Kernel_traits,Is_dropout,Is_causal>(Flash_fwd_params &, cudaStream_t) [with Kernel_traits=Flash_fwd_kernel_traits<512, 32, 32, 2, false, false, cutlass::half_t, Flash_kernel_traits<512, 32, 32, 2, cutlass::half_t>>, Is_dropout=true, Is_causal=true]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(264): here
            instantiation of "void run_mha_fwd_hdim512<T>(Flash_fwd_params &, cudaStream_t) [with T=cutlass::half_t]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_hdim512_fp16_sm80.cu(8): here

/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_kernel.h(140): warning: variable "MMA_M" was declared but never referenced
          detected during:
            instantiation of "void flash::compute_attn_1rowblock<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params,Is_Q_split_K>(const Params &, int, int, int) [with Kernel_traits=Flash_fwd_kernel_traits<512, 32, 32, 2, false, false, cutlass::half_t, Flash_kernel_traits<512, 32, 32, 2, cutlass::half_t>>, Is_dropout=true, Is_causal=true, Is_even_N=true, Is_even_K=true, Return_softmax=false, Params=Flash_fwd_params, Is_Q_split_K=false]" 
(589): here
            instantiation of "void flash::compute_attn<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &) [with Kernel_traits=Flash_fwd_kernel_traits<512, 32, 32, 2, false, false, cutlass::half_t, Flash_kernel_traits<512, 32, 32, 2, cutlass::half_t>>, Is_dropout=true, Is_causal=true, Is_even_N=true, Is_even_K=true, Return_softmax=false, Params=Flash_fwd_params]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(15): here
            instantiation of "void flash_fwd_kernel<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax>(Flash_fwd_params) [with Kernel_traits=Flash_fwd_kernel_traits<512, 32, 32, 2, false, false, cutlass::half_t, Flash_kernel_traits<512, 32, 32, 2, cutlass::half_t>>, Is_dropout=true, Is_causal=true, Is_even_N=true, Is_even_K=true, Return_softmax=false]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(34): here
            instantiation of "void run_flash_fwd<Kernel_traits,Is_dropout,Is_causal>(Flash_fwd_params &, cudaStream_t) [with Kernel_traits=Flash_fwd_kernel_traits<512, 32, 32, 2, false, false, cutlass::half_t, Flash_kernel_traits<512, 32, 32, 2, cutlass::half_t>>, Is_dropout=true, Is_causal=true]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(264): here
            instantiation of "void run_mha_fwd_hdim512<T>(Flash_fwd_params &, cudaStream_t) [with T=cutlass::half_t]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_hdim512_fp16_sm80.cu(8): here

/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_kernel.h(276): warning: variable "tQrQ" was declared but never referenced
          detected during:
            instantiation of "void flash::compute_attn_1rowblock<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params,Is_Q_split_K>(const Params &, int, int, int) [with Kernel_traits=Flash_fwd_kernel_traits<512, 32, 32, 2, false, false, cutlass::half_t, Flash_kernel_traits<512, 32, 32, 2, cutlass::half_t>>, Is_dropout=true, Is_causal=true, Is_even_N=true, Is_even_K=true, Return_softmax=false, Params=Flash_fwd_params, Is_Q_split_K=false]" 
(589): here
            instantiation of "void flash::compute_attn<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &) [with Kernel_traits=Flash_fwd_kernel_traits<512, 32, 32, 2, false, false, cutlass::half_t, Flash_kernel_traits<512, 32, 32, 2, cutlass::half_t>>, Is_dropout=true, Is_causal=true, Is_even_N=true, Is_even_K=true, Return_softmax=false, Params=Flash_fwd_params]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(15): here
            instantiation of "void flash_fwd_kernel<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax>(Flash_fwd_params) [with Kernel_traits=Flash_fwd_kernel_traits<512, 32, 32, 2, false, false, cutlass::half_t, Flash_kernel_traits<512, 32, 32, 2, cutlass::half_t>>, Is_dropout=true, Is_causal=true, Is_even_N=true, Is_even_K=true, Return_softmax=false]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(34): here
            instantiation of "void run_flash_fwd<Kernel_traits,Is_dropout,Is_causal>(Flash_fwd_params &, cudaStream_t) [with Kernel_traits=Flash_fwd_kernel_traits<512, 32, 32, 2, false, false, cutlass::half_t, Flash_kernel_traits<512, 32, 32, 2, cutlass::half_t>>, Is_dropout=true, Is_causal=true]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(264): here
            instantiation of "void run_mha_fwd_hdim512<T>(Flash_fwd_params &, cudaStream_t) [with T=cutlass::half_t]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_hdim512_fp16_sm80.cu(8): here

/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_kernel.h(140): warning: variable "MMA_M" was declared but never referenced
          detected during:
            instantiation of "void flash::compute_attn_1rowblock<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params,Is_Q_split_K>(const Params &, int, int, int) [with Kernel_traits=Flash_fwd_kernel_traits<512, 32, 32, 2, false, false, cutlass::half_t, Flash_kernel_traits<512, 32, 32, 2, cutlass::half_t>>, Is_dropout=true, Is_causal=true, Is_even_N=true, Is_even_K=false, Return_softmax=true, Params=Flash_fwd_params, Is_Q_split_K=false]" 
(589): here
            instantiation of "void flash::compute_attn<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &) [with Kernel_traits=Flash_fwd_kernel_traits<512, 32, 32, 2, false, false, cutlass::half_t, Flash_kernel_traits<512, 32, 32, 2, cutlass::half_t>>, Is_dropout=true, Is_causal=true, Is_even_N=true, Is_even_K=false, Return_softmax=true, Params=Flash_fwd_params]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(15): here
            instantiation of "void flash_fwd_kernel<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax>(Flash_fwd_params) [with Kernel_traits=Flash_fwd_kernel_traits<512, 32, 32, 2, false, false, cutlass::half_t, Flash_kernel_traits<512, 32, 32, 2, cutlass::half_t>>, Is_dropout=true, Is_causal=true, Is_even_N=true, Is_even_K=false, Return_softmax=true]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(34): here
            instantiation of "void run_flash_fwd<Kernel_traits,Is_dropout,Is_causal>(Flash_fwd_params &, cudaStream_t) [with Kernel_traits=Flash_fwd_kernel_traits<512, 32, 32, 2, false, false, cutlass::half_t, Flash_kernel_traits<512, 32, 32, 2, cutlass::half_t>>, Is_dropout=true, Is_causal=true]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(264): here
            instantiation of "void run_mha_fwd_hdim512<T>(Flash_fwd_params &, cudaStream_t) [with T=cutlass::half_t]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_hdim512_fp16_sm80.cu(8): here

/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_kernel.h(276): warning: variable "tQrQ" was declared but never referenced
          detected during:
            instantiation of "void flash::compute_attn_1rowblock<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params,Is_Q_split_K>(const Params &, int, int, int) [with Kernel_traits=Flash_fwd_kernel_traits<512, 32, 32, 2, false, false, cutlass::half_t, Flash_kernel_traits<512, 32, 32, 2, cutlass::half_t>>, Is_dropout=true, Is_causal=true, Is_even_N=true, Is_even_K=false, Return_softmax=true, Params=Flash_fwd_params, Is_Q_split_K=false]" 
(589): here
            instantiation of "void flash::compute_attn<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &) [with Kernel_traits=Flash_fwd_kernel_traits<512, 32, 32, 2, false, false, cutlass::half_t, Flash_kernel_traits<512, 32, 32, 2, cutlass::half_t>>, Is_dropout=true, Is_causal=true, Is_even_N=true, Is_even_K=false, Return_softmax=true, Params=Flash_fwd_params]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(15): here
            instantiation of "void flash_fwd_kernel<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax>(Flash_fwd_params) [with Kernel_traits=Flash_fwd_kernel_traits<512, 32, 32, 2, false, false, cutlass::half_t, Flash_kernel_traits<512, 32, 32, 2, cutlass::half_t>>, Is_dropout=true, Is_causal=true, Is_even_N=true, Is_even_K=false, Return_softmax=true]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(34): here
            instantiation of "void run_flash_fwd<Kernel_traits,Is_dropout,Is_causal>(Flash_fwd_params &, cudaStream_t) [with Kernel_traits=Flash_fwd_kernel_traits<512, 32, 32, 2, false, false, cutlass::half_t, Flash_kernel_traits<512, 32, 32, 2, cutlass::half_t>>, Is_dropout=true, Is_causal=true]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(264): here
            instantiation of "void run_mha_fwd_hdim512<T>(Flash_fwd_params &, cudaStream_t) [with T=cutlass::half_t]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_hdim512_fp16_sm80.cu(8): here

/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_kernel.h(140): warning: variable "MMA_M" was declared but never referenced
          detected during:
            instantiation of "void flash::compute_attn_1rowblock<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params,Is_Q_split_K>(const Params &, int, int, int) [with Kernel_traits=Flash_fwd_kernel_traits<512, 32, 32, 2, false, false, cutlass::half_t, Flash_kernel_traits<512, 32, 32, 2, cutlass::half_t>>, Is_dropout=true, Is_causal=true, Is_even_N=true, Is_even_K=false, Return_softmax=false, Params=Flash_fwd_params, Is_Q_split_K=false]" 
(589): here
            instantiation of "void flash::compute_attn<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &) [with Kernel_traits=Flash_fwd_kernel_traits<512, 32, 32, 2, false, false, cutlass::half_t, Flash_kernel_traits<512, 32, 32, 2, cutlass::half_t>>, Is_dropout=true, Is_causal=true, Is_even_N=true, Is_even_K=false, Return_softmax=false, Params=Flash_fwd_params]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(15): here
            instantiation of "void flash_fwd_kernel<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax>(Flash_fwd_params) [with Kernel_traits=Flash_fwd_kernel_traits<512, 32, 32, 2, false, false, cutlass::half_t, Flash_kernel_traits<512, 32, 32, 2, cutlass::half_t>>, Is_dropout=true, Is_causal=true, Is_even_N=true, Is_even_K=false, Return_softmax=false]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(34): here
            instantiation of "void run_flash_fwd<Kernel_traits,Is_dropout,Is_causal>(Flash_fwd_params &, cudaStream_t) [with Kernel_traits=Flash_fwd_kernel_traits<512, 32, 32, 2, false, false, cutlass::half_t, Flash_kernel_traits<512, 32, 32, 2, cutlass::half_t>>, Is_dropout=true, Is_causal=true]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(264): here
            instantiation of "void run_mha_fwd_hdim512<T>(Flash_fwd_params &, cudaStream_t) [with T=cutlass::half_t]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_hdim512_fp16_sm80.cu(8): here

/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_kernel.h(276): warning: variable "tQrQ" was declared but never referenced
          detected during:
            instantiation of "void flash::compute_attn_1rowblock<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params,Is_Q_split_K>(const Params &, int, int, int) [with Kernel_traits=Flash_fwd_kernel_traits<512, 32, 32, 2, false, false, cutlass::half_t, Flash_kernel_traits<512, 32, 32, 2, cutlass::half_t>>, Is_dropout=true, Is_causal=true, Is_even_N=true, Is_even_K=false, Return_softmax=false, Params=Flash_fwd_params, Is_Q_split_K=false]" 
(589): here
            instantiation of "void flash::compute_attn<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &) [with Kernel_traits=Flash_fwd_kernel_traits<512, 32, 32, 2, false, false, cutlass::half_t, Flash_kernel_traits<512, 32, 32, 2, cutlass::half_t>>, Is_dropout=true, Is_causal=true, Is_even_N=true, Is_even_K=false, Return_softmax=false, Params=Flash_fwd_params]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(15): here
            instantiation of "void flash_fwd_kernel<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax>(Flash_fwd_params) [with Kernel_traits=Flash_fwd_kernel_traits<512, 32, 32, 2, false, false, cutlass::half_t, Flash_kernel_traits<512, 32, 32, 2, cutlass::half_t>>, Is_dropout=true, Is_causal=true, Is_even_N=true, Is_even_K=false, Return_softmax=false]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(34): here
            instantiation of "void run_flash_fwd<Kernel_traits,Is_dropout,Is_causal>(Flash_fwd_params &, cudaStream_t) [with Kernel_traits=Flash_fwd_kernel_traits<512, 32, 32, 2, false, false, cutlass::half_t, Flash_kernel_traits<512, 32, 32, 2, cutlass::half_t>>, Is_dropout=true, Is_causal=true]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(264): here
            instantiation of "void run_mha_fwd_hdim512<T>(Flash_fwd_params &, cudaStream_t) [with T=cutlass::half_t]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_hdim512_fp16_sm80.cu(8): here

/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_kernel.h(140): warning: variable "MMA_M" was declared but never referenced
          detected during:
            instantiation of "void flash::compute_attn_1rowblock<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params,Is_Q_split_K>(const Params &, int, int, int) [with Kernel_traits=Flash_fwd_kernel_traits<512, 32, 32, 2, false, false, cutlass::half_t, Flash_kernel_traits<512, 32, 32, 2, cutlass::half_t>>, Is_dropout=true, Is_causal=true, Is_even_N=false, Is_even_K=true, Return_softmax=true, Params=Flash_fwd_params, Is_Q_split_K=false]" 
(589): here
            instantiation of "void flash::compute_attn<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &) [with Kernel_traits=Flash_fwd_kernel_traits<512, 32, 32, 2, false, false, cutlass::half_t, Flash_kernel_traits<512, 32, 32, 2, cutlass::half_t>>, Is_dropout=true, Is_causal=true, Is_even_N=false, Is_even_K=true, Return_softmax=true, Params=Flash_fwd_params]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(15): here
            instantiation of "void flash_fwd_kernel<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax>(Flash_fwd_params) [with Kernel_traits=Flash_fwd_kernel_traits<512, 32, 32, 2, false, false, cutlass::half_t, Flash_kernel_traits<512, 32, 32, 2, cutlass::half_t>>, Is_dropout=true, Is_causal=true, Is_even_N=false, Is_even_K=true, Return_softmax=true]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(34): here
            instantiation of "void run_flash_fwd<Kernel_traits,Is_dropout,Is_causal>(Flash_fwd_params &, cudaStream_t) [with Kernel_traits=Flash_fwd_kernel_traits<512, 32, 32, 2, false, false, cutlass::half_t, Flash_kernel_traits<512, 32, 32, 2, cutlass::half_t>>, Is_dropout=true, Is_causal=true]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(264): here
            instantiation of "void run_mha_fwd_hdim512<T>(Flash_fwd_params &, cudaStream_t) [with T=cutlass::half_t]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_hdim512_fp16_sm80.cu(8): here

/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_kernel.h(276): warning: variable "tQrQ" was declared but never referenced
          detected during:
            instantiation of "void flash::compute_attn_1rowblock<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params,Is_Q_split_K>(const Params &, int, int, int) [with Kernel_traits=Flash_fwd_kernel_traits<512, 32, 32, 2, false, false, cutlass::half_t, Flash_kernel_traits<512, 32, 32, 2, cutlass::half_t>>, Is_dropout=true, Is_causal=true, Is_even_N=false, Is_even_K=true, Return_softmax=true, Params=Flash_fwd_params, Is_Q_split_K=false]" 
(589): here
            instantiation of "void flash::compute_attn<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &) [with Kernel_traits=Flash_fwd_kernel_traits<512, 32, 32, 2, false, false, cutlass::half_t, Flash_kernel_traits<512, 32, 32, 2, cutlass::half_t>>, Is_dropout=true, Is_causal=true, Is_even_N=false, Is_even_K=true, Return_softmax=true, Params=Flash_fwd_params]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(15): here
            instantiation of "void flash_fwd_kernel<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax>(Flash_fwd_params) [with Kernel_traits=Flash_fwd_kernel_traits<512, 32, 32, 2, false, false, cutlass::half_t, Flash_kernel_traits<512, 32, 32, 2, cutlass::half_t>>, Is_dropout=true, Is_causal=true, Is_even_N=false, Is_even_K=true, Return_softmax=true]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(34): here
            instantiation of "void run_flash_fwd<Kernel_traits,Is_dropout,Is_causal>(Flash_fwd_params &, cudaStream_t) [with Kernel_traits=Flash_fwd_kernel_traits<512, 32, 32, 2, false, false, cutlass::half_t, Flash_kernel_traits<512, 32, 32, 2, cutlass::half_t>>, Is_dropout=true, Is_causal=true]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(264): here
            instantiation of "void run_mha_fwd_hdim512<T>(Flash_fwd_params &, cudaStream_t) [with T=cutlass::half_t]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_hdim512_fp16_sm80.cu(8): here

/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_kernel.h(140): warning: variable "MMA_M" was declared but never referenced
          detected during:
            instantiation of "void flash::compute_attn_1rowblock<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params,Is_Q_split_K>(const Params &, int, int, int) [with Kernel_traits=Flash_fwd_kernel_traits<512, 32, 32, 2, false, false, cutlass::half_t, Flash_kernel_traits<512, 32, 32, 2, cutlass::half_t>>, Is_dropout=true, Is_causal=true, Is_even_N=false, Is_even_K=true, Return_softmax=false, Params=Flash_fwd_params, Is_Q_split_K=false]" 
(589): here
            instantiation of "void flash::compute_attn<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &) [with Kernel_traits=Flash_fwd_kernel_traits<512, 32, 32, 2, false, false, cutlass::half_t, Flash_kernel_traits<512, 32, 32, 2, cutlass::half_t>>, Is_dropout=true, Is_causal=true, Is_even_N=false, Is_even_K=true, Return_softmax=false, Params=Flash_fwd_params]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(15): here
            instantiation of "void flash_fwd_kernel<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax>(Flash_fwd_params) [with Kernel_traits=Flash_fwd_kernel_traits<512, 32, 32, 2, false, false, cutlass::half_t, Flash_kernel_traits<512, 32, 32, 2, cutlass::half_t>>, Is_dropout=true, Is_causal=true, Is_even_N=false, Is_even_K=true, Return_softmax=false]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(34): here
            instantiation of "void run_flash_fwd<Kernel_traits,Is_dropout,Is_causal>(Flash_fwd_params &, cudaStream_t) [with Kernel_traits=Flash_fwd_kernel_traits<512, 32, 32, 2, false, false, cutlass::half_t, Flash_kernel_traits<512, 32, 32, 2, cutlass::half_t>>, Is_dropout=true, Is_causal=true]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(264): here
            instantiation of "void run_mha_fwd_hdim512<T>(Flash_fwd_params &, cudaStream_t) [with T=cutlass::half_t]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_hdim512_fp16_sm80.cu(8): here

/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_kernel.h(276): warning: variable "tQrQ" was declared but never referenced
          detected during:
            instantiation of "void flash::compute_attn_1rowblock<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params,Is_Q_split_K>(const Params &, int, int, int) [with Kernel_traits=Flash_fwd_kernel_traits<512, 32, 32, 2, false, false, cutlass::half_t, Flash_kernel_traits<512, 32, 32, 2, cutlass::half_t>>, Is_dropout=true, Is_causal=true, Is_even_N=false, Is_even_K=true, Return_softmax=false, Params=Flash_fwd_params, Is_Q_split_K=false]" 
(589): here
            instantiation of "void flash::compute_attn<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &) [with Kernel_traits=Flash_fwd_kernel_traits<512, 32, 32, 2, false, false, cutlass::half_t, Flash_kernel_traits<512, 32, 32, 2, cutlass::half_t>>, Is_dropout=true, Is_causal=true, Is_even_N=false, Is_even_K=true, Return_softmax=false, Params=Flash_fwd_params]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(15): here
            instantiation of "void flash_fwd_kernel<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax>(Flash_fwd_params) [with Kernel_traits=Flash_fwd_kernel_traits<512, 32, 32, 2, false, false, cutlass::half_t, Flash_kernel_traits<512, 32, 32, 2, cutlass::half_t>>, Is_dropout=true, Is_causal=true, Is_even_N=false, Is_even_K=true, Return_softmax=false]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(34): here
            instantiation of "void run_flash_fwd<Kernel_traits,Is_dropout,Is_causal>(Flash_fwd_params &, cudaStream_t) [with Kernel_traits=Flash_fwd_kernel_traits<512, 32, 32, 2, false, false, cutlass::half_t, Flash_kernel_traits<512, 32, 32, 2, cutlass::half_t>>, Is_dropout=true, Is_causal=true]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(264): here
            instantiation of "void run_mha_fwd_hdim512<T>(Flash_fwd_params &, cudaStream_t) [with T=cutlass::half_t]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_hdim512_fp16_sm80.cu(8): here

/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_kernel.h(140): warning: variable "MMA_M" was declared but never referenced
          detected during:
            instantiation of "void flash::compute_attn_1rowblock<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params,Is_Q_split_K>(const Params &, int, int, int) [with Kernel_traits=Flash_fwd_kernel_traits<512, 32, 32, 2, false, false, cutlass::half_t, Flash_kernel_traits<512, 32, 32, 2, cutlass::half_t>>, Is_dropout=true, Is_causal=true, Is_even_N=false, Is_even_K=false, Return_softmax=true, Params=Flash_fwd_params, Is_Q_split_K=false]" 
(589): here
            instantiation of "void flash::compute_attn<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &) [with Kernel_traits=Flash_fwd_kernel_traits<512, 32, 32, 2, false, false, cutlass::half_t, Flash_kernel_traits<512, 32, 32, 2, cutlass::half_t>>, Is_dropout=true, Is_causal=true, Is_even_N=false, Is_even_K=false, Return_softmax=true, Params=Flash_fwd_params]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(15): here
            instantiation of "void flash_fwd_kernel<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax>(Flash_fwd_params) [with Kernel_traits=Flash_fwd_kernel_traits<512, 32, 32, 2, false, false, cutlass::half_t, Flash_kernel_traits<512, 32, 32, 2, cutlass::half_t>>, Is_dropout=true, Is_causal=true, Is_even_N=false, Is_even_K=false, Return_softmax=true]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(34): here
            instantiation of "void run_flash_fwd<Kernel_traits,Is_dropout,Is_causal>(Flash_fwd_params &, cudaStream_t) [with Kernel_traits=Flash_fwd_kernel_traits<512, 32, 32, 2, false, false, cutlass::half_t, Flash_kernel_traits<512, 32, 32, 2, cutlass::half_t>>, Is_dropout=true, Is_causal=true]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(264): here
            instantiation of "void run_mha_fwd_hdim512<T>(Flash_fwd_params &, cudaStream_t) [with T=cutlass::half_t]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_hdim512_fp16_sm80.cu(8): here

/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_kernel.h(276): warning: variable "tQrQ" was declared but never referenced
          detected during:
            instantiation of "void flash::compute_attn_1rowblock<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params,Is_Q_split_K>(const Params &, int, int, int) [with Kernel_traits=Flash_fwd_kernel_traits<512, 32, 32, 2, false, false, cutlass::half_t, Flash_kernel_traits<512, 32, 32, 2, cutlass::half_t>>, Is_dropout=true, Is_causal=true, Is_even_N=false, Is_even_K=false, Return_softmax=true, Params=Flash_fwd_params, Is_Q_split_K=false]" 
(589): here
            instantiation of "void flash::compute_attn<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &) [with Kernel_traits=Flash_fwd_kernel_traits<512, 32, 32, 2, false, false, cutlass::half_t, Flash_kernel_traits<512, 32, 32, 2, cutlass::half_t>>, Is_dropout=true, Is_causal=true, Is_even_N=false, Is_even_K=false, Return_softmax=true, Params=Flash_fwd_params]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(15): here
            instantiation of "void flash_fwd_kernel<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax>(Flash_fwd_params) [with Kernel_traits=Flash_fwd_kernel_traits<512, 32, 32, 2, false, false, cutlass::half_t, Flash_kernel_traits<512, 32, 32, 2, cutlass::half_t>>, Is_dropout=true, Is_causal=true, Is_even_N=false, Is_even_K=false, Return_softmax=true]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(34): here
            instantiation of "void run_flash_fwd<Kernel_traits,Is_dropout,Is_causal>(Flash_fwd_params &, cudaStream_t) [with Kernel_traits=Flash_fwd_kernel_traits<512, 32, 32, 2, false, false, cutlass::half_t, Flash_kernel_traits<512, 32, 32, 2, cutlass::half_t>>, Is_dropout=true, Is_causal=true]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(264): here
            instantiation of "void run_mha_fwd_hdim512<T>(Flash_fwd_params &, cudaStream_t) [with T=cutlass::half_t]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_hdim512_fp16_sm80.cu(8): here

/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_kernel.h(140): warning: variable "MMA_M" was declared but never referenced
          detected during:
            instantiation of "void flash::compute_attn_1rowblock<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params,Is_Q_split_K>(const Params &, int, int, int) [with Kernel_traits=Flash_fwd_kernel_traits<512, 32, 32, 2, false, false, cutlass::half_t, Flash_kernel_traits<512, 32, 32, 2, cutlass::half_t>>, Is_dropout=true, Is_causal=true, Is_even_N=false, Is_even_K=false, Return_softmax=false, Params=Flash_fwd_params, Is_Q_split_K=false]" 
(589): here
            instantiation of "void flash::compute_attn<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &) [with Kernel_traits=Flash_fwd_kernel_traits<512, 32, 32, 2, false, false, cutlass::half_t, Flash_kernel_traits<512, 32, 32, 2, cutlass::half_t>>, Is_dropout=true, Is_causal=true, Is_even_N=false, Is_even_K=false, Return_softmax=false, Params=Flash_fwd_params]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(15): here
            instantiation of "void flash_fwd_kernel<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax>(Flash_fwd_params) [with Kernel_traits=Flash_fwd_kernel_traits<512, 32, 32, 2, false, false, cutlass::half_t, Flash_kernel_traits<512, 32, 32, 2, cutlass::half_t>>, Is_dropout=true, Is_causal=true, Is_even_N=false, Is_even_K=false, Return_softmax=false]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(34): here
            instantiation of "void run_flash_fwd<Kernel_traits,Is_dropout,Is_causal>(Flash_fwd_params &, cudaStream_t) [with Kernel_traits=Flash_fwd_kernel_traits<512, 32, 32, 2, false, false, cutlass::half_t, Flash_kernel_traits<512, 32, 32, 2, cutlass::half_t>>, Is_dropout=true, Is_causal=true]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(264): here
            instantiation of "void run_mha_fwd_hdim512<T>(Flash_fwd_params &, cudaStream_t) [with T=cutlass::half_t]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_hdim512_fp16_sm80.cu(8): here

/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_kernel.h(276): warning: variable "tQrQ" was declared but never referenced
          detected during:
            instantiation of "void flash::compute_attn_1rowblock<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params,Is_Q_split_K>(const Params &, int, int, int) [with Kernel_traits=Flash_fwd_kernel_traits<512, 32, 32, 2, false, false, cutlass::half_t, Flash_kernel_traits<512, 32, 32, 2, cutlass::half_t>>, Is_dropout=true, Is_causal=true, Is_even_N=false, Is_even_K=false, Return_softmax=false, Params=Flash_fwd_params, Is_Q_split_K=false]" 
(589): here
            instantiation of "void flash::compute_attn<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &) [with Kernel_traits=Flash_fwd_kernel_traits<512, 32, 32, 2, false, false, cutlass::half_t, Flash_kernel_traits<512, 32, 32, 2, cutlass::half_t>>, Is_dropout=true, Is_causal=true, Is_even_N=false, Is_even_K=false, Return_softmax=false, Params=Flash_fwd_params]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(15): here
            instantiation of "void flash_fwd_kernel<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax>(Flash_fwd_params) [with Kernel_traits=Flash_fwd_kernel_traits<512, 32, 32, 2, false, false, cutlass::half_t, Flash_kernel_traits<512, 32, 32, 2, cutlass::half_t>>, Is_dropout=true, Is_causal=true, Is_even_N=false, Is_even_K=false, Return_softmax=false]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(34): here
            instantiation of "void run_flash_fwd<Kernel_traits,Is_dropout,Is_causal>(Flash_fwd_params &, cudaStream_t) [with Kernel_traits=Flash_fwd_kernel_traits<512, 32, 32, 2, false, false, cutlass::half_t, Flash_kernel_traits<512, 32, 32, 2, cutlass::half_t>>, Is_dropout=true, Is_causal=true]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(264): here
            instantiation of "void run_mha_fwd_hdim512<T>(Flash_fwd_params &, cudaStream_t) [with T=cutlass::half_t]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_hdim512_fp16_sm80.cu(8): here

/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_kernel.h(140): warning: variable "MMA_M" was declared but never referenced
          detected during:
            instantiation of "void flash::compute_attn_1rowblock<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params,Is_Q_split_K>(const Params &, int, int, int) [with Kernel_traits=Flash_fwd_kernel_traits<512, 128, 32, 8, false, true, cutlass::half_t, Flash_kernel_traits<512, 128, 32, 8, cutlass::half_t>>, Is_dropout=true, Is_causal=false, Is_even_N=true, Is_even_K=true, Return_softmax=true, Params=Flash_fwd_params, Is_Q_split_K=false]" 
(589): here
            instantiation of "void flash::compute_attn<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &) [with Kernel_traits=Flash_fwd_kernel_traits<512, 128, 32, 8, false, true, cutlass::half_t, Flash_kernel_traits<512, 128, 32, 8, cutlass::half_t>>, Is_dropout=true, Is_causal=false, Is_even_N=true, Is_even_K=true, Return_softmax=true, Params=Flash_fwd_params]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(15): here
            instantiation of "void flash_fwd_kernel<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax>(Flash_fwd_params) [with Kernel_traits=Flash_fwd_kernel_traits<512, 128, 32, 8, false, true, cutlass::half_t, Flash_kernel_traits<512, 128, 32, 8, cutlass::half_t>>, Is_dropout=true, Is_causal=false, Is_even_N=true, Is_even_K=true, Return_softmax=true]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(34): here
            instantiation of "void run_flash_fwd<Kernel_traits,Is_dropout,Is_causal>(Flash_fwd_params &, cudaStream_t) [with Kernel_traits=Flash_fwd_kernel_traits<512, 128, 32, 8, false, true, cutlass::half_t, Flash_kernel_traits<512, 128, 32, 8, cutlass::half_t>>, Is_dropout=true, Is_causal=false]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(264): here
            instantiation of "void run_mha_fwd_hdim512<T>(Flash_fwd_params &, cudaStream_t) [with T=cutlass::half_t]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_hdim512_fp16_sm80.cu(8): here

/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_kernel.h(276): warning: variable "tQrQ" was declared but never referenced
          detected during:
            instantiation of "void flash::compute_attn_1rowblock<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params,Is_Q_split_K>(const Params &, int, int, int) [with Kernel_traits=Flash_fwd_kernel_traits<512, 128, 32, 8, false, true, cutlass::half_t, Flash_kernel_traits<512, 128, 32, 8, cutlass::half_t>>, Is_dropout=true, Is_causal=false, Is_even_N=true, Is_even_K=true, Return_softmax=true, Params=Flash_fwd_params, Is_Q_split_K=false]" 
(589): here
            instantiation of "void flash::compute_attn<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &) [with Kernel_traits=Flash_fwd_kernel_traits<512, 128, 32, 8, false, true, cutlass::half_t, Flash_kernel_traits<512, 128, 32, 8, cutlass::half_t>>, Is_dropout=true, Is_causal=false, Is_even_N=true, Is_even_K=true, Return_softmax=true, Params=Flash_fwd_params]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(15): here
            instantiation of "void flash_fwd_kernel<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax>(Flash_fwd_params) [with Kernel_traits=Flash_fwd_kernel_traits<512, 128, 32, 8, false, true, cutlass::half_t, Flash_kernel_traits<512, 128, 32, 8, cutlass::half_t>>, Is_dropout=true, Is_causal=false, Is_even_N=true, Is_even_K=true, Return_softmax=true]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(34): here
            instantiation of "void run_flash_fwd<Kernel_traits,Is_dropout,Is_causal>(Flash_fwd_params &, cudaStream_t) [with Kernel_traits=Flash_fwd_kernel_traits<512, 128, 32, 8, false, true, cutlass::half_t, Flash_kernel_traits<512, 128, 32, 8, cutlass::half_t>>, Is_dropout=true, Is_causal=false]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(264): here
            instantiation of "void run_mha_fwd_hdim512<T>(Flash_fwd_params &, cudaStream_t) [with T=cutlass::half_t]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_hdim512_fp16_sm80.cu(8): here

/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_kernel.h(140): warning: variable "MMA_M" was declared but never referenced
          detected during:
            instantiation of "void flash::compute_attn_1rowblock<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params,Is_Q_split_K>(const Params &, int, int, int) [with Kernel_traits=Flash_fwd_kernel_traits<512, 128, 32, 8, false, true, cutlass::half_t, Flash_kernel_traits<512, 128, 32, 8, cutlass::half_t>>, Is_dropout=true, Is_causal=false, Is_even_N=true, Is_even_K=true, Return_softmax=false, Params=Flash_fwd_params, Is_Q_split_K=false]" 
(589): here
            instantiation of "void flash::compute_attn<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &) [with Kernel_traits=Flash_fwd_kernel_traits<512, 128, 32, 8, false, true, cutlass::half_t, Flash_kernel_traits<512, 128, 32, 8, cutlass::half_t>>, Is_dropout=true, Is_causal=false, Is_even_N=true, Is_even_K=true, Return_softmax=false, Params=Flash_fwd_params]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(15): here
            instantiation of "void flash_fwd_kernel<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax>(Flash_fwd_params) [with Kernel_traits=Flash_fwd_kernel_traits<512, 128, 32, 8, false, true, cutlass::half_t, Flash_kernel_traits<512, 128, 32, 8, cutlass::half_t>>, Is_dropout=true, Is_causal=false, Is_even_N=true, Is_even_K=true, Return_softmax=false]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(34): here
            instantiation of "void run_flash_fwd<Kernel_traits,Is_dropout,Is_causal>(Flash_fwd_params &, cudaStream_t) [with Kernel_traits=Flash_fwd_kernel_traits<512, 128, 32, 8, false, true, cutlass::half_t, Flash_kernel_traits<512, 128, 32, 8, cutlass::half_t>>, Is_dropout=true, Is_causal=false]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(264): here
            instantiation of "void run_mha_fwd_hdim512<T>(Flash_fwd_params &, cudaStream_t) [with T=cutlass::half_t]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_hdim512_fp16_sm80.cu(8): here

/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_kernel.h(276): warning: variable "tQrQ" was declared but never referenced
          detected during:
            instantiation of "void flash::compute_attn_1rowblock<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params,Is_Q_split_K>(const Params &, int, int, int) [with Kernel_traits=Flash_fwd_kernel_traits<512, 128, 32, 8, false, true, cutlass::half_t, Flash_kernel_traits<512, 128, 32, 8, cutlass::half_t>>, Is_dropout=true, Is_causal=false, Is_even_N=true, Is_even_K=true, Return_softmax=false, Params=Flash_fwd_params, Is_Q_split_K=false]" 
(589): here
            instantiation of "void flash::compute_attn<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &) [with Kernel_traits=Flash_fwd_kernel_traits<512, 128, 32, 8, false, true, cutlass::half_t, Flash_kernel_traits<512, 128, 32, 8, cutlass::half_t>>, Is_dropout=true, Is_causal=false, Is_even_N=true, Is_even_K=true, Return_softmax=false, Params=Flash_fwd_params]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(15): here
            instantiation of "void flash_fwd_kernel<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax>(Flash_fwd_params) [with Kernel_traits=Flash_fwd_kernel_traits<512, 128, 32, 8, false, true, cutlass::half_t, Flash_kernel_traits<512, 128, 32, 8, cutlass::half_t>>, Is_dropout=true, Is_causal=false, Is_even_N=true, Is_even_K=true, Return_softmax=false]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(34): here
            instantiation of "void run_flash_fwd<Kernel_traits,Is_dropout,Is_causal>(Flash_fwd_params &, cudaStream_t) [with Kernel_traits=Flash_fwd_kernel_traits<512, 128, 32, 8, false, true, cutlass::half_t, Flash_kernel_traits<512, 128, 32, 8, cutlass::half_t>>, Is_dropout=true, Is_causal=false]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(264): here
            instantiation of "void run_mha_fwd_hdim512<T>(Flash_fwd_params &, cudaStream_t) [with T=cutlass::half_t]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_hdim512_fp16_sm80.cu(8): here

/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_kernel.h(140): warning: variable "MMA_M" was declared but never referenced
          detected during:
            instantiation of "void flash::compute_attn_1rowblock<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params,Is_Q_split_K>(const Params &, int, int, int) [with Kernel_traits=Flash_fwd_kernel_traits<512, 128, 32, 8, false, true, cutlass::half_t, Flash_kernel_traits<512, 128, 32, 8, cutlass::half_t>>, Is_dropout=true, Is_causal=false, Is_even_N=true, Is_even_K=false, Return_softmax=true, Params=Flash_fwd_params, Is_Q_split_K=false]" 
(589): here
            instantiation of "void flash::compute_attn<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &) [with Kernel_traits=Flash_fwd_kernel_traits<512, 128, 32, 8, false, true, cutlass::half_t, Flash_kernel_traits<512, 128, 32, 8, cutlass::half_t>>, Is_dropout=true, Is_causal=false, Is_even_N=true, Is_even_K=false, Return_softmax=true, Params=Flash_fwd_params]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(15): here
            instantiation of "void flash_fwd_kernel<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax>(Flash_fwd_params) [with Kernel_traits=Flash_fwd_kernel_traits<512, 128, 32, 8, false, true, cutlass::half_t, Flash_kernel_traits<512, 128, 32, 8, cutlass::half_t>>, Is_dropout=true, Is_causal=false, Is_even_N=true, Is_even_K=false, Return_softmax=true]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(34): here
            instantiation of "void run_flash_fwd<Kernel_traits,Is_dropout,Is_causal>(Flash_fwd_params &, cudaStream_t) [with Kernel_traits=Flash_fwd_kernel_traits<512, 128, 32, 8, false, true, cutlass::half_t, Flash_kernel_traits<512, 128, 32, 8, cutlass::half_t>>, Is_dropout=true, Is_causal=false]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(264): here
            instantiation of "void run_mha_fwd_hdim512<T>(Flash_fwd_params &, cudaStream_t) [with T=cutlass::half_t]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_hdim512_fp16_sm80.cu(8): here

/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_kernel.h(276): warning: variable "tQrQ" was declared but never referenced
          detected during:
            instantiation of "void flash::compute_attn_1rowblock<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params,Is_Q_split_K>(const Params &, int, int, int) [with Kernel_traits=Flash_fwd_kernel_traits<512, 128, 32, 8, false, true, cutlass::half_t, Flash_kernel_traits<512, 128, 32, 8, cutlass::half_t>>, Is_dropout=true, Is_causal=false, Is_even_N=true, Is_even_K=false, Return_softmax=true, Params=Flash_fwd_params, Is_Q_split_K=false]" 
(589): here
            instantiation of "void flash::compute_attn<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &) [with Kernel_traits=Flash_fwd_kernel_traits<512, 128, 32, 8, false, true, cutlass::half_t, Flash_kernel_traits<512, 128, 32, 8, cutlass::half_t>>, Is_dropout=true, Is_causal=false, Is_even_N=true, Is_even_K=false, Return_softmax=true, Params=Flash_fwd_params]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(15): here
            instantiation of "void flash_fwd_kernel<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax>(Flash_fwd_params) [with Kernel_traits=Flash_fwd_kernel_traits<512, 128, 32, 8, false, true, cutlass::half_t, Flash_kernel_traits<512, 128, 32, 8, cutlass::half_t>>, Is_dropout=true, Is_causal=false, Is_even_N=true, Is_even_K=false, Return_softmax=true]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(34): here
            instantiation of "void run_flash_fwd<Kernel_traits,Is_dropout,Is_causal>(Flash_fwd_params &, cudaStream_t) [with Kernel_traits=Flash_fwd_kernel_traits<512, 128, 32, 8, false, true, cutlass::half_t, Flash_kernel_traits<512, 128, 32, 8, cutlass::half_t>>, Is_dropout=true, Is_causal=false]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(264): here
            instantiation of "void run_mha_fwd_hdim512<T>(Flash_fwd_params &, cudaStream_t) [with T=cutlass::half_t]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_hdim512_fp16_sm80.cu(8): here

/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_kernel.h(140): warning: variable "MMA_M" was declared but never referenced
          detected during:
            instantiation of "void flash::compute_attn_1rowblock<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params,Is_Q_split_K>(const Params &, int, int, int) [with Kernel_traits=Flash_fwd_kernel_traits<512, 128, 32, 8, false, true, cutlass::half_t, Flash_kernel_traits<512, 128, 32, 8, cutlass::half_t>>, Is_dropout=true, Is_causal=false, Is_even_N=true, Is_even_K=false, Return_softmax=false, Params=Flash_fwd_params, Is_Q_split_K=false]" 
(589): here
            instantiation of "void flash::compute_attn<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &) [with Kernel_traits=Flash_fwd_kernel_traits<512, 128, 32, 8, false, true, cutlass::half_t, Flash_kernel_traits<512, 128, 32, 8, cutlass::half_t>>, Is_dropout=true, Is_causal=false, Is_even_N=true, Is_even_K=false, Return_softmax=false, Params=Flash_fwd_params]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(15): here
            instantiation of "void flash_fwd_kernel<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax>(Flash_fwd_params) [with Kernel_traits=Flash_fwd_kernel_traits<512, 128, 32, 8, false, true, cutlass::half_t, Flash_kernel_traits<512, 128, 32, 8, cutlass::half_t>>, Is_dropout=true, Is_causal=false, Is_even_N=true, Is_even_K=false, Return_softmax=false]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(34): here
            instantiation of "void run_flash_fwd<Kernel_traits,Is_dropout,Is_causal>(Flash_fwd_params &, cudaStream_t) [with Kernel_traits=Flash_fwd_kernel_traits<512, 128, 32, 8, false, true, cutlass::half_t, Flash_kernel_traits<512, 128, 32, 8, cutlass::half_t>>, Is_dropout=true, Is_causal=false]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(264): here
            instantiation of "void run_mha_fwd_hdim512<T>(Flash_fwd_params &, cudaStream_t) [with T=cutlass::half_t]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_hdim512_fp16_sm80.cu(8): here

/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_kernel.h(276): warning: variable "tQrQ" was declared but never referenced
          detected during:
            instantiation of "void flash::compute_attn_1rowblock<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params,Is_Q_split_K>(const Params &, int, int, int) [with Kernel_traits=Flash_fwd_kernel_traits<512, 128, 32, 8, false, true, cutlass::half_t, Flash_kernel_traits<512, 128, 32, 8, cutlass::half_t>>, Is_dropout=true, Is_causal=false, Is_even_N=true, Is_even_K=false, Return_softmax=false, Params=Flash_fwd_params, Is_Q_split_K=false]" 
(589): here
            instantiation of "void flash::compute_attn<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &) [with Kernel_traits=Flash_fwd_kernel_traits<512, 128, 32, 8, false, true, cutlass::half_t, Flash_kernel_traits<512, 128, 32, 8, cutlass::half_t>>, Is_dropout=true, Is_causal=false, Is_even_N=true, Is_even_K=false, Return_softmax=false, Params=Flash_fwd_params]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(15): here
            instantiation of "void flash_fwd_kernel<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax>(Flash_fwd_params) [with Kernel_traits=Flash_fwd_kernel_traits<512, 128, 32, 8, false, true, cutlass::half_t, Flash_kernel_traits<512, 128, 32, 8, cutlass::half_t>>, Is_dropout=true, Is_causal=false, Is_even_N=true, Is_even_K=false, Return_softmax=false]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(34): here
            instantiation of "void run_flash_fwd<Kernel_traits,Is_dropout,Is_causal>(Flash_fwd_params &, cudaStream_t) [with Kernel_traits=Flash_fwd_kernel_traits<512, 128, 32, 8, false, true, cutlass::half_t, Flash_kernel_traits<512, 128, 32, 8, cutlass::half_t>>, Is_dropout=true, Is_causal=false]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(264): here
            instantiation of "void run_mha_fwd_hdim512<T>(Flash_fwd_params &, cudaStream_t) [with T=cutlass::half_t]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_hdim512_fp16_sm80.cu(8): here

/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_kernel.h(140): warning: variable "MMA_M" was declared but never referenced
          detected during:
            instantiation of "void flash::compute_attn_1rowblock<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params,Is_Q_split_K>(const Params &, int, int, int) [with Kernel_traits=Flash_fwd_kernel_traits<512, 128, 32, 8, false, true, cutlass::half_t, Flash_kernel_traits<512, 128, 32, 8, cutlass::half_t>>, Is_dropout=true, Is_causal=false, Is_even_N=false, Is_even_K=true, Return_softmax=true, Params=Flash_fwd_params, Is_Q_split_K=false]" 
(589): here
            instantiation of "void flash::compute_attn<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &) [with Kernel_traits=Flash_fwd_kernel_traits<512, 128, 32, 8, false, true, cutlass::half_t, Flash_kernel_traits<512, 128, 32, 8, cutlass::half_t>>, Is_dropout=true, Is_causal=false, Is_even_N=false, Is_even_K=true, Return_softmax=true, Params=Flash_fwd_params]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(15): here
            instantiation of "void flash_fwd_kernel<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax>(Flash_fwd_params) [with Kernel_traits=Flash_fwd_kernel_traits<512, 128, 32, 8, false, true, cutlass::half_t, Flash_kernel_traits<512, 128, 32, 8, cutlass::half_t>>, Is_dropout=true, Is_causal=false, Is_even_N=false, Is_even_K=true, Return_softmax=true]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(34): here
            instantiation of "void run_flash_fwd<Kernel_traits,Is_dropout,Is_causal>(Flash_fwd_params &, cudaStream_t) [with Kernel_traits=Flash_fwd_kernel_traits<512, 128, 32, 8, false, true, cutlass::half_t, Flash_kernel_traits<512, 128, 32, 8, cutlass::half_t>>, Is_dropout=true, Is_causal=false]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(264): here
            instantiation of "void run_mha_fwd_hdim512<T>(Flash_fwd_params &, cudaStream_t) [with T=cutlass::half_t]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_hdim512_fp16_sm80.cu(8): here

/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_kernel.h(276): warning: variable "tQrQ" was declared but never referenced
          detected during:
            instantiation of "void flash::compute_attn_1rowblock<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params,Is_Q_split_K>(const Params &, int, int, int) [with Kernel_traits=Flash_fwd_kernel_traits<512, 128, 32, 8, false, true, cutlass::half_t, Flash_kernel_traits<512, 128, 32, 8, cutlass::half_t>>, Is_dropout=true, Is_causal=false, Is_even_N=false, Is_even_K=true, Return_softmax=true, Params=Flash_fwd_params, Is_Q_split_K=false]" 
(589): here
            instantiation of "void flash::compute_attn<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &) [with Kernel_traits=Flash_fwd_kernel_traits<512, 128, 32, 8, false, true, cutlass::half_t, Flash_kernel_traits<512, 128, 32, 8, cutlass::half_t>>, Is_dropout=true, Is_causal=false, Is_even_N=false, Is_even_K=true, Return_softmax=true, Params=Flash_fwd_params]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(15): here
            instantiation of "void flash_fwd_kernel<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax>(Flash_fwd_params) [with Kernel_traits=Flash_fwd_kernel_traits<512, 128, 32, 8, false, true, cutlass::half_t, Flash_kernel_traits<512, 128, 32, 8, cutlass::half_t>>, Is_dropout=true, Is_causal=false, Is_even_N=false, Is_even_K=true, Return_softmax=true]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(34): here
            instantiation of "void run_flash_fwd<Kernel_traits,Is_dropout,Is_causal>(Flash_fwd_params &, cudaStream_t) [with Kernel_traits=Flash_fwd_kernel_traits<512, 128, 32, 8, false, true, cutlass::half_t, Flash_kernel_traits<512, 128, 32, 8, cutlass::half_t>>, Is_dropout=true, Is_causal=false]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(264): here
            instantiation of "void run_mha_fwd_hdim512<T>(Flash_fwd_params &, cudaStream_t) [with T=cutlass::half_t]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_hdim512_fp16_sm80.cu(8): here

/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_kernel.h(140): warning: variable "MMA_M" was declared but never referenced
          detected during:
            instantiation of "void flash::compute_attn_1rowblock<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params,Is_Q_split_K>(const Params &, int, int, int) [with Kernel_traits=Flash_fwd_kernel_traits<512, 128, 32, 8, false, true, cutlass::half_t, Flash_kernel_traits<512, 128, 32, 8, cutlass::half_t>>, Is_dropout=true, Is_causal=false, Is_even_N=false, Is_even_K=true, Return_softmax=false, Params=Flash_fwd_params, Is_Q_split_K=false]" 
(589): here
            instantiation of "void flash::compute_attn<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &) [with Kernel_traits=Flash_fwd_kernel_traits<512, 128, 32, 8, false, true, cutlass::half_t, Flash_kernel_traits<512, 128, 32, 8, cutlass::half_t>>, Is_dropout=true, Is_causal=false, Is_even_N=false, Is_even_K=true, Return_softmax=false, Params=Flash_fwd_params]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(15): here
            instantiation of "void flash_fwd_kernel<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax>(Flash_fwd_params) [with Kernel_traits=Flash_fwd_kernel_traits<512, 128, 32, 8, false, true, cutlass::half_t, Flash_kernel_traits<512, 128, 32, 8, cutlass::half_t>>, Is_dropout=true, Is_causal=false, Is_even_N=false, Is_even_K=true, Return_softmax=false]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(34): here
            instantiation of "void run_flash_fwd<Kernel_traits,Is_dropout,Is_causal>(Flash_fwd_params &, cudaStream_t) [with Kernel_traits=Flash_fwd_kernel_traits<512, 128, 32, 8, false, true, cutlass::half_t, Flash_kernel_traits<512, 128, 32, 8, cutlass::half_t>>, Is_dropout=true, Is_causal=false]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(264): here
            instantiation of "void run_mha_fwd_hdim512<T>(Flash_fwd_params &, cudaStream_t) [with T=cutlass::half_t]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_hdim512_fp16_sm80.cu(8): here

/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_kernel.h(276): warning: variable "tQrQ" was declared but never referenced
          detected during:
            instantiation of "void flash::compute_attn_1rowblock<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params,Is_Q_split_K>(const Params &, int, int, int) [with Kernel_traits=Flash_fwd_kernel_traits<512, 128, 32, 8, false, true, cutlass::half_t, Flash_kernel_traits<512, 128, 32, 8, cutlass::half_t>>, Is_dropout=true, Is_causal=false, Is_even_N=false, Is_even_K=true, Return_softmax=false, Params=Flash_fwd_params, Is_Q_split_K=false]" 
(589): here
            instantiation of "void flash::compute_attn<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &) [with Kernel_traits=Flash_fwd_kernel_traits<512, 128, 32, 8, false, true, cutlass::half_t, Flash_kernel_traits<512, 128, 32, 8, cutlass::half_t>>, Is_dropout=true, Is_causal=false, Is_even_N=false, Is_even_K=true, Return_softmax=false, Params=Flash_fwd_params]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(15): here
            instantiation of "void flash_fwd_kernel<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax>(Flash_fwd_params) [with Kernel_traits=Flash_fwd_kernel_traits<512, 128, 32, 8, false, true, cutlass::half_t, Flash_kernel_traits<512, 128, 32, 8, cutlass::half_t>>, Is_dropout=true, Is_causal=false, Is_even_N=false, Is_even_K=true, Return_softmax=false]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(34): here
            instantiation of "void run_flash_fwd<Kernel_traits,Is_dropout,Is_causal>(Flash_fwd_params &, cudaStream_t) [with Kernel_traits=Flash_fwd_kernel_traits<512, 128, 32, 8, false, true, cutlass::half_t, Flash_kernel_traits<512, 128, 32, 8, cutlass::half_t>>, Is_dropout=true, Is_causal=false]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(264): here
            instantiation of "void run_mha_fwd_hdim512<T>(Flash_fwd_params &, cudaStream_t) [with T=cutlass::half_t]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_hdim512_fp16_sm80.cu(8): here

/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_kernel.h(140): warning: variable "MMA_M" was declared but never referenced
          detected during:
            instantiation of "void flash::compute_attn_1rowblock<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params,Is_Q_split_K>(const Params &, int, int, int) [with Kernel_traits=Flash_fwd_kernel_traits<512, 128, 32, 8, false, true, cutlass::half_t, Flash_kernel_traits<512, 128, 32, 8, cutlass::half_t>>, Is_dropout=true, Is_causal=false, Is_even_N=false, Is_even_K=false, Return_softmax=true, Params=Flash_fwd_params, Is_Q_split_K=false]" 
(589): here
            instantiation of "void flash::compute_attn<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &) [with Kernel_traits=Flash_fwd_kernel_traits<512, 128, 32, 8, false, true, cutlass::half_t, Flash_kernel_traits<512, 128, 32, 8, cutlass::half_t>>, Is_dropout=true, Is_causal=false, Is_even_N=false, Is_even_K=false, Return_softmax=true, Params=Flash_fwd_params]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(15): here
            instantiation of "void flash_fwd_kernel<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax>(Flash_fwd_params) [with Kernel_traits=Flash_fwd_kernel_traits<512, 128, 32, 8, false, true, cutlass::half_t, Flash_kernel_traits<512, 128, 32, 8, cutlass::half_t>>, Is_dropout=true, Is_causal=false, Is_even_N=false, Is_even_K=false, Return_softmax=true]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(34): here
            instantiation of "void run_flash_fwd<Kernel_traits,Is_dropout,Is_causal>(Flash_fwd_params &, cudaStream_t) [with Kernel_traits=Flash_fwd_kernel_traits<512, 128, 32, 8, false, true, cutlass::half_t, Flash_kernel_traits<512, 128, 32, 8, cutlass::half_t>>, Is_dropout=true, Is_causal=false]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(264): here
            instantiation of "void run_mha_fwd_hdim512<T>(Flash_fwd_params &, cudaStream_t) [with T=cutlass::half_t]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_hdim512_fp16_sm80.cu(8): here

/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_kernel.h(276): warning: variable "tQrQ" was declared but never referenced
          detected during:
            instantiation of "void flash::compute_attn_1rowblock<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params,Is_Q_split_K>(const Params &, int, int, int) [with Kernel_traits=Flash_fwd_kernel_traits<512, 128, 32, 8, false, true, cutlass::half_t, Flash_kernel_traits<512, 128, 32, 8, cutlass::half_t>>, Is_dropout=true, Is_causal=false, Is_even_N=false, Is_even_K=false, Return_softmax=true, Params=Flash_fwd_params, Is_Q_split_K=false]" 
(589): here
            instantiation of "void flash::compute_attn<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &) [with Kernel_traits=Flash_fwd_kernel_traits<512, 128, 32, 8, false, true, cutlass::half_t, Flash_kernel_traits<512, 128, 32, 8, cutlass::half_t>>, Is_dropout=true, Is_causal=false, Is_even_N=false, Is_even_K=false, Return_softmax=true, Params=Flash_fwd_params]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(15): here
            instantiation of "void flash_fwd_kernel<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax>(Flash_fwd_params) [with Kernel_traits=Flash_fwd_kernel_traits<512, 128, 32, 8, false, true, cutlass::half_t, Flash_kernel_traits<512, 128, 32, 8, cutlass::half_t>>, Is_dropout=true, Is_causal=false, Is_even_N=false, Is_even_K=false, Return_softmax=true]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(34): here
            instantiation of "void run_flash_fwd<Kernel_traits,Is_dropout,Is_causal>(Flash_fwd_params &, cudaStream_t) [with Kernel_traits=Flash_fwd_kernel_traits<512, 128, 32, 8, false, true, cutlass::half_t, Flash_kernel_traits<512, 128, 32, 8, cutlass::half_t>>, Is_dropout=true, Is_causal=false]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(264): here
            instantiation of "void run_mha_fwd_hdim512<T>(Flash_fwd_params &, cudaStream_t) [with T=cutlass::half_t]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_hdim512_fp16_sm80.cu(8): here

/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_kernel.h(140): warning: variable "MMA_M" was declared but never referenced
          detected during:
            instantiation of "void flash::compute_attn_1rowblock<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params,Is_Q_split_K>(const Params &, int, int, int) [with Kernel_traits=Flash_fwd_kernel_traits<512, 128, 32, 8, false, true, cutlass::half_t, Flash_kernel_traits<512, 128, 32, 8, cutlass::half_t>>, Is_dropout=true, Is_causal=false, Is_even_N=false, Is_even_K=false, Return_softmax=false, Params=Flash_fwd_params, Is_Q_split_K=false]" 
(589): here
            instantiation of "void flash::compute_attn<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &) [with Kernel_traits=Flash_fwd_kernel_traits<512, 128, 32, 8, false, true, cutlass::half_t, Flash_kernel_traits<512, 128, 32, 8, cutlass::half_t>>, Is_dropout=true, Is_causal=false, Is_even_N=false, Is_even_K=false, Return_softmax=false, Params=Flash_fwd_params]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(15): here
            instantiation of "void flash_fwd_kernel<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax>(Flash_fwd_params) [with Kernel_traits=Flash_fwd_kernel_traits<512, 128, 32, 8, false, true, cutlass::half_t, Flash_kernel_traits<512, 128, 32, 8, cutlass::half_t>>, Is_dropout=true, Is_causal=false, Is_even_N=false, Is_even_K=false, Return_softmax=false]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(34): here
            instantiation of "void run_flash_fwd<Kernel_traits,Is_dropout,Is_causal>(Flash_fwd_params &, cudaStream_t) [with Kernel_traits=Flash_fwd_kernel_traits<512, 128, 32, 8, false, true, cutlass::half_t, Flash_kernel_traits<512, 128, 32, 8, cutlass::half_t>>, Is_dropout=true, Is_causal=false]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(264): here
            instantiation of "void run_mha_fwd_hdim512<T>(Flash_fwd_params &, cudaStream_t) [with T=cutlass::half_t]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_hdim512_fp16_sm80.cu(8): here

/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_kernel.h(276): warning: variable "tQrQ" was declared but never referenced
          detected during:
            instantiation of "void flash::compute_attn_1rowblock<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params,Is_Q_split_K>(const Params &, int, int, int) [with Kernel_traits=Flash_fwd_kernel_traits<512, 128, 32, 8, false, true, cutlass::half_t, Flash_kernel_traits<512, 128, 32, 8, cutlass::half_t>>, Is_dropout=true, Is_causal=false, Is_even_N=false, Is_even_K=false, Return_softmax=false, Params=Flash_fwd_params, Is_Q_split_K=false]" 
(589): here
            instantiation of "void flash::compute_attn<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &) [with Kernel_traits=Flash_fwd_kernel_traits<512, 128, 32, 8, false, true, cutlass::half_t, Flash_kernel_traits<512, 128, 32, 8, cutlass::half_t>>, Is_dropout=true, Is_causal=false, Is_even_N=false, Is_even_K=false, Return_softmax=false, Params=Flash_fwd_params]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(15): here
            instantiation of "void flash_fwd_kernel<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax>(Flash_fwd_params) [with Kernel_traits=Flash_fwd_kernel_traits<512, 128, 32, 8, false, true, cutlass::half_t, Flash_kernel_traits<512, 128, 32, 8, cutlass::half_t>>, Is_dropout=true, Is_causal=false, Is_even_N=false, Is_even_K=false, Return_softmax=false]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(34): here
            instantiation of "void run_flash_fwd<Kernel_traits,Is_dropout,Is_causal>(Flash_fwd_params &, cudaStream_t) [with Kernel_traits=Flash_fwd_kernel_traits<512, 128, 32, 8, false, true, cutlass::half_t, Flash_kernel_traits<512, 128, 32, 8, cutlass::half_t>>, Is_dropout=true, Is_causal=false]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(264): here
            instantiation of "void run_mha_fwd_hdim512<T>(Flash_fwd_params &, cudaStream_t) [with T=cutlass::half_t]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_hdim512_fp16_sm80.cu(8): here

/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_kernel.h(140): warning: variable "MMA_M" was declared but never referenced
          detected during:
            instantiation of "void flash::compute_attn_1rowblock<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params,Is_Q_split_K>(const Params &, int, int, int) [with Kernel_traits=Flash_fwd_kernel_traits<512, 32, 32, 2, false, false, cutlass::half_t, Flash_kernel_traits<512, 32, 32, 2, cutlass::half_t>>, Is_dropout=true, Is_causal=false, Is_even_N=true, Is_even_K=true, Return_softmax=true, Params=Flash_fwd_params, Is_Q_split_K=false]" 
(589): here
            instantiation of "void flash::compute_attn<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &) [with Kernel_traits=Flash_fwd_kernel_traits<512, 32, 32, 2, false, false, cutlass::half_t, Flash_kernel_traits<512, 32, 32, 2, cutlass::half_t>>, Is_dropout=true, Is_causal=false, Is_even_N=true, Is_even_K=true, Return_softmax=true, Params=Flash_fwd_params]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(15): here
            instantiation of "void flash_fwd_kernel<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax>(Flash_fwd_params) [with Kernel_traits=Flash_fwd_kernel_traits<512, 32, 32, 2, false, false, cutlass::half_t, Flash_kernel_traits<512, 32, 32, 2, cutlass::half_t>>, Is_dropout=true, Is_causal=false, Is_even_N=true, Is_even_K=true, Return_softmax=true]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(34): here
            instantiation of "void run_flash_fwd<Kernel_traits,Is_dropout,Is_causal>(Flash_fwd_params &, cudaStream_t) [with Kernel_traits=Flash_fwd_kernel_traits<512, 32, 32, 2, false, false, cutlass::half_t, Flash_kernel_traits<512, 32, 32, 2, cutlass::half_t>>, Is_dropout=true, Is_causal=false]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(264): here
            instantiation of "void run_mha_fwd_hdim512<T>(Flash_fwd_params &, cudaStream_t) [with T=cutlass::half_t]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_hdim512_fp16_sm80.cu(8): here

/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_kernel.h(276): warning: variable "tQrQ" was declared but never referenced
          detected during:
            instantiation of "void flash::compute_attn_1rowblock<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params,Is_Q_split_K>(const Params &, int, int, int) [with Kernel_traits=Flash_fwd_kernel_traits<512, 32, 32, 2, false, false, cutlass::half_t, Flash_kernel_traits<512, 32, 32, 2, cutlass::half_t>>, Is_dropout=true, Is_causal=false, Is_even_N=true, Is_even_K=true, Return_softmax=true, Params=Flash_fwd_params, Is_Q_split_K=false]" 
(589): here
            instantiation of "void flash::compute_attn<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &) [with Kernel_traits=Flash_fwd_kernel_traits<512, 32, 32, 2, false, false, cutlass::half_t, Flash_kernel_traits<512, 32, 32, 2, cutlass::half_t>>, Is_dropout=true, Is_causal=false, Is_even_N=true, Is_even_K=true, Return_softmax=true, Params=Flash_fwd_params]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(15): here
            instantiation of "void flash_fwd_kernel<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax>(Flash_fwd_params) [with Kernel_traits=Flash_fwd_kernel_traits<512, 32, 32, 2, false, false, cutlass::half_t, Flash_kernel_traits<512, 32, 32, 2, cutlass::half_t>>, Is_dropout=true, Is_causal=false, Is_even_N=true, Is_even_K=true, Return_softmax=true]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(34): here
            instantiation of "void run_flash_fwd<Kernel_traits,Is_dropout,Is_causal>(Flash_fwd_params &, cudaStream_t) [with Kernel_traits=Flash_fwd_kernel_traits<512, 32, 32, 2, false, false, cutlass::half_t, Flash_kernel_traits<512, 32, 32, 2, cutlass::half_t>>, Is_dropout=true, Is_causal=false]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(264): here
            instantiation of "void run_mha_fwd_hdim512<T>(Flash_fwd_params &, cudaStream_t) [with T=cutlass::half_t]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_hdim512_fp16_sm80.cu(8): here

/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_kernel.h(140): warning: variable "MMA_M" was declared but never referenced
          detected during:
            instantiation of "void flash::compute_attn_1rowblock<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params,Is_Q_split_K>(const Params &, int, int, int) [with Kernel_traits=Flash_fwd_kernel_traits<512, 32, 32, 2, false, false, cutlass::half_t, Flash_kernel_traits<512, 32, 32, 2, cutlass::half_t>>, Is_dropout=true, Is_causal=false, Is_even_N=true, Is_even_K=true, Return_softmax=false, Params=Flash_fwd_params, Is_Q_split_K=false]" 
(589): here
            instantiation of "void flash::compute_attn<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &) [with Kernel_traits=Flash_fwd_kernel_traits<512, 32, 32, 2, false, false, cutlass::half_t, Flash_kernel_traits<512, 32, 32, 2, cutlass::half_t>>, Is_dropout=true, Is_causal=false, Is_even_N=true, Is_even_K=true, Return_softmax=false, Params=Flash_fwd_params]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(15): here
            instantiation of "void flash_fwd_kernel<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax>(Flash_fwd_params) [with Kernel_traits=Flash_fwd_kernel_traits<512, 32, 32, 2, false, false, cutlass::half_t, Flash_kernel_traits<512, 32, 32, 2, cutlass::half_t>>, Is_dropout=true, Is_causal=false, Is_even_N=true, Is_even_K=true, Return_softmax=false]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(34): here
            instantiation of "void run_flash_fwd<Kernel_traits,Is_dropout,Is_causal>(Flash_fwd_params &, cudaStream_t) [with Kernel_traits=Flash_fwd_kernel_traits<512, 32, 32, 2, false, false, cutlass::half_t, Flash_kernel_traits<512, 32, 32, 2, cutlass::half_t>>, Is_dropout=true, Is_causal=false]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(264): here
            instantiation of "void run_mha_fwd_hdim512<T>(Flash_fwd_params &, cudaStream_t) [with T=cutlass::half_t]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_hdim512_fp16_sm80.cu(8): here

/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_kernel.h(276): warning: variable "tQrQ" was declared but never referenced
          detected during:
            instantiation of "void flash::compute_attn_1rowblock<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params,Is_Q_split_K>(const Params &, int, int, int) [with Kernel_traits=Flash_fwd_kernel_traits<512, 32, 32, 2, false, false, cutlass::half_t, Flash_kernel_traits<512, 32, 32, 2, cutlass::half_t>>, Is_dropout=true, Is_causal=false, Is_even_N=true, Is_even_K=true, Return_softmax=false, Params=Flash_fwd_params, Is_Q_split_K=false]" 
(589): here
            instantiation of "void flash::compute_attn<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &) [with Kernel_traits=Flash_fwd_kernel_traits<512, 32, 32, 2, false, false, cutlass::half_t, Flash_kernel_traits<512, 32, 32, 2, cutlass::half_t>>, Is_dropout=true, Is_causal=false, Is_even_N=true, Is_even_K=true, Return_softmax=false, Params=Flash_fwd_params]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(15): here
            instantiation of "void flash_fwd_kernel<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax>(Flash_fwd_params) [with Kernel_traits=Flash_fwd_kernel_traits<512, 32, 32, 2, false, false, cutlass::half_t, Flash_kernel_traits<512, 32, 32, 2, cutlass::half_t>>, Is_dropout=true, Is_causal=false, Is_even_N=true, Is_even_K=true, Return_softmax=false]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(34): here
            instantiation of "void run_flash_fwd<Kernel_traits,Is_dropout,Is_causal>(Flash_fwd_params &, cudaStream_t) [with Kernel_traits=Flash_fwd_kernel_traits<512, 32, 32, 2, false, false, cutlass::half_t, Flash_kernel_traits<512, 32, 32, 2, cutlass::half_t>>, Is_dropout=true, Is_causal=false]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(264): here
            instantiation of "void run_mha_fwd_hdim512<T>(Flash_fwd_params &, cudaStream_t) [with T=cutlass::half_t]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_hdim512_fp16_sm80.cu(8): here

/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_kernel.h(140): warning: variable "MMA_M" was declared but never referenced
          detected during:
            instantiation of "void flash::compute_attn_1rowblock<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params,Is_Q_split_K>(const Params &, int, int, int) [with Kernel_traits=Flash_fwd_kernel_traits<512, 32, 32, 2, false, false, cutlass::half_t, Flash_kernel_traits<512, 32, 32, 2, cutlass::half_t>>, Is_dropout=true, Is_causal=false, Is_even_N=true, Is_even_K=false, Return_softmax=true, Params=Flash_fwd_params, Is_Q_split_K=false]" 
(589): here
            instantiation of "void flash::compute_attn<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &) [with Kernel_traits=Flash_fwd_kernel_traits<512, 32, 32, 2, false, false, cutlass::half_t, Flash_kernel_traits<512, 32, 32, 2, cutlass::half_t>>, Is_dropout=true, Is_causal=false, Is_even_N=true, Is_even_K=false, Return_softmax=true, Params=Flash_fwd_params]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(15): here
            instantiation of "void flash_fwd_kernel<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax>(Flash_fwd_params) [with Kernel_traits=Flash_fwd_kernel_traits<512, 32, 32, 2, false, false, cutlass::half_t, Flash_kernel_traits<512, 32, 32, 2, cutlass::half_t>>, Is_dropout=true, Is_causal=false, Is_even_N=true, Is_even_K=false, Return_softmax=true]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(34): here
            instantiation of "void run_flash_fwd<Kernel_traits,Is_dropout,Is_causal>(Flash_fwd_params &, cudaStream_t) [with Kernel_traits=Flash_fwd_kernel_traits<512, 32, 32, 2, false, false, cutlass::half_t, Flash_kernel_traits<512, 32, 32, 2, cutlass::half_t>>, Is_dropout=true, Is_causal=false]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(264): here
            instantiation of "void run_mha_fwd_hdim512<T>(Flash_fwd_params &, cudaStream_t) [with T=cutlass::half_t]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_hdim512_fp16_sm80.cu(8): here

/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_kernel.h(276): warning: variable "tQrQ" was declared but never referenced
          detected during:
            instantiation of "void flash::compute_attn_1rowblock<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params,Is_Q_split_K>(const Params &, int, int, int) [with Kernel_traits=Flash_fwd_kernel_traits<512, 32, 32, 2, false, false, cutlass::half_t, Flash_kernel_traits<512, 32, 32, 2, cutlass::half_t>>, Is_dropout=true, Is_causal=false, Is_even_N=true, Is_even_K=false, Return_softmax=true, Params=Flash_fwd_params, Is_Q_split_K=false]" 
(589): here
            instantiation of "void flash::compute_attn<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &) [with Kernel_traits=Flash_fwd_kernel_traits<512, 32, 32, 2, false, false, cutlass::half_t, Flash_kernel_traits<512, 32, 32, 2, cutlass::half_t>>, Is_dropout=true, Is_causal=false, Is_even_N=true, Is_even_K=false, Return_softmax=true, Params=Flash_fwd_params]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(15): here
            instantiation of "void flash_fwd_kernel<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax>(Flash_fwd_params) [with Kernel_traits=Flash_fwd_kernel_traits<512, 32, 32, 2, false, false, cutlass::half_t, Flash_kernel_traits<512, 32, 32, 2, cutlass::half_t>>, Is_dropout=true, Is_causal=false, Is_even_N=true, Is_even_K=false, Return_softmax=true]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(34): here
            instantiation of "void run_flash_fwd<Kernel_traits,Is_dropout,Is_causal>(Flash_fwd_params &, cudaStream_t) [with Kernel_traits=Flash_fwd_kernel_traits<512, 32, 32, 2, false, false, cutlass::half_t, Flash_kernel_traits<512, 32, 32, 2, cutlass::half_t>>, Is_dropout=true, Is_causal=false]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(264): here
            instantiation of "void run_mha_fwd_hdim512<T>(Flash_fwd_params &, cudaStream_t) [with T=cutlass::half_t]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_hdim512_fp16_sm80.cu(8): here

/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_kernel.h(140): warning: variable "MMA_M" was declared but never referenced
          detected during:
            instantiation of "void flash::compute_attn_1rowblock<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params,Is_Q_split_K>(const Params &, int, int, int) [with Kernel_traits=Flash_fwd_kernel_traits<512, 32, 32, 2, false, false, cutlass::half_t, Flash_kernel_traits<512, 32, 32, 2, cutlass::half_t>>, Is_dropout=true, Is_causal=false, Is_even_N=true, Is_even_K=false, Return_softmax=false, Params=Flash_fwd_params, Is_Q_split_K=false]" 
(589): here
            instantiation of "void flash::compute_attn<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &) [with Kernel_traits=Flash_fwd_kernel_traits<512, 32, 32, 2, false, false, cutlass::half_t, Flash_kernel_traits<512, 32, 32, 2, cutlass::half_t>>, Is_dropout=true, Is_causal=false, Is_even_N=true, Is_even_K=false, Return_softmax=false, Params=Flash_fwd_params]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(15): here
            instantiation of "void flash_fwd_kernel<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax>(Flash_fwd_params) [with Kernel_traits=Flash_fwd_kernel_traits<512, 32, 32, 2, false, false, cutlass::half_t, Flash_kernel_traits<512, 32, 32, 2, cutlass::half_t>>, Is_dropout=true, Is_causal=false, Is_even_N=true, Is_even_K=false, Return_softmax=false]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(34): here
            instantiation of "void run_flash_fwd<Kernel_traits,Is_dropout,Is_causal>(Flash_fwd_params &, cudaStream_t) [with Kernel_traits=Flash_fwd_kernel_traits<512, 32, 32, 2, false, false, cutlass::half_t, Flash_kernel_traits<512, 32, 32, 2, cutlass::half_t>>, Is_dropout=true, Is_causal=false]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(264): here
            instantiation of "void run_mha_fwd_hdim512<T>(Flash_fwd_params &, cudaStream_t) [with T=cutlass::half_t]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_hdim512_fp16_sm80.cu(8): here

/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_kernel.h(276): warning: variable "tQrQ" was declared but never referenced
          detected during:
            instantiation of "void flash::compute_attn_1rowblock<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params,Is_Q_split_K>(const Params &, int, int, int) [with Kernel_traits=Flash_fwd_kernel_traits<512, 32, 32, 2, false, false, cutlass::half_t, Flash_kernel_traits<512, 32, 32, 2, cutlass::half_t>>, Is_dropout=true, Is_causal=false, Is_even_N=true, Is_even_K=false, Return_softmax=false, Params=Flash_fwd_params, Is_Q_split_K=false]" 
(589): here
            instantiation of "void flash::compute_attn<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &) [with Kernel_traits=Flash_fwd_kernel_traits<512, 32, 32, 2, false, false, cutlass::half_t, Flash_kernel_traits<512, 32, 32, 2, cutlass::half_t>>, Is_dropout=true, Is_causal=false, Is_even_N=true, Is_even_K=false, Return_softmax=false, Params=Flash_fwd_params]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(15): here
            instantiation of "void flash_fwd_kernel<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax>(Flash_fwd_params) [with Kernel_traits=Flash_fwd_kernel_traits<512, 32, 32, 2, false, false, cutlass::half_t, Flash_kernel_traits<512, 32, 32, 2, cutlass::half_t>>, Is_dropout=true, Is_causal=false, Is_even_N=true, Is_even_K=false, Return_softmax=false]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(34): here
            instantiation of "void run_flash_fwd<Kernel_traits,Is_dropout,Is_causal>(Flash_fwd_params &, cudaStream_t) [with Kernel_traits=Flash_fwd_kernel_traits<512, 32, 32, 2, false, false, cutlass::half_t, Flash_kernel_traits<512, 32, 32, 2, cutlass::half_t>>, Is_dropout=true, Is_causal=false]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(264): here
            instantiation of "void run_mha_fwd_hdim512<T>(Flash_fwd_params &, cudaStream_t) [with T=cutlass::half_t]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_hdim512_fp16_sm80.cu(8): here

/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_kernel.h(140): warning: variable "MMA_M" was declared but never referenced
          detected during:
            instantiation of "void flash::compute_attn_1rowblock<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params,Is_Q_split_K>(const Params &, int, int, int) [with Kernel_traits=Flash_fwd_kernel_traits<512, 32, 32, 2, false, false, cutlass::half_t, Flash_kernel_traits<512, 32, 32, 2, cutlass::half_t>>, Is_dropout=true, Is_causal=false, Is_even_N=false, Is_even_K=true, Return_softmax=true, Params=Flash_fwd_params, Is_Q_split_K=false]" 
(589): here
            instantiation of "void flash::compute_attn<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &) [with Kernel_traits=Flash_fwd_kernel_traits<512, 32, 32, 2, false, false, cutlass::half_t, Flash_kernel_traits<512, 32, 32, 2, cutlass::half_t>>, Is_dropout=true, Is_causal=false, Is_even_N=false, Is_even_K=true, Return_softmax=true, Params=Flash_fwd_params]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(15): here
            instantiation of "void flash_fwd_kernel<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax>(Flash_fwd_params) [with Kernel_traits=Flash_fwd_kernel_traits<512, 32, 32, 2, false, false, cutlass::half_t, Flash_kernel_traits<512, 32, 32, 2, cutlass::half_t>>, Is_dropout=true, Is_causal=false, Is_even_N=false, Is_even_K=true, Return_softmax=true]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(34): here
            instantiation of "void run_flash_fwd<Kernel_traits,Is_dropout,Is_causal>(Flash_fwd_params &, cudaStream_t) [with Kernel_traits=Flash_fwd_kernel_traits<512, 32, 32, 2, false, false, cutlass::half_t, Flash_kernel_traits<512, 32, 32, 2, cutlass::half_t>>, Is_dropout=true, Is_causal=false]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(264): here
            instantiation of "void run_mha_fwd_hdim512<T>(Flash_fwd_params &, cudaStream_t) [with T=cutlass::half_t]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_hdim512_fp16_sm80.cu(8): here

/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_kernel.h(276): warning: variable "tQrQ" was declared but never referenced
          detected during:
            instantiation of "void flash::compute_attn_1rowblock<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params,Is_Q_split_K>(const Params &, int, int, int) [with Kernel_traits=Flash_fwd_kernel_traits<512, 32, 32, 2, false, false, cutlass::half_t, Flash_kernel_traits<512, 32, 32, 2, cutlass::half_t>>, Is_dropout=true, Is_causal=false, Is_even_N=false, Is_even_K=true, Return_softmax=true, Params=Flash_fwd_params, Is_Q_split_K=false]" 
(589): here
            instantiation of "void flash::compute_attn<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &) [with Kernel_traits=Flash_fwd_kernel_traits<512, 32, 32, 2, false, false, cutlass::half_t, Flash_kernel_traits<512, 32, 32, 2, cutlass::half_t>>, Is_dropout=true, Is_causal=false, Is_even_N=false, Is_even_K=true, Return_softmax=true, Params=Flash_fwd_params]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(15): here
            instantiation of "void flash_fwd_kernel<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax>(Flash_fwd_params) [with Kernel_traits=Flash_fwd_kernel_traits<512, 32, 32, 2, false, false, cutlass::half_t, Flash_kernel_traits<512, 32, 32, 2, cutlass::half_t>>, Is_dropout=true, Is_causal=false, Is_even_N=false, Is_even_K=true, Return_softmax=true]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(34): here
            instantiation of "void run_flash_fwd<Kernel_traits,Is_dropout,Is_causal>(Flash_fwd_params &, cudaStream_t) [with Kernel_traits=Flash_fwd_kernel_traits<512, 32, 32, 2, false, false, cutlass::half_t, Flash_kernel_traits<512, 32, 32, 2, cutlass::half_t>>, Is_dropout=true, Is_causal=false]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(264): here
            instantiation of "void run_mha_fwd_hdim512<T>(Flash_fwd_params &, cudaStream_t) [with T=cutlass::half_t]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_hdim512_fp16_sm80.cu(8): here

/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_kernel.h(140): warning: variable "MMA_M" was declared but never referenced
          detected during:
            instantiation of "void flash::compute_attn_1rowblock<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params,Is_Q_split_K>(const Params &, int, int, int) [with Kernel_traits=Flash_fwd_kernel_traits<512, 32, 32, 2, false, false, cutlass::half_t, Flash_kernel_traits<512, 32, 32, 2, cutlass::half_t>>, Is_dropout=true, Is_causal=false, Is_even_N=false, Is_even_K=true, Return_softmax=false, Params=Flash_fwd_params, Is_Q_split_K=false]" 
(589): here
            instantiation of "void flash::compute_attn<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &) [with Kernel_traits=Flash_fwd_kernel_traits<512, 32, 32, 2, false, false, cutlass::half_t, Flash_kernel_traits<512, 32, 32, 2, cutlass::half_t>>, Is_dropout=true, Is_causal=false, Is_even_N=false, Is_even_K=true, Return_softmax=false, Params=Flash_fwd_params]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(15): here
            instantiation of "void flash_fwd_kernel<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax>(Flash_fwd_params) [with Kernel_traits=Flash_fwd_kernel_traits<512, 32, 32, 2, false, false, cutlass::half_t, Flash_kernel_traits<512, 32, 32, 2, cutlass::half_t>>, Is_dropout=true, Is_causal=false, Is_even_N=false, Is_even_K=true, Return_softmax=false]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(34): here
            instantiation of "void run_flash_fwd<Kernel_traits,Is_dropout,Is_causal>(Flash_fwd_params &, cudaStream_t) [with Kernel_traits=Flash_fwd_kernel_traits<512, 32, 32, 2, false, false, cutlass::half_t, Flash_kernel_traits<512, 32, 32, 2, cutlass::half_t>>, Is_dropout=true, Is_causal=false]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(264): here
            instantiation of "void run_mha_fwd_hdim512<T>(Flash_fwd_params &, cudaStream_t) [with T=cutlass::half_t]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_hdim512_fp16_sm80.cu(8): here

/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_kernel.h(276): warning: variable "tQrQ" was declared but never referenced
          detected during:
            instantiation of "void flash::compute_attn_1rowblock<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params,Is_Q_split_K>(const Params &, int, int, int) [with Kernel_traits=Flash_fwd_kernel_traits<512, 32, 32, 2, false, false, cutlass::half_t, Flash_kernel_traits<512, 32, 32, 2, cutlass::half_t>>, Is_dropout=true, Is_causal=false, Is_even_N=false, Is_even_K=true, Return_softmax=false, Params=Flash_fwd_params, Is_Q_split_K=false]" 
(589): here
            instantiation of "void flash::compute_attn<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &) [with Kernel_traits=Flash_fwd_kernel_traits<512, 32, 32, 2, false, false, cutlass::half_t, Flash_kernel_traits<512, 32, 32, 2, cutlass::half_t>>, Is_dropout=true, Is_causal=false, Is_even_N=false, Is_even_K=true, Return_softmax=false, Params=Flash_fwd_params]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(15): here
            instantiation of "void flash_fwd_kernel<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax>(Flash_fwd_params) [with Kernel_traits=Flash_fwd_kernel_traits<512, 32, 32, 2, false, false, cutlass::half_t, Flash_kernel_traits<512, 32, 32, 2, cutlass::half_t>>, Is_dropout=true, Is_causal=false, Is_even_N=false, Is_even_K=true, Return_softmax=false]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(34): here
            instantiation of "void run_flash_fwd<Kernel_traits,Is_dropout,Is_causal>(Flash_fwd_params &, cudaStream_t) [with Kernel_traits=Flash_fwd_kernel_traits<512, 32, 32, 2, false, false, cutlass::half_t, Flash_kernel_traits<512, 32, 32, 2, cutlass::half_t>>, Is_dropout=true, Is_causal=false]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(264): here
            instantiation of "void run_mha_fwd_hdim512<T>(Flash_fwd_params &, cudaStream_t) [with T=cutlass::half_t]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_hdim512_fp16_sm80.cu(8): here

/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_kernel.h(140): warning: variable "MMA_M" was declared but never referenced
          detected during:
            instantiation of "void flash::compute_attn_1rowblock<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params,Is_Q_split_K>(const Params &, int, int, int) [with Kernel_traits=Flash_fwd_kernel_traits<512, 32, 32, 2, false, false, cutlass::half_t, Flash_kernel_traits<512, 32, 32, 2, cutlass::half_t>>, Is_dropout=true, Is_causal=false, Is_even_N=false, Is_even_K=false, Return_softmax=true, Params=Flash_fwd_params, Is_Q_split_K=false]" 
(589): here
            instantiation of "void flash::compute_attn<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &) [with Kernel_traits=Flash_fwd_kernel_traits<512, 32, 32, 2, false, false, cutlass::half_t, Flash_kernel_traits<512, 32, 32, 2, cutlass::half_t>>, Is_dropout=true, Is_causal=false, Is_even_N=false, Is_even_K=false, Return_softmax=true, Params=Flash_fwd_params]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(15): here
            instantiation of "void flash_fwd_kernel<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax>(Flash_fwd_params) [with Kernel_traits=Flash_fwd_kernel_traits<512, 32, 32, 2, false, false, cutlass::half_t, Flash_kernel_traits<512, 32, 32, 2, cutlass::half_t>>, Is_dropout=true, Is_causal=false, Is_even_N=false, Is_even_K=false, Return_softmax=true]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(34): here
            instantiation of "void run_flash_fwd<Kernel_traits,Is_dropout,Is_causal>(Flash_fwd_params &, cudaStream_t) [with Kernel_traits=Flash_fwd_kernel_traits<512, 32, 32, 2, false, false, cutlass::half_t, Flash_kernel_traits<512, 32, 32, 2, cutlass::half_t>>, Is_dropout=true, Is_causal=false]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(264): here
            instantiation of "void run_mha_fwd_hdim512<T>(Flash_fwd_params &, cudaStream_t) [with T=cutlass::half_t]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_hdim512_fp16_sm80.cu(8): here

/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_kernel.h(276): warning: variable "tQrQ" was declared but never referenced
          detected during:
            instantiation of "void flash::compute_attn_1rowblock<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params,Is_Q_split_K>(const Params &, int, int, int) [with Kernel_traits=Flash_fwd_kernel_traits<512, 32, 32, 2, false, false, cutlass::half_t, Flash_kernel_traits<512, 32, 32, 2, cutlass::half_t>>, Is_dropout=true, Is_causal=false, Is_even_N=false, Is_even_K=false, Return_softmax=true, Params=Flash_fwd_params, Is_Q_split_K=false]" 
(589): here
            instantiation of "void flash::compute_attn<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &) [with Kernel_traits=Flash_fwd_kernel_traits<512, 32, 32, 2, false, false, cutlass::half_t, Flash_kernel_traits<512, 32, 32, 2, cutlass::half_t>>, Is_dropout=true, Is_causal=false, Is_even_N=false, Is_even_K=false, Return_softmax=true, Params=Flash_fwd_params]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(15): here
            instantiation of "void flash_fwd_kernel<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax>(Flash_fwd_params) [with Kernel_traits=Flash_fwd_kernel_traits<512, 32, 32, 2, false, false, cutlass::half_t, Flash_kernel_traits<512, 32, 32, 2, cutlass::half_t>>, Is_dropout=true, Is_causal=false, Is_even_N=false, Is_even_K=false, Return_softmax=true]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(34): here
            instantiation of "void run_flash_fwd<Kernel_traits,Is_dropout,Is_causal>(Flash_fwd_params &, cudaStream_t) [with Kernel_traits=Flash_fwd_kernel_traits<512, 32, 32, 2, false, false, cutlass::half_t, Flash_kernel_traits<512, 32, 32, 2, cutlass::half_t>>, Is_dropout=true, Is_causal=false]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(264): here
            instantiation of "void run_mha_fwd_hdim512<T>(Flash_fwd_params &, cudaStream_t) [with T=cutlass::half_t]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_hdim512_fp16_sm80.cu(8): here

/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_kernel.h(140): warning: variable "MMA_M" was declared but never referenced
          detected during:
            instantiation of "void flash::compute_attn_1rowblock<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params,Is_Q_split_K>(const Params &, int, int, int) [with Kernel_traits=Flash_fwd_kernel_traits<512, 32, 32, 2, false, false, cutlass::half_t, Flash_kernel_traits<512, 32, 32, 2, cutlass::half_t>>, Is_dropout=true, Is_causal=false, Is_even_N=false, Is_even_K=false, Return_softmax=false, Params=Flash_fwd_params, Is_Q_split_K=false]" 
(589): here
            instantiation of "void flash::compute_attn<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &) [with Kernel_traits=Flash_fwd_kernel_traits<512, 32, 32, 2, false, false, cutlass::half_t, Flash_kernel_traits<512, 32, 32, 2, cutlass::half_t>>, Is_dropout=true, Is_causal=false, Is_even_N=false, Is_even_K=false, Return_softmax=false, Params=Flash_fwd_params]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(15): here
            instantiation of "void flash_fwd_kernel<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax>(Flash_fwd_params) [with Kernel_traits=Flash_fwd_kernel_traits<512, 32, 32, 2, false, false, cutlass::half_t, Flash_kernel_traits<512, 32, 32, 2, cutlass::half_t>>, Is_dropout=true, Is_causal=false, Is_even_N=false, Is_even_K=false, Return_softmax=false]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(34): here
            instantiation of "void run_flash_fwd<Kernel_traits,Is_dropout,Is_causal>(Flash_fwd_params &, cudaStream_t) [with Kernel_traits=Flash_fwd_kernel_traits<512, 32, 32, 2, false, false, cutlass::half_t, Flash_kernel_traits<512, 32, 32, 2, cutlass::half_t>>, Is_dropout=true, Is_causal=false]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(264): here
            instantiation of "void run_mha_fwd_hdim512<T>(Flash_fwd_params &, cudaStream_t) [with T=cutlass::half_t]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_hdim512_fp16_sm80.cu(8): here

/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_kernel.h(276): warning: variable "tQrQ" was declared but never referenced
          detected during:
            instantiation of "void flash::compute_attn_1rowblock<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params,Is_Q_split_K>(const Params &, int, int, int) [with Kernel_traits=Flash_fwd_kernel_traits<512, 32, 32, 2, false, false, cutlass::half_t, Flash_kernel_traits<512, 32, 32, 2, cutlass::half_t>>, Is_dropout=true, Is_causal=false, Is_even_N=false, Is_even_K=false, Return_softmax=false, Params=Flash_fwd_params, Is_Q_split_K=false]" 
(589): here
            instantiation of "void flash::compute_attn<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &) [with Kernel_traits=Flash_fwd_kernel_traits<512, 32, 32, 2, false, false, cutlass::half_t, Flash_kernel_traits<512, 32, 32, 2, cutlass::half_t>>, Is_dropout=true, Is_causal=false, Is_even_N=false, Is_even_K=false, Return_softmax=false, Params=Flash_fwd_params]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(15): here
            instantiation of "void flash_fwd_kernel<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax>(Flash_fwd_params) [with Kernel_traits=Flash_fwd_kernel_traits<512, 32, 32, 2, false, false, cutlass::half_t, Flash_kernel_traits<512, 32, 32, 2, cutlass::half_t>>, Is_dropout=true, Is_causal=false, Is_even_N=false, Is_even_K=false, Return_softmax=false]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(34): here
            instantiation of "void run_flash_fwd<Kernel_traits,Is_dropout,Is_causal>(Flash_fwd_params &, cudaStream_t) [with Kernel_traits=Flash_fwd_kernel_traits<512, 32, 32, 2, false, false, cutlass::half_t, Flash_kernel_traits<512, 32, 32, 2, cutlass::half_t>>, Is_dropout=true, Is_causal=false]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(264): here
            instantiation of "void run_mha_fwd_hdim512<T>(Flash_fwd_params &, cudaStream_t) [with T=cutlass::half_t]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_hdim512_fp16_sm80.cu(8): here

/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_kernel.h(140): warning: variable "MMA_M" was declared but never referenced
          detected during:
            instantiation of "void flash::compute_attn_1rowblock<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params,Is_Q_split_K>(const Params &, int, int, int) [with Kernel_traits=Flash_fwd_kernel_traits<512, 128, 32, 8, false, true, cutlass::half_t, Flash_kernel_traits<512, 128, 32, 8, cutlass::half_t>>, Is_dropout=false, Is_causal=true, Is_even_N=true, Is_even_K=true, Return_softmax=false, Params=Flash_fwd_params, Is_Q_split_K=false]" 
(589): here
            instantiation of "void flash::compute_attn<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &) [with Kernel_traits=Flash_fwd_kernel_traits<512, 128, 32, 8, false, true, cutlass::half_t, Flash_kernel_traits<512, 128, 32, 8, cutlass::half_t>>, Is_dropout=false, Is_causal=true, Is_even_N=true, Is_even_K=true, Return_softmax=false, Params=Flash_fwd_params]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(15): here
            instantiation of "void flash_fwd_kernel<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax>(Flash_fwd_params) [with Kernel_traits=Flash_fwd_kernel_traits<512, 128, 32, 8, false, true, cutlass::half_t, Flash_kernel_traits<512, 128, 32, 8, cutlass::half_t>>, Is_dropout=false, Is_causal=true, Is_even_N=true, Is_even_K=true, Return_softmax=false]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(34): here
            instantiation of "void run_flash_fwd<Kernel_traits,Is_dropout,Is_causal>(Flash_fwd_params &, cudaStream_t) [with Kernel_traits=Flash_fwd_kernel_traits<512, 128, 32, 8, false, true, cutlass::half_t, Flash_kernel_traits<512, 128, 32, 8, cutlass::half_t>>, Is_dropout=false, Is_causal=true]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(264): here
            instantiation of "void run_mha_fwd_hdim512<T>(Flash_fwd_params &, cudaStream_t) [with T=cutlass::half_t]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_hdim512_fp16_sm80.cu(8): here

/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_kernel.h(276): warning: variable "tQrQ" was declared but never referenced
          detected during:
            instantiation of "void flash::compute_attn_1rowblock<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params,Is_Q_split_K>(const Params &, int, int, int) [with Kernel_traits=Flash_fwd_kernel_traits<512, 128, 32, 8, false, true, cutlass::half_t, Flash_kernel_traits<512, 128, 32, 8, cutlass::half_t>>, Is_dropout=false, Is_causal=true, Is_even_N=true, Is_even_K=true, Return_softmax=false, Params=Flash_fwd_params, Is_Q_split_K=false]" 
(589): here
            instantiation of "void flash::compute_attn<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &) [with Kernel_traits=Flash_fwd_kernel_traits<512, 128, 32, 8, false, true, cutlass::half_t, Flash_kernel_traits<512, 128, 32, 8, cutlass::half_t>>, Is_dropout=false, Is_causal=true, Is_even_N=true, Is_even_K=true, Return_softmax=false, Params=Flash_fwd_params]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(15): here
            instantiation of "void flash_fwd_kernel<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax>(Flash_fwd_params) [with Kernel_traits=Flash_fwd_kernel_traits<512, 128, 32, 8, false, true, cutlass::half_t, Flash_kernel_traits<512, 128, 32, 8, cutlass::half_t>>, Is_dropout=false, Is_causal=true, Is_even_N=true, Is_even_K=true, Return_softmax=false]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(34): here
            instantiation of "void run_flash_fwd<Kernel_traits,Is_dropout,Is_causal>(Flash_fwd_params &, cudaStream_t) [with Kernel_traits=Flash_fwd_kernel_traits<512, 128, 32, 8, false, true, cutlass::half_t, Flash_kernel_traits<512, 128, 32, 8, cutlass::half_t>>, Is_dropout=false, Is_causal=true]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(264): here
            instantiation of "void run_mha_fwd_hdim512<T>(Flash_fwd_params &, cudaStream_t) [with T=cutlass::half_t]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_hdim512_fp16_sm80.cu(8): here

/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_kernel.h(140): warning: variable "MMA_M" was declared but never referenced
          detected during:
            instantiation of "void flash::compute_attn_1rowblock<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params,Is_Q_split_K>(const Params &, int, int, int) [with Kernel_traits=Flash_fwd_kernel_traits<512, 128, 32, 8, false, true, cutlass::half_t, Flash_kernel_traits<512, 128, 32, 8, cutlass::half_t>>, Is_dropout=false, Is_causal=true, Is_even_N=true, Is_even_K=false, Return_softmax=false, Params=Flash_fwd_params, Is_Q_split_K=false]" 
(589): here
            instantiation of "void flash::compute_attn<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &) [with Kernel_traits=Flash_fwd_kernel_traits<512, 128, 32, 8, false, true, cutlass::half_t, Flash_kernel_traits<512, 128, 32, 8, cutlass::half_t>>, Is_dropout=false, Is_causal=true, Is_even_N=true, Is_even_K=false, Return_softmax=false, Params=Flash_fwd_params]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(15): here
            instantiation of "void flash_fwd_kernel<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax>(Flash_fwd_params) [with Kernel_traits=Flash_fwd_kernel_traits<512, 128, 32, 8, false, true, cutlass::half_t, Flash_kernel_traits<512, 128, 32, 8, cutlass::half_t>>, Is_dropout=false, Is_causal=true, Is_even_N=true, Is_even_K=false, Return_softmax=false]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(34): here
            instantiation of "void run_flash_fwd<Kernel_traits,Is_dropout,Is_causal>(Flash_fwd_params &, cudaStream_t) [with Kernel_traits=Flash_fwd_kernel_traits<512, 128, 32, 8, false, true, cutlass::half_t, Flash_kernel_traits<512, 128, 32, 8, cutlass::half_t>>, Is_dropout=false, Is_causal=true]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(264): here
            instantiation of "void run_mha_fwd_hdim512<T>(Flash_fwd_params &, cudaStream_t) [with T=cutlass::half_t]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_hdim512_fp16_sm80.cu(8): here

/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_kernel.h(276): warning: variable "tQrQ" was declared but never referenced
          detected during:
            instantiation of "void flash::compute_attn_1rowblock<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params,Is_Q_split_K>(const Params &, int, int, int) [with Kernel_traits=Flash_fwd_kernel_traits<512, 128, 32, 8, false, true, cutlass::half_t, Flash_kernel_traits<512, 128, 32, 8, cutlass::half_t>>, Is_dropout=false, Is_causal=true, Is_even_N=true, Is_even_K=false, Return_softmax=false, Params=Flash_fwd_params, Is_Q_split_K=false]" 
(589): here
            instantiation of "void flash::compute_attn<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &) [with Kernel_traits=Flash_fwd_kernel_traits<512, 128, 32, 8, false, true, cutlass::half_t, Flash_kernel_traits<512, 128, 32, 8, cutlass::half_t>>, Is_dropout=false, Is_causal=true, Is_even_N=true, Is_even_K=false, Return_softmax=false, Params=Flash_fwd_params]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(15): here
            instantiation of "void flash_fwd_kernel<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax>(Flash_fwd_params) [with Kernel_traits=Flash_fwd_kernel_traits<512, 128, 32, 8, false, true, cutlass::half_t, Flash_kernel_traits<512, 128, 32, 8, cutlass::half_t>>, Is_dropout=false, Is_causal=true, Is_even_N=true, Is_even_K=false, Return_softmax=false]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(34): here
            instantiation of "void run_flash_fwd<Kernel_traits,Is_dropout,Is_causal>(Flash_fwd_params &, cudaStream_t) [with Kernel_traits=Flash_fwd_kernel_traits<512, 128, 32, 8, false, true, cutlass::half_t, Flash_kernel_traits<512, 128, 32, 8, cutlass::half_t>>, Is_dropout=false, Is_causal=true]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(264): here
            instantiation of "void run_mha_fwd_hdim512<T>(Flash_fwd_params &, cudaStream_t) [with T=cutlass::half_t]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_hdim512_fp16_sm80.cu(8): here

/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_kernel.h(140): warning: variable "MMA_M" was declared but never referenced
          detected during:
            instantiation of "void flash::compute_attn_1rowblock<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params,Is_Q_split_K>(const Params &, int, int, int) [with Kernel_traits=Flash_fwd_kernel_traits<512, 128, 32, 8, false, true, cutlass::half_t, Flash_kernel_traits<512, 128, 32, 8, cutlass::half_t>>, Is_dropout=false, Is_causal=true, Is_even_N=false, Is_even_K=true, Return_softmax=false, Params=Flash_fwd_params, Is_Q_split_K=false]" 
(589): here
            instantiation of "void flash::compute_attn<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &) [with Kernel_traits=Flash_fwd_kernel_traits<512, 128, 32, 8, false, true, cutlass::half_t, Flash_kernel_traits<512, 128, 32, 8, cutlass::half_t>>, Is_dropout=false, Is_causal=true, Is_even_N=false, Is_even_K=true, Return_softmax=false, Params=Flash_fwd_params]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(15): here
            instantiation of "void flash_fwd_kernel<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax>(Flash_fwd_params) [with Kernel_traits=Flash_fwd_kernel_traits<512, 128, 32, 8, false, true, cutlass::half_t, Flash_kernel_traits<512, 128, 32, 8, cutlass::half_t>>, Is_dropout=false, Is_causal=true, Is_even_N=false, Is_even_K=true, Return_softmax=false]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(34): here
            instantiation of "void run_flash_fwd<Kernel_traits,Is_dropout,Is_causal>(Flash_fwd_params &, cudaStream_t) [with Kernel_traits=Flash_fwd_kernel_traits<512, 128, 32, 8, false, true, cutlass::half_t, Flash_kernel_traits<512, 128, 32, 8, cutlass::half_t>>, Is_dropout=false, Is_causal=true]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(264): here
            instantiation of "void run_mha_fwd_hdim512<T>(Flash_fwd_params &, cudaStream_t) [with T=cutlass::half_t]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_hdim512_fp16_sm80.cu(8): here

/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_kernel.h(276): warning: variable "tQrQ" was declared but never referenced
          detected during:
            instantiation of "void flash::compute_attn_1rowblock<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params,Is_Q_split_K>(const Params &, int, int, int) [with Kernel_traits=Flash_fwd_kernel_traits<512, 128, 32, 8, false, true, cutlass::half_t, Flash_kernel_traits<512, 128, 32, 8, cutlass::half_t>>, Is_dropout=false, Is_causal=true, Is_even_N=false, Is_even_K=true, Return_softmax=false, Params=Flash_fwd_params, Is_Q_split_K=false]" 
(589): here
            instantiation of "void flash::compute_attn<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &) [with Kernel_traits=Flash_fwd_kernel_traits<512, 128, 32, 8, false, true, cutlass::half_t, Flash_kernel_traits<512, 128, 32, 8, cutlass::half_t>>, Is_dropout=false, Is_causal=true, Is_even_N=false, Is_even_K=true, Return_softmax=false, Params=Flash_fwd_params]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(15): here
            instantiation of "void flash_fwd_kernel<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax>(Flash_fwd_params) [with Kernel_traits=Flash_fwd_kernel_traits<512, 128, 32, 8, false, true, cutlass::half_t, Flash_kernel_traits<512, 128, 32, 8, cutlass::half_t>>, Is_dropout=false, Is_causal=true, Is_even_N=false, Is_even_K=true, Return_softmax=false]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(34): here
            instantiation of "void run_flash_fwd<Kernel_traits,Is_dropout,Is_causal>(Flash_fwd_params &, cudaStream_t) [with Kernel_traits=Flash_fwd_kernel_traits<512, 128, 32, 8, false, true, cutlass::half_t, Flash_kernel_traits<512, 128, 32, 8, cutlass::half_t>>, Is_dropout=false, Is_causal=true]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(264): here
            instantiation of "void run_mha_fwd_hdim512<T>(Flash_fwd_params &, cudaStream_t) [with T=cutlass::half_t]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_hdim512_fp16_sm80.cu(8): here

/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_kernel.h(140): warning: variable "MMA_M" was declared but never referenced
          detected during:
            instantiation of "void flash::compute_attn_1rowblock<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params,Is_Q_split_K>(const Params &, int, int, int) [with Kernel_traits=Flash_fwd_kernel_traits<512, 128, 32, 8, false, true, cutlass::half_t, Flash_kernel_traits<512, 128, 32, 8, cutlass::half_t>>, Is_dropout=false, Is_causal=true, Is_even_N=false, Is_even_K=false, Return_softmax=false, Params=Flash_fwd_params, Is_Q_split_K=false]" 
(589): here
            instantiation of "void flash::compute_attn<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &) [with Kernel_traits=Flash_fwd_kernel_traits<512, 128, 32, 8, false, true, cutlass::half_t, Flash_kernel_traits<512, 128, 32, 8, cutlass::half_t>>, Is_dropout=false, Is_causal=true, Is_even_N=false, Is_even_K=false, Return_softmax=false, Params=Flash_fwd_params]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(15): here
            instantiation of "void flash_fwd_kernel<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax>(Flash_fwd_params) [with Kernel_traits=Flash_fwd_kernel_traits<512, 128, 32, 8, false, true, cutlass::half_t, Flash_kernel_traits<512, 128, 32, 8, cutlass::half_t>>, Is_dropout=false, Is_causal=true, Is_even_N=false, Is_even_K=false, Return_softmax=false]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(34): here
            instantiation of "void run_flash_fwd<Kernel_traits,Is_dropout,Is_causal>(Flash_fwd_params &, cudaStream_t) [with Kernel_traits=Flash_fwd_kernel_traits<512, 128, 32, 8, false, true, cutlass::half_t, Flash_kernel_traits<512, 128, 32, 8, cutlass::half_t>>, Is_dropout=false, Is_causal=true]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(264): here
            instantiation of "void run_mha_fwd_hdim512<T>(Flash_fwd_params &, cudaStream_t) [with T=cutlass::half_t]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_hdim512_fp16_sm80.cu(8): here

/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_kernel.h(276): warning: variable "tQrQ" was declared but never referenced
          detected during:
            instantiation of "void flash::compute_attn_1rowblock<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params,Is_Q_split_K>(const Params &, int, int, int) [with Kernel_traits=Flash_fwd_kernel_traits<512, 128, 32, 8, false, true, cutlass::half_t, Flash_kernel_traits<512, 128, 32, 8, cutlass::half_t>>, Is_dropout=false, Is_causal=true, Is_even_N=false, Is_even_K=false, Return_softmax=false, Params=Flash_fwd_params, Is_Q_split_K=false]" 
(589): here
            instantiation of "void flash::compute_attn<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &) [with Kernel_traits=Flash_fwd_kernel_traits<512, 128, 32, 8, false, true, cutlass::half_t, Flash_kernel_traits<512, 128, 32, 8, cutlass::half_t>>, Is_dropout=false, Is_causal=true, Is_even_N=false, Is_even_K=false, Return_softmax=false, Params=Flash_fwd_params]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(15): here
            instantiation of "void flash_fwd_kernel<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax>(Flash_fwd_params) [with Kernel_traits=Flash_fwd_kernel_traits<512, 128, 32, 8, false, true, cutlass::half_t, Flash_kernel_traits<512, 128, 32, 8, cutlass::half_t>>, Is_dropout=false, Is_causal=true, Is_even_N=false, Is_even_K=false, Return_softmax=false]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(34): here
            instantiation of "void run_flash_fwd<Kernel_traits,Is_dropout,Is_causal>(Flash_fwd_params &, cudaStream_t) [with Kernel_traits=Flash_fwd_kernel_traits<512, 128, 32, 8, false, true, cutlass::half_t, Flash_kernel_traits<512, 128, 32, 8, cutlass::half_t>>, Is_dropout=false, Is_causal=true]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(264): here
            instantiation of "void run_mha_fwd_hdim512<T>(Flash_fwd_params &, cudaStream_t) [with T=cutlass::half_t]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_hdim512_fp16_sm80.cu(8): here

/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_kernel.h(140): warning: variable "MMA_M" was declared but never referenced
          detected during:
            instantiation of "void flash::compute_attn_1rowblock<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params,Is_Q_split_K>(const Params &, int, int, int) [with Kernel_traits=Flash_fwd_kernel_traits<512, 32, 32, 2, false, false, cutlass::half_t, Flash_kernel_traits<512, 32, 32, 2, cutlass::half_t>>, Is_dropout=false, Is_causal=true, Is_even_N=true, Is_even_K=true, Return_softmax=false, Params=Flash_fwd_params, Is_Q_split_K=false]" 
(589): here
            instantiation of "void flash::compute_attn<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &) [with Kernel_traits=Flash_fwd_kernel_traits<512, 32, 32, 2, false, false, cutlass::half_t, Flash_kernel_traits<512, 32, 32, 2, cutlass::half_t>>, Is_dropout=false, Is_causal=true, Is_even_N=true, Is_even_K=true, Return_softmax=false, Params=Flash_fwd_params]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(15): here
            instantiation of "void flash_fwd_kernel<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax>(Flash_fwd_params) [with Kernel_traits=Flash_fwd_kernel_traits<512, 32, 32, 2, false, false, cutlass::half_t, Flash_kernel_traits<512, 32, 32, 2, cutlass::half_t>>, Is_dropout=false, Is_causal=true, Is_even_N=true, Is_even_K=true, Return_softmax=false]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(34): here
            instantiation of "void run_flash_fwd<Kernel_traits,Is_dropout,Is_causal>(Flash_fwd_params &, cudaStream_t) [with Kernel_traits=Flash_fwd_kernel_traits<512, 32, 32, 2, false, false, cutlass::half_t, Flash_kernel_traits<512, 32, 32, 2, cutlass::half_t>>, Is_dropout=false, Is_causal=true]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(264): here
            instantiation of "void run_mha_fwd_hdim512<T>(Flash_fwd_params &, cudaStream_t) [with T=cutlass::half_t]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_hdim512_fp16_sm80.cu(8): here

/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_kernel.h(276): warning: variable "tQrQ" was declared but never referenced
          detected during:
            instantiation of "void flash::compute_attn_1rowblock<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params,Is_Q_split_K>(const Params &, int, int, int) [with Kernel_traits=Flash_fwd_kernel_traits<512, 32, 32, 2, false, false, cutlass::half_t, Flash_kernel_traits<512, 32, 32, 2, cutlass::half_t>>, Is_dropout=false, Is_causal=true, Is_even_N=true, Is_even_K=true, Return_softmax=false, Params=Flash_fwd_params, Is_Q_split_K=false]" 
(589): here
            instantiation of "void flash::compute_attn<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &) [with Kernel_traits=Flash_fwd_kernel_traits<512, 32, 32, 2, false, false, cutlass::half_t, Flash_kernel_traits<512, 32, 32, 2, cutlass::half_t>>, Is_dropout=false, Is_causal=true, Is_even_N=true, Is_even_K=true, Return_softmax=false, Params=Flash_fwd_params]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(15): here
            instantiation of "void flash_fwd_kernel<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax>(Flash_fwd_params) [with Kernel_traits=Flash_fwd_kernel_traits<512, 32, 32, 2, false, false, cutlass::half_t, Flash_kernel_traits<512, 32, 32, 2, cutlass::half_t>>, Is_dropout=false, Is_causal=true, Is_even_N=true, Is_even_K=true, Return_softmax=false]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(34): here
            instantiation of "void run_flash_fwd<Kernel_traits,Is_dropout,Is_causal>(Flash_fwd_params &, cudaStream_t) [with Kernel_traits=Flash_fwd_kernel_traits<512, 32, 32, 2, false, false, cutlass::half_t, Flash_kernel_traits<512, 32, 32, 2, cutlass::half_t>>, Is_dropout=false, Is_causal=true]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(264): here
            instantiation of "void run_mha_fwd_hdim512<T>(Flash_fwd_params &, cudaStream_t) [with T=cutlass::half_t]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_hdim512_fp16_sm80.cu(8): here

/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_kernel.h(140): warning: variable "MMA_M" was declared but never referenced
          detected during:
            instantiation of "void flash::compute_attn_1rowblock<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params,Is_Q_split_K>(const Params &, int, int, int) [with Kernel_traits=Flash_fwd_kernel_traits<512, 32, 32, 2, false, false, cutlass::half_t, Flash_kernel_traits<512, 32, 32, 2, cutlass::half_t>>, Is_dropout=false, Is_causal=true, Is_even_N=true, Is_even_K=false, Return_softmax=false, Params=Flash_fwd_params, Is_Q_split_K=false]" 
(589): here
            instantiation of "void flash::compute_attn<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &) [with Kernel_traits=Flash_fwd_kernel_traits<512, 32, 32, 2, false, false, cutlass::half_t, Flash_kernel_traits<512, 32, 32, 2, cutlass::half_t>>, Is_dropout=false, Is_causal=true, Is_even_N=true, Is_even_K=false, Return_softmax=false, Params=Flash_fwd_params]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(15): here
            instantiation of "void flash_fwd_kernel<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax>(Flash_fwd_params) [with Kernel_traits=Flash_fwd_kernel_traits<512, 32, 32, 2, false, false, cutlass::half_t, Flash_kernel_traits<512, 32, 32, 2, cutlass::half_t>>, Is_dropout=false, Is_causal=true, Is_even_N=true, Is_even_K=false, Return_softmax=false]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(34): here
            instantiation of "void run_flash_fwd<Kernel_traits,Is_dropout,Is_causal>(Flash_fwd_params &, cudaStream_t) [with Kernel_traits=Flash_fwd_kernel_traits<512, 32, 32, 2, false, false, cutlass::half_t, Flash_kernel_traits<512, 32, 32, 2, cutlass::half_t>>, Is_dropout=false, Is_causal=true]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(264): here
            instantiation of "void run_mha_fwd_hdim512<T>(Flash_fwd_params &, cudaStream_t) [with T=cutlass::half_t]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_hdim512_fp16_sm80.cu(8): here

/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_kernel.h(276): warning: variable "tQrQ" was declared but never referenced
          detected during:
            instantiation of "void flash::compute_attn_1rowblock<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params,Is_Q_split_K>(const Params &, int, int, int) [with Kernel_traits=Flash_fwd_kernel_traits<512, 32, 32, 2, false, false, cutlass::half_t, Flash_kernel_traits<512, 32, 32, 2, cutlass::half_t>>, Is_dropout=false, Is_causal=true, Is_even_N=true, Is_even_K=false, Return_softmax=false, Params=Flash_fwd_params, Is_Q_split_K=false]" 
(589): here
            instantiation of "void flash::compute_attn<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &) [with Kernel_traits=Flash_fwd_kernel_traits<512, 32, 32, 2, false, false, cutlass::half_t, Flash_kernel_traits<512, 32, 32, 2, cutlass::half_t>>, Is_dropout=false, Is_causal=true, Is_even_N=true, Is_even_K=false, Return_softmax=false, Params=Flash_fwd_params]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(15): here
            instantiation of "void flash_fwd_kernel<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax>(Flash_fwd_params) [with Kernel_traits=Flash_fwd_kernel_traits<512, 32, 32, 2, false, false, cutlass::half_t, Flash_kernel_traits<512, 32, 32, 2, cutlass::half_t>>, Is_dropout=false, Is_causal=true, Is_even_N=true, Is_even_K=false, Return_softmax=false]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(34): here
            instantiation of "void run_flash_fwd<Kernel_traits,Is_dropout,Is_causal>(Flash_fwd_params &, cudaStream_t) [with Kernel_traits=Flash_fwd_kernel_traits<512, 32, 32, 2, false, false, cutlass::half_t, Flash_kernel_traits<512, 32, 32, 2, cutlass::half_t>>, Is_dropout=false, Is_causal=true]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(264): here
            instantiation of "void run_mha_fwd_hdim512<T>(Flash_fwd_params &, cudaStream_t) [with T=cutlass::half_t]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_hdim512_fp16_sm80.cu(8): here

/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_kernel.h(140): warning: variable "MMA_M" was declared but never referenced
          detected during:
            instantiation of "void flash::compute_attn_1rowblock<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params,Is_Q_split_K>(const Params &, int, int, int) [with Kernel_traits=Flash_fwd_kernel_traits<512, 32, 32, 2, false, false, cutlass::half_t, Flash_kernel_traits<512, 32, 32, 2, cutlass::half_t>>, Is_dropout=false, Is_causal=true, Is_even_N=false, Is_even_K=true, Return_softmax=false, Params=Flash_fwd_params, Is_Q_split_K=false]" 
(589): here
            instantiation of "void flash::compute_attn<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &) [with Kernel_traits=Flash_fwd_kernel_traits<512, 32, 32, 2, false, false, cutlass::half_t, Flash_kernel_traits<512, 32, 32, 2, cutlass::half_t>>, Is_dropout=false, Is_causal=true, Is_even_N=false, Is_even_K=true, Return_softmax=false, Params=Flash_fwd_params]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(15): here
            instantiation of "void flash_fwd_kernel<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax>(Flash_fwd_params) [with Kernel_traits=Flash_fwd_kernel_traits<512, 32, 32, 2, false, false, cutlass::half_t, Flash_kernel_traits<512, 32, 32, 2, cutlass::half_t>>, Is_dropout=false, Is_causal=true, Is_even_N=false, Is_even_K=true, Return_softmax=false]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(34): here
            instantiation of "void run_flash_fwd<Kernel_traits,Is_dropout,Is_causal>(Flash_fwd_params &, cudaStream_t) [with Kernel_traits=Flash_fwd_kernel_traits<512, 32, 32, 2, false, false, cutlass::half_t, Flash_kernel_traits<512, 32, 32, 2, cutlass::half_t>>, Is_dropout=false, Is_causal=true]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(264): here
            instantiation of "void run_mha_fwd_hdim512<T>(Flash_fwd_params &, cudaStream_t) [with T=cutlass::half_t]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_hdim512_fp16_sm80.cu(8): here

/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_kernel.h(276): warning: variable "tQrQ" was declared but never referenced
          detected during:
            instantiation of "void flash::compute_attn_1rowblock<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params,Is_Q_split_K>(const Params &, int, int, int) [with Kernel_traits=Flash_fwd_kernel_traits<512, 32, 32, 2, false, false, cutlass::half_t, Flash_kernel_traits<512, 32, 32, 2, cutlass::half_t>>, Is_dropout=false, Is_causal=true, Is_even_N=false, Is_even_K=true, Return_softmax=false, Params=Flash_fwd_params, Is_Q_split_K=false]" 
(589): here
            instantiation of "void flash::compute_attn<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &) [with Kernel_traits=Flash_fwd_kernel_traits<512, 32, 32, 2, false, false, cutlass::half_t, Flash_kernel_traits<512, 32, 32, 2, cutlass::half_t>>, Is_dropout=false, Is_causal=true, Is_even_N=false, Is_even_K=true, Return_softmax=false, Params=Flash_fwd_params]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(15): here
            instantiation of "void flash_fwd_kernel<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax>(Flash_fwd_params) [with Kernel_traits=Flash_fwd_kernel_traits<512, 32, 32, 2, false, false, cutlass::half_t, Flash_kernel_traits<512, 32, 32, 2, cutlass::half_t>>, Is_dropout=false, Is_causal=true, Is_even_N=false, Is_even_K=true, Return_softmax=false]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(34): here
            instantiation of "void run_flash_fwd<Kernel_traits,Is_dropout,Is_causal>(Flash_fwd_params &, cudaStream_t) [with Kernel_traits=Flash_fwd_kernel_traits<512, 32, 32, 2, false, false, cutlass::half_t, Flash_kernel_traits<512, 32, 32, 2, cutlass::half_t>>, Is_dropout=false, Is_causal=true]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(264): here
            instantiation of "void run_mha_fwd_hdim512<T>(Flash_fwd_params &, cudaStream_t) [with T=cutlass::half_t]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_hdim512_fp16_sm80.cu(8): here

/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_kernel.h(140): warning: variable "MMA_M" was declared but never referenced
          detected during:
            instantiation of "void flash::compute_attn_1rowblock<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params,Is_Q_split_K>(const Params &, int, int, int) [with Kernel_traits=Flash_fwd_kernel_traits<512, 32, 32, 2, false, false, cutlass::half_t, Flash_kernel_traits<512, 32, 32, 2, cutlass::half_t>>, Is_dropout=false, Is_causal=true, Is_even_N=false, Is_even_K=false, Return_softmax=false, Params=Flash_fwd_params, Is_Q_split_K=false]" 
(589): here
            instantiation of "void flash::compute_attn<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &) [with Kernel_traits=Flash_fwd_kernel_traits<512, 32, 32, 2, false, false, cutlass::half_t, Flash_kernel_traits<512, 32, 32, 2, cutlass::half_t>>, Is_dropout=false, Is_causal=true, Is_even_N=false, Is_even_K=false, Return_softmax=false, Params=Flash_fwd_params]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(15): here
            instantiation of "void flash_fwd_kernel<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax>(Flash_fwd_params) [with Kernel_traits=Flash_fwd_kernel_traits<512, 32, 32, 2, false, false, cutlass::half_t, Flash_kernel_traits<512, 32, 32, 2, cutlass::half_t>>, Is_dropout=false, Is_causal=true, Is_even_N=false, Is_even_K=false, Return_softmax=false]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(34): here
            instantiation of "void run_flash_fwd<Kernel_traits,Is_dropout,Is_causal>(Flash_fwd_params &, cudaStream_t) [with Kernel_traits=Flash_fwd_kernel_traits<512, 32, 32, 2, false, false, cutlass::half_t, Flash_kernel_traits<512, 32, 32, 2, cutlass::half_t>>, Is_dropout=false, Is_causal=true]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(264): here
            instantiation of "void run_mha_fwd_hdim512<T>(Flash_fwd_params &, cudaStream_t) [with T=cutlass::half_t]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_hdim512_fp16_sm80.cu(8): here

/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_kernel.h(276): warning: variable "tQrQ" was declared but never referenced
          detected during:
            instantiation of "void flash::compute_attn_1rowblock<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params,Is_Q_split_K>(const Params &, int, int, int) [with Kernel_traits=Flash_fwd_kernel_traits<512, 32, 32, 2, false, false, cutlass::half_t, Flash_kernel_traits<512, 32, 32, 2, cutlass::half_t>>, Is_dropout=false, Is_causal=true, Is_even_N=false, Is_even_K=false, Return_softmax=false, Params=Flash_fwd_params, Is_Q_split_K=false]" 
(589): here
            instantiation of "void flash::compute_attn<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &) [with Kernel_traits=Flash_fwd_kernel_traits<512, 32, 32, 2, false, false, cutlass::half_t, Flash_kernel_traits<512, 32, 32, 2, cutlass::half_t>>, Is_dropout=false, Is_causal=true, Is_even_N=false, Is_even_K=false, Return_softmax=false, Params=Flash_fwd_params]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(15): here
            instantiation of "void flash_fwd_kernel<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax>(Flash_fwd_params) [with Kernel_traits=Flash_fwd_kernel_traits<512, 32, 32, 2, false, false, cutlass::half_t, Flash_kernel_traits<512, 32, 32, 2, cutlass::half_t>>, Is_dropout=false, Is_causal=true, Is_even_N=false, Is_even_K=false, Return_softmax=false]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(34): here
            instantiation of "void run_flash_fwd<Kernel_traits,Is_dropout,Is_causal>(Flash_fwd_params &, cudaStream_t) [with Kernel_traits=Flash_fwd_kernel_traits<512, 32, 32, 2, false, false, cutlass::half_t, Flash_kernel_traits<512, 32, 32, 2, cutlass::half_t>>, Is_dropout=false, Is_causal=true]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(264): here
            instantiation of "void run_mha_fwd_hdim512<T>(Flash_fwd_params &, cudaStream_t) [with T=cutlass::half_t]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_hdim512_fp16_sm80.cu(8): here

/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_kernel.h(140): warning: variable "MMA_M" was declared but never referenced
          detected during:
            instantiation of "void flash::compute_attn_1rowblock<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params,Is_Q_split_K>(const Params &, int, int, int) [with Kernel_traits=Flash_fwd_kernel_traits<512, 128, 32, 8, false, true, cutlass::half_t, Flash_kernel_traits<512, 128, 32, 8, cutlass::half_t>>, Is_dropout=false, Is_causal=false, Is_even_N=true, Is_even_K=true, Return_softmax=false, Params=Flash_fwd_params, Is_Q_split_K=false]" 
(589): here
            instantiation of "void flash::compute_attn<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &) [with Kernel_traits=Flash_fwd_kernel_traits<512, 128, 32, 8, false, true, cutlass::half_t, Flash_kernel_traits<512, 128, 32, 8, cutlass::half_t>>, Is_dropout=false, Is_causal=false, Is_even_N=true, Is_even_K=true, Return_softmax=false, Params=Flash_fwd_params]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(15): here
            instantiation of "void flash_fwd_kernel<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax>(Flash_fwd_params) [with Kernel_traits=Flash_fwd_kernel_traits<512, 128, 32, 8, false, true, cutlass::half_t, Flash_kernel_traits<512, 128, 32, 8, cutlass::half_t>>, Is_dropout=false, Is_causal=false, Is_even_N=true, Is_even_K=true, Return_softmax=false]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(34): here
            instantiation of "void run_flash_fwd<Kernel_traits,Is_dropout,Is_causal>(Flash_fwd_params &, cudaStream_t) [with Kernel_traits=Flash_fwd_kernel_traits<512, 128, 32, 8, false, true, cutlass::half_t, Flash_kernel_traits<512, 128, 32, 8, cutlass::half_t>>, Is_dropout=false, Is_causal=false]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(264): here
            instantiation of "void run_mha_fwd_hdim512<T>(Flash_fwd_params &, cudaStream_t) [with T=cutlass::half_t]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_hdim512_fp16_sm80.cu(8): here

/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_kernel.h(276): warning: variable "tQrQ" was declared but never referenced
          detected during:
            instantiation of "void flash::compute_attn_1rowblock<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params,Is_Q_split_K>(const Params &, int, int, int) [with Kernel_traits=Flash_fwd_kernel_traits<512, 128, 32, 8, false, true, cutlass::half_t, Flash_kernel_traits<512, 128, 32, 8, cutlass::half_t>>, Is_dropout=false, Is_causal=false, Is_even_N=true, Is_even_K=true, Return_softmax=false, Params=Flash_fwd_params, Is_Q_split_K=false]" 
(589): here
            instantiation of "void flash::compute_attn<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &) [with Kernel_traits=Flash_fwd_kernel_traits<512, 128, 32, 8, false, true, cutlass::half_t, Flash_kernel_traits<512, 128, 32, 8, cutlass::half_t>>, Is_dropout=false, Is_causal=false, Is_even_N=true, Is_even_K=true, Return_softmax=false, Params=Flash_fwd_params]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(15): here
            instantiation of "void flash_fwd_kernel<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax>(Flash_fwd_params) [with Kernel_traits=Flash_fwd_kernel_traits<512, 128, 32, 8, false, true, cutlass::half_t, Flash_kernel_traits<512, 128, 32, 8, cutlass::half_t>>, Is_dropout=false, Is_causal=false, Is_even_N=true, Is_even_K=true, Return_softmax=false]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(34): here
            instantiation of "void run_flash_fwd<Kernel_traits,Is_dropout,Is_causal>(Flash_fwd_params &, cudaStream_t) [with Kernel_traits=Flash_fwd_kernel_traits<512, 128, 32, 8, false, true, cutlass::half_t, Flash_kernel_traits<512, 128, 32, 8, cutlass::half_t>>, Is_dropout=false, Is_causal=false]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(264): here
            instantiation of "void run_mha_fwd_hdim512<T>(Flash_fwd_params &, cudaStream_t) [with T=cutlass::half_t]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_hdim512_fp16_sm80.cu(8): here

/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_kernel.h(140): warning: variable "MMA_M" was declared but never referenced
          detected during:
            instantiation of "void flash::compute_attn_1rowblock<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params,Is_Q_split_K>(const Params &, int, int, int) [with Kernel_traits=Flash_fwd_kernel_traits<512, 128, 32, 8, false, true, cutlass::half_t, Flash_kernel_traits<512, 128, 32, 8, cutlass::half_t>>, Is_dropout=false, Is_causal=false, Is_even_N=true, Is_even_K=false, Return_softmax=false, Params=Flash_fwd_params, Is_Q_split_K=false]" 
(589): here
            instantiation of "void flash::compute_attn<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &) [with Kernel_traits=Flash_fwd_kernel_traits<512, 128, 32, 8, false, true, cutlass::half_t, Flash_kernel_traits<512, 128, 32, 8, cutlass::half_t>>, Is_dropout=false, Is_causal=false, Is_even_N=true, Is_even_K=false, Return_softmax=false, Params=Flash_fwd_params]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(15): here
            instantiation of "void flash_fwd_kernel<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax>(Flash_fwd_params) [with Kernel_traits=Flash_fwd_kernel_traits<512, 128, 32, 8, false, true, cutlass::half_t, Flash_kernel_traits<512, 128, 32, 8, cutlass::half_t>>, Is_dropout=false, Is_causal=false, Is_even_N=true, Is_even_K=false, Return_softmax=false]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(34): here
            instantiation of "void run_flash_fwd<Kernel_traits,Is_dropout,Is_causal>(Flash_fwd_params &, cudaStream_t) [with Kernel_traits=Flash_fwd_kernel_traits<512, 128, 32, 8, false, true, cutlass::half_t, Flash_kernel_traits<512, 128, 32, 8, cutlass::half_t>>, Is_dropout=false, Is_causal=false]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(264): here
            instantiation of "void run_mha_fwd_hdim512<T>(Flash_fwd_params &, cudaStream_t) [with T=cutlass::half_t]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_hdim512_fp16_sm80.cu(8): here

/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_kernel.h(276): warning: variable "tQrQ" was declared but never referenced
          detected during:
            instantiation of "void flash::compute_attn_1rowblock<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params,Is_Q_split_K>(const Params &, int, int, int) [with Kernel_traits=Flash_fwd_kernel_traits<512, 128, 32, 8, false, true, cutlass::half_t, Flash_kernel_traits<512, 128, 32, 8, cutlass::half_t>>, Is_dropout=false, Is_causal=false, Is_even_N=true, Is_even_K=false, Return_softmax=false, Params=Flash_fwd_params, Is_Q_split_K=false]" 
(589): here
            instantiation of "void flash::compute_attn<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &) [with Kernel_traits=Flash_fwd_kernel_traits<512, 128, 32, 8, false, true, cutlass::half_t, Flash_kernel_traits<512, 128, 32, 8, cutlass::half_t>>, Is_dropout=false, Is_causal=false, Is_even_N=true, Is_even_K=false, Return_softmax=false, Params=Flash_fwd_params]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(15): here
            instantiation of "void flash_fwd_kernel<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax>(Flash_fwd_params) [with Kernel_traits=Flash_fwd_kernel_traits<512, 128, 32, 8, false, true, cutlass::half_t, Flash_kernel_traits<512, 128, 32, 8, cutlass::half_t>>, Is_dropout=false, Is_causal=false, Is_even_N=true, Is_even_K=false, Return_softmax=false]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(34): here
            instantiation of "void run_flash_fwd<Kernel_traits,Is_dropout,Is_causal>(Flash_fwd_params &, cudaStream_t) [with Kernel_traits=Flash_fwd_kernel_traits<512, 128, 32, 8, false, true, cutlass::half_t, Flash_kernel_traits<512, 128, 32, 8, cutlass::half_t>>, Is_dropout=false, Is_causal=false]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(264): here
            instantiation of "void run_mha_fwd_hdim512<T>(Flash_fwd_params &, cudaStream_t) [with T=cutlass::half_t]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_hdim512_fp16_sm80.cu(8): here

/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_kernel.h(140): warning: variable "MMA_M" was declared but never referenced
          detected during:
            instantiation of "void flash::compute_attn_1rowblock<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params,Is_Q_split_K>(const Params &, int, int, int) [with Kernel_traits=Flash_fwd_kernel_traits<512, 128, 32, 8, false, true, cutlass::half_t, Flash_kernel_traits<512, 128, 32, 8, cutlass::half_t>>, Is_dropout=false, Is_causal=false, Is_even_N=false, Is_even_K=true, Return_softmax=false, Params=Flash_fwd_params, Is_Q_split_K=false]" 
(589): here
            instantiation of "void flash::compute_attn<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &) [with Kernel_traits=Flash_fwd_kernel_traits<512, 128, 32, 8, false, true, cutlass::half_t, Flash_kernel_traits<512, 128, 32, 8, cutlass::half_t>>, Is_dropout=false, Is_causal=false, Is_even_N=false, Is_even_K=true, Return_softmax=false, Params=Flash_fwd_params]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(15): here
            instantiation of "void flash_fwd_kernel<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax>(Flash_fwd_params) [with Kernel_traits=Flash_fwd_kernel_traits<512, 128, 32, 8, false, true, cutlass::half_t, Flash_kernel_traits<512, 128, 32, 8, cutlass::half_t>>, Is_dropout=false, Is_causal=false, Is_even_N=false, Is_even_K=true, Return_softmax=false]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(34): here
            instantiation of "void run_flash_fwd<Kernel_traits,Is_dropout,Is_causal>(Flash_fwd_params &, cudaStream_t) [with Kernel_traits=Flash_fwd_kernel_traits<512, 128, 32, 8, false, true, cutlass::half_t, Flash_kernel_traits<512, 128, 32, 8, cutlass::half_t>>, Is_dropout=false, Is_causal=false]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(264): here
            instantiation of "void run_mha_fwd_hdim512<T>(Flash_fwd_params &, cudaStream_t) [with T=cutlass::half_t]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_hdim512_fp16_sm80.cu(8): here

/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_kernel.h(276): warning: variable "tQrQ" was declared but never referenced
          detected during:
            instantiation of "void flash::compute_attn_1rowblock<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params,Is_Q_split_K>(const Params &, int, int, int) [with Kernel_traits=Flash_fwd_kernel_traits<512, 128, 32, 8, false, true, cutlass::half_t, Flash_kernel_traits<512, 128, 32, 8, cutlass::half_t>>, Is_dropout=false, Is_causal=false, Is_even_N=false, Is_even_K=true, Return_softmax=false, Params=Flash_fwd_params, Is_Q_split_K=false]" 
(589): here
            instantiation of "void flash::compute_attn<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &) [with Kernel_traits=Flash_fwd_kernel_traits<512, 128, 32, 8, false, true, cutlass::half_t, Flash_kernel_traits<512, 128, 32, 8, cutlass::half_t>>, Is_dropout=false, Is_causal=false, Is_even_N=false, Is_even_K=true, Return_softmax=false, Params=Flash_fwd_params]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(15): here
            instantiation of "void flash_fwd_kernel<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax>(Flash_fwd_params) [with Kernel_traits=Flash_fwd_kernel_traits<512, 128, 32, 8, false, true, cutlass::half_t, Flash_kernel_traits<512, 128, 32, 8, cutlass::half_t>>, Is_dropout=false, Is_causal=false, Is_even_N=false, Is_even_K=true, Return_softmax=false]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(34): here
            instantiation of "void run_flash_fwd<Kernel_traits,Is_dropout,Is_causal>(Flash_fwd_params &, cudaStream_t) [with Kernel_traits=Flash_fwd_kernel_traits<512, 128, 32, 8, false, true, cutlass::half_t, Flash_kernel_traits<512, 128, 32, 8, cutlass::half_t>>, Is_dropout=false, Is_causal=false]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(264): here
            instantiation of "void run_mha_fwd_hdim512<T>(Flash_fwd_params &, cudaStream_t) [with T=cutlass::half_t]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_hdim512_fp16_sm80.cu(8): here

/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_kernel.h(140): warning: variable "MMA_M" was declared but never referenced
          detected during:
            instantiation of "void flash::compute_attn_1rowblock<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params,Is_Q_split_K>(const Params &, int, int, int) [with Kernel_traits=Flash_fwd_kernel_traits<512, 128, 32, 8, false, true, cutlass::half_t, Flash_kernel_traits<512, 128, 32, 8, cutlass::half_t>>, Is_dropout=false, Is_causal=false, Is_even_N=false, Is_even_K=false, Return_softmax=false, Params=Flash_fwd_params, Is_Q_split_K=false]" 
(589): here
            instantiation of "void flash::compute_attn<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &) [with Kernel_traits=Flash_fwd_kernel_traits<512, 128, 32, 8, false, true, cutlass::half_t, Flash_kernel_traits<512, 128, 32, 8, cutlass::half_t>>, Is_dropout=false, Is_causal=false, Is_even_N=false, Is_even_K=false, Return_softmax=false, Params=Flash_fwd_params]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(15): here
            instantiation of "void flash_fwd_kernel<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax>(Flash_fwd_params) [with Kernel_traits=Flash_fwd_kernel_traits<512, 128, 32, 8, false, true, cutlass::half_t, Flash_kernel_traits<512, 128, 32, 8, cutlass::half_t>>, Is_dropout=false, Is_causal=false, Is_even_N=false, Is_even_K=false, Return_softmax=false]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(34): here
            instantiation of "void run_flash_fwd<Kernel_traits,Is_dropout,Is_causal>(Flash_fwd_params &, cudaStream_t) [with Kernel_traits=Flash_fwd_kernel_traits<512, 128, 32, 8, false, true, cutlass::half_t, Flash_kernel_traits<512, 128, 32, 8, cutlass::half_t>>, Is_dropout=false, Is_causal=false]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(264): here
            instantiation of "void run_mha_fwd_hdim512<T>(Flash_fwd_params &, cudaStream_t) [with T=cutlass::half_t]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_hdim512_fp16_sm80.cu(8): here

/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_kernel.h(276): warning: variable "tQrQ" was declared but never referenced
          detected during:
            instantiation of "void flash::compute_attn_1rowblock<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params,Is_Q_split_K>(const Params &, int, int, int) [with Kernel_traits=Flash_fwd_kernel_traits<512, 128, 32, 8, false, true, cutlass::half_t, Flash_kernel_traits<512, 128, 32, 8, cutlass::half_t>>, Is_dropout=false, Is_causal=false, Is_even_N=false, Is_even_K=false, Return_softmax=false, Params=Flash_fwd_params, Is_Q_split_K=false]" 
(589): here
            instantiation of "void flash::compute_attn<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &) [with Kernel_traits=Flash_fwd_kernel_traits<512, 128, 32, 8, false, true, cutlass::half_t, Flash_kernel_traits<512, 128, 32, 8, cutlass::half_t>>, Is_dropout=false, Is_causal=false, Is_even_N=false, Is_even_K=false, Return_softmax=false, Params=Flash_fwd_params]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(15): here
            instantiation of "void flash_fwd_kernel<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax>(Flash_fwd_params) [with Kernel_traits=Flash_fwd_kernel_traits<512, 128, 32, 8, false, true, cutlass::half_t, Flash_kernel_traits<512, 128, 32, 8, cutlass::half_t>>, Is_dropout=false, Is_causal=false, Is_even_N=false, Is_even_K=false, Return_softmax=false]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(34): here
            instantiation of "void run_flash_fwd<Kernel_traits,Is_dropout,Is_causal>(Flash_fwd_params &, cudaStream_t) [with Kernel_traits=Flash_fwd_kernel_traits<512, 128, 32, 8, false, true, cutlass::half_t, Flash_kernel_traits<512, 128, 32, 8, cutlass::half_t>>, Is_dropout=false, Is_causal=false]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(264): here
            instantiation of "void run_mha_fwd_hdim512<T>(Flash_fwd_params &, cudaStream_t) [with T=cutlass::half_t]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_hdim512_fp16_sm80.cu(8): here

/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_kernel.h(140): warning: variable "MMA_M" was declared but never referenced
          detected during:
            instantiation of "void flash::compute_attn_1rowblock<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params,Is_Q_split_K>(const Params &, int, int, int) [with Kernel_traits=Flash_fwd_kernel_traits<512, 32, 32, 2, false, false, cutlass::half_t, Flash_kernel_traits<512, 32, 32, 2, cutlass::half_t>>, Is_dropout=false, Is_causal=false, Is_even_N=true, Is_even_K=true, Return_softmax=false, Params=Flash_fwd_params, Is_Q_split_K=false]" 
(589): here
            instantiation of "void flash::compute_attn<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &) [with Kernel_traits=Flash_fwd_kernel_traits<512, 32, 32, 2, false, false, cutlass::half_t, Flash_kernel_traits<512, 32, 32, 2, cutlass::half_t>>, Is_dropout=false, Is_causal=false, Is_even_N=true, Is_even_K=true, Return_softmax=false, Params=Flash_fwd_params]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(15): here
            instantiation of "void flash_fwd_kernel<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax>(Flash_fwd_params) [with Kernel_traits=Flash_fwd_kernel_traits<512, 32, 32, 2, false, false, cutlass::half_t, Flash_kernel_traits<512, 32, 32, 2, cutlass::half_t>>, Is_dropout=false, Is_causal=false, Is_even_N=true, Is_even_K=true, Return_softmax=false]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(34): here
            instantiation of "void run_flash_fwd<Kernel_traits,Is_dropout,Is_causal>(Flash_fwd_params &, cudaStream_t) [with Kernel_traits=Flash_fwd_kernel_traits<512, 32, 32, 2, false, false, cutlass::half_t, Flash_kernel_traits<512, 32, 32, 2, cutlass::half_t>>, Is_dropout=false, Is_causal=false]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(264): here
            instantiation of "void run_mha_fwd_hdim512<T>(Flash_fwd_params &, cudaStream_t) [with T=cutlass::half_t]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_hdim512_fp16_sm80.cu(8): here

/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_kernel.h(276): warning: variable "tQrQ" was declared but never referenced
          detected during:
            instantiation of "void flash::compute_attn_1rowblock<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params,Is_Q_split_K>(const Params &, int, int, int) [with Kernel_traits=Flash_fwd_kernel_traits<512, 32, 32, 2, false, false, cutlass::half_t, Flash_kernel_traits<512, 32, 32, 2, cutlass::half_t>>, Is_dropout=false, Is_causal=false, Is_even_N=true, Is_even_K=true, Return_softmax=false, Params=Flash_fwd_params, Is_Q_split_K=false]" 
(589): here
            instantiation of "void flash::compute_attn<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &) [with Kernel_traits=Flash_fwd_kernel_traits<512, 32, 32, 2, false, false, cutlass::half_t, Flash_kernel_traits<512, 32, 32, 2, cutlass::half_t>>, Is_dropout=false, Is_causal=false, Is_even_N=true, Is_even_K=true, Return_softmax=false, Params=Flash_fwd_params]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(15): here
            instantiation of "void flash_fwd_kernel<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax>(Flash_fwd_params) [with Kernel_traits=Flash_fwd_kernel_traits<512, 32, 32, 2, false, false, cutlass::half_t, Flash_kernel_traits<512, 32, 32, 2, cutlass::half_t>>, Is_dropout=false, Is_causal=false, Is_even_N=true, Is_even_K=true, Return_softmax=false]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(34): here
            instantiation of "void run_flash_fwd<Kernel_traits,Is_dropout,Is_causal>(Flash_fwd_params &, cudaStream_t) [with Kernel_traits=Flash_fwd_kernel_traits<512, 32, 32, 2, false, false, cutlass::half_t, Flash_kernel_traits<512, 32, 32, 2, cutlass::half_t>>, Is_dropout=false, Is_causal=false]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(264): here
            instantiation of "void run_mha_fwd_hdim512<T>(Flash_fwd_params &, cudaStream_t) [with T=cutlass::half_t]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_hdim512_fp16_sm80.cu(8): here

/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_kernel.h(140): warning: variable "MMA_M" was declared but never referenced
          detected during:
            instantiation of "void flash::compute_attn_1rowblock<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params,Is_Q_split_K>(const Params &, int, int, int) [with Kernel_traits=Flash_fwd_kernel_traits<512, 32, 32, 2, false, false, cutlass::half_t, Flash_kernel_traits<512, 32, 32, 2, cutlass::half_t>>, Is_dropout=false, Is_causal=false, Is_even_N=true, Is_even_K=false, Return_softmax=false, Params=Flash_fwd_params, Is_Q_split_K=false]" 
(589): here
            instantiation of "void flash::compute_attn<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &) [with Kernel_traits=Flash_fwd_kernel_traits<512, 32, 32, 2, false, false, cutlass::half_t, Flash_kernel_traits<512, 32, 32, 2, cutlass::half_t>>, Is_dropout=false, Is_causal=false, Is_even_N=true, Is_even_K=false, Return_softmax=false, Params=Flash_fwd_params]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(15): here
            instantiation of "void flash_fwd_kernel<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax>(Flash_fwd_params) [with Kernel_traits=Flash_fwd_kernel_traits<512, 32, 32, 2, false, false, cutlass::half_t, Flash_kernel_traits<512, 32, 32, 2, cutlass::half_t>>, Is_dropout=false, Is_causal=false, Is_even_N=true, Is_even_K=false, Return_softmax=false]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(34): here
            instantiation of "void run_flash_fwd<Kernel_traits,Is_dropout,Is_causal>(Flash_fwd_params &, cudaStream_t) [with Kernel_traits=Flash_fwd_kernel_traits<512, 32, 32, 2, false, false, cutlass::half_t, Flash_kernel_traits<512, 32, 32, 2, cutlass::half_t>>, Is_dropout=false, Is_causal=false]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(264): here
            instantiation of "void run_mha_fwd_hdim512<T>(Flash_fwd_params &, cudaStream_t) [with T=cutlass::half_t]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_hdim512_fp16_sm80.cu(8): here

/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_kernel.h(276): warning: variable "tQrQ" was declared but never referenced
          detected during:
            instantiation of "void flash::compute_attn_1rowblock<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params,Is_Q_split_K>(const Params &, int, int, int) [with Kernel_traits=Flash_fwd_kernel_traits<512, 32, 32, 2, false, false, cutlass::half_t, Flash_kernel_traits<512, 32, 32, 2, cutlass::half_t>>, Is_dropout=false, Is_causal=false, Is_even_N=true, Is_even_K=false, Return_softmax=false, Params=Flash_fwd_params, Is_Q_split_K=false]" 
(589): here
            instantiation of "void flash::compute_attn<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &) [with Kernel_traits=Flash_fwd_kernel_traits<512, 32, 32, 2, false, false, cutlass::half_t, Flash_kernel_traits<512, 32, 32, 2, cutlass::half_t>>, Is_dropout=false, Is_causal=false, Is_even_N=true, Is_even_K=false, Return_softmax=false, Params=Flash_fwd_params]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(15): here
            instantiation of "void flash_fwd_kernel<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax>(Flash_fwd_params) [with Kernel_traits=Flash_fwd_kernel_traits<512, 32, 32, 2, false, false, cutlass::half_t, Flash_kernel_traits<512, 32, 32, 2, cutlass::half_t>>, Is_dropout=false, Is_causal=false, Is_even_N=true, Is_even_K=false, Return_softmax=false]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(34): here
            instantiation of "void run_flash_fwd<Kernel_traits,Is_dropout,Is_causal>(Flash_fwd_params &, cudaStream_t) [with Kernel_traits=Flash_fwd_kernel_traits<512, 32, 32, 2, false, false, cutlass::half_t, Flash_kernel_traits<512, 32, 32, 2, cutlass::half_t>>, Is_dropout=false, Is_causal=false]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(264): here
            instantiation of "void run_mha_fwd_hdim512<T>(Flash_fwd_params &, cudaStream_t) [with T=cutlass::half_t]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_hdim512_fp16_sm80.cu(8): here

/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_kernel.h(140): warning: variable "MMA_M" was declared but never referenced
          detected during:
            instantiation of "void flash::compute_attn_1rowblock<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params,Is_Q_split_K>(const Params &, int, int, int) [with Kernel_traits=Flash_fwd_kernel_traits<512, 32, 32, 2, false, false, cutlass::half_t, Flash_kernel_traits<512, 32, 32, 2, cutlass::half_t>>, Is_dropout=false, Is_causal=false, Is_even_N=false, Is_even_K=true, Return_softmax=false, Params=Flash_fwd_params, Is_Q_split_K=false]" 
(589): here
            instantiation of "void flash::compute_attn<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &) [with Kernel_traits=Flash_fwd_kernel_traits<512, 32, 32, 2, false, false, cutlass::half_t, Flash_kernel_traits<512, 32, 32, 2, cutlass::half_t>>, Is_dropout=false, Is_causal=false, Is_even_N=false, Is_even_K=true, Return_softmax=false, Params=Flash_fwd_params]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(15): here
            instantiation of "void flash_fwd_kernel<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax>(Flash_fwd_params) [with Kernel_traits=Flash_fwd_kernel_traits<512, 32, 32, 2, false, false, cutlass::half_t, Flash_kernel_traits<512, 32, 32, 2, cutlass::half_t>>, Is_dropout=false, Is_causal=false, Is_even_N=false, Is_even_K=true, Return_softmax=false]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(34): here
            instantiation of "void run_flash_fwd<Kernel_traits,Is_dropout,Is_causal>(Flash_fwd_params &, cudaStream_t) [with Kernel_traits=Flash_fwd_kernel_traits<512, 32, 32, 2, false, false, cutlass::half_t, Flash_kernel_traits<512, 32, 32, 2, cutlass::half_t>>, Is_dropout=false, Is_causal=false]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(264): here
            instantiation of "void run_mha_fwd_hdim512<T>(Flash_fwd_params &, cudaStream_t) [with T=cutlass::half_t]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_hdim512_fp16_sm80.cu(8): here

/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_kernel.h(276): warning: variable "tQrQ" was declared but never referenced
          detected during:
            instantiation of "void flash::compute_attn_1rowblock<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params,Is_Q_split_K>(const Params &, int, int, int) [with Kernel_traits=Flash_fwd_kernel_traits<512, 32, 32, 2, false, false, cutlass::half_t, Flash_kernel_traits<512, 32, 32, 2, cutlass::half_t>>, Is_dropout=false, Is_causal=false, Is_even_N=false, Is_even_K=true, Return_softmax=false, Params=Flash_fwd_params, Is_Q_split_K=false]" 
(589): here
            instantiation of "void flash::compute_attn<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &) [with Kernel_traits=Flash_fwd_kernel_traits<512, 32, 32, 2, false, false, cutlass::half_t, Flash_kernel_traits<512, 32, 32, 2, cutlass::half_t>>, Is_dropout=false, Is_causal=false, Is_even_N=false, Is_even_K=true, Return_softmax=false, Params=Flash_fwd_params]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(15): here
            instantiation of "void flash_fwd_kernel<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax>(Flash_fwd_params) [with Kernel_traits=Flash_fwd_kernel_traits<512, 32, 32, 2, false, false, cutlass::half_t, Flash_kernel_traits<512, 32, 32, 2, cutlass::half_t>>, Is_dropout=false, Is_causal=false, Is_even_N=false, Is_even_K=true, Return_softmax=false]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(34): here
            instantiation of "void run_flash_fwd<Kernel_traits,Is_dropout,Is_causal>(Flash_fwd_params &, cudaStream_t) [with Kernel_traits=Flash_fwd_kernel_traits<512, 32, 32, 2, false, false, cutlass::half_t, Flash_kernel_traits<512, 32, 32, 2, cutlass::half_t>>, Is_dropout=false, Is_causal=false]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(264): here
            instantiation of "void run_mha_fwd_hdim512<T>(Flash_fwd_params &, cudaStream_t) [with T=cutlass::half_t]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_hdim512_fp16_sm80.cu(8): here

/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_kernel.h(140): warning: variable "MMA_M" was declared but never referenced
          detected during:
            instantiation of "void flash::compute_attn_1rowblock<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params,Is_Q_split_K>(const Params &, int, int, int) [with Kernel_traits=Flash_fwd_kernel_traits<512, 32, 32, 2, false, false, cutlass::half_t, Flash_kernel_traits<512, 32, 32, 2, cutlass::half_t>>, Is_dropout=false, Is_causal=false, Is_even_N=false, Is_even_K=false, Return_softmax=false, Params=Flash_fwd_params, Is_Q_split_K=false]" 
(589): here
            instantiation of "void flash::compute_attn<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &) [with Kernel_traits=Flash_fwd_kernel_traits<512, 32, 32, 2, false, false, cutlass::half_t, Flash_kernel_traits<512, 32, 32, 2, cutlass::half_t>>, Is_dropout=false, Is_causal=false, Is_even_N=false, Is_even_K=false, Return_softmax=false, Params=Flash_fwd_params]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(15): here
            instantiation of "void flash_fwd_kernel<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax>(Flash_fwd_params) [with Kernel_traits=Flash_fwd_kernel_traits<512, 32, 32, 2, false, false, cutlass::half_t, Flash_kernel_traits<512, 32, 32, 2, cutlass::half_t>>, Is_dropout=false, Is_causal=false, Is_even_N=false, Is_even_K=false, Return_softmax=false]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(34): here
            instantiation of "void run_flash_fwd<Kernel_traits,Is_dropout,Is_causal>(Flash_fwd_params &, cudaStream_t) [with Kernel_traits=Flash_fwd_kernel_traits<512, 32, 32, 2, false, false, cutlass::half_t, Flash_kernel_traits<512, 32, 32, 2, cutlass::half_t>>, Is_dropout=false, Is_causal=false]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(264): here
            instantiation of "void run_mha_fwd_hdim512<T>(Flash_fwd_params &, cudaStream_t) [with T=cutlass::half_t]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_hdim512_fp16_sm80.cu(8): here

/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_kernel.h(276): warning: variable "tQrQ" was declared but never referenced
          detected during:
            instantiation of "void flash::compute_attn_1rowblock<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params,Is_Q_split_K>(const Params &, int, int, int) [with Kernel_traits=Flash_fwd_kernel_traits<512, 32, 32, 2, false, false, cutlass::half_t, Flash_kernel_traits<512, 32, 32, 2, cutlass::half_t>>, Is_dropout=false, Is_causal=false, Is_even_N=false, Is_even_K=false, Return_softmax=false, Params=Flash_fwd_params, Is_Q_split_K=false]" 
(589): here
            instantiation of "void flash::compute_attn<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &) [with Kernel_traits=Flash_fwd_kernel_traits<512, 32, 32, 2, false, false, cutlass::half_t, Flash_kernel_traits<512, 32, 32, 2, cutlass::half_t>>, Is_dropout=false, Is_causal=false, Is_even_N=false, Is_even_K=false, Return_softmax=false, Params=Flash_fwd_params]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(15): here
            instantiation of "void flash_fwd_kernel<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax>(Flash_fwd_params) [with Kernel_traits=Flash_fwd_kernel_traits<512, 32, 32, 2, false, false, cutlass::half_t, Flash_kernel_traits<512, 32, 32, 2, cutlass::half_t>>, Is_dropout=false, Is_causal=false, Is_even_N=false, Is_even_K=false, Return_softmax=false]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(34): here
            instantiation of "void run_flash_fwd<Kernel_traits,Is_dropout,Is_causal>(Flash_fwd_params &, cudaStream_t) [with Kernel_traits=Flash_fwd_kernel_traits<512, 32, 32, 2, false, false, cutlass::half_t, Flash_kernel_traits<512, 32, 32, 2, cutlass::half_t>>, Is_dropout=false, Is_causal=false]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(264): here
            instantiation of "void run_mha_fwd_hdim512<T>(Flash_fwd_params &, cudaStream_t) [with T=cutlass::half_t]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_hdim512_fp16_sm80.cu(8): here

ptxas info    : 2 bytes gmem
ptxas info    : Compiling entry function '_Z16flash_fwd_kernelI23Flash_fwd_kernel_traitsILi512ELi32ELi32ELi2ELb0ELb0EN7cutlass6half_tE19Flash_kernel_traitsILi512ELi32ELi32ELi2ES2_EELb0ELb0ELb0ELb0ELb0EEv16Flash_fwd_params' for 'sm_80'
ptxas info    : Function properties for _Z16flash_fwd_kernelI23Flash_fwd_kernel_traitsILi512ELi32ELi32ELi2ELb0ELb0EN7cutlass6half_tE19Flash_kernel_traitsILi512ELi32ELi32ELi2ES2_EELb0ELb0ELb0ELb0ELb0EEv16Flash_fwd_params
    832 bytes stack frame, 1188 bytes spill stores, 1208 bytes spill loads
ptxas info    : Used 255 registers, 584 bytes cmem[0]
ptxas info    : Compiling entry function '_Z16flash_fwd_kernelI23Flash_fwd_kernel_traitsILi512ELi32ELi32ELi2ELb0ELb0EN7cutlass6half_tE19Flash_kernel_traitsILi512ELi32ELi32ELi2ES2_EELb0ELb0ELb0ELb1ELb0EEv16Flash_fwd_params' for 'sm_80'
ptxas info    : Function properties for _Z16flash_fwd_kernelI23Flash_fwd_kernel_traitsILi512ELi32ELi32ELi2ELb0ELb0EN7cutlass6half_tE19Flash_kernel_traitsILi512ELi32ELi32ELi2ES2_EELb0ELb0ELb0ELb1ELb0EEv16Flash_fwd_params
    456 bytes stack frame, 792 bytes spill stores, 784 bytes spill loads
ptxas info    : Used 255 registers, 584 bytes cmem[0]
ptxas info    : Compiling entry function '_Z16flash_fwd_kernelI23Flash_fwd_kernel_traitsILi512ELi32ELi32ELi2ELb0ELb0EN7cutlass6half_tE19Flash_kernel_traitsILi512ELi32ELi32ELi2ES2_EELb0ELb0ELb1ELb0ELb0EEv16Flash_fwd_params' for 'sm_80'
ptxas info    : Function properties for _Z16flash_fwd_kernelI23Flash_fwd_kernel_traitsILi512ELi32ELi32ELi2ELb0ELb0EN7cutlass6half_tE19Flash_kernel_traitsILi512ELi32ELi32ELi2ES2_EELb0ELb0ELb1ELb0ELb0EEv16Flash_fwd_params
    744 bytes stack frame, 1068 bytes spill stores, 1072 bytes spill loads
ptxas info    : Used 255 registers, 584 bytes cmem[0]
ptxas info    : Compiling entry function '_Z16flash_fwd_kernelI23Flash_fwd_kernel_traitsILi512ELi32ELi32ELi2ELb0ELb0EN7cutlass6half_tE19Flash_kernel_traitsILi512ELi32ELi32ELi2ES2_EELb0ELb0ELb1ELb1ELb0EEv16Flash_fwd_params' for 'sm_80'
ptxas info    : Function properties for _Z16flash_fwd_kernelI23Flash_fwd_kernel_traitsILi512ELi32ELi32ELi2ELb0ELb0EN7cutlass6half_tE19Flash_kernel_traitsILi512ELi32ELi32ELi2ES2_EELb0ELb0ELb1ELb1ELb0EEv16Flash_fwd_params
    456 bytes stack frame, 752 bytes spill stores, 772 bytes spill loads
ptxas info    : Used 255 registers, 584 bytes cmem[0]
ptxas info    : Compiling entry function '_Z16flash_fwd_kernelI23Flash_fwd_kernel_traitsILi512ELi128ELi32ELi8ELb0ELb1EN7cutlass6half_tE19Flash_kernel_traitsILi512ELi128ELi32ELi8ES2_EELb0ELb0ELb0ELb0ELb0EEv16Flash_fwd_params' for 'sm_80'
ptxas info    : Function properties for _Z16flash_fwd_kernelI23Flash_fwd_kernel_traitsILi512ELi128ELi32ELi8ELb0ELb1EN7cutlass6half_tE19Flash_kernel_traitsILi512ELi128ELi32ELi8ES2_EELb0ELb0ELb0ELb0ELb0EEv16Flash_fwd_params
    1536 bytes stack frame, 2604 bytes spill stores, 2600 bytes spill loads
ptxas info    : Used 255 registers, 584 bytes cmem[0]
ptxas info    : Compiling entry function '_Z16flash_fwd_kernelI23Flash_fwd_kernel_traitsILi512ELi128ELi32ELi8ELb0ELb1EN7cutlass6half_tE19Flash_kernel_traitsILi512ELi128ELi32ELi8ES2_EELb0ELb0ELb0ELb1ELb0EEv16Flash_fwd_params' for 'sm_80'
ptxas info    : Function properties for _Z16flash_fwd_kernelI23Flash_fwd_kernel_traitsILi512ELi128ELi32ELi8ELb0ELb1EN7cutlass6half_tE19Flash_kernel_traitsILi512ELi128ELi32ELi8ES2_EELb0ELb0ELb0ELb1ELb0EEv16Flash_fwd_params
    1440 bytes stack frame, 2464 bytes spill stores, 2448 bytes spill loads
ptxas info    : Used 255 registers, 584 bytes cmem[0]
ptxas info    : Compiling entry function '_Z16flash_fwd_kernelI23Flash_fwd_kernel_traitsILi512ELi128ELi32ELi8ELb0ELb1EN7cutlass6half_tE19Flash_kernel_traitsILi512ELi128ELi32ELi8ES2_EELb0ELb0ELb1ELb0ELb0EEv16Flash_fwd_params' for 'sm_80'
ptxas info    : Function properties for _Z16flash_fwd_kernelI23Flash_fwd_kernel_traitsILi512ELi128ELi32ELi8ELb0ELb1EN7cutlass6half_tE19Flash_kernel_traitsILi512ELi128ELi32ELi8ES2_EELb0ELb0ELb1ELb0ELb0EEv16Flash_fwd_params
    1440 bytes stack frame, 2476 bytes spill stores, 2484 bytes spill loads
ptxas info    : Used 255 registers, 584 bytes cmem[0]
ptxas info    : Compiling entry function '_Z16flash_fwd_kernelI23Flash_fwd_kernel_traitsILi512ELi128ELi32ELi8ELb0ELb1EN7cutlass6half_tE19Flash_kernel_traitsILi512ELi128ELi32ELi8ES2_EELb0ELb0ELb1ELb1ELb0EEv16Flash_fwd_params' for 'sm_80'
ptxas info    : Function properties for _Z16flash_fwd_kernelI23Flash_fwd_kernel_traitsILi512ELi128ELi32ELi8ELb0ELb1EN7cutlass6half_tE19Flash_kernel_traitsILi512ELi128ELi32ELi8ES2_EELb0ELb0ELb1ELb1ELb0EEv16Flash_fwd_params
    1424 bytes stack frame, 2348 bytes spill stores, 2340 bytes spill loads
ptxas info    : Used 255 registers, 584 bytes cmem[0]
ptxas info    : Compiling entry function '_Z16flash_fwd_kernelI23Flash_fwd_kernel_traitsILi512ELi32ELi32ELi2ELb0ELb0EN7cutlass6half_tE19Flash_kernel_traitsILi512ELi32ELi32ELi2ES2_EELb0ELb1ELb0ELb0ELb0EEv16Flash_fwd_params' for 'sm_80'
ptxas info    : Function properties for _Z16flash_fwd_kernelI23Flash_fwd_kernel_traitsILi512ELi32ELi32ELi2ELb0ELb0EN7cutlass6half_tE19Flash_kernel_traitsILi512ELi32ELi32ELi2ES2_EELb0ELb1ELb0ELb0ELb0EEv16Flash_fwd_params
    832 bytes stack frame, 1192 bytes spill stores, 1212 bytes spill loads
ptxas info    : Used 255 registers, 584 bytes cmem[0]
ptxas info    : Compiling entry function '_Z16flash_fwd_kernelI23Flash_fwd_kernel_traitsILi512ELi32ELi32ELi2ELb0ELb0EN7cutlass6half_tE19Flash_kernel_traitsILi512ELi32ELi32ELi2ES2_EELb0ELb1ELb0ELb1ELb0EEv16Flash_fwd_params' for 'sm_80'
ptxas info    : Function properties for _Z16flash_fwd_kernelI23Flash_fwd_kernel_traitsILi512ELi32ELi32ELi2ELb0ELb0EN7cutlass6half_tE19Flash_kernel_traitsILi512ELi32ELi32ELi2ES2_EELb0ELb1ELb0ELb1ELb0EEv16Flash_fwd_params
    464 bytes stack frame, 796 bytes spill stores, 788 bytes spill loads
ptxas info    : Used 255 registers, 584 bytes cmem[0]
ptxas info    : Compiling entry function '_Z16flash_fwd_kernelI23Flash_fwd_kernel_traitsILi512ELi32ELi32ELi2ELb0ELb0EN7cutlass6half_tE19Flash_kernel_traitsILi512ELi32ELi32ELi2ES2_EELb0ELb1ELb1ELb0ELb0EEv16Flash_fwd_params' for 'sm_80'
ptxas info    : Function properties for _Z16flash_fwd_kernelI23Flash_fwd_kernel_traitsILi512ELi32ELi32ELi2ELb0ELb0EN7cutlass6half_tE19Flash_kernel_traitsILi512ELi32ELi32ELi2ES2_EELb0ELb1ELb1ELb0ELb0EEv16Flash_fwd_params
    760 bytes stack frame, 1120 bytes spill stores, 1124 bytes spill loads
ptxas info    : Used 255 registers, 584 bytes cmem[0]
ptxas info    : Compiling entry function '_Z16flash_fwd_kernelI23Flash_fwd_kernel_traitsILi512ELi32ELi32ELi2ELb0ELb0EN7cutlass6half_tE19Flash_kernel_traitsILi512ELi32ELi32ELi2ES2_EELb0ELb1ELb1ELb1ELb0EEv16Flash_fwd_params' for 'sm_80'
ptxas info    : Function properties for _Z16flash_fwd_kernelI23Flash_fwd_kernel_traitsILi512ELi32ELi32ELi2ELb0ELb0EN7cutlass6half_tE19Flash_kernel_traitsILi512ELi32ELi32ELi2ES2_EELb0ELb1ELb1ELb1ELb0EEv16Flash_fwd_params
    464 bytes stack frame, 756 bytes spill stores, 776 bytes spill loads
ptxas info    : Used 255 registers, 584 bytes cmem[0]
ptxas info    : Compiling entry function '_Z16flash_fwd_kernelI23Flash_fwd_kernel_traitsILi512ELi128ELi32ELi8ELb0ELb1EN7cutlass6half_tE19Flash_kernel_traitsILi512ELi128ELi32ELi8ES2_EELb0ELb1ELb0ELb0ELb0EEv16Flash_fwd_params' for 'sm_80'
ptxas info    : Function properties for _Z16flash_fwd_kernelI23Flash_fwd_kernel_traitsILi512ELi128ELi32ELi8ELb0ELb1EN7cutlass6half_tE19Flash_kernel_traitsILi512ELi128ELi32ELi8ES2_EELb0ELb1ELb0ELb0ELb0EEv16Flash_fwd_params
    1920 bytes stack frame, 0 bytes spill stores, 0 bytes spill loads
ptxas info    : Used 255 registers, 584 bytes cmem[0]
ptxas info    : Function properties for _ZN5flash22compute_attn_1rowblockI23Flash_fwd_kernel_traitsILi512ELi128ELi32ELi8ELb0ELb1EN7cutlass6half_tE19Flash_kernel_traitsILi512ELi128ELi32ELi8ES3_EELb0ELb1ELb0ELb0ELb0E16Flash_fwd_paramsLb0EEEvRKT5_iii
    0 bytes stack frame, 7232 bytes spill stores, 7316 bytes spill loads
ptxas info    : Compiling entry function '_Z16flash_fwd_kernelI23Flash_fwd_kernel_traitsILi512ELi128ELi32ELi8ELb0ELb1EN7cutlass6half_tE19Flash_kernel_traitsILi512ELi128ELi32ELi8ES2_EELb0ELb1ELb0ELb1ELb0EEv16Flash_fwd_params' for 'sm_80'
ptxas info    : Function properties for _Z16flash_fwd_kernelI23Flash_fwd_kernel_traitsILi512ELi128ELi32ELi8ELb0ELb1EN7cutlass6half_tE19Flash_kernel_traitsILi512ELi128ELi32ELi8ES2_EELb0ELb1ELb0ELb1ELb0EEv16Flash_fwd_params
    1664 bytes stack frame, 7044 bytes spill stores, 7128 bytes spill loads
ptxas info    : Used 255 registers, 584 bytes cmem[0]
ptxas info    : Compiling entry function '_Z16flash_fwd_kernelI23Flash_fwd_kernel_traitsILi512ELi128ELi32ELi8ELb0ELb1EN7cutlass6half_tE19Flash_kernel_traitsILi512ELi128ELi32ELi8ES2_EELb0ELb1ELb1ELb0ELb0EEv16Flash_fwd_params' for 'sm_80'
ptxas info    : Function properties for _Z16flash_fwd_kernelI23Flash_fwd_kernel_traitsILi512ELi128ELi32ELi8ELb0ELb1EN7cutlass6half_tE19Flash_kernel_traitsILi512ELi128ELi32ELi8ES2_EELb0ELb1ELb1ELb0ELb0EEv16Flash_fwd_params
    1840 bytes stack frame, 0 bytes spill stores, 0 bytes spill loads
ptxas info    : Used 255 registers, 584 bytes cmem[0]
ptxas info    : Function properties for _ZN5flash22compute_attn_1rowblockI23Flash_fwd_kernel_traitsILi512ELi128ELi32ELi8ELb0ELb1EN7cutlass6half_tE19Flash_kernel_traitsILi512ELi128ELi32ELi8ES3_EELb0ELb1ELb1ELb0ELb0E16Flash_fwd_paramsLb0EEEvRKT5_iii
    0 bytes stack frame, 7108 bytes spill stores, 7220 bytes spill loads
ptxas info    : Compiling entry function '_Z16flash_fwd_kernelI23Flash_fwd_kernel_traitsILi512ELi128ELi32ELi8ELb0ELb1EN7cutlass6half_tE19Flash_kernel_traitsILi512ELi128ELi32ELi8ES2_EELb0ELb1ELb1ELb1ELb0EEv16Flash_fwd_params' for 'sm_80'
ptxas info    : Function properties for _Z16flash_fwd_kernelI23Flash_fwd_kernel_traitsILi512ELi128ELi32ELi8ELb0ELb1EN7cutlass6half_tE19Flash_kernel_traitsILi512ELi128ELi32ELi8ES2_EELb0ELb1ELb1ELb1ELb0EEv16Flash_fwd_params
    1728 bytes stack frame, 7116 bytes spill stores, 7248 bytes spill loads
ptxas info    : Used 255 registers, 584 bytes cmem[0]
ptxas info    : Compiling entry function '_Z16flash_fwd_kernelI23Flash_fwd_kernel_traitsILi512ELi32ELi32ELi2ELb0ELb0EN7cutlass6half_tE19Flash_kernel_traitsILi512ELi32ELi32ELi2ES2_EELb1ELb0ELb0ELb0ELb0EEv16Flash_fwd_params' for 'sm_80'
ptxas info    : Function properties for _Z16flash_fwd_kernelI23Flash_fwd_kernel_traitsILi512ELi32ELi32ELi2ELb0ELb0EN7cutlass6half_tE19Flash_kernel_traitsILi512ELi32ELi32ELi2ES2_EELb1ELb0ELb0ELb0ELb0EEv16Flash_fwd_params
    736 bytes stack frame, 1088 bytes spill stores, 1116 bytes spill loads
ptxas info    : Used 255 registers, 584 bytes cmem[0]
ptxas info    : Compiling entry function '_Z16flash_fwd_kernelI23Flash_fwd_kernel_traitsILi512ELi32ELi32ELi2ELb0ELb0EN7cutlass6half_tE19Flash_kernel_traitsILi512ELi32ELi32ELi2ES2_EELb1ELb0ELb0ELb0ELb1EEv16Flash_fwd_params' for 'sm_80'
ptxas info    : Function properties for _Z16flash_fwd_kernelI23Flash_fwd_kernel_traitsILi512ELi32ELi32ELi2ELb0ELb0EN7cutlass6half_tE19Flash_kernel_traitsILi512ELi32ELi32ELi2ES2_EELb1ELb0ELb0ELb0ELb1EEv16Flash_fwd_params
    712 bytes stack frame, 1040 bytes spill stores, 1048 bytes spill loads
ptxas info    : Used 255 registers, 584 bytes cmem[0]
ptxas info    : Compiling entry function '_Z16flash_fwd_kernelI23Flash_fwd_kernel_traitsILi512ELi32ELi32ELi2ELb0ELb0EN7cutlass6half_tE19Flash_kernel_traitsILi512ELi32ELi32ELi2ES2_EELb1ELb0ELb0ELb1ELb0EEv16Flash_fwd_params' for 'sm_80'
ptxas info    : Function properties for _Z16flash_fwd_kernelI23Flash_fwd_kernel_traitsILi512ELi32ELi32ELi2ELb0ELb0EN7cutlass6half_tE19Flash_kernel_traitsILi512ELi32ELi32ELi2ES2_EELb1ELb0ELb0ELb1ELb0EEv16Flash_fwd_params
    496 bytes stack frame, 792 bytes spill stores, 788 bytes spill loads
ptxas info    : Used 255 registers, 584 bytes cmem[0]
ptxas info    : Compiling entry function '_Z16flash_fwd_kernelI23Flash_fwd_kernel_traitsILi512ELi32ELi32ELi2ELb0ELb0EN7cutlass6half_tE19Flash_kernel_traitsILi512ELi32ELi32ELi2ES2_EELb1ELb0ELb0ELb1ELb1EEv16Flash_fwd_params' for 'sm_80'
ptxas info    : Function properties for _Z16flash_fwd_kernelI23Flash_fwd_kernel_traitsILi512ELi32ELi32ELi2ELb0ELb0EN7cutlass6half_tE19Flash_kernel_traitsILi512ELi32ELi32ELi2ES2_EELb1ELb0ELb0ELb1ELb1EEv16Flash_fwd_params
    784 bytes stack frame, 1088 bytes spill stores, 1068 bytes spill loads
ptxas info    : Used 255 registers, 584 bytes cmem[0]
ptxas info    : Compiling entry function '_Z16flash_fwd_kernelI23Flash_fwd_kernel_traitsILi512ELi32ELi32ELi2ELb0ELb0EN7cutlass6half_tE19Flash_kernel_traitsILi512ELi32ELi32ELi2ES2_EELb1ELb0ELb1ELb0ELb0EEv16Flash_fwd_params' for 'sm_80'
ptxas info    : Function properties for _Z16flash_fwd_kernelI23Flash_fwd_kernel_traitsILi512ELi32ELi32ELi2ELb0ELb0EN7cutlass6half_tE19Flash_kernel_traitsILi512ELi32ELi32ELi2ES2_EELb1ELb0ELb1ELb0ELb0EEv16Flash_fwd_params
    704 bytes stack frame, 1004 bytes spill stores, 1008 bytes spill loads
ptxas info    : Used 255 registers, 584 bytes cmem[0]
ptxas info    : Compiling entry function '_Z16flash_fwd_kernelI23Flash_fwd_kernel_traitsILi512ELi32ELi32ELi2ELb0ELb0EN7cutlass6half_tE19Flash_kernel_traitsILi512ELi32ELi32ELi2ES2_EELb1ELb0ELb1ELb0ELb1EEv16Flash_fwd_params' for 'sm_80'
ptxas info    : Function properties for _Z16flash_fwd_kernelI23Flash_fwd_kernel_traitsILi512ELi32ELi32ELi2ELb0ELb0EN7cutlass6half_tE19Flash_kernel_traitsILi512ELi32ELi32ELi2ES2_EELb1ELb0ELb1ELb0ELb1EEv16Flash_fwd_params
    704 bytes stack frame, 992 bytes spill stores, 980 bytes spill loads
ptxas info    : Used 255 registers, 584 bytes cmem[0]
ptxas info    : Compiling entry function '_Z16flash_fwd_kernelI23Flash_fwd_kernel_traitsILi512ELi32ELi32ELi2ELb0ELb0EN7cutlass6half_tE19Flash_kernel_traitsILi512ELi32ELi32ELi2ES2_EELb1ELb0ELb1ELb1ELb0EEv16Flash_fwd_params' for 'sm_80'
ptxas info    : Function properties for _Z16flash_fwd_kernelI23Flash_fwd_kernel_traitsILi512ELi32ELi32ELi2ELb0ELb0EN7cutlass6half_tE19Flash_kernel_traitsILi512ELi32ELi32ELi2ES2_EELb1ELb0ELb1ELb1ELb0EEv16Flash_fwd_params
    448 bytes stack frame, 720 bytes spill stores, 736 bytes spill loads
ptxas info    : Used 255 registers, 584 bytes cmem[0]
ptxas info    : Compiling entry function '_Z16flash_fwd_kernelI23Flash_fwd_kernel_traitsILi512ELi32ELi32ELi2ELb0ELb0EN7cutlass6half_tE19Flash_kernel_traitsILi512ELi32ELi32ELi2ES2_EELb1ELb0ELb1ELb1ELb1EEv16Flash_fwd_params' for 'sm_80'
ptxas info    : Function properties for _Z16flash_fwd_kernelI23Flash_fwd_kernel_traitsILi512ELi32ELi32ELi2ELb0ELb0EN7cutlass6half_tE19Flash_kernel_traitsILi512ELi32ELi32ELi2ES2_EELb1ELb0ELb1ELb1ELb1EEv16Flash_fwd_params
    792 bytes stack frame, 1152 bytes spill stores, 1140 bytes spill loads
ptxas info    : Used 255 registers, 584 bytes cmem[0]
ptxas info    : Compiling entry function '_Z16flash_fwd_kernelI23Flash_fwd_kernel_traitsILi512ELi128ELi32ELi8ELb0ELb1EN7cutlass6half_tE19Flash_kernel_traitsILi512ELi128ELi32ELi8ES2_EELb1ELb0ELb0ELb0ELb0EEv16Flash_fwd_params' for 'sm_80'
ptxas info    : Function properties for _Z16flash_fwd_kernelI23Flash_fwd_kernel_traitsILi512ELi128ELi32ELi8ELb0ELb1EN7cutlass6half_tE19Flash_kernel_traitsILi512ELi128ELi32ELi8ES2_EELb1ELb0ELb0ELb0ELb0EEv16Flash_fwd_params
    1976 bytes stack frame, 3124 bytes spill stores, 3120 bytes spill loads
ptxas info    : Used 255 registers, 584 bytes cmem[0]
ptxas info    : Compiling entry function '_Z16flash_fwd_kernelI23Flash_fwd_kernel_traitsILi512ELi128ELi32ELi8ELb0ELb1EN7cutlass6half_tE19Flash_kernel_traitsILi512ELi128ELi32ELi8ES2_EELb1ELb0ELb0ELb0ELb1EEv16Flash_fwd_params' for 'sm_80'
ptxas info    : Function properties for _Z16flash_fwd_kernelI23Flash_fwd_kernel_traitsILi512ELi128ELi32ELi8ELb0ELb1EN7cutlass6half_tE19Flash_kernel_traitsILi512ELi128ELi32ELi8ES2_EELb1ELb0ELb0ELb0ELb1EEv16Flash_fwd_params
    1992 bytes stack frame, 2776 bytes spill stores, 2790 bytes spill loads
ptxas info    : Used 255 registers, 584 bytes cmem[0]
ptxas info    : Compiling entry function '_Z16flash_fwd_kernelI23Flash_fwd_kernel_traitsILi512ELi128ELi32ELi8ELb0ELb1EN7cutlass6half_tE19Flash_kernel_traitsILi512ELi128ELi32ELi8ES2_EELb1ELb0ELb0ELb1ELb0EEv16Flash_fwd_params' for 'sm_80'
ptxas info    : Function properties for _Z16flash_fwd_kernelI23Flash_fwd_kernel_traitsILi512ELi128ELi32ELi8ELb0ELb1EN7cutlass6half_tE19Flash_kernel_traitsILi512ELi128ELi32ELi8ES2_EELb1ELb0ELb0ELb1ELb0EEv16Flash_fwd_params
    1928 bytes stack frame, 3032 bytes spill stores, 3008 bytes spill loads
ptxas info    : Used 255 registers, 584 bytes cmem[0]
ptxas info    : Compiling entry function '_Z16flash_fwd_kernelI23Flash_fwd_kernel_traitsILi512ELi128ELi32ELi8ELb0ELb1EN7cutlass6half_tE19Flash_kernel_traitsILi512ELi128ELi32ELi8ES2_EELb1ELb0ELb0ELb1ELb1EEv16Flash_fwd_params' for 'sm_80'
ptxas info    : Function properties for _Z16flash_fwd_kernelI23Flash_fwd_kernel_traitsILi512ELi128ELi32ELi8ELb0ELb1EN7cutlass6half_tE19Flash_kernel_traitsILi512ELi128ELi32ELi8ES2_EELb1ELb0ELb0ELb1ELb1EEv16Flash_fwd_params
    2024 bytes stack frame, 2646 bytes spill stores, 2632 bytes spill loads
ptxas info    : Used 255 registers, 584 bytes cmem[0]
ptxas info    : Compiling entry function '_Z16flash_fwd_kernelI23Flash_fwd_kernel_traitsILi512ELi128ELi32ELi8ELb0ELb1EN7cutlass6half_tE19Flash_kernel_traitsILi512ELi128ELi32ELi8ES2_EELb1ELb0ELb1ELb0ELb0EEv16Flash_fwd_params' for 'sm_80'
ptxas info    : Function properties for _Z16flash_fwd_kernelI23Flash_fwd_kernel_traitsILi512ELi128ELi32ELi8ELb0ELb1EN7cutlass6half_tE19Flash_kernel_traitsILi512ELi128ELi32ELi8ES2_EELb1ELb0ELb1ELb0ELb0EEv16Flash_fwd_params
    1736 bytes stack frame, 2436 bytes spill stores, 2428 bytes spill loads
ptxas info    : Used 255 registers, 584 bytes cmem[0]
ptxas info    : Compiling entry function '_Z16flash_fwd_kernelI23Flash_fwd_kernel_traitsILi512ELi128ELi32ELi8ELb0ELb1EN7cutlass6half_tE19Flash_kernel_traitsILi512ELi128ELi32ELi8ES2_EELb1ELb0ELb1ELb0ELb1EEv16Flash_fwd_params' for 'sm_80'
ptxas info    : Function properties for _Z16flash_fwd_kernelI23Flash_fwd_kernel_traitsILi512ELi128ELi32ELi8ELb0ELb1EN7cutlass6half_tE19Flash_kernel_traitsILi512ELi128ELi32ELi8ES2_EELb1ELb0ELb1ELb0ELb1EEv16Flash_fwd_params
    2008 bytes stack frame, 2640 bytes spill stores, 2624 bytes spill loads
ptxas info    : Used 255 registers, 584 bytes cmem[0]
ptxas info    : Compiling entry function '_Z16flash_fwd_kernelI23Flash_fwd_kernel_traitsILi512ELi128ELi32ELi8ELb0ELb1EN7cutlass6half_tE19Flash_kernel_traitsILi512ELi128ELi32ELi8ES2_EELb1ELb0ELb1ELb1ELb0EEv16Flash_fwd_params' for 'sm_80'
ptxas info    : Function properties for _Z16flash_fwd_kernelI23Flash_fwd_kernel_traitsILi512ELi128ELi32ELi8ELb0ELb1EN7cutlass6half_tE19Flash_kernel_traitsILi512ELi128ELi32ELi8ES2_EELb1ELb0ELb1ELb1ELb0EEv16Flash_fwd_params
    1480 bytes stack frame, 2392 bytes spill stores, 2376 bytes spill loads
ptxas info    : Used 255 registers, 584 bytes cmem[0]
ptxas info    : Compiling entry function '_Z16flash_fwd_kernelI23Flash_fwd_kernel_traitsILi512ELi128ELi32ELi8ELb0ELb1EN7cutlass6half_tE19Flash_kernel_traitsILi512ELi128ELi32ELi8ES2_EELb1ELb0ELb1ELb1ELb1EEv16Flash_fwd_params' for 'sm_80'
ptxas info    : Function properties for _Z16flash_fwd_kernelI23Flash_fwd_kernel_traitsILi512ELi128ELi32ELi8ELb0ELb1EN7cutlass6half_tE19Flash_kernel_traitsILi512ELi128ELi32ELi8ES2_EELb1ELb0ELb1ELb1ELb1EEv16Flash_fwd_params
    2120 bytes stack frame, 2722 bytes spill stores, 2686 bytes spill loads
ptxas info    : Used 255 registers, 584 bytes cmem[0]
ptxas info    : Compiling entry function '_Z16flash_fwd_kernelI23Flash_fwd_kernel_traitsILi512ELi32ELi32ELi2ELb0ELb0EN7cutlass6half_tE19Flash_kernel_traitsILi512ELi32ELi32ELi2ES2_EELb1ELb1ELb0ELb0ELb0EEv16Flash_fwd_params' for 'sm_80'
ptxas info    : Function properties for _Z16flash_fwd_kernelI23Flash_fwd_kernel_traitsILi512ELi32ELi32ELi2ELb0ELb0EN7cutlass6half_tE19Flash_kernel_traitsILi512ELi32ELi32ELi2ES2_EELb1ELb1ELb0ELb0ELb0EEv16Flash_fwd_params
    736 bytes stack frame, 1092 bytes spill stores, 1120 bytes spill loads
ptxas info    : Used 255 registers, 584 bytes cmem[0]
ptxas info    : Compiling entry function '_Z16flash_fwd_kernelI23Flash_fwd_kernel_traitsILi512ELi32ELi32ELi2ELb0ELb0EN7cutlass6half_tE19Flash_kernel_traitsILi512ELi32ELi32ELi2ES2_EELb1ELb1ELb0ELb0ELb1EEv16Flash_fwd_params' for 'sm_80'
ptxas info    : Function properties for _Z16flash_fwd_kernelI23Flash_fwd_kernel_traitsILi512ELi32ELi32ELi2ELb0ELb0EN7cutlass6half_tE19Flash_kernel_traitsILi512ELi32ELi32ELi2ES2_EELb1ELb1ELb0ELb0ELb1EEv16Flash_fwd_params
    720 bytes stack frame, 1052 bytes spill stores, 1060 bytes spill loads
ptxas info    : Used 255 registers, 584 bytes cmem[0]
ptxas info    : Compiling entry function '_Z16flash_fwd_kernelI23Flash_fwd_kernel_traitsILi512ELi32ELi32ELi2ELb0ELb0EN7cutlass6half_tE19Flash_kernel_traitsILi512ELi32ELi32ELi2ES2_EELb1ELb1ELb0ELb1ELb0EEv16Flash_fwd_params' for 'sm_80'
ptxas info    : Function properties for _Z16flash_fwd_kernelI23Flash_fwd_kernel_traitsILi512ELi32ELi32ELi2ELb0ELb0EN7cutlass6half_tE19Flash_kernel_traitsILi512ELi32ELi32ELi2ES2_EELb1ELb1ELb0ELb1ELb0EEv16Flash_fwd_params
    504 bytes stack frame, 796 bytes spill stores, 788 bytes spill loads
ptxas info    : Used 255 registers, 584 bytes cmem[0]
ptxas info    : Compiling entry function '_Z16flash_fwd_kernelI23Flash_fwd_kernel_traitsILi512ELi32ELi32ELi2ELb0ELb0EN7cutlass6half_tE19Flash_kernel_traitsILi512ELi32ELi32ELi2ES2_EELb1ELb1ELb0ELb1ELb1EEv16Flash_fwd_params' for 'sm_80'
ptxas info    : Function properties for _Z16flash_fwd_kernelI23Flash_fwd_kernel_traitsILi512ELi32ELi32ELi2ELb0ELb0EN7cutlass6half_tE19Flash_kernel_traitsILi512ELi32ELi32ELi2ES2_EELb1ELb1ELb0ELb1ELb1EEv16Flash_fwd_params
    792 bytes stack frame, 1092 bytes spill stores, 1072 bytes spill loads
ptxas info    : Used 255 registers, 584 bytes cmem[0]
ptxas info    : Compiling entry function '_Z16flash_fwd_kernelI23Flash_fwd_kernel_traitsILi512ELi32ELi32ELi2ELb0ELb0EN7cutlass6half_tE19Flash_kernel_traitsILi512ELi32ELi32ELi2ES2_EELb1ELb1ELb1ELb0ELb0EEv16Flash_fwd_params' for 'sm_80'
ptxas info    : Function properties for _Z16flash_fwd_kernelI23Flash_fwd_kernel_traitsILi512ELi32ELi32ELi2ELb0ELb0EN7cutlass6half_tE19Flash_kernel_traitsILi512ELi32ELi32ELi2ES2_EELb1ELb1ELb1ELb0ELb0EEv16Flash_fwd_params
    712 bytes stack frame, 1060 bytes spill stores, 1072 bytes spill loads
ptxas info    : Used 255 registers, 584 bytes cmem[0]
ptxas info    : Compiling entry function '_Z16flash_fwd_kernelI23Flash_fwd_kernel_traitsILi512ELi32ELi32ELi2ELb0ELb0EN7cutlass6half_tE19Flash_kernel_traitsILi512ELi32ELi32ELi2ES2_EELb1ELb1ELb1ELb0ELb1EEv16Flash_fwd_params' for 'sm_80'
ptxas info    : Function properties for _Z16flash_fwd_kernelI23Flash_fwd_kernel_traitsILi512ELi32ELi32ELi2ELb0ELb0EN7cutlass6half_tE19Flash_kernel_traitsILi512ELi32ELi32ELi2ES2_EELb1ELb1ELb1ELb0ELb1EEv16Flash_fwd_params
    720 bytes stack frame, 1052 bytes spill stores, 1044 bytes spill loads
ptxas info    : Used 255 registers, 584 bytes cmem[0]
ptxas info    : Compiling entry function '_Z16flash_fwd_kernelI23Flash_fwd_kernel_traitsILi512ELi32ELi32ELi2ELb0ELb0EN7cutlass6half_tE19Flash_kernel_traitsILi512ELi32ELi32ELi2ES2_EELb1ELb1ELb1ELb1ELb0EEv16Flash_fwd_params' for 'sm_80'
ptxas info    : Function properties for _Z16flash_fwd_kernelI23Flash_fwd_kernel_traitsILi512ELi32ELi32ELi2ELb0ELb0EN7cutlass6half_tE19Flash_kernel_traitsILi512ELi32ELi32ELi2ES2_EELb1ELb1ELb1ELb1ELb0EEv16Flash_fwd_params
    472 bytes stack frame, 752 bytes spill stores, 760 bytes spill loads
ptxas info    : Used 255 registers, 584 bytes cmem[0]
ptxas info    : Compiling entry function '_Z16flash_fwd_kernelI23Flash_fwd_kernel_traitsILi512ELi32ELi32ELi2ELb0ELb0EN7cutlass6half_tE19Flash_kernel_traitsILi512ELi32ELi32ELi2ES2_EELb1ELb1ELb1ELb1ELb1EEv16Flash_fwd_params' for 'sm_80'
ptxas info    : Function properties for _Z16flash_fwd_kernelI23Flash_fwd_kernel_traitsILi512ELi32ELi32ELi2ELb0ELb0EN7cutlass6half_tE19Flash_kernel_traitsILi512ELi32ELi32ELi2ES2_EELb1ELb1ELb1ELb1ELb1EEv16Flash_fwd_params
    752 bytes stack frame, 1088 bytes spill stores, 1082 bytes spill loads
ptxas info    : Used 255 registers, 584 bytes cmem[0]
ptxas info    : Compiling entry function '_Z16flash_fwd_kernelI23Flash_fwd_kernel_traitsILi512ELi128ELi32ELi8ELb0ELb1EN7cutlass6half_tE19Flash_kernel_traitsILi512ELi128ELi32ELi8ES2_EELb1ELb1ELb0ELb0ELb0EEv16Flash_fwd_params' for 'sm_80'
ptxas info    : Function properties for _Z16flash_fwd_kernelI23Flash_fwd_kernel_traitsILi512ELi128ELi32ELi8ELb0ELb1EN7cutlass6half_tE19Flash_kernel_traitsILi512ELi128ELi32ELi8ES2_EELb1ELb1ELb0ELb0ELb0EEv16Flash_fwd_params
    2048 bytes stack frame, 0 bytes spill stores, 0 bytes spill loads
ptxas info    : Used 255 registers, 584 bytes cmem[0]
ptxas info    : Function properties for _ZN5flash22compute_attn_1rowblockI23Flash_fwd_kernel_traitsILi512ELi128ELi32ELi8ELb0ELb1EN7cutlass6half_tE19Flash_kernel_traitsILi512ELi128ELi32ELi8ES3_EELb1ELb1ELb0ELb0ELb0E16Flash_fwd_paramsLb0EEEvRKT5_iii
    0 bytes stack frame, 7408 bytes spill stores, 7752 bytes spill loads
ptxas info    : Compiling entry function '_Z16flash_fwd_kernelI23Flash_fwd_kernel_traitsILi512ELi128ELi32ELi8ELb0ELb1EN7cutlass6half_tE19Flash_kernel_traitsILi512ELi128ELi32ELi8ES2_EELb1ELb1ELb0ELb0ELb1EEv16Flash_fwd_params' for 'sm_80'
ptxas info    : Function properties for _Z16flash_fwd_kernelI23Flash_fwd_kernel_traitsILi512ELi128ELi32ELi8ELb0ELb1EN7cutlass6half_tE19Flash_kernel_traitsILi512ELi128ELi32ELi8ES2_EELb1ELb1ELb0ELb0ELb1EEv16Flash_fwd_params
    2424 bytes stack frame, 0 bytes spill stores, 0 bytes spill loads
ptxas info    : Used 255 registers, 584 bytes cmem[0]
ptxas info    : Function properties for _ZN5flash22compute_attn_1rowblockI23Flash_fwd_kernel_traitsILi512ELi128ELi32ELi8ELb0ELb1EN7cutlass6half_tE19Flash_kernel_traitsILi512ELi128ELi32ELi8ES3_EELb1ELb1ELb0ELb0ELb1E16Flash_fwd_paramsLb0EEEvRKT5_iii
    0 bytes stack frame, 7892 bytes spill stores, 8194 bytes spill loads
ptxas info    : Compiling entry function '_Z16flash_fwd_kernelI23Flash_fwd_kernel_traitsILi512ELi128ELi32ELi8ELb0ELb1EN7cutlass6half_tE19Flash_kernel_traitsILi512ELi128ELi32ELi8ES2_EELb1ELb1ELb0ELb1ELb0EEv16Flash_fwd_params' for 'sm_80'
ptxas info    : Function properties for _Z16flash_fwd_kernelI23Flash_fwd_kernel_traitsILi512ELi128ELi32ELi8ELb0ELb1EN7cutlass6half_tE19Flash_kernel_traitsILi512ELi128ELi32ELi8ES2_EELb1ELb1ELb0ELb1ELb0EEv16Flash_fwd_params
    2008 bytes stack frame, 7580 bytes spill stores, 7716 bytes spill loads
ptxas info    : Used 255 registers, 584 bytes cmem[0]
ptxas info    : Compiling entry function '_Z16flash_fwd_kernelI23Flash_fwd_kernel_traitsILi512ELi128ELi32ELi8ELb0ELb1EN7cutlass6half_tE19Flash_kernel_traitsILi512ELi128ELi32ELi8ES2_EELb1ELb1ELb0ELb1ELb1EEv16Flash_fwd_params' for 'sm_80'
ptxas info    : Function properties for _Z16flash_fwd_kernelI23Flash_fwd_kernel_traitsILi512ELi128ELi32ELi8ELb0ELb1EN7cutlass6half_tE19Flash_kernel_traitsILi512ELi128ELi32ELi8ES2_EELb1ELb1ELb0ELb1ELb1EEv16Flash_fwd_params
    2136 bytes stack frame, 7802 bytes spill stores, 8018 bytes spill loads
ptxas info    : Used 255 registers, 584 bytes cmem[0]
ptxas info    : Compiling entry function '_Z16flash_fwd_kernelI23Flash_fwd_kernel_traitsILi512ELi128ELi32ELi8ELb0ELb1EN7cutlass6half_tE19Flash_kernel_traitsILi512ELi128ELi32ELi8ES2_EELb1ELb1ELb1ELb0ELb0EEv16Flash_fwd_params' for 'sm_80'
ptxas info    : Function properties for _Z16flash_fwd_kernelI23Flash_fwd_kernel_traitsILi512ELi128ELi32ELi8ELb0ELb1EN7cutlass6half_tE19Flash_kernel_traitsILi512ELi128ELi32ELi8ES2_EELb1ELb1ELb1ELb0ELb0EEv16Flash_fwd_params
    2384 bytes stack frame, 0 bytes spill stores, 0 bytes spill loads
ptxas info    : Used 255 registers, 584 bytes cmem[0]
ptxas info    : Function properties for _ZN5flash22compute_attn_1rowblockI23Flash_fwd_kernel_traitsILi512ELi128ELi32ELi8ELb0ELb1EN7cutlass6half_tE19Flash_kernel_traitsILi512ELi128ELi32ELi8ES3_EELb1ELb1ELb1ELb0ELb0E16Flash_fwd_paramsLb0EEEvRKT5_iii
    0 bytes stack frame, 8260 bytes spill stores, 8604 bytes spill loads
ptxas info    : Compiling entry function '_Z16flash_fwd_kernelI23Flash_fwd_kernel_traitsILi512ELi128ELi32ELi8ELb0ELb1EN7cutlass6half_tE19Flash_kernel_traitsILi512ELi128ELi32ELi8ES2_EELb1ELb1ELb1ELb0ELb1EEv16Flash_fwd_params' for 'sm_80'
ptxas info    : Function properties for _Z16flash_fwd_kernelI23Flash_fwd_kernel_traitsILi512ELi128ELi32ELi8ELb0ELb1EN7cutlass6half_tE19Flash_kernel_traitsILi512ELi128ELi32ELi8ES2_EELb1ELb1ELb1ELb0ELb1EEv16Flash_fwd_params
    2336 bytes stack frame, 0 bytes spill stores, 0 bytes spill loads
ptxas info    : Used 255 registers, 584 bytes cmem[0]
ptxas info    : Function properties for _ZN5flash22compute_attn_1rowblockI23Flash_fwd_kernel_traitsILi512ELi128ELi32ELi8ELb0ELb1EN7cutlass6half_tE19Flash_kernel_traitsILi512ELi128ELi32ELi8ES3_EELb1ELb1ELb1ELb0ELb1E16Flash_fwd_paramsLb0EEEvRKT5_iii
    0 bytes stack frame, 7576 bytes spill stores, 7992 bytes spill loads
ptxas info    : Compiling entry function '_Z16flash_fwd_kernelI23Flash_fwd_kernel_traitsILi512ELi128ELi32ELi8ELb0ELb1EN7cutlass6half_tE19Flash_kernel_traitsILi512ELi128ELi32ELi8ES2_EELb1ELb1ELb1ELb1ELb0EEv16Flash_fwd_params' for 'sm_80'
ptxas info    : Function properties for _Z16flash_fwd_kernelI23Flash_fwd_kernel_traitsILi512ELi128ELi32ELi8ELb0ELb1EN7cutlass6half_tE19Flash_kernel_traitsILi512ELi128ELi32ELi8ES2_EELb1ELb1ELb1ELb1ELb0EEv16Flash_fwd_params
    2072 bytes stack frame, 7792 bytes spill stores, 7972 bytes spill loads
ptxas info    : Used 255 registers, 584 bytes cmem[0]
ptxas info    : Compiling entry function '_Z16flash_fwd_kernelI23Flash_fwd_kernel_traitsILi512ELi128ELi32ELi8ELb0ELb1EN7cutlass6half_tE19Flash_kernel_traitsILi512ELi128ELi32ELi8ES2_EELb1ELb1ELb1ELb1ELb1EEv16Flash_fwd_params' for 'sm_80'
ptxas info    : Function properties for _Z16flash_fwd_kernelI23Flash_fwd_kernel_traitsILi512ELi128ELi32ELi8ELb0ELb1EN7cutlass6half_tE19Flash_kernel_traitsILi512ELi128ELi32ELi8ES2_EELb1ELb1ELb1ELb1ELb1EEv16Flash_fwd_params
    2080 bytes stack frame, 7330 bytes spill stores, 7528 bytes spill loads
ptxas info    : Used 255 registers, 584 bytes cmem[0]
/anaconda/envs/flashattention/lib/python3.9/site-packages/torch/include/c10/core/SymInt.h(84): warning: integer conversion resulted in a change of sign

g++ -pthread -B /anaconda/envs/flashattention/compiler_compat -shared -Wl,-rpath,/anaconda/envs/flashattention/lib -Wl,-rpath-link,/anaconda/envs/flashattention/lib -L/anaconda/envs/flashattention/lib -L/anaconda/envs/flashattention/lib -Wl,-rpath,/anaconda/envs/flashattention/lib -Wl,-rpath-link,/anaconda/envs/flashattention/lib -L/anaconda/envs/flashattention/lib /home/v-feiychen/flash-attention/build/temp.linux-x86_64-cpython-39/csrc/flash_attn/flash_api.o /home/v-feiychen/flash-attention/build/temp.linux-x86_64-cpython-39/csrc/flash_attn/src/flash_bwd_hdim128_bf16_sm80.o /home/v-feiychen/flash-attention/build/temp.linux-x86_64-cpython-39/csrc/flash_attn/src/flash_bwd_hdim128_fp16_sm80.o /home/v-feiychen/flash-attention/build/temp.linux-x86_64-cpython-39/csrc/flash_attn/src/flash_bwd_hdim160_bf16_sm80.o /home/v-feiychen/flash-attention/build/temp.linux-x86_64-cpython-39/csrc/flash_attn/src/flash_bwd_hdim160_fp16_sm80.o /home/v-feiychen/flash-attention/build/temp.linux-x86_64-cpython-39/csrc/flash_attn/src/flash_bwd_hdim192_bf16_sm80.o /home/v-feiychen/flash-attention/build/temp.linux-x86_64-cpython-39/csrc/flash_attn/src/flash_bwd_hdim192_fp16_sm80.o /home/v-feiychen/flash-attention/build/temp.linux-x86_64-cpython-39/csrc/flash_attn/src/flash_bwd_hdim224_bf16_sm80.o /home/v-feiychen/flash-attention/build/temp.linux-x86_64-cpython-39/csrc/flash_attn/src/flash_bwd_hdim224_fp16_sm80.o /home/v-feiychen/flash-attention/build/temp.linux-x86_64-cpython-39/csrc/flash_attn/src/flash_bwd_hdim256_bf16_sm80.o /home/v-feiychen/flash-attention/build/temp.linux-x86_64-cpython-39/csrc/flash_attn/src/flash_bwd_hdim256_fp16_sm80.o /home/v-feiychen/flash-attention/build/temp.linux-x86_64-cpython-39/csrc/flash_attn/src/flash_bwd_hdim32_bf16_sm80.o /home/v-feiychen/flash-attention/build/temp.linux-x86_64-cpython-39/csrc/flash_attn/src/flash_bwd_hdim32_fp16_sm80.o /home/v-feiychen/flash-attention/build/temp.linux-x86_64-cpython-39/csrc/flash_attn/src/flash_bwd_hdim512_bf16_sm80.o /home/v-feiychen/flash-attention/build/temp.linux-x86_64-cpython-39/csrc/flash_attn/src/flash_bwd_hdim512_fp16_sm80.o /home/v-feiychen/flash-attention/build/temp.linux-x86_64-cpython-39/csrc/flash_attn/src/flash_bwd_hdim64_bf16_sm80.o /home/v-feiychen/flash-attention/build/temp.linux-x86_64-cpython-39/csrc/flash_attn/src/flash_bwd_hdim64_fp16_sm80.o /home/v-feiychen/flash-attention/build/temp.linux-x86_64-cpython-39/csrc/flash_attn/src/flash_bwd_hdim96_bf16_sm80.o /home/v-feiychen/flash-attention/build/temp.linux-x86_64-cpython-39/csrc/flash_attn/src/flash_bwd_hdim96_fp16_sm80.o /home/v-feiychen/flash-attention/build/temp.linux-x86_64-cpython-39/csrc/flash_attn/src/flash_fwd_hdim128_bf16_sm80.o /home/v-feiychen/flash-attention/build/temp.linux-x86_64-cpython-39/csrc/flash_attn/src/flash_fwd_hdim128_fp16_sm80.o /home/v-feiychen/flash-attention/build/temp.linux-x86_64-cpython-39/csrc/flash_attn/src/flash_fwd_hdim160_bf16_sm80.o /home/v-feiychen/flash-attention/build/temp.linux-x86_64-cpython-39/csrc/flash_attn/src/flash_fwd_hdim160_fp16_sm80.o /home/v-feiychen/flash-attention/build/temp.linux-x86_64-cpython-39/csrc/flash_attn/src/flash_fwd_hdim192_bf16_sm80.o /home/v-feiychen/flash-attention/build/temp.linux-x86_64-cpython-39/csrc/flash_attn/src/flash_fwd_hdim192_fp16_sm80.o /home/v-feiychen/flash-attention/build/temp.linux-x86_64-cpython-39/csrc/flash_attn/src/flash_fwd_hdim224_bf16_sm80.o /home/v-feiychen/flash-attention/build/temp.linux-x86_64-cpython-39/csrc/flash_attn/src/flash_fwd_hdim224_fp16_sm80.o /home/v-feiychen/flash-attention/build/temp.linux-x86_64-cpython-39/csrc/flash_attn/src/flash_fwd_hdim256_bf16_sm80.o /home/v-feiychen/flash-attention/build/temp.linux-x86_64-cpython-39/csrc/flash_attn/src/flash_fwd_hdim256_fp16_sm80.o /home/v-feiychen/flash-attention/build/temp.linux-x86_64-cpython-39/csrc/flash_attn/src/flash_fwd_hdim32_bf16_sm80.o /home/v-feiychen/flash-attention/build/temp.linux-x86_64-cpython-39/csrc/flash_attn/src/flash_fwd_hdim32_fp16_sm80.o /home/v-feiychen/flash-attention/build/temp.linux-x86_64-cpython-39/csrc/flash_attn/src/flash_fwd_hdim512_bf16_sm80.o /home/v-feiychen/flash-attention/build/temp.linux-x86_64-cpython-39/csrc/flash_attn/src/flash_fwd_hdim512_fp16_sm80.o /home/v-feiychen/flash-attention/build/temp.linux-x86_64-cpython-39/csrc/flash_attn/src/flash_fwd_hdim64_bf16_sm80.o /home/v-feiychen/flash-attention/build/temp.linux-x86_64-cpython-39/csrc/flash_attn/src/flash_fwd_hdim64_fp16_sm80.o /home/v-feiychen/flash-attention/build/temp.linux-x86_64-cpython-39/csrc/flash_attn/src/flash_fwd_hdim96_bf16_sm80.o /home/v-feiychen/flash-attention/build/temp.linux-x86_64-cpython-39/csrc/flash_attn/src/flash_fwd_hdim96_fp16_sm80.o -L/anaconda/envs/flashattention/lib/python3.9/site-packages/torch/lib -L/usr/local/cuda-11.4/lib64 -lc10 -ltorch -ltorch_cpu -ltorch_python -lcudart -lc10_cuda -ltorch_cuda_cu -ltorch_cuda_cpp -o build/lib.linux-x86_64-cpython-39/flash_attn_2_cuda.cpython-39-x86_64-linux-gnu.so
creating build/bdist.linux-x86_64
creating build/bdist.linux-x86_64/egg
creating build/bdist.linux-x86_64/egg/flash_attn
copying build/lib.linux-x86_64-cpython-39/flash_attn/flash_attn_triton_og.py -> build/bdist.linux-x86_64/egg/flash_attn
copying build/lib.linux-x86_64-cpython-39/flash_attn/__init__.py -> build/bdist.linux-x86_64/egg/flash_attn
copying build/lib.linux-x86_64-cpython-39/flash_attn/fused_softmax.py -> build/bdist.linux-x86_64/egg/flash_attn
creating build/bdist.linux-x86_64/egg/flash_attn/ops
copying build/lib.linux-x86_64-cpython-39/flash_attn/ops/__init__.py -> build/bdist.linux-x86_64/egg/flash_attn/ops
copying build/lib.linux-x86_64-cpython-39/flash_attn/ops/activations.py -> build/bdist.linux-x86_64/egg/flash_attn/ops
copying build/lib.linux-x86_64-cpython-39/flash_attn/ops/fused_dense.py -> build/bdist.linux-x86_64/egg/flash_attn/ops
copying build/lib.linux-x86_64-cpython-39/flash_attn/ops/rms_norm.py -> build/bdist.linux-x86_64/egg/flash_attn/ops
copying build/lib.linux-x86_64-cpython-39/flash_attn/ops/layer_norm.py -> build/bdist.linux-x86_64/egg/flash_attn/ops
copying build/lib.linux-x86_64-cpython-39/flash_attn/flash_blocksparse_attn_interface.py -> build/bdist.linux-x86_64/egg/flash_attn
creating build/bdist.linux-x86_64/egg/flash_attn/layers
copying build/lib.linux-x86_64-cpython-39/flash_attn/layers/__init__.py -> build/bdist.linux-x86_64/egg/flash_attn/layers
copying build/lib.linux-x86_64-cpython-39/flash_attn/layers/patch_embed.py -> build/bdist.linux-x86_64/egg/flash_attn/layers
copying build/lib.linux-x86_64-cpython-39/flash_attn/layers/rotary.py -> build/bdist.linux-x86_64/egg/flash_attn/layers
copying build/lib.linux-x86_64-cpython-39/flash_attn/flash_attn_interface.py -> build/bdist.linux-x86_64/egg/flash_attn
creating build/bdist.linux-x86_64/egg/flash_attn/utils
copying build/lib.linux-x86_64-cpython-39/flash_attn/utils/__init__.py -> build/bdist.linux-x86_64/egg/flash_attn/utils
copying build/lib.linux-x86_64-cpython-39/flash_attn/utils/pretrained.py -> build/bdist.linux-x86_64/egg/flash_attn/utils
copying build/lib.linux-x86_64-cpython-39/flash_attn/utils/distributed.py -> build/bdist.linux-x86_64/egg/flash_attn/utils
copying build/lib.linux-x86_64-cpython-39/flash_attn/utils/benchmark.py -> build/bdist.linux-x86_64/egg/flash_attn/utils
copying build/lib.linux-x86_64-cpython-39/flash_attn/utils/generation.py -> build/bdist.linux-x86_64/egg/flash_attn/utils
creating build/bdist.linux-x86_64/egg/flash_attn/losses
copying build/lib.linux-x86_64-cpython-39/flash_attn/losses/__init__.py -> build/bdist.linux-x86_64/egg/flash_attn/losses
copying build/lib.linux-x86_64-cpython-39/flash_attn/losses/cross_entropy.py -> build/bdist.linux-x86_64/egg/flash_attn/losses
creating build/bdist.linux-x86_64/egg/flash_attn/modules
copying build/lib.linux-x86_64-cpython-39/flash_attn/modules/__init__.py -> build/bdist.linux-x86_64/egg/flash_attn/modules
copying build/lib.linux-x86_64-cpython-39/flash_attn/modules/block.py -> build/bdist.linux-x86_64/egg/flash_attn/modules
copying build/lib.linux-x86_64-cpython-39/flash_attn/modules/mlp.py -> build/bdist.linux-x86_64/egg/flash_attn/modules
copying build/lib.linux-x86_64-cpython-39/flash_attn/modules/mha.py -> build/bdist.linux-x86_64/egg/flash_attn/modules
copying build/lib.linux-x86_64-cpython-39/flash_attn/modules/embedding.py -> build/bdist.linux-x86_64/egg/flash_attn/modules
copying build/lib.linux-x86_64-cpython-39/flash_attn/bert_padding.py -> build/bdist.linux-x86_64/egg/flash_attn
copying build/lib.linux-x86_64-cpython-39/flash_attn/flash_attn_triton.py -> build/bdist.linux-x86_64/egg/flash_attn
copying build/lib.linux-x86_64-cpython-39/flash_attn/flash_blocksparse_attention.py -> build/bdist.linux-x86_64/egg/flash_attn
creating build/bdist.linux-x86_64/egg/flash_attn/models
copying build/lib.linux-x86_64-cpython-39/flash_attn/models/baichuan.py -> build/bdist.linux-x86_64/egg/flash_attn/models
copying build/lib.linux-x86_64-cpython-39/flash_attn/models/__init__.py -> build/bdist.linux-x86_64/egg/flash_attn/models
copying build/lib.linux-x86_64-cpython-39/flash_attn/models/opt.py -> build/bdist.linux-x86_64/egg/flash_attn/models
copying build/lib.linux-x86_64-cpython-39/flash_attn/models/falcon.py -> build/bdist.linux-x86_64/egg/flash_attn/models
copying build/lib.linux-x86_64-cpython-39/flash_attn/models/gpt_neox.py -> build/bdist.linux-x86_64/egg/flash_attn/models
copying build/lib.linux-x86_64-cpython-39/flash_attn/models/gptj.py -> build/bdist.linux-x86_64/egg/flash_attn/models
copying build/lib.linux-x86_64-cpython-39/flash_attn/models/vit.py -> build/bdist.linux-x86_64/egg/flash_attn/models
copying build/lib.linux-x86_64-cpython-39/flash_attn/models/llama.py -> build/bdist.linux-x86_64/egg/flash_attn/models
copying build/lib.linux-x86_64-cpython-39/flash_attn/models/gpt.py -> build/bdist.linux-x86_64/egg/flash_attn/models
copying build/lib.linux-x86_64-cpython-39/flash_attn/models/bert.py -> build/bdist.linux-x86_64/egg/flash_attn/models
copying build/lib.linux-x86_64-cpython-39/flash_attn_2_cuda.cpython-39-x86_64-linux-gnu.so -> build/bdist.linux-x86_64/egg
byte-compiling build/bdist.linux-x86_64/egg/flash_attn/flash_attn_triton_og.py to flash_attn_triton_og.cpython-39.pyc
byte-compiling build/bdist.linux-x86_64/egg/flash_attn/__init__.py to __init__.cpython-39.pyc
byte-compiling build/bdist.linux-x86_64/egg/flash_attn/fused_softmax.py to fused_softmax.cpython-39.pyc
byte-compiling build/bdist.linux-x86_64/egg/flash_attn/ops/__init__.py to __init__.cpython-39.pyc
byte-compiling build/bdist.linux-x86_64/egg/flash_attn/ops/activations.py to activations.cpython-39.pyc
byte-compiling build/bdist.linux-x86_64/egg/flash_attn/ops/fused_dense.py to fused_dense.cpython-39.pyc
byte-compiling build/bdist.linux-x86_64/egg/flash_attn/ops/rms_norm.py to rms_norm.cpython-39.pyc
byte-compiling build/bdist.linux-x86_64/egg/flash_attn/ops/layer_norm.py to layer_norm.cpython-39.pyc
byte-compiling build/bdist.linux-x86_64/egg/flash_attn/flash_blocksparse_attn_interface.py to flash_blocksparse_attn_interface.cpython-39.pyc
byte-compiling build/bdist.linux-x86_64/egg/flash_attn/layers/__init__.py to __init__.cpython-39.pyc
byte-compiling build/bdist.linux-x86_64/egg/flash_attn/layers/patch_embed.py to patch_embed.cpython-39.pyc
byte-compiling build/bdist.linux-x86_64/egg/flash_attn/layers/rotary.py to rotary.cpython-39.pyc
byte-compiling build/bdist.linux-x86_64/egg/flash_attn/flash_attn_interface.py to flash_attn_interface.cpython-39.pyc
byte-compiling build/bdist.linux-x86_64/egg/flash_attn/utils/__init__.py to __init__.cpython-39.pyc
byte-compiling build/bdist.linux-x86_64/egg/flash_attn/utils/pretrained.py to pretrained.cpython-39.pyc
byte-compiling build/bdist.linux-x86_64/egg/flash_attn/utils/distributed.py to distributed.cpython-39.pyc
byte-compiling build/bdist.linux-x86_64/egg/flash_attn/utils/benchmark.py to benchmark.cpython-39.pyc
byte-compiling build/bdist.linux-x86_64/egg/flash_attn/utils/generation.py to generation.cpython-39.pyc
byte-compiling build/bdist.linux-x86_64/egg/flash_attn/losses/__init__.py to __init__.cpython-39.pyc
byte-compiling build/bdist.linux-x86_64/egg/flash_attn/losses/cross_entropy.py to cross_entropy.cpython-39.pyc
byte-compiling build/bdist.linux-x86_64/egg/flash_attn/modules/__init__.py to __init__.cpython-39.pyc
byte-compiling build/bdist.linux-x86_64/egg/flash_attn/modules/block.py to block.cpython-39.pyc
byte-compiling build/bdist.linux-x86_64/egg/flash_attn/modules/mlp.py to mlp.cpython-39.pyc
byte-compiling build/bdist.linux-x86_64/egg/flash_attn/modules/mha.py to mha.cpython-39.pyc
byte-compiling build/bdist.linux-x86_64/egg/flash_attn/modules/embedding.py to embedding.cpython-39.pyc
byte-compiling build/bdist.linux-x86_64/egg/flash_attn/bert_padding.py to bert_padding.cpython-39.pyc
byte-compiling build/bdist.linux-x86_64/egg/flash_attn/flash_attn_triton.py to flash_attn_triton.cpython-39.pyc
byte-compiling build/bdist.linux-x86_64/egg/flash_attn/flash_blocksparse_attention.py to flash_blocksparse_attention.cpython-39.pyc
byte-compiling build/bdist.linux-x86_64/egg/flash_attn/models/baichuan.py to baichuan.cpython-39.pyc
byte-compiling build/bdist.linux-x86_64/egg/flash_attn/models/__init__.py to __init__.cpython-39.pyc
byte-compiling build/bdist.linux-x86_64/egg/flash_attn/models/opt.py to opt.cpython-39.pyc
byte-compiling build/bdist.linux-x86_64/egg/flash_attn/models/falcon.py to falcon.cpython-39.pyc
byte-compiling build/bdist.linux-x86_64/egg/flash_attn/models/gpt_neox.py to gpt_neox.cpython-39.pyc
byte-compiling build/bdist.linux-x86_64/egg/flash_attn/models/gptj.py to gptj.cpython-39.pyc
byte-compiling build/bdist.linux-x86_64/egg/flash_attn/models/vit.py to vit.cpython-39.pyc
byte-compiling build/bdist.linux-x86_64/egg/flash_attn/models/llama.py to llama.cpython-39.pyc
byte-compiling build/bdist.linux-x86_64/egg/flash_attn/models/gpt.py to gpt.cpython-39.pyc
byte-compiling build/bdist.linux-x86_64/egg/flash_attn/models/bert.py to bert.cpython-39.pyc
creating stub loader for flash_attn_2_cuda.cpython-39-x86_64-linux-gnu.so
byte-compiling build/bdist.linux-x86_64/egg/flash_attn_2_cuda.py to flash_attn_2_cuda.cpython-39.pyc
creating build/bdist.linux-x86_64/egg/EGG-INFO
copying flash_attn.egg-info/PKG-INFO -> build/bdist.linux-x86_64/egg/EGG-INFO
copying flash_attn.egg-info/SOURCES.txt -> build/bdist.linux-x86_64/egg/EGG-INFO
copying flash_attn.egg-info/dependency_links.txt -> build/bdist.linux-x86_64/egg/EGG-INFO
copying flash_attn.egg-info/requires.txt -> build/bdist.linux-x86_64/egg/EGG-INFO
copying flash_attn.egg-info/top_level.txt -> build/bdist.linux-x86_64/egg/EGG-INFO
writing build/bdist.linux-x86_64/egg/EGG-INFO/native_libs.txt
creating 'dist/flash_attn-2.0.9-py3.9-linux-x86_64.egg' and adding 'build/bdist.linux-x86_64/egg' to it
removing 'build/bdist.linux-x86_64/egg' (and everything under it)
Processing flash_attn-2.0.9-py3.9-linux-x86_64.egg
removing '/anaconda/envs/flashattention/lib/python3.9/site-packages/flash_attn-2.0.9-py3.9-linux-x86_64.egg' (and everything under it)
creating /anaconda/envs/flashattention/lib/python3.9/site-packages/flash_attn-2.0.9-py3.9-linux-x86_64.egg
Extracting flash_attn-2.0.9-py3.9-linux-x86_64.egg to /anaconda/envs/flashattention/lib/python3.9/site-packages
Adding flash-attn 2.0.9 to easy-install.pth file

Installed /anaconda/envs/flashattention/lib/python3.9/site-packages/flash_attn-2.0.9-py3.9-linux-x86_64.egg
Processing dependencies for flash-attn==2.0.9
Searching for ninja==1.11.1
Best match: ninja 1.11.1
Adding ninja 1.11.1 to easy-install.pth file
Installing ninja script to /anaconda/envs/flashattention/bin

Using /anaconda/envs/flashattention/lib/python3.9/site-packages
Searching for packaging==23.1
Best match: packaging 23.1
Adding packaging 23.1 to easy-install.pth file

Using /anaconda/envs/flashattention/lib/python3.9/site-packages
Searching for einops==0.6.1
Best match: einops 0.6.1
Adding einops 0.6.1 to easy-install.pth file

Using /anaconda/envs/flashattention/lib/python3.9/site-packages
Searching for torch==1.12.1
Best match: torch 1.12.1
Adding torch 1.12.1 to easy-install.pth file
Installing convert-caffe2-to-onnx script to /anaconda/envs/flashattention/bin
Installing convert-onnx-to-caffe2 script to /anaconda/envs/flashattention/bin
Installing torchrun script to /anaconda/envs/flashattention/bin

Using /anaconda/envs/flashattention/lib/python3.9/site-packages
Searching for typing-extensions==4.7.1
Best match: typing-extensions 4.7.1
Adding typing-extensions 4.7.1 to easy-install.pth file

Using /anaconda/envs/flashattention/lib/python3.9/site-packages
Finished processing dependencies for flash-attn==2.0.9
