

torch.__version__  = 1.12.1


running install
running bdist_egg
running egg_info
writing flash_attn.egg-info/PKG-INFO
writing dependency_links to flash_attn.egg-info/dependency_links.txt
writing requirements to flash_attn.egg-info/requires.txt
writing top-level names to flash_attn.egg-info/top_level.txt
reading manifest file 'flash_attn.egg-info/SOURCES.txt'
reading manifest template 'MANIFEST.in'
adding license file 'LICENSE'
adding license file 'AUTHORS'
writing manifest file 'flash_attn.egg-info/SOURCES.txt'
installing library code to build/bdist.linux-x86_64/egg
running install_lib
running build_py
running build_ext
building 'flash_attn_2_cuda' extension
Emitting ninja build file /home/v-feiychen/flash-attention/build/temp.linux-x86_64-cpython-39/build.ninja...
Compiling objects...
Allowing ninja to set a default number of workers... (overridable by setting the environment variable MAX_JOBS=N)
[1/36] /usr/local/cuda-11.4/bin/nvcc  -I/home/v-feiychen/flash-attention/csrc/flash_attn -I/home/v-feiychen/flash-attention/csrc/flash_attn/src -I/home/v-feiychen/flash-attention/csrc/cutlass/include -I/anaconda/envs/flashattention/lib/python3.9/site-packages/torch/include -I/anaconda/envs/flashattention/lib/python3.9/site-packages/torch/include/torch/csrc/api/include -I/anaconda/envs/flashattention/lib/python3.9/site-packages/torch/include/TH -I/anaconda/envs/flashattention/lib/python3.9/site-packages/torch/include/THC -I/usr/local/cuda-11.4/include -I/anaconda/envs/flashattention/include/python3.9 -c -c /home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_bwd_hdim512_fp16_sm80.cu -o /home/v-feiychen/flash-attention/build/temp.linux-x86_64-cpython-39/csrc/flash_attn/src/flash_bwd_hdim512_fp16_sm80.o -D__CUDA_NO_HALF_OPERATORS__ -D__CUDA_NO_HALF_CONVERSIONS__ -D__CUDA_NO_BFLOAT16_CONVERSIONS__ -D__CUDA_NO_HALF2_OPERATORS__ --expt-relaxed-constexpr --compiler-options ''"'"'-fPIC'"'"'' -O3 -std=c++17 -U__CUDA_NO_HALF_OPERATORS__ -U__CUDA_NO_HALF_CONVERSIONS__ -U__CUDA_NO_HALF2_OPERATORS__ -U__CUDA_NO_BFLOAT16_CONVERSIONS__ --expt-relaxed-constexpr --expt-extended-lambda --use_fast_math --ptxas-options=-v -lineinfo -gencode arch=compute_80,code=sm_80 --threads 4 -DTORCH_API_INCLUDE_EXTENSION_H '-DPYBIND11_COMPILER_TYPE="_gcc"' '-DPYBIND11_STDLIB="_libstdcpp"' '-DPYBIND11_BUILD_ABI="_cxxabi1011"' -DTORCH_EXTENSION_NAME=flash_attn_2_cuda -D_GLIBCXX_USE_CXX11_ABI=0
FAILED: /home/v-feiychen/flash-attention/build/temp.linux-x86_64-cpython-39/csrc/flash_attn/src/flash_bwd_hdim512_fp16_sm80.o 
/usr/local/cuda-11.4/bin/nvcc  -I/home/v-feiychen/flash-attention/csrc/flash_attn -I/home/v-feiychen/flash-attention/csrc/flash_attn/src -I/home/v-feiychen/flash-attention/csrc/cutlass/include -I/anaconda/envs/flashattention/lib/python3.9/site-packages/torch/include -I/anaconda/envs/flashattention/lib/python3.9/site-packages/torch/include/torch/csrc/api/include -I/anaconda/envs/flashattention/lib/python3.9/site-packages/torch/include/TH -I/anaconda/envs/flashattention/lib/python3.9/site-packages/torch/include/THC -I/usr/local/cuda-11.4/include -I/anaconda/envs/flashattention/include/python3.9 -c -c /home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_bwd_hdim512_fp16_sm80.cu -o /home/v-feiychen/flash-attention/build/temp.linux-x86_64-cpython-39/csrc/flash_attn/src/flash_bwd_hdim512_fp16_sm80.o -D__CUDA_NO_HALF_OPERATORS__ -D__CUDA_NO_HALF_CONVERSIONS__ -D__CUDA_NO_BFLOAT16_CONVERSIONS__ -D__CUDA_NO_HALF2_OPERATORS__ --expt-relaxed-constexpr --compiler-options ''"'"'-fPIC'"'"'' -O3 -std=c++17 -U__CUDA_NO_HALF_OPERATORS__ -U__CUDA_NO_HALF_CONVERSIONS__ -U__CUDA_NO_HALF2_OPERATORS__ -U__CUDA_NO_BFLOAT16_CONVERSIONS__ --expt-relaxed-constexpr --expt-extended-lambda --use_fast_math --ptxas-options=-v -lineinfo -gencode arch=compute_80,code=sm_80 --threads 4 -DTORCH_API_INCLUDE_EXTENSION_H '-DPYBIND11_COMPILER_TYPE="_gcc"' '-DPYBIND11_STDLIB="_libstdcpp"' '-DPYBIND11_BUILD_ABI="_cxxabi1011"' -DTORCH_EXTENSION_NAME=flash_attn_2_cuda -D_GLIBCXX_USE_CXX11_ABI=0
/anaconda/envs/flashattention/lib/python3.9/site-packages/torch/include/c10/core/SymInt.h(84): warning: integer conversion resulted in a change of sign

/home/v-feiychen/flash-attention/csrc/flash_attn/src/kernel_traits.h(246): error: static assertion failed
          detected during:
            instantiation of class "Flash_bwd_kernel_traits<kHeadDim_, kBlockM_, kBlockN_, kNWarps_, AtomLayoutMSdP_, AtomLayoutNdKV, AtomLayoutMdQ, Is_V_in_regs_, No_double_buffer_, elem_type, Base> [with kHeadDim_=512, kBlockM_=32, kBlockN_=32, kNWarps_=4, AtomLayoutMSdP_=2, AtomLayoutNdKV=2, AtomLayoutMdQ=2, Is_V_in_regs_=false, No_double_buffer_=false, elem_type=cutlass::half_t, Base=Flash_kernel_traits<512, 32, 32, 4, cutlass::half_t>]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_bwd_launch_template.h(48): here
            instantiation of "void run_flash_bwd_seqk_parallel<Kernel_traits,Is_dropout>(Flash_bwd_params &, cudaStream_t, __nv_bool) [with Kernel_traits=Flash_bwd_kernel_traits<512, 32, 32, 4, 2, 2, 2, false, false, cutlass::half_t, Flash_kernel_traits<512, 32, 32, 4, cutlass::half_t>>, Is_dropout=true]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_bwd_launch_template.h(134): here
            instantiation of "void run_flash_bwd<Kernel_traits,Is_dropout>(Flash_bwd_params &, cudaStream_t, __nv_bool) [with Kernel_traits=Flash_bwd_kernel_traits<512, 32, 32, 4, 2, 2, 2, false, false, cutlass::half_t, Flash_kernel_traits<512, 32, 32, 4, cutlass::half_t>>, Is_dropout=true]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_bwd_launch_template.h(365): here
            instantiation of "void run_mha_bwd_hdim512<T>(Flash_bwd_params &, cudaStream_t, __nv_bool) [with T=cutlass::half_t]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_bwd_hdim512_fp16_sm80.cu(9): here

/home/v-feiychen/flash-attention/csrc/cutlass/include/cute/layout.hpp(1395): error: static assertion failed with "Layout shape does not divide the target shape."
          detected during:
            instantiation of "auto cute::tile_to_shape(const cute::Layout<Shape, Stride> &, const TrgShape &, const ModeOrder &) [with Shape=cute::Shape<cute::Int<32>, cute::Int<64>>, Stride=cute::Stride<cute::_64, cute::_1>, TrgShape=cute::Shape<cute::Int<32>, cute::Int<32>>, ModeOrder=cute::GenColMajor]" 
/home/v-feiychen/flash-attention/csrc/cutlass/include/cute/swizzle_layout.hpp(854): here
            instantiation of "auto cute::tile_to_shape(const cute::ComposedLayout<Swizzle, Offset, LayoutA> &, const Shape &, const ModeOrder &) [with Swizzle=cute::Swizzle<3, 3, 3>, Offset=cute::Int<0>, LayoutA=cute::Layout<cute::Shape<cute::Int<32>, cute::Int<64>>, cute::Stride<cute::_64, cute::_1>>, Shape=cute::Shape<cute::Int<32>, cute::Int<32>>, ModeOrder=cute::GenColMajor]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/kernel_traits.h(258): here
            instantiation of class "Flash_bwd_kernel_traits<kHeadDim_, kBlockM_, kBlockN_, kNWarps_, AtomLayoutMSdP_, AtomLayoutNdKV, AtomLayoutMdQ, Is_V_in_regs_, No_double_buffer_, elem_type, Base> [with kHeadDim_=512, kBlockM_=32, kBlockN_=32, kNWarps_=4, AtomLayoutMSdP_=2, AtomLayoutNdKV=2, AtomLayoutMdQ=2, Is_V_in_regs_=false, No_double_buffer_=false, elem_type=cutlass::half_t, Base=Flash_kernel_traits<512, 32, 32, 4, cutlass::half_t>]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_bwd_launch_template.h(48): here
            instantiation of "void run_flash_bwd_seqk_parallel<Kernel_traits,Is_dropout>(Flash_bwd_params &, cudaStream_t, __nv_bool) [with Kernel_traits=Flash_bwd_kernel_traits<512, 32, 32, 4, 2, 2, 2, false, false, cutlass::half_t, Flash_kernel_traits<512, 32, 32, 4, cutlass::half_t>>, Is_dropout=true]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_bwd_launch_template.h(134): here
            instantiation of "void run_flash_bwd<Kernel_traits,Is_dropout>(Flash_bwd_params &, cudaStream_t, __nv_bool) [with Kernel_traits=Flash_bwd_kernel_traits<512, 32, 32, 4, 2, 2, 2, false, false, cutlass::half_t, Flash_kernel_traits<512, 32, 32, 4, cutlass::half_t>>, Is_dropout=true]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_bwd_launch_template.h(365): here
            instantiation of "void run_mha_bwd_hdim512<T>(Flash_bwd_params &, cudaStream_t, __nv_bool) [with T=cutlass::half_t]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_bwd_hdim512_fp16_sm80.cu(9): here

/home/v-feiychen/flash-attention/csrc/cutlass/include/cute/layout.hpp(1395): error: static assertion failed with "Layout shape does not divide the target shape."
          detected during:
            instantiation of "auto cute::tile_to_shape(const cute::Layout<Shape, Stride> &, const TrgShape &, const ModeOrder &) [with Shape=cute::Shape<cute::Int<64>, cute::Int<32>>, Stride=cute::Stride<cute::_1, cute::Int<64>>, TrgShape=cute::Shape<cute::Int<32>, cute::Int<32>>, ModeOrder=cute::GenColMajor]" 
/home/v-feiychen/flash-attention/csrc/cutlass/include/cute/swizzle_layout.hpp(854): here
            instantiation of "auto cute::tile_to_shape(const cute::ComposedLayout<Swizzle, Offset, LayoutA> &, const Shape &, const ModeOrder &) [with Swizzle=cute::Swizzle<3, 3, 3>, Offset=cute::Int<0>, LayoutA=cute::Layout<cute::Shape<cute::Int<64>, cute::Int<32>>, cute::Stride<cute::_1, cute::Int<64>>>, Shape=cute::Shape<cute::Int<32>, cute::Int<32>>, ModeOrder=cute::GenColMajor]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/kernel_traits.h(265): here
            instantiation of class "Flash_bwd_kernel_traits<kHeadDim_, kBlockM_, kBlockN_, kNWarps_, AtomLayoutMSdP_, AtomLayoutNdKV, AtomLayoutMdQ, Is_V_in_regs_, No_double_buffer_, elem_type, Base> [with kHeadDim_=512, kBlockM_=32, kBlockN_=32, kNWarps_=4, AtomLayoutMSdP_=2, AtomLayoutNdKV=2, AtomLayoutMdQ=2, Is_V_in_regs_=false, No_double_buffer_=false, elem_type=cutlass::half_t, Base=Flash_kernel_traits<512, 32, 32, 4, cutlass::half_t>]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_bwd_launch_template.h(48): here
            instantiation of "void run_flash_bwd_seqk_parallel<Kernel_traits,Is_dropout>(Flash_bwd_params &, cudaStream_t, __nv_bool) [with Kernel_traits=Flash_bwd_kernel_traits<512, 32, 32, 4, 2, 2, 2, false, false, cutlass::half_t, Flash_kernel_traits<512, 32, 32, 4, cutlass::half_t>>, Is_dropout=true]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_bwd_launch_template.h(134): here
            instantiation of "void run_flash_bwd<Kernel_traits,Is_dropout>(Flash_bwd_params &, cudaStream_t, __nv_bool) [with Kernel_traits=Flash_bwd_kernel_traits<512, 32, 32, 4, 2, 2, 2, false, false, cutlass::half_t, Flash_kernel_traits<512, 32, 32, 4, cutlass::half_t>>, Is_dropout=true]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_bwd_launch_template.h(365): here
            instantiation of "void run_mha_bwd_hdim512<T>(Flash_bwd_params &, cudaStream_t, __nv_bool) [with T=cutlass::half_t]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_bwd_hdim512_fp16_sm80.cu(9): here

/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_bwd_kernel.h(853): error: static assertion failed
          detected during:
            instantiation of "void flash::compute_dq_dk_dv_1colblock<Kernel_traits,Is_dropout,Is_causal,Is_even_MN,Is_even_K,Is_first,Is_last,Seq_parallel,Params>(const Params &, int, int, int) [with Kernel_traits=Flash_bwd_kernel_traits<512, 32, 32, 4, 2, 2, 2, false, false, cutlass::half_t, Flash_kernel_traits<512, 32, 32, 4, cutlass::half_t>>, Is_dropout=true, Is_causal=true, Is_even_MN=true, Is_even_K=true, Is_first=false, Is_last=false, Seq_parallel=true, Params=Flash_bwd_params]" 
(1591): here
            instantiation of "void flash::compute_dq_dk_dv_seqk_parallel<Kernel_traits,Is_dropout,Is_causal,Is_even_MN,Is_even_K,Params>(const Params &) [with Kernel_traits=Flash_bwd_kernel_traits<512, 32, 32, 4, 2, 2, 2, false, false, cutlass::half_t, Flash_kernel_traits<512, 32, 32, 4, cutlass::half_t>>, Is_dropout=true, Is_causal=true, Is_even_MN=true, Is_even_K=true, Params=Flash_bwd_params]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_bwd_launch_template.h(28): here
            instantiation of "void flash_bwd_dq_dk_dv_loop_seqk_parallel_kernel<Kernel_traits,Is_dropout,Is_causal,Is_even_MN,Is_even_K>(Flash_bwd_params) [with Kernel_traits=Flash_bwd_kernel_traits<512, 32, 32, 4, 2, 2, 2, false, false, cutlass::half_t, Flash_kernel_traits<512, 32, 32, 4, cutlass::half_t>>, Is_dropout=true, Is_causal=true, Is_even_MN=true, Is_even_K=true]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_bwd_launch_template.h(62): here
            instantiation of "void run_flash_bwd_seqk_parallel<Kernel_traits,Is_dropout>(Flash_bwd_params &, cudaStream_t, __nv_bool) [with Kernel_traits=Flash_bwd_kernel_traits<512, 32, 32, 4, 2, 2, 2, false, false, cutlass::half_t, Flash_kernel_traits<512, 32, 32, 4, cutlass::half_t>>, Is_dropout=true]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_bwd_launch_template.h(134): here
            instantiation of "void run_flash_bwd<Kernel_traits,Is_dropout>(Flash_bwd_params &, cudaStream_t, __nv_bool) [with Kernel_traits=Flash_bwd_kernel_traits<512, 32, 32, 4, 2, 2, 2, false, false, cutlass::half_t, Flash_kernel_traits<512, 32, 32, 4, cutlass::half_t>>, Is_dropout=true]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_bwd_launch_template.h(365): here
            instantiation of "void run_mha_bwd_hdim512<T>(Flash_bwd_params &, cudaStream_t, __nv_bool) [with T=cutlass::half_t]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_bwd_hdim512_fp16_sm80.cu(9): here

/home/v-feiychen/flash-attention/csrc/flash_attn/src/softmax.h(217): error: static assertion failed
          detected during:
            instantiation of "void flash::apply_dropout(cute::Tensor<Engine, Layout> &, uint8_t, unsigned long long, unsigned long long, uint32_t, uint32_t, uint32_t) [with encode_dropout_in_sign_bit=true, Engine=cute::ViewEngine<std::remove_cv_t<std::remove_reference_t<float &>> *>, Layout=cute::Layout<cute::Shape<cute::Shape<cute::_2, cute::_2, cute::_2>, cute::_1, cute::_1>, cute::Stride<cute::tuple<cute::_1, cute::_2, cute::constant<int, 4>>, cute::constant<int, 0>, cute::constant<int, 0>>>]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_bwd_kernel.h(859): here
            instantiation of "void flash::compute_dq_dk_dv_1colblock<Kernel_traits,Is_dropout,Is_causal,Is_even_MN,Is_even_K,Is_first,Is_last,Seq_parallel,Params>(const Params &, int, int, int) [with Kernel_traits=Flash_bwd_kernel_traits<512, 32, 32, 4, 2, 2, 2, false, false, cutlass::half_t, Flash_kernel_traits<512, 32, 32, 4, cutlass::half_t>>, Is_dropout=true, Is_causal=true, Is_even_MN=true, Is_even_K=true, Is_first=false, Is_last=false, Seq_parallel=true, Params=Flash_bwd_params]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_bwd_kernel.h(1591): here
            instantiation of "void flash::compute_dq_dk_dv_seqk_parallel<Kernel_traits,Is_dropout,Is_causal,Is_even_MN,Is_even_K,Params>(const Params &) [with Kernel_traits=Flash_bwd_kernel_traits<512, 32, 32, 4, 2, 2, 2, false, false, cutlass::half_t, Flash_kernel_traits<512, 32, 32, 4, cutlass::half_t>>, Is_dropout=true, Is_causal=true, Is_even_MN=true, Is_even_K=true, Params=Flash_bwd_params]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_bwd_launch_template.h(28): here
            instantiation of "void flash_bwd_dq_dk_dv_loop_seqk_parallel_kernel<Kernel_traits,Is_dropout,Is_causal,Is_even_MN,Is_even_K>(Flash_bwd_params) [with Kernel_traits=Flash_bwd_kernel_traits<512, 32, 32, 4, 2, 2, 2, false, false, cutlass::half_t, Flash_kernel_traits<512, 32, 32, 4, cutlass::half_t>>, Is_dropout=true, Is_causal=true, Is_even_MN=true, Is_even_K=true]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_bwd_launch_template.h(62): here
            instantiation of "void run_flash_bwd_seqk_parallel<Kernel_traits,Is_dropout>(Flash_bwd_params &, cudaStream_t, __nv_bool) [with Kernel_traits=Flash_bwd_kernel_traits<512, 32, 32, 4, 2, 2, 2, false, false, cutlass::half_t, Flash_kernel_traits<512, 32, 32, 4, cutlass::half_t>>, Is_dropout=true]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_bwd_launch_template.h(134): here
            instantiation of "void run_flash_bwd<Kernel_traits,Is_dropout>(Flash_bwd_params &, cudaStream_t, __nv_bool) [with Kernel_traits=Flash_bwd_kernel_traits<512, 32, 32, 4, 2, 2, 2, false, false, cutlass::half_t, Flash_kernel_traits<512, 32, 32, 4, cutlass::half_t>>, Is_dropout=true]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_bwd_launch_template.h(365): here
            instantiation of "void run_mha_bwd_hdim512<T>(Flash_bwd_params &, cudaStream_t, __nv_bool) [with T=cutlass::half_t]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_bwd_hdim512_fp16_sm80.cu(9): here

/home/v-feiychen/flash-attention/csrc/flash_attn/src/utils.h(182): error: static assertion failed
          detected during:
            instantiation of "void flash::gemm(Tensor0 &, Tensor1 &, Tensor2 &, const Tensor3 &, const Tensor4 &, TiledMma, TiledCopyA, TiledCopyB, ThrCopyA, ThrCopyB) [with A_in_regs=false, B_in_regs=false, Tensor0=cute::Tensor<cute::ArrayEngine<std::remove_cv_t<std::remove_reference_t<float &>>, 128>, cute::Layout<cute::tuple<cute::tuple<cute::_2, cute::_2>, cute::constant<int, 1>, cute::constant<int, 32>>, cute::tuple<cute::tuple<cute::_1, cute::_2>, cute::constant<int, 0>, cute::constant<int, 4>>>>, Tensor1=cute::Tensor<cute::ArrayEngine<std::remove_cv_t<std::remove_reference_t<cutlass::half_t &>>, 32>, cute::Layout<cute::tuple<cute::Shape<cute::_2, cute::_2, cute::_2>, cute::Int<2>, cute::_2>, cute::tuple<cute::tuple<cute::_1, cute::_2, cute::constant<int, 4>>, cute::_8, cute::_16>>>, Tensor2=cute::Tensor<cute::ArrayEngine<std::remove_cv_t<std::remove_reference_t<cutlass::half_t &>>, 256>, cute::Layout<cute::tuple<cute::tuple<cute::_2, cute::_2>, cute::tuple<cute::_4, cute::_8>, cute::_2>, cute::tuple<cute::tuple<cute::_1, cute::_2>, cute::Shape<cute::Int<4>, cute::_32>, cute::_16>>>, Tensor3=cute::Tensor<cute::ViewEngine<cute::smem_ptr<std::remove_cv_t<std::remove_reference_t<cutlass::half_t &>>>>, std::__tuple_element_t<(std::size_t)0UL, std::tuple<cute::Layout<cute::Shape<cute::tuple<cute::_8, cute::_1>, cute::Int<2>, cute::Int<2>>, cute::Stride<cute::tuple<cute::_1, cute::LayoutLeft::Apply<cute::_1>>, int, cute::_1024>>, unsigned int>>>, Tensor4=cute::Tensor<cute::ViewEngine<cute::smem_ptr<std::remove_cv_t<std::remove_reference_t<cutlass::half_t &>>>>, std::__tuple_element_t<(std::size_t)0UL, std::tuple<cute::Layout<cute::Shape<cute::tuple<cute::_8, cute::_1>, cute::tuple<cute::constant<int, 2>, cute::constant<int, 8>>, cute::Int<2>>, cute::Stride<cute::tuple<cute::_1, cute::LayoutLeft::Apply<cute::_1>>, cute::Stride<int, cute::constant<int, 2048>>, cute::_1024>>, unsigned int>>>, TiledMma=cute::TiledMMA<std::conditional_t<true, cute::MMA_Atom<cute::SM80_16x8x16_F32F16F16F32_TN>, cute::MMA_Atom<cute::SM80_16x8x16_F32BF16BF16F32_TN>>, cute::Layout<cute::Shape<cute::Int<2>, cute::Int<2>, cute::_1>, cute::LayoutLeft::Apply<cute::Shape<cute::Int<2>, cute::Int<2>, cute::_1>>>, cute::Layout<cute::Shape<cute::_1, cute::_2, cute::_1>, cute::LayoutLeft::Apply<cute::Shape<cute::_1, cute::_2, cute::_1>>>, cute::Tile<cute::Underscore, cute::Underscore, cute::Underscore>>, TiledCopyA=cute::TiledCopy<cute::Copy_Atom<cute::SM75_U16x8_LDSM_T, cutlass::half_t>, cute::Layout<cute::Shape<cute::tuple<cute::_4, cute::_8, cute::Int<2>, cute::Int<2>>, cute::tuple<cute::tuple<cute::_2, cute::_2, cute::_2>, cute::tuple<cute::_1, cute::_1>>>, cute::Stride<cute::tuple<cute::_64, cute::_1, cute::_16, cute::constant<int, 0>>, cute::tuple<cute::tuple<cute::constant<int, 32>, cute::_8, cute::_256>, cute::tuple<cute::Int<0>, cute::constant<int, 0>>>>>, cute::tuple<cute::Int<32>, cute::Int<16>>>, TiledCopyB=cute::TiledCopy<cute::Copy_Atom<cute::SM75_U16x8_LDSM_T, cutlass::half_t>, cute::Layout<cute::Shape<cute::tuple<cute::_4, cute::_8, cute::Int<2>, cute::Int<2>>, cute::tuple<cute::tuple<cute::_2, cute::_2>, cute::tuple<std::__tuple_element_t<1UL, std::tuple<cute::tuple<cute::_1, cute::Int<1>, cute::_1, cute::constant<int, 1>>, cute::constant<int, 2>>>, cute::constant<int, 1>>>>, cute::Stride<cute::tuple<cute::_64, cute::_1, cute::constant<int, 0>, cute::_8>, cute::tuple<cute::tuple<cute::constant<int, 32>, cute::constant<int, 256>>, cute::tuple<cute::_16, cute::constant<int, 0>>>>>, cute::tuple<cute::Int<32>, cute::Int<16>>>, ThrCopyA=cute::ThrCopy<cute::TiledCopy<cute::Copy_Atom<cute::SM75_U16x8_LDSM_T, cutlass::half_t>, cute::Layout<cute::Shape<cute::tuple<cute::_4, cute::_8, cute::Int<2>, cute::Int<2>>, cute::tuple<cute::tuple<cute::_2, cute::_2, cute::_2>, cute::tuple<cute::_1, cute::_1>>>, cute::Stride<cute::tuple<cute::_64, cute::_1, cute::_16, cute::constant<int, 0>>, cute::tuple<cute::tuple<cute::constant<int, 32>, cute::_8, cute::_256>, cute::tuple<cute::Int<0>, cute::constant<int, 0>>>>>, cute::tuple<cute::Int<32>, cute::Int<16>>>, int>, ThrCopyB=cute::ThrCopy<cute::TiledCopy<cute::Copy_Atom<cute::SM75_U16x8_LDSM_T, cutlass::half_t>, cute::Layout<cute::Shape<cute::tuple<cute::_4, cute::_8, cute::Int<2>, cute::Int<2>>, cute::tuple<cute::tuple<cute::_2, cute::_2>, cute::tuple<std::__tuple_element_t<1UL, std::tuple<cute::tuple<cute::_1, cute::Int<1>, cute::_1, cute::constant<int, 1>>, cute::constant<int, 2>>>, cute::constant<int, 1>>>>, cute::Stride<cute::tuple<cute::_64, cute::_1, cute::constant<int, 0>, cute::_8>, cute::tuple<cute::tuple<cute::constant<int, 32>, cute::constant<int, 256>>, cute::tuple<cute::_16, cute::constant<int, 0>>>>>, cute::tuple<cute::Int<32>, cute::Int<16>>>, int>]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_bwd_kernel.h(956): here
            instantiation of "void flash::compute_dq_dk_dv_1colblock<Kernel_traits,Is_dropout,Is_causal,Is_even_MN,Is_even_K,Is_first,Is_last,Seq_parallel,Params>(const Params &, int, int, int) [with Kernel_traits=Flash_bwd_kernel_traits<512, 32, 32, 4, 2, 2, 2, false, false, cutlass::half_t, Flash_kernel_traits<512, 32, 32, 4, cutlass::half_t>>, Is_dropout=true, Is_causal=true, Is_even_MN=true, Is_even_K=true, Is_first=false, Is_last=false, Seq_parallel=true, Params=Flash_bwd_params]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_bwd_kernel.h(1591): here
            instantiation of "void flash::compute_dq_dk_dv_seqk_parallel<Kernel_traits,Is_dropout,Is_causal,Is_even_MN,Is_even_K,Params>(const Params &) [with Kernel_traits=Flash_bwd_kernel_traits<512, 32, 32, 4, 2, 2, 2, false, false, cutlass::half_t, Flash_kernel_traits<512, 32, 32, 4, cutlass::half_t>>, Is_dropout=true, Is_causal=true, Is_even_MN=true, Is_even_K=true, Params=Flash_bwd_params]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_bwd_launch_template.h(28): here
            instantiation of "void flash_bwd_dq_dk_dv_loop_seqk_parallel_kernel<Kernel_traits,Is_dropout,Is_causal,Is_even_MN,Is_even_K>(Flash_bwd_params) [with Kernel_traits=Flash_bwd_kernel_traits<512, 32, 32, 4, 2, 2, 2, false, false, cutlass::half_t, Flash_kernel_traits<512, 32, 32, 4, cutlass::half_t>>, Is_dropout=true, Is_causal=true, Is_even_MN=true, Is_even_K=true]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_bwd_launch_template.h(62): here
            instantiation of "void run_flash_bwd_seqk_parallel<Kernel_traits,Is_dropout>(Flash_bwd_params &, cudaStream_t, __nv_bool) [with Kernel_traits=Flash_bwd_kernel_traits<512, 32, 32, 4, 2, 2, 2, false, false, cutlass::half_t, Flash_kernel_traits<512, 32, 32, 4, cutlass::half_t>>, Is_dropout=true]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_bwd_launch_template.h(134): here
            instantiation of "void run_flash_bwd<Kernel_traits,Is_dropout>(Flash_bwd_params &, cudaStream_t, __nv_bool) [with Kernel_traits=Flash_bwd_kernel_traits<512, 32, 32, 4, 2, 2, 2, false, false, cutlass::half_t, Flash_kernel_traits<512, 32, 32, 4, cutlass::half_t>>, Is_dropout=true]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_bwd_launch_template.h(365): here
            instantiation of "void run_mha_bwd_hdim512<T>(Flash_bwd_params &, cudaStream_t, __nv_bool) [with T=cutlass::half_t]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_bwd_hdim512_fp16_sm80.cu(9): here

/home/v-feiychen/flash-attention/csrc/cutlass/include/cute/algorithm/gemm.hpp(282): error: static assertion failed
          detected during:
            instantiation of "void cute::gemm(const cute::MMA_Atom<MMA> &, cute::Tensor<TD, DLayout> &, const cute::Tensor<TA, ALayout> &, const cute::Tensor<TB, BLayout> &, const cute::Tensor<TC, CLayout> &) [with MMA=cute::SM80_16x8x16_F32F16F16F32_TN, TD=cute::ArrayEngine<std::remove_cv_t<std::remove_reference_t<float &>>, 128>, DLayout=cute::Layout<cute::tuple<cute::tuple<cute::_2, cute::_2>, cute::constant<int, 1>, cute::constant<int, 32>>, cute::tuple<cute::tuple<cute::_1, cute::_2>, cute::constant<int, 0>, cute::constant<int, 4>>>, TA=cute::ViewEngine<std::remove_cv_t<std::remove_reference_t<cutlass::half_t &>> *>, ALayout=std::__tuple_element_t<(std::size_t)0UL, std::tuple<cute::Layout<cute::tuple<cute::Shape<cute::_2, cute::_2, cute::_2>, cute::Int<2>>, cute::tuple<cute::tuple<cute::_1, cute::_2, cute::constant<int, 4>>, cute::constant<int, 8>>>, int>>, TB=cute::ViewEngine<std::remove_cv_t<std::remove_reference_t<cutlass::half_t &>> *>, BLayout=std::__tuple_element_t<(std::size_t)0UL, std::tuple<cute::Layout<cute::tuple<cute::tuple<cute::_2, cute::_2>, cute::tuple<cute::_4, cute::_8>>, cute::tuple<cute::tuple<cute::_1, cute::_2>, cute::Shape<cute::Int<4>, cute::_32>>>, int>>, TC=cute::ArrayEngine<std::remove_cv_t<std::remove_reference_t<float &>>, 128>, CLayout=cute::Layout<cute::tuple<cute::tuple<cute::_2, cute::_2>, cute::constant<int, 1>, cute::constant<int, 32>>, cute::tuple<cute::tuple<cute::_1, cute::_2>, cute::constant<int, 0>, cute::constant<int, 4>>>, <unnamed>=(void *)nullptr]" 
(88): here
            instantiation of "void cute::gemm(const cute::MMA_Atom<MMA> &, const cute::Tensor<TA, ALayout> &, const cute::Tensor<TB, BLayout> &, cute::Tensor<TC, CLayout> &) [with MMA=cute::SM80_16x8x16_F32F16F16F32_TN, TA=cute::ViewEngine<std::remove_cv_t<std::remove_reference_t<cutlass::half_t &>> *>, ALayout=std::__tuple_element_t<(std::size_t)0UL, std::tuple<cute::Layout<cute::tuple<cute::Shape<cute::_2, cute::_2, cute::_2>, cute::Int<2>>, cute::tuple<cute::tuple<cute::_1, cute::_2, cute::constant<int, 4>>, cute::constant<int, 8>>>, int>>, TB=cute::ViewEngine<std::remove_cv_t<std::remove_reference_t<cutlass::half_t &>> *>, BLayout=std::__tuple_element_t<(std::size_t)0UL, std::tuple<cute::Layout<cute::tuple<cute::tuple<cute::_2, cute::_2>, cute::tuple<cute::_4, cute::_8>>, cute::tuple<cute::tuple<cute::_1, cute::_2>, cute::Shape<cute::Int<4>, cute::_32>>>, int>>, TC=cute::ArrayEngine<std::remove_cv_t<std::remove_reference_t<float &>>, 128>, CLayout=cute::Layout<cute::tuple<cute::tuple<cute::_2, cute::_2>, cute::constant<int, 1>, cute::constant<int, 32>>, cute::tuple<cute::tuple<cute::_1, cute::_2>, cute::constant<int, 0>, cute::constant<int, 4>>>]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/utils.h(197): here
            instantiation of "void flash::gemm(Tensor0 &, Tensor1 &, Tensor2 &, const Tensor3 &, const Tensor4 &, TiledMma, TiledCopyA, TiledCopyB, ThrCopyA, ThrCopyB) [with A_in_regs=false, B_in_regs=false, Tensor0=cute::Tensor<cute::ArrayEngine<std::remove_cv_t<std::remove_reference_t<float &>>, 128>, cute::Layout<cute::tuple<cute::tuple<cute::_2, cute::_2>, cute::constant<int, 1>, cute::constant<int, 32>>, cute::tuple<cute::tuple<cute::_1, cute::_2>, cute::constant<int, 0>, cute::constant<int, 4>>>>, Tensor1=cute::Tensor<cute::ArrayEngine<std::remove_cv_t<std::remove_reference_t<cutlass::half_t &>>, 32>, cute::Layout<cute::tuple<cute::Shape<cute::_2, cute::_2, cute::_2>, cute::Int<2>, cute::_2>, cute::tuple<cute::tuple<cute::_1, cute::_2, cute::constant<int, 4>>, cute::_8, cute::_16>>>, Tensor2=cute::Tensor<cute::ArrayEngine<std::remove_cv_t<std::remove_reference_t<cutlass::half_t &>>, 256>, cute::Layout<cute::tuple<cute::tuple<cute::_2, cute::_2>, cute::tuple<cute::_4, cute::_8>, cute::_2>, cute::tuple<cute::tuple<cute::_1, cute::_2>, cute::Shape<cute::Int<4>, cute::_32>, cute::_16>>>, Tensor3=cute::Tensor<cute::ViewEngine<cute::smem_ptr<std::remove_cv_t<std::remove_reference_t<cutlass::half_t &>>>>, std::__tuple_element_t<(std::size_t)0UL, std::tuple<cute::Layout<cute::Shape<cute::tuple<cute::_8, cute::_1>, cute::Int<2>, cute::Int<2>>, cute::Stride<cute::tuple<cute::_1, cute::LayoutLeft::Apply<cute::_1>>, int, cute::_1024>>, unsigned int>>>, Tensor4=cute::Tensor<cute::ViewEngine<cute::smem_ptr<std::remove_cv_t<std::remove_reference_t<cutlass::half_t &>>>>, std::__tuple_element_t<(std::size_t)0UL, std::tuple<cute::Layout<cute::Shape<cute::tuple<cute::_8, cute::_1>, cute::tuple<cute::constant<int, 2>, cute::constant<int, 8>>, cute::Int<2>>, cute::Stride<cute::tuple<cute::_1, cute::LayoutLeft::Apply<cute::_1>>, cute::Stride<int, cute::constant<int, 2048>>, cute::_1024>>, unsigned int>>>, TiledMma=cute::TiledMMA<std::conditional_t<true, cute::MMA_Atom<cute::SM80_16x8x16_F32F16F16F32_TN>, cute::MMA_Atom<cute::SM80_16x8x16_F32BF16BF16F32_TN>>, cute::Layout<cute::Shape<cute::Int<2>, cute::Int<2>, cute::_1>, cute::LayoutLeft::Apply<cute::Shape<cute::Int<2>, cute::Int<2>, cute::_1>>>, cute::Layout<cute::Shape<cute::_1, cute::_2, cute::_1>, cute::LayoutLeft::Apply<cute::Shape<cute::_1, cute::_2, cute::_1>>>, cute::Tile<cute::Underscore, cute::Underscore, cute::Underscore>>, TiledCopyA=cute::TiledCopy<cute::Copy_Atom<cute::SM75_U16x8_LDSM_T, cutlass::half_t>, cute::Layout<cute::Shape<cute::tuple<cute::_4, cute::_8, cute::Int<2>, cute::Int<2>>, cute::tuple<cute::tuple<cute::_2, cute::_2, cute::_2>, cute::tuple<cute::_1, cute::_1>>>, cute::Stride<cute::tuple<cute::_64, cute::_1, cute::_16, cute::constant<int, 0>>, cute::tuple<cute::tuple<cute::constant<int, 32>, cute::_8, cute::_256>, cute::tuple<cute::Int<0>, cute::constant<int, 0>>>>>, cute::tuple<cute::Int<32>, cute::Int<16>>>, TiledCopyB=cute::TiledCopy<cute::Copy_Atom<cute::SM75_U16x8_LDSM_T, cutlass::half_t>, cute::Layout<cute::Shape<cute::tuple<cute::_4, cute::_8, cute::Int<2>, cute::Int<2>>, cute::tuple<cute::tuple<cute::_2, cute::_2>, cute::tuple<std::__tuple_element_t<1UL, std::tuple<cute::tuple<cute::_1, cute::Int<1>, cute::_1, cute::constant<int, 1>>, cute::constant<int, 2>>>, cute::constant<int, 1>>>>, cute::Stride<cute::tuple<cute::_64, cute::_1, cute::constant<int, 0>, cute::_8>, cute::tuple<cute::tuple<cute::constant<int, 32>, cute::constant<int, 256>>, cute::tuple<cute::_16, cute::constant<int, 0>>>>>, cute::tuple<cute::Int<32>, cute::Int<16>>>, ThrCopyA=cute::ThrCopy<cute::TiledCopy<cute::Copy_Atom<cute::SM75_U16x8_LDSM_T, cutlass::half_t>, cute::Layout<cute::Shape<cute::tuple<cute::_4, cute::_8, cute::Int<2>, cute::Int<2>>, cute::tuple<cute::tuple<cute::_2, cute::_2, cute::_2>, cute::tuple<cute::_1, cute::_1>>>, cute::Stride<cute::tuple<cute::_64, cute::_1, cute::_16, cute::constant<int, 0>>, cute::tuple<cute::tuple<cute::constant<int, 32>, cute::_8, cute::_256>, cute::tuple<cute::Int<0>, cute::constant<int, 0>>>>>, cute::tuple<cute::Int<32>, cute::Int<16>>>, int>, ThrCopyB=cute::ThrCopy<cute::TiledCopy<cute::Copy_Atom<cute::SM75_U16x8_LDSM_T, cutlass::half_t>, cute::Layout<cute::Shape<cute::tuple<cute::_4, cute::_8, cute::Int<2>, cute::Int<2>>, cute::tuple<cute::tuple<cute::_2, cute::_2>, cute::tuple<std::__tuple_element_t<1UL, std::tuple<cute::tuple<cute::_1, cute::Int<1>, cute::_1, cute::constant<int, 1>>, cute::constant<int, 2>>>, cute::constant<int, 1>>>>, cute::Stride<cute::tuple<cute::_64, cute::_1, cute::constant<int, 0>, cute::_8>, cute::tuple<cute::tuple<cute::constant<int, 32>, cute::constant<int, 256>>, cute::tuple<cute::_16, cute::constant<int, 0>>>>>, cute::tuple<cute::Int<32>, cute::Int<16>>>, int>]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_bwd_kernel.h(956): here
            instantiation of "void flash::compute_dq_dk_dv_1colblock<Kernel_traits,Is_dropout,Is_causal,Is_even_MN,Is_even_K,Is_first,Is_last,Seq_parallel,Params>(const Params &, int, int, int) [with Kernel_traits=Flash_bwd_kernel_traits<512, 32, 32, 4, 2, 2, 2, false, false, cutlass::half_t, Flash_kernel_traits<512, 32, 32, 4, cutlass::half_t>>, Is_dropout=true, Is_causal=true, Is_even_MN=true, Is_even_K=true, Is_first=false, Is_last=false, Seq_parallel=true, Params=Flash_bwd_params]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_bwd_kernel.h(1591): here
            instantiation of "void flash::compute_dq_dk_dv_seqk_parallel<Kernel_traits,Is_dropout,Is_causal,Is_even_MN,Is_even_K,Params>(const Params &) [with Kernel_traits=Flash_bwd_kernel_traits<512, 32, 32, 4, 2, 2, 2, false, false, cutlass::half_t, Flash_kernel_traits<512, 32, 32, 4, cutlass::half_t>>, Is_dropout=true, Is_causal=true, Is_even_MN=true, Is_even_K=true, Params=Flash_bwd_params]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_bwd_launch_template.h(28): here
            instantiation of "void flash_bwd_dq_dk_dv_loop_seqk_parallel_kernel<Kernel_traits,Is_dropout,Is_causal,Is_even_MN,Is_even_K>(Flash_bwd_params) [with Kernel_traits=Flash_bwd_kernel_traits<512, 32, 32, 4, 2, 2, 2, false, false, cutlass::half_t, Flash_kernel_traits<512, 32, 32, 4, cutlass::half_t>>, Is_dropout=true, Is_causal=true, Is_even_MN=true, Is_even_K=true]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_bwd_launch_template.h(62): here
            instantiation of "void run_flash_bwd_seqk_parallel<Kernel_traits,Is_dropout>(Flash_bwd_params &, cudaStream_t, __nv_bool) [with Kernel_traits=Flash_bwd_kernel_traits<512, 32, 32, 4, 2, 2, 2, false, false, cutlass::half_t, Flash_kernel_traits<512, 32, 32, 4, cutlass::half_t>>, Is_dropout=true]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_bwd_launch_template.h(134): here
            instantiation of "void run_flash_bwd<Kernel_traits,Is_dropout>(Flash_bwd_params &, cudaStream_t, __nv_bool) [with Kernel_traits=Flash_bwd_kernel_traits<512, 32, 32, 4, 2, 2, 2, false, false, cutlass::half_t, Flash_kernel_traits<512, 32, 32, 4, cutlass::half_t>>, Is_dropout=true]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_bwd_launch_template.h(365): here
            instantiation of "void run_mha_bwd_hdim512<T>(Flash_bwd_params &, cudaStream_t, __nv_bool) [with T=cutlass::half_t]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_bwd_hdim512_fp16_sm80.cu(9): here

/home/v-feiychen/flash-attention/csrc/flash_attn/src/utils.h(184): error: static assertion failed
          detected during:
            instantiation of "void flash::gemm(Tensor0 &, Tensor1 &, Tensor2 &, const Tensor3 &, const Tensor4 &, TiledMma, TiledCopyA, TiledCopyB, ThrCopyA, ThrCopyB) [with A_in_regs=false, B_in_regs=false, Tensor0=cute::Tensor<cute::ArrayEngine<std::remove_cv_t<std::remove_reference_t<float &>>, 128>, cute::Layout<cute::tuple<cute::tuple<cute::_2, cute::_2>, cute::constant<int, 1>, cute::constant<int, 32>>, cute::tuple<cute::tuple<cute::_1, cute::_2>, cute::constant<int, 0>, cute::constant<int, 4>>>>, Tensor1=cute::Tensor<cute::ArrayEngine<std::remove_cv_t<std::remove_reference_t<cutlass::half_t &>>, 32>, cute::Layout<cute::Shape<cute::Shape<cute::_2, cute::_2, cute::_2>, cute::_1, cute::tuple<cute::_2, cute::_2>>, cute::tuple<cute::tuple<cute::_1, cute::_2, cute::constant<int, 4>>, cute::constant<int, 0>, cute::tuple<cute::Int<8>, cute::Int<16>>>>>, Tensor2=cute::Tensor<cute::ArrayEngine<std::remove_cv_t<std::remove_reference_t<cutlass::half_t &>>, 256>, cute::Layout<cute::tuple<cute::tuple<cute::_2, cute::_2>, cute::tuple<cute::_4, cute::_8>, cute::_2>, cute::tuple<cute::tuple<cute::_1, cute::_2>, cute::Shape<cute::Int<4>, cute::_32>, cute::_16>>>, Tensor3=cute::Tensor<cute::ViewEngine<cute::smem_ptr<std::remove_cv_t<std::remove_reference_t<cutlass::half_t &>>>>, std::__tuple_element_t<(std::size_t)0UL, std::tuple<cute::Layout<cute::Shape<cute::tuple<cute::_8, cute::_1>, cute::_1, cute::tuple<cute::_2, cute::_2>>, cute::Stride<cute::tuple<cute::_1, cute::LayoutLeft::Apply<cute::_1>>, cute::constant<int, 0>, cute::tuple<int, int>>>, unsigned int>>>, Tensor4=cute::Tensor<cute::ViewEngine<cute::smem_ptr<std::remove_cv_t<std::remove_reference_t<cutlass::half_t &>>>>, std::__tuple_element_t<(std::size_t)0UL, std::tuple<cute::Layout<cute::Shape<cute::tuple<cute::_8, cute::_1>, cute::tuple<cute::constant<int, 2>, cute::constant<int, 8>>, cute::Int<2>>, cute::Stride<cute::tuple<cute::_1, cute::LayoutLeft::Apply<cute::_1>>, cute::Stride<int, cute::constant<int, 2048>>, cute::_1024>>, unsigned int>>>, TiledMma=cute::TiledMMA<std::conditional_t<true, cute::MMA_Atom<cute::SM80_16x8x16_F32F16F16F32_TN>, cute::MMA_Atom<cute::SM80_16x8x16_F32BF16BF16F32_TN>>, cute::Layout<cute::Shape<cute::Int<2>, cute::Int<2>, cute::_1>, cute::LayoutLeft::Apply<cute::Shape<cute::Int<2>, cute::Int<2>, cute::_1>>>, cute::Layout<cute::Shape<cute::_1, cute::_2, cute::_1>, cute::LayoutLeft::Apply<cute::Shape<cute::_1, cute::_2, cute::_1>>>, cute::Tile<cute::Underscore, cute::Underscore, cute::Underscore>>, TiledCopyA=cute::TiledCopy<cute::Copy_Atom<cute::SM75_U32x4_LDSM_N, cutlass::half_t>, cute::Layout<cute::Shape<cute::tuple<cute::_4, cute::_8, cute::Int<2>, cute::Int<2>>, cute::tuple<cute::tuple<cute::_2, cute::_2, cute::_2>, cute::tuple<cute::_1, cute::_1>>>, cute::Stride<cute::tuple<cute::_64, cute::_1, cute::_16, cute::constant<int, 0>>, cute::tuple<cute::tuple<cute::constant<int, 32>, cute::_8, cute::_256>, cute::tuple<cute::Int<0>, cute::constant<int, 0>>>>>, cute::tuple<cute::Int<32>, cute::Int<16>>>, TiledCopyB=cute::TiledCopy<cute::Copy_Atom<cute::SM75_U16x8_LDSM_T, cutlass::half_t>, cute::Layout<cute::Shape<cute::tuple<cute::_4, cute::_8, cute::Int<2>, cute::Int<2>>, cute::tuple<cute::tuple<cute::_2, cute::_2>, cute::tuple<std::__tuple_element_t<1UL, std::tuple<cute::tuple<cute::_1, cute::Int<1>, cute::_1, cute::constant<int, 1>>, cute::constant<int, 2>>>, cute::constant<int, 1>>>>, cute::Stride<cute::tuple<cute::_64, cute::_1, cute::constant<int, 0>, cute::_8>, cute::tuple<cute::tuple<cute::constant<int, 32>, cute::constant<int, 256>>, cute::tuple<cute::_16, cute::constant<int, 0>>>>>, cute::tuple<cute::Int<32>, cute::Int<16>>>, ThrCopyA=cute::ThrCopy<cute::TiledCopy<cute::Copy_Atom<cute::SM75_U32x4_LDSM_N, cutlass::half_t>, cute::Layout<cute::Shape<cute::tuple<cute::_4, cute::_8, cute::Int<2>, cute::Int<2>>, cute::tuple<cute::tuple<cute::_2, cute::_2, cute::_2>, cute::tuple<cute::_1, cute::_1>>>, cute::Stride<cute::tuple<cute::_64, cute::_1, cute::_16, cute::constant<int, 0>>, cute::tuple<cute::tuple<cute::constant<int, 32>, cute::_8, cute::_256>, cute::tuple<cute::Int<0>, cute::constant<int, 0>>>>>, cute::tuple<cute::Int<32>, cute::Int<16>>>, int>, ThrCopyB=cute::ThrCopy<cute::TiledCopy<cute::Copy_Atom<cute::SM75_U16x8_LDSM_T, cutlass::half_t>, cute::Layout<cute::Shape<cute::tuple<cute::_4, cute::_8, cute::Int<2>, cute::Int<2>>, cute::tuple<cute::tuple<cute::_2, cute::_2>, cute::tuple<std::__tuple_element_t<1UL, std::tuple<cute::tuple<cute::_1, cute::Int<1>, cute::_1, cute::constant<int, 1>>, cute::constant<int, 2>>>, cute::constant<int, 1>>>>, cute::Stride<cute::tuple<cute::_64, cute::_1, cute::constant<int, 0>, cute::_8>, cute::tuple<cute::tuple<cute::constant<int, 32>, cute::constant<int, 256>>, cute::tuple<cute::_16, cute::constant<int, 0>>>>>, cute::tuple<cute::Int<32>, cute::Int<16>>>, int>]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_bwd_kernel.h(977): here
            instantiation of "void flash::compute_dq_dk_dv_1colblock<Kernel_traits,Is_dropout,Is_causal,Is_even_MN,Is_even_K,Is_first,Is_last,Seq_parallel,Params>(const Params &, int, int, int) [with Kernel_traits=Flash_bwd_kernel_traits<512, 32, 32, 4, 2, 2, 2, false, false, cutlass::half_t, Flash_kernel_traits<512, 32, 32, 4, cutlass::half_t>>, Is_dropout=true, Is_causal=true, Is_even_MN=true, Is_even_K=true, Is_first=false, Is_last=false, Seq_parallel=true, Params=Flash_bwd_params]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_bwd_kernel.h(1591): here
            instantiation of "void flash::compute_dq_dk_dv_seqk_parallel<Kernel_traits,Is_dropout,Is_causal,Is_even_MN,Is_even_K,Params>(const Params &) [with Kernel_traits=Flash_bwd_kernel_traits<512, 32, 32, 4, 2, 2, 2, false, false, cutlass::half_t, Flash_kernel_traits<512, 32, 32, 4, cutlass::half_t>>, Is_dropout=true, Is_causal=true, Is_even_MN=true, Is_even_K=true, Params=Flash_bwd_params]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_bwd_launch_template.h(28): here
            instantiation of "void flash_bwd_dq_dk_dv_loop_seqk_parallel_kernel<Kernel_traits,Is_dropout,Is_causal,Is_even_MN,Is_even_K>(Flash_bwd_params) [with Kernel_traits=Flash_bwd_kernel_traits<512, 32, 32, 4, 2, 2, 2, false, false, cutlass::half_t, Flash_kernel_traits<512, 32, 32, 4, cutlass::half_t>>, Is_dropout=true, Is_causal=true, Is_even_MN=true, Is_even_K=true]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_bwd_launch_template.h(62): here
            instantiation of "void run_flash_bwd_seqk_parallel<Kernel_traits,Is_dropout>(Flash_bwd_params &, cudaStream_t, __nv_bool) [with Kernel_traits=Flash_bwd_kernel_traits<512, 32, 32, 4, 2, 2, 2, false, false, cutlass::half_t, Flash_kernel_traits<512, 32, 32, 4, cutlass::half_t>>, Is_dropout=true]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_bwd_launch_template.h(134): here
            instantiation of "void run_flash_bwd<Kernel_traits,Is_dropout>(Flash_bwd_params &, cudaStream_t, __nv_bool) [with Kernel_traits=Flash_bwd_kernel_traits<512, 32, 32, 4, 2, 2, 2, false, false, cutlass::half_t, Flash_kernel_traits<512, 32, 32, 4, cutlass::half_t>>, Is_dropout=true]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_bwd_launch_template.h(365): here
            instantiation of "void run_mha_bwd_hdim512<T>(Flash_bwd_params &, cudaStream_t, __nv_bool) [with T=cutlass::half_t]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_bwd_hdim512_fp16_sm80.cu(9): here

/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_bwd_kernel.h(853): error: static assertion failed
          detected during:
            instantiation of "void flash::compute_dq_dk_dv_1colblock<Kernel_traits,Is_dropout,Is_causal,Is_even_MN,Is_even_K,Is_first,Is_last,Seq_parallel,Params>(const Params &, int, int, int) [with Kernel_traits=Flash_bwd_kernel_traits<512, 32, 32, 4, 2, 2, 2, false, false, cutlass::half_t, Flash_kernel_traits<512, 32, 32, 4, cutlass::half_t>>, Is_dropout=true, Is_causal=true, Is_even_MN=true, Is_even_K=false, Is_first=false, Is_last=false, Seq_parallel=true, Params=Flash_bwd_params]" 
(1591): here
            instantiation of "void flash::compute_dq_dk_dv_seqk_parallel<Kernel_traits,Is_dropout,Is_causal,Is_even_MN,Is_even_K,Params>(const Params &) [with Kernel_traits=Flash_bwd_kernel_traits<512, 32, 32, 4, 2, 2, 2, false, false, cutlass::half_t, Flash_kernel_traits<512, 32, 32, 4, cutlass::half_t>>, Is_dropout=true, Is_causal=true, Is_even_MN=true, Is_even_K=false, Params=Flash_bwd_params]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_bwd_launch_template.h(28): here
            instantiation of "void flash_bwd_dq_dk_dv_loop_seqk_parallel_kernel<Kernel_traits,Is_dropout,Is_causal,Is_even_MN,Is_even_K>(Flash_bwd_params) [with Kernel_traits=Flash_bwd_kernel_traits<512, 32, 32, 4, 2, 2, 2, false, false, cutlass::half_t, Flash_kernel_traits<512, 32, 32, 4, cutlass::half_t>>, Is_dropout=true, Is_causal=true, Is_even_MN=true, Is_even_K=false]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_bwd_launch_template.h(62): here
            instantiation of "void run_flash_bwd_seqk_parallel<Kernel_traits,Is_dropout>(Flash_bwd_params &, cudaStream_t, __nv_bool) [with Kernel_traits=Flash_bwd_kernel_traits<512, 32, 32, 4, 2, 2, 2, false, false, cutlass::half_t, Flash_kernel_traits<512, 32, 32, 4, cutlass::half_t>>, Is_dropout=true]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_bwd_launch_template.h(134): here
            instantiation of "void run_flash_bwd<Kernel_traits,Is_dropout>(Flash_bwd_params &, cudaStream_t, __nv_bool) [with Kernel_traits=Flash_bwd_kernel_traits<512, 32, 32, 4, 2, 2, 2, false, false, cutlass::half_t, Flash_kernel_traits<512, 32, 32, 4, cutlass::half_t>>, Is_dropout=true]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_bwd_launch_template.h(365): here
            instantiation of "void run_mha_bwd_hdim512<T>(Flash_bwd_params &, cudaStream_t, __nv_bool) [with T=cutlass::half_t]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_bwd_hdim512_fp16_sm80.cu(9): here

/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_bwd_kernel.h(853): error: static assertion failed
          detected during:
            instantiation of "void flash::compute_dq_dk_dv_1colblock<Kernel_traits,Is_dropout,Is_causal,Is_even_MN,Is_even_K,Is_first,Is_last,Seq_parallel,Params>(const Params &, int, int, int) [with Kernel_traits=Flash_bwd_kernel_traits<512, 32, 32, 4, 2, 2, 2, false, false, cutlass::half_t, Flash_kernel_traits<512, 32, 32, 4, cutlass::half_t>>, Is_dropout=true, Is_causal=true, Is_even_MN=false, Is_even_K=true, Is_first=false, Is_last=false, Seq_parallel=true, Params=Flash_bwd_params]" 
(1591): here
            instantiation of "void flash::compute_dq_dk_dv_seqk_parallel<Kernel_traits,Is_dropout,Is_causal,Is_even_MN,Is_even_K,Params>(const Params &) [with Kernel_traits=Flash_bwd_kernel_traits<512, 32, 32, 4, 2, 2, 2, false, false, cutlass::half_t, Flash_kernel_traits<512, 32, 32, 4, cutlass::half_t>>, Is_dropout=true, Is_causal=true, Is_even_MN=false, Is_even_K=true, Params=Flash_bwd_params]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_bwd_launch_template.h(28): here
            instantiation of "void flash_bwd_dq_dk_dv_loop_seqk_parallel_kernel<Kernel_traits,Is_dropout,Is_causal,Is_even_MN,Is_even_K>(Flash_bwd_params) [with Kernel_traits=Flash_bwd_kernel_traits<512, 32, 32, 4, 2, 2, 2, false, false, cutlass::half_t, Flash_kernel_traits<512, 32, 32, 4, cutlass::half_t>>, Is_dropout=true, Is_causal=true, Is_even_MN=false, Is_even_K=true]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_bwd_launch_template.h(62): here
            instantiation of "void run_flash_bwd_seqk_parallel<Kernel_traits,Is_dropout>(Flash_bwd_params &, cudaStream_t, __nv_bool) [with Kernel_traits=Flash_bwd_kernel_traits<512, 32, 32, 4, 2, 2, 2, false, false, cutlass::half_t, Flash_kernel_traits<512, 32, 32, 4, cutlass::half_t>>, Is_dropout=true]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_bwd_launch_template.h(134): here
            instantiation of "void run_flash_bwd<Kernel_traits,Is_dropout>(Flash_bwd_params &, cudaStream_t, __nv_bool) [with Kernel_traits=Flash_bwd_kernel_traits<512, 32, 32, 4, 2, 2, 2, false, false, cutlass::half_t, Flash_kernel_traits<512, 32, 32, 4, cutlass::half_t>>, Is_dropout=true]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_bwd_launch_template.h(365): here
            instantiation of "void run_mha_bwd_hdim512<T>(Flash_bwd_params &, cudaStream_t, __nv_bool) [with T=cutlass::half_t]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_bwd_hdim512_fp16_sm80.cu(9): here

/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_bwd_kernel.h(853): error: static assertion failed
          detected during:
            instantiation of "void flash::compute_dq_dk_dv_1colblock<Kernel_traits,Is_dropout,Is_causal,Is_even_MN,Is_even_K,Is_first,Is_last,Seq_parallel,Params>(const Params &, int, int, int) [with Kernel_traits=Flash_bwd_kernel_traits<512, 32, 32, 4, 2, 2, 2, false, false, cutlass::half_t, Flash_kernel_traits<512, 32, 32, 4, cutlass::half_t>>, Is_dropout=true, Is_causal=true, Is_even_MN=false, Is_even_K=false, Is_first=false, Is_last=false, Seq_parallel=true, Params=Flash_bwd_params]" 
(1591): here
            instantiation of "void flash::compute_dq_dk_dv_seqk_parallel<Kernel_traits,Is_dropout,Is_causal,Is_even_MN,Is_even_K,Params>(const Params &) [with Kernel_traits=Flash_bwd_kernel_traits<512, 32, 32, 4, 2, 2, 2, false, false, cutlass::half_t, Flash_kernel_traits<512, 32, 32, 4, cutlass::half_t>>, Is_dropout=true, Is_causal=true, Is_even_MN=false, Is_even_K=false, Params=Flash_bwd_params]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_bwd_launch_template.h(28): here
            instantiation of "void flash_bwd_dq_dk_dv_loop_seqk_parallel_kernel<Kernel_traits,Is_dropout,Is_causal,Is_even_MN,Is_even_K>(Flash_bwd_params) [with Kernel_traits=Flash_bwd_kernel_traits<512, 32, 32, 4, 2, 2, 2, false, false, cutlass::half_t, Flash_kernel_traits<512, 32, 32, 4, cutlass::half_t>>, Is_dropout=true, Is_causal=true, Is_even_MN=false, Is_even_K=false]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_bwd_launch_template.h(62): here
            instantiation of "void run_flash_bwd_seqk_parallel<Kernel_traits,Is_dropout>(Flash_bwd_params &, cudaStream_t, __nv_bool) [with Kernel_traits=Flash_bwd_kernel_traits<512, 32, 32, 4, 2, 2, 2, false, false, cutlass::half_t, Flash_kernel_traits<512, 32, 32, 4, cutlass::half_t>>, Is_dropout=true]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_bwd_launch_template.h(134): here
            instantiation of "void run_flash_bwd<Kernel_traits,Is_dropout>(Flash_bwd_params &, cudaStream_t, __nv_bool) [with Kernel_traits=Flash_bwd_kernel_traits<512, 32, 32, 4, 2, 2, 2, false, false, cutlass::half_t, Flash_kernel_traits<512, 32, 32, 4, cutlass::half_t>>, Is_dropout=true]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_bwd_launch_template.h(365): here
            instantiation of "void run_mha_bwd_hdim512<T>(Flash_bwd_params &, cudaStream_t, __nv_bool) [with T=cutlass::half_t]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_bwd_hdim512_fp16_sm80.cu(9): here

/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_bwd_kernel.h(853): error: static assertion failed
          detected during:
            instantiation of "void flash::compute_dq_dk_dv_1colblock<Kernel_traits,Is_dropout,Is_causal,Is_even_MN,Is_even_K,Is_first,Is_last,Seq_parallel,Params>(const Params &, int, int, int) [with Kernel_traits=Flash_bwd_kernel_traits<512, 32, 32, 4, 2, 2, 2, false, false, cutlass::half_t, Flash_kernel_traits<512, 32, 32, 4, cutlass::half_t>>, Is_dropout=true, Is_causal=false, Is_even_MN=true, Is_even_K=true, Is_first=false, Is_last=false, Seq_parallel=true, Params=Flash_bwd_params]" 
(1591): here
            instantiation of "void flash::compute_dq_dk_dv_seqk_parallel<Kernel_traits,Is_dropout,Is_causal,Is_even_MN,Is_even_K,Params>(const Params &) [with Kernel_traits=Flash_bwd_kernel_traits<512, 32, 32, 4, 2, 2, 2, false, false, cutlass::half_t, Flash_kernel_traits<512, 32, 32, 4, cutlass::half_t>>, Is_dropout=true, Is_causal=false, Is_even_MN=true, Is_even_K=true, Params=Flash_bwd_params]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_bwd_launch_template.h(28): here
            instantiation of "void flash_bwd_dq_dk_dv_loop_seqk_parallel_kernel<Kernel_traits,Is_dropout,Is_causal,Is_even_MN,Is_even_K>(Flash_bwd_params) [with Kernel_traits=Flash_bwd_kernel_traits<512, 32, 32, 4, 2, 2, 2, false, false, cutlass::half_t, Flash_kernel_traits<512, 32, 32, 4, cutlass::half_t>>, Is_dropout=true, Is_causal=false, Is_even_MN=true, Is_even_K=true]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_bwd_launch_template.h(62): here
            instantiation of "void run_flash_bwd_seqk_parallel<Kernel_traits,Is_dropout>(Flash_bwd_params &, cudaStream_t, __nv_bool) [with Kernel_traits=Flash_bwd_kernel_traits<512, 32, 32, 4, 2, 2, 2, false, false, cutlass::half_t, Flash_kernel_traits<512, 32, 32, 4, cutlass::half_t>>, Is_dropout=true]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_bwd_launch_template.h(134): here
            instantiation of "void run_flash_bwd<Kernel_traits,Is_dropout>(Flash_bwd_params &, cudaStream_t, __nv_bool) [with Kernel_traits=Flash_bwd_kernel_traits<512, 32, 32, 4, 2, 2, 2, false, false, cutlass::half_t, Flash_kernel_traits<512, 32, 32, 4, cutlass::half_t>>, Is_dropout=true]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_bwd_launch_template.h(365): here
            instantiation of "void run_mha_bwd_hdim512<T>(Flash_bwd_params &, cudaStream_t, __nv_bool) [with T=cutlass::half_t]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_bwd_hdim512_fp16_sm80.cu(9): here

/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_bwd_kernel.h(853): error: static assertion failed
          detected during:
            instantiation of "void flash::compute_dq_dk_dv_1colblock<Kernel_traits,Is_dropout,Is_causal,Is_even_MN,Is_even_K,Is_first,Is_last,Seq_parallel,Params>(const Params &, int, int, int) [with Kernel_traits=Flash_bwd_kernel_traits<512, 32, 32, 4, 2, 2, 2, false, false, cutlass::half_t, Flash_kernel_traits<512, 32, 32, 4, cutlass::half_t>>, Is_dropout=true, Is_causal=false, Is_even_MN=true, Is_even_K=false, Is_first=false, Is_last=false, Seq_parallel=true, Params=Flash_bwd_params]" 
(1591): here
            instantiation of "void flash::compute_dq_dk_dv_seqk_parallel<Kernel_traits,Is_dropout,Is_causal,Is_even_MN,Is_even_K,Params>(const Params &) [with Kernel_traits=Flash_bwd_kernel_traits<512, 32, 32, 4, 2, 2, 2, false, false, cutlass::half_t, Flash_kernel_traits<512, 32, 32, 4, cutlass::half_t>>, Is_dropout=true, Is_causal=false, Is_even_MN=true, Is_even_K=false, Params=Flash_bwd_params]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_bwd_launch_template.h(28): here
            instantiation of "void flash_bwd_dq_dk_dv_loop_seqk_parallel_kernel<Kernel_traits,Is_dropout,Is_causal,Is_even_MN,Is_even_K>(Flash_bwd_params) [with Kernel_traits=Flash_bwd_kernel_traits<512, 32, 32, 4, 2, 2, 2, false, false, cutlass::half_t, Flash_kernel_traits<512, 32, 32, 4, cutlass::half_t>>, Is_dropout=true, Is_causal=false, Is_even_MN=true, Is_even_K=false]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_bwd_launch_template.h(62): here
            instantiation of "void run_flash_bwd_seqk_parallel<Kernel_traits,Is_dropout>(Flash_bwd_params &, cudaStream_t, __nv_bool) [with Kernel_traits=Flash_bwd_kernel_traits<512, 32, 32, 4, 2, 2, 2, false, false, cutlass::half_t, Flash_kernel_traits<512, 32, 32, 4, cutlass::half_t>>, Is_dropout=true]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_bwd_launch_template.h(134): here
            instantiation of "void run_flash_bwd<Kernel_traits,Is_dropout>(Flash_bwd_params &, cudaStream_t, __nv_bool) [with Kernel_traits=Flash_bwd_kernel_traits<512, 32, 32, 4, 2, 2, 2, false, false, cutlass::half_t, Flash_kernel_traits<512, 32, 32, 4, cutlass::half_t>>, Is_dropout=true]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_bwd_launch_template.h(365): here
            instantiation of "void run_mha_bwd_hdim512<T>(Flash_bwd_params &, cudaStream_t, __nv_bool) [with T=cutlass::half_t]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_bwd_hdim512_fp16_sm80.cu(9): here

/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_bwd_kernel.h(853): error: static assertion failed
          detected during:
            instantiation of "void flash::compute_dq_dk_dv_1colblock<Kernel_traits,Is_dropout,Is_causal,Is_even_MN,Is_even_K,Is_first,Is_last,Seq_parallel,Params>(const Params &, int, int, int) [with Kernel_traits=Flash_bwd_kernel_traits<512, 32, 32, 4, 2, 2, 2, false, false, cutlass::half_t, Flash_kernel_traits<512, 32, 32, 4, cutlass::half_t>>, Is_dropout=true, Is_causal=false, Is_even_MN=false, Is_even_K=true, Is_first=false, Is_last=false, Seq_parallel=true, Params=Flash_bwd_params]" 
(1591): here
            instantiation of "void flash::compute_dq_dk_dv_seqk_parallel<Kernel_traits,Is_dropout,Is_causal,Is_even_MN,Is_even_K,Params>(const Params &) [with Kernel_traits=Flash_bwd_kernel_traits<512, 32, 32, 4, 2, 2, 2, false, false, cutlass::half_t, Flash_kernel_traits<512, 32, 32, 4, cutlass::half_t>>, Is_dropout=true, Is_causal=false, Is_even_MN=false, Is_even_K=true, Params=Flash_bwd_params]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_bwd_launch_template.h(28): here
            instantiation of "void flash_bwd_dq_dk_dv_loop_seqk_parallel_kernel<Kernel_traits,Is_dropout,Is_causal,Is_even_MN,Is_even_K>(Flash_bwd_params) [with Kernel_traits=Flash_bwd_kernel_traits<512, 32, 32, 4, 2, 2, 2, false, false, cutlass::half_t, Flash_kernel_traits<512, 32, 32, 4, cutlass::half_t>>, Is_dropout=true, Is_causal=false, Is_even_MN=false, Is_even_K=true]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_bwd_launch_template.h(62): here
            instantiation of "void run_flash_bwd_seqk_parallel<Kernel_traits,Is_dropout>(Flash_bwd_params &, cudaStream_t, __nv_bool) [with Kernel_traits=Flash_bwd_kernel_traits<512, 32, 32, 4, 2, 2, 2, false, false, cutlass::half_t, Flash_kernel_traits<512, 32, 32, 4, cutlass::half_t>>, Is_dropout=true]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_bwd_launch_template.h(134): here
            instantiation of "void run_flash_bwd<Kernel_traits,Is_dropout>(Flash_bwd_params &, cudaStream_t, __nv_bool) [with Kernel_traits=Flash_bwd_kernel_traits<512, 32, 32, 4, 2, 2, 2, false, false, cutlass::half_t, Flash_kernel_traits<512, 32, 32, 4, cutlass::half_t>>, Is_dropout=true]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_bwd_launch_template.h(365): here
            instantiation of "void run_mha_bwd_hdim512<T>(Flash_bwd_params &, cudaStream_t, __nv_bool) [with T=cutlass::half_t]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_bwd_hdim512_fp16_sm80.cu(9): here

/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_bwd_kernel.h(853): error: static assertion failed
          detected during:
            instantiation of "void flash::compute_dq_dk_dv_1colblock<Kernel_traits,Is_dropout,Is_causal,Is_even_MN,Is_even_K,Is_first,Is_last,Seq_parallel,Params>(const Params &, int, int, int) [with Kernel_traits=Flash_bwd_kernel_traits<512, 32, 32, 4, 2, 2, 2, false, false, cutlass::half_t, Flash_kernel_traits<512, 32, 32, 4, cutlass::half_t>>, Is_dropout=true, Is_causal=false, Is_even_MN=false, Is_even_K=false, Is_first=false, Is_last=false, Seq_parallel=true, Params=Flash_bwd_params]" 
(1591): here
            instantiation of "void flash::compute_dq_dk_dv_seqk_parallel<Kernel_traits,Is_dropout,Is_causal,Is_even_MN,Is_even_K,Params>(const Params &) [with Kernel_traits=Flash_bwd_kernel_traits<512, 32, 32, 4, 2, 2, 2, false, false, cutlass::half_t, Flash_kernel_traits<512, 32, 32, 4, cutlass::half_t>>, Is_dropout=true, Is_causal=false, Is_even_MN=false, Is_even_K=false, Params=Flash_bwd_params]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_bwd_launch_template.h(28): here
            instantiation of "void flash_bwd_dq_dk_dv_loop_seqk_parallel_kernel<Kernel_traits,Is_dropout,Is_causal,Is_even_MN,Is_even_K>(Flash_bwd_params) [with Kernel_traits=Flash_bwd_kernel_traits<512, 32, 32, 4, 2, 2, 2, false, false, cutlass::half_t, Flash_kernel_traits<512, 32, 32, 4, cutlass::half_t>>, Is_dropout=true, Is_causal=false, Is_even_MN=false, Is_even_K=false]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_bwd_launch_template.h(62): here
            instantiation of "void run_flash_bwd_seqk_parallel<Kernel_traits,Is_dropout>(Flash_bwd_params &, cudaStream_t, __nv_bool) [with Kernel_traits=Flash_bwd_kernel_traits<512, 32, 32, 4, 2, 2, 2, false, false, cutlass::half_t, Flash_kernel_traits<512, 32, 32, 4, cutlass::half_t>>, Is_dropout=true]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_bwd_launch_template.h(134): here
            instantiation of "void run_flash_bwd<Kernel_traits,Is_dropout>(Flash_bwd_params &, cudaStream_t, __nv_bool) [with Kernel_traits=Flash_bwd_kernel_traits<512, 32, 32, 4, 2, 2, 2, false, false, cutlass::half_t, Flash_kernel_traits<512, 32, 32, 4, cutlass::half_t>>, Is_dropout=true]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_bwd_launch_template.h(365): here
            instantiation of "void run_mha_bwd_hdim512<T>(Flash_bwd_params &, cudaStream_t, __nv_bool) [with T=cutlass::half_t]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_bwd_hdim512_fp16_sm80.cu(9): here

/home/v-feiychen/flash-attention/csrc/flash_attn/src/kernel_traits.h(246): error: static assertion failed
          detected during:
            instantiation of class "Flash_bwd_kernel_traits<kHeadDim_, kBlockM_, kBlockN_, kNWarps_, AtomLayoutMSdP_, AtomLayoutNdKV, AtomLayoutMdQ, Is_V_in_regs_, No_double_buffer_, elem_type, Base> [with kHeadDim_=512, kBlockM_=32, kBlockN_=32, kNWarps_=4, AtomLayoutMSdP_=2, AtomLayoutNdKV=2, AtomLayoutMdQ=2, Is_V_in_regs_=false, No_double_buffer_=true, elem_type=cutlass::half_t, Base=Flash_kernel_traits<512, 32, 32, 4, cutlass::half_t>]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_bwd_launch_template.h(48): here
            instantiation of "void run_flash_bwd_seqk_parallel<Kernel_traits,Is_dropout>(Flash_bwd_params &, cudaStream_t, __nv_bool) [with Kernel_traits=Flash_bwd_kernel_traits<512, 32, 32, 4, 2, 2, 2, false, true, cutlass::half_t, Flash_kernel_traits<512, 32, 32, 4, cutlass::half_t>>, Is_dropout=true]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_bwd_launch_template.h(134): here
            instantiation of "void run_flash_bwd<Kernel_traits,Is_dropout>(Flash_bwd_params &, cudaStream_t, __nv_bool) [with Kernel_traits=Flash_bwd_kernel_traits<512, 32, 32, 4, 2, 2, 2, false, true, cutlass::half_t, Flash_kernel_traits<512, 32, 32, 4, cutlass::half_t>>, Is_dropout=true]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_bwd_launch_template.h(365): here
            instantiation of "void run_mha_bwd_hdim512<T>(Flash_bwd_params &, cudaStream_t, __nv_bool) [with T=cutlass::half_t]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_bwd_hdim512_fp16_sm80.cu(9): here

/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_bwd_kernel.h(853): error: static assertion failed
          detected during:
            instantiation of "void flash::compute_dq_dk_dv_1colblock<Kernel_traits,Is_dropout,Is_causal,Is_even_MN,Is_even_K,Is_first,Is_last,Seq_parallel,Params>(const Params &, int, int, int) [with Kernel_traits=Flash_bwd_kernel_traits<512, 32, 32, 4, 2, 2, 2, false, true, cutlass::half_t, Flash_kernel_traits<512, 32, 32, 4, cutlass::half_t>>, Is_dropout=true, Is_causal=true, Is_even_MN=true, Is_even_K=true, Is_first=false, Is_last=false, Seq_parallel=true, Params=Flash_bwd_params]" 
(1591): here
            instantiation of "void flash::compute_dq_dk_dv_seqk_parallel<Kernel_traits,Is_dropout,Is_causal,Is_even_MN,Is_even_K,Params>(const Params &) [with Kernel_traits=Flash_bwd_kernel_traits<512, 32, 32, 4, 2, 2, 2, false, true, cutlass::half_t, Flash_kernel_traits<512, 32, 32, 4, cutlass::half_t>>, Is_dropout=true, Is_causal=true, Is_even_MN=true, Is_even_K=true, Params=Flash_bwd_params]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_bwd_launch_template.h(28): here
            instantiation of "void flash_bwd_dq_dk_dv_loop_seqk_parallel_kernel<Kernel_traits,Is_dropout,Is_causal,Is_even_MN,Is_even_K>(Flash_bwd_params) [with Kernel_traits=Flash_bwd_kernel_traits<512, 32, 32, 4, 2, 2, 2, false, true, cutlass::half_t, Flash_kernel_traits<512, 32, 32, 4, cutlass::half_t>>, Is_dropout=true, Is_causal=true, Is_even_MN=true, Is_even_K=true]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_bwd_launch_template.h(62): here
            instantiation of "void run_flash_bwd_seqk_parallel<Kernel_traits,Is_dropout>(Flash_bwd_params &, cudaStream_t, __nv_bool) [with Kernel_traits=Flash_bwd_kernel_traits<512, 32, 32, 4, 2, 2, 2, false, true, cutlass::half_t, Flash_kernel_traits<512, 32, 32, 4, cutlass::half_t>>, Is_dropout=true]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_bwd_launch_template.h(134): here
            instantiation of "void run_flash_bwd<Kernel_traits,Is_dropout>(Flash_bwd_params &, cudaStream_t, __nv_bool) [with Kernel_traits=Flash_bwd_kernel_traits<512, 32, 32, 4, 2, 2, 2, false, true, cutlass::half_t, Flash_kernel_traits<512, 32, 32, 4, cutlass::half_t>>, Is_dropout=true]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_bwd_launch_template.h(365): here
            instantiation of "void run_mha_bwd_hdim512<T>(Flash_bwd_params &, cudaStream_t, __nv_bool) [with T=cutlass::half_t]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_bwd_hdim512_fp16_sm80.cu(9): here

/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_bwd_kernel.h(853): error: static assertion failed
          detected during:
            instantiation of "void flash::compute_dq_dk_dv_1colblock<Kernel_traits,Is_dropout,Is_causal,Is_even_MN,Is_even_K,Is_first,Is_last,Seq_parallel,Params>(const Params &, int, int, int) [with Kernel_traits=Flash_bwd_kernel_traits<512, 32, 32, 4, 2, 2, 2, false, true, cutlass::half_t, Flash_kernel_traits<512, 32, 32, 4, cutlass::half_t>>, Is_dropout=true, Is_causal=true, Is_even_MN=true, Is_even_K=false, Is_first=false, Is_last=false, Seq_parallel=true, Params=Flash_bwd_params]" 
(1591): here
            instantiation of "void flash::compute_dq_dk_dv_seqk_parallel<Kernel_traits,Is_dropout,Is_causal,Is_even_MN,Is_even_K,Params>(const Params &) [with Kernel_traits=Flash_bwd_kernel_traits<512, 32, 32, 4, 2, 2, 2, false, true, cutlass::half_t, Flash_kernel_traits<512, 32, 32, 4, cutlass::half_t>>, Is_dropout=true, Is_causal=true, Is_even_MN=true, Is_even_K=false, Params=Flash_bwd_params]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_bwd_launch_template.h(28): here
            instantiation of "void flash_bwd_dq_dk_dv_loop_seqk_parallel_kernel<Kernel_traits,Is_dropout,Is_causal,Is_even_MN,Is_even_K>(Flash_bwd_params) [with Kernel_traits=Flash_bwd_kernel_traits<512, 32, 32, 4, 2, 2, 2, false, true, cutlass::half_t, Flash_kernel_traits<512, 32, 32, 4, cutlass::half_t>>, Is_dropout=true, Is_causal=true, Is_even_MN=true, Is_even_K=false]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_bwd_launch_template.h(62): here
            instantiation of "void run_flash_bwd_seqk_parallel<Kernel_traits,Is_dropout>(Flash_bwd_params &, cudaStream_t, __nv_bool) [with Kernel_traits=Flash_bwd_kernel_traits<512, 32, 32, 4, 2, 2, 2, false, true, cutlass::half_t, Flash_kernel_traits<512, 32, 32, 4, cutlass::half_t>>, Is_dropout=true]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_bwd_launch_template.h(134): here
            instantiation of "void run_flash_bwd<Kernel_traits,Is_dropout>(Flash_bwd_params &, cudaStream_t, __nv_bool) [with Kernel_traits=Flash_bwd_kernel_traits<512, 32, 32, 4, 2, 2, 2, false, true, cutlass::half_t, Flash_kernel_traits<512, 32, 32, 4, cutlass::half_t>>, Is_dropout=true]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_bwd_launch_template.h(365): here
            instantiation of "void run_mha_bwd_hdim512<T>(Flash_bwd_params &, cudaStream_t, __nv_bool) [with T=cutlass::half_t]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_bwd_hdim512_fp16_sm80.cu(9): here

/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_bwd_kernel.h(853): error: static assertion failed
          detected during:
            instantiation of "void flash::compute_dq_dk_dv_1colblock<Kernel_traits,Is_dropout,Is_causal,Is_even_MN,Is_even_K,Is_first,Is_last,Seq_parallel,Params>(const Params &, int, int, int) [with Kernel_traits=Flash_bwd_kernel_traits<512, 32, 32, 4, 2, 2, 2, false, true, cutlass::half_t, Flash_kernel_traits<512, 32, 32, 4, cutlass::half_t>>, Is_dropout=true, Is_causal=true, Is_even_MN=false, Is_even_K=true, Is_first=false, Is_last=false, Seq_parallel=true, Params=Flash_bwd_params]" 
(1591): here
            instantiation of "void flash::compute_dq_dk_dv_seqk_parallel<Kernel_traits,Is_dropout,Is_causal,Is_even_MN,Is_even_K,Params>(const Params &) [with Kernel_traits=Flash_bwd_kernel_traits<512, 32, 32, 4, 2, 2, 2, false, true, cutlass::half_t, Flash_kernel_traits<512, 32, 32, 4, cutlass::half_t>>, Is_dropout=true, Is_causal=true, Is_even_MN=false, Is_even_K=true, Params=Flash_bwd_params]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_bwd_launch_template.h(28): here
            instantiation of "void flash_bwd_dq_dk_dv_loop_seqk_parallel_kernel<Kernel_traits,Is_dropout,Is_causal,Is_even_MN,Is_even_K>(Flash_bwd_params) [with Kernel_traits=Flash_bwd_kernel_traits<512, 32, 32, 4, 2, 2, 2, false, true, cutlass::half_t, Flash_kernel_traits<512, 32, 32, 4, cutlass::half_t>>, Is_dropout=true, Is_causal=true, Is_even_MN=false, Is_even_K=true]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_bwd_launch_template.h(62): here
            instantiation of "void run_flash_bwd_seqk_parallel<Kernel_traits,Is_dropout>(Flash_bwd_params &, cudaStream_t, __nv_bool) [with Kernel_traits=Flash_bwd_kernel_traits<512, 32, 32, 4, 2, 2, 2, false, true, cutlass::half_t, Flash_kernel_traits<512, 32, 32, 4, cutlass::half_t>>, Is_dropout=true]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_bwd_launch_template.h(134): here
            instantiation of "void run_flash_bwd<Kernel_traits,Is_dropout>(Flash_bwd_params &, cudaStream_t, __nv_bool) [with Kernel_traits=Flash_bwd_kernel_traits<512, 32, 32, 4, 2, 2, 2, false, true, cutlass::half_t, Flash_kernel_traits<512, 32, 32, 4, cutlass::half_t>>, Is_dropout=true]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_bwd_launch_template.h(365): here
            instantiation of "void run_mha_bwd_hdim512<T>(Flash_bwd_params &, cudaStream_t, __nv_bool) [with T=cutlass::half_t]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_bwd_hdim512_fp16_sm80.cu(9): here

/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_bwd_kernel.h(853): error: static assertion failed
          detected during:
            instantiation of "void flash::compute_dq_dk_dv_1colblock<Kernel_traits,Is_dropout,Is_causal,Is_even_MN,Is_even_K,Is_first,Is_last,Seq_parallel,Params>(const Params &, int, int, int) [with Kernel_traits=Flash_bwd_kernel_traits<512, 32, 32, 4, 2, 2, 2, false, true, cutlass::half_t, Flash_kernel_traits<512, 32, 32, 4, cutlass::half_t>>, Is_dropout=true, Is_causal=true, Is_even_MN=false, Is_even_K=false, Is_first=false, Is_last=false, Seq_parallel=true, Params=Flash_bwd_params]" 
(1591): here
            instantiation of "void flash::compute_dq_dk_dv_seqk_parallel<Kernel_traits,Is_dropout,Is_causal,Is_even_MN,Is_even_K,Params>(const Params &) [with Kernel_traits=Flash_bwd_kernel_traits<512, 32, 32, 4, 2, 2, 2, false, true, cutlass::half_t, Flash_kernel_traits<512, 32, 32, 4, cutlass::half_t>>, Is_dropout=true, Is_causal=true, Is_even_MN=false, Is_even_K=false, Params=Flash_bwd_params]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_bwd_launch_template.h(28): here
            instantiation of "void flash_bwd_dq_dk_dv_loop_seqk_parallel_kernel<Kernel_traits,Is_dropout,Is_causal,Is_even_MN,Is_even_K>(Flash_bwd_params) [with Kernel_traits=Flash_bwd_kernel_traits<512, 32, 32, 4, 2, 2, 2, false, true, cutlass::half_t, Flash_kernel_traits<512, 32, 32, 4, cutlass::half_t>>, Is_dropout=true, Is_causal=true, Is_even_MN=false, Is_even_K=false]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_bwd_launch_template.h(62): here
            instantiation of "void run_flash_bwd_seqk_parallel<Kernel_traits,Is_dropout>(Flash_bwd_params &, cudaStream_t, __nv_bool) [with Kernel_traits=Flash_bwd_kernel_traits<512, 32, 32, 4, 2, 2, 2, false, true, cutlass::half_t, Flash_kernel_traits<512, 32, 32, 4, cutlass::half_t>>, Is_dropout=true]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_bwd_launch_template.h(134): here
            instantiation of "void run_flash_bwd<Kernel_traits,Is_dropout>(Flash_bwd_params &, cudaStream_t, __nv_bool) [with Kernel_traits=Flash_bwd_kernel_traits<512, 32, 32, 4, 2, 2, 2, false, true, cutlass::half_t, Flash_kernel_traits<512, 32, 32, 4, cutlass::half_t>>, Is_dropout=true]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_bwd_launch_template.h(365): here
            instantiation of "void run_mha_bwd_hdim512<T>(Flash_bwd_params &, cudaStream_t, __nv_bool) [with T=cutlass::half_t]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_bwd_hdim512_fp16_sm80.cu(9): here

/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_bwd_kernel.h(853): error: static assertion failed
          detected during:
            instantiation of "void flash::compute_dq_dk_dv_1colblock<Kernel_traits,Is_dropout,Is_causal,Is_even_MN,Is_even_K,Is_first,Is_last,Seq_parallel,Params>(const Params &, int, int, int) [with Kernel_traits=Flash_bwd_kernel_traits<512, 32, 32, 4, 2, 2, 2, false, true, cutlass::half_t, Flash_kernel_traits<512, 32, 32, 4, cutlass::half_t>>, Is_dropout=true, Is_causal=false, Is_even_MN=true, Is_even_K=true, Is_first=false, Is_last=false, Seq_parallel=true, Params=Flash_bwd_params]" 
(1591): here
            instantiation of "void flash::compute_dq_dk_dv_seqk_parallel<Kernel_traits,Is_dropout,Is_causal,Is_even_MN,Is_even_K,Params>(const Params &) [with Kernel_traits=Flash_bwd_kernel_traits<512, 32, 32, 4, 2, 2, 2, false, true, cutlass::half_t, Flash_kernel_traits<512, 32, 32, 4, cutlass::half_t>>, Is_dropout=true, Is_causal=false, Is_even_MN=true, Is_even_K=true, Params=Flash_bwd_params]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_bwd_launch_template.h(28): here
            instantiation of "void flash_bwd_dq_dk_dv_loop_seqk_parallel_kernel<Kernel_traits,Is_dropout,Is_causal,Is_even_MN,Is_even_K>(Flash_bwd_params) [with Kernel_traits=Flash_bwd_kernel_traits<512, 32, 32, 4, 2, 2, 2, false, true, cutlass::half_t, Flash_kernel_traits<512, 32, 32, 4, cutlass::half_t>>, Is_dropout=true, Is_causal=false, Is_even_MN=true, Is_even_K=true]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_bwd_launch_template.h(62): here
            instantiation of "void run_flash_bwd_seqk_parallel<Kernel_traits,Is_dropout>(Flash_bwd_params &, cudaStream_t, __nv_bool) [with Kernel_traits=Flash_bwd_kernel_traits<512, 32, 32, 4, 2, 2, 2, false, true, cutlass::half_t, Flash_kernel_traits<512, 32, 32, 4, cutlass::half_t>>, Is_dropout=true]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_bwd_launch_template.h(134): here
            instantiation of "void run_flash_bwd<Kernel_traits,Is_dropout>(Flash_bwd_params &, cudaStream_t, __nv_bool) [with Kernel_traits=Flash_bwd_kernel_traits<512, 32, 32, 4, 2, 2, 2, false, true, cutlass::half_t, Flash_kernel_traits<512, 32, 32, 4, cutlass::half_t>>, Is_dropout=true]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_bwd_launch_template.h(365): here
            instantiation of "void run_mha_bwd_hdim512<T>(Flash_bwd_params &, cudaStream_t, __nv_bool) [with T=cutlass::half_t]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_bwd_hdim512_fp16_sm80.cu(9): here

/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_bwd_kernel.h(853): error: static assertion failed
          detected during:
            instantiation of "void flash::compute_dq_dk_dv_1colblock<Kernel_traits,Is_dropout,Is_causal,Is_even_MN,Is_even_K,Is_first,Is_last,Seq_parallel,Params>(const Params &, int, int, int) [with Kernel_traits=Flash_bwd_kernel_traits<512, 32, 32, 4, 2, 2, 2, false, true, cutlass::half_t, Flash_kernel_traits<512, 32, 32, 4, cutlass::half_t>>, Is_dropout=true, Is_causal=false, Is_even_MN=true, Is_even_K=false, Is_first=false, Is_last=false, Seq_parallel=true, Params=Flash_bwd_params]" 
(1591): here
            instantiation of "void flash::compute_dq_dk_dv_seqk_parallel<Kernel_traits,Is_dropout,Is_causal,Is_even_MN,Is_even_K,Params>(const Params &) [with Kernel_traits=Flash_bwd_kernel_traits<512, 32, 32, 4, 2, 2, 2, false, true, cutlass::half_t, Flash_kernel_traits<512, 32, 32, 4, cutlass::half_t>>, Is_dropout=true, Is_causal=false, Is_even_MN=true, Is_even_K=false, Params=Flash_bwd_params]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_bwd_launch_template.h(28): here
            instantiation of "void flash_bwd_dq_dk_dv_loop_seqk_parallel_kernel<Kernel_traits,Is_dropout,Is_causal,Is_even_MN,Is_even_K>(Flash_bwd_params) [with Kernel_traits=Flash_bwd_kernel_traits<512, 32, 32, 4, 2, 2, 2, false, true, cutlass::half_t, Flash_kernel_traits<512, 32, 32, 4, cutlass::half_t>>, Is_dropout=true, Is_causal=false, Is_even_MN=true, Is_even_K=false]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_bwd_launch_template.h(62): here
            instantiation of "void run_flash_bwd_seqk_parallel<Kernel_traits,Is_dropout>(Flash_bwd_params &, cudaStream_t, __nv_bool) [with Kernel_traits=Flash_bwd_kernel_traits<512, 32, 32, 4, 2, 2, 2, false, true, cutlass::half_t, Flash_kernel_traits<512, 32, 32, 4, cutlass::half_t>>, Is_dropout=true]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_bwd_launch_template.h(134): here
            instantiation of "void run_flash_bwd<Kernel_traits,Is_dropout>(Flash_bwd_params &, cudaStream_t, __nv_bool) [with Kernel_traits=Flash_bwd_kernel_traits<512, 32, 32, 4, 2, 2, 2, false, true, cutlass::half_t, Flash_kernel_traits<512, 32, 32, 4, cutlass::half_t>>, Is_dropout=true]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_bwd_launch_template.h(365): here
            instantiation of "void run_mha_bwd_hdim512<T>(Flash_bwd_params &, cudaStream_t, __nv_bool) [with T=cutlass::half_t]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_bwd_hdim512_fp16_sm80.cu(9): here

/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_bwd_kernel.h(853): error: static assertion failed
          detected during:
            instantiation of "void flash::compute_dq_dk_dv_1colblock<Kernel_traits,Is_dropout,Is_causal,Is_even_MN,Is_even_K,Is_first,Is_last,Seq_parallel,Params>(const Params &, int, int, int) [with Kernel_traits=Flash_bwd_kernel_traits<512, 32, 32, 4, 2, 2, 2, false, true, cutlass::half_t, Flash_kernel_traits<512, 32, 32, 4, cutlass::half_t>>, Is_dropout=true, Is_causal=false, Is_even_MN=false, Is_even_K=true, Is_first=false, Is_last=false, Seq_parallel=true, Params=Flash_bwd_params]" 
(1591): here
            instantiation of "void flash::compute_dq_dk_dv_seqk_parallel<Kernel_traits,Is_dropout,Is_causal,Is_even_MN,Is_even_K,Params>(const Params &) [with Kernel_traits=Flash_bwd_kernel_traits<512, 32, 32, 4, 2, 2, 2, false, true, cutlass::half_t, Flash_kernel_traits<512, 32, 32, 4, cutlass::half_t>>, Is_dropout=true, Is_causal=false, Is_even_MN=false, Is_even_K=true, Params=Flash_bwd_params]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_bwd_launch_template.h(28): here
            instantiation of "void flash_bwd_dq_dk_dv_loop_seqk_parallel_kernel<Kernel_traits,Is_dropout,Is_causal,Is_even_MN,Is_even_K>(Flash_bwd_params) [with Kernel_traits=Flash_bwd_kernel_traits<512, 32, 32, 4, 2, 2, 2, false, true, cutlass::half_t, Flash_kernel_traits<512, 32, 32, 4, cutlass::half_t>>, Is_dropout=true, Is_causal=false, Is_even_MN=false, Is_even_K=true]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_bwd_launch_template.h(62): here
            instantiation of "void run_flash_bwd_seqk_parallel<Kernel_traits,Is_dropout>(Flash_bwd_params &, cudaStream_t, __nv_bool) [with Kernel_traits=Flash_bwd_kernel_traits<512, 32, 32, 4, 2, 2, 2, false, true, cutlass::half_t, Flash_kernel_traits<512, 32, 32, 4, cutlass::half_t>>, Is_dropout=true]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_bwd_launch_template.h(134): here
            instantiation of "void run_flash_bwd<Kernel_traits,Is_dropout>(Flash_bwd_params &, cudaStream_t, __nv_bool) [with Kernel_traits=Flash_bwd_kernel_traits<512, 32, 32, 4, 2, 2, 2, false, true, cutlass::half_t, Flash_kernel_traits<512, 32, 32, 4, cutlass::half_t>>, Is_dropout=true]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_bwd_launch_template.h(365): here
            instantiation of "void run_mha_bwd_hdim512<T>(Flash_bwd_params &, cudaStream_t, __nv_bool) [with T=cutlass::half_t]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_bwd_hdim512_fp16_sm80.cu(9): here

/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_bwd_kernel.h(853): error: static assertion failed
          detected during:
            instantiation of "void flash::compute_dq_dk_dv_1colblock<Kernel_traits,Is_dropout,Is_causal,Is_even_MN,Is_even_K,Is_first,Is_last,Seq_parallel,Params>(const Params &, int, int, int) [with Kernel_traits=Flash_bwd_kernel_traits<512, 32, 32, 4, 2, 2, 2, false, true, cutlass::half_t, Flash_kernel_traits<512, 32, 32, 4, cutlass::half_t>>, Is_dropout=true, Is_causal=false, Is_even_MN=false, Is_even_K=false, Is_first=false, Is_last=false, Seq_parallel=true, Params=Flash_bwd_params]" 
(1591): here
            instantiation of "void flash::compute_dq_dk_dv_seqk_parallel<Kernel_traits,Is_dropout,Is_causal,Is_even_MN,Is_even_K,Params>(const Params &) [with Kernel_traits=Flash_bwd_kernel_traits<512, 32, 32, 4, 2, 2, 2, false, true, cutlass::half_t, Flash_kernel_traits<512, 32, 32, 4, cutlass::half_t>>, Is_dropout=true, Is_causal=false, Is_even_MN=false, Is_even_K=false, Params=Flash_bwd_params]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_bwd_launch_template.h(28): here
            instantiation of "void flash_bwd_dq_dk_dv_loop_seqk_parallel_kernel<Kernel_traits,Is_dropout,Is_causal,Is_even_MN,Is_even_K>(Flash_bwd_params) [with Kernel_traits=Flash_bwd_kernel_traits<512, 32, 32, 4, 2, 2, 2, false, true, cutlass::half_t, Flash_kernel_traits<512, 32, 32, 4, cutlass::half_t>>, Is_dropout=true, Is_causal=false, Is_even_MN=false, Is_even_K=false]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_bwd_launch_template.h(62): here
            instantiation of "void run_flash_bwd_seqk_parallel<Kernel_traits,Is_dropout>(Flash_bwd_params &, cudaStream_t, __nv_bool) [with Kernel_traits=Flash_bwd_kernel_traits<512, 32, 32, 4, 2, 2, 2, false, true, cutlass::half_t, Flash_kernel_traits<512, 32, 32, 4, cutlass::half_t>>, Is_dropout=true]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_bwd_launch_template.h(134): here
            instantiation of "void run_flash_bwd<Kernel_traits,Is_dropout>(Flash_bwd_params &, cudaStream_t, __nv_bool) [with Kernel_traits=Flash_bwd_kernel_traits<512, 32, 32, 4, 2, 2, 2, false, true, cutlass::half_t, Flash_kernel_traits<512, 32, 32, 4, cutlass::half_t>>, Is_dropout=true]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_bwd_launch_template.h(365): here
            instantiation of "void run_mha_bwd_hdim512<T>(Flash_bwd_params &, cudaStream_t, __nv_bool) [with T=cutlass::half_t]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_bwd_hdim512_fp16_sm80.cu(9): here

/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_bwd_kernel.h(853): error: static assertion failed
          detected during:
            instantiation of "void flash::compute_dq_dk_dv_1colblock<Kernel_traits,Is_dropout,Is_causal,Is_even_MN,Is_even_K,Is_first,Is_last,Seq_parallel,Params>(const Params &, int, int, int) [with Kernel_traits=Flash_bwd_kernel_traits<512, 32, 32, 4, 2, 2, 2, false, false, cutlass::half_t, Flash_kernel_traits<512, 32, 32, 4, cutlass::half_t>>, Is_dropout=false, Is_causal=true, Is_even_MN=true, Is_even_K=true, Is_first=false, Is_last=false, Seq_parallel=true, Params=Flash_bwd_params]" 
(1591): here
            instantiation of "void flash::compute_dq_dk_dv_seqk_parallel<Kernel_traits,Is_dropout,Is_causal,Is_even_MN,Is_even_K,Params>(const Params &) [with Kernel_traits=Flash_bwd_kernel_traits<512, 32, 32, 4, 2, 2, 2, false, false, cutlass::half_t, Flash_kernel_traits<512, 32, 32, 4, cutlass::half_t>>, Is_dropout=false, Is_causal=true, Is_even_MN=true, Is_even_K=true, Params=Flash_bwd_params]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_bwd_launch_template.h(28): here
            instantiation of "void flash_bwd_dq_dk_dv_loop_seqk_parallel_kernel<Kernel_traits,Is_dropout,Is_causal,Is_even_MN,Is_even_K>(Flash_bwd_params) [with Kernel_traits=Flash_bwd_kernel_traits<512, 32, 32, 4, 2, 2, 2, false, false, cutlass::half_t, Flash_kernel_traits<512, 32, 32, 4, cutlass::half_t>>, Is_dropout=false, Is_causal=true, Is_even_MN=true, Is_even_K=true]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_bwd_launch_template.h(62): here
            instantiation of "void run_flash_bwd_seqk_parallel<Kernel_traits,Is_dropout>(Flash_bwd_params &, cudaStream_t, __nv_bool) [with Kernel_traits=Flash_bwd_kernel_traits<512, 32, 32, 4, 2, 2, 2, false, false, cutlass::half_t, Flash_kernel_traits<512, 32, 32, 4, cutlass::half_t>>, Is_dropout=false]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_bwd_launch_template.h(134): here
            instantiation of "void run_flash_bwd<Kernel_traits,Is_dropout>(Flash_bwd_params &, cudaStream_t, __nv_bool) [with Kernel_traits=Flash_bwd_kernel_traits<512, 32, 32, 4, 2, 2, 2, false, false, cutlass::half_t, Flash_kernel_traits<512, 32, 32, 4, cutlass::half_t>>, Is_dropout=false]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_bwd_launch_template.h(365): here
            instantiation of "void run_mha_bwd_hdim512<T>(Flash_bwd_params &, cudaStream_t, __nv_bool) [with T=cutlass::half_t]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_bwd_hdim512_fp16_sm80.cu(9): here

/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_bwd_kernel.h(853): error: static assertion failed
          detected during:
            instantiation of "void flash::compute_dq_dk_dv_1colblock<Kernel_traits,Is_dropout,Is_causal,Is_even_MN,Is_even_K,Is_first,Is_last,Seq_parallel,Params>(const Params &, int, int, int) [with Kernel_traits=Flash_bwd_kernel_traits<512, 32, 32, 4, 2, 2, 2, false, false, cutlass::half_t, Flash_kernel_traits<512, 32, 32, 4, cutlass::half_t>>, Is_dropout=false, Is_causal=true, Is_even_MN=true, Is_even_K=false, Is_first=false, Is_last=false, Seq_parallel=true, Params=Flash_bwd_params]" 
(1591): here
            instantiation of "void flash::compute_dq_dk_dv_seqk_parallel<Kernel_traits,Is_dropout,Is_causal,Is_even_MN,Is_even_K,Params>(const Params &) [with Kernel_traits=Flash_bwd_kernel_traits<512, 32, 32, 4, 2, 2, 2, false, false, cutlass::half_t, Flash_kernel_traits<512, 32, 32, 4, cutlass::half_t>>, Is_dropout=false, Is_causal=true, Is_even_MN=true, Is_even_K=false, Params=Flash_bwd_params]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_bwd_launch_template.h(28): here
            instantiation of "void flash_bwd_dq_dk_dv_loop_seqk_parallel_kernel<Kernel_traits,Is_dropout,Is_causal,Is_even_MN,Is_even_K>(Flash_bwd_params) [with Kernel_traits=Flash_bwd_kernel_traits<512, 32, 32, 4, 2, 2, 2, false, false, cutlass::half_t, Flash_kernel_traits<512, 32, 32, 4, cutlass::half_t>>, Is_dropout=false, Is_causal=true, Is_even_MN=true, Is_even_K=false]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_bwd_launch_template.h(62): here
            instantiation of "void run_flash_bwd_seqk_parallel<Kernel_traits,Is_dropout>(Flash_bwd_params &, cudaStream_t, __nv_bool) [with Kernel_traits=Flash_bwd_kernel_traits<512, 32, 32, 4, 2, 2, 2, false, false, cutlass::half_t, Flash_kernel_traits<512, 32, 32, 4, cutlass::half_t>>, Is_dropout=false]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_bwd_launch_template.h(134): here
            instantiation of "void run_flash_bwd<Kernel_traits,Is_dropout>(Flash_bwd_params &, cudaStream_t, __nv_bool) [with Kernel_traits=Flash_bwd_kernel_traits<512, 32, 32, 4, 2, 2, 2, false, false, cutlass::half_t, Flash_kernel_traits<512, 32, 32, 4, cutlass::half_t>>, Is_dropout=false]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_bwd_launch_template.h(365): here
            instantiation of "void run_mha_bwd_hdim512<T>(Flash_bwd_params &, cudaStream_t, __nv_bool) [with T=cutlass::half_t]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_bwd_hdim512_fp16_sm80.cu(9): here

/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_bwd_kernel.h(853): error: static assertion failed
          detected during:
            instantiation of "void flash::compute_dq_dk_dv_1colblock<Kernel_traits,Is_dropout,Is_causal,Is_even_MN,Is_even_K,Is_first,Is_last,Seq_parallel,Params>(const Params &, int, int, int) [with Kernel_traits=Flash_bwd_kernel_traits<512, 32, 32, 4, 2, 2, 2, false, false, cutlass::half_t, Flash_kernel_traits<512, 32, 32, 4, cutlass::half_t>>, Is_dropout=false, Is_causal=true, Is_even_MN=false, Is_even_K=true, Is_first=false, Is_last=false, Seq_parallel=true, Params=Flash_bwd_params]" 
(1591): here
            instantiation of "void flash::compute_dq_dk_dv_seqk_parallel<Kernel_traits,Is_dropout,Is_causal,Is_even_MN,Is_even_K,Params>(const Params &) [with Kernel_traits=Flash_bwd_kernel_traits<512, 32, 32, 4, 2, 2, 2, false, false, cutlass::half_t, Flash_kernel_traits<512, 32, 32, 4, cutlass::half_t>>, Is_dropout=false, Is_causal=true, Is_even_MN=false, Is_even_K=true, Params=Flash_bwd_params]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_bwd_launch_template.h(28): here
            instantiation of "void flash_bwd_dq_dk_dv_loop_seqk_parallel_kernel<Kernel_traits,Is_dropout,Is_causal,Is_even_MN,Is_even_K>(Flash_bwd_params) [with Kernel_traits=Flash_bwd_kernel_traits<512, 32, 32, 4, 2, 2, 2, false, false, cutlass::half_t, Flash_kernel_traits<512, 32, 32, 4, cutlass::half_t>>, Is_dropout=false, Is_causal=true, Is_even_MN=false, Is_even_K=true]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_bwd_launch_template.h(62): here
            instantiation of "void run_flash_bwd_seqk_parallel<Kernel_traits,Is_dropout>(Flash_bwd_params &, cudaStream_t, __nv_bool) [with Kernel_traits=Flash_bwd_kernel_traits<512, 32, 32, 4, 2, 2, 2, false, false, cutlass::half_t, Flash_kernel_traits<512, 32, 32, 4, cutlass::half_t>>, Is_dropout=false]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_bwd_launch_template.h(134): here
            instantiation of "void run_flash_bwd<Kernel_traits,Is_dropout>(Flash_bwd_params &, cudaStream_t, __nv_bool) [with Kernel_traits=Flash_bwd_kernel_traits<512, 32, 32, 4, 2, 2, 2, false, false, cutlass::half_t, Flash_kernel_traits<512, 32, 32, 4, cutlass::half_t>>, Is_dropout=false]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_bwd_launch_template.h(365): here
            instantiation of "void run_mha_bwd_hdim512<T>(Flash_bwd_params &, cudaStream_t, __nv_bool) [with T=cutlass::half_t]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_bwd_hdim512_fp16_sm80.cu(9): here

/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_bwd_kernel.h(853): error: static assertion failed
          detected during:
            instantiation of "void flash::compute_dq_dk_dv_1colblock<Kernel_traits,Is_dropout,Is_causal,Is_even_MN,Is_even_K,Is_first,Is_last,Seq_parallel,Params>(const Params &, int, int, int) [with Kernel_traits=Flash_bwd_kernel_traits<512, 32, 32, 4, 2, 2, 2, false, false, cutlass::half_t, Flash_kernel_traits<512, 32, 32, 4, cutlass::half_t>>, Is_dropout=false, Is_causal=true, Is_even_MN=false, Is_even_K=false, Is_first=false, Is_last=false, Seq_parallel=true, Params=Flash_bwd_params]" 
(1591): here
            instantiation of "void flash::compute_dq_dk_dv_seqk_parallel<Kernel_traits,Is_dropout,Is_causal,Is_even_MN,Is_even_K,Params>(const Params &) [with Kernel_traits=Flash_bwd_kernel_traits<512, 32, 32, 4, 2, 2, 2, false, false, cutlass::half_t, Flash_kernel_traits<512, 32, 32, 4, cutlass::half_t>>, Is_dropout=false, Is_causal=true, Is_even_MN=false, Is_even_K=false, Params=Flash_bwd_params]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_bwd_launch_template.h(28): here
            instantiation of "void flash_bwd_dq_dk_dv_loop_seqk_parallel_kernel<Kernel_traits,Is_dropout,Is_causal,Is_even_MN,Is_even_K>(Flash_bwd_params) [with Kernel_traits=Flash_bwd_kernel_traits<512, 32, 32, 4, 2, 2, 2, false, false, cutlass::half_t, Flash_kernel_traits<512, 32, 32, 4, cutlass::half_t>>, Is_dropout=false, Is_causal=true, Is_even_MN=false, Is_even_K=false]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_bwd_launch_template.h(62): here
            instantiation of "void run_flash_bwd_seqk_parallel<Kernel_traits,Is_dropout>(Flash_bwd_params &, cudaStream_t, __nv_bool) [with Kernel_traits=Flash_bwd_kernel_traits<512, 32, 32, 4, 2, 2, 2, false, false, cutlass::half_t, Flash_kernel_traits<512, 32, 32, 4, cutlass::half_t>>, Is_dropout=false]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_bwd_launch_template.h(134): here
            instantiation of "void run_flash_bwd<Kernel_traits,Is_dropout>(Flash_bwd_params &, cudaStream_t, __nv_bool) [with Kernel_traits=Flash_bwd_kernel_traits<512, 32, 32, 4, 2, 2, 2, false, false, cutlass::half_t, Flash_kernel_traits<512, 32, 32, 4, cutlass::half_t>>, Is_dropout=false]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_bwd_launch_template.h(365): here
            instantiation of "void run_mha_bwd_hdim512<T>(Flash_bwd_params &, cudaStream_t, __nv_bool) [with T=cutlass::half_t]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_bwd_hdim512_fp16_sm80.cu(9): here

/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_bwd_kernel.h(853): error: static assertion failed
          detected during:
            instantiation of "void flash::compute_dq_dk_dv_1colblock<Kernel_traits,Is_dropout,Is_causal,Is_even_MN,Is_even_K,Is_first,Is_last,Seq_parallel,Params>(const Params &, int, int, int) [with Kernel_traits=Flash_bwd_kernel_traits<512, 32, 32, 4, 2, 2, 2, false, false, cutlass::half_t, Flash_kernel_traits<512, 32, 32, 4, cutlass::half_t>>, Is_dropout=false, Is_causal=false, Is_even_MN=true, Is_even_K=true, Is_first=false, Is_last=false, Seq_parallel=true, Params=Flash_bwd_params]" 
(1591): here
            instantiation of "void flash::compute_dq_dk_dv_seqk_parallel<Kernel_traits,Is_dropout,Is_causal,Is_even_MN,Is_even_K,Params>(const Params &) [with Kernel_traits=Flash_bwd_kernel_traits<512, 32, 32, 4, 2, 2, 2, false, false, cutlass::half_t, Flash_kernel_traits<512, 32, 32, 4, cutlass::half_t>>, Is_dropout=false, Is_causal=false, Is_even_MN=true, Is_even_K=true, Params=Flash_bwd_params]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_bwd_launch_template.h(28): here
            instantiation of "void flash_bwd_dq_dk_dv_loop_seqk_parallel_kernel<Kernel_traits,Is_dropout,Is_causal,Is_even_MN,Is_even_K>(Flash_bwd_params) [with Kernel_traits=Flash_bwd_kernel_traits<512, 32, 32, 4, 2, 2, 2, false, false, cutlass::half_t, Flash_kernel_traits<512, 32, 32, 4, cutlass::half_t>>, Is_dropout=false, Is_causal=false, Is_even_MN=true, Is_even_K=true]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_bwd_launch_template.h(62): here
            instantiation of "void run_flash_bwd_seqk_parallel<Kernel_traits,Is_dropout>(Flash_bwd_params &, cudaStream_t, __nv_bool) [with Kernel_traits=Flash_bwd_kernel_traits<512, 32, 32, 4, 2, 2, 2, false, false, cutlass::half_t, Flash_kernel_traits<512, 32, 32, 4, cutlass::half_t>>, Is_dropout=false]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_bwd_launch_template.h(134): here
            instantiation of "void run_flash_bwd<Kernel_traits,Is_dropout>(Flash_bwd_params &, cudaStream_t, __nv_bool) [with Kernel_traits=Flash_bwd_kernel_traits<512, 32, 32, 4, 2, 2, 2, false, false, cutlass::half_t, Flash_kernel_traits<512, 32, 32, 4, cutlass::half_t>>, Is_dropout=false]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_bwd_launch_template.h(365): here
            instantiation of "void run_mha_bwd_hdim512<T>(Flash_bwd_params &, cudaStream_t, __nv_bool) [with T=cutlass::half_t]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_bwd_hdim512_fp16_sm80.cu(9): here

/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_bwd_kernel.h(853): error: static assertion failed
          detected during:
            instantiation of "void flash::compute_dq_dk_dv_1colblock<Kernel_traits,Is_dropout,Is_causal,Is_even_MN,Is_even_K,Is_first,Is_last,Seq_parallel,Params>(const Params &, int, int, int) [with Kernel_traits=Flash_bwd_kernel_traits<512, 32, 32, 4, 2, 2, 2, false, false, cutlass::half_t, Flash_kernel_traits<512, 32, 32, 4, cutlass::half_t>>, Is_dropout=false, Is_causal=false, Is_even_MN=true, Is_even_K=false, Is_first=false, Is_last=false, Seq_parallel=true, Params=Flash_bwd_params]" 
(1591): here
            instantiation of "void flash::compute_dq_dk_dv_seqk_parallel<Kernel_traits,Is_dropout,Is_causal,Is_even_MN,Is_even_K,Params>(const Params &) [with Kernel_traits=Flash_bwd_kernel_traits<512, 32, 32, 4, 2, 2, 2, false, false, cutlass::half_t, Flash_kernel_traits<512, 32, 32, 4, cutlass::half_t>>, Is_dropout=false, Is_causal=false, Is_even_MN=true, Is_even_K=false, Params=Flash_bwd_params]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_bwd_launch_template.h(28): here
            instantiation of "void flash_bwd_dq_dk_dv_loop_seqk_parallel_kernel<Kernel_traits,Is_dropout,Is_causal,Is_even_MN,Is_even_K>(Flash_bwd_params) [with Kernel_traits=Flash_bwd_kernel_traits<512, 32, 32, 4, 2, 2, 2, false, false, cutlass::half_t, Flash_kernel_traits<512, 32, 32, 4, cutlass::half_t>>, Is_dropout=false, Is_causal=false, Is_even_MN=true, Is_even_K=false]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_bwd_launch_template.h(62): here
            instantiation of "void run_flash_bwd_seqk_parallel<Kernel_traits,Is_dropout>(Flash_bwd_params &, cudaStream_t, __nv_bool) [with Kernel_traits=Flash_bwd_kernel_traits<512, 32, 32, 4, 2, 2, 2, false, false, cutlass::half_t, Flash_kernel_traits<512, 32, 32, 4, cutlass::half_t>>, Is_dropout=false]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_bwd_launch_template.h(134): here
            instantiation of "void run_flash_bwd<Kernel_traits,Is_dropout>(Flash_bwd_params &, cudaStream_t, __nv_bool) [with Kernel_traits=Flash_bwd_kernel_traits<512, 32, 32, 4, 2, 2, 2, false, false, cutlass::half_t, Flash_kernel_traits<512, 32, 32, 4, cutlass::half_t>>, Is_dropout=false]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_bwd_launch_template.h(365): here
            instantiation of "void run_mha_bwd_hdim512<T>(Flash_bwd_params &, cudaStream_t, __nv_bool) [with T=cutlass::half_t]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_bwd_hdim512_fp16_sm80.cu(9): here

/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_bwd_kernel.h(853): error: static assertion failed
          detected during:
            instantiation of "void flash::compute_dq_dk_dv_1colblock<Kernel_traits,Is_dropout,Is_causal,Is_even_MN,Is_even_K,Is_first,Is_last,Seq_parallel,Params>(const Params &, int, int, int) [with Kernel_traits=Flash_bwd_kernel_traits<512, 32, 32, 4, 2, 2, 2, false, false, cutlass::half_t, Flash_kernel_traits<512, 32, 32, 4, cutlass::half_t>>, Is_dropout=false, Is_causal=false, Is_even_MN=false, Is_even_K=true, Is_first=false, Is_last=false, Seq_parallel=true, Params=Flash_bwd_params]" 
(1591): here
            instantiation of "void flash::compute_dq_dk_dv_seqk_parallel<Kernel_traits,Is_dropout,Is_causal,Is_even_MN,Is_even_K,Params>(const Params &) [with Kernel_traits=Flash_bwd_kernel_traits<512, 32, 32, 4, 2, 2, 2, false, false, cutlass::half_t, Flash_kernel_traits<512, 32, 32, 4, cutlass::half_t>>, Is_dropout=false, Is_causal=false, Is_even_MN=false, Is_even_K=true, Params=Flash_bwd_params]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_bwd_launch_template.h(28): here
            instantiation of "void flash_bwd_dq_dk_dv_loop_seqk_parallel_kernel<Kernel_traits,Is_dropout,Is_causal,Is_even_MN,Is_even_K>(Flash_bwd_params) [with Kernel_traits=Flash_bwd_kernel_traits<512, 32, 32, 4, 2, 2, 2, false, false, cutlass::half_t, Flash_kernel_traits<512, 32, 32, 4, cutlass::half_t>>, Is_dropout=false, Is_causal=false, Is_even_MN=false, Is_even_K=true]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_bwd_launch_template.h(62): here
            instantiation of "void run_flash_bwd_seqk_parallel<Kernel_traits,Is_dropout>(Flash_bwd_params &, cudaStream_t, __nv_bool) [with Kernel_traits=Flash_bwd_kernel_traits<512, 32, 32, 4, 2, 2, 2, false, false, cutlass::half_t, Flash_kernel_traits<512, 32, 32, 4, cutlass::half_t>>, Is_dropout=false]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_bwd_launch_template.h(134): here
            instantiation of "void run_flash_bwd<Kernel_traits,Is_dropout>(Flash_bwd_params &, cudaStream_t, __nv_bool) [with Kernel_traits=Flash_bwd_kernel_traits<512, 32, 32, 4, 2, 2, 2, false, false, cutlass::half_t, Flash_kernel_traits<512, 32, 32, 4, cutlass::half_t>>, Is_dropout=false]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_bwd_launch_template.h(365): here
            instantiation of "void run_mha_bwd_hdim512<T>(Flash_bwd_params &, cudaStream_t, __nv_bool) [with T=cutlass::half_t]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_bwd_hdim512_fp16_sm80.cu(9): here

/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_bwd_kernel.h(853): error: static assertion failed
          detected during:
            instantiation of "void flash::compute_dq_dk_dv_1colblock<Kernel_traits,Is_dropout,Is_causal,Is_even_MN,Is_even_K,Is_first,Is_last,Seq_parallel,Params>(const Params &, int, int, int) [with Kernel_traits=Flash_bwd_kernel_traits<512, 32, 32, 4, 2, 2, 2, false, false, cutlass::half_t, Flash_kernel_traits<512, 32, 32, 4, cutlass::half_t>>, Is_dropout=false, Is_causal=false, Is_even_MN=false, Is_even_K=false, Is_first=false, Is_last=false, Seq_parallel=true, Params=Flash_bwd_params]" 
(1591): here
            instantiation of "void flash::compute_dq_dk_dv_seqk_parallel<Kernel_traits,Is_dropout,Is_causal,Is_even_MN,Is_even_K,Params>(const Params &) [with Kernel_traits=Flash_bwd_kernel_traits<512, 32, 32, 4, 2, 2, 2, false, false, cutlass::half_t, Flash_kernel_traits<512, 32, 32, 4, cutlass::half_t>>, Is_dropout=false, Is_causal=false, Is_even_MN=false, Is_even_K=false, Params=Flash_bwd_params]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_bwd_launch_template.h(28): here
            instantiation of "void flash_bwd_dq_dk_dv_loop_seqk_parallel_kernel<Kernel_traits,Is_dropout,Is_causal,Is_even_MN,Is_even_K>(Flash_bwd_params) [with Kernel_traits=Flash_bwd_kernel_traits<512, 32, 32, 4, 2, 2, 2, false, false, cutlass::half_t, Flash_kernel_traits<512, 32, 32, 4, cutlass::half_t>>, Is_dropout=false, Is_causal=false, Is_even_MN=false, Is_even_K=false]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_bwd_launch_template.h(62): here
            instantiation of "void run_flash_bwd_seqk_parallel<Kernel_traits,Is_dropout>(Flash_bwd_params &, cudaStream_t, __nv_bool) [with Kernel_traits=Flash_bwd_kernel_traits<512, 32, 32, 4, 2, 2, 2, false, false, cutlass::half_t, Flash_kernel_traits<512, 32, 32, 4, cutlass::half_t>>, Is_dropout=false]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_bwd_launch_template.h(134): here
            instantiation of "void run_flash_bwd<Kernel_traits,Is_dropout>(Flash_bwd_params &, cudaStream_t, __nv_bool) [with Kernel_traits=Flash_bwd_kernel_traits<512, 32, 32, 4, 2, 2, 2, false, false, cutlass::half_t, Flash_kernel_traits<512, 32, 32, 4, cutlass::half_t>>, Is_dropout=false]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_bwd_launch_template.h(365): here
            instantiation of "void run_mha_bwd_hdim512<T>(Flash_bwd_params &, cudaStream_t, __nv_bool) [with T=cutlass::half_t]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_bwd_hdim512_fp16_sm80.cu(9): here

/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_bwd_kernel.h(853): error: static assertion failed
          detected during:
            instantiation of "void flash::compute_dq_dk_dv_1colblock<Kernel_traits,Is_dropout,Is_causal,Is_even_MN,Is_even_K,Is_first,Is_last,Seq_parallel,Params>(const Params &, int, int, int) [with Kernel_traits=Flash_bwd_kernel_traits<512, 32, 32, 4, 2, 2, 2, false, true, cutlass::half_t, Flash_kernel_traits<512, 32, 32, 4, cutlass::half_t>>, Is_dropout=false, Is_causal=true, Is_even_MN=true, Is_even_K=true, Is_first=false, Is_last=false, Seq_parallel=true, Params=Flash_bwd_params]" 
(1591): here
            instantiation of "void flash::compute_dq_dk_dv_seqk_parallel<Kernel_traits,Is_dropout,Is_causal,Is_even_MN,Is_even_K,Params>(const Params &) [with Kernel_traits=Flash_bwd_kernel_traits<512, 32, 32, 4, 2, 2, 2, false, true, cutlass::half_t, Flash_kernel_traits<512, 32, 32, 4, cutlass::half_t>>, Is_dropout=false, Is_causal=true, Is_even_MN=true, Is_even_K=true, Params=Flash_bwd_params]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_bwd_launch_template.h(28): here
            instantiation of "void flash_bwd_dq_dk_dv_loop_seqk_parallel_kernel<Kernel_traits,Is_dropout,Is_causal,Is_even_MN,Is_even_K>(Flash_bwd_params) [with Kernel_traits=Flash_bwd_kernel_traits<512, 32, 32, 4, 2, 2, 2, false, true, cutlass::half_t, Flash_kernel_traits<512, 32, 32, 4, cutlass::half_t>>, Is_dropout=false, Is_causal=true, Is_even_MN=true, Is_even_K=true]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_bwd_launch_template.h(62): here
            instantiation of "void run_flash_bwd_seqk_parallel<Kernel_traits,Is_dropout>(Flash_bwd_params &, cudaStream_t, __nv_bool) [with Kernel_traits=Flash_bwd_kernel_traits<512, 32, 32, 4, 2, 2, 2, false, true, cutlass::half_t, Flash_kernel_traits<512, 32, 32, 4, cutlass::half_t>>, Is_dropout=false]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_bwd_launch_template.h(134): here
            instantiation of "void run_flash_bwd<Kernel_traits,Is_dropout>(Flash_bwd_params &, cudaStream_t, __nv_bool) [with Kernel_traits=Flash_bwd_kernel_traits<512, 32, 32, 4, 2, 2, 2, false, true, cutlass::half_t, Flash_kernel_traits<512, 32, 32, 4, cutlass::half_t>>, Is_dropout=false]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_bwd_launch_template.h(365): here
            instantiation of "void run_mha_bwd_hdim512<T>(Flash_bwd_params &, cudaStream_t, __nv_bool) [with T=cutlass::half_t]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_bwd_hdim512_fp16_sm80.cu(9): here

/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_bwd_kernel.h(853): error: static assertion failed
          detected during:
            instantiation of "void flash::compute_dq_dk_dv_1colblock<Kernel_traits,Is_dropout,Is_causal,Is_even_MN,Is_even_K,Is_first,Is_last,Seq_parallel,Params>(const Params &, int, int, int) [with Kernel_traits=Flash_bwd_kernel_traits<512, 32, 32, 4, 2, 2, 2, false, true, cutlass::half_t, Flash_kernel_traits<512, 32, 32, 4, cutlass::half_t>>, Is_dropout=false, Is_causal=true, Is_even_MN=true, Is_even_K=false, Is_first=false, Is_last=false, Seq_parallel=true, Params=Flash_bwd_params]" 
(1591): here
            instantiation of "void flash::compute_dq_dk_dv_seqk_parallel<Kernel_traits,Is_dropout,Is_causal,Is_even_MN,Is_even_K,Params>(const Params &) [with Kernel_traits=Flash_bwd_kernel_traits<512, 32, 32, 4, 2, 2, 2, false, true, cutlass::half_t, Flash_kernel_traits<512, 32, 32, 4, cutlass::half_t>>, Is_dropout=false, Is_causal=true, Is_even_MN=true, Is_even_K=false, Params=Flash_bwd_params]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_bwd_launch_template.h(28): here
            instantiation of "void flash_bwd_dq_dk_dv_loop_seqk_parallel_kernel<Kernel_traits,Is_dropout,Is_causal,Is_even_MN,Is_even_K>(Flash_bwd_params) [with Kernel_traits=Flash_bwd_kernel_traits<512, 32, 32, 4, 2, 2, 2, false, true, cutlass::half_t, Flash_kernel_traits<512, 32, 32, 4, cutlass::half_t>>, Is_dropout=false, Is_causal=true, Is_even_MN=true, Is_even_K=false]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_bwd_launch_template.h(62): here
            instantiation of "void run_flash_bwd_seqk_parallel<Kernel_traits,Is_dropout>(Flash_bwd_params &, cudaStream_t, __nv_bool) [with Kernel_traits=Flash_bwd_kernel_traits<512, 32, 32, 4, 2, 2, 2, false, true, cutlass::half_t, Flash_kernel_traits<512, 32, 32, 4, cutlass::half_t>>, Is_dropout=false]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_bwd_launch_template.h(134): here
            instantiation of "void run_flash_bwd<Kernel_traits,Is_dropout>(Flash_bwd_params &, cudaStream_t, __nv_bool) [with Kernel_traits=Flash_bwd_kernel_traits<512, 32, 32, 4, 2, 2, 2, false, true, cutlass::half_t, Flash_kernel_traits<512, 32, 32, 4, cutlass::half_t>>, Is_dropout=false]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_bwd_launch_template.h(365): here
            instantiation of "void run_mha_bwd_hdim512<T>(Flash_bwd_params &, cudaStream_t, __nv_bool) [with T=cutlass::half_t]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_bwd_hdim512_fp16_sm80.cu(9): here

/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_bwd_kernel.h(853): error: static assertion failed
          detected during:
            instantiation of "void flash::compute_dq_dk_dv_1colblock<Kernel_traits,Is_dropout,Is_causal,Is_even_MN,Is_even_K,Is_first,Is_last,Seq_parallel,Params>(const Params &, int, int, int) [with Kernel_traits=Flash_bwd_kernel_traits<512, 32, 32, 4, 2, 2, 2, false, true, cutlass::half_t, Flash_kernel_traits<512, 32, 32, 4, cutlass::half_t>>, Is_dropout=false, Is_causal=true, Is_even_MN=false, Is_even_K=true, Is_first=false, Is_last=false, Seq_parallel=true, Params=Flash_bwd_params]" 
(1591): here
            instantiation of "void flash::compute_dq_dk_dv_seqk_parallel<Kernel_traits,Is_dropout,Is_causal,Is_even_MN,Is_even_K,Params>(const Params &) [with Kernel_traits=Flash_bwd_kernel_traits<512, 32, 32, 4, 2, 2, 2, false, true, cutlass::half_t, Flash_kernel_traits<512, 32, 32, 4, cutlass::half_t>>, Is_dropout=false, Is_causal=true, Is_even_MN=false, Is_even_K=true, Params=Flash_bwd_params]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_bwd_launch_template.h(28): here
            instantiation of "void flash_bwd_dq_dk_dv_loop_seqk_parallel_kernel<Kernel_traits,Is_dropout,Is_causal,Is_even_MN,Is_even_K>(Flash_bwd_params) [with Kernel_traits=Flash_bwd_kernel_traits<512, 32, 32, 4, 2, 2, 2, false, true, cutlass::half_t, Flash_kernel_traits<512, 32, 32, 4, cutlass::half_t>>, Is_dropout=false, Is_causal=true, Is_even_MN=false, Is_even_K=true]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_bwd_launch_template.h(62): here
            instantiation of "void run_flash_bwd_seqk_parallel<Kernel_traits,Is_dropout>(Flash_bwd_params &, cudaStream_t, __nv_bool) [with Kernel_traits=Flash_bwd_kernel_traits<512, 32, 32, 4, 2, 2, 2, false, true, cutlass::half_t, Flash_kernel_traits<512, 32, 32, 4, cutlass::half_t>>, Is_dropout=false]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_bwd_launch_template.h(134): here
            instantiation of "void run_flash_bwd<Kernel_traits,Is_dropout>(Flash_bwd_params &, cudaStream_t, __nv_bool) [with Kernel_traits=Flash_bwd_kernel_traits<512, 32, 32, 4, 2, 2, 2, false, true, cutlass::half_t, Flash_kernel_traits<512, 32, 32, 4, cutlass::half_t>>, Is_dropout=false]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_bwd_launch_template.h(365): here
            instantiation of "void run_mha_bwd_hdim512<T>(Flash_bwd_params &, cudaStream_t, __nv_bool) [with T=cutlass::half_t]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_bwd_hdim512_fp16_sm80.cu(9): here

/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_bwd_kernel.h(853): error: static assertion failed
          detected during:
            instantiation of "void flash::compute_dq_dk_dv_1colblock<Kernel_traits,Is_dropout,Is_causal,Is_even_MN,Is_even_K,Is_first,Is_last,Seq_parallel,Params>(const Params &, int, int, int) [with Kernel_traits=Flash_bwd_kernel_traits<512, 32, 32, 4, 2, 2, 2, false, true, cutlass::half_t, Flash_kernel_traits<512, 32, 32, 4, cutlass::half_t>>, Is_dropout=false, Is_causal=true, Is_even_MN=false, Is_even_K=false, Is_first=false, Is_last=false, Seq_parallel=true, Params=Flash_bwd_params]" 
(1591): here
            instantiation of "void flash::compute_dq_dk_dv_seqk_parallel<Kernel_traits,Is_dropout,Is_causal,Is_even_MN,Is_even_K,Params>(const Params &) [with Kernel_traits=Flash_bwd_kernel_traits<512, 32, 32, 4, 2, 2, 2, false, true, cutlass::half_t, Flash_kernel_traits<512, 32, 32, 4, cutlass::half_t>>, Is_dropout=false, Is_causal=true, Is_even_MN=false, Is_even_K=false, Params=Flash_bwd_params]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_bwd_launch_template.h(28): here
            instantiation of "void flash_bwd_dq_dk_dv_loop_seqk_parallel_kernel<Kernel_traits,Is_dropout,Is_causal,Is_even_MN,Is_even_K>(Flash_bwd_params) [with Kernel_traits=Flash_bwd_kernel_traits<512, 32, 32, 4, 2, 2, 2, false, true, cutlass::half_t, Flash_kernel_traits<512, 32, 32, 4, cutlass::half_t>>, Is_dropout=false, Is_causal=true, Is_even_MN=false, Is_even_K=false]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_bwd_launch_template.h(62): here
            instantiation of "void run_flash_bwd_seqk_parallel<Kernel_traits,Is_dropout>(Flash_bwd_params &, cudaStream_t, __nv_bool) [with Kernel_traits=Flash_bwd_kernel_traits<512, 32, 32, 4, 2, 2, 2, false, true, cutlass::half_t, Flash_kernel_traits<512, 32, 32, 4, cutlass::half_t>>, Is_dropout=false]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_bwd_launch_template.h(134): here
            instantiation of "void run_flash_bwd<Kernel_traits,Is_dropout>(Flash_bwd_params &, cudaStream_t, __nv_bool) [with Kernel_traits=Flash_bwd_kernel_traits<512, 32, 32, 4, 2, 2, 2, false, true, cutlass::half_t, Flash_kernel_traits<512, 32, 32, 4, cutlass::half_t>>, Is_dropout=false]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_bwd_launch_template.h(365): here
            instantiation of "void run_mha_bwd_hdim512<T>(Flash_bwd_params &, cudaStream_t, __nv_bool) [with T=cutlass::half_t]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_bwd_hdim512_fp16_sm80.cu(9): here

/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_bwd_kernel.h(853): error: static assertion failed
          detected during:
            instantiation of "void flash::compute_dq_dk_dv_1colblock<Kernel_traits,Is_dropout,Is_causal,Is_even_MN,Is_even_K,Is_first,Is_last,Seq_parallel,Params>(const Params &, int, int, int) [with Kernel_traits=Flash_bwd_kernel_traits<512, 32, 32, 4, 2, 2, 2, false, true, cutlass::half_t, Flash_kernel_traits<512, 32, 32, 4, cutlass::half_t>>, Is_dropout=false, Is_causal=false, Is_even_MN=true, Is_even_K=true, Is_first=false, Is_last=false, Seq_parallel=true, Params=Flash_bwd_params]" 
(1591): here
            instantiation of "void flash::compute_dq_dk_dv_seqk_parallel<Kernel_traits,Is_dropout,Is_causal,Is_even_MN,Is_even_K,Params>(const Params &) [with Kernel_traits=Flash_bwd_kernel_traits<512, 32, 32, 4, 2, 2, 2, false, true, cutlass::half_t, Flash_kernel_traits<512, 32, 32, 4, cutlass::half_t>>, Is_dropout=false, Is_causal=false, Is_even_MN=true, Is_even_K=true, Params=Flash_bwd_params]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_bwd_launch_template.h(28): here
            instantiation of "void flash_bwd_dq_dk_dv_loop_seqk_parallel_kernel<Kernel_traits,Is_dropout,Is_causal,Is_even_MN,Is_even_K>(Flash_bwd_params) [with Kernel_traits=Flash_bwd_kernel_traits<512, 32, 32, 4, 2, 2, 2, false, true, cutlass::half_t, Flash_kernel_traits<512, 32, 32, 4, cutlass::half_t>>, Is_dropout=false, Is_causal=false, Is_even_MN=true, Is_even_K=true]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_bwd_launch_template.h(62): here
            instantiation of "void run_flash_bwd_seqk_parallel<Kernel_traits,Is_dropout>(Flash_bwd_params &, cudaStream_t, __nv_bool) [with Kernel_traits=Flash_bwd_kernel_traits<512, 32, 32, 4, 2, 2, 2, false, true, cutlass::half_t, Flash_kernel_traits<512, 32, 32, 4, cutlass::half_t>>, Is_dropout=false]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_bwd_launch_template.h(134): here
            instantiation of "void run_flash_bwd<Kernel_traits,Is_dropout>(Flash_bwd_params &, cudaStream_t, __nv_bool) [with Kernel_traits=Flash_bwd_kernel_traits<512, 32, 32, 4, 2, 2, 2, false, true, cutlass::half_t, Flash_kernel_traits<512, 32, 32, 4, cutlass::half_t>>, Is_dropout=false]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_bwd_launch_template.h(365): here
            instantiation of "void run_mha_bwd_hdim512<T>(Flash_bwd_params &, cudaStream_t, __nv_bool) [with T=cutlass::half_t]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_bwd_hdim512_fp16_sm80.cu(9): here

/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_bwd_kernel.h(853): error: static assertion failed
          detected during:
            instantiation of "void flash::compute_dq_dk_dv_1colblock<Kernel_traits,Is_dropout,Is_causal,Is_even_MN,Is_even_K,Is_first,Is_last,Seq_parallel,Params>(const Params &, int, int, int) [with Kernel_traits=Flash_bwd_kernel_traits<512, 32, 32, 4, 2, 2, 2, false, true, cutlass::half_t, Flash_kernel_traits<512, 32, 32, 4, cutlass::half_t>>, Is_dropout=false, Is_causal=false, Is_even_MN=true, Is_even_K=false, Is_first=false, Is_last=false, Seq_parallel=true, Params=Flash_bwd_params]" 
(1591): here
            instantiation of "void flash::compute_dq_dk_dv_seqk_parallel<Kernel_traits,Is_dropout,Is_causal,Is_even_MN,Is_even_K,Params>(const Params &) [with Kernel_traits=Flash_bwd_kernel_traits<512, 32, 32, 4, 2, 2, 2, false, true, cutlass::half_t, Flash_kernel_traits<512, 32, 32, 4, cutlass::half_t>>, Is_dropout=false, Is_causal=false, Is_even_MN=true, Is_even_K=false, Params=Flash_bwd_params]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_bwd_launch_template.h(28): here
            instantiation of "void flash_bwd_dq_dk_dv_loop_seqk_parallel_kernel<Kernel_traits,Is_dropout,Is_causal,Is_even_MN,Is_even_K>(Flash_bwd_params) [with Kernel_traits=Flash_bwd_kernel_traits<512, 32, 32, 4, 2, 2, 2, false, true, cutlass::half_t, Flash_kernel_traits<512, 32, 32, 4, cutlass::half_t>>, Is_dropout=false, Is_causal=false, Is_even_MN=true, Is_even_K=false]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_bwd_launch_template.h(62): here
            instantiation of "void run_flash_bwd_seqk_parallel<Kernel_traits,Is_dropout>(Flash_bwd_params &, cudaStream_t, __nv_bool) [with Kernel_traits=Flash_bwd_kernel_traits<512, 32, 32, 4, 2, 2, 2, false, true, cutlass::half_t, Flash_kernel_traits<512, 32, 32, 4, cutlass::half_t>>, Is_dropout=false]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_bwd_launch_template.h(134): here
            instantiation of "void run_flash_bwd<Kernel_traits,Is_dropout>(Flash_bwd_params &, cudaStream_t, __nv_bool) [with Kernel_traits=Flash_bwd_kernel_traits<512, 32, 32, 4, 2, 2, 2, false, true, cutlass::half_t, Flash_kernel_traits<512, 32, 32, 4, cutlass::half_t>>, Is_dropout=false]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_bwd_launch_template.h(365): here
            instantiation of "void run_mha_bwd_hdim512<T>(Flash_bwd_params &, cudaStream_t, __nv_bool) [with T=cutlass::half_t]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_bwd_hdim512_fp16_sm80.cu(9): here

/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_bwd_kernel.h(853): error: static assertion failed
          detected during:
            instantiation of "void flash::compute_dq_dk_dv_1colblock<Kernel_traits,Is_dropout,Is_causal,Is_even_MN,Is_even_K,Is_first,Is_last,Seq_parallel,Params>(const Params &, int, int, int) [with Kernel_traits=Flash_bwd_kernel_traits<512, 32, 32, 4, 2, 2, 2, false, true, cutlass::half_t, Flash_kernel_traits<512, 32, 32, 4, cutlass::half_t>>, Is_dropout=false, Is_causal=false, Is_even_MN=false, Is_even_K=true, Is_first=false, Is_last=false, Seq_parallel=true, Params=Flash_bwd_params]" 
(1591): here
            instantiation of "void flash::compute_dq_dk_dv_seqk_parallel<Kernel_traits,Is_dropout,Is_causal,Is_even_MN,Is_even_K,Params>(const Params &) [with Kernel_traits=Flash_bwd_kernel_traits<512, 32, 32, 4, 2, 2, 2, false, true, cutlass::half_t, Flash_kernel_traits<512, 32, 32, 4, cutlass::half_t>>, Is_dropout=false, Is_causal=false, Is_even_MN=false, Is_even_K=true, Params=Flash_bwd_params]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_bwd_launch_template.h(28): here
            instantiation of "void flash_bwd_dq_dk_dv_loop_seqk_parallel_kernel<Kernel_traits,Is_dropout,Is_causal,Is_even_MN,Is_even_K>(Flash_bwd_params) [with Kernel_traits=Flash_bwd_kernel_traits<512, 32, 32, 4, 2, 2, 2, false, true, cutlass::half_t, Flash_kernel_traits<512, 32, 32, 4, cutlass::half_t>>, Is_dropout=false, Is_causal=false, Is_even_MN=false, Is_even_K=true]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_bwd_launch_template.h(62): here
            instantiation of "void run_flash_bwd_seqk_parallel<Kernel_traits,Is_dropout>(Flash_bwd_params &, cudaStream_t, __nv_bool) [with Kernel_traits=Flash_bwd_kernel_traits<512, 32, 32, 4, 2, 2, 2, false, true, cutlass::half_t, Flash_kernel_traits<512, 32, 32, 4, cutlass::half_t>>, Is_dropout=false]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_bwd_launch_template.h(134): here
            instantiation of "void run_flash_bwd<Kernel_traits,Is_dropout>(Flash_bwd_params &, cudaStream_t, __nv_bool) [with Kernel_traits=Flash_bwd_kernel_traits<512, 32, 32, 4, 2, 2, 2, false, true, cutlass::half_t, Flash_kernel_traits<512, 32, 32, 4, cutlass::half_t>>, Is_dropout=false]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_bwd_launch_template.h(365): here
            instantiation of "void run_mha_bwd_hdim512<T>(Flash_bwd_params &, cudaStream_t, __nv_bool) [with T=cutlass::half_t]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_bwd_hdim512_fp16_sm80.cu(9): here

/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_bwd_kernel.h(853): error: static assertion failed
          detected during:
            instantiation of "void flash::compute_dq_dk_dv_1colblock<Kernel_traits,Is_dropout,Is_causal,Is_even_MN,Is_even_K,Is_first,Is_last,Seq_parallel,Params>(const Params &, int, int, int) [with Kernel_traits=Flash_bwd_kernel_traits<512, 32, 32, 4, 2, 2, 2, false, true, cutlass::half_t, Flash_kernel_traits<512, 32, 32, 4, cutlass::half_t>>, Is_dropout=false, Is_causal=false, Is_even_MN=false, Is_even_K=false, Is_first=false, Is_last=false, Seq_parallel=true, Params=Flash_bwd_params]" 
(1591): here
            instantiation of "void flash::compute_dq_dk_dv_seqk_parallel<Kernel_traits,Is_dropout,Is_causal,Is_even_MN,Is_even_K,Params>(const Params &) [with Kernel_traits=Flash_bwd_kernel_traits<512, 32, 32, 4, 2, 2, 2, false, true, cutlass::half_t, Flash_kernel_traits<512, 32, 32, 4, cutlass::half_t>>, Is_dropout=false, Is_causal=false, Is_even_MN=false, Is_even_K=false, Params=Flash_bwd_params]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_bwd_launch_template.h(28): here
            instantiation of "void flash_bwd_dq_dk_dv_loop_seqk_parallel_kernel<Kernel_traits,Is_dropout,Is_causal,Is_even_MN,Is_even_K>(Flash_bwd_params) [with Kernel_traits=Flash_bwd_kernel_traits<512, 32, 32, 4, 2, 2, 2, false, true, cutlass::half_t, Flash_kernel_traits<512, 32, 32, 4, cutlass::half_t>>, Is_dropout=false, Is_causal=false, Is_even_MN=false, Is_even_K=false]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_bwd_launch_template.h(62): here
            instantiation of "void run_flash_bwd_seqk_parallel<Kernel_traits,Is_dropout>(Flash_bwd_params &, cudaStream_t, __nv_bool) [with Kernel_traits=Flash_bwd_kernel_traits<512, 32, 32, 4, 2, 2, 2, false, true, cutlass::half_t, Flash_kernel_traits<512, 32, 32, 4, cutlass::half_t>>, Is_dropout=false]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_bwd_launch_template.h(134): here
            instantiation of "void run_flash_bwd<Kernel_traits,Is_dropout>(Flash_bwd_params &, cudaStream_t, __nv_bool) [with Kernel_traits=Flash_bwd_kernel_traits<512, 32, 32, 4, 2, 2, 2, false, true, cutlass::half_t, Flash_kernel_traits<512, 32, 32, 4, cutlass::half_t>>, Is_dropout=false]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_bwd_launch_template.h(365): here
            instantiation of "void run_mha_bwd_hdim512<T>(Flash_bwd_params &, cudaStream_t, __nv_bool) [with T=cutlass::half_t]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_bwd_hdim512_fp16_sm80.cu(9): here

40 errors detected in the compilation of "/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_bwd_hdim512_fp16_sm80.cu".
[2/36] /usr/local/cuda-11.4/bin/nvcc  -I/home/v-feiychen/flash-attention/csrc/flash_attn -I/home/v-feiychen/flash-attention/csrc/flash_attn/src -I/home/v-feiychen/flash-attention/csrc/cutlass/include -I/anaconda/envs/flashattention/lib/python3.9/site-packages/torch/include -I/anaconda/envs/flashattention/lib/python3.9/site-packages/torch/include/torch/csrc/api/include -I/anaconda/envs/flashattention/lib/python3.9/site-packages/torch/include/TH -I/anaconda/envs/flashattention/lib/python3.9/site-packages/torch/include/THC -I/usr/local/cuda-11.4/include -I/anaconda/envs/flashattention/include/python3.9 -c -c /home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_bwd_hdim512_bf16_sm80.cu -o /home/v-feiychen/flash-attention/build/temp.linux-x86_64-cpython-39/csrc/flash_attn/src/flash_bwd_hdim512_bf16_sm80.o -D__CUDA_NO_HALF_OPERATORS__ -D__CUDA_NO_HALF_CONVERSIONS__ -D__CUDA_NO_BFLOAT16_CONVERSIONS__ -D__CUDA_NO_HALF2_OPERATORS__ --expt-relaxed-constexpr --compiler-options ''"'"'-fPIC'"'"'' -O3 -std=c++17 -U__CUDA_NO_HALF_OPERATORS__ -U__CUDA_NO_HALF_CONVERSIONS__ -U__CUDA_NO_HALF2_OPERATORS__ -U__CUDA_NO_BFLOAT16_CONVERSIONS__ --expt-relaxed-constexpr --expt-extended-lambda --use_fast_math --ptxas-options=-v -lineinfo -gencode arch=compute_80,code=sm_80 --threads 4 -DTORCH_API_INCLUDE_EXTENSION_H '-DPYBIND11_COMPILER_TYPE="_gcc"' '-DPYBIND11_STDLIB="_libstdcpp"' '-DPYBIND11_BUILD_ABI="_cxxabi1011"' -DTORCH_EXTENSION_NAME=flash_attn_2_cuda -D_GLIBCXX_USE_CXX11_ABI=0
FAILED: /home/v-feiychen/flash-attention/build/temp.linux-x86_64-cpython-39/csrc/flash_attn/src/flash_bwd_hdim512_bf16_sm80.o 
/usr/local/cuda-11.4/bin/nvcc  -I/home/v-feiychen/flash-attention/csrc/flash_attn -I/home/v-feiychen/flash-attention/csrc/flash_attn/src -I/home/v-feiychen/flash-attention/csrc/cutlass/include -I/anaconda/envs/flashattention/lib/python3.9/site-packages/torch/include -I/anaconda/envs/flashattention/lib/python3.9/site-packages/torch/include/torch/csrc/api/include -I/anaconda/envs/flashattention/lib/python3.9/site-packages/torch/include/TH -I/anaconda/envs/flashattention/lib/python3.9/site-packages/torch/include/THC -I/usr/local/cuda-11.4/include -I/anaconda/envs/flashattention/include/python3.9 -c -c /home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_bwd_hdim512_bf16_sm80.cu -o /home/v-feiychen/flash-attention/build/temp.linux-x86_64-cpython-39/csrc/flash_attn/src/flash_bwd_hdim512_bf16_sm80.o -D__CUDA_NO_HALF_OPERATORS__ -D__CUDA_NO_HALF_CONVERSIONS__ -D__CUDA_NO_BFLOAT16_CONVERSIONS__ -D__CUDA_NO_HALF2_OPERATORS__ --expt-relaxed-constexpr --compiler-options ''"'"'-fPIC'"'"'' -O3 -std=c++17 -U__CUDA_NO_HALF_OPERATORS__ -U__CUDA_NO_HALF_CONVERSIONS__ -U__CUDA_NO_HALF2_OPERATORS__ -U__CUDA_NO_BFLOAT16_CONVERSIONS__ --expt-relaxed-constexpr --expt-extended-lambda --use_fast_math --ptxas-options=-v -lineinfo -gencode arch=compute_80,code=sm_80 --threads 4 -DTORCH_API_INCLUDE_EXTENSION_H '-DPYBIND11_COMPILER_TYPE="_gcc"' '-DPYBIND11_STDLIB="_libstdcpp"' '-DPYBIND11_BUILD_ABI="_cxxabi1011"' -DTORCH_EXTENSION_NAME=flash_attn_2_cuda -D_GLIBCXX_USE_CXX11_ABI=0
/anaconda/envs/flashattention/lib/python3.9/site-packages/torch/include/c10/core/SymInt.h(84): warning: integer conversion resulted in a change of sign

/home/v-feiychen/flash-attention/csrc/flash_attn/src/kernel_traits.h(246): error: static assertion failed
          detected during:
            instantiation of class "Flash_bwd_kernel_traits<kHeadDim_, kBlockM_, kBlockN_, kNWarps_, AtomLayoutMSdP_, AtomLayoutNdKV, AtomLayoutMdQ, Is_V_in_regs_, No_double_buffer_, elem_type, Base> [with kHeadDim_=512, kBlockM_=32, kBlockN_=32, kNWarps_=4, AtomLayoutMSdP_=2, AtomLayoutNdKV=2, AtomLayoutMdQ=2, Is_V_in_regs_=false, No_double_buffer_=false, elem_type=cutlass::bfloat16_t, Base=Flash_kernel_traits<512, 32, 32, 4, cutlass::bfloat16_t>]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_bwd_launch_template.h(48): here
            instantiation of "void run_flash_bwd_seqk_parallel<Kernel_traits,Is_dropout>(Flash_bwd_params &, cudaStream_t, __nv_bool) [with Kernel_traits=Flash_bwd_kernel_traits<512, 32, 32, 4, 2, 2, 2, false, false, cutlass::bfloat16_t, Flash_kernel_traits<512, 32, 32, 4, cutlass::bfloat16_t>>, Is_dropout=true]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_bwd_launch_template.h(134): here
            instantiation of "void run_flash_bwd<Kernel_traits,Is_dropout>(Flash_bwd_params &, cudaStream_t, __nv_bool) [with Kernel_traits=Flash_bwd_kernel_traits<512, 32, 32, 4, 2, 2, 2, false, false, cutlass::bfloat16_t, Flash_kernel_traits<512, 32, 32, 4, cutlass::bfloat16_t>>, Is_dropout=true]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_bwd_launch_template.h(365): here
            instantiation of "void run_mha_bwd_hdim512<T>(Flash_bwd_params &, cudaStream_t, __nv_bool) [with T=cutlass::bfloat16_t]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_bwd_hdim512_bf16_sm80.cu(9): here

/home/v-feiychen/flash-attention/csrc/cutlass/include/cute/layout.hpp(1395): error: static assertion failed with "Layout shape does not divide the target shape."
          detected during:
            instantiation of "auto cute::tile_to_shape(const cute::Layout<Shape, Stride> &, const TrgShape &, const ModeOrder &) [with Shape=cute::Shape<cute::Int<32>, cute::Int<64>>, Stride=cute::Stride<cute::_64, cute::_1>, TrgShape=cute::Shape<cute::Int<32>, cute::Int<32>>, ModeOrder=cute::GenColMajor]" 
/home/v-feiychen/flash-attention/csrc/cutlass/include/cute/swizzle_layout.hpp(854): here
            instantiation of "auto cute::tile_to_shape(const cute::ComposedLayout<Swizzle, Offset, LayoutA> &, const Shape &, const ModeOrder &) [with Swizzle=cute::Swizzle<3, 3, 3>, Offset=cute::Int<0>, LayoutA=cute::Layout<cute::Shape<cute::Int<32>, cute::Int<64>>, cute::Stride<cute::_64, cute::_1>>, Shape=cute::Shape<cute::Int<32>, cute::Int<32>>, ModeOrder=cute::GenColMajor]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/kernel_traits.h(258): here
            instantiation of class "Flash_bwd_kernel_traits<kHeadDim_, kBlockM_, kBlockN_, kNWarps_, AtomLayoutMSdP_, AtomLayoutNdKV, AtomLayoutMdQ, Is_V_in_regs_, No_double_buffer_, elem_type, Base> [with kHeadDim_=512, kBlockM_=32, kBlockN_=32, kNWarps_=4, AtomLayoutMSdP_=2, AtomLayoutNdKV=2, AtomLayoutMdQ=2, Is_V_in_regs_=false, No_double_buffer_=false, elem_type=cutlass::bfloat16_t, Base=Flash_kernel_traits<512, 32, 32, 4, cutlass::bfloat16_t>]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_bwd_launch_template.h(48): here
            instantiation of "void run_flash_bwd_seqk_parallel<Kernel_traits,Is_dropout>(Flash_bwd_params &, cudaStream_t, __nv_bool) [with Kernel_traits=Flash_bwd_kernel_traits<512, 32, 32, 4, 2, 2, 2, false, false, cutlass::bfloat16_t, Flash_kernel_traits<512, 32, 32, 4, cutlass::bfloat16_t>>, Is_dropout=true]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_bwd_launch_template.h(134): here
            instantiation of "void run_flash_bwd<Kernel_traits,Is_dropout>(Flash_bwd_params &, cudaStream_t, __nv_bool) [with Kernel_traits=Flash_bwd_kernel_traits<512, 32, 32, 4, 2, 2, 2, false, false, cutlass::bfloat16_t, Flash_kernel_traits<512, 32, 32, 4, cutlass::bfloat16_t>>, Is_dropout=true]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_bwd_launch_template.h(365): here
            instantiation of "void run_mha_bwd_hdim512<T>(Flash_bwd_params &, cudaStream_t, __nv_bool) [with T=cutlass::bfloat16_t]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_bwd_hdim512_bf16_sm80.cu(9): here

/home/v-feiychen/flash-attention/csrc/cutlass/include/cute/layout.hpp(1395): error: static assertion failed with "Layout shape does not divide the target shape."
          detected during:
            instantiation of "auto cute::tile_to_shape(const cute::Layout<Shape, Stride> &, const TrgShape &, const ModeOrder &) [with Shape=cute::Shape<cute::Int<64>, cute::Int<32>>, Stride=cute::Stride<cute::_1, cute::Int<64>>, TrgShape=cute::Shape<cute::Int<32>, cute::Int<32>>, ModeOrder=cute::GenColMajor]" 
/home/v-feiychen/flash-attention/csrc/cutlass/include/cute/swizzle_layout.hpp(854): here
            instantiation of "auto cute::tile_to_shape(const cute::ComposedLayout<Swizzle, Offset, LayoutA> &, const Shape &, const ModeOrder &) [with Swizzle=cute::Swizzle<3, 3, 3>, Offset=cute::Int<0>, LayoutA=cute::Layout<cute::Shape<cute::Int<64>, cute::Int<32>>, cute::Stride<cute::_1, cute::Int<64>>>, Shape=cute::Shape<cute::Int<32>, cute::Int<32>>, ModeOrder=cute::GenColMajor]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/kernel_traits.h(265): here
            instantiation of class "Flash_bwd_kernel_traits<kHeadDim_, kBlockM_, kBlockN_, kNWarps_, AtomLayoutMSdP_, AtomLayoutNdKV, AtomLayoutMdQ, Is_V_in_regs_, No_double_buffer_, elem_type, Base> [with kHeadDim_=512, kBlockM_=32, kBlockN_=32, kNWarps_=4, AtomLayoutMSdP_=2, AtomLayoutNdKV=2, AtomLayoutMdQ=2, Is_V_in_regs_=false, No_double_buffer_=false, elem_type=cutlass::bfloat16_t, Base=Flash_kernel_traits<512, 32, 32, 4, cutlass::bfloat16_t>]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_bwd_launch_template.h(48): here
            instantiation of "void run_flash_bwd_seqk_parallel<Kernel_traits,Is_dropout>(Flash_bwd_params &, cudaStream_t, __nv_bool) [with Kernel_traits=Flash_bwd_kernel_traits<512, 32, 32, 4, 2, 2, 2, false, false, cutlass::bfloat16_t, Flash_kernel_traits<512, 32, 32, 4, cutlass::bfloat16_t>>, Is_dropout=true]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_bwd_launch_template.h(134): here
            instantiation of "void run_flash_bwd<Kernel_traits,Is_dropout>(Flash_bwd_params &, cudaStream_t, __nv_bool) [with Kernel_traits=Flash_bwd_kernel_traits<512, 32, 32, 4, 2, 2, 2, false, false, cutlass::bfloat16_t, Flash_kernel_traits<512, 32, 32, 4, cutlass::bfloat16_t>>, Is_dropout=true]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_bwd_launch_template.h(365): here
            instantiation of "void run_mha_bwd_hdim512<T>(Flash_bwd_params &, cudaStream_t, __nv_bool) [with T=cutlass::bfloat16_t]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_bwd_hdim512_bf16_sm80.cu(9): here

/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_bwd_kernel.h(853): error: static assertion failed
          detected during:
            instantiation of "void flash::compute_dq_dk_dv_1colblock<Kernel_traits,Is_dropout,Is_causal,Is_even_MN,Is_even_K,Is_first,Is_last,Seq_parallel,Params>(const Params &, int, int, int) [with Kernel_traits=Flash_bwd_kernel_traits<512, 32, 32, 4, 2, 2, 2, false, false, cutlass::bfloat16_t, Flash_kernel_traits<512, 32, 32, 4, cutlass::bfloat16_t>>, Is_dropout=true, Is_causal=true, Is_even_MN=true, Is_even_K=true, Is_first=false, Is_last=false, Seq_parallel=true, Params=Flash_bwd_params]" 
(1591): here
            instantiation of "void flash::compute_dq_dk_dv_seqk_parallel<Kernel_traits,Is_dropout,Is_causal,Is_even_MN,Is_even_K,Params>(const Params &) [with Kernel_traits=Flash_bwd_kernel_traits<512, 32, 32, 4, 2, 2, 2, false, false, cutlass::bfloat16_t, Flash_kernel_traits<512, 32, 32, 4, cutlass::bfloat16_t>>, Is_dropout=true, Is_causal=true, Is_even_MN=true, Is_even_K=true, Params=Flash_bwd_params]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_bwd_launch_template.h(28): here
            instantiation of "void flash_bwd_dq_dk_dv_loop_seqk_parallel_kernel<Kernel_traits,Is_dropout,Is_causal,Is_even_MN,Is_even_K>(Flash_bwd_params) [with Kernel_traits=Flash_bwd_kernel_traits<512, 32, 32, 4, 2, 2, 2, false, false, cutlass::bfloat16_t, Flash_kernel_traits<512, 32, 32, 4, cutlass::bfloat16_t>>, Is_dropout=true, Is_causal=true, Is_even_MN=true, Is_even_K=true]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_bwd_launch_template.h(62): here
            instantiation of "void run_flash_bwd_seqk_parallel<Kernel_traits,Is_dropout>(Flash_bwd_params &, cudaStream_t, __nv_bool) [with Kernel_traits=Flash_bwd_kernel_traits<512, 32, 32, 4, 2, 2, 2, false, false, cutlass::bfloat16_t, Flash_kernel_traits<512, 32, 32, 4, cutlass::bfloat16_t>>, Is_dropout=true]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_bwd_launch_template.h(134): here
            instantiation of "void run_flash_bwd<Kernel_traits,Is_dropout>(Flash_bwd_params &, cudaStream_t, __nv_bool) [with Kernel_traits=Flash_bwd_kernel_traits<512, 32, 32, 4, 2, 2, 2, false, false, cutlass::bfloat16_t, Flash_kernel_traits<512, 32, 32, 4, cutlass::bfloat16_t>>, Is_dropout=true]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_bwd_launch_template.h(365): here
            instantiation of "void run_mha_bwd_hdim512<T>(Flash_bwd_params &, cudaStream_t, __nv_bool) [with T=cutlass::bfloat16_t]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_bwd_hdim512_bf16_sm80.cu(9): here

/home/v-feiychen/flash-attention/csrc/flash_attn/src/softmax.h(217): error: static assertion failed
          detected during:
            instantiation of "void flash::apply_dropout(cute::Tensor<Engine, Layout> &, uint8_t, unsigned long long, unsigned long long, uint32_t, uint32_t, uint32_t) [with encode_dropout_in_sign_bit=true, Engine=cute::ViewEngine<std::remove_cv_t<std::remove_reference_t<float &>> *>, Layout=cute::Layout<cute::Shape<cute::Shape<cute::_2, cute::_2, cute::_2>, cute::_1, cute::_1>, cute::Stride<cute::tuple<cute::_1, cute::_2, cute::constant<int, 4>>, cute::constant<int, 0>, cute::constant<int, 0>>>]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_bwd_kernel.h(859): here
            instantiation of "void flash::compute_dq_dk_dv_1colblock<Kernel_traits,Is_dropout,Is_causal,Is_even_MN,Is_even_K,Is_first,Is_last,Seq_parallel,Params>(const Params &, int, int, int) [with Kernel_traits=Flash_bwd_kernel_traits<512, 32, 32, 4, 2, 2, 2, false, false, cutlass::bfloat16_t, Flash_kernel_traits<512, 32, 32, 4, cutlass::bfloat16_t>>, Is_dropout=true, Is_causal=true, Is_even_MN=true, Is_even_K=true, Is_first=false, Is_last=false, Seq_parallel=true, Params=Flash_bwd_params]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_bwd_kernel.h(1591): here
            instantiation of "void flash::compute_dq_dk_dv_seqk_parallel<Kernel_traits,Is_dropout,Is_causal,Is_even_MN,Is_even_K,Params>(const Params &) [with Kernel_traits=Flash_bwd_kernel_traits<512, 32, 32, 4, 2, 2, 2, false, false, cutlass::bfloat16_t, Flash_kernel_traits<512, 32, 32, 4, cutlass::bfloat16_t>>, Is_dropout=true, Is_causal=true, Is_even_MN=true, Is_even_K=true, Params=Flash_bwd_params]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_bwd_launch_template.h(28): here
            instantiation of "void flash_bwd_dq_dk_dv_loop_seqk_parallel_kernel<Kernel_traits,Is_dropout,Is_causal,Is_even_MN,Is_even_K>(Flash_bwd_params) [with Kernel_traits=Flash_bwd_kernel_traits<512, 32, 32, 4, 2, 2, 2, false, false, cutlass::bfloat16_t, Flash_kernel_traits<512, 32, 32, 4, cutlass::bfloat16_t>>, Is_dropout=true, Is_causal=true, Is_even_MN=true, Is_even_K=true]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_bwd_launch_template.h(62): here
            instantiation of "void run_flash_bwd_seqk_parallel<Kernel_traits,Is_dropout>(Flash_bwd_params &, cudaStream_t, __nv_bool) [with Kernel_traits=Flash_bwd_kernel_traits<512, 32, 32, 4, 2, 2, 2, false, false, cutlass::bfloat16_t, Flash_kernel_traits<512, 32, 32, 4, cutlass::bfloat16_t>>, Is_dropout=true]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_bwd_launch_template.h(134): here
            instantiation of "void run_flash_bwd<Kernel_traits,Is_dropout>(Flash_bwd_params &, cudaStream_t, __nv_bool) [with Kernel_traits=Flash_bwd_kernel_traits<512, 32, 32, 4, 2, 2, 2, false, false, cutlass::bfloat16_t, Flash_kernel_traits<512, 32, 32, 4, cutlass::bfloat16_t>>, Is_dropout=true]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_bwd_launch_template.h(365): here
            instantiation of "void run_mha_bwd_hdim512<T>(Flash_bwd_params &, cudaStream_t, __nv_bool) [with T=cutlass::bfloat16_t]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_bwd_hdim512_bf16_sm80.cu(9): here

/home/v-feiychen/flash-attention/csrc/flash_attn/src/utils.h(182): error: static assertion failed
          detected during:
            instantiation of "void flash::gemm(Tensor0 &, Tensor1 &, Tensor2 &, const Tensor3 &, const Tensor4 &, TiledMma, TiledCopyA, TiledCopyB, ThrCopyA, ThrCopyB) [with A_in_regs=false, B_in_regs=false, Tensor0=cute::Tensor<cute::ArrayEngine<std::remove_cv_t<std::remove_reference_t<float &>>, 128>, cute::Layout<cute::tuple<cute::tuple<cute::_2, cute::_2>, cute::constant<int, 1>, cute::constant<int, 32>>, cute::tuple<cute::tuple<cute::_1, cute::_2>, cute::constant<int, 0>, cute::constant<int, 4>>>>, Tensor1=cute::Tensor<cute::ArrayEngine<std::remove_cv_t<std::remove_reference_t<cutlass::bfloat16_t &>>, 32>, cute::Layout<cute::tuple<cute::Shape<cute::_2, cute::_2, cute::_2>, cute::Int<2>, cute::_2>, cute::tuple<cute::tuple<cute::_1, cute::_2, cute::constant<int, 4>>, cute::_8, cute::_16>>>, Tensor2=cute::Tensor<cute::ArrayEngine<std::remove_cv_t<std::remove_reference_t<cutlass::bfloat16_t &>>, 256>, cute::Layout<cute::tuple<cute::tuple<cute::_2, cute::_2>, cute::tuple<cute::_4, cute::_8>, cute::_2>, cute::tuple<cute::tuple<cute::_1, cute::_2>, cute::Shape<cute::Int<4>, cute::_32>, cute::_16>>>, Tensor3=cute::Tensor<cute::ViewEngine<cute::smem_ptr<std::remove_cv_t<std::remove_reference_t<cutlass::bfloat16_t &>>>>, std::__tuple_element_t<(std::size_t)0UL, std::tuple<cute::Layout<cute::Shape<cute::tuple<cute::_8, cute::_1>, cute::Int<2>, cute::Int<2>>, cute::Stride<cute::tuple<cute::_1, cute::LayoutLeft::Apply<cute::_1>>, int, cute::_1024>>, unsigned int>>>, Tensor4=cute::Tensor<cute::ViewEngine<cute::smem_ptr<std::remove_cv_t<std::remove_reference_t<cutlass::bfloat16_t &>>>>, std::__tuple_element_t<(std::size_t)0UL, std::tuple<cute::Layout<cute::Shape<cute::tuple<cute::_8, cute::_1>, cute::tuple<cute::constant<int, 2>, cute::constant<int, 8>>, cute::Int<2>>, cute::Stride<cute::tuple<cute::_1, cute::LayoutLeft::Apply<cute::_1>>, cute::Stride<int, cute::constant<int, 2048>>, cute::_1024>>, unsigned int>>>, TiledMma=cute::TiledMMA<std::conditional_t<false, cute::MMA_Atom<cute::SM80_16x8x16_F32F16F16F32_TN>, cute::MMA_Atom<cute::SM80_16x8x16_F32BF16BF16F32_TN>>, cute::Layout<cute::Shape<cute::Int<2>, cute::Int<2>, cute::_1>, cute::LayoutLeft::Apply<cute::Shape<cute::Int<2>, cute::Int<2>, cute::_1>>>, cute::Layout<cute::Shape<cute::_1, cute::_2, cute::_1>, cute::LayoutLeft::Apply<cute::Shape<cute::_1, cute::_2, cute::_1>>>, cute::Tile<cute::Underscore, cute::Underscore, cute::Underscore>>, TiledCopyA=cute::TiledCopy<cute::Copy_Atom<cute::SM75_U16x8_LDSM_T, cutlass::bfloat16_t>, cute::Layout<cute::Shape<cute::tuple<cute::_4, cute::_8, cute::Int<2>, cute::Int<2>>, cute::tuple<cute::tuple<cute::_2, cute::_2, cute::_2>, cute::tuple<cute::_1, cute::_1>>>, cute::Stride<cute::tuple<cute::_64, cute::_1, cute::_16, cute::constant<int, 0>>, cute::tuple<cute::tuple<cute::constant<int, 32>, cute::_8, cute::_256>, cute::tuple<cute::Int<0>, cute::constant<int, 0>>>>>, cute::tuple<cute::Int<32>, cute::Int<16>>>, TiledCopyB=cute::TiledCopy<cute::Copy_Atom<cute::SM75_U16x8_LDSM_T, cutlass::bfloat16_t>, cute::Layout<cute::Shape<cute::tuple<cute::_4, cute::_8, cute::Int<2>, cute::Int<2>>, cute::tuple<cute::tuple<cute::_2, cute::_2>, cute::tuple<std::__tuple_element_t<1UL, std::tuple<cute::tuple<cute::_1, cute::Int<1>, cute::_1, cute::constant<int, 1>>, cute::constant<int, 2>>>, cute::constant<int, 1>>>>, cute::Stride<cute::tuple<cute::_64, cute::_1, cute::constant<int, 0>, cute::_8>, cute::tuple<cute::tuple<cute::constant<int, 32>, cute::constant<int, 256>>, cute::tuple<cute::_16, cute::constant<int, 0>>>>>, cute::tuple<cute::Int<32>, cute::Int<16>>>, ThrCopyA=cute::ThrCopy<cute::TiledCopy<cute::Copy_Atom<cute::SM75_U16x8_LDSM_T, cutlass::bfloat16_t>, cute::Layout<cute::Shape<cute::tuple<cute::_4, cute::_8, cute::Int<2>, cute::Int<2>>, cute::tuple<cute::tuple<cute::_2, cute::_2, cute::_2>, cute::tuple<cute::_1, cute::_1>>>, cute::Stride<cute::tuple<cute::_64, cute::_1, cute::_16, cute::constant<int, 0>>, cute::tuple<cute::tuple<cute::constant<int, 32>, cute::_8, cute::_256>, cute::tuple<cute::Int<0>, cute::constant<int, 0>>>>>, cute::tuple<cute::Int<32>, cute::Int<16>>>, int>, ThrCopyB=cute::ThrCopy<cute::TiledCopy<cute::Copy_Atom<cute::SM75_U16x8_LDSM_T, cutlass::bfloat16_t>, cute::Layout<cute::Shape<cute::tuple<cute::_4, cute::_8, cute::Int<2>, cute::Int<2>>, cute::tuple<cute::tuple<cute::_2, cute::_2>, cute::tuple<std::__tuple_element_t<1UL, std::tuple<cute::tuple<cute::_1, cute::Int<1>, cute::_1, cute::constant<int, 1>>, cute::constant<int, 2>>>, cute::constant<int, 1>>>>, cute::Stride<cute::tuple<cute::_64, cute::_1, cute::constant<int, 0>, cute::_8>, cute::tuple<cute::tuple<cute::constant<int, 32>, cute::constant<int, 256>>, cute::tuple<cute::_16, cute::constant<int, 0>>>>>, cute::tuple<cute::Int<32>, cute::Int<16>>>, int>]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_bwd_kernel.h(956): here
            instantiation of "void flash::compute_dq_dk_dv_1colblock<Kernel_traits,Is_dropout,Is_causal,Is_even_MN,Is_even_K,Is_first,Is_last,Seq_parallel,Params>(const Params &, int, int, int) [with Kernel_traits=Flash_bwd_kernel_traits<512, 32, 32, 4, 2, 2, 2, false, false, cutlass::bfloat16_t, Flash_kernel_traits<512, 32, 32, 4, cutlass::bfloat16_t>>, Is_dropout=true, Is_causal=true, Is_even_MN=true, Is_even_K=true, Is_first=false, Is_last=false, Seq_parallel=true, Params=Flash_bwd_params]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_bwd_kernel.h(1591): here
            instantiation of "void flash::compute_dq_dk_dv_seqk_parallel<Kernel_traits,Is_dropout,Is_causal,Is_even_MN,Is_even_K,Params>(const Params &) [with Kernel_traits=Flash_bwd_kernel_traits<512, 32, 32, 4, 2, 2, 2, false, false, cutlass::bfloat16_t, Flash_kernel_traits<512, 32, 32, 4, cutlass::bfloat16_t>>, Is_dropout=true, Is_causal=true, Is_even_MN=true, Is_even_K=true, Params=Flash_bwd_params]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_bwd_launch_template.h(28): here
            instantiation of "void flash_bwd_dq_dk_dv_loop_seqk_parallel_kernel<Kernel_traits,Is_dropout,Is_causal,Is_even_MN,Is_even_K>(Flash_bwd_params) [with Kernel_traits=Flash_bwd_kernel_traits<512, 32, 32, 4, 2, 2, 2, false, false, cutlass::bfloat16_t, Flash_kernel_traits<512, 32, 32, 4, cutlass::bfloat16_t>>, Is_dropout=true, Is_causal=true, Is_even_MN=true, Is_even_K=true]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_bwd_launch_template.h(62): here
            instantiation of "void run_flash_bwd_seqk_parallel<Kernel_traits,Is_dropout>(Flash_bwd_params &, cudaStream_t, __nv_bool) [with Kernel_traits=Flash_bwd_kernel_traits<512, 32, 32, 4, 2, 2, 2, false, false, cutlass::bfloat16_t, Flash_kernel_traits<512, 32, 32, 4, cutlass::bfloat16_t>>, Is_dropout=true]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_bwd_launch_template.h(134): here
            instantiation of "void run_flash_bwd<Kernel_traits,Is_dropout>(Flash_bwd_params &, cudaStream_t, __nv_bool) [with Kernel_traits=Flash_bwd_kernel_traits<512, 32, 32, 4, 2, 2, 2, false, false, cutlass::bfloat16_t, Flash_kernel_traits<512, 32, 32, 4, cutlass::bfloat16_t>>, Is_dropout=true]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_bwd_launch_template.h(365): here
            instantiation of "void run_mha_bwd_hdim512<T>(Flash_bwd_params &, cudaStream_t, __nv_bool) [with T=cutlass::bfloat16_t]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_bwd_hdim512_bf16_sm80.cu(9): here

/home/v-feiychen/flash-attention/csrc/cutlass/include/cute/algorithm/gemm.hpp(282): error: static assertion failed
          detected during:
            instantiation of "void cute::gemm(const cute::MMA_Atom<MMA> &, cute::Tensor<TD, DLayout> &, const cute::Tensor<TA, ALayout> &, const cute::Tensor<TB, BLayout> &, const cute::Tensor<TC, CLayout> &) [with MMA=cute::SM80_16x8x16_F32BF16BF16F32_TN, TD=cute::ArrayEngine<std::remove_cv_t<std::remove_reference_t<float &>>, 128>, DLayout=cute::Layout<cute::tuple<cute::tuple<cute::_2, cute::_2>, cute::constant<int, 1>, cute::constant<int, 32>>, cute::tuple<cute::tuple<cute::_1, cute::_2>, cute::constant<int, 0>, cute::constant<int, 4>>>, TA=cute::ViewEngine<std::remove_cv_t<std::remove_reference_t<cutlass::bfloat16_t &>> *>, ALayout=std::__tuple_element_t<(std::size_t)0UL, std::tuple<cute::Layout<cute::tuple<cute::Shape<cute::_2, cute::_2, cute::_2>, cute::Int<2>>, cute::tuple<cute::tuple<cute::_1, cute::_2, cute::constant<int, 4>>, cute::constant<int, 8>>>, int>>, TB=cute::ViewEngine<std::remove_cv_t<std::remove_reference_t<cutlass::bfloat16_t &>> *>, BLayout=std::__tuple_element_t<(std::size_t)0UL, std::tuple<cute::Layout<cute::tuple<cute::tuple<cute::_2, cute::_2>, cute::tuple<cute::_4, cute::_8>>, cute::tuple<cute::tuple<cute::_1, cute::_2>, cute::Shape<cute::Int<4>, cute::_32>>>, int>>, TC=cute::ArrayEngine<std::remove_cv_t<std::remove_reference_t<float &>>, 128>, CLayout=cute::Layout<cute::tuple<cute::tuple<cute::_2, cute::_2>, cute::constant<int, 1>, cute::constant<int, 32>>, cute::tuple<cute::tuple<cute::_1, cute::_2>, cute::constant<int, 0>, cute::constant<int, 4>>>, <unnamed>=(void *)nullptr]" 
(88): here
            instantiation of "void cute::gemm(const cute::MMA_Atom<MMA> &, const cute::Tensor<TA, ALayout> &, const cute::Tensor<TB, BLayout> &, cute::Tensor<TC, CLayout> &) [with MMA=cute::SM80_16x8x16_F32BF16BF16F32_TN, TA=cute::ViewEngine<std::remove_cv_t<std::remove_reference_t<cutlass::bfloat16_t &>> *>, ALayout=std::__tuple_element_t<(std::size_t)0UL, std::tuple<cute::Layout<cute::tuple<cute::Shape<cute::_2, cute::_2, cute::_2>, cute::Int<2>>, cute::tuple<cute::tuple<cute::_1, cute::_2, cute::constant<int, 4>>, cute::constant<int, 8>>>, int>>, TB=cute::ViewEngine<std::remove_cv_t<std::remove_reference_t<cutlass::bfloat16_t &>> *>, BLayout=std::__tuple_element_t<(std::size_t)0UL, std::tuple<cute::Layout<cute::tuple<cute::tuple<cute::_2, cute::_2>, cute::tuple<cute::_4, cute::_8>>, cute::tuple<cute::tuple<cute::_1, cute::_2>, cute::Shape<cute::Int<4>, cute::_32>>>, int>>, TC=cute::ArrayEngine<std::remove_cv_t<std::remove_reference_t<float &>>, 128>, CLayout=cute::Layout<cute::tuple<cute::tuple<cute::_2, cute::_2>, cute::constant<int, 1>, cute::constant<int, 32>>, cute::tuple<cute::tuple<cute::_1, cute::_2>, cute::constant<int, 0>, cute::constant<int, 4>>>]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/utils.h(197): here
            instantiation of "void flash::gemm(Tensor0 &, Tensor1 &, Tensor2 &, const Tensor3 &, const Tensor4 &, TiledMma, TiledCopyA, TiledCopyB, ThrCopyA, ThrCopyB) [with A_in_regs=false, B_in_regs=false, Tensor0=cute::Tensor<cute::ArrayEngine<std::remove_cv_t<std::remove_reference_t<float &>>, 128>, cute::Layout<cute::tuple<cute::tuple<cute::_2, cute::_2>, cute::constant<int, 1>, cute::constant<int, 32>>, cute::tuple<cute::tuple<cute::_1, cute::_2>, cute::constant<int, 0>, cute::constant<int, 4>>>>, Tensor1=cute::Tensor<cute::ArrayEngine<std::remove_cv_t<std::remove_reference_t<cutlass::bfloat16_t &>>, 32>, cute::Layout<cute::tuple<cute::Shape<cute::_2, cute::_2, cute::_2>, cute::Int<2>, cute::_2>, cute::tuple<cute::tuple<cute::_1, cute::_2, cute::constant<int, 4>>, cute::_8, cute::_16>>>, Tensor2=cute::Tensor<cute::ArrayEngine<std::remove_cv_t<std::remove_reference_t<cutlass::bfloat16_t &>>, 256>, cute::Layout<cute::tuple<cute::tuple<cute::_2, cute::_2>, cute::tuple<cute::_4, cute::_8>, cute::_2>, cute::tuple<cute::tuple<cute::_1, cute::_2>, cute::Shape<cute::Int<4>, cute::_32>, cute::_16>>>, Tensor3=cute::Tensor<cute::ViewEngine<cute::smem_ptr<std::remove_cv_t<std::remove_reference_t<cutlass::bfloat16_t &>>>>, std::__tuple_element_t<(std::size_t)0UL, std::tuple<cute::Layout<cute::Shape<cute::tuple<cute::_8, cute::_1>, cute::Int<2>, cute::Int<2>>, cute::Stride<cute::tuple<cute::_1, cute::LayoutLeft::Apply<cute::_1>>, int, cute::_1024>>, unsigned int>>>, Tensor4=cute::Tensor<cute::ViewEngine<cute::smem_ptr<std::remove_cv_t<std::remove_reference_t<cutlass::bfloat16_t &>>>>, std::__tuple_element_t<(std::size_t)0UL, std::tuple<cute::Layout<cute::Shape<cute::tuple<cute::_8, cute::_1>, cute::tuple<cute::constant<int, 2>, cute::constant<int, 8>>, cute::Int<2>>, cute::Stride<cute::tuple<cute::_1, cute::LayoutLeft::Apply<cute::_1>>, cute::Stride<int, cute::constant<int, 2048>>, cute::_1024>>, unsigned int>>>, TiledMma=cute::TiledMMA<std::conditional_t<false, cute::MMA_Atom<cute::SM80_16x8x16_F32F16F16F32_TN>, cute::MMA_Atom<cute::SM80_16x8x16_F32BF16BF16F32_TN>>, cute::Layout<cute::Shape<cute::Int<2>, cute::Int<2>, cute::_1>, cute::LayoutLeft::Apply<cute::Shape<cute::Int<2>, cute::Int<2>, cute::_1>>>, cute::Layout<cute::Shape<cute::_1, cute::_2, cute::_1>, cute::LayoutLeft::Apply<cute::Shape<cute::_1, cute::_2, cute::_1>>>, cute::Tile<cute::Underscore, cute::Underscore, cute::Underscore>>, TiledCopyA=cute::TiledCopy<cute::Copy_Atom<cute::SM75_U16x8_LDSM_T, cutlass::bfloat16_t>, cute::Layout<cute::Shape<cute::tuple<cute::_4, cute::_8, cute::Int<2>, cute::Int<2>>, cute::tuple<cute::tuple<cute::_2, cute::_2, cute::_2>, cute::tuple<cute::_1, cute::_1>>>, cute::Stride<cute::tuple<cute::_64, cute::_1, cute::_16, cute::constant<int, 0>>, cute::tuple<cute::tuple<cute::constant<int, 32>, cute::_8, cute::_256>, cute::tuple<cute::Int<0>, cute::constant<int, 0>>>>>, cute::tuple<cute::Int<32>, cute::Int<16>>>, TiledCopyB=cute::TiledCopy<cute::Copy_Atom<cute::SM75_U16x8_LDSM_T, cutlass::bfloat16_t>, cute::Layout<cute::Shape<cute::tuple<cute::_4, cute::_8, cute::Int<2>, cute::Int<2>>, cute::tuple<cute::tuple<cute::_2, cute::_2>, cute::tuple<std::__tuple_element_t<1UL, std::tuple<cute::tuple<cute::_1, cute::Int<1>, cute::_1, cute::constant<int, 1>>, cute::constant<int, 2>>>, cute::constant<int, 1>>>>, cute::Stride<cute::tuple<cute::_64, cute::_1, cute::constant<int, 0>, cute::_8>, cute::tuple<cute::tuple<cute::constant<int, 32>, cute::constant<int, 256>>, cute::tuple<cute::_16, cute::constant<int, 0>>>>>, cute::tuple<cute::Int<32>, cute::Int<16>>>, ThrCopyA=cute::ThrCopy<cute::TiledCopy<cute::Copy_Atom<cute::SM75_U16x8_LDSM_T, cutlass::bfloat16_t>, cute::Layout<cute::Shape<cute::tuple<cute::_4, cute::_8, cute::Int<2>, cute::Int<2>>, cute::tuple<cute::tuple<cute::_2, cute::_2, cute::_2>, cute::tuple<cute::_1, cute::_1>>>, cute::Stride<cute::tuple<cute::_64, cute::_1, cute::_16, cute::constant<int, 0>>, cute::tuple<cute::tuple<cute::constant<int, 32>, cute::_8, cute::_256>, cute::tuple<cute::Int<0>, cute::constant<int, 0>>>>>, cute::tuple<cute::Int<32>, cute::Int<16>>>, int>, ThrCopyB=cute::ThrCopy<cute::TiledCopy<cute::Copy_Atom<cute::SM75_U16x8_LDSM_T, cutlass::bfloat16_t>, cute::Layout<cute::Shape<cute::tuple<cute::_4, cute::_8, cute::Int<2>, cute::Int<2>>, cute::tuple<cute::tuple<cute::_2, cute::_2>, cute::tuple<std::__tuple_element_t<1UL, std::tuple<cute::tuple<cute::_1, cute::Int<1>, cute::_1, cute::constant<int, 1>>, cute::constant<int, 2>>>, cute::constant<int, 1>>>>, cute::Stride<cute::tuple<cute::_64, cute::_1, cute::constant<int, 0>, cute::_8>, cute::tuple<cute::tuple<cute::constant<int, 32>, cute::constant<int, 256>>, cute::tuple<cute::_16, cute::constant<int, 0>>>>>, cute::tuple<cute::Int<32>, cute::Int<16>>>, int>]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_bwd_kernel.h(956): here
            instantiation of "void flash::compute_dq_dk_dv_1colblock<Kernel_traits,Is_dropout,Is_causal,Is_even_MN,Is_even_K,Is_first,Is_last,Seq_parallel,Params>(const Params &, int, int, int) [with Kernel_traits=Flash_bwd_kernel_traits<512, 32, 32, 4, 2, 2, 2, false, false, cutlass::bfloat16_t, Flash_kernel_traits<512, 32, 32, 4, cutlass::bfloat16_t>>, Is_dropout=true, Is_causal=true, Is_even_MN=true, Is_even_K=true, Is_first=false, Is_last=false, Seq_parallel=true, Params=Flash_bwd_params]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_bwd_kernel.h(1591): here
            instantiation of "void flash::compute_dq_dk_dv_seqk_parallel<Kernel_traits,Is_dropout,Is_causal,Is_even_MN,Is_even_K,Params>(const Params &) [with Kernel_traits=Flash_bwd_kernel_traits<512, 32, 32, 4, 2, 2, 2, false, false, cutlass::bfloat16_t, Flash_kernel_traits<512, 32, 32, 4, cutlass::bfloat16_t>>, Is_dropout=true, Is_causal=true, Is_even_MN=true, Is_even_K=true, Params=Flash_bwd_params]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_bwd_launch_template.h(28): here
            instantiation of "void flash_bwd_dq_dk_dv_loop_seqk_parallel_kernel<Kernel_traits,Is_dropout,Is_causal,Is_even_MN,Is_even_K>(Flash_bwd_params) [with Kernel_traits=Flash_bwd_kernel_traits<512, 32, 32, 4, 2, 2, 2, false, false, cutlass::bfloat16_t, Flash_kernel_traits<512, 32, 32, 4, cutlass::bfloat16_t>>, Is_dropout=true, Is_causal=true, Is_even_MN=true, Is_even_K=true]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_bwd_launch_template.h(62): here
            instantiation of "void run_flash_bwd_seqk_parallel<Kernel_traits,Is_dropout>(Flash_bwd_params &, cudaStream_t, __nv_bool) [with Kernel_traits=Flash_bwd_kernel_traits<512, 32, 32, 4, 2, 2, 2, false, false, cutlass::bfloat16_t, Flash_kernel_traits<512, 32, 32, 4, cutlass::bfloat16_t>>, Is_dropout=true]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_bwd_launch_template.h(134): here
            instantiation of "void run_flash_bwd<Kernel_traits,Is_dropout>(Flash_bwd_params &, cudaStream_t, __nv_bool) [with Kernel_traits=Flash_bwd_kernel_traits<512, 32, 32, 4, 2, 2, 2, false, false, cutlass::bfloat16_t, Flash_kernel_traits<512, 32, 32, 4, cutlass::bfloat16_t>>, Is_dropout=true]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_bwd_launch_template.h(365): here
            instantiation of "void run_mha_bwd_hdim512<T>(Flash_bwd_params &, cudaStream_t, __nv_bool) [with T=cutlass::bfloat16_t]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_bwd_hdim512_bf16_sm80.cu(9): here

/home/v-feiychen/flash-attention/csrc/flash_attn/src/utils.h(184): error: static assertion failed
          detected during:
            instantiation of "void flash::gemm(Tensor0 &, Tensor1 &, Tensor2 &, const Tensor3 &, const Tensor4 &, TiledMma, TiledCopyA, TiledCopyB, ThrCopyA, ThrCopyB) [with A_in_regs=false, B_in_regs=false, Tensor0=cute::Tensor<cute::ArrayEngine<std::remove_cv_t<std::remove_reference_t<float &>>, 128>, cute::Layout<cute::tuple<cute::tuple<cute::_2, cute::_2>, cute::constant<int, 1>, cute::constant<int, 32>>, cute::tuple<cute::tuple<cute::_1, cute::_2>, cute::constant<int, 0>, cute::constant<int, 4>>>>, Tensor1=cute::Tensor<cute::ArrayEngine<std::remove_cv_t<std::remove_reference_t<cutlass::bfloat16_t &>>, 32>, cute::Layout<cute::Shape<cute::Shape<cute::_2, cute::_2, cute::_2>, cute::_1, cute::tuple<cute::_2, cute::_2>>, cute::tuple<cute::tuple<cute::_1, cute::_2, cute::constant<int, 4>>, cute::constant<int, 0>, cute::tuple<cute::Int<8>, cute::Int<16>>>>>, Tensor2=cute::Tensor<cute::ArrayEngine<std::remove_cv_t<std::remove_reference_t<cutlass::bfloat16_t &>>, 256>, cute::Layout<cute::tuple<cute::tuple<cute::_2, cute::_2>, cute::tuple<cute::_4, cute::_8>, cute::_2>, cute::tuple<cute::tuple<cute::_1, cute::_2>, cute::Shape<cute::Int<4>, cute::_32>, cute::_16>>>, Tensor3=cute::Tensor<cute::ViewEngine<cute::smem_ptr<std::remove_cv_t<std::remove_reference_t<cutlass::bfloat16_t &>>>>, std::__tuple_element_t<(std::size_t)0UL, std::tuple<cute::Layout<cute::Shape<cute::tuple<cute::_8, cute::_1>, cute::_1, cute::tuple<cute::_2, cute::_2>>, cute::Stride<cute::tuple<cute::_1, cute::LayoutLeft::Apply<cute::_1>>, cute::constant<int, 0>, cute::tuple<int, int>>>, unsigned int>>>, Tensor4=cute::Tensor<cute::ViewEngine<cute::smem_ptr<std::remove_cv_t<std::remove_reference_t<cutlass::bfloat16_t &>>>>, std::__tuple_element_t<(std::size_t)0UL, std::tuple<cute::Layout<cute::Shape<cute::tuple<cute::_8, cute::_1>, cute::tuple<cute::constant<int, 2>, cute::constant<int, 8>>, cute::Int<2>>, cute::Stride<cute::tuple<cute::_1, cute::LayoutLeft::Apply<cute::_1>>, cute::Stride<int, cute::constant<int, 2048>>, cute::_1024>>, unsigned int>>>, TiledMma=cute::TiledMMA<std::conditional_t<false, cute::MMA_Atom<cute::SM80_16x8x16_F32F16F16F32_TN>, cute::MMA_Atom<cute::SM80_16x8x16_F32BF16BF16F32_TN>>, cute::Layout<cute::Shape<cute::Int<2>, cute::Int<2>, cute::_1>, cute::LayoutLeft::Apply<cute::Shape<cute::Int<2>, cute::Int<2>, cute::_1>>>, cute::Layout<cute::Shape<cute::_1, cute::_2, cute::_1>, cute::LayoutLeft::Apply<cute::Shape<cute::_1, cute::_2, cute::_1>>>, cute::Tile<cute::Underscore, cute::Underscore, cute::Underscore>>, TiledCopyA=cute::TiledCopy<cute::Copy_Atom<cute::SM75_U32x4_LDSM_N, cutlass::bfloat16_t>, cute::Layout<cute::Shape<cute::tuple<cute::_4, cute::_8, cute::Int<2>, cute::Int<2>>, cute::tuple<cute::tuple<cute::_2, cute::_2, cute::_2>, cute::tuple<cute::_1, cute::_1>>>, cute::Stride<cute::tuple<cute::_64, cute::_1, cute::_16, cute::constant<int, 0>>, cute::tuple<cute::tuple<cute::constant<int, 32>, cute::_8, cute::_256>, cute::tuple<cute::Int<0>, cute::constant<int, 0>>>>>, cute::tuple<cute::Int<32>, cute::Int<16>>>, TiledCopyB=cute::TiledCopy<cute::Copy_Atom<cute::SM75_U16x8_LDSM_T, cutlass::bfloat16_t>, cute::Layout<cute::Shape<cute::tuple<cute::_4, cute::_8, cute::Int<2>, cute::Int<2>>, cute::tuple<cute::tuple<cute::_2, cute::_2>, cute::tuple<std::__tuple_element_t<1UL, std::tuple<cute::tuple<cute::_1, cute::Int<1>, cute::_1, cute::constant<int, 1>>, cute::constant<int, 2>>>, cute::constant<int, 1>>>>, cute::Stride<cute::tuple<cute::_64, cute::_1, cute::constant<int, 0>, cute::_8>, cute::tuple<cute::tuple<cute::constant<int, 32>, cute::constant<int, 256>>, cute::tuple<cute::_16, cute::constant<int, 0>>>>>, cute::tuple<cute::Int<32>, cute::Int<16>>>, ThrCopyA=cute::ThrCopy<cute::TiledCopy<cute::Copy_Atom<cute::SM75_U32x4_LDSM_N, cutlass::bfloat16_t>, cute::Layout<cute::Shape<cute::tuple<cute::_4, cute::_8, cute::Int<2>, cute::Int<2>>, cute::tuple<cute::tuple<cute::_2, cute::_2, cute::_2>, cute::tuple<cute::_1, cute::_1>>>, cute::Stride<cute::tuple<cute::_64, cute::_1, cute::_16, cute::constant<int, 0>>, cute::tuple<cute::tuple<cute::constant<int, 32>, cute::_8, cute::_256>, cute::tuple<cute::Int<0>, cute::constant<int, 0>>>>>, cute::tuple<cute::Int<32>, cute::Int<16>>>, int>, ThrCopyB=cute::ThrCopy<cute::TiledCopy<cute::Copy_Atom<cute::SM75_U16x8_LDSM_T, cutlass::bfloat16_t>, cute::Layout<cute::Shape<cute::tuple<cute::_4, cute::_8, cute::Int<2>, cute::Int<2>>, cute::tuple<cute::tuple<cute::_2, cute::_2>, cute::tuple<std::__tuple_element_t<1UL, std::tuple<cute::tuple<cute::_1, cute::Int<1>, cute::_1, cute::constant<int, 1>>, cute::constant<int, 2>>>, cute::constant<int, 1>>>>, cute::Stride<cute::tuple<cute::_64, cute::_1, cute::constant<int, 0>, cute::_8>, cute::tuple<cute::tuple<cute::constant<int, 32>, cute::constant<int, 256>>, cute::tuple<cute::_16, cute::constant<int, 0>>>>>, cute::tuple<cute::Int<32>, cute::Int<16>>>, int>]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_bwd_kernel.h(977): here
            instantiation of "void flash::compute_dq_dk_dv_1colblock<Kernel_traits,Is_dropout,Is_causal,Is_even_MN,Is_even_K,Is_first,Is_last,Seq_parallel,Params>(const Params &, int, int, int) [with Kernel_traits=Flash_bwd_kernel_traits<512, 32, 32, 4, 2, 2, 2, false, false, cutlass::bfloat16_t, Flash_kernel_traits<512, 32, 32, 4, cutlass::bfloat16_t>>, Is_dropout=true, Is_causal=true, Is_even_MN=true, Is_even_K=true, Is_first=false, Is_last=false, Seq_parallel=true, Params=Flash_bwd_params]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_bwd_kernel.h(1591): here
            instantiation of "void flash::compute_dq_dk_dv_seqk_parallel<Kernel_traits,Is_dropout,Is_causal,Is_even_MN,Is_even_K,Params>(const Params &) [with Kernel_traits=Flash_bwd_kernel_traits<512, 32, 32, 4, 2, 2, 2, false, false, cutlass::bfloat16_t, Flash_kernel_traits<512, 32, 32, 4, cutlass::bfloat16_t>>, Is_dropout=true, Is_causal=true, Is_even_MN=true, Is_even_K=true, Params=Flash_bwd_params]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_bwd_launch_template.h(28): here
            instantiation of "void flash_bwd_dq_dk_dv_loop_seqk_parallel_kernel<Kernel_traits,Is_dropout,Is_causal,Is_even_MN,Is_even_K>(Flash_bwd_params) [with Kernel_traits=Flash_bwd_kernel_traits<512, 32, 32, 4, 2, 2, 2, false, false, cutlass::bfloat16_t, Flash_kernel_traits<512, 32, 32, 4, cutlass::bfloat16_t>>, Is_dropout=true, Is_causal=true, Is_even_MN=true, Is_even_K=true]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_bwd_launch_template.h(62): here
            instantiation of "void run_flash_bwd_seqk_parallel<Kernel_traits,Is_dropout>(Flash_bwd_params &, cudaStream_t, __nv_bool) [with Kernel_traits=Flash_bwd_kernel_traits<512, 32, 32, 4, 2, 2, 2, false, false, cutlass::bfloat16_t, Flash_kernel_traits<512, 32, 32, 4, cutlass::bfloat16_t>>, Is_dropout=true]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_bwd_launch_template.h(134): here
            instantiation of "void run_flash_bwd<Kernel_traits,Is_dropout>(Flash_bwd_params &, cudaStream_t, __nv_bool) [with Kernel_traits=Flash_bwd_kernel_traits<512, 32, 32, 4, 2, 2, 2, false, false, cutlass::bfloat16_t, Flash_kernel_traits<512, 32, 32, 4, cutlass::bfloat16_t>>, Is_dropout=true]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_bwd_launch_template.h(365): here
            instantiation of "void run_mha_bwd_hdim512<T>(Flash_bwd_params &, cudaStream_t, __nv_bool) [with T=cutlass::bfloat16_t]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_bwd_hdim512_bf16_sm80.cu(9): here

/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_bwd_kernel.h(853): error: static assertion failed
          detected during:
            instantiation of "void flash::compute_dq_dk_dv_1colblock<Kernel_traits,Is_dropout,Is_causal,Is_even_MN,Is_even_K,Is_first,Is_last,Seq_parallel,Params>(const Params &, int, int, int) [with Kernel_traits=Flash_bwd_kernel_traits<512, 32, 32, 4, 2, 2, 2, false, false, cutlass::bfloat16_t, Flash_kernel_traits<512, 32, 32, 4, cutlass::bfloat16_t>>, Is_dropout=true, Is_causal=true, Is_even_MN=true, Is_even_K=false, Is_first=false, Is_last=false, Seq_parallel=true, Params=Flash_bwd_params]" 
(1591): here
            instantiation of "void flash::compute_dq_dk_dv_seqk_parallel<Kernel_traits,Is_dropout,Is_causal,Is_even_MN,Is_even_K,Params>(const Params &) [with Kernel_traits=Flash_bwd_kernel_traits<512, 32, 32, 4, 2, 2, 2, false, false, cutlass::bfloat16_t, Flash_kernel_traits<512, 32, 32, 4, cutlass::bfloat16_t>>, Is_dropout=true, Is_causal=true, Is_even_MN=true, Is_even_K=false, Params=Flash_bwd_params]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_bwd_launch_template.h(28): here
            instantiation of "void flash_bwd_dq_dk_dv_loop_seqk_parallel_kernel<Kernel_traits,Is_dropout,Is_causal,Is_even_MN,Is_even_K>(Flash_bwd_params) [with Kernel_traits=Flash_bwd_kernel_traits<512, 32, 32, 4, 2, 2, 2, false, false, cutlass::bfloat16_t, Flash_kernel_traits<512, 32, 32, 4, cutlass::bfloat16_t>>, Is_dropout=true, Is_causal=true, Is_even_MN=true, Is_even_K=false]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_bwd_launch_template.h(62): here
            instantiation of "void run_flash_bwd_seqk_parallel<Kernel_traits,Is_dropout>(Flash_bwd_params &, cudaStream_t, __nv_bool) [with Kernel_traits=Flash_bwd_kernel_traits<512, 32, 32, 4, 2, 2, 2, false, false, cutlass::bfloat16_t, Flash_kernel_traits<512, 32, 32, 4, cutlass::bfloat16_t>>, Is_dropout=true]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_bwd_launch_template.h(134): here
            instantiation of "void run_flash_bwd<Kernel_traits,Is_dropout>(Flash_bwd_params &, cudaStream_t, __nv_bool) [with Kernel_traits=Flash_bwd_kernel_traits<512, 32, 32, 4, 2, 2, 2, false, false, cutlass::bfloat16_t, Flash_kernel_traits<512, 32, 32, 4, cutlass::bfloat16_t>>, Is_dropout=true]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_bwd_launch_template.h(365): here
            instantiation of "void run_mha_bwd_hdim512<T>(Flash_bwd_params &, cudaStream_t, __nv_bool) [with T=cutlass::bfloat16_t]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_bwd_hdim512_bf16_sm80.cu(9): here

/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_bwd_kernel.h(853): error: static assertion failed
          detected during:
            instantiation of "void flash::compute_dq_dk_dv_1colblock<Kernel_traits,Is_dropout,Is_causal,Is_even_MN,Is_even_K,Is_first,Is_last,Seq_parallel,Params>(const Params &, int, int, int) [with Kernel_traits=Flash_bwd_kernel_traits<512, 32, 32, 4, 2, 2, 2, false, false, cutlass::bfloat16_t, Flash_kernel_traits<512, 32, 32, 4, cutlass::bfloat16_t>>, Is_dropout=true, Is_causal=true, Is_even_MN=false, Is_even_K=true, Is_first=false, Is_last=false, Seq_parallel=true, Params=Flash_bwd_params]" 
(1591): here
            instantiation of "void flash::compute_dq_dk_dv_seqk_parallel<Kernel_traits,Is_dropout,Is_causal,Is_even_MN,Is_even_K,Params>(const Params &) [with Kernel_traits=Flash_bwd_kernel_traits<512, 32, 32, 4, 2, 2, 2, false, false, cutlass::bfloat16_t, Flash_kernel_traits<512, 32, 32, 4, cutlass::bfloat16_t>>, Is_dropout=true, Is_causal=true, Is_even_MN=false, Is_even_K=true, Params=Flash_bwd_params]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_bwd_launch_template.h(28): here
            instantiation of "void flash_bwd_dq_dk_dv_loop_seqk_parallel_kernel<Kernel_traits,Is_dropout,Is_causal,Is_even_MN,Is_even_K>(Flash_bwd_params) [with Kernel_traits=Flash_bwd_kernel_traits<512, 32, 32, 4, 2, 2, 2, false, false, cutlass::bfloat16_t, Flash_kernel_traits<512, 32, 32, 4, cutlass::bfloat16_t>>, Is_dropout=true, Is_causal=true, Is_even_MN=false, Is_even_K=true]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_bwd_launch_template.h(62): here
            instantiation of "void run_flash_bwd_seqk_parallel<Kernel_traits,Is_dropout>(Flash_bwd_params &, cudaStream_t, __nv_bool) [with Kernel_traits=Flash_bwd_kernel_traits<512, 32, 32, 4, 2, 2, 2, false, false, cutlass::bfloat16_t, Flash_kernel_traits<512, 32, 32, 4, cutlass::bfloat16_t>>, Is_dropout=true]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_bwd_launch_template.h(134): here
            instantiation of "void run_flash_bwd<Kernel_traits,Is_dropout>(Flash_bwd_params &, cudaStream_t, __nv_bool) [with Kernel_traits=Flash_bwd_kernel_traits<512, 32, 32, 4, 2, 2, 2, false, false, cutlass::bfloat16_t, Flash_kernel_traits<512, 32, 32, 4, cutlass::bfloat16_t>>, Is_dropout=true]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_bwd_launch_template.h(365): here
            instantiation of "void run_mha_bwd_hdim512<T>(Flash_bwd_params &, cudaStream_t, __nv_bool) [with T=cutlass::bfloat16_t]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_bwd_hdim512_bf16_sm80.cu(9): here

/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_bwd_kernel.h(853): error: static assertion failed
          detected during:
            instantiation of "void flash::compute_dq_dk_dv_1colblock<Kernel_traits,Is_dropout,Is_causal,Is_even_MN,Is_even_K,Is_first,Is_last,Seq_parallel,Params>(const Params &, int, int, int) [with Kernel_traits=Flash_bwd_kernel_traits<512, 32, 32, 4, 2, 2, 2, false, false, cutlass::bfloat16_t, Flash_kernel_traits<512, 32, 32, 4, cutlass::bfloat16_t>>, Is_dropout=true, Is_causal=true, Is_even_MN=false, Is_even_K=false, Is_first=false, Is_last=false, Seq_parallel=true, Params=Flash_bwd_params]" 
(1591): here
            instantiation of "void flash::compute_dq_dk_dv_seqk_parallel<Kernel_traits,Is_dropout,Is_causal,Is_even_MN,Is_even_K,Params>(const Params &) [with Kernel_traits=Flash_bwd_kernel_traits<512, 32, 32, 4, 2, 2, 2, false, false, cutlass::bfloat16_t, Flash_kernel_traits<512, 32, 32, 4, cutlass::bfloat16_t>>, Is_dropout=true, Is_causal=true, Is_even_MN=false, Is_even_K=false, Params=Flash_bwd_params]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_bwd_launch_template.h(28): here
            instantiation of "void flash_bwd_dq_dk_dv_loop_seqk_parallel_kernel<Kernel_traits,Is_dropout,Is_causal,Is_even_MN,Is_even_K>(Flash_bwd_params) [with Kernel_traits=Flash_bwd_kernel_traits<512, 32, 32, 4, 2, 2, 2, false, false, cutlass::bfloat16_t, Flash_kernel_traits<512, 32, 32, 4, cutlass::bfloat16_t>>, Is_dropout=true, Is_causal=true, Is_even_MN=false, Is_even_K=false]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_bwd_launch_template.h(62): here
            instantiation of "void run_flash_bwd_seqk_parallel<Kernel_traits,Is_dropout>(Flash_bwd_params &, cudaStream_t, __nv_bool) [with Kernel_traits=Flash_bwd_kernel_traits<512, 32, 32, 4, 2, 2, 2, false, false, cutlass::bfloat16_t, Flash_kernel_traits<512, 32, 32, 4, cutlass::bfloat16_t>>, Is_dropout=true]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_bwd_launch_template.h(134): here
            instantiation of "void run_flash_bwd<Kernel_traits,Is_dropout>(Flash_bwd_params &, cudaStream_t, __nv_bool) [with Kernel_traits=Flash_bwd_kernel_traits<512, 32, 32, 4, 2, 2, 2, false, false, cutlass::bfloat16_t, Flash_kernel_traits<512, 32, 32, 4, cutlass::bfloat16_t>>, Is_dropout=true]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_bwd_launch_template.h(365): here
            instantiation of "void run_mha_bwd_hdim512<T>(Flash_bwd_params &, cudaStream_t, __nv_bool) [with T=cutlass::bfloat16_t]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_bwd_hdim512_bf16_sm80.cu(9): here

/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_bwd_kernel.h(853): error: static assertion failed
          detected during:
            instantiation of "void flash::compute_dq_dk_dv_1colblock<Kernel_traits,Is_dropout,Is_causal,Is_even_MN,Is_even_K,Is_first,Is_last,Seq_parallel,Params>(const Params &, int, int, int) [with Kernel_traits=Flash_bwd_kernel_traits<512, 32, 32, 4, 2, 2, 2, false, false, cutlass::bfloat16_t, Flash_kernel_traits<512, 32, 32, 4, cutlass::bfloat16_t>>, Is_dropout=true, Is_causal=false, Is_even_MN=true, Is_even_K=true, Is_first=false, Is_last=false, Seq_parallel=true, Params=Flash_bwd_params]" 
(1591): here
            instantiation of "void flash::compute_dq_dk_dv_seqk_parallel<Kernel_traits,Is_dropout,Is_causal,Is_even_MN,Is_even_K,Params>(const Params &) [with Kernel_traits=Flash_bwd_kernel_traits<512, 32, 32, 4, 2, 2, 2, false, false, cutlass::bfloat16_t, Flash_kernel_traits<512, 32, 32, 4, cutlass::bfloat16_t>>, Is_dropout=true, Is_causal=false, Is_even_MN=true, Is_even_K=true, Params=Flash_bwd_params]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_bwd_launch_template.h(28): here
            instantiation of "void flash_bwd_dq_dk_dv_loop_seqk_parallel_kernel<Kernel_traits,Is_dropout,Is_causal,Is_even_MN,Is_even_K>(Flash_bwd_params) [with Kernel_traits=Flash_bwd_kernel_traits<512, 32, 32, 4, 2, 2, 2, false, false, cutlass::bfloat16_t, Flash_kernel_traits<512, 32, 32, 4, cutlass::bfloat16_t>>, Is_dropout=true, Is_causal=false, Is_even_MN=true, Is_even_K=true]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_bwd_launch_template.h(62): here
            instantiation of "void run_flash_bwd_seqk_parallel<Kernel_traits,Is_dropout>(Flash_bwd_params &, cudaStream_t, __nv_bool) [with Kernel_traits=Flash_bwd_kernel_traits<512, 32, 32, 4, 2, 2, 2, false, false, cutlass::bfloat16_t, Flash_kernel_traits<512, 32, 32, 4, cutlass::bfloat16_t>>, Is_dropout=true]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_bwd_launch_template.h(134): here
            instantiation of "void run_flash_bwd<Kernel_traits,Is_dropout>(Flash_bwd_params &, cudaStream_t, __nv_bool) [with Kernel_traits=Flash_bwd_kernel_traits<512, 32, 32, 4, 2, 2, 2, false, false, cutlass::bfloat16_t, Flash_kernel_traits<512, 32, 32, 4, cutlass::bfloat16_t>>, Is_dropout=true]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_bwd_launch_template.h(365): here
            instantiation of "void run_mha_bwd_hdim512<T>(Flash_bwd_params &, cudaStream_t, __nv_bool) [with T=cutlass::bfloat16_t]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_bwd_hdim512_bf16_sm80.cu(9): here

/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_bwd_kernel.h(853): error: static assertion failed
          detected during:
            instantiation of "void flash::compute_dq_dk_dv_1colblock<Kernel_traits,Is_dropout,Is_causal,Is_even_MN,Is_even_K,Is_first,Is_last,Seq_parallel,Params>(const Params &, int, int, int) [with Kernel_traits=Flash_bwd_kernel_traits<512, 32, 32, 4, 2, 2, 2, false, false, cutlass::bfloat16_t, Flash_kernel_traits<512, 32, 32, 4, cutlass::bfloat16_t>>, Is_dropout=true, Is_causal=false, Is_even_MN=true, Is_even_K=false, Is_first=false, Is_last=false, Seq_parallel=true, Params=Flash_bwd_params]" 
(1591): here
            instantiation of "void flash::compute_dq_dk_dv_seqk_parallel<Kernel_traits,Is_dropout,Is_causal,Is_even_MN,Is_even_K,Params>(const Params &) [with Kernel_traits=Flash_bwd_kernel_traits<512, 32, 32, 4, 2, 2, 2, false, false, cutlass::bfloat16_t, Flash_kernel_traits<512, 32, 32, 4, cutlass::bfloat16_t>>, Is_dropout=true, Is_causal=false, Is_even_MN=true, Is_even_K=false, Params=Flash_bwd_params]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_bwd_launch_template.h(28): here
            instantiation of "void flash_bwd_dq_dk_dv_loop_seqk_parallel_kernel<Kernel_traits,Is_dropout,Is_causal,Is_even_MN,Is_even_K>(Flash_bwd_params) [with Kernel_traits=Flash_bwd_kernel_traits<512, 32, 32, 4, 2, 2, 2, false, false, cutlass::bfloat16_t, Flash_kernel_traits<512, 32, 32, 4, cutlass::bfloat16_t>>, Is_dropout=true, Is_causal=false, Is_even_MN=true, Is_even_K=false]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_bwd_launch_template.h(62): here
            instantiation of "void run_flash_bwd_seqk_parallel<Kernel_traits,Is_dropout>(Flash_bwd_params &, cudaStream_t, __nv_bool) [with Kernel_traits=Flash_bwd_kernel_traits<512, 32, 32, 4, 2, 2, 2, false, false, cutlass::bfloat16_t, Flash_kernel_traits<512, 32, 32, 4, cutlass::bfloat16_t>>, Is_dropout=true]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_bwd_launch_template.h(134): here
            instantiation of "void run_flash_bwd<Kernel_traits,Is_dropout>(Flash_bwd_params &, cudaStream_t, __nv_bool) [with Kernel_traits=Flash_bwd_kernel_traits<512, 32, 32, 4, 2, 2, 2, false, false, cutlass::bfloat16_t, Flash_kernel_traits<512, 32, 32, 4, cutlass::bfloat16_t>>, Is_dropout=true]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_bwd_launch_template.h(365): here
            instantiation of "void run_mha_bwd_hdim512<T>(Flash_bwd_params &, cudaStream_t, __nv_bool) [with T=cutlass::bfloat16_t]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_bwd_hdim512_bf16_sm80.cu(9): here

/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_bwd_kernel.h(853): error: static assertion failed
          detected during:
            instantiation of "void flash::compute_dq_dk_dv_1colblock<Kernel_traits,Is_dropout,Is_causal,Is_even_MN,Is_even_K,Is_first,Is_last,Seq_parallel,Params>(const Params &, int, int, int) [with Kernel_traits=Flash_bwd_kernel_traits<512, 32, 32, 4, 2, 2, 2, false, false, cutlass::bfloat16_t, Flash_kernel_traits<512, 32, 32, 4, cutlass::bfloat16_t>>, Is_dropout=true, Is_causal=false, Is_even_MN=false, Is_even_K=true, Is_first=false, Is_last=false, Seq_parallel=true, Params=Flash_bwd_params]" 
(1591): here
            instantiation of "void flash::compute_dq_dk_dv_seqk_parallel<Kernel_traits,Is_dropout,Is_causal,Is_even_MN,Is_even_K,Params>(const Params &) [with Kernel_traits=Flash_bwd_kernel_traits<512, 32, 32, 4, 2, 2, 2, false, false, cutlass::bfloat16_t, Flash_kernel_traits<512, 32, 32, 4, cutlass::bfloat16_t>>, Is_dropout=true, Is_causal=false, Is_even_MN=false, Is_even_K=true, Params=Flash_bwd_params]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_bwd_launch_template.h(28): here
            instantiation of "void flash_bwd_dq_dk_dv_loop_seqk_parallel_kernel<Kernel_traits,Is_dropout,Is_causal,Is_even_MN,Is_even_K>(Flash_bwd_params) [with Kernel_traits=Flash_bwd_kernel_traits<512, 32, 32, 4, 2, 2, 2, false, false, cutlass::bfloat16_t, Flash_kernel_traits<512, 32, 32, 4, cutlass::bfloat16_t>>, Is_dropout=true, Is_causal=false, Is_even_MN=false, Is_even_K=true]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_bwd_launch_template.h(62): here
            instantiation of "void run_flash_bwd_seqk_parallel<Kernel_traits,Is_dropout>(Flash_bwd_params &, cudaStream_t, __nv_bool) [with Kernel_traits=Flash_bwd_kernel_traits<512, 32, 32, 4, 2, 2, 2, false, false, cutlass::bfloat16_t, Flash_kernel_traits<512, 32, 32, 4, cutlass::bfloat16_t>>, Is_dropout=true]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_bwd_launch_template.h(134): here
            instantiation of "void run_flash_bwd<Kernel_traits,Is_dropout>(Flash_bwd_params &, cudaStream_t, __nv_bool) [with Kernel_traits=Flash_bwd_kernel_traits<512, 32, 32, 4, 2, 2, 2, false, false, cutlass::bfloat16_t, Flash_kernel_traits<512, 32, 32, 4, cutlass::bfloat16_t>>, Is_dropout=true]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_bwd_launch_template.h(365): here
            instantiation of "void run_mha_bwd_hdim512<T>(Flash_bwd_params &, cudaStream_t, __nv_bool) [with T=cutlass::bfloat16_t]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_bwd_hdim512_bf16_sm80.cu(9): here

/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_bwd_kernel.h(853): error: static assertion failed
          detected during:
            instantiation of "void flash::compute_dq_dk_dv_1colblock<Kernel_traits,Is_dropout,Is_causal,Is_even_MN,Is_even_K,Is_first,Is_last,Seq_parallel,Params>(const Params &, int, int, int) [with Kernel_traits=Flash_bwd_kernel_traits<512, 32, 32, 4, 2, 2, 2, false, false, cutlass::bfloat16_t, Flash_kernel_traits<512, 32, 32, 4, cutlass::bfloat16_t>>, Is_dropout=true, Is_causal=false, Is_even_MN=false, Is_even_K=false, Is_first=false, Is_last=false, Seq_parallel=true, Params=Flash_bwd_params]" 
(1591): here
            instantiation of "void flash::compute_dq_dk_dv_seqk_parallel<Kernel_traits,Is_dropout,Is_causal,Is_even_MN,Is_even_K,Params>(const Params &) [with Kernel_traits=Flash_bwd_kernel_traits<512, 32, 32, 4, 2, 2, 2, false, false, cutlass::bfloat16_t, Flash_kernel_traits<512, 32, 32, 4, cutlass::bfloat16_t>>, Is_dropout=true, Is_causal=false, Is_even_MN=false, Is_even_K=false, Params=Flash_bwd_params]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_bwd_launch_template.h(28): here
            instantiation of "void flash_bwd_dq_dk_dv_loop_seqk_parallel_kernel<Kernel_traits,Is_dropout,Is_causal,Is_even_MN,Is_even_K>(Flash_bwd_params) [with Kernel_traits=Flash_bwd_kernel_traits<512, 32, 32, 4, 2, 2, 2, false, false, cutlass::bfloat16_t, Flash_kernel_traits<512, 32, 32, 4, cutlass::bfloat16_t>>, Is_dropout=true, Is_causal=false, Is_even_MN=false, Is_even_K=false]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_bwd_launch_template.h(62): here
            instantiation of "void run_flash_bwd_seqk_parallel<Kernel_traits,Is_dropout>(Flash_bwd_params &, cudaStream_t, __nv_bool) [with Kernel_traits=Flash_bwd_kernel_traits<512, 32, 32, 4, 2, 2, 2, false, false, cutlass::bfloat16_t, Flash_kernel_traits<512, 32, 32, 4, cutlass::bfloat16_t>>, Is_dropout=true]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_bwd_launch_template.h(134): here
            instantiation of "void run_flash_bwd<Kernel_traits,Is_dropout>(Flash_bwd_params &, cudaStream_t, __nv_bool) [with Kernel_traits=Flash_bwd_kernel_traits<512, 32, 32, 4, 2, 2, 2, false, false, cutlass::bfloat16_t, Flash_kernel_traits<512, 32, 32, 4, cutlass::bfloat16_t>>, Is_dropout=true]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_bwd_launch_template.h(365): here
            instantiation of "void run_mha_bwd_hdim512<T>(Flash_bwd_params &, cudaStream_t, __nv_bool) [with T=cutlass::bfloat16_t]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_bwd_hdim512_bf16_sm80.cu(9): here

/home/v-feiychen/flash-attention/csrc/flash_attn/src/kernel_traits.h(246): error: static assertion failed
          detected during:
            instantiation of class "Flash_bwd_kernel_traits<kHeadDim_, kBlockM_, kBlockN_, kNWarps_, AtomLayoutMSdP_, AtomLayoutNdKV, AtomLayoutMdQ, Is_V_in_regs_, No_double_buffer_, elem_type, Base> [with kHeadDim_=512, kBlockM_=32, kBlockN_=32, kNWarps_=4, AtomLayoutMSdP_=2, AtomLayoutNdKV=2, AtomLayoutMdQ=2, Is_V_in_regs_=false, No_double_buffer_=true, elem_type=cutlass::bfloat16_t, Base=Flash_kernel_traits<512, 32, 32, 4, cutlass::bfloat16_t>]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_bwd_launch_template.h(48): here
            instantiation of "void run_flash_bwd_seqk_parallel<Kernel_traits,Is_dropout>(Flash_bwd_params &, cudaStream_t, __nv_bool) [with Kernel_traits=Flash_bwd_kernel_traits<512, 32, 32, 4, 2, 2, 2, false, true, cutlass::bfloat16_t, Flash_kernel_traits<512, 32, 32, 4, cutlass::bfloat16_t>>, Is_dropout=true]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_bwd_launch_template.h(134): here
            instantiation of "void run_flash_bwd<Kernel_traits,Is_dropout>(Flash_bwd_params &, cudaStream_t, __nv_bool) [with Kernel_traits=Flash_bwd_kernel_traits<512, 32, 32, 4, 2, 2, 2, false, true, cutlass::bfloat16_t, Flash_kernel_traits<512, 32, 32, 4, cutlass::bfloat16_t>>, Is_dropout=true]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_bwd_launch_template.h(365): here
            instantiation of "void run_mha_bwd_hdim512<T>(Flash_bwd_params &, cudaStream_t, __nv_bool) [with T=cutlass::bfloat16_t]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_bwd_hdim512_bf16_sm80.cu(9): here

/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_bwd_kernel.h(853): error: static assertion failed
          detected during:
            instantiation of "void flash::compute_dq_dk_dv_1colblock<Kernel_traits,Is_dropout,Is_causal,Is_even_MN,Is_even_K,Is_first,Is_last,Seq_parallel,Params>(const Params &, int, int, int) [with Kernel_traits=Flash_bwd_kernel_traits<512, 32, 32, 4, 2, 2, 2, false, true, cutlass::bfloat16_t, Flash_kernel_traits<512, 32, 32, 4, cutlass::bfloat16_t>>, Is_dropout=true, Is_causal=true, Is_even_MN=true, Is_even_K=true, Is_first=false, Is_last=false, Seq_parallel=true, Params=Flash_bwd_params]" 
(1591): here
            instantiation of "void flash::compute_dq_dk_dv_seqk_parallel<Kernel_traits,Is_dropout,Is_causal,Is_even_MN,Is_even_K,Params>(const Params &) [with Kernel_traits=Flash_bwd_kernel_traits<512, 32, 32, 4, 2, 2, 2, false, true, cutlass::bfloat16_t, Flash_kernel_traits<512, 32, 32, 4, cutlass::bfloat16_t>>, Is_dropout=true, Is_causal=true, Is_even_MN=true, Is_even_K=true, Params=Flash_bwd_params]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_bwd_launch_template.h(28): here
            instantiation of "void flash_bwd_dq_dk_dv_loop_seqk_parallel_kernel<Kernel_traits,Is_dropout,Is_causal,Is_even_MN,Is_even_K>(Flash_bwd_params) [with Kernel_traits=Flash_bwd_kernel_traits<512, 32, 32, 4, 2, 2, 2, false, true, cutlass::bfloat16_t, Flash_kernel_traits<512, 32, 32, 4, cutlass::bfloat16_t>>, Is_dropout=true, Is_causal=true, Is_even_MN=true, Is_even_K=true]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_bwd_launch_template.h(62): here
            instantiation of "void run_flash_bwd_seqk_parallel<Kernel_traits,Is_dropout>(Flash_bwd_params &, cudaStream_t, __nv_bool) [with Kernel_traits=Flash_bwd_kernel_traits<512, 32, 32, 4, 2, 2, 2, false, true, cutlass::bfloat16_t, Flash_kernel_traits<512, 32, 32, 4, cutlass::bfloat16_t>>, Is_dropout=true]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_bwd_launch_template.h(134): here
            instantiation of "void run_flash_bwd<Kernel_traits,Is_dropout>(Flash_bwd_params &, cudaStream_t, __nv_bool) [with Kernel_traits=Flash_bwd_kernel_traits<512, 32, 32, 4, 2, 2, 2, false, true, cutlass::bfloat16_t, Flash_kernel_traits<512, 32, 32, 4, cutlass::bfloat16_t>>, Is_dropout=true]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_bwd_launch_template.h(365): here
            instantiation of "void run_mha_bwd_hdim512<T>(Flash_bwd_params &, cudaStream_t, __nv_bool) [with T=cutlass::bfloat16_t]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_bwd_hdim512_bf16_sm80.cu(9): here

/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_bwd_kernel.h(853): error: static assertion failed
          detected during:
            instantiation of "void flash::compute_dq_dk_dv_1colblock<Kernel_traits,Is_dropout,Is_causal,Is_even_MN,Is_even_K,Is_first,Is_last,Seq_parallel,Params>(const Params &, int, int, int) [with Kernel_traits=Flash_bwd_kernel_traits<512, 32, 32, 4, 2, 2, 2, false, true, cutlass::bfloat16_t, Flash_kernel_traits<512, 32, 32, 4, cutlass::bfloat16_t>>, Is_dropout=true, Is_causal=true, Is_even_MN=true, Is_even_K=false, Is_first=false, Is_last=false, Seq_parallel=true, Params=Flash_bwd_params]" 
(1591): here
            instantiation of "void flash::compute_dq_dk_dv_seqk_parallel<Kernel_traits,Is_dropout,Is_causal,Is_even_MN,Is_even_K,Params>(const Params &) [with Kernel_traits=Flash_bwd_kernel_traits<512, 32, 32, 4, 2, 2, 2, false, true, cutlass::bfloat16_t, Flash_kernel_traits<512, 32, 32, 4, cutlass::bfloat16_t>>, Is_dropout=true, Is_causal=true, Is_even_MN=true, Is_even_K=false, Params=Flash_bwd_params]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_bwd_launch_template.h(28): here
            instantiation of "void flash_bwd_dq_dk_dv_loop_seqk_parallel_kernel<Kernel_traits,Is_dropout,Is_causal,Is_even_MN,Is_even_K>(Flash_bwd_params) [with Kernel_traits=Flash_bwd_kernel_traits<512, 32, 32, 4, 2, 2, 2, false, true, cutlass::bfloat16_t, Flash_kernel_traits<512, 32, 32, 4, cutlass::bfloat16_t>>, Is_dropout=true, Is_causal=true, Is_even_MN=true, Is_even_K=false]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_bwd_launch_template.h(62): here
            instantiation of "void run_flash_bwd_seqk_parallel<Kernel_traits,Is_dropout>(Flash_bwd_params &, cudaStream_t, __nv_bool) [with Kernel_traits=Flash_bwd_kernel_traits<512, 32, 32, 4, 2, 2, 2, false, true, cutlass::bfloat16_t, Flash_kernel_traits<512, 32, 32, 4, cutlass::bfloat16_t>>, Is_dropout=true]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_bwd_launch_template.h(134): here
            instantiation of "void run_flash_bwd<Kernel_traits,Is_dropout>(Flash_bwd_params &, cudaStream_t, __nv_bool) [with Kernel_traits=Flash_bwd_kernel_traits<512, 32, 32, 4, 2, 2, 2, false, true, cutlass::bfloat16_t, Flash_kernel_traits<512, 32, 32, 4, cutlass::bfloat16_t>>, Is_dropout=true]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_bwd_launch_template.h(365): here
            instantiation of "void run_mha_bwd_hdim512<T>(Flash_bwd_params &, cudaStream_t, __nv_bool) [with T=cutlass::bfloat16_t]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_bwd_hdim512_bf16_sm80.cu(9): here

/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_bwd_kernel.h(853): error: static assertion failed
          detected during:
            instantiation of "void flash::compute_dq_dk_dv_1colblock<Kernel_traits,Is_dropout,Is_causal,Is_even_MN,Is_even_K,Is_first,Is_last,Seq_parallel,Params>(const Params &, int, int, int) [with Kernel_traits=Flash_bwd_kernel_traits<512, 32, 32, 4, 2, 2, 2, false, true, cutlass::bfloat16_t, Flash_kernel_traits<512, 32, 32, 4, cutlass::bfloat16_t>>, Is_dropout=true, Is_causal=true, Is_even_MN=false, Is_even_K=true, Is_first=false, Is_last=false, Seq_parallel=true, Params=Flash_bwd_params]" 
(1591): here
            instantiation of "void flash::compute_dq_dk_dv_seqk_parallel<Kernel_traits,Is_dropout,Is_causal,Is_even_MN,Is_even_K,Params>(const Params &) [with Kernel_traits=Flash_bwd_kernel_traits<512, 32, 32, 4, 2, 2, 2, false, true, cutlass::bfloat16_t, Flash_kernel_traits<512, 32, 32, 4, cutlass::bfloat16_t>>, Is_dropout=true, Is_causal=true, Is_even_MN=false, Is_even_K=true, Params=Flash_bwd_params]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_bwd_launch_template.h(28): here
            instantiation of "void flash_bwd_dq_dk_dv_loop_seqk_parallel_kernel<Kernel_traits,Is_dropout,Is_causal,Is_even_MN,Is_even_K>(Flash_bwd_params) [with Kernel_traits=Flash_bwd_kernel_traits<512, 32, 32, 4, 2, 2, 2, false, true, cutlass::bfloat16_t, Flash_kernel_traits<512, 32, 32, 4, cutlass::bfloat16_t>>, Is_dropout=true, Is_causal=true, Is_even_MN=false, Is_even_K=true]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_bwd_launch_template.h(62): here
            instantiation of "void run_flash_bwd_seqk_parallel<Kernel_traits,Is_dropout>(Flash_bwd_params &, cudaStream_t, __nv_bool) [with Kernel_traits=Flash_bwd_kernel_traits<512, 32, 32, 4, 2, 2, 2, false, true, cutlass::bfloat16_t, Flash_kernel_traits<512, 32, 32, 4, cutlass::bfloat16_t>>, Is_dropout=true]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_bwd_launch_template.h(134): here
            instantiation of "void run_flash_bwd<Kernel_traits,Is_dropout>(Flash_bwd_params &, cudaStream_t, __nv_bool) [with Kernel_traits=Flash_bwd_kernel_traits<512, 32, 32, 4, 2, 2, 2, false, true, cutlass::bfloat16_t, Flash_kernel_traits<512, 32, 32, 4, cutlass::bfloat16_t>>, Is_dropout=true]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_bwd_launch_template.h(365): here
            instantiation of "void run_mha_bwd_hdim512<T>(Flash_bwd_params &, cudaStream_t, __nv_bool) [with T=cutlass::bfloat16_t]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_bwd_hdim512_bf16_sm80.cu(9): here

/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_bwd_kernel.h(853): error: static assertion failed
          detected during:
            instantiation of "void flash::compute_dq_dk_dv_1colblock<Kernel_traits,Is_dropout,Is_causal,Is_even_MN,Is_even_K,Is_first,Is_last,Seq_parallel,Params>(const Params &, int, int, int) [with Kernel_traits=Flash_bwd_kernel_traits<512, 32, 32, 4, 2, 2, 2, false, true, cutlass::bfloat16_t, Flash_kernel_traits<512, 32, 32, 4, cutlass::bfloat16_t>>, Is_dropout=true, Is_causal=true, Is_even_MN=false, Is_even_K=false, Is_first=false, Is_last=false, Seq_parallel=true, Params=Flash_bwd_params]" 
(1591): here
            instantiation of "void flash::compute_dq_dk_dv_seqk_parallel<Kernel_traits,Is_dropout,Is_causal,Is_even_MN,Is_even_K,Params>(const Params &) [with Kernel_traits=Flash_bwd_kernel_traits<512, 32, 32, 4, 2, 2, 2, false, true, cutlass::bfloat16_t, Flash_kernel_traits<512, 32, 32, 4, cutlass::bfloat16_t>>, Is_dropout=true, Is_causal=true, Is_even_MN=false, Is_even_K=false, Params=Flash_bwd_params]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_bwd_launch_template.h(28): here
            instantiation of "void flash_bwd_dq_dk_dv_loop_seqk_parallel_kernel<Kernel_traits,Is_dropout,Is_causal,Is_even_MN,Is_even_K>(Flash_bwd_params) [with Kernel_traits=Flash_bwd_kernel_traits<512, 32, 32, 4, 2, 2, 2, false, true, cutlass::bfloat16_t, Flash_kernel_traits<512, 32, 32, 4, cutlass::bfloat16_t>>, Is_dropout=true, Is_causal=true, Is_even_MN=false, Is_even_K=false]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_bwd_launch_template.h(62): here
            instantiation of "void run_flash_bwd_seqk_parallel<Kernel_traits,Is_dropout>(Flash_bwd_params &, cudaStream_t, __nv_bool) [with Kernel_traits=Flash_bwd_kernel_traits<512, 32, 32, 4, 2, 2, 2, false, true, cutlass::bfloat16_t, Flash_kernel_traits<512, 32, 32, 4, cutlass::bfloat16_t>>, Is_dropout=true]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_bwd_launch_template.h(134): here
            instantiation of "void run_flash_bwd<Kernel_traits,Is_dropout>(Flash_bwd_params &, cudaStream_t, __nv_bool) [with Kernel_traits=Flash_bwd_kernel_traits<512, 32, 32, 4, 2, 2, 2, false, true, cutlass::bfloat16_t, Flash_kernel_traits<512, 32, 32, 4, cutlass::bfloat16_t>>, Is_dropout=true]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_bwd_launch_template.h(365): here
            instantiation of "void run_mha_bwd_hdim512<T>(Flash_bwd_params &, cudaStream_t, __nv_bool) [with T=cutlass::bfloat16_t]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_bwd_hdim512_bf16_sm80.cu(9): here

/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_bwd_kernel.h(853): error: static assertion failed
          detected during:
            instantiation of "void flash::compute_dq_dk_dv_1colblock<Kernel_traits,Is_dropout,Is_causal,Is_even_MN,Is_even_K,Is_first,Is_last,Seq_parallel,Params>(const Params &, int, int, int) [with Kernel_traits=Flash_bwd_kernel_traits<512, 32, 32, 4, 2, 2, 2, false, true, cutlass::bfloat16_t, Flash_kernel_traits<512, 32, 32, 4, cutlass::bfloat16_t>>, Is_dropout=true, Is_causal=false, Is_even_MN=true, Is_even_K=true, Is_first=false, Is_last=false, Seq_parallel=true, Params=Flash_bwd_params]" 
(1591): here
            instantiation of "void flash::compute_dq_dk_dv_seqk_parallel<Kernel_traits,Is_dropout,Is_causal,Is_even_MN,Is_even_K,Params>(const Params &) [with Kernel_traits=Flash_bwd_kernel_traits<512, 32, 32, 4, 2, 2, 2, false, true, cutlass::bfloat16_t, Flash_kernel_traits<512, 32, 32, 4, cutlass::bfloat16_t>>, Is_dropout=true, Is_causal=false, Is_even_MN=true, Is_even_K=true, Params=Flash_bwd_params]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_bwd_launch_template.h(28): here
            instantiation of "void flash_bwd_dq_dk_dv_loop_seqk_parallel_kernel<Kernel_traits,Is_dropout,Is_causal,Is_even_MN,Is_even_K>(Flash_bwd_params) [with Kernel_traits=Flash_bwd_kernel_traits<512, 32, 32, 4, 2, 2, 2, false, true, cutlass::bfloat16_t, Flash_kernel_traits<512, 32, 32, 4, cutlass::bfloat16_t>>, Is_dropout=true, Is_causal=false, Is_even_MN=true, Is_even_K=true]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_bwd_launch_template.h(62): here
            instantiation of "void run_flash_bwd_seqk_parallel<Kernel_traits,Is_dropout>(Flash_bwd_params &, cudaStream_t, __nv_bool) [with Kernel_traits=Flash_bwd_kernel_traits<512, 32, 32, 4, 2, 2, 2, false, true, cutlass::bfloat16_t, Flash_kernel_traits<512, 32, 32, 4, cutlass::bfloat16_t>>, Is_dropout=true]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_bwd_launch_template.h(134): here
            instantiation of "void run_flash_bwd<Kernel_traits,Is_dropout>(Flash_bwd_params &, cudaStream_t, __nv_bool) [with Kernel_traits=Flash_bwd_kernel_traits<512, 32, 32, 4, 2, 2, 2, false, true, cutlass::bfloat16_t, Flash_kernel_traits<512, 32, 32, 4, cutlass::bfloat16_t>>, Is_dropout=true]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_bwd_launch_template.h(365): here
            instantiation of "void run_mha_bwd_hdim512<T>(Flash_bwd_params &, cudaStream_t, __nv_bool) [with T=cutlass::bfloat16_t]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_bwd_hdim512_bf16_sm80.cu(9): here

/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_bwd_kernel.h(853): error: static assertion failed
          detected during:
            instantiation of "void flash::compute_dq_dk_dv_1colblock<Kernel_traits,Is_dropout,Is_causal,Is_even_MN,Is_even_K,Is_first,Is_last,Seq_parallel,Params>(const Params &, int, int, int) [with Kernel_traits=Flash_bwd_kernel_traits<512, 32, 32, 4, 2, 2, 2, false, true, cutlass::bfloat16_t, Flash_kernel_traits<512, 32, 32, 4, cutlass::bfloat16_t>>, Is_dropout=true, Is_causal=false, Is_even_MN=true, Is_even_K=false, Is_first=false, Is_last=false, Seq_parallel=true, Params=Flash_bwd_params]" 
(1591): here
            instantiation of "void flash::compute_dq_dk_dv_seqk_parallel<Kernel_traits,Is_dropout,Is_causal,Is_even_MN,Is_even_K,Params>(const Params &) [with Kernel_traits=Flash_bwd_kernel_traits<512, 32, 32, 4, 2, 2, 2, false, true, cutlass::bfloat16_t, Flash_kernel_traits<512, 32, 32, 4, cutlass::bfloat16_t>>, Is_dropout=true, Is_causal=false, Is_even_MN=true, Is_even_K=false, Params=Flash_bwd_params]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_bwd_launch_template.h(28): here
            instantiation of "void flash_bwd_dq_dk_dv_loop_seqk_parallel_kernel<Kernel_traits,Is_dropout,Is_causal,Is_even_MN,Is_even_K>(Flash_bwd_params) [with Kernel_traits=Flash_bwd_kernel_traits<512, 32, 32, 4, 2, 2, 2, false, true, cutlass::bfloat16_t, Flash_kernel_traits<512, 32, 32, 4, cutlass::bfloat16_t>>, Is_dropout=true, Is_causal=false, Is_even_MN=true, Is_even_K=false]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_bwd_launch_template.h(62): here
            instantiation of "void run_flash_bwd_seqk_parallel<Kernel_traits,Is_dropout>(Flash_bwd_params &, cudaStream_t, __nv_bool) [with Kernel_traits=Flash_bwd_kernel_traits<512, 32, 32, 4, 2, 2, 2, false, true, cutlass::bfloat16_t, Flash_kernel_traits<512, 32, 32, 4, cutlass::bfloat16_t>>, Is_dropout=true]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_bwd_launch_template.h(134): here
            instantiation of "void run_flash_bwd<Kernel_traits,Is_dropout>(Flash_bwd_params &, cudaStream_t, __nv_bool) [with Kernel_traits=Flash_bwd_kernel_traits<512, 32, 32, 4, 2, 2, 2, false, true, cutlass::bfloat16_t, Flash_kernel_traits<512, 32, 32, 4, cutlass::bfloat16_t>>, Is_dropout=true]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_bwd_launch_template.h(365): here
            instantiation of "void run_mha_bwd_hdim512<T>(Flash_bwd_params &, cudaStream_t, __nv_bool) [with T=cutlass::bfloat16_t]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_bwd_hdim512_bf16_sm80.cu(9): here

/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_bwd_kernel.h(853): error: static assertion failed
          detected during:
            instantiation of "void flash::compute_dq_dk_dv_1colblock<Kernel_traits,Is_dropout,Is_causal,Is_even_MN,Is_even_K,Is_first,Is_last,Seq_parallel,Params>(const Params &, int, int, int) [with Kernel_traits=Flash_bwd_kernel_traits<512, 32, 32, 4, 2, 2, 2, false, true, cutlass::bfloat16_t, Flash_kernel_traits<512, 32, 32, 4, cutlass::bfloat16_t>>, Is_dropout=true, Is_causal=false, Is_even_MN=false, Is_even_K=true, Is_first=false, Is_last=false, Seq_parallel=true, Params=Flash_bwd_params]" 
(1591): here
            instantiation of "void flash::compute_dq_dk_dv_seqk_parallel<Kernel_traits,Is_dropout,Is_causal,Is_even_MN,Is_even_K,Params>(const Params &) [with Kernel_traits=Flash_bwd_kernel_traits<512, 32, 32, 4, 2, 2, 2, false, true, cutlass::bfloat16_t, Flash_kernel_traits<512, 32, 32, 4, cutlass::bfloat16_t>>, Is_dropout=true, Is_causal=false, Is_even_MN=false, Is_even_K=true, Params=Flash_bwd_params]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_bwd_launch_template.h(28): here
            instantiation of "void flash_bwd_dq_dk_dv_loop_seqk_parallel_kernel<Kernel_traits,Is_dropout,Is_causal,Is_even_MN,Is_even_K>(Flash_bwd_params) [with Kernel_traits=Flash_bwd_kernel_traits<512, 32, 32, 4, 2, 2, 2, false, true, cutlass::bfloat16_t, Flash_kernel_traits<512, 32, 32, 4, cutlass::bfloat16_t>>, Is_dropout=true, Is_causal=false, Is_even_MN=false, Is_even_K=true]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_bwd_launch_template.h(62): here
            instantiation of "void run_flash_bwd_seqk_parallel<Kernel_traits,Is_dropout>(Flash_bwd_params &, cudaStream_t, __nv_bool) [with Kernel_traits=Flash_bwd_kernel_traits<512, 32, 32, 4, 2, 2, 2, false, true, cutlass::bfloat16_t, Flash_kernel_traits<512, 32, 32, 4, cutlass::bfloat16_t>>, Is_dropout=true]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_bwd_launch_template.h(134): here
            instantiation of "void run_flash_bwd<Kernel_traits,Is_dropout>(Flash_bwd_params &, cudaStream_t, __nv_bool) [with Kernel_traits=Flash_bwd_kernel_traits<512, 32, 32, 4, 2, 2, 2, false, true, cutlass::bfloat16_t, Flash_kernel_traits<512, 32, 32, 4, cutlass::bfloat16_t>>, Is_dropout=true]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_bwd_launch_template.h(365): here
            instantiation of "void run_mha_bwd_hdim512<T>(Flash_bwd_params &, cudaStream_t, __nv_bool) [with T=cutlass::bfloat16_t]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_bwd_hdim512_bf16_sm80.cu(9): here

/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_bwd_kernel.h(853): error: static assertion failed
          detected during:
            instantiation of "void flash::compute_dq_dk_dv_1colblock<Kernel_traits,Is_dropout,Is_causal,Is_even_MN,Is_even_K,Is_first,Is_last,Seq_parallel,Params>(const Params &, int, int, int) [with Kernel_traits=Flash_bwd_kernel_traits<512, 32, 32, 4, 2, 2, 2, false, true, cutlass::bfloat16_t, Flash_kernel_traits<512, 32, 32, 4, cutlass::bfloat16_t>>, Is_dropout=true, Is_causal=false, Is_even_MN=false, Is_even_K=false, Is_first=false, Is_last=false, Seq_parallel=true, Params=Flash_bwd_params]" 
(1591): here
            instantiation of "void flash::compute_dq_dk_dv_seqk_parallel<Kernel_traits,Is_dropout,Is_causal,Is_even_MN,Is_even_K,Params>(const Params &) [with Kernel_traits=Flash_bwd_kernel_traits<512, 32, 32, 4, 2, 2, 2, false, true, cutlass::bfloat16_t, Flash_kernel_traits<512, 32, 32, 4, cutlass::bfloat16_t>>, Is_dropout=true, Is_causal=false, Is_even_MN=false, Is_even_K=false, Params=Flash_bwd_params]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_bwd_launch_template.h(28): here
            instantiation of "void flash_bwd_dq_dk_dv_loop_seqk_parallel_kernel<Kernel_traits,Is_dropout,Is_causal,Is_even_MN,Is_even_K>(Flash_bwd_params) [with Kernel_traits=Flash_bwd_kernel_traits<512, 32, 32, 4, 2, 2, 2, false, true, cutlass::bfloat16_t, Flash_kernel_traits<512, 32, 32, 4, cutlass::bfloat16_t>>, Is_dropout=true, Is_causal=false, Is_even_MN=false, Is_even_K=false]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_bwd_launch_template.h(62): here
            instantiation of "void run_flash_bwd_seqk_parallel<Kernel_traits,Is_dropout>(Flash_bwd_params &, cudaStream_t, __nv_bool) [with Kernel_traits=Flash_bwd_kernel_traits<512, 32, 32, 4, 2, 2, 2, false, true, cutlass::bfloat16_t, Flash_kernel_traits<512, 32, 32, 4, cutlass::bfloat16_t>>, Is_dropout=true]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_bwd_launch_template.h(134): here
            instantiation of "void run_flash_bwd<Kernel_traits,Is_dropout>(Flash_bwd_params &, cudaStream_t, __nv_bool) [with Kernel_traits=Flash_bwd_kernel_traits<512, 32, 32, 4, 2, 2, 2, false, true, cutlass::bfloat16_t, Flash_kernel_traits<512, 32, 32, 4, cutlass::bfloat16_t>>, Is_dropout=true]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_bwd_launch_template.h(365): here
            instantiation of "void run_mha_bwd_hdim512<T>(Flash_bwd_params &, cudaStream_t, __nv_bool) [with T=cutlass::bfloat16_t]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_bwd_hdim512_bf16_sm80.cu(9): here

/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_bwd_kernel.h(853): error: static assertion failed
          detected during:
            instantiation of "void flash::compute_dq_dk_dv_1colblock<Kernel_traits,Is_dropout,Is_causal,Is_even_MN,Is_even_K,Is_first,Is_last,Seq_parallel,Params>(const Params &, int, int, int) [with Kernel_traits=Flash_bwd_kernel_traits<512, 32, 32, 4, 2, 2, 2, false, false, cutlass::bfloat16_t, Flash_kernel_traits<512, 32, 32, 4, cutlass::bfloat16_t>>, Is_dropout=false, Is_causal=true, Is_even_MN=true, Is_even_K=true, Is_first=false, Is_last=false, Seq_parallel=true, Params=Flash_bwd_params]" 
(1591): here
            instantiation of "void flash::compute_dq_dk_dv_seqk_parallel<Kernel_traits,Is_dropout,Is_causal,Is_even_MN,Is_even_K,Params>(const Params &) [with Kernel_traits=Flash_bwd_kernel_traits<512, 32, 32, 4, 2, 2, 2, false, false, cutlass::bfloat16_t, Flash_kernel_traits<512, 32, 32, 4, cutlass::bfloat16_t>>, Is_dropout=false, Is_causal=true, Is_even_MN=true, Is_even_K=true, Params=Flash_bwd_params]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_bwd_launch_template.h(28): here
            instantiation of "void flash_bwd_dq_dk_dv_loop_seqk_parallel_kernel<Kernel_traits,Is_dropout,Is_causal,Is_even_MN,Is_even_K>(Flash_bwd_params) [with Kernel_traits=Flash_bwd_kernel_traits<512, 32, 32, 4, 2, 2, 2, false, false, cutlass::bfloat16_t, Flash_kernel_traits<512, 32, 32, 4, cutlass::bfloat16_t>>, Is_dropout=false, Is_causal=true, Is_even_MN=true, Is_even_K=true]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_bwd_launch_template.h(62): here
            instantiation of "void run_flash_bwd_seqk_parallel<Kernel_traits,Is_dropout>(Flash_bwd_params &, cudaStream_t, __nv_bool) [with Kernel_traits=Flash_bwd_kernel_traits<512, 32, 32, 4, 2, 2, 2, false, false, cutlass::bfloat16_t, Flash_kernel_traits<512, 32, 32, 4, cutlass::bfloat16_t>>, Is_dropout=false]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_bwd_launch_template.h(134): here
            instantiation of "void run_flash_bwd<Kernel_traits,Is_dropout>(Flash_bwd_params &, cudaStream_t, __nv_bool) [with Kernel_traits=Flash_bwd_kernel_traits<512, 32, 32, 4, 2, 2, 2, false, false, cutlass::bfloat16_t, Flash_kernel_traits<512, 32, 32, 4, cutlass::bfloat16_t>>, Is_dropout=false]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_bwd_launch_template.h(365): here
            instantiation of "void run_mha_bwd_hdim512<T>(Flash_bwd_params &, cudaStream_t, __nv_bool) [with T=cutlass::bfloat16_t]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_bwd_hdim512_bf16_sm80.cu(9): here

/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_bwd_kernel.h(853): error: static assertion failed
          detected during:
            instantiation of "void flash::compute_dq_dk_dv_1colblock<Kernel_traits,Is_dropout,Is_causal,Is_even_MN,Is_even_K,Is_first,Is_last,Seq_parallel,Params>(const Params &, int, int, int) [with Kernel_traits=Flash_bwd_kernel_traits<512, 32, 32, 4, 2, 2, 2, false, false, cutlass::bfloat16_t, Flash_kernel_traits<512, 32, 32, 4, cutlass::bfloat16_t>>, Is_dropout=false, Is_causal=true, Is_even_MN=true, Is_even_K=false, Is_first=false, Is_last=false, Seq_parallel=true, Params=Flash_bwd_params]" 
(1591): here
            instantiation of "void flash::compute_dq_dk_dv_seqk_parallel<Kernel_traits,Is_dropout,Is_causal,Is_even_MN,Is_even_K,Params>(const Params &) [with Kernel_traits=Flash_bwd_kernel_traits<512, 32, 32, 4, 2, 2, 2, false, false, cutlass::bfloat16_t, Flash_kernel_traits<512, 32, 32, 4, cutlass::bfloat16_t>>, Is_dropout=false, Is_causal=true, Is_even_MN=true, Is_even_K=false, Params=Flash_bwd_params]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_bwd_launch_template.h(28): here
            instantiation of "void flash_bwd_dq_dk_dv_loop_seqk_parallel_kernel<Kernel_traits,Is_dropout,Is_causal,Is_even_MN,Is_even_K>(Flash_bwd_params) [with Kernel_traits=Flash_bwd_kernel_traits<512, 32, 32, 4, 2, 2, 2, false, false, cutlass::bfloat16_t, Flash_kernel_traits<512, 32, 32, 4, cutlass::bfloat16_t>>, Is_dropout=false, Is_causal=true, Is_even_MN=true, Is_even_K=false]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_bwd_launch_template.h(62): here
            instantiation of "void run_flash_bwd_seqk_parallel<Kernel_traits,Is_dropout>(Flash_bwd_params &, cudaStream_t, __nv_bool) [with Kernel_traits=Flash_bwd_kernel_traits<512, 32, 32, 4, 2, 2, 2, false, false, cutlass::bfloat16_t, Flash_kernel_traits<512, 32, 32, 4, cutlass::bfloat16_t>>, Is_dropout=false]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_bwd_launch_template.h(134): here
            instantiation of "void run_flash_bwd<Kernel_traits,Is_dropout>(Flash_bwd_params &, cudaStream_t, __nv_bool) [with Kernel_traits=Flash_bwd_kernel_traits<512, 32, 32, 4, 2, 2, 2, false, false, cutlass::bfloat16_t, Flash_kernel_traits<512, 32, 32, 4, cutlass::bfloat16_t>>, Is_dropout=false]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_bwd_launch_template.h(365): here
            instantiation of "void run_mha_bwd_hdim512<T>(Flash_bwd_params &, cudaStream_t, __nv_bool) [with T=cutlass::bfloat16_t]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_bwd_hdim512_bf16_sm80.cu(9): here

/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_bwd_kernel.h(853): error: static assertion failed
          detected during:
            instantiation of "void flash::compute_dq_dk_dv_1colblock<Kernel_traits,Is_dropout,Is_causal,Is_even_MN,Is_even_K,Is_first,Is_last,Seq_parallel,Params>(const Params &, int, int, int) [with Kernel_traits=Flash_bwd_kernel_traits<512, 32, 32, 4, 2, 2, 2, false, false, cutlass::bfloat16_t, Flash_kernel_traits<512, 32, 32, 4, cutlass::bfloat16_t>>, Is_dropout=false, Is_causal=true, Is_even_MN=false, Is_even_K=true, Is_first=false, Is_last=false, Seq_parallel=true, Params=Flash_bwd_params]" 
(1591): here
            instantiation of "void flash::compute_dq_dk_dv_seqk_parallel<Kernel_traits,Is_dropout,Is_causal,Is_even_MN,Is_even_K,Params>(const Params &) [with Kernel_traits=Flash_bwd_kernel_traits<512, 32, 32, 4, 2, 2, 2, false, false, cutlass::bfloat16_t, Flash_kernel_traits<512, 32, 32, 4, cutlass::bfloat16_t>>, Is_dropout=false, Is_causal=true, Is_even_MN=false, Is_even_K=true, Params=Flash_bwd_params]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_bwd_launch_template.h(28): here
            instantiation of "void flash_bwd_dq_dk_dv_loop_seqk_parallel_kernel<Kernel_traits,Is_dropout,Is_causal,Is_even_MN,Is_even_K>(Flash_bwd_params) [with Kernel_traits=Flash_bwd_kernel_traits<512, 32, 32, 4, 2, 2, 2, false, false, cutlass::bfloat16_t, Flash_kernel_traits<512, 32, 32, 4, cutlass::bfloat16_t>>, Is_dropout=false, Is_causal=true, Is_even_MN=false, Is_even_K=true]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_bwd_launch_template.h(62): here
            instantiation of "void run_flash_bwd_seqk_parallel<Kernel_traits,Is_dropout>(Flash_bwd_params &, cudaStream_t, __nv_bool) [with Kernel_traits=Flash_bwd_kernel_traits<512, 32, 32, 4, 2, 2, 2, false, false, cutlass::bfloat16_t, Flash_kernel_traits<512, 32, 32, 4, cutlass::bfloat16_t>>, Is_dropout=false]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_bwd_launch_template.h(134): here
            instantiation of "void run_flash_bwd<Kernel_traits,Is_dropout>(Flash_bwd_params &, cudaStream_t, __nv_bool) [with Kernel_traits=Flash_bwd_kernel_traits<512, 32, 32, 4, 2, 2, 2, false, false, cutlass::bfloat16_t, Flash_kernel_traits<512, 32, 32, 4, cutlass::bfloat16_t>>, Is_dropout=false]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_bwd_launch_template.h(365): here
            instantiation of "void run_mha_bwd_hdim512<T>(Flash_bwd_params &, cudaStream_t, __nv_bool) [with T=cutlass::bfloat16_t]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_bwd_hdim512_bf16_sm80.cu(9): here

/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_bwd_kernel.h(853): error: static assertion failed
          detected during:
            instantiation of "void flash::compute_dq_dk_dv_1colblock<Kernel_traits,Is_dropout,Is_causal,Is_even_MN,Is_even_K,Is_first,Is_last,Seq_parallel,Params>(const Params &, int, int, int) [with Kernel_traits=Flash_bwd_kernel_traits<512, 32, 32, 4, 2, 2, 2, false, false, cutlass::bfloat16_t, Flash_kernel_traits<512, 32, 32, 4, cutlass::bfloat16_t>>, Is_dropout=false, Is_causal=true, Is_even_MN=false, Is_even_K=false, Is_first=false, Is_last=false, Seq_parallel=true, Params=Flash_bwd_params]" 
(1591): here
            instantiation of "void flash::compute_dq_dk_dv_seqk_parallel<Kernel_traits,Is_dropout,Is_causal,Is_even_MN,Is_even_K,Params>(const Params &) [with Kernel_traits=Flash_bwd_kernel_traits<512, 32, 32, 4, 2, 2, 2, false, false, cutlass::bfloat16_t, Flash_kernel_traits<512, 32, 32, 4, cutlass::bfloat16_t>>, Is_dropout=false, Is_causal=true, Is_even_MN=false, Is_even_K=false, Params=Flash_bwd_params]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_bwd_launch_template.h(28): here
            instantiation of "void flash_bwd_dq_dk_dv_loop_seqk_parallel_kernel<Kernel_traits,Is_dropout,Is_causal,Is_even_MN,Is_even_K>(Flash_bwd_params) [with Kernel_traits=Flash_bwd_kernel_traits<512, 32, 32, 4, 2, 2, 2, false, false, cutlass::bfloat16_t, Flash_kernel_traits<512, 32, 32, 4, cutlass::bfloat16_t>>, Is_dropout=false, Is_causal=true, Is_even_MN=false, Is_even_K=false]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_bwd_launch_template.h(62): here
            instantiation of "void run_flash_bwd_seqk_parallel<Kernel_traits,Is_dropout>(Flash_bwd_params &, cudaStream_t, __nv_bool) [with Kernel_traits=Flash_bwd_kernel_traits<512, 32, 32, 4, 2, 2, 2, false, false, cutlass::bfloat16_t, Flash_kernel_traits<512, 32, 32, 4, cutlass::bfloat16_t>>, Is_dropout=false]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_bwd_launch_template.h(134): here
            instantiation of "void run_flash_bwd<Kernel_traits,Is_dropout>(Flash_bwd_params &, cudaStream_t, __nv_bool) [with Kernel_traits=Flash_bwd_kernel_traits<512, 32, 32, 4, 2, 2, 2, false, false, cutlass::bfloat16_t, Flash_kernel_traits<512, 32, 32, 4, cutlass::bfloat16_t>>, Is_dropout=false]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_bwd_launch_template.h(365): here
            instantiation of "void run_mha_bwd_hdim512<T>(Flash_bwd_params &, cudaStream_t, __nv_bool) [with T=cutlass::bfloat16_t]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_bwd_hdim512_bf16_sm80.cu(9): here

/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_bwd_kernel.h(853): error: static assertion failed
          detected during:
            instantiation of "void flash::compute_dq_dk_dv_1colblock<Kernel_traits,Is_dropout,Is_causal,Is_even_MN,Is_even_K,Is_first,Is_last,Seq_parallel,Params>(const Params &, int, int, int) [with Kernel_traits=Flash_bwd_kernel_traits<512, 32, 32, 4, 2, 2, 2, false, false, cutlass::bfloat16_t, Flash_kernel_traits<512, 32, 32, 4, cutlass::bfloat16_t>>, Is_dropout=false, Is_causal=false, Is_even_MN=true, Is_even_K=true, Is_first=false, Is_last=false, Seq_parallel=true, Params=Flash_bwd_params]" 
(1591): here
            instantiation of "void flash::compute_dq_dk_dv_seqk_parallel<Kernel_traits,Is_dropout,Is_causal,Is_even_MN,Is_even_K,Params>(const Params &) [with Kernel_traits=Flash_bwd_kernel_traits<512, 32, 32, 4, 2, 2, 2, false, false, cutlass::bfloat16_t, Flash_kernel_traits<512, 32, 32, 4, cutlass::bfloat16_t>>, Is_dropout=false, Is_causal=false, Is_even_MN=true, Is_even_K=true, Params=Flash_bwd_params]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_bwd_launch_template.h(28): here
            instantiation of "void flash_bwd_dq_dk_dv_loop_seqk_parallel_kernel<Kernel_traits,Is_dropout,Is_causal,Is_even_MN,Is_even_K>(Flash_bwd_params) [with Kernel_traits=Flash_bwd_kernel_traits<512, 32, 32, 4, 2, 2, 2, false, false, cutlass::bfloat16_t, Flash_kernel_traits<512, 32, 32, 4, cutlass::bfloat16_t>>, Is_dropout=false, Is_causal=false, Is_even_MN=true, Is_even_K=true]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_bwd_launch_template.h(62): here
            instantiation of "void run_flash_bwd_seqk_parallel<Kernel_traits,Is_dropout>(Flash_bwd_params &, cudaStream_t, __nv_bool) [with Kernel_traits=Flash_bwd_kernel_traits<512, 32, 32, 4, 2, 2, 2, false, false, cutlass::bfloat16_t, Flash_kernel_traits<512, 32, 32, 4, cutlass::bfloat16_t>>, Is_dropout=false]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_bwd_launch_template.h(134): here
            instantiation of "void run_flash_bwd<Kernel_traits,Is_dropout>(Flash_bwd_params &, cudaStream_t, __nv_bool) [with Kernel_traits=Flash_bwd_kernel_traits<512, 32, 32, 4, 2, 2, 2, false, false, cutlass::bfloat16_t, Flash_kernel_traits<512, 32, 32, 4, cutlass::bfloat16_t>>, Is_dropout=false]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_bwd_launch_template.h(365): here
            instantiation of "void run_mha_bwd_hdim512<T>(Flash_bwd_params &, cudaStream_t, __nv_bool) [with T=cutlass::bfloat16_t]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_bwd_hdim512_bf16_sm80.cu(9): here

/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_bwd_kernel.h(853): error: static assertion failed
          detected during:
            instantiation of "void flash::compute_dq_dk_dv_1colblock<Kernel_traits,Is_dropout,Is_causal,Is_even_MN,Is_even_K,Is_first,Is_last,Seq_parallel,Params>(const Params &, int, int, int) [with Kernel_traits=Flash_bwd_kernel_traits<512, 32, 32, 4, 2, 2, 2, false, false, cutlass::bfloat16_t, Flash_kernel_traits<512, 32, 32, 4, cutlass::bfloat16_t>>, Is_dropout=false, Is_causal=false, Is_even_MN=true, Is_even_K=false, Is_first=false, Is_last=false, Seq_parallel=true, Params=Flash_bwd_params]" 
(1591): here
            instantiation of "void flash::compute_dq_dk_dv_seqk_parallel<Kernel_traits,Is_dropout,Is_causal,Is_even_MN,Is_even_K,Params>(const Params &) [with Kernel_traits=Flash_bwd_kernel_traits<512, 32, 32, 4, 2, 2, 2, false, false, cutlass::bfloat16_t, Flash_kernel_traits<512, 32, 32, 4, cutlass::bfloat16_t>>, Is_dropout=false, Is_causal=false, Is_even_MN=true, Is_even_K=false, Params=Flash_bwd_params]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_bwd_launch_template.h(28): here
            instantiation of "void flash_bwd_dq_dk_dv_loop_seqk_parallel_kernel<Kernel_traits,Is_dropout,Is_causal,Is_even_MN,Is_even_K>(Flash_bwd_params) [with Kernel_traits=Flash_bwd_kernel_traits<512, 32, 32, 4, 2, 2, 2, false, false, cutlass::bfloat16_t, Flash_kernel_traits<512, 32, 32, 4, cutlass::bfloat16_t>>, Is_dropout=false, Is_causal=false, Is_even_MN=true, Is_even_K=false]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_bwd_launch_template.h(62): here
            instantiation of "void run_flash_bwd_seqk_parallel<Kernel_traits,Is_dropout>(Flash_bwd_params &, cudaStream_t, __nv_bool) [with Kernel_traits=Flash_bwd_kernel_traits<512, 32, 32, 4, 2, 2, 2, false, false, cutlass::bfloat16_t, Flash_kernel_traits<512, 32, 32, 4, cutlass::bfloat16_t>>, Is_dropout=false]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_bwd_launch_template.h(134): here
            instantiation of "void run_flash_bwd<Kernel_traits,Is_dropout>(Flash_bwd_params &, cudaStream_t, __nv_bool) [with Kernel_traits=Flash_bwd_kernel_traits<512, 32, 32, 4, 2, 2, 2, false, false, cutlass::bfloat16_t, Flash_kernel_traits<512, 32, 32, 4, cutlass::bfloat16_t>>, Is_dropout=false]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_bwd_launch_template.h(365): here
            instantiation of "void run_mha_bwd_hdim512<T>(Flash_bwd_params &, cudaStream_t, __nv_bool) [with T=cutlass::bfloat16_t]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_bwd_hdim512_bf16_sm80.cu(9): here

/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_bwd_kernel.h(853): error: static assertion failed
          detected during:
            instantiation of "void flash::compute_dq_dk_dv_1colblock<Kernel_traits,Is_dropout,Is_causal,Is_even_MN,Is_even_K,Is_first,Is_last,Seq_parallel,Params>(const Params &, int, int, int) [with Kernel_traits=Flash_bwd_kernel_traits<512, 32, 32, 4, 2, 2, 2, false, false, cutlass::bfloat16_t, Flash_kernel_traits<512, 32, 32, 4, cutlass::bfloat16_t>>, Is_dropout=false, Is_causal=false, Is_even_MN=false, Is_even_K=true, Is_first=false, Is_last=false, Seq_parallel=true, Params=Flash_bwd_params]" 
(1591): here
            instantiation of "void flash::compute_dq_dk_dv_seqk_parallel<Kernel_traits,Is_dropout,Is_causal,Is_even_MN,Is_even_K,Params>(const Params &) [with Kernel_traits=Flash_bwd_kernel_traits<512, 32, 32, 4, 2, 2, 2, false, false, cutlass::bfloat16_t, Flash_kernel_traits<512, 32, 32, 4, cutlass::bfloat16_t>>, Is_dropout=false, Is_causal=false, Is_even_MN=false, Is_even_K=true, Params=Flash_bwd_params]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_bwd_launch_template.h(28): here
            instantiation of "void flash_bwd_dq_dk_dv_loop_seqk_parallel_kernel<Kernel_traits,Is_dropout,Is_causal,Is_even_MN,Is_even_K>(Flash_bwd_params) [with Kernel_traits=Flash_bwd_kernel_traits<512, 32, 32, 4, 2, 2, 2, false, false, cutlass::bfloat16_t, Flash_kernel_traits<512, 32, 32, 4, cutlass::bfloat16_t>>, Is_dropout=false, Is_causal=false, Is_even_MN=false, Is_even_K=true]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_bwd_launch_template.h(62): here
            instantiation of "void run_flash_bwd_seqk_parallel<Kernel_traits,Is_dropout>(Flash_bwd_params &, cudaStream_t, __nv_bool) [with Kernel_traits=Flash_bwd_kernel_traits<512, 32, 32, 4, 2, 2, 2, false, false, cutlass::bfloat16_t, Flash_kernel_traits<512, 32, 32, 4, cutlass::bfloat16_t>>, Is_dropout=false]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_bwd_launch_template.h(134): here
            instantiation of "void run_flash_bwd<Kernel_traits,Is_dropout>(Flash_bwd_params &, cudaStream_t, __nv_bool) [with Kernel_traits=Flash_bwd_kernel_traits<512, 32, 32, 4, 2, 2, 2, false, false, cutlass::bfloat16_t, Flash_kernel_traits<512, 32, 32, 4, cutlass::bfloat16_t>>, Is_dropout=false]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_bwd_launch_template.h(365): here
            instantiation of "void run_mha_bwd_hdim512<T>(Flash_bwd_params &, cudaStream_t, __nv_bool) [with T=cutlass::bfloat16_t]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_bwd_hdim512_bf16_sm80.cu(9): here

/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_bwd_kernel.h(853): error: static assertion failed
          detected during:
            instantiation of "void flash::compute_dq_dk_dv_1colblock<Kernel_traits,Is_dropout,Is_causal,Is_even_MN,Is_even_K,Is_first,Is_last,Seq_parallel,Params>(const Params &, int, int, int) [with Kernel_traits=Flash_bwd_kernel_traits<512, 32, 32, 4, 2, 2, 2, false, false, cutlass::bfloat16_t, Flash_kernel_traits<512, 32, 32, 4, cutlass::bfloat16_t>>, Is_dropout=false, Is_causal=false, Is_even_MN=false, Is_even_K=false, Is_first=false, Is_last=false, Seq_parallel=true, Params=Flash_bwd_params]" 
(1591): here
            instantiation of "void flash::compute_dq_dk_dv_seqk_parallel<Kernel_traits,Is_dropout,Is_causal,Is_even_MN,Is_even_K,Params>(const Params &) [with Kernel_traits=Flash_bwd_kernel_traits<512, 32, 32, 4, 2, 2, 2, false, false, cutlass::bfloat16_t, Flash_kernel_traits<512, 32, 32, 4, cutlass::bfloat16_t>>, Is_dropout=false, Is_causal=false, Is_even_MN=false, Is_even_K=false, Params=Flash_bwd_params]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_bwd_launch_template.h(28): here
            instantiation of "void flash_bwd_dq_dk_dv_loop_seqk_parallel_kernel<Kernel_traits,Is_dropout,Is_causal,Is_even_MN,Is_even_K>(Flash_bwd_params) [with Kernel_traits=Flash_bwd_kernel_traits<512, 32, 32, 4, 2, 2, 2, false, false, cutlass::bfloat16_t, Flash_kernel_traits<512, 32, 32, 4, cutlass::bfloat16_t>>, Is_dropout=false, Is_causal=false, Is_even_MN=false, Is_even_K=false]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_bwd_launch_template.h(62): here
            instantiation of "void run_flash_bwd_seqk_parallel<Kernel_traits,Is_dropout>(Flash_bwd_params &, cudaStream_t, __nv_bool) [with Kernel_traits=Flash_bwd_kernel_traits<512, 32, 32, 4, 2, 2, 2, false, false, cutlass::bfloat16_t, Flash_kernel_traits<512, 32, 32, 4, cutlass::bfloat16_t>>, Is_dropout=false]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_bwd_launch_template.h(134): here
            instantiation of "void run_flash_bwd<Kernel_traits,Is_dropout>(Flash_bwd_params &, cudaStream_t, __nv_bool) [with Kernel_traits=Flash_bwd_kernel_traits<512, 32, 32, 4, 2, 2, 2, false, false, cutlass::bfloat16_t, Flash_kernel_traits<512, 32, 32, 4, cutlass::bfloat16_t>>, Is_dropout=false]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_bwd_launch_template.h(365): here
            instantiation of "void run_mha_bwd_hdim512<T>(Flash_bwd_params &, cudaStream_t, __nv_bool) [with T=cutlass::bfloat16_t]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_bwd_hdim512_bf16_sm80.cu(9): here

/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_bwd_kernel.h(853): error: static assertion failed
          detected during:
            instantiation of "void flash::compute_dq_dk_dv_1colblock<Kernel_traits,Is_dropout,Is_causal,Is_even_MN,Is_even_K,Is_first,Is_last,Seq_parallel,Params>(const Params &, int, int, int) [with Kernel_traits=Flash_bwd_kernel_traits<512, 32, 32, 4, 2, 2, 2, false, true, cutlass::bfloat16_t, Flash_kernel_traits<512, 32, 32, 4, cutlass::bfloat16_t>>, Is_dropout=false, Is_causal=true, Is_even_MN=true, Is_even_K=true, Is_first=false, Is_last=false, Seq_parallel=true, Params=Flash_bwd_params]" 
(1591): here
            instantiation of "void flash::compute_dq_dk_dv_seqk_parallel<Kernel_traits,Is_dropout,Is_causal,Is_even_MN,Is_even_K,Params>(const Params &) [with Kernel_traits=Flash_bwd_kernel_traits<512, 32, 32, 4, 2, 2, 2, false, true, cutlass::bfloat16_t, Flash_kernel_traits<512, 32, 32, 4, cutlass::bfloat16_t>>, Is_dropout=false, Is_causal=true, Is_even_MN=true, Is_even_K=true, Params=Flash_bwd_params]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_bwd_launch_template.h(28): here
            instantiation of "void flash_bwd_dq_dk_dv_loop_seqk_parallel_kernel<Kernel_traits,Is_dropout,Is_causal,Is_even_MN,Is_even_K>(Flash_bwd_params) [with Kernel_traits=Flash_bwd_kernel_traits<512, 32, 32, 4, 2, 2, 2, false, true, cutlass::bfloat16_t, Flash_kernel_traits<512, 32, 32, 4, cutlass::bfloat16_t>>, Is_dropout=false, Is_causal=true, Is_even_MN=true, Is_even_K=true]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_bwd_launch_template.h(62): here
            instantiation of "void run_flash_bwd_seqk_parallel<Kernel_traits,Is_dropout>(Flash_bwd_params &, cudaStream_t, __nv_bool) [with Kernel_traits=Flash_bwd_kernel_traits<512, 32, 32, 4, 2, 2, 2, false, true, cutlass::bfloat16_t, Flash_kernel_traits<512, 32, 32, 4, cutlass::bfloat16_t>>, Is_dropout=false]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_bwd_launch_template.h(134): here
            instantiation of "void run_flash_bwd<Kernel_traits,Is_dropout>(Flash_bwd_params &, cudaStream_t, __nv_bool) [with Kernel_traits=Flash_bwd_kernel_traits<512, 32, 32, 4, 2, 2, 2, false, true, cutlass::bfloat16_t, Flash_kernel_traits<512, 32, 32, 4, cutlass::bfloat16_t>>, Is_dropout=false]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_bwd_launch_template.h(365): here
            instantiation of "void run_mha_bwd_hdim512<T>(Flash_bwd_params &, cudaStream_t, __nv_bool) [with T=cutlass::bfloat16_t]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_bwd_hdim512_bf16_sm80.cu(9): here

/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_bwd_kernel.h(853): error: static assertion failed
          detected during:
            instantiation of "void flash::compute_dq_dk_dv_1colblock<Kernel_traits,Is_dropout,Is_causal,Is_even_MN,Is_even_K,Is_first,Is_last,Seq_parallel,Params>(const Params &, int, int, int) [with Kernel_traits=Flash_bwd_kernel_traits<512, 32, 32, 4, 2, 2, 2, false, true, cutlass::bfloat16_t, Flash_kernel_traits<512, 32, 32, 4, cutlass::bfloat16_t>>, Is_dropout=false, Is_causal=true, Is_even_MN=true, Is_even_K=false, Is_first=false, Is_last=false, Seq_parallel=true, Params=Flash_bwd_params]" 
(1591): here
            instantiation of "void flash::compute_dq_dk_dv_seqk_parallel<Kernel_traits,Is_dropout,Is_causal,Is_even_MN,Is_even_K,Params>(const Params &) [with Kernel_traits=Flash_bwd_kernel_traits<512, 32, 32, 4, 2, 2, 2, false, true, cutlass::bfloat16_t, Flash_kernel_traits<512, 32, 32, 4, cutlass::bfloat16_t>>, Is_dropout=false, Is_causal=true, Is_even_MN=true, Is_even_K=false, Params=Flash_bwd_params]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_bwd_launch_template.h(28): here
            instantiation of "void flash_bwd_dq_dk_dv_loop_seqk_parallel_kernel<Kernel_traits,Is_dropout,Is_causal,Is_even_MN,Is_even_K>(Flash_bwd_params) [with Kernel_traits=Flash_bwd_kernel_traits<512, 32, 32, 4, 2, 2, 2, false, true, cutlass::bfloat16_t, Flash_kernel_traits<512, 32, 32, 4, cutlass::bfloat16_t>>, Is_dropout=false, Is_causal=true, Is_even_MN=true, Is_even_K=false]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_bwd_launch_template.h(62): here
            instantiation of "void run_flash_bwd_seqk_parallel<Kernel_traits,Is_dropout>(Flash_bwd_params &, cudaStream_t, __nv_bool) [with Kernel_traits=Flash_bwd_kernel_traits<512, 32, 32, 4, 2, 2, 2, false, true, cutlass::bfloat16_t, Flash_kernel_traits<512, 32, 32, 4, cutlass::bfloat16_t>>, Is_dropout=false]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_bwd_launch_template.h(134): here
            instantiation of "void run_flash_bwd<Kernel_traits,Is_dropout>(Flash_bwd_params &, cudaStream_t, __nv_bool) [with Kernel_traits=Flash_bwd_kernel_traits<512, 32, 32, 4, 2, 2, 2, false, true, cutlass::bfloat16_t, Flash_kernel_traits<512, 32, 32, 4, cutlass::bfloat16_t>>, Is_dropout=false]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_bwd_launch_template.h(365): here
            instantiation of "void run_mha_bwd_hdim512<T>(Flash_bwd_params &, cudaStream_t, __nv_bool) [with T=cutlass::bfloat16_t]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_bwd_hdim512_bf16_sm80.cu(9): here

/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_bwd_kernel.h(853): error: static assertion failed
          detected during:
            instantiation of "void flash::compute_dq_dk_dv_1colblock<Kernel_traits,Is_dropout,Is_causal,Is_even_MN,Is_even_K,Is_first,Is_last,Seq_parallel,Params>(const Params &, int, int, int) [with Kernel_traits=Flash_bwd_kernel_traits<512, 32, 32, 4, 2, 2, 2, false, true, cutlass::bfloat16_t, Flash_kernel_traits<512, 32, 32, 4, cutlass::bfloat16_t>>, Is_dropout=false, Is_causal=true, Is_even_MN=false, Is_even_K=true, Is_first=false, Is_last=false, Seq_parallel=true, Params=Flash_bwd_params]" 
(1591): here
            instantiation of "void flash::compute_dq_dk_dv_seqk_parallel<Kernel_traits,Is_dropout,Is_causal,Is_even_MN,Is_even_K,Params>(const Params &) [with Kernel_traits=Flash_bwd_kernel_traits<512, 32, 32, 4, 2, 2, 2, false, true, cutlass::bfloat16_t, Flash_kernel_traits<512, 32, 32, 4, cutlass::bfloat16_t>>, Is_dropout=false, Is_causal=true, Is_even_MN=false, Is_even_K=true, Params=Flash_bwd_params]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_bwd_launch_template.h(28): here
            instantiation of "void flash_bwd_dq_dk_dv_loop_seqk_parallel_kernel<Kernel_traits,Is_dropout,Is_causal,Is_even_MN,Is_even_K>(Flash_bwd_params) [with Kernel_traits=Flash_bwd_kernel_traits<512, 32, 32, 4, 2, 2, 2, false, true, cutlass::bfloat16_t, Flash_kernel_traits<512, 32, 32, 4, cutlass::bfloat16_t>>, Is_dropout=false, Is_causal=true, Is_even_MN=false, Is_even_K=true]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_bwd_launch_template.h(62): here
            instantiation of "void run_flash_bwd_seqk_parallel<Kernel_traits,Is_dropout>(Flash_bwd_params &, cudaStream_t, __nv_bool) [with Kernel_traits=Flash_bwd_kernel_traits<512, 32, 32, 4, 2, 2, 2, false, true, cutlass::bfloat16_t, Flash_kernel_traits<512, 32, 32, 4, cutlass::bfloat16_t>>, Is_dropout=false]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_bwd_launch_template.h(134): here
            instantiation of "void run_flash_bwd<Kernel_traits,Is_dropout>(Flash_bwd_params &, cudaStream_t, __nv_bool) [with Kernel_traits=Flash_bwd_kernel_traits<512, 32, 32, 4, 2, 2, 2, false, true, cutlass::bfloat16_t, Flash_kernel_traits<512, 32, 32, 4, cutlass::bfloat16_t>>, Is_dropout=false]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_bwd_launch_template.h(365): here
            instantiation of "void run_mha_bwd_hdim512<T>(Flash_bwd_params &, cudaStream_t, __nv_bool) [with T=cutlass::bfloat16_t]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_bwd_hdim512_bf16_sm80.cu(9): here

/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_bwd_kernel.h(853): error: static assertion failed
          detected during:
            instantiation of "void flash::compute_dq_dk_dv_1colblock<Kernel_traits,Is_dropout,Is_causal,Is_even_MN,Is_even_K,Is_first,Is_last,Seq_parallel,Params>(const Params &, int, int, int) [with Kernel_traits=Flash_bwd_kernel_traits<512, 32, 32, 4, 2, 2, 2, false, true, cutlass::bfloat16_t, Flash_kernel_traits<512, 32, 32, 4, cutlass::bfloat16_t>>, Is_dropout=false, Is_causal=true, Is_even_MN=false, Is_even_K=false, Is_first=false, Is_last=false, Seq_parallel=true, Params=Flash_bwd_params]" 
(1591): here
            instantiation of "void flash::compute_dq_dk_dv_seqk_parallel<Kernel_traits,Is_dropout,Is_causal,Is_even_MN,Is_even_K,Params>(const Params &) [with Kernel_traits=Flash_bwd_kernel_traits<512, 32, 32, 4, 2, 2, 2, false, true, cutlass::bfloat16_t, Flash_kernel_traits<512, 32, 32, 4, cutlass::bfloat16_t>>, Is_dropout=false, Is_causal=true, Is_even_MN=false, Is_even_K=false, Params=Flash_bwd_params]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_bwd_launch_template.h(28): here
            instantiation of "void flash_bwd_dq_dk_dv_loop_seqk_parallel_kernel<Kernel_traits,Is_dropout,Is_causal,Is_even_MN,Is_even_K>(Flash_bwd_params) [with Kernel_traits=Flash_bwd_kernel_traits<512, 32, 32, 4, 2, 2, 2, false, true, cutlass::bfloat16_t, Flash_kernel_traits<512, 32, 32, 4, cutlass::bfloat16_t>>, Is_dropout=false, Is_causal=true, Is_even_MN=false, Is_even_K=false]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_bwd_launch_template.h(62): here
            instantiation of "void run_flash_bwd_seqk_parallel<Kernel_traits,Is_dropout>(Flash_bwd_params &, cudaStream_t, __nv_bool) [with Kernel_traits=Flash_bwd_kernel_traits<512, 32, 32, 4, 2, 2, 2, false, true, cutlass::bfloat16_t, Flash_kernel_traits<512, 32, 32, 4, cutlass::bfloat16_t>>, Is_dropout=false]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_bwd_launch_template.h(134): here
            instantiation of "void run_flash_bwd<Kernel_traits,Is_dropout>(Flash_bwd_params &, cudaStream_t, __nv_bool) [with Kernel_traits=Flash_bwd_kernel_traits<512, 32, 32, 4, 2, 2, 2, false, true, cutlass::bfloat16_t, Flash_kernel_traits<512, 32, 32, 4, cutlass::bfloat16_t>>, Is_dropout=false]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_bwd_launch_template.h(365): here
            instantiation of "void run_mha_bwd_hdim512<T>(Flash_bwd_params &, cudaStream_t, __nv_bool) [with T=cutlass::bfloat16_t]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_bwd_hdim512_bf16_sm80.cu(9): here

/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_bwd_kernel.h(853): error: static assertion failed
          detected during:
            instantiation of "void flash::compute_dq_dk_dv_1colblock<Kernel_traits,Is_dropout,Is_causal,Is_even_MN,Is_even_K,Is_first,Is_last,Seq_parallel,Params>(const Params &, int, int, int) [with Kernel_traits=Flash_bwd_kernel_traits<512, 32, 32, 4, 2, 2, 2, false, true, cutlass::bfloat16_t, Flash_kernel_traits<512, 32, 32, 4, cutlass::bfloat16_t>>, Is_dropout=false, Is_causal=false, Is_even_MN=true, Is_even_K=true, Is_first=false, Is_last=false, Seq_parallel=true, Params=Flash_bwd_params]" 
(1591): here
            instantiation of "void flash::compute_dq_dk_dv_seqk_parallel<Kernel_traits,Is_dropout,Is_causal,Is_even_MN,Is_even_K,Params>(const Params &) [with Kernel_traits=Flash_bwd_kernel_traits<512, 32, 32, 4, 2, 2, 2, false, true, cutlass::bfloat16_t, Flash_kernel_traits<512, 32, 32, 4, cutlass::bfloat16_t>>, Is_dropout=false, Is_causal=false, Is_even_MN=true, Is_even_K=true, Params=Flash_bwd_params]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_bwd_launch_template.h(28): here
            instantiation of "void flash_bwd_dq_dk_dv_loop_seqk_parallel_kernel<Kernel_traits,Is_dropout,Is_causal,Is_even_MN,Is_even_K>(Flash_bwd_params) [with Kernel_traits=Flash_bwd_kernel_traits<512, 32, 32, 4, 2, 2, 2, false, true, cutlass::bfloat16_t, Flash_kernel_traits<512, 32, 32, 4, cutlass::bfloat16_t>>, Is_dropout=false, Is_causal=false, Is_even_MN=true, Is_even_K=true]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_bwd_launch_template.h(62): here
            instantiation of "void run_flash_bwd_seqk_parallel<Kernel_traits,Is_dropout>(Flash_bwd_params &, cudaStream_t, __nv_bool) [with Kernel_traits=Flash_bwd_kernel_traits<512, 32, 32, 4, 2, 2, 2, false, true, cutlass::bfloat16_t, Flash_kernel_traits<512, 32, 32, 4, cutlass::bfloat16_t>>, Is_dropout=false]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_bwd_launch_template.h(134): here
            instantiation of "void run_flash_bwd<Kernel_traits,Is_dropout>(Flash_bwd_params &, cudaStream_t, __nv_bool) [with Kernel_traits=Flash_bwd_kernel_traits<512, 32, 32, 4, 2, 2, 2, false, true, cutlass::bfloat16_t, Flash_kernel_traits<512, 32, 32, 4, cutlass::bfloat16_t>>, Is_dropout=false]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_bwd_launch_template.h(365): here
            instantiation of "void run_mha_bwd_hdim512<T>(Flash_bwd_params &, cudaStream_t, __nv_bool) [with T=cutlass::bfloat16_t]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_bwd_hdim512_bf16_sm80.cu(9): here

/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_bwd_kernel.h(853): error: static assertion failed
          detected during:
            instantiation of "void flash::compute_dq_dk_dv_1colblock<Kernel_traits,Is_dropout,Is_causal,Is_even_MN,Is_even_K,Is_first,Is_last,Seq_parallel,Params>(const Params &, int, int, int) [with Kernel_traits=Flash_bwd_kernel_traits<512, 32, 32, 4, 2, 2, 2, false, true, cutlass::bfloat16_t, Flash_kernel_traits<512, 32, 32, 4, cutlass::bfloat16_t>>, Is_dropout=false, Is_causal=false, Is_even_MN=true, Is_even_K=false, Is_first=false, Is_last=false, Seq_parallel=true, Params=Flash_bwd_params]" 
(1591): here
            instantiation of "void flash::compute_dq_dk_dv_seqk_parallel<Kernel_traits,Is_dropout,Is_causal,Is_even_MN,Is_even_K,Params>(const Params &) [with Kernel_traits=Flash_bwd_kernel_traits<512, 32, 32, 4, 2, 2, 2, false, true, cutlass::bfloat16_t, Flash_kernel_traits<512, 32, 32, 4, cutlass::bfloat16_t>>, Is_dropout=false, Is_causal=false, Is_even_MN=true, Is_even_K=false, Params=Flash_bwd_params]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_bwd_launch_template.h(28): here
            instantiation of "void flash_bwd_dq_dk_dv_loop_seqk_parallel_kernel<Kernel_traits,Is_dropout,Is_causal,Is_even_MN,Is_even_K>(Flash_bwd_params) [with Kernel_traits=Flash_bwd_kernel_traits<512, 32, 32, 4, 2, 2, 2, false, true, cutlass::bfloat16_t, Flash_kernel_traits<512, 32, 32, 4, cutlass::bfloat16_t>>, Is_dropout=false, Is_causal=false, Is_even_MN=true, Is_even_K=false]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_bwd_launch_template.h(62): here
            instantiation of "void run_flash_bwd_seqk_parallel<Kernel_traits,Is_dropout>(Flash_bwd_params &, cudaStream_t, __nv_bool) [with Kernel_traits=Flash_bwd_kernel_traits<512, 32, 32, 4, 2, 2, 2, false, true, cutlass::bfloat16_t, Flash_kernel_traits<512, 32, 32, 4, cutlass::bfloat16_t>>, Is_dropout=false]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_bwd_launch_template.h(134): here
            instantiation of "void run_flash_bwd<Kernel_traits,Is_dropout>(Flash_bwd_params &, cudaStream_t, __nv_bool) [with Kernel_traits=Flash_bwd_kernel_traits<512, 32, 32, 4, 2, 2, 2, false, true, cutlass::bfloat16_t, Flash_kernel_traits<512, 32, 32, 4, cutlass::bfloat16_t>>, Is_dropout=false]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_bwd_launch_template.h(365): here
            instantiation of "void run_mha_bwd_hdim512<T>(Flash_bwd_params &, cudaStream_t, __nv_bool) [with T=cutlass::bfloat16_t]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_bwd_hdim512_bf16_sm80.cu(9): here

/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_bwd_kernel.h(853): error: static assertion failed
          detected during:
            instantiation of "void flash::compute_dq_dk_dv_1colblock<Kernel_traits,Is_dropout,Is_causal,Is_even_MN,Is_even_K,Is_first,Is_last,Seq_parallel,Params>(const Params &, int, int, int) [with Kernel_traits=Flash_bwd_kernel_traits<512, 32, 32, 4, 2, 2, 2, false, true, cutlass::bfloat16_t, Flash_kernel_traits<512, 32, 32, 4, cutlass::bfloat16_t>>, Is_dropout=false, Is_causal=false, Is_even_MN=false, Is_even_K=true, Is_first=false, Is_last=false, Seq_parallel=true, Params=Flash_bwd_params]" 
(1591): here
            instantiation of "void flash::compute_dq_dk_dv_seqk_parallel<Kernel_traits,Is_dropout,Is_causal,Is_even_MN,Is_even_K,Params>(const Params &) [with Kernel_traits=Flash_bwd_kernel_traits<512, 32, 32, 4, 2, 2, 2, false, true, cutlass::bfloat16_t, Flash_kernel_traits<512, 32, 32, 4, cutlass::bfloat16_t>>, Is_dropout=false, Is_causal=false, Is_even_MN=false, Is_even_K=true, Params=Flash_bwd_params]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_bwd_launch_template.h(28): here
            instantiation of "void flash_bwd_dq_dk_dv_loop_seqk_parallel_kernel<Kernel_traits,Is_dropout,Is_causal,Is_even_MN,Is_even_K>(Flash_bwd_params) [with Kernel_traits=Flash_bwd_kernel_traits<512, 32, 32, 4, 2, 2, 2, false, true, cutlass::bfloat16_t, Flash_kernel_traits<512, 32, 32, 4, cutlass::bfloat16_t>>, Is_dropout=false, Is_causal=false, Is_even_MN=false, Is_even_K=true]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_bwd_launch_template.h(62): here
            instantiation of "void run_flash_bwd_seqk_parallel<Kernel_traits,Is_dropout>(Flash_bwd_params &, cudaStream_t, __nv_bool) [with Kernel_traits=Flash_bwd_kernel_traits<512, 32, 32, 4, 2, 2, 2, false, true, cutlass::bfloat16_t, Flash_kernel_traits<512, 32, 32, 4, cutlass::bfloat16_t>>, Is_dropout=false]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_bwd_launch_template.h(134): here
            instantiation of "void run_flash_bwd<Kernel_traits,Is_dropout>(Flash_bwd_params &, cudaStream_t, __nv_bool) [with Kernel_traits=Flash_bwd_kernel_traits<512, 32, 32, 4, 2, 2, 2, false, true, cutlass::bfloat16_t, Flash_kernel_traits<512, 32, 32, 4, cutlass::bfloat16_t>>, Is_dropout=false]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_bwd_launch_template.h(365): here
            instantiation of "void run_mha_bwd_hdim512<T>(Flash_bwd_params &, cudaStream_t, __nv_bool) [with T=cutlass::bfloat16_t]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_bwd_hdim512_bf16_sm80.cu(9): here

/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_bwd_kernel.h(853): error: static assertion failed
          detected during:
            instantiation of "void flash::compute_dq_dk_dv_1colblock<Kernel_traits,Is_dropout,Is_causal,Is_even_MN,Is_even_K,Is_first,Is_last,Seq_parallel,Params>(const Params &, int, int, int) [with Kernel_traits=Flash_bwd_kernel_traits<512, 32, 32, 4, 2, 2, 2, false, true, cutlass::bfloat16_t, Flash_kernel_traits<512, 32, 32, 4, cutlass::bfloat16_t>>, Is_dropout=false, Is_causal=false, Is_even_MN=false, Is_even_K=false, Is_first=false, Is_last=false, Seq_parallel=true, Params=Flash_bwd_params]" 
(1591): here
            instantiation of "void flash::compute_dq_dk_dv_seqk_parallel<Kernel_traits,Is_dropout,Is_causal,Is_even_MN,Is_even_K,Params>(const Params &) [with Kernel_traits=Flash_bwd_kernel_traits<512, 32, 32, 4, 2, 2, 2, false, true, cutlass::bfloat16_t, Flash_kernel_traits<512, 32, 32, 4, cutlass::bfloat16_t>>, Is_dropout=false, Is_causal=false, Is_even_MN=false, Is_even_K=false, Params=Flash_bwd_params]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_bwd_launch_template.h(28): here
            instantiation of "void flash_bwd_dq_dk_dv_loop_seqk_parallel_kernel<Kernel_traits,Is_dropout,Is_causal,Is_even_MN,Is_even_K>(Flash_bwd_params) [with Kernel_traits=Flash_bwd_kernel_traits<512, 32, 32, 4, 2, 2, 2, false, true, cutlass::bfloat16_t, Flash_kernel_traits<512, 32, 32, 4, cutlass::bfloat16_t>>, Is_dropout=false, Is_causal=false, Is_even_MN=false, Is_even_K=false]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_bwd_launch_template.h(62): here
            instantiation of "void run_flash_bwd_seqk_parallel<Kernel_traits,Is_dropout>(Flash_bwd_params &, cudaStream_t, __nv_bool) [with Kernel_traits=Flash_bwd_kernel_traits<512, 32, 32, 4, 2, 2, 2, false, true, cutlass::bfloat16_t, Flash_kernel_traits<512, 32, 32, 4, cutlass::bfloat16_t>>, Is_dropout=false]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_bwd_launch_template.h(134): here
            instantiation of "void run_flash_bwd<Kernel_traits,Is_dropout>(Flash_bwd_params &, cudaStream_t, __nv_bool) [with Kernel_traits=Flash_bwd_kernel_traits<512, 32, 32, 4, 2, 2, 2, false, true, cutlass::bfloat16_t, Flash_kernel_traits<512, 32, 32, 4, cutlass::bfloat16_t>>, Is_dropout=false]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_bwd_launch_template.h(365): here
            instantiation of "void run_mha_bwd_hdim512<T>(Flash_bwd_params &, cudaStream_t, __nv_bool) [with T=cutlass::bfloat16_t]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_bwd_hdim512_bf16_sm80.cu(9): here

40 errors detected in the compilation of "/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_bwd_hdim512_bf16_sm80.cu".
[3/36] /usr/local/cuda-11.4/bin/nvcc  -I/home/v-feiychen/flash-attention/csrc/flash_attn -I/home/v-feiychen/flash-attention/csrc/flash_attn/src -I/home/v-feiychen/flash-attention/csrc/cutlass/include -I/anaconda/envs/flashattention/lib/python3.9/site-packages/torch/include -I/anaconda/envs/flashattention/lib/python3.9/site-packages/torch/include/torch/csrc/api/include -I/anaconda/envs/flashattention/lib/python3.9/site-packages/torch/include/TH -I/anaconda/envs/flashattention/lib/python3.9/site-packages/torch/include/THC -I/usr/local/cuda-11.4/include -I/anaconda/envs/flashattention/include/python3.9 -c -c /home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_bwd_hdim224_fp16_sm80.cu -o /home/v-feiychen/flash-attention/build/temp.linux-x86_64-cpython-39/csrc/flash_attn/src/flash_bwd_hdim224_fp16_sm80.o -D__CUDA_NO_HALF_OPERATORS__ -D__CUDA_NO_HALF_CONVERSIONS__ -D__CUDA_NO_BFLOAT16_CONVERSIONS__ -D__CUDA_NO_HALF2_OPERATORS__ --expt-relaxed-constexpr --compiler-options ''"'"'-fPIC'"'"'' -O3 -std=c++17 -U__CUDA_NO_HALF_OPERATORS__ -U__CUDA_NO_HALF_CONVERSIONS__ -U__CUDA_NO_HALF2_OPERATORS__ -U__CUDA_NO_BFLOAT16_CONVERSIONS__ --expt-relaxed-constexpr --expt-extended-lambda --use_fast_math --ptxas-options=-v -lineinfo -gencode arch=compute_80,code=sm_80 --threads 4 -DTORCH_API_INCLUDE_EXTENSION_H '-DPYBIND11_COMPILER_TYPE="_gcc"' '-DPYBIND11_STDLIB="_libstdcpp"' '-DPYBIND11_BUILD_ABI="_cxxabi1011"' -DTORCH_EXTENSION_NAME=flash_attn_2_cuda -D_GLIBCXX_USE_CXX11_ABI=0
/anaconda/envs/flashattention/lib/python3.9/site-packages/torch/include/c10/core/SymInt.h(84): warning: integer conversion resulted in a change of sign

ptxas info    : 2 bytes gmem
ptxas info    : Compiling entry function '_Z44flash_bwd_dq_dk_dv_loop_seqk_parallel_kernelI23Flash_bwd_kernel_traitsILi224ELi64ELi64ELi8ELi4ELi4ELi4ELb0ELb0EN7cutlass6half_tE19Flash_kernel_traitsILi224ELi64ELi64ELi8ES2_EELb0ELb0ELb0ELb0EEv16Flash_bwd_params' for 'sm_80'
ptxas info    : Function properties for _Z44flash_bwd_dq_dk_dv_loop_seqk_parallel_kernelI23Flash_bwd_kernel_traitsILi224ELi64ELi64ELi8ELi4ELi4ELi4ELb0ELb0EN7cutlass6half_tE19Flash_kernel_traitsILi224ELi64ELi64ELi8ES2_EELb0ELb0ELb0ELb0EEv16Flash_bwd_params
    0 bytes stack frame, 0 bytes spill stores, 0 bytes spill loads
ptxas info    : Used 246 registers, 696 bytes cmem[0]
ptxas info    : Compiling entry function '_Z44flash_bwd_dq_dk_dv_loop_seqk_parallel_kernelI23Flash_bwd_kernel_traitsILi224ELi64ELi64ELi8ELi4ELi4ELi4ELb0ELb0EN7cutlass6half_tE19Flash_kernel_traitsILi224ELi64ELi64ELi8ES2_EELb0ELb0ELb0ELb1EEv16Flash_bwd_params' for 'sm_80'
ptxas info    : Function properties for _Z44flash_bwd_dq_dk_dv_loop_seqk_parallel_kernelI23Flash_bwd_kernel_traitsILi224ELi64ELi64ELi8ELi4ELi4ELi4ELb0ELb0EN7cutlass6half_tE19Flash_kernel_traitsILi224ELi64ELi64ELi8ES2_EELb0ELb0ELb0ELb1EEv16Flash_bwd_params
    0 bytes stack frame, 0 bytes spill stores, 0 bytes spill loads
ptxas info    : Used 246 registers, 696 bytes cmem[0]
ptxas info    : Compiling entry function '_Z44flash_bwd_dq_dk_dv_loop_seqk_parallel_kernelI23Flash_bwd_kernel_traitsILi224ELi64ELi64ELi8ELi4ELi4ELi4ELb0ELb0EN7cutlass6half_tE19Flash_kernel_traitsILi224ELi64ELi64ELi8ES2_EELb0ELb0ELb1ELb0EEv16Flash_bwd_params' for 'sm_80'
ptxas info    : Function properties for _Z44flash_bwd_dq_dk_dv_loop_seqk_parallel_kernelI23Flash_bwd_kernel_traitsILi224ELi64ELi64ELi8ELi4ELi4ELi4ELb0ELb0EN7cutlass6half_tE19Flash_kernel_traitsILi224ELi64ELi64ELi8ES2_EELb0ELb0ELb1ELb0EEv16Flash_bwd_params
    0 bytes stack frame, 0 bytes spill stores, 0 bytes spill loads
ptxas info    : Used 246 registers, 696 bytes cmem[0]
ptxas info    : Compiling entry function '_Z44flash_bwd_dq_dk_dv_loop_seqk_parallel_kernelI23Flash_bwd_kernel_traitsILi224ELi64ELi64ELi8ELi4ELi4ELi4ELb0ELb0EN7cutlass6half_tE19Flash_kernel_traitsILi224ELi64ELi64ELi8ES2_EELb0ELb0ELb1ELb1EEv16Flash_bwd_params' for 'sm_80'
ptxas info    : Function properties for _Z44flash_bwd_dq_dk_dv_loop_seqk_parallel_kernelI23Flash_bwd_kernel_traitsILi224ELi64ELi64ELi8ELi4ELi4ELi4ELb0ELb0EN7cutlass6half_tE19Flash_kernel_traitsILi224ELi64ELi64ELi8ES2_EELb0ELb0ELb1ELb1EEv16Flash_bwd_params
    0 bytes stack frame, 0 bytes spill stores, 0 bytes spill loads
ptxas info    : Used 242 registers, 696 bytes cmem[0]
ptxas info    : Compiling entry function '_Z44flash_bwd_dq_dk_dv_loop_seqk_parallel_kernelI23Flash_bwd_kernel_traitsILi224ELi64ELi64ELi8ELi4ELi4ELi4ELb0ELb0EN7cutlass6half_tE19Flash_kernel_traitsILi224ELi64ELi64ELi8ES2_EELb0ELb1ELb0ELb0EEv16Flash_bwd_params' for 'sm_80'
ptxas info    : Function properties for _Z44flash_bwd_dq_dk_dv_loop_seqk_parallel_kernelI23Flash_bwd_kernel_traitsILi224ELi64ELi64ELi8ELi4ELi4ELi4ELb0ELb0EN7cutlass6half_tE19Flash_kernel_traitsILi224ELi64ELi64ELi8ES2_EELb0ELb1ELb0ELb0EEv16Flash_bwd_params
    0 bytes stack frame, 0 bytes spill stores, 0 bytes spill loads
ptxas info    : Used 252 registers, 696 bytes cmem[0]
ptxas info    : Compiling entry function '_Z44flash_bwd_dq_dk_dv_loop_seqk_parallel_kernelI23Flash_bwd_kernel_traitsILi224ELi64ELi64ELi8ELi4ELi4ELi4ELb0ELb0EN7cutlass6half_tE19Flash_kernel_traitsILi224ELi64ELi64ELi8ES2_EELb0ELb1ELb0ELb1EEv16Flash_bwd_params' for 'sm_80'
ptxas info    : Function properties for _Z44flash_bwd_dq_dk_dv_loop_seqk_parallel_kernelI23Flash_bwd_kernel_traitsILi224ELi64ELi64ELi8ELi4ELi4ELi4ELb0ELb0EN7cutlass6half_tE19Flash_kernel_traitsILi224ELi64ELi64ELi8ES2_EELb0ELb1ELb0ELb1EEv16Flash_bwd_params
    0 bytes stack frame, 0 bytes spill stores, 0 bytes spill loads
ptxas info    : Used 248 registers, 696 bytes cmem[0]
ptxas info    : Compiling entry function '_Z44flash_bwd_dq_dk_dv_loop_seqk_parallel_kernelI23Flash_bwd_kernel_traitsILi224ELi64ELi64ELi8ELi4ELi4ELi4ELb0ELb0EN7cutlass6half_tE19Flash_kernel_traitsILi224ELi64ELi64ELi8ES2_EELb0ELb1ELb1ELb0EEv16Flash_bwd_params' for 'sm_80'
ptxas info    : Function properties for _Z44flash_bwd_dq_dk_dv_loop_seqk_parallel_kernelI23Flash_bwd_kernel_traitsILi224ELi64ELi64ELi8ELi4ELi4ELi4ELb0ELb0EN7cutlass6half_tE19Flash_kernel_traitsILi224ELi64ELi64ELi8ES2_EELb0ELb1ELb1ELb0EEv16Flash_bwd_params
    0 bytes stack frame, 0 bytes spill stores, 0 bytes spill loads
ptxas info    : Used 246 registers, 696 bytes cmem[0]
ptxas info    : Compiling entry function '_Z44flash_bwd_dq_dk_dv_loop_seqk_parallel_kernelI23Flash_bwd_kernel_traitsILi224ELi64ELi64ELi8ELi4ELi4ELi4ELb0ELb0EN7cutlass6half_tE19Flash_kernel_traitsILi224ELi64ELi64ELi8ES2_EELb0ELb1ELb1ELb1EEv16Flash_bwd_params' for 'sm_80'
ptxas info    : Function properties for _Z44flash_bwd_dq_dk_dv_loop_seqk_parallel_kernelI23Flash_bwd_kernel_traitsILi224ELi64ELi64ELi8ELi4ELi4ELi4ELb0ELb0EN7cutlass6half_tE19Flash_kernel_traitsILi224ELi64ELi64ELi8ES2_EELb0ELb1ELb1ELb1EEv16Flash_bwd_params
    0 bytes stack frame, 0 bytes spill stores, 0 bytes spill loads
ptxas info    : Used 244 registers, 696 bytes cmem[0]
ptxas info    : Compiling entry function '_Z27flash_bwd_convert_dq_kernelI23Flash_bwd_kernel_traitsILi224ELi64ELi64ELi8ELi4ELi4ELi4ELb0ELb0EN7cutlass6half_tE19Flash_kernel_traitsILi224ELi64ELi64ELi8ES2_EEEv16Flash_bwd_params' for 'sm_80'
ptxas info    : Function properties for _Z27flash_bwd_convert_dq_kernelI23Flash_bwd_kernel_traitsILi224ELi64ELi64ELi8ELi4ELi4ELi4ELb0ELb0EN7cutlass6half_tE19Flash_kernel_traitsILi224ELi64ELi64ELi8ES2_EEEv16Flash_bwd_params
    0 bytes stack frame, 0 bytes spill stores, 0 bytes spill loads
ptxas info    : Used 32 registers, 696 bytes cmem[0]
ptxas info    : Compiling entry function '_Z44flash_bwd_dq_dk_dv_loop_seqk_parallel_kernelI23Flash_bwd_kernel_traitsILi224ELi64ELi64ELi8ELi4ELi4ELi4ELb0ELb0EN7cutlass6half_tE19Flash_kernel_traitsILi224ELi64ELi64ELi8ES2_EELb1ELb0ELb0ELb0EEv16Flash_bwd_params' for 'sm_80'
ptxas info    : Function properties for _Z44flash_bwd_dq_dk_dv_loop_seqk_parallel_kernelI23Flash_bwd_kernel_traitsILi224ELi64ELi64ELi8ELi4ELi4ELi4ELb0ELb0EN7cutlass6half_tE19Flash_kernel_traitsILi224ELi64ELi64ELi8ES2_EELb1ELb0ELb0ELb0EEv16Flash_bwd_params
    0 bytes stack frame, 0 bytes spill stores, 0 bytes spill loads
ptxas info    : Used 248 registers, 696 bytes cmem[0]
ptxas info    : Compiling entry function '_Z44flash_bwd_dq_dk_dv_loop_seqk_parallel_kernelI23Flash_bwd_kernel_traitsILi224ELi64ELi64ELi8ELi4ELi4ELi4ELb0ELb0EN7cutlass6half_tE19Flash_kernel_traitsILi224ELi64ELi64ELi8ES2_EELb1ELb0ELb0ELb1EEv16Flash_bwd_params' for 'sm_80'
ptxas info    : Function properties for _Z44flash_bwd_dq_dk_dv_loop_seqk_parallel_kernelI23Flash_bwd_kernel_traitsILi224ELi64ELi64ELi8ELi4ELi4ELi4ELb0ELb0EN7cutlass6half_tE19Flash_kernel_traitsILi224ELi64ELi64ELi8ES2_EELb1ELb0ELb0ELb1EEv16Flash_bwd_params
    0 bytes stack frame, 0 bytes spill stores, 0 bytes spill loads
ptxas info    : Used 246 registers, 696 bytes cmem[0]
ptxas info    : Compiling entry function '_Z44flash_bwd_dq_dk_dv_loop_seqk_parallel_kernelI23Flash_bwd_kernel_traitsILi224ELi64ELi64ELi8ELi4ELi4ELi4ELb0ELb0EN7cutlass6half_tE19Flash_kernel_traitsILi224ELi64ELi64ELi8ES2_EELb1ELb0ELb1ELb0EEv16Flash_bwd_params' for 'sm_80'
ptxas info    : Function properties for _Z44flash_bwd_dq_dk_dv_loop_seqk_parallel_kernelI23Flash_bwd_kernel_traitsILi224ELi64ELi64ELi8ELi4ELi4ELi4ELb0ELb0EN7cutlass6half_tE19Flash_kernel_traitsILi224ELi64ELi64ELi8ES2_EELb1ELb0ELb1ELb0EEv16Flash_bwd_params
    96 bytes stack frame, 104 bytes spill stores, 104 bytes spill loads
ptxas info    : Used 255 registers, 696 bytes cmem[0]
ptxas info    : Compiling entry function '_Z44flash_bwd_dq_dk_dv_loop_seqk_parallel_kernelI23Flash_bwd_kernel_traitsILi224ELi64ELi64ELi8ELi4ELi4ELi4ELb0ELb0EN7cutlass6half_tE19Flash_kernel_traitsILi224ELi64ELi64ELi8ES2_EELb1ELb0ELb1ELb1EEv16Flash_bwd_params' for 'sm_80'
ptxas info    : Function properties for _Z44flash_bwd_dq_dk_dv_loop_seqk_parallel_kernelI23Flash_bwd_kernel_traitsILi224ELi64ELi64ELi8ELi4ELi4ELi4ELb0ELb0EN7cutlass6half_tE19Flash_kernel_traitsILi224ELi64ELi64ELi8ES2_EELb1ELb0ELb1ELb1EEv16Flash_bwd_params
    0 bytes stack frame, 0 bytes spill stores, 0 bytes spill loads
ptxas info    : Used 246 registers, 696 bytes cmem[0]
ptxas info    : Compiling entry function '_Z44flash_bwd_dq_dk_dv_loop_seqk_parallel_kernelI23Flash_bwd_kernel_traitsILi224ELi64ELi64ELi8ELi4ELi4ELi4ELb0ELb0EN7cutlass6half_tE19Flash_kernel_traitsILi224ELi64ELi64ELi8ES2_EELb1ELb1ELb0ELb0EEv16Flash_bwd_params' for 'sm_80'
ptxas info    : Function properties for _Z44flash_bwd_dq_dk_dv_loop_seqk_parallel_kernelI23Flash_bwd_kernel_traitsILi224ELi64ELi64ELi8ELi4ELi4ELi4ELb0ELb0EN7cutlass6half_tE19Flash_kernel_traitsILi224ELi64ELi64ELi8ES2_EELb1ELb1ELb0ELb0EEv16Flash_bwd_params
    16 bytes stack frame, 12 bytes spill stores, 12 bytes spill loads
ptxas info    : Used 255 registers, 696 bytes cmem[0]
ptxas info    : Compiling entry function '_Z44flash_bwd_dq_dk_dv_loop_seqk_parallel_kernelI23Flash_bwd_kernel_traitsILi224ELi64ELi64ELi8ELi4ELi4ELi4ELb0ELb0EN7cutlass6half_tE19Flash_kernel_traitsILi224ELi64ELi64ELi8ES2_EELb1ELb1ELb0ELb1EEv16Flash_bwd_params' for 'sm_80'
ptxas info    : Function properties for _Z44flash_bwd_dq_dk_dv_loop_seqk_parallel_kernelI23Flash_bwd_kernel_traitsILi224ELi64ELi64ELi8ELi4ELi4ELi4ELb0ELb0EN7cutlass6half_tE19Flash_kernel_traitsILi224ELi64ELi64ELi8ES2_EELb1ELb1ELb0ELb1EEv16Flash_bwd_params
    0 bytes stack frame, 0 bytes spill stores, 0 bytes spill loads
ptxas info    : Used 248 registers, 696 bytes cmem[0]
ptxas info    : Compiling entry function '_Z44flash_bwd_dq_dk_dv_loop_seqk_parallel_kernelI23Flash_bwd_kernel_traitsILi224ELi64ELi64ELi8ELi4ELi4ELi4ELb0ELb0EN7cutlass6half_tE19Flash_kernel_traitsILi224ELi64ELi64ELi8ES2_EELb1ELb1ELb1ELb0EEv16Flash_bwd_params' for 'sm_80'
ptxas info    : Function properties for _Z44flash_bwd_dq_dk_dv_loop_seqk_parallel_kernelI23Flash_bwd_kernel_traitsILi224ELi64ELi64ELi8ELi4ELi4ELi4ELb0ELb0EN7cutlass6half_tE19Flash_kernel_traitsILi224ELi64ELi64ELi8ES2_EELb1ELb1ELb1ELb0EEv16Flash_bwd_params
    80 bytes stack frame, 92 bytes spill stores, 112 bytes spill loads
ptxas info    : Used 255 registers, 696 bytes cmem[0]
ptxas info    : Compiling entry function '_Z44flash_bwd_dq_dk_dv_loop_seqk_parallel_kernelI23Flash_bwd_kernel_traitsILi224ELi64ELi64ELi8ELi4ELi4ELi4ELb0ELb0EN7cutlass6half_tE19Flash_kernel_traitsILi224ELi64ELi64ELi8ES2_EELb1ELb1ELb1ELb1EEv16Flash_bwd_params' for 'sm_80'
ptxas info    : Function properties for _Z44flash_bwd_dq_dk_dv_loop_seqk_parallel_kernelI23Flash_bwd_kernel_traitsILi224ELi64ELi64ELi8ELi4ELi4ELi4ELb0ELb0EN7cutlass6half_tE19Flash_kernel_traitsILi224ELi64ELi64ELi8ES2_EELb1ELb1ELb1ELb1EEv16Flash_bwd_params
    0 bytes stack frame, 0 bytes spill stores, 0 bytes spill loads
ptxas info    : Used 248 registers, 696 bytes cmem[0]
ptxas info    : Compiling entry function '_Z25flash_bwd_dot_do_o_kernelILb1E23Flash_bwd_kernel_traitsILi224ELi64ELi64ELi8ELi4ELi4ELi4ELb0ELb0EN7cutlass6half_tE19Flash_kernel_traitsILi224ELi64ELi64ELi8ES2_EEEv16Flash_bwd_params' for 'sm_80'
ptxas info    : Function properties for _Z25flash_bwd_dot_do_o_kernelILb1E23Flash_bwd_kernel_traitsILi224ELi64ELi64ELi8ELi4ELi4ELi4ELb0ELb0EN7cutlass6half_tE19Flash_kernel_traitsILi224ELi64ELi64ELi8ES2_EEEv16Flash_bwd_params
    0 bytes stack frame, 0 bytes spill stores, 0 bytes spill loads
ptxas info    : Used 79 registers, 696 bytes cmem[0]
/anaconda/envs/flashattention/lib/python3.9/site-packages/torch/include/c10/core/SymInt.h(84): warning: integer conversion resulted in a change of sign

[4/36] /usr/local/cuda-11.4/bin/nvcc  -I/home/v-feiychen/flash-attention/csrc/flash_attn -I/home/v-feiychen/flash-attention/csrc/flash_attn/src -I/home/v-feiychen/flash-attention/csrc/cutlass/include -I/anaconda/envs/flashattention/lib/python3.9/site-packages/torch/include -I/anaconda/envs/flashattention/lib/python3.9/site-packages/torch/include/torch/csrc/api/include -I/anaconda/envs/flashattention/lib/python3.9/site-packages/torch/include/TH -I/anaconda/envs/flashattention/lib/python3.9/site-packages/torch/include/THC -I/usr/local/cuda-11.4/include -I/anaconda/envs/flashattention/include/python3.9 -c -c /home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_bwd_hdim224_bf16_sm80.cu -o /home/v-feiychen/flash-attention/build/temp.linux-x86_64-cpython-39/csrc/flash_attn/src/flash_bwd_hdim224_bf16_sm80.o -D__CUDA_NO_HALF_OPERATORS__ -D__CUDA_NO_HALF_CONVERSIONS__ -D__CUDA_NO_BFLOAT16_CONVERSIONS__ -D__CUDA_NO_HALF2_OPERATORS__ --expt-relaxed-constexpr --compiler-options ''"'"'-fPIC'"'"'' -O3 -std=c++17 -U__CUDA_NO_HALF_OPERATORS__ -U__CUDA_NO_HALF_CONVERSIONS__ -U__CUDA_NO_HALF2_OPERATORS__ -U__CUDA_NO_BFLOAT16_CONVERSIONS__ --expt-relaxed-constexpr --expt-extended-lambda --use_fast_math --ptxas-options=-v -lineinfo -gencode arch=compute_80,code=sm_80 --threads 4 -DTORCH_API_INCLUDE_EXTENSION_H '-DPYBIND11_COMPILER_TYPE="_gcc"' '-DPYBIND11_STDLIB="_libstdcpp"' '-DPYBIND11_BUILD_ABI="_cxxabi1011"' -DTORCH_EXTENSION_NAME=flash_attn_2_cuda -D_GLIBCXX_USE_CXX11_ABI=0
/anaconda/envs/flashattention/lib/python3.9/site-packages/torch/include/c10/core/SymInt.h(84): warning: integer conversion resulted in a change of sign

ptxas info    : 2 bytes gmem
ptxas info    : Compiling entry function '_Z44flash_bwd_dq_dk_dv_loop_seqk_parallel_kernelI23Flash_bwd_kernel_traitsILi224ELi64ELi64ELi8ELi4ELi4ELi4ELb0ELb0EN7cutlass10bfloat16_tE19Flash_kernel_traitsILi224ELi64ELi64ELi8ES2_EELb0ELb0ELb0ELb0EEv16Flash_bwd_params' for 'sm_80'
ptxas info    : Function properties for _Z44flash_bwd_dq_dk_dv_loop_seqk_parallel_kernelI23Flash_bwd_kernel_traitsILi224ELi64ELi64ELi8ELi4ELi4ELi4ELb0ELb0EN7cutlass10bfloat16_tE19Flash_kernel_traitsILi224ELi64ELi64ELi8ES2_EELb0ELb0ELb0ELb0EEv16Flash_bwd_params
    0 bytes stack frame, 0 bytes spill stores, 0 bytes spill loads
ptxas info    : Used 246 registers, 696 bytes cmem[0]
ptxas info    : Compiling entry function '_Z44flash_bwd_dq_dk_dv_loop_seqk_parallel_kernelI23Flash_bwd_kernel_traitsILi224ELi64ELi64ELi8ELi4ELi4ELi4ELb0ELb0EN7cutlass10bfloat16_tE19Flash_kernel_traitsILi224ELi64ELi64ELi8ES2_EELb0ELb0ELb0ELb1EEv16Flash_bwd_params' for 'sm_80'
ptxas info    : Function properties for _Z44flash_bwd_dq_dk_dv_loop_seqk_parallel_kernelI23Flash_bwd_kernel_traitsILi224ELi64ELi64ELi8ELi4ELi4ELi4ELb0ELb0EN7cutlass10bfloat16_tE19Flash_kernel_traitsILi224ELi64ELi64ELi8ES2_EELb0ELb0ELb0ELb1EEv16Flash_bwd_params
    0 bytes stack frame, 0 bytes spill stores, 0 bytes spill loads
ptxas info    : Used 246 registers, 696 bytes cmem[0]
ptxas info    : Compiling entry function '_Z44flash_bwd_dq_dk_dv_loop_seqk_parallel_kernelI23Flash_bwd_kernel_traitsILi224ELi64ELi64ELi8ELi4ELi4ELi4ELb0ELb0EN7cutlass10bfloat16_tE19Flash_kernel_traitsILi224ELi64ELi64ELi8ES2_EELb0ELb0ELb1ELb0EEv16Flash_bwd_params' for 'sm_80'
ptxas info    : Function properties for _Z44flash_bwd_dq_dk_dv_loop_seqk_parallel_kernelI23Flash_bwd_kernel_traitsILi224ELi64ELi64ELi8ELi4ELi4ELi4ELb0ELb0EN7cutlass10bfloat16_tE19Flash_kernel_traitsILi224ELi64ELi64ELi8ES2_EELb0ELb0ELb1ELb0EEv16Flash_bwd_params
    0 bytes stack frame, 0 bytes spill stores, 0 bytes spill loads
ptxas info    : Used 246 registers, 696 bytes cmem[0]
ptxas info    : Compiling entry function '_Z44flash_bwd_dq_dk_dv_loop_seqk_parallel_kernelI23Flash_bwd_kernel_traitsILi224ELi64ELi64ELi8ELi4ELi4ELi4ELb0ELb0EN7cutlass10bfloat16_tE19Flash_kernel_traitsILi224ELi64ELi64ELi8ES2_EELb0ELb0ELb1ELb1EEv16Flash_bwd_params' for 'sm_80'
ptxas info    : Function properties for _Z44flash_bwd_dq_dk_dv_loop_seqk_parallel_kernelI23Flash_bwd_kernel_traitsILi224ELi64ELi64ELi8ELi4ELi4ELi4ELb0ELb0EN7cutlass10bfloat16_tE19Flash_kernel_traitsILi224ELi64ELi64ELi8ES2_EELb0ELb0ELb1ELb1EEv16Flash_bwd_params
    0 bytes stack frame, 0 bytes spill stores, 0 bytes spill loads
ptxas info    : Used 242 registers, 696 bytes cmem[0]
ptxas info    : Compiling entry function '_Z44flash_bwd_dq_dk_dv_loop_seqk_parallel_kernelI23Flash_bwd_kernel_traitsILi224ELi64ELi64ELi8ELi4ELi4ELi4ELb0ELb0EN7cutlass10bfloat16_tE19Flash_kernel_traitsILi224ELi64ELi64ELi8ES2_EELb0ELb1ELb0ELb0EEv16Flash_bwd_params' for 'sm_80'
ptxas info    : Function properties for _Z44flash_bwd_dq_dk_dv_loop_seqk_parallel_kernelI23Flash_bwd_kernel_traitsILi224ELi64ELi64ELi8ELi4ELi4ELi4ELb0ELb0EN7cutlass10bfloat16_tE19Flash_kernel_traitsILi224ELi64ELi64ELi8ES2_EELb0ELb1ELb0ELb0EEv16Flash_bwd_params
    0 bytes stack frame, 0 bytes spill stores, 0 bytes spill loads
ptxas info    : Used 252 registers, 696 bytes cmem[0]
ptxas info    : Compiling entry function '_Z44flash_bwd_dq_dk_dv_loop_seqk_parallel_kernelI23Flash_bwd_kernel_traitsILi224ELi64ELi64ELi8ELi4ELi4ELi4ELb0ELb0EN7cutlass10bfloat16_tE19Flash_kernel_traitsILi224ELi64ELi64ELi8ES2_EELb0ELb1ELb0ELb1EEv16Flash_bwd_params' for 'sm_80'
ptxas info    : Function properties for _Z44flash_bwd_dq_dk_dv_loop_seqk_parallel_kernelI23Flash_bwd_kernel_traitsILi224ELi64ELi64ELi8ELi4ELi4ELi4ELb0ELb0EN7cutlass10bfloat16_tE19Flash_kernel_traitsILi224ELi64ELi64ELi8ES2_EELb0ELb1ELb0ELb1EEv16Flash_bwd_params
    0 bytes stack frame, 0 bytes spill stores, 0 bytes spill loads
ptxas info    : Used 248 registers, 696 bytes cmem[0]
ptxas info    : Compiling entry function '_Z44flash_bwd_dq_dk_dv_loop_seqk_parallel_kernelI23Flash_bwd_kernel_traitsILi224ELi64ELi64ELi8ELi4ELi4ELi4ELb0ELb0EN7cutlass10bfloat16_tE19Flash_kernel_traitsILi224ELi64ELi64ELi8ES2_EELb0ELb1ELb1ELb0EEv16Flash_bwd_params' for 'sm_80'
ptxas info    : Function properties for _Z44flash_bwd_dq_dk_dv_loop_seqk_parallel_kernelI23Flash_bwd_kernel_traitsILi224ELi64ELi64ELi8ELi4ELi4ELi4ELb0ELb0EN7cutlass10bfloat16_tE19Flash_kernel_traitsILi224ELi64ELi64ELi8ES2_EELb0ELb1ELb1ELb0EEv16Flash_bwd_params
    0 bytes stack frame, 0 bytes spill stores, 0 bytes spill loads
ptxas info    : Used 246 registers, 696 bytes cmem[0]
ptxas info    : Compiling entry function '_Z44flash_bwd_dq_dk_dv_loop_seqk_parallel_kernelI23Flash_bwd_kernel_traitsILi224ELi64ELi64ELi8ELi4ELi4ELi4ELb0ELb0EN7cutlass10bfloat16_tE19Flash_kernel_traitsILi224ELi64ELi64ELi8ES2_EELb0ELb1ELb1ELb1EEv16Flash_bwd_params' for 'sm_80'
ptxas info    : Function properties for _Z44flash_bwd_dq_dk_dv_loop_seqk_parallel_kernelI23Flash_bwd_kernel_traitsILi224ELi64ELi64ELi8ELi4ELi4ELi4ELb0ELb0EN7cutlass10bfloat16_tE19Flash_kernel_traitsILi224ELi64ELi64ELi8ES2_EELb0ELb1ELb1ELb1EEv16Flash_bwd_params
    0 bytes stack frame, 0 bytes spill stores, 0 bytes spill loads
ptxas info    : Used 244 registers, 696 bytes cmem[0]
ptxas info    : Compiling entry function '_Z27flash_bwd_convert_dq_kernelI23Flash_bwd_kernel_traitsILi224ELi64ELi64ELi8ELi4ELi4ELi4ELb0ELb0EN7cutlass10bfloat16_tE19Flash_kernel_traitsILi224ELi64ELi64ELi8ES2_EEEv16Flash_bwd_params' for 'sm_80'
ptxas info    : Function properties for _Z27flash_bwd_convert_dq_kernelI23Flash_bwd_kernel_traitsILi224ELi64ELi64ELi8ELi4ELi4ELi4ELb0ELb0EN7cutlass10bfloat16_tE19Flash_kernel_traitsILi224ELi64ELi64ELi8ES2_EEEv16Flash_bwd_params
    0 bytes stack frame, 0 bytes spill stores, 0 bytes spill loads
ptxas info    : Used 32 registers, 696 bytes cmem[0]
ptxas info    : Compiling entry function '_Z44flash_bwd_dq_dk_dv_loop_seqk_parallel_kernelI23Flash_bwd_kernel_traitsILi224ELi64ELi64ELi8ELi4ELi4ELi4ELb0ELb0EN7cutlass10bfloat16_tE19Flash_kernel_traitsILi224ELi64ELi64ELi8ES2_EELb1ELb0ELb0ELb0EEv16Flash_bwd_params' for 'sm_80'
ptxas info    : Function properties for _Z44flash_bwd_dq_dk_dv_loop_seqk_parallel_kernelI23Flash_bwd_kernel_traitsILi224ELi64ELi64ELi8ELi4ELi4ELi4ELb0ELb0EN7cutlass10bfloat16_tE19Flash_kernel_traitsILi224ELi64ELi64ELi8ES2_EELb1ELb0ELb0ELb0EEv16Flash_bwd_params
    0 bytes stack frame, 0 bytes spill stores, 0 bytes spill loads
ptxas info    : Used 248 registers, 696 bytes cmem[0]
ptxas info    : Compiling entry function '_Z44flash_bwd_dq_dk_dv_loop_seqk_parallel_kernelI23Flash_bwd_kernel_traitsILi224ELi64ELi64ELi8ELi4ELi4ELi4ELb0ELb0EN7cutlass10bfloat16_tE19Flash_kernel_traitsILi224ELi64ELi64ELi8ES2_EELb1ELb0ELb0ELb1EEv16Flash_bwd_params' for 'sm_80'
ptxas info    : Function properties for _Z44flash_bwd_dq_dk_dv_loop_seqk_parallel_kernelI23Flash_bwd_kernel_traitsILi224ELi64ELi64ELi8ELi4ELi4ELi4ELb0ELb0EN7cutlass10bfloat16_tE19Flash_kernel_traitsILi224ELi64ELi64ELi8ES2_EELb1ELb0ELb0ELb1EEv16Flash_bwd_params
    0 bytes stack frame, 0 bytes spill stores, 0 bytes spill loads
ptxas info    : Used 246 registers, 696 bytes cmem[0]
ptxas info    : Compiling entry function '_Z44flash_bwd_dq_dk_dv_loop_seqk_parallel_kernelI23Flash_bwd_kernel_traitsILi224ELi64ELi64ELi8ELi4ELi4ELi4ELb0ELb0EN7cutlass10bfloat16_tE19Flash_kernel_traitsILi224ELi64ELi64ELi8ES2_EELb1ELb0ELb1ELb0EEv16Flash_bwd_params' for 'sm_80'
ptxas info    : Function properties for _Z44flash_bwd_dq_dk_dv_loop_seqk_parallel_kernelI23Flash_bwd_kernel_traitsILi224ELi64ELi64ELi8ELi4ELi4ELi4ELb0ELb0EN7cutlass10bfloat16_tE19Flash_kernel_traitsILi224ELi64ELi64ELi8ES2_EELb1ELb0ELb1ELb0EEv16Flash_bwd_params
    96 bytes stack frame, 104 bytes spill stores, 104 bytes spill loads
ptxas info    : Used 255 registers, 696 bytes cmem[0]
ptxas info    : Compiling entry function '_Z44flash_bwd_dq_dk_dv_loop_seqk_parallel_kernelI23Flash_bwd_kernel_traitsILi224ELi64ELi64ELi8ELi4ELi4ELi4ELb0ELb0EN7cutlass10bfloat16_tE19Flash_kernel_traitsILi224ELi64ELi64ELi8ES2_EELb1ELb0ELb1ELb1EEv16Flash_bwd_params' for 'sm_80'
ptxas info    : Function properties for _Z44flash_bwd_dq_dk_dv_loop_seqk_parallel_kernelI23Flash_bwd_kernel_traitsILi224ELi64ELi64ELi8ELi4ELi4ELi4ELb0ELb0EN7cutlass10bfloat16_tE19Flash_kernel_traitsILi224ELi64ELi64ELi8ES2_EELb1ELb0ELb1ELb1EEv16Flash_bwd_params
    0 bytes stack frame, 0 bytes spill stores, 0 bytes spill loads
ptxas info    : Used 246 registers, 696 bytes cmem[0]
ptxas info    : Compiling entry function '_Z44flash_bwd_dq_dk_dv_loop_seqk_parallel_kernelI23Flash_bwd_kernel_traitsILi224ELi64ELi64ELi8ELi4ELi4ELi4ELb0ELb0EN7cutlass10bfloat16_tE19Flash_kernel_traitsILi224ELi64ELi64ELi8ES2_EELb1ELb1ELb0ELb0EEv16Flash_bwd_params' for 'sm_80'
ptxas info    : Function properties for _Z44flash_bwd_dq_dk_dv_loop_seqk_parallel_kernelI23Flash_bwd_kernel_traitsILi224ELi64ELi64ELi8ELi4ELi4ELi4ELb0ELb0EN7cutlass10bfloat16_tE19Flash_kernel_traitsILi224ELi64ELi64ELi8ES2_EELb1ELb1ELb0ELb0EEv16Flash_bwd_params
    16 bytes stack frame, 12 bytes spill stores, 12 bytes spill loads
ptxas info    : Used 255 registers, 696 bytes cmem[0]
ptxas info    : Compiling entry function '_Z44flash_bwd_dq_dk_dv_loop_seqk_parallel_kernelI23Flash_bwd_kernel_traitsILi224ELi64ELi64ELi8ELi4ELi4ELi4ELb0ELb0EN7cutlass10bfloat16_tE19Flash_kernel_traitsILi224ELi64ELi64ELi8ES2_EELb1ELb1ELb0ELb1EEv16Flash_bwd_params' for 'sm_80'
ptxas info    : Function properties for _Z44flash_bwd_dq_dk_dv_loop_seqk_parallel_kernelI23Flash_bwd_kernel_traitsILi224ELi64ELi64ELi8ELi4ELi4ELi4ELb0ELb0EN7cutlass10bfloat16_tE19Flash_kernel_traitsILi224ELi64ELi64ELi8ES2_EELb1ELb1ELb0ELb1EEv16Flash_bwd_params
    0 bytes stack frame, 0 bytes spill stores, 0 bytes spill loads
ptxas info    : Used 248 registers, 696 bytes cmem[0]
ptxas info    : Compiling entry function '_Z44flash_bwd_dq_dk_dv_loop_seqk_parallel_kernelI23Flash_bwd_kernel_traitsILi224ELi64ELi64ELi8ELi4ELi4ELi4ELb0ELb0EN7cutlass10bfloat16_tE19Flash_kernel_traitsILi224ELi64ELi64ELi8ES2_EELb1ELb1ELb1ELb0EEv16Flash_bwd_params' for 'sm_80'
ptxas info    : Function properties for _Z44flash_bwd_dq_dk_dv_loop_seqk_parallel_kernelI23Flash_bwd_kernel_traitsILi224ELi64ELi64ELi8ELi4ELi4ELi4ELb0ELb0EN7cutlass10bfloat16_tE19Flash_kernel_traitsILi224ELi64ELi64ELi8ES2_EELb1ELb1ELb1ELb0EEv16Flash_bwd_params
    80 bytes stack frame, 92 bytes spill stores, 112 bytes spill loads
ptxas info    : Used 255 registers, 696 bytes cmem[0]
ptxas info    : Compiling entry function '_Z44flash_bwd_dq_dk_dv_loop_seqk_parallel_kernelI23Flash_bwd_kernel_traitsILi224ELi64ELi64ELi8ELi4ELi4ELi4ELb0ELb0EN7cutlass10bfloat16_tE19Flash_kernel_traitsILi224ELi64ELi64ELi8ES2_EELb1ELb1ELb1ELb1EEv16Flash_bwd_params' for 'sm_80'
ptxas info    : Function properties for _Z44flash_bwd_dq_dk_dv_loop_seqk_parallel_kernelI23Flash_bwd_kernel_traitsILi224ELi64ELi64ELi8ELi4ELi4ELi4ELb0ELb0EN7cutlass10bfloat16_tE19Flash_kernel_traitsILi224ELi64ELi64ELi8ES2_EELb1ELb1ELb1ELb1EEv16Flash_bwd_params
    0 bytes stack frame, 0 bytes spill stores, 0 bytes spill loads
ptxas info    : Used 248 registers, 696 bytes cmem[0]
ptxas info    : Compiling entry function '_Z25flash_bwd_dot_do_o_kernelILb1E23Flash_bwd_kernel_traitsILi224ELi64ELi64ELi8ELi4ELi4ELi4ELb0ELb0EN7cutlass10bfloat16_tE19Flash_kernel_traitsILi224ELi64ELi64ELi8ES2_EEEv16Flash_bwd_params' for 'sm_80'
ptxas info    : Function properties for _Z25flash_bwd_dot_do_o_kernelILb1E23Flash_bwd_kernel_traitsILi224ELi64ELi64ELi8ELi4ELi4ELi4ELb0ELb0EN7cutlass10bfloat16_tE19Flash_kernel_traitsILi224ELi64ELi64ELi8ES2_EEEv16Flash_bwd_params
    0 bytes stack frame, 0 bytes spill stores, 0 bytes spill loads
ptxas info    : Used 79 registers, 696 bytes cmem[0]
/anaconda/envs/flashattention/lib/python3.9/site-packages/torch/include/c10/core/SymInt.h(84): warning: integer conversion resulted in a change of sign

[5/36] /usr/local/cuda-11.4/bin/nvcc  -I/home/v-feiychen/flash-attention/csrc/flash_attn -I/home/v-feiychen/flash-attention/csrc/flash_attn/src -I/home/v-feiychen/flash-attention/csrc/cutlass/include -I/anaconda/envs/flashattention/lib/python3.9/site-packages/torch/include -I/anaconda/envs/flashattention/lib/python3.9/site-packages/torch/include/torch/csrc/api/include -I/anaconda/envs/flashattention/lib/python3.9/site-packages/torch/include/TH -I/anaconda/envs/flashattention/lib/python3.9/site-packages/torch/include/THC -I/usr/local/cuda-11.4/include -I/anaconda/envs/flashattention/include/python3.9 -c -c /home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_hdim192_fp16_sm80.cu -o /home/v-feiychen/flash-attention/build/temp.linux-x86_64-cpython-39/csrc/flash_attn/src/flash_fwd_hdim192_fp16_sm80.o -D__CUDA_NO_HALF_OPERATORS__ -D__CUDA_NO_HALF_CONVERSIONS__ -D__CUDA_NO_BFLOAT16_CONVERSIONS__ -D__CUDA_NO_HALF2_OPERATORS__ --expt-relaxed-constexpr --compiler-options ''"'"'-fPIC'"'"'' -O3 -std=c++17 -U__CUDA_NO_HALF_OPERATORS__ -U__CUDA_NO_HALF_CONVERSIONS__ -U__CUDA_NO_HALF2_OPERATORS__ -U__CUDA_NO_BFLOAT16_CONVERSIONS__ --expt-relaxed-constexpr --expt-extended-lambda --use_fast_math --ptxas-options=-v -lineinfo -gencode arch=compute_80,code=sm_80 --threads 4 -DTORCH_API_INCLUDE_EXTENSION_H '-DPYBIND11_COMPILER_TYPE="_gcc"' '-DPYBIND11_STDLIB="_libstdcpp"' '-DPYBIND11_BUILD_ABI="_cxxabi1011"' -DTORCH_EXTENSION_NAME=flash_attn_2_cuda -D_GLIBCXX_USE_CXX11_ABI=0
/anaconda/envs/flashattention/lib/python3.9/site-packages/torch/include/c10/core/SymInt.h(84): warning: integer conversion resulted in a change of sign

/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_kernel.h(140): warning: variable "MMA_M" was declared but never referenced
          detected during:
            instantiation of "void flash::compute_attn_1rowblock<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &, int, int, int) [with Kernel_traits=Flash_fwd_kernel_traits<192, 64, 64, 4, false, false, cutlass::half_t, Flash_kernel_traits<192, 64, 64, 4, cutlass::half_t>>, Is_dropout=true, Is_causal=true, Is_even_N=true, Is_even_K=true, Return_softmax=true, Params=Flash_fwd_params]" 
(588): here
            instantiation of "void flash::compute_attn<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &) [with Kernel_traits=Flash_fwd_kernel_traits<192, 64, 64, 4, false, false, cutlass::half_t, Flash_kernel_traits<192, 64, 64, 4, cutlass::half_t>>, Is_dropout=true, Is_causal=true, Is_even_N=true, Is_even_K=true, Return_softmax=true, Params=Flash_fwd_params]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(15): here
            instantiation of "void flash_fwd_kernel<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax>(Flash_fwd_params) [with Kernel_traits=Flash_fwd_kernel_traits<192, 64, 64, 4, false, false, cutlass::half_t, Flash_kernel_traits<192, 64, 64, 4, cutlass::half_t>>, Is_dropout=true, Is_causal=true, Is_even_N=true, Is_even_K=true, Return_softmax=true]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(34): here
            instantiation of "void run_flash_fwd<Kernel_traits,Is_dropout,Is_causal>(Flash_fwd_params &, cudaStream_t) [with Kernel_traits=Flash_fwd_kernel_traits<192, 64, 64, 4, false, false, cutlass::half_t, Flash_kernel_traits<192, 64, 64, 4, cutlass::half_t>>, Is_dropout=true, Is_causal=true]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(183): here
            instantiation of "void run_mha_fwd_hdim192<T>(Flash_fwd_params &, cudaStream_t) [with T=cutlass::half_t]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_hdim192_fp16_sm80.cu(26): here

/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_kernel.h(275): warning: variable "tQrQ" was declared but never referenced
          detected during:
            instantiation of "void flash::compute_attn_1rowblock<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &, int, int, int) [with Kernel_traits=Flash_fwd_kernel_traits<192, 64, 64, 4, false, false, cutlass::half_t, Flash_kernel_traits<192, 64, 64, 4, cutlass::half_t>>, Is_dropout=true, Is_causal=true, Is_even_N=true, Is_even_K=true, Return_softmax=true, Params=Flash_fwd_params]" 
(588): here
            instantiation of "void flash::compute_attn<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &) [with Kernel_traits=Flash_fwd_kernel_traits<192, 64, 64, 4, false, false, cutlass::half_t, Flash_kernel_traits<192, 64, 64, 4, cutlass::half_t>>, Is_dropout=true, Is_causal=true, Is_even_N=true, Is_even_K=true, Return_softmax=true, Params=Flash_fwd_params]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(15): here
            instantiation of "void flash_fwd_kernel<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax>(Flash_fwd_params) [with Kernel_traits=Flash_fwd_kernel_traits<192, 64, 64, 4, false, false, cutlass::half_t, Flash_kernel_traits<192, 64, 64, 4, cutlass::half_t>>, Is_dropout=true, Is_causal=true, Is_even_N=true, Is_even_K=true, Return_softmax=true]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(34): here
            instantiation of "void run_flash_fwd<Kernel_traits,Is_dropout,Is_causal>(Flash_fwd_params &, cudaStream_t) [with Kernel_traits=Flash_fwd_kernel_traits<192, 64, 64, 4, false, false, cutlass::half_t, Flash_kernel_traits<192, 64, 64, 4, cutlass::half_t>>, Is_dropout=true, Is_causal=true]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(183): here
            instantiation of "void run_mha_fwd_hdim192<T>(Flash_fwd_params &, cudaStream_t) [with T=cutlass::half_t]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_hdim192_fp16_sm80.cu(26): here

/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_kernel.h(140): warning: variable "MMA_M" was declared but never referenced
          detected during:
            instantiation of "void flash::compute_attn_1rowblock<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &, int, int, int) [with Kernel_traits=Flash_fwd_kernel_traits<192, 64, 64, 4, false, false, cutlass::half_t, Flash_kernel_traits<192, 64, 64, 4, cutlass::half_t>>, Is_dropout=true, Is_causal=true, Is_even_N=true, Is_even_K=true, Return_softmax=false, Params=Flash_fwd_params]" 
(588): here
            instantiation of "void flash::compute_attn<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &) [with Kernel_traits=Flash_fwd_kernel_traits<192, 64, 64, 4, false, false, cutlass::half_t, Flash_kernel_traits<192, 64, 64, 4, cutlass::half_t>>, Is_dropout=true, Is_causal=true, Is_even_N=true, Is_even_K=true, Return_softmax=false, Params=Flash_fwd_params]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(15): here
            instantiation of "void flash_fwd_kernel<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax>(Flash_fwd_params) [with Kernel_traits=Flash_fwd_kernel_traits<192, 64, 64, 4, false, false, cutlass::half_t, Flash_kernel_traits<192, 64, 64, 4, cutlass::half_t>>, Is_dropout=true, Is_causal=true, Is_even_N=true, Is_even_K=true, Return_softmax=false]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(34): here
            instantiation of "void run_flash_fwd<Kernel_traits,Is_dropout,Is_causal>(Flash_fwd_params &, cudaStream_t) [with Kernel_traits=Flash_fwd_kernel_traits<192, 64, 64, 4, false, false, cutlass::half_t, Flash_kernel_traits<192, 64, 64, 4, cutlass::half_t>>, Is_dropout=true, Is_causal=true]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(183): here
            instantiation of "void run_mha_fwd_hdim192<T>(Flash_fwd_params &, cudaStream_t) [with T=cutlass::half_t]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_hdim192_fp16_sm80.cu(26): here

/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_kernel.h(275): warning: variable "tQrQ" was declared but never referenced
          detected during:
            instantiation of "void flash::compute_attn_1rowblock<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &, int, int, int) [with Kernel_traits=Flash_fwd_kernel_traits<192, 64, 64, 4, false, false, cutlass::half_t, Flash_kernel_traits<192, 64, 64, 4, cutlass::half_t>>, Is_dropout=true, Is_causal=true, Is_even_N=true, Is_even_K=true, Return_softmax=false, Params=Flash_fwd_params]" 
(588): here
            instantiation of "void flash::compute_attn<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &) [with Kernel_traits=Flash_fwd_kernel_traits<192, 64, 64, 4, false, false, cutlass::half_t, Flash_kernel_traits<192, 64, 64, 4, cutlass::half_t>>, Is_dropout=true, Is_causal=true, Is_even_N=true, Is_even_K=true, Return_softmax=false, Params=Flash_fwd_params]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(15): here
            instantiation of "void flash_fwd_kernel<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax>(Flash_fwd_params) [with Kernel_traits=Flash_fwd_kernel_traits<192, 64, 64, 4, false, false, cutlass::half_t, Flash_kernel_traits<192, 64, 64, 4, cutlass::half_t>>, Is_dropout=true, Is_causal=true, Is_even_N=true, Is_even_K=true, Return_softmax=false]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(34): here
            instantiation of "void run_flash_fwd<Kernel_traits,Is_dropout,Is_causal>(Flash_fwd_params &, cudaStream_t) [with Kernel_traits=Flash_fwd_kernel_traits<192, 64, 64, 4, false, false, cutlass::half_t, Flash_kernel_traits<192, 64, 64, 4, cutlass::half_t>>, Is_dropout=true, Is_causal=true]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(183): here
            instantiation of "void run_mha_fwd_hdim192<T>(Flash_fwd_params &, cudaStream_t) [with T=cutlass::half_t]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_hdim192_fp16_sm80.cu(26): here

/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_kernel.h(140): warning: variable "MMA_M" was declared but never referenced
          detected during:
            instantiation of "void flash::compute_attn_1rowblock<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &, int, int, int) [with Kernel_traits=Flash_fwd_kernel_traits<192, 64, 64, 4, false, false, cutlass::half_t, Flash_kernel_traits<192, 64, 64, 4, cutlass::half_t>>, Is_dropout=true, Is_causal=true, Is_even_N=true, Is_even_K=false, Return_softmax=true, Params=Flash_fwd_params]" 
(588): here
            instantiation of "void flash::compute_attn<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &) [with Kernel_traits=Flash_fwd_kernel_traits<192, 64, 64, 4, false, false, cutlass::half_t, Flash_kernel_traits<192, 64, 64, 4, cutlass::half_t>>, Is_dropout=true, Is_causal=true, Is_even_N=true, Is_even_K=false, Return_softmax=true, Params=Flash_fwd_params]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(15): here
            instantiation of "void flash_fwd_kernel<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax>(Flash_fwd_params) [with Kernel_traits=Flash_fwd_kernel_traits<192, 64, 64, 4, false, false, cutlass::half_t, Flash_kernel_traits<192, 64, 64, 4, cutlass::half_t>>, Is_dropout=true, Is_causal=true, Is_even_N=true, Is_even_K=false, Return_softmax=true]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(34): here
            instantiation of "void run_flash_fwd<Kernel_traits,Is_dropout,Is_causal>(Flash_fwd_params &, cudaStream_t) [with Kernel_traits=Flash_fwd_kernel_traits<192, 64, 64, 4, false, false, cutlass::half_t, Flash_kernel_traits<192, 64, 64, 4, cutlass::half_t>>, Is_dropout=true, Is_causal=true]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(183): here
            instantiation of "void run_mha_fwd_hdim192<T>(Flash_fwd_params &, cudaStream_t) [with T=cutlass::half_t]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_hdim192_fp16_sm80.cu(26): here

/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_kernel.h(275): warning: variable "tQrQ" was declared but never referenced
          detected during:
            instantiation of "void flash::compute_attn_1rowblock<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &, int, int, int) [with Kernel_traits=Flash_fwd_kernel_traits<192, 64, 64, 4, false, false, cutlass::half_t, Flash_kernel_traits<192, 64, 64, 4, cutlass::half_t>>, Is_dropout=true, Is_causal=true, Is_even_N=true, Is_even_K=false, Return_softmax=true, Params=Flash_fwd_params]" 
(588): here
            instantiation of "void flash::compute_attn<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &) [with Kernel_traits=Flash_fwd_kernel_traits<192, 64, 64, 4, false, false, cutlass::half_t, Flash_kernel_traits<192, 64, 64, 4, cutlass::half_t>>, Is_dropout=true, Is_causal=true, Is_even_N=true, Is_even_K=false, Return_softmax=true, Params=Flash_fwd_params]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(15): here
            instantiation of "void flash_fwd_kernel<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax>(Flash_fwd_params) [with Kernel_traits=Flash_fwd_kernel_traits<192, 64, 64, 4, false, false, cutlass::half_t, Flash_kernel_traits<192, 64, 64, 4, cutlass::half_t>>, Is_dropout=true, Is_causal=true, Is_even_N=true, Is_even_K=false, Return_softmax=true]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(34): here
            instantiation of "void run_flash_fwd<Kernel_traits,Is_dropout,Is_causal>(Flash_fwd_params &, cudaStream_t) [with Kernel_traits=Flash_fwd_kernel_traits<192, 64, 64, 4, false, false, cutlass::half_t, Flash_kernel_traits<192, 64, 64, 4, cutlass::half_t>>, Is_dropout=true, Is_causal=true]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(183): here
            instantiation of "void run_mha_fwd_hdim192<T>(Flash_fwd_params &, cudaStream_t) [with T=cutlass::half_t]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_hdim192_fp16_sm80.cu(26): here

/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_kernel.h(140): warning: variable "MMA_M" was declared but never referenced
          detected during:
            instantiation of "void flash::compute_attn_1rowblock<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &, int, int, int) [with Kernel_traits=Flash_fwd_kernel_traits<192, 64, 64, 4, false, false, cutlass::half_t, Flash_kernel_traits<192, 64, 64, 4, cutlass::half_t>>, Is_dropout=true, Is_causal=true, Is_even_N=true, Is_even_K=false, Return_softmax=false, Params=Flash_fwd_params]" 
(588): here
            instantiation of "void flash::compute_attn<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &) [with Kernel_traits=Flash_fwd_kernel_traits<192, 64, 64, 4, false, false, cutlass::half_t, Flash_kernel_traits<192, 64, 64, 4, cutlass::half_t>>, Is_dropout=true, Is_causal=true, Is_even_N=true, Is_even_K=false, Return_softmax=false, Params=Flash_fwd_params]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(15): here
            instantiation of "void flash_fwd_kernel<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax>(Flash_fwd_params) [with Kernel_traits=Flash_fwd_kernel_traits<192, 64, 64, 4, false, false, cutlass::half_t, Flash_kernel_traits<192, 64, 64, 4, cutlass::half_t>>, Is_dropout=true, Is_causal=true, Is_even_N=true, Is_even_K=false, Return_softmax=false]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(34): here
            instantiation of "void run_flash_fwd<Kernel_traits,Is_dropout,Is_causal>(Flash_fwd_params &, cudaStream_t) [with Kernel_traits=Flash_fwd_kernel_traits<192, 64, 64, 4, false, false, cutlass::half_t, Flash_kernel_traits<192, 64, 64, 4, cutlass::half_t>>, Is_dropout=true, Is_causal=true]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(183): here
            instantiation of "void run_mha_fwd_hdim192<T>(Flash_fwd_params &, cudaStream_t) [with T=cutlass::half_t]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_hdim192_fp16_sm80.cu(26): here

/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_kernel.h(275): warning: variable "tQrQ" was declared but never referenced
          detected during:
            instantiation of "void flash::compute_attn_1rowblock<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &, int, int, int) [with Kernel_traits=Flash_fwd_kernel_traits<192, 64, 64, 4, false, false, cutlass::half_t, Flash_kernel_traits<192, 64, 64, 4, cutlass::half_t>>, Is_dropout=true, Is_causal=true, Is_even_N=true, Is_even_K=false, Return_softmax=false, Params=Flash_fwd_params]" 
(588): here
            instantiation of "void flash::compute_attn<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &) [with Kernel_traits=Flash_fwd_kernel_traits<192, 64, 64, 4, false, false, cutlass::half_t, Flash_kernel_traits<192, 64, 64, 4, cutlass::half_t>>, Is_dropout=true, Is_causal=true, Is_even_N=true, Is_even_K=false, Return_softmax=false, Params=Flash_fwd_params]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(15): here
            instantiation of "void flash_fwd_kernel<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax>(Flash_fwd_params) [with Kernel_traits=Flash_fwd_kernel_traits<192, 64, 64, 4, false, false, cutlass::half_t, Flash_kernel_traits<192, 64, 64, 4, cutlass::half_t>>, Is_dropout=true, Is_causal=true, Is_even_N=true, Is_even_K=false, Return_softmax=false]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(34): here
            instantiation of "void run_flash_fwd<Kernel_traits,Is_dropout,Is_causal>(Flash_fwd_params &, cudaStream_t) [with Kernel_traits=Flash_fwd_kernel_traits<192, 64, 64, 4, false, false, cutlass::half_t, Flash_kernel_traits<192, 64, 64, 4, cutlass::half_t>>, Is_dropout=true, Is_causal=true]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(183): here
            instantiation of "void run_mha_fwd_hdim192<T>(Flash_fwd_params &, cudaStream_t) [with T=cutlass::half_t]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_hdim192_fp16_sm80.cu(26): here

/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_kernel.h(140): warning: variable "MMA_M" was declared but never referenced
          detected during:
            instantiation of "void flash::compute_attn_1rowblock<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &, int, int, int) [with Kernel_traits=Flash_fwd_kernel_traits<192, 64, 64, 4, false, false, cutlass::half_t, Flash_kernel_traits<192, 64, 64, 4, cutlass::half_t>>, Is_dropout=true, Is_causal=true, Is_even_N=false, Is_even_K=true, Return_softmax=true, Params=Flash_fwd_params]" 
(588): here
            instantiation of "void flash::compute_attn<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &) [with Kernel_traits=Flash_fwd_kernel_traits<192, 64, 64, 4, false, false, cutlass::half_t, Flash_kernel_traits<192, 64, 64, 4, cutlass::half_t>>, Is_dropout=true, Is_causal=true, Is_even_N=false, Is_even_K=true, Return_softmax=true, Params=Flash_fwd_params]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(15): here
            instantiation of "void flash_fwd_kernel<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax>(Flash_fwd_params) [with Kernel_traits=Flash_fwd_kernel_traits<192, 64, 64, 4, false, false, cutlass::half_t, Flash_kernel_traits<192, 64, 64, 4, cutlass::half_t>>, Is_dropout=true, Is_causal=true, Is_even_N=false, Is_even_K=true, Return_softmax=true]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(34): here
            instantiation of "void run_flash_fwd<Kernel_traits,Is_dropout,Is_causal>(Flash_fwd_params &, cudaStream_t) [with Kernel_traits=Flash_fwd_kernel_traits<192, 64, 64, 4, false, false, cutlass::half_t, Flash_kernel_traits<192, 64, 64, 4, cutlass::half_t>>, Is_dropout=true, Is_causal=true]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(183): here
            instantiation of "void run_mha_fwd_hdim192<T>(Flash_fwd_params &, cudaStream_t) [with T=cutlass::half_t]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_hdim192_fp16_sm80.cu(26): here

/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_kernel.h(275): warning: variable "tQrQ" was declared but never referenced
          detected during:
            instantiation of "void flash::compute_attn_1rowblock<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &, int, int, int) [with Kernel_traits=Flash_fwd_kernel_traits<192, 64, 64, 4, false, false, cutlass::half_t, Flash_kernel_traits<192, 64, 64, 4, cutlass::half_t>>, Is_dropout=true, Is_causal=true, Is_even_N=false, Is_even_K=true, Return_softmax=true, Params=Flash_fwd_params]" 
(588): here
            instantiation of "void flash::compute_attn<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &) [with Kernel_traits=Flash_fwd_kernel_traits<192, 64, 64, 4, false, false, cutlass::half_t, Flash_kernel_traits<192, 64, 64, 4, cutlass::half_t>>, Is_dropout=true, Is_causal=true, Is_even_N=false, Is_even_K=true, Return_softmax=true, Params=Flash_fwd_params]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(15): here
            instantiation of "void flash_fwd_kernel<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax>(Flash_fwd_params) [with Kernel_traits=Flash_fwd_kernel_traits<192, 64, 64, 4, false, false, cutlass::half_t, Flash_kernel_traits<192, 64, 64, 4, cutlass::half_t>>, Is_dropout=true, Is_causal=true, Is_even_N=false, Is_even_K=true, Return_softmax=true]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(34): here
            instantiation of "void run_flash_fwd<Kernel_traits,Is_dropout,Is_causal>(Flash_fwd_params &, cudaStream_t) [with Kernel_traits=Flash_fwd_kernel_traits<192, 64, 64, 4, false, false, cutlass::half_t, Flash_kernel_traits<192, 64, 64, 4, cutlass::half_t>>, Is_dropout=true, Is_causal=true]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(183): here
            instantiation of "void run_mha_fwd_hdim192<T>(Flash_fwd_params &, cudaStream_t) [with T=cutlass::half_t]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_hdim192_fp16_sm80.cu(26): here

/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_kernel.h(140): warning: variable "MMA_M" was declared but never referenced
          detected during:
            instantiation of "void flash::compute_attn_1rowblock<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &, int, int, int) [with Kernel_traits=Flash_fwd_kernel_traits<192, 64, 64, 4, false, false, cutlass::half_t, Flash_kernel_traits<192, 64, 64, 4, cutlass::half_t>>, Is_dropout=true, Is_causal=true, Is_even_N=false, Is_even_K=true, Return_softmax=false, Params=Flash_fwd_params]" 
(588): here
            instantiation of "void flash::compute_attn<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &) [with Kernel_traits=Flash_fwd_kernel_traits<192, 64, 64, 4, false, false, cutlass::half_t, Flash_kernel_traits<192, 64, 64, 4, cutlass::half_t>>, Is_dropout=true, Is_causal=true, Is_even_N=false, Is_even_K=true, Return_softmax=false, Params=Flash_fwd_params]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(15): here
            instantiation of "void flash_fwd_kernel<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax>(Flash_fwd_params) [with Kernel_traits=Flash_fwd_kernel_traits<192, 64, 64, 4, false, false, cutlass::half_t, Flash_kernel_traits<192, 64, 64, 4, cutlass::half_t>>, Is_dropout=true, Is_causal=true, Is_even_N=false, Is_even_K=true, Return_softmax=false]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(34): here
            instantiation of "void run_flash_fwd<Kernel_traits,Is_dropout,Is_causal>(Flash_fwd_params &, cudaStream_t) [with Kernel_traits=Flash_fwd_kernel_traits<192, 64, 64, 4, false, false, cutlass::half_t, Flash_kernel_traits<192, 64, 64, 4, cutlass::half_t>>, Is_dropout=true, Is_causal=true]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(183): here
            instantiation of "void run_mha_fwd_hdim192<T>(Flash_fwd_params &, cudaStream_t) [with T=cutlass::half_t]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_hdim192_fp16_sm80.cu(26): here

/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_kernel.h(275): warning: variable "tQrQ" was declared but never referenced
          detected during:
            instantiation of "void flash::compute_attn_1rowblock<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &, int, int, int) [with Kernel_traits=Flash_fwd_kernel_traits<192, 64, 64, 4, false, false, cutlass::half_t, Flash_kernel_traits<192, 64, 64, 4, cutlass::half_t>>, Is_dropout=true, Is_causal=true, Is_even_N=false, Is_even_K=true, Return_softmax=false, Params=Flash_fwd_params]" 
(588): here
            instantiation of "void flash::compute_attn<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &) [with Kernel_traits=Flash_fwd_kernel_traits<192, 64, 64, 4, false, false, cutlass::half_t, Flash_kernel_traits<192, 64, 64, 4, cutlass::half_t>>, Is_dropout=true, Is_causal=true, Is_even_N=false, Is_even_K=true, Return_softmax=false, Params=Flash_fwd_params]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(15): here
            instantiation of "void flash_fwd_kernel<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax>(Flash_fwd_params) [with Kernel_traits=Flash_fwd_kernel_traits<192, 64, 64, 4, false, false, cutlass::half_t, Flash_kernel_traits<192, 64, 64, 4, cutlass::half_t>>, Is_dropout=true, Is_causal=true, Is_even_N=false, Is_even_K=true, Return_softmax=false]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(34): here
            instantiation of "void run_flash_fwd<Kernel_traits,Is_dropout,Is_causal>(Flash_fwd_params &, cudaStream_t) [with Kernel_traits=Flash_fwd_kernel_traits<192, 64, 64, 4, false, false, cutlass::half_t, Flash_kernel_traits<192, 64, 64, 4, cutlass::half_t>>, Is_dropout=true, Is_causal=true]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(183): here
            instantiation of "void run_mha_fwd_hdim192<T>(Flash_fwd_params &, cudaStream_t) [with T=cutlass::half_t]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_hdim192_fp16_sm80.cu(26): here

/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_kernel.h(140): warning: variable "MMA_M" was declared but never referenced
          detected during:
            instantiation of "void flash::compute_attn_1rowblock<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &, int, int, int) [with Kernel_traits=Flash_fwd_kernel_traits<192, 64, 64, 4, false, false, cutlass::half_t, Flash_kernel_traits<192, 64, 64, 4, cutlass::half_t>>, Is_dropout=true, Is_causal=true, Is_even_N=false, Is_even_K=false, Return_softmax=true, Params=Flash_fwd_params]" 
(588): here
            instantiation of "void flash::compute_attn<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &) [with Kernel_traits=Flash_fwd_kernel_traits<192, 64, 64, 4, false, false, cutlass::half_t, Flash_kernel_traits<192, 64, 64, 4, cutlass::half_t>>, Is_dropout=true, Is_causal=true, Is_even_N=false, Is_even_K=false, Return_softmax=true, Params=Flash_fwd_params]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(15): here
            instantiation of "void flash_fwd_kernel<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax>(Flash_fwd_params) [with Kernel_traits=Flash_fwd_kernel_traits<192, 64, 64, 4, false, false, cutlass::half_t, Flash_kernel_traits<192, 64, 64, 4, cutlass::half_t>>, Is_dropout=true, Is_causal=true, Is_even_N=false, Is_even_K=false, Return_softmax=true]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(34): here
            instantiation of "void run_flash_fwd<Kernel_traits,Is_dropout,Is_causal>(Flash_fwd_params &, cudaStream_t) [with Kernel_traits=Flash_fwd_kernel_traits<192, 64, 64, 4, false, false, cutlass::half_t, Flash_kernel_traits<192, 64, 64, 4, cutlass::half_t>>, Is_dropout=true, Is_causal=true]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(183): here
            instantiation of "void run_mha_fwd_hdim192<T>(Flash_fwd_params &, cudaStream_t) [with T=cutlass::half_t]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_hdim192_fp16_sm80.cu(26): here

/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_kernel.h(275): warning: variable "tQrQ" was declared but never referenced
          detected during:
            instantiation of "void flash::compute_attn_1rowblock<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &, int, int, int) [with Kernel_traits=Flash_fwd_kernel_traits<192, 64, 64, 4, false, false, cutlass::half_t, Flash_kernel_traits<192, 64, 64, 4, cutlass::half_t>>, Is_dropout=true, Is_causal=true, Is_even_N=false, Is_even_K=false, Return_softmax=true, Params=Flash_fwd_params]" 
(588): here
            instantiation of "void flash::compute_attn<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &) [with Kernel_traits=Flash_fwd_kernel_traits<192, 64, 64, 4, false, false, cutlass::half_t, Flash_kernel_traits<192, 64, 64, 4, cutlass::half_t>>, Is_dropout=true, Is_causal=true, Is_even_N=false, Is_even_K=false, Return_softmax=true, Params=Flash_fwd_params]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(15): here
            instantiation of "void flash_fwd_kernel<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax>(Flash_fwd_params) [with Kernel_traits=Flash_fwd_kernel_traits<192, 64, 64, 4, false, false, cutlass::half_t, Flash_kernel_traits<192, 64, 64, 4, cutlass::half_t>>, Is_dropout=true, Is_causal=true, Is_even_N=false, Is_even_K=false, Return_softmax=true]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(34): here
            instantiation of "void run_flash_fwd<Kernel_traits,Is_dropout,Is_causal>(Flash_fwd_params &, cudaStream_t) [with Kernel_traits=Flash_fwd_kernel_traits<192, 64, 64, 4, false, false, cutlass::half_t, Flash_kernel_traits<192, 64, 64, 4, cutlass::half_t>>, Is_dropout=true, Is_causal=true]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(183): here
            instantiation of "void run_mha_fwd_hdim192<T>(Flash_fwd_params &, cudaStream_t) [with T=cutlass::half_t]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_hdim192_fp16_sm80.cu(26): here

/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_kernel.h(140): warning: variable "MMA_M" was declared but never referenced
          detected during:
            instantiation of "void flash::compute_attn_1rowblock<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &, int, int, int) [with Kernel_traits=Flash_fwd_kernel_traits<192, 64, 64, 4, false, false, cutlass::half_t, Flash_kernel_traits<192, 64, 64, 4, cutlass::half_t>>, Is_dropout=true, Is_causal=true, Is_even_N=false, Is_even_K=false, Return_softmax=false, Params=Flash_fwd_params]" 
(588): here
            instantiation of "void flash::compute_attn<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &) [with Kernel_traits=Flash_fwd_kernel_traits<192, 64, 64, 4, false, false, cutlass::half_t, Flash_kernel_traits<192, 64, 64, 4, cutlass::half_t>>, Is_dropout=true, Is_causal=true, Is_even_N=false, Is_even_K=false, Return_softmax=false, Params=Flash_fwd_params]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(15): here
            instantiation of "void flash_fwd_kernel<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax>(Flash_fwd_params) [with Kernel_traits=Flash_fwd_kernel_traits<192, 64, 64, 4, false, false, cutlass::half_t, Flash_kernel_traits<192, 64, 64, 4, cutlass::half_t>>, Is_dropout=true, Is_causal=true, Is_even_N=false, Is_even_K=false, Return_softmax=false]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(34): here
            instantiation of "void run_flash_fwd<Kernel_traits,Is_dropout,Is_causal>(Flash_fwd_params &, cudaStream_t) [with Kernel_traits=Flash_fwd_kernel_traits<192, 64, 64, 4, false, false, cutlass::half_t, Flash_kernel_traits<192, 64, 64, 4, cutlass::half_t>>, Is_dropout=true, Is_causal=true]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(183): here
            instantiation of "void run_mha_fwd_hdim192<T>(Flash_fwd_params &, cudaStream_t) [with T=cutlass::half_t]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_hdim192_fp16_sm80.cu(26): here

/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_kernel.h(275): warning: variable "tQrQ" was declared but never referenced
          detected during:
            instantiation of "void flash::compute_attn_1rowblock<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &, int, int, int) [with Kernel_traits=Flash_fwd_kernel_traits<192, 64, 64, 4, false, false, cutlass::half_t, Flash_kernel_traits<192, 64, 64, 4, cutlass::half_t>>, Is_dropout=true, Is_causal=true, Is_even_N=false, Is_even_K=false, Return_softmax=false, Params=Flash_fwd_params]" 
(588): here
            instantiation of "void flash::compute_attn<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &) [with Kernel_traits=Flash_fwd_kernel_traits<192, 64, 64, 4, false, false, cutlass::half_t, Flash_kernel_traits<192, 64, 64, 4, cutlass::half_t>>, Is_dropout=true, Is_causal=true, Is_even_N=false, Is_even_K=false, Return_softmax=false, Params=Flash_fwd_params]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(15): here
            instantiation of "void flash_fwd_kernel<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax>(Flash_fwd_params) [with Kernel_traits=Flash_fwd_kernel_traits<192, 64, 64, 4, false, false, cutlass::half_t, Flash_kernel_traits<192, 64, 64, 4, cutlass::half_t>>, Is_dropout=true, Is_causal=true, Is_even_N=false, Is_even_K=false, Return_softmax=false]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(34): here
            instantiation of "void run_flash_fwd<Kernel_traits,Is_dropout,Is_causal>(Flash_fwd_params &, cudaStream_t) [with Kernel_traits=Flash_fwd_kernel_traits<192, 64, 64, 4, false, false, cutlass::half_t, Flash_kernel_traits<192, 64, 64, 4, cutlass::half_t>>, Is_dropout=true, Is_causal=true]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(183): here
            instantiation of "void run_mha_fwd_hdim192<T>(Flash_fwd_params &, cudaStream_t) [with T=cutlass::half_t]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_hdim192_fp16_sm80.cu(26): here

/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_kernel.h(140): warning: variable "MMA_M" was declared but never referenced
          detected during:
            instantiation of "void flash::compute_attn_1rowblock<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &, int, int, int) [with Kernel_traits=Flash_fwd_kernel_traits<192, 64, 64, 4, false, false, cutlass::half_t, Flash_kernel_traits<192, 64, 64, 4, cutlass::half_t>>, Is_dropout=true, Is_causal=false, Is_even_N=true, Is_even_K=true, Return_softmax=true, Params=Flash_fwd_params]" 
(588): here
            instantiation of "void flash::compute_attn<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &) [with Kernel_traits=Flash_fwd_kernel_traits<192, 64, 64, 4, false, false, cutlass::half_t, Flash_kernel_traits<192, 64, 64, 4, cutlass::half_t>>, Is_dropout=true, Is_causal=false, Is_even_N=true, Is_even_K=true, Return_softmax=true, Params=Flash_fwd_params]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(15): here
            instantiation of "void flash_fwd_kernel<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax>(Flash_fwd_params) [with Kernel_traits=Flash_fwd_kernel_traits<192, 64, 64, 4, false, false, cutlass::half_t, Flash_kernel_traits<192, 64, 64, 4, cutlass::half_t>>, Is_dropout=true, Is_causal=false, Is_even_N=true, Is_even_K=true, Return_softmax=true]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(34): here
            instantiation of "void run_flash_fwd<Kernel_traits,Is_dropout,Is_causal>(Flash_fwd_params &, cudaStream_t) [with Kernel_traits=Flash_fwd_kernel_traits<192, 64, 64, 4, false, false, cutlass::half_t, Flash_kernel_traits<192, 64, 64, 4, cutlass::half_t>>, Is_dropout=true, Is_causal=false]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(183): here
            instantiation of "void run_mha_fwd_hdim192<T>(Flash_fwd_params &, cudaStream_t) [with T=cutlass::half_t]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_hdim192_fp16_sm80.cu(26): here

/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_kernel.h(275): warning: variable "tQrQ" was declared but never referenced
          detected during:
            instantiation of "void flash::compute_attn_1rowblock<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &, int, int, int) [with Kernel_traits=Flash_fwd_kernel_traits<192, 64, 64, 4, false, false, cutlass::half_t, Flash_kernel_traits<192, 64, 64, 4, cutlass::half_t>>, Is_dropout=true, Is_causal=false, Is_even_N=true, Is_even_K=true, Return_softmax=true, Params=Flash_fwd_params]" 
(588): here
            instantiation of "void flash::compute_attn<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &) [with Kernel_traits=Flash_fwd_kernel_traits<192, 64, 64, 4, false, false, cutlass::half_t, Flash_kernel_traits<192, 64, 64, 4, cutlass::half_t>>, Is_dropout=true, Is_causal=false, Is_even_N=true, Is_even_K=true, Return_softmax=true, Params=Flash_fwd_params]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(15): here
            instantiation of "void flash_fwd_kernel<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax>(Flash_fwd_params) [with Kernel_traits=Flash_fwd_kernel_traits<192, 64, 64, 4, false, false, cutlass::half_t, Flash_kernel_traits<192, 64, 64, 4, cutlass::half_t>>, Is_dropout=true, Is_causal=false, Is_even_N=true, Is_even_K=true, Return_softmax=true]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(34): here
            instantiation of "void run_flash_fwd<Kernel_traits,Is_dropout,Is_causal>(Flash_fwd_params &, cudaStream_t) [with Kernel_traits=Flash_fwd_kernel_traits<192, 64, 64, 4, false, false, cutlass::half_t, Flash_kernel_traits<192, 64, 64, 4, cutlass::half_t>>, Is_dropout=true, Is_causal=false]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(183): here
            instantiation of "void run_mha_fwd_hdim192<T>(Flash_fwd_params &, cudaStream_t) [with T=cutlass::half_t]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_hdim192_fp16_sm80.cu(26): here

/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_kernel.h(140): warning: variable "MMA_M" was declared but never referenced
          detected during:
            instantiation of "void flash::compute_attn_1rowblock<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &, int, int, int) [with Kernel_traits=Flash_fwd_kernel_traits<192, 64, 64, 4, false, false, cutlass::half_t, Flash_kernel_traits<192, 64, 64, 4, cutlass::half_t>>, Is_dropout=true, Is_causal=false, Is_even_N=true, Is_even_K=true, Return_softmax=false, Params=Flash_fwd_params]" 
(588): here
            instantiation of "void flash::compute_attn<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &) [with Kernel_traits=Flash_fwd_kernel_traits<192, 64, 64, 4, false, false, cutlass::half_t, Flash_kernel_traits<192, 64, 64, 4, cutlass::half_t>>, Is_dropout=true, Is_causal=false, Is_even_N=true, Is_even_K=true, Return_softmax=false, Params=Flash_fwd_params]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(15): here
            instantiation of "void flash_fwd_kernel<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax>(Flash_fwd_params) [with Kernel_traits=Flash_fwd_kernel_traits<192, 64, 64, 4, false, false, cutlass::half_t, Flash_kernel_traits<192, 64, 64, 4, cutlass::half_t>>, Is_dropout=true, Is_causal=false, Is_even_N=true, Is_even_K=true, Return_softmax=false]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(34): here
            instantiation of "void run_flash_fwd<Kernel_traits,Is_dropout,Is_causal>(Flash_fwd_params &, cudaStream_t) [with Kernel_traits=Flash_fwd_kernel_traits<192, 64, 64, 4, false, false, cutlass::half_t, Flash_kernel_traits<192, 64, 64, 4, cutlass::half_t>>, Is_dropout=true, Is_causal=false]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(183): here
            instantiation of "void run_mha_fwd_hdim192<T>(Flash_fwd_params &, cudaStream_t) [with T=cutlass::half_t]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_hdim192_fp16_sm80.cu(26): here

/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_kernel.h(275): warning: variable "tQrQ" was declared but never referenced
          detected during:
            instantiation of "void flash::compute_attn_1rowblock<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &, int, int, int) [with Kernel_traits=Flash_fwd_kernel_traits<192, 64, 64, 4, false, false, cutlass::half_t, Flash_kernel_traits<192, 64, 64, 4, cutlass::half_t>>, Is_dropout=true, Is_causal=false, Is_even_N=true, Is_even_K=true, Return_softmax=false, Params=Flash_fwd_params]" 
(588): here
            instantiation of "void flash::compute_attn<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &) [with Kernel_traits=Flash_fwd_kernel_traits<192, 64, 64, 4, false, false, cutlass::half_t, Flash_kernel_traits<192, 64, 64, 4, cutlass::half_t>>, Is_dropout=true, Is_causal=false, Is_even_N=true, Is_even_K=true, Return_softmax=false, Params=Flash_fwd_params]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(15): here
            instantiation of "void flash_fwd_kernel<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax>(Flash_fwd_params) [with Kernel_traits=Flash_fwd_kernel_traits<192, 64, 64, 4, false, false, cutlass::half_t, Flash_kernel_traits<192, 64, 64, 4, cutlass::half_t>>, Is_dropout=true, Is_causal=false, Is_even_N=true, Is_even_K=true, Return_softmax=false]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(34): here
            instantiation of "void run_flash_fwd<Kernel_traits,Is_dropout,Is_causal>(Flash_fwd_params &, cudaStream_t) [with Kernel_traits=Flash_fwd_kernel_traits<192, 64, 64, 4, false, false, cutlass::half_t, Flash_kernel_traits<192, 64, 64, 4, cutlass::half_t>>, Is_dropout=true, Is_causal=false]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(183): here
            instantiation of "void run_mha_fwd_hdim192<T>(Flash_fwd_params &, cudaStream_t) [with T=cutlass::half_t]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_hdim192_fp16_sm80.cu(26): here

/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_kernel.h(140): warning: variable "MMA_M" was declared but never referenced
          detected during:
            instantiation of "void flash::compute_attn_1rowblock<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &, int, int, int) [with Kernel_traits=Flash_fwd_kernel_traits<192, 64, 64, 4, false, false, cutlass::half_t, Flash_kernel_traits<192, 64, 64, 4, cutlass::half_t>>, Is_dropout=true, Is_causal=false, Is_even_N=true, Is_even_K=false, Return_softmax=true, Params=Flash_fwd_params]" 
(588): here
            instantiation of "void flash::compute_attn<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &) [with Kernel_traits=Flash_fwd_kernel_traits<192, 64, 64, 4, false, false, cutlass::half_t, Flash_kernel_traits<192, 64, 64, 4, cutlass::half_t>>, Is_dropout=true, Is_causal=false, Is_even_N=true, Is_even_K=false, Return_softmax=true, Params=Flash_fwd_params]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(15): here
            instantiation of "void flash_fwd_kernel<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax>(Flash_fwd_params) [with Kernel_traits=Flash_fwd_kernel_traits<192, 64, 64, 4, false, false, cutlass::half_t, Flash_kernel_traits<192, 64, 64, 4, cutlass::half_t>>, Is_dropout=true, Is_causal=false, Is_even_N=true, Is_even_K=false, Return_softmax=true]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(34): here
            instantiation of "void run_flash_fwd<Kernel_traits,Is_dropout,Is_causal>(Flash_fwd_params &, cudaStream_t) [with Kernel_traits=Flash_fwd_kernel_traits<192, 64, 64, 4, false, false, cutlass::half_t, Flash_kernel_traits<192, 64, 64, 4, cutlass::half_t>>, Is_dropout=true, Is_causal=false]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(183): here
            instantiation of "void run_mha_fwd_hdim192<T>(Flash_fwd_params &, cudaStream_t) [with T=cutlass::half_t]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_hdim192_fp16_sm80.cu(26): here

/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_kernel.h(275): warning: variable "tQrQ" was declared but never referenced
          detected during:
            instantiation of "void flash::compute_attn_1rowblock<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &, int, int, int) [with Kernel_traits=Flash_fwd_kernel_traits<192, 64, 64, 4, false, false, cutlass::half_t, Flash_kernel_traits<192, 64, 64, 4, cutlass::half_t>>, Is_dropout=true, Is_causal=false, Is_even_N=true, Is_even_K=false, Return_softmax=true, Params=Flash_fwd_params]" 
(588): here
            instantiation of "void flash::compute_attn<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &) [with Kernel_traits=Flash_fwd_kernel_traits<192, 64, 64, 4, false, false, cutlass::half_t, Flash_kernel_traits<192, 64, 64, 4, cutlass::half_t>>, Is_dropout=true, Is_causal=false, Is_even_N=true, Is_even_K=false, Return_softmax=true, Params=Flash_fwd_params]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(15): here
            instantiation of "void flash_fwd_kernel<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax>(Flash_fwd_params) [with Kernel_traits=Flash_fwd_kernel_traits<192, 64, 64, 4, false, false, cutlass::half_t, Flash_kernel_traits<192, 64, 64, 4, cutlass::half_t>>, Is_dropout=true, Is_causal=false, Is_even_N=true, Is_even_K=false, Return_softmax=true]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(34): here
            instantiation of "void run_flash_fwd<Kernel_traits,Is_dropout,Is_causal>(Flash_fwd_params &, cudaStream_t) [with Kernel_traits=Flash_fwd_kernel_traits<192, 64, 64, 4, false, false, cutlass::half_t, Flash_kernel_traits<192, 64, 64, 4, cutlass::half_t>>, Is_dropout=true, Is_causal=false]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(183): here
            instantiation of "void run_mha_fwd_hdim192<T>(Flash_fwd_params &, cudaStream_t) [with T=cutlass::half_t]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_hdim192_fp16_sm80.cu(26): here

/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_kernel.h(140): warning: variable "MMA_M" was declared but never referenced
          detected during:
            instantiation of "void flash::compute_attn_1rowblock<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &, int, int, int) [with Kernel_traits=Flash_fwd_kernel_traits<192, 64, 64, 4, false, false, cutlass::half_t, Flash_kernel_traits<192, 64, 64, 4, cutlass::half_t>>, Is_dropout=true, Is_causal=false, Is_even_N=true, Is_even_K=false, Return_softmax=false, Params=Flash_fwd_params]" 
(588): here
            instantiation of "void flash::compute_attn<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &) [with Kernel_traits=Flash_fwd_kernel_traits<192, 64, 64, 4, false, false, cutlass::half_t, Flash_kernel_traits<192, 64, 64, 4, cutlass::half_t>>, Is_dropout=true, Is_causal=false, Is_even_N=true, Is_even_K=false, Return_softmax=false, Params=Flash_fwd_params]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(15): here
            instantiation of "void flash_fwd_kernel<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax>(Flash_fwd_params) [with Kernel_traits=Flash_fwd_kernel_traits<192, 64, 64, 4, false, false, cutlass::half_t, Flash_kernel_traits<192, 64, 64, 4, cutlass::half_t>>, Is_dropout=true, Is_causal=false, Is_even_N=true, Is_even_K=false, Return_softmax=false]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(34): here
            instantiation of "void run_flash_fwd<Kernel_traits,Is_dropout,Is_causal>(Flash_fwd_params &, cudaStream_t) [with Kernel_traits=Flash_fwd_kernel_traits<192, 64, 64, 4, false, false, cutlass::half_t, Flash_kernel_traits<192, 64, 64, 4, cutlass::half_t>>, Is_dropout=true, Is_causal=false]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(183): here
            instantiation of "void run_mha_fwd_hdim192<T>(Flash_fwd_params &, cudaStream_t) [with T=cutlass::half_t]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_hdim192_fp16_sm80.cu(26): here

/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_kernel.h(275): warning: variable "tQrQ" was declared but never referenced
          detected during:
            instantiation of "void flash::compute_attn_1rowblock<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &, int, int, int) [with Kernel_traits=Flash_fwd_kernel_traits<192, 64, 64, 4, false, false, cutlass::half_t, Flash_kernel_traits<192, 64, 64, 4, cutlass::half_t>>, Is_dropout=true, Is_causal=false, Is_even_N=true, Is_even_K=false, Return_softmax=false, Params=Flash_fwd_params]" 
(588): here
            instantiation of "void flash::compute_attn<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &) [with Kernel_traits=Flash_fwd_kernel_traits<192, 64, 64, 4, false, false, cutlass::half_t, Flash_kernel_traits<192, 64, 64, 4, cutlass::half_t>>, Is_dropout=true, Is_causal=false, Is_even_N=true, Is_even_K=false, Return_softmax=false, Params=Flash_fwd_params]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(15): here
            instantiation of "void flash_fwd_kernel<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax>(Flash_fwd_params) [with Kernel_traits=Flash_fwd_kernel_traits<192, 64, 64, 4, false, false, cutlass::half_t, Flash_kernel_traits<192, 64, 64, 4, cutlass::half_t>>, Is_dropout=true, Is_causal=false, Is_even_N=true, Is_even_K=false, Return_softmax=false]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(34): here
            instantiation of "void run_flash_fwd<Kernel_traits,Is_dropout,Is_causal>(Flash_fwd_params &, cudaStream_t) [with Kernel_traits=Flash_fwd_kernel_traits<192, 64, 64, 4, false, false, cutlass::half_t, Flash_kernel_traits<192, 64, 64, 4, cutlass::half_t>>, Is_dropout=true, Is_causal=false]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(183): here
            instantiation of "void run_mha_fwd_hdim192<T>(Flash_fwd_params &, cudaStream_t) [with T=cutlass::half_t]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_hdim192_fp16_sm80.cu(26): here

/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_kernel.h(140): warning: variable "MMA_M" was declared but never referenced
          detected during:
            instantiation of "void flash::compute_attn_1rowblock<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &, int, int, int) [with Kernel_traits=Flash_fwd_kernel_traits<192, 64, 64, 4, false, false, cutlass::half_t, Flash_kernel_traits<192, 64, 64, 4, cutlass::half_t>>, Is_dropout=true, Is_causal=false, Is_even_N=false, Is_even_K=true, Return_softmax=true, Params=Flash_fwd_params]" 
(588): here
            instantiation of "void flash::compute_attn<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &) [with Kernel_traits=Flash_fwd_kernel_traits<192, 64, 64, 4, false, false, cutlass::half_t, Flash_kernel_traits<192, 64, 64, 4, cutlass::half_t>>, Is_dropout=true, Is_causal=false, Is_even_N=false, Is_even_K=true, Return_softmax=true, Params=Flash_fwd_params]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(15): here
            instantiation of "void flash_fwd_kernel<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax>(Flash_fwd_params) [with Kernel_traits=Flash_fwd_kernel_traits<192, 64, 64, 4, false, false, cutlass::half_t, Flash_kernel_traits<192, 64, 64, 4, cutlass::half_t>>, Is_dropout=true, Is_causal=false, Is_even_N=false, Is_even_K=true, Return_softmax=true]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(34): here
            instantiation of "void run_flash_fwd<Kernel_traits,Is_dropout,Is_causal>(Flash_fwd_params &, cudaStream_t) [with Kernel_traits=Flash_fwd_kernel_traits<192, 64, 64, 4, false, false, cutlass::half_t, Flash_kernel_traits<192, 64, 64, 4, cutlass::half_t>>, Is_dropout=true, Is_causal=false]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(183): here
            instantiation of "void run_mha_fwd_hdim192<T>(Flash_fwd_params &, cudaStream_t) [with T=cutlass::half_t]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_hdim192_fp16_sm80.cu(26): here

/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_kernel.h(275): warning: variable "tQrQ" was declared but never referenced
          detected during:
            instantiation of "void flash::compute_attn_1rowblock<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &, int, int, int) [with Kernel_traits=Flash_fwd_kernel_traits<192, 64, 64, 4, false, false, cutlass::half_t, Flash_kernel_traits<192, 64, 64, 4, cutlass::half_t>>, Is_dropout=true, Is_causal=false, Is_even_N=false, Is_even_K=true, Return_softmax=true, Params=Flash_fwd_params]" 
(588): here
            instantiation of "void flash::compute_attn<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &) [with Kernel_traits=Flash_fwd_kernel_traits<192, 64, 64, 4, false, false, cutlass::half_t, Flash_kernel_traits<192, 64, 64, 4, cutlass::half_t>>, Is_dropout=true, Is_causal=false, Is_even_N=false, Is_even_K=true, Return_softmax=true, Params=Flash_fwd_params]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(15): here
            instantiation of "void flash_fwd_kernel<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax>(Flash_fwd_params) [with Kernel_traits=Flash_fwd_kernel_traits<192, 64, 64, 4, false, false, cutlass::half_t, Flash_kernel_traits<192, 64, 64, 4, cutlass::half_t>>, Is_dropout=true, Is_causal=false, Is_even_N=false, Is_even_K=true, Return_softmax=true]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(34): here
            instantiation of "void run_flash_fwd<Kernel_traits,Is_dropout,Is_causal>(Flash_fwd_params &, cudaStream_t) [with Kernel_traits=Flash_fwd_kernel_traits<192, 64, 64, 4, false, false, cutlass::half_t, Flash_kernel_traits<192, 64, 64, 4, cutlass::half_t>>, Is_dropout=true, Is_causal=false]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(183): here
            instantiation of "void run_mha_fwd_hdim192<T>(Flash_fwd_params &, cudaStream_t) [with T=cutlass::half_t]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_hdim192_fp16_sm80.cu(26): here

/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_kernel.h(140): warning: variable "MMA_M" was declared but never referenced
          detected during:
            instantiation of "void flash::compute_attn_1rowblock<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &, int, int, int) [with Kernel_traits=Flash_fwd_kernel_traits<192, 64, 64, 4, false, false, cutlass::half_t, Flash_kernel_traits<192, 64, 64, 4, cutlass::half_t>>, Is_dropout=true, Is_causal=false, Is_even_N=false, Is_even_K=true, Return_softmax=false, Params=Flash_fwd_params]" 
(588): here
            instantiation of "void flash::compute_attn<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &) [with Kernel_traits=Flash_fwd_kernel_traits<192, 64, 64, 4, false, false, cutlass::half_t, Flash_kernel_traits<192, 64, 64, 4, cutlass::half_t>>, Is_dropout=true, Is_causal=false, Is_even_N=false, Is_even_K=true, Return_softmax=false, Params=Flash_fwd_params]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(15): here
            instantiation of "void flash_fwd_kernel<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax>(Flash_fwd_params) [with Kernel_traits=Flash_fwd_kernel_traits<192, 64, 64, 4, false, false, cutlass::half_t, Flash_kernel_traits<192, 64, 64, 4, cutlass::half_t>>, Is_dropout=true, Is_causal=false, Is_even_N=false, Is_even_K=true, Return_softmax=false]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(34): here
            instantiation of "void run_flash_fwd<Kernel_traits,Is_dropout,Is_causal>(Flash_fwd_params &, cudaStream_t) [with Kernel_traits=Flash_fwd_kernel_traits<192, 64, 64, 4, false, false, cutlass::half_t, Flash_kernel_traits<192, 64, 64, 4, cutlass::half_t>>, Is_dropout=true, Is_causal=false]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(183): here
            instantiation of "void run_mha_fwd_hdim192<T>(Flash_fwd_params &, cudaStream_t) [with T=cutlass::half_t]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_hdim192_fp16_sm80.cu(26): here

/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_kernel.h(275): warning: variable "tQrQ" was declared but never referenced
          detected during:
            instantiation of "void flash::compute_attn_1rowblock<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &, int, int, int) [with Kernel_traits=Flash_fwd_kernel_traits<192, 64, 64, 4, false, false, cutlass::half_t, Flash_kernel_traits<192, 64, 64, 4, cutlass::half_t>>, Is_dropout=true, Is_causal=false, Is_even_N=false, Is_even_K=true, Return_softmax=false, Params=Flash_fwd_params]" 
(588): here
            instantiation of "void flash::compute_attn<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &) [with Kernel_traits=Flash_fwd_kernel_traits<192, 64, 64, 4, false, false, cutlass::half_t, Flash_kernel_traits<192, 64, 64, 4, cutlass::half_t>>, Is_dropout=true, Is_causal=false, Is_even_N=false, Is_even_K=true, Return_softmax=false, Params=Flash_fwd_params]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(15): here
            instantiation of "void flash_fwd_kernel<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax>(Flash_fwd_params) [with Kernel_traits=Flash_fwd_kernel_traits<192, 64, 64, 4, false, false, cutlass::half_t, Flash_kernel_traits<192, 64, 64, 4, cutlass::half_t>>, Is_dropout=true, Is_causal=false, Is_even_N=false, Is_even_K=true, Return_softmax=false]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(34): here
            instantiation of "void run_flash_fwd<Kernel_traits,Is_dropout,Is_causal>(Flash_fwd_params &, cudaStream_t) [with Kernel_traits=Flash_fwd_kernel_traits<192, 64, 64, 4, false, false, cutlass::half_t, Flash_kernel_traits<192, 64, 64, 4, cutlass::half_t>>, Is_dropout=true, Is_causal=false]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(183): here
            instantiation of "void run_mha_fwd_hdim192<T>(Flash_fwd_params &, cudaStream_t) [with T=cutlass::half_t]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_hdim192_fp16_sm80.cu(26): here

/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_kernel.h(140): warning: variable "MMA_M" was declared but never referenced
          detected during:
            instantiation of "void flash::compute_attn_1rowblock<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &, int, int, int) [with Kernel_traits=Flash_fwd_kernel_traits<192, 64, 64, 4, false, false, cutlass::half_t, Flash_kernel_traits<192, 64, 64, 4, cutlass::half_t>>, Is_dropout=true, Is_causal=false, Is_even_N=false, Is_even_K=false, Return_softmax=true, Params=Flash_fwd_params]" 
(588): here
            instantiation of "void flash::compute_attn<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &) [with Kernel_traits=Flash_fwd_kernel_traits<192, 64, 64, 4, false, false, cutlass::half_t, Flash_kernel_traits<192, 64, 64, 4, cutlass::half_t>>, Is_dropout=true, Is_causal=false, Is_even_N=false, Is_even_K=false, Return_softmax=true, Params=Flash_fwd_params]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(15): here
            instantiation of "void flash_fwd_kernel<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax>(Flash_fwd_params) [with Kernel_traits=Flash_fwd_kernel_traits<192, 64, 64, 4, false, false, cutlass::half_t, Flash_kernel_traits<192, 64, 64, 4, cutlass::half_t>>, Is_dropout=true, Is_causal=false, Is_even_N=false, Is_even_K=false, Return_softmax=true]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(34): here
            instantiation of "void run_flash_fwd<Kernel_traits,Is_dropout,Is_causal>(Flash_fwd_params &, cudaStream_t) [with Kernel_traits=Flash_fwd_kernel_traits<192, 64, 64, 4, false, false, cutlass::half_t, Flash_kernel_traits<192, 64, 64, 4, cutlass::half_t>>, Is_dropout=true, Is_causal=false]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(183): here
            instantiation of "void run_mha_fwd_hdim192<T>(Flash_fwd_params &, cudaStream_t) [with T=cutlass::half_t]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_hdim192_fp16_sm80.cu(26): here

/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_kernel.h(275): warning: variable "tQrQ" was declared but never referenced
          detected during:
            instantiation of "void flash::compute_attn_1rowblock<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &, int, int, int) [with Kernel_traits=Flash_fwd_kernel_traits<192, 64, 64, 4, false, false, cutlass::half_t, Flash_kernel_traits<192, 64, 64, 4, cutlass::half_t>>, Is_dropout=true, Is_causal=false, Is_even_N=false, Is_even_K=false, Return_softmax=true, Params=Flash_fwd_params]" 
(588): here
            instantiation of "void flash::compute_attn<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &) [with Kernel_traits=Flash_fwd_kernel_traits<192, 64, 64, 4, false, false, cutlass::half_t, Flash_kernel_traits<192, 64, 64, 4, cutlass::half_t>>, Is_dropout=true, Is_causal=false, Is_even_N=false, Is_even_K=false, Return_softmax=true, Params=Flash_fwd_params]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(15): here
            instantiation of "void flash_fwd_kernel<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax>(Flash_fwd_params) [with Kernel_traits=Flash_fwd_kernel_traits<192, 64, 64, 4, false, false, cutlass::half_t, Flash_kernel_traits<192, 64, 64, 4, cutlass::half_t>>, Is_dropout=true, Is_causal=false, Is_even_N=false, Is_even_K=false, Return_softmax=true]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(34): here
            instantiation of "void run_flash_fwd<Kernel_traits,Is_dropout,Is_causal>(Flash_fwd_params &, cudaStream_t) [with Kernel_traits=Flash_fwd_kernel_traits<192, 64, 64, 4, false, false, cutlass::half_t, Flash_kernel_traits<192, 64, 64, 4, cutlass::half_t>>, Is_dropout=true, Is_causal=false]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(183): here
            instantiation of "void run_mha_fwd_hdim192<T>(Flash_fwd_params &, cudaStream_t) [with T=cutlass::half_t]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_hdim192_fp16_sm80.cu(26): here

/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_kernel.h(140): warning: variable "MMA_M" was declared but never referenced
          detected during:
            instantiation of "void flash::compute_attn_1rowblock<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &, int, int, int) [with Kernel_traits=Flash_fwd_kernel_traits<192, 64, 64, 4, false, false, cutlass::half_t, Flash_kernel_traits<192, 64, 64, 4, cutlass::half_t>>, Is_dropout=true, Is_causal=false, Is_even_N=false, Is_even_K=false, Return_softmax=false, Params=Flash_fwd_params]" 
(588): here
            instantiation of "void flash::compute_attn<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &) [with Kernel_traits=Flash_fwd_kernel_traits<192, 64, 64, 4, false, false, cutlass::half_t, Flash_kernel_traits<192, 64, 64, 4, cutlass::half_t>>, Is_dropout=true, Is_causal=false, Is_even_N=false, Is_even_K=false, Return_softmax=false, Params=Flash_fwd_params]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(15): here
            instantiation of "void flash_fwd_kernel<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax>(Flash_fwd_params) [with Kernel_traits=Flash_fwd_kernel_traits<192, 64, 64, 4, false, false, cutlass::half_t, Flash_kernel_traits<192, 64, 64, 4, cutlass::half_t>>, Is_dropout=true, Is_causal=false, Is_even_N=false, Is_even_K=false, Return_softmax=false]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(34): here
            instantiation of "void run_flash_fwd<Kernel_traits,Is_dropout,Is_causal>(Flash_fwd_params &, cudaStream_t) [with Kernel_traits=Flash_fwd_kernel_traits<192, 64, 64, 4, false, false, cutlass::half_t, Flash_kernel_traits<192, 64, 64, 4, cutlass::half_t>>, Is_dropout=true, Is_causal=false]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(183): here
            instantiation of "void run_mha_fwd_hdim192<T>(Flash_fwd_params &, cudaStream_t) [with T=cutlass::half_t]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_hdim192_fp16_sm80.cu(26): here

/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_kernel.h(275): warning: variable "tQrQ" was declared but never referenced
          detected during:
            instantiation of "void flash::compute_attn_1rowblock<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &, int, int, int) [with Kernel_traits=Flash_fwd_kernel_traits<192, 64, 64, 4, false, false, cutlass::half_t, Flash_kernel_traits<192, 64, 64, 4, cutlass::half_t>>, Is_dropout=true, Is_causal=false, Is_even_N=false, Is_even_K=false, Return_softmax=false, Params=Flash_fwd_params]" 
(588): here
            instantiation of "void flash::compute_attn<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &) [with Kernel_traits=Flash_fwd_kernel_traits<192, 64, 64, 4, false, false, cutlass::half_t, Flash_kernel_traits<192, 64, 64, 4, cutlass::half_t>>, Is_dropout=true, Is_causal=false, Is_even_N=false, Is_even_K=false, Return_softmax=false, Params=Flash_fwd_params]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(15): here
            instantiation of "void flash_fwd_kernel<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax>(Flash_fwd_params) [with Kernel_traits=Flash_fwd_kernel_traits<192, 64, 64, 4, false, false, cutlass::half_t, Flash_kernel_traits<192, 64, 64, 4, cutlass::half_t>>, Is_dropout=true, Is_causal=false, Is_even_N=false, Is_even_K=false, Return_softmax=false]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(34): here
            instantiation of "void run_flash_fwd<Kernel_traits,Is_dropout,Is_causal>(Flash_fwd_params &, cudaStream_t) [with Kernel_traits=Flash_fwd_kernel_traits<192, 64, 64, 4, false, false, cutlass::half_t, Flash_kernel_traits<192, 64, 64, 4, cutlass::half_t>>, Is_dropout=true, Is_causal=false]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(183): here
            instantiation of "void run_mha_fwd_hdim192<T>(Flash_fwd_params &, cudaStream_t) [with T=cutlass::half_t]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_hdim192_fp16_sm80.cu(26): here

/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_kernel.h(140): warning: variable "MMA_M" was declared but never referenced
          detected during:
            instantiation of "void flash::compute_attn_1rowblock<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &, int, int, int) [with Kernel_traits=Flash_fwd_kernel_traits<192, 128, 64, 8, false, false, cutlass::half_t, Flash_kernel_traits<192, 128, 64, 8, cutlass::half_t>>, Is_dropout=false, Is_causal=true, Is_even_N=true, Is_even_K=true, Return_softmax=false, Params=Flash_fwd_params]" 
(588): here
            instantiation of "void flash::compute_attn<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &) [with Kernel_traits=Flash_fwd_kernel_traits<192, 128, 64, 8, false, false, cutlass::half_t, Flash_kernel_traits<192, 128, 64, 8, cutlass::half_t>>, Is_dropout=false, Is_causal=true, Is_even_N=true, Is_even_K=true, Return_softmax=false, Params=Flash_fwd_params]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(15): here
            instantiation of "void flash_fwd_kernel<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax>(Flash_fwd_params) [with Kernel_traits=Flash_fwd_kernel_traits<192, 128, 64, 8, false, false, cutlass::half_t, Flash_kernel_traits<192, 128, 64, 8, cutlass::half_t>>, Is_dropout=false, Is_causal=true, Is_even_N=true, Is_even_K=true, Return_softmax=false]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(34): here
            instantiation of "void run_flash_fwd<Kernel_traits,Is_dropout,Is_causal>(Flash_fwd_params &, cudaStream_t) [with Kernel_traits=Flash_fwd_kernel_traits<192, 128, 64, 8, false, false, cutlass::half_t, Flash_kernel_traits<192, 128, 64, 8, cutlass::half_t>>, Is_dropout=false, Is_causal=true]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(183): here
            instantiation of "void run_mha_fwd_hdim192<T>(Flash_fwd_params &, cudaStream_t) [with T=cutlass::half_t]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_hdim192_fp16_sm80.cu(26): here

/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_kernel.h(275): warning: variable "tQrQ" was declared but never referenced
          detected during:
            instantiation of "void flash::compute_attn_1rowblock<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &, int, int, int) [with Kernel_traits=Flash_fwd_kernel_traits<192, 128, 64, 8, false, false, cutlass::half_t, Flash_kernel_traits<192, 128, 64, 8, cutlass::half_t>>, Is_dropout=false, Is_causal=true, Is_even_N=true, Is_even_K=true, Return_softmax=false, Params=Flash_fwd_params]" 
(588): here
            instantiation of "void flash::compute_attn<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &) [with Kernel_traits=Flash_fwd_kernel_traits<192, 128, 64, 8, false, false, cutlass::half_t, Flash_kernel_traits<192, 128, 64, 8, cutlass::half_t>>, Is_dropout=false, Is_causal=true, Is_even_N=true, Is_even_K=true, Return_softmax=false, Params=Flash_fwd_params]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(15): here
            instantiation of "void flash_fwd_kernel<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax>(Flash_fwd_params) [with Kernel_traits=Flash_fwd_kernel_traits<192, 128, 64, 8, false, false, cutlass::half_t, Flash_kernel_traits<192, 128, 64, 8, cutlass::half_t>>, Is_dropout=false, Is_causal=true, Is_even_N=true, Is_even_K=true, Return_softmax=false]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(34): here
            instantiation of "void run_flash_fwd<Kernel_traits,Is_dropout,Is_causal>(Flash_fwd_params &, cudaStream_t) [with Kernel_traits=Flash_fwd_kernel_traits<192, 128, 64, 8, false, false, cutlass::half_t, Flash_kernel_traits<192, 128, 64, 8, cutlass::half_t>>, Is_dropout=false, Is_causal=true]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(183): here
            instantiation of "void run_mha_fwd_hdim192<T>(Flash_fwd_params &, cudaStream_t) [with T=cutlass::half_t]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_hdim192_fp16_sm80.cu(26): here

/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_kernel.h(140): warning: variable "MMA_M" was declared but never referenced
          detected during:
            instantiation of "void flash::compute_attn_1rowblock<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &, int, int, int) [with Kernel_traits=Flash_fwd_kernel_traits<192, 128, 64, 8, false, false, cutlass::half_t, Flash_kernel_traits<192, 128, 64, 8, cutlass::half_t>>, Is_dropout=false, Is_causal=true, Is_even_N=true, Is_even_K=false, Return_softmax=false, Params=Flash_fwd_params]" 
(588): here
            instantiation of "void flash::compute_attn<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &) [with Kernel_traits=Flash_fwd_kernel_traits<192, 128, 64, 8, false, false, cutlass::half_t, Flash_kernel_traits<192, 128, 64, 8, cutlass::half_t>>, Is_dropout=false, Is_causal=true, Is_even_N=true, Is_even_K=false, Return_softmax=false, Params=Flash_fwd_params]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(15): here
            instantiation of "void flash_fwd_kernel<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax>(Flash_fwd_params) [with Kernel_traits=Flash_fwd_kernel_traits<192, 128, 64, 8, false, false, cutlass::half_t, Flash_kernel_traits<192, 128, 64, 8, cutlass::half_t>>, Is_dropout=false, Is_causal=true, Is_even_N=true, Is_even_K=false, Return_softmax=false]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(34): here
            instantiation of "void run_flash_fwd<Kernel_traits,Is_dropout,Is_causal>(Flash_fwd_params &, cudaStream_t) [with Kernel_traits=Flash_fwd_kernel_traits<192, 128, 64, 8, false, false, cutlass::half_t, Flash_kernel_traits<192, 128, 64, 8, cutlass::half_t>>, Is_dropout=false, Is_causal=true]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(183): here
            instantiation of "void run_mha_fwd_hdim192<T>(Flash_fwd_params &, cudaStream_t) [with T=cutlass::half_t]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_hdim192_fp16_sm80.cu(26): here

/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_kernel.h(275): warning: variable "tQrQ" was declared but never referenced
          detected during:
            instantiation of "void flash::compute_attn_1rowblock<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &, int, int, int) [with Kernel_traits=Flash_fwd_kernel_traits<192, 128, 64, 8, false, false, cutlass::half_t, Flash_kernel_traits<192, 128, 64, 8, cutlass::half_t>>, Is_dropout=false, Is_causal=true, Is_even_N=true, Is_even_K=false, Return_softmax=false, Params=Flash_fwd_params]" 
(588): here
            instantiation of "void flash::compute_attn<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &) [with Kernel_traits=Flash_fwd_kernel_traits<192, 128, 64, 8, false, false, cutlass::half_t, Flash_kernel_traits<192, 128, 64, 8, cutlass::half_t>>, Is_dropout=false, Is_causal=true, Is_even_N=true, Is_even_K=false, Return_softmax=false, Params=Flash_fwd_params]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(15): here
            instantiation of "void flash_fwd_kernel<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax>(Flash_fwd_params) [with Kernel_traits=Flash_fwd_kernel_traits<192, 128, 64, 8, false, false, cutlass::half_t, Flash_kernel_traits<192, 128, 64, 8, cutlass::half_t>>, Is_dropout=false, Is_causal=true, Is_even_N=true, Is_even_K=false, Return_softmax=false]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(34): here
            instantiation of "void run_flash_fwd<Kernel_traits,Is_dropout,Is_causal>(Flash_fwd_params &, cudaStream_t) [with Kernel_traits=Flash_fwd_kernel_traits<192, 128, 64, 8, false, false, cutlass::half_t, Flash_kernel_traits<192, 128, 64, 8, cutlass::half_t>>, Is_dropout=false, Is_causal=true]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(183): here
            instantiation of "void run_mha_fwd_hdim192<T>(Flash_fwd_params &, cudaStream_t) [with T=cutlass::half_t]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_hdim192_fp16_sm80.cu(26): here

/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_kernel.h(140): warning: variable "MMA_M" was declared but never referenced
          detected during:
            instantiation of "void flash::compute_attn_1rowblock<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &, int, int, int) [with Kernel_traits=Flash_fwd_kernel_traits<192, 128, 64, 8, false, false, cutlass::half_t, Flash_kernel_traits<192, 128, 64, 8, cutlass::half_t>>, Is_dropout=false, Is_causal=true, Is_even_N=false, Is_even_K=true, Return_softmax=false, Params=Flash_fwd_params]" 
(588): here
            instantiation of "void flash::compute_attn<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &) [with Kernel_traits=Flash_fwd_kernel_traits<192, 128, 64, 8, false, false, cutlass::half_t, Flash_kernel_traits<192, 128, 64, 8, cutlass::half_t>>, Is_dropout=false, Is_causal=true, Is_even_N=false, Is_even_K=true, Return_softmax=false, Params=Flash_fwd_params]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(15): here
            instantiation of "void flash_fwd_kernel<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax>(Flash_fwd_params) [with Kernel_traits=Flash_fwd_kernel_traits<192, 128, 64, 8, false, false, cutlass::half_t, Flash_kernel_traits<192, 128, 64, 8, cutlass::half_t>>, Is_dropout=false, Is_causal=true, Is_even_N=false, Is_even_K=true, Return_softmax=false]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(34): here
            instantiation of "void run_flash_fwd<Kernel_traits,Is_dropout,Is_causal>(Flash_fwd_params &, cudaStream_t) [with Kernel_traits=Flash_fwd_kernel_traits<192, 128, 64, 8, false, false, cutlass::half_t, Flash_kernel_traits<192, 128, 64, 8, cutlass::half_t>>, Is_dropout=false, Is_causal=true]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(183): here
            instantiation of "void run_mha_fwd_hdim192<T>(Flash_fwd_params &, cudaStream_t) [with T=cutlass::half_t]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_hdim192_fp16_sm80.cu(26): here

/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_kernel.h(275): warning: variable "tQrQ" was declared but never referenced
          detected during:
            instantiation of "void flash::compute_attn_1rowblock<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &, int, int, int) [with Kernel_traits=Flash_fwd_kernel_traits<192, 128, 64, 8, false, false, cutlass::half_t, Flash_kernel_traits<192, 128, 64, 8, cutlass::half_t>>, Is_dropout=false, Is_causal=true, Is_even_N=false, Is_even_K=true, Return_softmax=false, Params=Flash_fwd_params]" 
(588): here
            instantiation of "void flash::compute_attn<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &) [with Kernel_traits=Flash_fwd_kernel_traits<192, 128, 64, 8, false, false, cutlass::half_t, Flash_kernel_traits<192, 128, 64, 8, cutlass::half_t>>, Is_dropout=false, Is_causal=true, Is_even_N=false, Is_even_K=true, Return_softmax=false, Params=Flash_fwd_params]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(15): here
            instantiation of "void flash_fwd_kernel<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax>(Flash_fwd_params) [with Kernel_traits=Flash_fwd_kernel_traits<192, 128, 64, 8, false, false, cutlass::half_t, Flash_kernel_traits<192, 128, 64, 8, cutlass::half_t>>, Is_dropout=false, Is_causal=true, Is_even_N=false, Is_even_K=true, Return_softmax=false]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(34): here
            instantiation of "void run_flash_fwd<Kernel_traits,Is_dropout,Is_causal>(Flash_fwd_params &, cudaStream_t) [with Kernel_traits=Flash_fwd_kernel_traits<192, 128, 64, 8, false, false, cutlass::half_t, Flash_kernel_traits<192, 128, 64, 8, cutlass::half_t>>, Is_dropout=false, Is_causal=true]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(183): here
            instantiation of "void run_mha_fwd_hdim192<T>(Flash_fwd_params &, cudaStream_t) [with T=cutlass::half_t]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_hdim192_fp16_sm80.cu(26): here

/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_kernel.h(140): warning: variable "MMA_M" was declared but never referenced
          detected during:
            instantiation of "void flash::compute_attn_1rowblock<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &, int, int, int) [with Kernel_traits=Flash_fwd_kernel_traits<192, 128, 64, 8, false, false, cutlass::half_t, Flash_kernel_traits<192, 128, 64, 8, cutlass::half_t>>, Is_dropout=false, Is_causal=true, Is_even_N=false, Is_even_K=false, Return_softmax=false, Params=Flash_fwd_params]" 
(588): here
            instantiation of "void flash::compute_attn<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &) [with Kernel_traits=Flash_fwd_kernel_traits<192, 128, 64, 8, false, false, cutlass::half_t, Flash_kernel_traits<192, 128, 64, 8, cutlass::half_t>>, Is_dropout=false, Is_causal=true, Is_even_N=false, Is_even_K=false, Return_softmax=false, Params=Flash_fwd_params]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(15): here
            instantiation of "void flash_fwd_kernel<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax>(Flash_fwd_params) [with Kernel_traits=Flash_fwd_kernel_traits<192, 128, 64, 8, false, false, cutlass::half_t, Flash_kernel_traits<192, 128, 64, 8, cutlass::half_t>>, Is_dropout=false, Is_causal=true, Is_even_N=false, Is_even_K=false, Return_softmax=false]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(34): here
            instantiation of "void run_flash_fwd<Kernel_traits,Is_dropout,Is_causal>(Flash_fwd_params &, cudaStream_t) [with Kernel_traits=Flash_fwd_kernel_traits<192, 128, 64, 8, false, false, cutlass::half_t, Flash_kernel_traits<192, 128, 64, 8, cutlass::half_t>>, Is_dropout=false, Is_causal=true]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(183): here
            instantiation of "void run_mha_fwd_hdim192<T>(Flash_fwd_params &, cudaStream_t) [with T=cutlass::half_t]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_hdim192_fp16_sm80.cu(26): here

/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_kernel.h(275): warning: variable "tQrQ" was declared but never referenced
          detected during:
            instantiation of "void flash::compute_attn_1rowblock<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &, int, int, int) [with Kernel_traits=Flash_fwd_kernel_traits<192, 128, 64, 8, false, false, cutlass::half_t, Flash_kernel_traits<192, 128, 64, 8, cutlass::half_t>>, Is_dropout=false, Is_causal=true, Is_even_N=false, Is_even_K=false, Return_softmax=false, Params=Flash_fwd_params]" 
(588): here
            instantiation of "void flash::compute_attn<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &) [with Kernel_traits=Flash_fwd_kernel_traits<192, 128, 64, 8, false, false, cutlass::half_t, Flash_kernel_traits<192, 128, 64, 8, cutlass::half_t>>, Is_dropout=false, Is_causal=true, Is_even_N=false, Is_even_K=false, Return_softmax=false, Params=Flash_fwd_params]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(15): here
            instantiation of "void flash_fwd_kernel<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax>(Flash_fwd_params) [with Kernel_traits=Flash_fwd_kernel_traits<192, 128, 64, 8, false, false, cutlass::half_t, Flash_kernel_traits<192, 128, 64, 8, cutlass::half_t>>, Is_dropout=false, Is_causal=true, Is_even_N=false, Is_even_K=false, Return_softmax=false]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(34): here
            instantiation of "void run_flash_fwd<Kernel_traits,Is_dropout,Is_causal>(Flash_fwd_params &, cudaStream_t) [with Kernel_traits=Flash_fwd_kernel_traits<192, 128, 64, 8, false, false, cutlass::half_t, Flash_kernel_traits<192, 128, 64, 8, cutlass::half_t>>, Is_dropout=false, Is_causal=true]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(183): here
            instantiation of "void run_mha_fwd_hdim192<T>(Flash_fwd_params &, cudaStream_t) [with T=cutlass::half_t]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_hdim192_fp16_sm80.cu(26): here

/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_kernel.h(140): warning: variable "MMA_M" was declared but never referenced
          detected during:
            instantiation of "void flash::compute_attn_1rowblock<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &, int, int, int) [with Kernel_traits=Flash_fwd_kernel_traits<192, 128, 64, 8, false, false, cutlass::half_t, Flash_kernel_traits<192, 128, 64, 8, cutlass::half_t>>, Is_dropout=false, Is_causal=false, Is_even_N=true, Is_even_K=true, Return_softmax=false, Params=Flash_fwd_params]" 
(588): here
            instantiation of "void flash::compute_attn<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &) [with Kernel_traits=Flash_fwd_kernel_traits<192, 128, 64, 8, false, false, cutlass::half_t, Flash_kernel_traits<192, 128, 64, 8, cutlass::half_t>>, Is_dropout=false, Is_causal=false, Is_even_N=true, Is_even_K=true, Return_softmax=false, Params=Flash_fwd_params]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(15): here
            instantiation of "void flash_fwd_kernel<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax>(Flash_fwd_params) [with Kernel_traits=Flash_fwd_kernel_traits<192, 128, 64, 8, false, false, cutlass::half_t, Flash_kernel_traits<192, 128, 64, 8, cutlass::half_t>>, Is_dropout=false, Is_causal=false, Is_even_N=true, Is_even_K=true, Return_softmax=false]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(34): here
            instantiation of "void run_flash_fwd<Kernel_traits,Is_dropout,Is_causal>(Flash_fwd_params &, cudaStream_t) [with Kernel_traits=Flash_fwd_kernel_traits<192, 128, 64, 8, false, false, cutlass::half_t, Flash_kernel_traits<192, 128, 64, 8, cutlass::half_t>>, Is_dropout=false, Is_causal=false]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(183): here
            instantiation of "void run_mha_fwd_hdim192<T>(Flash_fwd_params &, cudaStream_t) [with T=cutlass::half_t]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_hdim192_fp16_sm80.cu(26): here

/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_kernel.h(275): warning: variable "tQrQ" was declared but never referenced
          detected during:
            instantiation of "void flash::compute_attn_1rowblock<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &, int, int, int) [with Kernel_traits=Flash_fwd_kernel_traits<192, 128, 64, 8, false, false, cutlass::half_t, Flash_kernel_traits<192, 128, 64, 8, cutlass::half_t>>, Is_dropout=false, Is_causal=false, Is_even_N=true, Is_even_K=true, Return_softmax=false, Params=Flash_fwd_params]" 
(588): here
            instantiation of "void flash::compute_attn<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &) [with Kernel_traits=Flash_fwd_kernel_traits<192, 128, 64, 8, false, false, cutlass::half_t, Flash_kernel_traits<192, 128, 64, 8, cutlass::half_t>>, Is_dropout=false, Is_causal=false, Is_even_N=true, Is_even_K=true, Return_softmax=false, Params=Flash_fwd_params]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(15): here
            instantiation of "void flash_fwd_kernel<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax>(Flash_fwd_params) [with Kernel_traits=Flash_fwd_kernel_traits<192, 128, 64, 8, false, false, cutlass::half_t, Flash_kernel_traits<192, 128, 64, 8, cutlass::half_t>>, Is_dropout=false, Is_causal=false, Is_even_N=true, Is_even_K=true, Return_softmax=false]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(34): here
            instantiation of "void run_flash_fwd<Kernel_traits,Is_dropout,Is_causal>(Flash_fwd_params &, cudaStream_t) [with Kernel_traits=Flash_fwd_kernel_traits<192, 128, 64, 8, false, false, cutlass::half_t, Flash_kernel_traits<192, 128, 64, 8, cutlass::half_t>>, Is_dropout=false, Is_causal=false]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(183): here
            instantiation of "void run_mha_fwd_hdim192<T>(Flash_fwd_params &, cudaStream_t) [with T=cutlass::half_t]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_hdim192_fp16_sm80.cu(26): here

/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_kernel.h(140): warning: variable "MMA_M" was declared but never referenced
          detected during:
            instantiation of "void flash::compute_attn_1rowblock<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &, int, int, int) [with Kernel_traits=Flash_fwd_kernel_traits<192, 128, 64, 8, false, false, cutlass::half_t, Flash_kernel_traits<192, 128, 64, 8, cutlass::half_t>>, Is_dropout=false, Is_causal=false, Is_even_N=true, Is_even_K=false, Return_softmax=false, Params=Flash_fwd_params]" 
(588): here
            instantiation of "void flash::compute_attn<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &) [with Kernel_traits=Flash_fwd_kernel_traits<192, 128, 64, 8, false, false, cutlass::half_t, Flash_kernel_traits<192, 128, 64, 8, cutlass::half_t>>, Is_dropout=false, Is_causal=false, Is_even_N=true, Is_even_K=false, Return_softmax=false, Params=Flash_fwd_params]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(15): here
            instantiation of "void flash_fwd_kernel<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax>(Flash_fwd_params) [with Kernel_traits=Flash_fwd_kernel_traits<192, 128, 64, 8, false, false, cutlass::half_t, Flash_kernel_traits<192, 128, 64, 8, cutlass::half_t>>, Is_dropout=false, Is_causal=false, Is_even_N=true, Is_even_K=false, Return_softmax=false]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(34): here
            instantiation of "void run_flash_fwd<Kernel_traits,Is_dropout,Is_causal>(Flash_fwd_params &, cudaStream_t) [with Kernel_traits=Flash_fwd_kernel_traits<192, 128, 64, 8, false, false, cutlass::half_t, Flash_kernel_traits<192, 128, 64, 8, cutlass::half_t>>, Is_dropout=false, Is_causal=false]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(183): here
            instantiation of "void run_mha_fwd_hdim192<T>(Flash_fwd_params &, cudaStream_t) [with T=cutlass::half_t]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_hdim192_fp16_sm80.cu(26): here

/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_kernel.h(275): warning: variable "tQrQ" was declared but never referenced
          detected during:
            instantiation of "void flash::compute_attn_1rowblock<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &, int, int, int) [with Kernel_traits=Flash_fwd_kernel_traits<192, 128, 64, 8, false, false, cutlass::half_t, Flash_kernel_traits<192, 128, 64, 8, cutlass::half_t>>, Is_dropout=false, Is_causal=false, Is_even_N=true, Is_even_K=false, Return_softmax=false, Params=Flash_fwd_params]" 
(588): here
            instantiation of "void flash::compute_attn<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &) [with Kernel_traits=Flash_fwd_kernel_traits<192, 128, 64, 8, false, false, cutlass::half_t, Flash_kernel_traits<192, 128, 64, 8, cutlass::half_t>>, Is_dropout=false, Is_causal=false, Is_even_N=true, Is_even_K=false, Return_softmax=false, Params=Flash_fwd_params]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(15): here
            instantiation of "void flash_fwd_kernel<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax>(Flash_fwd_params) [with Kernel_traits=Flash_fwd_kernel_traits<192, 128, 64, 8, false, false, cutlass::half_t, Flash_kernel_traits<192, 128, 64, 8, cutlass::half_t>>, Is_dropout=false, Is_causal=false, Is_even_N=true, Is_even_K=false, Return_softmax=false]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(34): here
            instantiation of "void run_flash_fwd<Kernel_traits,Is_dropout,Is_causal>(Flash_fwd_params &, cudaStream_t) [with Kernel_traits=Flash_fwd_kernel_traits<192, 128, 64, 8, false, false, cutlass::half_t, Flash_kernel_traits<192, 128, 64, 8, cutlass::half_t>>, Is_dropout=false, Is_causal=false]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(183): here
            instantiation of "void run_mha_fwd_hdim192<T>(Flash_fwd_params &, cudaStream_t) [with T=cutlass::half_t]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_hdim192_fp16_sm80.cu(26): here

/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_kernel.h(140): warning: variable "MMA_M" was declared but never referenced
          detected during:
            instantiation of "void flash::compute_attn_1rowblock<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &, int, int, int) [with Kernel_traits=Flash_fwd_kernel_traits<192, 128, 64, 8, false, false, cutlass::half_t, Flash_kernel_traits<192, 128, 64, 8, cutlass::half_t>>, Is_dropout=false, Is_causal=false, Is_even_N=false, Is_even_K=true, Return_softmax=false, Params=Flash_fwd_params]" 
(588): here
            instantiation of "void flash::compute_attn<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &) [with Kernel_traits=Flash_fwd_kernel_traits<192, 128, 64, 8, false, false, cutlass::half_t, Flash_kernel_traits<192, 128, 64, 8, cutlass::half_t>>, Is_dropout=false, Is_causal=false, Is_even_N=false, Is_even_K=true, Return_softmax=false, Params=Flash_fwd_params]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(15): here
            instantiation of "void flash_fwd_kernel<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax>(Flash_fwd_params) [with Kernel_traits=Flash_fwd_kernel_traits<192, 128, 64, 8, false, false, cutlass::half_t, Flash_kernel_traits<192, 128, 64, 8, cutlass::half_t>>, Is_dropout=false, Is_causal=false, Is_even_N=false, Is_even_K=true, Return_softmax=false]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(34): here
            instantiation of "void run_flash_fwd<Kernel_traits,Is_dropout,Is_causal>(Flash_fwd_params &, cudaStream_t) [with Kernel_traits=Flash_fwd_kernel_traits<192, 128, 64, 8, false, false, cutlass::half_t, Flash_kernel_traits<192, 128, 64, 8, cutlass::half_t>>, Is_dropout=false, Is_causal=false]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(183): here
            instantiation of "void run_mha_fwd_hdim192<T>(Flash_fwd_params &, cudaStream_t) [with T=cutlass::half_t]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_hdim192_fp16_sm80.cu(26): here

/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_kernel.h(275): warning: variable "tQrQ" was declared but never referenced
          detected during:
            instantiation of "void flash::compute_attn_1rowblock<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &, int, int, int) [with Kernel_traits=Flash_fwd_kernel_traits<192, 128, 64, 8, false, false, cutlass::half_t, Flash_kernel_traits<192, 128, 64, 8, cutlass::half_t>>, Is_dropout=false, Is_causal=false, Is_even_N=false, Is_even_K=true, Return_softmax=false, Params=Flash_fwd_params]" 
(588): here
            instantiation of "void flash::compute_attn<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &) [with Kernel_traits=Flash_fwd_kernel_traits<192, 128, 64, 8, false, false, cutlass::half_t, Flash_kernel_traits<192, 128, 64, 8, cutlass::half_t>>, Is_dropout=false, Is_causal=false, Is_even_N=false, Is_even_K=true, Return_softmax=false, Params=Flash_fwd_params]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(15): here
            instantiation of "void flash_fwd_kernel<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax>(Flash_fwd_params) [with Kernel_traits=Flash_fwd_kernel_traits<192, 128, 64, 8, false, false, cutlass::half_t, Flash_kernel_traits<192, 128, 64, 8, cutlass::half_t>>, Is_dropout=false, Is_causal=false, Is_even_N=false, Is_even_K=true, Return_softmax=false]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(34): here
            instantiation of "void run_flash_fwd<Kernel_traits,Is_dropout,Is_causal>(Flash_fwd_params &, cudaStream_t) [with Kernel_traits=Flash_fwd_kernel_traits<192, 128, 64, 8, false, false, cutlass::half_t, Flash_kernel_traits<192, 128, 64, 8, cutlass::half_t>>, Is_dropout=false, Is_causal=false]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(183): here
            instantiation of "void run_mha_fwd_hdim192<T>(Flash_fwd_params &, cudaStream_t) [with T=cutlass::half_t]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_hdim192_fp16_sm80.cu(26): here

/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_kernel.h(140): warning: variable "MMA_M" was declared but never referenced
          detected during:
            instantiation of "void flash::compute_attn_1rowblock<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &, int, int, int) [with Kernel_traits=Flash_fwd_kernel_traits<192, 128, 64, 8, false, false, cutlass::half_t, Flash_kernel_traits<192, 128, 64, 8, cutlass::half_t>>, Is_dropout=false, Is_causal=false, Is_even_N=false, Is_even_K=false, Return_softmax=false, Params=Flash_fwd_params]" 
(588): here
            instantiation of "void flash::compute_attn<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &) [with Kernel_traits=Flash_fwd_kernel_traits<192, 128, 64, 8, false, false, cutlass::half_t, Flash_kernel_traits<192, 128, 64, 8, cutlass::half_t>>, Is_dropout=false, Is_causal=false, Is_even_N=false, Is_even_K=false, Return_softmax=false, Params=Flash_fwd_params]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(15): here
            instantiation of "void flash_fwd_kernel<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax>(Flash_fwd_params) [with Kernel_traits=Flash_fwd_kernel_traits<192, 128, 64, 8, false, false, cutlass::half_t, Flash_kernel_traits<192, 128, 64, 8, cutlass::half_t>>, Is_dropout=false, Is_causal=false, Is_even_N=false, Is_even_K=false, Return_softmax=false]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(34): here
            instantiation of "void run_flash_fwd<Kernel_traits,Is_dropout,Is_causal>(Flash_fwd_params &, cudaStream_t) [with Kernel_traits=Flash_fwd_kernel_traits<192, 128, 64, 8, false, false, cutlass::half_t, Flash_kernel_traits<192, 128, 64, 8, cutlass::half_t>>, Is_dropout=false, Is_causal=false]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(183): here
            instantiation of "void run_mha_fwd_hdim192<T>(Flash_fwd_params &, cudaStream_t) [with T=cutlass::half_t]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_hdim192_fp16_sm80.cu(26): here

/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_kernel.h(275): warning: variable "tQrQ" was declared but never referenced
          detected during:
            instantiation of "void flash::compute_attn_1rowblock<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &, int, int, int) [with Kernel_traits=Flash_fwd_kernel_traits<192, 128, 64, 8, false, false, cutlass::half_t, Flash_kernel_traits<192, 128, 64, 8, cutlass::half_t>>, Is_dropout=false, Is_causal=false, Is_even_N=false, Is_even_K=false, Return_softmax=false, Params=Flash_fwd_params]" 
(588): here
            instantiation of "void flash::compute_attn<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &) [with Kernel_traits=Flash_fwd_kernel_traits<192, 128, 64, 8, false, false, cutlass::half_t, Flash_kernel_traits<192, 128, 64, 8, cutlass::half_t>>, Is_dropout=false, Is_causal=false, Is_even_N=false, Is_even_K=false, Return_softmax=false, Params=Flash_fwd_params]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(15): here
            instantiation of "void flash_fwd_kernel<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax>(Flash_fwd_params) [with Kernel_traits=Flash_fwd_kernel_traits<192, 128, 64, 8, false, false, cutlass::half_t, Flash_kernel_traits<192, 128, 64, 8, cutlass::half_t>>, Is_dropout=false, Is_causal=false, Is_even_N=false, Is_even_K=false, Return_softmax=false]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(34): here
            instantiation of "void run_flash_fwd<Kernel_traits,Is_dropout,Is_causal>(Flash_fwd_params &, cudaStream_t) [with Kernel_traits=Flash_fwd_kernel_traits<192, 128, 64, 8, false, false, cutlass::half_t, Flash_kernel_traits<192, 128, 64, 8, cutlass::half_t>>, Is_dropout=false, Is_causal=false]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(183): here
            instantiation of "void run_mha_fwd_hdim192<T>(Flash_fwd_params &, cudaStream_t) [with T=cutlass::half_t]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_hdim192_fp16_sm80.cu(26): here

ptxas info    : 2 bytes gmem
ptxas info    : Compiling entry function '_Z16flash_fwd_kernelI23Flash_fwd_kernel_traitsILi192ELi128ELi64ELi8ELb0ELb0EN7cutlass6half_tE19Flash_kernel_traitsILi192ELi128ELi64ELi8ES2_EELb0ELb0ELb0ELb0ELb0EEv16Flash_fwd_params' for 'sm_80'
ptxas info    : Function properties for _Z16flash_fwd_kernelI23Flash_fwd_kernel_traitsILi192ELi128ELi64ELi8ELb0ELb0EN7cutlass6half_tE19Flash_kernel_traitsILi192ELi128ELi64ELi8ES2_EELb0ELb0ELb0ELb0ELb0EEv16Flash_fwd_params
    0 bytes stack frame, 0 bytes spill stores, 0 bytes spill loads
ptxas info    : Used 209 registers, 584 bytes cmem[0]
ptxas info    : Compiling entry function '_Z16flash_fwd_kernelI23Flash_fwd_kernel_traitsILi192ELi128ELi64ELi8ELb0ELb0EN7cutlass6half_tE19Flash_kernel_traitsILi192ELi128ELi64ELi8ES2_EELb0ELb0ELb0ELb1ELb0EEv16Flash_fwd_params' for 'sm_80'
ptxas info    : Function properties for _Z16flash_fwd_kernelI23Flash_fwd_kernel_traitsILi192ELi128ELi64ELi8ELb0ELb0EN7cutlass6half_tE19Flash_kernel_traitsILi192ELi128ELi64ELi8ES2_EELb0ELb0ELb0ELb1ELb0EEv16Flash_fwd_params
    0 bytes stack frame, 0 bytes spill stores, 0 bytes spill loads
ptxas info    : Used 179 registers, 584 bytes cmem[0]
ptxas info    : Compiling entry function '_Z16flash_fwd_kernelI23Flash_fwd_kernel_traitsILi192ELi128ELi64ELi8ELb0ELb0EN7cutlass6half_tE19Flash_kernel_traitsILi192ELi128ELi64ELi8ES2_EELb0ELb0ELb1ELb0ELb0EEv16Flash_fwd_params' for 'sm_80'
ptxas info    : Function properties for _Z16flash_fwd_kernelI23Flash_fwd_kernel_traitsILi192ELi128ELi64ELi8ELb0ELb0EN7cutlass6half_tE19Flash_kernel_traitsILi192ELi128ELi64ELi8ES2_EELb0ELb0ELb1ELb0ELb0EEv16Flash_fwd_params
    0 bytes stack frame, 0 bytes spill stores, 0 bytes spill loads
ptxas info    : Used 173 registers, 584 bytes cmem[0]
ptxas info    : Compiling entry function '_Z16flash_fwd_kernelI23Flash_fwd_kernel_traitsILi192ELi128ELi64ELi8ELb0ELb0EN7cutlass6half_tE19Flash_kernel_traitsILi192ELi128ELi64ELi8ES2_EELb0ELb0ELb1ELb1ELb0EEv16Flash_fwd_params' for 'sm_80'
ptxas info    : Function properties for _Z16flash_fwd_kernelI23Flash_fwd_kernel_traitsILi192ELi128ELi64ELi8ELb0ELb0EN7cutlass6half_tE19Flash_kernel_traitsILi192ELi128ELi64ELi8ES2_EELb0ELb0ELb1ELb1ELb0EEv16Flash_fwd_params
    0 bytes stack frame, 0 bytes spill stores, 0 bytes spill loads
ptxas info    : Used 173 registers, 584 bytes cmem[0]
ptxas info    : Compiling entry function '_Z16flash_fwd_kernelI23Flash_fwd_kernel_traitsILi192ELi128ELi64ELi8ELb0ELb0EN7cutlass6half_tE19Flash_kernel_traitsILi192ELi128ELi64ELi8ES2_EELb0ELb1ELb0ELb0ELb0EEv16Flash_fwd_params' for 'sm_80'
ptxas info    : Function properties for _Z16flash_fwd_kernelI23Flash_fwd_kernel_traitsILi192ELi128ELi64ELi8ELb0ELb0EN7cutlass6half_tE19Flash_kernel_traitsILi192ELi128ELi64ELi8ES2_EELb0ELb1ELb0ELb0ELb0EEv16Flash_fwd_params
    0 bytes stack frame, 0 bytes spill stores, 0 bytes spill loads
ptxas info    : Used 220 registers, 584 bytes cmem[0]
ptxas info    : Compiling entry function '_Z16flash_fwd_kernelI23Flash_fwd_kernel_traitsILi192ELi128ELi64ELi8ELb0ELb0EN7cutlass6half_tE19Flash_kernel_traitsILi192ELi128ELi64ELi8ES2_EELb0ELb1ELb0ELb1ELb0EEv16Flash_fwd_params' for 'sm_80'
ptxas info    : Function properties for _Z16flash_fwd_kernelI23Flash_fwd_kernel_traitsILi192ELi128ELi64ELi8ELb0ELb0EN7cutlass6half_tE19Flash_kernel_traitsILi192ELi128ELi64ELi8ES2_EELb0ELb1ELb0ELb1ELb0EEv16Flash_fwd_params
    0 bytes stack frame, 0 bytes spill stores, 0 bytes spill loads
ptxas info    : Used 221 registers, 584 bytes cmem[0]
ptxas info    : Compiling entry function '_Z16flash_fwd_kernelI23Flash_fwd_kernel_traitsILi192ELi128ELi64ELi8ELb0ELb0EN7cutlass6half_tE19Flash_kernel_traitsILi192ELi128ELi64ELi8ES2_EELb0ELb1ELb1ELb0ELb0EEv16Flash_fwd_params' for 'sm_80'
ptxas info    : Function properties for _Z16flash_fwd_kernelI23Flash_fwd_kernel_traitsILi192ELi128ELi64ELi8ELb0ELb0EN7cutlass6half_tE19Flash_kernel_traitsILi192ELi128ELi64ELi8ES2_EELb0ELb1ELb1ELb0ELb0EEv16Flash_fwd_params
    0 bytes stack frame, 0 bytes spill stores, 0 bytes spill loads
ptxas info    : Used 225 registers, 584 bytes cmem[0]
ptxas info    : Compiling entry function '_Z16flash_fwd_kernelI23Flash_fwd_kernel_traitsILi192ELi128ELi64ELi8ELb0ELb0EN7cutlass6half_tE19Flash_kernel_traitsILi192ELi128ELi64ELi8ES2_EELb0ELb1ELb1ELb1ELb0EEv16Flash_fwd_params' for 'sm_80'
ptxas info    : Function properties for _Z16flash_fwd_kernelI23Flash_fwd_kernel_traitsILi192ELi128ELi64ELi8ELb0ELb0EN7cutlass6half_tE19Flash_kernel_traitsILi192ELi128ELi64ELi8ES2_EELb0ELb1ELb1ELb1ELb0EEv16Flash_fwd_params
    0 bytes stack frame, 0 bytes spill stores, 0 bytes spill loads
ptxas info    : Used 230 registers, 584 bytes cmem[0]
ptxas info    : Compiling entry function '_Z16flash_fwd_kernelI23Flash_fwd_kernel_traitsILi192ELi64ELi64ELi4ELb0ELb0EN7cutlass6half_tE19Flash_kernel_traitsILi192ELi64ELi64ELi4ES2_EELb1ELb0ELb0ELb0ELb0EEv16Flash_fwd_params' for 'sm_80'
ptxas info    : Function properties for _Z16flash_fwd_kernelI23Flash_fwd_kernel_traitsILi192ELi64ELi64ELi4ELb0ELb0EN7cutlass6half_tE19Flash_kernel_traitsILi192ELi64ELi64ELi4ES2_EELb1ELb0ELb0ELb0ELb0EEv16Flash_fwd_params
    0 bytes stack frame, 0 bytes spill stores, 0 bytes spill loads
ptxas info    : Used 222 registers, 584 bytes cmem[0]
ptxas info    : Compiling entry function '_Z16flash_fwd_kernelI23Flash_fwd_kernel_traitsILi192ELi64ELi64ELi4ELb0ELb0EN7cutlass6half_tE19Flash_kernel_traitsILi192ELi64ELi64ELi4ES2_EELb1ELb0ELb0ELb0ELb1EEv16Flash_fwd_params' for 'sm_80'
ptxas info    : Function properties for _Z16flash_fwd_kernelI23Flash_fwd_kernel_traitsILi192ELi64ELi64ELi4ELb0ELb0EN7cutlass6half_tE19Flash_kernel_traitsILi192ELi64ELi64ELi4ES2_EELb1ELb0ELb0ELb0ELb1EEv16Flash_fwd_params
    40 bytes stack frame, 40 bytes spill stores, 40 bytes spill loads
ptxas info    : Used 255 registers, 584 bytes cmem[0]
ptxas info    : Compiling entry function '_Z16flash_fwd_kernelI23Flash_fwd_kernel_traitsILi192ELi64ELi64ELi4ELb0ELb0EN7cutlass6half_tE19Flash_kernel_traitsILi192ELi64ELi64ELi4ES2_EELb1ELb0ELb0ELb1ELb0EEv16Flash_fwd_params' for 'sm_80'
ptxas info    : Function properties for _Z16flash_fwd_kernelI23Flash_fwd_kernel_traitsILi192ELi64ELi64ELi4ELb0ELb0EN7cutlass6half_tE19Flash_kernel_traitsILi192ELi64ELi64ELi4ES2_EELb1ELb0ELb0ELb1ELb0EEv16Flash_fwd_params
    0 bytes stack frame, 0 bytes spill stores, 0 bytes spill loads
ptxas info    : Used 221 registers, 584 bytes cmem[0]
ptxas info    : Compiling entry function '_Z16flash_fwd_kernelI23Flash_fwd_kernel_traitsILi192ELi64ELi64ELi4ELb0ELb0EN7cutlass6half_tE19Flash_kernel_traitsILi192ELi64ELi64ELi4ES2_EELb1ELb0ELb0ELb1ELb1EEv16Flash_fwd_params' for 'sm_80'
ptxas info    : Function properties for _Z16flash_fwd_kernelI23Flash_fwd_kernel_traitsILi192ELi64ELi64ELi4ELb0ELb0EN7cutlass6half_tE19Flash_kernel_traitsILi192ELi64ELi64ELi4ES2_EELb1ELb0ELb0ELb1ELb1EEv16Flash_fwd_params
    24 bytes stack frame, 24 bytes spill stores, 24 bytes spill loads
ptxas info    : Used 255 registers, 584 bytes cmem[0]
ptxas info    : Compiling entry function '_Z16flash_fwd_kernelI23Flash_fwd_kernel_traitsILi192ELi64ELi64ELi4ELb0ELb0EN7cutlass6half_tE19Flash_kernel_traitsILi192ELi64ELi64ELi4ES2_EELb1ELb0ELb1ELb0ELb0EEv16Flash_fwd_params' for 'sm_80'
ptxas info    : Function properties for _Z16flash_fwd_kernelI23Flash_fwd_kernel_traitsILi192ELi64ELi64ELi4ELb0ELb0EN7cutlass6half_tE19Flash_kernel_traitsILi192ELi64ELi64ELi4ES2_EELb1ELb0ELb1ELb0ELb0EEv16Flash_fwd_params
    0 bytes stack frame, 0 bytes spill stores, 0 bytes spill loads
ptxas info    : Used 223 registers, 584 bytes cmem[0]
ptxas info    : Compiling entry function '_Z16flash_fwd_kernelI23Flash_fwd_kernel_traitsILi192ELi64ELi64ELi4ELb0ELb0EN7cutlass6half_tE19Flash_kernel_traitsILi192ELi64ELi64ELi4ES2_EELb1ELb0ELb1ELb0ELb1EEv16Flash_fwd_params' for 'sm_80'
ptxas info    : Function properties for _Z16flash_fwd_kernelI23Flash_fwd_kernel_traitsILi192ELi64ELi64ELi4ELb0ELb0EN7cutlass6half_tE19Flash_kernel_traitsILi192ELi64ELi64ELi4ES2_EELb1ELb0ELb1ELb0ELb1EEv16Flash_fwd_params
    32 bytes stack frame, 32 bytes spill stores, 32 bytes spill loads
ptxas info    : Used 255 registers, 584 bytes cmem[0]
ptxas info    : Compiling entry function '_Z16flash_fwd_kernelI23Flash_fwd_kernel_traitsILi192ELi64ELi64ELi4ELb0ELb0EN7cutlass6half_tE19Flash_kernel_traitsILi192ELi64ELi64ELi4ES2_EELb1ELb0ELb1ELb1ELb0EEv16Flash_fwd_params' for 'sm_80'
ptxas info    : Function properties for _Z16flash_fwd_kernelI23Flash_fwd_kernel_traitsILi192ELi64ELi64ELi4ELb0ELb0EN7cutlass6half_tE19Flash_kernel_traitsILi192ELi64ELi64ELi4ES2_EELb1ELb0ELb1ELb1ELb0EEv16Flash_fwd_params
    0 bytes stack frame, 0 bytes spill stores, 0 bytes spill loads
ptxas info    : Used 227 registers, 584 bytes cmem[0]
ptxas info    : Compiling entry function '_Z16flash_fwd_kernelI23Flash_fwd_kernel_traitsILi192ELi64ELi64ELi4ELb0ELb0EN7cutlass6half_tE19Flash_kernel_traitsILi192ELi64ELi64ELi4ES2_EELb1ELb0ELb1ELb1ELb1EEv16Flash_fwd_params' for 'sm_80'
ptxas info    : Function properties for _Z16flash_fwd_kernelI23Flash_fwd_kernel_traitsILi192ELi64ELi64ELi4ELb0ELb0EN7cutlass6half_tE19Flash_kernel_traitsILi192ELi64ELi64ELi4ES2_EELb1ELb0ELb1ELb1ELb1EEv16Flash_fwd_params
    24 bytes stack frame, 28 bytes spill stores, 20 bytes spill loads
ptxas info    : Used 255 registers, 584 bytes cmem[0]
ptxas info    : Compiling entry function '_Z16flash_fwd_kernelI23Flash_fwd_kernel_traitsILi192ELi64ELi64ELi4ELb0ELb0EN7cutlass6half_tE19Flash_kernel_traitsILi192ELi64ELi64ELi4ES2_EELb1ELb1ELb0ELb0ELb0EEv16Flash_fwd_params' for 'sm_80'
ptxas info    : Function properties for _Z16flash_fwd_kernelI23Flash_fwd_kernel_traitsILi192ELi64ELi64ELi4ELb0ELb0EN7cutlass6half_tE19Flash_kernel_traitsILi192ELi64ELi64ELi4ES2_EELb1ELb1ELb0ELb0ELb0EEv16Flash_fwd_params
    0 bytes stack frame, 0 bytes spill stores, 0 bytes spill loads
ptxas info    : Used 222 registers, 584 bytes cmem[0]
ptxas info    : Compiling entry function '_Z16flash_fwd_kernelI23Flash_fwd_kernel_traitsILi192ELi64ELi64ELi4ELb0ELb0EN7cutlass6half_tE19Flash_kernel_traitsILi192ELi64ELi64ELi4ES2_EELb1ELb1ELb0ELb0ELb1EEv16Flash_fwd_params' for 'sm_80'
ptxas info    : Function properties for _Z16flash_fwd_kernelI23Flash_fwd_kernel_traitsILi192ELi64ELi64ELi4ELb0ELb0EN7cutlass6half_tE19Flash_kernel_traitsILi192ELi64ELi64ELi4ES2_EELb1ELb1ELb0ELb0ELb1EEv16Flash_fwd_params
    40 bytes stack frame, 48 bytes spill stores, 48 bytes spill loads
ptxas info    : Used 255 registers, 584 bytes cmem[0]
ptxas info    : Compiling entry function '_Z16flash_fwd_kernelI23Flash_fwd_kernel_traitsILi192ELi64ELi64ELi4ELb0ELb0EN7cutlass6half_tE19Flash_kernel_traitsILi192ELi64ELi64ELi4ES2_EELb1ELb1ELb0ELb1ELb0EEv16Flash_fwd_params' for 'sm_80'
ptxas info    : Function properties for _Z16flash_fwd_kernelI23Flash_fwd_kernel_traitsILi192ELi64ELi64ELi4ELb0ELb0EN7cutlass6half_tE19Flash_kernel_traitsILi192ELi64ELi64ELi4ES2_EELb1ELb1ELb0ELb1ELb0EEv16Flash_fwd_params
    0 bytes stack frame, 0 bytes spill stores, 0 bytes spill loads
ptxas info    : Used 223 registers, 584 bytes cmem[0]
ptxas info    : Compiling entry function '_Z16flash_fwd_kernelI23Flash_fwd_kernel_traitsILi192ELi64ELi64ELi4ELb0ELb0EN7cutlass6half_tE19Flash_kernel_traitsILi192ELi64ELi64ELi4ES2_EELb1ELb1ELb0ELb1ELb1EEv16Flash_fwd_params' for 'sm_80'
ptxas info    : Function properties for _Z16flash_fwd_kernelI23Flash_fwd_kernel_traitsILi192ELi64ELi64ELi4ELb0ELb0EN7cutlass6half_tE19Flash_kernel_traitsILi192ELi64ELi64ELi4ES2_EELb1ELb1ELb0ELb1ELb1EEv16Flash_fwd_params
    24 bytes stack frame, 24 bytes spill stores, 24 bytes spill loads
ptxas info    : Used 255 registers, 584 bytes cmem[0]
ptxas info    : Compiling entry function '_Z16flash_fwd_kernelI23Flash_fwd_kernel_traitsILi192ELi64ELi64ELi4ELb0ELb0EN7cutlass6half_tE19Flash_kernel_traitsILi192ELi64ELi64ELi4ES2_EELb1ELb1ELb1ELb0ELb0EEv16Flash_fwd_params' for 'sm_80'
ptxas info    : Function properties for _Z16flash_fwd_kernelI23Flash_fwd_kernel_traitsILi192ELi64ELi64ELi4ELb0ELb0EN7cutlass6half_tE19Flash_kernel_traitsILi192ELi64ELi64ELi4ES2_EELb1ELb1ELb1ELb0ELb0EEv16Flash_fwd_params
    0 bytes stack frame, 0 bytes spill stores, 0 bytes spill loads
ptxas info    : Used 231 registers, 584 bytes cmem[0]
ptxas info    : Compiling entry function '_Z16flash_fwd_kernelI23Flash_fwd_kernel_traitsILi192ELi64ELi64ELi4ELb0ELb0EN7cutlass6half_tE19Flash_kernel_traitsILi192ELi64ELi64ELi4ES2_EELb1ELb1ELb1ELb0ELb1EEv16Flash_fwd_params' for 'sm_80'
ptxas info    : Function properties for _Z16flash_fwd_kernelI23Flash_fwd_kernel_traitsILi192ELi64ELi64ELi4ELb0ELb0EN7cutlass6half_tE19Flash_kernel_traitsILi192ELi64ELi64ELi4ES2_EELb1ELb1ELb1ELb0ELb1EEv16Flash_fwd_params
    40 bytes stack frame, 38 bytes spill stores, 38 bytes spill loads
ptxas info    : Used 255 registers, 584 bytes cmem[0]
ptxas info    : Compiling entry function '_Z16flash_fwd_kernelI23Flash_fwd_kernel_traitsILi192ELi64ELi64ELi4ELb0ELb0EN7cutlass6half_tE19Flash_kernel_traitsILi192ELi64ELi64ELi4ES2_EELb1ELb1ELb1ELb1ELb0EEv16Flash_fwd_params' for 'sm_80'
ptxas info    : Function properties for _Z16flash_fwd_kernelI23Flash_fwd_kernel_traitsILi192ELi64ELi64ELi4ELb0ELb0EN7cutlass6half_tE19Flash_kernel_traitsILi192ELi64ELi64ELi4ES2_EELb1ELb1ELb1ELb1ELb0EEv16Flash_fwd_params
    0 bytes stack frame, 0 bytes spill stores, 0 bytes spill loads
ptxas info    : Used 226 registers, 584 bytes cmem[0]
ptxas info    : Compiling entry function '_Z16flash_fwd_kernelI23Flash_fwd_kernel_traitsILi192ELi64ELi64ELi4ELb0ELb0EN7cutlass6half_tE19Flash_kernel_traitsILi192ELi64ELi64ELi4ES2_EELb1ELb1ELb1ELb1ELb1EEv16Flash_fwd_params' for 'sm_80'
ptxas info    : Function properties for _Z16flash_fwd_kernelI23Flash_fwd_kernel_traitsILi192ELi64ELi64ELi4ELb0ELb0EN7cutlass6half_tE19Flash_kernel_traitsILi192ELi64ELi64ELi4ES2_EELb1ELb1ELb1ELb1ELb1EEv16Flash_fwd_params
    32 bytes stack frame, 36 bytes spill stores, 28 bytes spill loads
ptxas info    : Used 255 registers, 584 bytes cmem[0]
/anaconda/envs/flashattention/lib/python3.9/site-packages/torch/include/c10/core/SymInt.h(84): warning: integer conversion resulted in a change of sign

[6/36] /usr/local/cuda-11.4/bin/nvcc  -I/home/v-feiychen/flash-attention/csrc/flash_attn -I/home/v-feiychen/flash-attention/csrc/flash_attn/src -I/home/v-feiychen/flash-attention/csrc/cutlass/include -I/anaconda/envs/flashattention/lib/python3.9/site-packages/torch/include -I/anaconda/envs/flashattention/lib/python3.9/site-packages/torch/include/torch/csrc/api/include -I/anaconda/envs/flashattention/lib/python3.9/site-packages/torch/include/TH -I/anaconda/envs/flashattention/lib/python3.9/site-packages/torch/include/THC -I/usr/local/cuda-11.4/include -I/anaconda/envs/flashattention/include/python3.9 -c -c /home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_hdim192_bf16_sm80.cu -o /home/v-feiychen/flash-attention/build/temp.linux-x86_64-cpython-39/csrc/flash_attn/src/flash_fwd_hdim192_bf16_sm80.o -D__CUDA_NO_HALF_OPERATORS__ -D__CUDA_NO_HALF_CONVERSIONS__ -D__CUDA_NO_BFLOAT16_CONVERSIONS__ -D__CUDA_NO_HALF2_OPERATORS__ --expt-relaxed-constexpr --compiler-options ''"'"'-fPIC'"'"'' -O3 -std=c++17 -U__CUDA_NO_HALF_OPERATORS__ -U__CUDA_NO_HALF_CONVERSIONS__ -U__CUDA_NO_HALF2_OPERATORS__ -U__CUDA_NO_BFLOAT16_CONVERSIONS__ --expt-relaxed-constexpr --expt-extended-lambda --use_fast_math --ptxas-options=-v -lineinfo -gencode arch=compute_80,code=sm_80 --threads 4 -DTORCH_API_INCLUDE_EXTENSION_H '-DPYBIND11_COMPILER_TYPE="_gcc"' '-DPYBIND11_STDLIB="_libstdcpp"' '-DPYBIND11_BUILD_ABI="_cxxabi1011"' -DTORCH_EXTENSION_NAME=flash_attn_2_cuda -D_GLIBCXX_USE_CXX11_ABI=0
/anaconda/envs/flashattention/lib/python3.9/site-packages/torch/include/c10/core/SymInt.h(84): warning: integer conversion resulted in a change of sign

/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_kernel.h(140): warning: variable "MMA_M" was declared but never referenced
          detected during:
            instantiation of "void flash::compute_attn_1rowblock<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &, int, int, int) [with Kernel_traits=Flash_fwd_kernel_traits<192, 64, 64, 4, false, false, cutlass::bfloat16_t, Flash_kernel_traits<192, 64, 64, 4, cutlass::bfloat16_t>>, Is_dropout=true, Is_causal=true, Is_even_N=true, Is_even_K=true, Return_softmax=true, Params=Flash_fwd_params]" 
(588): here
            instantiation of "void flash::compute_attn<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &) [with Kernel_traits=Flash_fwd_kernel_traits<192, 64, 64, 4, false, false, cutlass::bfloat16_t, Flash_kernel_traits<192, 64, 64, 4, cutlass::bfloat16_t>>, Is_dropout=true, Is_causal=true, Is_even_N=true, Is_even_K=true, Return_softmax=true, Params=Flash_fwd_params]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(15): here
            instantiation of "void flash_fwd_kernel<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax>(Flash_fwd_params) [with Kernel_traits=Flash_fwd_kernel_traits<192, 64, 64, 4, false, false, cutlass::bfloat16_t, Flash_kernel_traits<192, 64, 64, 4, cutlass::bfloat16_t>>, Is_dropout=true, Is_causal=true, Is_even_N=true, Is_even_K=true, Return_softmax=true]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(34): here
            instantiation of "void run_flash_fwd<Kernel_traits,Is_dropout,Is_causal>(Flash_fwd_params &, cudaStream_t) [with Kernel_traits=Flash_fwd_kernel_traits<192, 64, 64, 4, false, false, cutlass::bfloat16_t, Flash_kernel_traits<192, 64, 64, 4, cutlass::bfloat16_t>>, Is_dropout=true, Is_causal=true]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(183): here
            instantiation of "void run_mha_fwd_hdim192<T>(Flash_fwd_params &, cudaStream_t) [with T=cutlass::bfloat16_t]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_hdim192_bf16_sm80.cu(15): here

/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_kernel.h(275): warning: variable "tQrQ" was declared but never referenced
          detected during:
            instantiation of "void flash::compute_attn_1rowblock<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &, int, int, int) [with Kernel_traits=Flash_fwd_kernel_traits<192, 64, 64, 4, false, false, cutlass::bfloat16_t, Flash_kernel_traits<192, 64, 64, 4, cutlass::bfloat16_t>>, Is_dropout=true, Is_causal=true, Is_even_N=true, Is_even_K=true, Return_softmax=true, Params=Flash_fwd_params]" 
(588): here
            instantiation of "void flash::compute_attn<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &) [with Kernel_traits=Flash_fwd_kernel_traits<192, 64, 64, 4, false, false, cutlass::bfloat16_t, Flash_kernel_traits<192, 64, 64, 4, cutlass::bfloat16_t>>, Is_dropout=true, Is_causal=true, Is_even_N=true, Is_even_K=true, Return_softmax=true, Params=Flash_fwd_params]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(15): here
            instantiation of "void flash_fwd_kernel<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax>(Flash_fwd_params) [with Kernel_traits=Flash_fwd_kernel_traits<192, 64, 64, 4, false, false, cutlass::bfloat16_t, Flash_kernel_traits<192, 64, 64, 4, cutlass::bfloat16_t>>, Is_dropout=true, Is_causal=true, Is_even_N=true, Is_even_K=true, Return_softmax=true]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(34): here
            instantiation of "void run_flash_fwd<Kernel_traits,Is_dropout,Is_causal>(Flash_fwd_params &, cudaStream_t) [with Kernel_traits=Flash_fwd_kernel_traits<192, 64, 64, 4, false, false, cutlass::bfloat16_t, Flash_kernel_traits<192, 64, 64, 4, cutlass::bfloat16_t>>, Is_dropout=true, Is_causal=true]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(183): here
            instantiation of "void run_mha_fwd_hdim192<T>(Flash_fwd_params &, cudaStream_t) [with T=cutlass::bfloat16_t]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_hdim192_bf16_sm80.cu(15): here

/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_kernel.h(140): warning: variable "MMA_M" was declared but never referenced
          detected during:
            instantiation of "void flash::compute_attn_1rowblock<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &, int, int, int) [with Kernel_traits=Flash_fwd_kernel_traits<192, 64, 64, 4, false, false, cutlass::bfloat16_t, Flash_kernel_traits<192, 64, 64, 4, cutlass::bfloat16_t>>, Is_dropout=true, Is_causal=true, Is_even_N=true, Is_even_K=true, Return_softmax=false, Params=Flash_fwd_params]" 
(588): here
            instantiation of "void flash::compute_attn<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &) [with Kernel_traits=Flash_fwd_kernel_traits<192, 64, 64, 4, false, false, cutlass::bfloat16_t, Flash_kernel_traits<192, 64, 64, 4, cutlass::bfloat16_t>>, Is_dropout=true, Is_causal=true, Is_even_N=true, Is_even_K=true, Return_softmax=false, Params=Flash_fwd_params]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(15): here
            instantiation of "void flash_fwd_kernel<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax>(Flash_fwd_params) [with Kernel_traits=Flash_fwd_kernel_traits<192, 64, 64, 4, false, false, cutlass::bfloat16_t, Flash_kernel_traits<192, 64, 64, 4, cutlass::bfloat16_t>>, Is_dropout=true, Is_causal=true, Is_even_N=true, Is_even_K=true, Return_softmax=false]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(34): here
            instantiation of "void run_flash_fwd<Kernel_traits,Is_dropout,Is_causal>(Flash_fwd_params &, cudaStream_t) [with Kernel_traits=Flash_fwd_kernel_traits<192, 64, 64, 4, false, false, cutlass::bfloat16_t, Flash_kernel_traits<192, 64, 64, 4, cutlass::bfloat16_t>>, Is_dropout=true, Is_causal=true]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(183): here
            instantiation of "void run_mha_fwd_hdim192<T>(Flash_fwd_params &, cudaStream_t) [with T=cutlass::bfloat16_t]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_hdim192_bf16_sm80.cu(15): here

/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_kernel.h(275): warning: variable "tQrQ" was declared but never referenced
          detected during:
            instantiation of "void flash::compute_attn_1rowblock<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &, int, int, int) [with Kernel_traits=Flash_fwd_kernel_traits<192, 64, 64, 4, false, false, cutlass::bfloat16_t, Flash_kernel_traits<192, 64, 64, 4, cutlass::bfloat16_t>>, Is_dropout=true, Is_causal=true, Is_even_N=true, Is_even_K=true, Return_softmax=false, Params=Flash_fwd_params]" 
(588): here
            instantiation of "void flash::compute_attn<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &) [with Kernel_traits=Flash_fwd_kernel_traits<192, 64, 64, 4, false, false, cutlass::bfloat16_t, Flash_kernel_traits<192, 64, 64, 4, cutlass::bfloat16_t>>, Is_dropout=true, Is_causal=true, Is_even_N=true, Is_even_K=true, Return_softmax=false, Params=Flash_fwd_params]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(15): here
            instantiation of "void flash_fwd_kernel<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax>(Flash_fwd_params) [with Kernel_traits=Flash_fwd_kernel_traits<192, 64, 64, 4, false, false, cutlass::bfloat16_t, Flash_kernel_traits<192, 64, 64, 4, cutlass::bfloat16_t>>, Is_dropout=true, Is_causal=true, Is_even_N=true, Is_even_K=true, Return_softmax=false]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(34): here
            instantiation of "void run_flash_fwd<Kernel_traits,Is_dropout,Is_causal>(Flash_fwd_params &, cudaStream_t) [with Kernel_traits=Flash_fwd_kernel_traits<192, 64, 64, 4, false, false, cutlass::bfloat16_t, Flash_kernel_traits<192, 64, 64, 4, cutlass::bfloat16_t>>, Is_dropout=true, Is_causal=true]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(183): here
            instantiation of "void run_mha_fwd_hdim192<T>(Flash_fwd_params &, cudaStream_t) [with T=cutlass::bfloat16_t]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_hdim192_bf16_sm80.cu(15): here

/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_kernel.h(140): warning: variable "MMA_M" was declared but never referenced
          detected during:
            instantiation of "void flash::compute_attn_1rowblock<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &, int, int, int) [with Kernel_traits=Flash_fwd_kernel_traits<192, 64, 64, 4, false, false, cutlass::bfloat16_t, Flash_kernel_traits<192, 64, 64, 4, cutlass::bfloat16_t>>, Is_dropout=true, Is_causal=true, Is_even_N=true, Is_even_K=false, Return_softmax=true, Params=Flash_fwd_params]" 
(588): here
            instantiation of "void flash::compute_attn<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &) [with Kernel_traits=Flash_fwd_kernel_traits<192, 64, 64, 4, false, false, cutlass::bfloat16_t, Flash_kernel_traits<192, 64, 64, 4, cutlass::bfloat16_t>>, Is_dropout=true, Is_causal=true, Is_even_N=true, Is_even_K=false, Return_softmax=true, Params=Flash_fwd_params]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(15): here
            instantiation of "void flash_fwd_kernel<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax>(Flash_fwd_params) [with Kernel_traits=Flash_fwd_kernel_traits<192, 64, 64, 4, false, false, cutlass::bfloat16_t, Flash_kernel_traits<192, 64, 64, 4, cutlass::bfloat16_t>>, Is_dropout=true, Is_causal=true, Is_even_N=true, Is_even_K=false, Return_softmax=true]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(34): here
            instantiation of "void run_flash_fwd<Kernel_traits,Is_dropout,Is_causal>(Flash_fwd_params &, cudaStream_t) [with Kernel_traits=Flash_fwd_kernel_traits<192, 64, 64, 4, false, false, cutlass::bfloat16_t, Flash_kernel_traits<192, 64, 64, 4, cutlass::bfloat16_t>>, Is_dropout=true, Is_causal=true]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(183): here
            instantiation of "void run_mha_fwd_hdim192<T>(Flash_fwd_params &, cudaStream_t) [with T=cutlass::bfloat16_t]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_hdim192_bf16_sm80.cu(15): here

/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_kernel.h(275): warning: variable "tQrQ" was declared but never referenced
          detected during:
            instantiation of "void flash::compute_attn_1rowblock<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &, int, int, int) [with Kernel_traits=Flash_fwd_kernel_traits<192, 64, 64, 4, false, false, cutlass::bfloat16_t, Flash_kernel_traits<192, 64, 64, 4, cutlass::bfloat16_t>>, Is_dropout=true, Is_causal=true, Is_even_N=true, Is_even_K=false, Return_softmax=true, Params=Flash_fwd_params]" 
(588): here
            instantiation of "void flash::compute_attn<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &) [with Kernel_traits=Flash_fwd_kernel_traits<192, 64, 64, 4, false, false, cutlass::bfloat16_t, Flash_kernel_traits<192, 64, 64, 4, cutlass::bfloat16_t>>, Is_dropout=true, Is_causal=true, Is_even_N=true, Is_even_K=false, Return_softmax=true, Params=Flash_fwd_params]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(15): here
            instantiation of "void flash_fwd_kernel<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax>(Flash_fwd_params) [with Kernel_traits=Flash_fwd_kernel_traits<192, 64, 64, 4, false, false, cutlass::bfloat16_t, Flash_kernel_traits<192, 64, 64, 4, cutlass::bfloat16_t>>, Is_dropout=true, Is_causal=true, Is_even_N=true, Is_even_K=false, Return_softmax=true]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(34): here
            instantiation of "void run_flash_fwd<Kernel_traits,Is_dropout,Is_causal>(Flash_fwd_params &, cudaStream_t) [with Kernel_traits=Flash_fwd_kernel_traits<192, 64, 64, 4, false, false, cutlass::bfloat16_t, Flash_kernel_traits<192, 64, 64, 4, cutlass::bfloat16_t>>, Is_dropout=true, Is_causal=true]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(183): here
            instantiation of "void run_mha_fwd_hdim192<T>(Flash_fwd_params &, cudaStream_t) [with T=cutlass::bfloat16_t]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_hdim192_bf16_sm80.cu(15): here

/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_kernel.h(140): warning: variable "MMA_M" was declared but never referenced
          detected during:
            instantiation of "void flash::compute_attn_1rowblock<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &, int, int, int) [with Kernel_traits=Flash_fwd_kernel_traits<192, 64, 64, 4, false, false, cutlass::bfloat16_t, Flash_kernel_traits<192, 64, 64, 4, cutlass::bfloat16_t>>, Is_dropout=true, Is_causal=true, Is_even_N=true, Is_even_K=false, Return_softmax=false, Params=Flash_fwd_params]" 
(588): here
            instantiation of "void flash::compute_attn<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &) [with Kernel_traits=Flash_fwd_kernel_traits<192, 64, 64, 4, false, false, cutlass::bfloat16_t, Flash_kernel_traits<192, 64, 64, 4, cutlass::bfloat16_t>>, Is_dropout=true, Is_causal=true, Is_even_N=true, Is_even_K=false, Return_softmax=false, Params=Flash_fwd_params]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(15): here
            instantiation of "void flash_fwd_kernel<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax>(Flash_fwd_params) [with Kernel_traits=Flash_fwd_kernel_traits<192, 64, 64, 4, false, false, cutlass::bfloat16_t, Flash_kernel_traits<192, 64, 64, 4, cutlass::bfloat16_t>>, Is_dropout=true, Is_causal=true, Is_even_N=true, Is_even_K=false, Return_softmax=false]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(34): here
            instantiation of "void run_flash_fwd<Kernel_traits,Is_dropout,Is_causal>(Flash_fwd_params &, cudaStream_t) [with Kernel_traits=Flash_fwd_kernel_traits<192, 64, 64, 4, false, false, cutlass::bfloat16_t, Flash_kernel_traits<192, 64, 64, 4, cutlass::bfloat16_t>>, Is_dropout=true, Is_causal=true]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(183): here
            instantiation of "void run_mha_fwd_hdim192<T>(Flash_fwd_params &, cudaStream_t) [with T=cutlass::bfloat16_t]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_hdim192_bf16_sm80.cu(15): here

/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_kernel.h(275): warning: variable "tQrQ" was declared but never referenced
          detected during:
            instantiation of "void flash::compute_attn_1rowblock<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &, int, int, int) [with Kernel_traits=Flash_fwd_kernel_traits<192, 64, 64, 4, false, false, cutlass::bfloat16_t, Flash_kernel_traits<192, 64, 64, 4, cutlass::bfloat16_t>>, Is_dropout=true, Is_causal=true, Is_even_N=true, Is_even_K=false, Return_softmax=false, Params=Flash_fwd_params]" 
(588): here
            instantiation of "void flash::compute_attn<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &) [with Kernel_traits=Flash_fwd_kernel_traits<192, 64, 64, 4, false, false, cutlass::bfloat16_t, Flash_kernel_traits<192, 64, 64, 4, cutlass::bfloat16_t>>, Is_dropout=true, Is_causal=true, Is_even_N=true, Is_even_K=false, Return_softmax=false, Params=Flash_fwd_params]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(15): here
            instantiation of "void flash_fwd_kernel<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax>(Flash_fwd_params) [with Kernel_traits=Flash_fwd_kernel_traits<192, 64, 64, 4, false, false, cutlass::bfloat16_t, Flash_kernel_traits<192, 64, 64, 4, cutlass::bfloat16_t>>, Is_dropout=true, Is_causal=true, Is_even_N=true, Is_even_K=false, Return_softmax=false]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(34): here
            instantiation of "void run_flash_fwd<Kernel_traits,Is_dropout,Is_causal>(Flash_fwd_params &, cudaStream_t) [with Kernel_traits=Flash_fwd_kernel_traits<192, 64, 64, 4, false, false, cutlass::bfloat16_t, Flash_kernel_traits<192, 64, 64, 4, cutlass::bfloat16_t>>, Is_dropout=true, Is_causal=true]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(183): here
            instantiation of "void run_mha_fwd_hdim192<T>(Flash_fwd_params &, cudaStream_t) [with T=cutlass::bfloat16_t]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_hdim192_bf16_sm80.cu(15): here

/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_kernel.h(140): warning: variable "MMA_M" was declared but never referenced
          detected during:
            instantiation of "void flash::compute_attn_1rowblock<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &, int, int, int) [with Kernel_traits=Flash_fwd_kernel_traits<192, 64, 64, 4, false, false, cutlass::bfloat16_t, Flash_kernel_traits<192, 64, 64, 4, cutlass::bfloat16_t>>, Is_dropout=true, Is_causal=true, Is_even_N=false, Is_even_K=true, Return_softmax=true, Params=Flash_fwd_params]" 
(588): here
            instantiation of "void flash::compute_attn<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &) [with Kernel_traits=Flash_fwd_kernel_traits<192, 64, 64, 4, false, false, cutlass::bfloat16_t, Flash_kernel_traits<192, 64, 64, 4, cutlass::bfloat16_t>>, Is_dropout=true, Is_causal=true, Is_even_N=false, Is_even_K=true, Return_softmax=true, Params=Flash_fwd_params]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(15): here
            instantiation of "void flash_fwd_kernel<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax>(Flash_fwd_params) [with Kernel_traits=Flash_fwd_kernel_traits<192, 64, 64, 4, false, false, cutlass::bfloat16_t, Flash_kernel_traits<192, 64, 64, 4, cutlass::bfloat16_t>>, Is_dropout=true, Is_causal=true, Is_even_N=false, Is_even_K=true, Return_softmax=true]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(34): here
            instantiation of "void run_flash_fwd<Kernel_traits,Is_dropout,Is_causal>(Flash_fwd_params &, cudaStream_t) [with Kernel_traits=Flash_fwd_kernel_traits<192, 64, 64, 4, false, false, cutlass::bfloat16_t, Flash_kernel_traits<192, 64, 64, 4, cutlass::bfloat16_t>>, Is_dropout=true, Is_causal=true]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(183): here
            instantiation of "void run_mha_fwd_hdim192<T>(Flash_fwd_params &, cudaStream_t) [with T=cutlass::bfloat16_t]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_hdim192_bf16_sm80.cu(15): here

/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_kernel.h(275): warning: variable "tQrQ" was declared but never referenced
          detected during:
            instantiation of "void flash::compute_attn_1rowblock<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &, int, int, int) [with Kernel_traits=Flash_fwd_kernel_traits<192, 64, 64, 4, false, false, cutlass::bfloat16_t, Flash_kernel_traits<192, 64, 64, 4, cutlass::bfloat16_t>>, Is_dropout=true, Is_causal=true, Is_even_N=false, Is_even_K=true, Return_softmax=true, Params=Flash_fwd_params]" 
(588): here
            instantiation of "void flash::compute_attn<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &) [with Kernel_traits=Flash_fwd_kernel_traits<192, 64, 64, 4, false, false, cutlass::bfloat16_t, Flash_kernel_traits<192, 64, 64, 4, cutlass::bfloat16_t>>, Is_dropout=true, Is_causal=true, Is_even_N=false, Is_even_K=true, Return_softmax=true, Params=Flash_fwd_params]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(15): here
            instantiation of "void flash_fwd_kernel<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax>(Flash_fwd_params) [with Kernel_traits=Flash_fwd_kernel_traits<192, 64, 64, 4, false, false, cutlass::bfloat16_t, Flash_kernel_traits<192, 64, 64, 4, cutlass::bfloat16_t>>, Is_dropout=true, Is_causal=true, Is_even_N=false, Is_even_K=true, Return_softmax=true]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(34): here
            instantiation of "void run_flash_fwd<Kernel_traits,Is_dropout,Is_causal>(Flash_fwd_params &, cudaStream_t) [with Kernel_traits=Flash_fwd_kernel_traits<192, 64, 64, 4, false, false, cutlass::bfloat16_t, Flash_kernel_traits<192, 64, 64, 4, cutlass::bfloat16_t>>, Is_dropout=true, Is_causal=true]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(183): here
            instantiation of "void run_mha_fwd_hdim192<T>(Flash_fwd_params &, cudaStream_t) [with T=cutlass::bfloat16_t]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_hdim192_bf16_sm80.cu(15): here

/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_kernel.h(140): warning: variable "MMA_M" was declared but never referenced
          detected during:
            instantiation of "void flash::compute_attn_1rowblock<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &, int, int, int) [with Kernel_traits=Flash_fwd_kernel_traits<192, 64, 64, 4, false, false, cutlass::bfloat16_t, Flash_kernel_traits<192, 64, 64, 4, cutlass::bfloat16_t>>, Is_dropout=true, Is_causal=true, Is_even_N=false, Is_even_K=true, Return_softmax=false, Params=Flash_fwd_params]" 
(588): here
            instantiation of "void flash::compute_attn<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &) [with Kernel_traits=Flash_fwd_kernel_traits<192, 64, 64, 4, false, false, cutlass::bfloat16_t, Flash_kernel_traits<192, 64, 64, 4, cutlass::bfloat16_t>>, Is_dropout=true, Is_causal=true, Is_even_N=false, Is_even_K=true, Return_softmax=false, Params=Flash_fwd_params]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(15): here
            instantiation of "void flash_fwd_kernel<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax>(Flash_fwd_params) [with Kernel_traits=Flash_fwd_kernel_traits<192, 64, 64, 4, false, false, cutlass::bfloat16_t, Flash_kernel_traits<192, 64, 64, 4, cutlass::bfloat16_t>>, Is_dropout=true, Is_causal=true, Is_even_N=false, Is_even_K=true, Return_softmax=false]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(34): here
            instantiation of "void run_flash_fwd<Kernel_traits,Is_dropout,Is_causal>(Flash_fwd_params &, cudaStream_t) [with Kernel_traits=Flash_fwd_kernel_traits<192, 64, 64, 4, false, false, cutlass::bfloat16_t, Flash_kernel_traits<192, 64, 64, 4, cutlass::bfloat16_t>>, Is_dropout=true, Is_causal=true]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(183): here
            instantiation of "void run_mha_fwd_hdim192<T>(Flash_fwd_params &, cudaStream_t) [with T=cutlass::bfloat16_t]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_hdim192_bf16_sm80.cu(15): here

/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_kernel.h(275): warning: variable "tQrQ" was declared but never referenced
          detected during:
            instantiation of "void flash::compute_attn_1rowblock<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &, int, int, int) [with Kernel_traits=Flash_fwd_kernel_traits<192, 64, 64, 4, false, false, cutlass::bfloat16_t, Flash_kernel_traits<192, 64, 64, 4, cutlass::bfloat16_t>>, Is_dropout=true, Is_causal=true, Is_even_N=false, Is_even_K=true, Return_softmax=false, Params=Flash_fwd_params]" 
(588): here
            instantiation of "void flash::compute_attn<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &) [with Kernel_traits=Flash_fwd_kernel_traits<192, 64, 64, 4, false, false, cutlass::bfloat16_t, Flash_kernel_traits<192, 64, 64, 4, cutlass::bfloat16_t>>, Is_dropout=true, Is_causal=true, Is_even_N=false, Is_even_K=true, Return_softmax=false, Params=Flash_fwd_params]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(15): here
            instantiation of "void flash_fwd_kernel<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax>(Flash_fwd_params) [with Kernel_traits=Flash_fwd_kernel_traits<192, 64, 64, 4, false, false, cutlass::bfloat16_t, Flash_kernel_traits<192, 64, 64, 4, cutlass::bfloat16_t>>, Is_dropout=true, Is_causal=true, Is_even_N=false, Is_even_K=true, Return_softmax=false]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(34): here
            instantiation of "void run_flash_fwd<Kernel_traits,Is_dropout,Is_causal>(Flash_fwd_params &, cudaStream_t) [with Kernel_traits=Flash_fwd_kernel_traits<192, 64, 64, 4, false, false, cutlass::bfloat16_t, Flash_kernel_traits<192, 64, 64, 4, cutlass::bfloat16_t>>, Is_dropout=true, Is_causal=true]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(183): here
            instantiation of "void run_mha_fwd_hdim192<T>(Flash_fwd_params &, cudaStream_t) [with T=cutlass::bfloat16_t]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_hdim192_bf16_sm80.cu(15): here

/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_kernel.h(140): warning: variable "MMA_M" was declared but never referenced
          detected during:
            instantiation of "void flash::compute_attn_1rowblock<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &, int, int, int) [with Kernel_traits=Flash_fwd_kernel_traits<192, 64, 64, 4, false, false, cutlass::bfloat16_t, Flash_kernel_traits<192, 64, 64, 4, cutlass::bfloat16_t>>, Is_dropout=true, Is_causal=true, Is_even_N=false, Is_even_K=false, Return_softmax=true, Params=Flash_fwd_params]" 
(588): here
            instantiation of "void flash::compute_attn<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &) [with Kernel_traits=Flash_fwd_kernel_traits<192, 64, 64, 4, false, false, cutlass::bfloat16_t, Flash_kernel_traits<192, 64, 64, 4, cutlass::bfloat16_t>>, Is_dropout=true, Is_causal=true, Is_even_N=false, Is_even_K=false, Return_softmax=true, Params=Flash_fwd_params]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(15): here
            instantiation of "void flash_fwd_kernel<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax>(Flash_fwd_params) [with Kernel_traits=Flash_fwd_kernel_traits<192, 64, 64, 4, false, false, cutlass::bfloat16_t, Flash_kernel_traits<192, 64, 64, 4, cutlass::bfloat16_t>>, Is_dropout=true, Is_causal=true, Is_even_N=false, Is_even_K=false, Return_softmax=true]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(34): here
            instantiation of "void run_flash_fwd<Kernel_traits,Is_dropout,Is_causal>(Flash_fwd_params &, cudaStream_t) [with Kernel_traits=Flash_fwd_kernel_traits<192, 64, 64, 4, false, false, cutlass::bfloat16_t, Flash_kernel_traits<192, 64, 64, 4, cutlass::bfloat16_t>>, Is_dropout=true, Is_causal=true]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(183): here
            instantiation of "void run_mha_fwd_hdim192<T>(Flash_fwd_params &, cudaStream_t) [with T=cutlass::bfloat16_t]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_hdim192_bf16_sm80.cu(15): here

/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_kernel.h(275): warning: variable "tQrQ" was declared but never referenced
          detected during:
            instantiation of "void flash::compute_attn_1rowblock<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &, int, int, int) [with Kernel_traits=Flash_fwd_kernel_traits<192, 64, 64, 4, false, false, cutlass::bfloat16_t, Flash_kernel_traits<192, 64, 64, 4, cutlass::bfloat16_t>>, Is_dropout=true, Is_causal=true, Is_even_N=false, Is_even_K=false, Return_softmax=true, Params=Flash_fwd_params]" 
(588): here
            instantiation of "void flash::compute_attn<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &) [with Kernel_traits=Flash_fwd_kernel_traits<192, 64, 64, 4, false, false, cutlass::bfloat16_t, Flash_kernel_traits<192, 64, 64, 4, cutlass::bfloat16_t>>, Is_dropout=true, Is_causal=true, Is_even_N=false, Is_even_K=false, Return_softmax=true, Params=Flash_fwd_params]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(15): here
            instantiation of "void flash_fwd_kernel<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax>(Flash_fwd_params) [with Kernel_traits=Flash_fwd_kernel_traits<192, 64, 64, 4, false, false, cutlass::bfloat16_t, Flash_kernel_traits<192, 64, 64, 4, cutlass::bfloat16_t>>, Is_dropout=true, Is_causal=true, Is_even_N=false, Is_even_K=false, Return_softmax=true]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(34): here
            instantiation of "void run_flash_fwd<Kernel_traits,Is_dropout,Is_causal>(Flash_fwd_params &, cudaStream_t) [with Kernel_traits=Flash_fwd_kernel_traits<192, 64, 64, 4, false, false, cutlass::bfloat16_t, Flash_kernel_traits<192, 64, 64, 4, cutlass::bfloat16_t>>, Is_dropout=true, Is_causal=true]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(183): here
            instantiation of "void run_mha_fwd_hdim192<T>(Flash_fwd_params &, cudaStream_t) [with T=cutlass::bfloat16_t]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_hdim192_bf16_sm80.cu(15): here

/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_kernel.h(140): warning: variable "MMA_M" was declared but never referenced
          detected during:
            instantiation of "void flash::compute_attn_1rowblock<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &, int, int, int) [with Kernel_traits=Flash_fwd_kernel_traits<192, 64, 64, 4, false, false, cutlass::bfloat16_t, Flash_kernel_traits<192, 64, 64, 4, cutlass::bfloat16_t>>, Is_dropout=true, Is_causal=true, Is_even_N=false, Is_even_K=false, Return_softmax=false, Params=Flash_fwd_params]" 
(588): here
            instantiation of "void flash::compute_attn<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &) [with Kernel_traits=Flash_fwd_kernel_traits<192, 64, 64, 4, false, false, cutlass::bfloat16_t, Flash_kernel_traits<192, 64, 64, 4, cutlass::bfloat16_t>>, Is_dropout=true, Is_causal=true, Is_even_N=false, Is_even_K=false, Return_softmax=false, Params=Flash_fwd_params]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(15): here
            instantiation of "void flash_fwd_kernel<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax>(Flash_fwd_params) [with Kernel_traits=Flash_fwd_kernel_traits<192, 64, 64, 4, false, false, cutlass::bfloat16_t, Flash_kernel_traits<192, 64, 64, 4, cutlass::bfloat16_t>>, Is_dropout=true, Is_causal=true, Is_even_N=false, Is_even_K=false, Return_softmax=false]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(34): here
            instantiation of "void run_flash_fwd<Kernel_traits,Is_dropout,Is_causal>(Flash_fwd_params &, cudaStream_t) [with Kernel_traits=Flash_fwd_kernel_traits<192, 64, 64, 4, false, false, cutlass::bfloat16_t, Flash_kernel_traits<192, 64, 64, 4, cutlass::bfloat16_t>>, Is_dropout=true, Is_causal=true]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(183): here
            instantiation of "void run_mha_fwd_hdim192<T>(Flash_fwd_params &, cudaStream_t) [with T=cutlass::bfloat16_t]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_hdim192_bf16_sm80.cu(15): here

/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_kernel.h(275): warning: variable "tQrQ" was declared but never referenced
          detected during:
            instantiation of "void flash::compute_attn_1rowblock<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &, int, int, int) [with Kernel_traits=Flash_fwd_kernel_traits<192, 64, 64, 4, false, false, cutlass::bfloat16_t, Flash_kernel_traits<192, 64, 64, 4, cutlass::bfloat16_t>>, Is_dropout=true, Is_causal=true, Is_even_N=false, Is_even_K=false, Return_softmax=false, Params=Flash_fwd_params]" 
(588): here
            instantiation of "void flash::compute_attn<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &) [with Kernel_traits=Flash_fwd_kernel_traits<192, 64, 64, 4, false, false, cutlass::bfloat16_t, Flash_kernel_traits<192, 64, 64, 4, cutlass::bfloat16_t>>, Is_dropout=true, Is_causal=true, Is_even_N=false, Is_even_K=false, Return_softmax=false, Params=Flash_fwd_params]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(15): here
            instantiation of "void flash_fwd_kernel<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax>(Flash_fwd_params) [with Kernel_traits=Flash_fwd_kernel_traits<192, 64, 64, 4, false, false, cutlass::bfloat16_t, Flash_kernel_traits<192, 64, 64, 4, cutlass::bfloat16_t>>, Is_dropout=true, Is_causal=true, Is_even_N=false, Is_even_K=false, Return_softmax=false]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(34): here
            instantiation of "void run_flash_fwd<Kernel_traits,Is_dropout,Is_causal>(Flash_fwd_params &, cudaStream_t) [with Kernel_traits=Flash_fwd_kernel_traits<192, 64, 64, 4, false, false, cutlass::bfloat16_t, Flash_kernel_traits<192, 64, 64, 4, cutlass::bfloat16_t>>, Is_dropout=true, Is_causal=true]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(183): here
            instantiation of "void run_mha_fwd_hdim192<T>(Flash_fwd_params &, cudaStream_t) [with T=cutlass::bfloat16_t]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_hdim192_bf16_sm80.cu(15): here

/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_kernel.h(140): warning: variable "MMA_M" was declared but never referenced
          detected during:
            instantiation of "void flash::compute_attn_1rowblock<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &, int, int, int) [with Kernel_traits=Flash_fwd_kernel_traits<192, 64, 64, 4, false, false, cutlass::bfloat16_t, Flash_kernel_traits<192, 64, 64, 4, cutlass::bfloat16_t>>, Is_dropout=true, Is_causal=false, Is_even_N=true, Is_even_K=true, Return_softmax=true, Params=Flash_fwd_params]" 
(588): here
            instantiation of "void flash::compute_attn<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &) [with Kernel_traits=Flash_fwd_kernel_traits<192, 64, 64, 4, false, false, cutlass::bfloat16_t, Flash_kernel_traits<192, 64, 64, 4, cutlass::bfloat16_t>>, Is_dropout=true, Is_causal=false, Is_even_N=true, Is_even_K=true, Return_softmax=true, Params=Flash_fwd_params]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(15): here
            instantiation of "void flash_fwd_kernel<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax>(Flash_fwd_params) [with Kernel_traits=Flash_fwd_kernel_traits<192, 64, 64, 4, false, false, cutlass::bfloat16_t, Flash_kernel_traits<192, 64, 64, 4, cutlass::bfloat16_t>>, Is_dropout=true, Is_causal=false, Is_even_N=true, Is_even_K=true, Return_softmax=true]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(34): here
            instantiation of "void run_flash_fwd<Kernel_traits,Is_dropout,Is_causal>(Flash_fwd_params &, cudaStream_t) [with Kernel_traits=Flash_fwd_kernel_traits<192, 64, 64, 4, false, false, cutlass::bfloat16_t, Flash_kernel_traits<192, 64, 64, 4, cutlass::bfloat16_t>>, Is_dropout=true, Is_causal=false]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(183): here
            instantiation of "void run_mha_fwd_hdim192<T>(Flash_fwd_params &, cudaStream_t) [with T=cutlass::bfloat16_t]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_hdim192_bf16_sm80.cu(15): here

/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_kernel.h(275): warning: variable "tQrQ" was declared but never referenced
          detected during:
            instantiation of "void flash::compute_attn_1rowblock<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &, int, int, int) [with Kernel_traits=Flash_fwd_kernel_traits<192, 64, 64, 4, false, false, cutlass::bfloat16_t, Flash_kernel_traits<192, 64, 64, 4, cutlass::bfloat16_t>>, Is_dropout=true, Is_causal=false, Is_even_N=true, Is_even_K=true, Return_softmax=true, Params=Flash_fwd_params]" 
(588): here
            instantiation of "void flash::compute_attn<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &) [with Kernel_traits=Flash_fwd_kernel_traits<192, 64, 64, 4, false, false, cutlass::bfloat16_t, Flash_kernel_traits<192, 64, 64, 4, cutlass::bfloat16_t>>, Is_dropout=true, Is_causal=false, Is_even_N=true, Is_even_K=true, Return_softmax=true, Params=Flash_fwd_params]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(15): here
            instantiation of "void flash_fwd_kernel<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax>(Flash_fwd_params) [with Kernel_traits=Flash_fwd_kernel_traits<192, 64, 64, 4, false, false, cutlass::bfloat16_t, Flash_kernel_traits<192, 64, 64, 4, cutlass::bfloat16_t>>, Is_dropout=true, Is_causal=false, Is_even_N=true, Is_even_K=true, Return_softmax=true]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(34): here
            instantiation of "void run_flash_fwd<Kernel_traits,Is_dropout,Is_causal>(Flash_fwd_params &, cudaStream_t) [with Kernel_traits=Flash_fwd_kernel_traits<192, 64, 64, 4, false, false, cutlass::bfloat16_t, Flash_kernel_traits<192, 64, 64, 4, cutlass::bfloat16_t>>, Is_dropout=true, Is_causal=false]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(183): here
            instantiation of "void run_mha_fwd_hdim192<T>(Flash_fwd_params &, cudaStream_t) [with T=cutlass::bfloat16_t]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_hdim192_bf16_sm80.cu(15): here

/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_kernel.h(140): warning: variable "MMA_M" was declared but never referenced
          detected during:
            instantiation of "void flash::compute_attn_1rowblock<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &, int, int, int) [with Kernel_traits=Flash_fwd_kernel_traits<192, 64, 64, 4, false, false, cutlass::bfloat16_t, Flash_kernel_traits<192, 64, 64, 4, cutlass::bfloat16_t>>, Is_dropout=true, Is_causal=false, Is_even_N=true, Is_even_K=true, Return_softmax=false, Params=Flash_fwd_params]" 
(588): here
            instantiation of "void flash::compute_attn<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &) [with Kernel_traits=Flash_fwd_kernel_traits<192, 64, 64, 4, false, false, cutlass::bfloat16_t, Flash_kernel_traits<192, 64, 64, 4, cutlass::bfloat16_t>>, Is_dropout=true, Is_causal=false, Is_even_N=true, Is_even_K=true, Return_softmax=false, Params=Flash_fwd_params]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(15): here
            instantiation of "void flash_fwd_kernel<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax>(Flash_fwd_params) [with Kernel_traits=Flash_fwd_kernel_traits<192, 64, 64, 4, false, false, cutlass::bfloat16_t, Flash_kernel_traits<192, 64, 64, 4, cutlass::bfloat16_t>>, Is_dropout=true, Is_causal=false, Is_even_N=true, Is_even_K=true, Return_softmax=false]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(34): here
            instantiation of "void run_flash_fwd<Kernel_traits,Is_dropout,Is_causal>(Flash_fwd_params &, cudaStream_t) [with Kernel_traits=Flash_fwd_kernel_traits<192, 64, 64, 4, false, false, cutlass::bfloat16_t, Flash_kernel_traits<192, 64, 64, 4, cutlass::bfloat16_t>>, Is_dropout=true, Is_causal=false]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(183): here
            instantiation of "void run_mha_fwd_hdim192<T>(Flash_fwd_params &, cudaStream_t) [with T=cutlass::bfloat16_t]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_hdim192_bf16_sm80.cu(15): here

/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_kernel.h(275): warning: variable "tQrQ" was declared but never referenced
          detected during:
            instantiation of "void flash::compute_attn_1rowblock<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &, int, int, int) [with Kernel_traits=Flash_fwd_kernel_traits<192, 64, 64, 4, false, false, cutlass::bfloat16_t, Flash_kernel_traits<192, 64, 64, 4, cutlass::bfloat16_t>>, Is_dropout=true, Is_causal=false, Is_even_N=true, Is_even_K=true, Return_softmax=false, Params=Flash_fwd_params]" 
(588): here
            instantiation of "void flash::compute_attn<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &) [with Kernel_traits=Flash_fwd_kernel_traits<192, 64, 64, 4, false, false, cutlass::bfloat16_t, Flash_kernel_traits<192, 64, 64, 4, cutlass::bfloat16_t>>, Is_dropout=true, Is_causal=false, Is_even_N=true, Is_even_K=true, Return_softmax=false, Params=Flash_fwd_params]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(15): here
            instantiation of "void flash_fwd_kernel<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax>(Flash_fwd_params) [with Kernel_traits=Flash_fwd_kernel_traits<192, 64, 64, 4, false, false, cutlass::bfloat16_t, Flash_kernel_traits<192, 64, 64, 4, cutlass::bfloat16_t>>, Is_dropout=true, Is_causal=false, Is_even_N=true, Is_even_K=true, Return_softmax=false]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(34): here
            instantiation of "void run_flash_fwd<Kernel_traits,Is_dropout,Is_causal>(Flash_fwd_params &, cudaStream_t) [with Kernel_traits=Flash_fwd_kernel_traits<192, 64, 64, 4, false, false, cutlass::bfloat16_t, Flash_kernel_traits<192, 64, 64, 4, cutlass::bfloat16_t>>, Is_dropout=true, Is_causal=false]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(183): here
            instantiation of "void run_mha_fwd_hdim192<T>(Flash_fwd_params &, cudaStream_t) [with T=cutlass::bfloat16_t]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_hdim192_bf16_sm80.cu(15): here

/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_kernel.h(140): warning: variable "MMA_M" was declared but never referenced
          detected during:
            instantiation of "void flash::compute_attn_1rowblock<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &, int, int, int) [with Kernel_traits=Flash_fwd_kernel_traits<192, 64, 64, 4, false, false, cutlass::bfloat16_t, Flash_kernel_traits<192, 64, 64, 4, cutlass::bfloat16_t>>, Is_dropout=true, Is_causal=false, Is_even_N=true, Is_even_K=false, Return_softmax=true, Params=Flash_fwd_params]" 
(588): here
            instantiation of "void flash::compute_attn<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &) [with Kernel_traits=Flash_fwd_kernel_traits<192, 64, 64, 4, false, false, cutlass::bfloat16_t, Flash_kernel_traits<192, 64, 64, 4, cutlass::bfloat16_t>>, Is_dropout=true, Is_causal=false, Is_even_N=true, Is_even_K=false, Return_softmax=true, Params=Flash_fwd_params]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(15): here
            instantiation of "void flash_fwd_kernel<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax>(Flash_fwd_params) [with Kernel_traits=Flash_fwd_kernel_traits<192, 64, 64, 4, false, false, cutlass::bfloat16_t, Flash_kernel_traits<192, 64, 64, 4, cutlass::bfloat16_t>>, Is_dropout=true, Is_causal=false, Is_even_N=true, Is_even_K=false, Return_softmax=true]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(34): here
            instantiation of "void run_flash_fwd<Kernel_traits,Is_dropout,Is_causal>(Flash_fwd_params &, cudaStream_t) [with Kernel_traits=Flash_fwd_kernel_traits<192, 64, 64, 4, false, false, cutlass::bfloat16_t, Flash_kernel_traits<192, 64, 64, 4, cutlass::bfloat16_t>>, Is_dropout=true, Is_causal=false]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(183): here
            instantiation of "void run_mha_fwd_hdim192<T>(Flash_fwd_params &, cudaStream_t) [with T=cutlass::bfloat16_t]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_hdim192_bf16_sm80.cu(15): here

/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_kernel.h(275): warning: variable "tQrQ" was declared but never referenced
          detected during:
            instantiation of "void flash::compute_attn_1rowblock<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &, int, int, int) [with Kernel_traits=Flash_fwd_kernel_traits<192, 64, 64, 4, false, false, cutlass::bfloat16_t, Flash_kernel_traits<192, 64, 64, 4, cutlass::bfloat16_t>>, Is_dropout=true, Is_causal=false, Is_even_N=true, Is_even_K=false, Return_softmax=true, Params=Flash_fwd_params]" 
(588): here
            instantiation of "void flash::compute_attn<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &) [with Kernel_traits=Flash_fwd_kernel_traits<192, 64, 64, 4, false, false, cutlass::bfloat16_t, Flash_kernel_traits<192, 64, 64, 4, cutlass::bfloat16_t>>, Is_dropout=true, Is_causal=false, Is_even_N=true, Is_even_K=false, Return_softmax=true, Params=Flash_fwd_params]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(15): here
            instantiation of "void flash_fwd_kernel<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax>(Flash_fwd_params) [with Kernel_traits=Flash_fwd_kernel_traits<192, 64, 64, 4, false, false, cutlass::bfloat16_t, Flash_kernel_traits<192, 64, 64, 4, cutlass::bfloat16_t>>, Is_dropout=true, Is_causal=false, Is_even_N=true, Is_even_K=false, Return_softmax=true]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(34): here
            instantiation of "void run_flash_fwd<Kernel_traits,Is_dropout,Is_causal>(Flash_fwd_params &, cudaStream_t) [with Kernel_traits=Flash_fwd_kernel_traits<192, 64, 64, 4, false, false, cutlass::bfloat16_t, Flash_kernel_traits<192, 64, 64, 4, cutlass::bfloat16_t>>, Is_dropout=true, Is_causal=false]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(183): here
            instantiation of "void run_mha_fwd_hdim192<T>(Flash_fwd_params &, cudaStream_t) [with T=cutlass::bfloat16_t]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_hdim192_bf16_sm80.cu(15): here

/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_kernel.h(140): warning: variable "MMA_M" was declared but never referenced
          detected during:
            instantiation of "void flash::compute_attn_1rowblock<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &, int, int, int) [with Kernel_traits=Flash_fwd_kernel_traits<192, 64, 64, 4, false, false, cutlass::bfloat16_t, Flash_kernel_traits<192, 64, 64, 4, cutlass::bfloat16_t>>, Is_dropout=true, Is_causal=false, Is_even_N=true, Is_even_K=false, Return_softmax=false, Params=Flash_fwd_params]" 
(588): here
            instantiation of "void flash::compute_attn<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &) [with Kernel_traits=Flash_fwd_kernel_traits<192, 64, 64, 4, false, false, cutlass::bfloat16_t, Flash_kernel_traits<192, 64, 64, 4, cutlass::bfloat16_t>>, Is_dropout=true, Is_causal=false, Is_even_N=true, Is_even_K=false, Return_softmax=false, Params=Flash_fwd_params]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(15): here
            instantiation of "void flash_fwd_kernel<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax>(Flash_fwd_params) [with Kernel_traits=Flash_fwd_kernel_traits<192, 64, 64, 4, false, false, cutlass::bfloat16_t, Flash_kernel_traits<192, 64, 64, 4, cutlass::bfloat16_t>>, Is_dropout=true, Is_causal=false, Is_even_N=true, Is_even_K=false, Return_softmax=false]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(34): here
            instantiation of "void run_flash_fwd<Kernel_traits,Is_dropout,Is_causal>(Flash_fwd_params &, cudaStream_t) [with Kernel_traits=Flash_fwd_kernel_traits<192, 64, 64, 4, false, false, cutlass::bfloat16_t, Flash_kernel_traits<192, 64, 64, 4, cutlass::bfloat16_t>>, Is_dropout=true, Is_causal=false]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(183): here
            instantiation of "void run_mha_fwd_hdim192<T>(Flash_fwd_params &, cudaStream_t) [with T=cutlass::bfloat16_t]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_hdim192_bf16_sm80.cu(15): here

/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_kernel.h(275): warning: variable "tQrQ" was declared but never referenced
          detected during:
            instantiation of "void flash::compute_attn_1rowblock<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &, int, int, int) [with Kernel_traits=Flash_fwd_kernel_traits<192, 64, 64, 4, false, false, cutlass::bfloat16_t, Flash_kernel_traits<192, 64, 64, 4, cutlass::bfloat16_t>>, Is_dropout=true, Is_causal=false, Is_even_N=true, Is_even_K=false, Return_softmax=false, Params=Flash_fwd_params]" 
(588): here
            instantiation of "void flash::compute_attn<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &) [with Kernel_traits=Flash_fwd_kernel_traits<192, 64, 64, 4, false, false, cutlass::bfloat16_t, Flash_kernel_traits<192, 64, 64, 4, cutlass::bfloat16_t>>, Is_dropout=true, Is_causal=false, Is_even_N=true, Is_even_K=false, Return_softmax=false, Params=Flash_fwd_params]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(15): here
            instantiation of "void flash_fwd_kernel<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax>(Flash_fwd_params) [with Kernel_traits=Flash_fwd_kernel_traits<192, 64, 64, 4, false, false, cutlass::bfloat16_t, Flash_kernel_traits<192, 64, 64, 4, cutlass::bfloat16_t>>, Is_dropout=true, Is_causal=false, Is_even_N=true, Is_even_K=false, Return_softmax=false]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(34): here
            instantiation of "void run_flash_fwd<Kernel_traits,Is_dropout,Is_causal>(Flash_fwd_params &, cudaStream_t) [with Kernel_traits=Flash_fwd_kernel_traits<192, 64, 64, 4, false, false, cutlass::bfloat16_t, Flash_kernel_traits<192, 64, 64, 4, cutlass::bfloat16_t>>, Is_dropout=true, Is_causal=false]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(183): here
            instantiation of "void run_mha_fwd_hdim192<T>(Flash_fwd_params &, cudaStream_t) [with T=cutlass::bfloat16_t]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_hdim192_bf16_sm80.cu(15): here

/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_kernel.h(140): warning: variable "MMA_M" was declared but never referenced
          detected during:
            instantiation of "void flash::compute_attn_1rowblock<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &, int, int, int) [with Kernel_traits=Flash_fwd_kernel_traits<192, 64, 64, 4, false, false, cutlass::bfloat16_t, Flash_kernel_traits<192, 64, 64, 4, cutlass::bfloat16_t>>, Is_dropout=true, Is_causal=false, Is_even_N=false, Is_even_K=true, Return_softmax=true, Params=Flash_fwd_params]" 
(588): here
            instantiation of "void flash::compute_attn<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &) [with Kernel_traits=Flash_fwd_kernel_traits<192, 64, 64, 4, false, false, cutlass::bfloat16_t, Flash_kernel_traits<192, 64, 64, 4, cutlass::bfloat16_t>>, Is_dropout=true, Is_causal=false, Is_even_N=false, Is_even_K=true, Return_softmax=true, Params=Flash_fwd_params]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(15): here
            instantiation of "void flash_fwd_kernel<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax>(Flash_fwd_params) [with Kernel_traits=Flash_fwd_kernel_traits<192, 64, 64, 4, false, false, cutlass::bfloat16_t, Flash_kernel_traits<192, 64, 64, 4, cutlass::bfloat16_t>>, Is_dropout=true, Is_causal=false, Is_even_N=false, Is_even_K=true, Return_softmax=true]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(34): here
            instantiation of "void run_flash_fwd<Kernel_traits,Is_dropout,Is_causal>(Flash_fwd_params &, cudaStream_t) [with Kernel_traits=Flash_fwd_kernel_traits<192, 64, 64, 4, false, false, cutlass::bfloat16_t, Flash_kernel_traits<192, 64, 64, 4, cutlass::bfloat16_t>>, Is_dropout=true, Is_causal=false]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(183): here
            instantiation of "void run_mha_fwd_hdim192<T>(Flash_fwd_params &, cudaStream_t) [with T=cutlass::bfloat16_t]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_hdim192_bf16_sm80.cu(15): here

/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_kernel.h(275): warning: variable "tQrQ" was declared but never referenced
          detected during:
            instantiation of "void flash::compute_attn_1rowblock<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &, int, int, int) [with Kernel_traits=Flash_fwd_kernel_traits<192, 64, 64, 4, false, false, cutlass::bfloat16_t, Flash_kernel_traits<192, 64, 64, 4, cutlass::bfloat16_t>>, Is_dropout=true, Is_causal=false, Is_even_N=false, Is_even_K=true, Return_softmax=true, Params=Flash_fwd_params]" 
(588): here
            instantiation of "void flash::compute_attn<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &) [with Kernel_traits=Flash_fwd_kernel_traits<192, 64, 64, 4, false, false, cutlass::bfloat16_t, Flash_kernel_traits<192, 64, 64, 4, cutlass::bfloat16_t>>, Is_dropout=true, Is_causal=false, Is_even_N=false, Is_even_K=true, Return_softmax=true, Params=Flash_fwd_params]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(15): here
            instantiation of "void flash_fwd_kernel<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax>(Flash_fwd_params) [with Kernel_traits=Flash_fwd_kernel_traits<192, 64, 64, 4, false, false, cutlass::bfloat16_t, Flash_kernel_traits<192, 64, 64, 4, cutlass::bfloat16_t>>, Is_dropout=true, Is_causal=false, Is_even_N=false, Is_even_K=true, Return_softmax=true]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(34): here
            instantiation of "void run_flash_fwd<Kernel_traits,Is_dropout,Is_causal>(Flash_fwd_params &, cudaStream_t) [with Kernel_traits=Flash_fwd_kernel_traits<192, 64, 64, 4, false, false, cutlass::bfloat16_t, Flash_kernel_traits<192, 64, 64, 4, cutlass::bfloat16_t>>, Is_dropout=true, Is_causal=false]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(183): here
            instantiation of "void run_mha_fwd_hdim192<T>(Flash_fwd_params &, cudaStream_t) [with T=cutlass::bfloat16_t]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_hdim192_bf16_sm80.cu(15): here

/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_kernel.h(140): warning: variable "MMA_M" was declared but never referenced
          detected during:
            instantiation of "void flash::compute_attn_1rowblock<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &, int, int, int) [with Kernel_traits=Flash_fwd_kernel_traits<192, 64, 64, 4, false, false, cutlass::bfloat16_t, Flash_kernel_traits<192, 64, 64, 4, cutlass::bfloat16_t>>, Is_dropout=true, Is_causal=false, Is_even_N=false, Is_even_K=true, Return_softmax=false, Params=Flash_fwd_params]" 
(588): here
            instantiation of "void flash::compute_attn<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &) [with Kernel_traits=Flash_fwd_kernel_traits<192, 64, 64, 4, false, false, cutlass::bfloat16_t, Flash_kernel_traits<192, 64, 64, 4, cutlass::bfloat16_t>>, Is_dropout=true, Is_causal=false, Is_even_N=false, Is_even_K=true, Return_softmax=false, Params=Flash_fwd_params]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(15): here
            instantiation of "void flash_fwd_kernel<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax>(Flash_fwd_params) [with Kernel_traits=Flash_fwd_kernel_traits<192, 64, 64, 4, false, false, cutlass::bfloat16_t, Flash_kernel_traits<192, 64, 64, 4, cutlass::bfloat16_t>>, Is_dropout=true, Is_causal=false, Is_even_N=false, Is_even_K=true, Return_softmax=false]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(34): here
            instantiation of "void run_flash_fwd<Kernel_traits,Is_dropout,Is_causal>(Flash_fwd_params &, cudaStream_t) [with Kernel_traits=Flash_fwd_kernel_traits<192, 64, 64, 4, false, false, cutlass::bfloat16_t, Flash_kernel_traits<192, 64, 64, 4, cutlass::bfloat16_t>>, Is_dropout=true, Is_causal=false]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(183): here
            instantiation of "void run_mha_fwd_hdim192<T>(Flash_fwd_params &, cudaStream_t) [with T=cutlass::bfloat16_t]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_hdim192_bf16_sm80.cu(15): here

/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_kernel.h(275): warning: variable "tQrQ" was declared but never referenced
          detected during:
            instantiation of "void flash::compute_attn_1rowblock<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &, int, int, int) [with Kernel_traits=Flash_fwd_kernel_traits<192, 64, 64, 4, false, false, cutlass::bfloat16_t, Flash_kernel_traits<192, 64, 64, 4, cutlass::bfloat16_t>>, Is_dropout=true, Is_causal=false, Is_even_N=false, Is_even_K=true, Return_softmax=false, Params=Flash_fwd_params]" 
(588): here
            instantiation of "void flash::compute_attn<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &) [with Kernel_traits=Flash_fwd_kernel_traits<192, 64, 64, 4, false, false, cutlass::bfloat16_t, Flash_kernel_traits<192, 64, 64, 4, cutlass::bfloat16_t>>, Is_dropout=true, Is_causal=false, Is_even_N=false, Is_even_K=true, Return_softmax=false, Params=Flash_fwd_params]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(15): here
            instantiation of "void flash_fwd_kernel<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax>(Flash_fwd_params) [with Kernel_traits=Flash_fwd_kernel_traits<192, 64, 64, 4, false, false, cutlass::bfloat16_t, Flash_kernel_traits<192, 64, 64, 4, cutlass::bfloat16_t>>, Is_dropout=true, Is_causal=false, Is_even_N=false, Is_even_K=true, Return_softmax=false]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(34): here
            instantiation of "void run_flash_fwd<Kernel_traits,Is_dropout,Is_causal>(Flash_fwd_params &, cudaStream_t) [with Kernel_traits=Flash_fwd_kernel_traits<192, 64, 64, 4, false, false, cutlass::bfloat16_t, Flash_kernel_traits<192, 64, 64, 4, cutlass::bfloat16_t>>, Is_dropout=true, Is_causal=false]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(183): here
            instantiation of "void run_mha_fwd_hdim192<T>(Flash_fwd_params &, cudaStream_t) [with T=cutlass::bfloat16_t]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_hdim192_bf16_sm80.cu(15): here

/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_kernel.h(140): warning: variable "MMA_M" was declared but never referenced
          detected during:
            instantiation of "void flash::compute_attn_1rowblock<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &, int, int, int) [with Kernel_traits=Flash_fwd_kernel_traits<192, 64, 64, 4, false, false, cutlass::bfloat16_t, Flash_kernel_traits<192, 64, 64, 4, cutlass::bfloat16_t>>, Is_dropout=true, Is_causal=false, Is_even_N=false, Is_even_K=false, Return_softmax=true, Params=Flash_fwd_params]" 
(588): here
            instantiation of "void flash::compute_attn<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &) [with Kernel_traits=Flash_fwd_kernel_traits<192, 64, 64, 4, false, false, cutlass::bfloat16_t, Flash_kernel_traits<192, 64, 64, 4, cutlass::bfloat16_t>>, Is_dropout=true, Is_causal=false, Is_even_N=false, Is_even_K=false, Return_softmax=true, Params=Flash_fwd_params]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(15): here
            instantiation of "void flash_fwd_kernel<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax>(Flash_fwd_params) [with Kernel_traits=Flash_fwd_kernel_traits<192, 64, 64, 4, false, false, cutlass::bfloat16_t, Flash_kernel_traits<192, 64, 64, 4, cutlass::bfloat16_t>>, Is_dropout=true, Is_causal=false, Is_even_N=false, Is_even_K=false, Return_softmax=true]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(34): here
            instantiation of "void run_flash_fwd<Kernel_traits,Is_dropout,Is_causal>(Flash_fwd_params &, cudaStream_t) [with Kernel_traits=Flash_fwd_kernel_traits<192, 64, 64, 4, false, false, cutlass::bfloat16_t, Flash_kernel_traits<192, 64, 64, 4, cutlass::bfloat16_t>>, Is_dropout=true, Is_causal=false]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(183): here
            instantiation of "void run_mha_fwd_hdim192<T>(Flash_fwd_params &, cudaStream_t) [with T=cutlass::bfloat16_t]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_hdim192_bf16_sm80.cu(15): here

/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_kernel.h(275): warning: variable "tQrQ" was declared but never referenced
          detected during:
            instantiation of "void flash::compute_attn_1rowblock<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &, int, int, int) [with Kernel_traits=Flash_fwd_kernel_traits<192, 64, 64, 4, false, false, cutlass::bfloat16_t, Flash_kernel_traits<192, 64, 64, 4, cutlass::bfloat16_t>>, Is_dropout=true, Is_causal=false, Is_even_N=false, Is_even_K=false, Return_softmax=true, Params=Flash_fwd_params]" 
(588): here
            instantiation of "void flash::compute_attn<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &) [with Kernel_traits=Flash_fwd_kernel_traits<192, 64, 64, 4, false, false, cutlass::bfloat16_t, Flash_kernel_traits<192, 64, 64, 4, cutlass::bfloat16_t>>, Is_dropout=true, Is_causal=false, Is_even_N=false, Is_even_K=false, Return_softmax=true, Params=Flash_fwd_params]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(15): here
            instantiation of "void flash_fwd_kernel<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax>(Flash_fwd_params) [with Kernel_traits=Flash_fwd_kernel_traits<192, 64, 64, 4, false, false, cutlass::bfloat16_t, Flash_kernel_traits<192, 64, 64, 4, cutlass::bfloat16_t>>, Is_dropout=true, Is_causal=false, Is_even_N=false, Is_even_K=false, Return_softmax=true]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(34): here
            instantiation of "void run_flash_fwd<Kernel_traits,Is_dropout,Is_causal>(Flash_fwd_params &, cudaStream_t) [with Kernel_traits=Flash_fwd_kernel_traits<192, 64, 64, 4, false, false, cutlass::bfloat16_t, Flash_kernel_traits<192, 64, 64, 4, cutlass::bfloat16_t>>, Is_dropout=true, Is_causal=false]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(183): here
            instantiation of "void run_mha_fwd_hdim192<T>(Flash_fwd_params &, cudaStream_t) [with T=cutlass::bfloat16_t]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_hdim192_bf16_sm80.cu(15): here

/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_kernel.h(140): warning: variable "MMA_M" was declared but never referenced
          detected during:
            instantiation of "void flash::compute_attn_1rowblock<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &, int, int, int) [with Kernel_traits=Flash_fwd_kernel_traits<192, 64, 64, 4, false, false, cutlass::bfloat16_t, Flash_kernel_traits<192, 64, 64, 4, cutlass::bfloat16_t>>, Is_dropout=true, Is_causal=false, Is_even_N=false, Is_even_K=false, Return_softmax=false, Params=Flash_fwd_params]" 
(588): here
            instantiation of "void flash::compute_attn<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &) [with Kernel_traits=Flash_fwd_kernel_traits<192, 64, 64, 4, false, false, cutlass::bfloat16_t, Flash_kernel_traits<192, 64, 64, 4, cutlass::bfloat16_t>>, Is_dropout=true, Is_causal=false, Is_even_N=false, Is_even_K=false, Return_softmax=false, Params=Flash_fwd_params]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(15): here
            instantiation of "void flash_fwd_kernel<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax>(Flash_fwd_params) [with Kernel_traits=Flash_fwd_kernel_traits<192, 64, 64, 4, false, false, cutlass::bfloat16_t, Flash_kernel_traits<192, 64, 64, 4, cutlass::bfloat16_t>>, Is_dropout=true, Is_causal=false, Is_even_N=false, Is_even_K=false, Return_softmax=false]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(34): here
            instantiation of "void run_flash_fwd<Kernel_traits,Is_dropout,Is_causal>(Flash_fwd_params &, cudaStream_t) [with Kernel_traits=Flash_fwd_kernel_traits<192, 64, 64, 4, false, false, cutlass::bfloat16_t, Flash_kernel_traits<192, 64, 64, 4, cutlass::bfloat16_t>>, Is_dropout=true, Is_causal=false]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(183): here
            instantiation of "void run_mha_fwd_hdim192<T>(Flash_fwd_params &, cudaStream_t) [with T=cutlass::bfloat16_t]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_hdim192_bf16_sm80.cu(15): here

/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_kernel.h(275): warning: variable "tQrQ" was declared but never referenced
          detected during:
            instantiation of "void flash::compute_attn_1rowblock<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &, int, int, int) [with Kernel_traits=Flash_fwd_kernel_traits<192, 64, 64, 4, false, false, cutlass::bfloat16_t, Flash_kernel_traits<192, 64, 64, 4, cutlass::bfloat16_t>>, Is_dropout=true, Is_causal=false, Is_even_N=false, Is_even_K=false, Return_softmax=false, Params=Flash_fwd_params]" 
(588): here
            instantiation of "void flash::compute_attn<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &) [with Kernel_traits=Flash_fwd_kernel_traits<192, 64, 64, 4, false, false, cutlass::bfloat16_t, Flash_kernel_traits<192, 64, 64, 4, cutlass::bfloat16_t>>, Is_dropout=true, Is_causal=false, Is_even_N=false, Is_even_K=false, Return_softmax=false, Params=Flash_fwd_params]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(15): here
            instantiation of "void flash_fwd_kernel<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax>(Flash_fwd_params) [with Kernel_traits=Flash_fwd_kernel_traits<192, 64, 64, 4, false, false, cutlass::bfloat16_t, Flash_kernel_traits<192, 64, 64, 4, cutlass::bfloat16_t>>, Is_dropout=true, Is_causal=false, Is_even_N=false, Is_even_K=false, Return_softmax=false]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(34): here
            instantiation of "void run_flash_fwd<Kernel_traits,Is_dropout,Is_causal>(Flash_fwd_params &, cudaStream_t) [with Kernel_traits=Flash_fwd_kernel_traits<192, 64, 64, 4, false, false, cutlass::bfloat16_t, Flash_kernel_traits<192, 64, 64, 4, cutlass::bfloat16_t>>, Is_dropout=true, Is_causal=false]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(183): here
            instantiation of "void run_mha_fwd_hdim192<T>(Flash_fwd_params &, cudaStream_t) [with T=cutlass::bfloat16_t]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_hdim192_bf16_sm80.cu(15): here

/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_kernel.h(140): warning: variable "MMA_M" was declared but never referenced
          detected during:
            instantiation of "void flash::compute_attn_1rowblock<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &, int, int, int) [with Kernel_traits=Flash_fwd_kernel_traits<192, 128, 64, 8, false, false, cutlass::bfloat16_t, Flash_kernel_traits<192, 128, 64, 8, cutlass::bfloat16_t>>, Is_dropout=false, Is_causal=true, Is_even_N=true, Is_even_K=true, Return_softmax=false, Params=Flash_fwd_params]" 
(588): here
            instantiation of "void flash::compute_attn<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &) [with Kernel_traits=Flash_fwd_kernel_traits<192, 128, 64, 8, false, false, cutlass::bfloat16_t, Flash_kernel_traits<192, 128, 64, 8, cutlass::bfloat16_t>>, Is_dropout=false, Is_causal=true, Is_even_N=true, Is_even_K=true, Return_softmax=false, Params=Flash_fwd_params]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(15): here
            instantiation of "void flash_fwd_kernel<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax>(Flash_fwd_params) [with Kernel_traits=Flash_fwd_kernel_traits<192, 128, 64, 8, false, false, cutlass::bfloat16_t, Flash_kernel_traits<192, 128, 64, 8, cutlass::bfloat16_t>>, Is_dropout=false, Is_causal=true, Is_even_N=true, Is_even_K=true, Return_softmax=false]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(34): here
            instantiation of "void run_flash_fwd<Kernel_traits,Is_dropout,Is_causal>(Flash_fwd_params &, cudaStream_t) [with Kernel_traits=Flash_fwd_kernel_traits<192, 128, 64, 8, false, false, cutlass::bfloat16_t, Flash_kernel_traits<192, 128, 64, 8, cutlass::bfloat16_t>>, Is_dropout=false, Is_causal=true]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(183): here
            instantiation of "void run_mha_fwd_hdim192<T>(Flash_fwd_params &, cudaStream_t) [with T=cutlass::bfloat16_t]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_hdim192_bf16_sm80.cu(15): here

/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_kernel.h(275): warning: variable "tQrQ" was declared but never referenced
          detected during:
            instantiation of "void flash::compute_attn_1rowblock<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &, int, int, int) [with Kernel_traits=Flash_fwd_kernel_traits<192, 128, 64, 8, false, false, cutlass::bfloat16_t, Flash_kernel_traits<192, 128, 64, 8, cutlass::bfloat16_t>>, Is_dropout=false, Is_causal=true, Is_even_N=true, Is_even_K=true, Return_softmax=false, Params=Flash_fwd_params]" 
(588): here
            instantiation of "void flash::compute_attn<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &) [with Kernel_traits=Flash_fwd_kernel_traits<192, 128, 64, 8, false, false, cutlass::bfloat16_t, Flash_kernel_traits<192, 128, 64, 8, cutlass::bfloat16_t>>, Is_dropout=false, Is_causal=true, Is_even_N=true, Is_even_K=true, Return_softmax=false, Params=Flash_fwd_params]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(15): here
            instantiation of "void flash_fwd_kernel<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax>(Flash_fwd_params) [with Kernel_traits=Flash_fwd_kernel_traits<192, 128, 64, 8, false, false, cutlass::bfloat16_t, Flash_kernel_traits<192, 128, 64, 8, cutlass::bfloat16_t>>, Is_dropout=false, Is_causal=true, Is_even_N=true, Is_even_K=true, Return_softmax=false]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(34): here
            instantiation of "void run_flash_fwd<Kernel_traits,Is_dropout,Is_causal>(Flash_fwd_params &, cudaStream_t) [with Kernel_traits=Flash_fwd_kernel_traits<192, 128, 64, 8, false, false, cutlass::bfloat16_t, Flash_kernel_traits<192, 128, 64, 8, cutlass::bfloat16_t>>, Is_dropout=false, Is_causal=true]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(183): here
            instantiation of "void run_mha_fwd_hdim192<T>(Flash_fwd_params &, cudaStream_t) [with T=cutlass::bfloat16_t]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_hdim192_bf16_sm80.cu(15): here

/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_kernel.h(140): warning: variable "MMA_M" was declared but never referenced
          detected during:
            instantiation of "void flash::compute_attn_1rowblock<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &, int, int, int) [with Kernel_traits=Flash_fwd_kernel_traits<192, 128, 64, 8, false, false, cutlass::bfloat16_t, Flash_kernel_traits<192, 128, 64, 8, cutlass::bfloat16_t>>, Is_dropout=false, Is_causal=true, Is_even_N=true, Is_even_K=false, Return_softmax=false, Params=Flash_fwd_params]" 
(588): here
            instantiation of "void flash::compute_attn<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &) [with Kernel_traits=Flash_fwd_kernel_traits<192, 128, 64, 8, false, false, cutlass::bfloat16_t, Flash_kernel_traits<192, 128, 64, 8, cutlass::bfloat16_t>>, Is_dropout=false, Is_causal=true, Is_even_N=true, Is_even_K=false, Return_softmax=false, Params=Flash_fwd_params]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(15): here
            instantiation of "void flash_fwd_kernel<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax>(Flash_fwd_params) [with Kernel_traits=Flash_fwd_kernel_traits<192, 128, 64, 8, false, false, cutlass::bfloat16_t, Flash_kernel_traits<192, 128, 64, 8, cutlass::bfloat16_t>>, Is_dropout=false, Is_causal=true, Is_even_N=true, Is_even_K=false, Return_softmax=false]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(34): here
            instantiation of "void run_flash_fwd<Kernel_traits,Is_dropout,Is_causal>(Flash_fwd_params &, cudaStream_t) [with Kernel_traits=Flash_fwd_kernel_traits<192, 128, 64, 8, false, false, cutlass::bfloat16_t, Flash_kernel_traits<192, 128, 64, 8, cutlass::bfloat16_t>>, Is_dropout=false, Is_causal=true]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(183): here
            instantiation of "void run_mha_fwd_hdim192<T>(Flash_fwd_params &, cudaStream_t) [with T=cutlass::bfloat16_t]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_hdim192_bf16_sm80.cu(15): here

/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_kernel.h(275): warning: variable "tQrQ" was declared but never referenced
          detected during:
            instantiation of "void flash::compute_attn_1rowblock<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &, int, int, int) [with Kernel_traits=Flash_fwd_kernel_traits<192, 128, 64, 8, false, false, cutlass::bfloat16_t, Flash_kernel_traits<192, 128, 64, 8, cutlass::bfloat16_t>>, Is_dropout=false, Is_causal=true, Is_even_N=true, Is_even_K=false, Return_softmax=false, Params=Flash_fwd_params]" 
(588): here
            instantiation of "void flash::compute_attn<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &) [with Kernel_traits=Flash_fwd_kernel_traits<192, 128, 64, 8, false, false, cutlass::bfloat16_t, Flash_kernel_traits<192, 128, 64, 8, cutlass::bfloat16_t>>, Is_dropout=false, Is_causal=true, Is_even_N=true, Is_even_K=false, Return_softmax=false, Params=Flash_fwd_params]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(15): here
            instantiation of "void flash_fwd_kernel<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax>(Flash_fwd_params) [with Kernel_traits=Flash_fwd_kernel_traits<192, 128, 64, 8, false, false, cutlass::bfloat16_t, Flash_kernel_traits<192, 128, 64, 8, cutlass::bfloat16_t>>, Is_dropout=false, Is_causal=true, Is_even_N=true, Is_even_K=false, Return_softmax=false]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(34): here
            instantiation of "void run_flash_fwd<Kernel_traits,Is_dropout,Is_causal>(Flash_fwd_params &, cudaStream_t) [with Kernel_traits=Flash_fwd_kernel_traits<192, 128, 64, 8, false, false, cutlass::bfloat16_t, Flash_kernel_traits<192, 128, 64, 8, cutlass::bfloat16_t>>, Is_dropout=false, Is_causal=true]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(183): here
            instantiation of "void run_mha_fwd_hdim192<T>(Flash_fwd_params &, cudaStream_t) [with T=cutlass::bfloat16_t]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_hdim192_bf16_sm80.cu(15): here

/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_kernel.h(140): warning: variable "MMA_M" was declared but never referenced
          detected during:
            instantiation of "void flash::compute_attn_1rowblock<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &, int, int, int) [with Kernel_traits=Flash_fwd_kernel_traits<192, 128, 64, 8, false, false, cutlass::bfloat16_t, Flash_kernel_traits<192, 128, 64, 8, cutlass::bfloat16_t>>, Is_dropout=false, Is_causal=true, Is_even_N=false, Is_even_K=true, Return_softmax=false, Params=Flash_fwd_params]" 
(588): here
            instantiation of "void flash::compute_attn<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &) [with Kernel_traits=Flash_fwd_kernel_traits<192, 128, 64, 8, false, false, cutlass::bfloat16_t, Flash_kernel_traits<192, 128, 64, 8, cutlass::bfloat16_t>>, Is_dropout=false, Is_causal=true, Is_even_N=false, Is_even_K=true, Return_softmax=false, Params=Flash_fwd_params]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(15): here
            instantiation of "void flash_fwd_kernel<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax>(Flash_fwd_params) [with Kernel_traits=Flash_fwd_kernel_traits<192, 128, 64, 8, false, false, cutlass::bfloat16_t, Flash_kernel_traits<192, 128, 64, 8, cutlass::bfloat16_t>>, Is_dropout=false, Is_causal=true, Is_even_N=false, Is_even_K=true, Return_softmax=false]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(34): here
            instantiation of "void run_flash_fwd<Kernel_traits,Is_dropout,Is_causal>(Flash_fwd_params &, cudaStream_t) [with Kernel_traits=Flash_fwd_kernel_traits<192, 128, 64, 8, false, false, cutlass::bfloat16_t, Flash_kernel_traits<192, 128, 64, 8, cutlass::bfloat16_t>>, Is_dropout=false, Is_causal=true]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(183): here
            instantiation of "void run_mha_fwd_hdim192<T>(Flash_fwd_params &, cudaStream_t) [with T=cutlass::bfloat16_t]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_hdim192_bf16_sm80.cu(15): here

/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_kernel.h(275): warning: variable "tQrQ" was declared but never referenced
          detected during:
            instantiation of "void flash::compute_attn_1rowblock<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &, int, int, int) [with Kernel_traits=Flash_fwd_kernel_traits<192, 128, 64, 8, false, false, cutlass::bfloat16_t, Flash_kernel_traits<192, 128, 64, 8, cutlass::bfloat16_t>>, Is_dropout=false, Is_causal=true, Is_even_N=false, Is_even_K=true, Return_softmax=false, Params=Flash_fwd_params]" 
(588): here
            instantiation of "void flash::compute_attn<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &) [with Kernel_traits=Flash_fwd_kernel_traits<192, 128, 64, 8, false, false, cutlass::bfloat16_t, Flash_kernel_traits<192, 128, 64, 8, cutlass::bfloat16_t>>, Is_dropout=false, Is_causal=true, Is_even_N=false, Is_even_K=true, Return_softmax=false, Params=Flash_fwd_params]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(15): here
            instantiation of "void flash_fwd_kernel<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax>(Flash_fwd_params) [with Kernel_traits=Flash_fwd_kernel_traits<192, 128, 64, 8, false, false, cutlass::bfloat16_t, Flash_kernel_traits<192, 128, 64, 8, cutlass::bfloat16_t>>, Is_dropout=false, Is_causal=true, Is_even_N=false, Is_even_K=true, Return_softmax=false]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(34): here
            instantiation of "void run_flash_fwd<Kernel_traits,Is_dropout,Is_causal>(Flash_fwd_params &, cudaStream_t) [with Kernel_traits=Flash_fwd_kernel_traits<192, 128, 64, 8, false, false, cutlass::bfloat16_t, Flash_kernel_traits<192, 128, 64, 8, cutlass::bfloat16_t>>, Is_dropout=false, Is_causal=true]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(183): here
            instantiation of "void run_mha_fwd_hdim192<T>(Flash_fwd_params &, cudaStream_t) [with T=cutlass::bfloat16_t]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_hdim192_bf16_sm80.cu(15): here

/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_kernel.h(140): warning: variable "MMA_M" was declared but never referenced
          detected during:
            instantiation of "void flash::compute_attn_1rowblock<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &, int, int, int) [with Kernel_traits=Flash_fwd_kernel_traits<192, 128, 64, 8, false, false, cutlass::bfloat16_t, Flash_kernel_traits<192, 128, 64, 8, cutlass::bfloat16_t>>, Is_dropout=false, Is_causal=true, Is_even_N=false, Is_even_K=false, Return_softmax=false, Params=Flash_fwd_params]" 
(588): here
            instantiation of "void flash::compute_attn<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &) [with Kernel_traits=Flash_fwd_kernel_traits<192, 128, 64, 8, false, false, cutlass::bfloat16_t, Flash_kernel_traits<192, 128, 64, 8, cutlass::bfloat16_t>>, Is_dropout=false, Is_causal=true, Is_even_N=false, Is_even_K=false, Return_softmax=false, Params=Flash_fwd_params]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(15): here
            instantiation of "void flash_fwd_kernel<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax>(Flash_fwd_params) [with Kernel_traits=Flash_fwd_kernel_traits<192, 128, 64, 8, false, false, cutlass::bfloat16_t, Flash_kernel_traits<192, 128, 64, 8, cutlass::bfloat16_t>>, Is_dropout=false, Is_causal=true, Is_even_N=false, Is_even_K=false, Return_softmax=false]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(34): here
            instantiation of "void run_flash_fwd<Kernel_traits,Is_dropout,Is_causal>(Flash_fwd_params &, cudaStream_t) [with Kernel_traits=Flash_fwd_kernel_traits<192, 128, 64, 8, false, false, cutlass::bfloat16_t, Flash_kernel_traits<192, 128, 64, 8, cutlass::bfloat16_t>>, Is_dropout=false, Is_causal=true]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(183): here
            instantiation of "void run_mha_fwd_hdim192<T>(Flash_fwd_params &, cudaStream_t) [with T=cutlass::bfloat16_t]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_hdim192_bf16_sm80.cu(15): here

/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_kernel.h(275): warning: variable "tQrQ" was declared but never referenced
          detected during:
            instantiation of "void flash::compute_attn_1rowblock<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &, int, int, int) [with Kernel_traits=Flash_fwd_kernel_traits<192, 128, 64, 8, false, false, cutlass::bfloat16_t, Flash_kernel_traits<192, 128, 64, 8, cutlass::bfloat16_t>>, Is_dropout=false, Is_causal=true, Is_even_N=false, Is_even_K=false, Return_softmax=false, Params=Flash_fwd_params]" 
(588): here
            instantiation of "void flash::compute_attn<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &) [with Kernel_traits=Flash_fwd_kernel_traits<192, 128, 64, 8, false, false, cutlass::bfloat16_t, Flash_kernel_traits<192, 128, 64, 8, cutlass::bfloat16_t>>, Is_dropout=false, Is_causal=true, Is_even_N=false, Is_even_K=false, Return_softmax=false, Params=Flash_fwd_params]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(15): here
            instantiation of "void flash_fwd_kernel<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax>(Flash_fwd_params) [with Kernel_traits=Flash_fwd_kernel_traits<192, 128, 64, 8, false, false, cutlass::bfloat16_t, Flash_kernel_traits<192, 128, 64, 8, cutlass::bfloat16_t>>, Is_dropout=false, Is_causal=true, Is_even_N=false, Is_even_K=false, Return_softmax=false]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(34): here
            instantiation of "void run_flash_fwd<Kernel_traits,Is_dropout,Is_causal>(Flash_fwd_params &, cudaStream_t) [with Kernel_traits=Flash_fwd_kernel_traits<192, 128, 64, 8, false, false, cutlass::bfloat16_t, Flash_kernel_traits<192, 128, 64, 8, cutlass::bfloat16_t>>, Is_dropout=false, Is_causal=true]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(183): here
            instantiation of "void run_mha_fwd_hdim192<T>(Flash_fwd_params &, cudaStream_t) [with T=cutlass::bfloat16_t]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_hdim192_bf16_sm80.cu(15): here

/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_kernel.h(140): warning: variable "MMA_M" was declared but never referenced
          detected during:
            instantiation of "void flash::compute_attn_1rowblock<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &, int, int, int) [with Kernel_traits=Flash_fwd_kernel_traits<192, 128, 64, 8, false, false, cutlass::bfloat16_t, Flash_kernel_traits<192, 128, 64, 8, cutlass::bfloat16_t>>, Is_dropout=false, Is_causal=false, Is_even_N=true, Is_even_K=true, Return_softmax=false, Params=Flash_fwd_params]" 
(588): here
            instantiation of "void flash::compute_attn<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &) [with Kernel_traits=Flash_fwd_kernel_traits<192, 128, 64, 8, false, false, cutlass::bfloat16_t, Flash_kernel_traits<192, 128, 64, 8, cutlass::bfloat16_t>>, Is_dropout=false, Is_causal=false, Is_even_N=true, Is_even_K=true, Return_softmax=false, Params=Flash_fwd_params]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(15): here
            instantiation of "void flash_fwd_kernel<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax>(Flash_fwd_params) [with Kernel_traits=Flash_fwd_kernel_traits<192, 128, 64, 8, false, false, cutlass::bfloat16_t, Flash_kernel_traits<192, 128, 64, 8, cutlass::bfloat16_t>>, Is_dropout=false, Is_causal=false, Is_even_N=true, Is_even_K=true, Return_softmax=false]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(34): here
            instantiation of "void run_flash_fwd<Kernel_traits,Is_dropout,Is_causal>(Flash_fwd_params &, cudaStream_t) [with Kernel_traits=Flash_fwd_kernel_traits<192, 128, 64, 8, false, false, cutlass::bfloat16_t, Flash_kernel_traits<192, 128, 64, 8, cutlass::bfloat16_t>>, Is_dropout=false, Is_causal=false]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(183): here
            instantiation of "void run_mha_fwd_hdim192<T>(Flash_fwd_params &, cudaStream_t) [with T=cutlass::bfloat16_t]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_hdim192_bf16_sm80.cu(15): here

/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_kernel.h(275): warning: variable "tQrQ" was declared but never referenced
          detected during:
            instantiation of "void flash::compute_attn_1rowblock<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &, int, int, int) [with Kernel_traits=Flash_fwd_kernel_traits<192, 128, 64, 8, false, false, cutlass::bfloat16_t, Flash_kernel_traits<192, 128, 64, 8, cutlass::bfloat16_t>>, Is_dropout=false, Is_causal=false, Is_even_N=true, Is_even_K=true, Return_softmax=false, Params=Flash_fwd_params]" 
(588): here
            instantiation of "void flash::compute_attn<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &) [with Kernel_traits=Flash_fwd_kernel_traits<192, 128, 64, 8, false, false, cutlass::bfloat16_t, Flash_kernel_traits<192, 128, 64, 8, cutlass::bfloat16_t>>, Is_dropout=false, Is_causal=false, Is_even_N=true, Is_even_K=true, Return_softmax=false, Params=Flash_fwd_params]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(15): here
            instantiation of "void flash_fwd_kernel<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax>(Flash_fwd_params) [with Kernel_traits=Flash_fwd_kernel_traits<192, 128, 64, 8, false, false, cutlass::bfloat16_t, Flash_kernel_traits<192, 128, 64, 8, cutlass::bfloat16_t>>, Is_dropout=false, Is_causal=false, Is_even_N=true, Is_even_K=true, Return_softmax=false]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(34): here
            instantiation of "void run_flash_fwd<Kernel_traits,Is_dropout,Is_causal>(Flash_fwd_params &, cudaStream_t) [with Kernel_traits=Flash_fwd_kernel_traits<192, 128, 64, 8, false, false, cutlass::bfloat16_t, Flash_kernel_traits<192, 128, 64, 8, cutlass::bfloat16_t>>, Is_dropout=false, Is_causal=false]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(183): here
            instantiation of "void run_mha_fwd_hdim192<T>(Flash_fwd_params &, cudaStream_t) [with T=cutlass::bfloat16_t]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_hdim192_bf16_sm80.cu(15): here

/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_kernel.h(140): warning: variable "MMA_M" was declared but never referenced
          detected during:
            instantiation of "void flash::compute_attn_1rowblock<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &, int, int, int) [with Kernel_traits=Flash_fwd_kernel_traits<192, 128, 64, 8, false, false, cutlass::bfloat16_t, Flash_kernel_traits<192, 128, 64, 8, cutlass::bfloat16_t>>, Is_dropout=false, Is_causal=false, Is_even_N=true, Is_even_K=false, Return_softmax=false, Params=Flash_fwd_params]" 
(588): here
            instantiation of "void flash::compute_attn<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &) [with Kernel_traits=Flash_fwd_kernel_traits<192, 128, 64, 8, false, false, cutlass::bfloat16_t, Flash_kernel_traits<192, 128, 64, 8, cutlass::bfloat16_t>>, Is_dropout=false, Is_causal=false, Is_even_N=true, Is_even_K=false, Return_softmax=false, Params=Flash_fwd_params]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(15): here
            instantiation of "void flash_fwd_kernel<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax>(Flash_fwd_params) [with Kernel_traits=Flash_fwd_kernel_traits<192, 128, 64, 8, false, false, cutlass::bfloat16_t, Flash_kernel_traits<192, 128, 64, 8, cutlass::bfloat16_t>>, Is_dropout=false, Is_causal=false, Is_even_N=true, Is_even_K=false, Return_softmax=false]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(34): here
            instantiation of "void run_flash_fwd<Kernel_traits,Is_dropout,Is_causal>(Flash_fwd_params &, cudaStream_t) [with Kernel_traits=Flash_fwd_kernel_traits<192, 128, 64, 8, false, false, cutlass::bfloat16_t, Flash_kernel_traits<192, 128, 64, 8, cutlass::bfloat16_t>>, Is_dropout=false, Is_causal=false]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(183): here
            instantiation of "void run_mha_fwd_hdim192<T>(Flash_fwd_params &, cudaStream_t) [with T=cutlass::bfloat16_t]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_hdim192_bf16_sm80.cu(15): here

/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_kernel.h(275): warning: variable "tQrQ" was declared but never referenced
          detected during:
            instantiation of "void flash::compute_attn_1rowblock<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &, int, int, int) [with Kernel_traits=Flash_fwd_kernel_traits<192, 128, 64, 8, false, false, cutlass::bfloat16_t, Flash_kernel_traits<192, 128, 64, 8, cutlass::bfloat16_t>>, Is_dropout=false, Is_causal=false, Is_even_N=true, Is_even_K=false, Return_softmax=false, Params=Flash_fwd_params]" 
(588): here
            instantiation of "void flash::compute_attn<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &) [with Kernel_traits=Flash_fwd_kernel_traits<192, 128, 64, 8, false, false, cutlass::bfloat16_t, Flash_kernel_traits<192, 128, 64, 8, cutlass::bfloat16_t>>, Is_dropout=false, Is_causal=false, Is_even_N=true, Is_even_K=false, Return_softmax=false, Params=Flash_fwd_params]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(15): here
            instantiation of "void flash_fwd_kernel<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax>(Flash_fwd_params) [with Kernel_traits=Flash_fwd_kernel_traits<192, 128, 64, 8, false, false, cutlass::bfloat16_t, Flash_kernel_traits<192, 128, 64, 8, cutlass::bfloat16_t>>, Is_dropout=false, Is_causal=false, Is_even_N=true, Is_even_K=false, Return_softmax=false]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(34): here
            instantiation of "void run_flash_fwd<Kernel_traits,Is_dropout,Is_causal>(Flash_fwd_params &, cudaStream_t) [with Kernel_traits=Flash_fwd_kernel_traits<192, 128, 64, 8, false, false, cutlass::bfloat16_t, Flash_kernel_traits<192, 128, 64, 8, cutlass::bfloat16_t>>, Is_dropout=false, Is_causal=false]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(183): here
            instantiation of "void run_mha_fwd_hdim192<T>(Flash_fwd_params &, cudaStream_t) [with T=cutlass::bfloat16_t]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_hdim192_bf16_sm80.cu(15): here

/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_kernel.h(140): warning: variable "MMA_M" was declared but never referenced
          detected during:
            instantiation of "void flash::compute_attn_1rowblock<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &, int, int, int) [with Kernel_traits=Flash_fwd_kernel_traits<192, 128, 64, 8, false, false, cutlass::bfloat16_t, Flash_kernel_traits<192, 128, 64, 8, cutlass::bfloat16_t>>, Is_dropout=false, Is_causal=false, Is_even_N=false, Is_even_K=true, Return_softmax=false, Params=Flash_fwd_params]" 
(588): here
            instantiation of "void flash::compute_attn<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &) [with Kernel_traits=Flash_fwd_kernel_traits<192, 128, 64, 8, false, false, cutlass::bfloat16_t, Flash_kernel_traits<192, 128, 64, 8, cutlass::bfloat16_t>>, Is_dropout=false, Is_causal=false, Is_even_N=false, Is_even_K=true, Return_softmax=false, Params=Flash_fwd_params]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(15): here
            instantiation of "void flash_fwd_kernel<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax>(Flash_fwd_params) [with Kernel_traits=Flash_fwd_kernel_traits<192, 128, 64, 8, false, false, cutlass::bfloat16_t, Flash_kernel_traits<192, 128, 64, 8, cutlass::bfloat16_t>>, Is_dropout=false, Is_causal=false, Is_even_N=false, Is_even_K=true, Return_softmax=false]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(34): here
            instantiation of "void run_flash_fwd<Kernel_traits,Is_dropout,Is_causal>(Flash_fwd_params &, cudaStream_t) [with Kernel_traits=Flash_fwd_kernel_traits<192, 128, 64, 8, false, false, cutlass::bfloat16_t, Flash_kernel_traits<192, 128, 64, 8, cutlass::bfloat16_t>>, Is_dropout=false, Is_causal=false]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(183): here
            instantiation of "void run_mha_fwd_hdim192<T>(Flash_fwd_params &, cudaStream_t) [with T=cutlass::bfloat16_t]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_hdim192_bf16_sm80.cu(15): here

/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_kernel.h(275): warning: variable "tQrQ" was declared but never referenced
          detected during:
            instantiation of "void flash::compute_attn_1rowblock<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &, int, int, int) [with Kernel_traits=Flash_fwd_kernel_traits<192, 128, 64, 8, false, false, cutlass::bfloat16_t, Flash_kernel_traits<192, 128, 64, 8, cutlass::bfloat16_t>>, Is_dropout=false, Is_causal=false, Is_even_N=false, Is_even_K=true, Return_softmax=false, Params=Flash_fwd_params]" 
(588): here
            instantiation of "void flash::compute_attn<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &) [with Kernel_traits=Flash_fwd_kernel_traits<192, 128, 64, 8, false, false, cutlass::bfloat16_t, Flash_kernel_traits<192, 128, 64, 8, cutlass::bfloat16_t>>, Is_dropout=false, Is_causal=false, Is_even_N=false, Is_even_K=true, Return_softmax=false, Params=Flash_fwd_params]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(15): here
            instantiation of "void flash_fwd_kernel<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax>(Flash_fwd_params) [with Kernel_traits=Flash_fwd_kernel_traits<192, 128, 64, 8, false, false, cutlass::bfloat16_t, Flash_kernel_traits<192, 128, 64, 8, cutlass::bfloat16_t>>, Is_dropout=false, Is_causal=false, Is_even_N=false, Is_even_K=true, Return_softmax=false]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(34): here
            instantiation of "void run_flash_fwd<Kernel_traits,Is_dropout,Is_causal>(Flash_fwd_params &, cudaStream_t) [with Kernel_traits=Flash_fwd_kernel_traits<192, 128, 64, 8, false, false, cutlass::bfloat16_t, Flash_kernel_traits<192, 128, 64, 8, cutlass::bfloat16_t>>, Is_dropout=false, Is_causal=false]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(183): here
            instantiation of "void run_mha_fwd_hdim192<T>(Flash_fwd_params &, cudaStream_t) [with T=cutlass::bfloat16_t]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_hdim192_bf16_sm80.cu(15): here

/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_kernel.h(140): warning: variable "MMA_M" was declared but never referenced
          detected during:
            instantiation of "void flash::compute_attn_1rowblock<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &, int, int, int) [with Kernel_traits=Flash_fwd_kernel_traits<192, 128, 64, 8, false, false, cutlass::bfloat16_t, Flash_kernel_traits<192, 128, 64, 8, cutlass::bfloat16_t>>, Is_dropout=false, Is_causal=false, Is_even_N=false, Is_even_K=false, Return_softmax=false, Params=Flash_fwd_params]" 
(588): here
            instantiation of "void flash::compute_attn<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &) [with Kernel_traits=Flash_fwd_kernel_traits<192, 128, 64, 8, false, false, cutlass::bfloat16_t, Flash_kernel_traits<192, 128, 64, 8, cutlass::bfloat16_t>>, Is_dropout=false, Is_causal=false, Is_even_N=false, Is_even_K=false, Return_softmax=false, Params=Flash_fwd_params]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(15): here
            instantiation of "void flash_fwd_kernel<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax>(Flash_fwd_params) [with Kernel_traits=Flash_fwd_kernel_traits<192, 128, 64, 8, false, false, cutlass::bfloat16_t, Flash_kernel_traits<192, 128, 64, 8, cutlass::bfloat16_t>>, Is_dropout=false, Is_causal=false, Is_even_N=false, Is_even_K=false, Return_softmax=false]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(34): here
            instantiation of "void run_flash_fwd<Kernel_traits,Is_dropout,Is_causal>(Flash_fwd_params &, cudaStream_t) [with Kernel_traits=Flash_fwd_kernel_traits<192, 128, 64, 8, false, false, cutlass::bfloat16_t, Flash_kernel_traits<192, 128, 64, 8, cutlass::bfloat16_t>>, Is_dropout=false, Is_causal=false]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(183): here
            instantiation of "void run_mha_fwd_hdim192<T>(Flash_fwd_params &, cudaStream_t) [with T=cutlass::bfloat16_t]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_hdim192_bf16_sm80.cu(15): here

/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_kernel.h(275): warning: variable "tQrQ" was declared but never referenced
          detected during:
            instantiation of "void flash::compute_attn_1rowblock<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &, int, int, int) [with Kernel_traits=Flash_fwd_kernel_traits<192, 128, 64, 8, false, false, cutlass::bfloat16_t, Flash_kernel_traits<192, 128, 64, 8, cutlass::bfloat16_t>>, Is_dropout=false, Is_causal=false, Is_even_N=false, Is_even_K=false, Return_softmax=false, Params=Flash_fwd_params]" 
(588): here
            instantiation of "void flash::compute_attn<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &) [with Kernel_traits=Flash_fwd_kernel_traits<192, 128, 64, 8, false, false, cutlass::bfloat16_t, Flash_kernel_traits<192, 128, 64, 8, cutlass::bfloat16_t>>, Is_dropout=false, Is_causal=false, Is_even_N=false, Is_even_K=false, Return_softmax=false, Params=Flash_fwd_params]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(15): here
            instantiation of "void flash_fwd_kernel<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax>(Flash_fwd_params) [with Kernel_traits=Flash_fwd_kernel_traits<192, 128, 64, 8, false, false, cutlass::bfloat16_t, Flash_kernel_traits<192, 128, 64, 8, cutlass::bfloat16_t>>, Is_dropout=false, Is_causal=false, Is_even_N=false, Is_even_K=false, Return_softmax=false]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(34): here
            instantiation of "void run_flash_fwd<Kernel_traits,Is_dropout,Is_causal>(Flash_fwd_params &, cudaStream_t) [with Kernel_traits=Flash_fwd_kernel_traits<192, 128, 64, 8, false, false, cutlass::bfloat16_t, Flash_kernel_traits<192, 128, 64, 8, cutlass::bfloat16_t>>, Is_dropout=false, Is_causal=false]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(183): here
            instantiation of "void run_mha_fwd_hdim192<T>(Flash_fwd_params &, cudaStream_t) [with T=cutlass::bfloat16_t]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_hdim192_bf16_sm80.cu(15): here

ptxas info    : 2 bytes gmem
ptxas info    : Compiling entry function '_Z16flash_fwd_kernelI23Flash_fwd_kernel_traitsILi192ELi128ELi64ELi8ELb0ELb0EN7cutlass10bfloat16_tE19Flash_kernel_traitsILi192ELi128ELi64ELi8ES2_EELb0ELb0ELb0ELb0ELb0EEv16Flash_fwd_params' for 'sm_80'
ptxas info    : Function properties for _Z16flash_fwd_kernelI23Flash_fwd_kernel_traitsILi192ELi128ELi64ELi8ELb0ELb0EN7cutlass10bfloat16_tE19Flash_kernel_traitsILi192ELi128ELi64ELi8ES2_EELb0ELb0ELb0ELb0ELb0EEv16Flash_fwd_params
    0 bytes stack frame, 0 bytes spill stores, 0 bytes spill loads
ptxas info    : Used 209 registers, 584 bytes cmem[0]
ptxas info    : Compiling entry function '_Z16flash_fwd_kernelI23Flash_fwd_kernel_traitsILi192ELi128ELi64ELi8ELb0ELb0EN7cutlass10bfloat16_tE19Flash_kernel_traitsILi192ELi128ELi64ELi8ES2_EELb0ELb0ELb0ELb1ELb0EEv16Flash_fwd_params' for 'sm_80'
ptxas info    : Function properties for _Z16flash_fwd_kernelI23Flash_fwd_kernel_traitsILi192ELi128ELi64ELi8ELb0ELb0EN7cutlass10bfloat16_tE19Flash_kernel_traitsILi192ELi128ELi64ELi8ES2_EELb0ELb0ELb0ELb1ELb0EEv16Flash_fwd_params
    0 bytes stack frame, 0 bytes spill stores, 0 bytes spill loads
ptxas info    : Used 179 registers, 584 bytes cmem[0]
ptxas info    : Compiling entry function '_Z16flash_fwd_kernelI23Flash_fwd_kernel_traitsILi192ELi128ELi64ELi8ELb0ELb0EN7cutlass10bfloat16_tE19Flash_kernel_traitsILi192ELi128ELi64ELi8ES2_EELb0ELb0ELb1ELb0ELb0EEv16Flash_fwd_params' for 'sm_80'
ptxas info    : Function properties for _Z16flash_fwd_kernelI23Flash_fwd_kernel_traitsILi192ELi128ELi64ELi8ELb0ELb0EN7cutlass10bfloat16_tE19Flash_kernel_traitsILi192ELi128ELi64ELi8ES2_EELb0ELb0ELb1ELb0ELb0EEv16Flash_fwd_params
    0 bytes stack frame, 0 bytes spill stores, 0 bytes spill loads
ptxas info    : Used 173 registers, 584 bytes cmem[0]
ptxas info    : Compiling entry function '_Z16flash_fwd_kernelI23Flash_fwd_kernel_traitsILi192ELi128ELi64ELi8ELb0ELb0EN7cutlass10bfloat16_tE19Flash_kernel_traitsILi192ELi128ELi64ELi8ES2_EELb0ELb0ELb1ELb1ELb0EEv16Flash_fwd_params' for 'sm_80'
ptxas info    : Function properties for _Z16flash_fwd_kernelI23Flash_fwd_kernel_traitsILi192ELi128ELi64ELi8ELb0ELb0EN7cutlass10bfloat16_tE19Flash_kernel_traitsILi192ELi128ELi64ELi8ES2_EELb0ELb0ELb1ELb1ELb0EEv16Flash_fwd_params
    0 bytes stack frame, 0 bytes spill stores, 0 bytes spill loads
ptxas info    : Used 173 registers, 584 bytes cmem[0]
ptxas info    : Compiling entry function '_Z16flash_fwd_kernelI23Flash_fwd_kernel_traitsILi192ELi128ELi64ELi8ELb0ELb0EN7cutlass10bfloat16_tE19Flash_kernel_traitsILi192ELi128ELi64ELi8ES2_EELb0ELb1ELb0ELb0ELb0EEv16Flash_fwd_params' for 'sm_80'
ptxas info    : Function properties for _Z16flash_fwd_kernelI23Flash_fwd_kernel_traitsILi192ELi128ELi64ELi8ELb0ELb0EN7cutlass10bfloat16_tE19Flash_kernel_traitsILi192ELi128ELi64ELi8ES2_EELb0ELb1ELb0ELb0ELb0EEv16Flash_fwd_params
    0 bytes stack frame, 0 bytes spill stores, 0 bytes spill loads
ptxas info    : Used 220 registers, 584 bytes cmem[0]
ptxas info    : Compiling entry function '_Z16flash_fwd_kernelI23Flash_fwd_kernel_traitsILi192ELi128ELi64ELi8ELb0ELb0EN7cutlass10bfloat16_tE19Flash_kernel_traitsILi192ELi128ELi64ELi8ES2_EELb0ELb1ELb0ELb1ELb0EEv16Flash_fwd_params' for 'sm_80'
ptxas info    : Function properties for _Z16flash_fwd_kernelI23Flash_fwd_kernel_traitsILi192ELi128ELi64ELi8ELb0ELb0EN7cutlass10bfloat16_tE19Flash_kernel_traitsILi192ELi128ELi64ELi8ES2_EELb0ELb1ELb0ELb1ELb0EEv16Flash_fwd_params
    0 bytes stack frame, 0 bytes spill stores, 0 bytes spill loads
ptxas info    : Used 221 registers, 584 bytes cmem[0]
ptxas info    : Compiling entry function '_Z16flash_fwd_kernelI23Flash_fwd_kernel_traitsILi192ELi128ELi64ELi8ELb0ELb0EN7cutlass10bfloat16_tE19Flash_kernel_traitsILi192ELi128ELi64ELi8ES2_EELb0ELb1ELb1ELb0ELb0EEv16Flash_fwd_params' for 'sm_80'
ptxas info    : Function properties for _Z16flash_fwd_kernelI23Flash_fwd_kernel_traitsILi192ELi128ELi64ELi8ELb0ELb0EN7cutlass10bfloat16_tE19Flash_kernel_traitsILi192ELi128ELi64ELi8ES2_EELb0ELb1ELb1ELb0ELb0EEv16Flash_fwd_params
    0 bytes stack frame, 0 bytes spill stores, 0 bytes spill loads
ptxas info    : Used 225 registers, 584 bytes cmem[0]
ptxas info    : Compiling entry function '_Z16flash_fwd_kernelI23Flash_fwd_kernel_traitsILi192ELi128ELi64ELi8ELb0ELb0EN7cutlass10bfloat16_tE19Flash_kernel_traitsILi192ELi128ELi64ELi8ES2_EELb0ELb1ELb1ELb1ELb0EEv16Flash_fwd_params' for 'sm_80'
ptxas info    : Function properties for _Z16flash_fwd_kernelI23Flash_fwd_kernel_traitsILi192ELi128ELi64ELi8ELb0ELb0EN7cutlass10bfloat16_tE19Flash_kernel_traitsILi192ELi128ELi64ELi8ES2_EELb0ELb1ELb1ELb1ELb0EEv16Flash_fwd_params
    0 bytes stack frame, 0 bytes spill stores, 0 bytes spill loads
ptxas info    : Used 230 registers, 584 bytes cmem[0]
ptxas info    : Compiling entry function '_Z16flash_fwd_kernelI23Flash_fwd_kernel_traitsILi192ELi64ELi64ELi4ELb0ELb0EN7cutlass10bfloat16_tE19Flash_kernel_traitsILi192ELi64ELi64ELi4ES2_EELb1ELb0ELb0ELb0ELb0EEv16Flash_fwd_params' for 'sm_80'
ptxas info    : Function properties for _Z16flash_fwd_kernelI23Flash_fwd_kernel_traitsILi192ELi64ELi64ELi4ELb0ELb0EN7cutlass10bfloat16_tE19Flash_kernel_traitsILi192ELi64ELi64ELi4ES2_EELb1ELb0ELb0ELb0ELb0EEv16Flash_fwd_params
    0 bytes stack frame, 0 bytes spill stores, 0 bytes spill loads
ptxas info    : Used 222 registers, 584 bytes cmem[0]
ptxas info    : Compiling entry function '_Z16flash_fwd_kernelI23Flash_fwd_kernel_traitsILi192ELi64ELi64ELi4ELb0ELb0EN7cutlass10bfloat16_tE19Flash_kernel_traitsILi192ELi64ELi64ELi4ES2_EELb1ELb0ELb0ELb0ELb1EEv16Flash_fwd_params' for 'sm_80'
ptxas info    : Function properties for _Z16flash_fwd_kernelI23Flash_fwd_kernel_traitsILi192ELi64ELi64ELi4ELb0ELb0EN7cutlass10bfloat16_tE19Flash_kernel_traitsILi192ELi64ELi64ELi4ES2_EELb1ELb0ELb0ELb0ELb1EEv16Flash_fwd_params
    0 bytes stack frame, 0 bytes spill stores, 0 bytes spill loads
ptxas info    : Used 242 registers, 584 bytes cmem[0]
ptxas info    : Compiling entry function '_Z16flash_fwd_kernelI23Flash_fwd_kernel_traitsILi192ELi64ELi64ELi4ELb0ELb0EN7cutlass10bfloat16_tE19Flash_kernel_traitsILi192ELi64ELi64ELi4ES2_EELb1ELb0ELb0ELb1ELb0EEv16Flash_fwd_params' for 'sm_80'
ptxas info    : Function properties for _Z16flash_fwd_kernelI23Flash_fwd_kernel_traitsILi192ELi64ELi64ELi4ELb0ELb0EN7cutlass10bfloat16_tE19Flash_kernel_traitsILi192ELi64ELi64ELi4ES2_EELb1ELb0ELb0ELb1ELb0EEv16Flash_fwd_params
    0 bytes stack frame, 0 bytes spill stores, 0 bytes spill loads
ptxas info    : Used 221 registers, 584 bytes cmem[0]
ptxas info    : Compiling entry function '_Z16flash_fwd_kernelI23Flash_fwd_kernel_traitsILi192ELi64ELi64ELi4ELb0ELb0EN7cutlass10bfloat16_tE19Flash_kernel_traitsILi192ELi64ELi64ELi4ES2_EELb1ELb0ELb0ELb1ELb1EEv16Flash_fwd_params' for 'sm_80'
ptxas info    : Function properties for _Z16flash_fwd_kernelI23Flash_fwd_kernel_traitsILi192ELi64ELi64ELi4ELb0ELb0EN7cutlass10bfloat16_tE19Flash_kernel_traitsILi192ELi64ELi64ELi4ES2_EELb1ELb0ELb0ELb1ELb1EEv16Flash_fwd_params
    0 bytes stack frame, 0 bytes spill stores, 0 bytes spill loads
ptxas info    : Used 250 registers, 584 bytes cmem[0]
ptxas info    : Compiling entry function '_Z16flash_fwd_kernelI23Flash_fwd_kernel_traitsILi192ELi64ELi64ELi4ELb0ELb0EN7cutlass10bfloat16_tE19Flash_kernel_traitsILi192ELi64ELi64ELi4ES2_EELb1ELb0ELb1ELb0ELb0EEv16Flash_fwd_params' for 'sm_80'
ptxas info    : Function properties for _Z16flash_fwd_kernelI23Flash_fwd_kernel_traitsILi192ELi64ELi64ELi4ELb0ELb0EN7cutlass10bfloat16_tE19Flash_kernel_traitsILi192ELi64ELi64ELi4ES2_EELb1ELb0ELb1ELb0ELb0EEv16Flash_fwd_params
    0 bytes stack frame, 0 bytes spill stores, 0 bytes spill loads
ptxas info    : Used 223 registers, 584 bytes cmem[0]
ptxas info    : Compiling entry function '_Z16flash_fwd_kernelI23Flash_fwd_kernel_traitsILi192ELi64ELi64ELi4ELb0ELb0EN7cutlass10bfloat16_tE19Flash_kernel_traitsILi192ELi64ELi64ELi4ES2_EELb1ELb0ELb1ELb0ELb1EEv16Flash_fwd_params' for 'sm_80'
ptxas info    : Function properties for _Z16flash_fwd_kernelI23Flash_fwd_kernel_traitsILi192ELi64ELi64ELi4ELb0ELb0EN7cutlass10bfloat16_tE19Flash_kernel_traitsILi192ELi64ELi64ELi4ES2_EELb1ELb0ELb1ELb0ELb1EEv16Flash_fwd_params
    0 bytes stack frame, 0 bytes spill stores, 0 bytes spill loads
ptxas info    : Used 240 registers, 584 bytes cmem[0]
ptxas info    : Compiling entry function '_Z16flash_fwd_kernelI23Flash_fwd_kernel_traitsILi192ELi64ELi64ELi4ELb0ELb0EN7cutlass10bfloat16_tE19Flash_kernel_traitsILi192ELi64ELi64ELi4ES2_EELb1ELb0ELb1ELb1ELb0EEv16Flash_fwd_params' for 'sm_80'
ptxas info    : Function properties for _Z16flash_fwd_kernelI23Flash_fwd_kernel_traitsILi192ELi64ELi64ELi4ELb0ELb0EN7cutlass10bfloat16_tE19Flash_kernel_traitsILi192ELi64ELi64ELi4ES2_EELb1ELb0ELb1ELb1ELb0EEv16Flash_fwd_params
    0 bytes stack frame, 0 bytes spill stores, 0 bytes spill loads
ptxas info    : Used 227 registers, 584 bytes cmem[0]
ptxas info    : Compiling entry function '_Z16flash_fwd_kernelI23Flash_fwd_kernel_traitsILi192ELi64ELi64ELi4ELb0ELb0EN7cutlass10bfloat16_tE19Flash_kernel_traitsILi192ELi64ELi64ELi4ES2_EELb1ELb0ELb1ELb1ELb1EEv16Flash_fwd_params' for 'sm_80'
ptxas info    : Function properties for _Z16flash_fwd_kernelI23Flash_fwd_kernel_traitsILi192ELi64ELi64ELi4ELb0ELb0EN7cutlass10bfloat16_tE19Flash_kernel_traitsILi192ELi64ELi64ELi4ES2_EELb1ELb0ELb1ELb1ELb1EEv16Flash_fwd_params
    0 bytes stack frame, 0 bytes spill stores, 0 bytes spill loads
ptxas info    : Used 246 registers, 584 bytes cmem[0]
ptxas info    : Compiling entry function '_Z16flash_fwd_kernelI23Flash_fwd_kernel_traitsILi192ELi64ELi64ELi4ELb0ELb0EN7cutlass10bfloat16_tE19Flash_kernel_traitsILi192ELi64ELi64ELi4ES2_EELb1ELb1ELb0ELb0ELb0EEv16Flash_fwd_params' for 'sm_80'
ptxas info    : Function properties for _Z16flash_fwd_kernelI23Flash_fwd_kernel_traitsILi192ELi64ELi64ELi4ELb0ELb0EN7cutlass10bfloat16_tE19Flash_kernel_traitsILi192ELi64ELi64ELi4ES2_EELb1ELb1ELb0ELb0ELb0EEv16Flash_fwd_params
    0 bytes stack frame, 0 bytes spill stores, 0 bytes spill loads
ptxas info    : Used 222 registers, 584 bytes cmem[0]
ptxas info    : Compiling entry function '_Z16flash_fwd_kernelI23Flash_fwd_kernel_traitsILi192ELi64ELi64ELi4ELb0ELb0EN7cutlass10bfloat16_tE19Flash_kernel_traitsILi192ELi64ELi64ELi4ES2_EELb1ELb1ELb0ELb0ELb1EEv16Flash_fwd_params' for 'sm_80'
ptxas info    : Function properties for _Z16flash_fwd_kernelI23Flash_fwd_kernel_traitsILi192ELi64ELi64ELi4ELb0ELb0EN7cutlass10bfloat16_tE19Flash_kernel_traitsILi192ELi64ELi64ELi4ES2_EELb1ELb1ELb0ELb0ELb1EEv16Flash_fwd_params
    0 bytes stack frame, 0 bytes spill stores, 0 bytes spill loads
ptxas info    : Used 250 registers, 584 bytes cmem[0]
ptxas info    : Compiling entry function '_Z16flash_fwd_kernelI23Flash_fwd_kernel_traitsILi192ELi64ELi64ELi4ELb0ELb0EN7cutlass10bfloat16_tE19Flash_kernel_traitsILi192ELi64ELi64ELi4ES2_EELb1ELb1ELb0ELb1ELb0EEv16Flash_fwd_params' for 'sm_80'
ptxas info    : Function properties for _Z16flash_fwd_kernelI23Flash_fwd_kernel_traitsILi192ELi64ELi64ELi4ELb0ELb0EN7cutlass10bfloat16_tE19Flash_kernel_traitsILi192ELi64ELi64ELi4ES2_EELb1ELb1ELb0ELb1ELb0EEv16Flash_fwd_params
    0 bytes stack frame, 0 bytes spill stores, 0 bytes spill loads
ptxas info    : Used 223 registers, 584 bytes cmem[0]
ptxas info    : Compiling entry function '_Z16flash_fwd_kernelI23Flash_fwd_kernel_traitsILi192ELi64ELi64ELi4ELb0ELb0EN7cutlass10bfloat16_tE19Flash_kernel_traitsILi192ELi64ELi64ELi4ES2_EELb1ELb1ELb0ELb1ELb1EEv16Flash_fwd_params' for 'sm_80'
ptxas info    : Function properties for _Z16flash_fwd_kernelI23Flash_fwd_kernel_traitsILi192ELi64ELi64ELi4ELb0ELb0EN7cutlass10bfloat16_tE19Flash_kernel_traitsILi192ELi64ELi64ELi4ES2_EELb1ELb1ELb0ELb1ELb1EEv16Flash_fwd_params
    0 bytes stack frame, 0 bytes spill stores, 0 bytes spill loads
ptxas info    : Used 250 registers, 584 bytes cmem[0]
ptxas info    : Compiling entry function '_Z16flash_fwd_kernelI23Flash_fwd_kernel_traitsILi192ELi64ELi64ELi4ELb0ELb0EN7cutlass10bfloat16_tE19Flash_kernel_traitsILi192ELi64ELi64ELi4ES2_EELb1ELb1ELb1ELb0ELb0EEv16Flash_fwd_params' for 'sm_80'
ptxas info    : Function properties for _Z16flash_fwd_kernelI23Flash_fwd_kernel_traitsILi192ELi64ELi64ELi4ELb0ELb0EN7cutlass10bfloat16_tE19Flash_kernel_traitsILi192ELi64ELi64ELi4ES2_EELb1ELb1ELb1ELb0ELb0EEv16Flash_fwd_params
    0 bytes stack frame, 0 bytes spill stores, 0 bytes spill loads
ptxas info    : Used 231 registers, 584 bytes cmem[0]
ptxas info    : Compiling entry function '_Z16flash_fwd_kernelI23Flash_fwd_kernel_traitsILi192ELi64ELi64ELi4ELb0ELb0EN7cutlass10bfloat16_tE19Flash_kernel_traitsILi192ELi64ELi64ELi4ES2_EELb1ELb1ELb1ELb0ELb1EEv16Flash_fwd_params' for 'sm_80'
ptxas info    : Function properties for _Z16flash_fwd_kernelI23Flash_fwd_kernel_traitsILi192ELi64ELi64ELi4ELb0ELb0EN7cutlass10bfloat16_tE19Flash_kernel_traitsILi192ELi64ELi64ELi4ES2_EELb1ELb1ELb1ELb0ELb1EEv16Flash_fwd_params
    0 bytes stack frame, 0 bytes spill stores, 0 bytes spill loads
ptxas info    : Used 252 registers, 584 bytes cmem[0]
ptxas info    : Compiling entry function '_Z16flash_fwd_kernelI23Flash_fwd_kernel_traitsILi192ELi64ELi64ELi4ELb0ELb0EN7cutlass10bfloat16_tE19Flash_kernel_traitsILi192ELi64ELi64ELi4ES2_EELb1ELb1ELb1ELb1ELb0EEv16Flash_fwd_params' for 'sm_80'
ptxas info    : Function properties for _Z16flash_fwd_kernelI23Flash_fwd_kernel_traitsILi192ELi64ELi64ELi4ELb0ELb0EN7cutlass10bfloat16_tE19Flash_kernel_traitsILi192ELi64ELi64ELi4ES2_EELb1ELb1ELb1ELb1ELb0EEv16Flash_fwd_params
    0 bytes stack frame, 0 bytes spill stores, 0 bytes spill loads
ptxas info    : Used 226 registers, 584 bytes cmem[0]
ptxas info    : Compiling entry function '_Z16flash_fwd_kernelI23Flash_fwd_kernel_traitsILi192ELi64ELi64ELi4ELb0ELb0EN7cutlass10bfloat16_tE19Flash_kernel_traitsILi192ELi64ELi64ELi4ES2_EELb1ELb1ELb1ELb1ELb1EEv16Flash_fwd_params' for 'sm_80'
ptxas info    : Function properties for _Z16flash_fwd_kernelI23Flash_fwd_kernel_traitsILi192ELi64ELi64ELi4ELb0ELb0EN7cutlass10bfloat16_tE19Flash_kernel_traitsILi192ELi64ELi64ELi4ES2_EELb1ELb1ELb1ELb1ELb1EEv16Flash_fwd_params
    0 bytes stack frame, 0 bytes spill stores, 0 bytes spill loads
ptxas info    : Used 254 registers, 584 bytes cmem[0]
/anaconda/envs/flashattention/lib/python3.9/site-packages/torch/include/c10/core/SymInt.h(84): warning: integer conversion resulted in a change of sign

[7/36] /usr/local/cuda-11.4/bin/nvcc  -I/home/v-feiychen/flash-attention/csrc/flash_attn -I/home/v-feiychen/flash-attention/csrc/flash_attn/src -I/home/v-feiychen/flash-attention/csrc/cutlass/include -I/anaconda/envs/flashattention/lib/python3.9/site-packages/torch/include -I/anaconda/envs/flashattention/lib/python3.9/site-packages/torch/include/torch/csrc/api/include -I/anaconda/envs/flashattention/lib/python3.9/site-packages/torch/include/TH -I/anaconda/envs/flashattention/lib/python3.9/site-packages/torch/include/THC -I/usr/local/cuda-11.4/include -I/anaconda/envs/flashattention/include/python3.9 -c -c /home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_bwd_hdim32_fp16_sm80.cu -o /home/v-feiychen/flash-attention/build/temp.linux-x86_64-cpython-39/csrc/flash_attn/src/flash_bwd_hdim32_fp16_sm80.o -D__CUDA_NO_HALF_OPERATORS__ -D__CUDA_NO_HALF_CONVERSIONS__ -D__CUDA_NO_BFLOAT16_CONVERSIONS__ -D__CUDA_NO_HALF2_OPERATORS__ --expt-relaxed-constexpr --compiler-options ''"'"'-fPIC'"'"'' -O3 -std=c++17 -U__CUDA_NO_HALF_OPERATORS__ -U__CUDA_NO_HALF_CONVERSIONS__ -U__CUDA_NO_HALF2_OPERATORS__ -U__CUDA_NO_BFLOAT16_CONVERSIONS__ --expt-relaxed-constexpr --expt-extended-lambda --use_fast_math --ptxas-options=-v -lineinfo -gencode arch=compute_80,code=sm_80 --threads 4 -DTORCH_API_INCLUDE_EXTENSION_H '-DPYBIND11_COMPILER_TYPE="_gcc"' '-DPYBIND11_STDLIB="_libstdcpp"' '-DPYBIND11_BUILD_ABI="_cxxabi1011"' -DTORCH_EXTENSION_NAME=flash_attn_2_cuda -D_GLIBCXX_USE_CXX11_ABI=0
/anaconda/envs/flashattention/lib/python3.9/site-packages/torch/include/c10/core/SymInt.h(84): warning: integer conversion resulted in a change of sign

ptxas info    : 2 bytes gmem
ptxas info    : Compiling entry function '_Z44flash_bwd_dq_dk_dv_loop_seqk_parallel_kernelI23Flash_bwd_kernel_traitsILi32ELi128ELi128ELi8ELi4ELi4ELi4ELb1ELb0EN7cutlass6half_tE19Flash_kernel_traitsILi32ELi128ELi128ELi8ES2_EELb0ELb0ELb0ELb0EEv16Flash_bwd_params' for 'sm_80'
ptxas info    : Function properties for _Z44flash_bwd_dq_dk_dv_loop_seqk_parallel_kernelI23Flash_bwd_kernel_traitsILi32ELi128ELi128ELi8ELi4ELi4ELi4ELb1ELb0EN7cutlass6half_tE19Flash_kernel_traitsILi32ELi128ELi128ELi8ES2_EELb0ELb0ELb0ELb0EEv16Flash_bwd_params
    0 bytes stack frame, 0 bytes spill stores, 0 bytes spill loads
ptxas info    : Used 254 registers, 696 bytes cmem[0]
ptxas info    : Compiling entry function '_Z44flash_bwd_dq_dk_dv_loop_seqk_parallel_kernelI23Flash_bwd_kernel_traitsILi32ELi128ELi128ELi8ELi4ELi4ELi4ELb1ELb0EN7cutlass6half_tE19Flash_kernel_traitsILi32ELi128ELi128ELi8ES2_EELb0ELb0ELb0ELb1EEv16Flash_bwd_params' for 'sm_80'
ptxas info    : Function properties for _Z44flash_bwd_dq_dk_dv_loop_seqk_parallel_kernelI23Flash_bwd_kernel_traitsILi32ELi128ELi128ELi8ELi4ELi4ELi4ELb1ELb0EN7cutlass6half_tE19Flash_kernel_traitsILi32ELi128ELi128ELi8ES2_EELb0ELb0ELb0ELb1EEv16Flash_bwd_params
    0 bytes stack frame, 0 bytes spill stores, 0 bytes spill loads
ptxas info    : Used 254 registers, 696 bytes cmem[0]
ptxas info    : Compiling entry function '_Z44flash_bwd_dq_dk_dv_loop_seqk_parallel_kernelI23Flash_bwd_kernel_traitsILi32ELi128ELi128ELi8ELi4ELi4ELi4ELb1ELb0EN7cutlass6half_tE19Flash_kernel_traitsILi32ELi128ELi128ELi8ES2_EELb0ELb0ELb1ELb0EEv16Flash_bwd_params' for 'sm_80'
ptxas info    : Function properties for _Z44flash_bwd_dq_dk_dv_loop_seqk_parallel_kernelI23Flash_bwd_kernel_traitsILi32ELi128ELi128ELi8ELi4ELi4ELi4ELb1ELb0EN7cutlass6half_tE19Flash_kernel_traitsILi32ELi128ELi128ELi8ES2_EELb0ELb0ELb1ELb0EEv16Flash_bwd_params
    0 bytes stack frame, 0 bytes spill stores, 0 bytes spill loads
ptxas info    : Used 251 registers, 696 bytes cmem[0]
ptxas info    : Compiling entry function '_Z44flash_bwd_dq_dk_dv_loop_seqk_parallel_kernelI23Flash_bwd_kernel_traitsILi32ELi128ELi128ELi8ELi4ELi4ELi4ELb1ELb0EN7cutlass6half_tE19Flash_kernel_traitsILi32ELi128ELi128ELi8ES2_EELb0ELb0ELb1ELb1EEv16Flash_bwd_params' for 'sm_80'
ptxas info    : Function properties for _Z44flash_bwd_dq_dk_dv_loop_seqk_parallel_kernelI23Flash_bwd_kernel_traitsILi32ELi128ELi128ELi8ELi4ELi4ELi4ELb1ELb0EN7cutlass6half_tE19Flash_kernel_traitsILi32ELi128ELi128ELi8ES2_EELb0ELb0ELb1ELb1EEv16Flash_bwd_params
    0 bytes stack frame, 0 bytes spill stores, 0 bytes spill loads
ptxas info    : Used 251 registers, 696 bytes cmem[0]
ptxas info    : Compiling entry function '_Z44flash_bwd_dq_dk_dv_loop_seqk_parallel_kernelI23Flash_bwd_kernel_traitsILi32ELi128ELi128ELi8ELi4ELi4ELi4ELb1ELb0EN7cutlass6half_tE19Flash_kernel_traitsILi32ELi128ELi128ELi8ES2_EELb0ELb1ELb0ELb0EEv16Flash_bwd_params' for 'sm_80'
ptxas info    : Function properties for _Z44flash_bwd_dq_dk_dv_loop_seqk_parallel_kernelI23Flash_bwd_kernel_traitsILi32ELi128ELi128ELi8ELi4ELi4ELi4ELb1ELb0EN7cutlass6half_tE19Flash_kernel_traitsILi32ELi128ELi128ELi8ES2_EELb0ELb1ELb0ELb0EEv16Flash_bwd_params
    0 bytes stack frame, 0 bytes spill stores, 0 bytes spill loads
ptxas info    : Used 253 registers, 696 bytes cmem[0]
ptxas info    : Compiling entry function '_Z44flash_bwd_dq_dk_dv_loop_seqk_parallel_kernelI23Flash_bwd_kernel_traitsILi32ELi128ELi128ELi8ELi4ELi4ELi4ELb1ELb0EN7cutlass6half_tE19Flash_kernel_traitsILi32ELi128ELi128ELi8ES2_EELb0ELb1ELb0ELb1EEv16Flash_bwd_params' for 'sm_80'
ptxas info    : Function properties for _Z44flash_bwd_dq_dk_dv_loop_seqk_parallel_kernelI23Flash_bwd_kernel_traitsILi32ELi128ELi128ELi8ELi4ELi4ELi4ELb1ELb0EN7cutlass6half_tE19Flash_kernel_traitsILi32ELi128ELi128ELi8ES2_EELb0ELb1ELb0ELb1EEv16Flash_bwd_params
    0 bytes stack frame, 0 bytes spill stores, 0 bytes spill loads
ptxas info    : Used 252 registers, 696 bytes cmem[0]
ptxas info    : Compiling entry function '_Z44flash_bwd_dq_dk_dv_loop_seqk_parallel_kernelI23Flash_bwd_kernel_traitsILi32ELi128ELi128ELi8ELi4ELi4ELi4ELb1ELb0EN7cutlass6half_tE19Flash_kernel_traitsILi32ELi128ELi128ELi8ES2_EELb0ELb1ELb1ELb0EEv16Flash_bwd_params' for 'sm_80'
ptxas info    : Function properties for _Z44flash_bwd_dq_dk_dv_loop_seqk_parallel_kernelI23Flash_bwd_kernel_traitsILi32ELi128ELi128ELi8ELi4ELi4ELi4ELb1ELb0EN7cutlass6half_tE19Flash_kernel_traitsILi32ELi128ELi128ELi8ES2_EELb0ELb1ELb1ELb0EEv16Flash_bwd_params
    0 bytes stack frame, 0 bytes spill stores, 0 bytes spill loads
ptxas info    : Used 246 registers, 696 bytes cmem[0]
ptxas info    : Compiling entry function '_Z44flash_bwd_dq_dk_dv_loop_seqk_parallel_kernelI23Flash_bwd_kernel_traitsILi32ELi128ELi128ELi8ELi4ELi4ELi4ELb1ELb0EN7cutlass6half_tE19Flash_kernel_traitsILi32ELi128ELi128ELi8ES2_EELb0ELb1ELb1ELb1EEv16Flash_bwd_params' for 'sm_80'
ptxas info    : Function properties for _Z44flash_bwd_dq_dk_dv_loop_seqk_parallel_kernelI23Flash_bwd_kernel_traitsILi32ELi128ELi128ELi8ELi4ELi4ELi4ELb1ELb0EN7cutlass6half_tE19Flash_kernel_traitsILi32ELi128ELi128ELi8ES2_EELb0ELb1ELb1ELb1EEv16Flash_bwd_params
    0 bytes stack frame, 0 bytes spill stores, 0 bytes spill loads
ptxas info    : Used 249 registers, 696 bytes cmem[0]
ptxas info    : Compiling entry function '_Z27flash_bwd_convert_dq_kernelI23Flash_bwd_kernel_traitsILi32ELi128ELi128ELi8ELi4ELi4ELi4ELb1ELb0EN7cutlass6half_tE19Flash_kernel_traitsILi32ELi128ELi128ELi8ES2_EEEv16Flash_bwd_params' for 'sm_80'
ptxas info    : Function properties for _Z27flash_bwd_convert_dq_kernelI23Flash_bwd_kernel_traitsILi32ELi128ELi128ELi8ELi4ELi4ELi4ELb1ELb0EN7cutlass6half_tE19Flash_kernel_traitsILi32ELi128ELi128ELi8ES2_EEEv16Flash_bwd_params
    0 bytes stack frame, 0 bytes spill stores, 0 bytes spill loads
ptxas info    : Used 32 registers, 696 bytes cmem[0]
ptxas info    : Compiling entry function '_Z44flash_bwd_dq_dk_dv_loop_seqk_parallel_kernelI23Flash_bwd_kernel_traitsILi32ELi128ELi128ELi8ELi4ELi4ELi4ELb1ELb0EN7cutlass6half_tE19Flash_kernel_traitsILi32ELi128ELi128ELi8ES2_EELb1ELb0ELb0ELb0EEv16Flash_bwd_params' for 'sm_80'
ptxas info    : Function properties for _Z44flash_bwd_dq_dk_dv_loop_seqk_parallel_kernelI23Flash_bwd_kernel_traitsILi32ELi128ELi128ELi8ELi4ELi4ELi4ELb1ELb0EN7cutlass6half_tE19Flash_kernel_traitsILi32ELi128ELi128ELi8ES2_EELb1ELb0ELb0ELb0EEv16Flash_bwd_params
    0 bytes stack frame, 0 bytes spill stores, 0 bytes spill loads
ptxas info    : Used 253 registers, 696 bytes cmem[0]
ptxas info    : Compiling entry function '_Z44flash_bwd_dq_dk_dv_loop_seqk_parallel_kernelI23Flash_bwd_kernel_traitsILi32ELi128ELi128ELi8ELi4ELi4ELi4ELb1ELb0EN7cutlass6half_tE19Flash_kernel_traitsILi32ELi128ELi128ELi8ES2_EELb1ELb0ELb0ELb1EEv16Flash_bwd_params' for 'sm_80'
ptxas info    : Function properties for _Z44flash_bwd_dq_dk_dv_loop_seqk_parallel_kernelI23Flash_bwd_kernel_traitsILi32ELi128ELi128ELi8ELi4ELi4ELi4ELb1ELb0EN7cutlass6half_tE19Flash_kernel_traitsILi32ELi128ELi128ELi8ES2_EELb1ELb0ELb0ELb1EEv16Flash_bwd_params
    0 bytes stack frame, 0 bytes spill stores, 0 bytes spill loads
ptxas info    : Used 246 registers, 696 bytes cmem[0]
ptxas info    : Compiling entry function '_Z44flash_bwd_dq_dk_dv_loop_seqk_parallel_kernelI23Flash_bwd_kernel_traitsILi32ELi128ELi128ELi8ELi4ELi4ELi4ELb1ELb0EN7cutlass6half_tE19Flash_kernel_traitsILi32ELi128ELi128ELi8ES2_EELb1ELb0ELb1ELb0EEv16Flash_bwd_params' for 'sm_80'
ptxas info    : Function properties for _Z44flash_bwd_dq_dk_dv_loop_seqk_parallel_kernelI23Flash_bwd_kernel_traitsILi32ELi128ELi128ELi8ELi4ELi4ELi4ELb1ELb0EN7cutlass6half_tE19Flash_kernel_traitsILi32ELi128ELi128ELi8ES2_EELb1ELb0ELb1ELb0EEv16Flash_bwd_params
    0 bytes stack frame, 0 bytes spill stores, 0 bytes spill loads
ptxas info    : Used 245 registers, 696 bytes cmem[0]
ptxas info    : Compiling entry function '_Z44flash_bwd_dq_dk_dv_loop_seqk_parallel_kernelI23Flash_bwd_kernel_traitsILi32ELi128ELi128ELi8ELi4ELi4ELi4ELb1ELb0EN7cutlass6half_tE19Flash_kernel_traitsILi32ELi128ELi128ELi8ES2_EELb1ELb0ELb1ELb1EEv16Flash_bwd_params' for 'sm_80'
ptxas info    : Function properties for _Z44flash_bwd_dq_dk_dv_loop_seqk_parallel_kernelI23Flash_bwd_kernel_traitsILi32ELi128ELi128ELi8ELi4ELi4ELi4ELb1ELb0EN7cutlass6half_tE19Flash_kernel_traitsILi32ELi128ELi128ELi8ES2_EELb1ELb0ELb1ELb1EEv16Flash_bwd_params
    0 bytes stack frame, 0 bytes spill stores, 0 bytes spill loads
ptxas info    : Used 248 registers, 696 bytes cmem[0]
ptxas info    : Compiling entry function '_Z44flash_bwd_dq_dk_dv_loop_seqk_parallel_kernelI23Flash_bwd_kernel_traitsILi32ELi128ELi128ELi8ELi4ELi4ELi4ELb1ELb0EN7cutlass6half_tE19Flash_kernel_traitsILi32ELi128ELi128ELi8ES2_EELb1ELb1ELb0ELb0EEv16Flash_bwd_params' for 'sm_80'
ptxas info    : Function properties for _Z44flash_bwd_dq_dk_dv_loop_seqk_parallel_kernelI23Flash_bwd_kernel_traitsILi32ELi128ELi128ELi8ELi4ELi4ELi4ELb1ELb0EN7cutlass6half_tE19Flash_kernel_traitsILi32ELi128ELi128ELi8ES2_EELb1ELb1ELb0ELb0EEv16Flash_bwd_params
    0 bytes stack frame, 0 bytes spill stores, 0 bytes spill loads
ptxas info    : Used 251 registers, 696 bytes cmem[0]
ptxas info    : Compiling entry function '_Z44flash_bwd_dq_dk_dv_loop_seqk_parallel_kernelI23Flash_bwd_kernel_traitsILi32ELi128ELi128ELi8ELi4ELi4ELi4ELb1ELb0EN7cutlass6half_tE19Flash_kernel_traitsILi32ELi128ELi128ELi8ES2_EELb1ELb1ELb0ELb1EEv16Flash_bwd_params' for 'sm_80'
ptxas info    : Function properties for _Z44flash_bwd_dq_dk_dv_loop_seqk_parallel_kernelI23Flash_bwd_kernel_traitsILi32ELi128ELi128ELi8ELi4ELi4ELi4ELb1ELb0EN7cutlass6half_tE19Flash_kernel_traitsILi32ELi128ELi128ELi8ES2_EELb1ELb1ELb0ELb1EEv16Flash_bwd_params
    0 bytes stack frame, 0 bytes spill stores, 0 bytes spill loads
ptxas info    : Used 251 registers, 696 bytes cmem[0]
ptxas info    : Compiling entry function '_Z44flash_bwd_dq_dk_dv_loop_seqk_parallel_kernelI23Flash_bwd_kernel_traitsILi32ELi128ELi128ELi8ELi4ELi4ELi4ELb1ELb0EN7cutlass6half_tE19Flash_kernel_traitsILi32ELi128ELi128ELi8ES2_EELb1ELb1ELb1ELb0EEv16Flash_bwd_params' for 'sm_80'
ptxas info    : Function properties for _Z44flash_bwd_dq_dk_dv_loop_seqk_parallel_kernelI23Flash_bwd_kernel_traitsILi32ELi128ELi128ELi8ELi4ELi4ELi4ELb1ELb0EN7cutlass6half_tE19Flash_kernel_traitsILi32ELi128ELi128ELi8ES2_EELb1ELb1ELb1ELb0EEv16Flash_bwd_params
    0 bytes stack frame, 0 bytes spill stores, 0 bytes spill loads
ptxas info    : Used 255 registers, 696 bytes cmem[0]
ptxas info    : Compiling entry function '_Z44flash_bwd_dq_dk_dv_loop_seqk_parallel_kernelI23Flash_bwd_kernel_traitsILi32ELi128ELi128ELi8ELi4ELi4ELi4ELb1ELb0EN7cutlass6half_tE19Flash_kernel_traitsILi32ELi128ELi128ELi8ES2_EELb1ELb1ELb1ELb1EEv16Flash_bwd_params' for 'sm_80'
ptxas info    : Function properties for _Z44flash_bwd_dq_dk_dv_loop_seqk_parallel_kernelI23Flash_bwd_kernel_traitsILi32ELi128ELi128ELi8ELi4ELi4ELi4ELb1ELb0EN7cutlass6half_tE19Flash_kernel_traitsILi32ELi128ELi128ELi8ES2_EELb1ELb1ELb1ELb1EEv16Flash_bwd_params
    0 bytes stack frame, 0 bytes spill stores, 0 bytes spill loads
ptxas info    : Used 254 registers, 696 bytes cmem[0]
ptxas info    : Compiling entry function '_Z25flash_bwd_dot_do_o_kernelILb1E23Flash_bwd_kernel_traitsILi32ELi128ELi128ELi8ELi4ELi4ELi4ELb1ELb0EN7cutlass6half_tE19Flash_kernel_traitsILi32ELi128ELi128ELi8ES2_EEEv16Flash_bwd_params' for 'sm_80'
ptxas info    : Function properties for _Z25flash_bwd_dot_do_o_kernelILb1E23Flash_bwd_kernel_traitsILi32ELi128ELi128ELi8ELi4ELi4ELi4ELb1ELb0EN7cutlass6half_tE19Flash_kernel_traitsILi32ELi128ELi128ELi8ES2_EEEv16Flash_bwd_params
    0 bytes stack frame, 0 bytes spill stores, 0 bytes spill loads
ptxas info    : Used 31 registers, 696 bytes cmem[0]
ptxas info    : Compiling entry function '_Z27flash_bwd_convert_dq_kernelI23Flash_bwd_kernel_traitsILi32ELi128ELi128ELi8ELi4ELi4ELi4ELb0ELb0EN7cutlass6half_tE19Flash_kernel_traitsILi32ELi128ELi128ELi8ES2_EEEv16Flash_bwd_params' for 'sm_80'
ptxas info    : Function properties for _Z27flash_bwd_convert_dq_kernelI23Flash_bwd_kernel_traitsILi32ELi128ELi128ELi8ELi4ELi4ELi4ELb0ELb0EN7cutlass6half_tE19Flash_kernel_traitsILi32ELi128ELi128ELi8ES2_EEEv16Flash_bwd_params
    0 bytes stack frame, 0 bytes spill stores, 0 bytes spill loads
ptxas info    : Used 32 registers, 696 bytes cmem[0]
ptxas info    : Compiling entry function '_Z44flash_bwd_dq_dk_dv_loop_seqk_parallel_kernelI23Flash_bwd_kernel_traitsILi32ELi128ELi128ELi8ELi4ELi4ELi4ELb0ELb0EN7cutlass6half_tE19Flash_kernel_traitsILi32ELi128ELi128ELi8ES2_EELb1ELb0ELb0ELb0EEv16Flash_bwd_params' for 'sm_80'
ptxas info    : Function properties for _Z44flash_bwd_dq_dk_dv_loop_seqk_parallel_kernelI23Flash_bwd_kernel_traitsILi32ELi128ELi128ELi8ELi4ELi4ELi4ELb0ELb0EN7cutlass6half_tE19Flash_kernel_traitsILi32ELi128ELi128ELi8ES2_EELb1ELb0ELb0ELb0EEv16Flash_bwd_params
    0 bytes stack frame, 0 bytes spill stores, 0 bytes spill loads
ptxas info    : Used 240 registers, 696 bytes cmem[0]
ptxas info    : Compiling entry function '_Z44flash_bwd_dq_dk_dv_loop_seqk_parallel_kernelI23Flash_bwd_kernel_traitsILi32ELi128ELi128ELi8ELi4ELi4ELi4ELb0ELb0EN7cutlass6half_tE19Flash_kernel_traitsILi32ELi128ELi128ELi8ES2_EELb1ELb0ELb0ELb1EEv16Flash_bwd_params' for 'sm_80'
ptxas info    : Function properties for _Z44flash_bwd_dq_dk_dv_loop_seqk_parallel_kernelI23Flash_bwd_kernel_traitsILi32ELi128ELi128ELi8ELi4ELi4ELi4ELb0ELb0EN7cutlass6half_tE19Flash_kernel_traitsILi32ELi128ELi128ELi8ES2_EELb1ELb0ELb0ELb1EEv16Flash_bwd_params
    0 bytes stack frame, 0 bytes spill stores, 0 bytes spill loads
ptxas info    : Used 243 registers, 696 bytes cmem[0]
ptxas info    : Compiling entry function '_Z44flash_bwd_dq_dk_dv_loop_seqk_parallel_kernelI23Flash_bwd_kernel_traitsILi32ELi128ELi128ELi8ELi4ELi4ELi4ELb0ELb0EN7cutlass6half_tE19Flash_kernel_traitsILi32ELi128ELi128ELi8ES2_EELb1ELb0ELb1ELb0EEv16Flash_bwd_params' for 'sm_80'
ptxas info    : Function properties for _Z44flash_bwd_dq_dk_dv_loop_seqk_parallel_kernelI23Flash_bwd_kernel_traitsILi32ELi128ELi128ELi8ELi4ELi4ELi4ELb0ELb0EN7cutlass6half_tE19Flash_kernel_traitsILi32ELi128ELi128ELi8ES2_EELb1ELb0ELb1ELb0EEv16Flash_bwd_params
    0 bytes stack frame, 0 bytes spill stores, 0 bytes spill loads
ptxas info    : Used 242 registers, 696 bytes cmem[0]
ptxas info    : Compiling entry function '_Z44flash_bwd_dq_dk_dv_loop_seqk_parallel_kernelI23Flash_bwd_kernel_traitsILi32ELi128ELi128ELi8ELi4ELi4ELi4ELb0ELb0EN7cutlass6half_tE19Flash_kernel_traitsILi32ELi128ELi128ELi8ES2_EELb1ELb0ELb1ELb1EEv16Flash_bwd_params' for 'sm_80'
ptxas info    : Function properties for _Z44flash_bwd_dq_dk_dv_loop_seqk_parallel_kernelI23Flash_bwd_kernel_traitsILi32ELi128ELi128ELi8ELi4ELi4ELi4ELb0ELb0EN7cutlass6half_tE19Flash_kernel_traitsILi32ELi128ELi128ELi8ES2_EELb1ELb0ELb1ELb1EEv16Flash_bwd_params
    0 bytes stack frame, 0 bytes spill stores, 0 bytes spill loads
ptxas info    : Used 244 registers, 696 bytes cmem[0]
ptxas info    : Compiling entry function '_Z44flash_bwd_dq_dk_dv_loop_seqk_parallel_kernelI23Flash_bwd_kernel_traitsILi32ELi128ELi128ELi8ELi4ELi4ELi4ELb0ELb0EN7cutlass6half_tE19Flash_kernel_traitsILi32ELi128ELi128ELi8ES2_EELb1ELb1ELb0ELb0EEv16Flash_bwd_params' for 'sm_80'
ptxas info    : Function properties for _Z44flash_bwd_dq_dk_dv_loop_seqk_parallel_kernelI23Flash_bwd_kernel_traitsILi32ELi128ELi128ELi8ELi4ELi4ELi4ELb0ELb0EN7cutlass6half_tE19Flash_kernel_traitsILi32ELi128ELi128ELi8ES2_EELb1ELb1ELb0ELb0EEv16Flash_bwd_params
    0 bytes stack frame, 0 bytes spill stores, 0 bytes spill loads
ptxas info    : Used 251 registers, 696 bytes cmem[0]
ptxas info    : Compiling entry function '_Z44flash_bwd_dq_dk_dv_loop_seqk_parallel_kernelI23Flash_bwd_kernel_traitsILi32ELi128ELi128ELi8ELi4ELi4ELi4ELb0ELb0EN7cutlass6half_tE19Flash_kernel_traitsILi32ELi128ELi128ELi8ES2_EELb1ELb1ELb0ELb1EEv16Flash_bwd_params' for 'sm_80'
ptxas info    : Function properties for _Z44flash_bwd_dq_dk_dv_loop_seqk_parallel_kernelI23Flash_bwd_kernel_traitsILi32ELi128ELi128ELi8ELi4ELi4ELi4ELb0ELb0EN7cutlass6half_tE19Flash_kernel_traitsILi32ELi128ELi128ELi8ES2_EELb1ELb1ELb0ELb1EEv16Flash_bwd_params
    0 bytes stack frame, 0 bytes spill stores, 0 bytes spill loads
ptxas info    : Used 250 registers, 696 bytes cmem[0]
ptxas info    : Compiling entry function '_Z44flash_bwd_dq_dk_dv_loop_seqk_parallel_kernelI23Flash_bwd_kernel_traitsILi32ELi128ELi128ELi8ELi4ELi4ELi4ELb0ELb0EN7cutlass6half_tE19Flash_kernel_traitsILi32ELi128ELi128ELi8ES2_EELb1ELb1ELb1ELb0EEv16Flash_bwd_params' for 'sm_80'
ptxas info    : Function properties for _Z44flash_bwd_dq_dk_dv_loop_seqk_parallel_kernelI23Flash_bwd_kernel_traitsILi32ELi128ELi128ELi8ELi4ELi4ELi4ELb0ELb0EN7cutlass6half_tE19Flash_kernel_traitsILi32ELi128ELi128ELi8ES2_EELb1ELb1ELb1ELb0EEv16Flash_bwd_params
    0 bytes stack frame, 0 bytes spill stores, 0 bytes spill loads
ptxas info    : Used 244 registers, 696 bytes cmem[0]
ptxas info    : Compiling entry function '_Z44flash_bwd_dq_dk_dv_loop_seqk_parallel_kernelI23Flash_bwd_kernel_traitsILi32ELi128ELi128ELi8ELi4ELi4ELi4ELb0ELb0EN7cutlass6half_tE19Flash_kernel_traitsILi32ELi128ELi128ELi8ES2_EELb1ELb1ELb1ELb1EEv16Flash_bwd_params' for 'sm_80'
ptxas info    : Function properties for _Z44flash_bwd_dq_dk_dv_loop_seqk_parallel_kernelI23Flash_bwd_kernel_traitsILi32ELi128ELi128ELi8ELi4ELi4ELi4ELb0ELb0EN7cutlass6half_tE19Flash_kernel_traitsILi32ELi128ELi128ELi8ES2_EELb1ELb1ELb1ELb1EEv16Flash_bwd_params
    0 bytes stack frame, 0 bytes spill stores, 0 bytes spill loads
ptxas info    : Used 247 registers, 696 bytes cmem[0]
ptxas info    : Compiling entry function '_Z25flash_bwd_dot_do_o_kernelILb1E23Flash_bwd_kernel_traitsILi32ELi128ELi128ELi8ELi4ELi4ELi4ELb0ELb0EN7cutlass6half_tE19Flash_kernel_traitsILi32ELi128ELi128ELi8ES2_EEEv16Flash_bwd_params' for 'sm_80'
ptxas info    : Function properties for _Z25flash_bwd_dot_do_o_kernelILb1E23Flash_bwd_kernel_traitsILi32ELi128ELi128ELi8ELi4ELi4ELi4ELb0ELb0EN7cutlass6half_tE19Flash_kernel_traitsILi32ELi128ELi128ELi8ES2_EEEv16Flash_bwd_params
    0 bytes stack frame, 0 bytes spill stores, 0 bytes spill loads
ptxas info    : Used 31 registers, 696 bytes cmem[0]
/anaconda/envs/flashattention/lib/python3.9/site-packages/torch/include/c10/core/SymInt.h(84): warning: integer conversion resulted in a change of sign

[8/36] /usr/local/cuda-11.4/bin/nvcc  -I/home/v-feiychen/flash-attention/csrc/flash_attn -I/home/v-feiychen/flash-attention/csrc/flash_attn/src -I/home/v-feiychen/flash-attention/csrc/cutlass/include -I/anaconda/envs/flashattention/lib/python3.9/site-packages/torch/include -I/anaconda/envs/flashattention/lib/python3.9/site-packages/torch/include/torch/csrc/api/include -I/anaconda/envs/flashattention/lib/python3.9/site-packages/torch/include/TH -I/anaconda/envs/flashattention/lib/python3.9/site-packages/torch/include/THC -I/usr/local/cuda-11.4/include -I/anaconda/envs/flashattention/include/python3.9 -c -c /home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_bwd_hdim160_fp16_sm80.cu -o /home/v-feiychen/flash-attention/build/temp.linux-x86_64-cpython-39/csrc/flash_attn/src/flash_bwd_hdim160_fp16_sm80.o -D__CUDA_NO_HALF_OPERATORS__ -D__CUDA_NO_HALF_CONVERSIONS__ -D__CUDA_NO_BFLOAT16_CONVERSIONS__ -D__CUDA_NO_HALF2_OPERATORS__ --expt-relaxed-constexpr --compiler-options ''"'"'-fPIC'"'"'' -O3 -std=c++17 -U__CUDA_NO_HALF_OPERATORS__ -U__CUDA_NO_HALF_CONVERSIONS__ -U__CUDA_NO_HALF2_OPERATORS__ -U__CUDA_NO_BFLOAT16_CONVERSIONS__ --expt-relaxed-constexpr --expt-extended-lambda --use_fast_math --ptxas-options=-v -lineinfo -gencode arch=compute_80,code=sm_80 --threads 4 -DTORCH_API_INCLUDE_EXTENSION_H '-DPYBIND11_COMPILER_TYPE="_gcc"' '-DPYBIND11_STDLIB="_libstdcpp"' '-DPYBIND11_BUILD_ABI="_cxxabi1011"' -DTORCH_EXTENSION_NAME=flash_attn_2_cuda -D_GLIBCXX_USE_CXX11_ABI=0
/anaconda/envs/flashattention/lib/python3.9/site-packages/torch/include/c10/core/SymInt.h(84): warning: integer conversion resulted in a change of sign

ptxas info    : 2 bytes gmem
ptxas info    : Compiling entry function '_Z44flash_bwd_dq_dk_dv_loop_seqk_parallel_kernelI23Flash_bwd_kernel_traitsILi160ELi64ELi64ELi8ELi4ELi4ELi4ELb0ELb1EN7cutlass6half_tE19Flash_kernel_traitsILi160ELi64ELi64ELi8ES2_EELb0ELb0ELb0ELb0EEv16Flash_bwd_params' for 'sm_80'
ptxas info    : Function properties for _Z44flash_bwd_dq_dk_dv_loop_seqk_parallel_kernelI23Flash_bwd_kernel_traitsILi160ELi64ELi64ELi8ELi4ELi4ELi4ELb0ELb1EN7cutlass6half_tE19Flash_kernel_traitsILi160ELi64ELi64ELi8ES2_EELb0ELb0ELb0ELb0EEv16Flash_bwd_params
    0 bytes stack frame, 0 bytes spill stores, 0 bytes spill loads
ptxas info    : Used 204 registers, 696 bytes cmem[0]
ptxas info    : Compiling entry function '_Z44flash_bwd_dq_dk_dv_loop_seqk_parallel_kernelI23Flash_bwd_kernel_traitsILi160ELi64ELi64ELi8ELi4ELi4ELi4ELb0ELb1EN7cutlass6half_tE19Flash_kernel_traitsILi160ELi64ELi64ELi8ES2_EELb0ELb0ELb0ELb1EEv16Flash_bwd_params' for 'sm_80'
ptxas info    : Function properties for _Z44flash_bwd_dq_dk_dv_loop_seqk_parallel_kernelI23Flash_bwd_kernel_traitsILi160ELi64ELi64ELi8ELi4ELi4ELi4ELb0ELb1EN7cutlass6half_tE19Flash_kernel_traitsILi160ELi64ELi64ELi8ES2_EELb0ELb0ELb0ELb1EEv16Flash_bwd_params
    0 bytes stack frame, 0 bytes spill stores, 0 bytes spill loads
ptxas info    : Used 204 registers, 696 bytes cmem[0]
ptxas info    : Compiling entry function '_Z44flash_bwd_dq_dk_dv_loop_seqk_parallel_kernelI23Flash_bwd_kernel_traitsILi160ELi64ELi64ELi8ELi4ELi4ELi4ELb0ELb1EN7cutlass6half_tE19Flash_kernel_traitsILi160ELi64ELi64ELi8ES2_EELb0ELb0ELb1ELb0EEv16Flash_bwd_params' for 'sm_80'
ptxas info    : Function properties for _Z44flash_bwd_dq_dk_dv_loop_seqk_parallel_kernelI23Flash_bwd_kernel_traitsILi160ELi64ELi64ELi8ELi4ELi4ELi4ELb0ELb1EN7cutlass6half_tE19Flash_kernel_traitsILi160ELi64ELi64ELi8ES2_EELb0ELb0ELb1ELb0EEv16Flash_bwd_params
    0 bytes stack frame, 0 bytes spill stores, 0 bytes spill loads
ptxas info    : Used 190 registers, 696 bytes cmem[0]
ptxas info    : Compiling entry function '_Z44flash_bwd_dq_dk_dv_loop_seqk_parallel_kernelI23Flash_bwd_kernel_traitsILi160ELi64ELi64ELi8ELi4ELi4ELi4ELb0ELb1EN7cutlass6half_tE19Flash_kernel_traitsILi160ELi64ELi64ELi8ES2_EELb0ELb0ELb1ELb1EEv16Flash_bwd_params' for 'sm_80'
ptxas info    : Function properties for _Z44flash_bwd_dq_dk_dv_loop_seqk_parallel_kernelI23Flash_bwd_kernel_traitsILi160ELi64ELi64ELi8ELi4ELi4ELi4ELb0ELb1EN7cutlass6half_tE19Flash_kernel_traitsILi160ELi64ELi64ELi8ES2_EELb0ELb0ELb1ELb1EEv16Flash_bwd_params
    0 bytes stack frame, 0 bytes spill stores, 0 bytes spill loads
ptxas info    : Used 196 registers, 696 bytes cmem[0]
ptxas info    : Compiling entry function '_Z44flash_bwd_dq_dk_dv_loop_seqk_parallel_kernelI23Flash_bwd_kernel_traitsILi160ELi64ELi64ELi8ELi4ELi4ELi4ELb0ELb1EN7cutlass6half_tE19Flash_kernel_traitsILi160ELi64ELi64ELi8ES2_EELb0ELb1ELb0ELb0EEv16Flash_bwd_params' for 'sm_80'
ptxas info    : Function properties for _Z44flash_bwd_dq_dk_dv_loop_seqk_parallel_kernelI23Flash_bwd_kernel_traitsILi160ELi64ELi64ELi8ELi4ELi4ELi4ELb0ELb1EN7cutlass6half_tE19Flash_kernel_traitsILi160ELi64ELi64ELi8ES2_EELb0ELb1ELb0ELb0EEv16Flash_bwd_params
    0 bytes stack frame, 0 bytes spill stores, 0 bytes spill loads
ptxas info    : Used 210 registers, 696 bytes cmem[0]
ptxas info    : Compiling entry function '_Z44flash_bwd_dq_dk_dv_loop_seqk_parallel_kernelI23Flash_bwd_kernel_traitsILi160ELi64ELi64ELi8ELi4ELi4ELi4ELb0ELb1EN7cutlass6half_tE19Flash_kernel_traitsILi160ELi64ELi64ELi8ES2_EELb0ELb1ELb0ELb1EEv16Flash_bwd_params' for 'sm_80'
ptxas info    : Function properties for _Z44flash_bwd_dq_dk_dv_loop_seqk_parallel_kernelI23Flash_bwd_kernel_traitsILi160ELi64ELi64ELi8ELi4ELi4ELi4ELb0ELb1EN7cutlass6half_tE19Flash_kernel_traitsILi160ELi64ELi64ELi8ES2_EELb0ELb1ELb0ELb1EEv16Flash_bwd_params
    0 bytes stack frame, 0 bytes spill stores, 0 bytes spill loads
ptxas info    : Used 206 registers, 696 bytes cmem[0]
ptxas info    : Compiling entry function '_Z44flash_bwd_dq_dk_dv_loop_seqk_parallel_kernelI23Flash_bwd_kernel_traitsILi160ELi64ELi64ELi8ELi4ELi4ELi4ELb0ELb1EN7cutlass6half_tE19Flash_kernel_traitsILi160ELi64ELi64ELi8ES2_EELb0ELb1ELb1ELb0EEv16Flash_bwd_params' for 'sm_80'
ptxas info    : Function properties for _Z44flash_bwd_dq_dk_dv_loop_seqk_parallel_kernelI23Flash_bwd_kernel_traitsILi160ELi64ELi64ELi8ELi4ELi4ELi4ELb0ELb1EN7cutlass6half_tE19Flash_kernel_traitsILi160ELi64ELi64ELi8ES2_EELb0ELb1ELb1ELb0EEv16Flash_bwd_params
    0 bytes stack frame, 0 bytes spill stores, 0 bytes spill loads
ptxas info    : Used 202 registers, 696 bytes cmem[0]
ptxas info    : Compiling entry function '_Z44flash_bwd_dq_dk_dv_loop_seqk_parallel_kernelI23Flash_bwd_kernel_traitsILi160ELi64ELi64ELi8ELi4ELi4ELi4ELb0ELb1EN7cutlass6half_tE19Flash_kernel_traitsILi160ELi64ELi64ELi8ES2_EELb0ELb1ELb1ELb1EEv16Flash_bwd_params' for 'sm_80'
ptxas info    : Function properties for _Z44flash_bwd_dq_dk_dv_loop_seqk_parallel_kernelI23Flash_bwd_kernel_traitsILi160ELi64ELi64ELi8ELi4ELi4ELi4ELb0ELb1EN7cutlass6half_tE19Flash_kernel_traitsILi160ELi64ELi64ELi8ES2_EELb0ELb1ELb1ELb1EEv16Flash_bwd_params
    0 bytes stack frame, 0 bytes spill stores, 0 bytes spill loads
ptxas info    : Used 200 registers, 696 bytes cmem[0]
ptxas info    : Compiling entry function '_Z44flash_bwd_dq_dk_dv_loop_seqk_parallel_kernelI23Flash_bwd_kernel_traitsILi160ELi64ELi64ELi8ELi4ELi4ELi4ELb0ELb0EN7cutlass6half_tE19Flash_kernel_traitsILi160ELi64ELi64ELi8ES2_EELb0ELb0ELb0ELb0EEv16Flash_bwd_params' for 'sm_80'
ptxas info    : Function properties for _Z44flash_bwd_dq_dk_dv_loop_seqk_parallel_kernelI23Flash_bwd_kernel_traitsILi160ELi64ELi64ELi8ELi4ELi4ELi4ELb0ELb0EN7cutlass6half_tE19Flash_kernel_traitsILi160ELi64ELi64ELi8ES2_EELb0ELb0ELb0ELb0EEv16Flash_bwd_params
    0 bytes stack frame, 0 bytes spill stores, 0 bytes spill loads
ptxas info    : Used 210 registers, 696 bytes cmem[0]
ptxas info    : Compiling entry function '_Z44flash_bwd_dq_dk_dv_loop_seqk_parallel_kernelI23Flash_bwd_kernel_traitsILi160ELi64ELi64ELi8ELi4ELi4ELi4ELb0ELb0EN7cutlass6half_tE19Flash_kernel_traitsILi160ELi64ELi64ELi8ES2_EELb0ELb0ELb0ELb1EEv16Flash_bwd_params' for 'sm_80'
ptxas info    : Function properties for _Z44flash_bwd_dq_dk_dv_loop_seqk_parallel_kernelI23Flash_bwd_kernel_traitsILi160ELi64ELi64ELi8ELi4ELi4ELi4ELb0ELb0EN7cutlass6half_tE19Flash_kernel_traitsILi160ELi64ELi64ELi8ES2_EELb0ELb0ELb0ELb1EEv16Flash_bwd_params
    0 bytes stack frame, 0 bytes spill stores, 0 bytes spill loads
ptxas info    : Used 212 registers, 696 bytes cmem[0]
ptxas info    : Compiling entry function '_Z44flash_bwd_dq_dk_dv_loop_seqk_parallel_kernelI23Flash_bwd_kernel_traitsILi160ELi64ELi64ELi8ELi4ELi4ELi4ELb0ELb0EN7cutlass6half_tE19Flash_kernel_traitsILi160ELi64ELi64ELi8ES2_EELb0ELb0ELb1ELb0EEv16Flash_bwd_params' for 'sm_80'
ptxas info    : Function properties for _Z44flash_bwd_dq_dk_dv_loop_seqk_parallel_kernelI23Flash_bwd_kernel_traitsILi160ELi64ELi64ELi8ELi4ELi4ELi4ELb0ELb0EN7cutlass6half_tE19Flash_kernel_traitsILi160ELi64ELi64ELi8ES2_EELb0ELb0ELb1ELb0EEv16Flash_bwd_params
    0 bytes stack frame, 0 bytes spill stores, 0 bytes spill loads
ptxas info    : Used 204 registers, 696 bytes cmem[0]
ptxas info    : Compiling entry function '_Z44flash_bwd_dq_dk_dv_loop_seqk_parallel_kernelI23Flash_bwd_kernel_traitsILi160ELi64ELi64ELi8ELi4ELi4ELi4ELb0ELb0EN7cutlass6half_tE19Flash_kernel_traitsILi160ELi64ELi64ELi8ES2_EELb0ELb0ELb1ELb1EEv16Flash_bwd_params' for 'sm_80'
ptxas info    : Function properties for _Z44flash_bwd_dq_dk_dv_loop_seqk_parallel_kernelI23Flash_bwd_kernel_traitsILi160ELi64ELi64ELi8ELi4ELi4ELi4ELb0ELb0EN7cutlass6half_tE19Flash_kernel_traitsILi160ELi64ELi64ELi8ES2_EELb0ELb0ELb1ELb1EEv16Flash_bwd_params
    0 bytes stack frame, 0 bytes spill stores, 0 bytes spill loads
ptxas info    : Used 204 registers, 696 bytes cmem[0]
ptxas info    : Compiling entry function '_Z44flash_bwd_dq_dk_dv_loop_seqk_parallel_kernelI23Flash_bwd_kernel_traitsILi160ELi64ELi64ELi8ELi4ELi4ELi4ELb0ELb0EN7cutlass6half_tE19Flash_kernel_traitsILi160ELi64ELi64ELi8ES2_EELb0ELb1ELb0ELb0EEv16Flash_bwd_params' for 'sm_80'
ptxas info    : Function properties for _Z44flash_bwd_dq_dk_dv_loop_seqk_parallel_kernelI23Flash_bwd_kernel_traitsILi160ELi64ELi64ELi8ELi4ELi4ELi4ELb0ELb0EN7cutlass6half_tE19Flash_kernel_traitsILi160ELi64ELi64ELi8ES2_EELb0ELb1ELb0ELb0EEv16Flash_bwd_params
    0 bytes stack frame, 0 bytes spill stores, 0 bytes spill loads
ptxas info    : Used 216 registers, 696 bytes cmem[0]
ptxas info    : Compiling entry function '_Z44flash_bwd_dq_dk_dv_loop_seqk_parallel_kernelI23Flash_bwd_kernel_traitsILi160ELi64ELi64ELi8ELi4ELi4ELi4ELb0ELb0EN7cutlass6half_tE19Flash_kernel_traitsILi160ELi64ELi64ELi8ES2_EELb0ELb1ELb0ELb1EEv16Flash_bwd_params' for 'sm_80'
ptxas info    : Function properties for _Z44flash_bwd_dq_dk_dv_loop_seqk_parallel_kernelI23Flash_bwd_kernel_traitsILi160ELi64ELi64ELi8ELi4ELi4ELi4ELb0ELb0EN7cutlass6half_tE19Flash_kernel_traitsILi160ELi64ELi64ELi8ES2_EELb0ELb1ELb0ELb1EEv16Flash_bwd_params
    0 bytes stack frame, 0 bytes spill stores, 0 bytes spill loads
ptxas info    : Used 214 registers, 696 bytes cmem[0]
ptxas info    : Compiling entry function '_Z44flash_bwd_dq_dk_dv_loop_seqk_parallel_kernelI23Flash_bwd_kernel_traitsILi160ELi64ELi64ELi8ELi4ELi4ELi4ELb0ELb0EN7cutlass6half_tE19Flash_kernel_traitsILi160ELi64ELi64ELi8ES2_EELb0ELb1ELb1ELb0EEv16Flash_bwd_params' for 'sm_80'
ptxas info    : Function properties for _Z44flash_bwd_dq_dk_dv_loop_seqk_parallel_kernelI23Flash_bwd_kernel_traitsILi160ELi64ELi64ELi8ELi4ELi4ELi4ELb0ELb0EN7cutlass6half_tE19Flash_kernel_traitsILi160ELi64ELi64ELi8ES2_EELb0ELb1ELb1ELb0EEv16Flash_bwd_params
    0 bytes stack frame, 0 bytes spill stores, 0 bytes spill loads
ptxas info    : Used 208 registers, 696 bytes cmem[0]
ptxas info    : Compiling entry function '_Z44flash_bwd_dq_dk_dv_loop_seqk_parallel_kernelI23Flash_bwd_kernel_traitsILi160ELi64ELi64ELi8ELi4ELi4ELi4ELb0ELb0EN7cutlass6half_tE19Flash_kernel_traitsILi160ELi64ELi64ELi8ES2_EELb0ELb1ELb1ELb1EEv16Flash_bwd_params' for 'sm_80'
ptxas info    : Function properties for _Z44flash_bwd_dq_dk_dv_loop_seqk_parallel_kernelI23Flash_bwd_kernel_traitsILi160ELi64ELi64ELi8ELi4ELi4ELi4ELb0ELb0EN7cutlass6half_tE19Flash_kernel_traitsILi160ELi64ELi64ELi8ES2_EELb0ELb1ELb1ELb1EEv16Flash_bwd_params
    0 bytes stack frame, 0 bytes spill stores, 0 bytes spill loads
ptxas info    : Used 206 registers, 696 bytes cmem[0]
ptxas info    : Compiling entry function '_Z27flash_bwd_convert_dq_kernelI23Flash_bwd_kernel_traitsILi160ELi64ELi64ELi8ELi4ELi4ELi4ELb0ELb1EN7cutlass6half_tE19Flash_kernel_traitsILi160ELi64ELi64ELi8ES2_EEEv16Flash_bwd_params' for 'sm_80'
ptxas info    : Function properties for _Z27flash_bwd_convert_dq_kernelI23Flash_bwd_kernel_traitsILi160ELi64ELi64ELi8ELi4ELi4ELi4ELb0ELb1EN7cutlass6half_tE19Flash_kernel_traitsILi160ELi64ELi64ELi8ES2_EEEv16Flash_bwd_params
    0 bytes stack frame, 0 bytes spill stores, 0 bytes spill loads
ptxas info    : Used 32 registers, 696 bytes cmem[0]
ptxas info    : Compiling entry function '_Z44flash_bwd_dq_dk_dv_loop_seqk_parallel_kernelI23Flash_bwd_kernel_traitsILi160ELi64ELi64ELi8ELi4ELi4ELi4ELb0ELb1EN7cutlass6half_tE19Flash_kernel_traitsILi160ELi64ELi64ELi8ES2_EELb1ELb0ELb0ELb0EEv16Flash_bwd_params' for 'sm_80'
ptxas info    : Function properties for _Z44flash_bwd_dq_dk_dv_loop_seqk_parallel_kernelI23Flash_bwd_kernel_traitsILi160ELi64ELi64ELi8ELi4ELi4ELi4ELb0ELb1EN7cutlass6half_tE19Flash_kernel_traitsILi160ELi64ELi64ELi8ES2_EELb1ELb0ELb0ELb0EEv16Flash_bwd_params
    0 bytes stack frame, 0 bytes spill stores, 0 bytes spill loads
ptxas info    : Used 222 registers, 696 bytes cmem[0]
ptxas info    : Compiling entry function '_Z44flash_bwd_dq_dk_dv_loop_seqk_parallel_kernelI23Flash_bwd_kernel_traitsILi160ELi64ELi64ELi8ELi4ELi4ELi4ELb0ELb1EN7cutlass6half_tE19Flash_kernel_traitsILi160ELi64ELi64ELi8ES2_EELb1ELb0ELb0ELb1EEv16Flash_bwd_params' for 'sm_80'
ptxas info    : Function properties for _Z44flash_bwd_dq_dk_dv_loop_seqk_parallel_kernelI23Flash_bwd_kernel_traitsILi160ELi64ELi64ELi8ELi4ELi4ELi4ELb0ELb1EN7cutlass6half_tE19Flash_kernel_traitsILi160ELi64ELi64ELi8ES2_EELb1ELb0ELb0ELb1EEv16Flash_bwd_params
    0 bytes stack frame, 0 bytes spill stores, 0 bytes spill loads
ptxas info    : Used 222 registers, 696 bytes cmem[0]
ptxas info    : Compiling entry function '_Z44flash_bwd_dq_dk_dv_loop_seqk_parallel_kernelI23Flash_bwd_kernel_traitsILi160ELi64ELi64ELi8ELi4ELi4ELi4ELb0ELb1EN7cutlass6half_tE19Flash_kernel_traitsILi160ELi64ELi64ELi8ES2_EELb1ELb0ELb1ELb0EEv16Flash_bwd_params' for 'sm_80'
ptxas info    : Function properties for _Z44flash_bwd_dq_dk_dv_loop_seqk_parallel_kernelI23Flash_bwd_kernel_traitsILi160ELi64ELi64ELi8ELi4ELi4ELi4ELb0ELb1EN7cutlass6half_tE19Flash_kernel_traitsILi160ELi64ELi64ELi8ES2_EELb1ELb0ELb1ELb0EEv16Flash_bwd_params
    0 bytes stack frame, 0 bytes spill stores, 0 bytes spill loads
ptxas info    : Used 208 registers, 696 bytes cmem[0]
ptxas info    : Compiling entry function '_Z44flash_bwd_dq_dk_dv_loop_seqk_parallel_kernelI23Flash_bwd_kernel_traitsILi160ELi64ELi64ELi8ELi4ELi4ELi4ELb0ELb1EN7cutlass6half_tE19Flash_kernel_traitsILi160ELi64ELi64ELi8ES2_EELb1ELb0ELb1ELb1EEv16Flash_bwd_params' for 'sm_80'
ptxas info    : Function properties for _Z44flash_bwd_dq_dk_dv_loop_seqk_parallel_kernelI23Flash_bwd_kernel_traitsILi160ELi64ELi64ELi8ELi4ELi4ELi4ELb0ELb1EN7cutlass6half_tE19Flash_kernel_traitsILi160ELi64ELi64ELi8ES2_EELb1ELb0ELb1ELb1EEv16Flash_bwd_params
    0 bytes stack frame, 0 bytes spill stores, 0 bytes spill loads
ptxas info    : Used 214 registers, 696 bytes cmem[0]
ptxas info    : Compiling entry function '_Z44flash_bwd_dq_dk_dv_loop_seqk_parallel_kernelI23Flash_bwd_kernel_traitsILi160ELi64ELi64ELi8ELi4ELi4ELi4ELb0ELb1EN7cutlass6half_tE19Flash_kernel_traitsILi160ELi64ELi64ELi8ES2_EELb1ELb1ELb0ELb0EEv16Flash_bwd_params' for 'sm_80'
ptxas info    : Function properties for _Z44flash_bwd_dq_dk_dv_loop_seqk_parallel_kernelI23Flash_bwd_kernel_traitsILi160ELi64ELi64ELi8ELi4ELi4ELi4ELb0ELb1EN7cutlass6half_tE19Flash_kernel_traitsILi160ELi64ELi64ELi8ES2_EELb1ELb1ELb0ELb0EEv16Flash_bwd_params
    0 bytes stack frame, 0 bytes spill stores, 0 bytes spill loads
ptxas info    : Used 216 registers, 696 bytes cmem[0]
ptxas info    : Compiling entry function '_Z44flash_bwd_dq_dk_dv_loop_seqk_parallel_kernelI23Flash_bwd_kernel_traitsILi160ELi64ELi64ELi8ELi4ELi4ELi4ELb0ELb1EN7cutlass6half_tE19Flash_kernel_traitsILi160ELi64ELi64ELi8ES2_EELb1ELb1ELb0ELb1EEv16Flash_bwd_params' for 'sm_80'
ptxas info    : Function properties for _Z44flash_bwd_dq_dk_dv_loop_seqk_parallel_kernelI23Flash_bwd_kernel_traitsILi160ELi64ELi64ELi8ELi4ELi4ELi4ELb0ELb1EN7cutlass6half_tE19Flash_kernel_traitsILi160ELi64ELi64ELi8ES2_EELb1ELb1ELb0ELb1EEv16Flash_bwd_params
    0 bytes stack frame, 0 bytes spill stores, 0 bytes spill loads
ptxas info    : Used 212 registers, 696 bytes cmem[0]
ptxas info    : Compiling entry function '_Z44flash_bwd_dq_dk_dv_loop_seqk_parallel_kernelI23Flash_bwd_kernel_traitsILi160ELi64ELi64ELi8ELi4ELi4ELi4ELb0ELb1EN7cutlass6half_tE19Flash_kernel_traitsILi160ELi64ELi64ELi8ES2_EELb1ELb1ELb1ELb0EEv16Flash_bwd_params' for 'sm_80'
ptxas info    : Function properties for _Z44flash_bwd_dq_dk_dv_loop_seqk_parallel_kernelI23Flash_bwd_kernel_traitsILi160ELi64ELi64ELi8ELi4ELi4ELi4ELb0ELb1EN7cutlass6half_tE19Flash_kernel_traitsILi160ELi64ELi64ELi8ES2_EELb1ELb1ELb1ELb0EEv16Flash_bwd_params
    0 bytes stack frame, 0 bytes spill stores, 0 bytes spill loads
ptxas info    : Used 208 registers, 696 bytes cmem[0]
ptxas info    : Compiling entry function '_Z44flash_bwd_dq_dk_dv_loop_seqk_parallel_kernelI23Flash_bwd_kernel_traitsILi160ELi64ELi64ELi8ELi4ELi4ELi4ELb0ELb1EN7cutlass6half_tE19Flash_kernel_traitsILi160ELi64ELi64ELi8ES2_EELb1ELb1ELb1ELb1EEv16Flash_bwd_params' for 'sm_80'
ptxas info    : Function properties for _Z44flash_bwd_dq_dk_dv_loop_seqk_parallel_kernelI23Flash_bwd_kernel_traitsILi160ELi64ELi64ELi8ELi4ELi4ELi4ELb0ELb1EN7cutlass6half_tE19Flash_kernel_traitsILi160ELi64ELi64ELi8ES2_EELb1ELb1ELb1ELb1EEv16Flash_bwd_params
    0 bytes stack frame, 0 bytes spill stores, 0 bytes spill loads
ptxas info    : Used 206 registers, 696 bytes cmem[0]
ptxas info    : Compiling entry function '_Z25flash_bwd_dot_do_o_kernelILb1E23Flash_bwd_kernel_traitsILi160ELi64ELi64ELi8ELi4ELi4ELi4ELb0ELb1EN7cutlass6half_tE19Flash_kernel_traitsILi160ELi64ELi64ELi8ES2_EEEv16Flash_bwd_params' for 'sm_80'
ptxas info    : Function properties for _Z25flash_bwd_dot_do_o_kernelILb1E23Flash_bwd_kernel_traitsILi160ELi64ELi64ELi8ELi4ELi4ELi4ELb0ELb1EN7cutlass6half_tE19Flash_kernel_traitsILi160ELi64ELi64ELi8ES2_EEEv16Flash_bwd_params
    0 bytes stack frame, 0 bytes spill stores, 0 bytes spill loads
ptxas info    : Used 64 registers, 696 bytes cmem[0]
ptxas info    : Compiling entry function '_Z27flash_bwd_convert_dq_kernelI23Flash_bwd_kernel_traitsILi160ELi64ELi64ELi8ELi4ELi4ELi4ELb0ELb0EN7cutlass6half_tE19Flash_kernel_traitsILi160ELi64ELi64ELi8ES2_EEEv16Flash_bwd_params' for 'sm_80'
ptxas info    : Function properties for _Z27flash_bwd_convert_dq_kernelI23Flash_bwd_kernel_traitsILi160ELi64ELi64ELi8ELi4ELi4ELi4ELb0ELb0EN7cutlass6half_tE19Flash_kernel_traitsILi160ELi64ELi64ELi8ES2_EEEv16Flash_bwd_params
    0 bytes stack frame, 0 bytes spill stores, 0 bytes spill loads
ptxas info    : Used 32 registers, 696 bytes cmem[0]
ptxas info    : Compiling entry function '_Z44flash_bwd_dq_dk_dv_loop_seqk_parallel_kernelI23Flash_bwd_kernel_traitsILi160ELi64ELi64ELi8ELi4ELi4ELi4ELb0ELb0EN7cutlass6half_tE19Flash_kernel_traitsILi160ELi64ELi64ELi8ES2_EELb1ELb0ELb0ELb0EEv16Flash_bwd_params' for 'sm_80'
ptxas info    : Function properties for _Z44flash_bwd_dq_dk_dv_loop_seqk_parallel_kernelI23Flash_bwd_kernel_traitsILi160ELi64ELi64ELi8ELi4ELi4ELi4ELb0ELb0EN7cutlass6half_tE19Flash_kernel_traitsILi160ELi64ELi64ELi8ES2_EELb1ELb0ELb0ELb0EEv16Flash_bwd_params
    0 bytes stack frame, 0 bytes spill stores, 0 bytes spill loads
ptxas info    : Used 226 registers, 696 bytes cmem[0]
ptxas info    : Compiling entry function '_Z44flash_bwd_dq_dk_dv_loop_seqk_parallel_kernelI23Flash_bwd_kernel_traitsILi160ELi64ELi64ELi8ELi4ELi4ELi4ELb0ELb0EN7cutlass6half_tE19Flash_kernel_traitsILi160ELi64ELi64ELi8ES2_EELb1ELb0ELb0ELb1EEv16Flash_bwd_params' for 'sm_80'
ptxas info    : Function properties for _Z44flash_bwd_dq_dk_dv_loop_seqk_parallel_kernelI23Flash_bwd_kernel_traitsILi160ELi64ELi64ELi8ELi4ELi4ELi4ELb0ELb0EN7cutlass6half_tE19Flash_kernel_traitsILi160ELi64ELi64ELi8ES2_EELb1ELb0ELb0ELb1EEv16Flash_bwd_params
    0 bytes stack frame, 0 bytes spill stores, 0 bytes spill loads
ptxas info    : Used 216 registers, 696 bytes cmem[0]
ptxas info    : Compiling entry function '_Z44flash_bwd_dq_dk_dv_loop_seqk_parallel_kernelI23Flash_bwd_kernel_traitsILi160ELi64ELi64ELi8ELi4ELi4ELi4ELb0ELb0EN7cutlass6half_tE19Flash_kernel_traitsILi160ELi64ELi64ELi8ES2_EELb1ELb0ELb1ELb0EEv16Flash_bwd_params' for 'sm_80'
ptxas info    : Function properties for _Z44flash_bwd_dq_dk_dv_loop_seqk_parallel_kernelI23Flash_bwd_kernel_traitsILi160ELi64ELi64ELi8ELi4ELi4ELi4ELb0ELb0EN7cutlass6half_tE19Flash_kernel_traitsILi160ELi64ELi64ELi8ES2_EELb1ELb0ELb1ELb0EEv16Flash_bwd_params
    0 bytes stack frame, 0 bytes spill stores, 0 bytes spill loads
ptxas info    : Used 220 registers, 696 bytes cmem[0]
ptxas info    : Compiling entry function '_Z44flash_bwd_dq_dk_dv_loop_seqk_parallel_kernelI23Flash_bwd_kernel_traitsILi160ELi64ELi64ELi8ELi4ELi4ELi4ELb0ELb0EN7cutlass6half_tE19Flash_kernel_traitsILi160ELi64ELi64ELi8ES2_EELb1ELb0ELb1ELb1EEv16Flash_bwd_params' for 'sm_80'
ptxas info    : Function properties for _Z44flash_bwd_dq_dk_dv_loop_seqk_parallel_kernelI23Flash_bwd_kernel_traitsILi160ELi64ELi64ELi8ELi4ELi4ELi4ELb0ELb0EN7cutlass6half_tE19Flash_kernel_traitsILi160ELi64ELi64ELi8ES2_EELb1ELb0ELb1ELb1EEv16Flash_bwd_params
    0 bytes stack frame, 0 bytes spill stores, 0 bytes spill loads
ptxas info    : Used 222 registers, 696 bytes cmem[0]
ptxas info    : Compiling entry function '_Z44flash_bwd_dq_dk_dv_loop_seqk_parallel_kernelI23Flash_bwd_kernel_traitsILi160ELi64ELi64ELi8ELi4ELi4ELi4ELb0ELb0EN7cutlass6half_tE19Flash_kernel_traitsILi160ELi64ELi64ELi8ES2_EELb1ELb1ELb0ELb0EEv16Flash_bwd_params' for 'sm_80'
ptxas info    : Function properties for _Z44flash_bwd_dq_dk_dv_loop_seqk_parallel_kernelI23Flash_bwd_kernel_traitsILi160ELi64ELi64ELi8ELi4ELi4ELi4ELb0ELb0EN7cutlass6half_tE19Flash_kernel_traitsILi160ELi64ELi64ELi8ES2_EELb1ELb1ELb0ELb0EEv16Flash_bwd_params
    0 bytes stack frame, 0 bytes spill stores, 0 bytes spill loads
ptxas info    : Used 222 registers, 696 bytes cmem[0]
ptxas info    : Compiling entry function '_Z44flash_bwd_dq_dk_dv_loop_seqk_parallel_kernelI23Flash_bwd_kernel_traitsILi160ELi64ELi64ELi8ELi4ELi4ELi4ELb0ELb0EN7cutlass6half_tE19Flash_kernel_traitsILi160ELi64ELi64ELi8ES2_EELb1ELb1ELb0ELb1EEv16Flash_bwd_params' for 'sm_80'
ptxas info    : Function properties for _Z44flash_bwd_dq_dk_dv_loop_seqk_parallel_kernelI23Flash_bwd_kernel_traitsILi160ELi64ELi64ELi8ELi4ELi4ELi4ELb0ELb0EN7cutlass6half_tE19Flash_kernel_traitsILi160ELi64ELi64ELi8ES2_EELb1ELb1ELb0ELb1EEv16Flash_bwd_params
    0 bytes stack frame, 0 bytes spill stores, 0 bytes spill loads
ptxas info    : Used 220 registers, 696 bytes cmem[0]
ptxas info    : Compiling entry function '_Z44flash_bwd_dq_dk_dv_loop_seqk_parallel_kernelI23Flash_bwd_kernel_traitsILi160ELi64ELi64ELi8ELi4ELi4ELi4ELb0ELb0EN7cutlass6half_tE19Flash_kernel_traitsILi160ELi64ELi64ELi8ES2_EELb1ELb1ELb1ELb0EEv16Flash_bwd_params' for 'sm_80'
ptxas info    : Function properties for _Z44flash_bwd_dq_dk_dv_loop_seqk_parallel_kernelI23Flash_bwd_kernel_traitsILi160ELi64ELi64ELi8ELi4ELi4ELi4ELb0ELb0EN7cutlass6half_tE19Flash_kernel_traitsILi160ELi64ELi64ELi8ES2_EELb1ELb1ELb1ELb0EEv16Flash_bwd_params
    0 bytes stack frame, 0 bytes spill stores, 0 bytes spill loads
ptxas info    : Used 212 registers, 696 bytes cmem[0]
ptxas info    : Compiling entry function '_Z44flash_bwd_dq_dk_dv_loop_seqk_parallel_kernelI23Flash_bwd_kernel_traitsILi160ELi64ELi64ELi8ELi4ELi4ELi4ELb0ELb0EN7cutlass6half_tE19Flash_kernel_traitsILi160ELi64ELi64ELi8ES2_EELb1ELb1ELb1ELb1EEv16Flash_bwd_params' for 'sm_80'
ptxas info    : Function properties for _Z44flash_bwd_dq_dk_dv_loop_seqk_parallel_kernelI23Flash_bwd_kernel_traitsILi160ELi64ELi64ELi8ELi4ELi4ELi4ELb0ELb0EN7cutlass6half_tE19Flash_kernel_traitsILi160ELi64ELi64ELi8ES2_EELb1ELb1ELb1ELb1EEv16Flash_bwd_params
    0 bytes stack frame, 0 bytes spill stores, 0 bytes spill loads
ptxas info    : Used 212 registers, 696 bytes cmem[0]
ptxas info    : Compiling entry function '_Z25flash_bwd_dot_do_o_kernelILb1E23Flash_bwd_kernel_traitsILi160ELi64ELi64ELi8ELi4ELi4ELi4ELb0ELb0EN7cutlass6half_tE19Flash_kernel_traitsILi160ELi64ELi64ELi8ES2_EEEv16Flash_bwd_params' for 'sm_80'
ptxas info    : Function properties for _Z25flash_bwd_dot_do_o_kernelILb1E23Flash_bwd_kernel_traitsILi160ELi64ELi64ELi8ELi4ELi4ELi4ELb0ELb0EN7cutlass6half_tE19Flash_kernel_traitsILi160ELi64ELi64ELi8ES2_EEEv16Flash_bwd_params
    0 bytes stack frame, 0 bytes spill stores, 0 bytes spill loads
ptxas info    : Used 64 registers, 696 bytes cmem[0]
/anaconda/envs/flashattention/lib/python3.9/site-packages/torch/include/c10/core/SymInt.h(84): warning: integer conversion resulted in a change of sign

[9/36] /usr/local/cuda-11.4/bin/nvcc  -I/home/v-feiychen/flash-attention/csrc/flash_attn -I/home/v-feiychen/flash-attention/csrc/flash_attn/src -I/home/v-feiychen/flash-attention/csrc/cutlass/include -I/anaconda/envs/flashattention/lib/python3.9/site-packages/torch/include -I/anaconda/envs/flashattention/lib/python3.9/site-packages/torch/include/torch/csrc/api/include -I/anaconda/envs/flashattention/lib/python3.9/site-packages/torch/include/TH -I/anaconda/envs/flashattention/lib/python3.9/site-packages/torch/include/THC -I/usr/local/cuda-11.4/include -I/anaconda/envs/flashattention/include/python3.9 -c -c /home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_bwd_hdim32_bf16_sm80.cu -o /home/v-feiychen/flash-attention/build/temp.linux-x86_64-cpython-39/csrc/flash_attn/src/flash_bwd_hdim32_bf16_sm80.o -D__CUDA_NO_HALF_OPERATORS__ -D__CUDA_NO_HALF_CONVERSIONS__ -D__CUDA_NO_BFLOAT16_CONVERSIONS__ -D__CUDA_NO_HALF2_OPERATORS__ --expt-relaxed-constexpr --compiler-options ''"'"'-fPIC'"'"'' -O3 -std=c++17 -U__CUDA_NO_HALF_OPERATORS__ -U__CUDA_NO_HALF_CONVERSIONS__ -U__CUDA_NO_HALF2_OPERATORS__ -U__CUDA_NO_BFLOAT16_CONVERSIONS__ --expt-relaxed-constexpr --expt-extended-lambda --use_fast_math --ptxas-options=-v -lineinfo -gencode arch=compute_80,code=sm_80 --threads 4 -DTORCH_API_INCLUDE_EXTENSION_H '-DPYBIND11_COMPILER_TYPE="_gcc"' '-DPYBIND11_STDLIB="_libstdcpp"' '-DPYBIND11_BUILD_ABI="_cxxabi1011"' -DTORCH_EXTENSION_NAME=flash_attn_2_cuda -D_GLIBCXX_USE_CXX11_ABI=0
/anaconda/envs/flashattention/lib/python3.9/site-packages/torch/include/c10/core/SymInt.h(84): warning: integer conversion resulted in a change of sign

ptxas info    : 2 bytes gmem
ptxas info    : Compiling entry function '_Z44flash_bwd_dq_dk_dv_loop_seqk_parallel_kernelI23Flash_bwd_kernel_traitsILi32ELi128ELi128ELi8ELi4ELi4ELi4ELb1ELb0EN7cutlass10bfloat16_tE19Flash_kernel_traitsILi32ELi128ELi128ELi8ES2_EELb0ELb0ELb0ELb0EEv16Flash_bwd_params' for 'sm_80'
ptxas info    : Function properties for _Z44flash_bwd_dq_dk_dv_loop_seqk_parallel_kernelI23Flash_bwd_kernel_traitsILi32ELi128ELi128ELi8ELi4ELi4ELi4ELb1ELb0EN7cutlass10bfloat16_tE19Flash_kernel_traitsILi32ELi128ELi128ELi8ES2_EELb0ELb0ELb0ELb0EEv16Flash_bwd_params
    0 bytes stack frame, 0 bytes spill stores, 0 bytes spill loads
ptxas info    : Used 254 registers, 696 bytes cmem[0]
ptxas info    : Compiling entry function '_Z44flash_bwd_dq_dk_dv_loop_seqk_parallel_kernelI23Flash_bwd_kernel_traitsILi32ELi128ELi128ELi8ELi4ELi4ELi4ELb1ELb0EN7cutlass10bfloat16_tE19Flash_kernel_traitsILi32ELi128ELi128ELi8ES2_EELb0ELb0ELb0ELb1EEv16Flash_bwd_params' for 'sm_80'
ptxas info    : Function properties for _Z44flash_bwd_dq_dk_dv_loop_seqk_parallel_kernelI23Flash_bwd_kernel_traitsILi32ELi128ELi128ELi8ELi4ELi4ELi4ELb1ELb0EN7cutlass10bfloat16_tE19Flash_kernel_traitsILi32ELi128ELi128ELi8ES2_EELb0ELb0ELb0ELb1EEv16Flash_bwd_params
    0 bytes stack frame, 0 bytes spill stores, 0 bytes spill loads
ptxas info    : Used 254 registers, 696 bytes cmem[0]
ptxas info    : Compiling entry function '_Z44flash_bwd_dq_dk_dv_loop_seqk_parallel_kernelI23Flash_bwd_kernel_traitsILi32ELi128ELi128ELi8ELi4ELi4ELi4ELb1ELb0EN7cutlass10bfloat16_tE19Flash_kernel_traitsILi32ELi128ELi128ELi8ES2_EELb0ELb0ELb1ELb0EEv16Flash_bwd_params' for 'sm_80'
ptxas info    : Function properties for _Z44flash_bwd_dq_dk_dv_loop_seqk_parallel_kernelI23Flash_bwd_kernel_traitsILi32ELi128ELi128ELi8ELi4ELi4ELi4ELb1ELb0EN7cutlass10bfloat16_tE19Flash_kernel_traitsILi32ELi128ELi128ELi8ES2_EELb0ELb0ELb1ELb0EEv16Flash_bwd_params
    0 bytes stack frame, 0 bytes spill stores, 0 bytes spill loads
ptxas info    : Used 251 registers, 696 bytes cmem[0]
ptxas info    : Compiling entry function '_Z44flash_bwd_dq_dk_dv_loop_seqk_parallel_kernelI23Flash_bwd_kernel_traitsILi32ELi128ELi128ELi8ELi4ELi4ELi4ELb1ELb0EN7cutlass10bfloat16_tE19Flash_kernel_traitsILi32ELi128ELi128ELi8ES2_EELb0ELb0ELb1ELb1EEv16Flash_bwd_params' for 'sm_80'
ptxas info    : Function properties for _Z44flash_bwd_dq_dk_dv_loop_seqk_parallel_kernelI23Flash_bwd_kernel_traitsILi32ELi128ELi128ELi8ELi4ELi4ELi4ELb1ELb0EN7cutlass10bfloat16_tE19Flash_kernel_traitsILi32ELi128ELi128ELi8ES2_EELb0ELb0ELb1ELb1EEv16Flash_bwd_params
    0 bytes stack frame, 0 bytes spill stores, 0 bytes spill loads
ptxas info    : Used 251 registers, 696 bytes cmem[0]
ptxas info    : Compiling entry function '_Z44flash_bwd_dq_dk_dv_loop_seqk_parallel_kernelI23Flash_bwd_kernel_traitsILi32ELi128ELi128ELi8ELi4ELi4ELi4ELb1ELb0EN7cutlass10bfloat16_tE19Flash_kernel_traitsILi32ELi128ELi128ELi8ES2_EELb0ELb1ELb0ELb0EEv16Flash_bwd_params' for 'sm_80'
ptxas info    : Function properties for _Z44flash_bwd_dq_dk_dv_loop_seqk_parallel_kernelI23Flash_bwd_kernel_traitsILi32ELi128ELi128ELi8ELi4ELi4ELi4ELb1ELb0EN7cutlass10bfloat16_tE19Flash_kernel_traitsILi32ELi128ELi128ELi8ES2_EELb0ELb1ELb0ELb0EEv16Flash_bwd_params
    0 bytes stack frame, 0 bytes spill stores, 0 bytes spill loads
ptxas info    : Used 253 registers, 696 bytes cmem[0]
ptxas info    : Compiling entry function '_Z44flash_bwd_dq_dk_dv_loop_seqk_parallel_kernelI23Flash_bwd_kernel_traitsILi32ELi128ELi128ELi8ELi4ELi4ELi4ELb1ELb0EN7cutlass10bfloat16_tE19Flash_kernel_traitsILi32ELi128ELi128ELi8ES2_EELb0ELb1ELb0ELb1EEv16Flash_bwd_params' for 'sm_80'
ptxas info    : Function properties for _Z44flash_bwd_dq_dk_dv_loop_seqk_parallel_kernelI23Flash_bwd_kernel_traitsILi32ELi128ELi128ELi8ELi4ELi4ELi4ELb1ELb0EN7cutlass10bfloat16_tE19Flash_kernel_traitsILi32ELi128ELi128ELi8ES2_EELb0ELb1ELb0ELb1EEv16Flash_bwd_params
    0 bytes stack frame, 0 bytes spill stores, 0 bytes spill loads
ptxas info    : Used 252 registers, 696 bytes cmem[0]
ptxas info    : Compiling entry function '_Z44flash_bwd_dq_dk_dv_loop_seqk_parallel_kernelI23Flash_bwd_kernel_traitsILi32ELi128ELi128ELi8ELi4ELi4ELi4ELb1ELb0EN7cutlass10bfloat16_tE19Flash_kernel_traitsILi32ELi128ELi128ELi8ES2_EELb0ELb1ELb1ELb0EEv16Flash_bwd_params' for 'sm_80'
ptxas info    : Function properties for _Z44flash_bwd_dq_dk_dv_loop_seqk_parallel_kernelI23Flash_bwd_kernel_traitsILi32ELi128ELi128ELi8ELi4ELi4ELi4ELb1ELb0EN7cutlass10bfloat16_tE19Flash_kernel_traitsILi32ELi128ELi128ELi8ES2_EELb0ELb1ELb1ELb0EEv16Flash_bwd_params
    0 bytes stack frame, 0 bytes spill stores, 0 bytes spill loads
ptxas info    : Used 246 registers, 696 bytes cmem[0]
ptxas info    : Compiling entry function '_Z44flash_bwd_dq_dk_dv_loop_seqk_parallel_kernelI23Flash_bwd_kernel_traitsILi32ELi128ELi128ELi8ELi4ELi4ELi4ELb1ELb0EN7cutlass10bfloat16_tE19Flash_kernel_traitsILi32ELi128ELi128ELi8ES2_EELb0ELb1ELb1ELb1EEv16Flash_bwd_params' for 'sm_80'
ptxas info    : Function properties for _Z44flash_bwd_dq_dk_dv_loop_seqk_parallel_kernelI23Flash_bwd_kernel_traitsILi32ELi128ELi128ELi8ELi4ELi4ELi4ELb1ELb0EN7cutlass10bfloat16_tE19Flash_kernel_traitsILi32ELi128ELi128ELi8ES2_EELb0ELb1ELb1ELb1EEv16Flash_bwd_params
    0 bytes stack frame, 0 bytes spill stores, 0 bytes spill loads
ptxas info    : Used 249 registers, 696 bytes cmem[0]
ptxas info    : Compiling entry function '_Z27flash_bwd_convert_dq_kernelI23Flash_bwd_kernel_traitsILi32ELi128ELi128ELi8ELi4ELi4ELi4ELb1ELb0EN7cutlass10bfloat16_tE19Flash_kernel_traitsILi32ELi128ELi128ELi8ES2_EEEv16Flash_bwd_params' for 'sm_80'
ptxas info    : Function properties for _Z27flash_bwd_convert_dq_kernelI23Flash_bwd_kernel_traitsILi32ELi128ELi128ELi8ELi4ELi4ELi4ELb1ELb0EN7cutlass10bfloat16_tE19Flash_kernel_traitsILi32ELi128ELi128ELi8ES2_EEEv16Flash_bwd_params
    0 bytes stack frame, 0 bytes spill stores, 0 bytes spill loads
ptxas info    : Used 32 registers, 696 bytes cmem[0]
ptxas info    : Compiling entry function '_Z44flash_bwd_dq_dk_dv_loop_seqk_parallel_kernelI23Flash_bwd_kernel_traitsILi32ELi128ELi128ELi8ELi4ELi4ELi4ELb1ELb0EN7cutlass10bfloat16_tE19Flash_kernel_traitsILi32ELi128ELi128ELi8ES2_EELb1ELb0ELb0ELb0EEv16Flash_bwd_params' for 'sm_80'
ptxas info    : Function properties for _Z44flash_bwd_dq_dk_dv_loop_seqk_parallel_kernelI23Flash_bwd_kernel_traitsILi32ELi128ELi128ELi8ELi4ELi4ELi4ELb1ELb0EN7cutlass10bfloat16_tE19Flash_kernel_traitsILi32ELi128ELi128ELi8ES2_EELb1ELb0ELb0ELb0EEv16Flash_bwd_params
    0 bytes stack frame, 0 bytes spill stores, 0 bytes spill loads
ptxas info    : Used 253 registers, 696 bytes cmem[0]
ptxas info    : Compiling entry function '_Z44flash_bwd_dq_dk_dv_loop_seqk_parallel_kernelI23Flash_bwd_kernel_traitsILi32ELi128ELi128ELi8ELi4ELi4ELi4ELb1ELb0EN7cutlass10bfloat16_tE19Flash_kernel_traitsILi32ELi128ELi128ELi8ES2_EELb1ELb0ELb0ELb1EEv16Flash_bwd_params' for 'sm_80'
ptxas info    : Function properties for _Z44flash_bwd_dq_dk_dv_loop_seqk_parallel_kernelI23Flash_bwd_kernel_traitsILi32ELi128ELi128ELi8ELi4ELi4ELi4ELb1ELb0EN7cutlass10bfloat16_tE19Flash_kernel_traitsILi32ELi128ELi128ELi8ES2_EELb1ELb0ELb0ELb1EEv16Flash_bwd_params
    0 bytes stack frame, 0 bytes spill stores, 0 bytes spill loads
ptxas info    : Used 246 registers, 696 bytes cmem[0]
ptxas info    : Compiling entry function '_Z44flash_bwd_dq_dk_dv_loop_seqk_parallel_kernelI23Flash_bwd_kernel_traitsILi32ELi128ELi128ELi8ELi4ELi4ELi4ELb1ELb0EN7cutlass10bfloat16_tE19Flash_kernel_traitsILi32ELi128ELi128ELi8ES2_EELb1ELb0ELb1ELb0EEv16Flash_bwd_params' for 'sm_80'
ptxas info    : Function properties for _Z44flash_bwd_dq_dk_dv_loop_seqk_parallel_kernelI23Flash_bwd_kernel_traitsILi32ELi128ELi128ELi8ELi4ELi4ELi4ELb1ELb0EN7cutlass10bfloat16_tE19Flash_kernel_traitsILi32ELi128ELi128ELi8ES2_EELb1ELb0ELb1ELb0EEv16Flash_bwd_params
    0 bytes stack frame, 0 bytes spill stores, 0 bytes spill loads
ptxas info    : Used 245 registers, 696 bytes cmem[0]
ptxas info    : Compiling entry function '_Z44flash_bwd_dq_dk_dv_loop_seqk_parallel_kernelI23Flash_bwd_kernel_traitsILi32ELi128ELi128ELi8ELi4ELi4ELi4ELb1ELb0EN7cutlass10bfloat16_tE19Flash_kernel_traitsILi32ELi128ELi128ELi8ES2_EELb1ELb0ELb1ELb1EEv16Flash_bwd_params' for 'sm_80'
ptxas info    : Function properties for _Z44flash_bwd_dq_dk_dv_loop_seqk_parallel_kernelI23Flash_bwd_kernel_traitsILi32ELi128ELi128ELi8ELi4ELi4ELi4ELb1ELb0EN7cutlass10bfloat16_tE19Flash_kernel_traitsILi32ELi128ELi128ELi8ES2_EELb1ELb0ELb1ELb1EEv16Flash_bwd_params
    0 bytes stack frame, 0 bytes spill stores, 0 bytes spill loads
ptxas info    : Used 248 registers, 696 bytes cmem[0]
ptxas info    : Compiling entry function '_Z44flash_bwd_dq_dk_dv_loop_seqk_parallel_kernelI23Flash_bwd_kernel_traitsILi32ELi128ELi128ELi8ELi4ELi4ELi4ELb1ELb0EN7cutlass10bfloat16_tE19Flash_kernel_traitsILi32ELi128ELi128ELi8ES2_EELb1ELb1ELb0ELb0EEv16Flash_bwd_params' for 'sm_80'
ptxas info    : Function properties for _Z44flash_bwd_dq_dk_dv_loop_seqk_parallel_kernelI23Flash_bwd_kernel_traitsILi32ELi128ELi128ELi8ELi4ELi4ELi4ELb1ELb0EN7cutlass10bfloat16_tE19Flash_kernel_traitsILi32ELi128ELi128ELi8ES2_EELb1ELb1ELb0ELb0EEv16Flash_bwd_params
    0 bytes stack frame, 0 bytes spill stores, 0 bytes spill loads
ptxas info    : Used 251 registers, 696 bytes cmem[0]
ptxas info    : Compiling entry function '_Z44flash_bwd_dq_dk_dv_loop_seqk_parallel_kernelI23Flash_bwd_kernel_traitsILi32ELi128ELi128ELi8ELi4ELi4ELi4ELb1ELb0EN7cutlass10bfloat16_tE19Flash_kernel_traitsILi32ELi128ELi128ELi8ES2_EELb1ELb1ELb0ELb1EEv16Flash_bwd_params' for 'sm_80'
ptxas info    : Function properties for _Z44flash_bwd_dq_dk_dv_loop_seqk_parallel_kernelI23Flash_bwd_kernel_traitsILi32ELi128ELi128ELi8ELi4ELi4ELi4ELb1ELb0EN7cutlass10bfloat16_tE19Flash_kernel_traitsILi32ELi128ELi128ELi8ES2_EELb1ELb1ELb0ELb1EEv16Flash_bwd_params
    0 bytes stack frame, 0 bytes spill stores, 0 bytes spill loads
ptxas info    : Used 251 registers, 696 bytes cmem[0]
ptxas info    : Compiling entry function '_Z44flash_bwd_dq_dk_dv_loop_seqk_parallel_kernelI23Flash_bwd_kernel_traitsILi32ELi128ELi128ELi8ELi4ELi4ELi4ELb1ELb0EN7cutlass10bfloat16_tE19Flash_kernel_traitsILi32ELi128ELi128ELi8ES2_EELb1ELb1ELb1ELb0EEv16Flash_bwd_params' for 'sm_80'
ptxas info    : Function properties for _Z44flash_bwd_dq_dk_dv_loop_seqk_parallel_kernelI23Flash_bwd_kernel_traitsILi32ELi128ELi128ELi8ELi4ELi4ELi4ELb1ELb0EN7cutlass10bfloat16_tE19Flash_kernel_traitsILi32ELi128ELi128ELi8ES2_EELb1ELb1ELb1ELb0EEv16Flash_bwd_params
    0 bytes stack frame, 0 bytes spill stores, 0 bytes spill loads
ptxas info    : Used 255 registers, 696 bytes cmem[0]
ptxas info    : Compiling entry function '_Z44flash_bwd_dq_dk_dv_loop_seqk_parallel_kernelI23Flash_bwd_kernel_traitsILi32ELi128ELi128ELi8ELi4ELi4ELi4ELb1ELb0EN7cutlass10bfloat16_tE19Flash_kernel_traitsILi32ELi128ELi128ELi8ES2_EELb1ELb1ELb1ELb1EEv16Flash_bwd_params' for 'sm_80'
ptxas info    : Function properties for _Z44flash_bwd_dq_dk_dv_loop_seqk_parallel_kernelI23Flash_bwd_kernel_traitsILi32ELi128ELi128ELi8ELi4ELi4ELi4ELb1ELb0EN7cutlass10bfloat16_tE19Flash_kernel_traitsILi32ELi128ELi128ELi8ES2_EELb1ELb1ELb1ELb1EEv16Flash_bwd_params
    0 bytes stack frame, 0 bytes spill stores, 0 bytes spill loads
ptxas info    : Used 254 registers, 696 bytes cmem[0]
ptxas info    : Compiling entry function '_Z25flash_bwd_dot_do_o_kernelILb1E23Flash_bwd_kernel_traitsILi32ELi128ELi128ELi8ELi4ELi4ELi4ELb1ELb0EN7cutlass10bfloat16_tE19Flash_kernel_traitsILi32ELi128ELi128ELi8ES2_EEEv16Flash_bwd_params' for 'sm_80'
ptxas info    : Function properties for _Z25flash_bwd_dot_do_o_kernelILb1E23Flash_bwd_kernel_traitsILi32ELi128ELi128ELi8ELi4ELi4ELi4ELb1ELb0EN7cutlass10bfloat16_tE19Flash_kernel_traitsILi32ELi128ELi128ELi8ES2_EEEv16Flash_bwd_params
    0 bytes stack frame, 0 bytes spill stores, 0 bytes spill loads
ptxas info    : Used 30 registers, 696 bytes cmem[0]
ptxas info    : Compiling entry function '_Z27flash_bwd_convert_dq_kernelI23Flash_bwd_kernel_traitsILi32ELi128ELi128ELi8ELi4ELi4ELi4ELb0ELb0EN7cutlass10bfloat16_tE19Flash_kernel_traitsILi32ELi128ELi128ELi8ES2_EEEv16Flash_bwd_params' for 'sm_80'
ptxas info    : Function properties for _Z27flash_bwd_convert_dq_kernelI23Flash_bwd_kernel_traitsILi32ELi128ELi128ELi8ELi4ELi4ELi4ELb0ELb0EN7cutlass10bfloat16_tE19Flash_kernel_traitsILi32ELi128ELi128ELi8ES2_EEEv16Flash_bwd_params
    0 bytes stack frame, 0 bytes spill stores, 0 bytes spill loads
ptxas info    : Used 32 registers, 696 bytes cmem[0]
ptxas info    : Compiling entry function '_Z44flash_bwd_dq_dk_dv_loop_seqk_parallel_kernelI23Flash_bwd_kernel_traitsILi32ELi128ELi128ELi8ELi4ELi4ELi4ELb0ELb0EN7cutlass10bfloat16_tE19Flash_kernel_traitsILi32ELi128ELi128ELi8ES2_EELb1ELb0ELb0ELb0EEv16Flash_bwd_params' for 'sm_80'
ptxas info    : Function properties for _Z44flash_bwd_dq_dk_dv_loop_seqk_parallel_kernelI23Flash_bwd_kernel_traitsILi32ELi128ELi128ELi8ELi4ELi4ELi4ELb0ELb0EN7cutlass10bfloat16_tE19Flash_kernel_traitsILi32ELi128ELi128ELi8ES2_EELb1ELb0ELb0ELb0EEv16Flash_bwd_params
    0 bytes stack frame, 0 bytes spill stores, 0 bytes spill loads
ptxas info    : Used 240 registers, 696 bytes cmem[0]
ptxas info    : Compiling entry function '_Z44flash_bwd_dq_dk_dv_loop_seqk_parallel_kernelI23Flash_bwd_kernel_traitsILi32ELi128ELi128ELi8ELi4ELi4ELi4ELb0ELb0EN7cutlass10bfloat16_tE19Flash_kernel_traitsILi32ELi128ELi128ELi8ES2_EELb1ELb0ELb0ELb1EEv16Flash_bwd_params' for 'sm_80'
ptxas info    : Function properties for _Z44flash_bwd_dq_dk_dv_loop_seqk_parallel_kernelI23Flash_bwd_kernel_traitsILi32ELi128ELi128ELi8ELi4ELi4ELi4ELb0ELb0EN7cutlass10bfloat16_tE19Flash_kernel_traitsILi32ELi128ELi128ELi8ES2_EELb1ELb0ELb0ELb1EEv16Flash_bwd_params
    0 bytes stack frame, 0 bytes spill stores, 0 bytes spill loads
ptxas info    : Used 243 registers, 696 bytes cmem[0]
ptxas info    : Compiling entry function '_Z44flash_bwd_dq_dk_dv_loop_seqk_parallel_kernelI23Flash_bwd_kernel_traitsILi32ELi128ELi128ELi8ELi4ELi4ELi4ELb0ELb0EN7cutlass10bfloat16_tE19Flash_kernel_traitsILi32ELi128ELi128ELi8ES2_EELb1ELb0ELb1ELb0EEv16Flash_bwd_params' for 'sm_80'
ptxas info    : Function properties for _Z44flash_bwd_dq_dk_dv_loop_seqk_parallel_kernelI23Flash_bwd_kernel_traitsILi32ELi128ELi128ELi8ELi4ELi4ELi4ELb0ELb0EN7cutlass10bfloat16_tE19Flash_kernel_traitsILi32ELi128ELi128ELi8ES2_EELb1ELb0ELb1ELb0EEv16Flash_bwd_params
    0 bytes stack frame, 0 bytes spill stores, 0 bytes spill loads
ptxas info    : Used 242 registers, 696 bytes cmem[0]
ptxas info    : Compiling entry function '_Z44flash_bwd_dq_dk_dv_loop_seqk_parallel_kernelI23Flash_bwd_kernel_traitsILi32ELi128ELi128ELi8ELi4ELi4ELi4ELb0ELb0EN7cutlass10bfloat16_tE19Flash_kernel_traitsILi32ELi128ELi128ELi8ES2_EELb1ELb0ELb1ELb1EEv16Flash_bwd_params' for 'sm_80'
ptxas info    : Function properties for _Z44flash_bwd_dq_dk_dv_loop_seqk_parallel_kernelI23Flash_bwd_kernel_traitsILi32ELi128ELi128ELi8ELi4ELi4ELi4ELb0ELb0EN7cutlass10bfloat16_tE19Flash_kernel_traitsILi32ELi128ELi128ELi8ES2_EELb1ELb0ELb1ELb1EEv16Flash_bwd_params
    0 bytes stack frame, 0 bytes spill stores, 0 bytes spill loads
ptxas info    : Used 244 registers, 696 bytes cmem[0]
ptxas info    : Compiling entry function '_Z44flash_bwd_dq_dk_dv_loop_seqk_parallel_kernelI23Flash_bwd_kernel_traitsILi32ELi128ELi128ELi8ELi4ELi4ELi4ELb0ELb0EN7cutlass10bfloat16_tE19Flash_kernel_traitsILi32ELi128ELi128ELi8ES2_EELb1ELb1ELb0ELb0EEv16Flash_bwd_params' for 'sm_80'
ptxas info    : Function properties for _Z44flash_bwd_dq_dk_dv_loop_seqk_parallel_kernelI23Flash_bwd_kernel_traitsILi32ELi128ELi128ELi8ELi4ELi4ELi4ELb0ELb0EN7cutlass10bfloat16_tE19Flash_kernel_traitsILi32ELi128ELi128ELi8ES2_EELb1ELb1ELb0ELb0EEv16Flash_bwd_params
    0 bytes stack frame, 0 bytes spill stores, 0 bytes spill loads
ptxas info    : Used 251 registers, 696 bytes cmem[0]
ptxas info    : Compiling entry function '_Z44flash_bwd_dq_dk_dv_loop_seqk_parallel_kernelI23Flash_bwd_kernel_traitsILi32ELi128ELi128ELi8ELi4ELi4ELi4ELb0ELb0EN7cutlass10bfloat16_tE19Flash_kernel_traitsILi32ELi128ELi128ELi8ES2_EELb1ELb1ELb0ELb1EEv16Flash_bwd_params' for 'sm_80'
ptxas info    : Function properties for _Z44flash_bwd_dq_dk_dv_loop_seqk_parallel_kernelI23Flash_bwd_kernel_traitsILi32ELi128ELi128ELi8ELi4ELi4ELi4ELb0ELb0EN7cutlass10bfloat16_tE19Flash_kernel_traitsILi32ELi128ELi128ELi8ES2_EELb1ELb1ELb0ELb1EEv16Flash_bwd_params
    0 bytes stack frame, 0 bytes spill stores, 0 bytes spill loads
ptxas info    : Used 250 registers, 696 bytes cmem[0]
ptxas info    : Compiling entry function '_Z44flash_bwd_dq_dk_dv_loop_seqk_parallel_kernelI23Flash_bwd_kernel_traitsILi32ELi128ELi128ELi8ELi4ELi4ELi4ELb0ELb0EN7cutlass10bfloat16_tE19Flash_kernel_traitsILi32ELi128ELi128ELi8ES2_EELb1ELb1ELb1ELb0EEv16Flash_bwd_params' for 'sm_80'
ptxas info    : Function properties for _Z44flash_bwd_dq_dk_dv_loop_seqk_parallel_kernelI23Flash_bwd_kernel_traitsILi32ELi128ELi128ELi8ELi4ELi4ELi4ELb0ELb0EN7cutlass10bfloat16_tE19Flash_kernel_traitsILi32ELi128ELi128ELi8ES2_EELb1ELb1ELb1ELb0EEv16Flash_bwd_params
    0 bytes stack frame, 0 bytes spill stores, 0 bytes spill loads
ptxas info    : Used 244 registers, 696 bytes cmem[0]
ptxas info    : Compiling entry function '_Z44flash_bwd_dq_dk_dv_loop_seqk_parallel_kernelI23Flash_bwd_kernel_traitsILi32ELi128ELi128ELi8ELi4ELi4ELi4ELb0ELb0EN7cutlass10bfloat16_tE19Flash_kernel_traitsILi32ELi128ELi128ELi8ES2_EELb1ELb1ELb1ELb1EEv16Flash_bwd_params' for 'sm_80'
ptxas info    : Function properties for _Z44flash_bwd_dq_dk_dv_loop_seqk_parallel_kernelI23Flash_bwd_kernel_traitsILi32ELi128ELi128ELi8ELi4ELi4ELi4ELb0ELb0EN7cutlass10bfloat16_tE19Flash_kernel_traitsILi32ELi128ELi128ELi8ES2_EELb1ELb1ELb1ELb1EEv16Flash_bwd_params
    0 bytes stack frame, 0 bytes spill stores, 0 bytes spill loads
ptxas info    : Used 247 registers, 696 bytes cmem[0]
ptxas info    : Compiling entry function '_Z25flash_bwd_dot_do_o_kernelILb1E23Flash_bwd_kernel_traitsILi32ELi128ELi128ELi8ELi4ELi4ELi4ELb0ELb0EN7cutlass10bfloat16_tE19Flash_kernel_traitsILi32ELi128ELi128ELi8ES2_EEEv16Flash_bwd_params' for 'sm_80'
ptxas info    : Function properties for _Z25flash_bwd_dot_do_o_kernelILb1E23Flash_bwd_kernel_traitsILi32ELi128ELi128ELi8ELi4ELi4ELi4ELb0ELb0EN7cutlass10bfloat16_tE19Flash_kernel_traitsILi32ELi128ELi128ELi8ES2_EEEv16Flash_bwd_params
    0 bytes stack frame, 0 bytes spill stores, 0 bytes spill loads
ptxas info    : Used 30 registers, 696 bytes cmem[0]
/anaconda/envs/flashattention/lib/python3.9/site-packages/torch/include/c10/core/SymInt.h(84): warning: integer conversion resulted in a change of sign

[10/36] /usr/local/cuda-11.4/bin/nvcc  -I/home/v-feiychen/flash-attention/csrc/flash_attn -I/home/v-feiychen/flash-attention/csrc/flash_attn/src -I/home/v-feiychen/flash-attention/csrc/cutlass/include -I/anaconda/envs/flashattention/lib/python3.9/site-packages/torch/include -I/anaconda/envs/flashattention/lib/python3.9/site-packages/torch/include/torch/csrc/api/include -I/anaconda/envs/flashattention/lib/python3.9/site-packages/torch/include/TH -I/anaconda/envs/flashattention/lib/python3.9/site-packages/torch/include/THC -I/usr/local/cuda-11.4/include -I/anaconda/envs/flashattention/include/python3.9 -c -c /home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_bwd_hdim160_bf16_sm80.cu -o /home/v-feiychen/flash-attention/build/temp.linux-x86_64-cpython-39/csrc/flash_attn/src/flash_bwd_hdim160_bf16_sm80.o -D__CUDA_NO_HALF_OPERATORS__ -D__CUDA_NO_HALF_CONVERSIONS__ -D__CUDA_NO_BFLOAT16_CONVERSIONS__ -D__CUDA_NO_HALF2_OPERATORS__ --expt-relaxed-constexpr --compiler-options ''"'"'-fPIC'"'"'' -O3 -std=c++17 -U__CUDA_NO_HALF_OPERATORS__ -U__CUDA_NO_HALF_CONVERSIONS__ -U__CUDA_NO_HALF2_OPERATORS__ -U__CUDA_NO_BFLOAT16_CONVERSIONS__ --expt-relaxed-constexpr --expt-extended-lambda --use_fast_math --ptxas-options=-v -lineinfo -gencode arch=compute_80,code=sm_80 --threads 4 -DTORCH_API_INCLUDE_EXTENSION_H '-DPYBIND11_COMPILER_TYPE="_gcc"' '-DPYBIND11_STDLIB="_libstdcpp"' '-DPYBIND11_BUILD_ABI="_cxxabi1011"' -DTORCH_EXTENSION_NAME=flash_attn_2_cuda -D_GLIBCXX_USE_CXX11_ABI=0
/anaconda/envs/flashattention/lib/python3.9/site-packages/torch/include/c10/core/SymInt.h(84): warning: integer conversion resulted in a change of sign

ptxas info    : 2 bytes gmem
ptxas info    : Compiling entry function '_Z44flash_bwd_dq_dk_dv_loop_seqk_parallel_kernelI23Flash_bwd_kernel_traitsILi160ELi64ELi64ELi8ELi4ELi4ELi4ELb0ELb1EN7cutlass10bfloat16_tE19Flash_kernel_traitsILi160ELi64ELi64ELi8ES2_EELb0ELb0ELb0ELb0EEv16Flash_bwd_params' for 'sm_80'
ptxas info    : Function properties for _Z44flash_bwd_dq_dk_dv_loop_seqk_parallel_kernelI23Flash_bwd_kernel_traitsILi160ELi64ELi64ELi8ELi4ELi4ELi4ELb0ELb1EN7cutlass10bfloat16_tE19Flash_kernel_traitsILi160ELi64ELi64ELi8ES2_EELb0ELb0ELb0ELb0EEv16Flash_bwd_params
    0 bytes stack frame, 0 bytes spill stores, 0 bytes spill loads
ptxas info    : Used 204 registers, 696 bytes cmem[0]
ptxas info    : Compiling entry function '_Z44flash_bwd_dq_dk_dv_loop_seqk_parallel_kernelI23Flash_bwd_kernel_traitsILi160ELi64ELi64ELi8ELi4ELi4ELi4ELb0ELb1EN7cutlass10bfloat16_tE19Flash_kernel_traitsILi160ELi64ELi64ELi8ES2_EELb0ELb0ELb0ELb1EEv16Flash_bwd_params' for 'sm_80'
ptxas info    : Function properties for _Z44flash_bwd_dq_dk_dv_loop_seqk_parallel_kernelI23Flash_bwd_kernel_traitsILi160ELi64ELi64ELi8ELi4ELi4ELi4ELb0ELb1EN7cutlass10bfloat16_tE19Flash_kernel_traitsILi160ELi64ELi64ELi8ES2_EELb0ELb0ELb0ELb1EEv16Flash_bwd_params
    0 bytes stack frame, 0 bytes spill stores, 0 bytes spill loads
ptxas info    : Used 204 registers, 696 bytes cmem[0]
ptxas info    : Compiling entry function '_Z44flash_bwd_dq_dk_dv_loop_seqk_parallel_kernelI23Flash_bwd_kernel_traitsILi160ELi64ELi64ELi8ELi4ELi4ELi4ELb0ELb1EN7cutlass10bfloat16_tE19Flash_kernel_traitsILi160ELi64ELi64ELi8ES2_EELb0ELb0ELb1ELb0EEv16Flash_bwd_params' for 'sm_80'
ptxas info    : Function properties for _Z44flash_bwd_dq_dk_dv_loop_seqk_parallel_kernelI23Flash_bwd_kernel_traitsILi160ELi64ELi64ELi8ELi4ELi4ELi4ELb0ELb1EN7cutlass10bfloat16_tE19Flash_kernel_traitsILi160ELi64ELi64ELi8ES2_EELb0ELb0ELb1ELb0EEv16Flash_bwd_params
    0 bytes stack frame, 0 bytes spill stores, 0 bytes spill loads
ptxas info    : Used 190 registers, 696 bytes cmem[0]
ptxas info    : Compiling entry function '_Z44flash_bwd_dq_dk_dv_loop_seqk_parallel_kernelI23Flash_bwd_kernel_traitsILi160ELi64ELi64ELi8ELi4ELi4ELi4ELb0ELb1EN7cutlass10bfloat16_tE19Flash_kernel_traitsILi160ELi64ELi64ELi8ES2_EELb0ELb0ELb1ELb1EEv16Flash_bwd_params' for 'sm_80'
ptxas info    : Function properties for _Z44flash_bwd_dq_dk_dv_loop_seqk_parallel_kernelI23Flash_bwd_kernel_traitsILi160ELi64ELi64ELi8ELi4ELi4ELi4ELb0ELb1EN7cutlass10bfloat16_tE19Flash_kernel_traitsILi160ELi64ELi64ELi8ES2_EELb0ELb0ELb1ELb1EEv16Flash_bwd_params
    0 bytes stack frame, 0 bytes spill stores, 0 bytes spill loads
ptxas info    : Used 196 registers, 696 bytes cmem[0]
ptxas info    : Compiling entry function '_Z44flash_bwd_dq_dk_dv_loop_seqk_parallel_kernelI23Flash_bwd_kernel_traitsILi160ELi64ELi64ELi8ELi4ELi4ELi4ELb0ELb1EN7cutlass10bfloat16_tE19Flash_kernel_traitsILi160ELi64ELi64ELi8ES2_EELb0ELb1ELb0ELb0EEv16Flash_bwd_params' for 'sm_80'
ptxas info    : Function properties for _Z44flash_bwd_dq_dk_dv_loop_seqk_parallel_kernelI23Flash_bwd_kernel_traitsILi160ELi64ELi64ELi8ELi4ELi4ELi4ELb0ELb1EN7cutlass10bfloat16_tE19Flash_kernel_traitsILi160ELi64ELi64ELi8ES2_EELb0ELb1ELb0ELb0EEv16Flash_bwd_params
    0 bytes stack frame, 0 bytes spill stores, 0 bytes spill loads
ptxas info    : Used 210 registers, 696 bytes cmem[0]
ptxas info    : Compiling entry function '_Z44flash_bwd_dq_dk_dv_loop_seqk_parallel_kernelI23Flash_bwd_kernel_traitsILi160ELi64ELi64ELi8ELi4ELi4ELi4ELb0ELb1EN7cutlass10bfloat16_tE19Flash_kernel_traitsILi160ELi64ELi64ELi8ES2_EELb0ELb1ELb0ELb1EEv16Flash_bwd_params' for 'sm_80'
ptxas info    : Function properties for _Z44flash_bwd_dq_dk_dv_loop_seqk_parallel_kernelI23Flash_bwd_kernel_traitsILi160ELi64ELi64ELi8ELi4ELi4ELi4ELb0ELb1EN7cutlass10bfloat16_tE19Flash_kernel_traitsILi160ELi64ELi64ELi8ES2_EELb0ELb1ELb0ELb1EEv16Flash_bwd_params
    0 bytes stack frame, 0 bytes spill stores, 0 bytes spill loads
ptxas info    : Used 206 registers, 696 bytes cmem[0]
ptxas info    : Compiling entry function '_Z44flash_bwd_dq_dk_dv_loop_seqk_parallel_kernelI23Flash_bwd_kernel_traitsILi160ELi64ELi64ELi8ELi4ELi4ELi4ELb0ELb1EN7cutlass10bfloat16_tE19Flash_kernel_traitsILi160ELi64ELi64ELi8ES2_EELb0ELb1ELb1ELb0EEv16Flash_bwd_params' for 'sm_80'
ptxas info    : Function properties for _Z44flash_bwd_dq_dk_dv_loop_seqk_parallel_kernelI23Flash_bwd_kernel_traitsILi160ELi64ELi64ELi8ELi4ELi4ELi4ELb0ELb1EN7cutlass10bfloat16_tE19Flash_kernel_traitsILi160ELi64ELi64ELi8ES2_EELb0ELb1ELb1ELb0EEv16Flash_bwd_params
    0 bytes stack frame, 0 bytes spill stores, 0 bytes spill loads
ptxas info    : Used 202 registers, 696 bytes cmem[0]
ptxas info    : Compiling entry function '_Z44flash_bwd_dq_dk_dv_loop_seqk_parallel_kernelI23Flash_bwd_kernel_traitsILi160ELi64ELi64ELi8ELi4ELi4ELi4ELb0ELb1EN7cutlass10bfloat16_tE19Flash_kernel_traitsILi160ELi64ELi64ELi8ES2_EELb0ELb1ELb1ELb1EEv16Flash_bwd_params' for 'sm_80'
ptxas info    : Function properties for _Z44flash_bwd_dq_dk_dv_loop_seqk_parallel_kernelI23Flash_bwd_kernel_traitsILi160ELi64ELi64ELi8ELi4ELi4ELi4ELb0ELb1EN7cutlass10bfloat16_tE19Flash_kernel_traitsILi160ELi64ELi64ELi8ES2_EELb0ELb1ELb1ELb1EEv16Flash_bwd_params
    0 bytes stack frame, 0 bytes spill stores, 0 bytes spill loads
ptxas info    : Used 200 registers, 696 bytes cmem[0]
ptxas info    : Compiling entry function '_Z44flash_bwd_dq_dk_dv_loop_seqk_parallel_kernelI23Flash_bwd_kernel_traitsILi160ELi64ELi64ELi8ELi4ELi4ELi4ELb0ELb0EN7cutlass10bfloat16_tE19Flash_kernel_traitsILi160ELi64ELi64ELi8ES2_EELb0ELb0ELb0ELb0EEv16Flash_bwd_params' for 'sm_80'
ptxas info    : Function properties for _Z44flash_bwd_dq_dk_dv_loop_seqk_parallel_kernelI23Flash_bwd_kernel_traitsILi160ELi64ELi64ELi8ELi4ELi4ELi4ELb0ELb0EN7cutlass10bfloat16_tE19Flash_kernel_traitsILi160ELi64ELi64ELi8ES2_EELb0ELb0ELb0ELb0EEv16Flash_bwd_params
    0 bytes stack frame, 0 bytes spill stores, 0 bytes spill loads
ptxas info    : Used 210 registers, 696 bytes cmem[0]
ptxas info    : Compiling entry function '_Z44flash_bwd_dq_dk_dv_loop_seqk_parallel_kernelI23Flash_bwd_kernel_traitsILi160ELi64ELi64ELi8ELi4ELi4ELi4ELb0ELb0EN7cutlass10bfloat16_tE19Flash_kernel_traitsILi160ELi64ELi64ELi8ES2_EELb0ELb0ELb0ELb1EEv16Flash_bwd_params' for 'sm_80'
ptxas info    : Function properties for _Z44flash_bwd_dq_dk_dv_loop_seqk_parallel_kernelI23Flash_bwd_kernel_traitsILi160ELi64ELi64ELi8ELi4ELi4ELi4ELb0ELb0EN7cutlass10bfloat16_tE19Flash_kernel_traitsILi160ELi64ELi64ELi8ES2_EELb0ELb0ELb0ELb1EEv16Flash_bwd_params
    0 bytes stack frame, 0 bytes spill stores, 0 bytes spill loads
ptxas info    : Used 212 registers, 696 bytes cmem[0]
ptxas info    : Compiling entry function '_Z44flash_bwd_dq_dk_dv_loop_seqk_parallel_kernelI23Flash_bwd_kernel_traitsILi160ELi64ELi64ELi8ELi4ELi4ELi4ELb0ELb0EN7cutlass10bfloat16_tE19Flash_kernel_traitsILi160ELi64ELi64ELi8ES2_EELb0ELb0ELb1ELb0EEv16Flash_bwd_params' for 'sm_80'
ptxas info    : Function properties for _Z44flash_bwd_dq_dk_dv_loop_seqk_parallel_kernelI23Flash_bwd_kernel_traitsILi160ELi64ELi64ELi8ELi4ELi4ELi4ELb0ELb0EN7cutlass10bfloat16_tE19Flash_kernel_traitsILi160ELi64ELi64ELi8ES2_EELb0ELb0ELb1ELb0EEv16Flash_bwd_params
    0 bytes stack frame, 0 bytes spill stores, 0 bytes spill loads
ptxas info    : Used 204 registers, 696 bytes cmem[0]
ptxas info    : Compiling entry function '_Z44flash_bwd_dq_dk_dv_loop_seqk_parallel_kernelI23Flash_bwd_kernel_traitsILi160ELi64ELi64ELi8ELi4ELi4ELi4ELb0ELb0EN7cutlass10bfloat16_tE19Flash_kernel_traitsILi160ELi64ELi64ELi8ES2_EELb0ELb0ELb1ELb1EEv16Flash_bwd_params' for 'sm_80'
ptxas info    : Function properties for _Z44flash_bwd_dq_dk_dv_loop_seqk_parallel_kernelI23Flash_bwd_kernel_traitsILi160ELi64ELi64ELi8ELi4ELi4ELi4ELb0ELb0EN7cutlass10bfloat16_tE19Flash_kernel_traitsILi160ELi64ELi64ELi8ES2_EELb0ELb0ELb1ELb1EEv16Flash_bwd_params
    0 bytes stack frame, 0 bytes spill stores, 0 bytes spill loads
ptxas info    : Used 204 registers, 696 bytes cmem[0]
ptxas info    : Compiling entry function '_Z44flash_bwd_dq_dk_dv_loop_seqk_parallel_kernelI23Flash_bwd_kernel_traitsILi160ELi64ELi64ELi8ELi4ELi4ELi4ELb0ELb0EN7cutlass10bfloat16_tE19Flash_kernel_traitsILi160ELi64ELi64ELi8ES2_EELb0ELb1ELb0ELb0EEv16Flash_bwd_params' for 'sm_80'
ptxas info    : Function properties for _Z44flash_bwd_dq_dk_dv_loop_seqk_parallel_kernelI23Flash_bwd_kernel_traitsILi160ELi64ELi64ELi8ELi4ELi4ELi4ELb0ELb0EN7cutlass10bfloat16_tE19Flash_kernel_traitsILi160ELi64ELi64ELi8ES2_EELb0ELb1ELb0ELb0EEv16Flash_bwd_params
    0 bytes stack frame, 0 bytes spill stores, 0 bytes spill loads
ptxas info    : Used 216 registers, 696 bytes cmem[0]
ptxas info    : Compiling entry function '_Z44flash_bwd_dq_dk_dv_loop_seqk_parallel_kernelI23Flash_bwd_kernel_traitsILi160ELi64ELi64ELi8ELi4ELi4ELi4ELb0ELb0EN7cutlass10bfloat16_tE19Flash_kernel_traitsILi160ELi64ELi64ELi8ES2_EELb0ELb1ELb0ELb1EEv16Flash_bwd_params' for 'sm_80'
ptxas info    : Function properties for _Z44flash_bwd_dq_dk_dv_loop_seqk_parallel_kernelI23Flash_bwd_kernel_traitsILi160ELi64ELi64ELi8ELi4ELi4ELi4ELb0ELb0EN7cutlass10bfloat16_tE19Flash_kernel_traitsILi160ELi64ELi64ELi8ES2_EELb0ELb1ELb0ELb1EEv16Flash_bwd_params
    0 bytes stack frame, 0 bytes spill stores, 0 bytes spill loads
ptxas info    : Used 214 registers, 696 bytes cmem[0]
ptxas info    : Compiling entry function '_Z44flash_bwd_dq_dk_dv_loop_seqk_parallel_kernelI23Flash_bwd_kernel_traitsILi160ELi64ELi64ELi8ELi4ELi4ELi4ELb0ELb0EN7cutlass10bfloat16_tE19Flash_kernel_traitsILi160ELi64ELi64ELi8ES2_EELb0ELb1ELb1ELb0EEv16Flash_bwd_params' for 'sm_80'
ptxas info    : Function properties for _Z44flash_bwd_dq_dk_dv_loop_seqk_parallel_kernelI23Flash_bwd_kernel_traitsILi160ELi64ELi64ELi8ELi4ELi4ELi4ELb0ELb0EN7cutlass10bfloat16_tE19Flash_kernel_traitsILi160ELi64ELi64ELi8ES2_EELb0ELb1ELb1ELb0EEv16Flash_bwd_params
    0 bytes stack frame, 0 bytes spill stores, 0 bytes spill loads
ptxas info    : Used 208 registers, 696 bytes cmem[0]
ptxas info    : Compiling entry function '_Z44flash_bwd_dq_dk_dv_loop_seqk_parallel_kernelI23Flash_bwd_kernel_traitsILi160ELi64ELi64ELi8ELi4ELi4ELi4ELb0ELb0EN7cutlass10bfloat16_tE19Flash_kernel_traitsILi160ELi64ELi64ELi8ES2_EELb0ELb1ELb1ELb1EEv16Flash_bwd_params' for 'sm_80'
ptxas info    : Function properties for _Z44flash_bwd_dq_dk_dv_loop_seqk_parallel_kernelI23Flash_bwd_kernel_traitsILi160ELi64ELi64ELi8ELi4ELi4ELi4ELb0ELb0EN7cutlass10bfloat16_tE19Flash_kernel_traitsILi160ELi64ELi64ELi8ES2_EELb0ELb1ELb1ELb1EEv16Flash_bwd_params
    0 bytes stack frame, 0 bytes spill stores, 0 bytes spill loads
ptxas info    : Used 206 registers, 696 bytes cmem[0]
ptxas info    : Compiling entry function '_Z27flash_bwd_convert_dq_kernelI23Flash_bwd_kernel_traitsILi160ELi64ELi64ELi8ELi4ELi4ELi4ELb0ELb1EN7cutlass10bfloat16_tE19Flash_kernel_traitsILi160ELi64ELi64ELi8ES2_EEEv16Flash_bwd_params' for 'sm_80'
ptxas info    : Function properties for _Z27flash_bwd_convert_dq_kernelI23Flash_bwd_kernel_traitsILi160ELi64ELi64ELi8ELi4ELi4ELi4ELb0ELb1EN7cutlass10bfloat16_tE19Flash_kernel_traitsILi160ELi64ELi64ELi8ES2_EEEv16Flash_bwd_params
    0 bytes stack frame, 0 bytes spill stores, 0 bytes spill loads
ptxas info    : Used 32 registers, 696 bytes cmem[0]
ptxas info    : Compiling entry function '_Z44flash_bwd_dq_dk_dv_loop_seqk_parallel_kernelI23Flash_bwd_kernel_traitsILi160ELi64ELi64ELi8ELi4ELi4ELi4ELb0ELb1EN7cutlass10bfloat16_tE19Flash_kernel_traitsILi160ELi64ELi64ELi8ES2_EELb1ELb0ELb0ELb0EEv16Flash_bwd_params' for 'sm_80'
ptxas info    : Function properties for _Z44flash_bwd_dq_dk_dv_loop_seqk_parallel_kernelI23Flash_bwd_kernel_traitsILi160ELi64ELi64ELi8ELi4ELi4ELi4ELb0ELb1EN7cutlass10bfloat16_tE19Flash_kernel_traitsILi160ELi64ELi64ELi8ES2_EELb1ELb0ELb0ELb0EEv16Flash_bwd_params
    0 bytes stack frame, 0 bytes spill stores, 0 bytes spill loads
ptxas info    : Used 222 registers, 696 bytes cmem[0]
ptxas info    : Compiling entry function '_Z44flash_bwd_dq_dk_dv_loop_seqk_parallel_kernelI23Flash_bwd_kernel_traitsILi160ELi64ELi64ELi8ELi4ELi4ELi4ELb0ELb1EN7cutlass10bfloat16_tE19Flash_kernel_traitsILi160ELi64ELi64ELi8ES2_EELb1ELb0ELb0ELb1EEv16Flash_bwd_params' for 'sm_80'
ptxas info    : Function properties for _Z44flash_bwd_dq_dk_dv_loop_seqk_parallel_kernelI23Flash_bwd_kernel_traitsILi160ELi64ELi64ELi8ELi4ELi4ELi4ELb0ELb1EN7cutlass10bfloat16_tE19Flash_kernel_traitsILi160ELi64ELi64ELi8ES2_EELb1ELb0ELb0ELb1EEv16Flash_bwd_params
    0 bytes stack frame, 0 bytes spill stores, 0 bytes spill loads
ptxas info    : Used 222 registers, 696 bytes cmem[0]
ptxas info    : Compiling entry function '_Z44flash_bwd_dq_dk_dv_loop_seqk_parallel_kernelI23Flash_bwd_kernel_traitsILi160ELi64ELi64ELi8ELi4ELi4ELi4ELb0ELb1EN7cutlass10bfloat16_tE19Flash_kernel_traitsILi160ELi64ELi64ELi8ES2_EELb1ELb0ELb1ELb0EEv16Flash_bwd_params' for 'sm_80'
ptxas info    : Function properties for _Z44flash_bwd_dq_dk_dv_loop_seqk_parallel_kernelI23Flash_bwd_kernel_traitsILi160ELi64ELi64ELi8ELi4ELi4ELi4ELb0ELb1EN7cutlass10bfloat16_tE19Flash_kernel_traitsILi160ELi64ELi64ELi8ES2_EELb1ELb0ELb1ELb0EEv16Flash_bwd_params
    0 bytes stack frame, 0 bytes spill stores, 0 bytes spill loads
ptxas info    : Used 208 registers, 696 bytes cmem[0]
ptxas info    : Compiling entry function '_Z44flash_bwd_dq_dk_dv_loop_seqk_parallel_kernelI23Flash_bwd_kernel_traitsILi160ELi64ELi64ELi8ELi4ELi4ELi4ELb0ELb1EN7cutlass10bfloat16_tE19Flash_kernel_traitsILi160ELi64ELi64ELi8ES2_EELb1ELb0ELb1ELb1EEv16Flash_bwd_params' for 'sm_80'
ptxas info    : Function properties for _Z44flash_bwd_dq_dk_dv_loop_seqk_parallel_kernelI23Flash_bwd_kernel_traitsILi160ELi64ELi64ELi8ELi4ELi4ELi4ELb0ELb1EN7cutlass10bfloat16_tE19Flash_kernel_traitsILi160ELi64ELi64ELi8ES2_EELb1ELb0ELb1ELb1EEv16Flash_bwd_params
    0 bytes stack frame, 0 bytes spill stores, 0 bytes spill loads
ptxas info    : Used 214 registers, 696 bytes cmem[0]
ptxas info    : Compiling entry function '_Z44flash_bwd_dq_dk_dv_loop_seqk_parallel_kernelI23Flash_bwd_kernel_traitsILi160ELi64ELi64ELi8ELi4ELi4ELi4ELb0ELb1EN7cutlass10bfloat16_tE19Flash_kernel_traitsILi160ELi64ELi64ELi8ES2_EELb1ELb1ELb0ELb0EEv16Flash_bwd_params' for 'sm_80'
ptxas info    : Function properties for _Z44flash_bwd_dq_dk_dv_loop_seqk_parallel_kernelI23Flash_bwd_kernel_traitsILi160ELi64ELi64ELi8ELi4ELi4ELi4ELb0ELb1EN7cutlass10bfloat16_tE19Flash_kernel_traitsILi160ELi64ELi64ELi8ES2_EELb1ELb1ELb0ELb0EEv16Flash_bwd_params
    0 bytes stack frame, 0 bytes spill stores, 0 bytes spill loads
ptxas info    : Used 216 registers, 696 bytes cmem[0]
ptxas info    : Compiling entry function '_Z44flash_bwd_dq_dk_dv_loop_seqk_parallel_kernelI23Flash_bwd_kernel_traitsILi160ELi64ELi64ELi8ELi4ELi4ELi4ELb0ELb1EN7cutlass10bfloat16_tE19Flash_kernel_traitsILi160ELi64ELi64ELi8ES2_EELb1ELb1ELb0ELb1EEv16Flash_bwd_params' for 'sm_80'
ptxas info    : Function properties for _Z44flash_bwd_dq_dk_dv_loop_seqk_parallel_kernelI23Flash_bwd_kernel_traitsILi160ELi64ELi64ELi8ELi4ELi4ELi4ELb0ELb1EN7cutlass10bfloat16_tE19Flash_kernel_traitsILi160ELi64ELi64ELi8ES2_EELb1ELb1ELb0ELb1EEv16Flash_bwd_params
    0 bytes stack frame, 0 bytes spill stores, 0 bytes spill loads
ptxas info    : Used 212 registers, 696 bytes cmem[0]
ptxas info    : Compiling entry function '_Z44flash_bwd_dq_dk_dv_loop_seqk_parallel_kernelI23Flash_bwd_kernel_traitsILi160ELi64ELi64ELi8ELi4ELi4ELi4ELb0ELb1EN7cutlass10bfloat16_tE19Flash_kernel_traitsILi160ELi64ELi64ELi8ES2_EELb1ELb1ELb1ELb0EEv16Flash_bwd_params' for 'sm_80'
ptxas info    : Function properties for _Z44flash_bwd_dq_dk_dv_loop_seqk_parallel_kernelI23Flash_bwd_kernel_traitsILi160ELi64ELi64ELi8ELi4ELi4ELi4ELb0ELb1EN7cutlass10bfloat16_tE19Flash_kernel_traitsILi160ELi64ELi64ELi8ES2_EELb1ELb1ELb1ELb0EEv16Flash_bwd_params
    0 bytes stack frame, 0 bytes spill stores, 0 bytes spill loads
ptxas info    : Used 208 registers, 696 bytes cmem[0]
ptxas info    : Compiling entry function '_Z44flash_bwd_dq_dk_dv_loop_seqk_parallel_kernelI23Flash_bwd_kernel_traitsILi160ELi64ELi64ELi8ELi4ELi4ELi4ELb0ELb1EN7cutlass10bfloat16_tE19Flash_kernel_traitsILi160ELi64ELi64ELi8ES2_EELb1ELb1ELb1ELb1EEv16Flash_bwd_params' for 'sm_80'
ptxas info    : Function properties for _Z44flash_bwd_dq_dk_dv_loop_seqk_parallel_kernelI23Flash_bwd_kernel_traitsILi160ELi64ELi64ELi8ELi4ELi4ELi4ELb0ELb1EN7cutlass10bfloat16_tE19Flash_kernel_traitsILi160ELi64ELi64ELi8ES2_EELb1ELb1ELb1ELb1EEv16Flash_bwd_params
    0 bytes stack frame, 0 bytes spill stores, 0 bytes spill loads
ptxas info    : Used 206 registers, 696 bytes cmem[0]
ptxas info    : Compiling entry function '_Z25flash_bwd_dot_do_o_kernelILb1E23Flash_bwd_kernel_traitsILi160ELi64ELi64ELi8ELi4ELi4ELi4ELb0ELb1EN7cutlass10bfloat16_tE19Flash_kernel_traitsILi160ELi64ELi64ELi8ES2_EEEv16Flash_bwd_params' for 'sm_80'
ptxas info    : Function properties for _Z25flash_bwd_dot_do_o_kernelILb1E23Flash_bwd_kernel_traitsILi160ELi64ELi64ELi8ELi4ELi4ELi4ELb0ELb1EN7cutlass10bfloat16_tE19Flash_kernel_traitsILi160ELi64ELi64ELi8ES2_EEEv16Flash_bwd_params
    0 bytes stack frame, 0 bytes spill stores, 0 bytes spill loads
ptxas info    : Used 64 registers, 696 bytes cmem[0]
ptxas info    : Compiling entry function '_Z27flash_bwd_convert_dq_kernelI23Flash_bwd_kernel_traitsILi160ELi64ELi64ELi8ELi4ELi4ELi4ELb0ELb0EN7cutlass10bfloat16_tE19Flash_kernel_traitsILi160ELi64ELi64ELi8ES2_EEEv16Flash_bwd_params' for 'sm_80'
ptxas info    : Function properties for _Z27flash_bwd_convert_dq_kernelI23Flash_bwd_kernel_traitsILi160ELi64ELi64ELi8ELi4ELi4ELi4ELb0ELb0EN7cutlass10bfloat16_tE19Flash_kernel_traitsILi160ELi64ELi64ELi8ES2_EEEv16Flash_bwd_params
    0 bytes stack frame, 0 bytes spill stores, 0 bytes spill loads
ptxas info    : Used 32 registers, 696 bytes cmem[0]
ptxas info    : Compiling entry function '_Z44flash_bwd_dq_dk_dv_loop_seqk_parallel_kernelI23Flash_bwd_kernel_traitsILi160ELi64ELi64ELi8ELi4ELi4ELi4ELb0ELb0EN7cutlass10bfloat16_tE19Flash_kernel_traitsILi160ELi64ELi64ELi8ES2_EELb1ELb0ELb0ELb0EEv16Flash_bwd_params' for 'sm_80'
ptxas info    : Function properties for _Z44flash_bwd_dq_dk_dv_loop_seqk_parallel_kernelI23Flash_bwd_kernel_traitsILi160ELi64ELi64ELi8ELi4ELi4ELi4ELb0ELb0EN7cutlass10bfloat16_tE19Flash_kernel_traitsILi160ELi64ELi64ELi8ES2_EELb1ELb0ELb0ELb0EEv16Flash_bwd_params
    0 bytes stack frame, 0 bytes spill stores, 0 bytes spill loads
ptxas info    : Used 226 registers, 696 bytes cmem[0]
ptxas info    : Compiling entry function '_Z44flash_bwd_dq_dk_dv_loop_seqk_parallel_kernelI23Flash_bwd_kernel_traitsILi160ELi64ELi64ELi8ELi4ELi4ELi4ELb0ELb0EN7cutlass10bfloat16_tE19Flash_kernel_traitsILi160ELi64ELi64ELi8ES2_EELb1ELb0ELb0ELb1EEv16Flash_bwd_params' for 'sm_80'
ptxas info    : Function properties for _Z44flash_bwd_dq_dk_dv_loop_seqk_parallel_kernelI23Flash_bwd_kernel_traitsILi160ELi64ELi64ELi8ELi4ELi4ELi4ELb0ELb0EN7cutlass10bfloat16_tE19Flash_kernel_traitsILi160ELi64ELi64ELi8ES2_EELb1ELb0ELb0ELb1EEv16Flash_bwd_params
    0 bytes stack frame, 0 bytes spill stores, 0 bytes spill loads
ptxas info    : Used 216 registers, 696 bytes cmem[0]
ptxas info    : Compiling entry function '_Z44flash_bwd_dq_dk_dv_loop_seqk_parallel_kernelI23Flash_bwd_kernel_traitsILi160ELi64ELi64ELi8ELi4ELi4ELi4ELb0ELb0EN7cutlass10bfloat16_tE19Flash_kernel_traitsILi160ELi64ELi64ELi8ES2_EELb1ELb0ELb1ELb0EEv16Flash_bwd_params' for 'sm_80'
ptxas info    : Function properties for _Z44flash_bwd_dq_dk_dv_loop_seqk_parallel_kernelI23Flash_bwd_kernel_traitsILi160ELi64ELi64ELi8ELi4ELi4ELi4ELb0ELb0EN7cutlass10bfloat16_tE19Flash_kernel_traitsILi160ELi64ELi64ELi8ES2_EELb1ELb0ELb1ELb0EEv16Flash_bwd_params
    0 bytes stack frame, 0 bytes spill stores, 0 bytes spill loads
ptxas info    : Used 220 registers, 696 bytes cmem[0]
ptxas info    : Compiling entry function '_Z44flash_bwd_dq_dk_dv_loop_seqk_parallel_kernelI23Flash_bwd_kernel_traitsILi160ELi64ELi64ELi8ELi4ELi4ELi4ELb0ELb0EN7cutlass10bfloat16_tE19Flash_kernel_traitsILi160ELi64ELi64ELi8ES2_EELb1ELb0ELb1ELb1EEv16Flash_bwd_params' for 'sm_80'
ptxas info    : Function properties for _Z44flash_bwd_dq_dk_dv_loop_seqk_parallel_kernelI23Flash_bwd_kernel_traitsILi160ELi64ELi64ELi8ELi4ELi4ELi4ELb0ELb0EN7cutlass10bfloat16_tE19Flash_kernel_traitsILi160ELi64ELi64ELi8ES2_EELb1ELb0ELb1ELb1EEv16Flash_bwd_params
    0 bytes stack frame, 0 bytes spill stores, 0 bytes spill loads
ptxas info    : Used 222 registers, 696 bytes cmem[0]
ptxas info    : Compiling entry function '_Z44flash_bwd_dq_dk_dv_loop_seqk_parallel_kernelI23Flash_bwd_kernel_traitsILi160ELi64ELi64ELi8ELi4ELi4ELi4ELb0ELb0EN7cutlass10bfloat16_tE19Flash_kernel_traitsILi160ELi64ELi64ELi8ES2_EELb1ELb1ELb0ELb0EEv16Flash_bwd_params' for 'sm_80'
ptxas info    : Function properties for _Z44flash_bwd_dq_dk_dv_loop_seqk_parallel_kernelI23Flash_bwd_kernel_traitsILi160ELi64ELi64ELi8ELi4ELi4ELi4ELb0ELb0EN7cutlass10bfloat16_tE19Flash_kernel_traitsILi160ELi64ELi64ELi8ES2_EELb1ELb1ELb0ELb0EEv16Flash_bwd_params
    0 bytes stack frame, 0 bytes spill stores, 0 bytes spill loads
ptxas info    : Used 222 registers, 696 bytes cmem[0]
ptxas info    : Compiling entry function '_Z44flash_bwd_dq_dk_dv_loop_seqk_parallel_kernelI23Flash_bwd_kernel_traitsILi160ELi64ELi64ELi8ELi4ELi4ELi4ELb0ELb0EN7cutlass10bfloat16_tE19Flash_kernel_traitsILi160ELi64ELi64ELi8ES2_EELb1ELb1ELb0ELb1EEv16Flash_bwd_params' for 'sm_80'
ptxas info    : Function properties for _Z44flash_bwd_dq_dk_dv_loop_seqk_parallel_kernelI23Flash_bwd_kernel_traitsILi160ELi64ELi64ELi8ELi4ELi4ELi4ELb0ELb0EN7cutlass10bfloat16_tE19Flash_kernel_traitsILi160ELi64ELi64ELi8ES2_EELb1ELb1ELb0ELb1EEv16Flash_bwd_params
    0 bytes stack frame, 0 bytes spill stores, 0 bytes spill loads
ptxas info    : Used 220 registers, 696 bytes cmem[0]
ptxas info    : Compiling entry function '_Z44flash_bwd_dq_dk_dv_loop_seqk_parallel_kernelI23Flash_bwd_kernel_traitsILi160ELi64ELi64ELi8ELi4ELi4ELi4ELb0ELb0EN7cutlass10bfloat16_tE19Flash_kernel_traitsILi160ELi64ELi64ELi8ES2_EELb1ELb1ELb1ELb0EEv16Flash_bwd_params' for 'sm_80'
ptxas info    : Function properties for _Z44flash_bwd_dq_dk_dv_loop_seqk_parallel_kernelI23Flash_bwd_kernel_traitsILi160ELi64ELi64ELi8ELi4ELi4ELi4ELb0ELb0EN7cutlass10bfloat16_tE19Flash_kernel_traitsILi160ELi64ELi64ELi8ES2_EELb1ELb1ELb1ELb0EEv16Flash_bwd_params
    0 bytes stack frame, 0 bytes spill stores, 0 bytes spill loads
ptxas info    : Used 212 registers, 696 bytes cmem[0]
ptxas info    : Compiling entry function '_Z44flash_bwd_dq_dk_dv_loop_seqk_parallel_kernelI23Flash_bwd_kernel_traitsILi160ELi64ELi64ELi8ELi4ELi4ELi4ELb0ELb0EN7cutlass10bfloat16_tE19Flash_kernel_traitsILi160ELi64ELi64ELi8ES2_EELb1ELb1ELb1ELb1EEv16Flash_bwd_params' for 'sm_80'
ptxas info    : Function properties for _Z44flash_bwd_dq_dk_dv_loop_seqk_parallel_kernelI23Flash_bwd_kernel_traitsILi160ELi64ELi64ELi8ELi4ELi4ELi4ELb0ELb0EN7cutlass10bfloat16_tE19Flash_kernel_traitsILi160ELi64ELi64ELi8ES2_EELb1ELb1ELb1ELb1EEv16Flash_bwd_params
    0 bytes stack frame, 0 bytes spill stores, 0 bytes spill loads
ptxas info    : Used 212 registers, 696 bytes cmem[0]
ptxas info    : Compiling entry function '_Z25flash_bwd_dot_do_o_kernelILb1E23Flash_bwd_kernel_traitsILi160ELi64ELi64ELi8ELi4ELi4ELi4ELb0ELb0EN7cutlass10bfloat16_tE19Flash_kernel_traitsILi160ELi64ELi64ELi8ES2_EEEv16Flash_bwd_params' for 'sm_80'
ptxas info    : Function properties for _Z25flash_bwd_dot_do_o_kernelILb1E23Flash_bwd_kernel_traitsILi160ELi64ELi64ELi8ELi4ELi4ELi4ELb0ELb0EN7cutlass10bfloat16_tE19Flash_kernel_traitsILi160ELi64ELi64ELi8ES2_EEEv16Flash_bwd_params
    0 bytes stack frame, 0 bytes spill stores, 0 bytes spill loads
ptxas info    : Used 64 registers, 696 bytes cmem[0]
/anaconda/envs/flashattention/lib/python3.9/site-packages/torch/include/c10/core/SymInt.h(84): warning: integer conversion resulted in a change of sign

[11/36] /usr/local/cuda-11.4/bin/nvcc  -I/home/v-feiychen/flash-attention/csrc/flash_attn -I/home/v-feiychen/flash-attention/csrc/flash_attn/src -I/home/v-feiychen/flash-attention/csrc/cutlass/include -I/anaconda/envs/flashattention/lib/python3.9/site-packages/torch/include -I/anaconda/envs/flashattention/lib/python3.9/site-packages/torch/include/torch/csrc/api/include -I/anaconda/envs/flashattention/lib/python3.9/site-packages/torch/include/TH -I/anaconda/envs/flashattention/lib/python3.9/site-packages/torch/include/THC -I/usr/local/cuda-11.4/include -I/anaconda/envs/flashattention/include/python3.9 -c -c /home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_bwd_hdim96_bf16_sm80.cu -o /home/v-feiychen/flash-attention/build/temp.linux-x86_64-cpython-39/csrc/flash_attn/src/flash_bwd_hdim96_bf16_sm80.o -D__CUDA_NO_HALF_OPERATORS__ -D__CUDA_NO_HALF_CONVERSIONS__ -D__CUDA_NO_BFLOAT16_CONVERSIONS__ -D__CUDA_NO_HALF2_OPERATORS__ --expt-relaxed-constexpr --compiler-options ''"'"'-fPIC'"'"'' -O3 -std=c++17 -U__CUDA_NO_HALF_OPERATORS__ -U__CUDA_NO_HALF_CONVERSIONS__ -U__CUDA_NO_HALF2_OPERATORS__ -U__CUDA_NO_BFLOAT16_CONVERSIONS__ --expt-relaxed-constexpr --expt-extended-lambda --use_fast_math --ptxas-options=-v -lineinfo -gencode arch=compute_80,code=sm_80 --threads 4 -DTORCH_API_INCLUDE_EXTENSION_H '-DPYBIND11_COMPILER_TYPE="_gcc"' '-DPYBIND11_STDLIB="_libstdcpp"' '-DPYBIND11_BUILD_ABI="_cxxabi1011"' -DTORCH_EXTENSION_NAME=flash_attn_2_cuda -D_GLIBCXX_USE_CXX11_ABI=0
/anaconda/envs/flashattention/lib/python3.9/site-packages/torch/include/c10/core/SymInt.h(84): warning: integer conversion resulted in a change of sign

ptxas info    : 2 bytes gmem
ptxas info    : Compiling entry function '_Z44flash_bwd_dq_dk_dv_loop_seqk_parallel_kernelI23Flash_bwd_kernel_traitsILi96ELi64ELi128ELi8ELi2ELi4ELi4ELb1ELb0EN7cutlass10bfloat16_tE19Flash_kernel_traitsILi96ELi64ELi128ELi8ES2_EELb0ELb0ELb0ELb0EEv16Flash_bwd_params' for 'sm_80'
ptxas info    : Function properties for _Z44flash_bwd_dq_dk_dv_loop_seqk_parallel_kernelI23Flash_bwd_kernel_traitsILi96ELi64ELi128ELi8ELi2ELi4ELi4ELb1ELb0EN7cutlass10bfloat16_tE19Flash_kernel_traitsILi96ELi64ELi128ELi8ES2_EELb0ELb0ELb0ELb0EEv16Flash_bwd_params
    16 bytes stack frame, 12 bytes spill stores, 12 bytes spill loads
ptxas info    : Used 255 registers, 696 bytes cmem[0]
ptxas info    : Compiling entry function '_Z44flash_bwd_dq_dk_dv_loop_seqk_parallel_kernelI23Flash_bwd_kernel_traitsILi96ELi64ELi128ELi8ELi2ELi4ELi4ELb1ELb0EN7cutlass10bfloat16_tE19Flash_kernel_traitsILi96ELi64ELi128ELi8ES2_EELb0ELb0ELb0ELb1EEv16Flash_bwd_params' for 'sm_80'
ptxas info    : Function properties for _Z44flash_bwd_dq_dk_dv_loop_seqk_parallel_kernelI23Flash_bwd_kernel_traitsILi96ELi64ELi128ELi8ELi2ELi4ELi4ELb1ELb0EN7cutlass10bfloat16_tE19Flash_kernel_traitsILi96ELi64ELi128ELi8ES2_EELb0ELb0ELb0ELb1EEv16Flash_bwd_params
    24 bytes stack frame, 20 bytes spill stores, 20 bytes spill loads
ptxas info    : Used 255 registers, 696 bytes cmem[0]
ptxas info    : Compiling entry function '_Z44flash_bwd_dq_dk_dv_loop_seqk_parallel_kernelI23Flash_bwd_kernel_traitsILi96ELi64ELi128ELi8ELi2ELi4ELi4ELb1ELb0EN7cutlass10bfloat16_tE19Flash_kernel_traitsILi96ELi64ELi128ELi8ES2_EELb0ELb0ELb1ELb0EEv16Flash_bwd_params' for 'sm_80'
ptxas info    : Function properties for _Z44flash_bwd_dq_dk_dv_loop_seqk_parallel_kernelI23Flash_bwd_kernel_traitsILi96ELi64ELi128ELi8ELi2ELi4ELi4ELb1ELb0EN7cutlass10bfloat16_tE19Flash_kernel_traitsILi96ELi64ELi128ELi8ES2_EELb0ELb0ELb1ELb0EEv16Flash_bwd_params
    0 bytes stack frame, 0 bytes spill stores, 0 bytes spill loads
ptxas info    : Used 254 registers, 696 bytes cmem[0]
ptxas info    : Compiling entry function '_Z44flash_bwd_dq_dk_dv_loop_seqk_parallel_kernelI23Flash_bwd_kernel_traitsILi96ELi64ELi128ELi8ELi2ELi4ELi4ELb1ELb0EN7cutlass10bfloat16_tE19Flash_kernel_traitsILi96ELi64ELi128ELi8ES2_EELb0ELb0ELb1ELb1EEv16Flash_bwd_params' for 'sm_80'
ptxas info    : Function properties for _Z44flash_bwd_dq_dk_dv_loop_seqk_parallel_kernelI23Flash_bwd_kernel_traitsILi96ELi64ELi128ELi8ELi2ELi4ELi4ELb1ELb0EN7cutlass10bfloat16_tE19Flash_kernel_traitsILi96ELi64ELi128ELi8ES2_EELb0ELb0ELb1ELb1EEv16Flash_bwd_params
    0 bytes stack frame, 0 bytes spill stores, 0 bytes spill loads
ptxas info    : Used 254 registers, 696 bytes cmem[0]
ptxas info    : Compiling entry function '_Z44flash_bwd_dq_dk_dv_loop_seqk_parallel_kernelI23Flash_bwd_kernel_traitsILi96ELi64ELi128ELi8ELi2ELi4ELi4ELb1ELb0EN7cutlass10bfloat16_tE19Flash_kernel_traitsILi96ELi64ELi128ELi8ES2_EELb0ELb1ELb0ELb0EEv16Flash_bwd_params' for 'sm_80'
ptxas info    : Function properties for _Z44flash_bwd_dq_dk_dv_loop_seqk_parallel_kernelI23Flash_bwd_kernel_traitsILi96ELi64ELi128ELi8ELi2ELi4ELi4ELb1ELb0EN7cutlass10bfloat16_tE19Flash_kernel_traitsILi96ELi64ELi128ELi8ES2_EELb0ELb1ELb0ELb0EEv16Flash_bwd_params
    0 bytes stack frame, 0 bytes spill stores, 0 bytes spill loads
ptxas info    : Used 255 registers, 696 bytes cmem[0]
ptxas info    : Compiling entry function '_Z44flash_bwd_dq_dk_dv_loop_seqk_parallel_kernelI23Flash_bwd_kernel_traitsILi96ELi64ELi128ELi8ELi2ELi4ELi4ELb1ELb0EN7cutlass10bfloat16_tE19Flash_kernel_traitsILi96ELi64ELi128ELi8ES2_EELb0ELb1ELb0ELb1EEv16Flash_bwd_params' for 'sm_80'
ptxas info    : Function properties for _Z44flash_bwd_dq_dk_dv_loop_seqk_parallel_kernelI23Flash_bwd_kernel_traitsILi96ELi64ELi128ELi8ELi2ELi4ELi4ELb1ELb0EN7cutlass10bfloat16_tE19Flash_kernel_traitsILi96ELi64ELi128ELi8ES2_EELb0ELb1ELb0ELb1EEv16Flash_bwd_params
    0 bytes stack frame, 0 bytes spill stores, 0 bytes spill loads
ptxas info    : Used 254 registers, 696 bytes cmem[0]
ptxas info    : Compiling entry function '_Z44flash_bwd_dq_dk_dv_loop_seqk_parallel_kernelI23Flash_bwd_kernel_traitsILi96ELi64ELi128ELi8ELi2ELi4ELi4ELb1ELb0EN7cutlass10bfloat16_tE19Flash_kernel_traitsILi96ELi64ELi128ELi8ES2_EELb0ELb1ELb1ELb0EEv16Flash_bwd_params' for 'sm_80'
ptxas info    : Function properties for _Z44flash_bwd_dq_dk_dv_loop_seqk_parallel_kernelI23Flash_bwd_kernel_traitsILi96ELi64ELi128ELi8ELi2ELi4ELi4ELb1ELb0EN7cutlass10bfloat16_tE19Flash_kernel_traitsILi96ELi64ELi128ELi8ES2_EELb0ELb1ELb1ELb0EEv16Flash_bwd_params
    0 bytes stack frame, 0 bytes spill stores, 0 bytes spill loads
ptxas info    : Used 254 registers, 696 bytes cmem[0]
ptxas info    : Compiling entry function '_Z44flash_bwd_dq_dk_dv_loop_seqk_parallel_kernelI23Flash_bwd_kernel_traitsILi96ELi64ELi128ELi8ELi2ELi4ELi4ELb1ELb0EN7cutlass10bfloat16_tE19Flash_kernel_traitsILi96ELi64ELi128ELi8ES2_EELb0ELb1ELb1ELb1EEv16Flash_bwd_params' for 'sm_80'
ptxas info    : Function properties for _Z44flash_bwd_dq_dk_dv_loop_seqk_parallel_kernelI23Flash_bwd_kernel_traitsILi96ELi64ELi128ELi8ELi2ELi4ELi4ELb1ELb0EN7cutlass10bfloat16_tE19Flash_kernel_traitsILi96ELi64ELi128ELi8ES2_EELb0ELb1ELb1ELb1EEv16Flash_bwd_params
    0 bytes stack frame, 0 bytes spill stores, 0 bytes spill loads
ptxas info    : Used 254 registers, 696 bytes cmem[0]
ptxas info    : Compiling entry function '_Z27flash_bwd_convert_dq_kernelI23Flash_bwd_kernel_traitsILi96ELi64ELi128ELi8ELi2ELi4ELi4ELb1ELb0EN7cutlass10bfloat16_tE19Flash_kernel_traitsILi96ELi64ELi128ELi8ES2_EEEv16Flash_bwd_params' for 'sm_80'
ptxas info    : Function properties for _Z27flash_bwd_convert_dq_kernelI23Flash_bwd_kernel_traitsILi96ELi64ELi128ELi8ELi2ELi4ELi4ELb1ELb0EN7cutlass10bfloat16_tE19Flash_kernel_traitsILi96ELi64ELi128ELi8ES2_EEEv16Flash_bwd_params
    0 bytes stack frame, 0 bytes spill stores, 0 bytes spill loads
ptxas info    : Used 32 registers, 696 bytes cmem[0]
ptxas info    : Compiling entry function '_Z44flash_bwd_dq_dk_dv_loop_seqk_parallel_kernelI23Flash_bwd_kernel_traitsILi96ELi64ELi128ELi8ELi2ELi4ELi4ELb1ELb0EN7cutlass10bfloat16_tE19Flash_kernel_traitsILi96ELi64ELi128ELi8ES2_EELb1ELb0ELb0ELb0EEv16Flash_bwd_params' for 'sm_80'
ptxas info    : Function properties for _Z44flash_bwd_dq_dk_dv_loop_seqk_parallel_kernelI23Flash_bwd_kernel_traitsILi96ELi64ELi128ELi8ELi2ELi4ELi4ELb1ELb0EN7cutlass10bfloat16_tE19Flash_kernel_traitsILi96ELi64ELi128ELi8ES2_EELb1ELb0ELb0ELb0EEv16Flash_bwd_params
    16 bytes stack frame, 12 bytes spill stores, 12 bytes spill loads
ptxas info    : Used 255 registers, 696 bytes cmem[0]
ptxas info    : Compiling entry function '_Z44flash_bwd_dq_dk_dv_loop_seqk_parallel_kernelI23Flash_bwd_kernel_traitsILi96ELi64ELi128ELi8ELi2ELi4ELi4ELb1ELb0EN7cutlass10bfloat16_tE19Flash_kernel_traitsILi96ELi64ELi128ELi8ES2_EELb1ELb0ELb0ELb1EEv16Flash_bwd_params' for 'sm_80'
ptxas info    : Function properties for _Z44flash_bwd_dq_dk_dv_loop_seqk_parallel_kernelI23Flash_bwd_kernel_traitsILi96ELi64ELi128ELi8ELi2ELi4ELi4ELb1ELb0EN7cutlass10bfloat16_tE19Flash_kernel_traitsILi96ELi64ELi128ELi8ES2_EELb1ELb0ELb0ELb1EEv16Flash_bwd_params
    0 bytes stack frame, 0 bytes spill stores, 0 bytes spill loads
ptxas info    : Used 254 registers, 696 bytes cmem[0]
ptxas info    : Compiling entry function '_Z44flash_bwd_dq_dk_dv_loop_seqk_parallel_kernelI23Flash_bwd_kernel_traitsILi96ELi64ELi128ELi8ELi2ELi4ELi4ELb1ELb0EN7cutlass10bfloat16_tE19Flash_kernel_traitsILi96ELi64ELi128ELi8ES2_EELb1ELb0ELb1ELb0EEv16Flash_bwd_params' for 'sm_80'
ptxas info    : Function properties for _Z44flash_bwd_dq_dk_dv_loop_seqk_parallel_kernelI23Flash_bwd_kernel_traitsILi96ELi64ELi128ELi8ELi2ELi4ELi4ELb1ELb0EN7cutlass10bfloat16_tE19Flash_kernel_traitsILi96ELi64ELi128ELi8ES2_EELb1ELb0ELb1ELb0EEv16Flash_bwd_params
    8 bytes stack frame, 4 bytes spill stores, 4 bytes spill loads
ptxas info    : Used 255 registers, 696 bytes cmem[0]
ptxas info    : Compiling entry function '_Z44flash_bwd_dq_dk_dv_loop_seqk_parallel_kernelI23Flash_bwd_kernel_traitsILi96ELi64ELi128ELi8ELi2ELi4ELi4ELb1ELb0EN7cutlass10bfloat16_tE19Flash_kernel_traitsILi96ELi64ELi128ELi8ES2_EELb1ELb0ELb1ELb1EEv16Flash_bwd_params' for 'sm_80'
ptxas info    : Function properties for _Z44flash_bwd_dq_dk_dv_loop_seqk_parallel_kernelI23Flash_bwd_kernel_traitsILi96ELi64ELi128ELi8ELi2ELi4ELi4ELb1ELb0EN7cutlass10bfloat16_tE19Flash_kernel_traitsILi96ELi64ELi128ELi8ES2_EELb1ELb0ELb1ELb1EEv16Flash_bwd_params
    0 bytes stack frame, 0 bytes spill stores, 0 bytes spill loads
ptxas info    : Used 255 registers, 696 bytes cmem[0]
ptxas info    : Compiling entry function '_Z44flash_bwd_dq_dk_dv_loop_seqk_parallel_kernelI23Flash_bwd_kernel_traitsILi96ELi64ELi128ELi8ELi2ELi4ELi4ELb1ELb0EN7cutlass10bfloat16_tE19Flash_kernel_traitsILi96ELi64ELi128ELi8ES2_EELb1ELb1ELb0ELb0EEv16Flash_bwd_params' for 'sm_80'
ptxas info    : Function properties for _Z44flash_bwd_dq_dk_dv_loop_seqk_parallel_kernelI23Flash_bwd_kernel_traitsILi96ELi64ELi128ELi8ELi2ELi4ELi4ELb1ELb0EN7cutlass10bfloat16_tE19Flash_kernel_traitsILi96ELi64ELi128ELi8ES2_EELb1ELb1ELb0ELb0EEv16Flash_bwd_params
    0 bytes stack frame, 0 bytes spill stores, 0 bytes spill loads
ptxas info    : Used 255 registers, 696 bytes cmem[0]
ptxas info    : Compiling entry function '_Z44flash_bwd_dq_dk_dv_loop_seqk_parallel_kernelI23Flash_bwd_kernel_traitsILi96ELi64ELi128ELi8ELi2ELi4ELi4ELb1ELb0EN7cutlass10bfloat16_tE19Flash_kernel_traitsILi96ELi64ELi128ELi8ES2_EELb1ELb1ELb0ELb1EEv16Flash_bwd_params' for 'sm_80'
ptxas info    : Function properties for _Z44flash_bwd_dq_dk_dv_loop_seqk_parallel_kernelI23Flash_bwd_kernel_traitsILi96ELi64ELi128ELi8ELi2ELi4ELi4ELb1ELb0EN7cutlass10bfloat16_tE19Flash_kernel_traitsILi96ELi64ELi128ELi8ES2_EELb1ELb1ELb0ELb1EEv16Flash_bwd_params
    0 bytes stack frame, 0 bytes spill stores, 0 bytes spill loads
ptxas info    : Used 255 registers, 696 bytes cmem[0]
ptxas info    : Compiling entry function '_Z44flash_bwd_dq_dk_dv_loop_seqk_parallel_kernelI23Flash_bwd_kernel_traitsILi96ELi64ELi128ELi8ELi2ELi4ELi4ELb1ELb0EN7cutlass10bfloat16_tE19Flash_kernel_traitsILi96ELi64ELi128ELi8ES2_EELb1ELb1ELb1ELb0EEv16Flash_bwd_params' for 'sm_80'
ptxas info    : Function properties for _Z44flash_bwd_dq_dk_dv_loop_seqk_parallel_kernelI23Flash_bwd_kernel_traitsILi96ELi64ELi128ELi8ELi2ELi4ELi4ELb1ELb0EN7cutlass10bfloat16_tE19Flash_kernel_traitsILi96ELi64ELi128ELi8ES2_EELb1ELb1ELb1ELb0EEv16Flash_bwd_params
    0 bytes stack frame, 0 bytes spill stores, 0 bytes spill loads
ptxas info    : Used 255 registers, 696 bytes cmem[0]
ptxas info    : Compiling entry function '_Z44flash_bwd_dq_dk_dv_loop_seqk_parallel_kernelI23Flash_bwd_kernel_traitsILi96ELi64ELi128ELi8ELi2ELi4ELi4ELb1ELb0EN7cutlass10bfloat16_tE19Flash_kernel_traitsILi96ELi64ELi128ELi8ES2_EELb1ELb1ELb1ELb1EEv16Flash_bwd_params' for 'sm_80'
ptxas info    : Function properties for _Z44flash_bwd_dq_dk_dv_loop_seqk_parallel_kernelI23Flash_bwd_kernel_traitsILi96ELi64ELi128ELi8ELi2ELi4ELi4ELb1ELb0EN7cutlass10bfloat16_tE19Flash_kernel_traitsILi96ELi64ELi128ELi8ES2_EELb1ELb1ELb1ELb1EEv16Flash_bwd_params
    0 bytes stack frame, 0 bytes spill stores, 0 bytes spill loads
ptxas info    : Used 255 registers, 696 bytes cmem[0]
ptxas info    : Compiling entry function '_Z25flash_bwd_dot_do_o_kernelILb1E23Flash_bwd_kernel_traitsILi96ELi64ELi128ELi8ELi2ELi4ELi4ELb1ELb0EN7cutlass10bfloat16_tE19Flash_kernel_traitsILi96ELi64ELi128ELi8ES2_EEEv16Flash_bwd_params' for 'sm_80'
ptxas info    : Function properties for _Z25flash_bwd_dot_do_o_kernelILb1E23Flash_bwd_kernel_traitsILi96ELi64ELi128ELi8ELi2ELi4ELi4ELb1ELb0EN7cutlass10bfloat16_tE19Flash_kernel_traitsILi96ELi64ELi128ELi8ES2_EEEv16Flash_bwd_params
    0 bytes stack frame, 0 bytes spill stores, 0 bytes spill loads
ptxas info    : Used 44 registers, 696 bytes cmem[0]
ptxas info    : Compiling entry function '_Z27flash_bwd_convert_dq_kernelI23Flash_bwd_kernel_traitsILi96ELi64ELi128ELi8ELi2ELi4ELi4ELb0ELb0EN7cutlass10bfloat16_tE19Flash_kernel_traitsILi96ELi64ELi128ELi8ES2_EEEv16Flash_bwd_params' for 'sm_80'
ptxas info    : Function properties for _Z27flash_bwd_convert_dq_kernelI23Flash_bwd_kernel_traitsILi96ELi64ELi128ELi8ELi2ELi4ELi4ELb0ELb0EN7cutlass10bfloat16_tE19Flash_kernel_traitsILi96ELi64ELi128ELi8ES2_EEEv16Flash_bwd_params
    0 bytes stack frame, 0 bytes spill stores, 0 bytes spill loads
ptxas info    : Used 32 registers, 696 bytes cmem[0]
ptxas info    : Compiling entry function '_Z44flash_bwd_dq_dk_dv_loop_seqk_parallel_kernelI23Flash_bwd_kernel_traitsILi96ELi64ELi128ELi8ELi2ELi4ELi4ELb0ELb0EN7cutlass10bfloat16_tE19Flash_kernel_traitsILi96ELi64ELi128ELi8ES2_EELb1ELb0ELb0ELb0EEv16Flash_bwd_params' for 'sm_80'
ptxas info    : Function properties for _Z44flash_bwd_dq_dk_dv_loop_seqk_parallel_kernelI23Flash_bwd_kernel_traitsILi96ELi64ELi128ELi8ELi2ELi4ELi4ELb0ELb0EN7cutlass10bfloat16_tE19Flash_kernel_traitsILi96ELi64ELi128ELi8ES2_EELb1ELb0ELb0ELb0EEv16Flash_bwd_params
    0 bytes stack frame, 0 bytes spill stores, 0 bytes spill loads
ptxas info    : Used 238 registers, 696 bytes cmem[0]
ptxas info    : Compiling entry function '_Z44flash_bwd_dq_dk_dv_loop_seqk_parallel_kernelI23Flash_bwd_kernel_traitsILi96ELi64ELi128ELi8ELi2ELi4ELi4ELb0ELb0EN7cutlass10bfloat16_tE19Flash_kernel_traitsILi96ELi64ELi128ELi8ES2_EELb1ELb0ELb0ELb1EEv16Flash_bwd_params' for 'sm_80'
ptxas info    : Function properties for _Z44flash_bwd_dq_dk_dv_loop_seqk_parallel_kernelI23Flash_bwd_kernel_traitsILi96ELi64ELi128ELi8ELi2ELi4ELi4ELb0ELb0EN7cutlass10bfloat16_tE19Flash_kernel_traitsILi96ELi64ELi128ELi8ES2_EELb1ELb0ELb0ELb1EEv16Flash_bwd_params
    0 bytes stack frame, 0 bytes spill stores, 0 bytes spill loads
ptxas info    : Used 238 registers, 696 bytes cmem[0]
ptxas info    : Compiling entry function '_Z44flash_bwd_dq_dk_dv_loop_seqk_parallel_kernelI23Flash_bwd_kernel_traitsILi96ELi64ELi128ELi8ELi2ELi4ELi4ELb0ELb0EN7cutlass10bfloat16_tE19Flash_kernel_traitsILi96ELi64ELi128ELi8ES2_EELb1ELb0ELb1ELb0EEv16Flash_bwd_params' for 'sm_80'
ptxas info    : Function properties for _Z44flash_bwd_dq_dk_dv_loop_seqk_parallel_kernelI23Flash_bwd_kernel_traitsILi96ELi64ELi128ELi8ELi2ELi4ELi4ELb0ELb0EN7cutlass10bfloat16_tE19Flash_kernel_traitsILi96ELi64ELi128ELi8ES2_EELb1ELb0ELb1ELb0EEv16Flash_bwd_params
    0 bytes stack frame, 0 bytes spill stores, 0 bytes spill loads
ptxas info    : Used 238 registers, 696 bytes cmem[0]
ptxas info    : Compiling entry function '_Z44flash_bwd_dq_dk_dv_loop_seqk_parallel_kernelI23Flash_bwd_kernel_traitsILi96ELi64ELi128ELi8ELi2ELi4ELi4ELb0ELb0EN7cutlass10bfloat16_tE19Flash_kernel_traitsILi96ELi64ELi128ELi8ES2_EELb1ELb0ELb1ELb1EEv16Flash_bwd_params' for 'sm_80'
ptxas info    : Function properties for _Z44flash_bwd_dq_dk_dv_loop_seqk_parallel_kernelI23Flash_bwd_kernel_traitsILi96ELi64ELi128ELi8ELi2ELi4ELi4ELb0ELb0EN7cutlass10bfloat16_tE19Flash_kernel_traitsILi96ELi64ELi128ELi8ES2_EELb1ELb0ELb1ELb1EEv16Flash_bwd_params
    0 bytes stack frame, 0 bytes spill stores, 0 bytes spill loads
ptxas info    : Used 243 registers, 696 bytes cmem[0]
ptxas info    : Compiling entry function '_Z44flash_bwd_dq_dk_dv_loop_seqk_parallel_kernelI23Flash_bwd_kernel_traitsILi96ELi64ELi128ELi8ELi2ELi4ELi4ELb0ELb0EN7cutlass10bfloat16_tE19Flash_kernel_traitsILi96ELi64ELi128ELi8ES2_EELb1ELb1ELb0ELb0EEv16Flash_bwd_params' for 'sm_80'
ptxas info    : Function properties for _Z44flash_bwd_dq_dk_dv_loop_seqk_parallel_kernelI23Flash_bwd_kernel_traitsILi96ELi64ELi128ELi8ELi2ELi4ELi4ELb0ELb0EN7cutlass10bfloat16_tE19Flash_kernel_traitsILi96ELi64ELi128ELi8ES2_EELb1ELb1ELb0ELb0EEv16Flash_bwd_params
    0 bytes stack frame, 0 bytes spill stores, 0 bytes spill loads
ptxas info    : Used 255 registers, 696 bytes cmem[0]
ptxas info    : Compiling entry function '_Z44flash_bwd_dq_dk_dv_loop_seqk_parallel_kernelI23Flash_bwd_kernel_traitsILi96ELi64ELi128ELi8ELi2ELi4ELi4ELb0ELb0EN7cutlass10bfloat16_tE19Flash_kernel_traitsILi96ELi64ELi128ELi8ES2_EELb1ELb1ELb0ELb1EEv16Flash_bwd_params' for 'sm_80'
ptxas info    : Function properties for _Z44flash_bwd_dq_dk_dv_loop_seqk_parallel_kernelI23Flash_bwd_kernel_traitsILi96ELi64ELi128ELi8ELi2ELi4ELi4ELb0ELb0EN7cutlass10bfloat16_tE19Flash_kernel_traitsILi96ELi64ELi128ELi8ES2_EELb1ELb1ELb0ELb1EEv16Flash_bwd_params
    0 bytes stack frame, 0 bytes spill stores, 0 bytes spill loads
ptxas info    : Used 253 registers, 696 bytes cmem[0]
ptxas info    : Compiling entry function '_Z44flash_bwd_dq_dk_dv_loop_seqk_parallel_kernelI23Flash_bwd_kernel_traitsILi96ELi64ELi128ELi8ELi2ELi4ELi4ELb0ELb0EN7cutlass10bfloat16_tE19Flash_kernel_traitsILi96ELi64ELi128ELi8ES2_EELb1ELb1ELb1ELb0EEv16Flash_bwd_params' for 'sm_80'
ptxas info    : Function properties for _Z44flash_bwd_dq_dk_dv_loop_seqk_parallel_kernelI23Flash_bwd_kernel_traitsILi96ELi64ELi128ELi8ELi2ELi4ELi4ELb0ELb0EN7cutlass10bfloat16_tE19Flash_kernel_traitsILi96ELi64ELi128ELi8ES2_EELb1ELb1ELb1ELb0EEv16Flash_bwd_params
    0 bytes stack frame, 0 bytes spill stores, 0 bytes spill loads
ptxas info    : Used 250 registers, 696 bytes cmem[0]
ptxas info    : Compiling entry function '_Z44flash_bwd_dq_dk_dv_loop_seqk_parallel_kernelI23Flash_bwd_kernel_traitsILi96ELi64ELi128ELi8ELi2ELi4ELi4ELb0ELb0EN7cutlass10bfloat16_tE19Flash_kernel_traitsILi96ELi64ELi128ELi8ES2_EELb1ELb1ELb1ELb1EEv16Flash_bwd_params' for 'sm_80'
ptxas info    : Function properties for _Z44flash_bwd_dq_dk_dv_loop_seqk_parallel_kernelI23Flash_bwd_kernel_traitsILi96ELi64ELi128ELi8ELi2ELi4ELi4ELb0ELb0EN7cutlass10bfloat16_tE19Flash_kernel_traitsILi96ELi64ELi128ELi8ES2_EELb1ELb1ELb1ELb1EEv16Flash_bwd_params
    0 bytes stack frame, 0 bytes spill stores, 0 bytes spill loads
ptxas info    : Used 245 registers, 696 bytes cmem[0]
ptxas info    : Compiling entry function '_Z25flash_bwd_dot_do_o_kernelILb1E23Flash_bwd_kernel_traitsILi96ELi64ELi128ELi8ELi2ELi4ELi4ELb0ELb0EN7cutlass10bfloat16_tE19Flash_kernel_traitsILi96ELi64ELi128ELi8ES2_EEEv16Flash_bwd_params' for 'sm_80'
ptxas info    : Function properties for _Z25flash_bwd_dot_do_o_kernelILb1E23Flash_bwd_kernel_traitsILi96ELi64ELi128ELi8ELi2ELi4ELi4ELb0ELb0EN7cutlass10bfloat16_tE19Flash_kernel_traitsILi96ELi64ELi128ELi8ES2_EEEv16Flash_bwd_params
    0 bytes stack frame, 0 bytes spill stores, 0 bytes spill loads
ptxas info    : Used 44 registers, 696 bytes cmem[0]
/anaconda/envs/flashattention/lib/python3.9/site-packages/torch/include/c10/core/SymInt.h(84): warning: integer conversion resulted in a change of sign

[12/36] /usr/local/cuda-11.4/bin/nvcc  -I/home/v-feiychen/flash-attention/csrc/flash_attn -I/home/v-feiychen/flash-attention/csrc/flash_attn/src -I/home/v-feiychen/flash-attention/csrc/cutlass/include -I/anaconda/envs/flashattention/lib/python3.9/site-packages/torch/include -I/anaconda/envs/flashattention/lib/python3.9/site-packages/torch/include/torch/csrc/api/include -I/anaconda/envs/flashattention/lib/python3.9/site-packages/torch/include/TH -I/anaconda/envs/flashattention/lib/python3.9/site-packages/torch/include/THC -I/usr/local/cuda-11.4/include -I/anaconda/envs/flashattention/include/python3.9 -c -c /home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_hdim128_bf16_sm80.cu -o /home/v-feiychen/flash-attention/build/temp.linux-x86_64-cpython-39/csrc/flash_attn/src/flash_fwd_hdim128_bf16_sm80.o -D__CUDA_NO_HALF_OPERATORS__ -D__CUDA_NO_HALF_CONVERSIONS__ -D__CUDA_NO_BFLOAT16_CONVERSIONS__ -D__CUDA_NO_HALF2_OPERATORS__ --expt-relaxed-constexpr --compiler-options ''"'"'-fPIC'"'"'' -O3 -std=c++17 -U__CUDA_NO_HALF_OPERATORS__ -U__CUDA_NO_HALF_CONVERSIONS__ -U__CUDA_NO_HALF2_OPERATORS__ -U__CUDA_NO_BFLOAT16_CONVERSIONS__ --expt-relaxed-constexpr --expt-extended-lambda --use_fast_math --ptxas-options=-v -lineinfo -gencode arch=compute_80,code=sm_80 --threads 4 -DTORCH_API_INCLUDE_EXTENSION_H '-DPYBIND11_COMPILER_TYPE="_gcc"' '-DPYBIND11_STDLIB="_libstdcpp"' '-DPYBIND11_BUILD_ABI="_cxxabi1011"' -DTORCH_EXTENSION_NAME=flash_attn_2_cuda -D_GLIBCXX_USE_CXX11_ABI=0
/anaconda/envs/flashattention/lib/python3.9/site-packages/torch/include/c10/core/SymInt.h(84): warning: integer conversion resulted in a change of sign

/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_kernel.h(140): warning: variable "MMA_M" was declared but never referenced
          detected during:
            instantiation of "void flash::compute_attn_1rowblock<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &, int, int, int) [with Kernel_traits=Flash_fwd_kernel_traits<128, 128, 32, 4, false, false, cutlass::bfloat16_t, Flash_kernel_traits<128, 128, 32, 4, cutlass::bfloat16_t>>, Is_dropout=true, Is_causal=true, Is_even_N=true, Is_even_K=true, Return_softmax=true, Params=Flash_fwd_params]" 
(588): here
            instantiation of "void flash::compute_attn<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &) [with Kernel_traits=Flash_fwd_kernel_traits<128, 128, 32, 4, false, false, cutlass::bfloat16_t, Flash_kernel_traits<128, 128, 32, 4, cutlass::bfloat16_t>>, Is_dropout=true, Is_causal=true, Is_even_N=true, Is_even_K=true, Return_softmax=true, Params=Flash_fwd_params]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(15): here
            instantiation of "void flash_fwd_kernel<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax>(Flash_fwd_params) [with Kernel_traits=Flash_fwd_kernel_traits<128, 128, 32, 4, false, false, cutlass::bfloat16_t, Flash_kernel_traits<128, 128, 32, 4, cutlass::bfloat16_t>>, Is_dropout=true, Is_causal=true, Is_even_N=true, Is_even_K=true, Return_softmax=true]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(34): here
            instantiation of "void run_flash_fwd<Kernel_traits,Is_dropout,Is_causal>(Flash_fwd_params &, cudaStream_t) [with Kernel_traits=Flash_fwd_kernel_traits<128, 128, 32, 4, false, false, cutlass::bfloat16_t, Flash_kernel_traits<128, 128, 32, 4, cutlass::bfloat16_t>>, Is_dropout=true, Is_causal=true]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(118): here
            instantiation of "void run_mha_fwd_hdim128<T>(Flash_fwd_params &, cudaStream_t) [with T=cutlass::bfloat16_t]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_hdim128_bf16_sm80.cu(18): here

/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_kernel.h(275): warning: variable "tQrQ" was declared but never referenced
          detected during:
            instantiation of "void flash::compute_attn_1rowblock<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &, int, int, int) [with Kernel_traits=Flash_fwd_kernel_traits<128, 128, 32, 4, false, false, cutlass::bfloat16_t, Flash_kernel_traits<128, 128, 32, 4, cutlass::bfloat16_t>>, Is_dropout=true, Is_causal=true, Is_even_N=true, Is_even_K=true, Return_softmax=true, Params=Flash_fwd_params]" 
(588): here
            instantiation of "void flash::compute_attn<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &) [with Kernel_traits=Flash_fwd_kernel_traits<128, 128, 32, 4, false, false, cutlass::bfloat16_t, Flash_kernel_traits<128, 128, 32, 4, cutlass::bfloat16_t>>, Is_dropout=true, Is_causal=true, Is_even_N=true, Is_even_K=true, Return_softmax=true, Params=Flash_fwd_params]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(15): here
            instantiation of "void flash_fwd_kernel<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax>(Flash_fwd_params) [with Kernel_traits=Flash_fwd_kernel_traits<128, 128, 32, 4, false, false, cutlass::bfloat16_t, Flash_kernel_traits<128, 128, 32, 4, cutlass::bfloat16_t>>, Is_dropout=true, Is_causal=true, Is_even_N=true, Is_even_K=true, Return_softmax=true]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(34): here
            instantiation of "void run_flash_fwd<Kernel_traits,Is_dropout,Is_causal>(Flash_fwd_params &, cudaStream_t) [with Kernel_traits=Flash_fwd_kernel_traits<128, 128, 32, 4, false, false, cutlass::bfloat16_t, Flash_kernel_traits<128, 128, 32, 4, cutlass::bfloat16_t>>, Is_dropout=true, Is_causal=true]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(118): here
            instantiation of "void run_mha_fwd_hdim128<T>(Flash_fwd_params &, cudaStream_t) [with T=cutlass::bfloat16_t]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_hdim128_bf16_sm80.cu(18): here

/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_kernel.h(140): warning: variable "MMA_M" was declared but never referenced
          detected during:
            instantiation of "void flash::compute_attn_1rowblock<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &, int, int, int) [with Kernel_traits=Flash_fwd_kernel_traits<128, 128, 32, 4, false, false, cutlass::bfloat16_t, Flash_kernel_traits<128, 128, 32, 4, cutlass::bfloat16_t>>, Is_dropout=true, Is_causal=true, Is_even_N=true, Is_even_K=true, Return_softmax=false, Params=Flash_fwd_params]" 
(588): here
            instantiation of "void flash::compute_attn<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &) [with Kernel_traits=Flash_fwd_kernel_traits<128, 128, 32, 4, false, false, cutlass::bfloat16_t, Flash_kernel_traits<128, 128, 32, 4, cutlass::bfloat16_t>>, Is_dropout=true, Is_causal=true, Is_even_N=true, Is_even_K=true, Return_softmax=false, Params=Flash_fwd_params]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(15): here
            instantiation of "void flash_fwd_kernel<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax>(Flash_fwd_params) [with Kernel_traits=Flash_fwd_kernel_traits<128, 128, 32, 4, false, false, cutlass::bfloat16_t, Flash_kernel_traits<128, 128, 32, 4, cutlass::bfloat16_t>>, Is_dropout=true, Is_causal=true, Is_even_N=true, Is_even_K=true, Return_softmax=false]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(34): here
            instantiation of "void run_flash_fwd<Kernel_traits,Is_dropout,Is_causal>(Flash_fwd_params &, cudaStream_t) [with Kernel_traits=Flash_fwd_kernel_traits<128, 128, 32, 4, false, false, cutlass::bfloat16_t, Flash_kernel_traits<128, 128, 32, 4, cutlass::bfloat16_t>>, Is_dropout=true, Is_causal=true]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(118): here
            instantiation of "void run_mha_fwd_hdim128<T>(Flash_fwd_params &, cudaStream_t) [with T=cutlass::bfloat16_t]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_hdim128_bf16_sm80.cu(18): here

/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_kernel.h(275): warning: variable "tQrQ" was declared but never referenced
          detected during:
            instantiation of "void flash::compute_attn_1rowblock<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &, int, int, int) [with Kernel_traits=Flash_fwd_kernel_traits<128, 128, 32, 4, false, false, cutlass::bfloat16_t, Flash_kernel_traits<128, 128, 32, 4, cutlass::bfloat16_t>>, Is_dropout=true, Is_causal=true, Is_even_N=true, Is_even_K=true, Return_softmax=false, Params=Flash_fwd_params]" 
(588): here
            instantiation of "void flash::compute_attn<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &) [with Kernel_traits=Flash_fwd_kernel_traits<128, 128, 32, 4, false, false, cutlass::bfloat16_t, Flash_kernel_traits<128, 128, 32, 4, cutlass::bfloat16_t>>, Is_dropout=true, Is_causal=true, Is_even_N=true, Is_even_K=true, Return_softmax=false, Params=Flash_fwd_params]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(15): here
            instantiation of "void flash_fwd_kernel<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax>(Flash_fwd_params) [with Kernel_traits=Flash_fwd_kernel_traits<128, 128, 32, 4, false, false, cutlass::bfloat16_t, Flash_kernel_traits<128, 128, 32, 4, cutlass::bfloat16_t>>, Is_dropout=true, Is_causal=true, Is_even_N=true, Is_even_K=true, Return_softmax=false]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(34): here
            instantiation of "void run_flash_fwd<Kernel_traits,Is_dropout,Is_causal>(Flash_fwd_params &, cudaStream_t) [with Kernel_traits=Flash_fwd_kernel_traits<128, 128, 32, 4, false, false, cutlass::bfloat16_t, Flash_kernel_traits<128, 128, 32, 4, cutlass::bfloat16_t>>, Is_dropout=true, Is_causal=true]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(118): here
            instantiation of "void run_mha_fwd_hdim128<T>(Flash_fwd_params &, cudaStream_t) [with T=cutlass::bfloat16_t]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_hdim128_bf16_sm80.cu(18): here

/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_kernel.h(140): warning: variable "MMA_M" was declared but never referenced
          detected during:
            instantiation of "void flash::compute_attn_1rowblock<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &, int, int, int) [with Kernel_traits=Flash_fwd_kernel_traits<128, 128, 32, 4, false, false, cutlass::bfloat16_t, Flash_kernel_traits<128, 128, 32, 4, cutlass::bfloat16_t>>, Is_dropout=true, Is_causal=true, Is_even_N=true, Is_even_K=false, Return_softmax=true, Params=Flash_fwd_params]" 
(588): here
            instantiation of "void flash::compute_attn<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &) [with Kernel_traits=Flash_fwd_kernel_traits<128, 128, 32, 4, false, false, cutlass::bfloat16_t, Flash_kernel_traits<128, 128, 32, 4, cutlass::bfloat16_t>>, Is_dropout=true, Is_causal=true, Is_even_N=true, Is_even_K=false, Return_softmax=true, Params=Flash_fwd_params]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(15): here
            instantiation of "void flash_fwd_kernel<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax>(Flash_fwd_params) [with Kernel_traits=Flash_fwd_kernel_traits<128, 128, 32, 4, false, false, cutlass::bfloat16_t, Flash_kernel_traits<128, 128, 32, 4, cutlass::bfloat16_t>>, Is_dropout=true, Is_causal=true, Is_even_N=true, Is_even_K=false, Return_softmax=true]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(34): here
            instantiation of "void run_flash_fwd<Kernel_traits,Is_dropout,Is_causal>(Flash_fwd_params &, cudaStream_t) [with Kernel_traits=Flash_fwd_kernel_traits<128, 128, 32, 4, false, false, cutlass::bfloat16_t, Flash_kernel_traits<128, 128, 32, 4, cutlass::bfloat16_t>>, Is_dropout=true, Is_causal=true]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(118): here
            instantiation of "void run_mha_fwd_hdim128<T>(Flash_fwd_params &, cudaStream_t) [with T=cutlass::bfloat16_t]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_hdim128_bf16_sm80.cu(18): here

/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_kernel.h(275): warning: variable "tQrQ" was declared but never referenced
          detected during:
            instantiation of "void flash::compute_attn_1rowblock<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &, int, int, int) [with Kernel_traits=Flash_fwd_kernel_traits<128, 128, 32, 4, false, false, cutlass::bfloat16_t, Flash_kernel_traits<128, 128, 32, 4, cutlass::bfloat16_t>>, Is_dropout=true, Is_causal=true, Is_even_N=true, Is_even_K=false, Return_softmax=true, Params=Flash_fwd_params]" 
(588): here
            instantiation of "void flash::compute_attn<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &) [with Kernel_traits=Flash_fwd_kernel_traits<128, 128, 32, 4, false, false, cutlass::bfloat16_t, Flash_kernel_traits<128, 128, 32, 4, cutlass::bfloat16_t>>, Is_dropout=true, Is_causal=true, Is_even_N=true, Is_even_K=false, Return_softmax=true, Params=Flash_fwd_params]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(15): here
            instantiation of "void flash_fwd_kernel<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax>(Flash_fwd_params) [with Kernel_traits=Flash_fwd_kernel_traits<128, 128, 32, 4, false, false, cutlass::bfloat16_t, Flash_kernel_traits<128, 128, 32, 4, cutlass::bfloat16_t>>, Is_dropout=true, Is_causal=true, Is_even_N=true, Is_even_K=false, Return_softmax=true]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(34): here
            instantiation of "void run_flash_fwd<Kernel_traits,Is_dropout,Is_causal>(Flash_fwd_params &, cudaStream_t) [with Kernel_traits=Flash_fwd_kernel_traits<128, 128, 32, 4, false, false, cutlass::bfloat16_t, Flash_kernel_traits<128, 128, 32, 4, cutlass::bfloat16_t>>, Is_dropout=true, Is_causal=true]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(118): here
            instantiation of "void run_mha_fwd_hdim128<T>(Flash_fwd_params &, cudaStream_t) [with T=cutlass::bfloat16_t]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_hdim128_bf16_sm80.cu(18): here

/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_kernel.h(140): warning: variable "MMA_M" was declared but never referenced
          detected during:
            instantiation of "void flash::compute_attn_1rowblock<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &, int, int, int) [with Kernel_traits=Flash_fwd_kernel_traits<128, 128, 32, 4, false, false, cutlass::bfloat16_t, Flash_kernel_traits<128, 128, 32, 4, cutlass::bfloat16_t>>, Is_dropout=true, Is_causal=true, Is_even_N=true, Is_even_K=false, Return_softmax=false, Params=Flash_fwd_params]" 
(588): here
            instantiation of "void flash::compute_attn<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &) [with Kernel_traits=Flash_fwd_kernel_traits<128, 128, 32, 4, false, false, cutlass::bfloat16_t, Flash_kernel_traits<128, 128, 32, 4, cutlass::bfloat16_t>>, Is_dropout=true, Is_causal=true, Is_even_N=true, Is_even_K=false, Return_softmax=false, Params=Flash_fwd_params]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(15): here
            instantiation of "void flash_fwd_kernel<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax>(Flash_fwd_params) [with Kernel_traits=Flash_fwd_kernel_traits<128, 128, 32, 4, false, false, cutlass::bfloat16_t, Flash_kernel_traits<128, 128, 32, 4, cutlass::bfloat16_t>>, Is_dropout=true, Is_causal=true, Is_even_N=true, Is_even_K=false, Return_softmax=false]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(34): here
            instantiation of "void run_flash_fwd<Kernel_traits,Is_dropout,Is_causal>(Flash_fwd_params &, cudaStream_t) [with Kernel_traits=Flash_fwd_kernel_traits<128, 128, 32, 4, false, false, cutlass::bfloat16_t, Flash_kernel_traits<128, 128, 32, 4, cutlass::bfloat16_t>>, Is_dropout=true, Is_causal=true]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(118): here
            instantiation of "void run_mha_fwd_hdim128<T>(Flash_fwd_params &, cudaStream_t) [with T=cutlass::bfloat16_t]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_hdim128_bf16_sm80.cu(18): here

/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_kernel.h(275): warning: variable "tQrQ" was declared but never referenced
          detected during:
            instantiation of "void flash::compute_attn_1rowblock<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &, int, int, int) [with Kernel_traits=Flash_fwd_kernel_traits<128, 128, 32, 4, false, false, cutlass::bfloat16_t, Flash_kernel_traits<128, 128, 32, 4, cutlass::bfloat16_t>>, Is_dropout=true, Is_causal=true, Is_even_N=true, Is_even_K=false, Return_softmax=false, Params=Flash_fwd_params]" 
(588): here
            instantiation of "void flash::compute_attn<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &) [with Kernel_traits=Flash_fwd_kernel_traits<128, 128, 32, 4, false, false, cutlass::bfloat16_t, Flash_kernel_traits<128, 128, 32, 4, cutlass::bfloat16_t>>, Is_dropout=true, Is_causal=true, Is_even_N=true, Is_even_K=false, Return_softmax=false, Params=Flash_fwd_params]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(15): here
            instantiation of "void flash_fwd_kernel<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax>(Flash_fwd_params) [with Kernel_traits=Flash_fwd_kernel_traits<128, 128, 32, 4, false, false, cutlass::bfloat16_t, Flash_kernel_traits<128, 128, 32, 4, cutlass::bfloat16_t>>, Is_dropout=true, Is_causal=true, Is_even_N=true, Is_even_K=false, Return_softmax=false]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(34): here
            instantiation of "void run_flash_fwd<Kernel_traits,Is_dropout,Is_causal>(Flash_fwd_params &, cudaStream_t) [with Kernel_traits=Flash_fwd_kernel_traits<128, 128, 32, 4, false, false, cutlass::bfloat16_t, Flash_kernel_traits<128, 128, 32, 4, cutlass::bfloat16_t>>, Is_dropout=true, Is_causal=true]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(118): here
            instantiation of "void run_mha_fwd_hdim128<T>(Flash_fwd_params &, cudaStream_t) [with T=cutlass::bfloat16_t]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_hdim128_bf16_sm80.cu(18): here

/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_kernel.h(140): warning: variable "MMA_M" was declared but never referenced
          detected during:
            instantiation of "void flash::compute_attn_1rowblock<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &, int, int, int) [with Kernel_traits=Flash_fwd_kernel_traits<128, 128, 32, 4, false, false, cutlass::bfloat16_t, Flash_kernel_traits<128, 128, 32, 4, cutlass::bfloat16_t>>, Is_dropout=true, Is_causal=true, Is_even_N=false, Is_even_K=true, Return_softmax=true, Params=Flash_fwd_params]" 
(588): here
            instantiation of "void flash::compute_attn<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &) [with Kernel_traits=Flash_fwd_kernel_traits<128, 128, 32, 4, false, false, cutlass::bfloat16_t, Flash_kernel_traits<128, 128, 32, 4, cutlass::bfloat16_t>>, Is_dropout=true, Is_causal=true, Is_even_N=false, Is_even_K=true, Return_softmax=true, Params=Flash_fwd_params]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(15): here
            instantiation of "void flash_fwd_kernel<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax>(Flash_fwd_params) [with Kernel_traits=Flash_fwd_kernel_traits<128, 128, 32, 4, false, false, cutlass::bfloat16_t, Flash_kernel_traits<128, 128, 32, 4, cutlass::bfloat16_t>>, Is_dropout=true, Is_causal=true, Is_even_N=false, Is_even_K=true, Return_softmax=true]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(34): here
            instantiation of "void run_flash_fwd<Kernel_traits,Is_dropout,Is_causal>(Flash_fwd_params &, cudaStream_t) [with Kernel_traits=Flash_fwd_kernel_traits<128, 128, 32, 4, false, false, cutlass::bfloat16_t, Flash_kernel_traits<128, 128, 32, 4, cutlass::bfloat16_t>>, Is_dropout=true, Is_causal=true]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(118): here
            instantiation of "void run_mha_fwd_hdim128<T>(Flash_fwd_params &, cudaStream_t) [with T=cutlass::bfloat16_t]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_hdim128_bf16_sm80.cu(18): here

/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_kernel.h(275): warning: variable "tQrQ" was declared but never referenced
          detected during:
            instantiation of "void flash::compute_attn_1rowblock<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &, int, int, int) [with Kernel_traits=Flash_fwd_kernel_traits<128, 128, 32, 4, false, false, cutlass::bfloat16_t, Flash_kernel_traits<128, 128, 32, 4, cutlass::bfloat16_t>>, Is_dropout=true, Is_causal=true, Is_even_N=false, Is_even_K=true, Return_softmax=true, Params=Flash_fwd_params]" 
(588): here
            instantiation of "void flash::compute_attn<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &) [with Kernel_traits=Flash_fwd_kernel_traits<128, 128, 32, 4, false, false, cutlass::bfloat16_t, Flash_kernel_traits<128, 128, 32, 4, cutlass::bfloat16_t>>, Is_dropout=true, Is_causal=true, Is_even_N=false, Is_even_K=true, Return_softmax=true, Params=Flash_fwd_params]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(15): here
            instantiation of "void flash_fwd_kernel<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax>(Flash_fwd_params) [with Kernel_traits=Flash_fwd_kernel_traits<128, 128, 32, 4, false, false, cutlass::bfloat16_t, Flash_kernel_traits<128, 128, 32, 4, cutlass::bfloat16_t>>, Is_dropout=true, Is_causal=true, Is_even_N=false, Is_even_K=true, Return_softmax=true]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(34): here
            instantiation of "void run_flash_fwd<Kernel_traits,Is_dropout,Is_causal>(Flash_fwd_params &, cudaStream_t) [with Kernel_traits=Flash_fwd_kernel_traits<128, 128, 32, 4, false, false, cutlass::bfloat16_t, Flash_kernel_traits<128, 128, 32, 4, cutlass::bfloat16_t>>, Is_dropout=true, Is_causal=true]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(118): here
            instantiation of "void run_mha_fwd_hdim128<T>(Flash_fwd_params &, cudaStream_t) [with T=cutlass::bfloat16_t]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_hdim128_bf16_sm80.cu(18): here

/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_kernel.h(140): warning: variable "MMA_M" was declared but never referenced
          detected during:
            instantiation of "void flash::compute_attn_1rowblock<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &, int, int, int) [with Kernel_traits=Flash_fwd_kernel_traits<128, 128, 32, 4, false, false, cutlass::bfloat16_t, Flash_kernel_traits<128, 128, 32, 4, cutlass::bfloat16_t>>, Is_dropout=true, Is_causal=true, Is_even_N=false, Is_even_K=true, Return_softmax=false, Params=Flash_fwd_params]" 
(588): here
            instantiation of "void flash::compute_attn<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &) [with Kernel_traits=Flash_fwd_kernel_traits<128, 128, 32, 4, false, false, cutlass::bfloat16_t, Flash_kernel_traits<128, 128, 32, 4, cutlass::bfloat16_t>>, Is_dropout=true, Is_causal=true, Is_even_N=false, Is_even_K=true, Return_softmax=false, Params=Flash_fwd_params]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(15): here
            instantiation of "void flash_fwd_kernel<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax>(Flash_fwd_params) [with Kernel_traits=Flash_fwd_kernel_traits<128, 128, 32, 4, false, false, cutlass::bfloat16_t, Flash_kernel_traits<128, 128, 32, 4, cutlass::bfloat16_t>>, Is_dropout=true, Is_causal=true, Is_even_N=false, Is_even_K=true, Return_softmax=false]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(34): here
            instantiation of "void run_flash_fwd<Kernel_traits,Is_dropout,Is_causal>(Flash_fwd_params &, cudaStream_t) [with Kernel_traits=Flash_fwd_kernel_traits<128, 128, 32, 4, false, false, cutlass::bfloat16_t, Flash_kernel_traits<128, 128, 32, 4, cutlass::bfloat16_t>>, Is_dropout=true, Is_causal=true]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(118): here
            instantiation of "void run_mha_fwd_hdim128<T>(Flash_fwd_params &, cudaStream_t) [with T=cutlass::bfloat16_t]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_hdim128_bf16_sm80.cu(18): here

/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_kernel.h(275): warning: variable "tQrQ" was declared but never referenced
          detected during:
            instantiation of "void flash::compute_attn_1rowblock<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &, int, int, int) [with Kernel_traits=Flash_fwd_kernel_traits<128, 128, 32, 4, false, false, cutlass::bfloat16_t, Flash_kernel_traits<128, 128, 32, 4, cutlass::bfloat16_t>>, Is_dropout=true, Is_causal=true, Is_even_N=false, Is_even_K=true, Return_softmax=false, Params=Flash_fwd_params]" 
(588): here
            instantiation of "void flash::compute_attn<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &) [with Kernel_traits=Flash_fwd_kernel_traits<128, 128, 32, 4, false, false, cutlass::bfloat16_t, Flash_kernel_traits<128, 128, 32, 4, cutlass::bfloat16_t>>, Is_dropout=true, Is_causal=true, Is_even_N=false, Is_even_K=true, Return_softmax=false, Params=Flash_fwd_params]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(15): here
            instantiation of "void flash_fwd_kernel<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax>(Flash_fwd_params) [with Kernel_traits=Flash_fwd_kernel_traits<128, 128, 32, 4, false, false, cutlass::bfloat16_t, Flash_kernel_traits<128, 128, 32, 4, cutlass::bfloat16_t>>, Is_dropout=true, Is_causal=true, Is_even_N=false, Is_even_K=true, Return_softmax=false]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(34): here
            instantiation of "void run_flash_fwd<Kernel_traits,Is_dropout,Is_causal>(Flash_fwd_params &, cudaStream_t) [with Kernel_traits=Flash_fwd_kernel_traits<128, 128, 32, 4, false, false, cutlass::bfloat16_t, Flash_kernel_traits<128, 128, 32, 4, cutlass::bfloat16_t>>, Is_dropout=true, Is_causal=true]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(118): here
            instantiation of "void run_mha_fwd_hdim128<T>(Flash_fwd_params &, cudaStream_t) [with T=cutlass::bfloat16_t]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_hdim128_bf16_sm80.cu(18): here

/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_kernel.h(140): warning: variable "MMA_M" was declared but never referenced
          detected during:
            instantiation of "void flash::compute_attn_1rowblock<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &, int, int, int) [with Kernel_traits=Flash_fwd_kernel_traits<128, 128, 32, 4, false, false, cutlass::bfloat16_t, Flash_kernel_traits<128, 128, 32, 4, cutlass::bfloat16_t>>, Is_dropout=true, Is_causal=true, Is_even_N=false, Is_even_K=false, Return_softmax=true, Params=Flash_fwd_params]" 
(588): here
            instantiation of "void flash::compute_attn<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &) [with Kernel_traits=Flash_fwd_kernel_traits<128, 128, 32, 4, false, false, cutlass::bfloat16_t, Flash_kernel_traits<128, 128, 32, 4, cutlass::bfloat16_t>>, Is_dropout=true, Is_causal=true, Is_even_N=false, Is_even_K=false, Return_softmax=true, Params=Flash_fwd_params]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(15): here
            instantiation of "void flash_fwd_kernel<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax>(Flash_fwd_params) [with Kernel_traits=Flash_fwd_kernel_traits<128, 128, 32, 4, false, false, cutlass::bfloat16_t, Flash_kernel_traits<128, 128, 32, 4, cutlass::bfloat16_t>>, Is_dropout=true, Is_causal=true, Is_even_N=false, Is_even_K=false, Return_softmax=true]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(34): here
            instantiation of "void run_flash_fwd<Kernel_traits,Is_dropout,Is_causal>(Flash_fwd_params &, cudaStream_t) [with Kernel_traits=Flash_fwd_kernel_traits<128, 128, 32, 4, false, false, cutlass::bfloat16_t, Flash_kernel_traits<128, 128, 32, 4, cutlass::bfloat16_t>>, Is_dropout=true, Is_causal=true]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(118): here
            instantiation of "void run_mha_fwd_hdim128<T>(Flash_fwd_params &, cudaStream_t) [with T=cutlass::bfloat16_t]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_hdim128_bf16_sm80.cu(18): here

/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_kernel.h(275): warning: variable "tQrQ" was declared but never referenced
          detected during:
            instantiation of "void flash::compute_attn_1rowblock<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &, int, int, int) [with Kernel_traits=Flash_fwd_kernel_traits<128, 128, 32, 4, false, false, cutlass::bfloat16_t, Flash_kernel_traits<128, 128, 32, 4, cutlass::bfloat16_t>>, Is_dropout=true, Is_causal=true, Is_even_N=false, Is_even_K=false, Return_softmax=true, Params=Flash_fwd_params]" 
(588): here
            instantiation of "void flash::compute_attn<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &) [with Kernel_traits=Flash_fwd_kernel_traits<128, 128, 32, 4, false, false, cutlass::bfloat16_t, Flash_kernel_traits<128, 128, 32, 4, cutlass::bfloat16_t>>, Is_dropout=true, Is_causal=true, Is_even_N=false, Is_even_K=false, Return_softmax=true, Params=Flash_fwd_params]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(15): here
            instantiation of "void flash_fwd_kernel<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax>(Flash_fwd_params) [with Kernel_traits=Flash_fwd_kernel_traits<128, 128, 32, 4, false, false, cutlass::bfloat16_t, Flash_kernel_traits<128, 128, 32, 4, cutlass::bfloat16_t>>, Is_dropout=true, Is_causal=true, Is_even_N=false, Is_even_K=false, Return_softmax=true]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(34): here
            instantiation of "void run_flash_fwd<Kernel_traits,Is_dropout,Is_causal>(Flash_fwd_params &, cudaStream_t) [with Kernel_traits=Flash_fwd_kernel_traits<128, 128, 32, 4, false, false, cutlass::bfloat16_t, Flash_kernel_traits<128, 128, 32, 4, cutlass::bfloat16_t>>, Is_dropout=true, Is_causal=true]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(118): here
            instantiation of "void run_mha_fwd_hdim128<T>(Flash_fwd_params &, cudaStream_t) [with T=cutlass::bfloat16_t]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_hdim128_bf16_sm80.cu(18): here

/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_kernel.h(140): warning: variable "MMA_M" was declared but never referenced
          detected during:
            instantiation of "void flash::compute_attn_1rowblock<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &, int, int, int) [with Kernel_traits=Flash_fwd_kernel_traits<128, 128, 32, 4, false, false, cutlass::bfloat16_t, Flash_kernel_traits<128, 128, 32, 4, cutlass::bfloat16_t>>, Is_dropout=true, Is_causal=true, Is_even_N=false, Is_even_K=false, Return_softmax=false, Params=Flash_fwd_params]" 
(588): here
            instantiation of "void flash::compute_attn<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &) [with Kernel_traits=Flash_fwd_kernel_traits<128, 128, 32, 4, false, false, cutlass::bfloat16_t, Flash_kernel_traits<128, 128, 32, 4, cutlass::bfloat16_t>>, Is_dropout=true, Is_causal=true, Is_even_N=false, Is_even_K=false, Return_softmax=false, Params=Flash_fwd_params]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(15): here
            instantiation of "void flash_fwd_kernel<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax>(Flash_fwd_params) [with Kernel_traits=Flash_fwd_kernel_traits<128, 128, 32, 4, false, false, cutlass::bfloat16_t, Flash_kernel_traits<128, 128, 32, 4, cutlass::bfloat16_t>>, Is_dropout=true, Is_causal=true, Is_even_N=false, Is_even_K=false, Return_softmax=false]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(34): here
            instantiation of "void run_flash_fwd<Kernel_traits,Is_dropout,Is_causal>(Flash_fwd_params &, cudaStream_t) [with Kernel_traits=Flash_fwd_kernel_traits<128, 128, 32, 4, false, false, cutlass::bfloat16_t, Flash_kernel_traits<128, 128, 32, 4, cutlass::bfloat16_t>>, Is_dropout=true, Is_causal=true]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(118): here
            instantiation of "void run_mha_fwd_hdim128<T>(Flash_fwd_params &, cudaStream_t) [with T=cutlass::bfloat16_t]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_hdim128_bf16_sm80.cu(18): here

/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_kernel.h(275): warning: variable "tQrQ" was declared but never referenced
          detected during:
            instantiation of "void flash::compute_attn_1rowblock<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &, int, int, int) [with Kernel_traits=Flash_fwd_kernel_traits<128, 128, 32, 4, false, false, cutlass::bfloat16_t, Flash_kernel_traits<128, 128, 32, 4, cutlass::bfloat16_t>>, Is_dropout=true, Is_causal=true, Is_even_N=false, Is_even_K=false, Return_softmax=false, Params=Flash_fwd_params]" 
(588): here
            instantiation of "void flash::compute_attn<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &) [with Kernel_traits=Flash_fwd_kernel_traits<128, 128, 32, 4, false, false, cutlass::bfloat16_t, Flash_kernel_traits<128, 128, 32, 4, cutlass::bfloat16_t>>, Is_dropout=true, Is_causal=true, Is_even_N=false, Is_even_K=false, Return_softmax=false, Params=Flash_fwd_params]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(15): here
            instantiation of "void flash_fwd_kernel<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax>(Flash_fwd_params) [with Kernel_traits=Flash_fwd_kernel_traits<128, 128, 32, 4, false, false, cutlass::bfloat16_t, Flash_kernel_traits<128, 128, 32, 4, cutlass::bfloat16_t>>, Is_dropout=true, Is_causal=true, Is_even_N=false, Is_even_K=false, Return_softmax=false]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(34): here
            instantiation of "void run_flash_fwd<Kernel_traits,Is_dropout,Is_causal>(Flash_fwd_params &, cudaStream_t) [with Kernel_traits=Flash_fwd_kernel_traits<128, 128, 32, 4, false, false, cutlass::bfloat16_t, Flash_kernel_traits<128, 128, 32, 4, cutlass::bfloat16_t>>, Is_dropout=true, Is_causal=true]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(118): here
            instantiation of "void run_mha_fwd_hdim128<T>(Flash_fwd_params &, cudaStream_t) [with T=cutlass::bfloat16_t]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_hdim128_bf16_sm80.cu(18): here

/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_kernel.h(140): warning: variable "MMA_M" was declared but never referenced
          detected during:
            instantiation of "void flash::compute_attn_1rowblock<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &, int, int, int) [with Kernel_traits=Flash_fwd_kernel_traits<128, 128, 32, 4, false, false, cutlass::bfloat16_t, Flash_kernel_traits<128, 128, 32, 4, cutlass::bfloat16_t>>, Is_dropout=true, Is_causal=false, Is_even_N=true, Is_even_K=true, Return_softmax=true, Params=Flash_fwd_params]" 
(588): here
            instantiation of "void flash::compute_attn<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &) [with Kernel_traits=Flash_fwd_kernel_traits<128, 128, 32, 4, false, false, cutlass::bfloat16_t, Flash_kernel_traits<128, 128, 32, 4, cutlass::bfloat16_t>>, Is_dropout=true, Is_causal=false, Is_even_N=true, Is_even_K=true, Return_softmax=true, Params=Flash_fwd_params]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(15): here
            instantiation of "void flash_fwd_kernel<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax>(Flash_fwd_params) [with Kernel_traits=Flash_fwd_kernel_traits<128, 128, 32, 4, false, false, cutlass::bfloat16_t, Flash_kernel_traits<128, 128, 32, 4, cutlass::bfloat16_t>>, Is_dropout=true, Is_causal=false, Is_even_N=true, Is_even_K=true, Return_softmax=true]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(34): here
            instantiation of "void run_flash_fwd<Kernel_traits,Is_dropout,Is_causal>(Flash_fwd_params &, cudaStream_t) [with Kernel_traits=Flash_fwd_kernel_traits<128, 128, 32, 4, false, false, cutlass::bfloat16_t, Flash_kernel_traits<128, 128, 32, 4, cutlass::bfloat16_t>>, Is_dropout=true, Is_causal=false]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(118): here
            instantiation of "void run_mha_fwd_hdim128<T>(Flash_fwd_params &, cudaStream_t) [with T=cutlass::bfloat16_t]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_hdim128_bf16_sm80.cu(18): here

/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_kernel.h(275): warning: variable "tQrQ" was declared but never referenced
          detected during:
            instantiation of "void flash::compute_attn_1rowblock<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &, int, int, int) [with Kernel_traits=Flash_fwd_kernel_traits<128, 128, 32, 4, false, false, cutlass::bfloat16_t, Flash_kernel_traits<128, 128, 32, 4, cutlass::bfloat16_t>>, Is_dropout=true, Is_causal=false, Is_even_N=true, Is_even_K=true, Return_softmax=true, Params=Flash_fwd_params]" 
(588): here
            instantiation of "void flash::compute_attn<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &) [with Kernel_traits=Flash_fwd_kernel_traits<128, 128, 32, 4, false, false, cutlass::bfloat16_t, Flash_kernel_traits<128, 128, 32, 4, cutlass::bfloat16_t>>, Is_dropout=true, Is_causal=false, Is_even_N=true, Is_even_K=true, Return_softmax=true, Params=Flash_fwd_params]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(15): here
            instantiation of "void flash_fwd_kernel<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax>(Flash_fwd_params) [with Kernel_traits=Flash_fwd_kernel_traits<128, 128, 32, 4, false, false, cutlass::bfloat16_t, Flash_kernel_traits<128, 128, 32, 4, cutlass::bfloat16_t>>, Is_dropout=true, Is_causal=false, Is_even_N=true, Is_even_K=true, Return_softmax=true]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(34): here
            instantiation of "void run_flash_fwd<Kernel_traits,Is_dropout,Is_causal>(Flash_fwd_params &, cudaStream_t) [with Kernel_traits=Flash_fwd_kernel_traits<128, 128, 32, 4, false, false, cutlass::bfloat16_t, Flash_kernel_traits<128, 128, 32, 4, cutlass::bfloat16_t>>, Is_dropout=true, Is_causal=false]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(118): here
            instantiation of "void run_mha_fwd_hdim128<T>(Flash_fwd_params &, cudaStream_t) [with T=cutlass::bfloat16_t]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_hdim128_bf16_sm80.cu(18): here

/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_kernel.h(140): warning: variable "MMA_M" was declared but never referenced
          detected during:
            instantiation of "void flash::compute_attn_1rowblock<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &, int, int, int) [with Kernel_traits=Flash_fwd_kernel_traits<128, 128, 32, 4, false, false, cutlass::bfloat16_t, Flash_kernel_traits<128, 128, 32, 4, cutlass::bfloat16_t>>, Is_dropout=true, Is_causal=false, Is_even_N=true, Is_even_K=true, Return_softmax=false, Params=Flash_fwd_params]" 
(588): here
            instantiation of "void flash::compute_attn<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &) [with Kernel_traits=Flash_fwd_kernel_traits<128, 128, 32, 4, false, false, cutlass::bfloat16_t, Flash_kernel_traits<128, 128, 32, 4, cutlass::bfloat16_t>>, Is_dropout=true, Is_causal=false, Is_even_N=true, Is_even_K=true, Return_softmax=false, Params=Flash_fwd_params]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(15): here
            instantiation of "void flash_fwd_kernel<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax>(Flash_fwd_params) [with Kernel_traits=Flash_fwd_kernel_traits<128, 128, 32, 4, false, false, cutlass::bfloat16_t, Flash_kernel_traits<128, 128, 32, 4, cutlass::bfloat16_t>>, Is_dropout=true, Is_causal=false, Is_even_N=true, Is_even_K=true, Return_softmax=false]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(34): here
            instantiation of "void run_flash_fwd<Kernel_traits,Is_dropout,Is_causal>(Flash_fwd_params &, cudaStream_t) [with Kernel_traits=Flash_fwd_kernel_traits<128, 128, 32, 4, false, false, cutlass::bfloat16_t, Flash_kernel_traits<128, 128, 32, 4, cutlass::bfloat16_t>>, Is_dropout=true, Is_causal=false]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(118): here
            instantiation of "void run_mha_fwd_hdim128<T>(Flash_fwd_params &, cudaStream_t) [with T=cutlass::bfloat16_t]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_hdim128_bf16_sm80.cu(18): here

/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_kernel.h(275): warning: variable "tQrQ" was declared but never referenced
          detected during:
            instantiation of "void flash::compute_attn_1rowblock<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &, int, int, int) [with Kernel_traits=Flash_fwd_kernel_traits<128, 128, 32, 4, false, false, cutlass::bfloat16_t, Flash_kernel_traits<128, 128, 32, 4, cutlass::bfloat16_t>>, Is_dropout=true, Is_causal=false, Is_even_N=true, Is_even_K=true, Return_softmax=false, Params=Flash_fwd_params]" 
(588): here
            instantiation of "void flash::compute_attn<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &) [with Kernel_traits=Flash_fwd_kernel_traits<128, 128, 32, 4, false, false, cutlass::bfloat16_t, Flash_kernel_traits<128, 128, 32, 4, cutlass::bfloat16_t>>, Is_dropout=true, Is_causal=false, Is_even_N=true, Is_even_K=true, Return_softmax=false, Params=Flash_fwd_params]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(15): here
            instantiation of "void flash_fwd_kernel<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax>(Flash_fwd_params) [with Kernel_traits=Flash_fwd_kernel_traits<128, 128, 32, 4, false, false, cutlass::bfloat16_t, Flash_kernel_traits<128, 128, 32, 4, cutlass::bfloat16_t>>, Is_dropout=true, Is_causal=false, Is_even_N=true, Is_even_K=true, Return_softmax=false]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(34): here
            instantiation of "void run_flash_fwd<Kernel_traits,Is_dropout,Is_causal>(Flash_fwd_params &, cudaStream_t) [with Kernel_traits=Flash_fwd_kernel_traits<128, 128, 32, 4, false, false, cutlass::bfloat16_t, Flash_kernel_traits<128, 128, 32, 4, cutlass::bfloat16_t>>, Is_dropout=true, Is_causal=false]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(118): here
            instantiation of "void run_mha_fwd_hdim128<T>(Flash_fwd_params &, cudaStream_t) [with T=cutlass::bfloat16_t]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_hdim128_bf16_sm80.cu(18): here

/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_kernel.h(140): warning: variable "MMA_M" was declared but never referenced
          detected during:
            instantiation of "void flash::compute_attn_1rowblock<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &, int, int, int) [with Kernel_traits=Flash_fwd_kernel_traits<128, 128, 32, 4, false, false, cutlass::bfloat16_t, Flash_kernel_traits<128, 128, 32, 4, cutlass::bfloat16_t>>, Is_dropout=true, Is_causal=false, Is_even_N=true, Is_even_K=false, Return_softmax=true, Params=Flash_fwd_params]" 
(588): here
            instantiation of "void flash::compute_attn<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &) [with Kernel_traits=Flash_fwd_kernel_traits<128, 128, 32, 4, false, false, cutlass::bfloat16_t, Flash_kernel_traits<128, 128, 32, 4, cutlass::bfloat16_t>>, Is_dropout=true, Is_causal=false, Is_even_N=true, Is_even_K=false, Return_softmax=true, Params=Flash_fwd_params]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(15): here
            instantiation of "void flash_fwd_kernel<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax>(Flash_fwd_params) [with Kernel_traits=Flash_fwd_kernel_traits<128, 128, 32, 4, false, false, cutlass::bfloat16_t, Flash_kernel_traits<128, 128, 32, 4, cutlass::bfloat16_t>>, Is_dropout=true, Is_causal=false, Is_even_N=true, Is_even_K=false, Return_softmax=true]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(34): here
            instantiation of "void run_flash_fwd<Kernel_traits,Is_dropout,Is_causal>(Flash_fwd_params &, cudaStream_t) [with Kernel_traits=Flash_fwd_kernel_traits<128, 128, 32, 4, false, false, cutlass::bfloat16_t, Flash_kernel_traits<128, 128, 32, 4, cutlass::bfloat16_t>>, Is_dropout=true, Is_causal=false]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(118): here
            instantiation of "void run_mha_fwd_hdim128<T>(Flash_fwd_params &, cudaStream_t) [with T=cutlass::bfloat16_t]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_hdim128_bf16_sm80.cu(18): here

/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_kernel.h(275): warning: variable "tQrQ" was declared but never referenced
          detected during:
            instantiation of "void flash::compute_attn_1rowblock<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &, int, int, int) [with Kernel_traits=Flash_fwd_kernel_traits<128, 128, 32, 4, false, false, cutlass::bfloat16_t, Flash_kernel_traits<128, 128, 32, 4, cutlass::bfloat16_t>>, Is_dropout=true, Is_causal=false, Is_even_N=true, Is_even_K=false, Return_softmax=true, Params=Flash_fwd_params]" 
(588): here
            instantiation of "void flash::compute_attn<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &) [with Kernel_traits=Flash_fwd_kernel_traits<128, 128, 32, 4, false, false, cutlass::bfloat16_t, Flash_kernel_traits<128, 128, 32, 4, cutlass::bfloat16_t>>, Is_dropout=true, Is_causal=false, Is_even_N=true, Is_even_K=false, Return_softmax=true, Params=Flash_fwd_params]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(15): here
            instantiation of "void flash_fwd_kernel<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax>(Flash_fwd_params) [with Kernel_traits=Flash_fwd_kernel_traits<128, 128, 32, 4, false, false, cutlass::bfloat16_t, Flash_kernel_traits<128, 128, 32, 4, cutlass::bfloat16_t>>, Is_dropout=true, Is_causal=false, Is_even_N=true, Is_even_K=false, Return_softmax=true]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(34): here
            instantiation of "void run_flash_fwd<Kernel_traits,Is_dropout,Is_causal>(Flash_fwd_params &, cudaStream_t) [with Kernel_traits=Flash_fwd_kernel_traits<128, 128, 32, 4, false, false, cutlass::bfloat16_t, Flash_kernel_traits<128, 128, 32, 4, cutlass::bfloat16_t>>, Is_dropout=true, Is_causal=false]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(118): here
            instantiation of "void run_mha_fwd_hdim128<T>(Flash_fwd_params &, cudaStream_t) [with T=cutlass::bfloat16_t]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_hdim128_bf16_sm80.cu(18): here

/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_kernel.h(140): warning: variable "MMA_M" was declared but never referenced
          detected during:
            instantiation of "void flash::compute_attn_1rowblock<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &, int, int, int) [with Kernel_traits=Flash_fwd_kernel_traits<128, 128, 32, 4, false, false, cutlass::bfloat16_t, Flash_kernel_traits<128, 128, 32, 4, cutlass::bfloat16_t>>, Is_dropout=true, Is_causal=false, Is_even_N=true, Is_even_K=false, Return_softmax=false, Params=Flash_fwd_params]" 
(588): here
            instantiation of "void flash::compute_attn<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &) [with Kernel_traits=Flash_fwd_kernel_traits<128, 128, 32, 4, false, false, cutlass::bfloat16_t, Flash_kernel_traits<128, 128, 32, 4, cutlass::bfloat16_t>>, Is_dropout=true, Is_causal=false, Is_even_N=true, Is_even_K=false, Return_softmax=false, Params=Flash_fwd_params]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(15): here
            instantiation of "void flash_fwd_kernel<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax>(Flash_fwd_params) [with Kernel_traits=Flash_fwd_kernel_traits<128, 128, 32, 4, false, false, cutlass::bfloat16_t, Flash_kernel_traits<128, 128, 32, 4, cutlass::bfloat16_t>>, Is_dropout=true, Is_causal=false, Is_even_N=true, Is_even_K=false, Return_softmax=false]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(34): here
            instantiation of "void run_flash_fwd<Kernel_traits,Is_dropout,Is_causal>(Flash_fwd_params &, cudaStream_t) [with Kernel_traits=Flash_fwd_kernel_traits<128, 128, 32, 4, false, false, cutlass::bfloat16_t, Flash_kernel_traits<128, 128, 32, 4, cutlass::bfloat16_t>>, Is_dropout=true, Is_causal=false]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(118): here
            instantiation of "void run_mha_fwd_hdim128<T>(Flash_fwd_params &, cudaStream_t) [with T=cutlass::bfloat16_t]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_hdim128_bf16_sm80.cu(18): here

/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_kernel.h(275): warning: variable "tQrQ" was declared but never referenced
          detected during:
            instantiation of "void flash::compute_attn_1rowblock<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &, int, int, int) [with Kernel_traits=Flash_fwd_kernel_traits<128, 128, 32, 4, false, false, cutlass::bfloat16_t, Flash_kernel_traits<128, 128, 32, 4, cutlass::bfloat16_t>>, Is_dropout=true, Is_causal=false, Is_even_N=true, Is_even_K=false, Return_softmax=false, Params=Flash_fwd_params]" 
(588): here
            instantiation of "void flash::compute_attn<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &) [with Kernel_traits=Flash_fwd_kernel_traits<128, 128, 32, 4, false, false, cutlass::bfloat16_t, Flash_kernel_traits<128, 128, 32, 4, cutlass::bfloat16_t>>, Is_dropout=true, Is_causal=false, Is_even_N=true, Is_even_K=false, Return_softmax=false, Params=Flash_fwd_params]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(15): here
            instantiation of "void flash_fwd_kernel<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax>(Flash_fwd_params) [with Kernel_traits=Flash_fwd_kernel_traits<128, 128, 32, 4, false, false, cutlass::bfloat16_t, Flash_kernel_traits<128, 128, 32, 4, cutlass::bfloat16_t>>, Is_dropout=true, Is_causal=false, Is_even_N=true, Is_even_K=false, Return_softmax=false]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(34): here
            instantiation of "void run_flash_fwd<Kernel_traits,Is_dropout,Is_causal>(Flash_fwd_params &, cudaStream_t) [with Kernel_traits=Flash_fwd_kernel_traits<128, 128, 32, 4, false, false, cutlass::bfloat16_t, Flash_kernel_traits<128, 128, 32, 4, cutlass::bfloat16_t>>, Is_dropout=true, Is_causal=false]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(118): here
            instantiation of "void run_mha_fwd_hdim128<T>(Flash_fwd_params &, cudaStream_t) [with T=cutlass::bfloat16_t]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_hdim128_bf16_sm80.cu(18): here

/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_kernel.h(140): warning: variable "MMA_M" was declared but never referenced
          detected during:
            instantiation of "void flash::compute_attn_1rowblock<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &, int, int, int) [with Kernel_traits=Flash_fwd_kernel_traits<128, 128, 32, 4, false, false, cutlass::bfloat16_t, Flash_kernel_traits<128, 128, 32, 4, cutlass::bfloat16_t>>, Is_dropout=true, Is_causal=false, Is_even_N=false, Is_even_K=true, Return_softmax=true, Params=Flash_fwd_params]" 
(588): here
            instantiation of "void flash::compute_attn<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &) [with Kernel_traits=Flash_fwd_kernel_traits<128, 128, 32, 4, false, false, cutlass::bfloat16_t, Flash_kernel_traits<128, 128, 32, 4, cutlass::bfloat16_t>>, Is_dropout=true, Is_causal=false, Is_even_N=false, Is_even_K=true, Return_softmax=true, Params=Flash_fwd_params]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(15): here
            instantiation of "void flash_fwd_kernel<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax>(Flash_fwd_params) [with Kernel_traits=Flash_fwd_kernel_traits<128, 128, 32, 4, false, false, cutlass::bfloat16_t, Flash_kernel_traits<128, 128, 32, 4, cutlass::bfloat16_t>>, Is_dropout=true, Is_causal=false, Is_even_N=false, Is_even_K=true, Return_softmax=true]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(34): here
            instantiation of "void run_flash_fwd<Kernel_traits,Is_dropout,Is_causal>(Flash_fwd_params &, cudaStream_t) [with Kernel_traits=Flash_fwd_kernel_traits<128, 128, 32, 4, false, false, cutlass::bfloat16_t, Flash_kernel_traits<128, 128, 32, 4, cutlass::bfloat16_t>>, Is_dropout=true, Is_causal=false]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(118): here
            instantiation of "void run_mha_fwd_hdim128<T>(Flash_fwd_params &, cudaStream_t) [with T=cutlass::bfloat16_t]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_hdim128_bf16_sm80.cu(18): here

/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_kernel.h(275): warning: variable "tQrQ" was declared but never referenced
          detected during:
            instantiation of "void flash::compute_attn_1rowblock<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &, int, int, int) [with Kernel_traits=Flash_fwd_kernel_traits<128, 128, 32, 4, false, false, cutlass::bfloat16_t, Flash_kernel_traits<128, 128, 32, 4, cutlass::bfloat16_t>>, Is_dropout=true, Is_causal=false, Is_even_N=false, Is_even_K=true, Return_softmax=true, Params=Flash_fwd_params]" 
(588): here
            instantiation of "void flash::compute_attn<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &) [with Kernel_traits=Flash_fwd_kernel_traits<128, 128, 32, 4, false, false, cutlass::bfloat16_t, Flash_kernel_traits<128, 128, 32, 4, cutlass::bfloat16_t>>, Is_dropout=true, Is_causal=false, Is_even_N=false, Is_even_K=true, Return_softmax=true, Params=Flash_fwd_params]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(15): here
            instantiation of "void flash_fwd_kernel<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax>(Flash_fwd_params) [with Kernel_traits=Flash_fwd_kernel_traits<128, 128, 32, 4, false, false, cutlass::bfloat16_t, Flash_kernel_traits<128, 128, 32, 4, cutlass::bfloat16_t>>, Is_dropout=true, Is_causal=false, Is_even_N=false, Is_even_K=true, Return_softmax=true]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(34): here
            instantiation of "void run_flash_fwd<Kernel_traits,Is_dropout,Is_causal>(Flash_fwd_params &, cudaStream_t) [with Kernel_traits=Flash_fwd_kernel_traits<128, 128, 32, 4, false, false, cutlass::bfloat16_t, Flash_kernel_traits<128, 128, 32, 4, cutlass::bfloat16_t>>, Is_dropout=true, Is_causal=false]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(118): here
            instantiation of "void run_mha_fwd_hdim128<T>(Flash_fwd_params &, cudaStream_t) [with T=cutlass::bfloat16_t]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_hdim128_bf16_sm80.cu(18): here

/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_kernel.h(140): warning: variable "MMA_M" was declared but never referenced
          detected during:
            instantiation of "void flash::compute_attn_1rowblock<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &, int, int, int) [with Kernel_traits=Flash_fwd_kernel_traits<128, 128, 32, 4, false, false, cutlass::bfloat16_t, Flash_kernel_traits<128, 128, 32, 4, cutlass::bfloat16_t>>, Is_dropout=true, Is_causal=false, Is_even_N=false, Is_even_K=true, Return_softmax=false, Params=Flash_fwd_params]" 
(588): here
            instantiation of "void flash::compute_attn<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &) [with Kernel_traits=Flash_fwd_kernel_traits<128, 128, 32, 4, false, false, cutlass::bfloat16_t, Flash_kernel_traits<128, 128, 32, 4, cutlass::bfloat16_t>>, Is_dropout=true, Is_causal=false, Is_even_N=false, Is_even_K=true, Return_softmax=false, Params=Flash_fwd_params]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(15): here
            instantiation of "void flash_fwd_kernel<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax>(Flash_fwd_params) [with Kernel_traits=Flash_fwd_kernel_traits<128, 128, 32, 4, false, false, cutlass::bfloat16_t, Flash_kernel_traits<128, 128, 32, 4, cutlass::bfloat16_t>>, Is_dropout=true, Is_causal=false, Is_even_N=false, Is_even_K=true, Return_softmax=false]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(34): here
            instantiation of "void run_flash_fwd<Kernel_traits,Is_dropout,Is_causal>(Flash_fwd_params &, cudaStream_t) [with Kernel_traits=Flash_fwd_kernel_traits<128, 128, 32, 4, false, false, cutlass::bfloat16_t, Flash_kernel_traits<128, 128, 32, 4, cutlass::bfloat16_t>>, Is_dropout=true, Is_causal=false]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(118): here
            instantiation of "void run_mha_fwd_hdim128<T>(Flash_fwd_params &, cudaStream_t) [with T=cutlass::bfloat16_t]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_hdim128_bf16_sm80.cu(18): here

/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_kernel.h(275): warning: variable "tQrQ" was declared but never referenced
          detected during:
            instantiation of "void flash::compute_attn_1rowblock<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &, int, int, int) [with Kernel_traits=Flash_fwd_kernel_traits<128, 128, 32, 4, false, false, cutlass::bfloat16_t, Flash_kernel_traits<128, 128, 32, 4, cutlass::bfloat16_t>>, Is_dropout=true, Is_causal=false, Is_even_N=false, Is_even_K=true, Return_softmax=false, Params=Flash_fwd_params]" 
(588): here
            instantiation of "void flash::compute_attn<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &) [with Kernel_traits=Flash_fwd_kernel_traits<128, 128, 32, 4, false, false, cutlass::bfloat16_t, Flash_kernel_traits<128, 128, 32, 4, cutlass::bfloat16_t>>, Is_dropout=true, Is_causal=false, Is_even_N=false, Is_even_K=true, Return_softmax=false, Params=Flash_fwd_params]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(15): here
            instantiation of "void flash_fwd_kernel<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax>(Flash_fwd_params) [with Kernel_traits=Flash_fwd_kernel_traits<128, 128, 32, 4, false, false, cutlass::bfloat16_t, Flash_kernel_traits<128, 128, 32, 4, cutlass::bfloat16_t>>, Is_dropout=true, Is_causal=false, Is_even_N=false, Is_even_K=true, Return_softmax=false]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(34): here
            instantiation of "void run_flash_fwd<Kernel_traits,Is_dropout,Is_causal>(Flash_fwd_params &, cudaStream_t) [with Kernel_traits=Flash_fwd_kernel_traits<128, 128, 32, 4, false, false, cutlass::bfloat16_t, Flash_kernel_traits<128, 128, 32, 4, cutlass::bfloat16_t>>, Is_dropout=true, Is_causal=false]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(118): here
            instantiation of "void run_mha_fwd_hdim128<T>(Flash_fwd_params &, cudaStream_t) [with T=cutlass::bfloat16_t]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_hdim128_bf16_sm80.cu(18): here

/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_kernel.h(140): warning: variable "MMA_M" was declared but never referenced
          detected during:
            instantiation of "void flash::compute_attn_1rowblock<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &, int, int, int) [with Kernel_traits=Flash_fwd_kernel_traits<128, 128, 32, 4, false, false, cutlass::bfloat16_t, Flash_kernel_traits<128, 128, 32, 4, cutlass::bfloat16_t>>, Is_dropout=true, Is_causal=false, Is_even_N=false, Is_even_K=false, Return_softmax=true, Params=Flash_fwd_params]" 
(588): here
            instantiation of "void flash::compute_attn<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &) [with Kernel_traits=Flash_fwd_kernel_traits<128, 128, 32, 4, false, false, cutlass::bfloat16_t, Flash_kernel_traits<128, 128, 32, 4, cutlass::bfloat16_t>>, Is_dropout=true, Is_causal=false, Is_even_N=false, Is_even_K=false, Return_softmax=true, Params=Flash_fwd_params]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(15): here
            instantiation of "void flash_fwd_kernel<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax>(Flash_fwd_params) [with Kernel_traits=Flash_fwd_kernel_traits<128, 128, 32, 4, false, false, cutlass::bfloat16_t, Flash_kernel_traits<128, 128, 32, 4, cutlass::bfloat16_t>>, Is_dropout=true, Is_causal=false, Is_even_N=false, Is_even_K=false, Return_softmax=true]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(34): here
            instantiation of "void run_flash_fwd<Kernel_traits,Is_dropout,Is_causal>(Flash_fwd_params &, cudaStream_t) [with Kernel_traits=Flash_fwd_kernel_traits<128, 128, 32, 4, false, false, cutlass::bfloat16_t, Flash_kernel_traits<128, 128, 32, 4, cutlass::bfloat16_t>>, Is_dropout=true, Is_causal=false]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(118): here
            instantiation of "void run_mha_fwd_hdim128<T>(Flash_fwd_params &, cudaStream_t) [with T=cutlass::bfloat16_t]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_hdim128_bf16_sm80.cu(18): here

/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_kernel.h(275): warning: variable "tQrQ" was declared but never referenced
          detected during:
            instantiation of "void flash::compute_attn_1rowblock<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &, int, int, int) [with Kernel_traits=Flash_fwd_kernel_traits<128, 128, 32, 4, false, false, cutlass::bfloat16_t, Flash_kernel_traits<128, 128, 32, 4, cutlass::bfloat16_t>>, Is_dropout=true, Is_causal=false, Is_even_N=false, Is_even_K=false, Return_softmax=true, Params=Flash_fwd_params]" 
(588): here
            instantiation of "void flash::compute_attn<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &) [with Kernel_traits=Flash_fwd_kernel_traits<128, 128, 32, 4, false, false, cutlass::bfloat16_t, Flash_kernel_traits<128, 128, 32, 4, cutlass::bfloat16_t>>, Is_dropout=true, Is_causal=false, Is_even_N=false, Is_even_K=false, Return_softmax=true, Params=Flash_fwd_params]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(15): here
            instantiation of "void flash_fwd_kernel<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax>(Flash_fwd_params) [with Kernel_traits=Flash_fwd_kernel_traits<128, 128, 32, 4, false, false, cutlass::bfloat16_t, Flash_kernel_traits<128, 128, 32, 4, cutlass::bfloat16_t>>, Is_dropout=true, Is_causal=false, Is_even_N=false, Is_even_K=false, Return_softmax=true]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(34): here
            instantiation of "void run_flash_fwd<Kernel_traits,Is_dropout,Is_causal>(Flash_fwd_params &, cudaStream_t) [with Kernel_traits=Flash_fwd_kernel_traits<128, 128, 32, 4, false, false, cutlass::bfloat16_t, Flash_kernel_traits<128, 128, 32, 4, cutlass::bfloat16_t>>, Is_dropout=true, Is_causal=false]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(118): here
            instantiation of "void run_mha_fwd_hdim128<T>(Flash_fwd_params &, cudaStream_t) [with T=cutlass::bfloat16_t]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_hdim128_bf16_sm80.cu(18): here

/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_kernel.h(140): warning: variable "MMA_M" was declared but never referenced
          detected during:
            instantiation of "void flash::compute_attn_1rowblock<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &, int, int, int) [with Kernel_traits=Flash_fwd_kernel_traits<128, 128, 32, 4, false, false, cutlass::bfloat16_t, Flash_kernel_traits<128, 128, 32, 4, cutlass::bfloat16_t>>, Is_dropout=true, Is_causal=false, Is_even_N=false, Is_even_K=false, Return_softmax=false, Params=Flash_fwd_params]" 
(588): here
            instantiation of "void flash::compute_attn<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &) [with Kernel_traits=Flash_fwd_kernel_traits<128, 128, 32, 4, false, false, cutlass::bfloat16_t, Flash_kernel_traits<128, 128, 32, 4, cutlass::bfloat16_t>>, Is_dropout=true, Is_causal=false, Is_even_N=false, Is_even_K=false, Return_softmax=false, Params=Flash_fwd_params]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(15): here
            instantiation of "void flash_fwd_kernel<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax>(Flash_fwd_params) [with Kernel_traits=Flash_fwd_kernel_traits<128, 128, 32, 4, false, false, cutlass::bfloat16_t, Flash_kernel_traits<128, 128, 32, 4, cutlass::bfloat16_t>>, Is_dropout=true, Is_causal=false, Is_even_N=false, Is_even_K=false, Return_softmax=false]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(34): here
            instantiation of "void run_flash_fwd<Kernel_traits,Is_dropout,Is_causal>(Flash_fwd_params &, cudaStream_t) [with Kernel_traits=Flash_fwd_kernel_traits<128, 128, 32, 4, false, false, cutlass::bfloat16_t, Flash_kernel_traits<128, 128, 32, 4, cutlass::bfloat16_t>>, Is_dropout=true, Is_causal=false]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(118): here
            instantiation of "void run_mha_fwd_hdim128<T>(Flash_fwd_params &, cudaStream_t) [with T=cutlass::bfloat16_t]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_hdim128_bf16_sm80.cu(18): here

/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_kernel.h(275): warning: variable "tQrQ" was declared but never referenced
          detected during:
            instantiation of "void flash::compute_attn_1rowblock<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &, int, int, int) [with Kernel_traits=Flash_fwd_kernel_traits<128, 128, 32, 4, false, false, cutlass::bfloat16_t, Flash_kernel_traits<128, 128, 32, 4, cutlass::bfloat16_t>>, Is_dropout=true, Is_causal=false, Is_even_N=false, Is_even_K=false, Return_softmax=false, Params=Flash_fwd_params]" 
(588): here
            instantiation of "void flash::compute_attn<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &) [with Kernel_traits=Flash_fwd_kernel_traits<128, 128, 32, 4, false, false, cutlass::bfloat16_t, Flash_kernel_traits<128, 128, 32, 4, cutlass::bfloat16_t>>, Is_dropout=true, Is_causal=false, Is_even_N=false, Is_even_K=false, Return_softmax=false, Params=Flash_fwd_params]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(15): here
            instantiation of "void flash_fwd_kernel<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax>(Flash_fwd_params) [with Kernel_traits=Flash_fwd_kernel_traits<128, 128, 32, 4, false, false, cutlass::bfloat16_t, Flash_kernel_traits<128, 128, 32, 4, cutlass::bfloat16_t>>, Is_dropout=true, Is_causal=false, Is_even_N=false, Is_even_K=false, Return_softmax=false]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(34): here
            instantiation of "void run_flash_fwd<Kernel_traits,Is_dropout,Is_causal>(Flash_fwd_params &, cudaStream_t) [with Kernel_traits=Flash_fwd_kernel_traits<128, 128, 32, 4, false, false, cutlass::bfloat16_t, Flash_kernel_traits<128, 128, 32, 4, cutlass::bfloat16_t>>, Is_dropout=true, Is_causal=false]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(118): here
            instantiation of "void run_mha_fwd_hdim128<T>(Flash_fwd_params &, cudaStream_t) [with T=cutlass::bfloat16_t]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_hdim128_bf16_sm80.cu(18): here

/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_kernel.h(140): warning: variable "MMA_M" was declared but never referenced
          detected during:
            instantiation of "void flash::compute_attn_1rowblock<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &, int, int, int) [with Kernel_traits=Flash_fwd_kernel_traits<128, 64, 64, 4, false, false, cutlass::bfloat16_t, Flash_kernel_traits<128, 64, 64, 4, cutlass::bfloat16_t>>, Is_dropout=false, Is_causal=true, Is_even_N=true, Is_even_K=true, Return_softmax=false, Params=Flash_fwd_params]" 
(588): here
            instantiation of "void flash::compute_attn<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &) [with Kernel_traits=Flash_fwd_kernel_traits<128, 64, 64, 4, false, false, cutlass::bfloat16_t, Flash_kernel_traits<128, 64, 64, 4, cutlass::bfloat16_t>>, Is_dropout=false, Is_causal=true, Is_even_N=true, Is_even_K=true, Return_softmax=false, Params=Flash_fwd_params]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(15): here
            instantiation of "void flash_fwd_kernel<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax>(Flash_fwd_params) [with Kernel_traits=Flash_fwd_kernel_traits<128, 64, 64, 4, false, false, cutlass::bfloat16_t, Flash_kernel_traits<128, 64, 64, 4, cutlass::bfloat16_t>>, Is_dropout=false, Is_causal=true, Is_even_N=true, Is_even_K=true, Return_softmax=false]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(34): here
            instantiation of "void run_flash_fwd<Kernel_traits,Is_dropout,Is_causal>(Flash_fwd_params &, cudaStream_t) [with Kernel_traits=Flash_fwd_kernel_traits<128, 64, 64, 4, false, false, cutlass::bfloat16_t, Flash_kernel_traits<128, 64, 64, 4, cutlass::bfloat16_t>>, Is_dropout=false, Is_causal=true]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(118): here
            instantiation of "void run_mha_fwd_hdim128<T>(Flash_fwd_params &, cudaStream_t) [with T=cutlass::bfloat16_t]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_hdim128_bf16_sm80.cu(18): here

/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_kernel.h(275): warning: variable "tQrQ" was declared but never referenced
          detected during:
            instantiation of "void flash::compute_attn_1rowblock<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &, int, int, int) [with Kernel_traits=Flash_fwd_kernel_traits<128, 64, 64, 4, false, false, cutlass::bfloat16_t, Flash_kernel_traits<128, 64, 64, 4, cutlass::bfloat16_t>>, Is_dropout=false, Is_causal=true, Is_even_N=true, Is_even_K=true, Return_softmax=false, Params=Flash_fwd_params]" 
(588): here
            instantiation of "void flash::compute_attn<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &) [with Kernel_traits=Flash_fwd_kernel_traits<128, 64, 64, 4, false, false, cutlass::bfloat16_t, Flash_kernel_traits<128, 64, 64, 4, cutlass::bfloat16_t>>, Is_dropout=false, Is_causal=true, Is_even_N=true, Is_even_K=true, Return_softmax=false, Params=Flash_fwd_params]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(15): here
            instantiation of "void flash_fwd_kernel<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax>(Flash_fwd_params) [with Kernel_traits=Flash_fwd_kernel_traits<128, 64, 64, 4, false, false, cutlass::bfloat16_t, Flash_kernel_traits<128, 64, 64, 4, cutlass::bfloat16_t>>, Is_dropout=false, Is_causal=true, Is_even_N=true, Is_even_K=true, Return_softmax=false]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(34): here
            instantiation of "void run_flash_fwd<Kernel_traits,Is_dropout,Is_causal>(Flash_fwd_params &, cudaStream_t) [with Kernel_traits=Flash_fwd_kernel_traits<128, 64, 64, 4, false, false, cutlass::bfloat16_t, Flash_kernel_traits<128, 64, 64, 4, cutlass::bfloat16_t>>, Is_dropout=false, Is_causal=true]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(118): here
            instantiation of "void run_mha_fwd_hdim128<T>(Flash_fwd_params &, cudaStream_t) [with T=cutlass::bfloat16_t]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_hdim128_bf16_sm80.cu(18): here

/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_kernel.h(140): warning: variable "MMA_M" was declared but never referenced
          detected during:
            instantiation of "void flash::compute_attn_1rowblock<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &, int, int, int) [with Kernel_traits=Flash_fwd_kernel_traits<128, 64, 64, 4, false, false, cutlass::bfloat16_t, Flash_kernel_traits<128, 64, 64, 4, cutlass::bfloat16_t>>, Is_dropout=false, Is_causal=true, Is_even_N=true, Is_even_K=false, Return_softmax=false, Params=Flash_fwd_params]" 
(588): here
            instantiation of "void flash::compute_attn<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &) [with Kernel_traits=Flash_fwd_kernel_traits<128, 64, 64, 4, false, false, cutlass::bfloat16_t, Flash_kernel_traits<128, 64, 64, 4, cutlass::bfloat16_t>>, Is_dropout=false, Is_causal=true, Is_even_N=true, Is_even_K=false, Return_softmax=false, Params=Flash_fwd_params]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(15): here
            instantiation of "void flash_fwd_kernel<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax>(Flash_fwd_params) [with Kernel_traits=Flash_fwd_kernel_traits<128, 64, 64, 4, false, false, cutlass::bfloat16_t, Flash_kernel_traits<128, 64, 64, 4, cutlass::bfloat16_t>>, Is_dropout=false, Is_causal=true, Is_even_N=true, Is_even_K=false, Return_softmax=false]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(34): here
            instantiation of "void run_flash_fwd<Kernel_traits,Is_dropout,Is_causal>(Flash_fwd_params &, cudaStream_t) [with Kernel_traits=Flash_fwd_kernel_traits<128, 64, 64, 4, false, false, cutlass::bfloat16_t, Flash_kernel_traits<128, 64, 64, 4, cutlass::bfloat16_t>>, Is_dropout=false, Is_causal=true]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(118): here
            instantiation of "void run_mha_fwd_hdim128<T>(Flash_fwd_params &, cudaStream_t) [with T=cutlass::bfloat16_t]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_hdim128_bf16_sm80.cu(18): here

/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_kernel.h(275): warning: variable "tQrQ" was declared but never referenced
          detected during:
            instantiation of "void flash::compute_attn_1rowblock<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &, int, int, int) [with Kernel_traits=Flash_fwd_kernel_traits<128, 64, 64, 4, false, false, cutlass::bfloat16_t, Flash_kernel_traits<128, 64, 64, 4, cutlass::bfloat16_t>>, Is_dropout=false, Is_causal=true, Is_even_N=true, Is_even_K=false, Return_softmax=false, Params=Flash_fwd_params]" 
(588): here
            instantiation of "void flash::compute_attn<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &) [with Kernel_traits=Flash_fwd_kernel_traits<128, 64, 64, 4, false, false, cutlass::bfloat16_t, Flash_kernel_traits<128, 64, 64, 4, cutlass::bfloat16_t>>, Is_dropout=false, Is_causal=true, Is_even_N=true, Is_even_K=false, Return_softmax=false, Params=Flash_fwd_params]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(15): here
            instantiation of "void flash_fwd_kernel<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax>(Flash_fwd_params) [with Kernel_traits=Flash_fwd_kernel_traits<128, 64, 64, 4, false, false, cutlass::bfloat16_t, Flash_kernel_traits<128, 64, 64, 4, cutlass::bfloat16_t>>, Is_dropout=false, Is_causal=true, Is_even_N=true, Is_even_K=false, Return_softmax=false]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(34): here
            instantiation of "void run_flash_fwd<Kernel_traits,Is_dropout,Is_causal>(Flash_fwd_params &, cudaStream_t) [with Kernel_traits=Flash_fwd_kernel_traits<128, 64, 64, 4, false, false, cutlass::bfloat16_t, Flash_kernel_traits<128, 64, 64, 4, cutlass::bfloat16_t>>, Is_dropout=false, Is_causal=true]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(118): here
            instantiation of "void run_mha_fwd_hdim128<T>(Flash_fwd_params &, cudaStream_t) [with T=cutlass::bfloat16_t]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_hdim128_bf16_sm80.cu(18): here

/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_kernel.h(140): warning: variable "MMA_M" was declared but never referenced
          detected during:
            instantiation of "void flash::compute_attn_1rowblock<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &, int, int, int) [with Kernel_traits=Flash_fwd_kernel_traits<128, 64, 64, 4, false, false, cutlass::bfloat16_t, Flash_kernel_traits<128, 64, 64, 4, cutlass::bfloat16_t>>, Is_dropout=false, Is_causal=true, Is_even_N=false, Is_even_K=true, Return_softmax=false, Params=Flash_fwd_params]" 
(588): here
            instantiation of "void flash::compute_attn<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &) [with Kernel_traits=Flash_fwd_kernel_traits<128, 64, 64, 4, false, false, cutlass::bfloat16_t, Flash_kernel_traits<128, 64, 64, 4, cutlass::bfloat16_t>>, Is_dropout=false, Is_causal=true, Is_even_N=false, Is_even_K=true, Return_softmax=false, Params=Flash_fwd_params]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(15): here
            instantiation of "void flash_fwd_kernel<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax>(Flash_fwd_params) [with Kernel_traits=Flash_fwd_kernel_traits<128, 64, 64, 4, false, false, cutlass::bfloat16_t, Flash_kernel_traits<128, 64, 64, 4, cutlass::bfloat16_t>>, Is_dropout=false, Is_causal=true, Is_even_N=false, Is_even_K=true, Return_softmax=false]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(34): here
            instantiation of "void run_flash_fwd<Kernel_traits,Is_dropout,Is_causal>(Flash_fwd_params &, cudaStream_t) [with Kernel_traits=Flash_fwd_kernel_traits<128, 64, 64, 4, false, false, cutlass::bfloat16_t, Flash_kernel_traits<128, 64, 64, 4, cutlass::bfloat16_t>>, Is_dropout=false, Is_causal=true]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(118): here
            instantiation of "void run_mha_fwd_hdim128<T>(Flash_fwd_params &, cudaStream_t) [with T=cutlass::bfloat16_t]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_hdim128_bf16_sm80.cu(18): here

/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_kernel.h(275): warning: variable "tQrQ" was declared but never referenced
          detected during:
            instantiation of "void flash::compute_attn_1rowblock<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &, int, int, int) [with Kernel_traits=Flash_fwd_kernel_traits<128, 64, 64, 4, false, false, cutlass::bfloat16_t, Flash_kernel_traits<128, 64, 64, 4, cutlass::bfloat16_t>>, Is_dropout=false, Is_causal=true, Is_even_N=false, Is_even_K=true, Return_softmax=false, Params=Flash_fwd_params]" 
(588): here
            instantiation of "void flash::compute_attn<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &) [with Kernel_traits=Flash_fwd_kernel_traits<128, 64, 64, 4, false, false, cutlass::bfloat16_t, Flash_kernel_traits<128, 64, 64, 4, cutlass::bfloat16_t>>, Is_dropout=false, Is_causal=true, Is_even_N=false, Is_even_K=true, Return_softmax=false, Params=Flash_fwd_params]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(15): here
            instantiation of "void flash_fwd_kernel<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax>(Flash_fwd_params) [with Kernel_traits=Flash_fwd_kernel_traits<128, 64, 64, 4, false, false, cutlass::bfloat16_t, Flash_kernel_traits<128, 64, 64, 4, cutlass::bfloat16_t>>, Is_dropout=false, Is_causal=true, Is_even_N=false, Is_even_K=true, Return_softmax=false]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(34): here
            instantiation of "void run_flash_fwd<Kernel_traits,Is_dropout,Is_causal>(Flash_fwd_params &, cudaStream_t) [with Kernel_traits=Flash_fwd_kernel_traits<128, 64, 64, 4, false, false, cutlass::bfloat16_t, Flash_kernel_traits<128, 64, 64, 4, cutlass::bfloat16_t>>, Is_dropout=false, Is_causal=true]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(118): here
            instantiation of "void run_mha_fwd_hdim128<T>(Flash_fwd_params &, cudaStream_t) [with T=cutlass::bfloat16_t]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_hdim128_bf16_sm80.cu(18): here

/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_kernel.h(140): warning: variable "MMA_M" was declared but never referenced
          detected during:
            instantiation of "void flash::compute_attn_1rowblock<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &, int, int, int) [with Kernel_traits=Flash_fwd_kernel_traits<128, 64, 64, 4, false, false, cutlass::bfloat16_t, Flash_kernel_traits<128, 64, 64, 4, cutlass::bfloat16_t>>, Is_dropout=false, Is_causal=true, Is_even_N=false, Is_even_K=false, Return_softmax=false, Params=Flash_fwd_params]" 
(588): here
            instantiation of "void flash::compute_attn<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &) [with Kernel_traits=Flash_fwd_kernel_traits<128, 64, 64, 4, false, false, cutlass::bfloat16_t, Flash_kernel_traits<128, 64, 64, 4, cutlass::bfloat16_t>>, Is_dropout=false, Is_causal=true, Is_even_N=false, Is_even_K=false, Return_softmax=false, Params=Flash_fwd_params]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(15): here
            instantiation of "void flash_fwd_kernel<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax>(Flash_fwd_params) [with Kernel_traits=Flash_fwd_kernel_traits<128, 64, 64, 4, false, false, cutlass::bfloat16_t, Flash_kernel_traits<128, 64, 64, 4, cutlass::bfloat16_t>>, Is_dropout=false, Is_causal=true, Is_even_N=false, Is_even_K=false, Return_softmax=false]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(34): here
            instantiation of "void run_flash_fwd<Kernel_traits,Is_dropout,Is_causal>(Flash_fwd_params &, cudaStream_t) [with Kernel_traits=Flash_fwd_kernel_traits<128, 64, 64, 4, false, false, cutlass::bfloat16_t, Flash_kernel_traits<128, 64, 64, 4, cutlass::bfloat16_t>>, Is_dropout=false, Is_causal=true]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(118): here
            instantiation of "void run_mha_fwd_hdim128<T>(Flash_fwd_params &, cudaStream_t) [with T=cutlass::bfloat16_t]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_hdim128_bf16_sm80.cu(18): here

/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_kernel.h(275): warning: variable "tQrQ" was declared but never referenced
          detected during:
            instantiation of "void flash::compute_attn_1rowblock<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &, int, int, int) [with Kernel_traits=Flash_fwd_kernel_traits<128, 64, 64, 4, false, false, cutlass::bfloat16_t, Flash_kernel_traits<128, 64, 64, 4, cutlass::bfloat16_t>>, Is_dropout=false, Is_causal=true, Is_even_N=false, Is_even_K=false, Return_softmax=false, Params=Flash_fwd_params]" 
(588): here
            instantiation of "void flash::compute_attn<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &) [with Kernel_traits=Flash_fwd_kernel_traits<128, 64, 64, 4, false, false, cutlass::bfloat16_t, Flash_kernel_traits<128, 64, 64, 4, cutlass::bfloat16_t>>, Is_dropout=false, Is_causal=true, Is_even_N=false, Is_even_K=false, Return_softmax=false, Params=Flash_fwd_params]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(15): here
            instantiation of "void flash_fwd_kernel<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax>(Flash_fwd_params) [with Kernel_traits=Flash_fwd_kernel_traits<128, 64, 64, 4, false, false, cutlass::bfloat16_t, Flash_kernel_traits<128, 64, 64, 4, cutlass::bfloat16_t>>, Is_dropout=false, Is_causal=true, Is_even_N=false, Is_even_K=false, Return_softmax=false]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(34): here
            instantiation of "void run_flash_fwd<Kernel_traits,Is_dropout,Is_causal>(Flash_fwd_params &, cudaStream_t) [with Kernel_traits=Flash_fwd_kernel_traits<128, 64, 64, 4, false, false, cutlass::bfloat16_t, Flash_kernel_traits<128, 64, 64, 4, cutlass::bfloat16_t>>, Is_dropout=false, Is_causal=true]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(118): here
            instantiation of "void run_mha_fwd_hdim128<T>(Flash_fwd_params &, cudaStream_t) [with T=cutlass::bfloat16_t]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_hdim128_bf16_sm80.cu(18): here

/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_kernel.h(140): warning: variable "MMA_M" was declared but never referenced
          detected during:
            instantiation of "void flash::compute_attn_1rowblock<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &, int, int, int) [with Kernel_traits=Flash_fwd_kernel_traits<128, 128, 64, 4, false, false, cutlass::bfloat16_t, Flash_kernel_traits<128, 128, 64, 4, cutlass::bfloat16_t>>, Is_dropout=false, Is_causal=true, Is_even_N=true, Is_even_K=true, Return_softmax=false, Params=Flash_fwd_params]" 
(588): here
            instantiation of "void flash::compute_attn<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &) [with Kernel_traits=Flash_fwd_kernel_traits<128, 128, 64, 4, false, false, cutlass::bfloat16_t, Flash_kernel_traits<128, 128, 64, 4, cutlass::bfloat16_t>>, Is_dropout=false, Is_causal=true, Is_even_N=true, Is_even_K=true, Return_softmax=false, Params=Flash_fwd_params]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(15): here
            instantiation of "void flash_fwd_kernel<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax>(Flash_fwd_params) [with Kernel_traits=Flash_fwd_kernel_traits<128, 128, 64, 4, false, false, cutlass::bfloat16_t, Flash_kernel_traits<128, 128, 64, 4, cutlass::bfloat16_t>>, Is_dropout=false, Is_causal=true, Is_even_N=true, Is_even_K=true, Return_softmax=false]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(34): here
            instantiation of "void run_flash_fwd<Kernel_traits,Is_dropout,Is_causal>(Flash_fwd_params &, cudaStream_t) [with Kernel_traits=Flash_fwd_kernel_traits<128, 128, 64, 4, false, false, cutlass::bfloat16_t, Flash_kernel_traits<128, 128, 64, 4, cutlass::bfloat16_t>>, Is_dropout=false, Is_causal=true]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(118): here
            instantiation of "void run_mha_fwd_hdim128<T>(Flash_fwd_params &, cudaStream_t) [with T=cutlass::bfloat16_t]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_hdim128_bf16_sm80.cu(18): here

/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_kernel.h(275): warning: variable "tQrQ" was declared but never referenced
          detected during:
            instantiation of "void flash::compute_attn_1rowblock<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &, int, int, int) [with Kernel_traits=Flash_fwd_kernel_traits<128, 128, 64, 4, false, false, cutlass::bfloat16_t, Flash_kernel_traits<128, 128, 64, 4, cutlass::bfloat16_t>>, Is_dropout=false, Is_causal=true, Is_even_N=true, Is_even_K=true, Return_softmax=false, Params=Flash_fwd_params]" 
(588): here
            instantiation of "void flash::compute_attn<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &) [with Kernel_traits=Flash_fwd_kernel_traits<128, 128, 64, 4, false, false, cutlass::bfloat16_t, Flash_kernel_traits<128, 128, 64, 4, cutlass::bfloat16_t>>, Is_dropout=false, Is_causal=true, Is_even_N=true, Is_even_K=true, Return_softmax=false, Params=Flash_fwd_params]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(15): here
            instantiation of "void flash_fwd_kernel<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax>(Flash_fwd_params) [with Kernel_traits=Flash_fwd_kernel_traits<128, 128, 64, 4, false, false, cutlass::bfloat16_t, Flash_kernel_traits<128, 128, 64, 4, cutlass::bfloat16_t>>, Is_dropout=false, Is_causal=true, Is_even_N=true, Is_even_K=true, Return_softmax=false]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(34): here
            instantiation of "void run_flash_fwd<Kernel_traits,Is_dropout,Is_causal>(Flash_fwd_params &, cudaStream_t) [with Kernel_traits=Flash_fwd_kernel_traits<128, 128, 64, 4, false, false, cutlass::bfloat16_t, Flash_kernel_traits<128, 128, 64, 4, cutlass::bfloat16_t>>, Is_dropout=false, Is_causal=true]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(118): here
            instantiation of "void run_mha_fwd_hdim128<T>(Flash_fwd_params &, cudaStream_t) [with T=cutlass::bfloat16_t]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_hdim128_bf16_sm80.cu(18): here

/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_kernel.h(140): warning: variable "MMA_M" was declared but never referenced
          detected during:
            instantiation of "void flash::compute_attn_1rowblock<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &, int, int, int) [with Kernel_traits=Flash_fwd_kernel_traits<128, 128, 64, 4, false, false, cutlass::bfloat16_t, Flash_kernel_traits<128, 128, 64, 4, cutlass::bfloat16_t>>, Is_dropout=false, Is_causal=true, Is_even_N=true, Is_even_K=false, Return_softmax=false, Params=Flash_fwd_params]" 
(588): here
            instantiation of "void flash::compute_attn<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &) [with Kernel_traits=Flash_fwd_kernel_traits<128, 128, 64, 4, false, false, cutlass::bfloat16_t, Flash_kernel_traits<128, 128, 64, 4, cutlass::bfloat16_t>>, Is_dropout=false, Is_causal=true, Is_even_N=true, Is_even_K=false, Return_softmax=false, Params=Flash_fwd_params]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(15): here
            instantiation of "void flash_fwd_kernel<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax>(Flash_fwd_params) [with Kernel_traits=Flash_fwd_kernel_traits<128, 128, 64, 4, false, false, cutlass::bfloat16_t, Flash_kernel_traits<128, 128, 64, 4, cutlass::bfloat16_t>>, Is_dropout=false, Is_causal=true, Is_even_N=true, Is_even_K=false, Return_softmax=false]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(34): here
            instantiation of "void run_flash_fwd<Kernel_traits,Is_dropout,Is_causal>(Flash_fwd_params &, cudaStream_t) [with Kernel_traits=Flash_fwd_kernel_traits<128, 128, 64, 4, false, false, cutlass::bfloat16_t, Flash_kernel_traits<128, 128, 64, 4, cutlass::bfloat16_t>>, Is_dropout=false, Is_causal=true]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(118): here
            instantiation of "void run_mha_fwd_hdim128<T>(Flash_fwd_params &, cudaStream_t) [with T=cutlass::bfloat16_t]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_hdim128_bf16_sm80.cu(18): here

/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_kernel.h(275): warning: variable "tQrQ" was declared but never referenced
          detected during:
            instantiation of "void flash::compute_attn_1rowblock<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &, int, int, int) [with Kernel_traits=Flash_fwd_kernel_traits<128, 128, 64, 4, false, false, cutlass::bfloat16_t, Flash_kernel_traits<128, 128, 64, 4, cutlass::bfloat16_t>>, Is_dropout=false, Is_causal=true, Is_even_N=true, Is_even_K=false, Return_softmax=false, Params=Flash_fwd_params]" 
(588): here
            instantiation of "void flash::compute_attn<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &) [with Kernel_traits=Flash_fwd_kernel_traits<128, 128, 64, 4, false, false, cutlass::bfloat16_t, Flash_kernel_traits<128, 128, 64, 4, cutlass::bfloat16_t>>, Is_dropout=false, Is_causal=true, Is_even_N=true, Is_even_K=false, Return_softmax=false, Params=Flash_fwd_params]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(15): here
            instantiation of "void flash_fwd_kernel<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax>(Flash_fwd_params) [with Kernel_traits=Flash_fwd_kernel_traits<128, 128, 64, 4, false, false, cutlass::bfloat16_t, Flash_kernel_traits<128, 128, 64, 4, cutlass::bfloat16_t>>, Is_dropout=false, Is_causal=true, Is_even_N=true, Is_even_K=false, Return_softmax=false]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(34): here
            instantiation of "void run_flash_fwd<Kernel_traits,Is_dropout,Is_causal>(Flash_fwd_params &, cudaStream_t) [with Kernel_traits=Flash_fwd_kernel_traits<128, 128, 64, 4, false, false, cutlass::bfloat16_t, Flash_kernel_traits<128, 128, 64, 4, cutlass::bfloat16_t>>, Is_dropout=false, Is_causal=true]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(118): here
            instantiation of "void run_mha_fwd_hdim128<T>(Flash_fwd_params &, cudaStream_t) [with T=cutlass::bfloat16_t]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_hdim128_bf16_sm80.cu(18): here

/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_kernel.h(140): warning: variable "MMA_M" was declared but never referenced
          detected during:
            instantiation of "void flash::compute_attn_1rowblock<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &, int, int, int) [with Kernel_traits=Flash_fwd_kernel_traits<128, 128, 64, 4, false, false, cutlass::bfloat16_t, Flash_kernel_traits<128, 128, 64, 4, cutlass::bfloat16_t>>, Is_dropout=false, Is_causal=true, Is_even_N=false, Is_even_K=true, Return_softmax=false, Params=Flash_fwd_params]" 
(588): here
            instantiation of "void flash::compute_attn<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &) [with Kernel_traits=Flash_fwd_kernel_traits<128, 128, 64, 4, false, false, cutlass::bfloat16_t, Flash_kernel_traits<128, 128, 64, 4, cutlass::bfloat16_t>>, Is_dropout=false, Is_causal=true, Is_even_N=false, Is_even_K=true, Return_softmax=false, Params=Flash_fwd_params]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(15): here
            instantiation of "void flash_fwd_kernel<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax>(Flash_fwd_params) [with Kernel_traits=Flash_fwd_kernel_traits<128, 128, 64, 4, false, false, cutlass::bfloat16_t, Flash_kernel_traits<128, 128, 64, 4, cutlass::bfloat16_t>>, Is_dropout=false, Is_causal=true, Is_even_N=false, Is_even_K=true, Return_softmax=false]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(34): here
            instantiation of "void run_flash_fwd<Kernel_traits,Is_dropout,Is_causal>(Flash_fwd_params &, cudaStream_t) [with Kernel_traits=Flash_fwd_kernel_traits<128, 128, 64, 4, false, false, cutlass::bfloat16_t, Flash_kernel_traits<128, 128, 64, 4, cutlass::bfloat16_t>>, Is_dropout=false, Is_causal=true]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(118): here
            instantiation of "void run_mha_fwd_hdim128<T>(Flash_fwd_params &, cudaStream_t) [with T=cutlass::bfloat16_t]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_hdim128_bf16_sm80.cu(18): here

/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_kernel.h(275): warning: variable "tQrQ" was declared but never referenced
          detected during:
            instantiation of "void flash::compute_attn_1rowblock<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &, int, int, int) [with Kernel_traits=Flash_fwd_kernel_traits<128, 128, 64, 4, false, false, cutlass::bfloat16_t, Flash_kernel_traits<128, 128, 64, 4, cutlass::bfloat16_t>>, Is_dropout=false, Is_causal=true, Is_even_N=false, Is_even_K=true, Return_softmax=false, Params=Flash_fwd_params]" 
(588): here
            instantiation of "void flash::compute_attn<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &) [with Kernel_traits=Flash_fwd_kernel_traits<128, 128, 64, 4, false, false, cutlass::bfloat16_t, Flash_kernel_traits<128, 128, 64, 4, cutlass::bfloat16_t>>, Is_dropout=false, Is_causal=true, Is_even_N=false, Is_even_K=true, Return_softmax=false, Params=Flash_fwd_params]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(15): here
            instantiation of "void flash_fwd_kernel<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax>(Flash_fwd_params) [with Kernel_traits=Flash_fwd_kernel_traits<128, 128, 64, 4, false, false, cutlass::bfloat16_t, Flash_kernel_traits<128, 128, 64, 4, cutlass::bfloat16_t>>, Is_dropout=false, Is_causal=true, Is_even_N=false, Is_even_K=true, Return_softmax=false]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(34): here
            instantiation of "void run_flash_fwd<Kernel_traits,Is_dropout,Is_causal>(Flash_fwd_params &, cudaStream_t) [with Kernel_traits=Flash_fwd_kernel_traits<128, 128, 64, 4, false, false, cutlass::bfloat16_t, Flash_kernel_traits<128, 128, 64, 4, cutlass::bfloat16_t>>, Is_dropout=false, Is_causal=true]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(118): here
            instantiation of "void run_mha_fwd_hdim128<T>(Flash_fwd_params &, cudaStream_t) [with T=cutlass::bfloat16_t]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_hdim128_bf16_sm80.cu(18): here

/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_kernel.h(140): warning: variable "MMA_M" was declared but never referenced
          detected during:
            instantiation of "void flash::compute_attn_1rowblock<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &, int, int, int) [with Kernel_traits=Flash_fwd_kernel_traits<128, 128, 64, 4, false, false, cutlass::bfloat16_t, Flash_kernel_traits<128, 128, 64, 4, cutlass::bfloat16_t>>, Is_dropout=false, Is_causal=true, Is_even_N=false, Is_even_K=false, Return_softmax=false, Params=Flash_fwd_params]" 
(588): here
            instantiation of "void flash::compute_attn<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &) [with Kernel_traits=Flash_fwd_kernel_traits<128, 128, 64, 4, false, false, cutlass::bfloat16_t, Flash_kernel_traits<128, 128, 64, 4, cutlass::bfloat16_t>>, Is_dropout=false, Is_causal=true, Is_even_N=false, Is_even_K=false, Return_softmax=false, Params=Flash_fwd_params]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(15): here
            instantiation of "void flash_fwd_kernel<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax>(Flash_fwd_params) [with Kernel_traits=Flash_fwd_kernel_traits<128, 128, 64, 4, false, false, cutlass::bfloat16_t, Flash_kernel_traits<128, 128, 64, 4, cutlass::bfloat16_t>>, Is_dropout=false, Is_causal=true, Is_even_N=false, Is_even_K=false, Return_softmax=false]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(34): here
            instantiation of "void run_flash_fwd<Kernel_traits,Is_dropout,Is_causal>(Flash_fwd_params &, cudaStream_t) [with Kernel_traits=Flash_fwd_kernel_traits<128, 128, 64, 4, false, false, cutlass::bfloat16_t, Flash_kernel_traits<128, 128, 64, 4, cutlass::bfloat16_t>>, Is_dropout=false, Is_causal=true]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(118): here
            instantiation of "void run_mha_fwd_hdim128<T>(Flash_fwd_params &, cudaStream_t) [with T=cutlass::bfloat16_t]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_hdim128_bf16_sm80.cu(18): here

/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_kernel.h(275): warning: variable "tQrQ" was declared but never referenced
          detected during:
            instantiation of "void flash::compute_attn_1rowblock<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &, int, int, int) [with Kernel_traits=Flash_fwd_kernel_traits<128, 128, 64, 4, false, false, cutlass::bfloat16_t, Flash_kernel_traits<128, 128, 64, 4, cutlass::bfloat16_t>>, Is_dropout=false, Is_causal=true, Is_even_N=false, Is_even_K=false, Return_softmax=false, Params=Flash_fwd_params]" 
(588): here
            instantiation of "void flash::compute_attn<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &) [with Kernel_traits=Flash_fwd_kernel_traits<128, 128, 64, 4, false, false, cutlass::bfloat16_t, Flash_kernel_traits<128, 128, 64, 4, cutlass::bfloat16_t>>, Is_dropout=false, Is_causal=true, Is_even_N=false, Is_even_K=false, Return_softmax=false, Params=Flash_fwd_params]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(15): here
            instantiation of "void flash_fwd_kernel<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax>(Flash_fwd_params) [with Kernel_traits=Flash_fwd_kernel_traits<128, 128, 64, 4, false, false, cutlass::bfloat16_t, Flash_kernel_traits<128, 128, 64, 4, cutlass::bfloat16_t>>, Is_dropout=false, Is_causal=true, Is_even_N=false, Is_even_K=false, Return_softmax=false]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(34): here
            instantiation of "void run_flash_fwd<Kernel_traits,Is_dropout,Is_causal>(Flash_fwd_params &, cudaStream_t) [with Kernel_traits=Flash_fwd_kernel_traits<128, 128, 64, 4, false, false, cutlass::bfloat16_t, Flash_kernel_traits<128, 128, 64, 4, cutlass::bfloat16_t>>, Is_dropout=false, Is_causal=true]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(118): here
            instantiation of "void run_mha_fwd_hdim128<T>(Flash_fwd_params &, cudaStream_t) [with T=cutlass::bfloat16_t]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_hdim128_bf16_sm80.cu(18): here

/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_kernel.h(140): warning: variable "MMA_M" was declared but never referenced
          detected during:
            instantiation of "void flash::compute_attn_1rowblock<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &, int, int, int) [with Kernel_traits=Flash_fwd_kernel_traits<128, 128, 32, 4, false, false, cutlass::bfloat16_t, Flash_kernel_traits<128, 128, 32, 4, cutlass::bfloat16_t>>, Is_dropout=false, Is_causal=false, Is_even_N=true, Is_even_K=true, Return_softmax=false, Params=Flash_fwd_params]" 
(588): here
            instantiation of "void flash::compute_attn<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &) [with Kernel_traits=Flash_fwd_kernel_traits<128, 128, 32, 4, false, false, cutlass::bfloat16_t, Flash_kernel_traits<128, 128, 32, 4, cutlass::bfloat16_t>>, Is_dropout=false, Is_causal=false, Is_even_N=true, Is_even_K=true, Return_softmax=false, Params=Flash_fwd_params]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(15): here
            instantiation of "void flash_fwd_kernel<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax>(Flash_fwd_params) [with Kernel_traits=Flash_fwd_kernel_traits<128, 128, 32, 4, false, false, cutlass::bfloat16_t, Flash_kernel_traits<128, 128, 32, 4, cutlass::bfloat16_t>>, Is_dropout=false, Is_causal=false, Is_even_N=true, Is_even_K=true, Return_softmax=false]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(34): here
            instantiation of "void run_flash_fwd<Kernel_traits,Is_dropout,Is_causal>(Flash_fwd_params &, cudaStream_t) [with Kernel_traits=Flash_fwd_kernel_traits<128, 128, 32, 4, false, false, cutlass::bfloat16_t, Flash_kernel_traits<128, 128, 32, 4, cutlass::bfloat16_t>>, Is_dropout=false, Is_causal=false]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(118): here
            instantiation of "void run_mha_fwd_hdim128<T>(Flash_fwd_params &, cudaStream_t) [with T=cutlass::bfloat16_t]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_hdim128_bf16_sm80.cu(18): here

/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_kernel.h(275): warning: variable "tQrQ" was declared but never referenced
          detected during:
            instantiation of "void flash::compute_attn_1rowblock<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &, int, int, int) [with Kernel_traits=Flash_fwd_kernel_traits<128, 128, 32, 4, false, false, cutlass::bfloat16_t, Flash_kernel_traits<128, 128, 32, 4, cutlass::bfloat16_t>>, Is_dropout=false, Is_causal=false, Is_even_N=true, Is_even_K=true, Return_softmax=false, Params=Flash_fwd_params]" 
(588): here
            instantiation of "void flash::compute_attn<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &) [with Kernel_traits=Flash_fwd_kernel_traits<128, 128, 32, 4, false, false, cutlass::bfloat16_t, Flash_kernel_traits<128, 128, 32, 4, cutlass::bfloat16_t>>, Is_dropout=false, Is_causal=false, Is_even_N=true, Is_even_K=true, Return_softmax=false, Params=Flash_fwd_params]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(15): here
            instantiation of "void flash_fwd_kernel<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax>(Flash_fwd_params) [with Kernel_traits=Flash_fwd_kernel_traits<128, 128, 32, 4, false, false, cutlass::bfloat16_t, Flash_kernel_traits<128, 128, 32, 4, cutlass::bfloat16_t>>, Is_dropout=false, Is_causal=false, Is_even_N=true, Is_even_K=true, Return_softmax=false]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(34): here
            instantiation of "void run_flash_fwd<Kernel_traits,Is_dropout,Is_causal>(Flash_fwd_params &, cudaStream_t) [with Kernel_traits=Flash_fwd_kernel_traits<128, 128, 32, 4, false, false, cutlass::bfloat16_t, Flash_kernel_traits<128, 128, 32, 4, cutlass::bfloat16_t>>, Is_dropout=false, Is_causal=false]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(118): here
            instantiation of "void run_mha_fwd_hdim128<T>(Flash_fwd_params &, cudaStream_t) [with T=cutlass::bfloat16_t]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_hdim128_bf16_sm80.cu(18): here

/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_kernel.h(140): warning: variable "MMA_M" was declared but never referenced
          detected during:
            instantiation of "void flash::compute_attn_1rowblock<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &, int, int, int) [with Kernel_traits=Flash_fwd_kernel_traits<128, 128, 32, 4, false, false, cutlass::bfloat16_t, Flash_kernel_traits<128, 128, 32, 4, cutlass::bfloat16_t>>, Is_dropout=false, Is_causal=false, Is_even_N=true, Is_even_K=false, Return_softmax=false, Params=Flash_fwd_params]" 
(588): here
            instantiation of "void flash::compute_attn<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &) [with Kernel_traits=Flash_fwd_kernel_traits<128, 128, 32, 4, false, false, cutlass::bfloat16_t, Flash_kernel_traits<128, 128, 32, 4, cutlass::bfloat16_t>>, Is_dropout=false, Is_causal=false, Is_even_N=true, Is_even_K=false, Return_softmax=false, Params=Flash_fwd_params]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(15): here
            instantiation of "void flash_fwd_kernel<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax>(Flash_fwd_params) [with Kernel_traits=Flash_fwd_kernel_traits<128, 128, 32, 4, false, false, cutlass::bfloat16_t, Flash_kernel_traits<128, 128, 32, 4, cutlass::bfloat16_t>>, Is_dropout=false, Is_causal=false, Is_even_N=true, Is_even_K=false, Return_softmax=false]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(34): here
            instantiation of "void run_flash_fwd<Kernel_traits,Is_dropout,Is_causal>(Flash_fwd_params &, cudaStream_t) [with Kernel_traits=Flash_fwd_kernel_traits<128, 128, 32, 4, false, false, cutlass::bfloat16_t, Flash_kernel_traits<128, 128, 32, 4, cutlass::bfloat16_t>>, Is_dropout=false, Is_causal=false]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(118): here
            instantiation of "void run_mha_fwd_hdim128<T>(Flash_fwd_params &, cudaStream_t) [with T=cutlass::bfloat16_t]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_hdim128_bf16_sm80.cu(18): here

/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_kernel.h(275): warning: variable "tQrQ" was declared but never referenced
          detected during:
            instantiation of "void flash::compute_attn_1rowblock<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &, int, int, int) [with Kernel_traits=Flash_fwd_kernel_traits<128, 128, 32, 4, false, false, cutlass::bfloat16_t, Flash_kernel_traits<128, 128, 32, 4, cutlass::bfloat16_t>>, Is_dropout=false, Is_causal=false, Is_even_N=true, Is_even_K=false, Return_softmax=false, Params=Flash_fwd_params]" 
(588): here
            instantiation of "void flash::compute_attn<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &) [with Kernel_traits=Flash_fwd_kernel_traits<128, 128, 32, 4, false, false, cutlass::bfloat16_t, Flash_kernel_traits<128, 128, 32, 4, cutlass::bfloat16_t>>, Is_dropout=false, Is_causal=false, Is_even_N=true, Is_even_K=false, Return_softmax=false, Params=Flash_fwd_params]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(15): here
            instantiation of "void flash_fwd_kernel<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax>(Flash_fwd_params) [with Kernel_traits=Flash_fwd_kernel_traits<128, 128, 32, 4, false, false, cutlass::bfloat16_t, Flash_kernel_traits<128, 128, 32, 4, cutlass::bfloat16_t>>, Is_dropout=false, Is_causal=false, Is_even_N=true, Is_even_K=false, Return_softmax=false]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(34): here
            instantiation of "void run_flash_fwd<Kernel_traits,Is_dropout,Is_causal>(Flash_fwd_params &, cudaStream_t) [with Kernel_traits=Flash_fwd_kernel_traits<128, 128, 32, 4, false, false, cutlass::bfloat16_t, Flash_kernel_traits<128, 128, 32, 4, cutlass::bfloat16_t>>, Is_dropout=false, Is_causal=false]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(118): here
            instantiation of "void run_mha_fwd_hdim128<T>(Flash_fwd_params &, cudaStream_t) [with T=cutlass::bfloat16_t]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_hdim128_bf16_sm80.cu(18): here

/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_kernel.h(140): warning: variable "MMA_M" was declared but never referenced
          detected during:
            instantiation of "void flash::compute_attn_1rowblock<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &, int, int, int) [with Kernel_traits=Flash_fwd_kernel_traits<128, 128, 32, 4, false, false, cutlass::bfloat16_t, Flash_kernel_traits<128, 128, 32, 4, cutlass::bfloat16_t>>, Is_dropout=false, Is_causal=false, Is_even_N=false, Is_even_K=true, Return_softmax=false, Params=Flash_fwd_params]" 
(588): here
            instantiation of "void flash::compute_attn<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &) [with Kernel_traits=Flash_fwd_kernel_traits<128, 128, 32, 4, false, false, cutlass::bfloat16_t, Flash_kernel_traits<128, 128, 32, 4, cutlass::bfloat16_t>>, Is_dropout=false, Is_causal=false, Is_even_N=false, Is_even_K=true, Return_softmax=false, Params=Flash_fwd_params]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(15): here
            instantiation of "void flash_fwd_kernel<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax>(Flash_fwd_params) [with Kernel_traits=Flash_fwd_kernel_traits<128, 128, 32, 4, false, false, cutlass::bfloat16_t, Flash_kernel_traits<128, 128, 32, 4, cutlass::bfloat16_t>>, Is_dropout=false, Is_causal=false, Is_even_N=false, Is_even_K=true, Return_softmax=false]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(34): here
            instantiation of "void run_flash_fwd<Kernel_traits,Is_dropout,Is_causal>(Flash_fwd_params &, cudaStream_t) [with Kernel_traits=Flash_fwd_kernel_traits<128, 128, 32, 4, false, false, cutlass::bfloat16_t, Flash_kernel_traits<128, 128, 32, 4, cutlass::bfloat16_t>>, Is_dropout=false, Is_causal=false]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(118): here
            instantiation of "void run_mha_fwd_hdim128<T>(Flash_fwd_params &, cudaStream_t) [with T=cutlass::bfloat16_t]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_hdim128_bf16_sm80.cu(18): here

/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_kernel.h(275): warning: variable "tQrQ" was declared but never referenced
          detected during:
            instantiation of "void flash::compute_attn_1rowblock<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &, int, int, int) [with Kernel_traits=Flash_fwd_kernel_traits<128, 128, 32, 4, false, false, cutlass::bfloat16_t, Flash_kernel_traits<128, 128, 32, 4, cutlass::bfloat16_t>>, Is_dropout=false, Is_causal=false, Is_even_N=false, Is_even_K=true, Return_softmax=false, Params=Flash_fwd_params]" 
(588): here
            instantiation of "void flash::compute_attn<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &) [with Kernel_traits=Flash_fwd_kernel_traits<128, 128, 32, 4, false, false, cutlass::bfloat16_t, Flash_kernel_traits<128, 128, 32, 4, cutlass::bfloat16_t>>, Is_dropout=false, Is_causal=false, Is_even_N=false, Is_even_K=true, Return_softmax=false, Params=Flash_fwd_params]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(15): here
            instantiation of "void flash_fwd_kernel<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax>(Flash_fwd_params) [with Kernel_traits=Flash_fwd_kernel_traits<128, 128, 32, 4, false, false, cutlass::bfloat16_t, Flash_kernel_traits<128, 128, 32, 4, cutlass::bfloat16_t>>, Is_dropout=false, Is_causal=false, Is_even_N=false, Is_even_K=true, Return_softmax=false]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(34): here
            instantiation of "void run_flash_fwd<Kernel_traits,Is_dropout,Is_causal>(Flash_fwd_params &, cudaStream_t) [with Kernel_traits=Flash_fwd_kernel_traits<128, 128, 32, 4, false, false, cutlass::bfloat16_t, Flash_kernel_traits<128, 128, 32, 4, cutlass::bfloat16_t>>, Is_dropout=false, Is_causal=false]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(118): here
            instantiation of "void run_mha_fwd_hdim128<T>(Flash_fwd_params &, cudaStream_t) [with T=cutlass::bfloat16_t]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_hdim128_bf16_sm80.cu(18): here

/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_kernel.h(140): warning: variable "MMA_M" was declared but never referenced
          detected during:
            instantiation of "void flash::compute_attn_1rowblock<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &, int, int, int) [with Kernel_traits=Flash_fwd_kernel_traits<128, 128, 32, 4, false, false, cutlass::bfloat16_t, Flash_kernel_traits<128, 128, 32, 4, cutlass::bfloat16_t>>, Is_dropout=false, Is_causal=false, Is_even_N=false, Is_even_K=false, Return_softmax=false, Params=Flash_fwd_params]" 
(588): here
            instantiation of "void flash::compute_attn<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &) [with Kernel_traits=Flash_fwd_kernel_traits<128, 128, 32, 4, false, false, cutlass::bfloat16_t, Flash_kernel_traits<128, 128, 32, 4, cutlass::bfloat16_t>>, Is_dropout=false, Is_causal=false, Is_even_N=false, Is_even_K=false, Return_softmax=false, Params=Flash_fwd_params]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(15): here
            instantiation of "void flash_fwd_kernel<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax>(Flash_fwd_params) [with Kernel_traits=Flash_fwd_kernel_traits<128, 128, 32, 4, false, false, cutlass::bfloat16_t, Flash_kernel_traits<128, 128, 32, 4, cutlass::bfloat16_t>>, Is_dropout=false, Is_causal=false, Is_even_N=false, Is_even_K=false, Return_softmax=false]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(34): here
            instantiation of "void run_flash_fwd<Kernel_traits,Is_dropout,Is_causal>(Flash_fwd_params &, cudaStream_t) [with Kernel_traits=Flash_fwd_kernel_traits<128, 128, 32, 4, false, false, cutlass::bfloat16_t, Flash_kernel_traits<128, 128, 32, 4, cutlass::bfloat16_t>>, Is_dropout=false, Is_causal=false]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(118): here
            instantiation of "void run_mha_fwd_hdim128<T>(Flash_fwd_params &, cudaStream_t) [with T=cutlass::bfloat16_t]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_hdim128_bf16_sm80.cu(18): here

/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_kernel.h(275): warning: variable "tQrQ" was declared but never referenced
          detected during:
            instantiation of "void flash::compute_attn_1rowblock<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &, int, int, int) [with Kernel_traits=Flash_fwd_kernel_traits<128, 128, 32, 4, false, false, cutlass::bfloat16_t, Flash_kernel_traits<128, 128, 32, 4, cutlass::bfloat16_t>>, Is_dropout=false, Is_causal=false, Is_even_N=false, Is_even_K=false, Return_softmax=false, Params=Flash_fwd_params]" 
(588): here
            instantiation of "void flash::compute_attn<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &) [with Kernel_traits=Flash_fwd_kernel_traits<128, 128, 32, 4, false, false, cutlass::bfloat16_t, Flash_kernel_traits<128, 128, 32, 4, cutlass::bfloat16_t>>, Is_dropout=false, Is_causal=false, Is_even_N=false, Is_even_K=false, Return_softmax=false, Params=Flash_fwd_params]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(15): here
            instantiation of "void flash_fwd_kernel<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax>(Flash_fwd_params) [with Kernel_traits=Flash_fwd_kernel_traits<128, 128, 32, 4, false, false, cutlass::bfloat16_t, Flash_kernel_traits<128, 128, 32, 4, cutlass::bfloat16_t>>, Is_dropout=false, Is_causal=false, Is_even_N=false, Is_even_K=false, Return_softmax=false]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(34): here
            instantiation of "void run_flash_fwd<Kernel_traits,Is_dropout,Is_causal>(Flash_fwd_params &, cudaStream_t) [with Kernel_traits=Flash_fwd_kernel_traits<128, 128, 32, 4, false, false, cutlass::bfloat16_t, Flash_kernel_traits<128, 128, 32, 4, cutlass::bfloat16_t>>, Is_dropout=false, Is_causal=false]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(118): here
            instantiation of "void run_mha_fwd_hdim128<T>(Flash_fwd_params &, cudaStream_t) [with T=cutlass::bfloat16_t]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_hdim128_bf16_sm80.cu(18): here

/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_kernel.h(140): warning: variable "MMA_M" was declared but never referenced
          detected during:
            instantiation of "void flash::compute_attn_1rowblock<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &, int, int, int) [with Kernel_traits=Flash_fwd_kernel_traits<128, 128, 64, 4, false, false, cutlass::bfloat16_t, Flash_kernel_traits<128, 128, 64, 4, cutlass::bfloat16_t>>, Is_dropout=false, Is_causal=false, Is_even_N=true, Is_even_K=true, Return_softmax=false, Params=Flash_fwd_params]" 
(588): here
            instantiation of "void flash::compute_attn<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &) [with Kernel_traits=Flash_fwd_kernel_traits<128, 128, 64, 4, false, false, cutlass::bfloat16_t, Flash_kernel_traits<128, 128, 64, 4, cutlass::bfloat16_t>>, Is_dropout=false, Is_causal=false, Is_even_N=true, Is_even_K=true, Return_softmax=false, Params=Flash_fwd_params]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(15): here
            instantiation of "void flash_fwd_kernel<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax>(Flash_fwd_params) [with Kernel_traits=Flash_fwd_kernel_traits<128, 128, 64, 4, false, false, cutlass::bfloat16_t, Flash_kernel_traits<128, 128, 64, 4, cutlass::bfloat16_t>>, Is_dropout=false, Is_causal=false, Is_even_N=true, Is_even_K=true, Return_softmax=false]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(34): here
            instantiation of "void run_flash_fwd<Kernel_traits,Is_dropout,Is_causal>(Flash_fwd_params &, cudaStream_t) [with Kernel_traits=Flash_fwd_kernel_traits<128, 128, 64, 4, false, false, cutlass::bfloat16_t, Flash_kernel_traits<128, 128, 64, 4, cutlass::bfloat16_t>>, Is_dropout=false, Is_causal=false]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(118): here
            instantiation of "void run_mha_fwd_hdim128<T>(Flash_fwd_params &, cudaStream_t) [with T=cutlass::bfloat16_t]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_hdim128_bf16_sm80.cu(18): here

/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_kernel.h(275): warning: variable "tQrQ" was declared but never referenced
          detected during:
            instantiation of "void flash::compute_attn_1rowblock<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &, int, int, int) [with Kernel_traits=Flash_fwd_kernel_traits<128, 128, 64, 4, false, false, cutlass::bfloat16_t, Flash_kernel_traits<128, 128, 64, 4, cutlass::bfloat16_t>>, Is_dropout=false, Is_causal=false, Is_even_N=true, Is_even_K=true, Return_softmax=false, Params=Flash_fwd_params]" 
(588): here
            instantiation of "void flash::compute_attn<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &) [with Kernel_traits=Flash_fwd_kernel_traits<128, 128, 64, 4, false, false, cutlass::bfloat16_t, Flash_kernel_traits<128, 128, 64, 4, cutlass::bfloat16_t>>, Is_dropout=false, Is_causal=false, Is_even_N=true, Is_even_K=true, Return_softmax=false, Params=Flash_fwd_params]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(15): here
            instantiation of "void flash_fwd_kernel<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax>(Flash_fwd_params) [with Kernel_traits=Flash_fwd_kernel_traits<128, 128, 64, 4, false, false, cutlass::bfloat16_t, Flash_kernel_traits<128, 128, 64, 4, cutlass::bfloat16_t>>, Is_dropout=false, Is_causal=false, Is_even_N=true, Is_even_K=true, Return_softmax=false]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(34): here
            instantiation of "void run_flash_fwd<Kernel_traits,Is_dropout,Is_causal>(Flash_fwd_params &, cudaStream_t) [with Kernel_traits=Flash_fwd_kernel_traits<128, 128, 64, 4, false, false, cutlass::bfloat16_t, Flash_kernel_traits<128, 128, 64, 4, cutlass::bfloat16_t>>, Is_dropout=false, Is_causal=false]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(118): here
            instantiation of "void run_mha_fwd_hdim128<T>(Flash_fwd_params &, cudaStream_t) [with T=cutlass::bfloat16_t]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_hdim128_bf16_sm80.cu(18): here

/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_kernel.h(140): warning: variable "MMA_M" was declared but never referenced
          detected during:
            instantiation of "void flash::compute_attn_1rowblock<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &, int, int, int) [with Kernel_traits=Flash_fwd_kernel_traits<128, 128, 64, 4, false, false, cutlass::bfloat16_t, Flash_kernel_traits<128, 128, 64, 4, cutlass::bfloat16_t>>, Is_dropout=false, Is_causal=false, Is_even_N=true, Is_even_K=false, Return_softmax=false, Params=Flash_fwd_params]" 
(588): here
            instantiation of "void flash::compute_attn<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &) [with Kernel_traits=Flash_fwd_kernel_traits<128, 128, 64, 4, false, false, cutlass::bfloat16_t, Flash_kernel_traits<128, 128, 64, 4, cutlass::bfloat16_t>>, Is_dropout=false, Is_causal=false, Is_even_N=true, Is_even_K=false, Return_softmax=false, Params=Flash_fwd_params]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(15): here
            instantiation of "void flash_fwd_kernel<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax>(Flash_fwd_params) [with Kernel_traits=Flash_fwd_kernel_traits<128, 128, 64, 4, false, false, cutlass::bfloat16_t, Flash_kernel_traits<128, 128, 64, 4, cutlass::bfloat16_t>>, Is_dropout=false, Is_causal=false, Is_even_N=true, Is_even_K=false, Return_softmax=false]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(34): here
            instantiation of "void run_flash_fwd<Kernel_traits,Is_dropout,Is_causal>(Flash_fwd_params &, cudaStream_t) [with Kernel_traits=Flash_fwd_kernel_traits<128, 128, 64, 4, false, false, cutlass::bfloat16_t, Flash_kernel_traits<128, 128, 64, 4, cutlass::bfloat16_t>>, Is_dropout=false, Is_causal=false]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(118): here
            instantiation of "void run_mha_fwd_hdim128<T>(Flash_fwd_params &, cudaStream_t) [with T=cutlass::bfloat16_t]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_hdim128_bf16_sm80.cu(18): here

/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_kernel.h(275): warning: variable "tQrQ" was declared but never referenced
          detected during:
            instantiation of "void flash::compute_attn_1rowblock<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &, int, int, int) [with Kernel_traits=Flash_fwd_kernel_traits<128, 128, 64, 4, false, false, cutlass::bfloat16_t, Flash_kernel_traits<128, 128, 64, 4, cutlass::bfloat16_t>>, Is_dropout=false, Is_causal=false, Is_even_N=true, Is_even_K=false, Return_softmax=false, Params=Flash_fwd_params]" 
(588): here
            instantiation of "void flash::compute_attn<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &) [with Kernel_traits=Flash_fwd_kernel_traits<128, 128, 64, 4, false, false, cutlass::bfloat16_t, Flash_kernel_traits<128, 128, 64, 4, cutlass::bfloat16_t>>, Is_dropout=false, Is_causal=false, Is_even_N=true, Is_even_K=false, Return_softmax=false, Params=Flash_fwd_params]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(15): here
            instantiation of "void flash_fwd_kernel<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax>(Flash_fwd_params) [with Kernel_traits=Flash_fwd_kernel_traits<128, 128, 64, 4, false, false, cutlass::bfloat16_t, Flash_kernel_traits<128, 128, 64, 4, cutlass::bfloat16_t>>, Is_dropout=false, Is_causal=false, Is_even_N=true, Is_even_K=false, Return_softmax=false]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(34): here
            instantiation of "void run_flash_fwd<Kernel_traits,Is_dropout,Is_causal>(Flash_fwd_params &, cudaStream_t) [with Kernel_traits=Flash_fwd_kernel_traits<128, 128, 64, 4, false, false, cutlass::bfloat16_t, Flash_kernel_traits<128, 128, 64, 4, cutlass::bfloat16_t>>, Is_dropout=false, Is_causal=false]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(118): here
            instantiation of "void run_mha_fwd_hdim128<T>(Flash_fwd_params &, cudaStream_t) [with T=cutlass::bfloat16_t]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_hdim128_bf16_sm80.cu(18): here

/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_kernel.h(140): warning: variable "MMA_M" was declared but never referenced
          detected during:
            instantiation of "void flash::compute_attn_1rowblock<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &, int, int, int) [with Kernel_traits=Flash_fwd_kernel_traits<128, 128, 64, 4, false, false, cutlass::bfloat16_t, Flash_kernel_traits<128, 128, 64, 4, cutlass::bfloat16_t>>, Is_dropout=false, Is_causal=false, Is_even_N=false, Is_even_K=true, Return_softmax=false, Params=Flash_fwd_params]" 
(588): here
            instantiation of "void flash::compute_attn<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &) [with Kernel_traits=Flash_fwd_kernel_traits<128, 128, 64, 4, false, false, cutlass::bfloat16_t, Flash_kernel_traits<128, 128, 64, 4, cutlass::bfloat16_t>>, Is_dropout=false, Is_causal=false, Is_even_N=false, Is_even_K=true, Return_softmax=false, Params=Flash_fwd_params]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(15): here
            instantiation of "void flash_fwd_kernel<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax>(Flash_fwd_params) [with Kernel_traits=Flash_fwd_kernel_traits<128, 128, 64, 4, false, false, cutlass::bfloat16_t, Flash_kernel_traits<128, 128, 64, 4, cutlass::bfloat16_t>>, Is_dropout=false, Is_causal=false, Is_even_N=false, Is_even_K=true, Return_softmax=false]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(34): here
            instantiation of "void run_flash_fwd<Kernel_traits,Is_dropout,Is_causal>(Flash_fwd_params &, cudaStream_t) [with Kernel_traits=Flash_fwd_kernel_traits<128, 128, 64, 4, false, false, cutlass::bfloat16_t, Flash_kernel_traits<128, 128, 64, 4, cutlass::bfloat16_t>>, Is_dropout=false, Is_causal=false]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(118): here
            instantiation of "void run_mha_fwd_hdim128<T>(Flash_fwd_params &, cudaStream_t) [with T=cutlass::bfloat16_t]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_hdim128_bf16_sm80.cu(18): here

/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_kernel.h(275): warning: variable "tQrQ" was declared but never referenced
          detected during:
            instantiation of "void flash::compute_attn_1rowblock<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &, int, int, int) [with Kernel_traits=Flash_fwd_kernel_traits<128, 128, 64, 4, false, false, cutlass::bfloat16_t, Flash_kernel_traits<128, 128, 64, 4, cutlass::bfloat16_t>>, Is_dropout=false, Is_causal=false, Is_even_N=false, Is_even_K=true, Return_softmax=false, Params=Flash_fwd_params]" 
(588): here
            instantiation of "void flash::compute_attn<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &) [with Kernel_traits=Flash_fwd_kernel_traits<128, 128, 64, 4, false, false, cutlass::bfloat16_t, Flash_kernel_traits<128, 128, 64, 4, cutlass::bfloat16_t>>, Is_dropout=false, Is_causal=false, Is_even_N=false, Is_even_K=true, Return_softmax=false, Params=Flash_fwd_params]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(15): here
            instantiation of "void flash_fwd_kernel<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax>(Flash_fwd_params) [with Kernel_traits=Flash_fwd_kernel_traits<128, 128, 64, 4, false, false, cutlass::bfloat16_t, Flash_kernel_traits<128, 128, 64, 4, cutlass::bfloat16_t>>, Is_dropout=false, Is_causal=false, Is_even_N=false, Is_even_K=true, Return_softmax=false]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(34): here
            instantiation of "void run_flash_fwd<Kernel_traits,Is_dropout,Is_causal>(Flash_fwd_params &, cudaStream_t) [with Kernel_traits=Flash_fwd_kernel_traits<128, 128, 64, 4, false, false, cutlass::bfloat16_t, Flash_kernel_traits<128, 128, 64, 4, cutlass::bfloat16_t>>, Is_dropout=false, Is_causal=false]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(118): here
            instantiation of "void run_mha_fwd_hdim128<T>(Flash_fwd_params &, cudaStream_t) [with T=cutlass::bfloat16_t]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_hdim128_bf16_sm80.cu(18): here

/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_kernel.h(140): warning: variable "MMA_M" was declared but never referenced
          detected during:
            instantiation of "void flash::compute_attn_1rowblock<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &, int, int, int) [with Kernel_traits=Flash_fwd_kernel_traits<128, 128, 64, 4, false, false, cutlass::bfloat16_t, Flash_kernel_traits<128, 128, 64, 4, cutlass::bfloat16_t>>, Is_dropout=false, Is_causal=false, Is_even_N=false, Is_even_K=false, Return_softmax=false, Params=Flash_fwd_params]" 
(588): here
            instantiation of "void flash::compute_attn<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &) [with Kernel_traits=Flash_fwd_kernel_traits<128, 128, 64, 4, false, false, cutlass::bfloat16_t, Flash_kernel_traits<128, 128, 64, 4, cutlass::bfloat16_t>>, Is_dropout=false, Is_causal=false, Is_even_N=false, Is_even_K=false, Return_softmax=false, Params=Flash_fwd_params]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(15): here
            instantiation of "void flash_fwd_kernel<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax>(Flash_fwd_params) [with Kernel_traits=Flash_fwd_kernel_traits<128, 128, 64, 4, false, false, cutlass::bfloat16_t, Flash_kernel_traits<128, 128, 64, 4, cutlass::bfloat16_t>>, Is_dropout=false, Is_causal=false, Is_even_N=false, Is_even_K=false, Return_softmax=false]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(34): here
            instantiation of "void run_flash_fwd<Kernel_traits,Is_dropout,Is_causal>(Flash_fwd_params &, cudaStream_t) [with Kernel_traits=Flash_fwd_kernel_traits<128, 128, 64, 4, false, false, cutlass::bfloat16_t, Flash_kernel_traits<128, 128, 64, 4, cutlass::bfloat16_t>>, Is_dropout=false, Is_causal=false]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(118): here
            instantiation of "void run_mha_fwd_hdim128<T>(Flash_fwd_params &, cudaStream_t) [with T=cutlass::bfloat16_t]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_hdim128_bf16_sm80.cu(18): here

/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_kernel.h(275): warning: variable "tQrQ" was declared but never referenced
          detected during:
            instantiation of "void flash::compute_attn_1rowblock<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &, int, int, int) [with Kernel_traits=Flash_fwd_kernel_traits<128, 128, 64, 4, false, false, cutlass::bfloat16_t, Flash_kernel_traits<128, 128, 64, 4, cutlass::bfloat16_t>>, Is_dropout=false, Is_causal=false, Is_even_N=false, Is_even_K=false, Return_softmax=false, Params=Flash_fwd_params]" 
(588): here
            instantiation of "void flash::compute_attn<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &) [with Kernel_traits=Flash_fwd_kernel_traits<128, 128, 64, 4, false, false, cutlass::bfloat16_t, Flash_kernel_traits<128, 128, 64, 4, cutlass::bfloat16_t>>, Is_dropout=false, Is_causal=false, Is_even_N=false, Is_even_K=false, Return_softmax=false, Params=Flash_fwd_params]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(15): here
            instantiation of "void flash_fwd_kernel<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax>(Flash_fwd_params) [with Kernel_traits=Flash_fwd_kernel_traits<128, 128, 64, 4, false, false, cutlass::bfloat16_t, Flash_kernel_traits<128, 128, 64, 4, cutlass::bfloat16_t>>, Is_dropout=false, Is_causal=false, Is_even_N=false, Is_even_K=false, Return_softmax=false]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(34): here
            instantiation of "void run_flash_fwd<Kernel_traits,Is_dropout,Is_causal>(Flash_fwd_params &, cudaStream_t) [with Kernel_traits=Flash_fwd_kernel_traits<128, 128, 64, 4, false, false, cutlass::bfloat16_t, Flash_kernel_traits<128, 128, 64, 4, cutlass::bfloat16_t>>, Is_dropout=false, Is_causal=false]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(118): here
            instantiation of "void run_mha_fwd_hdim128<T>(Flash_fwd_params &, cudaStream_t) [with T=cutlass::bfloat16_t]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_hdim128_bf16_sm80.cu(18): here

ptxas info    : 2 bytes gmem
ptxas info    : Compiling entry function '_Z16flash_fwd_kernelI23Flash_fwd_kernel_traitsILi128ELi128ELi64ELi4ELb0ELb0EN7cutlass10bfloat16_tE19Flash_kernel_traitsILi128ELi128ELi64ELi4ES2_EELb0ELb0ELb0ELb0ELb0EEv16Flash_fwd_params' for 'sm_80'
ptxas info    : Function properties for _Z16flash_fwd_kernelI23Flash_fwd_kernel_traitsILi128ELi128ELi64ELi4ELb0ELb0EN7cutlass10bfloat16_tE19Flash_kernel_traitsILi128ELi128ELi64ELi4ES2_EELb0ELb0ELb0ELb0ELb0EEv16Flash_fwd_params
    24 bytes stack frame, 32 bytes spill stores, 32 bytes spill loads
ptxas info    : Used 255 registers, 584 bytes cmem[0]
ptxas info    : Compiling entry function '_Z16flash_fwd_kernelI23Flash_fwd_kernel_traitsILi128ELi128ELi64ELi4ELb0ELb0EN7cutlass10bfloat16_tE19Flash_kernel_traitsILi128ELi128ELi64ELi4ES2_EELb0ELb0ELb0ELb1ELb0EEv16Flash_fwd_params' for 'sm_80'
ptxas info    : Function properties for _Z16flash_fwd_kernelI23Flash_fwd_kernel_traitsILi128ELi128ELi64ELi4ELb0ELb0EN7cutlass10bfloat16_tE19Flash_kernel_traitsILi128ELi128ELi64ELi4ES2_EELb0ELb0ELb0ELb1ELb0EEv16Flash_fwd_params
    16 bytes stack frame, 32 bytes spill stores, 16 bytes spill loads
ptxas info    : Used 255 registers, 584 bytes cmem[0]
ptxas info    : Compiling entry function '_Z16flash_fwd_kernelI23Flash_fwd_kernel_traitsILi128ELi128ELi64ELi4ELb0ELb0EN7cutlass10bfloat16_tE19Flash_kernel_traitsILi128ELi128ELi64ELi4ES2_EELb0ELb0ELb1ELb0ELb0EEv16Flash_fwd_params' for 'sm_80'
ptxas info    : Function properties for _Z16flash_fwd_kernelI23Flash_fwd_kernel_traitsILi128ELi128ELi64ELi4ELb0ELb0EN7cutlass10bfloat16_tE19Flash_kernel_traitsILi128ELi128ELi64ELi4ES2_EELb0ELb0ELb1ELb0ELb0EEv16Flash_fwd_params
    16 bytes stack frame, 24 bytes spill stores, 24 bytes spill loads
ptxas info    : Used 255 registers, 584 bytes cmem[0]
ptxas info    : Compiling entry function '_Z16flash_fwd_kernelI23Flash_fwd_kernel_traitsILi128ELi128ELi64ELi4ELb0ELb0EN7cutlass10bfloat16_tE19Flash_kernel_traitsILi128ELi128ELi64ELi4ES2_EELb0ELb0ELb1ELb1ELb0EEv16Flash_fwd_params' for 'sm_80'
ptxas info    : Function properties for _Z16flash_fwd_kernelI23Flash_fwd_kernel_traitsILi128ELi128ELi64ELi4ELb0ELb0EN7cutlass10bfloat16_tE19Flash_kernel_traitsILi128ELi128ELi64ELi4ES2_EELb0ELb0ELb1ELb1ELb0EEv16Flash_fwd_params
    24 bytes stack frame, 36 bytes spill stores, 36 bytes spill loads
ptxas info    : Used 255 registers, 584 bytes cmem[0]
ptxas info    : Compiling entry function '_Z16flash_fwd_kernelI23Flash_fwd_kernel_traitsILi128ELi128ELi32ELi4ELb0ELb0EN7cutlass10bfloat16_tE19Flash_kernel_traitsILi128ELi128ELi32ELi4ES2_EELb0ELb0ELb0ELb0ELb0EEv16Flash_fwd_params' for 'sm_80'
ptxas info    : Function properties for _Z16flash_fwd_kernelI23Flash_fwd_kernel_traitsILi128ELi128ELi32ELi4ELb0ELb0EN7cutlass10bfloat16_tE19Flash_kernel_traitsILi128ELi128ELi32ELi4ES2_EELb0ELb0ELb0ELb0ELb0EEv16Flash_fwd_params
    0 bytes stack frame, 0 bytes spill stores, 0 bytes spill loads
ptxas info    : Used 243 registers, 584 bytes cmem[0]
ptxas info    : Compiling entry function '_Z16flash_fwd_kernelI23Flash_fwd_kernel_traitsILi128ELi128ELi32ELi4ELb0ELb0EN7cutlass10bfloat16_tE19Flash_kernel_traitsILi128ELi128ELi32ELi4ES2_EELb0ELb0ELb0ELb1ELb0EEv16Flash_fwd_params' for 'sm_80'
ptxas info    : Function properties for _Z16flash_fwd_kernelI23Flash_fwd_kernel_traitsILi128ELi128ELi32ELi4ELb0ELb0EN7cutlass10bfloat16_tE19Flash_kernel_traitsILi128ELi128ELi32ELi4ES2_EELb0ELb0ELb0ELb1ELb0EEv16Flash_fwd_params
    0 bytes stack frame, 0 bytes spill stores, 0 bytes spill loads
ptxas info    : Used 242 registers, 584 bytes cmem[0]
ptxas info    : Compiling entry function '_Z16flash_fwd_kernelI23Flash_fwd_kernel_traitsILi128ELi128ELi32ELi4ELb0ELb0EN7cutlass10bfloat16_tE19Flash_kernel_traitsILi128ELi128ELi32ELi4ES2_EELb0ELb0ELb1ELb0ELb0EEv16Flash_fwd_params' for 'sm_80'
ptxas info    : Function properties for _Z16flash_fwd_kernelI23Flash_fwd_kernel_traitsILi128ELi128ELi32ELi4ELb0ELb0EN7cutlass10bfloat16_tE19Flash_kernel_traitsILi128ELi128ELi32ELi4ES2_EELb0ELb0ELb1ELb0ELb0EEv16Flash_fwd_params
    0 bytes stack frame, 0 bytes spill stores, 0 bytes spill loads
ptxas info    : Used 242 registers, 584 bytes cmem[0]
ptxas info    : Compiling entry function '_Z16flash_fwd_kernelI23Flash_fwd_kernel_traitsILi128ELi128ELi32ELi4ELb0ELb0EN7cutlass10bfloat16_tE19Flash_kernel_traitsILi128ELi128ELi32ELi4ES2_EELb0ELb0ELb1ELb1ELb0EEv16Flash_fwd_params' for 'sm_80'
ptxas info    : Function properties for _Z16flash_fwd_kernelI23Flash_fwd_kernel_traitsILi128ELi128ELi32ELi4ELb0ELb0EN7cutlass10bfloat16_tE19Flash_kernel_traitsILi128ELi128ELi32ELi4ES2_EELb0ELb0ELb1ELb1ELb0EEv16Flash_fwd_params
    0 bytes stack frame, 0 bytes spill stores, 0 bytes spill loads
ptxas info    : Used 242 registers, 584 bytes cmem[0]
ptxas info    : Compiling entry function '_Z16flash_fwd_kernelI23Flash_fwd_kernel_traitsILi128ELi128ELi64ELi4ELb0ELb0EN7cutlass10bfloat16_tE19Flash_kernel_traitsILi128ELi128ELi64ELi4ES2_EELb0ELb1ELb0ELb0ELb0EEv16Flash_fwd_params' for 'sm_80'
ptxas info    : Function properties for _Z16flash_fwd_kernelI23Flash_fwd_kernel_traitsILi128ELi128ELi64ELi4ELb0ELb0EN7cutlass10bfloat16_tE19Flash_kernel_traitsILi128ELi128ELi64ELi4ES2_EELb0ELb1ELb0ELb0ELb0EEv16Flash_fwd_params
    256 bytes stack frame, 320 bytes spill stores, 356 bytes spill loads
ptxas info    : Used 255 registers, 584 bytes cmem[0]
ptxas info    : Compiling entry function '_Z16flash_fwd_kernelI23Flash_fwd_kernel_traitsILi128ELi128ELi64ELi4ELb0ELb0EN7cutlass10bfloat16_tE19Flash_kernel_traitsILi128ELi128ELi64ELi4ES2_EELb0ELb1ELb0ELb1ELb0EEv16Flash_fwd_params' for 'sm_80'
ptxas info    : Function properties for _Z16flash_fwd_kernelI23Flash_fwd_kernel_traitsILi128ELi128ELi64ELi4ELb0ELb0EN7cutlass10bfloat16_tE19Flash_kernel_traitsILi128ELi128ELi64ELi4ES2_EELb0ELb1ELb0ELb1ELb0EEv16Flash_fwd_params
    192 bytes stack frame, 292 bytes spill stores, 316 bytes spill loads
ptxas info    : Used 255 registers, 584 bytes cmem[0]
ptxas info    : Compiling entry function '_Z16flash_fwd_kernelI23Flash_fwd_kernel_traitsILi128ELi128ELi64ELi4ELb0ELb0EN7cutlass10bfloat16_tE19Flash_kernel_traitsILi128ELi128ELi64ELi4ES2_EELb0ELb1ELb1ELb0ELb0EEv16Flash_fwd_params' for 'sm_80'
ptxas info    : Function properties for _Z16flash_fwd_kernelI23Flash_fwd_kernel_traitsILi128ELi128ELi64ELi4ELb0ELb0EN7cutlass10bfloat16_tE19Flash_kernel_traitsILi128ELi128ELi64ELi4ES2_EELb0ELb1ELb1ELb0ELb0EEv16Flash_fwd_params
    128 bytes stack frame, 192 bytes spill stores, 220 bytes spill loads
ptxas info    : Used 255 registers, 584 bytes cmem[0]
ptxas info    : Compiling entry function '_Z16flash_fwd_kernelI23Flash_fwd_kernel_traitsILi128ELi128ELi64ELi4ELb0ELb0EN7cutlass10bfloat16_tE19Flash_kernel_traitsILi128ELi128ELi64ELi4ES2_EELb0ELb1ELb1ELb1ELb0EEv16Flash_fwd_params' for 'sm_80'
ptxas info    : Function properties for _Z16flash_fwd_kernelI23Flash_fwd_kernel_traitsILi128ELi128ELi64ELi4ELb0ELb0EN7cutlass10bfloat16_tE19Flash_kernel_traitsILi128ELi128ELi64ELi4ES2_EELb0ELb1ELb1ELb1ELb0EEv16Flash_fwd_params
    160 bytes stack frame, 220 bytes spill stores, 180 bytes spill loads
ptxas info    : Used 255 registers, 584 bytes cmem[0]
ptxas info    : Compiling entry function '_Z16flash_fwd_kernelI23Flash_fwd_kernel_traitsILi128ELi64ELi64ELi4ELb0ELb0EN7cutlass10bfloat16_tE19Flash_kernel_traitsILi128ELi64ELi64ELi4ES2_EELb0ELb1ELb0ELb0ELb0EEv16Flash_fwd_params' for 'sm_80'
ptxas info    : Function properties for _Z16flash_fwd_kernelI23Flash_fwd_kernel_traitsILi128ELi64ELi64ELi4ELb0ELb0EN7cutlass10bfloat16_tE19Flash_kernel_traitsILi128ELi64ELi64ELi4ES2_EELb0ELb1ELb0ELb0ELb0EEv16Flash_fwd_params
    0 bytes stack frame, 0 bytes spill stores, 0 bytes spill loads
ptxas info    : Used 163 registers, 584 bytes cmem[0]
ptxas info    : Compiling entry function '_Z16flash_fwd_kernelI23Flash_fwd_kernel_traitsILi128ELi64ELi64ELi4ELb0ELb0EN7cutlass10bfloat16_tE19Flash_kernel_traitsILi128ELi64ELi64ELi4ES2_EELb0ELb1ELb0ELb1ELb0EEv16Flash_fwd_params' for 'sm_80'
ptxas info    : Function properties for _Z16flash_fwd_kernelI23Flash_fwd_kernel_traitsILi128ELi64ELi64ELi4ELb0ELb0EN7cutlass10bfloat16_tE19Flash_kernel_traitsILi128ELi64ELi64ELi4ES2_EELb0ELb1ELb0ELb1ELb0EEv16Flash_fwd_params
    0 bytes stack frame, 0 bytes spill stores, 0 bytes spill loads
ptxas info    : Used 160 registers, 584 bytes cmem[0]
ptxas info    : Compiling entry function '_Z16flash_fwd_kernelI23Flash_fwd_kernel_traitsILi128ELi64ELi64ELi4ELb0ELb0EN7cutlass10bfloat16_tE19Flash_kernel_traitsILi128ELi64ELi64ELi4ES2_EELb0ELb1ELb1ELb0ELb0EEv16Flash_fwd_params' for 'sm_80'
ptxas info    : Function properties for _Z16flash_fwd_kernelI23Flash_fwd_kernel_traitsILi128ELi64ELi64ELi4ELb0ELb0EN7cutlass10bfloat16_tE19Flash_kernel_traitsILi128ELi64ELi64ELi4ES2_EELb0ELb1ELb1ELb0ELb0EEv16Flash_fwd_params
    0 bytes stack frame, 0 bytes spill stores, 0 bytes spill loads
ptxas info    : Used 167 registers, 584 bytes cmem[0]
ptxas info    : Compiling entry function '_Z16flash_fwd_kernelI23Flash_fwd_kernel_traitsILi128ELi64ELi64ELi4ELb0ELb0EN7cutlass10bfloat16_tE19Flash_kernel_traitsILi128ELi64ELi64ELi4ES2_EELb0ELb1ELb1ELb1ELb0EEv16Flash_fwd_params' for 'sm_80'
ptxas info    : Function properties for _Z16flash_fwd_kernelI23Flash_fwd_kernel_traitsILi128ELi64ELi64ELi4ELb0ELb0EN7cutlass10bfloat16_tE19Flash_kernel_traitsILi128ELi64ELi64ELi4ES2_EELb0ELb1ELb1ELb1ELb0EEv16Flash_fwd_params
    0 bytes stack frame, 0 bytes spill stores, 0 bytes spill loads
ptxas info    : Used 166 registers, 584 bytes cmem[0]
ptxas info    : Compiling entry function '_Z16flash_fwd_kernelI23Flash_fwd_kernel_traitsILi128ELi128ELi32ELi4ELb0ELb0EN7cutlass10bfloat16_tE19Flash_kernel_traitsILi128ELi128ELi32ELi4ES2_EELb1ELb0ELb0ELb0ELb0EEv16Flash_fwd_params' for 'sm_80'
ptxas info    : Function properties for _Z16flash_fwd_kernelI23Flash_fwd_kernel_traitsILi128ELi128ELi32ELi4ELb0ELb0EN7cutlass10bfloat16_tE19Flash_kernel_traitsILi128ELi128ELi32ELi4ES2_EELb1ELb0ELb0ELb0ELb0EEv16Flash_fwd_params
    0 bytes stack frame, 0 bytes spill stores, 0 bytes spill loads
ptxas info    : Used 252 registers, 584 bytes cmem[0]
ptxas info    : Compiling entry function '_Z16flash_fwd_kernelI23Flash_fwd_kernel_traitsILi128ELi128ELi32ELi4ELb0ELb0EN7cutlass10bfloat16_tE19Flash_kernel_traitsILi128ELi128ELi32ELi4ES2_EELb1ELb0ELb0ELb0ELb1EEv16Flash_fwd_params' for 'sm_80'
ptxas info    : Function properties for _Z16flash_fwd_kernelI23Flash_fwd_kernel_traitsILi128ELi128ELi32ELi4ELb0ELb0EN7cutlass10bfloat16_tE19Flash_kernel_traitsILi128ELi128ELi32ELi4ES2_EELb1ELb0ELb0ELb0ELb1EEv16Flash_fwd_params
    8 bytes stack frame, 4 bytes spill stores, 4 bytes spill loads
ptxas info    : Used 255 registers, 584 bytes cmem[0]
ptxas info    : Compiling entry function '_Z16flash_fwd_kernelI23Flash_fwd_kernel_traitsILi128ELi128ELi32ELi4ELb0ELb0EN7cutlass10bfloat16_tE19Flash_kernel_traitsILi128ELi128ELi32ELi4ES2_EELb1ELb0ELb0ELb1ELb0EEv16Flash_fwd_params' for 'sm_80'
ptxas info    : Function properties for _Z16flash_fwd_kernelI23Flash_fwd_kernel_traitsILi128ELi128ELi32ELi4ELb0ELb0EN7cutlass10bfloat16_tE19Flash_kernel_traitsILi128ELi128ELi32ELi4ES2_EELb1ELb0ELb0ELb1ELb0EEv16Flash_fwd_params
    0 bytes stack frame, 0 bytes spill stores, 0 bytes spill loads
ptxas info    : Used 252 registers, 584 bytes cmem[0]
ptxas info    : Compiling entry function '_Z16flash_fwd_kernelI23Flash_fwd_kernel_traitsILi128ELi128ELi32ELi4ELb0ELb0EN7cutlass10bfloat16_tE19Flash_kernel_traitsILi128ELi128ELi32ELi4ES2_EELb1ELb0ELb0ELb1ELb1EEv16Flash_fwd_params' for 'sm_80'
ptxas info    : Function properties for _Z16flash_fwd_kernelI23Flash_fwd_kernel_traitsILi128ELi128ELi32ELi4ELb0ELb0EN7cutlass10bfloat16_tE19Flash_kernel_traitsILi128ELi128ELi32ELi4ES2_EELb1ELb0ELb0ELb1ELb1EEv16Flash_fwd_params
    0 bytes stack frame, 0 bytes spill stores, 0 bytes spill loads
ptxas info    : Used 255 registers, 584 bytes cmem[0]
ptxas info    : Compiling entry function '_Z16flash_fwd_kernelI23Flash_fwd_kernel_traitsILi128ELi128ELi32ELi4ELb0ELb0EN7cutlass10bfloat16_tE19Flash_kernel_traitsILi128ELi128ELi32ELi4ES2_EELb1ELb0ELb1ELb0ELb0EEv16Flash_fwd_params' for 'sm_80'
ptxas info    : Function properties for _Z16flash_fwd_kernelI23Flash_fwd_kernel_traitsILi128ELi128ELi32ELi4ELb0ELb0EN7cutlass10bfloat16_tE19Flash_kernel_traitsILi128ELi128ELi32ELi4ES2_EELb1ELb0ELb1ELb0ELb0EEv16Flash_fwd_params
    0 bytes stack frame, 0 bytes spill stores, 0 bytes spill loads
ptxas info    : Used 252 registers, 584 bytes cmem[0]
ptxas info    : Compiling entry function '_Z16flash_fwd_kernelI23Flash_fwd_kernel_traitsILi128ELi128ELi32ELi4ELb0ELb0EN7cutlass10bfloat16_tE19Flash_kernel_traitsILi128ELi128ELi32ELi4ES2_EELb1ELb0ELb1ELb0ELb1EEv16Flash_fwd_params' for 'sm_80'
ptxas info    : Function properties for _Z16flash_fwd_kernelI23Flash_fwd_kernel_traitsILi128ELi128ELi32ELi4ELb0ELb0EN7cutlass10bfloat16_tE19Flash_kernel_traitsILi128ELi128ELi32ELi4ES2_EELb1ELb0ELb1ELb0ELb1EEv16Flash_fwd_params
    0 bytes stack frame, 0 bytes spill stores, 0 bytes spill loads
ptxas info    : Used 255 registers, 584 bytes cmem[0]
ptxas info    : Compiling entry function '_Z16flash_fwd_kernelI23Flash_fwd_kernel_traitsILi128ELi128ELi32ELi4ELb0ELb0EN7cutlass10bfloat16_tE19Flash_kernel_traitsILi128ELi128ELi32ELi4ES2_EELb1ELb0ELb1ELb1ELb0EEv16Flash_fwd_params' for 'sm_80'
ptxas info    : Function properties for _Z16flash_fwd_kernelI23Flash_fwd_kernel_traitsILi128ELi128ELi32ELi4ELb0ELb0EN7cutlass10bfloat16_tE19Flash_kernel_traitsILi128ELi128ELi32ELi4ES2_EELb1ELb0ELb1ELb1ELb0EEv16Flash_fwd_params
    0 bytes stack frame, 0 bytes spill stores, 0 bytes spill loads
ptxas info    : Used 252 registers, 584 bytes cmem[0]
ptxas info    : Compiling entry function '_Z16flash_fwd_kernelI23Flash_fwd_kernel_traitsILi128ELi128ELi32ELi4ELb0ELb0EN7cutlass10bfloat16_tE19Flash_kernel_traitsILi128ELi128ELi32ELi4ES2_EELb1ELb0ELb1ELb1ELb1EEv16Flash_fwd_params' for 'sm_80'
ptxas info    : Function properties for _Z16flash_fwd_kernelI23Flash_fwd_kernel_traitsILi128ELi128ELi32ELi4ELb0ELb0EN7cutlass10bfloat16_tE19Flash_kernel_traitsILi128ELi128ELi32ELi4ES2_EELb1ELb0ELb1ELb1ELb1EEv16Flash_fwd_params
    0 bytes stack frame, 0 bytes spill stores, 0 bytes spill loads
ptxas info    : Used 255 registers, 584 bytes cmem[0]
ptxas info    : Compiling entry function '_Z16flash_fwd_kernelI23Flash_fwd_kernel_traitsILi128ELi128ELi32ELi4ELb0ELb0EN7cutlass10bfloat16_tE19Flash_kernel_traitsILi128ELi128ELi32ELi4ES2_EELb1ELb1ELb0ELb0ELb0EEv16Flash_fwd_params' for 'sm_80'
ptxas info    : Function properties for _Z16flash_fwd_kernelI23Flash_fwd_kernel_traitsILi128ELi128ELi32ELi4ELb0ELb0EN7cutlass10bfloat16_tE19Flash_kernel_traitsILi128ELi128ELi32ELi4ES2_EELb1ELb1ELb0ELb0ELb0EEv16Flash_fwd_params
    16 bytes stack frame, 16 bytes spill stores, 24 bytes spill loads
ptxas info    : Used 255 registers, 584 bytes cmem[0]
ptxas info    : Compiling entry function '_Z16flash_fwd_kernelI23Flash_fwd_kernel_traitsILi128ELi128ELi32ELi4ELb0ELb0EN7cutlass10bfloat16_tE19Flash_kernel_traitsILi128ELi128ELi32ELi4ES2_EELb1ELb1ELb0ELb0ELb1EEv16Flash_fwd_params' for 'sm_80'
ptxas info    : Function properties for _Z16flash_fwd_kernelI23Flash_fwd_kernel_traitsILi128ELi128ELi32ELi4ELb0ELb0EN7cutlass10bfloat16_tE19Flash_kernel_traitsILi128ELi128ELi32ELi4ES2_EELb1ELb1ELb0ELb0ELb1EEv16Flash_fwd_params
    48 bytes stack frame, 44 bytes spill stores, 64 bytes spill loads
ptxas info    : Used 255 registers, 584 bytes cmem[0]
ptxas info    : Compiling entry function '_Z16flash_fwd_kernelI23Flash_fwd_kernel_traitsILi128ELi128ELi32ELi4ELb0ELb0EN7cutlass10bfloat16_tE19Flash_kernel_traitsILi128ELi128ELi32ELi4ES2_EELb1ELb1ELb0ELb1ELb0EEv16Flash_fwd_params' for 'sm_80'
ptxas info    : Function properties for _Z16flash_fwd_kernelI23Flash_fwd_kernel_traitsILi128ELi128ELi32ELi4ELb0ELb0EN7cutlass10bfloat16_tE19Flash_kernel_traitsILi128ELi128ELi32ELi4ES2_EELb1ELb1ELb0ELb1ELb0EEv16Flash_fwd_params
    16 bytes stack frame, 12 bytes spill stores, 28 bytes spill loads
ptxas info    : Used 255 registers, 584 bytes cmem[0]
ptxas info    : Compiling entry function '_Z16flash_fwd_kernelI23Flash_fwd_kernel_traitsILi128ELi128ELi32ELi4ELb0ELb0EN7cutlass10bfloat16_tE19Flash_kernel_traitsILi128ELi128ELi32ELi4ES2_EELb1ELb1ELb0ELb1ELb1EEv16Flash_fwd_params' for 'sm_80'
ptxas info    : Function properties for _Z16flash_fwd_kernelI23Flash_fwd_kernel_traitsILi128ELi128ELi32ELi4ELb0ELb0EN7cutlass10bfloat16_tE19Flash_kernel_traitsILi128ELi128ELi32ELi4ES2_EELb1ELb1ELb0ELb1ELb1EEv16Flash_fwd_params
    112 bytes stack frame, 114 bytes spill stores, 138 bytes spill loads
ptxas info    : Used 255 registers, 584 bytes cmem[0]
ptxas info    : Compiling entry function '_Z16flash_fwd_kernelI23Flash_fwd_kernel_traitsILi128ELi128ELi32ELi4ELb0ELb0EN7cutlass10bfloat16_tE19Flash_kernel_traitsILi128ELi128ELi32ELi4ES2_EELb1ELb1ELb1ELb0ELb0EEv16Flash_fwd_params' for 'sm_80'
ptxas info    : Function properties for _Z16flash_fwd_kernelI23Flash_fwd_kernel_traitsILi128ELi128ELi32ELi4ELb0ELb0EN7cutlass10bfloat16_tE19Flash_kernel_traitsILi128ELi128ELi32ELi4ES2_EELb1ELb1ELb1ELb0ELb0EEv16Flash_fwd_params
    16 bytes stack frame, 16 bytes spill stores, 24 bytes spill loads
ptxas info    : Used 255 registers, 584 bytes cmem[0]
ptxas info    : Compiling entry function '_Z16flash_fwd_kernelI23Flash_fwd_kernel_traitsILi128ELi128ELi32ELi4ELb0ELb0EN7cutlass10bfloat16_tE19Flash_kernel_traitsILi128ELi128ELi32ELi4ES2_EELb1ELb1ELb1ELb0ELb1EEv16Flash_fwd_params' for 'sm_80'
ptxas info    : Function properties for _Z16flash_fwd_kernelI23Flash_fwd_kernel_traitsILi128ELi128ELi32ELi4ELb0ELb0EN7cutlass10bfloat16_tE19Flash_kernel_traitsILi128ELi128ELi32ELi4ES2_EELb1ELb1ELb1ELb0ELb1EEv16Flash_fwd_params
    8 bytes stack frame, 8 bytes spill stores, 12 bytes spill loads
ptxas info    : Used 255 registers, 584 bytes cmem[0]
ptxas info    : Compiling entry function '_Z16flash_fwd_kernelI23Flash_fwd_kernel_traitsILi128ELi128ELi32ELi4ELb0ELb0EN7cutlass10bfloat16_tE19Flash_kernel_traitsILi128ELi128ELi32ELi4ES2_EELb1ELb1ELb1ELb1ELb0EEv16Flash_fwd_params' for 'sm_80'
ptxas info    : Function properties for _Z16flash_fwd_kernelI23Flash_fwd_kernel_traitsILi128ELi128ELi32ELi4ELb0ELb0EN7cutlass10bfloat16_tE19Flash_kernel_traitsILi128ELi128ELi32ELi4ES2_EELb1ELb1ELb1ELb1ELb0EEv16Flash_fwd_params
    8 bytes stack frame, 4 bytes spill stores, 4 bytes spill loads
ptxas info    : Used 255 registers, 584 bytes cmem[0]
ptxas info    : Compiling entry function '_Z16flash_fwd_kernelI23Flash_fwd_kernel_traitsILi128ELi128ELi32ELi4ELb0ELb0EN7cutlass10bfloat16_tE19Flash_kernel_traitsILi128ELi128ELi32ELi4ES2_EELb1ELb1ELb1ELb1ELb1EEv16Flash_fwd_params' for 'sm_80'
ptxas info    : Function properties for _Z16flash_fwd_kernelI23Flash_fwd_kernel_traitsILi128ELi128ELi32ELi4ELb0ELb0EN7cutlass10bfloat16_tE19Flash_kernel_traitsILi128ELi128ELi32ELi4ES2_EELb1ELb1ELb1ELb1ELb1EEv16Flash_fwd_params
    56 bytes stack frame, 56 bytes spill stores, 68 bytes spill loads
ptxas info    : Used 255 registers, 584 bytes cmem[0]
/anaconda/envs/flashattention/lib/python3.9/site-packages/torch/include/c10/core/SymInt.h(84): warning: integer conversion resulted in a change of sign

[13/36] /usr/local/cuda-11.4/bin/nvcc  -I/home/v-feiychen/flash-attention/csrc/flash_attn -I/home/v-feiychen/flash-attention/csrc/flash_attn/src -I/home/v-feiychen/flash-attention/csrc/cutlass/include -I/anaconda/envs/flashattention/lib/python3.9/site-packages/torch/include -I/anaconda/envs/flashattention/lib/python3.9/site-packages/torch/include/torch/csrc/api/include -I/anaconda/envs/flashattention/lib/python3.9/site-packages/torch/include/TH -I/anaconda/envs/flashattention/lib/python3.9/site-packages/torch/include/THC -I/usr/local/cuda-11.4/include -I/anaconda/envs/flashattention/include/python3.9 -c -c /home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_bwd_hdim96_fp16_sm80.cu -o /home/v-feiychen/flash-attention/build/temp.linux-x86_64-cpython-39/csrc/flash_attn/src/flash_bwd_hdim96_fp16_sm80.o -D__CUDA_NO_HALF_OPERATORS__ -D__CUDA_NO_HALF_CONVERSIONS__ -D__CUDA_NO_BFLOAT16_CONVERSIONS__ -D__CUDA_NO_HALF2_OPERATORS__ --expt-relaxed-constexpr --compiler-options ''"'"'-fPIC'"'"'' -O3 -std=c++17 -U__CUDA_NO_HALF_OPERATORS__ -U__CUDA_NO_HALF_CONVERSIONS__ -U__CUDA_NO_HALF2_OPERATORS__ -U__CUDA_NO_BFLOAT16_CONVERSIONS__ --expt-relaxed-constexpr --expt-extended-lambda --use_fast_math --ptxas-options=-v -lineinfo -gencode arch=compute_80,code=sm_80 --threads 4 -DTORCH_API_INCLUDE_EXTENSION_H '-DPYBIND11_COMPILER_TYPE="_gcc"' '-DPYBIND11_STDLIB="_libstdcpp"' '-DPYBIND11_BUILD_ABI="_cxxabi1011"' -DTORCH_EXTENSION_NAME=flash_attn_2_cuda -D_GLIBCXX_USE_CXX11_ABI=0
/anaconda/envs/flashattention/lib/python3.9/site-packages/torch/include/c10/core/SymInt.h(84): warning: integer conversion resulted in a change of sign

ptxas info    : 2 bytes gmem
ptxas info    : Compiling entry function '_Z44flash_bwd_dq_dk_dv_loop_seqk_parallel_kernelI23Flash_bwd_kernel_traitsILi96ELi64ELi128ELi8ELi2ELi4ELi4ELb1ELb0EN7cutlass6half_tE19Flash_kernel_traitsILi96ELi64ELi128ELi8ES2_EELb0ELb0ELb0ELb0EEv16Flash_bwd_params' for 'sm_80'
ptxas info    : Function properties for _Z44flash_bwd_dq_dk_dv_loop_seqk_parallel_kernelI23Flash_bwd_kernel_traitsILi96ELi64ELi128ELi8ELi2ELi4ELi4ELb1ELb0EN7cutlass6half_tE19Flash_kernel_traitsILi96ELi64ELi128ELi8ES2_EELb0ELb0ELb0ELb0EEv16Flash_bwd_params
    16 bytes stack frame, 12 bytes spill stores, 12 bytes spill loads
ptxas info    : Used 255 registers, 696 bytes cmem[0]
ptxas info    : Compiling entry function '_Z44flash_bwd_dq_dk_dv_loop_seqk_parallel_kernelI23Flash_bwd_kernel_traitsILi96ELi64ELi128ELi8ELi2ELi4ELi4ELb1ELb0EN7cutlass6half_tE19Flash_kernel_traitsILi96ELi64ELi128ELi8ES2_EELb0ELb0ELb0ELb1EEv16Flash_bwd_params' for 'sm_80'
ptxas info    : Function properties for _Z44flash_bwd_dq_dk_dv_loop_seqk_parallel_kernelI23Flash_bwd_kernel_traitsILi96ELi64ELi128ELi8ELi2ELi4ELi4ELb1ELb0EN7cutlass6half_tE19Flash_kernel_traitsILi96ELi64ELi128ELi8ES2_EELb0ELb0ELb0ELb1EEv16Flash_bwd_params
    24 bytes stack frame, 20 bytes spill stores, 20 bytes spill loads
ptxas info    : Used 255 registers, 696 bytes cmem[0]
ptxas info    : Compiling entry function '_Z44flash_bwd_dq_dk_dv_loop_seqk_parallel_kernelI23Flash_bwd_kernel_traitsILi96ELi64ELi128ELi8ELi2ELi4ELi4ELb1ELb0EN7cutlass6half_tE19Flash_kernel_traitsILi96ELi64ELi128ELi8ES2_EELb0ELb0ELb1ELb0EEv16Flash_bwd_params' for 'sm_80'
ptxas info    : Function properties for _Z44flash_bwd_dq_dk_dv_loop_seqk_parallel_kernelI23Flash_bwd_kernel_traitsILi96ELi64ELi128ELi8ELi2ELi4ELi4ELb1ELb0EN7cutlass6half_tE19Flash_kernel_traitsILi96ELi64ELi128ELi8ES2_EELb0ELb0ELb1ELb0EEv16Flash_bwd_params
    0 bytes stack frame, 0 bytes spill stores, 0 bytes spill loads
ptxas info    : Used 254 registers, 696 bytes cmem[0]
ptxas info    : Compiling entry function '_Z44flash_bwd_dq_dk_dv_loop_seqk_parallel_kernelI23Flash_bwd_kernel_traitsILi96ELi64ELi128ELi8ELi2ELi4ELi4ELb1ELb0EN7cutlass6half_tE19Flash_kernel_traitsILi96ELi64ELi128ELi8ES2_EELb0ELb0ELb1ELb1EEv16Flash_bwd_params' for 'sm_80'
ptxas info    : Function properties for _Z44flash_bwd_dq_dk_dv_loop_seqk_parallel_kernelI23Flash_bwd_kernel_traitsILi96ELi64ELi128ELi8ELi2ELi4ELi4ELb1ELb0EN7cutlass6half_tE19Flash_kernel_traitsILi96ELi64ELi128ELi8ES2_EELb0ELb0ELb1ELb1EEv16Flash_bwd_params
    0 bytes stack frame, 0 bytes spill stores, 0 bytes spill loads
ptxas info    : Used 254 registers, 696 bytes cmem[0]
ptxas info    : Compiling entry function '_Z44flash_bwd_dq_dk_dv_loop_seqk_parallel_kernelI23Flash_bwd_kernel_traitsILi96ELi64ELi128ELi8ELi2ELi4ELi4ELb1ELb0EN7cutlass6half_tE19Flash_kernel_traitsILi96ELi64ELi128ELi8ES2_EELb0ELb1ELb0ELb0EEv16Flash_bwd_params' for 'sm_80'
ptxas info    : Function properties for _Z44flash_bwd_dq_dk_dv_loop_seqk_parallel_kernelI23Flash_bwd_kernel_traitsILi96ELi64ELi128ELi8ELi2ELi4ELi4ELb1ELb0EN7cutlass6half_tE19Flash_kernel_traitsILi96ELi64ELi128ELi8ES2_EELb0ELb1ELb0ELb0EEv16Flash_bwd_params
    0 bytes stack frame, 0 bytes spill stores, 0 bytes spill loads
ptxas info    : Used 255 registers, 696 bytes cmem[0]
ptxas info    : Compiling entry function '_Z44flash_bwd_dq_dk_dv_loop_seqk_parallel_kernelI23Flash_bwd_kernel_traitsILi96ELi64ELi128ELi8ELi2ELi4ELi4ELb1ELb0EN7cutlass6half_tE19Flash_kernel_traitsILi96ELi64ELi128ELi8ES2_EELb0ELb1ELb0ELb1EEv16Flash_bwd_params' for 'sm_80'
ptxas info    : Function properties for _Z44flash_bwd_dq_dk_dv_loop_seqk_parallel_kernelI23Flash_bwd_kernel_traitsILi96ELi64ELi128ELi8ELi2ELi4ELi4ELb1ELb0EN7cutlass6half_tE19Flash_kernel_traitsILi96ELi64ELi128ELi8ES2_EELb0ELb1ELb0ELb1EEv16Flash_bwd_params
    0 bytes stack frame, 0 bytes spill stores, 0 bytes spill loads
ptxas info    : Used 254 registers, 696 bytes cmem[0]
ptxas info    : Compiling entry function '_Z44flash_bwd_dq_dk_dv_loop_seqk_parallel_kernelI23Flash_bwd_kernel_traitsILi96ELi64ELi128ELi8ELi2ELi4ELi4ELb1ELb0EN7cutlass6half_tE19Flash_kernel_traitsILi96ELi64ELi128ELi8ES2_EELb0ELb1ELb1ELb0EEv16Flash_bwd_params' for 'sm_80'
ptxas info    : Function properties for _Z44flash_bwd_dq_dk_dv_loop_seqk_parallel_kernelI23Flash_bwd_kernel_traitsILi96ELi64ELi128ELi8ELi2ELi4ELi4ELb1ELb0EN7cutlass6half_tE19Flash_kernel_traitsILi96ELi64ELi128ELi8ES2_EELb0ELb1ELb1ELb0EEv16Flash_bwd_params
    0 bytes stack frame, 0 bytes spill stores, 0 bytes spill loads
ptxas info    : Used 254 registers, 696 bytes cmem[0]
ptxas info    : Compiling entry function '_Z44flash_bwd_dq_dk_dv_loop_seqk_parallel_kernelI23Flash_bwd_kernel_traitsILi96ELi64ELi128ELi8ELi2ELi4ELi4ELb1ELb0EN7cutlass6half_tE19Flash_kernel_traitsILi96ELi64ELi128ELi8ES2_EELb0ELb1ELb1ELb1EEv16Flash_bwd_params' for 'sm_80'
ptxas info    : Function properties for _Z44flash_bwd_dq_dk_dv_loop_seqk_parallel_kernelI23Flash_bwd_kernel_traitsILi96ELi64ELi128ELi8ELi2ELi4ELi4ELb1ELb0EN7cutlass6half_tE19Flash_kernel_traitsILi96ELi64ELi128ELi8ES2_EELb0ELb1ELb1ELb1EEv16Flash_bwd_params
    0 bytes stack frame, 0 bytes spill stores, 0 bytes spill loads
ptxas info    : Used 254 registers, 696 bytes cmem[0]
ptxas info    : Compiling entry function '_Z27flash_bwd_convert_dq_kernelI23Flash_bwd_kernel_traitsILi96ELi64ELi128ELi8ELi2ELi4ELi4ELb1ELb0EN7cutlass6half_tE19Flash_kernel_traitsILi96ELi64ELi128ELi8ES2_EEEv16Flash_bwd_params' for 'sm_80'
ptxas info    : Function properties for _Z27flash_bwd_convert_dq_kernelI23Flash_bwd_kernel_traitsILi96ELi64ELi128ELi8ELi2ELi4ELi4ELb1ELb0EN7cutlass6half_tE19Flash_kernel_traitsILi96ELi64ELi128ELi8ES2_EEEv16Flash_bwd_params
    0 bytes stack frame, 0 bytes spill stores, 0 bytes spill loads
ptxas info    : Used 32 registers, 696 bytes cmem[0]
ptxas info    : Compiling entry function '_Z44flash_bwd_dq_dk_dv_loop_seqk_parallel_kernelI23Flash_bwd_kernel_traitsILi96ELi64ELi128ELi8ELi2ELi4ELi4ELb1ELb0EN7cutlass6half_tE19Flash_kernel_traitsILi96ELi64ELi128ELi8ES2_EELb1ELb0ELb0ELb0EEv16Flash_bwd_params' for 'sm_80'
ptxas info    : Function properties for _Z44flash_bwd_dq_dk_dv_loop_seqk_parallel_kernelI23Flash_bwd_kernel_traitsILi96ELi64ELi128ELi8ELi2ELi4ELi4ELb1ELb0EN7cutlass6half_tE19Flash_kernel_traitsILi96ELi64ELi128ELi8ES2_EELb1ELb0ELb0ELb0EEv16Flash_bwd_params
    16 bytes stack frame, 12 bytes spill stores, 12 bytes spill loads
ptxas info    : Used 255 registers, 696 bytes cmem[0]
ptxas info    : Compiling entry function '_Z44flash_bwd_dq_dk_dv_loop_seqk_parallel_kernelI23Flash_bwd_kernel_traitsILi96ELi64ELi128ELi8ELi2ELi4ELi4ELb1ELb0EN7cutlass6half_tE19Flash_kernel_traitsILi96ELi64ELi128ELi8ES2_EELb1ELb0ELb0ELb1EEv16Flash_bwd_params' for 'sm_80'
ptxas info    : Function properties for _Z44flash_bwd_dq_dk_dv_loop_seqk_parallel_kernelI23Flash_bwd_kernel_traitsILi96ELi64ELi128ELi8ELi2ELi4ELi4ELb1ELb0EN7cutlass6half_tE19Flash_kernel_traitsILi96ELi64ELi128ELi8ES2_EELb1ELb0ELb0ELb1EEv16Flash_bwd_params
    0 bytes stack frame, 0 bytes spill stores, 0 bytes spill loads
ptxas info    : Used 254 registers, 696 bytes cmem[0]
ptxas info    : Compiling entry function '_Z44flash_bwd_dq_dk_dv_loop_seqk_parallel_kernelI23Flash_bwd_kernel_traitsILi96ELi64ELi128ELi8ELi2ELi4ELi4ELb1ELb0EN7cutlass6half_tE19Flash_kernel_traitsILi96ELi64ELi128ELi8ES2_EELb1ELb0ELb1ELb0EEv16Flash_bwd_params' for 'sm_80'
ptxas info    : Function properties for _Z44flash_bwd_dq_dk_dv_loop_seqk_parallel_kernelI23Flash_bwd_kernel_traitsILi96ELi64ELi128ELi8ELi2ELi4ELi4ELb1ELb0EN7cutlass6half_tE19Flash_kernel_traitsILi96ELi64ELi128ELi8ES2_EELb1ELb0ELb1ELb0EEv16Flash_bwd_params
    8 bytes stack frame, 4 bytes spill stores, 4 bytes spill loads
ptxas info    : Used 255 registers, 696 bytes cmem[0]
ptxas info    : Compiling entry function '_Z44flash_bwd_dq_dk_dv_loop_seqk_parallel_kernelI23Flash_bwd_kernel_traitsILi96ELi64ELi128ELi8ELi2ELi4ELi4ELb1ELb0EN7cutlass6half_tE19Flash_kernel_traitsILi96ELi64ELi128ELi8ES2_EELb1ELb0ELb1ELb1EEv16Flash_bwd_params' for 'sm_80'
ptxas info    : Function properties for _Z44flash_bwd_dq_dk_dv_loop_seqk_parallel_kernelI23Flash_bwd_kernel_traitsILi96ELi64ELi128ELi8ELi2ELi4ELi4ELb1ELb0EN7cutlass6half_tE19Flash_kernel_traitsILi96ELi64ELi128ELi8ES2_EELb1ELb0ELb1ELb1EEv16Flash_bwd_params
    0 bytes stack frame, 0 bytes spill stores, 0 bytes spill loads
ptxas info    : Used 255 registers, 696 bytes cmem[0]
ptxas info    : Compiling entry function '_Z44flash_bwd_dq_dk_dv_loop_seqk_parallel_kernelI23Flash_bwd_kernel_traitsILi96ELi64ELi128ELi8ELi2ELi4ELi4ELb1ELb0EN7cutlass6half_tE19Flash_kernel_traitsILi96ELi64ELi128ELi8ES2_EELb1ELb1ELb0ELb0EEv16Flash_bwd_params' for 'sm_80'
ptxas info    : Function properties for _Z44flash_bwd_dq_dk_dv_loop_seqk_parallel_kernelI23Flash_bwd_kernel_traitsILi96ELi64ELi128ELi8ELi2ELi4ELi4ELb1ELb0EN7cutlass6half_tE19Flash_kernel_traitsILi96ELi64ELi128ELi8ES2_EELb1ELb1ELb0ELb0EEv16Flash_bwd_params
    0 bytes stack frame, 0 bytes spill stores, 0 bytes spill loads
ptxas info    : Used 255 registers, 696 bytes cmem[0]
ptxas info    : Compiling entry function '_Z44flash_bwd_dq_dk_dv_loop_seqk_parallel_kernelI23Flash_bwd_kernel_traitsILi96ELi64ELi128ELi8ELi2ELi4ELi4ELb1ELb0EN7cutlass6half_tE19Flash_kernel_traitsILi96ELi64ELi128ELi8ES2_EELb1ELb1ELb0ELb1EEv16Flash_bwd_params' for 'sm_80'
ptxas info    : Function properties for _Z44flash_bwd_dq_dk_dv_loop_seqk_parallel_kernelI23Flash_bwd_kernel_traitsILi96ELi64ELi128ELi8ELi2ELi4ELi4ELb1ELb0EN7cutlass6half_tE19Flash_kernel_traitsILi96ELi64ELi128ELi8ES2_EELb1ELb1ELb0ELb1EEv16Flash_bwd_params
    0 bytes stack frame, 0 bytes spill stores, 0 bytes spill loads
ptxas info    : Used 255 registers, 696 bytes cmem[0]
ptxas info    : Compiling entry function '_Z44flash_bwd_dq_dk_dv_loop_seqk_parallel_kernelI23Flash_bwd_kernel_traitsILi96ELi64ELi128ELi8ELi2ELi4ELi4ELb1ELb0EN7cutlass6half_tE19Flash_kernel_traitsILi96ELi64ELi128ELi8ES2_EELb1ELb1ELb1ELb0EEv16Flash_bwd_params' for 'sm_80'
ptxas info    : Function properties for _Z44flash_bwd_dq_dk_dv_loop_seqk_parallel_kernelI23Flash_bwd_kernel_traitsILi96ELi64ELi128ELi8ELi2ELi4ELi4ELb1ELb0EN7cutlass6half_tE19Flash_kernel_traitsILi96ELi64ELi128ELi8ES2_EELb1ELb1ELb1ELb0EEv16Flash_bwd_params
    0 bytes stack frame, 0 bytes spill stores, 0 bytes spill loads
ptxas info    : Used 255 registers, 696 bytes cmem[0]
ptxas info    : Compiling entry function '_Z44flash_bwd_dq_dk_dv_loop_seqk_parallel_kernelI23Flash_bwd_kernel_traitsILi96ELi64ELi128ELi8ELi2ELi4ELi4ELb1ELb0EN7cutlass6half_tE19Flash_kernel_traitsILi96ELi64ELi128ELi8ES2_EELb1ELb1ELb1ELb1EEv16Flash_bwd_params' for 'sm_80'
ptxas info    : Function properties for _Z44flash_bwd_dq_dk_dv_loop_seqk_parallel_kernelI23Flash_bwd_kernel_traitsILi96ELi64ELi128ELi8ELi2ELi4ELi4ELb1ELb0EN7cutlass6half_tE19Flash_kernel_traitsILi96ELi64ELi128ELi8ES2_EELb1ELb1ELb1ELb1EEv16Flash_bwd_params
    0 bytes stack frame, 0 bytes spill stores, 0 bytes spill loads
ptxas info    : Used 255 registers, 696 bytes cmem[0]
ptxas info    : Compiling entry function '_Z25flash_bwd_dot_do_o_kernelILb1E23Flash_bwd_kernel_traitsILi96ELi64ELi128ELi8ELi2ELi4ELi4ELb1ELb0EN7cutlass6half_tE19Flash_kernel_traitsILi96ELi64ELi128ELi8ES2_EEEv16Flash_bwd_params' for 'sm_80'
ptxas info    : Function properties for _Z25flash_bwd_dot_do_o_kernelILb1E23Flash_bwd_kernel_traitsILi96ELi64ELi128ELi8ELi2ELi4ELi4ELb1ELb0EN7cutlass6half_tE19Flash_kernel_traitsILi96ELi64ELi128ELi8ES2_EEEv16Flash_bwd_params
    0 bytes stack frame, 0 bytes spill stores, 0 bytes spill loads
ptxas info    : Used 42 registers, 696 bytes cmem[0]
ptxas info    : Compiling entry function '_Z27flash_bwd_convert_dq_kernelI23Flash_bwd_kernel_traitsILi96ELi64ELi128ELi8ELi2ELi4ELi4ELb0ELb0EN7cutlass6half_tE19Flash_kernel_traitsILi96ELi64ELi128ELi8ES2_EEEv16Flash_bwd_params' for 'sm_80'
ptxas info    : Function properties for _Z27flash_bwd_convert_dq_kernelI23Flash_bwd_kernel_traitsILi96ELi64ELi128ELi8ELi2ELi4ELi4ELb0ELb0EN7cutlass6half_tE19Flash_kernel_traitsILi96ELi64ELi128ELi8ES2_EEEv16Flash_bwd_params
    0 bytes stack frame, 0 bytes spill stores, 0 bytes spill loads
ptxas info    : Used 32 registers, 696 bytes cmem[0]
ptxas info    : Compiling entry function '_Z44flash_bwd_dq_dk_dv_loop_seqk_parallel_kernelI23Flash_bwd_kernel_traitsILi96ELi64ELi128ELi8ELi2ELi4ELi4ELb0ELb0EN7cutlass6half_tE19Flash_kernel_traitsILi96ELi64ELi128ELi8ES2_EELb1ELb0ELb0ELb0EEv16Flash_bwd_params' for 'sm_80'
ptxas info    : Function properties for _Z44flash_bwd_dq_dk_dv_loop_seqk_parallel_kernelI23Flash_bwd_kernel_traitsILi96ELi64ELi128ELi8ELi2ELi4ELi4ELb0ELb0EN7cutlass6half_tE19Flash_kernel_traitsILi96ELi64ELi128ELi8ES2_EELb1ELb0ELb0ELb0EEv16Flash_bwd_params
    0 bytes stack frame, 0 bytes spill stores, 0 bytes spill loads
ptxas info    : Used 238 registers, 696 bytes cmem[0]
ptxas info    : Compiling entry function '_Z44flash_bwd_dq_dk_dv_loop_seqk_parallel_kernelI23Flash_bwd_kernel_traitsILi96ELi64ELi128ELi8ELi2ELi4ELi4ELb0ELb0EN7cutlass6half_tE19Flash_kernel_traitsILi96ELi64ELi128ELi8ES2_EELb1ELb0ELb0ELb1EEv16Flash_bwd_params' for 'sm_80'
ptxas info    : Function properties for _Z44flash_bwd_dq_dk_dv_loop_seqk_parallel_kernelI23Flash_bwd_kernel_traitsILi96ELi64ELi128ELi8ELi2ELi4ELi4ELb0ELb0EN7cutlass6half_tE19Flash_kernel_traitsILi96ELi64ELi128ELi8ES2_EELb1ELb0ELb0ELb1EEv16Flash_bwd_params
    0 bytes stack frame, 0 bytes spill stores, 0 bytes spill loads
ptxas info    : Used 238 registers, 696 bytes cmem[0]
ptxas info    : Compiling entry function '_Z44flash_bwd_dq_dk_dv_loop_seqk_parallel_kernelI23Flash_bwd_kernel_traitsILi96ELi64ELi128ELi8ELi2ELi4ELi4ELb0ELb0EN7cutlass6half_tE19Flash_kernel_traitsILi96ELi64ELi128ELi8ES2_EELb1ELb0ELb1ELb0EEv16Flash_bwd_params' for 'sm_80'
ptxas info    : Function properties for _Z44flash_bwd_dq_dk_dv_loop_seqk_parallel_kernelI23Flash_bwd_kernel_traitsILi96ELi64ELi128ELi8ELi2ELi4ELi4ELb0ELb0EN7cutlass6half_tE19Flash_kernel_traitsILi96ELi64ELi128ELi8ES2_EELb1ELb0ELb1ELb0EEv16Flash_bwd_params
    0 bytes stack frame, 0 bytes spill stores, 0 bytes spill loads
ptxas info    : Used 238 registers, 696 bytes cmem[0]
ptxas info    : Compiling entry function '_Z44flash_bwd_dq_dk_dv_loop_seqk_parallel_kernelI23Flash_bwd_kernel_traitsILi96ELi64ELi128ELi8ELi2ELi4ELi4ELb0ELb0EN7cutlass6half_tE19Flash_kernel_traitsILi96ELi64ELi128ELi8ES2_EELb1ELb0ELb1ELb1EEv16Flash_bwd_params' for 'sm_80'
ptxas info    : Function properties for _Z44flash_bwd_dq_dk_dv_loop_seqk_parallel_kernelI23Flash_bwd_kernel_traitsILi96ELi64ELi128ELi8ELi2ELi4ELi4ELb0ELb0EN7cutlass6half_tE19Flash_kernel_traitsILi96ELi64ELi128ELi8ES2_EELb1ELb0ELb1ELb1EEv16Flash_bwd_params
    0 bytes stack frame, 0 bytes spill stores, 0 bytes spill loads
ptxas info    : Used 243 registers, 696 bytes cmem[0]
ptxas info    : Compiling entry function '_Z44flash_bwd_dq_dk_dv_loop_seqk_parallel_kernelI23Flash_bwd_kernel_traitsILi96ELi64ELi128ELi8ELi2ELi4ELi4ELb0ELb0EN7cutlass6half_tE19Flash_kernel_traitsILi96ELi64ELi128ELi8ES2_EELb1ELb1ELb0ELb0EEv16Flash_bwd_params' for 'sm_80'
ptxas info    : Function properties for _Z44flash_bwd_dq_dk_dv_loop_seqk_parallel_kernelI23Flash_bwd_kernel_traitsILi96ELi64ELi128ELi8ELi2ELi4ELi4ELb0ELb0EN7cutlass6half_tE19Flash_kernel_traitsILi96ELi64ELi128ELi8ES2_EELb1ELb1ELb0ELb0EEv16Flash_bwd_params
    0 bytes stack frame, 0 bytes spill stores, 0 bytes spill loads
ptxas info    : Used 255 registers, 696 bytes cmem[0]
ptxas info    : Compiling entry function '_Z44flash_bwd_dq_dk_dv_loop_seqk_parallel_kernelI23Flash_bwd_kernel_traitsILi96ELi64ELi128ELi8ELi2ELi4ELi4ELb0ELb0EN7cutlass6half_tE19Flash_kernel_traitsILi96ELi64ELi128ELi8ES2_EELb1ELb1ELb0ELb1EEv16Flash_bwd_params' for 'sm_80'
ptxas info    : Function properties for _Z44flash_bwd_dq_dk_dv_loop_seqk_parallel_kernelI23Flash_bwd_kernel_traitsILi96ELi64ELi128ELi8ELi2ELi4ELi4ELb0ELb0EN7cutlass6half_tE19Flash_kernel_traitsILi96ELi64ELi128ELi8ES2_EELb1ELb1ELb0ELb1EEv16Flash_bwd_params
    0 bytes stack frame, 0 bytes spill stores, 0 bytes spill loads
ptxas info    : Used 253 registers, 696 bytes cmem[0]
ptxas info    : Compiling entry function '_Z44flash_bwd_dq_dk_dv_loop_seqk_parallel_kernelI23Flash_bwd_kernel_traitsILi96ELi64ELi128ELi8ELi2ELi4ELi4ELb0ELb0EN7cutlass6half_tE19Flash_kernel_traitsILi96ELi64ELi128ELi8ES2_EELb1ELb1ELb1ELb0EEv16Flash_bwd_params' for 'sm_80'
ptxas info    : Function properties for _Z44flash_bwd_dq_dk_dv_loop_seqk_parallel_kernelI23Flash_bwd_kernel_traitsILi96ELi64ELi128ELi8ELi2ELi4ELi4ELb0ELb0EN7cutlass6half_tE19Flash_kernel_traitsILi96ELi64ELi128ELi8ES2_EELb1ELb1ELb1ELb0EEv16Flash_bwd_params
    0 bytes stack frame, 0 bytes spill stores, 0 bytes spill loads
ptxas info    : Used 250 registers, 696 bytes cmem[0]
ptxas info    : Compiling entry function '_Z44flash_bwd_dq_dk_dv_loop_seqk_parallel_kernelI23Flash_bwd_kernel_traitsILi96ELi64ELi128ELi8ELi2ELi4ELi4ELb0ELb0EN7cutlass6half_tE19Flash_kernel_traitsILi96ELi64ELi128ELi8ES2_EELb1ELb1ELb1ELb1EEv16Flash_bwd_params' for 'sm_80'
ptxas info    : Function properties for _Z44flash_bwd_dq_dk_dv_loop_seqk_parallel_kernelI23Flash_bwd_kernel_traitsILi96ELi64ELi128ELi8ELi2ELi4ELi4ELb0ELb0EN7cutlass6half_tE19Flash_kernel_traitsILi96ELi64ELi128ELi8ES2_EELb1ELb1ELb1ELb1EEv16Flash_bwd_params
    0 bytes stack frame, 0 bytes spill stores, 0 bytes spill loads
ptxas info    : Used 245 registers, 696 bytes cmem[0]
ptxas info    : Compiling entry function '_Z25flash_bwd_dot_do_o_kernelILb1E23Flash_bwd_kernel_traitsILi96ELi64ELi128ELi8ELi2ELi4ELi4ELb0ELb0EN7cutlass6half_tE19Flash_kernel_traitsILi96ELi64ELi128ELi8ES2_EEEv16Flash_bwd_params' for 'sm_80'
ptxas info    : Function properties for _Z25flash_bwd_dot_do_o_kernelILb1E23Flash_bwd_kernel_traitsILi96ELi64ELi128ELi8ELi2ELi4ELi4ELb0ELb0EN7cutlass6half_tE19Flash_kernel_traitsILi96ELi64ELi128ELi8ES2_EEEv16Flash_bwd_params
    0 bytes stack frame, 0 bytes spill stores, 0 bytes spill loads
ptxas info    : Used 42 registers, 696 bytes cmem[0]
/anaconda/envs/flashattention/lib/python3.9/site-packages/torch/include/c10/core/SymInt.h(84): warning: integer conversion resulted in a change of sign

[14/36] /usr/local/cuda-11.4/bin/nvcc  -I/home/v-feiychen/flash-attention/csrc/flash_attn -I/home/v-feiychen/flash-attention/csrc/flash_attn/src -I/home/v-feiychen/flash-attention/csrc/cutlass/include -I/anaconda/envs/flashattention/lib/python3.9/site-packages/torch/include -I/anaconda/envs/flashattention/lib/python3.9/site-packages/torch/include/torch/csrc/api/include -I/anaconda/envs/flashattention/lib/python3.9/site-packages/torch/include/TH -I/anaconda/envs/flashattention/lib/python3.9/site-packages/torch/include/THC -I/usr/local/cuda-11.4/include -I/anaconda/envs/flashattention/include/python3.9 -c -c /home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_hdim128_fp16_sm80.cu -o /home/v-feiychen/flash-attention/build/temp.linux-x86_64-cpython-39/csrc/flash_attn/src/flash_fwd_hdim128_fp16_sm80.o -D__CUDA_NO_HALF_OPERATORS__ -D__CUDA_NO_HALF_CONVERSIONS__ -D__CUDA_NO_BFLOAT16_CONVERSIONS__ -D__CUDA_NO_HALF2_OPERATORS__ --expt-relaxed-constexpr --compiler-options ''"'"'-fPIC'"'"'' -O3 -std=c++17 -U__CUDA_NO_HALF_OPERATORS__ -U__CUDA_NO_HALF_CONVERSIONS__ -U__CUDA_NO_HALF2_OPERATORS__ -U__CUDA_NO_BFLOAT16_CONVERSIONS__ --expt-relaxed-constexpr --expt-extended-lambda --use_fast_math --ptxas-options=-v -lineinfo -gencode arch=compute_80,code=sm_80 --threads 4 -DTORCH_API_INCLUDE_EXTENSION_H '-DPYBIND11_COMPILER_TYPE="_gcc"' '-DPYBIND11_STDLIB="_libstdcpp"' '-DPYBIND11_BUILD_ABI="_cxxabi1011"' -DTORCH_EXTENSION_NAME=flash_attn_2_cuda -D_GLIBCXX_USE_CXX11_ABI=0
/anaconda/envs/flashattention/lib/python3.9/site-packages/torch/include/c10/core/SymInt.h(84): warning: integer conversion resulted in a change of sign

/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_kernel.h(140): warning: variable "MMA_M" was declared but never referenced
          detected during:
            instantiation of "void flash::compute_attn_1rowblock<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &, int, int, int) [with Kernel_traits=Flash_fwd_kernel_traits<128, 128, 32, 4, false, false, cutlass::half_t, Flash_kernel_traits<128, 128, 32, 4, cutlass::half_t>>, Is_dropout=true, Is_causal=true, Is_even_N=true, Is_even_K=true, Return_softmax=true, Params=Flash_fwd_params]" 
(588): here
            instantiation of "void flash::compute_attn<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &) [with Kernel_traits=Flash_fwd_kernel_traits<128, 128, 32, 4, false, false, cutlass::half_t, Flash_kernel_traits<128, 128, 32, 4, cutlass::half_t>>, Is_dropout=true, Is_causal=true, Is_even_N=true, Is_even_K=true, Return_softmax=true, Params=Flash_fwd_params]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(15): here
            instantiation of "void flash_fwd_kernel<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax>(Flash_fwd_params) [with Kernel_traits=Flash_fwd_kernel_traits<128, 128, 32, 4, false, false, cutlass::half_t, Flash_kernel_traits<128, 128, 32, 4, cutlass::half_t>>, Is_dropout=true, Is_causal=true, Is_even_N=true, Is_even_K=true, Return_softmax=true]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(34): here
            instantiation of "void run_flash_fwd<Kernel_traits,Is_dropout,Is_causal>(Flash_fwd_params &, cudaStream_t) [with Kernel_traits=Flash_fwd_kernel_traits<128, 128, 32, 4, false, false, cutlass::half_t, Flash_kernel_traits<128, 128, 32, 4, cutlass::half_t>>, Is_dropout=true, Is_causal=true]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(118): here
            instantiation of "void run_mha_fwd_hdim128<T>(Flash_fwd_params &, cudaStream_t) [with T=cutlass::half_t]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_hdim128_fp16_sm80.cu(31): here

/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_kernel.h(275): warning: variable "tQrQ" was declared but never referenced
          detected during:
            instantiation of "void flash::compute_attn_1rowblock<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &, int, int, int) [with Kernel_traits=Flash_fwd_kernel_traits<128, 128, 32, 4, false, false, cutlass::half_t, Flash_kernel_traits<128, 128, 32, 4, cutlass::half_t>>, Is_dropout=true, Is_causal=true, Is_even_N=true, Is_even_K=true, Return_softmax=true, Params=Flash_fwd_params]" 
(588): here
            instantiation of "void flash::compute_attn<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &) [with Kernel_traits=Flash_fwd_kernel_traits<128, 128, 32, 4, false, false, cutlass::half_t, Flash_kernel_traits<128, 128, 32, 4, cutlass::half_t>>, Is_dropout=true, Is_causal=true, Is_even_N=true, Is_even_K=true, Return_softmax=true, Params=Flash_fwd_params]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(15): here
            instantiation of "void flash_fwd_kernel<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax>(Flash_fwd_params) [with Kernel_traits=Flash_fwd_kernel_traits<128, 128, 32, 4, false, false, cutlass::half_t, Flash_kernel_traits<128, 128, 32, 4, cutlass::half_t>>, Is_dropout=true, Is_causal=true, Is_even_N=true, Is_even_K=true, Return_softmax=true]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(34): here
            instantiation of "void run_flash_fwd<Kernel_traits,Is_dropout,Is_causal>(Flash_fwd_params &, cudaStream_t) [with Kernel_traits=Flash_fwd_kernel_traits<128, 128, 32, 4, false, false, cutlass::half_t, Flash_kernel_traits<128, 128, 32, 4, cutlass::half_t>>, Is_dropout=true, Is_causal=true]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(118): here
            instantiation of "void run_mha_fwd_hdim128<T>(Flash_fwd_params &, cudaStream_t) [with T=cutlass::half_t]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_hdim128_fp16_sm80.cu(31): here

/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_kernel.h(140): warning: variable "MMA_M" was declared but never referenced
          detected during:
            instantiation of "void flash::compute_attn_1rowblock<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &, int, int, int) [with Kernel_traits=Flash_fwd_kernel_traits<128, 128, 32, 4, false, false, cutlass::half_t, Flash_kernel_traits<128, 128, 32, 4, cutlass::half_t>>, Is_dropout=true, Is_causal=true, Is_even_N=true, Is_even_K=true, Return_softmax=false, Params=Flash_fwd_params]" 
(588): here
            instantiation of "void flash::compute_attn<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &) [with Kernel_traits=Flash_fwd_kernel_traits<128, 128, 32, 4, false, false, cutlass::half_t, Flash_kernel_traits<128, 128, 32, 4, cutlass::half_t>>, Is_dropout=true, Is_causal=true, Is_even_N=true, Is_even_K=true, Return_softmax=false, Params=Flash_fwd_params]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(15): here
            instantiation of "void flash_fwd_kernel<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax>(Flash_fwd_params) [with Kernel_traits=Flash_fwd_kernel_traits<128, 128, 32, 4, false, false, cutlass::half_t, Flash_kernel_traits<128, 128, 32, 4, cutlass::half_t>>, Is_dropout=true, Is_causal=true, Is_even_N=true, Is_even_K=true, Return_softmax=false]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(34): here
            instantiation of "void run_flash_fwd<Kernel_traits,Is_dropout,Is_causal>(Flash_fwd_params &, cudaStream_t) [with Kernel_traits=Flash_fwd_kernel_traits<128, 128, 32, 4, false, false, cutlass::half_t, Flash_kernel_traits<128, 128, 32, 4, cutlass::half_t>>, Is_dropout=true, Is_causal=true]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(118): here
            instantiation of "void run_mha_fwd_hdim128<T>(Flash_fwd_params &, cudaStream_t) [with T=cutlass::half_t]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_hdim128_fp16_sm80.cu(31): here

/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_kernel.h(275): warning: variable "tQrQ" was declared but never referenced
          detected during:
            instantiation of "void flash::compute_attn_1rowblock<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &, int, int, int) [with Kernel_traits=Flash_fwd_kernel_traits<128, 128, 32, 4, false, false, cutlass::half_t, Flash_kernel_traits<128, 128, 32, 4, cutlass::half_t>>, Is_dropout=true, Is_causal=true, Is_even_N=true, Is_even_K=true, Return_softmax=false, Params=Flash_fwd_params]" 
(588): here
            instantiation of "void flash::compute_attn<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &) [with Kernel_traits=Flash_fwd_kernel_traits<128, 128, 32, 4, false, false, cutlass::half_t, Flash_kernel_traits<128, 128, 32, 4, cutlass::half_t>>, Is_dropout=true, Is_causal=true, Is_even_N=true, Is_even_K=true, Return_softmax=false, Params=Flash_fwd_params]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(15): here
            instantiation of "void flash_fwd_kernel<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax>(Flash_fwd_params) [with Kernel_traits=Flash_fwd_kernel_traits<128, 128, 32, 4, false, false, cutlass::half_t, Flash_kernel_traits<128, 128, 32, 4, cutlass::half_t>>, Is_dropout=true, Is_causal=true, Is_even_N=true, Is_even_K=true, Return_softmax=false]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(34): here
            instantiation of "void run_flash_fwd<Kernel_traits,Is_dropout,Is_causal>(Flash_fwd_params &, cudaStream_t) [with Kernel_traits=Flash_fwd_kernel_traits<128, 128, 32, 4, false, false, cutlass::half_t, Flash_kernel_traits<128, 128, 32, 4, cutlass::half_t>>, Is_dropout=true, Is_causal=true]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(118): here
            instantiation of "void run_mha_fwd_hdim128<T>(Flash_fwd_params &, cudaStream_t) [with T=cutlass::half_t]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_hdim128_fp16_sm80.cu(31): here

/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_kernel.h(140): warning: variable "MMA_M" was declared but never referenced
          detected during:
            instantiation of "void flash::compute_attn_1rowblock<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &, int, int, int) [with Kernel_traits=Flash_fwd_kernel_traits<128, 128, 32, 4, false, false, cutlass::half_t, Flash_kernel_traits<128, 128, 32, 4, cutlass::half_t>>, Is_dropout=true, Is_causal=true, Is_even_N=true, Is_even_K=false, Return_softmax=true, Params=Flash_fwd_params]" 
(588): here
            instantiation of "void flash::compute_attn<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &) [with Kernel_traits=Flash_fwd_kernel_traits<128, 128, 32, 4, false, false, cutlass::half_t, Flash_kernel_traits<128, 128, 32, 4, cutlass::half_t>>, Is_dropout=true, Is_causal=true, Is_even_N=true, Is_even_K=false, Return_softmax=true, Params=Flash_fwd_params]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(15): here
            instantiation of "void flash_fwd_kernel<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax>(Flash_fwd_params) [with Kernel_traits=Flash_fwd_kernel_traits<128, 128, 32, 4, false, false, cutlass::half_t, Flash_kernel_traits<128, 128, 32, 4, cutlass::half_t>>, Is_dropout=true, Is_causal=true, Is_even_N=true, Is_even_K=false, Return_softmax=true]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(34): here
            instantiation of "void run_flash_fwd<Kernel_traits,Is_dropout,Is_causal>(Flash_fwd_params &, cudaStream_t) [with Kernel_traits=Flash_fwd_kernel_traits<128, 128, 32, 4, false, false, cutlass::half_t, Flash_kernel_traits<128, 128, 32, 4, cutlass::half_t>>, Is_dropout=true, Is_causal=true]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(118): here
            instantiation of "void run_mha_fwd_hdim128<T>(Flash_fwd_params &, cudaStream_t) [with T=cutlass::half_t]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_hdim128_fp16_sm80.cu(31): here

/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_kernel.h(275): warning: variable "tQrQ" was declared but never referenced
          detected during:
            instantiation of "void flash::compute_attn_1rowblock<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &, int, int, int) [with Kernel_traits=Flash_fwd_kernel_traits<128, 128, 32, 4, false, false, cutlass::half_t, Flash_kernel_traits<128, 128, 32, 4, cutlass::half_t>>, Is_dropout=true, Is_causal=true, Is_even_N=true, Is_even_K=false, Return_softmax=true, Params=Flash_fwd_params]" 
(588): here
            instantiation of "void flash::compute_attn<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &) [with Kernel_traits=Flash_fwd_kernel_traits<128, 128, 32, 4, false, false, cutlass::half_t, Flash_kernel_traits<128, 128, 32, 4, cutlass::half_t>>, Is_dropout=true, Is_causal=true, Is_even_N=true, Is_even_K=false, Return_softmax=true, Params=Flash_fwd_params]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(15): here
            instantiation of "void flash_fwd_kernel<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax>(Flash_fwd_params) [with Kernel_traits=Flash_fwd_kernel_traits<128, 128, 32, 4, false, false, cutlass::half_t, Flash_kernel_traits<128, 128, 32, 4, cutlass::half_t>>, Is_dropout=true, Is_causal=true, Is_even_N=true, Is_even_K=false, Return_softmax=true]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(34): here
            instantiation of "void run_flash_fwd<Kernel_traits,Is_dropout,Is_causal>(Flash_fwd_params &, cudaStream_t) [with Kernel_traits=Flash_fwd_kernel_traits<128, 128, 32, 4, false, false, cutlass::half_t, Flash_kernel_traits<128, 128, 32, 4, cutlass::half_t>>, Is_dropout=true, Is_causal=true]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(118): here
            instantiation of "void run_mha_fwd_hdim128<T>(Flash_fwd_params &, cudaStream_t) [with T=cutlass::half_t]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_hdim128_fp16_sm80.cu(31): here

/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_kernel.h(140): warning: variable "MMA_M" was declared but never referenced
          detected during:
            instantiation of "void flash::compute_attn_1rowblock<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &, int, int, int) [with Kernel_traits=Flash_fwd_kernel_traits<128, 128, 32, 4, false, false, cutlass::half_t, Flash_kernel_traits<128, 128, 32, 4, cutlass::half_t>>, Is_dropout=true, Is_causal=true, Is_even_N=true, Is_even_K=false, Return_softmax=false, Params=Flash_fwd_params]" 
(588): here
            instantiation of "void flash::compute_attn<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &) [with Kernel_traits=Flash_fwd_kernel_traits<128, 128, 32, 4, false, false, cutlass::half_t, Flash_kernel_traits<128, 128, 32, 4, cutlass::half_t>>, Is_dropout=true, Is_causal=true, Is_even_N=true, Is_even_K=false, Return_softmax=false, Params=Flash_fwd_params]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(15): here
            instantiation of "void flash_fwd_kernel<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax>(Flash_fwd_params) [with Kernel_traits=Flash_fwd_kernel_traits<128, 128, 32, 4, false, false, cutlass::half_t, Flash_kernel_traits<128, 128, 32, 4, cutlass::half_t>>, Is_dropout=true, Is_causal=true, Is_even_N=true, Is_even_K=false, Return_softmax=false]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(34): here
            instantiation of "void run_flash_fwd<Kernel_traits,Is_dropout,Is_causal>(Flash_fwd_params &, cudaStream_t) [with Kernel_traits=Flash_fwd_kernel_traits<128, 128, 32, 4, false, false, cutlass::half_t, Flash_kernel_traits<128, 128, 32, 4, cutlass::half_t>>, Is_dropout=true, Is_causal=true]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(118): here
            instantiation of "void run_mha_fwd_hdim128<T>(Flash_fwd_params &, cudaStream_t) [with T=cutlass::half_t]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_hdim128_fp16_sm80.cu(31): here

/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_kernel.h(275): warning: variable "tQrQ" was declared but never referenced
          detected during:
            instantiation of "void flash::compute_attn_1rowblock<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &, int, int, int) [with Kernel_traits=Flash_fwd_kernel_traits<128, 128, 32, 4, false, false, cutlass::half_t, Flash_kernel_traits<128, 128, 32, 4, cutlass::half_t>>, Is_dropout=true, Is_causal=true, Is_even_N=true, Is_even_K=false, Return_softmax=false, Params=Flash_fwd_params]" 
(588): here
            instantiation of "void flash::compute_attn<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &) [with Kernel_traits=Flash_fwd_kernel_traits<128, 128, 32, 4, false, false, cutlass::half_t, Flash_kernel_traits<128, 128, 32, 4, cutlass::half_t>>, Is_dropout=true, Is_causal=true, Is_even_N=true, Is_even_K=false, Return_softmax=false, Params=Flash_fwd_params]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(15): here
            instantiation of "void flash_fwd_kernel<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax>(Flash_fwd_params) [with Kernel_traits=Flash_fwd_kernel_traits<128, 128, 32, 4, false, false, cutlass::half_t, Flash_kernel_traits<128, 128, 32, 4, cutlass::half_t>>, Is_dropout=true, Is_causal=true, Is_even_N=true, Is_even_K=false, Return_softmax=false]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(34): here
            instantiation of "void run_flash_fwd<Kernel_traits,Is_dropout,Is_causal>(Flash_fwd_params &, cudaStream_t) [with Kernel_traits=Flash_fwd_kernel_traits<128, 128, 32, 4, false, false, cutlass::half_t, Flash_kernel_traits<128, 128, 32, 4, cutlass::half_t>>, Is_dropout=true, Is_causal=true]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(118): here
            instantiation of "void run_mha_fwd_hdim128<T>(Flash_fwd_params &, cudaStream_t) [with T=cutlass::half_t]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_hdim128_fp16_sm80.cu(31): here

/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_kernel.h(140): warning: variable "MMA_M" was declared but never referenced
          detected during:
            instantiation of "void flash::compute_attn_1rowblock<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &, int, int, int) [with Kernel_traits=Flash_fwd_kernel_traits<128, 128, 32, 4, false, false, cutlass::half_t, Flash_kernel_traits<128, 128, 32, 4, cutlass::half_t>>, Is_dropout=true, Is_causal=true, Is_even_N=false, Is_even_K=true, Return_softmax=true, Params=Flash_fwd_params]" 
(588): here
            instantiation of "void flash::compute_attn<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &) [with Kernel_traits=Flash_fwd_kernel_traits<128, 128, 32, 4, false, false, cutlass::half_t, Flash_kernel_traits<128, 128, 32, 4, cutlass::half_t>>, Is_dropout=true, Is_causal=true, Is_even_N=false, Is_even_K=true, Return_softmax=true, Params=Flash_fwd_params]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(15): here
            instantiation of "void flash_fwd_kernel<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax>(Flash_fwd_params) [with Kernel_traits=Flash_fwd_kernel_traits<128, 128, 32, 4, false, false, cutlass::half_t, Flash_kernel_traits<128, 128, 32, 4, cutlass::half_t>>, Is_dropout=true, Is_causal=true, Is_even_N=false, Is_even_K=true, Return_softmax=true]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(34): here
            instantiation of "void run_flash_fwd<Kernel_traits,Is_dropout,Is_causal>(Flash_fwd_params &, cudaStream_t) [with Kernel_traits=Flash_fwd_kernel_traits<128, 128, 32, 4, false, false, cutlass::half_t, Flash_kernel_traits<128, 128, 32, 4, cutlass::half_t>>, Is_dropout=true, Is_causal=true]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(118): here
            instantiation of "void run_mha_fwd_hdim128<T>(Flash_fwd_params &, cudaStream_t) [with T=cutlass::half_t]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_hdim128_fp16_sm80.cu(31): here

/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_kernel.h(275): warning: variable "tQrQ" was declared but never referenced
          detected during:
            instantiation of "void flash::compute_attn_1rowblock<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &, int, int, int) [with Kernel_traits=Flash_fwd_kernel_traits<128, 128, 32, 4, false, false, cutlass::half_t, Flash_kernel_traits<128, 128, 32, 4, cutlass::half_t>>, Is_dropout=true, Is_causal=true, Is_even_N=false, Is_even_K=true, Return_softmax=true, Params=Flash_fwd_params]" 
(588): here
            instantiation of "void flash::compute_attn<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &) [with Kernel_traits=Flash_fwd_kernel_traits<128, 128, 32, 4, false, false, cutlass::half_t, Flash_kernel_traits<128, 128, 32, 4, cutlass::half_t>>, Is_dropout=true, Is_causal=true, Is_even_N=false, Is_even_K=true, Return_softmax=true, Params=Flash_fwd_params]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(15): here
            instantiation of "void flash_fwd_kernel<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax>(Flash_fwd_params) [with Kernel_traits=Flash_fwd_kernel_traits<128, 128, 32, 4, false, false, cutlass::half_t, Flash_kernel_traits<128, 128, 32, 4, cutlass::half_t>>, Is_dropout=true, Is_causal=true, Is_even_N=false, Is_even_K=true, Return_softmax=true]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(34): here
            instantiation of "void run_flash_fwd<Kernel_traits,Is_dropout,Is_causal>(Flash_fwd_params &, cudaStream_t) [with Kernel_traits=Flash_fwd_kernel_traits<128, 128, 32, 4, false, false, cutlass::half_t, Flash_kernel_traits<128, 128, 32, 4, cutlass::half_t>>, Is_dropout=true, Is_causal=true]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(118): here
            instantiation of "void run_mha_fwd_hdim128<T>(Flash_fwd_params &, cudaStream_t) [with T=cutlass::half_t]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_hdim128_fp16_sm80.cu(31): here

/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_kernel.h(140): warning: variable "MMA_M" was declared but never referenced
          detected during:
            instantiation of "void flash::compute_attn_1rowblock<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &, int, int, int) [with Kernel_traits=Flash_fwd_kernel_traits<128, 128, 32, 4, false, false, cutlass::half_t, Flash_kernel_traits<128, 128, 32, 4, cutlass::half_t>>, Is_dropout=true, Is_causal=true, Is_even_N=false, Is_even_K=true, Return_softmax=false, Params=Flash_fwd_params]" 
(588): here
            instantiation of "void flash::compute_attn<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &) [with Kernel_traits=Flash_fwd_kernel_traits<128, 128, 32, 4, false, false, cutlass::half_t, Flash_kernel_traits<128, 128, 32, 4, cutlass::half_t>>, Is_dropout=true, Is_causal=true, Is_even_N=false, Is_even_K=true, Return_softmax=false, Params=Flash_fwd_params]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(15): here
            instantiation of "void flash_fwd_kernel<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax>(Flash_fwd_params) [with Kernel_traits=Flash_fwd_kernel_traits<128, 128, 32, 4, false, false, cutlass::half_t, Flash_kernel_traits<128, 128, 32, 4, cutlass::half_t>>, Is_dropout=true, Is_causal=true, Is_even_N=false, Is_even_K=true, Return_softmax=false]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(34): here
            instantiation of "void run_flash_fwd<Kernel_traits,Is_dropout,Is_causal>(Flash_fwd_params &, cudaStream_t) [with Kernel_traits=Flash_fwd_kernel_traits<128, 128, 32, 4, false, false, cutlass::half_t, Flash_kernel_traits<128, 128, 32, 4, cutlass::half_t>>, Is_dropout=true, Is_causal=true]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(118): here
            instantiation of "void run_mha_fwd_hdim128<T>(Flash_fwd_params &, cudaStream_t) [with T=cutlass::half_t]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_hdim128_fp16_sm80.cu(31): here

/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_kernel.h(275): warning: variable "tQrQ" was declared but never referenced
          detected during:
            instantiation of "void flash::compute_attn_1rowblock<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &, int, int, int) [with Kernel_traits=Flash_fwd_kernel_traits<128, 128, 32, 4, false, false, cutlass::half_t, Flash_kernel_traits<128, 128, 32, 4, cutlass::half_t>>, Is_dropout=true, Is_causal=true, Is_even_N=false, Is_even_K=true, Return_softmax=false, Params=Flash_fwd_params]" 
(588): here
            instantiation of "void flash::compute_attn<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &) [with Kernel_traits=Flash_fwd_kernel_traits<128, 128, 32, 4, false, false, cutlass::half_t, Flash_kernel_traits<128, 128, 32, 4, cutlass::half_t>>, Is_dropout=true, Is_causal=true, Is_even_N=false, Is_even_K=true, Return_softmax=false, Params=Flash_fwd_params]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(15): here
            instantiation of "void flash_fwd_kernel<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax>(Flash_fwd_params) [with Kernel_traits=Flash_fwd_kernel_traits<128, 128, 32, 4, false, false, cutlass::half_t, Flash_kernel_traits<128, 128, 32, 4, cutlass::half_t>>, Is_dropout=true, Is_causal=true, Is_even_N=false, Is_even_K=true, Return_softmax=false]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(34): here
            instantiation of "void run_flash_fwd<Kernel_traits,Is_dropout,Is_causal>(Flash_fwd_params &, cudaStream_t) [with Kernel_traits=Flash_fwd_kernel_traits<128, 128, 32, 4, false, false, cutlass::half_t, Flash_kernel_traits<128, 128, 32, 4, cutlass::half_t>>, Is_dropout=true, Is_causal=true]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(118): here
            instantiation of "void run_mha_fwd_hdim128<T>(Flash_fwd_params &, cudaStream_t) [with T=cutlass::half_t]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_hdim128_fp16_sm80.cu(31): here

/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_kernel.h(140): warning: variable "MMA_M" was declared but never referenced
          detected during:
            instantiation of "void flash::compute_attn_1rowblock<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &, int, int, int) [with Kernel_traits=Flash_fwd_kernel_traits<128, 128, 32, 4, false, false, cutlass::half_t, Flash_kernel_traits<128, 128, 32, 4, cutlass::half_t>>, Is_dropout=true, Is_causal=true, Is_even_N=false, Is_even_K=false, Return_softmax=true, Params=Flash_fwd_params]" 
(588): here
            instantiation of "void flash::compute_attn<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &) [with Kernel_traits=Flash_fwd_kernel_traits<128, 128, 32, 4, false, false, cutlass::half_t, Flash_kernel_traits<128, 128, 32, 4, cutlass::half_t>>, Is_dropout=true, Is_causal=true, Is_even_N=false, Is_even_K=false, Return_softmax=true, Params=Flash_fwd_params]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(15): here
            instantiation of "void flash_fwd_kernel<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax>(Flash_fwd_params) [with Kernel_traits=Flash_fwd_kernel_traits<128, 128, 32, 4, false, false, cutlass::half_t, Flash_kernel_traits<128, 128, 32, 4, cutlass::half_t>>, Is_dropout=true, Is_causal=true, Is_even_N=false, Is_even_K=false, Return_softmax=true]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(34): here
            instantiation of "void run_flash_fwd<Kernel_traits,Is_dropout,Is_causal>(Flash_fwd_params &, cudaStream_t) [with Kernel_traits=Flash_fwd_kernel_traits<128, 128, 32, 4, false, false, cutlass::half_t, Flash_kernel_traits<128, 128, 32, 4, cutlass::half_t>>, Is_dropout=true, Is_causal=true]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(118): here
            instantiation of "void run_mha_fwd_hdim128<T>(Flash_fwd_params &, cudaStream_t) [with T=cutlass::half_t]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_hdim128_fp16_sm80.cu(31): here

/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_kernel.h(275): warning: variable "tQrQ" was declared but never referenced
          detected during:
            instantiation of "void flash::compute_attn_1rowblock<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &, int, int, int) [with Kernel_traits=Flash_fwd_kernel_traits<128, 128, 32, 4, false, false, cutlass::half_t, Flash_kernel_traits<128, 128, 32, 4, cutlass::half_t>>, Is_dropout=true, Is_causal=true, Is_even_N=false, Is_even_K=false, Return_softmax=true, Params=Flash_fwd_params]" 
(588): here
            instantiation of "void flash::compute_attn<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &) [with Kernel_traits=Flash_fwd_kernel_traits<128, 128, 32, 4, false, false, cutlass::half_t, Flash_kernel_traits<128, 128, 32, 4, cutlass::half_t>>, Is_dropout=true, Is_causal=true, Is_even_N=false, Is_even_K=false, Return_softmax=true, Params=Flash_fwd_params]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(15): here
            instantiation of "void flash_fwd_kernel<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax>(Flash_fwd_params) [with Kernel_traits=Flash_fwd_kernel_traits<128, 128, 32, 4, false, false, cutlass::half_t, Flash_kernel_traits<128, 128, 32, 4, cutlass::half_t>>, Is_dropout=true, Is_causal=true, Is_even_N=false, Is_even_K=false, Return_softmax=true]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(34): here
            instantiation of "void run_flash_fwd<Kernel_traits,Is_dropout,Is_causal>(Flash_fwd_params &, cudaStream_t) [with Kernel_traits=Flash_fwd_kernel_traits<128, 128, 32, 4, false, false, cutlass::half_t, Flash_kernel_traits<128, 128, 32, 4, cutlass::half_t>>, Is_dropout=true, Is_causal=true]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(118): here
            instantiation of "void run_mha_fwd_hdim128<T>(Flash_fwd_params &, cudaStream_t) [with T=cutlass::half_t]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_hdim128_fp16_sm80.cu(31): here

/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_kernel.h(140): warning: variable "MMA_M" was declared but never referenced
          detected during:
            instantiation of "void flash::compute_attn_1rowblock<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &, int, int, int) [with Kernel_traits=Flash_fwd_kernel_traits<128, 128, 32, 4, false, false, cutlass::half_t, Flash_kernel_traits<128, 128, 32, 4, cutlass::half_t>>, Is_dropout=true, Is_causal=true, Is_even_N=false, Is_even_K=false, Return_softmax=false, Params=Flash_fwd_params]" 
(588): here
            instantiation of "void flash::compute_attn<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &) [with Kernel_traits=Flash_fwd_kernel_traits<128, 128, 32, 4, false, false, cutlass::half_t, Flash_kernel_traits<128, 128, 32, 4, cutlass::half_t>>, Is_dropout=true, Is_causal=true, Is_even_N=false, Is_even_K=false, Return_softmax=false, Params=Flash_fwd_params]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(15): here
            instantiation of "void flash_fwd_kernel<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax>(Flash_fwd_params) [with Kernel_traits=Flash_fwd_kernel_traits<128, 128, 32, 4, false, false, cutlass::half_t, Flash_kernel_traits<128, 128, 32, 4, cutlass::half_t>>, Is_dropout=true, Is_causal=true, Is_even_N=false, Is_even_K=false, Return_softmax=false]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(34): here
            instantiation of "void run_flash_fwd<Kernel_traits,Is_dropout,Is_causal>(Flash_fwd_params &, cudaStream_t) [with Kernel_traits=Flash_fwd_kernel_traits<128, 128, 32, 4, false, false, cutlass::half_t, Flash_kernel_traits<128, 128, 32, 4, cutlass::half_t>>, Is_dropout=true, Is_causal=true]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(118): here
            instantiation of "void run_mha_fwd_hdim128<T>(Flash_fwd_params &, cudaStream_t) [with T=cutlass::half_t]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_hdim128_fp16_sm80.cu(31): here

/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_kernel.h(275): warning: variable "tQrQ" was declared but never referenced
          detected during:
            instantiation of "void flash::compute_attn_1rowblock<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &, int, int, int) [with Kernel_traits=Flash_fwd_kernel_traits<128, 128, 32, 4, false, false, cutlass::half_t, Flash_kernel_traits<128, 128, 32, 4, cutlass::half_t>>, Is_dropout=true, Is_causal=true, Is_even_N=false, Is_even_K=false, Return_softmax=false, Params=Flash_fwd_params]" 
(588): here
            instantiation of "void flash::compute_attn<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &) [with Kernel_traits=Flash_fwd_kernel_traits<128, 128, 32, 4, false, false, cutlass::half_t, Flash_kernel_traits<128, 128, 32, 4, cutlass::half_t>>, Is_dropout=true, Is_causal=true, Is_even_N=false, Is_even_K=false, Return_softmax=false, Params=Flash_fwd_params]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(15): here
            instantiation of "void flash_fwd_kernel<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax>(Flash_fwd_params) [with Kernel_traits=Flash_fwd_kernel_traits<128, 128, 32, 4, false, false, cutlass::half_t, Flash_kernel_traits<128, 128, 32, 4, cutlass::half_t>>, Is_dropout=true, Is_causal=true, Is_even_N=false, Is_even_K=false, Return_softmax=false]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(34): here
            instantiation of "void run_flash_fwd<Kernel_traits,Is_dropout,Is_causal>(Flash_fwd_params &, cudaStream_t) [with Kernel_traits=Flash_fwd_kernel_traits<128, 128, 32, 4, false, false, cutlass::half_t, Flash_kernel_traits<128, 128, 32, 4, cutlass::half_t>>, Is_dropout=true, Is_causal=true]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(118): here
            instantiation of "void run_mha_fwd_hdim128<T>(Flash_fwd_params &, cudaStream_t) [with T=cutlass::half_t]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_hdim128_fp16_sm80.cu(31): here

/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_kernel.h(140): warning: variable "MMA_M" was declared but never referenced
          detected during:
            instantiation of "void flash::compute_attn_1rowblock<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &, int, int, int) [with Kernel_traits=Flash_fwd_kernel_traits<128, 128, 32, 4, false, false, cutlass::half_t, Flash_kernel_traits<128, 128, 32, 4, cutlass::half_t>>, Is_dropout=true, Is_causal=false, Is_even_N=true, Is_even_K=true, Return_softmax=true, Params=Flash_fwd_params]" 
(588): here
            instantiation of "void flash::compute_attn<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &) [with Kernel_traits=Flash_fwd_kernel_traits<128, 128, 32, 4, false, false, cutlass::half_t, Flash_kernel_traits<128, 128, 32, 4, cutlass::half_t>>, Is_dropout=true, Is_causal=false, Is_even_N=true, Is_even_K=true, Return_softmax=true, Params=Flash_fwd_params]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(15): here
            instantiation of "void flash_fwd_kernel<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax>(Flash_fwd_params) [with Kernel_traits=Flash_fwd_kernel_traits<128, 128, 32, 4, false, false, cutlass::half_t, Flash_kernel_traits<128, 128, 32, 4, cutlass::half_t>>, Is_dropout=true, Is_causal=false, Is_even_N=true, Is_even_K=true, Return_softmax=true]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(34): here
            instantiation of "void run_flash_fwd<Kernel_traits,Is_dropout,Is_causal>(Flash_fwd_params &, cudaStream_t) [with Kernel_traits=Flash_fwd_kernel_traits<128, 128, 32, 4, false, false, cutlass::half_t, Flash_kernel_traits<128, 128, 32, 4, cutlass::half_t>>, Is_dropout=true, Is_causal=false]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(118): here
            instantiation of "void run_mha_fwd_hdim128<T>(Flash_fwd_params &, cudaStream_t) [with T=cutlass::half_t]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_hdim128_fp16_sm80.cu(31): here

/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_kernel.h(275): warning: variable "tQrQ" was declared but never referenced
          detected during:
            instantiation of "void flash::compute_attn_1rowblock<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &, int, int, int) [with Kernel_traits=Flash_fwd_kernel_traits<128, 128, 32, 4, false, false, cutlass::half_t, Flash_kernel_traits<128, 128, 32, 4, cutlass::half_t>>, Is_dropout=true, Is_causal=false, Is_even_N=true, Is_even_K=true, Return_softmax=true, Params=Flash_fwd_params]" 
(588): here
            instantiation of "void flash::compute_attn<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &) [with Kernel_traits=Flash_fwd_kernel_traits<128, 128, 32, 4, false, false, cutlass::half_t, Flash_kernel_traits<128, 128, 32, 4, cutlass::half_t>>, Is_dropout=true, Is_causal=false, Is_even_N=true, Is_even_K=true, Return_softmax=true, Params=Flash_fwd_params]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(15): here
            instantiation of "void flash_fwd_kernel<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax>(Flash_fwd_params) [with Kernel_traits=Flash_fwd_kernel_traits<128, 128, 32, 4, false, false, cutlass::half_t, Flash_kernel_traits<128, 128, 32, 4, cutlass::half_t>>, Is_dropout=true, Is_causal=false, Is_even_N=true, Is_even_K=true, Return_softmax=true]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(34): here
            instantiation of "void run_flash_fwd<Kernel_traits,Is_dropout,Is_causal>(Flash_fwd_params &, cudaStream_t) [with Kernel_traits=Flash_fwd_kernel_traits<128, 128, 32, 4, false, false, cutlass::half_t, Flash_kernel_traits<128, 128, 32, 4, cutlass::half_t>>, Is_dropout=true, Is_causal=false]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(118): here
            instantiation of "void run_mha_fwd_hdim128<T>(Flash_fwd_params &, cudaStream_t) [with T=cutlass::half_t]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_hdim128_fp16_sm80.cu(31): here

/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_kernel.h(140): warning: variable "MMA_M" was declared but never referenced
          detected during:
            instantiation of "void flash::compute_attn_1rowblock<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &, int, int, int) [with Kernel_traits=Flash_fwd_kernel_traits<128, 128, 32, 4, false, false, cutlass::half_t, Flash_kernel_traits<128, 128, 32, 4, cutlass::half_t>>, Is_dropout=true, Is_causal=false, Is_even_N=true, Is_even_K=true, Return_softmax=false, Params=Flash_fwd_params]" 
(588): here
            instantiation of "void flash::compute_attn<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &) [with Kernel_traits=Flash_fwd_kernel_traits<128, 128, 32, 4, false, false, cutlass::half_t, Flash_kernel_traits<128, 128, 32, 4, cutlass::half_t>>, Is_dropout=true, Is_causal=false, Is_even_N=true, Is_even_K=true, Return_softmax=false, Params=Flash_fwd_params]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(15): here
            instantiation of "void flash_fwd_kernel<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax>(Flash_fwd_params) [with Kernel_traits=Flash_fwd_kernel_traits<128, 128, 32, 4, false, false, cutlass::half_t, Flash_kernel_traits<128, 128, 32, 4, cutlass::half_t>>, Is_dropout=true, Is_causal=false, Is_even_N=true, Is_even_K=true, Return_softmax=false]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(34): here
            instantiation of "void run_flash_fwd<Kernel_traits,Is_dropout,Is_causal>(Flash_fwd_params &, cudaStream_t) [with Kernel_traits=Flash_fwd_kernel_traits<128, 128, 32, 4, false, false, cutlass::half_t, Flash_kernel_traits<128, 128, 32, 4, cutlass::half_t>>, Is_dropout=true, Is_causal=false]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(118): here
            instantiation of "void run_mha_fwd_hdim128<T>(Flash_fwd_params &, cudaStream_t) [with T=cutlass::half_t]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_hdim128_fp16_sm80.cu(31): here

/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_kernel.h(275): warning: variable "tQrQ" was declared but never referenced
          detected during:
            instantiation of "void flash::compute_attn_1rowblock<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &, int, int, int) [with Kernel_traits=Flash_fwd_kernel_traits<128, 128, 32, 4, false, false, cutlass::half_t, Flash_kernel_traits<128, 128, 32, 4, cutlass::half_t>>, Is_dropout=true, Is_causal=false, Is_even_N=true, Is_even_K=true, Return_softmax=false, Params=Flash_fwd_params]" 
(588): here
            instantiation of "void flash::compute_attn<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &) [with Kernel_traits=Flash_fwd_kernel_traits<128, 128, 32, 4, false, false, cutlass::half_t, Flash_kernel_traits<128, 128, 32, 4, cutlass::half_t>>, Is_dropout=true, Is_causal=false, Is_even_N=true, Is_even_K=true, Return_softmax=false, Params=Flash_fwd_params]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(15): here
            instantiation of "void flash_fwd_kernel<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax>(Flash_fwd_params) [with Kernel_traits=Flash_fwd_kernel_traits<128, 128, 32, 4, false, false, cutlass::half_t, Flash_kernel_traits<128, 128, 32, 4, cutlass::half_t>>, Is_dropout=true, Is_causal=false, Is_even_N=true, Is_even_K=true, Return_softmax=false]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(34): here
            instantiation of "void run_flash_fwd<Kernel_traits,Is_dropout,Is_causal>(Flash_fwd_params &, cudaStream_t) [with Kernel_traits=Flash_fwd_kernel_traits<128, 128, 32, 4, false, false, cutlass::half_t, Flash_kernel_traits<128, 128, 32, 4, cutlass::half_t>>, Is_dropout=true, Is_causal=false]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(118): here
            instantiation of "void run_mha_fwd_hdim128<T>(Flash_fwd_params &, cudaStream_t) [with T=cutlass::half_t]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_hdim128_fp16_sm80.cu(31): here

/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_kernel.h(140): warning: variable "MMA_M" was declared but never referenced
          detected during:
            instantiation of "void flash::compute_attn_1rowblock<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &, int, int, int) [with Kernel_traits=Flash_fwd_kernel_traits<128, 128, 32, 4, false, false, cutlass::half_t, Flash_kernel_traits<128, 128, 32, 4, cutlass::half_t>>, Is_dropout=true, Is_causal=false, Is_even_N=true, Is_even_K=false, Return_softmax=true, Params=Flash_fwd_params]" 
(588): here
            instantiation of "void flash::compute_attn<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &) [with Kernel_traits=Flash_fwd_kernel_traits<128, 128, 32, 4, false, false, cutlass::half_t, Flash_kernel_traits<128, 128, 32, 4, cutlass::half_t>>, Is_dropout=true, Is_causal=false, Is_even_N=true, Is_even_K=false, Return_softmax=true, Params=Flash_fwd_params]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(15): here
            instantiation of "void flash_fwd_kernel<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax>(Flash_fwd_params) [with Kernel_traits=Flash_fwd_kernel_traits<128, 128, 32, 4, false, false, cutlass::half_t, Flash_kernel_traits<128, 128, 32, 4, cutlass::half_t>>, Is_dropout=true, Is_causal=false, Is_even_N=true, Is_even_K=false, Return_softmax=true]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(34): here
            instantiation of "void run_flash_fwd<Kernel_traits,Is_dropout,Is_causal>(Flash_fwd_params &, cudaStream_t) [with Kernel_traits=Flash_fwd_kernel_traits<128, 128, 32, 4, false, false, cutlass::half_t, Flash_kernel_traits<128, 128, 32, 4, cutlass::half_t>>, Is_dropout=true, Is_causal=false]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(118): here
            instantiation of "void run_mha_fwd_hdim128<T>(Flash_fwd_params &, cudaStream_t) [with T=cutlass::half_t]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_hdim128_fp16_sm80.cu(31): here

/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_kernel.h(275): warning: variable "tQrQ" was declared but never referenced
          detected during:
            instantiation of "void flash::compute_attn_1rowblock<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &, int, int, int) [with Kernel_traits=Flash_fwd_kernel_traits<128, 128, 32, 4, false, false, cutlass::half_t, Flash_kernel_traits<128, 128, 32, 4, cutlass::half_t>>, Is_dropout=true, Is_causal=false, Is_even_N=true, Is_even_K=false, Return_softmax=true, Params=Flash_fwd_params]" 
(588): here
            instantiation of "void flash::compute_attn<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &) [with Kernel_traits=Flash_fwd_kernel_traits<128, 128, 32, 4, false, false, cutlass::half_t, Flash_kernel_traits<128, 128, 32, 4, cutlass::half_t>>, Is_dropout=true, Is_causal=false, Is_even_N=true, Is_even_K=false, Return_softmax=true, Params=Flash_fwd_params]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(15): here
            instantiation of "void flash_fwd_kernel<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax>(Flash_fwd_params) [with Kernel_traits=Flash_fwd_kernel_traits<128, 128, 32, 4, false, false, cutlass::half_t, Flash_kernel_traits<128, 128, 32, 4, cutlass::half_t>>, Is_dropout=true, Is_causal=false, Is_even_N=true, Is_even_K=false, Return_softmax=true]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(34): here
            instantiation of "void run_flash_fwd<Kernel_traits,Is_dropout,Is_causal>(Flash_fwd_params &, cudaStream_t) [with Kernel_traits=Flash_fwd_kernel_traits<128, 128, 32, 4, false, false, cutlass::half_t, Flash_kernel_traits<128, 128, 32, 4, cutlass::half_t>>, Is_dropout=true, Is_causal=false]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(118): here
            instantiation of "void run_mha_fwd_hdim128<T>(Flash_fwd_params &, cudaStream_t) [with T=cutlass::half_t]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_hdim128_fp16_sm80.cu(31): here

/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_kernel.h(140): warning: variable "MMA_M" was declared but never referenced
          detected during:
            instantiation of "void flash::compute_attn_1rowblock<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &, int, int, int) [with Kernel_traits=Flash_fwd_kernel_traits<128, 128, 32, 4, false, false, cutlass::half_t, Flash_kernel_traits<128, 128, 32, 4, cutlass::half_t>>, Is_dropout=true, Is_causal=false, Is_even_N=true, Is_even_K=false, Return_softmax=false, Params=Flash_fwd_params]" 
(588): here
            instantiation of "void flash::compute_attn<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &) [with Kernel_traits=Flash_fwd_kernel_traits<128, 128, 32, 4, false, false, cutlass::half_t, Flash_kernel_traits<128, 128, 32, 4, cutlass::half_t>>, Is_dropout=true, Is_causal=false, Is_even_N=true, Is_even_K=false, Return_softmax=false, Params=Flash_fwd_params]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(15): here
            instantiation of "void flash_fwd_kernel<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax>(Flash_fwd_params) [with Kernel_traits=Flash_fwd_kernel_traits<128, 128, 32, 4, false, false, cutlass::half_t, Flash_kernel_traits<128, 128, 32, 4, cutlass::half_t>>, Is_dropout=true, Is_causal=false, Is_even_N=true, Is_even_K=false, Return_softmax=false]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(34): here
            instantiation of "void run_flash_fwd<Kernel_traits,Is_dropout,Is_causal>(Flash_fwd_params &, cudaStream_t) [with Kernel_traits=Flash_fwd_kernel_traits<128, 128, 32, 4, false, false, cutlass::half_t, Flash_kernel_traits<128, 128, 32, 4, cutlass::half_t>>, Is_dropout=true, Is_causal=false]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(118): here
            instantiation of "void run_mha_fwd_hdim128<T>(Flash_fwd_params &, cudaStream_t) [with T=cutlass::half_t]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_hdim128_fp16_sm80.cu(31): here

/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_kernel.h(275): warning: variable "tQrQ" was declared but never referenced
          detected during:
            instantiation of "void flash::compute_attn_1rowblock<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &, int, int, int) [with Kernel_traits=Flash_fwd_kernel_traits<128, 128, 32, 4, false, false, cutlass::half_t, Flash_kernel_traits<128, 128, 32, 4, cutlass::half_t>>, Is_dropout=true, Is_causal=false, Is_even_N=true, Is_even_K=false, Return_softmax=false, Params=Flash_fwd_params]" 
(588): here
            instantiation of "void flash::compute_attn<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &) [with Kernel_traits=Flash_fwd_kernel_traits<128, 128, 32, 4, false, false, cutlass::half_t, Flash_kernel_traits<128, 128, 32, 4, cutlass::half_t>>, Is_dropout=true, Is_causal=false, Is_even_N=true, Is_even_K=false, Return_softmax=false, Params=Flash_fwd_params]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(15): here
            instantiation of "void flash_fwd_kernel<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax>(Flash_fwd_params) [with Kernel_traits=Flash_fwd_kernel_traits<128, 128, 32, 4, false, false, cutlass::half_t, Flash_kernel_traits<128, 128, 32, 4, cutlass::half_t>>, Is_dropout=true, Is_causal=false, Is_even_N=true, Is_even_K=false, Return_softmax=false]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(34): here
            instantiation of "void run_flash_fwd<Kernel_traits,Is_dropout,Is_causal>(Flash_fwd_params &, cudaStream_t) [with Kernel_traits=Flash_fwd_kernel_traits<128, 128, 32, 4, false, false, cutlass::half_t, Flash_kernel_traits<128, 128, 32, 4, cutlass::half_t>>, Is_dropout=true, Is_causal=false]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(118): here
            instantiation of "void run_mha_fwd_hdim128<T>(Flash_fwd_params &, cudaStream_t) [with T=cutlass::half_t]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_hdim128_fp16_sm80.cu(31): here

/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_kernel.h(140): warning: variable "MMA_M" was declared but never referenced
          detected during:
            instantiation of "void flash::compute_attn_1rowblock<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &, int, int, int) [with Kernel_traits=Flash_fwd_kernel_traits<128, 128, 32, 4, false, false, cutlass::half_t, Flash_kernel_traits<128, 128, 32, 4, cutlass::half_t>>, Is_dropout=true, Is_causal=false, Is_even_N=false, Is_even_K=true, Return_softmax=true, Params=Flash_fwd_params]" 
(588): here
            instantiation of "void flash::compute_attn<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &) [with Kernel_traits=Flash_fwd_kernel_traits<128, 128, 32, 4, false, false, cutlass::half_t, Flash_kernel_traits<128, 128, 32, 4, cutlass::half_t>>, Is_dropout=true, Is_causal=false, Is_even_N=false, Is_even_K=true, Return_softmax=true, Params=Flash_fwd_params]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(15): here
            instantiation of "void flash_fwd_kernel<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax>(Flash_fwd_params) [with Kernel_traits=Flash_fwd_kernel_traits<128, 128, 32, 4, false, false, cutlass::half_t, Flash_kernel_traits<128, 128, 32, 4, cutlass::half_t>>, Is_dropout=true, Is_causal=false, Is_even_N=false, Is_even_K=true, Return_softmax=true]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(34): here
            instantiation of "void run_flash_fwd<Kernel_traits,Is_dropout,Is_causal>(Flash_fwd_params &, cudaStream_t) [with Kernel_traits=Flash_fwd_kernel_traits<128, 128, 32, 4, false, false, cutlass::half_t, Flash_kernel_traits<128, 128, 32, 4, cutlass::half_t>>, Is_dropout=true, Is_causal=false]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(118): here
            instantiation of "void run_mha_fwd_hdim128<T>(Flash_fwd_params &, cudaStream_t) [with T=cutlass::half_t]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_hdim128_fp16_sm80.cu(31): here

/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_kernel.h(275): warning: variable "tQrQ" was declared but never referenced
          detected during:
            instantiation of "void flash::compute_attn_1rowblock<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &, int, int, int) [with Kernel_traits=Flash_fwd_kernel_traits<128, 128, 32, 4, false, false, cutlass::half_t, Flash_kernel_traits<128, 128, 32, 4, cutlass::half_t>>, Is_dropout=true, Is_causal=false, Is_even_N=false, Is_even_K=true, Return_softmax=true, Params=Flash_fwd_params]" 
(588): here
            instantiation of "void flash::compute_attn<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &) [with Kernel_traits=Flash_fwd_kernel_traits<128, 128, 32, 4, false, false, cutlass::half_t, Flash_kernel_traits<128, 128, 32, 4, cutlass::half_t>>, Is_dropout=true, Is_causal=false, Is_even_N=false, Is_even_K=true, Return_softmax=true, Params=Flash_fwd_params]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(15): here
            instantiation of "void flash_fwd_kernel<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax>(Flash_fwd_params) [with Kernel_traits=Flash_fwd_kernel_traits<128, 128, 32, 4, false, false, cutlass::half_t, Flash_kernel_traits<128, 128, 32, 4, cutlass::half_t>>, Is_dropout=true, Is_causal=false, Is_even_N=false, Is_even_K=true, Return_softmax=true]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(34): here
            instantiation of "void run_flash_fwd<Kernel_traits,Is_dropout,Is_causal>(Flash_fwd_params &, cudaStream_t) [with Kernel_traits=Flash_fwd_kernel_traits<128, 128, 32, 4, false, false, cutlass::half_t, Flash_kernel_traits<128, 128, 32, 4, cutlass::half_t>>, Is_dropout=true, Is_causal=false]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(118): here
            instantiation of "void run_mha_fwd_hdim128<T>(Flash_fwd_params &, cudaStream_t) [with T=cutlass::half_t]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_hdim128_fp16_sm80.cu(31): here

/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_kernel.h(140): warning: variable "MMA_M" was declared but never referenced
          detected during:
            instantiation of "void flash::compute_attn_1rowblock<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &, int, int, int) [with Kernel_traits=Flash_fwd_kernel_traits<128, 128, 32, 4, false, false, cutlass::half_t, Flash_kernel_traits<128, 128, 32, 4, cutlass::half_t>>, Is_dropout=true, Is_causal=false, Is_even_N=false, Is_even_K=true, Return_softmax=false, Params=Flash_fwd_params]" 
(588): here
            instantiation of "void flash::compute_attn<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &) [with Kernel_traits=Flash_fwd_kernel_traits<128, 128, 32, 4, false, false, cutlass::half_t, Flash_kernel_traits<128, 128, 32, 4, cutlass::half_t>>, Is_dropout=true, Is_causal=false, Is_even_N=false, Is_even_K=true, Return_softmax=false, Params=Flash_fwd_params]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(15): here
            instantiation of "void flash_fwd_kernel<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax>(Flash_fwd_params) [with Kernel_traits=Flash_fwd_kernel_traits<128, 128, 32, 4, false, false, cutlass::half_t, Flash_kernel_traits<128, 128, 32, 4, cutlass::half_t>>, Is_dropout=true, Is_causal=false, Is_even_N=false, Is_even_K=true, Return_softmax=false]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(34): here
            instantiation of "void run_flash_fwd<Kernel_traits,Is_dropout,Is_causal>(Flash_fwd_params &, cudaStream_t) [with Kernel_traits=Flash_fwd_kernel_traits<128, 128, 32, 4, false, false, cutlass::half_t, Flash_kernel_traits<128, 128, 32, 4, cutlass::half_t>>, Is_dropout=true, Is_causal=false]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(118): here
            instantiation of "void run_mha_fwd_hdim128<T>(Flash_fwd_params &, cudaStream_t) [with T=cutlass::half_t]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_hdim128_fp16_sm80.cu(31): here

/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_kernel.h(275): warning: variable "tQrQ" was declared but never referenced
          detected during:
            instantiation of "void flash::compute_attn_1rowblock<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &, int, int, int) [with Kernel_traits=Flash_fwd_kernel_traits<128, 128, 32, 4, false, false, cutlass::half_t, Flash_kernel_traits<128, 128, 32, 4, cutlass::half_t>>, Is_dropout=true, Is_causal=false, Is_even_N=false, Is_even_K=true, Return_softmax=false, Params=Flash_fwd_params]" 
(588): here
            instantiation of "void flash::compute_attn<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &) [with Kernel_traits=Flash_fwd_kernel_traits<128, 128, 32, 4, false, false, cutlass::half_t, Flash_kernel_traits<128, 128, 32, 4, cutlass::half_t>>, Is_dropout=true, Is_causal=false, Is_even_N=false, Is_even_K=true, Return_softmax=false, Params=Flash_fwd_params]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(15): here
            instantiation of "void flash_fwd_kernel<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax>(Flash_fwd_params) [with Kernel_traits=Flash_fwd_kernel_traits<128, 128, 32, 4, false, false, cutlass::half_t, Flash_kernel_traits<128, 128, 32, 4, cutlass::half_t>>, Is_dropout=true, Is_causal=false, Is_even_N=false, Is_even_K=true, Return_softmax=false]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(34): here
            instantiation of "void run_flash_fwd<Kernel_traits,Is_dropout,Is_causal>(Flash_fwd_params &, cudaStream_t) [with Kernel_traits=Flash_fwd_kernel_traits<128, 128, 32, 4, false, false, cutlass::half_t, Flash_kernel_traits<128, 128, 32, 4, cutlass::half_t>>, Is_dropout=true, Is_causal=false]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(118): here
            instantiation of "void run_mha_fwd_hdim128<T>(Flash_fwd_params &, cudaStream_t) [with T=cutlass::half_t]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_hdim128_fp16_sm80.cu(31): here

/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_kernel.h(140): warning: variable "MMA_M" was declared but never referenced
          detected during:
            instantiation of "void flash::compute_attn_1rowblock<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &, int, int, int) [with Kernel_traits=Flash_fwd_kernel_traits<128, 128, 32, 4, false, false, cutlass::half_t, Flash_kernel_traits<128, 128, 32, 4, cutlass::half_t>>, Is_dropout=true, Is_causal=false, Is_even_N=false, Is_even_K=false, Return_softmax=true, Params=Flash_fwd_params]" 
(588): here
            instantiation of "void flash::compute_attn<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &) [with Kernel_traits=Flash_fwd_kernel_traits<128, 128, 32, 4, false, false, cutlass::half_t, Flash_kernel_traits<128, 128, 32, 4, cutlass::half_t>>, Is_dropout=true, Is_causal=false, Is_even_N=false, Is_even_K=false, Return_softmax=true, Params=Flash_fwd_params]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(15): here
            instantiation of "void flash_fwd_kernel<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax>(Flash_fwd_params) [with Kernel_traits=Flash_fwd_kernel_traits<128, 128, 32, 4, false, false, cutlass::half_t, Flash_kernel_traits<128, 128, 32, 4, cutlass::half_t>>, Is_dropout=true, Is_causal=false, Is_even_N=false, Is_even_K=false, Return_softmax=true]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(34): here
            instantiation of "void run_flash_fwd<Kernel_traits,Is_dropout,Is_causal>(Flash_fwd_params &, cudaStream_t) [with Kernel_traits=Flash_fwd_kernel_traits<128, 128, 32, 4, false, false, cutlass::half_t, Flash_kernel_traits<128, 128, 32, 4, cutlass::half_t>>, Is_dropout=true, Is_causal=false]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(118): here
            instantiation of "void run_mha_fwd_hdim128<T>(Flash_fwd_params &, cudaStream_t) [with T=cutlass::half_t]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_hdim128_fp16_sm80.cu(31): here

/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_kernel.h(275): warning: variable "tQrQ" was declared but never referenced
          detected during:
            instantiation of "void flash::compute_attn_1rowblock<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &, int, int, int) [with Kernel_traits=Flash_fwd_kernel_traits<128, 128, 32, 4, false, false, cutlass::half_t, Flash_kernel_traits<128, 128, 32, 4, cutlass::half_t>>, Is_dropout=true, Is_causal=false, Is_even_N=false, Is_even_K=false, Return_softmax=true, Params=Flash_fwd_params]" 
(588): here
            instantiation of "void flash::compute_attn<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &) [with Kernel_traits=Flash_fwd_kernel_traits<128, 128, 32, 4, false, false, cutlass::half_t, Flash_kernel_traits<128, 128, 32, 4, cutlass::half_t>>, Is_dropout=true, Is_causal=false, Is_even_N=false, Is_even_K=false, Return_softmax=true, Params=Flash_fwd_params]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(15): here
            instantiation of "void flash_fwd_kernel<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax>(Flash_fwd_params) [with Kernel_traits=Flash_fwd_kernel_traits<128, 128, 32, 4, false, false, cutlass::half_t, Flash_kernel_traits<128, 128, 32, 4, cutlass::half_t>>, Is_dropout=true, Is_causal=false, Is_even_N=false, Is_even_K=false, Return_softmax=true]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(34): here
            instantiation of "void run_flash_fwd<Kernel_traits,Is_dropout,Is_causal>(Flash_fwd_params &, cudaStream_t) [with Kernel_traits=Flash_fwd_kernel_traits<128, 128, 32, 4, false, false, cutlass::half_t, Flash_kernel_traits<128, 128, 32, 4, cutlass::half_t>>, Is_dropout=true, Is_causal=false]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(118): here
            instantiation of "void run_mha_fwd_hdim128<T>(Flash_fwd_params &, cudaStream_t) [with T=cutlass::half_t]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_hdim128_fp16_sm80.cu(31): here

/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_kernel.h(140): warning: variable "MMA_M" was declared but never referenced
          detected during:
            instantiation of "void flash::compute_attn_1rowblock<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &, int, int, int) [with Kernel_traits=Flash_fwd_kernel_traits<128, 128, 32, 4, false, false, cutlass::half_t, Flash_kernel_traits<128, 128, 32, 4, cutlass::half_t>>, Is_dropout=true, Is_causal=false, Is_even_N=false, Is_even_K=false, Return_softmax=false, Params=Flash_fwd_params]" 
(588): here
            instantiation of "void flash::compute_attn<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &) [with Kernel_traits=Flash_fwd_kernel_traits<128, 128, 32, 4, false, false, cutlass::half_t, Flash_kernel_traits<128, 128, 32, 4, cutlass::half_t>>, Is_dropout=true, Is_causal=false, Is_even_N=false, Is_even_K=false, Return_softmax=false, Params=Flash_fwd_params]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(15): here
            instantiation of "void flash_fwd_kernel<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax>(Flash_fwd_params) [with Kernel_traits=Flash_fwd_kernel_traits<128, 128, 32, 4, false, false, cutlass::half_t, Flash_kernel_traits<128, 128, 32, 4, cutlass::half_t>>, Is_dropout=true, Is_causal=false, Is_even_N=false, Is_even_K=false, Return_softmax=false]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(34): here
            instantiation of "void run_flash_fwd<Kernel_traits,Is_dropout,Is_causal>(Flash_fwd_params &, cudaStream_t) [with Kernel_traits=Flash_fwd_kernel_traits<128, 128, 32, 4, false, false, cutlass::half_t, Flash_kernel_traits<128, 128, 32, 4, cutlass::half_t>>, Is_dropout=true, Is_causal=false]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(118): here
            instantiation of "void run_mha_fwd_hdim128<T>(Flash_fwd_params &, cudaStream_t) [with T=cutlass::half_t]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_hdim128_fp16_sm80.cu(31): here

/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_kernel.h(275): warning: variable "tQrQ" was declared but never referenced
          detected during:
            instantiation of "void flash::compute_attn_1rowblock<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &, int, int, int) [with Kernel_traits=Flash_fwd_kernel_traits<128, 128, 32, 4, false, false, cutlass::half_t, Flash_kernel_traits<128, 128, 32, 4, cutlass::half_t>>, Is_dropout=true, Is_causal=false, Is_even_N=false, Is_even_K=false, Return_softmax=false, Params=Flash_fwd_params]" 
(588): here
            instantiation of "void flash::compute_attn<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &) [with Kernel_traits=Flash_fwd_kernel_traits<128, 128, 32, 4, false, false, cutlass::half_t, Flash_kernel_traits<128, 128, 32, 4, cutlass::half_t>>, Is_dropout=true, Is_causal=false, Is_even_N=false, Is_even_K=false, Return_softmax=false, Params=Flash_fwd_params]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(15): here
            instantiation of "void flash_fwd_kernel<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax>(Flash_fwd_params) [with Kernel_traits=Flash_fwd_kernel_traits<128, 128, 32, 4, false, false, cutlass::half_t, Flash_kernel_traits<128, 128, 32, 4, cutlass::half_t>>, Is_dropout=true, Is_causal=false, Is_even_N=false, Is_even_K=false, Return_softmax=false]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(34): here
            instantiation of "void run_flash_fwd<Kernel_traits,Is_dropout,Is_causal>(Flash_fwd_params &, cudaStream_t) [with Kernel_traits=Flash_fwd_kernel_traits<128, 128, 32, 4, false, false, cutlass::half_t, Flash_kernel_traits<128, 128, 32, 4, cutlass::half_t>>, Is_dropout=true, Is_causal=false]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(118): here
            instantiation of "void run_mha_fwd_hdim128<T>(Flash_fwd_params &, cudaStream_t) [with T=cutlass::half_t]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_hdim128_fp16_sm80.cu(31): here

/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_kernel.h(140): warning: variable "MMA_M" was declared but never referenced
          detected during:
            instantiation of "void flash::compute_attn_1rowblock<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &, int, int, int) [with Kernel_traits=Flash_fwd_kernel_traits<128, 64, 64, 4, false, false, cutlass::half_t, Flash_kernel_traits<128, 64, 64, 4, cutlass::half_t>>, Is_dropout=false, Is_causal=true, Is_even_N=true, Is_even_K=true, Return_softmax=false, Params=Flash_fwd_params]" 
(588): here
            instantiation of "void flash::compute_attn<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &) [with Kernel_traits=Flash_fwd_kernel_traits<128, 64, 64, 4, false, false, cutlass::half_t, Flash_kernel_traits<128, 64, 64, 4, cutlass::half_t>>, Is_dropout=false, Is_causal=true, Is_even_N=true, Is_even_K=true, Return_softmax=false, Params=Flash_fwd_params]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(15): here
            instantiation of "void flash_fwd_kernel<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax>(Flash_fwd_params) [with Kernel_traits=Flash_fwd_kernel_traits<128, 64, 64, 4, false, false, cutlass::half_t, Flash_kernel_traits<128, 64, 64, 4, cutlass::half_t>>, Is_dropout=false, Is_causal=true, Is_even_N=true, Is_even_K=true, Return_softmax=false]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(34): here
            instantiation of "void run_flash_fwd<Kernel_traits,Is_dropout,Is_causal>(Flash_fwd_params &, cudaStream_t) [with Kernel_traits=Flash_fwd_kernel_traits<128, 64, 64, 4, false, false, cutlass::half_t, Flash_kernel_traits<128, 64, 64, 4, cutlass::half_t>>, Is_dropout=false, Is_causal=true]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(118): here
            instantiation of "void run_mha_fwd_hdim128<T>(Flash_fwd_params &, cudaStream_t) [with T=cutlass::half_t]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_hdim128_fp16_sm80.cu(31): here

/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_kernel.h(275): warning: variable "tQrQ" was declared but never referenced
          detected during:
            instantiation of "void flash::compute_attn_1rowblock<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &, int, int, int) [with Kernel_traits=Flash_fwd_kernel_traits<128, 64, 64, 4, false, false, cutlass::half_t, Flash_kernel_traits<128, 64, 64, 4, cutlass::half_t>>, Is_dropout=false, Is_causal=true, Is_even_N=true, Is_even_K=true, Return_softmax=false, Params=Flash_fwd_params]" 
(588): here
            instantiation of "void flash::compute_attn<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &) [with Kernel_traits=Flash_fwd_kernel_traits<128, 64, 64, 4, false, false, cutlass::half_t, Flash_kernel_traits<128, 64, 64, 4, cutlass::half_t>>, Is_dropout=false, Is_causal=true, Is_even_N=true, Is_even_K=true, Return_softmax=false, Params=Flash_fwd_params]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(15): here
            instantiation of "void flash_fwd_kernel<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax>(Flash_fwd_params) [with Kernel_traits=Flash_fwd_kernel_traits<128, 64, 64, 4, false, false, cutlass::half_t, Flash_kernel_traits<128, 64, 64, 4, cutlass::half_t>>, Is_dropout=false, Is_causal=true, Is_even_N=true, Is_even_K=true, Return_softmax=false]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(34): here
            instantiation of "void run_flash_fwd<Kernel_traits,Is_dropout,Is_causal>(Flash_fwd_params &, cudaStream_t) [with Kernel_traits=Flash_fwd_kernel_traits<128, 64, 64, 4, false, false, cutlass::half_t, Flash_kernel_traits<128, 64, 64, 4, cutlass::half_t>>, Is_dropout=false, Is_causal=true]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(118): here
            instantiation of "void run_mha_fwd_hdim128<T>(Flash_fwd_params &, cudaStream_t) [with T=cutlass::half_t]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_hdim128_fp16_sm80.cu(31): here

/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_kernel.h(140): warning: variable "MMA_M" was declared but never referenced
          detected during:
            instantiation of "void flash::compute_attn_1rowblock<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &, int, int, int) [with Kernel_traits=Flash_fwd_kernel_traits<128, 64, 64, 4, false, false, cutlass::half_t, Flash_kernel_traits<128, 64, 64, 4, cutlass::half_t>>, Is_dropout=false, Is_causal=true, Is_even_N=true, Is_even_K=false, Return_softmax=false, Params=Flash_fwd_params]" 
(588): here
            instantiation of "void flash::compute_attn<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &) [with Kernel_traits=Flash_fwd_kernel_traits<128, 64, 64, 4, false, false, cutlass::half_t, Flash_kernel_traits<128, 64, 64, 4, cutlass::half_t>>, Is_dropout=false, Is_causal=true, Is_even_N=true, Is_even_K=false, Return_softmax=false, Params=Flash_fwd_params]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(15): here
            instantiation of "void flash_fwd_kernel<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax>(Flash_fwd_params) [with Kernel_traits=Flash_fwd_kernel_traits<128, 64, 64, 4, false, false, cutlass::half_t, Flash_kernel_traits<128, 64, 64, 4, cutlass::half_t>>, Is_dropout=false, Is_causal=true, Is_even_N=true, Is_even_K=false, Return_softmax=false]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(34): here
            instantiation of "void run_flash_fwd<Kernel_traits,Is_dropout,Is_causal>(Flash_fwd_params &, cudaStream_t) [with Kernel_traits=Flash_fwd_kernel_traits<128, 64, 64, 4, false, false, cutlass::half_t, Flash_kernel_traits<128, 64, 64, 4, cutlass::half_t>>, Is_dropout=false, Is_causal=true]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(118): here
            instantiation of "void run_mha_fwd_hdim128<T>(Flash_fwd_params &, cudaStream_t) [with T=cutlass::half_t]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_hdim128_fp16_sm80.cu(31): here

/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_kernel.h(275): warning: variable "tQrQ" was declared but never referenced
          detected during:
            instantiation of "void flash::compute_attn_1rowblock<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &, int, int, int) [with Kernel_traits=Flash_fwd_kernel_traits<128, 64, 64, 4, false, false, cutlass::half_t, Flash_kernel_traits<128, 64, 64, 4, cutlass::half_t>>, Is_dropout=false, Is_causal=true, Is_even_N=true, Is_even_K=false, Return_softmax=false, Params=Flash_fwd_params]" 
(588): here
            instantiation of "void flash::compute_attn<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &) [with Kernel_traits=Flash_fwd_kernel_traits<128, 64, 64, 4, false, false, cutlass::half_t, Flash_kernel_traits<128, 64, 64, 4, cutlass::half_t>>, Is_dropout=false, Is_causal=true, Is_even_N=true, Is_even_K=false, Return_softmax=false, Params=Flash_fwd_params]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(15): here
            instantiation of "void flash_fwd_kernel<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax>(Flash_fwd_params) [with Kernel_traits=Flash_fwd_kernel_traits<128, 64, 64, 4, false, false, cutlass::half_t, Flash_kernel_traits<128, 64, 64, 4, cutlass::half_t>>, Is_dropout=false, Is_causal=true, Is_even_N=true, Is_even_K=false, Return_softmax=false]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(34): here
            instantiation of "void run_flash_fwd<Kernel_traits,Is_dropout,Is_causal>(Flash_fwd_params &, cudaStream_t) [with Kernel_traits=Flash_fwd_kernel_traits<128, 64, 64, 4, false, false, cutlass::half_t, Flash_kernel_traits<128, 64, 64, 4, cutlass::half_t>>, Is_dropout=false, Is_causal=true]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(118): here
            instantiation of "void run_mha_fwd_hdim128<T>(Flash_fwd_params &, cudaStream_t) [with T=cutlass::half_t]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_hdim128_fp16_sm80.cu(31): here

/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_kernel.h(140): warning: variable "MMA_M" was declared but never referenced
          detected during:
            instantiation of "void flash::compute_attn_1rowblock<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &, int, int, int) [with Kernel_traits=Flash_fwd_kernel_traits<128, 64, 64, 4, false, false, cutlass::half_t, Flash_kernel_traits<128, 64, 64, 4, cutlass::half_t>>, Is_dropout=false, Is_causal=true, Is_even_N=false, Is_even_K=true, Return_softmax=false, Params=Flash_fwd_params]" 
(588): here
            instantiation of "void flash::compute_attn<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &) [with Kernel_traits=Flash_fwd_kernel_traits<128, 64, 64, 4, false, false, cutlass::half_t, Flash_kernel_traits<128, 64, 64, 4, cutlass::half_t>>, Is_dropout=false, Is_causal=true, Is_even_N=false, Is_even_K=true, Return_softmax=false, Params=Flash_fwd_params]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(15): here
            instantiation of "void flash_fwd_kernel<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax>(Flash_fwd_params) [with Kernel_traits=Flash_fwd_kernel_traits<128, 64, 64, 4, false, false, cutlass::half_t, Flash_kernel_traits<128, 64, 64, 4, cutlass::half_t>>, Is_dropout=false, Is_causal=true, Is_even_N=false, Is_even_K=true, Return_softmax=false]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(34): here
            instantiation of "void run_flash_fwd<Kernel_traits,Is_dropout,Is_causal>(Flash_fwd_params &, cudaStream_t) [with Kernel_traits=Flash_fwd_kernel_traits<128, 64, 64, 4, false, false, cutlass::half_t, Flash_kernel_traits<128, 64, 64, 4, cutlass::half_t>>, Is_dropout=false, Is_causal=true]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(118): here
            instantiation of "void run_mha_fwd_hdim128<T>(Flash_fwd_params &, cudaStream_t) [with T=cutlass::half_t]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_hdim128_fp16_sm80.cu(31): here

/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_kernel.h(275): warning: variable "tQrQ" was declared but never referenced
          detected during:
            instantiation of "void flash::compute_attn_1rowblock<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &, int, int, int) [with Kernel_traits=Flash_fwd_kernel_traits<128, 64, 64, 4, false, false, cutlass::half_t, Flash_kernel_traits<128, 64, 64, 4, cutlass::half_t>>, Is_dropout=false, Is_causal=true, Is_even_N=false, Is_even_K=true, Return_softmax=false, Params=Flash_fwd_params]" 
(588): here
            instantiation of "void flash::compute_attn<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &) [with Kernel_traits=Flash_fwd_kernel_traits<128, 64, 64, 4, false, false, cutlass::half_t, Flash_kernel_traits<128, 64, 64, 4, cutlass::half_t>>, Is_dropout=false, Is_causal=true, Is_even_N=false, Is_even_K=true, Return_softmax=false, Params=Flash_fwd_params]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(15): here
            instantiation of "void flash_fwd_kernel<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax>(Flash_fwd_params) [with Kernel_traits=Flash_fwd_kernel_traits<128, 64, 64, 4, false, false, cutlass::half_t, Flash_kernel_traits<128, 64, 64, 4, cutlass::half_t>>, Is_dropout=false, Is_causal=true, Is_even_N=false, Is_even_K=true, Return_softmax=false]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(34): here
            instantiation of "void run_flash_fwd<Kernel_traits,Is_dropout,Is_causal>(Flash_fwd_params &, cudaStream_t) [with Kernel_traits=Flash_fwd_kernel_traits<128, 64, 64, 4, false, false, cutlass::half_t, Flash_kernel_traits<128, 64, 64, 4, cutlass::half_t>>, Is_dropout=false, Is_causal=true]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(118): here
            instantiation of "void run_mha_fwd_hdim128<T>(Flash_fwd_params &, cudaStream_t) [with T=cutlass::half_t]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_hdim128_fp16_sm80.cu(31): here

/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_kernel.h(140): warning: variable "MMA_M" was declared but never referenced
          detected during:
            instantiation of "void flash::compute_attn_1rowblock<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &, int, int, int) [with Kernel_traits=Flash_fwd_kernel_traits<128, 64, 64, 4, false, false, cutlass::half_t, Flash_kernel_traits<128, 64, 64, 4, cutlass::half_t>>, Is_dropout=false, Is_causal=true, Is_even_N=false, Is_even_K=false, Return_softmax=false, Params=Flash_fwd_params]" 
(588): here
            instantiation of "void flash::compute_attn<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &) [with Kernel_traits=Flash_fwd_kernel_traits<128, 64, 64, 4, false, false, cutlass::half_t, Flash_kernel_traits<128, 64, 64, 4, cutlass::half_t>>, Is_dropout=false, Is_causal=true, Is_even_N=false, Is_even_K=false, Return_softmax=false, Params=Flash_fwd_params]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(15): here
            instantiation of "void flash_fwd_kernel<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax>(Flash_fwd_params) [with Kernel_traits=Flash_fwd_kernel_traits<128, 64, 64, 4, false, false, cutlass::half_t, Flash_kernel_traits<128, 64, 64, 4, cutlass::half_t>>, Is_dropout=false, Is_causal=true, Is_even_N=false, Is_even_K=false, Return_softmax=false]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(34): here
            instantiation of "void run_flash_fwd<Kernel_traits,Is_dropout,Is_causal>(Flash_fwd_params &, cudaStream_t) [with Kernel_traits=Flash_fwd_kernel_traits<128, 64, 64, 4, false, false, cutlass::half_t, Flash_kernel_traits<128, 64, 64, 4, cutlass::half_t>>, Is_dropout=false, Is_causal=true]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(118): here
            instantiation of "void run_mha_fwd_hdim128<T>(Flash_fwd_params &, cudaStream_t) [with T=cutlass::half_t]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_hdim128_fp16_sm80.cu(31): here

/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_kernel.h(275): warning: variable "tQrQ" was declared but never referenced
          detected during:
            instantiation of "void flash::compute_attn_1rowblock<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &, int, int, int) [with Kernel_traits=Flash_fwd_kernel_traits<128, 64, 64, 4, false, false, cutlass::half_t, Flash_kernel_traits<128, 64, 64, 4, cutlass::half_t>>, Is_dropout=false, Is_causal=true, Is_even_N=false, Is_even_K=false, Return_softmax=false, Params=Flash_fwd_params]" 
(588): here
            instantiation of "void flash::compute_attn<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &) [with Kernel_traits=Flash_fwd_kernel_traits<128, 64, 64, 4, false, false, cutlass::half_t, Flash_kernel_traits<128, 64, 64, 4, cutlass::half_t>>, Is_dropout=false, Is_causal=true, Is_even_N=false, Is_even_K=false, Return_softmax=false, Params=Flash_fwd_params]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(15): here
            instantiation of "void flash_fwd_kernel<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax>(Flash_fwd_params) [with Kernel_traits=Flash_fwd_kernel_traits<128, 64, 64, 4, false, false, cutlass::half_t, Flash_kernel_traits<128, 64, 64, 4, cutlass::half_t>>, Is_dropout=false, Is_causal=true, Is_even_N=false, Is_even_K=false, Return_softmax=false]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(34): here
            instantiation of "void run_flash_fwd<Kernel_traits,Is_dropout,Is_causal>(Flash_fwd_params &, cudaStream_t) [with Kernel_traits=Flash_fwd_kernel_traits<128, 64, 64, 4, false, false, cutlass::half_t, Flash_kernel_traits<128, 64, 64, 4, cutlass::half_t>>, Is_dropout=false, Is_causal=true]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(118): here
            instantiation of "void run_mha_fwd_hdim128<T>(Flash_fwd_params &, cudaStream_t) [with T=cutlass::half_t]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_hdim128_fp16_sm80.cu(31): here

/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_kernel.h(140): warning: variable "MMA_M" was declared but never referenced
          detected during:
            instantiation of "void flash::compute_attn_1rowblock<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &, int, int, int) [with Kernel_traits=Flash_fwd_kernel_traits<128, 128, 64, 4, false, false, cutlass::half_t, Flash_kernel_traits<128, 128, 64, 4, cutlass::half_t>>, Is_dropout=false, Is_causal=true, Is_even_N=true, Is_even_K=true, Return_softmax=false, Params=Flash_fwd_params]" 
(588): here
            instantiation of "void flash::compute_attn<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &) [with Kernel_traits=Flash_fwd_kernel_traits<128, 128, 64, 4, false, false, cutlass::half_t, Flash_kernel_traits<128, 128, 64, 4, cutlass::half_t>>, Is_dropout=false, Is_causal=true, Is_even_N=true, Is_even_K=true, Return_softmax=false, Params=Flash_fwd_params]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(15): here
            instantiation of "void flash_fwd_kernel<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax>(Flash_fwd_params) [with Kernel_traits=Flash_fwd_kernel_traits<128, 128, 64, 4, false, false, cutlass::half_t, Flash_kernel_traits<128, 128, 64, 4, cutlass::half_t>>, Is_dropout=false, Is_causal=true, Is_even_N=true, Is_even_K=true, Return_softmax=false]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(34): here
            instantiation of "void run_flash_fwd<Kernel_traits,Is_dropout,Is_causal>(Flash_fwd_params &, cudaStream_t) [with Kernel_traits=Flash_fwd_kernel_traits<128, 128, 64, 4, false, false, cutlass::half_t, Flash_kernel_traits<128, 128, 64, 4, cutlass::half_t>>, Is_dropout=false, Is_causal=true]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(118): here
            instantiation of "void run_mha_fwd_hdim128<T>(Flash_fwd_params &, cudaStream_t) [with T=cutlass::half_t]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_hdim128_fp16_sm80.cu(31): here

/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_kernel.h(275): warning: variable "tQrQ" was declared but never referenced
          detected during:
            instantiation of "void flash::compute_attn_1rowblock<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &, int, int, int) [with Kernel_traits=Flash_fwd_kernel_traits<128, 128, 64, 4, false, false, cutlass::half_t, Flash_kernel_traits<128, 128, 64, 4, cutlass::half_t>>, Is_dropout=false, Is_causal=true, Is_even_N=true, Is_even_K=true, Return_softmax=false, Params=Flash_fwd_params]" 
(588): here
            instantiation of "void flash::compute_attn<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &) [with Kernel_traits=Flash_fwd_kernel_traits<128, 128, 64, 4, false, false, cutlass::half_t, Flash_kernel_traits<128, 128, 64, 4, cutlass::half_t>>, Is_dropout=false, Is_causal=true, Is_even_N=true, Is_even_K=true, Return_softmax=false, Params=Flash_fwd_params]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(15): here
            instantiation of "void flash_fwd_kernel<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax>(Flash_fwd_params) [with Kernel_traits=Flash_fwd_kernel_traits<128, 128, 64, 4, false, false, cutlass::half_t, Flash_kernel_traits<128, 128, 64, 4, cutlass::half_t>>, Is_dropout=false, Is_causal=true, Is_even_N=true, Is_even_K=true, Return_softmax=false]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(34): here
            instantiation of "void run_flash_fwd<Kernel_traits,Is_dropout,Is_causal>(Flash_fwd_params &, cudaStream_t) [with Kernel_traits=Flash_fwd_kernel_traits<128, 128, 64, 4, false, false, cutlass::half_t, Flash_kernel_traits<128, 128, 64, 4, cutlass::half_t>>, Is_dropout=false, Is_causal=true]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(118): here
            instantiation of "void run_mha_fwd_hdim128<T>(Flash_fwd_params &, cudaStream_t) [with T=cutlass::half_t]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_hdim128_fp16_sm80.cu(31): here

/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_kernel.h(140): warning: variable "MMA_M" was declared but never referenced
          detected during:
            instantiation of "void flash::compute_attn_1rowblock<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &, int, int, int) [with Kernel_traits=Flash_fwd_kernel_traits<128, 128, 64, 4, false, false, cutlass::half_t, Flash_kernel_traits<128, 128, 64, 4, cutlass::half_t>>, Is_dropout=false, Is_causal=true, Is_even_N=true, Is_even_K=false, Return_softmax=false, Params=Flash_fwd_params]" 
(588): here
            instantiation of "void flash::compute_attn<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &) [with Kernel_traits=Flash_fwd_kernel_traits<128, 128, 64, 4, false, false, cutlass::half_t, Flash_kernel_traits<128, 128, 64, 4, cutlass::half_t>>, Is_dropout=false, Is_causal=true, Is_even_N=true, Is_even_K=false, Return_softmax=false, Params=Flash_fwd_params]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(15): here
            instantiation of "void flash_fwd_kernel<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax>(Flash_fwd_params) [with Kernel_traits=Flash_fwd_kernel_traits<128, 128, 64, 4, false, false, cutlass::half_t, Flash_kernel_traits<128, 128, 64, 4, cutlass::half_t>>, Is_dropout=false, Is_causal=true, Is_even_N=true, Is_even_K=false, Return_softmax=false]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(34): here
            instantiation of "void run_flash_fwd<Kernel_traits,Is_dropout,Is_causal>(Flash_fwd_params &, cudaStream_t) [with Kernel_traits=Flash_fwd_kernel_traits<128, 128, 64, 4, false, false, cutlass::half_t, Flash_kernel_traits<128, 128, 64, 4, cutlass::half_t>>, Is_dropout=false, Is_causal=true]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(118): here
            instantiation of "void run_mha_fwd_hdim128<T>(Flash_fwd_params &, cudaStream_t) [with T=cutlass::half_t]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_hdim128_fp16_sm80.cu(31): here

/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_kernel.h(275): warning: variable "tQrQ" was declared but never referenced
          detected during:
            instantiation of "void flash::compute_attn_1rowblock<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &, int, int, int) [with Kernel_traits=Flash_fwd_kernel_traits<128, 128, 64, 4, false, false, cutlass::half_t, Flash_kernel_traits<128, 128, 64, 4, cutlass::half_t>>, Is_dropout=false, Is_causal=true, Is_even_N=true, Is_even_K=false, Return_softmax=false, Params=Flash_fwd_params]" 
(588): here
            instantiation of "void flash::compute_attn<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &) [with Kernel_traits=Flash_fwd_kernel_traits<128, 128, 64, 4, false, false, cutlass::half_t, Flash_kernel_traits<128, 128, 64, 4, cutlass::half_t>>, Is_dropout=false, Is_causal=true, Is_even_N=true, Is_even_K=false, Return_softmax=false, Params=Flash_fwd_params]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(15): here
            instantiation of "void flash_fwd_kernel<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax>(Flash_fwd_params) [with Kernel_traits=Flash_fwd_kernel_traits<128, 128, 64, 4, false, false, cutlass::half_t, Flash_kernel_traits<128, 128, 64, 4, cutlass::half_t>>, Is_dropout=false, Is_causal=true, Is_even_N=true, Is_even_K=false, Return_softmax=false]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(34): here
            instantiation of "void run_flash_fwd<Kernel_traits,Is_dropout,Is_causal>(Flash_fwd_params &, cudaStream_t) [with Kernel_traits=Flash_fwd_kernel_traits<128, 128, 64, 4, false, false, cutlass::half_t, Flash_kernel_traits<128, 128, 64, 4, cutlass::half_t>>, Is_dropout=false, Is_causal=true]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(118): here
            instantiation of "void run_mha_fwd_hdim128<T>(Flash_fwd_params &, cudaStream_t) [with T=cutlass::half_t]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_hdim128_fp16_sm80.cu(31): here

/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_kernel.h(140): warning: variable "MMA_M" was declared but never referenced
          detected during:
            instantiation of "void flash::compute_attn_1rowblock<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &, int, int, int) [with Kernel_traits=Flash_fwd_kernel_traits<128, 128, 64, 4, false, false, cutlass::half_t, Flash_kernel_traits<128, 128, 64, 4, cutlass::half_t>>, Is_dropout=false, Is_causal=true, Is_even_N=false, Is_even_K=true, Return_softmax=false, Params=Flash_fwd_params]" 
(588): here
            instantiation of "void flash::compute_attn<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &) [with Kernel_traits=Flash_fwd_kernel_traits<128, 128, 64, 4, false, false, cutlass::half_t, Flash_kernel_traits<128, 128, 64, 4, cutlass::half_t>>, Is_dropout=false, Is_causal=true, Is_even_N=false, Is_even_K=true, Return_softmax=false, Params=Flash_fwd_params]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(15): here
            instantiation of "void flash_fwd_kernel<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax>(Flash_fwd_params) [with Kernel_traits=Flash_fwd_kernel_traits<128, 128, 64, 4, false, false, cutlass::half_t, Flash_kernel_traits<128, 128, 64, 4, cutlass::half_t>>, Is_dropout=false, Is_causal=true, Is_even_N=false, Is_even_K=true, Return_softmax=false]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(34): here
            instantiation of "void run_flash_fwd<Kernel_traits,Is_dropout,Is_causal>(Flash_fwd_params &, cudaStream_t) [with Kernel_traits=Flash_fwd_kernel_traits<128, 128, 64, 4, false, false, cutlass::half_t, Flash_kernel_traits<128, 128, 64, 4, cutlass::half_t>>, Is_dropout=false, Is_causal=true]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(118): here
            instantiation of "void run_mha_fwd_hdim128<T>(Flash_fwd_params &, cudaStream_t) [with T=cutlass::half_t]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_hdim128_fp16_sm80.cu(31): here

/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_kernel.h(275): warning: variable "tQrQ" was declared but never referenced
          detected during:
            instantiation of "void flash::compute_attn_1rowblock<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &, int, int, int) [with Kernel_traits=Flash_fwd_kernel_traits<128, 128, 64, 4, false, false, cutlass::half_t, Flash_kernel_traits<128, 128, 64, 4, cutlass::half_t>>, Is_dropout=false, Is_causal=true, Is_even_N=false, Is_even_K=true, Return_softmax=false, Params=Flash_fwd_params]" 
(588): here
            instantiation of "void flash::compute_attn<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &) [with Kernel_traits=Flash_fwd_kernel_traits<128, 128, 64, 4, false, false, cutlass::half_t, Flash_kernel_traits<128, 128, 64, 4, cutlass::half_t>>, Is_dropout=false, Is_causal=true, Is_even_N=false, Is_even_K=true, Return_softmax=false, Params=Flash_fwd_params]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(15): here
            instantiation of "void flash_fwd_kernel<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax>(Flash_fwd_params) [with Kernel_traits=Flash_fwd_kernel_traits<128, 128, 64, 4, false, false, cutlass::half_t, Flash_kernel_traits<128, 128, 64, 4, cutlass::half_t>>, Is_dropout=false, Is_causal=true, Is_even_N=false, Is_even_K=true, Return_softmax=false]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(34): here
            instantiation of "void run_flash_fwd<Kernel_traits,Is_dropout,Is_causal>(Flash_fwd_params &, cudaStream_t) [with Kernel_traits=Flash_fwd_kernel_traits<128, 128, 64, 4, false, false, cutlass::half_t, Flash_kernel_traits<128, 128, 64, 4, cutlass::half_t>>, Is_dropout=false, Is_causal=true]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(118): here
            instantiation of "void run_mha_fwd_hdim128<T>(Flash_fwd_params &, cudaStream_t) [with T=cutlass::half_t]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_hdim128_fp16_sm80.cu(31): here

/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_kernel.h(140): warning: variable "MMA_M" was declared but never referenced
          detected during:
            instantiation of "void flash::compute_attn_1rowblock<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &, int, int, int) [with Kernel_traits=Flash_fwd_kernel_traits<128, 128, 64, 4, false, false, cutlass::half_t, Flash_kernel_traits<128, 128, 64, 4, cutlass::half_t>>, Is_dropout=false, Is_causal=true, Is_even_N=false, Is_even_K=false, Return_softmax=false, Params=Flash_fwd_params]" 
(588): here
            instantiation of "void flash::compute_attn<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &) [with Kernel_traits=Flash_fwd_kernel_traits<128, 128, 64, 4, false, false, cutlass::half_t, Flash_kernel_traits<128, 128, 64, 4, cutlass::half_t>>, Is_dropout=false, Is_causal=true, Is_even_N=false, Is_even_K=false, Return_softmax=false, Params=Flash_fwd_params]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(15): here
            instantiation of "void flash_fwd_kernel<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax>(Flash_fwd_params) [with Kernel_traits=Flash_fwd_kernel_traits<128, 128, 64, 4, false, false, cutlass::half_t, Flash_kernel_traits<128, 128, 64, 4, cutlass::half_t>>, Is_dropout=false, Is_causal=true, Is_even_N=false, Is_even_K=false, Return_softmax=false]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(34): here
            instantiation of "void run_flash_fwd<Kernel_traits,Is_dropout,Is_causal>(Flash_fwd_params &, cudaStream_t) [with Kernel_traits=Flash_fwd_kernel_traits<128, 128, 64, 4, false, false, cutlass::half_t, Flash_kernel_traits<128, 128, 64, 4, cutlass::half_t>>, Is_dropout=false, Is_causal=true]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(118): here
            instantiation of "void run_mha_fwd_hdim128<T>(Flash_fwd_params &, cudaStream_t) [with T=cutlass::half_t]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_hdim128_fp16_sm80.cu(31): here

/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_kernel.h(275): warning: variable "tQrQ" was declared but never referenced
          detected during:
            instantiation of "void flash::compute_attn_1rowblock<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &, int, int, int) [with Kernel_traits=Flash_fwd_kernel_traits<128, 128, 64, 4, false, false, cutlass::half_t, Flash_kernel_traits<128, 128, 64, 4, cutlass::half_t>>, Is_dropout=false, Is_causal=true, Is_even_N=false, Is_even_K=false, Return_softmax=false, Params=Flash_fwd_params]" 
(588): here
            instantiation of "void flash::compute_attn<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &) [with Kernel_traits=Flash_fwd_kernel_traits<128, 128, 64, 4, false, false, cutlass::half_t, Flash_kernel_traits<128, 128, 64, 4, cutlass::half_t>>, Is_dropout=false, Is_causal=true, Is_even_N=false, Is_even_K=false, Return_softmax=false, Params=Flash_fwd_params]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(15): here
            instantiation of "void flash_fwd_kernel<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax>(Flash_fwd_params) [with Kernel_traits=Flash_fwd_kernel_traits<128, 128, 64, 4, false, false, cutlass::half_t, Flash_kernel_traits<128, 128, 64, 4, cutlass::half_t>>, Is_dropout=false, Is_causal=true, Is_even_N=false, Is_even_K=false, Return_softmax=false]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(34): here
            instantiation of "void run_flash_fwd<Kernel_traits,Is_dropout,Is_causal>(Flash_fwd_params &, cudaStream_t) [with Kernel_traits=Flash_fwd_kernel_traits<128, 128, 64, 4, false, false, cutlass::half_t, Flash_kernel_traits<128, 128, 64, 4, cutlass::half_t>>, Is_dropout=false, Is_causal=true]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(118): here
            instantiation of "void run_mha_fwd_hdim128<T>(Flash_fwd_params &, cudaStream_t) [with T=cutlass::half_t]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_hdim128_fp16_sm80.cu(31): here

/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_kernel.h(140): warning: variable "MMA_M" was declared but never referenced
          detected during:
            instantiation of "void flash::compute_attn_1rowblock<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &, int, int, int) [with Kernel_traits=Flash_fwd_kernel_traits<128, 128, 32, 4, false, false, cutlass::half_t, Flash_kernel_traits<128, 128, 32, 4, cutlass::half_t>>, Is_dropout=false, Is_causal=false, Is_even_N=true, Is_even_K=true, Return_softmax=false, Params=Flash_fwd_params]" 
(588): here
            instantiation of "void flash::compute_attn<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &) [with Kernel_traits=Flash_fwd_kernel_traits<128, 128, 32, 4, false, false, cutlass::half_t, Flash_kernel_traits<128, 128, 32, 4, cutlass::half_t>>, Is_dropout=false, Is_causal=false, Is_even_N=true, Is_even_K=true, Return_softmax=false, Params=Flash_fwd_params]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(15): here
            instantiation of "void flash_fwd_kernel<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax>(Flash_fwd_params) [with Kernel_traits=Flash_fwd_kernel_traits<128, 128, 32, 4, false, false, cutlass::half_t, Flash_kernel_traits<128, 128, 32, 4, cutlass::half_t>>, Is_dropout=false, Is_causal=false, Is_even_N=true, Is_even_K=true, Return_softmax=false]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(34): here
            instantiation of "void run_flash_fwd<Kernel_traits,Is_dropout,Is_causal>(Flash_fwd_params &, cudaStream_t) [with Kernel_traits=Flash_fwd_kernel_traits<128, 128, 32, 4, false, false, cutlass::half_t, Flash_kernel_traits<128, 128, 32, 4, cutlass::half_t>>, Is_dropout=false, Is_causal=false]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(118): here
            instantiation of "void run_mha_fwd_hdim128<T>(Flash_fwd_params &, cudaStream_t) [with T=cutlass::half_t]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_hdim128_fp16_sm80.cu(31): here

/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_kernel.h(275): warning: variable "tQrQ" was declared but never referenced
          detected during:
            instantiation of "void flash::compute_attn_1rowblock<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &, int, int, int) [with Kernel_traits=Flash_fwd_kernel_traits<128, 128, 32, 4, false, false, cutlass::half_t, Flash_kernel_traits<128, 128, 32, 4, cutlass::half_t>>, Is_dropout=false, Is_causal=false, Is_even_N=true, Is_even_K=true, Return_softmax=false, Params=Flash_fwd_params]" 
(588): here
            instantiation of "void flash::compute_attn<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &) [with Kernel_traits=Flash_fwd_kernel_traits<128, 128, 32, 4, false, false, cutlass::half_t, Flash_kernel_traits<128, 128, 32, 4, cutlass::half_t>>, Is_dropout=false, Is_causal=false, Is_even_N=true, Is_even_K=true, Return_softmax=false, Params=Flash_fwd_params]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(15): here
            instantiation of "void flash_fwd_kernel<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax>(Flash_fwd_params) [with Kernel_traits=Flash_fwd_kernel_traits<128, 128, 32, 4, false, false, cutlass::half_t, Flash_kernel_traits<128, 128, 32, 4, cutlass::half_t>>, Is_dropout=false, Is_causal=false, Is_even_N=true, Is_even_K=true, Return_softmax=false]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(34): here
            instantiation of "void run_flash_fwd<Kernel_traits,Is_dropout,Is_causal>(Flash_fwd_params &, cudaStream_t) [with Kernel_traits=Flash_fwd_kernel_traits<128, 128, 32, 4, false, false, cutlass::half_t, Flash_kernel_traits<128, 128, 32, 4, cutlass::half_t>>, Is_dropout=false, Is_causal=false]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(118): here
            instantiation of "void run_mha_fwd_hdim128<T>(Flash_fwd_params &, cudaStream_t) [with T=cutlass::half_t]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_hdim128_fp16_sm80.cu(31): here

/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_kernel.h(140): warning: variable "MMA_M" was declared but never referenced
          detected during:
            instantiation of "void flash::compute_attn_1rowblock<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &, int, int, int) [with Kernel_traits=Flash_fwd_kernel_traits<128, 128, 32, 4, false, false, cutlass::half_t, Flash_kernel_traits<128, 128, 32, 4, cutlass::half_t>>, Is_dropout=false, Is_causal=false, Is_even_N=true, Is_even_K=false, Return_softmax=false, Params=Flash_fwd_params]" 
(588): here
            instantiation of "void flash::compute_attn<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &) [with Kernel_traits=Flash_fwd_kernel_traits<128, 128, 32, 4, false, false, cutlass::half_t, Flash_kernel_traits<128, 128, 32, 4, cutlass::half_t>>, Is_dropout=false, Is_causal=false, Is_even_N=true, Is_even_K=false, Return_softmax=false, Params=Flash_fwd_params]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(15): here
            instantiation of "void flash_fwd_kernel<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax>(Flash_fwd_params) [with Kernel_traits=Flash_fwd_kernel_traits<128, 128, 32, 4, false, false, cutlass::half_t, Flash_kernel_traits<128, 128, 32, 4, cutlass::half_t>>, Is_dropout=false, Is_causal=false, Is_even_N=true, Is_even_K=false, Return_softmax=false]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(34): here
            instantiation of "void run_flash_fwd<Kernel_traits,Is_dropout,Is_causal>(Flash_fwd_params &, cudaStream_t) [with Kernel_traits=Flash_fwd_kernel_traits<128, 128, 32, 4, false, false, cutlass::half_t, Flash_kernel_traits<128, 128, 32, 4, cutlass::half_t>>, Is_dropout=false, Is_causal=false]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(118): here
            instantiation of "void run_mha_fwd_hdim128<T>(Flash_fwd_params &, cudaStream_t) [with T=cutlass::half_t]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_hdim128_fp16_sm80.cu(31): here

/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_kernel.h(275): warning: variable "tQrQ" was declared but never referenced
          detected during:
            instantiation of "void flash::compute_attn_1rowblock<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &, int, int, int) [with Kernel_traits=Flash_fwd_kernel_traits<128, 128, 32, 4, false, false, cutlass::half_t, Flash_kernel_traits<128, 128, 32, 4, cutlass::half_t>>, Is_dropout=false, Is_causal=false, Is_even_N=true, Is_even_K=false, Return_softmax=false, Params=Flash_fwd_params]" 
(588): here
            instantiation of "void flash::compute_attn<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &) [with Kernel_traits=Flash_fwd_kernel_traits<128, 128, 32, 4, false, false, cutlass::half_t, Flash_kernel_traits<128, 128, 32, 4, cutlass::half_t>>, Is_dropout=false, Is_causal=false, Is_even_N=true, Is_even_K=false, Return_softmax=false, Params=Flash_fwd_params]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(15): here
            instantiation of "void flash_fwd_kernel<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax>(Flash_fwd_params) [with Kernel_traits=Flash_fwd_kernel_traits<128, 128, 32, 4, false, false, cutlass::half_t, Flash_kernel_traits<128, 128, 32, 4, cutlass::half_t>>, Is_dropout=false, Is_causal=false, Is_even_N=true, Is_even_K=false, Return_softmax=false]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(34): here
            instantiation of "void run_flash_fwd<Kernel_traits,Is_dropout,Is_causal>(Flash_fwd_params &, cudaStream_t) [with Kernel_traits=Flash_fwd_kernel_traits<128, 128, 32, 4, false, false, cutlass::half_t, Flash_kernel_traits<128, 128, 32, 4, cutlass::half_t>>, Is_dropout=false, Is_causal=false]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(118): here
            instantiation of "void run_mha_fwd_hdim128<T>(Flash_fwd_params &, cudaStream_t) [with T=cutlass::half_t]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_hdim128_fp16_sm80.cu(31): here

/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_kernel.h(140): warning: variable "MMA_M" was declared but never referenced
          detected during:
            instantiation of "void flash::compute_attn_1rowblock<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &, int, int, int) [with Kernel_traits=Flash_fwd_kernel_traits<128, 128, 32, 4, false, false, cutlass::half_t, Flash_kernel_traits<128, 128, 32, 4, cutlass::half_t>>, Is_dropout=false, Is_causal=false, Is_even_N=false, Is_even_K=true, Return_softmax=false, Params=Flash_fwd_params]" 
(588): here
            instantiation of "void flash::compute_attn<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &) [with Kernel_traits=Flash_fwd_kernel_traits<128, 128, 32, 4, false, false, cutlass::half_t, Flash_kernel_traits<128, 128, 32, 4, cutlass::half_t>>, Is_dropout=false, Is_causal=false, Is_even_N=false, Is_even_K=true, Return_softmax=false, Params=Flash_fwd_params]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(15): here
            instantiation of "void flash_fwd_kernel<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax>(Flash_fwd_params) [with Kernel_traits=Flash_fwd_kernel_traits<128, 128, 32, 4, false, false, cutlass::half_t, Flash_kernel_traits<128, 128, 32, 4, cutlass::half_t>>, Is_dropout=false, Is_causal=false, Is_even_N=false, Is_even_K=true, Return_softmax=false]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(34): here
            instantiation of "void run_flash_fwd<Kernel_traits,Is_dropout,Is_causal>(Flash_fwd_params &, cudaStream_t) [with Kernel_traits=Flash_fwd_kernel_traits<128, 128, 32, 4, false, false, cutlass::half_t, Flash_kernel_traits<128, 128, 32, 4, cutlass::half_t>>, Is_dropout=false, Is_causal=false]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(118): here
            instantiation of "void run_mha_fwd_hdim128<T>(Flash_fwd_params &, cudaStream_t) [with T=cutlass::half_t]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_hdim128_fp16_sm80.cu(31): here

/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_kernel.h(275): warning: variable "tQrQ" was declared but never referenced
          detected during:
            instantiation of "void flash::compute_attn_1rowblock<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &, int, int, int) [with Kernel_traits=Flash_fwd_kernel_traits<128, 128, 32, 4, false, false, cutlass::half_t, Flash_kernel_traits<128, 128, 32, 4, cutlass::half_t>>, Is_dropout=false, Is_causal=false, Is_even_N=false, Is_even_K=true, Return_softmax=false, Params=Flash_fwd_params]" 
(588): here
            instantiation of "void flash::compute_attn<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &) [with Kernel_traits=Flash_fwd_kernel_traits<128, 128, 32, 4, false, false, cutlass::half_t, Flash_kernel_traits<128, 128, 32, 4, cutlass::half_t>>, Is_dropout=false, Is_causal=false, Is_even_N=false, Is_even_K=true, Return_softmax=false, Params=Flash_fwd_params]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(15): here
            instantiation of "void flash_fwd_kernel<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax>(Flash_fwd_params) [with Kernel_traits=Flash_fwd_kernel_traits<128, 128, 32, 4, false, false, cutlass::half_t, Flash_kernel_traits<128, 128, 32, 4, cutlass::half_t>>, Is_dropout=false, Is_causal=false, Is_even_N=false, Is_even_K=true, Return_softmax=false]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(34): here
            instantiation of "void run_flash_fwd<Kernel_traits,Is_dropout,Is_causal>(Flash_fwd_params &, cudaStream_t) [with Kernel_traits=Flash_fwd_kernel_traits<128, 128, 32, 4, false, false, cutlass::half_t, Flash_kernel_traits<128, 128, 32, 4, cutlass::half_t>>, Is_dropout=false, Is_causal=false]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(118): here
            instantiation of "void run_mha_fwd_hdim128<T>(Flash_fwd_params &, cudaStream_t) [with T=cutlass::half_t]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_hdim128_fp16_sm80.cu(31): here

/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_kernel.h(140): warning: variable "MMA_M" was declared but never referenced
          detected during:
            instantiation of "void flash::compute_attn_1rowblock<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &, int, int, int) [with Kernel_traits=Flash_fwd_kernel_traits<128, 128, 32, 4, false, false, cutlass::half_t, Flash_kernel_traits<128, 128, 32, 4, cutlass::half_t>>, Is_dropout=false, Is_causal=false, Is_even_N=false, Is_even_K=false, Return_softmax=false, Params=Flash_fwd_params]" 
(588): here
            instantiation of "void flash::compute_attn<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &) [with Kernel_traits=Flash_fwd_kernel_traits<128, 128, 32, 4, false, false, cutlass::half_t, Flash_kernel_traits<128, 128, 32, 4, cutlass::half_t>>, Is_dropout=false, Is_causal=false, Is_even_N=false, Is_even_K=false, Return_softmax=false, Params=Flash_fwd_params]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(15): here
            instantiation of "void flash_fwd_kernel<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax>(Flash_fwd_params) [with Kernel_traits=Flash_fwd_kernel_traits<128, 128, 32, 4, false, false, cutlass::half_t, Flash_kernel_traits<128, 128, 32, 4, cutlass::half_t>>, Is_dropout=false, Is_causal=false, Is_even_N=false, Is_even_K=false, Return_softmax=false]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(34): here
            instantiation of "void run_flash_fwd<Kernel_traits,Is_dropout,Is_causal>(Flash_fwd_params &, cudaStream_t) [with Kernel_traits=Flash_fwd_kernel_traits<128, 128, 32, 4, false, false, cutlass::half_t, Flash_kernel_traits<128, 128, 32, 4, cutlass::half_t>>, Is_dropout=false, Is_causal=false]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(118): here
            instantiation of "void run_mha_fwd_hdim128<T>(Flash_fwd_params &, cudaStream_t) [with T=cutlass::half_t]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_hdim128_fp16_sm80.cu(31): here

/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_kernel.h(275): warning: variable "tQrQ" was declared but never referenced
          detected during:
            instantiation of "void flash::compute_attn_1rowblock<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &, int, int, int) [with Kernel_traits=Flash_fwd_kernel_traits<128, 128, 32, 4, false, false, cutlass::half_t, Flash_kernel_traits<128, 128, 32, 4, cutlass::half_t>>, Is_dropout=false, Is_causal=false, Is_even_N=false, Is_even_K=false, Return_softmax=false, Params=Flash_fwd_params]" 
(588): here
            instantiation of "void flash::compute_attn<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &) [with Kernel_traits=Flash_fwd_kernel_traits<128, 128, 32, 4, false, false, cutlass::half_t, Flash_kernel_traits<128, 128, 32, 4, cutlass::half_t>>, Is_dropout=false, Is_causal=false, Is_even_N=false, Is_even_K=false, Return_softmax=false, Params=Flash_fwd_params]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(15): here
            instantiation of "void flash_fwd_kernel<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax>(Flash_fwd_params) [with Kernel_traits=Flash_fwd_kernel_traits<128, 128, 32, 4, false, false, cutlass::half_t, Flash_kernel_traits<128, 128, 32, 4, cutlass::half_t>>, Is_dropout=false, Is_causal=false, Is_even_N=false, Is_even_K=false, Return_softmax=false]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(34): here
            instantiation of "void run_flash_fwd<Kernel_traits,Is_dropout,Is_causal>(Flash_fwd_params &, cudaStream_t) [with Kernel_traits=Flash_fwd_kernel_traits<128, 128, 32, 4, false, false, cutlass::half_t, Flash_kernel_traits<128, 128, 32, 4, cutlass::half_t>>, Is_dropout=false, Is_causal=false]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(118): here
            instantiation of "void run_mha_fwd_hdim128<T>(Flash_fwd_params &, cudaStream_t) [with T=cutlass::half_t]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_hdim128_fp16_sm80.cu(31): here

/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_kernel.h(140): warning: variable "MMA_M" was declared but never referenced
          detected during:
            instantiation of "void flash::compute_attn_1rowblock<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &, int, int, int) [with Kernel_traits=Flash_fwd_kernel_traits<128, 128, 64, 4, false, false, cutlass::half_t, Flash_kernel_traits<128, 128, 64, 4, cutlass::half_t>>, Is_dropout=false, Is_causal=false, Is_even_N=true, Is_even_K=true, Return_softmax=false, Params=Flash_fwd_params]" 
(588): here
            instantiation of "void flash::compute_attn<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &) [with Kernel_traits=Flash_fwd_kernel_traits<128, 128, 64, 4, false, false, cutlass::half_t, Flash_kernel_traits<128, 128, 64, 4, cutlass::half_t>>, Is_dropout=false, Is_causal=false, Is_even_N=true, Is_even_K=true, Return_softmax=false, Params=Flash_fwd_params]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(15): here
            instantiation of "void flash_fwd_kernel<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax>(Flash_fwd_params) [with Kernel_traits=Flash_fwd_kernel_traits<128, 128, 64, 4, false, false, cutlass::half_t, Flash_kernel_traits<128, 128, 64, 4, cutlass::half_t>>, Is_dropout=false, Is_causal=false, Is_even_N=true, Is_even_K=true, Return_softmax=false]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(34): here
            instantiation of "void run_flash_fwd<Kernel_traits,Is_dropout,Is_causal>(Flash_fwd_params &, cudaStream_t) [with Kernel_traits=Flash_fwd_kernel_traits<128, 128, 64, 4, false, false, cutlass::half_t, Flash_kernel_traits<128, 128, 64, 4, cutlass::half_t>>, Is_dropout=false, Is_causal=false]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(118): here
            instantiation of "void run_mha_fwd_hdim128<T>(Flash_fwd_params &, cudaStream_t) [with T=cutlass::half_t]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_hdim128_fp16_sm80.cu(31): here

/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_kernel.h(275): warning: variable "tQrQ" was declared but never referenced
          detected during:
            instantiation of "void flash::compute_attn_1rowblock<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &, int, int, int) [with Kernel_traits=Flash_fwd_kernel_traits<128, 128, 64, 4, false, false, cutlass::half_t, Flash_kernel_traits<128, 128, 64, 4, cutlass::half_t>>, Is_dropout=false, Is_causal=false, Is_even_N=true, Is_even_K=true, Return_softmax=false, Params=Flash_fwd_params]" 
(588): here
            instantiation of "void flash::compute_attn<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &) [with Kernel_traits=Flash_fwd_kernel_traits<128, 128, 64, 4, false, false, cutlass::half_t, Flash_kernel_traits<128, 128, 64, 4, cutlass::half_t>>, Is_dropout=false, Is_causal=false, Is_even_N=true, Is_even_K=true, Return_softmax=false, Params=Flash_fwd_params]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(15): here
            instantiation of "void flash_fwd_kernel<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax>(Flash_fwd_params) [with Kernel_traits=Flash_fwd_kernel_traits<128, 128, 64, 4, false, false, cutlass::half_t, Flash_kernel_traits<128, 128, 64, 4, cutlass::half_t>>, Is_dropout=false, Is_causal=false, Is_even_N=true, Is_even_K=true, Return_softmax=false]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(34): here
            instantiation of "void run_flash_fwd<Kernel_traits,Is_dropout,Is_causal>(Flash_fwd_params &, cudaStream_t) [with Kernel_traits=Flash_fwd_kernel_traits<128, 128, 64, 4, false, false, cutlass::half_t, Flash_kernel_traits<128, 128, 64, 4, cutlass::half_t>>, Is_dropout=false, Is_causal=false]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(118): here
            instantiation of "void run_mha_fwd_hdim128<T>(Flash_fwd_params &, cudaStream_t) [with T=cutlass::half_t]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_hdim128_fp16_sm80.cu(31): here

/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_kernel.h(140): warning: variable "MMA_M" was declared but never referenced
          detected during:
            instantiation of "void flash::compute_attn_1rowblock<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &, int, int, int) [with Kernel_traits=Flash_fwd_kernel_traits<128, 128, 64, 4, false, false, cutlass::half_t, Flash_kernel_traits<128, 128, 64, 4, cutlass::half_t>>, Is_dropout=false, Is_causal=false, Is_even_N=true, Is_even_K=false, Return_softmax=false, Params=Flash_fwd_params]" 
(588): here
            instantiation of "void flash::compute_attn<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &) [with Kernel_traits=Flash_fwd_kernel_traits<128, 128, 64, 4, false, false, cutlass::half_t, Flash_kernel_traits<128, 128, 64, 4, cutlass::half_t>>, Is_dropout=false, Is_causal=false, Is_even_N=true, Is_even_K=false, Return_softmax=false, Params=Flash_fwd_params]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(15): here
            instantiation of "void flash_fwd_kernel<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax>(Flash_fwd_params) [with Kernel_traits=Flash_fwd_kernel_traits<128, 128, 64, 4, false, false, cutlass::half_t, Flash_kernel_traits<128, 128, 64, 4, cutlass::half_t>>, Is_dropout=false, Is_causal=false, Is_even_N=true, Is_even_K=false, Return_softmax=false]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(34): here
            instantiation of "void run_flash_fwd<Kernel_traits,Is_dropout,Is_causal>(Flash_fwd_params &, cudaStream_t) [with Kernel_traits=Flash_fwd_kernel_traits<128, 128, 64, 4, false, false, cutlass::half_t, Flash_kernel_traits<128, 128, 64, 4, cutlass::half_t>>, Is_dropout=false, Is_causal=false]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(118): here
            instantiation of "void run_mha_fwd_hdim128<T>(Flash_fwd_params &, cudaStream_t) [with T=cutlass::half_t]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_hdim128_fp16_sm80.cu(31): here

/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_kernel.h(275): warning: variable "tQrQ" was declared but never referenced
          detected during:
            instantiation of "void flash::compute_attn_1rowblock<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &, int, int, int) [with Kernel_traits=Flash_fwd_kernel_traits<128, 128, 64, 4, false, false, cutlass::half_t, Flash_kernel_traits<128, 128, 64, 4, cutlass::half_t>>, Is_dropout=false, Is_causal=false, Is_even_N=true, Is_even_K=false, Return_softmax=false, Params=Flash_fwd_params]" 
(588): here
            instantiation of "void flash::compute_attn<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &) [with Kernel_traits=Flash_fwd_kernel_traits<128, 128, 64, 4, false, false, cutlass::half_t, Flash_kernel_traits<128, 128, 64, 4, cutlass::half_t>>, Is_dropout=false, Is_causal=false, Is_even_N=true, Is_even_K=false, Return_softmax=false, Params=Flash_fwd_params]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(15): here
            instantiation of "void flash_fwd_kernel<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax>(Flash_fwd_params) [with Kernel_traits=Flash_fwd_kernel_traits<128, 128, 64, 4, false, false, cutlass::half_t, Flash_kernel_traits<128, 128, 64, 4, cutlass::half_t>>, Is_dropout=false, Is_causal=false, Is_even_N=true, Is_even_K=false, Return_softmax=false]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(34): here
            instantiation of "void run_flash_fwd<Kernel_traits,Is_dropout,Is_causal>(Flash_fwd_params &, cudaStream_t) [with Kernel_traits=Flash_fwd_kernel_traits<128, 128, 64, 4, false, false, cutlass::half_t, Flash_kernel_traits<128, 128, 64, 4, cutlass::half_t>>, Is_dropout=false, Is_causal=false]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(118): here
            instantiation of "void run_mha_fwd_hdim128<T>(Flash_fwd_params &, cudaStream_t) [with T=cutlass::half_t]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_hdim128_fp16_sm80.cu(31): here

/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_kernel.h(140): warning: variable "MMA_M" was declared but never referenced
          detected during:
            instantiation of "void flash::compute_attn_1rowblock<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &, int, int, int) [with Kernel_traits=Flash_fwd_kernel_traits<128, 128, 64, 4, false, false, cutlass::half_t, Flash_kernel_traits<128, 128, 64, 4, cutlass::half_t>>, Is_dropout=false, Is_causal=false, Is_even_N=false, Is_even_K=true, Return_softmax=false, Params=Flash_fwd_params]" 
(588): here
            instantiation of "void flash::compute_attn<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &) [with Kernel_traits=Flash_fwd_kernel_traits<128, 128, 64, 4, false, false, cutlass::half_t, Flash_kernel_traits<128, 128, 64, 4, cutlass::half_t>>, Is_dropout=false, Is_causal=false, Is_even_N=false, Is_even_K=true, Return_softmax=false, Params=Flash_fwd_params]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(15): here
            instantiation of "void flash_fwd_kernel<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax>(Flash_fwd_params) [with Kernel_traits=Flash_fwd_kernel_traits<128, 128, 64, 4, false, false, cutlass::half_t, Flash_kernel_traits<128, 128, 64, 4, cutlass::half_t>>, Is_dropout=false, Is_causal=false, Is_even_N=false, Is_even_K=true, Return_softmax=false]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(34): here
            instantiation of "void run_flash_fwd<Kernel_traits,Is_dropout,Is_causal>(Flash_fwd_params &, cudaStream_t) [with Kernel_traits=Flash_fwd_kernel_traits<128, 128, 64, 4, false, false, cutlass::half_t, Flash_kernel_traits<128, 128, 64, 4, cutlass::half_t>>, Is_dropout=false, Is_causal=false]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(118): here
            instantiation of "void run_mha_fwd_hdim128<T>(Flash_fwd_params &, cudaStream_t) [with T=cutlass::half_t]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_hdim128_fp16_sm80.cu(31): here

/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_kernel.h(275): warning: variable "tQrQ" was declared but never referenced
          detected during:
            instantiation of "void flash::compute_attn_1rowblock<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &, int, int, int) [with Kernel_traits=Flash_fwd_kernel_traits<128, 128, 64, 4, false, false, cutlass::half_t, Flash_kernel_traits<128, 128, 64, 4, cutlass::half_t>>, Is_dropout=false, Is_causal=false, Is_even_N=false, Is_even_K=true, Return_softmax=false, Params=Flash_fwd_params]" 
(588): here
            instantiation of "void flash::compute_attn<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &) [with Kernel_traits=Flash_fwd_kernel_traits<128, 128, 64, 4, false, false, cutlass::half_t, Flash_kernel_traits<128, 128, 64, 4, cutlass::half_t>>, Is_dropout=false, Is_causal=false, Is_even_N=false, Is_even_K=true, Return_softmax=false, Params=Flash_fwd_params]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(15): here
            instantiation of "void flash_fwd_kernel<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax>(Flash_fwd_params) [with Kernel_traits=Flash_fwd_kernel_traits<128, 128, 64, 4, false, false, cutlass::half_t, Flash_kernel_traits<128, 128, 64, 4, cutlass::half_t>>, Is_dropout=false, Is_causal=false, Is_even_N=false, Is_even_K=true, Return_softmax=false]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(34): here
            instantiation of "void run_flash_fwd<Kernel_traits,Is_dropout,Is_causal>(Flash_fwd_params &, cudaStream_t) [with Kernel_traits=Flash_fwd_kernel_traits<128, 128, 64, 4, false, false, cutlass::half_t, Flash_kernel_traits<128, 128, 64, 4, cutlass::half_t>>, Is_dropout=false, Is_causal=false]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(118): here
            instantiation of "void run_mha_fwd_hdim128<T>(Flash_fwd_params &, cudaStream_t) [with T=cutlass::half_t]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_hdim128_fp16_sm80.cu(31): here

/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_kernel.h(140): warning: variable "MMA_M" was declared but never referenced
          detected during:
            instantiation of "void flash::compute_attn_1rowblock<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &, int, int, int) [with Kernel_traits=Flash_fwd_kernel_traits<128, 128, 64, 4, false, false, cutlass::half_t, Flash_kernel_traits<128, 128, 64, 4, cutlass::half_t>>, Is_dropout=false, Is_causal=false, Is_even_N=false, Is_even_K=false, Return_softmax=false, Params=Flash_fwd_params]" 
(588): here
            instantiation of "void flash::compute_attn<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &) [with Kernel_traits=Flash_fwd_kernel_traits<128, 128, 64, 4, false, false, cutlass::half_t, Flash_kernel_traits<128, 128, 64, 4, cutlass::half_t>>, Is_dropout=false, Is_causal=false, Is_even_N=false, Is_even_K=false, Return_softmax=false, Params=Flash_fwd_params]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(15): here
            instantiation of "void flash_fwd_kernel<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax>(Flash_fwd_params) [with Kernel_traits=Flash_fwd_kernel_traits<128, 128, 64, 4, false, false, cutlass::half_t, Flash_kernel_traits<128, 128, 64, 4, cutlass::half_t>>, Is_dropout=false, Is_causal=false, Is_even_N=false, Is_even_K=false, Return_softmax=false]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(34): here
            instantiation of "void run_flash_fwd<Kernel_traits,Is_dropout,Is_causal>(Flash_fwd_params &, cudaStream_t) [with Kernel_traits=Flash_fwd_kernel_traits<128, 128, 64, 4, false, false, cutlass::half_t, Flash_kernel_traits<128, 128, 64, 4, cutlass::half_t>>, Is_dropout=false, Is_causal=false]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(118): here
            instantiation of "void run_mha_fwd_hdim128<T>(Flash_fwd_params &, cudaStream_t) [with T=cutlass::half_t]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_hdim128_fp16_sm80.cu(31): here

/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_kernel.h(275): warning: variable "tQrQ" was declared but never referenced
          detected during:
            instantiation of "void flash::compute_attn_1rowblock<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &, int, int, int) [with Kernel_traits=Flash_fwd_kernel_traits<128, 128, 64, 4, false, false, cutlass::half_t, Flash_kernel_traits<128, 128, 64, 4, cutlass::half_t>>, Is_dropout=false, Is_causal=false, Is_even_N=false, Is_even_K=false, Return_softmax=false, Params=Flash_fwd_params]" 
(588): here
            instantiation of "void flash::compute_attn<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &) [with Kernel_traits=Flash_fwd_kernel_traits<128, 128, 64, 4, false, false, cutlass::half_t, Flash_kernel_traits<128, 128, 64, 4, cutlass::half_t>>, Is_dropout=false, Is_causal=false, Is_even_N=false, Is_even_K=false, Return_softmax=false, Params=Flash_fwd_params]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(15): here
            instantiation of "void flash_fwd_kernel<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax>(Flash_fwd_params) [with Kernel_traits=Flash_fwd_kernel_traits<128, 128, 64, 4, false, false, cutlass::half_t, Flash_kernel_traits<128, 128, 64, 4, cutlass::half_t>>, Is_dropout=false, Is_causal=false, Is_even_N=false, Is_even_K=false, Return_softmax=false]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(34): here
            instantiation of "void run_flash_fwd<Kernel_traits,Is_dropout,Is_causal>(Flash_fwd_params &, cudaStream_t) [with Kernel_traits=Flash_fwd_kernel_traits<128, 128, 64, 4, false, false, cutlass::half_t, Flash_kernel_traits<128, 128, 64, 4, cutlass::half_t>>, Is_dropout=false, Is_causal=false]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(118): here
            instantiation of "void run_mha_fwd_hdim128<T>(Flash_fwd_params &, cudaStream_t) [with T=cutlass::half_t]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_hdim128_fp16_sm80.cu(31): here

ptxas info    : 2 bytes gmem
ptxas info    : Compiling entry function '_Z16flash_fwd_kernelI23Flash_fwd_kernel_traitsILi128ELi128ELi64ELi4ELb0ELb0EN7cutlass6half_tE19Flash_kernel_traitsILi128ELi128ELi64ELi4ES2_EELb0ELb0ELb0ELb0ELb0EEv16Flash_fwd_params' for 'sm_80'
ptxas info    : Function properties for _Z16flash_fwd_kernelI23Flash_fwd_kernel_traitsILi128ELi128ELi64ELi4ELb0ELb0EN7cutlass6half_tE19Flash_kernel_traitsILi128ELi128ELi64ELi4ES2_EELb0ELb0ELb0ELb0ELb0EEv16Flash_fwd_params
    24 bytes stack frame, 32 bytes spill stores, 32 bytes spill loads
ptxas info    : Used 255 registers, 584 bytes cmem[0]
ptxas info    : Compiling entry function '_Z16flash_fwd_kernelI23Flash_fwd_kernel_traitsILi128ELi128ELi64ELi4ELb0ELb0EN7cutlass6half_tE19Flash_kernel_traitsILi128ELi128ELi64ELi4ES2_EELb0ELb0ELb0ELb1ELb0EEv16Flash_fwd_params' for 'sm_80'
ptxas info    : Function properties for _Z16flash_fwd_kernelI23Flash_fwd_kernel_traitsILi128ELi128ELi64ELi4ELb0ELb0EN7cutlass6half_tE19Flash_kernel_traitsILi128ELi128ELi64ELi4ES2_EELb0ELb0ELb0ELb1ELb0EEv16Flash_fwd_params
    16 bytes stack frame, 32 bytes spill stores, 16 bytes spill loads
ptxas info    : Used 255 registers, 584 bytes cmem[0]
ptxas info    : Compiling entry function '_Z16flash_fwd_kernelI23Flash_fwd_kernel_traitsILi128ELi128ELi64ELi4ELb0ELb0EN7cutlass6half_tE19Flash_kernel_traitsILi128ELi128ELi64ELi4ES2_EELb0ELb0ELb1ELb0ELb0EEv16Flash_fwd_params' for 'sm_80'
ptxas info    : Function properties for _Z16flash_fwd_kernelI23Flash_fwd_kernel_traitsILi128ELi128ELi64ELi4ELb0ELb0EN7cutlass6half_tE19Flash_kernel_traitsILi128ELi128ELi64ELi4ES2_EELb0ELb0ELb1ELb0ELb0EEv16Flash_fwd_params
    16 bytes stack frame, 24 bytes spill stores, 24 bytes spill loads
ptxas info    : Used 255 registers, 584 bytes cmem[0]
ptxas info    : Compiling entry function '_Z16flash_fwd_kernelI23Flash_fwd_kernel_traitsILi128ELi128ELi64ELi4ELb0ELb0EN7cutlass6half_tE19Flash_kernel_traitsILi128ELi128ELi64ELi4ES2_EELb0ELb0ELb1ELb1ELb0EEv16Flash_fwd_params' for 'sm_80'
ptxas info    : Function properties for _Z16flash_fwd_kernelI23Flash_fwd_kernel_traitsILi128ELi128ELi64ELi4ELb0ELb0EN7cutlass6half_tE19Flash_kernel_traitsILi128ELi128ELi64ELi4ES2_EELb0ELb0ELb1ELb1ELb0EEv16Flash_fwd_params
    24 bytes stack frame, 36 bytes spill stores, 36 bytes spill loads
ptxas info    : Used 255 registers, 584 bytes cmem[0]
ptxas info    : Compiling entry function '_Z16flash_fwd_kernelI23Flash_fwd_kernel_traitsILi128ELi128ELi32ELi4ELb0ELb0EN7cutlass6half_tE19Flash_kernel_traitsILi128ELi128ELi32ELi4ES2_EELb0ELb0ELb0ELb0ELb0EEv16Flash_fwd_params' for 'sm_80'
ptxas info    : Function properties for _Z16flash_fwd_kernelI23Flash_fwd_kernel_traitsILi128ELi128ELi32ELi4ELb0ELb0EN7cutlass6half_tE19Flash_kernel_traitsILi128ELi128ELi32ELi4ES2_EELb0ELb0ELb0ELb0ELb0EEv16Flash_fwd_params
    0 bytes stack frame, 0 bytes spill stores, 0 bytes spill loads
ptxas info    : Used 243 registers, 584 bytes cmem[0]
ptxas info    : Compiling entry function '_Z16flash_fwd_kernelI23Flash_fwd_kernel_traitsILi128ELi128ELi32ELi4ELb0ELb0EN7cutlass6half_tE19Flash_kernel_traitsILi128ELi128ELi32ELi4ES2_EELb0ELb0ELb0ELb1ELb0EEv16Flash_fwd_params' for 'sm_80'
ptxas info    : Function properties for _Z16flash_fwd_kernelI23Flash_fwd_kernel_traitsILi128ELi128ELi32ELi4ELb0ELb0EN7cutlass6half_tE19Flash_kernel_traitsILi128ELi128ELi32ELi4ES2_EELb0ELb0ELb0ELb1ELb0EEv16Flash_fwd_params
    0 bytes stack frame, 0 bytes spill stores, 0 bytes spill loads
ptxas info    : Used 242 registers, 584 bytes cmem[0]
ptxas info    : Compiling entry function '_Z16flash_fwd_kernelI23Flash_fwd_kernel_traitsILi128ELi128ELi32ELi4ELb0ELb0EN7cutlass6half_tE19Flash_kernel_traitsILi128ELi128ELi32ELi4ES2_EELb0ELb0ELb1ELb0ELb0EEv16Flash_fwd_params' for 'sm_80'
ptxas info    : Function properties for _Z16flash_fwd_kernelI23Flash_fwd_kernel_traitsILi128ELi128ELi32ELi4ELb0ELb0EN7cutlass6half_tE19Flash_kernel_traitsILi128ELi128ELi32ELi4ES2_EELb0ELb0ELb1ELb0ELb0EEv16Flash_fwd_params
    0 bytes stack frame, 0 bytes spill stores, 0 bytes spill loads
ptxas info    : Used 242 registers, 584 bytes cmem[0]
ptxas info    : Compiling entry function '_Z16flash_fwd_kernelI23Flash_fwd_kernel_traitsILi128ELi128ELi32ELi4ELb0ELb0EN7cutlass6half_tE19Flash_kernel_traitsILi128ELi128ELi32ELi4ES2_EELb0ELb0ELb1ELb1ELb0EEv16Flash_fwd_params' for 'sm_80'
ptxas info    : Function properties for _Z16flash_fwd_kernelI23Flash_fwd_kernel_traitsILi128ELi128ELi32ELi4ELb0ELb0EN7cutlass6half_tE19Flash_kernel_traitsILi128ELi128ELi32ELi4ES2_EELb0ELb0ELb1ELb1ELb0EEv16Flash_fwd_params
    0 bytes stack frame, 0 bytes spill stores, 0 bytes spill loads
ptxas info    : Used 242 registers, 584 bytes cmem[0]
ptxas info    : Compiling entry function '_Z16flash_fwd_kernelI23Flash_fwd_kernel_traitsILi128ELi128ELi64ELi4ELb0ELb0EN7cutlass6half_tE19Flash_kernel_traitsILi128ELi128ELi64ELi4ES2_EELb0ELb1ELb0ELb0ELb0EEv16Flash_fwd_params' for 'sm_80'
ptxas info    : Function properties for _Z16flash_fwd_kernelI23Flash_fwd_kernel_traitsILi128ELi128ELi64ELi4ELb0ELb0EN7cutlass6half_tE19Flash_kernel_traitsILi128ELi128ELi64ELi4ES2_EELb0ELb1ELb0ELb0ELb0EEv16Flash_fwd_params
    256 bytes stack frame, 320 bytes spill stores, 356 bytes spill loads
ptxas info    : Used 255 registers, 584 bytes cmem[0]
ptxas info    : Compiling entry function '_Z16flash_fwd_kernelI23Flash_fwd_kernel_traitsILi128ELi128ELi64ELi4ELb0ELb0EN7cutlass6half_tE19Flash_kernel_traitsILi128ELi128ELi64ELi4ES2_EELb0ELb1ELb0ELb1ELb0EEv16Flash_fwd_params' for 'sm_80'
ptxas info    : Function properties for _Z16flash_fwd_kernelI23Flash_fwd_kernel_traitsILi128ELi128ELi64ELi4ELb0ELb0EN7cutlass6half_tE19Flash_kernel_traitsILi128ELi128ELi64ELi4ES2_EELb0ELb1ELb0ELb1ELb0EEv16Flash_fwd_params
    192 bytes stack frame, 292 bytes spill stores, 316 bytes spill loads
ptxas info    : Used 255 registers, 584 bytes cmem[0]
ptxas info    : Compiling entry function '_Z16flash_fwd_kernelI23Flash_fwd_kernel_traitsILi128ELi128ELi64ELi4ELb0ELb0EN7cutlass6half_tE19Flash_kernel_traitsILi128ELi128ELi64ELi4ES2_EELb0ELb1ELb1ELb0ELb0EEv16Flash_fwd_params' for 'sm_80'
ptxas info    : Function properties for _Z16flash_fwd_kernelI23Flash_fwd_kernel_traitsILi128ELi128ELi64ELi4ELb0ELb0EN7cutlass6half_tE19Flash_kernel_traitsILi128ELi128ELi64ELi4ES2_EELb0ELb1ELb1ELb0ELb0EEv16Flash_fwd_params
    128 bytes stack frame, 192 bytes spill stores, 220 bytes spill loads
ptxas info    : Used 255 registers, 584 bytes cmem[0]
ptxas info    : Compiling entry function '_Z16flash_fwd_kernelI23Flash_fwd_kernel_traitsILi128ELi128ELi64ELi4ELb0ELb0EN7cutlass6half_tE19Flash_kernel_traitsILi128ELi128ELi64ELi4ES2_EELb0ELb1ELb1ELb1ELb0EEv16Flash_fwd_params' for 'sm_80'
ptxas info    : Function properties for _Z16flash_fwd_kernelI23Flash_fwd_kernel_traitsILi128ELi128ELi64ELi4ELb0ELb0EN7cutlass6half_tE19Flash_kernel_traitsILi128ELi128ELi64ELi4ES2_EELb0ELb1ELb1ELb1ELb0EEv16Flash_fwd_params
    160 bytes stack frame, 220 bytes spill stores, 180 bytes spill loads
ptxas info    : Used 255 registers, 584 bytes cmem[0]
ptxas info    : Compiling entry function '_Z16flash_fwd_kernelI23Flash_fwd_kernel_traitsILi128ELi64ELi64ELi4ELb0ELb0EN7cutlass6half_tE19Flash_kernel_traitsILi128ELi64ELi64ELi4ES2_EELb0ELb1ELb0ELb0ELb0EEv16Flash_fwd_params' for 'sm_80'
ptxas info    : Function properties for _Z16flash_fwd_kernelI23Flash_fwd_kernel_traitsILi128ELi64ELi64ELi4ELb0ELb0EN7cutlass6half_tE19Flash_kernel_traitsILi128ELi64ELi64ELi4ES2_EELb0ELb1ELb0ELb0ELb0EEv16Flash_fwd_params
    0 bytes stack frame, 0 bytes spill stores, 0 bytes spill loads
ptxas info    : Used 163 registers, 584 bytes cmem[0]
ptxas info    : Compiling entry function '_Z16flash_fwd_kernelI23Flash_fwd_kernel_traitsILi128ELi64ELi64ELi4ELb0ELb0EN7cutlass6half_tE19Flash_kernel_traitsILi128ELi64ELi64ELi4ES2_EELb0ELb1ELb0ELb1ELb0EEv16Flash_fwd_params' for 'sm_80'
ptxas info    : Function properties for _Z16flash_fwd_kernelI23Flash_fwd_kernel_traitsILi128ELi64ELi64ELi4ELb0ELb0EN7cutlass6half_tE19Flash_kernel_traitsILi128ELi64ELi64ELi4ES2_EELb0ELb1ELb0ELb1ELb0EEv16Flash_fwd_params
    0 bytes stack frame, 0 bytes spill stores, 0 bytes spill loads
ptxas info    : Used 160 registers, 584 bytes cmem[0]
ptxas info    : Compiling entry function '_Z16flash_fwd_kernelI23Flash_fwd_kernel_traitsILi128ELi64ELi64ELi4ELb0ELb0EN7cutlass6half_tE19Flash_kernel_traitsILi128ELi64ELi64ELi4ES2_EELb0ELb1ELb1ELb0ELb0EEv16Flash_fwd_params' for 'sm_80'
ptxas info    : Function properties for _Z16flash_fwd_kernelI23Flash_fwd_kernel_traitsILi128ELi64ELi64ELi4ELb0ELb0EN7cutlass6half_tE19Flash_kernel_traitsILi128ELi64ELi64ELi4ES2_EELb0ELb1ELb1ELb0ELb0EEv16Flash_fwd_params
    0 bytes stack frame, 0 bytes spill stores, 0 bytes spill loads
ptxas info    : Used 167 registers, 584 bytes cmem[0]
ptxas info    : Compiling entry function '_Z16flash_fwd_kernelI23Flash_fwd_kernel_traitsILi128ELi64ELi64ELi4ELb0ELb0EN7cutlass6half_tE19Flash_kernel_traitsILi128ELi64ELi64ELi4ES2_EELb0ELb1ELb1ELb1ELb0EEv16Flash_fwd_params' for 'sm_80'
ptxas info    : Function properties for _Z16flash_fwd_kernelI23Flash_fwd_kernel_traitsILi128ELi64ELi64ELi4ELb0ELb0EN7cutlass6half_tE19Flash_kernel_traitsILi128ELi64ELi64ELi4ES2_EELb0ELb1ELb1ELb1ELb0EEv16Flash_fwd_params
    0 bytes stack frame, 0 bytes spill stores, 0 bytes spill loads
ptxas info    : Used 166 registers, 584 bytes cmem[0]
ptxas info    : Compiling entry function '_Z16flash_fwd_kernelI23Flash_fwd_kernel_traitsILi128ELi128ELi32ELi4ELb0ELb0EN7cutlass6half_tE19Flash_kernel_traitsILi128ELi128ELi32ELi4ES2_EELb1ELb0ELb0ELb0ELb0EEv16Flash_fwd_params' for 'sm_80'
ptxas info    : Function properties for _Z16flash_fwd_kernelI23Flash_fwd_kernel_traitsILi128ELi128ELi32ELi4ELb0ELb0EN7cutlass6half_tE19Flash_kernel_traitsILi128ELi128ELi32ELi4ES2_EELb1ELb0ELb0ELb0ELb0EEv16Flash_fwd_params
    0 bytes stack frame, 0 bytes spill stores, 0 bytes spill loads
ptxas info    : Used 252 registers, 584 bytes cmem[0]
ptxas info    : Compiling entry function '_Z16flash_fwd_kernelI23Flash_fwd_kernel_traitsILi128ELi128ELi32ELi4ELb0ELb0EN7cutlass6half_tE19Flash_kernel_traitsILi128ELi128ELi32ELi4ES2_EELb1ELb0ELb0ELb0ELb1EEv16Flash_fwd_params' for 'sm_80'
ptxas info    : Function properties for _Z16flash_fwd_kernelI23Flash_fwd_kernel_traitsILi128ELi128ELi32ELi4ELb0ELb0EN7cutlass6half_tE19Flash_kernel_traitsILi128ELi128ELi32ELi4ES2_EELb1ELb0ELb0ELb0ELb1EEv16Flash_fwd_params
    120 bytes stack frame, 106 bytes spill stores, 106 bytes spill loads
ptxas info    : Used 255 registers, 584 bytes cmem[0]
ptxas info    : Compiling entry function '_Z16flash_fwd_kernelI23Flash_fwd_kernel_traitsILi128ELi128ELi32ELi4ELb0ELb0EN7cutlass6half_tE19Flash_kernel_traitsILi128ELi128ELi32ELi4ES2_EELb1ELb0ELb0ELb1ELb0EEv16Flash_fwd_params' for 'sm_80'
ptxas info    : Function properties for _Z16flash_fwd_kernelI23Flash_fwd_kernel_traitsILi128ELi128ELi32ELi4ELb0ELb0EN7cutlass6half_tE19Flash_kernel_traitsILi128ELi128ELi32ELi4ES2_EELb1ELb0ELb0ELb1ELb0EEv16Flash_fwd_params
    0 bytes stack frame, 0 bytes spill stores, 0 bytes spill loads
ptxas info    : Used 252 registers, 584 bytes cmem[0]
ptxas info    : Compiling entry function '_Z16flash_fwd_kernelI23Flash_fwd_kernel_traitsILi128ELi128ELi32ELi4ELb0ELb0EN7cutlass6half_tE19Flash_kernel_traitsILi128ELi128ELi32ELi4ES2_EELb1ELb0ELb0ELb1ELb1EEv16Flash_fwd_params' for 'sm_80'
ptxas info    : Function properties for _Z16flash_fwd_kernelI23Flash_fwd_kernel_traitsILi128ELi128ELi32ELi4ELb0ELb0EN7cutlass6half_tE19Flash_kernel_traitsILi128ELi128ELi32ELi4ES2_EELb1ELb0ELb0ELb1ELb1EEv16Flash_fwd_params
    152 bytes stack frame, 92 bytes spill stores, 122 bytes spill loads
ptxas info    : Used 255 registers, 584 bytes cmem[0]
ptxas info    : Compiling entry function '_Z16flash_fwd_kernelI23Flash_fwd_kernel_traitsILi128ELi128ELi32ELi4ELb0ELb0EN7cutlass6half_tE19Flash_kernel_traitsILi128ELi128ELi32ELi4ES2_EELb1ELb0ELb1ELb0ELb0EEv16Flash_fwd_params' for 'sm_80'
ptxas info    : Function properties for _Z16flash_fwd_kernelI23Flash_fwd_kernel_traitsILi128ELi128ELi32ELi4ELb0ELb0EN7cutlass6half_tE19Flash_kernel_traitsILi128ELi128ELi32ELi4ES2_EELb1ELb0ELb1ELb0ELb0EEv16Flash_fwd_params
    0 bytes stack frame, 0 bytes spill stores, 0 bytes spill loads
ptxas info    : Used 252 registers, 584 bytes cmem[0]
ptxas info    : Compiling entry function '_Z16flash_fwd_kernelI23Flash_fwd_kernel_traitsILi128ELi128ELi32ELi4ELb0ELb0EN7cutlass6half_tE19Flash_kernel_traitsILi128ELi128ELi32ELi4ES2_EELb1ELb0ELb1ELb0ELb1EEv16Flash_fwd_params' for 'sm_80'
ptxas info    : Function properties for _Z16flash_fwd_kernelI23Flash_fwd_kernel_traitsILi128ELi128ELi32ELi4ELb0ELb0EN7cutlass6half_tE19Flash_kernel_traitsILi128ELi128ELi32ELi4ES2_EELb1ELb0ELb1ELb0ELb1EEv16Flash_fwd_params
    72 bytes stack frame, 74 bytes spill stores, 74 bytes spill loads
ptxas info    : Used 255 registers, 584 bytes cmem[0]
ptxas info    : Compiling entry function '_Z16flash_fwd_kernelI23Flash_fwd_kernel_traitsILi128ELi128ELi32ELi4ELb0ELb0EN7cutlass6half_tE19Flash_kernel_traitsILi128ELi128ELi32ELi4ES2_EELb1ELb0ELb1ELb1ELb0EEv16Flash_fwd_params' for 'sm_80'
ptxas info    : Function properties for _Z16flash_fwd_kernelI23Flash_fwd_kernel_traitsILi128ELi128ELi32ELi4ELb0ELb0EN7cutlass6half_tE19Flash_kernel_traitsILi128ELi128ELi32ELi4ES2_EELb1ELb0ELb1ELb1ELb0EEv16Flash_fwd_params
    0 bytes stack frame, 0 bytes spill stores, 0 bytes spill loads
ptxas info    : Used 252 registers, 584 bytes cmem[0]
ptxas info    : Compiling entry function '_Z16flash_fwd_kernelI23Flash_fwd_kernel_traitsILi128ELi128ELi32ELi4ELb0ELb0EN7cutlass6half_tE19Flash_kernel_traitsILi128ELi128ELi32ELi4ES2_EELb1ELb0ELb1ELb1ELb1EEv16Flash_fwd_params' for 'sm_80'
ptxas info    : Function properties for _Z16flash_fwd_kernelI23Flash_fwd_kernel_traitsILi128ELi128ELi32ELi4ELb0ELb0EN7cutlass6half_tE19Flash_kernel_traitsILi128ELi128ELi32ELi4ES2_EELb1ELb0ELb1ELb1ELb1EEv16Flash_fwd_params
    80 bytes stack frame, 84 bytes spill stores, 102 bytes spill loads
ptxas info    : Used 255 registers, 584 bytes cmem[0]
ptxas info    : Compiling entry function '_Z16flash_fwd_kernelI23Flash_fwd_kernel_traitsILi128ELi128ELi32ELi4ELb0ELb0EN7cutlass6half_tE19Flash_kernel_traitsILi128ELi128ELi32ELi4ES2_EELb1ELb1ELb0ELb0ELb0EEv16Flash_fwd_params' for 'sm_80'
ptxas info    : Function properties for _Z16flash_fwd_kernelI23Flash_fwd_kernel_traitsILi128ELi128ELi32ELi4ELb0ELb0EN7cutlass6half_tE19Flash_kernel_traitsILi128ELi128ELi32ELi4ES2_EELb1ELb1ELb0ELb0ELb0EEv16Flash_fwd_params
    16 bytes stack frame, 16 bytes spill stores, 24 bytes spill loads
ptxas info    : Used 255 registers, 584 bytes cmem[0]
ptxas info    : Compiling entry function '_Z16flash_fwd_kernelI23Flash_fwd_kernel_traitsILi128ELi128ELi32ELi4ELb0ELb0EN7cutlass6half_tE19Flash_kernel_traitsILi128ELi128ELi32ELi4ES2_EELb1ELb1ELb0ELb0ELb1EEv16Flash_fwd_params' for 'sm_80'
ptxas info    : Function properties for _Z16flash_fwd_kernelI23Flash_fwd_kernel_traitsILi128ELi128ELi32ELi4ELb0ELb0EN7cutlass6half_tE19Flash_kernel_traitsILi128ELi128ELi32ELi4ES2_EELb1ELb1ELb0ELb0ELb1EEv16Flash_fwd_params
    152 bytes stack frame, 190 bytes spill stores, 246 bytes spill loads
ptxas info    : Used 255 registers, 584 bytes cmem[0]
ptxas info    : Compiling entry function '_Z16flash_fwd_kernelI23Flash_fwd_kernel_traitsILi128ELi128ELi32ELi4ELb0ELb0EN7cutlass6half_tE19Flash_kernel_traitsILi128ELi128ELi32ELi4ES2_EELb1ELb1ELb0ELb1ELb0EEv16Flash_fwd_params' for 'sm_80'
ptxas info    : Function properties for _Z16flash_fwd_kernelI23Flash_fwd_kernel_traitsILi128ELi128ELi32ELi4ELb0ELb0EN7cutlass6half_tE19Flash_kernel_traitsILi128ELi128ELi32ELi4ES2_EELb1ELb1ELb0ELb1ELb0EEv16Flash_fwd_params
    16 bytes stack frame, 12 bytes spill stores, 28 bytes spill loads
ptxas info    : Used 255 registers, 584 bytes cmem[0]
ptxas info    : Compiling entry function '_Z16flash_fwd_kernelI23Flash_fwd_kernel_traitsILi128ELi128ELi32ELi4ELb0ELb0EN7cutlass6half_tE19Flash_kernel_traitsILi128ELi128ELi32ELi4ES2_EELb1ELb1ELb0ELb1ELb1EEv16Flash_fwd_params' for 'sm_80'
ptxas info    : Function properties for _Z16flash_fwd_kernelI23Flash_fwd_kernel_traitsILi128ELi128ELi32ELi4ELb0ELb0EN7cutlass6half_tE19Flash_kernel_traitsILi128ELi128ELi32ELi4ES2_EELb1ELb1ELb0ELb1ELb1EEv16Flash_fwd_params
    160 bytes stack frame, 180 bytes spill stores, 266 bytes spill loads
ptxas info    : Used 255 registers, 584 bytes cmem[0]
ptxas info    : Compiling entry function '_Z16flash_fwd_kernelI23Flash_fwd_kernel_traitsILi128ELi128ELi32ELi4ELb0ELb0EN7cutlass6half_tE19Flash_kernel_traitsILi128ELi128ELi32ELi4ES2_EELb1ELb1ELb1ELb0ELb0EEv16Flash_fwd_params' for 'sm_80'
ptxas info    : Function properties for _Z16flash_fwd_kernelI23Flash_fwd_kernel_traitsILi128ELi128ELi32ELi4ELb0ELb0EN7cutlass6half_tE19Flash_kernel_traitsILi128ELi128ELi32ELi4ES2_EELb1ELb1ELb1ELb0ELb0EEv16Flash_fwd_params
    16 bytes stack frame, 16 bytes spill stores, 24 bytes spill loads
ptxas info    : Used 255 registers, 584 bytes cmem[0]
ptxas info    : Compiling entry function '_Z16flash_fwd_kernelI23Flash_fwd_kernel_traitsILi128ELi128ELi32ELi4ELb0ELb0EN7cutlass6half_tE19Flash_kernel_traitsILi128ELi128ELi32ELi4ES2_EELb1ELb1ELb1ELb0ELb1EEv16Flash_fwd_params' for 'sm_80'
ptxas info    : Function properties for _Z16flash_fwd_kernelI23Flash_fwd_kernel_traitsILi128ELi128ELi32ELi4ELb0ELb0EN7cutlass6half_tE19Flash_kernel_traitsILi128ELi128ELi32ELi4ES2_EELb1ELb1ELb1ELb0ELb1EEv16Flash_fwd_params
    112 bytes stack frame, 150 bytes spill stores, 174 bytes spill loads
ptxas info    : Used 255 registers, 584 bytes cmem[0]
ptxas info    : Compiling entry function '_Z16flash_fwd_kernelI23Flash_fwd_kernel_traitsILi128ELi128ELi32ELi4ELb0ELb0EN7cutlass6half_tE19Flash_kernel_traitsILi128ELi128ELi32ELi4ES2_EELb1ELb1ELb1ELb1ELb0EEv16Flash_fwd_params' for 'sm_80'
ptxas info    : Function properties for _Z16flash_fwd_kernelI23Flash_fwd_kernel_traitsILi128ELi128ELi32ELi4ELb0ELb0EN7cutlass6half_tE19Flash_kernel_traitsILi128ELi128ELi32ELi4ES2_EELb1ELb1ELb1ELb1ELb0EEv16Flash_fwd_params
    8 bytes stack frame, 4 bytes spill stores, 4 bytes spill loads
ptxas info    : Used 255 registers, 584 bytes cmem[0]
ptxas info    : Compiling entry function '_Z16flash_fwd_kernelI23Flash_fwd_kernel_traitsILi128ELi128ELi32ELi4ELb0ELb0EN7cutlass6half_tE19Flash_kernel_traitsILi128ELi128ELi32ELi4ES2_EELb1ELb1ELb1ELb1ELb1EEv16Flash_fwd_params' for 'sm_80'
ptxas info    : Function properties for _Z16flash_fwd_kernelI23Flash_fwd_kernel_traitsILi128ELi128ELi32ELi4ELb0ELb0EN7cutlass6half_tE19Flash_kernel_traitsILi128ELi128ELi32ELi4ES2_EELb1ELb1ELb1ELb1ELb1EEv16Flash_fwd_params
    144 bytes stack frame, 180 bytes spill stores, 248 bytes spill loads
ptxas info    : Used 255 registers, 584 bytes cmem[0]
/anaconda/envs/flashattention/lib/python3.9/site-packages/torch/include/c10/core/SymInt.h(84): warning: integer conversion resulted in a change of sign

[15/36] /usr/local/cuda-11.4/bin/nvcc  -I/home/v-feiychen/flash-attention/csrc/flash_attn -I/home/v-feiychen/flash-attention/csrc/flash_attn/src -I/home/v-feiychen/flash-attention/csrc/cutlass/include -I/anaconda/envs/flashattention/lib/python3.9/site-packages/torch/include -I/anaconda/envs/flashattention/lib/python3.9/site-packages/torch/include/torch/csrc/api/include -I/anaconda/envs/flashattention/lib/python3.9/site-packages/torch/include/TH -I/anaconda/envs/flashattention/lib/python3.9/site-packages/torch/include/THC -I/usr/local/cuda-11.4/include -I/anaconda/envs/flashattention/include/python3.9 -c -c /home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_bwd_hdim192_fp16_sm80.cu -o /home/v-feiychen/flash-attention/build/temp.linux-x86_64-cpython-39/csrc/flash_attn/src/flash_bwd_hdim192_fp16_sm80.o -D__CUDA_NO_HALF_OPERATORS__ -D__CUDA_NO_HALF_CONVERSIONS__ -D__CUDA_NO_BFLOAT16_CONVERSIONS__ -D__CUDA_NO_HALF2_OPERATORS__ --expt-relaxed-constexpr --compiler-options ''"'"'-fPIC'"'"'' -O3 -std=c++17 -U__CUDA_NO_HALF_OPERATORS__ -U__CUDA_NO_HALF_CONVERSIONS__ -U__CUDA_NO_HALF2_OPERATORS__ -U__CUDA_NO_BFLOAT16_CONVERSIONS__ --expt-relaxed-constexpr --expt-extended-lambda --use_fast_math --ptxas-options=-v -lineinfo -gencode arch=compute_80,code=sm_80 --threads 4 -DTORCH_API_INCLUDE_EXTENSION_H '-DPYBIND11_COMPILER_TYPE="_gcc"' '-DPYBIND11_STDLIB="_libstdcpp"' '-DPYBIND11_BUILD_ABI="_cxxabi1011"' -DTORCH_EXTENSION_NAME=flash_attn_2_cuda -D_GLIBCXX_USE_CXX11_ABI=0
/anaconda/envs/flashattention/lib/python3.9/site-packages/torch/include/c10/core/SymInt.h(84): warning: integer conversion resulted in a change of sign

ptxas info    : 2 bytes gmem
ptxas info    : Compiling entry function '_Z44flash_bwd_dq_dk_dv_loop_seqk_parallel_kernelI23Flash_bwd_kernel_traitsILi192ELi64ELi64ELi8ELi4ELi2ELi2ELb1ELb1EN7cutlass6half_tE19Flash_kernel_traitsILi192ELi64ELi64ELi8ES2_EELb0ELb0ELb0ELb0EEv16Flash_bwd_params' for 'sm_80'
ptxas info    : Function properties for _Z44flash_bwd_dq_dk_dv_loop_seqk_parallel_kernelI23Flash_bwd_kernel_traitsILi192ELi64ELi64ELi8ELi4ELi2ELi2ELb1ELb1EN7cutlass6half_tE19Flash_kernel_traitsILi192ELi64ELi64ELi8ES2_EELb0ELb0ELb0ELb0EEv16Flash_bwd_params
    176 bytes stack frame, 192 bytes spill stores, 192 bytes spill loads
ptxas info    : Used 255 registers, 696 bytes cmem[0]
ptxas info    : Compiling entry function '_Z44flash_bwd_dq_dk_dv_loop_seqk_parallel_kernelI23Flash_bwd_kernel_traitsILi192ELi64ELi64ELi8ELi4ELi2ELi2ELb1ELb1EN7cutlass6half_tE19Flash_kernel_traitsILi192ELi64ELi64ELi8ES2_EELb0ELb0ELb0ELb1EEv16Flash_bwd_params' for 'sm_80'
ptxas info    : Function properties for _Z44flash_bwd_dq_dk_dv_loop_seqk_parallel_kernelI23Flash_bwd_kernel_traitsILi192ELi64ELi64ELi8ELi4ELi2ELi2ELb1ELb1EN7cutlass6half_tE19Flash_kernel_traitsILi192ELi64ELi64ELi8ES2_EELb0ELb0ELb0ELb1EEv16Flash_bwd_params
    176 bytes stack frame, 188 bytes spill stores, 192 bytes spill loads
ptxas info    : Used 255 registers, 696 bytes cmem[0]
ptxas info    : Compiling entry function '_Z44flash_bwd_dq_dk_dv_loop_seqk_parallel_kernelI23Flash_bwd_kernel_traitsILi192ELi64ELi64ELi8ELi4ELi2ELi2ELb1ELb1EN7cutlass6half_tE19Flash_kernel_traitsILi192ELi64ELi64ELi8ES2_EELb0ELb0ELb1ELb0EEv16Flash_bwd_params' for 'sm_80'
ptxas info    : Function properties for _Z44flash_bwd_dq_dk_dv_loop_seqk_parallel_kernelI23Flash_bwd_kernel_traitsILi192ELi64ELi64ELi8ELi4ELi2ELi2ELb1ELb1EN7cutlass6half_tE19Flash_kernel_traitsILi192ELi64ELi64ELi8ES2_EELb0ELb0ELb1ELb0EEv16Flash_bwd_params
    168 bytes stack frame, 176 bytes spill stores, 180 bytes spill loads
ptxas info    : Used 255 registers, 696 bytes cmem[0]
ptxas info    : Compiling entry function '_Z44flash_bwd_dq_dk_dv_loop_seqk_parallel_kernelI23Flash_bwd_kernel_traitsILi192ELi64ELi64ELi8ELi4ELi2ELi2ELb1ELb1EN7cutlass6half_tE19Flash_kernel_traitsILi192ELi64ELi64ELi8ES2_EELb0ELb0ELb1ELb1EEv16Flash_bwd_params' for 'sm_80'
ptxas info    : Function properties for _Z44flash_bwd_dq_dk_dv_loop_seqk_parallel_kernelI23Flash_bwd_kernel_traitsILi192ELi64ELi64ELi8ELi4ELi2ELi2ELb1ELb1EN7cutlass6half_tE19Flash_kernel_traitsILi192ELi64ELi64ELi8ES2_EELb0ELb0ELb1ELb1EEv16Flash_bwd_params
    168 bytes stack frame, 176 bytes spill stores, 180 bytes spill loads
ptxas info    : Used 255 registers, 696 bytes cmem[0]
ptxas info    : Compiling entry function '_Z44flash_bwd_dq_dk_dv_loop_seqk_parallel_kernelI23Flash_bwd_kernel_traitsILi192ELi64ELi64ELi8ELi4ELi2ELi2ELb1ELb1EN7cutlass6half_tE19Flash_kernel_traitsILi192ELi64ELi64ELi8ES2_EELb0ELb1ELb0ELb0EEv16Flash_bwd_params' for 'sm_80'
ptxas info    : Function properties for _Z44flash_bwd_dq_dk_dv_loop_seqk_parallel_kernelI23Flash_bwd_kernel_traitsILi192ELi64ELi64ELi8ELi4ELi2ELi2ELb1ELb1EN7cutlass6half_tE19Flash_kernel_traitsILi192ELi64ELi64ELi8ES2_EELb0ELb1ELb0ELb0EEv16Flash_bwd_params
    192 bytes stack frame, 196 bytes spill stores, 208 bytes spill loads
ptxas info    : Used 255 registers, 696 bytes cmem[0]
ptxas info    : Compiling entry function '_Z44flash_bwd_dq_dk_dv_loop_seqk_parallel_kernelI23Flash_bwd_kernel_traitsILi192ELi64ELi64ELi8ELi4ELi2ELi2ELb1ELb1EN7cutlass6half_tE19Flash_kernel_traitsILi192ELi64ELi64ELi8ES2_EELb0ELb1ELb0ELb1EEv16Flash_bwd_params' for 'sm_80'
ptxas info    : Function properties for _Z44flash_bwd_dq_dk_dv_loop_seqk_parallel_kernelI23Flash_bwd_kernel_traitsILi192ELi64ELi64ELi8ELi4ELi2ELi2ELb1ELb1EN7cutlass6half_tE19Flash_kernel_traitsILi192ELi64ELi64ELi8ES2_EELb0ELb1ELb0ELb1EEv16Flash_bwd_params
    176 bytes stack frame, 192 bytes spill stores, 196 bytes spill loads
ptxas info    : Used 255 registers, 696 bytes cmem[0]
ptxas info    : Compiling entry function '_Z44flash_bwd_dq_dk_dv_loop_seqk_parallel_kernelI23Flash_bwd_kernel_traitsILi192ELi64ELi64ELi8ELi4ELi2ELi2ELb1ELb1EN7cutlass6half_tE19Flash_kernel_traitsILi192ELi64ELi64ELi8ES2_EELb0ELb1ELb1ELb0EEv16Flash_bwd_params' for 'sm_80'
ptxas info    : Function properties for _Z44flash_bwd_dq_dk_dv_loop_seqk_parallel_kernelI23Flash_bwd_kernel_traitsILi192ELi64ELi64ELi8ELi4ELi2ELi2ELb1ELb1EN7cutlass6half_tE19Flash_kernel_traitsILi192ELi64ELi64ELi8ES2_EELb0ELb1ELb1ELb0EEv16Flash_bwd_params
    176 bytes stack frame, 180 bytes spill stores, 192 bytes spill loads
ptxas info    : Used 255 registers, 696 bytes cmem[0]
ptxas info    : Compiling entry function '_Z44flash_bwd_dq_dk_dv_loop_seqk_parallel_kernelI23Flash_bwd_kernel_traitsILi192ELi64ELi64ELi8ELi4ELi2ELi2ELb1ELb1EN7cutlass6half_tE19Flash_kernel_traitsILi192ELi64ELi64ELi8ES2_EELb0ELb1ELb1ELb1EEv16Flash_bwd_params' for 'sm_80'
ptxas info    : Function properties for _Z44flash_bwd_dq_dk_dv_loop_seqk_parallel_kernelI23Flash_bwd_kernel_traitsILi192ELi64ELi64ELi8ELi4ELi2ELi2ELb1ELb1EN7cutlass6half_tE19Flash_kernel_traitsILi192ELi64ELi64ELi8ES2_EELb0ELb1ELb1ELb1EEv16Flash_bwd_params
    176 bytes stack frame, 180 bytes spill stores, 192 bytes spill loads
ptxas info    : Used 255 registers, 696 bytes cmem[0]
ptxas info    : Compiling entry function '_Z44flash_bwd_dq_dk_dv_loop_seqk_parallel_kernelI23Flash_bwd_kernel_traitsILi192ELi64ELi64ELi8ELi4ELi2ELi2ELb0ELb0EN7cutlass6half_tE19Flash_kernel_traitsILi192ELi64ELi64ELi8ES2_EELb0ELb0ELb0ELb0EEv16Flash_bwd_params' for 'sm_80'
ptxas info    : Function properties for _Z44flash_bwd_dq_dk_dv_loop_seqk_parallel_kernelI23Flash_bwd_kernel_traitsILi192ELi64ELi64ELi8ELi4ELi2ELi2ELb0ELb0EN7cutlass6half_tE19Flash_kernel_traitsILi192ELi64ELi64ELi8ES2_EELb0ELb0ELb0ELb0EEv16Flash_bwd_params
    0 bytes stack frame, 0 bytes spill stores, 0 bytes spill loads
ptxas info    : Used 236 registers, 696 bytes cmem[0]
ptxas info    : Compiling entry function '_Z44flash_bwd_dq_dk_dv_loop_seqk_parallel_kernelI23Flash_bwd_kernel_traitsILi192ELi64ELi64ELi8ELi4ELi2ELi2ELb0ELb0EN7cutlass6half_tE19Flash_kernel_traitsILi192ELi64ELi64ELi8ES2_EELb0ELb0ELb0ELb1EEv16Flash_bwd_params' for 'sm_80'
ptxas info    : Function properties for _Z44flash_bwd_dq_dk_dv_loop_seqk_parallel_kernelI23Flash_bwd_kernel_traitsILi192ELi64ELi64ELi8ELi4ELi2ELi2ELb0ELb0EN7cutlass6half_tE19Flash_kernel_traitsILi192ELi64ELi64ELi8ES2_EELb0ELb0ELb0ELb1EEv16Flash_bwd_params
    0 bytes stack frame, 0 bytes spill stores, 0 bytes spill loads
ptxas info    : Used 236 registers, 696 bytes cmem[0]
ptxas info    : Compiling entry function '_Z44flash_bwd_dq_dk_dv_loop_seqk_parallel_kernelI23Flash_bwd_kernel_traitsILi192ELi64ELi64ELi8ELi4ELi2ELi2ELb0ELb0EN7cutlass6half_tE19Flash_kernel_traitsILi192ELi64ELi64ELi8ES2_EELb0ELb0ELb1ELb0EEv16Flash_bwd_params' for 'sm_80'
ptxas info    : Function properties for _Z44flash_bwd_dq_dk_dv_loop_seqk_parallel_kernelI23Flash_bwd_kernel_traitsILi192ELi64ELi64ELi8ELi4ELi2ELi2ELb0ELb0EN7cutlass6half_tE19Flash_kernel_traitsILi192ELi64ELi64ELi8ES2_EELb0ELb0ELb1ELb0EEv16Flash_bwd_params
    0 bytes stack frame, 0 bytes spill stores, 0 bytes spill loads
ptxas info    : Used 231 registers, 696 bytes cmem[0]
ptxas info    : Compiling entry function '_Z44flash_bwd_dq_dk_dv_loop_seqk_parallel_kernelI23Flash_bwd_kernel_traitsILi192ELi64ELi64ELi8ELi4ELi2ELi2ELb0ELb0EN7cutlass6half_tE19Flash_kernel_traitsILi192ELi64ELi64ELi8ES2_EELb0ELb0ELb1ELb1EEv16Flash_bwd_params' for 'sm_80'
ptxas info    : Function properties for _Z44flash_bwd_dq_dk_dv_loop_seqk_parallel_kernelI23Flash_bwd_kernel_traitsILi192ELi64ELi64ELi8ELi4ELi2ELi2ELb0ELb0EN7cutlass6half_tE19Flash_kernel_traitsILi192ELi64ELi64ELi8ES2_EELb0ELb0ELb1ELb1EEv16Flash_bwd_params
    0 bytes stack frame, 0 bytes spill stores, 0 bytes spill loads
ptxas info    : Used 235 registers, 696 bytes cmem[0]
ptxas info    : Compiling entry function '_Z44flash_bwd_dq_dk_dv_loop_seqk_parallel_kernelI23Flash_bwd_kernel_traitsILi192ELi64ELi64ELi8ELi4ELi2ELi2ELb0ELb0EN7cutlass6half_tE19Flash_kernel_traitsILi192ELi64ELi64ELi8ES2_EELb0ELb1ELb0ELb0EEv16Flash_bwd_params' for 'sm_80'
ptxas info    : Function properties for _Z44flash_bwd_dq_dk_dv_loop_seqk_parallel_kernelI23Flash_bwd_kernel_traitsILi192ELi64ELi64ELi8ELi4ELi2ELi2ELb0ELb0EN7cutlass6half_tE19Flash_kernel_traitsILi192ELi64ELi64ELi8ES2_EELb0ELb1ELb0ELb0EEv16Flash_bwd_params
    0 bytes stack frame, 0 bytes spill stores, 0 bytes spill loads
ptxas info    : Used 236 registers, 696 bytes cmem[0]
ptxas info    : Compiling entry function '_Z44flash_bwd_dq_dk_dv_loop_seqk_parallel_kernelI23Flash_bwd_kernel_traitsILi192ELi64ELi64ELi8ELi4ELi2ELi2ELb0ELb0EN7cutlass6half_tE19Flash_kernel_traitsILi192ELi64ELi64ELi8ES2_EELb0ELb1ELb0ELb1EEv16Flash_bwd_params' for 'sm_80'
ptxas info    : Function properties for _Z44flash_bwd_dq_dk_dv_loop_seqk_parallel_kernelI23Flash_bwd_kernel_traitsILi192ELi64ELi64ELi8ELi4ELi2ELi2ELb0ELb0EN7cutlass6half_tE19Flash_kernel_traitsILi192ELi64ELi64ELi8ES2_EELb0ELb1ELb0ELb1EEv16Flash_bwd_params
    0 bytes stack frame, 0 bytes spill stores, 0 bytes spill loads
ptxas info    : Used 236 registers, 696 bytes cmem[0]
ptxas info    : Compiling entry function '_Z44flash_bwd_dq_dk_dv_loop_seqk_parallel_kernelI23Flash_bwd_kernel_traitsILi192ELi64ELi64ELi8ELi4ELi2ELi2ELb0ELb0EN7cutlass6half_tE19Flash_kernel_traitsILi192ELi64ELi64ELi8ES2_EELb0ELb1ELb1ELb0EEv16Flash_bwd_params' for 'sm_80'
ptxas info    : Function properties for _Z44flash_bwd_dq_dk_dv_loop_seqk_parallel_kernelI23Flash_bwd_kernel_traitsILi192ELi64ELi64ELi8ELi4ELi2ELi2ELb0ELb0EN7cutlass6half_tE19Flash_kernel_traitsILi192ELi64ELi64ELi8ES2_EELb0ELb1ELb1ELb0EEv16Flash_bwd_params
    0 bytes stack frame, 0 bytes spill stores, 0 bytes spill loads
ptxas info    : Used 234 registers, 696 bytes cmem[0]
ptxas info    : Compiling entry function '_Z44flash_bwd_dq_dk_dv_loop_seqk_parallel_kernelI23Flash_bwd_kernel_traitsILi192ELi64ELi64ELi8ELi4ELi2ELi2ELb0ELb0EN7cutlass6half_tE19Flash_kernel_traitsILi192ELi64ELi64ELi8ES2_EELb0ELb1ELb1ELb1EEv16Flash_bwd_params' for 'sm_80'
ptxas info    : Function properties for _Z44flash_bwd_dq_dk_dv_loop_seqk_parallel_kernelI23Flash_bwd_kernel_traitsILi192ELi64ELi64ELi8ELi4ELi2ELi2ELb0ELb0EN7cutlass6half_tE19Flash_kernel_traitsILi192ELi64ELi64ELi8ES2_EELb0ELb1ELb1ELb1EEv16Flash_bwd_params
    0 bytes stack frame, 0 bytes spill stores, 0 bytes spill loads
ptxas info    : Used 238 registers, 696 bytes cmem[0]
ptxas info    : Compiling entry function '_Z27flash_bwd_convert_dq_kernelI23Flash_bwd_kernel_traitsILi192ELi64ELi64ELi8ELi4ELi2ELi2ELb1ELb1EN7cutlass6half_tE19Flash_kernel_traitsILi192ELi64ELi64ELi8ES2_EEEv16Flash_bwd_params' for 'sm_80'
ptxas info    : Function properties for _Z27flash_bwd_convert_dq_kernelI23Flash_bwd_kernel_traitsILi192ELi64ELi64ELi8ELi4ELi2ELi2ELb1ELb1EN7cutlass6half_tE19Flash_kernel_traitsILi192ELi64ELi64ELi8ES2_EEEv16Flash_bwd_params
    0 bytes stack frame, 0 bytes spill stores, 0 bytes spill loads
ptxas info    : Used 32 registers, 696 bytes cmem[0]
ptxas info    : Compiling entry function '_Z44flash_bwd_dq_dk_dv_loop_seqk_parallel_kernelI23Flash_bwd_kernel_traitsILi192ELi64ELi64ELi8ELi4ELi2ELi2ELb1ELb1EN7cutlass6half_tE19Flash_kernel_traitsILi192ELi64ELi64ELi8ES2_EELb1ELb0ELb0ELb0EEv16Flash_bwd_params' for 'sm_80'
ptxas info    : Function properties for _Z44flash_bwd_dq_dk_dv_loop_seqk_parallel_kernelI23Flash_bwd_kernel_traitsILi192ELi64ELi64ELi8ELi4ELi2ELi2ELb1ELb1EN7cutlass6half_tE19Flash_kernel_traitsILi192ELi64ELi64ELi8ES2_EELb1ELb0ELb0ELb0EEv16Flash_bwd_params
    192 bytes stack frame, 200 bytes spill stores, 208 bytes spill loads
ptxas info    : Used 255 registers, 696 bytes cmem[0]
ptxas info    : Compiling entry function '_Z44flash_bwd_dq_dk_dv_loop_seqk_parallel_kernelI23Flash_bwd_kernel_traitsILi192ELi64ELi64ELi8ELi4ELi2ELi2ELb1ELb1EN7cutlass6half_tE19Flash_kernel_traitsILi192ELi64ELi64ELi8ES2_EELb1ELb0ELb0ELb1EEv16Flash_bwd_params' for 'sm_80'
ptxas info    : Function properties for _Z44flash_bwd_dq_dk_dv_loop_seqk_parallel_kernelI23Flash_bwd_kernel_traitsILi192ELi64ELi64ELi8ELi4ELi2ELi2ELb1ELb1EN7cutlass6half_tE19Flash_kernel_traitsILi192ELi64ELi64ELi8ES2_EELb1ELb0ELb0ELb1EEv16Flash_bwd_params
    192 bytes stack frame, 196 bytes spill stores, 204 bytes spill loads
ptxas info    : Used 255 registers, 696 bytes cmem[0]
ptxas info    : Compiling entry function '_Z44flash_bwd_dq_dk_dv_loop_seqk_parallel_kernelI23Flash_bwd_kernel_traitsILi192ELi64ELi64ELi8ELi4ELi2ELi2ELb1ELb1EN7cutlass6half_tE19Flash_kernel_traitsILi192ELi64ELi64ELi8ES2_EELb1ELb0ELb1ELb0EEv16Flash_bwd_params' for 'sm_80'
ptxas info    : Function properties for _Z44flash_bwd_dq_dk_dv_loop_seqk_parallel_kernelI23Flash_bwd_kernel_traitsILi192ELi64ELi64ELi8ELi4ELi2ELi2ELb1ELb1EN7cutlass6half_tE19Flash_kernel_traitsILi192ELi64ELi64ELi8ES2_EELb1ELb0ELb1ELb0EEv16Flash_bwd_params
    192 bytes stack frame, 196 bytes spill stores, 204 bytes spill loads
ptxas info    : Used 255 registers, 696 bytes cmem[0]
ptxas info    : Compiling entry function '_Z44flash_bwd_dq_dk_dv_loop_seqk_parallel_kernelI23Flash_bwd_kernel_traitsILi192ELi64ELi64ELi8ELi4ELi2ELi2ELb1ELb1EN7cutlass6half_tE19Flash_kernel_traitsILi192ELi64ELi64ELi8ES2_EELb1ELb0ELb1ELb1EEv16Flash_bwd_params' for 'sm_80'
ptxas info    : Function properties for _Z44flash_bwd_dq_dk_dv_loop_seqk_parallel_kernelI23Flash_bwd_kernel_traitsILi192ELi64ELi64ELi8ELi4ELi2ELi2ELb1ELb1EN7cutlass6half_tE19Flash_kernel_traitsILi192ELi64ELi64ELi8ES2_EELb1ELb0ELb1ELb1EEv16Flash_bwd_params
    192 bytes stack frame, 196 bytes spill stores, 196 bytes spill loads
ptxas info    : Used 255 registers, 696 bytes cmem[0]
ptxas info    : Compiling entry function '_Z44flash_bwd_dq_dk_dv_loop_seqk_parallel_kernelI23Flash_bwd_kernel_traitsILi192ELi64ELi64ELi8ELi4ELi2ELi2ELb1ELb1EN7cutlass6half_tE19Flash_kernel_traitsILi192ELi64ELi64ELi8ES2_EELb1ELb1ELb0ELb0EEv16Flash_bwd_params' for 'sm_80'
ptxas info    : Function properties for _Z44flash_bwd_dq_dk_dv_loop_seqk_parallel_kernelI23Flash_bwd_kernel_traitsILi192ELi64ELi64ELi8ELi4ELi2ELi2ELb1ELb1EN7cutlass6half_tE19Flash_kernel_traitsILi192ELi64ELi64ELi8ES2_EELb1ELb1ELb0ELb0EEv16Flash_bwd_params
    208 bytes stack frame, 212 bytes spill stores, 220 bytes spill loads
ptxas info    : Used 255 registers, 696 bytes cmem[0]
ptxas info    : Compiling entry function '_Z44flash_bwd_dq_dk_dv_loop_seqk_parallel_kernelI23Flash_bwd_kernel_traitsILi192ELi64ELi64ELi8ELi4ELi2ELi2ELb1ELb1EN7cutlass6half_tE19Flash_kernel_traitsILi192ELi64ELi64ELi8ES2_EELb1ELb1ELb0ELb1EEv16Flash_bwd_params' for 'sm_80'
ptxas info    : Function properties for _Z44flash_bwd_dq_dk_dv_loop_seqk_parallel_kernelI23Flash_bwd_kernel_traitsILi192ELi64ELi64ELi8ELi4ELi2ELi2ELb1ELb1EN7cutlass6half_tE19Flash_kernel_traitsILi192ELi64ELi64ELi8ES2_EELb1ELb1ELb0ELb1EEv16Flash_bwd_params
    192 bytes stack frame, 200 bytes spill stores, 208 bytes spill loads
ptxas info    : Used 255 registers, 696 bytes cmem[0]
ptxas info    : Compiling entry function '_Z44flash_bwd_dq_dk_dv_loop_seqk_parallel_kernelI23Flash_bwd_kernel_traitsILi192ELi64ELi64ELi8ELi4ELi2ELi2ELb1ELb1EN7cutlass6half_tE19Flash_kernel_traitsILi192ELi64ELi64ELi8ES2_EELb1ELb1ELb1ELb0EEv16Flash_bwd_params' for 'sm_80'
ptxas info    : Function properties for _Z44flash_bwd_dq_dk_dv_loop_seqk_parallel_kernelI23Flash_bwd_kernel_traitsILi192ELi64ELi64ELi8ELi4ELi2ELi2ELb1ELb1EN7cutlass6half_tE19Flash_kernel_traitsILi192ELi64ELi64ELi8ES2_EELb1ELb1ELb1ELb0EEv16Flash_bwd_params
    208 bytes stack frame, 212 bytes spill stores, 220 bytes spill loads
ptxas info    : Used 255 registers, 696 bytes cmem[0]
ptxas info    : Compiling entry function '_Z44flash_bwd_dq_dk_dv_loop_seqk_parallel_kernelI23Flash_bwd_kernel_traitsILi192ELi64ELi64ELi8ELi4ELi2ELi2ELb1ELb1EN7cutlass6half_tE19Flash_kernel_traitsILi192ELi64ELi64ELi8ES2_EELb1ELb1ELb1ELb1EEv16Flash_bwd_params' for 'sm_80'
ptxas info    : Function properties for _Z44flash_bwd_dq_dk_dv_loop_seqk_parallel_kernelI23Flash_bwd_kernel_traitsILi192ELi64ELi64ELi8ELi4ELi2ELi2ELb1ELb1EN7cutlass6half_tE19Flash_kernel_traitsILi192ELi64ELi64ELi8ES2_EELb1ELb1ELb1ELb1EEv16Flash_bwd_params
    192 bytes stack frame, 196 bytes spill stores, 204 bytes spill loads
ptxas info    : Used 255 registers, 696 bytes cmem[0]
ptxas info    : Compiling entry function '_Z25flash_bwd_dot_do_o_kernelILb1E23Flash_bwd_kernel_traitsILi192ELi64ELi64ELi8ELi4ELi2ELi2ELb1ELb1EN7cutlass6half_tE19Flash_kernel_traitsILi192ELi64ELi64ELi8ES2_EEEv16Flash_bwd_params' for 'sm_80'
ptxas info    : Function properties for _Z25flash_bwd_dot_do_o_kernelILb1E23Flash_bwd_kernel_traitsILi192ELi64ELi64ELi8ELi4ELi2ELi2ELb1ELb1EN7cutlass6half_tE19Flash_kernel_traitsILi192ELi64ELi64ELi8ES2_EEEv16Flash_bwd_params
    0 bytes stack frame, 0 bytes spill stores, 0 bytes spill loads
ptxas info    : Used 69 registers, 696 bytes cmem[0]
ptxas info    : Compiling entry function '_Z27flash_bwd_convert_dq_kernelI23Flash_bwd_kernel_traitsILi192ELi64ELi64ELi8ELi4ELi2ELi2ELb0ELb0EN7cutlass6half_tE19Flash_kernel_traitsILi192ELi64ELi64ELi8ES2_EEEv16Flash_bwd_params' for 'sm_80'
ptxas info    : Function properties for _Z27flash_bwd_convert_dq_kernelI23Flash_bwd_kernel_traitsILi192ELi64ELi64ELi8ELi4ELi2ELi2ELb0ELb0EN7cutlass6half_tE19Flash_kernel_traitsILi192ELi64ELi64ELi8ES2_EEEv16Flash_bwd_params
    0 bytes stack frame, 0 bytes spill stores, 0 bytes spill loads
ptxas info    : Used 32 registers, 696 bytes cmem[0]
ptxas info    : Compiling entry function '_Z44flash_bwd_dq_dk_dv_loop_seqk_parallel_kernelI23Flash_bwd_kernel_traitsILi192ELi64ELi64ELi8ELi4ELi2ELi2ELb0ELb0EN7cutlass6half_tE19Flash_kernel_traitsILi192ELi64ELi64ELi8ES2_EELb1ELb0ELb0ELb0EEv16Flash_bwd_params' for 'sm_80'
ptxas info    : Function properties for _Z44flash_bwd_dq_dk_dv_loop_seqk_parallel_kernelI23Flash_bwd_kernel_traitsILi192ELi64ELi64ELi8ELi4ELi2ELi2ELb0ELb0EN7cutlass6half_tE19Flash_kernel_traitsILi192ELi64ELi64ELi8ES2_EELb1ELb0ELb0ELb0EEv16Flash_bwd_params
    0 bytes stack frame, 0 bytes spill stores, 0 bytes spill loads
ptxas info    : Used 237 registers, 696 bytes cmem[0]
ptxas info    : Compiling entry function '_Z44flash_bwd_dq_dk_dv_loop_seqk_parallel_kernelI23Flash_bwd_kernel_traitsILi192ELi64ELi64ELi8ELi4ELi2ELi2ELb0ELb0EN7cutlass6half_tE19Flash_kernel_traitsILi192ELi64ELi64ELi8ES2_EELb1ELb0ELb0ELb1EEv16Flash_bwd_params' for 'sm_80'
ptxas info    : Function properties for _Z44flash_bwd_dq_dk_dv_loop_seqk_parallel_kernelI23Flash_bwd_kernel_traitsILi192ELi64ELi64ELi8ELi4ELi2ELi2ELb0ELb0EN7cutlass6half_tE19Flash_kernel_traitsILi192ELi64ELi64ELi8ES2_EELb1ELb0ELb0ELb1EEv16Flash_bwd_params
    0 bytes stack frame, 0 bytes spill stores, 0 bytes spill loads
ptxas info    : Used 238 registers, 696 bytes cmem[0]
ptxas info    : Compiling entry function '_Z44flash_bwd_dq_dk_dv_loop_seqk_parallel_kernelI23Flash_bwd_kernel_traitsILi192ELi64ELi64ELi8ELi4ELi2ELi2ELb0ELb0EN7cutlass6half_tE19Flash_kernel_traitsILi192ELi64ELi64ELi8ES2_EELb1ELb0ELb1ELb0EEv16Flash_bwd_params' for 'sm_80'
ptxas info    : Function properties for _Z44flash_bwd_dq_dk_dv_loop_seqk_parallel_kernelI23Flash_bwd_kernel_traitsILi192ELi64ELi64ELi8ELi4ELi2ELi2ELb0ELb0EN7cutlass6half_tE19Flash_kernel_traitsILi192ELi64ELi64ELi8ES2_EELb1ELb0ELb1ELb0EEv16Flash_bwd_params
    0 bytes stack frame, 0 bytes spill stores, 0 bytes spill loads
ptxas info    : Used 232 registers, 696 bytes cmem[0]
ptxas info    : Compiling entry function '_Z44flash_bwd_dq_dk_dv_loop_seqk_parallel_kernelI23Flash_bwd_kernel_traitsILi192ELi64ELi64ELi8ELi4ELi2ELi2ELb0ELb0EN7cutlass6half_tE19Flash_kernel_traitsILi192ELi64ELi64ELi8ES2_EELb1ELb0ELb1ELb1EEv16Flash_bwd_params' for 'sm_80'
ptxas info    : Function properties for _Z44flash_bwd_dq_dk_dv_loop_seqk_parallel_kernelI23Flash_bwd_kernel_traitsILi192ELi64ELi64ELi8ELi4ELi2ELi2ELb0ELb0EN7cutlass6half_tE19Flash_kernel_traitsILi192ELi64ELi64ELi8ES2_EELb1ELb0ELb1ELb1EEv16Flash_bwd_params
    0 bytes stack frame, 0 bytes spill stores, 0 bytes spill loads
ptxas info    : Used 233 registers, 696 bytes cmem[0]
ptxas info    : Compiling entry function '_Z44flash_bwd_dq_dk_dv_loop_seqk_parallel_kernelI23Flash_bwd_kernel_traitsILi192ELi64ELi64ELi8ELi4ELi2ELi2ELb0ELb0EN7cutlass6half_tE19Flash_kernel_traitsILi192ELi64ELi64ELi8ES2_EELb1ELb1ELb0ELb0EEv16Flash_bwd_params' for 'sm_80'
ptxas info    : Function properties for _Z44flash_bwd_dq_dk_dv_loop_seqk_parallel_kernelI23Flash_bwd_kernel_traitsILi192ELi64ELi64ELi8ELi4ELi2ELi2ELb0ELb0EN7cutlass6half_tE19Flash_kernel_traitsILi192ELi64ELi64ELi8ES2_EELb1ELb1ELb0ELb0EEv16Flash_bwd_params
    0 bytes stack frame, 0 bytes spill stores, 0 bytes spill loads
ptxas info    : Used 240 registers, 696 bytes cmem[0]
ptxas info    : Compiling entry function '_Z44flash_bwd_dq_dk_dv_loop_seqk_parallel_kernelI23Flash_bwd_kernel_traitsILi192ELi64ELi64ELi8ELi4ELi2ELi2ELb0ELb0EN7cutlass6half_tE19Flash_kernel_traitsILi192ELi64ELi64ELi8ES2_EELb1ELb1ELb0ELb1EEv16Flash_bwd_params' for 'sm_80'
ptxas info    : Function properties for _Z44flash_bwd_dq_dk_dv_loop_seqk_parallel_kernelI23Flash_bwd_kernel_traitsILi192ELi64ELi64ELi8ELi4ELi2ELi2ELb0ELb0EN7cutlass6half_tE19Flash_kernel_traitsILi192ELi64ELi64ELi8ES2_EELb1ELb1ELb0ELb1EEv16Flash_bwd_params
    0 bytes stack frame, 0 bytes spill stores, 0 bytes spill loads
ptxas info    : Used 239 registers, 696 bytes cmem[0]
ptxas info    : Compiling entry function '_Z44flash_bwd_dq_dk_dv_loop_seqk_parallel_kernelI23Flash_bwd_kernel_traitsILi192ELi64ELi64ELi8ELi4ELi2ELi2ELb0ELb0EN7cutlass6half_tE19Flash_kernel_traitsILi192ELi64ELi64ELi8ES2_EELb1ELb1ELb1ELb0EEv16Flash_bwd_params' for 'sm_80'
ptxas info    : Function properties for _Z44flash_bwd_dq_dk_dv_loop_seqk_parallel_kernelI23Flash_bwd_kernel_traitsILi192ELi64ELi64ELi8ELi4ELi2ELi2ELb0ELb0EN7cutlass6half_tE19Flash_kernel_traitsILi192ELi64ELi64ELi8ES2_EELb1ELb1ELb1ELb0EEv16Flash_bwd_params
    0 bytes stack frame, 0 bytes spill stores, 0 bytes spill loads
ptxas info    : Used 236 registers, 696 bytes cmem[0]
ptxas info    : Compiling entry function '_Z44flash_bwd_dq_dk_dv_loop_seqk_parallel_kernelI23Flash_bwd_kernel_traitsILi192ELi64ELi64ELi8ELi4ELi2ELi2ELb0ELb0EN7cutlass6half_tE19Flash_kernel_traitsILi192ELi64ELi64ELi8ES2_EELb1ELb1ELb1ELb1EEv16Flash_bwd_params' for 'sm_80'
ptxas info    : Function properties for _Z44flash_bwd_dq_dk_dv_loop_seqk_parallel_kernelI23Flash_bwd_kernel_traitsILi192ELi64ELi64ELi8ELi4ELi2ELi2ELb0ELb0EN7cutlass6half_tE19Flash_kernel_traitsILi192ELi64ELi64ELi8ES2_EELb1ELb1ELb1ELb1EEv16Flash_bwd_params
    0 bytes stack frame, 0 bytes spill stores, 0 bytes spill loads
ptxas info    : Used 240 registers, 696 bytes cmem[0]
ptxas info    : Compiling entry function '_Z25flash_bwd_dot_do_o_kernelILb1E23Flash_bwd_kernel_traitsILi192ELi64ELi64ELi8ELi4ELi2ELi2ELb0ELb0EN7cutlass6half_tE19Flash_kernel_traitsILi192ELi64ELi64ELi8ES2_EEEv16Flash_bwd_params' for 'sm_80'
ptxas info    : Function properties for _Z25flash_bwd_dot_do_o_kernelILb1E23Flash_bwd_kernel_traitsILi192ELi64ELi64ELi8ELi4ELi2ELi2ELb0ELb0EN7cutlass6half_tE19Flash_kernel_traitsILi192ELi64ELi64ELi8ES2_EEEv16Flash_bwd_params
    0 bytes stack frame, 0 bytes spill stores, 0 bytes spill loads
ptxas info    : Used 69 registers, 696 bytes cmem[0]
/anaconda/envs/flashattention/lib/python3.9/site-packages/torch/include/c10/core/SymInt.h(84): warning: integer conversion resulted in a change of sign

[16/36] /usr/local/cuda-11.4/bin/nvcc  -I/home/v-feiychen/flash-attention/csrc/flash_attn -I/home/v-feiychen/flash-attention/csrc/flash_attn/src -I/home/v-feiychen/flash-attention/csrc/cutlass/include -I/anaconda/envs/flashattention/lib/python3.9/site-packages/torch/include -I/anaconda/envs/flashattention/lib/python3.9/site-packages/torch/include/torch/csrc/api/include -I/anaconda/envs/flashattention/lib/python3.9/site-packages/torch/include/TH -I/anaconda/envs/flashattention/lib/python3.9/site-packages/torch/include/THC -I/usr/local/cuda-11.4/include -I/anaconda/envs/flashattention/include/python3.9 -c -c /home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_bwd_hdim192_bf16_sm80.cu -o /home/v-feiychen/flash-attention/build/temp.linux-x86_64-cpython-39/csrc/flash_attn/src/flash_bwd_hdim192_bf16_sm80.o -D__CUDA_NO_HALF_OPERATORS__ -D__CUDA_NO_HALF_CONVERSIONS__ -D__CUDA_NO_BFLOAT16_CONVERSIONS__ -D__CUDA_NO_HALF2_OPERATORS__ --expt-relaxed-constexpr --compiler-options ''"'"'-fPIC'"'"'' -O3 -std=c++17 -U__CUDA_NO_HALF_OPERATORS__ -U__CUDA_NO_HALF_CONVERSIONS__ -U__CUDA_NO_HALF2_OPERATORS__ -U__CUDA_NO_BFLOAT16_CONVERSIONS__ --expt-relaxed-constexpr --expt-extended-lambda --use_fast_math --ptxas-options=-v -lineinfo -gencode arch=compute_80,code=sm_80 --threads 4 -DTORCH_API_INCLUDE_EXTENSION_H '-DPYBIND11_COMPILER_TYPE="_gcc"' '-DPYBIND11_STDLIB="_libstdcpp"' '-DPYBIND11_BUILD_ABI="_cxxabi1011"' -DTORCH_EXTENSION_NAME=flash_attn_2_cuda -D_GLIBCXX_USE_CXX11_ABI=0
/anaconda/envs/flashattention/lib/python3.9/site-packages/torch/include/c10/core/SymInt.h(84): warning: integer conversion resulted in a change of sign

ptxas info    : 2 bytes gmem
ptxas info    : Compiling entry function '_Z44flash_bwd_dq_dk_dv_loop_seqk_parallel_kernelI23Flash_bwd_kernel_traitsILi192ELi64ELi64ELi8ELi4ELi2ELi2ELb1ELb1EN7cutlass10bfloat16_tE19Flash_kernel_traitsILi192ELi64ELi64ELi8ES2_EELb0ELb0ELb0ELb0EEv16Flash_bwd_params' for 'sm_80'
ptxas info    : Function properties for _Z44flash_bwd_dq_dk_dv_loop_seqk_parallel_kernelI23Flash_bwd_kernel_traitsILi192ELi64ELi64ELi8ELi4ELi2ELi2ELb1ELb1EN7cutlass10bfloat16_tE19Flash_kernel_traitsILi192ELi64ELi64ELi8ES2_EELb0ELb0ELb0ELb0EEv16Flash_bwd_params
    176 bytes stack frame, 192 bytes spill stores, 192 bytes spill loads
ptxas info    : Used 255 registers, 696 bytes cmem[0]
ptxas info    : Compiling entry function '_Z44flash_bwd_dq_dk_dv_loop_seqk_parallel_kernelI23Flash_bwd_kernel_traitsILi192ELi64ELi64ELi8ELi4ELi2ELi2ELb1ELb1EN7cutlass10bfloat16_tE19Flash_kernel_traitsILi192ELi64ELi64ELi8ES2_EELb0ELb0ELb0ELb1EEv16Flash_bwd_params' for 'sm_80'
ptxas info    : Function properties for _Z44flash_bwd_dq_dk_dv_loop_seqk_parallel_kernelI23Flash_bwd_kernel_traitsILi192ELi64ELi64ELi8ELi4ELi2ELi2ELb1ELb1EN7cutlass10bfloat16_tE19Flash_kernel_traitsILi192ELi64ELi64ELi8ES2_EELb0ELb0ELb0ELb1EEv16Flash_bwd_params
    176 bytes stack frame, 188 bytes spill stores, 192 bytes spill loads
ptxas info    : Used 255 registers, 696 bytes cmem[0]
ptxas info    : Compiling entry function '_Z44flash_bwd_dq_dk_dv_loop_seqk_parallel_kernelI23Flash_bwd_kernel_traitsILi192ELi64ELi64ELi8ELi4ELi2ELi2ELb1ELb1EN7cutlass10bfloat16_tE19Flash_kernel_traitsILi192ELi64ELi64ELi8ES2_EELb0ELb0ELb1ELb0EEv16Flash_bwd_params' for 'sm_80'
ptxas info    : Function properties for _Z44flash_bwd_dq_dk_dv_loop_seqk_parallel_kernelI23Flash_bwd_kernel_traitsILi192ELi64ELi64ELi8ELi4ELi2ELi2ELb1ELb1EN7cutlass10bfloat16_tE19Flash_kernel_traitsILi192ELi64ELi64ELi8ES2_EELb0ELb0ELb1ELb0EEv16Flash_bwd_params
    168 bytes stack frame, 176 bytes spill stores, 180 bytes spill loads
ptxas info    : Used 255 registers, 696 bytes cmem[0]
ptxas info    : Compiling entry function '_Z44flash_bwd_dq_dk_dv_loop_seqk_parallel_kernelI23Flash_bwd_kernel_traitsILi192ELi64ELi64ELi8ELi4ELi2ELi2ELb1ELb1EN7cutlass10bfloat16_tE19Flash_kernel_traitsILi192ELi64ELi64ELi8ES2_EELb0ELb0ELb1ELb1EEv16Flash_bwd_params' for 'sm_80'
ptxas info    : Function properties for _Z44flash_bwd_dq_dk_dv_loop_seqk_parallel_kernelI23Flash_bwd_kernel_traitsILi192ELi64ELi64ELi8ELi4ELi2ELi2ELb1ELb1EN7cutlass10bfloat16_tE19Flash_kernel_traitsILi192ELi64ELi64ELi8ES2_EELb0ELb0ELb1ELb1EEv16Flash_bwd_params
    168 bytes stack frame, 176 bytes spill stores, 180 bytes spill loads
ptxas info    : Used 255 registers, 696 bytes cmem[0]
ptxas info    : Compiling entry function '_Z44flash_bwd_dq_dk_dv_loop_seqk_parallel_kernelI23Flash_bwd_kernel_traitsILi192ELi64ELi64ELi8ELi4ELi2ELi2ELb1ELb1EN7cutlass10bfloat16_tE19Flash_kernel_traitsILi192ELi64ELi64ELi8ES2_EELb0ELb1ELb0ELb0EEv16Flash_bwd_params' for 'sm_80'
ptxas info    : Function properties for _Z44flash_bwd_dq_dk_dv_loop_seqk_parallel_kernelI23Flash_bwd_kernel_traitsILi192ELi64ELi64ELi8ELi4ELi2ELi2ELb1ELb1EN7cutlass10bfloat16_tE19Flash_kernel_traitsILi192ELi64ELi64ELi8ES2_EELb0ELb1ELb0ELb0EEv16Flash_bwd_params
    192 bytes stack frame, 196 bytes spill stores, 208 bytes spill loads
ptxas info    : Used 255 registers, 696 bytes cmem[0]
ptxas info    : Compiling entry function '_Z44flash_bwd_dq_dk_dv_loop_seqk_parallel_kernelI23Flash_bwd_kernel_traitsILi192ELi64ELi64ELi8ELi4ELi2ELi2ELb1ELb1EN7cutlass10bfloat16_tE19Flash_kernel_traitsILi192ELi64ELi64ELi8ES2_EELb0ELb1ELb0ELb1EEv16Flash_bwd_params' for 'sm_80'
ptxas info    : Function properties for _Z44flash_bwd_dq_dk_dv_loop_seqk_parallel_kernelI23Flash_bwd_kernel_traitsILi192ELi64ELi64ELi8ELi4ELi2ELi2ELb1ELb1EN7cutlass10bfloat16_tE19Flash_kernel_traitsILi192ELi64ELi64ELi8ES2_EELb0ELb1ELb0ELb1EEv16Flash_bwd_params
    176 bytes stack frame, 192 bytes spill stores, 196 bytes spill loads
ptxas info    : Used 255 registers, 696 bytes cmem[0]
ptxas info    : Compiling entry function '_Z44flash_bwd_dq_dk_dv_loop_seqk_parallel_kernelI23Flash_bwd_kernel_traitsILi192ELi64ELi64ELi8ELi4ELi2ELi2ELb1ELb1EN7cutlass10bfloat16_tE19Flash_kernel_traitsILi192ELi64ELi64ELi8ES2_EELb0ELb1ELb1ELb0EEv16Flash_bwd_params' for 'sm_80'
ptxas info    : Function properties for _Z44flash_bwd_dq_dk_dv_loop_seqk_parallel_kernelI23Flash_bwd_kernel_traitsILi192ELi64ELi64ELi8ELi4ELi2ELi2ELb1ELb1EN7cutlass10bfloat16_tE19Flash_kernel_traitsILi192ELi64ELi64ELi8ES2_EELb0ELb1ELb1ELb0EEv16Flash_bwd_params
    176 bytes stack frame, 180 bytes spill stores, 192 bytes spill loads
ptxas info    : Used 255 registers, 696 bytes cmem[0]
ptxas info    : Compiling entry function '_Z44flash_bwd_dq_dk_dv_loop_seqk_parallel_kernelI23Flash_bwd_kernel_traitsILi192ELi64ELi64ELi8ELi4ELi2ELi2ELb1ELb1EN7cutlass10bfloat16_tE19Flash_kernel_traitsILi192ELi64ELi64ELi8ES2_EELb0ELb1ELb1ELb1EEv16Flash_bwd_params' for 'sm_80'
ptxas info    : Function properties for _Z44flash_bwd_dq_dk_dv_loop_seqk_parallel_kernelI23Flash_bwd_kernel_traitsILi192ELi64ELi64ELi8ELi4ELi2ELi2ELb1ELb1EN7cutlass10bfloat16_tE19Flash_kernel_traitsILi192ELi64ELi64ELi8ES2_EELb0ELb1ELb1ELb1EEv16Flash_bwd_params
    176 bytes stack frame, 180 bytes spill stores, 192 bytes spill loads
ptxas info    : Used 255 registers, 696 bytes cmem[0]
ptxas info    : Compiling entry function '_Z44flash_bwd_dq_dk_dv_loop_seqk_parallel_kernelI23Flash_bwd_kernel_traitsILi192ELi64ELi64ELi8ELi4ELi2ELi2ELb0ELb0EN7cutlass10bfloat16_tE19Flash_kernel_traitsILi192ELi64ELi64ELi8ES2_EELb0ELb0ELb0ELb0EEv16Flash_bwd_params' for 'sm_80'
ptxas info    : Function properties for _Z44flash_bwd_dq_dk_dv_loop_seqk_parallel_kernelI23Flash_bwd_kernel_traitsILi192ELi64ELi64ELi8ELi4ELi2ELi2ELb0ELb0EN7cutlass10bfloat16_tE19Flash_kernel_traitsILi192ELi64ELi64ELi8ES2_EELb0ELb0ELb0ELb0EEv16Flash_bwd_params
    0 bytes stack frame, 0 bytes spill stores, 0 bytes spill loads
ptxas info    : Used 236 registers, 696 bytes cmem[0]
ptxas info    : Compiling entry function '_Z44flash_bwd_dq_dk_dv_loop_seqk_parallel_kernelI23Flash_bwd_kernel_traitsILi192ELi64ELi64ELi8ELi4ELi2ELi2ELb0ELb0EN7cutlass10bfloat16_tE19Flash_kernel_traitsILi192ELi64ELi64ELi8ES2_EELb0ELb0ELb0ELb1EEv16Flash_bwd_params' for 'sm_80'
ptxas info    : Function properties for _Z44flash_bwd_dq_dk_dv_loop_seqk_parallel_kernelI23Flash_bwd_kernel_traitsILi192ELi64ELi64ELi8ELi4ELi2ELi2ELb0ELb0EN7cutlass10bfloat16_tE19Flash_kernel_traitsILi192ELi64ELi64ELi8ES2_EELb0ELb0ELb0ELb1EEv16Flash_bwd_params
    0 bytes stack frame, 0 bytes spill stores, 0 bytes spill loads
ptxas info    : Used 236 registers, 696 bytes cmem[0]
ptxas info    : Compiling entry function '_Z44flash_bwd_dq_dk_dv_loop_seqk_parallel_kernelI23Flash_bwd_kernel_traitsILi192ELi64ELi64ELi8ELi4ELi2ELi2ELb0ELb0EN7cutlass10bfloat16_tE19Flash_kernel_traitsILi192ELi64ELi64ELi8ES2_EELb0ELb0ELb1ELb0EEv16Flash_bwd_params' for 'sm_80'
ptxas info    : Function properties for _Z44flash_bwd_dq_dk_dv_loop_seqk_parallel_kernelI23Flash_bwd_kernel_traitsILi192ELi64ELi64ELi8ELi4ELi2ELi2ELb0ELb0EN7cutlass10bfloat16_tE19Flash_kernel_traitsILi192ELi64ELi64ELi8ES2_EELb0ELb0ELb1ELb0EEv16Flash_bwd_params
    0 bytes stack frame, 0 bytes spill stores, 0 bytes spill loads
ptxas info    : Used 231 registers, 696 bytes cmem[0]
ptxas info    : Compiling entry function '_Z44flash_bwd_dq_dk_dv_loop_seqk_parallel_kernelI23Flash_bwd_kernel_traitsILi192ELi64ELi64ELi8ELi4ELi2ELi2ELb0ELb0EN7cutlass10bfloat16_tE19Flash_kernel_traitsILi192ELi64ELi64ELi8ES2_EELb0ELb0ELb1ELb1EEv16Flash_bwd_params' for 'sm_80'
ptxas info    : Function properties for _Z44flash_bwd_dq_dk_dv_loop_seqk_parallel_kernelI23Flash_bwd_kernel_traitsILi192ELi64ELi64ELi8ELi4ELi2ELi2ELb0ELb0EN7cutlass10bfloat16_tE19Flash_kernel_traitsILi192ELi64ELi64ELi8ES2_EELb0ELb0ELb1ELb1EEv16Flash_bwd_params
    0 bytes stack frame, 0 bytes spill stores, 0 bytes spill loads
ptxas info    : Used 235 registers, 696 bytes cmem[0]
ptxas info    : Compiling entry function '_Z44flash_bwd_dq_dk_dv_loop_seqk_parallel_kernelI23Flash_bwd_kernel_traitsILi192ELi64ELi64ELi8ELi4ELi2ELi2ELb0ELb0EN7cutlass10bfloat16_tE19Flash_kernel_traitsILi192ELi64ELi64ELi8ES2_EELb0ELb1ELb0ELb0EEv16Flash_bwd_params' for 'sm_80'
ptxas info    : Function properties for _Z44flash_bwd_dq_dk_dv_loop_seqk_parallel_kernelI23Flash_bwd_kernel_traitsILi192ELi64ELi64ELi8ELi4ELi2ELi2ELb0ELb0EN7cutlass10bfloat16_tE19Flash_kernel_traitsILi192ELi64ELi64ELi8ES2_EELb0ELb1ELb0ELb0EEv16Flash_bwd_params
    0 bytes stack frame, 0 bytes spill stores, 0 bytes spill loads
ptxas info    : Used 236 registers, 696 bytes cmem[0]
ptxas info    : Compiling entry function '_Z44flash_bwd_dq_dk_dv_loop_seqk_parallel_kernelI23Flash_bwd_kernel_traitsILi192ELi64ELi64ELi8ELi4ELi2ELi2ELb0ELb0EN7cutlass10bfloat16_tE19Flash_kernel_traitsILi192ELi64ELi64ELi8ES2_EELb0ELb1ELb0ELb1EEv16Flash_bwd_params' for 'sm_80'
ptxas info    : Function properties for _Z44flash_bwd_dq_dk_dv_loop_seqk_parallel_kernelI23Flash_bwd_kernel_traitsILi192ELi64ELi64ELi8ELi4ELi2ELi2ELb0ELb0EN7cutlass10bfloat16_tE19Flash_kernel_traitsILi192ELi64ELi64ELi8ES2_EELb0ELb1ELb0ELb1EEv16Flash_bwd_params
    0 bytes stack frame, 0 bytes spill stores, 0 bytes spill loads
ptxas info    : Used 236 registers, 696 bytes cmem[0]
ptxas info    : Compiling entry function '_Z44flash_bwd_dq_dk_dv_loop_seqk_parallel_kernelI23Flash_bwd_kernel_traitsILi192ELi64ELi64ELi8ELi4ELi2ELi2ELb0ELb0EN7cutlass10bfloat16_tE19Flash_kernel_traitsILi192ELi64ELi64ELi8ES2_EELb0ELb1ELb1ELb0EEv16Flash_bwd_params' for 'sm_80'
ptxas info    : Function properties for _Z44flash_bwd_dq_dk_dv_loop_seqk_parallel_kernelI23Flash_bwd_kernel_traitsILi192ELi64ELi64ELi8ELi4ELi2ELi2ELb0ELb0EN7cutlass10bfloat16_tE19Flash_kernel_traitsILi192ELi64ELi64ELi8ES2_EELb0ELb1ELb1ELb0EEv16Flash_bwd_params
    0 bytes stack frame, 0 bytes spill stores, 0 bytes spill loads
ptxas info    : Used 234 registers, 696 bytes cmem[0]
ptxas info    : Compiling entry function '_Z44flash_bwd_dq_dk_dv_loop_seqk_parallel_kernelI23Flash_bwd_kernel_traitsILi192ELi64ELi64ELi8ELi4ELi2ELi2ELb0ELb0EN7cutlass10bfloat16_tE19Flash_kernel_traitsILi192ELi64ELi64ELi8ES2_EELb0ELb1ELb1ELb1EEv16Flash_bwd_params' for 'sm_80'
ptxas info    : Function properties for _Z44flash_bwd_dq_dk_dv_loop_seqk_parallel_kernelI23Flash_bwd_kernel_traitsILi192ELi64ELi64ELi8ELi4ELi2ELi2ELb0ELb0EN7cutlass10bfloat16_tE19Flash_kernel_traitsILi192ELi64ELi64ELi8ES2_EELb0ELb1ELb1ELb1EEv16Flash_bwd_params
    0 bytes stack frame, 0 bytes spill stores, 0 bytes spill loads
ptxas info    : Used 238 registers, 696 bytes cmem[0]
ptxas info    : Compiling entry function '_Z27flash_bwd_convert_dq_kernelI23Flash_bwd_kernel_traitsILi192ELi64ELi64ELi8ELi4ELi2ELi2ELb1ELb1EN7cutlass10bfloat16_tE19Flash_kernel_traitsILi192ELi64ELi64ELi8ES2_EEEv16Flash_bwd_params' for 'sm_80'
ptxas info    : Function properties for _Z27flash_bwd_convert_dq_kernelI23Flash_bwd_kernel_traitsILi192ELi64ELi64ELi8ELi4ELi2ELi2ELb1ELb1EN7cutlass10bfloat16_tE19Flash_kernel_traitsILi192ELi64ELi64ELi8ES2_EEEv16Flash_bwd_params
    0 bytes stack frame, 0 bytes spill stores, 0 bytes spill loads
ptxas info    : Used 32 registers, 696 bytes cmem[0]
ptxas info    : Compiling entry function '_Z44flash_bwd_dq_dk_dv_loop_seqk_parallel_kernelI23Flash_bwd_kernel_traitsILi192ELi64ELi64ELi8ELi4ELi2ELi2ELb1ELb1EN7cutlass10bfloat16_tE19Flash_kernel_traitsILi192ELi64ELi64ELi8ES2_EELb1ELb0ELb0ELb0EEv16Flash_bwd_params' for 'sm_80'
ptxas info    : Function properties for _Z44flash_bwd_dq_dk_dv_loop_seqk_parallel_kernelI23Flash_bwd_kernel_traitsILi192ELi64ELi64ELi8ELi4ELi2ELi2ELb1ELb1EN7cutlass10bfloat16_tE19Flash_kernel_traitsILi192ELi64ELi64ELi8ES2_EELb1ELb0ELb0ELb0EEv16Flash_bwd_params
    192 bytes stack frame, 200 bytes spill stores, 208 bytes spill loads
ptxas info    : Used 255 registers, 696 bytes cmem[0]
ptxas info    : Compiling entry function '_Z44flash_bwd_dq_dk_dv_loop_seqk_parallel_kernelI23Flash_bwd_kernel_traitsILi192ELi64ELi64ELi8ELi4ELi2ELi2ELb1ELb1EN7cutlass10bfloat16_tE19Flash_kernel_traitsILi192ELi64ELi64ELi8ES2_EELb1ELb0ELb0ELb1EEv16Flash_bwd_params' for 'sm_80'
ptxas info    : Function properties for _Z44flash_bwd_dq_dk_dv_loop_seqk_parallel_kernelI23Flash_bwd_kernel_traitsILi192ELi64ELi64ELi8ELi4ELi2ELi2ELb1ELb1EN7cutlass10bfloat16_tE19Flash_kernel_traitsILi192ELi64ELi64ELi8ES2_EELb1ELb0ELb0ELb1EEv16Flash_bwd_params
    192 bytes stack frame, 196 bytes spill stores, 204 bytes spill loads
ptxas info    : Used 255 registers, 696 bytes cmem[0]
ptxas info    : Compiling entry function '_Z44flash_bwd_dq_dk_dv_loop_seqk_parallel_kernelI23Flash_bwd_kernel_traitsILi192ELi64ELi64ELi8ELi4ELi2ELi2ELb1ELb1EN7cutlass10bfloat16_tE19Flash_kernel_traitsILi192ELi64ELi64ELi8ES2_EELb1ELb0ELb1ELb0EEv16Flash_bwd_params' for 'sm_80'
ptxas info    : Function properties for _Z44flash_bwd_dq_dk_dv_loop_seqk_parallel_kernelI23Flash_bwd_kernel_traitsILi192ELi64ELi64ELi8ELi4ELi2ELi2ELb1ELb1EN7cutlass10bfloat16_tE19Flash_kernel_traitsILi192ELi64ELi64ELi8ES2_EELb1ELb0ELb1ELb0EEv16Flash_bwd_params
    192 bytes stack frame, 196 bytes spill stores, 204 bytes spill loads
ptxas info    : Used 255 registers, 696 bytes cmem[0]
ptxas info    : Compiling entry function '_Z44flash_bwd_dq_dk_dv_loop_seqk_parallel_kernelI23Flash_bwd_kernel_traitsILi192ELi64ELi64ELi8ELi4ELi2ELi2ELb1ELb1EN7cutlass10bfloat16_tE19Flash_kernel_traitsILi192ELi64ELi64ELi8ES2_EELb1ELb0ELb1ELb1EEv16Flash_bwd_params' for 'sm_80'
ptxas info    : Function properties for _Z44flash_bwd_dq_dk_dv_loop_seqk_parallel_kernelI23Flash_bwd_kernel_traitsILi192ELi64ELi64ELi8ELi4ELi2ELi2ELb1ELb1EN7cutlass10bfloat16_tE19Flash_kernel_traitsILi192ELi64ELi64ELi8ES2_EELb1ELb0ELb1ELb1EEv16Flash_bwd_params
    192 bytes stack frame, 196 bytes spill stores, 196 bytes spill loads
ptxas info    : Used 255 registers, 696 bytes cmem[0]
ptxas info    : Compiling entry function '_Z44flash_bwd_dq_dk_dv_loop_seqk_parallel_kernelI23Flash_bwd_kernel_traitsILi192ELi64ELi64ELi8ELi4ELi2ELi2ELb1ELb1EN7cutlass10bfloat16_tE19Flash_kernel_traitsILi192ELi64ELi64ELi8ES2_EELb1ELb1ELb0ELb0EEv16Flash_bwd_params' for 'sm_80'
ptxas info    : Function properties for _Z44flash_bwd_dq_dk_dv_loop_seqk_parallel_kernelI23Flash_bwd_kernel_traitsILi192ELi64ELi64ELi8ELi4ELi2ELi2ELb1ELb1EN7cutlass10bfloat16_tE19Flash_kernel_traitsILi192ELi64ELi64ELi8ES2_EELb1ELb1ELb0ELb0EEv16Flash_bwd_params
    208 bytes stack frame, 212 bytes spill stores, 220 bytes spill loads
ptxas info    : Used 255 registers, 696 bytes cmem[0]
ptxas info    : Compiling entry function '_Z44flash_bwd_dq_dk_dv_loop_seqk_parallel_kernelI23Flash_bwd_kernel_traitsILi192ELi64ELi64ELi8ELi4ELi2ELi2ELb1ELb1EN7cutlass10bfloat16_tE19Flash_kernel_traitsILi192ELi64ELi64ELi8ES2_EELb1ELb1ELb0ELb1EEv16Flash_bwd_params' for 'sm_80'
ptxas info    : Function properties for _Z44flash_bwd_dq_dk_dv_loop_seqk_parallel_kernelI23Flash_bwd_kernel_traitsILi192ELi64ELi64ELi8ELi4ELi2ELi2ELb1ELb1EN7cutlass10bfloat16_tE19Flash_kernel_traitsILi192ELi64ELi64ELi8ES2_EELb1ELb1ELb0ELb1EEv16Flash_bwd_params
    192 bytes stack frame, 200 bytes spill stores, 208 bytes spill loads
ptxas info    : Used 255 registers, 696 bytes cmem[0]
ptxas info    : Compiling entry function '_Z44flash_bwd_dq_dk_dv_loop_seqk_parallel_kernelI23Flash_bwd_kernel_traitsILi192ELi64ELi64ELi8ELi4ELi2ELi2ELb1ELb1EN7cutlass10bfloat16_tE19Flash_kernel_traitsILi192ELi64ELi64ELi8ES2_EELb1ELb1ELb1ELb0EEv16Flash_bwd_params' for 'sm_80'
ptxas info    : Function properties for _Z44flash_bwd_dq_dk_dv_loop_seqk_parallel_kernelI23Flash_bwd_kernel_traitsILi192ELi64ELi64ELi8ELi4ELi2ELi2ELb1ELb1EN7cutlass10bfloat16_tE19Flash_kernel_traitsILi192ELi64ELi64ELi8ES2_EELb1ELb1ELb1ELb0EEv16Flash_bwd_params
    208 bytes stack frame, 212 bytes spill stores, 220 bytes spill loads
ptxas info    : Used 255 registers, 696 bytes cmem[0]
ptxas info    : Compiling entry function '_Z44flash_bwd_dq_dk_dv_loop_seqk_parallel_kernelI23Flash_bwd_kernel_traitsILi192ELi64ELi64ELi8ELi4ELi2ELi2ELb1ELb1EN7cutlass10bfloat16_tE19Flash_kernel_traitsILi192ELi64ELi64ELi8ES2_EELb1ELb1ELb1ELb1EEv16Flash_bwd_params' for 'sm_80'
ptxas info    : Function properties for _Z44flash_bwd_dq_dk_dv_loop_seqk_parallel_kernelI23Flash_bwd_kernel_traitsILi192ELi64ELi64ELi8ELi4ELi2ELi2ELb1ELb1EN7cutlass10bfloat16_tE19Flash_kernel_traitsILi192ELi64ELi64ELi8ES2_EELb1ELb1ELb1ELb1EEv16Flash_bwd_params
    192 bytes stack frame, 196 bytes spill stores, 204 bytes spill loads
ptxas info    : Used 255 registers, 696 bytes cmem[0]
ptxas info    : Compiling entry function '_Z25flash_bwd_dot_do_o_kernelILb1E23Flash_bwd_kernel_traitsILi192ELi64ELi64ELi8ELi4ELi2ELi2ELb1ELb1EN7cutlass10bfloat16_tE19Flash_kernel_traitsILi192ELi64ELi64ELi8ES2_EEEv16Flash_bwd_params' for 'sm_80'
ptxas info    : Function properties for _Z25flash_bwd_dot_do_o_kernelILb1E23Flash_bwd_kernel_traitsILi192ELi64ELi64ELi8ELi4ELi2ELi2ELb1ELb1EN7cutlass10bfloat16_tE19Flash_kernel_traitsILi192ELi64ELi64ELi8ES2_EEEv16Flash_bwd_params
    0 bytes stack frame, 0 bytes spill stores, 0 bytes spill loads
ptxas info    : Used 69 registers, 696 bytes cmem[0]
ptxas info    : Compiling entry function '_Z27flash_bwd_convert_dq_kernelI23Flash_bwd_kernel_traitsILi192ELi64ELi64ELi8ELi4ELi2ELi2ELb0ELb0EN7cutlass10bfloat16_tE19Flash_kernel_traitsILi192ELi64ELi64ELi8ES2_EEEv16Flash_bwd_params' for 'sm_80'
ptxas info    : Function properties for _Z27flash_bwd_convert_dq_kernelI23Flash_bwd_kernel_traitsILi192ELi64ELi64ELi8ELi4ELi2ELi2ELb0ELb0EN7cutlass10bfloat16_tE19Flash_kernel_traitsILi192ELi64ELi64ELi8ES2_EEEv16Flash_bwd_params
    0 bytes stack frame, 0 bytes spill stores, 0 bytes spill loads
ptxas info    : Used 32 registers, 696 bytes cmem[0]
ptxas info    : Compiling entry function '_Z44flash_bwd_dq_dk_dv_loop_seqk_parallel_kernelI23Flash_bwd_kernel_traitsILi192ELi64ELi64ELi8ELi4ELi2ELi2ELb0ELb0EN7cutlass10bfloat16_tE19Flash_kernel_traitsILi192ELi64ELi64ELi8ES2_EELb1ELb0ELb0ELb0EEv16Flash_bwd_params' for 'sm_80'
ptxas info    : Function properties for _Z44flash_bwd_dq_dk_dv_loop_seqk_parallel_kernelI23Flash_bwd_kernel_traitsILi192ELi64ELi64ELi8ELi4ELi2ELi2ELb0ELb0EN7cutlass10bfloat16_tE19Flash_kernel_traitsILi192ELi64ELi64ELi8ES2_EELb1ELb0ELb0ELb0EEv16Flash_bwd_params
    0 bytes stack frame, 0 bytes spill stores, 0 bytes spill loads
ptxas info    : Used 237 registers, 696 bytes cmem[0]
ptxas info    : Compiling entry function '_Z44flash_bwd_dq_dk_dv_loop_seqk_parallel_kernelI23Flash_bwd_kernel_traitsILi192ELi64ELi64ELi8ELi4ELi2ELi2ELb0ELb0EN7cutlass10bfloat16_tE19Flash_kernel_traitsILi192ELi64ELi64ELi8ES2_EELb1ELb0ELb0ELb1EEv16Flash_bwd_params' for 'sm_80'
ptxas info    : Function properties for _Z44flash_bwd_dq_dk_dv_loop_seqk_parallel_kernelI23Flash_bwd_kernel_traitsILi192ELi64ELi64ELi8ELi4ELi2ELi2ELb0ELb0EN7cutlass10bfloat16_tE19Flash_kernel_traitsILi192ELi64ELi64ELi8ES2_EELb1ELb0ELb0ELb1EEv16Flash_bwd_params
    0 bytes stack frame, 0 bytes spill stores, 0 bytes spill loads
ptxas info    : Used 238 registers, 696 bytes cmem[0]
ptxas info    : Compiling entry function '_Z44flash_bwd_dq_dk_dv_loop_seqk_parallel_kernelI23Flash_bwd_kernel_traitsILi192ELi64ELi64ELi8ELi4ELi2ELi2ELb0ELb0EN7cutlass10bfloat16_tE19Flash_kernel_traitsILi192ELi64ELi64ELi8ES2_EELb1ELb0ELb1ELb0EEv16Flash_bwd_params' for 'sm_80'
ptxas info    : Function properties for _Z44flash_bwd_dq_dk_dv_loop_seqk_parallel_kernelI23Flash_bwd_kernel_traitsILi192ELi64ELi64ELi8ELi4ELi2ELi2ELb0ELb0EN7cutlass10bfloat16_tE19Flash_kernel_traitsILi192ELi64ELi64ELi8ES2_EELb1ELb0ELb1ELb0EEv16Flash_bwd_params
    0 bytes stack frame, 0 bytes spill stores, 0 bytes spill loads
ptxas info    : Used 232 registers, 696 bytes cmem[0]
ptxas info    : Compiling entry function '_Z44flash_bwd_dq_dk_dv_loop_seqk_parallel_kernelI23Flash_bwd_kernel_traitsILi192ELi64ELi64ELi8ELi4ELi2ELi2ELb0ELb0EN7cutlass10bfloat16_tE19Flash_kernel_traitsILi192ELi64ELi64ELi8ES2_EELb1ELb0ELb1ELb1EEv16Flash_bwd_params' for 'sm_80'
ptxas info    : Function properties for _Z44flash_bwd_dq_dk_dv_loop_seqk_parallel_kernelI23Flash_bwd_kernel_traitsILi192ELi64ELi64ELi8ELi4ELi2ELi2ELb0ELb0EN7cutlass10bfloat16_tE19Flash_kernel_traitsILi192ELi64ELi64ELi8ES2_EELb1ELb0ELb1ELb1EEv16Flash_bwd_params
    0 bytes stack frame, 0 bytes spill stores, 0 bytes spill loads
ptxas info    : Used 233 registers, 696 bytes cmem[0]
ptxas info    : Compiling entry function '_Z44flash_bwd_dq_dk_dv_loop_seqk_parallel_kernelI23Flash_bwd_kernel_traitsILi192ELi64ELi64ELi8ELi4ELi2ELi2ELb0ELb0EN7cutlass10bfloat16_tE19Flash_kernel_traitsILi192ELi64ELi64ELi8ES2_EELb1ELb1ELb0ELb0EEv16Flash_bwd_params' for 'sm_80'
ptxas info    : Function properties for _Z44flash_bwd_dq_dk_dv_loop_seqk_parallel_kernelI23Flash_bwd_kernel_traitsILi192ELi64ELi64ELi8ELi4ELi2ELi2ELb0ELb0EN7cutlass10bfloat16_tE19Flash_kernel_traitsILi192ELi64ELi64ELi8ES2_EELb1ELb1ELb0ELb0EEv16Flash_bwd_params
    0 bytes stack frame, 0 bytes spill stores, 0 bytes spill loads
ptxas info    : Used 240 registers, 696 bytes cmem[0]
ptxas info    : Compiling entry function '_Z44flash_bwd_dq_dk_dv_loop_seqk_parallel_kernelI23Flash_bwd_kernel_traitsILi192ELi64ELi64ELi8ELi4ELi2ELi2ELb0ELb0EN7cutlass10bfloat16_tE19Flash_kernel_traitsILi192ELi64ELi64ELi8ES2_EELb1ELb1ELb0ELb1EEv16Flash_bwd_params' for 'sm_80'
ptxas info    : Function properties for _Z44flash_bwd_dq_dk_dv_loop_seqk_parallel_kernelI23Flash_bwd_kernel_traitsILi192ELi64ELi64ELi8ELi4ELi2ELi2ELb0ELb0EN7cutlass10bfloat16_tE19Flash_kernel_traitsILi192ELi64ELi64ELi8ES2_EELb1ELb1ELb0ELb1EEv16Flash_bwd_params
    0 bytes stack frame, 0 bytes spill stores, 0 bytes spill loads
ptxas info    : Used 239 registers, 696 bytes cmem[0]
ptxas info    : Compiling entry function '_Z44flash_bwd_dq_dk_dv_loop_seqk_parallel_kernelI23Flash_bwd_kernel_traitsILi192ELi64ELi64ELi8ELi4ELi2ELi2ELb0ELb0EN7cutlass10bfloat16_tE19Flash_kernel_traitsILi192ELi64ELi64ELi8ES2_EELb1ELb1ELb1ELb0EEv16Flash_bwd_params' for 'sm_80'
ptxas info    : Function properties for _Z44flash_bwd_dq_dk_dv_loop_seqk_parallel_kernelI23Flash_bwd_kernel_traitsILi192ELi64ELi64ELi8ELi4ELi2ELi2ELb0ELb0EN7cutlass10bfloat16_tE19Flash_kernel_traitsILi192ELi64ELi64ELi8ES2_EELb1ELb1ELb1ELb0EEv16Flash_bwd_params
    0 bytes stack frame, 0 bytes spill stores, 0 bytes spill loads
ptxas info    : Used 236 registers, 696 bytes cmem[0]
ptxas info    : Compiling entry function '_Z44flash_bwd_dq_dk_dv_loop_seqk_parallel_kernelI23Flash_bwd_kernel_traitsILi192ELi64ELi64ELi8ELi4ELi2ELi2ELb0ELb0EN7cutlass10bfloat16_tE19Flash_kernel_traitsILi192ELi64ELi64ELi8ES2_EELb1ELb1ELb1ELb1EEv16Flash_bwd_params' for 'sm_80'
ptxas info    : Function properties for _Z44flash_bwd_dq_dk_dv_loop_seqk_parallel_kernelI23Flash_bwd_kernel_traitsILi192ELi64ELi64ELi8ELi4ELi2ELi2ELb0ELb0EN7cutlass10bfloat16_tE19Flash_kernel_traitsILi192ELi64ELi64ELi8ES2_EELb1ELb1ELb1ELb1EEv16Flash_bwd_params
    0 bytes stack frame, 0 bytes spill stores, 0 bytes spill loads
ptxas info    : Used 240 registers, 696 bytes cmem[0]
ptxas info    : Compiling entry function '_Z25flash_bwd_dot_do_o_kernelILb1E23Flash_bwd_kernel_traitsILi192ELi64ELi64ELi8ELi4ELi2ELi2ELb0ELb0EN7cutlass10bfloat16_tE19Flash_kernel_traitsILi192ELi64ELi64ELi8ES2_EEEv16Flash_bwd_params' for 'sm_80'
ptxas info    : Function properties for _Z25flash_bwd_dot_do_o_kernelILb1E23Flash_bwd_kernel_traitsILi192ELi64ELi64ELi8ELi4ELi2ELi2ELb0ELb0EN7cutlass10bfloat16_tE19Flash_kernel_traitsILi192ELi64ELi64ELi8ES2_EEEv16Flash_bwd_params
    0 bytes stack frame, 0 bytes spill stores, 0 bytes spill loads
ptxas info    : Used 69 registers, 696 bytes cmem[0]
/anaconda/envs/flashattention/lib/python3.9/site-packages/torch/include/c10/core/SymInt.h(84): warning: integer conversion resulted in a change of sign

[17/36] /usr/local/cuda-11.4/bin/nvcc  -I/home/v-feiychen/flash-attention/csrc/flash_attn -I/home/v-feiychen/flash-attention/csrc/flash_attn/src -I/home/v-feiychen/flash-attention/csrc/cutlass/include -I/anaconda/envs/flashattention/lib/python3.9/site-packages/torch/include -I/anaconda/envs/flashattention/lib/python3.9/site-packages/torch/include/torch/csrc/api/include -I/anaconda/envs/flashattention/lib/python3.9/site-packages/torch/include/TH -I/anaconda/envs/flashattention/lib/python3.9/site-packages/torch/include/THC -I/usr/local/cuda-11.4/include -I/anaconda/envs/flashattention/include/python3.9 -c -c /home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_bwd_hdim256_fp16_sm80.cu -o /home/v-feiychen/flash-attention/build/temp.linux-x86_64-cpython-39/csrc/flash_attn/src/flash_bwd_hdim256_fp16_sm80.o -D__CUDA_NO_HALF_OPERATORS__ -D__CUDA_NO_HALF_CONVERSIONS__ -D__CUDA_NO_BFLOAT16_CONVERSIONS__ -D__CUDA_NO_HALF2_OPERATORS__ --expt-relaxed-constexpr --compiler-options ''"'"'-fPIC'"'"'' -O3 -std=c++17 -U__CUDA_NO_HALF_OPERATORS__ -U__CUDA_NO_HALF_CONVERSIONS__ -U__CUDA_NO_HALF2_OPERATORS__ -U__CUDA_NO_BFLOAT16_CONVERSIONS__ --expt-relaxed-constexpr --expt-extended-lambda --use_fast_math --ptxas-options=-v -lineinfo -gencode arch=compute_80,code=sm_80 --threads 4 -DTORCH_API_INCLUDE_EXTENSION_H '-DPYBIND11_COMPILER_TYPE="_gcc"' '-DPYBIND11_STDLIB="_libstdcpp"' '-DPYBIND11_BUILD_ABI="_cxxabi1011"' -DTORCH_EXTENSION_NAME=flash_attn_2_cuda -D_GLIBCXX_USE_CXX11_ABI=0
/anaconda/envs/flashattention/lib/python3.9/site-packages/torch/include/c10/core/SymInt.h(84): warning: integer conversion resulted in a change of sign

ptxas info    : 2 bytes gmem
ptxas info    : Compiling entry function '_Z44flash_bwd_dq_dk_dv_loop_seqk_parallel_kernelI23Flash_bwd_kernel_traitsILi256ELi64ELi64ELi8ELi4ELi2ELi2ELb0ELb1EN7cutlass6half_tE19Flash_kernel_traitsILi256ELi64ELi64ELi8ES2_EELb0ELb0ELb0ELb0EEv16Flash_bwd_params' for 'sm_80'
ptxas info    : Function properties for _Z44flash_bwd_dq_dk_dv_loop_seqk_parallel_kernelI23Flash_bwd_kernel_traitsILi256ELi64ELi64ELi8ELi4ELi2ELi2ELb0ELb1EN7cutlass6half_tE19Flash_kernel_traitsILi256ELi64ELi64ELi8ES2_EELb0ELb0ELb0ELb0EEv16Flash_bwd_params
    16 bytes stack frame, 12 bytes spill stores, 12 bytes spill loads
ptxas info    : Used 255 registers, 696 bytes cmem[0]
ptxas info    : Compiling entry function '_Z44flash_bwd_dq_dk_dv_loop_seqk_parallel_kernelI23Flash_bwd_kernel_traitsILi256ELi64ELi64ELi8ELi4ELi2ELi2ELb0ELb1EN7cutlass6half_tE19Flash_kernel_traitsILi256ELi64ELi64ELi8ES2_EELb0ELb0ELb0ELb1EEv16Flash_bwd_params' for 'sm_80'
ptxas info    : Function properties for _Z44flash_bwd_dq_dk_dv_loop_seqk_parallel_kernelI23Flash_bwd_kernel_traitsILi256ELi64ELi64ELi8ELi4ELi2ELi2ELb0ELb1EN7cutlass6half_tE19Flash_kernel_traitsILi256ELi64ELi64ELi8ES2_EELb0ELb0ELb0ELb1EEv16Flash_bwd_params
    0 bytes stack frame, 0 bytes spill stores, 0 bytes spill loads
ptxas info    : Used 252 registers, 696 bytes cmem[0]
ptxas info    : Compiling entry function '_Z44flash_bwd_dq_dk_dv_loop_seqk_parallel_kernelI23Flash_bwd_kernel_traitsILi256ELi64ELi64ELi8ELi4ELi2ELi2ELb0ELb1EN7cutlass6half_tE19Flash_kernel_traitsILi256ELi64ELi64ELi8ES2_EELb0ELb0ELb1ELb0EEv16Flash_bwd_params' for 'sm_80'
ptxas info    : Function properties for _Z44flash_bwd_dq_dk_dv_loop_seqk_parallel_kernelI23Flash_bwd_kernel_traitsILi256ELi64ELi64ELi8ELi4ELi2ELi2ELb0ELb1EN7cutlass6half_tE19Flash_kernel_traitsILi256ELi64ELi64ELi8ES2_EELb0ELb0ELb1ELb0EEv16Flash_bwd_params
    0 bytes stack frame, 0 bytes spill stores, 0 bytes spill loads
ptxas info    : Used 254 registers, 696 bytes cmem[0]
ptxas info    : Compiling entry function '_Z44flash_bwd_dq_dk_dv_loop_seqk_parallel_kernelI23Flash_bwd_kernel_traitsILi256ELi64ELi64ELi8ELi4ELi2ELi2ELb0ELb1EN7cutlass6half_tE19Flash_kernel_traitsILi256ELi64ELi64ELi8ES2_EELb0ELb0ELb1ELb1EEv16Flash_bwd_params' for 'sm_80'
ptxas info    : Function properties for _Z44flash_bwd_dq_dk_dv_loop_seqk_parallel_kernelI23Flash_bwd_kernel_traitsILi256ELi64ELi64ELi8ELi4ELi2ELi2ELb0ELb1EN7cutlass6half_tE19Flash_kernel_traitsILi256ELi64ELi64ELi8ES2_EELb0ELb0ELb1ELb1EEv16Flash_bwd_params
    8 bytes stack frame, 4 bytes spill stores, 4 bytes spill loads
ptxas info    : Used 255 registers, 696 bytes cmem[0]
ptxas info    : Compiling entry function '_Z44flash_bwd_dq_dk_dv_loop_seqk_parallel_kernelI23Flash_bwd_kernel_traitsILi256ELi64ELi64ELi8ELi4ELi2ELi2ELb0ELb1EN7cutlass6half_tE19Flash_kernel_traitsILi256ELi64ELi64ELi8ES2_EELb0ELb1ELb0ELb0EEv16Flash_bwd_params' for 'sm_80'
ptxas info    : Function properties for _Z44flash_bwd_dq_dk_dv_loop_seqk_parallel_kernelI23Flash_bwd_kernel_traitsILi256ELi64ELi64ELi8ELi4ELi2ELi2ELb0ELb1EN7cutlass6half_tE19Flash_kernel_traitsILi256ELi64ELi64ELi8ES2_EELb0ELb1ELb0ELb0EEv16Flash_bwd_params
    16 bytes stack frame, 12 bytes spill stores, 12 bytes spill loads
ptxas info    : Used 255 registers, 696 bytes cmem[0]
ptxas info    : Compiling entry function '_Z44flash_bwd_dq_dk_dv_loop_seqk_parallel_kernelI23Flash_bwd_kernel_traitsILi256ELi64ELi64ELi8ELi4ELi2ELi2ELb0ELb1EN7cutlass6half_tE19Flash_kernel_traitsILi256ELi64ELi64ELi8ES2_EELb0ELb1ELb0ELb1EEv16Flash_bwd_params' for 'sm_80'
ptxas info    : Function properties for _Z44flash_bwd_dq_dk_dv_loop_seqk_parallel_kernelI23Flash_bwd_kernel_traitsILi256ELi64ELi64ELi8ELi4ELi2ELi2ELb0ELb1EN7cutlass6half_tE19Flash_kernel_traitsILi256ELi64ELi64ELi8ES2_EELb0ELb1ELb0ELb1EEv16Flash_bwd_params
    8 bytes stack frame, 8 bytes spill stores, 8 bytes spill loads
ptxas info    : Used 255 registers, 696 bytes cmem[0]
ptxas info    : Compiling entry function '_Z44flash_bwd_dq_dk_dv_loop_seqk_parallel_kernelI23Flash_bwd_kernel_traitsILi256ELi64ELi64ELi8ELi4ELi2ELi2ELb0ELb1EN7cutlass6half_tE19Flash_kernel_traitsILi256ELi64ELi64ELi8ES2_EELb0ELb1ELb1ELb0EEv16Flash_bwd_params' for 'sm_80'
ptxas info    : Function properties for _Z44flash_bwd_dq_dk_dv_loop_seqk_parallel_kernelI23Flash_bwd_kernel_traitsILi256ELi64ELi64ELi8ELi4ELi2ELi2ELb0ELb1EN7cutlass6half_tE19Flash_kernel_traitsILi256ELi64ELi64ELi8ES2_EELb0ELb1ELb1ELb0EEv16Flash_bwd_params
    0 bytes stack frame, 0 bytes spill stores, 0 bytes spill loads
ptxas info    : Used 252 registers, 696 bytes cmem[0]
ptxas info    : Compiling entry function '_Z44flash_bwd_dq_dk_dv_loop_seqk_parallel_kernelI23Flash_bwd_kernel_traitsILi256ELi64ELi64ELi8ELi4ELi2ELi2ELb0ELb1EN7cutlass6half_tE19Flash_kernel_traitsILi256ELi64ELi64ELi8ES2_EELb0ELb1ELb1ELb1EEv16Flash_bwd_params' for 'sm_80'
ptxas info    : Function properties for _Z44flash_bwd_dq_dk_dv_loop_seqk_parallel_kernelI23Flash_bwd_kernel_traitsILi256ELi64ELi64ELi8ELi4ELi2ELi2ELb0ELb1EN7cutlass6half_tE19Flash_kernel_traitsILi256ELi64ELi64ELi8ES2_EELb0ELb1ELb1ELb1EEv16Flash_bwd_params
    0 bytes stack frame, 0 bytes spill stores, 0 bytes spill loads
ptxas info    : Used 255 registers, 696 bytes cmem[0]
ptxas info    : Compiling entry function '_Z44flash_bwd_dq_dk_dv_loop_seqk_parallel_kernelI23Flash_bwd_kernel_traitsILi256ELi64ELi64ELi8ELi4ELi2ELi2ELb0ELb0EN7cutlass6half_tE19Flash_kernel_traitsILi256ELi64ELi64ELi8ES2_EELb0ELb0ELb0ELb0EEv16Flash_bwd_params' for 'sm_80'
ptxas info    : Function properties for _Z44flash_bwd_dq_dk_dv_loop_seqk_parallel_kernelI23Flash_bwd_kernel_traitsILi256ELi64ELi64ELi8ELi4ELi2ELi2ELb0ELb0EN7cutlass6half_tE19Flash_kernel_traitsILi256ELi64ELi64ELi8ES2_EELb0ELb0ELb0ELb0EEv16Flash_bwd_params
    8 bytes stack frame, 8 bytes spill stores, 8 bytes spill loads
ptxas info    : Used 255 registers, 696 bytes cmem[0]
ptxas info    : Compiling entry function '_Z44flash_bwd_dq_dk_dv_loop_seqk_parallel_kernelI23Flash_bwd_kernel_traitsILi256ELi64ELi64ELi8ELi4ELi2ELi2ELb0ELb0EN7cutlass6half_tE19Flash_kernel_traitsILi256ELi64ELi64ELi8ES2_EELb0ELb0ELb0ELb1EEv16Flash_bwd_params' for 'sm_80'
ptxas info    : Function properties for _Z44flash_bwd_dq_dk_dv_loop_seqk_parallel_kernelI23Flash_bwd_kernel_traitsILi256ELi64ELi64ELi8ELi4ELi2ELi2ELb0ELb0EN7cutlass6half_tE19Flash_kernel_traitsILi256ELi64ELi64ELi8ES2_EELb0ELb0ELb0ELb1EEv16Flash_bwd_params
    16 bytes stack frame, 12 bytes spill stores, 12 bytes spill loads
ptxas info    : Used 255 registers, 696 bytes cmem[0]
ptxas info    : Compiling entry function '_Z44flash_bwd_dq_dk_dv_loop_seqk_parallel_kernelI23Flash_bwd_kernel_traitsILi256ELi64ELi64ELi8ELi4ELi2ELi2ELb0ELb0EN7cutlass6half_tE19Flash_kernel_traitsILi256ELi64ELi64ELi8ES2_EELb0ELb0ELb1ELb0EEv16Flash_bwd_params' for 'sm_80'
ptxas info    : Function properties for _Z44flash_bwd_dq_dk_dv_loop_seqk_parallel_kernelI23Flash_bwd_kernel_traitsILi256ELi64ELi64ELi8ELi4ELi2ELi2ELb0ELb0EN7cutlass6half_tE19Flash_kernel_traitsILi256ELi64ELi64ELi8ES2_EELb0ELb0ELb1ELb0EEv16Flash_bwd_params
    0 bytes stack frame, 0 bytes spill stores, 0 bytes spill loads
ptxas info    : Used 254 registers, 696 bytes cmem[0]
ptxas info    : Compiling entry function '_Z44flash_bwd_dq_dk_dv_loop_seqk_parallel_kernelI23Flash_bwd_kernel_traitsILi256ELi64ELi64ELi8ELi4ELi2ELi2ELb0ELb0EN7cutlass6half_tE19Flash_kernel_traitsILi256ELi64ELi64ELi8ES2_EELb0ELb0ELb1ELb1EEv16Flash_bwd_params' for 'sm_80'
ptxas info    : Function properties for _Z44flash_bwd_dq_dk_dv_loop_seqk_parallel_kernelI23Flash_bwd_kernel_traitsILi256ELi64ELi64ELi8ELi4ELi2ELi2ELb0ELb0EN7cutlass6half_tE19Flash_kernel_traitsILi256ELi64ELi64ELi8ES2_EELb0ELb0ELb1ELb1EEv16Flash_bwd_params
    0 bytes stack frame, 0 bytes spill stores, 0 bytes spill loads
ptxas info    : Used 252 registers, 696 bytes cmem[0]
ptxas info    : Compiling entry function '_Z44flash_bwd_dq_dk_dv_loop_seqk_parallel_kernelI23Flash_bwd_kernel_traitsILi256ELi64ELi64ELi8ELi4ELi2ELi2ELb0ELb0EN7cutlass6half_tE19Flash_kernel_traitsILi256ELi64ELi64ELi8ES2_EELb0ELb1ELb0ELb0EEv16Flash_bwd_params' for 'sm_80'
ptxas info    : Function properties for _Z44flash_bwd_dq_dk_dv_loop_seqk_parallel_kernelI23Flash_bwd_kernel_traitsILi256ELi64ELi64ELi8ELi4ELi2ELi2ELb0ELb0EN7cutlass6half_tE19Flash_kernel_traitsILi256ELi64ELi64ELi8ES2_EELb0ELb1ELb0ELb0EEv16Flash_bwd_params
    8 bytes stack frame, 8 bytes spill stores, 8 bytes spill loads
ptxas info    : Used 255 registers, 696 bytes cmem[0]
ptxas info    : Compiling entry function '_Z44flash_bwd_dq_dk_dv_loop_seqk_parallel_kernelI23Flash_bwd_kernel_traitsILi256ELi64ELi64ELi8ELi4ELi2ELi2ELb0ELb0EN7cutlass6half_tE19Flash_kernel_traitsILi256ELi64ELi64ELi8ES2_EELb0ELb1ELb0ELb1EEv16Flash_bwd_params' for 'sm_80'
ptxas info    : Function properties for _Z44flash_bwd_dq_dk_dv_loop_seqk_parallel_kernelI23Flash_bwd_kernel_traitsILi256ELi64ELi64ELi8ELi4ELi2ELi2ELb0ELb0EN7cutlass6half_tE19Flash_kernel_traitsILi256ELi64ELi64ELi8ES2_EELb0ELb1ELb0ELb1EEv16Flash_bwd_params
    16 bytes stack frame, 12 bytes spill stores, 12 bytes spill loads
ptxas info    : Used 255 registers, 696 bytes cmem[0]
ptxas info    : Compiling entry function '_Z44flash_bwd_dq_dk_dv_loop_seqk_parallel_kernelI23Flash_bwd_kernel_traitsILi256ELi64ELi64ELi8ELi4ELi2ELi2ELb0ELb0EN7cutlass6half_tE19Flash_kernel_traitsILi256ELi64ELi64ELi8ES2_EELb0ELb1ELb1ELb0EEv16Flash_bwd_params' for 'sm_80'
ptxas info    : Function properties for _Z44flash_bwd_dq_dk_dv_loop_seqk_parallel_kernelI23Flash_bwd_kernel_traitsILi256ELi64ELi64ELi8ELi4ELi2ELi2ELb0ELb0EN7cutlass6half_tE19Flash_kernel_traitsILi256ELi64ELi64ELi8ES2_EELb0ELb1ELb1ELb0EEv16Flash_bwd_params
    0 bytes stack frame, 0 bytes spill stores, 0 bytes spill loads
ptxas info    : Used 254 registers, 696 bytes cmem[0]
ptxas info    : Compiling entry function '_Z44flash_bwd_dq_dk_dv_loop_seqk_parallel_kernelI23Flash_bwd_kernel_traitsILi256ELi64ELi64ELi8ELi4ELi2ELi2ELb0ELb0EN7cutlass6half_tE19Flash_kernel_traitsILi256ELi64ELi64ELi8ES2_EELb0ELb1ELb1ELb1EEv16Flash_bwd_params' for 'sm_80'
ptxas info    : Function properties for _Z44flash_bwd_dq_dk_dv_loop_seqk_parallel_kernelI23Flash_bwd_kernel_traitsILi256ELi64ELi64ELi8ELi4ELi2ELi2ELb0ELb0EN7cutlass6half_tE19Flash_kernel_traitsILi256ELi64ELi64ELi8ES2_EELb0ELb1ELb1ELb1EEv16Flash_bwd_params
    0 bytes stack frame, 0 bytes spill stores, 0 bytes spill loads
ptxas info    : Used 255 registers, 696 bytes cmem[0]
ptxas info    : Compiling entry function '_Z27flash_bwd_convert_dq_kernelI23Flash_bwd_kernel_traitsILi256ELi64ELi64ELi8ELi4ELi2ELi2ELb0ELb1EN7cutlass6half_tE19Flash_kernel_traitsILi256ELi64ELi64ELi8ES2_EEEv16Flash_bwd_params' for 'sm_80'
ptxas info    : Function properties for _Z27flash_bwd_convert_dq_kernelI23Flash_bwd_kernel_traitsILi256ELi64ELi64ELi8ELi4ELi2ELi2ELb0ELb1EN7cutlass6half_tE19Flash_kernel_traitsILi256ELi64ELi64ELi8ES2_EEEv16Flash_bwd_params
    0 bytes stack frame, 0 bytes spill stores, 0 bytes spill loads
ptxas info    : Used 40 registers, 696 bytes cmem[0]
ptxas info    : Compiling entry function '_Z44flash_bwd_dq_dk_dv_loop_seqk_parallel_kernelI23Flash_bwd_kernel_traitsILi256ELi64ELi64ELi8ELi4ELi2ELi2ELb0ELb1EN7cutlass6half_tE19Flash_kernel_traitsILi256ELi64ELi64ELi8ES2_EELb1ELb0ELb0ELb0EEv16Flash_bwd_params' for 'sm_80'
ptxas info    : Function properties for _Z44flash_bwd_dq_dk_dv_loop_seqk_parallel_kernelI23Flash_bwd_kernel_traitsILi256ELi64ELi64ELi8ELi4ELi2ELi2ELb0ELb1EN7cutlass6half_tE19Flash_kernel_traitsILi256ELi64ELi64ELi8ES2_EELb1ELb0ELb0ELb0EEv16Flash_bwd_params
    16 bytes stack frame, 12 bytes spill stores, 12 bytes spill loads
ptxas info    : Used 255 registers, 696 bytes cmem[0]
ptxas info    : Compiling entry function '_Z44flash_bwd_dq_dk_dv_loop_seqk_parallel_kernelI23Flash_bwd_kernel_traitsILi256ELi64ELi64ELi8ELi4ELi2ELi2ELb0ELb1EN7cutlass6half_tE19Flash_kernel_traitsILi256ELi64ELi64ELi8ES2_EELb1ELb0ELb0ELb1EEv16Flash_bwd_params' for 'sm_80'
ptxas info    : Function properties for _Z44flash_bwd_dq_dk_dv_loop_seqk_parallel_kernelI23Flash_bwd_kernel_traitsILi256ELi64ELi64ELi8ELi4ELi2ELi2ELb0ELb1EN7cutlass6half_tE19Flash_kernel_traitsILi256ELi64ELi64ELi8ES2_EELb1ELb0ELb0ELb1EEv16Flash_bwd_params
    8 bytes stack frame, 4 bytes spill stores, 4 bytes spill loads
ptxas info    : Used 255 registers, 696 bytes cmem[0]
ptxas info    : Compiling entry function '_Z44flash_bwd_dq_dk_dv_loop_seqk_parallel_kernelI23Flash_bwd_kernel_traitsILi256ELi64ELi64ELi8ELi4ELi2ELi2ELb0ELb1EN7cutlass6half_tE19Flash_kernel_traitsILi256ELi64ELi64ELi8ES2_EELb1ELb0ELb1ELb0EEv16Flash_bwd_params' for 'sm_80'
ptxas info    : Function properties for _Z44flash_bwd_dq_dk_dv_loop_seqk_parallel_kernelI23Flash_bwd_kernel_traitsILi256ELi64ELi64ELi8ELi4ELi2ELi2ELb0ELb1EN7cutlass6half_tE19Flash_kernel_traitsILi256ELi64ELi64ELi8ES2_EELb1ELb0ELb1ELb0EEv16Flash_bwd_params
    24 bytes stack frame, 24 bytes spill stores, 24 bytes spill loads
ptxas info    : Used 255 registers, 696 bytes cmem[0]
ptxas info    : Compiling entry function '_Z44flash_bwd_dq_dk_dv_loop_seqk_parallel_kernelI23Flash_bwd_kernel_traitsILi256ELi64ELi64ELi8ELi4ELi2ELi2ELb0ELb1EN7cutlass6half_tE19Flash_kernel_traitsILi256ELi64ELi64ELi8ES2_EELb1ELb0ELb1ELb1EEv16Flash_bwd_params' for 'sm_80'
ptxas info    : Function properties for _Z44flash_bwd_dq_dk_dv_loop_seqk_parallel_kernelI23Flash_bwd_kernel_traitsILi256ELi64ELi64ELi8ELi4ELi2ELi2ELb0ELb1EN7cutlass6half_tE19Flash_kernel_traitsILi256ELi64ELi64ELi8ES2_EELb1ELb0ELb1ELb1EEv16Flash_bwd_params
    24 bytes stack frame, 20 bytes spill stores, 20 bytes spill loads
ptxas info    : Used 255 registers, 696 bytes cmem[0]
ptxas info    : Compiling entry function '_Z44flash_bwd_dq_dk_dv_loop_seqk_parallel_kernelI23Flash_bwd_kernel_traitsILi256ELi64ELi64ELi8ELi4ELi2ELi2ELb0ELb1EN7cutlass6half_tE19Flash_kernel_traitsILi256ELi64ELi64ELi8ES2_EELb1ELb1ELb0ELb0EEv16Flash_bwd_params' for 'sm_80'
ptxas info    : Function properties for _Z44flash_bwd_dq_dk_dv_loop_seqk_parallel_kernelI23Flash_bwd_kernel_traitsILi256ELi64ELi64ELi8ELi4ELi2ELi2ELb0ELb1EN7cutlass6half_tE19Flash_kernel_traitsILi256ELi64ELi64ELi8ES2_EELb1ELb1ELb0ELb0EEv16Flash_bwd_params
    48 bytes stack frame, 44 bytes spill stores, 44 bytes spill loads
ptxas info    : Used 255 registers, 696 bytes cmem[0]
ptxas info    : Compiling entry function '_Z44flash_bwd_dq_dk_dv_loop_seqk_parallel_kernelI23Flash_bwd_kernel_traitsILi256ELi64ELi64ELi8ELi4ELi2ELi2ELb0ELb1EN7cutlass6half_tE19Flash_kernel_traitsILi256ELi64ELi64ELi8ES2_EELb1ELb1ELb0ELb1EEv16Flash_bwd_params' for 'sm_80'
ptxas info    : Function properties for _Z44flash_bwd_dq_dk_dv_loop_seqk_parallel_kernelI23Flash_bwd_kernel_traitsILi256ELi64ELi64ELi8ELi4ELi2ELi2ELb0ELb1EN7cutlass6half_tE19Flash_kernel_traitsILi256ELi64ELi64ELi8ES2_EELb1ELb1ELb0ELb1EEv16Flash_bwd_params
    8 bytes stack frame, 8 bytes spill stores, 8 bytes spill loads
ptxas info    : Used 255 registers, 696 bytes cmem[0]
ptxas info    : Compiling entry function '_Z44flash_bwd_dq_dk_dv_loop_seqk_parallel_kernelI23Flash_bwd_kernel_traitsILi256ELi64ELi64ELi8ELi4ELi2ELi2ELb0ELb1EN7cutlass6half_tE19Flash_kernel_traitsILi256ELi64ELi64ELi8ES2_EELb1ELb1ELb1ELb0EEv16Flash_bwd_params' for 'sm_80'
ptxas info    : Function properties for _Z44flash_bwd_dq_dk_dv_loop_seqk_parallel_kernelI23Flash_bwd_kernel_traitsILi256ELi64ELi64ELi8ELi4ELi2ELi2ELb0ELb1EN7cutlass6half_tE19Flash_kernel_traitsILi256ELi64ELi64ELi8ES2_EELb1ELb1ELb1ELb0EEv16Flash_bwd_params
    8 bytes stack frame, 4 bytes spill stores, 4 bytes spill loads
ptxas info    : Used 255 registers, 696 bytes cmem[0]
ptxas info    : Compiling entry function '_Z44flash_bwd_dq_dk_dv_loop_seqk_parallel_kernelI23Flash_bwd_kernel_traitsILi256ELi64ELi64ELi8ELi4ELi2ELi2ELb0ELb1EN7cutlass6half_tE19Flash_kernel_traitsILi256ELi64ELi64ELi8ES2_EELb1ELb1ELb1ELb1EEv16Flash_bwd_params' for 'sm_80'
ptxas info    : Function properties for _Z44flash_bwd_dq_dk_dv_loop_seqk_parallel_kernelI23Flash_bwd_kernel_traitsILi256ELi64ELi64ELi8ELi4ELi2ELi2ELb0ELb1EN7cutlass6half_tE19Flash_kernel_traitsILi256ELi64ELi64ELi8ES2_EELb1ELb1ELb1ELb1EEv16Flash_bwd_params
    8 bytes stack frame, 4 bytes spill stores, 4 bytes spill loads
ptxas info    : Used 255 registers, 696 bytes cmem[0]
ptxas info    : Compiling entry function '_Z25flash_bwd_dot_do_o_kernelILb1E23Flash_bwd_kernel_traitsILi256ELi64ELi64ELi8ELi4ELi2ELi2ELb0ELb1EN7cutlass6half_tE19Flash_kernel_traitsILi256ELi64ELi64ELi8ES2_EEEv16Flash_bwd_params' for 'sm_80'
ptxas info    : Function properties for _Z25flash_bwd_dot_do_o_kernelILb1E23Flash_bwd_kernel_traitsILi256ELi64ELi64ELi8ELi4ELi2ELi2ELb0ELb1EN7cutlass6half_tE19Flash_kernel_traitsILi256ELi64ELi64ELi8ES2_EEEv16Flash_bwd_params
    0 bytes stack frame, 0 bytes spill stores, 0 bytes spill loads
ptxas info    : Used 92 registers, 696 bytes cmem[0]
ptxas info    : Compiling entry function '_Z27flash_bwd_convert_dq_kernelI23Flash_bwd_kernel_traitsILi256ELi64ELi64ELi8ELi4ELi2ELi2ELb0ELb0EN7cutlass6half_tE19Flash_kernel_traitsILi256ELi64ELi64ELi8ES2_EEEv16Flash_bwd_params' for 'sm_80'
ptxas info    : Function properties for _Z27flash_bwd_convert_dq_kernelI23Flash_bwd_kernel_traitsILi256ELi64ELi64ELi8ELi4ELi2ELi2ELb0ELb0EN7cutlass6half_tE19Flash_kernel_traitsILi256ELi64ELi64ELi8ES2_EEEv16Flash_bwd_params
    0 bytes stack frame, 0 bytes spill stores, 0 bytes spill loads
ptxas info    : Used 40 registers, 696 bytes cmem[0]
ptxas info    : Compiling entry function '_Z44flash_bwd_dq_dk_dv_loop_seqk_parallel_kernelI23Flash_bwd_kernel_traitsILi256ELi64ELi64ELi8ELi4ELi2ELi2ELb0ELb0EN7cutlass6half_tE19Flash_kernel_traitsILi256ELi64ELi64ELi8ES2_EELb1ELb0ELb0ELb0EEv16Flash_bwd_params' for 'sm_80'
ptxas info    : Function properties for _Z44flash_bwd_dq_dk_dv_loop_seqk_parallel_kernelI23Flash_bwd_kernel_traitsILi256ELi64ELi64ELi8ELi4ELi2ELi2ELb0ELb0EN7cutlass6half_tE19Flash_kernel_traitsILi256ELi64ELi64ELi8ES2_EELb1ELb0ELb0ELb0EEv16Flash_bwd_params
    24 bytes stack frame, 20 bytes spill stores, 20 bytes spill loads
ptxas info    : Used 255 registers, 696 bytes cmem[0]
ptxas info    : Compiling entry function '_Z44flash_bwd_dq_dk_dv_loop_seqk_parallel_kernelI23Flash_bwd_kernel_traitsILi256ELi64ELi64ELi8ELi4ELi2ELi2ELb0ELb0EN7cutlass6half_tE19Flash_kernel_traitsILi256ELi64ELi64ELi8ES2_EELb1ELb0ELb0ELb1EEv16Flash_bwd_params' for 'sm_80'
ptxas info    : Function properties for _Z44flash_bwd_dq_dk_dv_loop_seqk_parallel_kernelI23Flash_bwd_kernel_traitsILi256ELi64ELi64ELi8ELi4ELi2ELi2ELb0ELb0EN7cutlass6half_tE19Flash_kernel_traitsILi256ELi64ELi64ELi8ES2_EELb1ELb0ELb0ELb1EEv16Flash_bwd_params
    8 bytes stack frame, 8 bytes spill stores, 8 bytes spill loads
ptxas info    : Used 255 registers, 696 bytes cmem[0]
ptxas info    : Compiling entry function '_Z44flash_bwd_dq_dk_dv_loop_seqk_parallel_kernelI23Flash_bwd_kernel_traitsILi256ELi64ELi64ELi8ELi4ELi2ELi2ELb0ELb0EN7cutlass6half_tE19Flash_kernel_traitsILi256ELi64ELi64ELi8ES2_EELb1ELb0ELb1ELb0EEv16Flash_bwd_params' for 'sm_80'
ptxas info    : Function properties for _Z44flash_bwd_dq_dk_dv_loop_seqk_parallel_kernelI23Flash_bwd_kernel_traitsILi256ELi64ELi64ELi8ELi4ELi2ELi2ELb0ELb0EN7cutlass6half_tE19Flash_kernel_traitsILi256ELi64ELi64ELi8ES2_EELb1ELb0ELb1ELb0EEv16Flash_bwd_params
    24 bytes stack frame, 20 bytes spill stores, 20 bytes spill loads
ptxas info    : Used 255 registers, 696 bytes cmem[0]
ptxas info    : Compiling entry function '_Z44flash_bwd_dq_dk_dv_loop_seqk_parallel_kernelI23Flash_bwd_kernel_traitsILi256ELi64ELi64ELi8ELi4ELi2ELi2ELb0ELb0EN7cutlass6half_tE19Flash_kernel_traitsILi256ELi64ELi64ELi8ES2_EELb1ELb0ELb1ELb1EEv16Flash_bwd_params' for 'sm_80'
ptxas info    : Function properties for _Z44flash_bwd_dq_dk_dv_loop_seqk_parallel_kernelI23Flash_bwd_kernel_traitsILi256ELi64ELi64ELi8ELi4ELi2ELi2ELb0ELb0EN7cutlass6half_tE19Flash_kernel_traitsILi256ELi64ELi64ELi8ES2_EELb1ELb0ELb1ELb1EEv16Flash_bwd_params
    0 bytes stack frame, 0 bytes spill stores, 0 bytes spill loads
ptxas info    : Used 255 registers, 696 bytes cmem[0]
ptxas info    : Compiling entry function '_Z44flash_bwd_dq_dk_dv_loop_seqk_parallel_kernelI23Flash_bwd_kernel_traitsILi256ELi64ELi64ELi8ELi4ELi2ELi2ELb0ELb0EN7cutlass6half_tE19Flash_kernel_traitsILi256ELi64ELi64ELi8ES2_EELb1ELb1ELb0ELb0EEv16Flash_bwd_params' for 'sm_80'
ptxas info    : Function properties for _Z44flash_bwd_dq_dk_dv_loop_seqk_parallel_kernelI23Flash_bwd_kernel_traitsILi256ELi64ELi64ELi8ELi4ELi2ELi2ELb0ELb0EN7cutlass6half_tE19Flash_kernel_traitsILi256ELi64ELi64ELi8ES2_EELb1ELb1ELb0ELb0EEv16Flash_bwd_params
    32 bytes stack frame, 28 bytes spill stores, 28 bytes spill loads
ptxas info    : Used 255 registers, 696 bytes cmem[0]
ptxas info    : Compiling entry function '_Z44flash_bwd_dq_dk_dv_loop_seqk_parallel_kernelI23Flash_bwd_kernel_traitsILi256ELi64ELi64ELi8ELi4ELi2ELi2ELb0ELb0EN7cutlass6half_tE19Flash_kernel_traitsILi256ELi64ELi64ELi8ES2_EELb1ELb1ELb0ELb1EEv16Flash_bwd_params' for 'sm_80'
ptxas info    : Function properties for _Z44flash_bwd_dq_dk_dv_loop_seqk_parallel_kernelI23Flash_bwd_kernel_traitsILi256ELi64ELi64ELi8ELi4ELi2ELi2ELb0ELb0EN7cutlass6half_tE19Flash_kernel_traitsILi256ELi64ELi64ELi8ES2_EELb1ELb1ELb0ELb1EEv16Flash_bwd_params
    8 bytes stack frame, 8 bytes spill stores, 8 bytes spill loads
ptxas info    : Used 255 registers, 696 bytes cmem[0]
ptxas info    : Compiling entry function '_Z44flash_bwd_dq_dk_dv_loop_seqk_parallel_kernelI23Flash_bwd_kernel_traitsILi256ELi64ELi64ELi8ELi4ELi2ELi2ELb0ELb0EN7cutlass6half_tE19Flash_kernel_traitsILi256ELi64ELi64ELi8ES2_EELb1ELb1ELb1ELb0EEv16Flash_bwd_params' for 'sm_80'
ptxas info    : Function properties for _Z44flash_bwd_dq_dk_dv_loop_seqk_parallel_kernelI23Flash_bwd_kernel_traitsILi256ELi64ELi64ELi8ELi4ELi2ELi2ELb0ELb0EN7cutlass6half_tE19Flash_kernel_traitsILi256ELi64ELi64ELi8ES2_EELb1ELb1ELb1ELb0EEv16Flash_bwd_params
    8 bytes stack frame, 8 bytes spill stores, 8 bytes spill loads
ptxas info    : Used 255 registers, 696 bytes cmem[0]
ptxas info    : Compiling entry function '_Z44flash_bwd_dq_dk_dv_loop_seqk_parallel_kernelI23Flash_bwd_kernel_traitsILi256ELi64ELi64ELi8ELi4ELi2ELi2ELb0ELb0EN7cutlass6half_tE19Flash_kernel_traitsILi256ELi64ELi64ELi8ES2_EELb1ELb1ELb1ELb1EEv16Flash_bwd_params' for 'sm_80'
ptxas info    : Function properties for _Z44flash_bwd_dq_dk_dv_loop_seqk_parallel_kernelI23Flash_bwd_kernel_traitsILi256ELi64ELi64ELi8ELi4ELi2ELi2ELb0ELb0EN7cutlass6half_tE19Flash_kernel_traitsILi256ELi64ELi64ELi8ES2_EELb1ELb1ELb1ELb1EEv16Flash_bwd_params
    8 bytes stack frame, 8 bytes spill stores, 8 bytes spill loads
ptxas info    : Used 255 registers, 696 bytes cmem[0]
ptxas info    : Compiling entry function '_Z25flash_bwd_dot_do_o_kernelILb1E23Flash_bwd_kernel_traitsILi256ELi64ELi64ELi8ELi4ELi2ELi2ELb0ELb0EN7cutlass6half_tE19Flash_kernel_traitsILi256ELi64ELi64ELi8ES2_EEEv16Flash_bwd_params' for 'sm_80'
ptxas info    : Function properties for _Z25flash_bwd_dot_do_o_kernelILb1E23Flash_bwd_kernel_traitsILi256ELi64ELi64ELi8ELi4ELi2ELi2ELb0ELb0EN7cutlass6half_tE19Flash_kernel_traitsILi256ELi64ELi64ELi8ES2_EEEv16Flash_bwd_params
    0 bytes stack frame, 0 bytes spill stores, 0 bytes spill loads
ptxas info    : Used 92 registers, 696 bytes cmem[0]
/anaconda/envs/flashattention/lib/python3.9/site-packages/torch/include/c10/core/SymInt.h(84): warning: integer conversion resulted in a change of sign

[18/36] /usr/local/cuda-11.4/bin/nvcc  -I/home/v-feiychen/flash-attention/csrc/flash_attn -I/home/v-feiychen/flash-attention/csrc/flash_attn/src -I/home/v-feiychen/flash-attention/csrc/cutlass/include -I/anaconda/envs/flashattention/lib/python3.9/site-packages/torch/include -I/anaconda/envs/flashattention/lib/python3.9/site-packages/torch/include/torch/csrc/api/include -I/anaconda/envs/flashattention/lib/python3.9/site-packages/torch/include/TH -I/anaconda/envs/flashattention/lib/python3.9/site-packages/torch/include/THC -I/usr/local/cuda-11.4/include -I/anaconda/envs/flashattention/include/python3.9 -c -c /home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_bwd_hdim256_bf16_sm80.cu -o /home/v-feiychen/flash-attention/build/temp.linux-x86_64-cpython-39/csrc/flash_attn/src/flash_bwd_hdim256_bf16_sm80.o -D__CUDA_NO_HALF_OPERATORS__ -D__CUDA_NO_HALF_CONVERSIONS__ -D__CUDA_NO_BFLOAT16_CONVERSIONS__ -D__CUDA_NO_HALF2_OPERATORS__ --expt-relaxed-constexpr --compiler-options ''"'"'-fPIC'"'"'' -O3 -std=c++17 -U__CUDA_NO_HALF_OPERATORS__ -U__CUDA_NO_HALF_CONVERSIONS__ -U__CUDA_NO_HALF2_OPERATORS__ -U__CUDA_NO_BFLOAT16_CONVERSIONS__ --expt-relaxed-constexpr --expt-extended-lambda --use_fast_math --ptxas-options=-v -lineinfo -gencode arch=compute_80,code=sm_80 --threads 4 -DTORCH_API_INCLUDE_EXTENSION_H '-DPYBIND11_COMPILER_TYPE="_gcc"' '-DPYBIND11_STDLIB="_libstdcpp"' '-DPYBIND11_BUILD_ABI="_cxxabi1011"' -DTORCH_EXTENSION_NAME=flash_attn_2_cuda -D_GLIBCXX_USE_CXX11_ABI=0
/anaconda/envs/flashattention/lib/python3.9/site-packages/torch/include/c10/core/SymInt.h(84): warning: integer conversion resulted in a change of sign

ptxas info    : 2 bytes gmem
ptxas info    : Compiling entry function '_Z44flash_bwd_dq_dk_dv_loop_seqk_parallel_kernelI23Flash_bwd_kernel_traitsILi256ELi64ELi64ELi8ELi4ELi2ELi2ELb0ELb1EN7cutlass10bfloat16_tE19Flash_kernel_traitsILi256ELi64ELi64ELi8ES2_EELb0ELb0ELb0ELb0EEv16Flash_bwd_params' for 'sm_80'
ptxas info    : Function properties for _Z44flash_bwd_dq_dk_dv_loop_seqk_parallel_kernelI23Flash_bwd_kernel_traitsILi256ELi64ELi64ELi8ELi4ELi2ELi2ELb0ELb1EN7cutlass10bfloat16_tE19Flash_kernel_traitsILi256ELi64ELi64ELi8ES2_EELb0ELb0ELb0ELb0EEv16Flash_bwd_params
    16 bytes stack frame, 12 bytes spill stores, 12 bytes spill loads
ptxas info    : Used 255 registers, 696 bytes cmem[0]
ptxas info    : Compiling entry function '_Z44flash_bwd_dq_dk_dv_loop_seqk_parallel_kernelI23Flash_bwd_kernel_traitsILi256ELi64ELi64ELi8ELi4ELi2ELi2ELb0ELb1EN7cutlass10bfloat16_tE19Flash_kernel_traitsILi256ELi64ELi64ELi8ES2_EELb0ELb0ELb0ELb1EEv16Flash_bwd_params' for 'sm_80'
ptxas info    : Function properties for _Z44flash_bwd_dq_dk_dv_loop_seqk_parallel_kernelI23Flash_bwd_kernel_traitsILi256ELi64ELi64ELi8ELi4ELi2ELi2ELb0ELb1EN7cutlass10bfloat16_tE19Flash_kernel_traitsILi256ELi64ELi64ELi8ES2_EELb0ELb0ELb0ELb1EEv16Flash_bwd_params
    0 bytes stack frame, 0 bytes spill stores, 0 bytes spill loads
ptxas info    : Used 252 registers, 696 bytes cmem[0]
ptxas info    : Compiling entry function '_Z44flash_bwd_dq_dk_dv_loop_seqk_parallel_kernelI23Flash_bwd_kernel_traitsILi256ELi64ELi64ELi8ELi4ELi2ELi2ELb0ELb1EN7cutlass10bfloat16_tE19Flash_kernel_traitsILi256ELi64ELi64ELi8ES2_EELb0ELb0ELb1ELb0EEv16Flash_bwd_params' for 'sm_80'
ptxas info    : Function properties for _Z44flash_bwd_dq_dk_dv_loop_seqk_parallel_kernelI23Flash_bwd_kernel_traitsILi256ELi64ELi64ELi8ELi4ELi2ELi2ELb0ELb1EN7cutlass10bfloat16_tE19Flash_kernel_traitsILi256ELi64ELi64ELi8ES2_EELb0ELb0ELb1ELb0EEv16Flash_bwd_params
    0 bytes stack frame, 0 bytes spill stores, 0 bytes spill loads
ptxas info    : Used 254 registers, 696 bytes cmem[0]
ptxas info    : Compiling entry function '_Z44flash_bwd_dq_dk_dv_loop_seqk_parallel_kernelI23Flash_bwd_kernel_traitsILi256ELi64ELi64ELi8ELi4ELi2ELi2ELb0ELb1EN7cutlass10bfloat16_tE19Flash_kernel_traitsILi256ELi64ELi64ELi8ES2_EELb0ELb0ELb1ELb1EEv16Flash_bwd_params' for 'sm_80'
ptxas info    : Function properties for _Z44flash_bwd_dq_dk_dv_loop_seqk_parallel_kernelI23Flash_bwd_kernel_traitsILi256ELi64ELi64ELi8ELi4ELi2ELi2ELb0ELb1EN7cutlass10bfloat16_tE19Flash_kernel_traitsILi256ELi64ELi64ELi8ES2_EELb0ELb0ELb1ELb1EEv16Flash_bwd_params
    8 bytes stack frame, 4 bytes spill stores, 4 bytes spill loads
ptxas info    : Used 255 registers, 696 bytes cmem[0]
ptxas info    : Compiling entry function '_Z44flash_bwd_dq_dk_dv_loop_seqk_parallel_kernelI23Flash_bwd_kernel_traitsILi256ELi64ELi64ELi8ELi4ELi2ELi2ELb0ELb1EN7cutlass10bfloat16_tE19Flash_kernel_traitsILi256ELi64ELi64ELi8ES2_EELb0ELb1ELb0ELb0EEv16Flash_bwd_params' for 'sm_80'
ptxas info    : Function properties for _Z44flash_bwd_dq_dk_dv_loop_seqk_parallel_kernelI23Flash_bwd_kernel_traitsILi256ELi64ELi64ELi8ELi4ELi2ELi2ELb0ELb1EN7cutlass10bfloat16_tE19Flash_kernel_traitsILi256ELi64ELi64ELi8ES2_EELb0ELb1ELb0ELb0EEv16Flash_bwd_params
    16 bytes stack frame, 12 bytes spill stores, 12 bytes spill loads
ptxas info    : Used 255 registers, 696 bytes cmem[0]
ptxas info    : Compiling entry function '_Z44flash_bwd_dq_dk_dv_loop_seqk_parallel_kernelI23Flash_bwd_kernel_traitsILi256ELi64ELi64ELi8ELi4ELi2ELi2ELb0ELb1EN7cutlass10bfloat16_tE19Flash_kernel_traitsILi256ELi64ELi64ELi8ES2_EELb0ELb1ELb0ELb1EEv16Flash_bwd_params' for 'sm_80'
ptxas info    : Function properties for _Z44flash_bwd_dq_dk_dv_loop_seqk_parallel_kernelI23Flash_bwd_kernel_traitsILi256ELi64ELi64ELi8ELi4ELi2ELi2ELb0ELb1EN7cutlass10bfloat16_tE19Flash_kernel_traitsILi256ELi64ELi64ELi8ES2_EELb0ELb1ELb0ELb1EEv16Flash_bwd_params
    8 bytes stack frame, 8 bytes spill stores, 8 bytes spill loads
ptxas info    : Used 255 registers, 696 bytes cmem[0]
ptxas info    : Compiling entry function '_Z44flash_bwd_dq_dk_dv_loop_seqk_parallel_kernelI23Flash_bwd_kernel_traitsILi256ELi64ELi64ELi8ELi4ELi2ELi2ELb0ELb1EN7cutlass10bfloat16_tE19Flash_kernel_traitsILi256ELi64ELi64ELi8ES2_EELb0ELb1ELb1ELb0EEv16Flash_bwd_params' for 'sm_80'
ptxas info    : Function properties for _Z44flash_bwd_dq_dk_dv_loop_seqk_parallel_kernelI23Flash_bwd_kernel_traitsILi256ELi64ELi64ELi8ELi4ELi2ELi2ELb0ELb1EN7cutlass10bfloat16_tE19Flash_kernel_traitsILi256ELi64ELi64ELi8ES2_EELb0ELb1ELb1ELb0EEv16Flash_bwd_params
    0 bytes stack frame, 0 bytes spill stores, 0 bytes spill loads
ptxas info    : Used 252 registers, 696 bytes cmem[0]
ptxas info    : Compiling entry function '_Z44flash_bwd_dq_dk_dv_loop_seqk_parallel_kernelI23Flash_bwd_kernel_traitsILi256ELi64ELi64ELi8ELi4ELi2ELi2ELb0ELb1EN7cutlass10bfloat16_tE19Flash_kernel_traitsILi256ELi64ELi64ELi8ES2_EELb0ELb1ELb1ELb1EEv16Flash_bwd_params' for 'sm_80'
ptxas info    : Function properties for _Z44flash_bwd_dq_dk_dv_loop_seqk_parallel_kernelI23Flash_bwd_kernel_traitsILi256ELi64ELi64ELi8ELi4ELi2ELi2ELb0ELb1EN7cutlass10bfloat16_tE19Flash_kernel_traitsILi256ELi64ELi64ELi8ES2_EELb0ELb1ELb1ELb1EEv16Flash_bwd_params
    0 bytes stack frame, 0 bytes spill stores, 0 bytes spill loads
ptxas info    : Used 255 registers, 696 bytes cmem[0]
ptxas info    : Compiling entry function '_Z44flash_bwd_dq_dk_dv_loop_seqk_parallel_kernelI23Flash_bwd_kernel_traitsILi256ELi64ELi64ELi8ELi4ELi2ELi2ELb0ELb0EN7cutlass10bfloat16_tE19Flash_kernel_traitsILi256ELi64ELi64ELi8ES2_EELb0ELb0ELb0ELb0EEv16Flash_bwd_params' for 'sm_80'
ptxas info    : Function properties for _Z44flash_bwd_dq_dk_dv_loop_seqk_parallel_kernelI23Flash_bwd_kernel_traitsILi256ELi64ELi64ELi8ELi4ELi2ELi2ELb0ELb0EN7cutlass10bfloat16_tE19Flash_kernel_traitsILi256ELi64ELi64ELi8ES2_EELb0ELb0ELb0ELb0EEv16Flash_bwd_params
    8 bytes stack frame, 8 bytes spill stores, 8 bytes spill loads
ptxas info    : Used 255 registers, 696 bytes cmem[0]
ptxas info    : Compiling entry function '_Z44flash_bwd_dq_dk_dv_loop_seqk_parallel_kernelI23Flash_bwd_kernel_traitsILi256ELi64ELi64ELi8ELi4ELi2ELi2ELb0ELb0EN7cutlass10bfloat16_tE19Flash_kernel_traitsILi256ELi64ELi64ELi8ES2_EELb0ELb0ELb0ELb1EEv16Flash_bwd_params' for 'sm_80'
ptxas info    : Function properties for _Z44flash_bwd_dq_dk_dv_loop_seqk_parallel_kernelI23Flash_bwd_kernel_traitsILi256ELi64ELi64ELi8ELi4ELi2ELi2ELb0ELb0EN7cutlass10bfloat16_tE19Flash_kernel_traitsILi256ELi64ELi64ELi8ES2_EELb0ELb0ELb0ELb1EEv16Flash_bwd_params
    16 bytes stack frame, 12 bytes spill stores, 12 bytes spill loads
ptxas info    : Used 255 registers, 696 bytes cmem[0]
ptxas info    : Compiling entry function '_Z44flash_bwd_dq_dk_dv_loop_seqk_parallel_kernelI23Flash_bwd_kernel_traitsILi256ELi64ELi64ELi8ELi4ELi2ELi2ELb0ELb0EN7cutlass10bfloat16_tE19Flash_kernel_traitsILi256ELi64ELi64ELi8ES2_EELb0ELb0ELb1ELb0EEv16Flash_bwd_params' for 'sm_80'
ptxas info    : Function properties for _Z44flash_bwd_dq_dk_dv_loop_seqk_parallel_kernelI23Flash_bwd_kernel_traitsILi256ELi64ELi64ELi8ELi4ELi2ELi2ELb0ELb0EN7cutlass10bfloat16_tE19Flash_kernel_traitsILi256ELi64ELi64ELi8ES2_EELb0ELb0ELb1ELb0EEv16Flash_bwd_params
    0 bytes stack frame, 0 bytes spill stores, 0 bytes spill loads
ptxas info    : Used 254 registers, 696 bytes cmem[0]
ptxas info    : Compiling entry function '_Z44flash_bwd_dq_dk_dv_loop_seqk_parallel_kernelI23Flash_bwd_kernel_traitsILi256ELi64ELi64ELi8ELi4ELi2ELi2ELb0ELb0EN7cutlass10bfloat16_tE19Flash_kernel_traitsILi256ELi64ELi64ELi8ES2_EELb0ELb0ELb1ELb1EEv16Flash_bwd_params' for 'sm_80'
ptxas info    : Function properties for _Z44flash_bwd_dq_dk_dv_loop_seqk_parallel_kernelI23Flash_bwd_kernel_traitsILi256ELi64ELi64ELi8ELi4ELi2ELi2ELb0ELb0EN7cutlass10bfloat16_tE19Flash_kernel_traitsILi256ELi64ELi64ELi8ES2_EELb0ELb0ELb1ELb1EEv16Flash_bwd_params
    0 bytes stack frame, 0 bytes spill stores, 0 bytes spill loads
ptxas info    : Used 252 registers, 696 bytes cmem[0]
ptxas info    : Compiling entry function '_Z44flash_bwd_dq_dk_dv_loop_seqk_parallel_kernelI23Flash_bwd_kernel_traitsILi256ELi64ELi64ELi8ELi4ELi2ELi2ELb0ELb0EN7cutlass10bfloat16_tE19Flash_kernel_traitsILi256ELi64ELi64ELi8ES2_EELb0ELb1ELb0ELb0EEv16Flash_bwd_params' for 'sm_80'
ptxas info    : Function properties for _Z44flash_bwd_dq_dk_dv_loop_seqk_parallel_kernelI23Flash_bwd_kernel_traitsILi256ELi64ELi64ELi8ELi4ELi2ELi2ELb0ELb0EN7cutlass10bfloat16_tE19Flash_kernel_traitsILi256ELi64ELi64ELi8ES2_EELb0ELb1ELb0ELb0EEv16Flash_bwd_params
    8 bytes stack frame, 8 bytes spill stores, 8 bytes spill loads
ptxas info    : Used 255 registers, 696 bytes cmem[0]
ptxas info    : Compiling entry function '_Z44flash_bwd_dq_dk_dv_loop_seqk_parallel_kernelI23Flash_bwd_kernel_traitsILi256ELi64ELi64ELi8ELi4ELi2ELi2ELb0ELb0EN7cutlass10bfloat16_tE19Flash_kernel_traitsILi256ELi64ELi64ELi8ES2_EELb0ELb1ELb0ELb1EEv16Flash_bwd_params' for 'sm_80'
ptxas info    : Function properties for _Z44flash_bwd_dq_dk_dv_loop_seqk_parallel_kernelI23Flash_bwd_kernel_traitsILi256ELi64ELi64ELi8ELi4ELi2ELi2ELb0ELb0EN7cutlass10bfloat16_tE19Flash_kernel_traitsILi256ELi64ELi64ELi8ES2_EELb0ELb1ELb0ELb1EEv16Flash_bwd_params
    16 bytes stack frame, 12 bytes spill stores, 12 bytes spill loads
ptxas info    : Used 255 registers, 696 bytes cmem[0]
ptxas info    : Compiling entry function '_Z44flash_bwd_dq_dk_dv_loop_seqk_parallel_kernelI23Flash_bwd_kernel_traitsILi256ELi64ELi64ELi8ELi4ELi2ELi2ELb0ELb0EN7cutlass10bfloat16_tE19Flash_kernel_traitsILi256ELi64ELi64ELi8ES2_EELb0ELb1ELb1ELb0EEv16Flash_bwd_params' for 'sm_80'
ptxas info    : Function properties for _Z44flash_bwd_dq_dk_dv_loop_seqk_parallel_kernelI23Flash_bwd_kernel_traitsILi256ELi64ELi64ELi8ELi4ELi2ELi2ELb0ELb0EN7cutlass10bfloat16_tE19Flash_kernel_traitsILi256ELi64ELi64ELi8ES2_EELb0ELb1ELb1ELb0EEv16Flash_bwd_params
    0 bytes stack frame, 0 bytes spill stores, 0 bytes spill loads
ptxas info    : Used 254 registers, 696 bytes cmem[0]
ptxas info    : Compiling entry function '_Z44flash_bwd_dq_dk_dv_loop_seqk_parallel_kernelI23Flash_bwd_kernel_traitsILi256ELi64ELi64ELi8ELi4ELi2ELi2ELb0ELb0EN7cutlass10bfloat16_tE19Flash_kernel_traitsILi256ELi64ELi64ELi8ES2_EELb0ELb1ELb1ELb1EEv16Flash_bwd_params' for 'sm_80'
ptxas info    : Function properties for _Z44flash_bwd_dq_dk_dv_loop_seqk_parallel_kernelI23Flash_bwd_kernel_traitsILi256ELi64ELi64ELi8ELi4ELi2ELi2ELb0ELb0EN7cutlass10bfloat16_tE19Flash_kernel_traitsILi256ELi64ELi64ELi8ES2_EELb0ELb1ELb1ELb1EEv16Flash_bwd_params
    0 bytes stack frame, 0 bytes spill stores, 0 bytes spill loads
ptxas info    : Used 255 registers, 696 bytes cmem[0]
ptxas info    : Compiling entry function '_Z27flash_bwd_convert_dq_kernelI23Flash_bwd_kernel_traitsILi256ELi64ELi64ELi8ELi4ELi2ELi2ELb0ELb1EN7cutlass10bfloat16_tE19Flash_kernel_traitsILi256ELi64ELi64ELi8ES2_EEEv16Flash_bwd_params' for 'sm_80'
ptxas info    : Function properties for _Z27flash_bwd_convert_dq_kernelI23Flash_bwd_kernel_traitsILi256ELi64ELi64ELi8ELi4ELi2ELi2ELb0ELb1EN7cutlass10bfloat16_tE19Flash_kernel_traitsILi256ELi64ELi64ELi8ES2_EEEv16Flash_bwd_params
    0 bytes stack frame, 0 bytes spill stores, 0 bytes spill loads
ptxas info    : Used 40 registers, 696 bytes cmem[0]
ptxas info    : Compiling entry function '_Z44flash_bwd_dq_dk_dv_loop_seqk_parallel_kernelI23Flash_bwd_kernel_traitsILi256ELi64ELi64ELi8ELi4ELi2ELi2ELb0ELb1EN7cutlass10bfloat16_tE19Flash_kernel_traitsILi256ELi64ELi64ELi8ES2_EELb1ELb0ELb0ELb0EEv16Flash_bwd_params' for 'sm_80'
ptxas info    : Function properties for _Z44flash_bwd_dq_dk_dv_loop_seqk_parallel_kernelI23Flash_bwd_kernel_traitsILi256ELi64ELi64ELi8ELi4ELi2ELi2ELb0ELb1EN7cutlass10bfloat16_tE19Flash_kernel_traitsILi256ELi64ELi64ELi8ES2_EELb1ELb0ELb0ELb0EEv16Flash_bwd_params
    16 bytes stack frame, 12 bytes spill stores, 12 bytes spill loads
ptxas info    : Used 255 registers, 696 bytes cmem[0]
ptxas info    : Compiling entry function '_Z44flash_bwd_dq_dk_dv_loop_seqk_parallel_kernelI23Flash_bwd_kernel_traitsILi256ELi64ELi64ELi8ELi4ELi2ELi2ELb0ELb1EN7cutlass10bfloat16_tE19Flash_kernel_traitsILi256ELi64ELi64ELi8ES2_EELb1ELb0ELb0ELb1EEv16Flash_bwd_params' for 'sm_80'
ptxas info    : Function properties for _Z44flash_bwd_dq_dk_dv_loop_seqk_parallel_kernelI23Flash_bwd_kernel_traitsILi256ELi64ELi64ELi8ELi4ELi2ELi2ELb0ELb1EN7cutlass10bfloat16_tE19Flash_kernel_traitsILi256ELi64ELi64ELi8ES2_EELb1ELb0ELb0ELb1EEv16Flash_bwd_params
    8 bytes stack frame, 4 bytes spill stores, 4 bytes spill loads
ptxas info    : Used 255 registers, 696 bytes cmem[0]
ptxas info    : Compiling entry function '_Z44flash_bwd_dq_dk_dv_loop_seqk_parallel_kernelI23Flash_bwd_kernel_traitsILi256ELi64ELi64ELi8ELi4ELi2ELi2ELb0ELb1EN7cutlass10bfloat16_tE19Flash_kernel_traitsILi256ELi64ELi64ELi8ES2_EELb1ELb0ELb1ELb0EEv16Flash_bwd_params' for 'sm_80'
ptxas info    : Function properties for _Z44flash_bwd_dq_dk_dv_loop_seqk_parallel_kernelI23Flash_bwd_kernel_traitsILi256ELi64ELi64ELi8ELi4ELi2ELi2ELb0ELb1EN7cutlass10bfloat16_tE19Flash_kernel_traitsILi256ELi64ELi64ELi8ES2_EELb1ELb0ELb1ELb0EEv16Flash_bwd_params
    24 bytes stack frame, 24 bytes spill stores, 24 bytes spill loads
ptxas info    : Used 255 registers, 696 bytes cmem[0]
ptxas info    : Compiling entry function '_Z44flash_bwd_dq_dk_dv_loop_seqk_parallel_kernelI23Flash_bwd_kernel_traitsILi256ELi64ELi64ELi8ELi4ELi2ELi2ELb0ELb1EN7cutlass10bfloat16_tE19Flash_kernel_traitsILi256ELi64ELi64ELi8ES2_EELb1ELb0ELb1ELb1EEv16Flash_bwd_params' for 'sm_80'
ptxas info    : Function properties for _Z44flash_bwd_dq_dk_dv_loop_seqk_parallel_kernelI23Flash_bwd_kernel_traitsILi256ELi64ELi64ELi8ELi4ELi2ELi2ELb0ELb1EN7cutlass10bfloat16_tE19Flash_kernel_traitsILi256ELi64ELi64ELi8ES2_EELb1ELb0ELb1ELb1EEv16Flash_bwd_params
    24 bytes stack frame, 20 bytes spill stores, 20 bytes spill loads
ptxas info    : Used 255 registers, 696 bytes cmem[0]
ptxas info    : Compiling entry function '_Z44flash_bwd_dq_dk_dv_loop_seqk_parallel_kernelI23Flash_bwd_kernel_traitsILi256ELi64ELi64ELi8ELi4ELi2ELi2ELb0ELb1EN7cutlass10bfloat16_tE19Flash_kernel_traitsILi256ELi64ELi64ELi8ES2_EELb1ELb1ELb0ELb0EEv16Flash_bwd_params' for 'sm_80'
ptxas info    : Function properties for _Z44flash_bwd_dq_dk_dv_loop_seqk_parallel_kernelI23Flash_bwd_kernel_traitsILi256ELi64ELi64ELi8ELi4ELi2ELi2ELb0ELb1EN7cutlass10bfloat16_tE19Flash_kernel_traitsILi256ELi64ELi64ELi8ES2_EELb1ELb1ELb0ELb0EEv16Flash_bwd_params
    48 bytes stack frame, 44 bytes spill stores, 44 bytes spill loads
ptxas info    : Used 255 registers, 696 bytes cmem[0]
ptxas info    : Compiling entry function '_Z44flash_bwd_dq_dk_dv_loop_seqk_parallel_kernelI23Flash_bwd_kernel_traitsILi256ELi64ELi64ELi8ELi4ELi2ELi2ELb0ELb1EN7cutlass10bfloat16_tE19Flash_kernel_traitsILi256ELi64ELi64ELi8ES2_EELb1ELb1ELb0ELb1EEv16Flash_bwd_params' for 'sm_80'
ptxas info    : Function properties for _Z44flash_bwd_dq_dk_dv_loop_seqk_parallel_kernelI23Flash_bwd_kernel_traitsILi256ELi64ELi64ELi8ELi4ELi2ELi2ELb0ELb1EN7cutlass10bfloat16_tE19Flash_kernel_traitsILi256ELi64ELi64ELi8ES2_EELb1ELb1ELb0ELb1EEv16Flash_bwd_params
    8 bytes stack frame, 8 bytes spill stores, 8 bytes spill loads
ptxas info    : Used 255 registers, 696 bytes cmem[0]
ptxas info    : Compiling entry function '_Z44flash_bwd_dq_dk_dv_loop_seqk_parallel_kernelI23Flash_bwd_kernel_traitsILi256ELi64ELi64ELi8ELi4ELi2ELi2ELb0ELb1EN7cutlass10bfloat16_tE19Flash_kernel_traitsILi256ELi64ELi64ELi8ES2_EELb1ELb1ELb1ELb0EEv16Flash_bwd_params' for 'sm_80'
ptxas info    : Function properties for _Z44flash_bwd_dq_dk_dv_loop_seqk_parallel_kernelI23Flash_bwd_kernel_traitsILi256ELi64ELi64ELi8ELi4ELi2ELi2ELb0ELb1EN7cutlass10bfloat16_tE19Flash_kernel_traitsILi256ELi64ELi64ELi8ES2_EELb1ELb1ELb1ELb0EEv16Flash_bwd_params
    8 bytes stack frame, 4 bytes spill stores, 4 bytes spill loads
ptxas info    : Used 255 registers, 696 bytes cmem[0]
ptxas info    : Compiling entry function '_Z44flash_bwd_dq_dk_dv_loop_seqk_parallel_kernelI23Flash_bwd_kernel_traitsILi256ELi64ELi64ELi8ELi4ELi2ELi2ELb0ELb1EN7cutlass10bfloat16_tE19Flash_kernel_traitsILi256ELi64ELi64ELi8ES2_EELb1ELb1ELb1ELb1EEv16Flash_bwd_params' for 'sm_80'
ptxas info    : Function properties for _Z44flash_bwd_dq_dk_dv_loop_seqk_parallel_kernelI23Flash_bwd_kernel_traitsILi256ELi64ELi64ELi8ELi4ELi2ELi2ELb0ELb1EN7cutlass10bfloat16_tE19Flash_kernel_traitsILi256ELi64ELi64ELi8ES2_EELb1ELb1ELb1ELb1EEv16Flash_bwd_params
    8 bytes stack frame, 4 bytes spill stores, 4 bytes spill loads
ptxas info    : Used 255 registers, 696 bytes cmem[0]
ptxas info    : Compiling entry function '_Z25flash_bwd_dot_do_o_kernelILb1E23Flash_bwd_kernel_traitsILi256ELi64ELi64ELi8ELi4ELi2ELi2ELb0ELb1EN7cutlass10bfloat16_tE19Flash_kernel_traitsILi256ELi64ELi64ELi8ES2_EEEv16Flash_bwd_params' for 'sm_80'
ptxas info    : Function properties for _Z25flash_bwd_dot_do_o_kernelILb1E23Flash_bwd_kernel_traitsILi256ELi64ELi64ELi8ELi4ELi2ELi2ELb0ELb1EN7cutlass10bfloat16_tE19Flash_kernel_traitsILi256ELi64ELi64ELi8ES2_EEEv16Flash_bwd_params
    0 bytes stack frame, 0 bytes spill stores, 0 bytes spill loads
ptxas info    : Used 92 registers, 696 bytes cmem[0]
ptxas info    : Compiling entry function '_Z27flash_bwd_convert_dq_kernelI23Flash_bwd_kernel_traitsILi256ELi64ELi64ELi8ELi4ELi2ELi2ELb0ELb0EN7cutlass10bfloat16_tE19Flash_kernel_traitsILi256ELi64ELi64ELi8ES2_EEEv16Flash_bwd_params' for 'sm_80'
ptxas info    : Function properties for _Z27flash_bwd_convert_dq_kernelI23Flash_bwd_kernel_traitsILi256ELi64ELi64ELi8ELi4ELi2ELi2ELb0ELb0EN7cutlass10bfloat16_tE19Flash_kernel_traitsILi256ELi64ELi64ELi8ES2_EEEv16Flash_bwd_params
    0 bytes stack frame, 0 bytes spill stores, 0 bytes spill loads
ptxas info    : Used 40 registers, 696 bytes cmem[0]
ptxas info    : Compiling entry function '_Z44flash_bwd_dq_dk_dv_loop_seqk_parallel_kernelI23Flash_bwd_kernel_traitsILi256ELi64ELi64ELi8ELi4ELi2ELi2ELb0ELb0EN7cutlass10bfloat16_tE19Flash_kernel_traitsILi256ELi64ELi64ELi8ES2_EELb1ELb0ELb0ELb0EEv16Flash_bwd_params' for 'sm_80'
ptxas info    : Function properties for _Z44flash_bwd_dq_dk_dv_loop_seqk_parallel_kernelI23Flash_bwd_kernel_traitsILi256ELi64ELi64ELi8ELi4ELi2ELi2ELb0ELb0EN7cutlass10bfloat16_tE19Flash_kernel_traitsILi256ELi64ELi64ELi8ES2_EELb1ELb0ELb0ELb0EEv16Flash_bwd_params
    24 bytes stack frame, 20 bytes spill stores, 20 bytes spill loads
ptxas info    : Used 255 registers, 696 bytes cmem[0]
ptxas info    : Compiling entry function '_Z44flash_bwd_dq_dk_dv_loop_seqk_parallel_kernelI23Flash_bwd_kernel_traitsILi256ELi64ELi64ELi8ELi4ELi2ELi2ELb0ELb0EN7cutlass10bfloat16_tE19Flash_kernel_traitsILi256ELi64ELi64ELi8ES2_EELb1ELb0ELb0ELb1EEv16Flash_bwd_params' for 'sm_80'
ptxas info    : Function properties for _Z44flash_bwd_dq_dk_dv_loop_seqk_parallel_kernelI23Flash_bwd_kernel_traitsILi256ELi64ELi64ELi8ELi4ELi2ELi2ELb0ELb0EN7cutlass10bfloat16_tE19Flash_kernel_traitsILi256ELi64ELi64ELi8ES2_EELb1ELb0ELb0ELb1EEv16Flash_bwd_params
    8 bytes stack frame, 8 bytes spill stores, 8 bytes spill loads
ptxas info    : Used 255 registers, 696 bytes cmem[0]
ptxas info    : Compiling entry function '_Z44flash_bwd_dq_dk_dv_loop_seqk_parallel_kernelI23Flash_bwd_kernel_traitsILi256ELi64ELi64ELi8ELi4ELi2ELi2ELb0ELb0EN7cutlass10bfloat16_tE19Flash_kernel_traitsILi256ELi64ELi64ELi8ES2_EELb1ELb0ELb1ELb0EEv16Flash_bwd_params' for 'sm_80'
ptxas info    : Function properties for _Z44flash_bwd_dq_dk_dv_loop_seqk_parallel_kernelI23Flash_bwd_kernel_traitsILi256ELi64ELi64ELi8ELi4ELi2ELi2ELb0ELb0EN7cutlass10bfloat16_tE19Flash_kernel_traitsILi256ELi64ELi64ELi8ES2_EELb1ELb0ELb1ELb0EEv16Flash_bwd_params
    24 bytes stack frame, 20 bytes spill stores, 20 bytes spill loads
ptxas info    : Used 255 registers, 696 bytes cmem[0]
ptxas info    : Compiling entry function '_Z44flash_bwd_dq_dk_dv_loop_seqk_parallel_kernelI23Flash_bwd_kernel_traitsILi256ELi64ELi64ELi8ELi4ELi2ELi2ELb0ELb0EN7cutlass10bfloat16_tE19Flash_kernel_traitsILi256ELi64ELi64ELi8ES2_EELb1ELb0ELb1ELb1EEv16Flash_bwd_params' for 'sm_80'
ptxas info    : Function properties for _Z44flash_bwd_dq_dk_dv_loop_seqk_parallel_kernelI23Flash_bwd_kernel_traitsILi256ELi64ELi64ELi8ELi4ELi2ELi2ELb0ELb0EN7cutlass10bfloat16_tE19Flash_kernel_traitsILi256ELi64ELi64ELi8ES2_EELb1ELb0ELb1ELb1EEv16Flash_bwd_params
    0 bytes stack frame, 0 bytes spill stores, 0 bytes spill loads
ptxas info    : Used 255 registers, 696 bytes cmem[0]
ptxas info    : Compiling entry function '_Z44flash_bwd_dq_dk_dv_loop_seqk_parallel_kernelI23Flash_bwd_kernel_traitsILi256ELi64ELi64ELi8ELi4ELi2ELi2ELb0ELb0EN7cutlass10bfloat16_tE19Flash_kernel_traitsILi256ELi64ELi64ELi8ES2_EELb1ELb1ELb0ELb0EEv16Flash_bwd_params' for 'sm_80'
ptxas info    : Function properties for _Z44flash_bwd_dq_dk_dv_loop_seqk_parallel_kernelI23Flash_bwd_kernel_traitsILi256ELi64ELi64ELi8ELi4ELi2ELi2ELb0ELb0EN7cutlass10bfloat16_tE19Flash_kernel_traitsILi256ELi64ELi64ELi8ES2_EELb1ELb1ELb0ELb0EEv16Flash_bwd_params
    32 bytes stack frame, 28 bytes spill stores, 28 bytes spill loads
ptxas info    : Used 255 registers, 696 bytes cmem[0]
ptxas info    : Compiling entry function '_Z44flash_bwd_dq_dk_dv_loop_seqk_parallel_kernelI23Flash_bwd_kernel_traitsILi256ELi64ELi64ELi8ELi4ELi2ELi2ELb0ELb0EN7cutlass10bfloat16_tE19Flash_kernel_traitsILi256ELi64ELi64ELi8ES2_EELb1ELb1ELb0ELb1EEv16Flash_bwd_params' for 'sm_80'
ptxas info    : Function properties for _Z44flash_bwd_dq_dk_dv_loop_seqk_parallel_kernelI23Flash_bwd_kernel_traitsILi256ELi64ELi64ELi8ELi4ELi2ELi2ELb0ELb0EN7cutlass10bfloat16_tE19Flash_kernel_traitsILi256ELi64ELi64ELi8ES2_EELb1ELb1ELb0ELb1EEv16Flash_bwd_params
    8 bytes stack frame, 8 bytes spill stores, 8 bytes spill loads
ptxas info    : Used 255 registers, 696 bytes cmem[0]
ptxas info    : Compiling entry function '_Z44flash_bwd_dq_dk_dv_loop_seqk_parallel_kernelI23Flash_bwd_kernel_traitsILi256ELi64ELi64ELi8ELi4ELi2ELi2ELb0ELb0EN7cutlass10bfloat16_tE19Flash_kernel_traitsILi256ELi64ELi64ELi8ES2_EELb1ELb1ELb1ELb0EEv16Flash_bwd_params' for 'sm_80'
ptxas info    : Function properties for _Z44flash_bwd_dq_dk_dv_loop_seqk_parallel_kernelI23Flash_bwd_kernel_traitsILi256ELi64ELi64ELi8ELi4ELi2ELi2ELb0ELb0EN7cutlass10bfloat16_tE19Flash_kernel_traitsILi256ELi64ELi64ELi8ES2_EELb1ELb1ELb1ELb0EEv16Flash_bwd_params
    8 bytes stack frame, 8 bytes spill stores, 8 bytes spill loads
ptxas info    : Used 255 registers, 696 bytes cmem[0]
ptxas info    : Compiling entry function '_Z44flash_bwd_dq_dk_dv_loop_seqk_parallel_kernelI23Flash_bwd_kernel_traitsILi256ELi64ELi64ELi8ELi4ELi2ELi2ELb0ELb0EN7cutlass10bfloat16_tE19Flash_kernel_traitsILi256ELi64ELi64ELi8ES2_EELb1ELb1ELb1ELb1EEv16Flash_bwd_params' for 'sm_80'
ptxas info    : Function properties for _Z44flash_bwd_dq_dk_dv_loop_seqk_parallel_kernelI23Flash_bwd_kernel_traitsILi256ELi64ELi64ELi8ELi4ELi2ELi2ELb0ELb0EN7cutlass10bfloat16_tE19Flash_kernel_traitsILi256ELi64ELi64ELi8ES2_EELb1ELb1ELb1ELb1EEv16Flash_bwd_params
    8 bytes stack frame, 8 bytes spill stores, 8 bytes spill loads
ptxas info    : Used 255 registers, 696 bytes cmem[0]
ptxas info    : Compiling entry function '_Z25flash_bwd_dot_do_o_kernelILb1E23Flash_bwd_kernel_traitsILi256ELi64ELi64ELi8ELi4ELi2ELi2ELb0ELb0EN7cutlass10bfloat16_tE19Flash_kernel_traitsILi256ELi64ELi64ELi8ES2_EEEv16Flash_bwd_params' for 'sm_80'
ptxas info    : Function properties for _Z25flash_bwd_dot_do_o_kernelILb1E23Flash_bwd_kernel_traitsILi256ELi64ELi64ELi8ELi4ELi2ELi2ELb0ELb0EN7cutlass10bfloat16_tE19Flash_kernel_traitsILi256ELi64ELi64ELi8ES2_EEEv16Flash_bwd_params
    0 bytes stack frame, 0 bytes spill stores, 0 bytes spill loads
ptxas info    : Used 92 registers, 696 bytes cmem[0]
/anaconda/envs/flashattention/lib/python3.9/site-packages/torch/include/c10/core/SymInt.h(84): warning: integer conversion resulted in a change of sign

[19/36] /usr/local/cuda-11.4/bin/nvcc  -I/home/v-feiychen/flash-attention/csrc/flash_attn -I/home/v-feiychen/flash-attention/csrc/flash_attn/src -I/home/v-feiychen/flash-attention/csrc/cutlass/include -I/anaconda/envs/flashattention/lib/python3.9/site-packages/torch/include -I/anaconda/envs/flashattention/lib/python3.9/site-packages/torch/include/torch/csrc/api/include -I/anaconda/envs/flashattention/lib/python3.9/site-packages/torch/include/TH -I/anaconda/envs/flashattention/lib/python3.9/site-packages/torch/include/THC -I/usr/local/cuda-11.4/include -I/anaconda/envs/flashattention/include/python3.9 -c -c /home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_hdim224_fp16_sm80.cu -o /home/v-feiychen/flash-attention/build/temp.linux-x86_64-cpython-39/csrc/flash_attn/src/flash_fwd_hdim224_fp16_sm80.o -D__CUDA_NO_HALF_OPERATORS__ -D__CUDA_NO_HALF_CONVERSIONS__ -D__CUDA_NO_BFLOAT16_CONVERSIONS__ -D__CUDA_NO_HALF2_OPERATORS__ --expt-relaxed-constexpr --compiler-options ''"'"'-fPIC'"'"'' -O3 -std=c++17 -U__CUDA_NO_HALF_OPERATORS__ -U__CUDA_NO_HALF_CONVERSIONS__ -U__CUDA_NO_HALF2_OPERATORS__ -U__CUDA_NO_BFLOAT16_CONVERSIONS__ --expt-relaxed-constexpr --expt-extended-lambda --use_fast_math --ptxas-options=-v -lineinfo -gencode arch=compute_80,code=sm_80 --threads 4 -DTORCH_API_INCLUDE_EXTENSION_H '-DPYBIND11_COMPILER_TYPE="_gcc"' '-DPYBIND11_STDLIB="_libstdcpp"' '-DPYBIND11_BUILD_ABI="_cxxabi1011"' -DTORCH_EXTENSION_NAME=flash_attn_2_cuda -D_GLIBCXX_USE_CXX11_ABI=0
/anaconda/envs/flashattention/lib/python3.9/site-packages/torch/include/c10/core/SymInt.h(84): warning: integer conversion resulted in a change of sign

/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_kernel.h(140): warning: variable "MMA_M" was declared but never referenced
          detected during:
            instantiation of "void flash::compute_attn_1rowblock<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &, int, int, int) [with Kernel_traits=Flash_fwd_kernel_traits<224, 128, 64, 8, false, false, cutlass::half_t, Flash_kernel_traits<224, 128, 64, 8, cutlass::half_t>>, Is_dropout=true, Is_causal=true, Is_even_N=true, Is_even_K=true, Return_softmax=true, Params=Flash_fwd_params]" 
(588): here
            instantiation of "void flash::compute_attn<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &) [with Kernel_traits=Flash_fwd_kernel_traits<224, 128, 64, 8, false, false, cutlass::half_t, Flash_kernel_traits<224, 128, 64, 8, cutlass::half_t>>, Is_dropout=true, Is_causal=true, Is_even_N=true, Is_even_K=true, Return_softmax=true, Params=Flash_fwd_params]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(15): here
            instantiation of "void flash_fwd_kernel<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax>(Flash_fwd_params) [with Kernel_traits=Flash_fwd_kernel_traits<224, 128, 64, 8, false, false, cutlass::half_t, Flash_kernel_traits<224, 128, 64, 8, cutlass::half_t>>, Is_dropout=true, Is_causal=true, Is_even_N=true, Is_even_K=true, Return_softmax=true]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(34): here
            instantiation of "void run_flash_fwd<Kernel_traits,Is_dropout,Is_causal>(Flash_fwd_params &, cudaStream_t) [with Kernel_traits=Flash_fwd_kernel_traits<224, 128, 64, 8, false, false, cutlass::half_t, Flash_kernel_traits<224, 128, 64, 8, cutlass::half_t>>, Is_dropout=true, Is_causal=true]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(208): here
            instantiation of "void run_mha_fwd_hdim224<T>(Flash_fwd_params &, cudaStream_t) [with T=cutlass::half_t]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_hdim224_fp16_sm80.cu(8): here

/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_kernel.h(275): warning: variable "tQrQ" was declared but never referenced
          detected during:
            instantiation of "void flash::compute_attn_1rowblock<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &, int, int, int) [with Kernel_traits=Flash_fwd_kernel_traits<224, 128, 64, 8, false, false, cutlass::half_t, Flash_kernel_traits<224, 128, 64, 8, cutlass::half_t>>, Is_dropout=true, Is_causal=true, Is_even_N=true, Is_even_K=true, Return_softmax=true, Params=Flash_fwd_params]" 
(588): here
            instantiation of "void flash::compute_attn<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &) [with Kernel_traits=Flash_fwd_kernel_traits<224, 128, 64, 8, false, false, cutlass::half_t, Flash_kernel_traits<224, 128, 64, 8, cutlass::half_t>>, Is_dropout=true, Is_causal=true, Is_even_N=true, Is_even_K=true, Return_softmax=true, Params=Flash_fwd_params]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(15): here
            instantiation of "void flash_fwd_kernel<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax>(Flash_fwd_params) [with Kernel_traits=Flash_fwd_kernel_traits<224, 128, 64, 8, false, false, cutlass::half_t, Flash_kernel_traits<224, 128, 64, 8, cutlass::half_t>>, Is_dropout=true, Is_causal=true, Is_even_N=true, Is_even_K=true, Return_softmax=true]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(34): here
            instantiation of "void run_flash_fwd<Kernel_traits,Is_dropout,Is_causal>(Flash_fwd_params &, cudaStream_t) [with Kernel_traits=Flash_fwd_kernel_traits<224, 128, 64, 8, false, false, cutlass::half_t, Flash_kernel_traits<224, 128, 64, 8, cutlass::half_t>>, Is_dropout=true, Is_causal=true]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(208): here
            instantiation of "void run_mha_fwd_hdim224<T>(Flash_fwd_params &, cudaStream_t) [with T=cutlass::half_t]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_hdim224_fp16_sm80.cu(8): here

/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_kernel.h(140): warning: variable "MMA_M" was declared but never referenced
          detected during:
            instantiation of "void flash::compute_attn_1rowblock<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &, int, int, int) [with Kernel_traits=Flash_fwd_kernel_traits<224, 128, 64, 8, false, false, cutlass::half_t, Flash_kernel_traits<224, 128, 64, 8, cutlass::half_t>>, Is_dropout=true, Is_causal=true, Is_even_N=true, Is_even_K=true, Return_softmax=false, Params=Flash_fwd_params]" 
(588): here
            instantiation of "void flash::compute_attn<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &) [with Kernel_traits=Flash_fwd_kernel_traits<224, 128, 64, 8, false, false, cutlass::half_t, Flash_kernel_traits<224, 128, 64, 8, cutlass::half_t>>, Is_dropout=true, Is_causal=true, Is_even_N=true, Is_even_K=true, Return_softmax=false, Params=Flash_fwd_params]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(15): here
            instantiation of "void flash_fwd_kernel<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax>(Flash_fwd_params) [with Kernel_traits=Flash_fwd_kernel_traits<224, 128, 64, 8, false, false, cutlass::half_t, Flash_kernel_traits<224, 128, 64, 8, cutlass::half_t>>, Is_dropout=true, Is_causal=true, Is_even_N=true, Is_even_K=true, Return_softmax=false]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(34): here
            instantiation of "void run_flash_fwd<Kernel_traits,Is_dropout,Is_causal>(Flash_fwd_params &, cudaStream_t) [with Kernel_traits=Flash_fwd_kernel_traits<224, 128, 64, 8, false, false, cutlass::half_t, Flash_kernel_traits<224, 128, 64, 8, cutlass::half_t>>, Is_dropout=true, Is_causal=true]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(208): here
            instantiation of "void run_mha_fwd_hdim224<T>(Flash_fwd_params &, cudaStream_t) [with T=cutlass::half_t]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_hdim224_fp16_sm80.cu(8): here

/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_kernel.h(275): warning: variable "tQrQ" was declared but never referenced
          detected during:
            instantiation of "void flash::compute_attn_1rowblock<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &, int, int, int) [with Kernel_traits=Flash_fwd_kernel_traits<224, 128, 64, 8, false, false, cutlass::half_t, Flash_kernel_traits<224, 128, 64, 8, cutlass::half_t>>, Is_dropout=true, Is_causal=true, Is_even_N=true, Is_even_K=true, Return_softmax=false, Params=Flash_fwd_params]" 
(588): here
            instantiation of "void flash::compute_attn<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &) [with Kernel_traits=Flash_fwd_kernel_traits<224, 128, 64, 8, false, false, cutlass::half_t, Flash_kernel_traits<224, 128, 64, 8, cutlass::half_t>>, Is_dropout=true, Is_causal=true, Is_even_N=true, Is_even_K=true, Return_softmax=false, Params=Flash_fwd_params]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(15): here
            instantiation of "void flash_fwd_kernel<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax>(Flash_fwd_params) [with Kernel_traits=Flash_fwd_kernel_traits<224, 128, 64, 8, false, false, cutlass::half_t, Flash_kernel_traits<224, 128, 64, 8, cutlass::half_t>>, Is_dropout=true, Is_causal=true, Is_even_N=true, Is_even_K=true, Return_softmax=false]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(34): here
            instantiation of "void run_flash_fwd<Kernel_traits,Is_dropout,Is_causal>(Flash_fwd_params &, cudaStream_t) [with Kernel_traits=Flash_fwd_kernel_traits<224, 128, 64, 8, false, false, cutlass::half_t, Flash_kernel_traits<224, 128, 64, 8, cutlass::half_t>>, Is_dropout=true, Is_causal=true]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(208): here
            instantiation of "void run_mha_fwd_hdim224<T>(Flash_fwd_params &, cudaStream_t) [with T=cutlass::half_t]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_hdim224_fp16_sm80.cu(8): here

/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_kernel.h(140): warning: variable "MMA_M" was declared but never referenced
          detected during:
            instantiation of "void flash::compute_attn_1rowblock<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &, int, int, int) [with Kernel_traits=Flash_fwd_kernel_traits<224, 128, 64, 8, false, false, cutlass::half_t, Flash_kernel_traits<224, 128, 64, 8, cutlass::half_t>>, Is_dropout=true, Is_causal=true, Is_even_N=true, Is_even_K=false, Return_softmax=true, Params=Flash_fwd_params]" 
(588): here
            instantiation of "void flash::compute_attn<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &) [with Kernel_traits=Flash_fwd_kernel_traits<224, 128, 64, 8, false, false, cutlass::half_t, Flash_kernel_traits<224, 128, 64, 8, cutlass::half_t>>, Is_dropout=true, Is_causal=true, Is_even_N=true, Is_even_K=false, Return_softmax=true, Params=Flash_fwd_params]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(15): here
            instantiation of "void flash_fwd_kernel<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax>(Flash_fwd_params) [with Kernel_traits=Flash_fwd_kernel_traits<224, 128, 64, 8, false, false, cutlass::half_t, Flash_kernel_traits<224, 128, 64, 8, cutlass::half_t>>, Is_dropout=true, Is_causal=true, Is_even_N=true, Is_even_K=false, Return_softmax=true]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(34): here
            instantiation of "void run_flash_fwd<Kernel_traits,Is_dropout,Is_causal>(Flash_fwd_params &, cudaStream_t) [with Kernel_traits=Flash_fwd_kernel_traits<224, 128, 64, 8, false, false, cutlass::half_t, Flash_kernel_traits<224, 128, 64, 8, cutlass::half_t>>, Is_dropout=true, Is_causal=true]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(208): here
            instantiation of "void run_mha_fwd_hdim224<T>(Flash_fwd_params &, cudaStream_t) [with T=cutlass::half_t]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_hdim224_fp16_sm80.cu(8): here

/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_kernel.h(275): warning: variable "tQrQ" was declared but never referenced
          detected during:
            instantiation of "void flash::compute_attn_1rowblock<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &, int, int, int) [with Kernel_traits=Flash_fwd_kernel_traits<224, 128, 64, 8, false, false, cutlass::half_t, Flash_kernel_traits<224, 128, 64, 8, cutlass::half_t>>, Is_dropout=true, Is_causal=true, Is_even_N=true, Is_even_K=false, Return_softmax=true, Params=Flash_fwd_params]" 
(588): here
            instantiation of "void flash::compute_attn<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &) [with Kernel_traits=Flash_fwd_kernel_traits<224, 128, 64, 8, false, false, cutlass::half_t, Flash_kernel_traits<224, 128, 64, 8, cutlass::half_t>>, Is_dropout=true, Is_causal=true, Is_even_N=true, Is_even_K=false, Return_softmax=true, Params=Flash_fwd_params]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(15): here
            instantiation of "void flash_fwd_kernel<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax>(Flash_fwd_params) [with Kernel_traits=Flash_fwd_kernel_traits<224, 128, 64, 8, false, false, cutlass::half_t, Flash_kernel_traits<224, 128, 64, 8, cutlass::half_t>>, Is_dropout=true, Is_causal=true, Is_even_N=true, Is_even_K=false, Return_softmax=true]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(34): here
            instantiation of "void run_flash_fwd<Kernel_traits,Is_dropout,Is_causal>(Flash_fwd_params &, cudaStream_t) [with Kernel_traits=Flash_fwd_kernel_traits<224, 128, 64, 8, false, false, cutlass::half_t, Flash_kernel_traits<224, 128, 64, 8, cutlass::half_t>>, Is_dropout=true, Is_causal=true]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(208): here
            instantiation of "void run_mha_fwd_hdim224<T>(Flash_fwd_params &, cudaStream_t) [with T=cutlass::half_t]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_hdim224_fp16_sm80.cu(8): here

/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_kernel.h(140): warning: variable "MMA_M" was declared but never referenced
          detected during:
            instantiation of "void flash::compute_attn_1rowblock<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &, int, int, int) [with Kernel_traits=Flash_fwd_kernel_traits<224, 128, 64, 8, false, false, cutlass::half_t, Flash_kernel_traits<224, 128, 64, 8, cutlass::half_t>>, Is_dropout=true, Is_causal=true, Is_even_N=true, Is_even_K=false, Return_softmax=false, Params=Flash_fwd_params]" 
(588): here
            instantiation of "void flash::compute_attn<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &) [with Kernel_traits=Flash_fwd_kernel_traits<224, 128, 64, 8, false, false, cutlass::half_t, Flash_kernel_traits<224, 128, 64, 8, cutlass::half_t>>, Is_dropout=true, Is_causal=true, Is_even_N=true, Is_even_K=false, Return_softmax=false, Params=Flash_fwd_params]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(15): here
            instantiation of "void flash_fwd_kernel<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax>(Flash_fwd_params) [with Kernel_traits=Flash_fwd_kernel_traits<224, 128, 64, 8, false, false, cutlass::half_t, Flash_kernel_traits<224, 128, 64, 8, cutlass::half_t>>, Is_dropout=true, Is_causal=true, Is_even_N=true, Is_even_K=false, Return_softmax=false]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(34): here
            instantiation of "void run_flash_fwd<Kernel_traits,Is_dropout,Is_causal>(Flash_fwd_params &, cudaStream_t) [with Kernel_traits=Flash_fwd_kernel_traits<224, 128, 64, 8, false, false, cutlass::half_t, Flash_kernel_traits<224, 128, 64, 8, cutlass::half_t>>, Is_dropout=true, Is_causal=true]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(208): here
            instantiation of "void run_mha_fwd_hdim224<T>(Flash_fwd_params &, cudaStream_t) [with T=cutlass::half_t]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_hdim224_fp16_sm80.cu(8): here

/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_kernel.h(275): warning: variable "tQrQ" was declared but never referenced
          detected during:
            instantiation of "void flash::compute_attn_1rowblock<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &, int, int, int) [with Kernel_traits=Flash_fwd_kernel_traits<224, 128, 64, 8, false, false, cutlass::half_t, Flash_kernel_traits<224, 128, 64, 8, cutlass::half_t>>, Is_dropout=true, Is_causal=true, Is_even_N=true, Is_even_K=false, Return_softmax=false, Params=Flash_fwd_params]" 
(588): here
            instantiation of "void flash::compute_attn<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &) [with Kernel_traits=Flash_fwd_kernel_traits<224, 128, 64, 8, false, false, cutlass::half_t, Flash_kernel_traits<224, 128, 64, 8, cutlass::half_t>>, Is_dropout=true, Is_causal=true, Is_even_N=true, Is_even_K=false, Return_softmax=false, Params=Flash_fwd_params]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(15): here
            instantiation of "void flash_fwd_kernel<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax>(Flash_fwd_params) [with Kernel_traits=Flash_fwd_kernel_traits<224, 128, 64, 8, false, false, cutlass::half_t, Flash_kernel_traits<224, 128, 64, 8, cutlass::half_t>>, Is_dropout=true, Is_causal=true, Is_even_N=true, Is_even_K=false, Return_softmax=false]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(34): here
            instantiation of "void run_flash_fwd<Kernel_traits,Is_dropout,Is_causal>(Flash_fwd_params &, cudaStream_t) [with Kernel_traits=Flash_fwd_kernel_traits<224, 128, 64, 8, false, false, cutlass::half_t, Flash_kernel_traits<224, 128, 64, 8, cutlass::half_t>>, Is_dropout=true, Is_causal=true]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(208): here
            instantiation of "void run_mha_fwd_hdim224<T>(Flash_fwd_params &, cudaStream_t) [with T=cutlass::half_t]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_hdim224_fp16_sm80.cu(8): here

/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_kernel.h(140): warning: variable "MMA_M" was declared but never referenced
          detected during:
            instantiation of "void flash::compute_attn_1rowblock<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &, int, int, int) [with Kernel_traits=Flash_fwd_kernel_traits<224, 128, 64, 8, false, false, cutlass::half_t, Flash_kernel_traits<224, 128, 64, 8, cutlass::half_t>>, Is_dropout=true, Is_causal=true, Is_even_N=false, Is_even_K=true, Return_softmax=true, Params=Flash_fwd_params]" 
(588): here
            instantiation of "void flash::compute_attn<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &) [with Kernel_traits=Flash_fwd_kernel_traits<224, 128, 64, 8, false, false, cutlass::half_t, Flash_kernel_traits<224, 128, 64, 8, cutlass::half_t>>, Is_dropout=true, Is_causal=true, Is_even_N=false, Is_even_K=true, Return_softmax=true, Params=Flash_fwd_params]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(15): here
            instantiation of "void flash_fwd_kernel<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax>(Flash_fwd_params) [with Kernel_traits=Flash_fwd_kernel_traits<224, 128, 64, 8, false, false, cutlass::half_t, Flash_kernel_traits<224, 128, 64, 8, cutlass::half_t>>, Is_dropout=true, Is_causal=true, Is_even_N=false, Is_even_K=true, Return_softmax=true]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(34): here
            instantiation of "void run_flash_fwd<Kernel_traits,Is_dropout,Is_causal>(Flash_fwd_params &, cudaStream_t) [with Kernel_traits=Flash_fwd_kernel_traits<224, 128, 64, 8, false, false, cutlass::half_t, Flash_kernel_traits<224, 128, 64, 8, cutlass::half_t>>, Is_dropout=true, Is_causal=true]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(208): here
            instantiation of "void run_mha_fwd_hdim224<T>(Flash_fwd_params &, cudaStream_t) [with T=cutlass::half_t]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_hdim224_fp16_sm80.cu(8): here

/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_kernel.h(275): warning: variable "tQrQ" was declared but never referenced
          detected during:
            instantiation of "void flash::compute_attn_1rowblock<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &, int, int, int) [with Kernel_traits=Flash_fwd_kernel_traits<224, 128, 64, 8, false, false, cutlass::half_t, Flash_kernel_traits<224, 128, 64, 8, cutlass::half_t>>, Is_dropout=true, Is_causal=true, Is_even_N=false, Is_even_K=true, Return_softmax=true, Params=Flash_fwd_params]" 
(588): here
            instantiation of "void flash::compute_attn<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &) [with Kernel_traits=Flash_fwd_kernel_traits<224, 128, 64, 8, false, false, cutlass::half_t, Flash_kernel_traits<224, 128, 64, 8, cutlass::half_t>>, Is_dropout=true, Is_causal=true, Is_even_N=false, Is_even_K=true, Return_softmax=true, Params=Flash_fwd_params]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(15): here
            instantiation of "void flash_fwd_kernel<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax>(Flash_fwd_params) [with Kernel_traits=Flash_fwd_kernel_traits<224, 128, 64, 8, false, false, cutlass::half_t, Flash_kernel_traits<224, 128, 64, 8, cutlass::half_t>>, Is_dropout=true, Is_causal=true, Is_even_N=false, Is_even_K=true, Return_softmax=true]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(34): here
            instantiation of "void run_flash_fwd<Kernel_traits,Is_dropout,Is_causal>(Flash_fwd_params &, cudaStream_t) [with Kernel_traits=Flash_fwd_kernel_traits<224, 128, 64, 8, false, false, cutlass::half_t, Flash_kernel_traits<224, 128, 64, 8, cutlass::half_t>>, Is_dropout=true, Is_causal=true]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(208): here
            instantiation of "void run_mha_fwd_hdim224<T>(Flash_fwd_params &, cudaStream_t) [with T=cutlass::half_t]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_hdim224_fp16_sm80.cu(8): here

/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_kernel.h(140): warning: variable "MMA_M" was declared but never referenced
          detected during:
            instantiation of "void flash::compute_attn_1rowblock<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &, int, int, int) [with Kernel_traits=Flash_fwd_kernel_traits<224, 128, 64, 8, false, false, cutlass::half_t, Flash_kernel_traits<224, 128, 64, 8, cutlass::half_t>>, Is_dropout=true, Is_causal=true, Is_even_N=false, Is_even_K=true, Return_softmax=false, Params=Flash_fwd_params]" 
(588): here
            instantiation of "void flash::compute_attn<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &) [with Kernel_traits=Flash_fwd_kernel_traits<224, 128, 64, 8, false, false, cutlass::half_t, Flash_kernel_traits<224, 128, 64, 8, cutlass::half_t>>, Is_dropout=true, Is_causal=true, Is_even_N=false, Is_even_K=true, Return_softmax=false, Params=Flash_fwd_params]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(15): here
            instantiation of "void flash_fwd_kernel<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax>(Flash_fwd_params) [with Kernel_traits=Flash_fwd_kernel_traits<224, 128, 64, 8, false, false, cutlass::half_t, Flash_kernel_traits<224, 128, 64, 8, cutlass::half_t>>, Is_dropout=true, Is_causal=true, Is_even_N=false, Is_even_K=true, Return_softmax=false]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(34): here
            instantiation of "void run_flash_fwd<Kernel_traits,Is_dropout,Is_causal>(Flash_fwd_params &, cudaStream_t) [with Kernel_traits=Flash_fwd_kernel_traits<224, 128, 64, 8, false, false, cutlass::half_t, Flash_kernel_traits<224, 128, 64, 8, cutlass::half_t>>, Is_dropout=true, Is_causal=true]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(208): here
            instantiation of "void run_mha_fwd_hdim224<T>(Flash_fwd_params &, cudaStream_t) [with T=cutlass::half_t]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_hdim224_fp16_sm80.cu(8): here

/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_kernel.h(275): warning: variable "tQrQ" was declared but never referenced
          detected during:
            instantiation of "void flash::compute_attn_1rowblock<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &, int, int, int) [with Kernel_traits=Flash_fwd_kernel_traits<224, 128, 64, 8, false, false, cutlass::half_t, Flash_kernel_traits<224, 128, 64, 8, cutlass::half_t>>, Is_dropout=true, Is_causal=true, Is_even_N=false, Is_even_K=true, Return_softmax=false, Params=Flash_fwd_params]" 
(588): here
            instantiation of "void flash::compute_attn<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &) [with Kernel_traits=Flash_fwd_kernel_traits<224, 128, 64, 8, false, false, cutlass::half_t, Flash_kernel_traits<224, 128, 64, 8, cutlass::half_t>>, Is_dropout=true, Is_causal=true, Is_even_N=false, Is_even_K=true, Return_softmax=false, Params=Flash_fwd_params]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(15): here
            instantiation of "void flash_fwd_kernel<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax>(Flash_fwd_params) [with Kernel_traits=Flash_fwd_kernel_traits<224, 128, 64, 8, false, false, cutlass::half_t, Flash_kernel_traits<224, 128, 64, 8, cutlass::half_t>>, Is_dropout=true, Is_causal=true, Is_even_N=false, Is_even_K=true, Return_softmax=false]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(34): here
            instantiation of "void run_flash_fwd<Kernel_traits,Is_dropout,Is_causal>(Flash_fwd_params &, cudaStream_t) [with Kernel_traits=Flash_fwd_kernel_traits<224, 128, 64, 8, false, false, cutlass::half_t, Flash_kernel_traits<224, 128, 64, 8, cutlass::half_t>>, Is_dropout=true, Is_causal=true]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(208): here
            instantiation of "void run_mha_fwd_hdim224<T>(Flash_fwd_params &, cudaStream_t) [with T=cutlass::half_t]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_hdim224_fp16_sm80.cu(8): here

/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_kernel.h(140): warning: variable "MMA_M" was declared but never referenced
          detected during:
            instantiation of "void flash::compute_attn_1rowblock<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &, int, int, int) [with Kernel_traits=Flash_fwd_kernel_traits<224, 128, 64, 8, false, false, cutlass::half_t, Flash_kernel_traits<224, 128, 64, 8, cutlass::half_t>>, Is_dropout=true, Is_causal=true, Is_even_N=false, Is_even_K=false, Return_softmax=true, Params=Flash_fwd_params]" 
(588): here
            instantiation of "void flash::compute_attn<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &) [with Kernel_traits=Flash_fwd_kernel_traits<224, 128, 64, 8, false, false, cutlass::half_t, Flash_kernel_traits<224, 128, 64, 8, cutlass::half_t>>, Is_dropout=true, Is_causal=true, Is_even_N=false, Is_even_K=false, Return_softmax=true, Params=Flash_fwd_params]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(15): here
            instantiation of "void flash_fwd_kernel<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax>(Flash_fwd_params) [with Kernel_traits=Flash_fwd_kernel_traits<224, 128, 64, 8, false, false, cutlass::half_t, Flash_kernel_traits<224, 128, 64, 8, cutlass::half_t>>, Is_dropout=true, Is_causal=true, Is_even_N=false, Is_even_K=false, Return_softmax=true]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(34): here
            instantiation of "void run_flash_fwd<Kernel_traits,Is_dropout,Is_causal>(Flash_fwd_params &, cudaStream_t) [with Kernel_traits=Flash_fwd_kernel_traits<224, 128, 64, 8, false, false, cutlass::half_t, Flash_kernel_traits<224, 128, 64, 8, cutlass::half_t>>, Is_dropout=true, Is_causal=true]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(208): here
            instantiation of "void run_mha_fwd_hdim224<T>(Flash_fwd_params &, cudaStream_t) [with T=cutlass::half_t]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_hdim224_fp16_sm80.cu(8): here

/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_kernel.h(275): warning: variable "tQrQ" was declared but never referenced
          detected during:
            instantiation of "void flash::compute_attn_1rowblock<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &, int, int, int) [with Kernel_traits=Flash_fwd_kernel_traits<224, 128, 64, 8, false, false, cutlass::half_t, Flash_kernel_traits<224, 128, 64, 8, cutlass::half_t>>, Is_dropout=true, Is_causal=true, Is_even_N=false, Is_even_K=false, Return_softmax=true, Params=Flash_fwd_params]" 
(588): here
            instantiation of "void flash::compute_attn<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &) [with Kernel_traits=Flash_fwd_kernel_traits<224, 128, 64, 8, false, false, cutlass::half_t, Flash_kernel_traits<224, 128, 64, 8, cutlass::half_t>>, Is_dropout=true, Is_causal=true, Is_even_N=false, Is_even_K=false, Return_softmax=true, Params=Flash_fwd_params]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(15): here
            instantiation of "void flash_fwd_kernel<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax>(Flash_fwd_params) [with Kernel_traits=Flash_fwd_kernel_traits<224, 128, 64, 8, false, false, cutlass::half_t, Flash_kernel_traits<224, 128, 64, 8, cutlass::half_t>>, Is_dropout=true, Is_causal=true, Is_even_N=false, Is_even_K=false, Return_softmax=true]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(34): here
            instantiation of "void run_flash_fwd<Kernel_traits,Is_dropout,Is_causal>(Flash_fwd_params &, cudaStream_t) [with Kernel_traits=Flash_fwd_kernel_traits<224, 128, 64, 8, false, false, cutlass::half_t, Flash_kernel_traits<224, 128, 64, 8, cutlass::half_t>>, Is_dropout=true, Is_causal=true]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(208): here
            instantiation of "void run_mha_fwd_hdim224<T>(Flash_fwd_params &, cudaStream_t) [with T=cutlass::half_t]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_hdim224_fp16_sm80.cu(8): here

/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_kernel.h(140): warning: variable "MMA_M" was declared but never referenced
          detected during:
            instantiation of "void flash::compute_attn_1rowblock<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &, int, int, int) [with Kernel_traits=Flash_fwd_kernel_traits<224, 128, 64, 8, false, false, cutlass::half_t, Flash_kernel_traits<224, 128, 64, 8, cutlass::half_t>>, Is_dropout=true, Is_causal=true, Is_even_N=false, Is_even_K=false, Return_softmax=false, Params=Flash_fwd_params]" 
(588): here
            instantiation of "void flash::compute_attn<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &) [with Kernel_traits=Flash_fwd_kernel_traits<224, 128, 64, 8, false, false, cutlass::half_t, Flash_kernel_traits<224, 128, 64, 8, cutlass::half_t>>, Is_dropout=true, Is_causal=true, Is_even_N=false, Is_even_K=false, Return_softmax=false, Params=Flash_fwd_params]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(15): here
            instantiation of "void flash_fwd_kernel<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax>(Flash_fwd_params) [with Kernel_traits=Flash_fwd_kernel_traits<224, 128, 64, 8, false, false, cutlass::half_t, Flash_kernel_traits<224, 128, 64, 8, cutlass::half_t>>, Is_dropout=true, Is_causal=true, Is_even_N=false, Is_even_K=false, Return_softmax=false]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(34): here
            instantiation of "void run_flash_fwd<Kernel_traits,Is_dropout,Is_causal>(Flash_fwd_params &, cudaStream_t) [with Kernel_traits=Flash_fwd_kernel_traits<224, 128, 64, 8, false, false, cutlass::half_t, Flash_kernel_traits<224, 128, 64, 8, cutlass::half_t>>, Is_dropout=true, Is_causal=true]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(208): here
            instantiation of "void run_mha_fwd_hdim224<T>(Flash_fwd_params &, cudaStream_t) [with T=cutlass::half_t]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_hdim224_fp16_sm80.cu(8): here

/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_kernel.h(275): warning: variable "tQrQ" was declared but never referenced
          detected during:
            instantiation of "void flash::compute_attn_1rowblock<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &, int, int, int) [with Kernel_traits=Flash_fwd_kernel_traits<224, 128, 64, 8, false, false, cutlass::half_t, Flash_kernel_traits<224, 128, 64, 8, cutlass::half_t>>, Is_dropout=true, Is_causal=true, Is_even_N=false, Is_even_K=false, Return_softmax=false, Params=Flash_fwd_params]" 
(588): here
            instantiation of "void flash::compute_attn<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &) [with Kernel_traits=Flash_fwd_kernel_traits<224, 128, 64, 8, false, false, cutlass::half_t, Flash_kernel_traits<224, 128, 64, 8, cutlass::half_t>>, Is_dropout=true, Is_causal=true, Is_even_N=false, Is_even_K=false, Return_softmax=false, Params=Flash_fwd_params]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(15): here
            instantiation of "void flash_fwd_kernel<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax>(Flash_fwd_params) [with Kernel_traits=Flash_fwd_kernel_traits<224, 128, 64, 8, false, false, cutlass::half_t, Flash_kernel_traits<224, 128, 64, 8, cutlass::half_t>>, Is_dropout=true, Is_causal=true, Is_even_N=false, Is_even_K=false, Return_softmax=false]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(34): here
            instantiation of "void run_flash_fwd<Kernel_traits,Is_dropout,Is_causal>(Flash_fwd_params &, cudaStream_t) [with Kernel_traits=Flash_fwd_kernel_traits<224, 128, 64, 8, false, false, cutlass::half_t, Flash_kernel_traits<224, 128, 64, 8, cutlass::half_t>>, Is_dropout=true, Is_causal=true]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(208): here
            instantiation of "void run_mha_fwd_hdim224<T>(Flash_fwd_params &, cudaStream_t) [with T=cutlass::half_t]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_hdim224_fp16_sm80.cu(8): here

/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_kernel.h(140): warning: variable "MMA_M" was declared but never referenced
          detected during:
            instantiation of "void flash::compute_attn_1rowblock<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &, int, int, int) [with Kernel_traits=Flash_fwd_kernel_traits<224, 64, 64, 4, false, false, cutlass::half_t, Flash_kernel_traits<224, 64, 64, 4, cutlass::half_t>>, Is_dropout=true, Is_causal=true, Is_even_N=true, Is_even_K=true, Return_softmax=true, Params=Flash_fwd_params]" 
(588): here
            instantiation of "void flash::compute_attn<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &) [with Kernel_traits=Flash_fwd_kernel_traits<224, 64, 64, 4, false, false, cutlass::half_t, Flash_kernel_traits<224, 64, 64, 4, cutlass::half_t>>, Is_dropout=true, Is_causal=true, Is_even_N=true, Is_even_K=true, Return_softmax=true, Params=Flash_fwd_params]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(15): here
            instantiation of "void flash_fwd_kernel<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax>(Flash_fwd_params) [with Kernel_traits=Flash_fwd_kernel_traits<224, 64, 64, 4, false, false, cutlass::half_t, Flash_kernel_traits<224, 64, 64, 4, cutlass::half_t>>, Is_dropout=true, Is_causal=true, Is_even_N=true, Is_even_K=true, Return_softmax=true]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(34): here
            instantiation of "void run_flash_fwd<Kernel_traits,Is_dropout,Is_causal>(Flash_fwd_params &, cudaStream_t) [with Kernel_traits=Flash_fwd_kernel_traits<224, 64, 64, 4, false, false, cutlass::half_t, Flash_kernel_traits<224, 64, 64, 4, cutlass::half_t>>, Is_dropout=true, Is_causal=true]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(208): here
            instantiation of "void run_mha_fwd_hdim224<T>(Flash_fwd_params &, cudaStream_t) [with T=cutlass::half_t]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_hdim224_fp16_sm80.cu(8): here

/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_kernel.h(275): warning: variable "tQrQ" was declared but never referenced
          detected during:
            instantiation of "void flash::compute_attn_1rowblock<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &, int, int, int) [with Kernel_traits=Flash_fwd_kernel_traits<224, 64, 64, 4, false, false, cutlass::half_t, Flash_kernel_traits<224, 64, 64, 4, cutlass::half_t>>, Is_dropout=true, Is_causal=true, Is_even_N=true, Is_even_K=true, Return_softmax=true, Params=Flash_fwd_params]" 
(588): here
            instantiation of "void flash::compute_attn<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &) [with Kernel_traits=Flash_fwd_kernel_traits<224, 64, 64, 4, false, false, cutlass::half_t, Flash_kernel_traits<224, 64, 64, 4, cutlass::half_t>>, Is_dropout=true, Is_causal=true, Is_even_N=true, Is_even_K=true, Return_softmax=true, Params=Flash_fwd_params]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(15): here
            instantiation of "void flash_fwd_kernel<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax>(Flash_fwd_params) [with Kernel_traits=Flash_fwd_kernel_traits<224, 64, 64, 4, false, false, cutlass::half_t, Flash_kernel_traits<224, 64, 64, 4, cutlass::half_t>>, Is_dropout=true, Is_causal=true, Is_even_N=true, Is_even_K=true, Return_softmax=true]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(34): here
            instantiation of "void run_flash_fwd<Kernel_traits,Is_dropout,Is_causal>(Flash_fwd_params &, cudaStream_t) [with Kernel_traits=Flash_fwd_kernel_traits<224, 64, 64, 4, false, false, cutlass::half_t, Flash_kernel_traits<224, 64, 64, 4, cutlass::half_t>>, Is_dropout=true, Is_causal=true]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(208): here
            instantiation of "void run_mha_fwd_hdim224<T>(Flash_fwd_params &, cudaStream_t) [with T=cutlass::half_t]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_hdim224_fp16_sm80.cu(8): here

/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_kernel.h(140): warning: variable "MMA_M" was declared but never referenced
          detected during:
            instantiation of "void flash::compute_attn_1rowblock<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &, int, int, int) [with Kernel_traits=Flash_fwd_kernel_traits<224, 64, 64, 4, false, false, cutlass::half_t, Flash_kernel_traits<224, 64, 64, 4, cutlass::half_t>>, Is_dropout=true, Is_causal=true, Is_even_N=true, Is_even_K=true, Return_softmax=false, Params=Flash_fwd_params]" 
(588): here
            instantiation of "void flash::compute_attn<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &) [with Kernel_traits=Flash_fwd_kernel_traits<224, 64, 64, 4, false, false, cutlass::half_t, Flash_kernel_traits<224, 64, 64, 4, cutlass::half_t>>, Is_dropout=true, Is_causal=true, Is_even_N=true, Is_even_K=true, Return_softmax=false, Params=Flash_fwd_params]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(15): here
            instantiation of "void flash_fwd_kernel<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax>(Flash_fwd_params) [with Kernel_traits=Flash_fwd_kernel_traits<224, 64, 64, 4, false, false, cutlass::half_t, Flash_kernel_traits<224, 64, 64, 4, cutlass::half_t>>, Is_dropout=true, Is_causal=true, Is_even_N=true, Is_even_K=true, Return_softmax=false]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(34): here
            instantiation of "void run_flash_fwd<Kernel_traits,Is_dropout,Is_causal>(Flash_fwd_params &, cudaStream_t) [with Kernel_traits=Flash_fwd_kernel_traits<224, 64, 64, 4, false, false, cutlass::half_t, Flash_kernel_traits<224, 64, 64, 4, cutlass::half_t>>, Is_dropout=true, Is_causal=true]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(208): here
            instantiation of "void run_mha_fwd_hdim224<T>(Flash_fwd_params &, cudaStream_t) [with T=cutlass::half_t]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_hdim224_fp16_sm80.cu(8): here

/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_kernel.h(275): warning: variable "tQrQ" was declared but never referenced
          detected during:
            instantiation of "void flash::compute_attn_1rowblock<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &, int, int, int) [with Kernel_traits=Flash_fwd_kernel_traits<224, 64, 64, 4, false, false, cutlass::half_t, Flash_kernel_traits<224, 64, 64, 4, cutlass::half_t>>, Is_dropout=true, Is_causal=true, Is_even_N=true, Is_even_K=true, Return_softmax=false, Params=Flash_fwd_params]" 
(588): here
            instantiation of "void flash::compute_attn<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &) [with Kernel_traits=Flash_fwd_kernel_traits<224, 64, 64, 4, false, false, cutlass::half_t, Flash_kernel_traits<224, 64, 64, 4, cutlass::half_t>>, Is_dropout=true, Is_causal=true, Is_even_N=true, Is_even_K=true, Return_softmax=false, Params=Flash_fwd_params]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(15): here
            instantiation of "void flash_fwd_kernel<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax>(Flash_fwd_params) [with Kernel_traits=Flash_fwd_kernel_traits<224, 64, 64, 4, false, false, cutlass::half_t, Flash_kernel_traits<224, 64, 64, 4, cutlass::half_t>>, Is_dropout=true, Is_causal=true, Is_even_N=true, Is_even_K=true, Return_softmax=false]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(34): here
            instantiation of "void run_flash_fwd<Kernel_traits,Is_dropout,Is_causal>(Flash_fwd_params &, cudaStream_t) [with Kernel_traits=Flash_fwd_kernel_traits<224, 64, 64, 4, false, false, cutlass::half_t, Flash_kernel_traits<224, 64, 64, 4, cutlass::half_t>>, Is_dropout=true, Is_causal=true]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(208): here
            instantiation of "void run_mha_fwd_hdim224<T>(Flash_fwd_params &, cudaStream_t) [with T=cutlass::half_t]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_hdim224_fp16_sm80.cu(8): here

/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_kernel.h(140): warning: variable "MMA_M" was declared but never referenced
          detected during:
            instantiation of "void flash::compute_attn_1rowblock<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &, int, int, int) [with Kernel_traits=Flash_fwd_kernel_traits<224, 64, 64, 4, false, false, cutlass::half_t, Flash_kernel_traits<224, 64, 64, 4, cutlass::half_t>>, Is_dropout=true, Is_causal=true, Is_even_N=true, Is_even_K=false, Return_softmax=true, Params=Flash_fwd_params]" 
(588): here
            instantiation of "void flash::compute_attn<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &) [with Kernel_traits=Flash_fwd_kernel_traits<224, 64, 64, 4, false, false, cutlass::half_t, Flash_kernel_traits<224, 64, 64, 4, cutlass::half_t>>, Is_dropout=true, Is_causal=true, Is_even_N=true, Is_even_K=false, Return_softmax=true, Params=Flash_fwd_params]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(15): here
            instantiation of "void flash_fwd_kernel<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax>(Flash_fwd_params) [with Kernel_traits=Flash_fwd_kernel_traits<224, 64, 64, 4, false, false, cutlass::half_t, Flash_kernel_traits<224, 64, 64, 4, cutlass::half_t>>, Is_dropout=true, Is_causal=true, Is_even_N=true, Is_even_K=false, Return_softmax=true]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(34): here
            instantiation of "void run_flash_fwd<Kernel_traits,Is_dropout,Is_causal>(Flash_fwd_params &, cudaStream_t) [with Kernel_traits=Flash_fwd_kernel_traits<224, 64, 64, 4, false, false, cutlass::half_t, Flash_kernel_traits<224, 64, 64, 4, cutlass::half_t>>, Is_dropout=true, Is_causal=true]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(208): here
            instantiation of "void run_mha_fwd_hdim224<T>(Flash_fwd_params &, cudaStream_t) [with T=cutlass::half_t]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_hdim224_fp16_sm80.cu(8): here

/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_kernel.h(275): warning: variable "tQrQ" was declared but never referenced
          detected during:
            instantiation of "void flash::compute_attn_1rowblock<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &, int, int, int) [with Kernel_traits=Flash_fwd_kernel_traits<224, 64, 64, 4, false, false, cutlass::half_t, Flash_kernel_traits<224, 64, 64, 4, cutlass::half_t>>, Is_dropout=true, Is_causal=true, Is_even_N=true, Is_even_K=false, Return_softmax=true, Params=Flash_fwd_params]" 
(588): here
            instantiation of "void flash::compute_attn<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &) [with Kernel_traits=Flash_fwd_kernel_traits<224, 64, 64, 4, false, false, cutlass::half_t, Flash_kernel_traits<224, 64, 64, 4, cutlass::half_t>>, Is_dropout=true, Is_causal=true, Is_even_N=true, Is_even_K=false, Return_softmax=true, Params=Flash_fwd_params]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(15): here
            instantiation of "void flash_fwd_kernel<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax>(Flash_fwd_params) [with Kernel_traits=Flash_fwd_kernel_traits<224, 64, 64, 4, false, false, cutlass::half_t, Flash_kernel_traits<224, 64, 64, 4, cutlass::half_t>>, Is_dropout=true, Is_causal=true, Is_even_N=true, Is_even_K=false, Return_softmax=true]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(34): here
            instantiation of "void run_flash_fwd<Kernel_traits,Is_dropout,Is_causal>(Flash_fwd_params &, cudaStream_t) [with Kernel_traits=Flash_fwd_kernel_traits<224, 64, 64, 4, false, false, cutlass::half_t, Flash_kernel_traits<224, 64, 64, 4, cutlass::half_t>>, Is_dropout=true, Is_causal=true]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(208): here
            instantiation of "void run_mha_fwd_hdim224<T>(Flash_fwd_params &, cudaStream_t) [with T=cutlass::half_t]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_hdim224_fp16_sm80.cu(8): here

/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_kernel.h(140): warning: variable "MMA_M" was declared but never referenced
          detected during:
            instantiation of "void flash::compute_attn_1rowblock<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &, int, int, int) [with Kernel_traits=Flash_fwd_kernel_traits<224, 64, 64, 4, false, false, cutlass::half_t, Flash_kernel_traits<224, 64, 64, 4, cutlass::half_t>>, Is_dropout=true, Is_causal=true, Is_even_N=true, Is_even_K=false, Return_softmax=false, Params=Flash_fwd_params]" 
(588): here
            instantiation of "void flash::compute_attn<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &) [with Kernel_traits=Flash_fwd_kernel_traits<224, 64, 64, 4, false, false, cutlass::half_t, Flash_kernel_traits<224, 64, 64, 4, cutlass::half_t>>, Is_dropout=true, Is_causal=true, Is_even_N=true, Is_even_K=false, Return_softmax=false, Params=Flash_fwd_params]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(15): here
            instantiation of "void flash_fwd_kernel<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax>(Flash_fwd_params) [with Kernel_traits=Flash_fwd_kernel_traits<224, 64, 64, 4, false, false, cutlass::half_t, Flash_kernel_traits<224, 64, 64, 4, cutlass::half_t>>, Is_dropout=true, Is_causal=true, Is_even_N=true, Is_even_K=false, Return_softmax=false]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(34): here
            instantiation of "void run_flash_fwd<Kernel_traits,Is_dropout,Is_causal>(Flash_fwd_params &, cudaStream_t) [with Kernel_traits=Flash_fwd_kernel_traits<224, 64, 64, 4, false, false, cutlass::half_t, Flash_kernel_traits<224, 64, 64, 4, cutlass::half_t>>, Is_dropout=true, Is_causal=true]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(208): here
            instantiation of "void run_mha_fwd_hdim224<T>(Flash_fwd_params &, cudaStream_t) [with T=cutlass::half_t]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_hdim224_fp16_sm80.cu(8): here

/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_kernel.h(275): warning: variable "tQrQ" was declared but never referenced
          detected during:
            instantiation of "void flash::compute_attn_1rowblock<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &, int, int, int) [with Kernel_traits=Flash_fwd_kernel_traits<224, 64, 64, 4, false, false, cutlass::half_t, Flash_kernel_traits<224, 64, 64, 4, cutlass::half_t>>, Is_dropout=true, Is_causal=true, Is_even_N=true, Is_even_K=false, Return_softmax=false, Params=Flash_fwd_params]" 
(588): here
            instantiation of "void flash::compute_attn<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &) [with Kernel_traits=Flash_fwd_kernel_traits<224, 64, 64, 4, false, false, cutlass::half_t, Flash_kernel_traits<224, 64, 64, 4, cutlass::half_t>>, Is_dropout=true, Is_causal=true, Is_even_N=true, Is_even_K=false, Return_softmax=false, Params=Flash_fwd_params]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(15): here
            instantiation of "void flash_fwd_kernel<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax>(Flash_fwd_params) [with Kernel_traits=Flash_fwd_kernel_traits<224, 64, 64, 4, false, false, cutlass::half_t, Flash_kernel_traits<224, 64, 64, 4, cutlass::half_t>>, Is_dropout=true, Is_causal=true, Is_even_N=true, Is_even_K=false, Return_softmax=false]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(34): here
            instantiation of "void run_flash_fwd<Kernel_traits,Is_dropout,Is_causal>(Flash_fwd_params &, cudaStream_t) [with Kernel_traits=Flash_fwd_kernel_traits<224, 64, 64, 4, false, false, cutlass::half_t, Flash_kernel_traits<224, 64, 64, 4, cutlass::half_t>>, Is_dropout=true, Is_causal=true]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(208): here
            instantiation of "void run_mha_fwd_hdim224<T>(Flash_fwd_params &, cudaStream_t) [with T=cutlass::half_t]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_hdim224_fp16_sm80.cu(8): here

/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_kernel.h(140): warning: variable "MMA_M" was declared but never referenced
          detected during:
            instantiation of "void flash::compute_attn_1rowblock<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &, int, int, int) [with Kernel_traits=Flash_fwd_kernel_traits<224, 64, 64, 4, false, false, cutlass::half_t, Flash_kernel_traits<224, 64, 64, 4, cutlass::half_t>>, Is_dropout=true, Is_causal=true, Is_even_N=false, Is_even_K=true, Return_softmax=true, Params=Flash_fwd_params]" 
(588): here
            instantiation of "void flash::compute_attn<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &) [with Kernel_traits=Flash_fwd_kernel_traits<224, 64, 64, 4, false, false, cutlass::half_t, Flash_kernel_traits<224, 64, 64, 4, cutlass::half_t>>, Is_dropout=true, Is_causal=true, Is_even_N=false, Is_even_K=true, Return_softmax=true, Params=Flash_fwd_params]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(15): here
            instantiation of "void flash_fwd_kernel<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax>(Flash_fwd_params) [with Kernel_traits=Flash_fwd_kernel_traits<224, 64, 64, 4, false, false, cutlass::half_t, Flash_kernel_traits<224, 64, 64, 4, cutlass::half_t>>, Is_dropout=true, Is_causal=true, Is_even_N=false, Is_even_K=true, Return_softmax=true]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(34): here
            instantiation of "void run_flash_fwd<Kernel_traits,Is_dropout,Is_causal>(Flash_fwd_params &, cudaStream_t) [with Kernel_traits=Flash_fwd_kernel_traits<224, 64, 64, 4, false, false, cutlass::half_t, Flash_kernel_traits<224, 64, 64, 4, cutlass::half_t>>, Is_dropout=true, Is_causal=true]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(208): here
            instantiation of "void run_mha_fwd_hdim224<T>(Flash_fwd_params &, cudaStream_t) [with T=cutlass::half_t]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_hdim224_fp16_sm80.cu(8): here

/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_kernel.h(275): warning: variable "tQrQ" was declared but never referenced
          detected during:
            instantiation of "void flash::compute_attn_1rowblock<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &, int, int, int) [with Kernel_traits=Flash_fwd_kernel_traits<224, 64, 64, 4, false, false, cutlass::half_t, Flash_kernel_traits<224, 64, 64, 4, cutlass::half_t>>, Is_dropout=true, Is_causal=true, Is_even_N=false, Is_even_K=true, Return_softmax=true, Params=Flash_fwd_params]" 
(588): here
            instantiation of "void flash::compute_attn<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &) [with Kernel_traits=Flash_fwd_kernel_traits<224, 64, 64, 4, false, false, cutlass::half_t, Flash_kernel_traits<224, 64, 64, 4, cutlass::half_t>>, Is_dropout=true, Is_causal=true, Is_even_N=false, Is_even_K=true, Return_softmax=true, Params=Flash_fwd_params]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(15): here
            instantiation of "void flash_fwd_kernel<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax>(Flash_fwd_params) [with Kernel_traits=Flash_fwd_kernel_traits<224, 64, 64, 4, false, false, cutlass::half_t, Flash_kernel_traits<224, 64, 64, 4, cutlass::half_t>>, Is_dropout=true, Is_causal=true, Is_even_N=false, Is_even_K=true, Return_softmax=true]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(34): here
            instantiation of "void run_flash_fwd<Kernel_traits,Is_dropout,Is_causal>(Flash_fwd_params &, cudaStream_t) [with Kernel_traits=Flash_fwd_kernel_traits<224, 64, 64, 4, false, false, cutlass::half_t, Flash_kernel_traits<224, 64, 64, 4, cutlass::half_t>>, Is_dropout=true, Is_causal=true]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(208): here
            instantiation of "void run_mha_fwd_hdim224<T>(Flash_fwd_params &, cudaStream_t) [with T=cutlass::half_t]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_hdim224_fp16_sm80.cu(8): here

/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_kernel.h(140): warning: variable "MMA_M" was declared but never referenced
          detected during:
            instantiation of "void flash::compute_attn_1rowblock<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &, int, int, int) [with Kernel_traits=Flash_fwd_kernel_traits<224, 64, 64, 4, false, false, cutlass::half_t, Flash_kernel_traits<224, 64, 64, 4, cutlass::half_t>>, Is_dropout=true, Is_causal=true, Is_even_N=false, Is_even_K=true, Return_softmax=false, Params=Flash_fwd_params]" 
(588): here
            instantiation of "void flash::compute_attn<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &) [with Kernel_traits=Flash_fwd_kernel_traits<224, 64, 64, 4, false, false, cutlass::half_t, Flash_kernel_traits<224, 64, 64, 4, cutlass::half_t>>, Is_dropout=true, Is_causal=true, Is_even_N=false, Is_even_K=true, Return_softmax=false, Params=Flash_fwd_params]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(15): here
            instantiation of "void flash_fwd_kernel<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax>(Flash_fwd_params) [with Kernel_traits=Flash_fwd_kernel_traits<224, 64, 64, 4, false, false, cutlass::half_t, Flash_kernel_traits<224, 64, 64, 4, cutlass::half_t>>, Is_dropout=true, Is_causal=true, Is_even_N=false, Is_even_K=true, Return_softmax=false]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(34): here
            instantiation of "void run_flash_fwd<Kernel_traits,Is_dropout,Is_causal>(Flash_fwd_params &, cudaStream_t) [with Kernel_traits=Flash_fwd_kernel_traits<224, 64, 64, 4, false, false, cutlass::half_t, Flash_kernel_traits<224, 64, 64, 4, cutlass::half_t>>, Is_dropout=true, Is_causal=true]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(208): here
            instantiation of "void run_mha_fwd_hdim224<T>(Flash_fwd_params &, cudaStream_t) [with T=cutlass::half_t]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_hdim224_fp16_sm80.cu(8): here

/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_kernel.h(275): warning: variable "tQrQ" was declared but never referenced
          detected during:
            instantiation of "void flash::compute_attn_1rowblock<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &, int, int, int) [with Kernel_traits=Flash_fwd_kernel_traits<224, 64, 64, 4, false, false, cutlass::half_t, Flash_kernel_traits<224, 64, 64, 4, cutlass::half_t>>, Is_dropout=true, Is_causal=true, Is_even_N=false, Is_even_K=true, Return_softmax=false, Params=Flash_fwd_params]" 
(588): here
            instantiation of "void flash::compute_attn<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &) [with Kernel_traits=Flash_fwd_kernel_traits<224, 64, 64, 4, false, false, cutlass::half_t, Flash_kernel_traits<224, 64, 64, 4, cutlass::half_t>>, Is_dropout=true, Is_causal=true, Is_even_N=false, Is_even_K=true, Return_softmax=false, Params=Flash_fwd_params]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(15): here
            instantiation of "void flash_fwd_kernel<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax>(Flash_fwd_params) [with Kernel_traits=Flash_fwd_kernel_traits<224, 64, 64, 4, false, false, cutlass::half_t, Flash_kernel_traits<224, 64, 64, 4, cutlass::half_t>>, Is_dropout=true, Is_causal=true, Is_even_N=false, Is_even_K=true, Return_softmax=false]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(34): here
            instantiation of "void run_flash_fwd<Kernel_traits,Is_dropout,Is_causal>(Flash_fwd_params &, cudaStream_t) [with Kernel_traits=Flash_fwd_kernel_traits<224, 64, 64, 4, false, false, cutlass::half_t, Flash_kernel_traits<224, 64, 64, 4, cutlass::half_t>>, Is_dropout=true, Is_causal=true]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(208): here
            instantiation of "void run_mha_fwd_hdim224<T>(Flash_fwd_params &, cudaStream_t) [with T=cutlass::half_t]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_hdim224_fp16_sm80.cu(8): here

/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_kernel.h(140): warning: variable "MMA_M" was declared but never referenced
          detected during:
            instantiation of "void flash::compute_attn_1rowblock<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &, int, int, int) [with Kernel_traits=Flash_fwd_kernel_traits<224, 64, 64, 4, false, false, cutlass::half_t, Flash_kernel_traits<224, 64, 64, 4, cutlass::half_t>>, Is_dropout=true, Is_causal=true, Is_even_N=false, Is_even_K=false, Return_softmax=true, Params=Flash_fwd_params]" 
(588): here
            instantiation of "void flash::compute_attn<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &) [with Kernel_traits=Flash_fwd_kernel_traits<224, 64, 64, 4, false, false, cutlass::half_t, Flash_kernel_traits<224, 64, 64, 4, cutlass::half_t>>, Is_dropout=true, Is_causal=true, Is_even_N=false, Is_even_K=false, Return_softmax=true, Params=Flash_fwd_params]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(15): here
            instantiation of "void flash_fwd_kernel<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax>(Flash_fwd_params) [with Kernel_traits=Flash_fwd_kernel_traits<224, 64, 64, 4, false, false, cutlass::half_t, Flash_kernel_traits<224, 64, 64, 4, cutlass::half_t>>, Is_dropout=true, Is_causal=true, Is_even_N=false, Is_even_K=false, Return_softmax=true]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(34): here
            instantiation of "void run_flash_fwd<Kernel_traits,Is_dropout,Is_causal>(Flash_fwd_params &, cudaStream_t) [with Kernel_traits=Flash_fwd_kernel_traits<224, 64, 64, 4, false, false, cutlass::half_t, Flash_kernel_traits<224, 64, 64, 4, cutlass::half_t>>, Is_dropout=true, Is_causal=true]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(208): here
            instantiation of "void run_mha_fwd_hdim224<T>(Flash_fwd_params &, cudaStream_t) [with T=cutlass::half_t]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_hdim224_fp16_sm80.cu(8): here

/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_kernel.h(275): warning: variable "tQrQ" was declared but never referenced
          detected during:
            instantiation of "void flash::compute_attn_1rowblock<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &, int, int, int) [with Kernel_traits=Flash_fwd_kernel_traits<224, 64, 64, 4, false, false, cutlass::half_t, Flash_kernel_traits<224, 64, 64, 4, cutlass::half_t>>, Is_dropout=true, Is_causal=true, Is_even_N=false, Is_even_K=false, Return_softmax=true, Params=Flash_fwd_params]" 
(588): here
            instantiation of "void flash::compute_attn<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &) [with Kernel_traits=Flash_fwd_kernel_traits<224, 64, 64, 4, false, false, cutlass::half_t, Flash_kernel_traits<224, 64, 64, 4, cutlass::half_t>>, Is_dropout=true, Is_causal=true, Is_even_N=false, Is_even_K=false, Return_softmax=true, Params=Flash_fwd_params]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(15): here
            instantiation of "void flash_fwd_kernel<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax>(Flash_fwd_params) [with Kernel_traits=Flash_fwd_kernel_traits<224, 64, 64, 4, false, false, cutlass::half_t, Flash_kernel_traits<224, 64, 64, 4, cutlass::half_t>>, Is_dropout=true, Is_causal=true, Is_even_N=false, Is_even_K=false, Return_softmax=true]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(34): here
            instantiation of "void run_flash_fwd<Kernel_traits,Is_dropout,Is_causal>(Flash_fwd_params &, cudaStream_t) [with Kernel_traits=Flash_fwd_kernel_traits<224, 64, 64, 4, false, false, cutlass::half_t, Flash_kernel_traits<224, 64, 64, 4, cutlass::half_t>>, Is_dropout=true, Is_causal=true]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(208): here
            instantiation of "void run_mha_fwd_hdim224<T>(Flash_fwd_params &, cudaStream_t) [with T=cutlass::half_t]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_hdim224_fp16_sm80.cu(8): here

/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_kernel.h(140): warning: variable "MMA_M" was declared but never referenced
          detected during:
            instantiation of "void flash::compute_attn_1rowblock<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &, int, int, int) [with Kernel_traits=Flash_fwd_kernel_traits<224, 64, 64, 4, false, false, cutlass::half_t, Flash_kernel_traits<224, 64, 64, 4, cutlass::half_t>>, Is_dropout=true, Is_causal=true, Is_even_N=false, Is_even_K=false, Return_softmax=false, Params=Flash_fwd_params]" 
(588): here
            instantiation of "void flash::compute_attn<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &) [with Kernel_traits=Flash_fwd_kernel_traits<224, 64, 64, 4, false, false, cutlass::half_t, Flash_kernel_traits<224, 64, 64, 4, cutlass::half_t>>, Is_dropout=true, Is_causal=true, Is_even_N=false, Is_even_K=false, Return_softmax=false, Params=Flash_fwd_params]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(15): here
            instantiation of "void flash_fwd_kernel<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax>(Flash_fwd_params) [with Kernel_traits=Flash_fwd_kernel_traits<224, 64, 64, 4, false, false, cutlass::half_t, Flash_kernel_traits<224, 64, 64, 4, cutlass::half_t>>, Is_dropout=true, Is_causal=true, Is_even_N=false, Is_even_K=false, Return_softmax=false]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(34): here
            instantiation of "void run_flash_fwd<Kernel_traits,Is_dropout,Is_causal>(Flash_fwd_params &, cudaStream_t) [with Kernel_traits=Flash_fwd_kernel_traits<224, 64, 64, 4, false, false, cutlass::half_t, Flash_kernel_traits<224, 64, 64, 4, cutlass::half_t>>, Is_dropout=true, Is_causal=true]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(208): here
            instantiation of "void run_mha_fwd_hdim224<T>(Flash_fwd_params &, cudaStream_t) [with T=cutlass::half_t]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_hdim224_fp16_sm80.cu(8): here

/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_kernel.h(275): warning: variable "tQrQ" was declared but never referenced
          detected during:
            instantiation of "void flash::compute_attn_1rowblock<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &, int, int, int) [with Kernel_traits=Flash_fwd_kernel_traits<224, 64, 64, 4, false, false, cutlass::half_t, Flash_kernel_traits<224, 64, 64, 4, cutlass::half_t>>, Is_dropout=true, Is_causal=true, Is_even_N=false, Is_even_K=false, Return_softmax=false, Params=Flash_fwd_params]" 
(588): here
            instantiation of "void flash::compute_attn<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &) [with Kernel_traits=Flash_fwd_kernel_traits<224, 64, 64, 4, false, false, cutlass::half_t, Flash_kernel_traits<224, 64, 64, 4, cutlass::half_t>>, Is_dropout=true, Is_causal=true, Is_even_N=false, Is_even_K=false, Return_softmax=false, Params=Flash_fwd_params]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(15): here
            instantiation of "void flash_fwd_kernel<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax>(Flash_fwd_params) [with Kernel_traits=Flash_fwd_kernel_traits<224, 64, 64, 4, false, false, cutlass::half_t, Flash_kernel_traits<224, 64, 64, 4, cutlass::half_t>>, Is_dropout=true, Is_causal=true, Is_even_N=false, Is_even_K=false, Return_softmax=false]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(34): here
            instantiation of "void run_flash_fwd<Kernel_traits,Is_dropout,Is_causal>(Flash_fwd_params &, cudaStream_t) [with Kernel_traits=Flash_fwd_kernel_traits<224, 64, 64, 4, false, false, cutlass::half_t, Flash_kernel_traits<224, 64, 64, 4, cutlass::half_t>>, Is_dropout=true, Is_causal=true]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(208): here
            instantiation of "void run_mha_fwd_hdim224<T>(Flash_fwd_params &, cudaStream_t) [with T=cutlass::half_t]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_hdim224_fp16_sm80.cu(8): here

/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_kernel.h(140): warning: variable "MMA_M" was declared but never referenced
          detected during:
            instantiation of "void flash::compute_attn_1rowblock<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &, int, int, int) [with Kernel_traits=Flash_fwd_kernel_traits<224, 128, 64, 8, false, false, cutlass::half_t, Flash_kernel_traits<224, 128, 64, 8, cutlass::half_t>>, Is_dropout=true, Is_causal=false, Is_even_N=true, Is_even_K=true, Return_softmax=true, Params=Flash_fwd_params]" 
(588): here
            instantiation of "void flash::compute_attn<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &) [with Kernel_traits=Flash_fwd_kernel_traits<224, 128, 64, 8, false, false, cutlass::half_t, Flash_kernel_traits<224, 128, 64, 8, cutlass::half_t>>, Is_dropout=true, Is_causal=false, Is_even_N=true, Is_even_K=true, Return_softmax=true, Params=Flash_fwd_params]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(15): here
            instantiation of "void flash_fwd_kernel<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax>(Flash_fwd_params) [with Kernel_traits=Flash_fwd_kernel_traits<224, 128, 64, 8, false, false, cutlass::half_t, Flash_kernel_traits<224, 128, 64, 8, cutlass::half_t>>, Is_dropout=true, Is_causal=false, Is_even_N=true, Is_even_K=true, Return_softmax=true]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(34): here
            instantiation of "void run_flash_fwd<Kernel_traits,Is_dropout,Is_causal>(Flash_fwd_params &, cudaStream_t) [with Kernel_traits=Flash_fwd_kernel_traits<224, 128, 64, 8, false, false, cutlass::half_t, Flash_kernel_traits<224, 128, 64, 8, cutlass::half_t>>, Is_dropout=true, Is_causal=false]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(208): here
            instantiation of "void run_mha_fwd_hdim224<T>(Flash_fwd_params &, cudaStream_t) [with T=cutlass::half_t]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_hdim224_fp16_sm80.cu(8): here

/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_kernel.h(275): warning: variable "tQrQ" was declared but never referenced
          detected during:
            instantiation of "void flash::compute_attn_1rowblock<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &, int, int, int) [with Kernel_traits=Flash_fwd_kernel_traits<224, 128, 64, 8, false, false, cutlass::half_t, Flash_kernel_traits<224, 128, 64, 8, cutlass::half_t>>, Is_dropout=true, Is_causal=false, Is_even_N=true, Is_even_K=true, Return_softmax=true, Params=Flash_fwd_params]" 
(588): here
            instantiation of "void flash::compute_attn<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &) [with Kernel_traits=Flash_fwd_kernel_traits<224, 128, 64, 8, false, false, cutlass::half_t, Flash_kernel_traits<224, 128, 64, 8, cutlass::half_t>>, Is_dropout=true, Is_causal=false, Is_even_N=true, Is_even_K=true, Return_softmax=true, Params=Flash_fwd_params]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(15): here
            instantiation of "void flash_fwd_kernel<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax>(Flash_fwd_params) [with Kernel_traits=Flash_fwd_kernel_traits<224, 128, 64, 8, false, false, cutlass::half_t, Flash_kernel_traits<224, 128, 64, 8, cutlass::half_t>>, Is_dropout=true, Is_causal=false, Is_even_N=true, Is_even_K=true, Return_softmax=true]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(34): here
            instantiation of "void run_flash_fwd<Kernel_traits,Is_dropout,Is_causal>(Flash_fwd_params &, cudaStream_t) [with Kernel_traits=Flash_fwd_kernel_traits<224, 128, 64, 8, false, false, cutlass::half_t, Flash_kernel_traits<224, 128, 64, 8, cutlass::half_t>>, Is_dropout=true, Is_causal=false]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(208): here
            instantiation of "void run_mha_fwd_hdim224<T>(Flash_fwd_params &, cudaStream_t) [with T=cutlass::half_t]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_hdim224_fp16_sm80.cu(8): here

/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_kernel.h(140): warning: variable "MMA_M" was declared but never referenced
          detected during:
            instantiation of "void flash::compute_attn_1rowblock<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &, int, int, int) [with Kernel_traits=Flash_fwd_kernel_traits<224, 128, 64, 8, false, false, cutlass::half_t, Flash_kernel_traits<224, 128, 64, 8, cutlass::half_t>>, Is_dropout=true, Is_causal=false, Is_even_N=true, Is_even_K=true, Return_softmax=false, Params=Flash_fwd_params]" 
(588): here
            instantiation of "void flash::compute_attn<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &) [with Kernel_traits=Flash_fwd_kernel_traits<224, 128, 64, 8, false, false, cutlass::half_t, Flash_kernel_traits<224, 128, 64, 8, cutlass::half_t>>, Is_dropout=true, Is_causal=false, Is_even_N=true, Is_even_K=true, Return_softmax=false, Params=Flash_fwd_params]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(15): here
            instantiation of "void flash_fwd_kernel<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax>(Flash_fwd_params) [with Kernel_traits=Flash_fwd_kernel_traits<224, 128, 64, 8, false, false, cutlass::half_t, Flash_kernel_traits<224, 128, 64, 8, cutlass::half_t>>, Is_dropout=true, Is_causal=false, Is_even_N=true, Is_even_K=true, Return_softmax=false]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(34): here
            instantiation of "void run_flash_fwd<Kernel_traits,Is_dropout,Is_causal>(Flash_fwd_params &, cudaStream_t) [with Kernel_traits=Flash_fwd_kernel_traits<224, 128, 64, 8, false, false, cutlass::half_t, Flash_kernel_traits<224, 128, 64, 8, cutlass::half_t>>, Is_dropout=true, Is_causal=false]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(208): here
            instantiation of "void run_mha_fwd_hdim224<T>(Flash_fwd_params &, cudaStream_t) [with T=cutlass::half_t]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_hdim224_fp16_sm80.cu(8): here

/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_kernel.h(275): warning: variable "tQrQ" was declared but never referenced
          detected during:
            instantiation of "void flash::compute_attn_1rowblock<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &, int, int, int) [with Kernel_traits=Flash_fwd_kernel_traits<224, 128, 64, 8, false, false, cutlass::half_t, Flash_kernel_traits<224, 128, 64, 8, cutlass::half_t>>, Is_dropout=true, Is_causal=false, Is_even_N=true, Is_even_K=true, Return_softmax=false, Params=Flash_fwd_params]" 
(588): here
            instantiation of "void flash::compute_attn<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &) [with Kernel_traits=Flash_fwd_kernel_traits<224, 128, 64, 8, false, false, cutlass::half_t, Flash_kernel_traits<224, 128, 64, 8, cutlass::half_t>>, Is_dropout=true, Is_causal=false, Is_even_N=true, Is_even_K=true, Return_softmax=false, Params=Flash_fwd_params]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(15): here
            instantiation of "void flash_fwd_kernel<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax>(Flash_fwd_params) [with Kernel_traits=Flash_fwd_kernel_traits<224, 128, 64, 8, false, false, cutlass::half_t, Flash_kernel_traits<224, 128, 64, 8, cutlass::half_t>>, Is_dropout=true, Is_causal=false, Is_even_N=true, Is_even_K=true, Return_softmax=false]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(34): here
            instantiation of "void run_flash_fwd<Kernel_traits,Is_dropout,Is_causal>(Flash_fwd_params &, cudaStream_t) [with Kernel_traits=Flash_fwd_kernel_traits<224, 128, 64, 8, false, false, cutlass::half_t, Flash_kernel_traits<224, 128, 64, 8, cutlass::half_t>>, Is_dropout=true, Is_causal=false]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(208): here
            instantiation of "void run_mha_fwd_hdim224<T>(Flash_fwd_params &, cudaStream_t) [with T=cutlass::half_t]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_hdim224_fp16_sm80.cu(8): here

/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_kernel.h(140): warning: variable "MMA_M" was declared but never referenced
          detected during:
            instantiation of "void flash::compute_attn_1rowblock<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &, int, int, int) [with Kernel_traits=Flash_fwd_kernel_traits<224, 128, 64, 8, false, false, cutlass::half_t, Flash_kernel_traits<224, 128, 64, 8, cutlass::half_t>>, Is_dropout=true, Is_causal=false, Is_even_N=true, Is_even_K=false, Return_softmax=true, Params=Flash_fwd_params]" 
(588): here
            instantiation of "void flash::compute_attn<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &) [with Kernel_traits=Flash_fwd_kernel_traits<224, 128, 64, 8, false, false, cutlass::half_t, Flash_kernel_traits<224, 128, 64, 8, cutlass::half_t>>, Is_dropout=true, Is_causal=false, Is_even_N=true, Is_even_K=false, Return_softmax=true, Params=Flash_fwd_params]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(15): here
            instantiation of "void flash_fwd_kernel<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax>(Flash_fwd_params) [with Kernel_traits=Flash_fwd_kernel_traits<224, 128, 64, 8, false, false, cutlass::half_t, Flash_kernel_traits<224, 128, 64, 8, cutlass::half_t>>, Is_dropout=true, Is_causal=false, Is_even_N=true, Is_even_K=false, Return_softmax=true]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(34): here
            instantiation of "void run_flash_fwd<Kernel_traits,Is_dropout,Is_causal>(Flash_fwd_params &, cudaStream_t) [with Kernel_traits=Flash_fwd_kernel_traits<224, 128, 64, 8, false, false, cutlass::half_t, Flash_kernel_traits<224, 128, 64, 8, cutlass::half_t>>, Is_dropout=true, Is_causal=false]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(208): here
            instantiation of "void run_mha_fwd_hdim224<T>(Flash_fwd_params &, cudaStream_t) [with T=cutlass::half_t]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_hdim224_fp16_sm80.cu(8): here

/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_kernel.h(275): warning: variable "tQrQ" was declared but never referenced
          detected during:
            instantiation of "void flash::compute_attn_1rowblock<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &, int, int, int) [with Kernel_traits=Flash_fwd_kernel_traits<224, 128, 64, 8, false, false, cutlass::half_t, Flash_kernel_traits<224, 128, 64, 8, cutlass::half_t>>, Is_dropout=true, Is_causal=false, Is_even_N=true, Is_even_K=false, Return_softmax=true, Params=Flash_fwd_params]" 
(588): here
            instantiation of "void flash::compute_attn<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &) [with Kernel_traits=Flash_fwd_kernel_traits<224, 128, 64, 8, false, false, cutlass::half_t, Flash_kernel_traits<224, 128, 64, 8, cutlass::half_t>>, Is_dropout=true, Is_causal=false, Is_even_N=true, Is_even_K=false, Return_softmax=true, Params=Flash_fwd_params]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(15): here
            instantiation of "void flash_fwd_kernel<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax>(Flash_fwd_params) [with Kernel_traits=Flash_fwd_kernel_traits<224, 128, 64, 8, false, false, cutlass::half_t, Flash_kernel_traits<224, 128, 64, 8, cutlass::half_t>>, Is_dropout=true, Is_causal=false, Is_even_N=true, Is_even_K=false, Return_softmax=true]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(34): here
            instantiation of "void run_flash_fwd<Kernel_traits,Is_dropout,Is_causal>(Flash_fwd_params &, cudaStream_t) [with Kernel_traits=Flash_fwd_kernel_traits<224, 128, 64, 8, false, false, cutlass::half_t, Flash_kernel_traits<224, 128, 64, 8, cutlass::half_t>>, Is_dropout=true, Is_causal=false]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(208): here
            instantiation of "void run_mha_fwd_hdim224<T>(Flash_fwd_params &, cudaStream_t) [with T=cutlass::half_t]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_hdim224_fp16_sm80.cu(8): here

/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_kernel.h(140): warning: variable "MMA_M" was declared but never referenced
          detected during:
            instantiation of "void flash::compute_attn_1rowblock<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &, int, int, int) [with Kernel_traits=Flash_fwd_kernel_traits<224, 128, 64, 8, false, false, cutlass::half_t, Flash_kernel_traits<224, 128, 64, 8, cutlass::half_t>>, Is_dropout=true, Is_causal=false, Is_even_N=true, Is_even_K=false, Return_softmax=false, Params=Flash_fwd_params]" 
(588): here
            instantiation of "void flash::compute_attn<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &) [with Kernel_traits=Flash_fwd_kernel_traits<224, 128, 64, 8, false, false, cutlass::half_t, Flash_kernel_traits<224, 128, 64, 8, cutlass::half_t>>, Is_dropout=true, Is_causal=false, Is_even_N=true, Is_even_K=false, Return_softmax=false, Params=Flash_fwd_params]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(15): here
            instantiation of "void flash_fwd_kernel<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax>(Flash_fwd_params) [with Kernel_traits=Flash_fwd_kernel_traits<224, 128, 64, 8, false, false, cutlass::half_t, Flash_kernel_traits<224, 128, 64, 8, cutlass::half_t>>, Is_dropout=true, Is_causal=false, Is_even_N=true, Is_even_K=false, Return_softmax=false]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(34): here
            instantiation of "void run_flash_fwd<Kernel_traits,Is_dropout,Is_causal>(Flash_fwd_params &, cudaStream_t) [with Kernel_traits=Flash_fwd_kernel_traits<224, 128, 64, 8, false, false, cutlass::half_t, Flash_kernel_traits<224, 128, 64, 8, cutlass::half_t>>, Is_dropout=true, Is_causal=false]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(208): here
            instantiation of "void run_mha_fwd_hdim224<T>(Flash_fwd_params &, cudaStream_t) [with T=cutlass::half_t]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_hdim224_fp16_sm80.cu(8): here

/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_kernel.h(275): warning: variable "tQrQ" was declared but never referenced
          detected during:
            instantiation of "void flash::compute_attn_1rowblock<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &, int, int, int) [with Kernel_traits=Flash_fwd_kernel_traits<224, 128, 64, 8, false, false, cutlass::half_t, Flash_kernel_traits<224, 128, 64, 8, cutlass::half_t>>, Is_dropout=true, Is_causal=false, Is_even_N=true, Is_even_K=false, Return_softmax=false, Params=Flash_fwd_params]" 
(588): here
            instantiation of "void flash::compute_attn<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &) [with Kernel_traits=Flash_fwd_kernel_traits<224, 128, 64, 8, false, false, cutlass::half_t, Flash_kernel_traits<224, 128, 64, 8, cutlass::half_t>>, Is_dropout=true, Is_causal=false, Is_even_N=true, Is_even_K=false, Return_softmax=false, Params=Flash_fwd_params]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(15): here
            instantiation of "void flash_fwd_kernel<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax>(Flash_fwd_params) [with Kernel_traits=Flash_fwd_kernel_traits<224, 128, 64, 8, false, false, cutlass::half_t, Flash_kernel_traits<224, 128, 64, 8, cutlass::half_t>>, Is_dropout=true, Is_causal=false, Is_even_N=true, Is_even_K=false, Return_softmax=false]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(34): here
            instantiation of "void run_flash_fwd<Kernel_traits,Is_dropout,Is_causal>(Flash_fwd_params &, cudaStream_t) [with Kernel_traits=Flash_fwd_kernel_traits<224, 128, 64, 8, false, false, cutlass::half_t, Flash_kernel_traits<224, 128, 64, 8, cutlass::half_t>>, Is_dropout=true, Is_causal=false]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(208): here
            instantiation of "void run_mha_fwd_hdim224<T>(Flash_fwd_params &, cudaStream_t) [with T=cutlass::half_t]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_hdim224_fp16_sm80.cu(8): here

/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_kernel.h(140): warning: variable "MMA_M" was declared but never referenced
          detected during:
            instantiation of "void flash::compute_attn_1rowblock<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &, int, int, int) [with Kernel_traits=Flash_fwd_kernel_traits<224, 128, 64, 8, false, false, cutlass::half_t, Flash_kernel_traits<224, 128, 64, 8, cutlass::half_t>>, Is_dropout=true, Is_causal=false, Is_even_N=false, Is_even_K=true, Return_softmax=true, Params=Flash_fwd_params]" 
(588): here
            instantiation of "void flash::compute_attn<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &) [with Kernel_traits=Flash_fwd_kernel_traits<224, 128, 64, 8, false, false, cutlass::half_t, Flash_kernel_traits<224, 128, 64, 8, cutlass::half_t>>, Is_dropout=true, Is_causal=false, Is_even_N=false, Is_even_K=true, Return_softmax=true, Params=Flash_fwd_params]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(15): here
            instantiation of "void flash_fwd_kernel<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax>(Flash_fwd_params) [with Kernel_traits=Flash_fwd_kernel_traits<224, 128, 64, 8, false, false, cutlass::half_t, Flash_kernel_traits<224, 128, 64, 8, cutlass::half_t>>, Is_dropout=true, Is_causal=false, Is_even_N=false, Is_even_K=true, Return_softmax=true]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(34): here
            instantiation of "void run_flash_fwd<Kernel_traits,Is_dropout,Is_causal>(Flash_fwd_params &, cudaStream_t) [with Kernel_traits=Flash_fwd_kernel_traits<224, 128, 64, 8, false, false, cutlass::half_t, Flash_kernel_traits<224, 128, 64, 8, cutlass::half_t>>, Is_dropout=true, Is_causal=false]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(208): here
            instantiation of "void run_mha_fwd_hdim224<T>(Flash_fwd_params &, cudaStream_t) [with T=cutlass::half_t]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_hdim224_fp16_sm80.cu(8): here

/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_kernel.h(275): warning: variable "tQrQ" was declared but never referenced
          detected during:
            instantiation of "void flash::compute_attn_1rowblock<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &, int, int, int) [with Kernel_traits=Flash_fwd_kernel_traits<224, 128, 64, 8, false, false, cutlass::half_t, Flash_kernel_traits<224, 128, 64, 8, cutlass::half_t>>, Is_dropout=true, Is_causal=false, Is_even_N=false, Is_even_K=true, Return_softmax=true, Params=Flash_fwd_params]" 
(588): here
            instantiation of "void flash::compute_attn<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &) [with Kernel_traits=Flash_fwd_kernel_traits<224, 128, 64, 8, false, false, cutlass::half_t, Flash_kernel_traits<224, 128, 64, 8, cutlass::half_t>>, Is_dropout=true, Is_causal=false, Is_even_N=false, Is_even_K=true, Return_softmax=true, Params=Flash_fwd_params]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(15): here
            instantiation of "void flash_fwd_kernel<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax>(Flash_fwd_params) [with Kernel_traits=Flash_fwd_kernel_traits<224, 128, 64, 8, false, false, cutlass::half_t, Flash_kernel_traits<224, 128, 64, 8, cutlass::half_t>>, Is_dropout=true, Is_causal=false, Is_even_N=false, Is_even_K=true, Return_softmax=true]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(34): here
            instantiation of "void run_flash_fwd<Kernel_traits,Is_dropout,Is_causal>(Flash_fwd_params &, cudaStream_t) [with Kernel_traits=Flash_fwd_kernel_traits<224, 128, 64, 8, false, false, cutlass::half_t, Flash_kernel_traits<224, 128, 64, 8, cutlass::half_t>>, Is_dropout=true, Is_causal=false]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(208): here
            instantiation of "void run_mha_fwd_hdim224<T>(Flash_fwd_params &, cudaStream_t) [with T=cutlass::half_t]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_hdim224_fp16_sm80.cu(8): here

/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_kernel.h(140): warning: variable "MMA_M" was declared but never referenced
          detected during:
            instantiation of "void flash::compute_attn_1rowblock<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &, int, int, int) [with Kernel_traits=Flash_fwd_kernel_traits<224, 128, 64, 8, false, false, cutlass::half_t, Flash_kernel_traits<224, 128, 64, 8, cutlass::half_t>>, Is_dropout=true, Is_causal=false, Is_even_N=false, Is_even_K=true, Return_softmax=false, Params=Flash_fwd_params]" 
(588): here
            instantiation of "void flash::compute_attn<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &) [with Kernel_traits=Flash_fwd_kernel_traits<224, 128, 64, 8, false, false, cutlass::half_t, Flash_kernel_traits<224, 128, 64, 8, cutlass::half_t>>, Is_dropout=true, Is_causal=false, Is_even_N=false, Is_even_K=true, Return_softmax=false, Params=Flash_fwd_params]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(15): here
            instantiation of "void flash_fwd_kernel<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax>(Flash_fwd_params) [with Kernel_traits=Flash_fwd_kernel_traits<224, 128, 64, 8, false, false, cutlass::half_t, Flash_kernel_traits<224, 128, 64, 8, cutlass::half_t>>, Is_dropout=true, Is_causal=false, Is_even_N=false, Is_even_K=true, Return_softmax=false]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(34): here
            instantiation of "void run_flash_fwd<Kernel_traits,Is_dropout,Is_causal>(Flash_fwd_params &, cudaStream_t) [with Kernel_traits=Flash_fwd_kernel_traits<224, 128, 64, 8, false, false, cutlass::half_t, Flash_kernel_traits<224, 128, 64, 8, cutlass::half_t>>, Is_dropout=true, Is_causal=false]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(208): here
            instantiation of "void run_mha_fwd_hdim224<T>(Flash_fwd_params &, cudaStream_t) [with T=cutlass::half_t]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_hdim224_fp16_sm80.cu(8): here

/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_kernel.h(275): warning: variable "tQrQ" was declared but never referenced
          detected during:
            instantiation of "void flash::compute_attn_1rowblock<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &, int, int, int) [with Kernel_traits=Flash_fwd_kernel_traits<224, 128, 64, 8, false, false, cutlass::half_t, Flash_kernel_traits<224, 128, 64, 8, cutlass::half_t>>, Is_dropout=true, Is_causal=false, Is_even_N=false, Is_even_K=true, Return_softmax=false, Params=Flash_fwd_params]" 
(588): here
            instantiation of "void flash::compute_attn<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &) [with Kernel_traits=Flash_fwd_kernel_traits<224, 128, 64, 8, false, false, cutlass::half_t, Flash_kernel_traits<224, 128, 64, 8, cutlass::half_t>>, Is_dropout=true, Is_causal=false, Is_even_N=false, Is_even_K=true, Return_softmax=false, Params=Flash_fwd_params]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(15): here
            instantiation of "void flash_fwd_kernel<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax>(Flash_fwd_params) [with Kernel_traits=Flash_fwd_kernel_traits<224, 128, 64, 8, false, false, cutlass::half_t, Flash_kernel_traits<224, 128, 64, 8, cutlass::half_t>>, Is_dropout=true, Is_causal=false, Is_even_N=false, Is_even_K=true, Return_softmax=false]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(34): here
            instantiation of "void run_flash_fwd<Kernel_traits,Is_dropout,Is_causal>(Flash_fwd_params &, cudaStream_t) [with Kernel_traits=Flash_fwd_kernel_traits<224, 128, 64, 8, false, false, cutlass::half_t, Flash_kernel_traits<224, 128, 64, 8, cutlass::half_t>>, Is_dropout=true, Is_causal=false]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(208): here
            instantiation of "void run_mha_fwd_hdim224<T>(Flash_fwd_params &, cudaStream_t) [with T=cutlass::half_t]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_hdim224_fp16_sm80.cu(8): here

/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_kernel.h(140): warning: variable "MMA_M" was declared but never referenced
          detected during:
            instantiation of "void flash::compute_attn_1rowblock<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &, int, int, int) [with Kernel_traits=Flash_fwd_kernel_traits<224, 128, 64, 8, false, false, cutlass::half_t, Flash_kernel_traits<224, 128, 64, 8, cutlass::half_t>>, Is_dropout=true, Is_causal=false, Is_even_N=false, Is_even_K=false, Return_softmax=true, Params=Flash_fwd_params]" 
(588): here
            instantiation of "void flash::compute_attn<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &) [with Kernel_traits=Flash_fwd_kernel_traits<224, 128, 64, 8, false, false, cutlass::half_t, Flash_kernel_traits<224, 128, 64, 8, cutlass::half_t>>, Is_dropout=true, Is_causal=false, Is_even_N=false, Is_even_K=false, Return_softmax=true, Params=Flash_fwd_params]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(15): here
            instantiation of "void flash_fwd_kernel<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax>(Flash_fwd_params) [with Kernel_traits=Flash_fwd_kernel_traits<224, 128, 64, 8, false, false, cutlass::half_t, Flash_kernel_traits<224, 128, 64, 8, cutlass::half_t>>, Is_dropout=true, Is_causal=false, Is_even_N=false, Is_even_K=false, Return_softmax=true]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(34): here
            instantiation of "void run_flash_fwd<Kernel_traits,Is_dropout,Is_causal>(Flash_fwd_params &, cudaStream_t) [with Kernel_traits=Flash_fwd_kernel_traits<224, 128, 64, 8, false, false, cutlass::half_t, Flash_kernel_traits<224, 128, 64, 8, cutlass::half_t>>, Is_dropout=true, Is_causal=false]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(208): here
            instantiation of "void run_mha_fwd_hdim224<T>(Flash_fwd_params &, cudaStream_t) [with T=cutlass::half_t]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_hdim224_fp16_sm80.cu(8): here

/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_kernel.h(275): warning: variable "tQrQ" was declared but never referenced
          detected during:
            instantiation of "void flash::compute_attn_1rowblock<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &, int, int, int) [with Kernel_traits=Flash_fwd_kernel_traits<224, 128, 64, 8, false, false, cutlass::half_t, Flash_kernel_traits<224, 128, 64, 8, cutlass::half_t>>, Is_dropout=true, Is_causal=false, Is_even_N=false, Is_even_K=false, Return_softmax=true, Params=Flash_fwd_params]" 
(588): here
            instantiation of "void flash::compute_attn<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &) [with Kernel_traits=Flash_fwd_kernel_traits<224, 128, 64, 8, false, false, cutlass::half_t, Flash_kernel_traits<224, 128, 64, 8, cutlass::half_t>>, Is_dropout=true, Is_causal=false, Is_even_N=false, Is_even_K=false, Return_softmax=true, Params=Flash_fwd_params]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(15): here
            instantiation of "void flash_fwd_kernel<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax>(Flash_fwd_params) [with Kernel_traits=Flash_fwd_kernel_traits<224, 128, 64, 8, false, false, cutlass::half_t, Flash_kernel_traits<224, 128, 64, 8, cutlass::half_t>>, Is_dropout=true, Is_causal=false, Is_even_N=false, Is_even_K=false, Return_softmax=true]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(34): here
            instantiation of "void run_flash_fwd<Kernel_traits,Is_dropout,Is_causal>(Flash_fwd_params &, cudaStream_t) [with Kernel_traits=Flash_fwd_kernel_traits<224, 128, 64, 8, false, false, cutlass::half_t, Flash_kernel_traits<224, 128, 64, 8, cutlass::half_t>>, Is_dropout=true, Is_causal=false]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(208): here
            instantiation of "void run_mha_fwd_hdim224<T>(Flash_fwd_params &, cudaStream_t) [with T=cutlass::half_t]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_hdim224_fp16_sm80.cu(8): here

/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_kernel.h(140): warning: variable "MMA_M" was declared but never referenced
          detected during:
            instantiation of "void flash::compute_attn_1rowblock<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &, int, int, int) [with Kernel_traits=Flash_fwd_kernel_traits<224, 128, 64, 8, false, false, cutlass::half_t, Flash_kernel_traits<224, 128, 64, 8, cutlass::half_t>>, Is_dropout=true, Is_causal=false, Is_even_N=false, Is_even_K=false, Return_softmax=false, Params=Flash_fwd_params]" 
(588): here
            instantiation of "void flash::compute_attn<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &) [with Kernel_traits=Flash_fwd_kernel_traits<224, 128, 64, 8, false, false, cutlass::half_t, Flash_kernel_traits<224, 128, 64, 8, cutlass::half_t>>, Is_dropout=true, Is_causal=false, Is_even_N=false, Is_even_K=false, Return_softmax=false, Params=Flash_fwd_params]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(15): here
            instantiation of "void flash_fwd_kernel<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax>(Flash_fwd_params) [with Kernel_traits=Flash_fwd_kernel_traits<224, 128, 64, 8, false, false, cutlass::half_t, Flash_kernel_traits<224, 128, 64, 8, cutlass::half_t>>, Is_dropout=true, Is_causal=false, Is_even_N=false, Is_even_K=false, Return_softmax=false]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(34): here
            instantiation of "void run_flash_fwd<Kernel_traits,Is_dropout,Is_causal>(Flash_fwd_params &, cudaStream_t) [with Kernel_traits=Flash_fwd_kernel_traits<224, 128, 64, 8, false, false, cutlass::half_t, Flash_kernel_traits<224, 128, 64, 8, cutlass::half_t>>, Is_dropout=true, Is_causal=false]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(208): here
            instantiation of "void run_mha_fwd_hdim224<T>(Flash_fwd_params &, cudaStream_t) [with T=cutlass::half_t]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_hdim224_fp16_sm80.cu(8): here

/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_kernel.h(275): warning: variable "tQrQ" was declared but never referenced
          detected during:
            instantiation of "void flash::compute_attn_1rowblock<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &, int, int, int) [with Kernel_traits=Flash_fwd_kernel_traits<224, 128, 64, 8, false, false, cutlass::half_t, Flash_kernel_traits<224, 128, 64, 8, cutlass::half_t>>, Is_dropout=true, Is_causal=false, Is_even_N=false, Is_even_K=false, Return_softmax=false, Params=Flash_fwd_params]" 
(588): here
            instantiation of "void flash::compute_attn<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &) [with Kernel_traits=Flash_fwd_kernel_traits<224, 128, 64, 8, false, false, cutlass::half_t, Flash_kernel_traits<224, 128, 64, 8, cutlass::half_t>>, Is_dropout=true, Is_causal=false, Is_even_N=false, Is_even_K=false, Return_softmax=false, Params=Flash_fwd_params]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(15): here
            instantiation of "void flash_fwd_kernel<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax>(Flash_fwd_params) [with Kernel_traits=Flash_fwd_kernel_traits<224, 128, 64, 8, false, false, cutlass::half_t, Flash_kernel_traits<224, 128, 64, 8, cutlass::half_t>>, Is_dropout=true, Is_causal=false, Is_even_N=false, Is_even_K=false, Return_softmax=false]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(34): here
            instantiation of "void run_flash_fwd<Kernel_traits,Is_dropout,Is_causal>(Flash_fwd_params &, cudaStream_t) [with Kernel_traits=Flash_fwd_kernel_traits<224, 128, 64, 8, false, false, cutlass::half_t, Flash_kernel_traits<224, 128, 64, 8, cutlass::half_t>>, Is_dropout=true, Is_causal=false]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(208): here
            instantiation of "void run_mha_fwd_hdim224<T>(Flash_fwd_params &, cudaStream_t) [with T=cutlass::half_t]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_hdim224_fp16_sm80.cu(8): here

/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_kernel.h(140): warning: variable "MMA_M" was declared but never referenced
          detected during:
            instantiation of "void flash::compute_attn_1rowblock<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &, int, int, int) [with Kernel_traits=Flash_fwd_kernel_traits<224, 64, 64, 4, false, false, cutlass::half_t, Flash_kernel_traits<224, 64, 64, 4, cutlass::half_t>>, Is_dropout=true, Is_causal=false, Is_even_N=true, Is_even_K=true, Return_softmax=true, Params=Flash_fwd_params]" 
(588): here
            instantiation of "void flash::compute_attn<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &) [with Kernel_traits=Flash_fwd_kernel_traits<224, 64, 64, 4, false, false, cutlass::half_t, Flash_kernel_traits<224, 64, 64, 4, cutlass::half_t>>, Is_dropout=true, Is_causal=false, Is_even_N=true, Is_even_K=true, Return_softmax=true, Params=Flash_fwd_params]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(15): here
            instantiation of "void flash_fwd_kernel<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax>(Flash_fwd_params) [with Kernel_traits=Flash_fwd_kernel_traits<224, 64, 64, 4, false, false, cutlass::half_t, Flash_kernel_traits<224, 64, 64, 4, cutlass::half_t>>, Is_dropout=true, Is_causal=false, Is_even_N=true, Is_even_K=true, Return_softmax=true]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(34): here
            instantiation of "void run_flash_fwd<Kernel_traits,Is_dropout,Is_causal>(Flash_fwd_params &, cudaStream_t) [with Kernel_traits=Flash_fwd_kernel_traits<224, 64, 64, 4, false, false, cutlass::half_t, Flash_kernel_traits<224, 64, 64, 4, cutlass::half_t>>, Is_dropout=true, Is_causal=false]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(208): here
            instantiation of "void run_mha_fwd_hdim224<T>(Flash_fwd_params &, cudaStream_t) [with T=cutlass::half_t]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_hdim224_fp16_sm80.cu(8): here

/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_kernel.h(275): warning: variable "tQrQ" was declared but never referenced
          detected during:
            instantiation of "void flash::compute_attn_1rowblock<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &, int, int, int) [with Kernel_traits=Flash_fwd_kernel_traits<224, 64, 64, 4, false, false, cutlass::half_t, Flash_kernel_traits<224, 64, 64, 4, cutlass::half_t>>, Is_dropout=true, Is_causal=false, Is_even_N=true, Is_even_K=true, Return_softmax=true, Params=Flash_fwd_params]" 
(588): here
            instantiation of "void flash::compute_attn<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &) [with Kernel_traits=Flash_fwd_kernel_traits<224, 64, 64, 4, false, false, cutlass::half_t, Flash_kernel_traits<224, 64, 64, 4, cutlass::half_t>>, Is_dropout=true, Is_causal=false, Is_even_N=true, Is_even_K=true, Return_softmax=true, Params=Flash_fwd_params]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(15): here
            instantiation of "void flash_fwd_kernel<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax>(Flash_fwd_params) [with Kernel_traits=Flash_fwd_kernel_traits<224, 64, 64, 4, false, false, cutlass::half_t, Flash_kernel_traits<224, 64, 64, 4, cutlass::half_t>>, Is_dropout=true, Is_causal=false, Is_even_N=true, Is_even_K=true, Return_softmax=true]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(34): here
            instantiation of "void run_flash_fwd<Kernel_traits,Is_dropout,Is_causal>(Flash_fwd_params &, cudaStream_t) [with Kernel_traits=Flash_fwd_kernel_traits<224, 64, 64, 4, false, false, cutlass::half_t, Flash_kernel_traits<224, 64, 64, 4, cutlass::half_t>>, Is_dropout=true, Is_causal=false]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(208): here
            instantiation of "void run_mha_fwd_hdim224<T>(Flash_fwd_params &, cudaStream_t) [with T=cutlass::half_t]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_hdim224_fp16_sm80.cu(8): here

/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_kernel.h(140): warning: variable "MMA_M" was declared but never referenced
          detected during:
            instantiation of "void flash::compute_attn_1rowblock<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &, int, int, int) [with Kernel_traits=Flash_fwd_kernel_traits<224, 64, 64, 4, false, false, cutlass::half_t, Flash_kernel_traits<224, 64, 64, 4, cutlass::half_t>>, Is_dropout=true, Is_causal=false, Is_even_N=true, Is_even_K=true, Return_softmax=false, Params=Flash_fwd_params]" 
(588): here
            instantiation of "void flash::compute_attn<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &) [with Kernel_traits=Flash_fwd_kernel_traits<224, 64, 64, 4, false, false, cutlass::half_t, Flash_kernel_traits<224, 64, 64, 4, cutlass::half_t>>, Is_dropout=true, Is_causal=false, Is_even_N=true, Is_even_K=true, Return_softmax=false, Params=Flash_fwd_params]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(15): here
            instantiation of "void flash_fwd_kernel<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax>(Flash_fwd_params) [with Kernel_traits=Flash_fwd_kernel_traits<224, 64, 64, 4, false, false, cutlass::half_t, Flash_kernel_traits<224, 64, 64, 4, cutlass::half_t>>, Is_dropout=true, Is_causal=false, Is_even_N=true, Is_even_K=true, Return_softmax=false]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(34): here
            instantiation of "void run_flash_fwd<Kernel_traits,Is_dropout,Is_causal>(Flash_fwd_params &, cudaStream_t) [with Kernel_traits=Flash_fwd_kernel_traits<224, 64, 64, 4, false, false, cutlass::half_t, Flash_kernel_traits<224, 64, 64, 4, cutlass::half_t>>, Is_dropout=true, Is_causal=false]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(208): here
            instantiation of "void run_mha_fwd_hdim224<T>(Flash_fwd_params &, cudaStream_t) [with T=cutlass::half_t]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_hdim224_fp16_sm80.cu(8): here

/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_kernel.h(275): warning: variable "tQrQ" was declared but never referenced
          detected during:
            instantiation of "void flash::compute_attn_1rowblock<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &, int, int, int) [with Kernel_traits=Flash_fwd_kernel_traits<224, 64, 64, 4, false, false, cutlass::half_t, Flash_kernel_traits<224, 64, 64, 4, cutlass::half_t>>, Is_dropout=true, Is_causal=false, Is_even_N=true, Is_even_K=true, Return_softmax=false, Params=Flash_fwd_params]" 
(588): here
            instantiation of "void flash::compute_attn<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &) [with Kernel_traits=Flash_fwd_kernel_traits<224, 64, 64, 4, false, false, cutlass::half_t, Flash_kernel_traits<224, 64, 64, 4, cutlass::half_t>>, Is_dropout=true, Is_causal=false, Is_even_N=true, Is_even_K=true, Return_softmax=false, Params=Flash_fwd_params]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(15): here
            instantiation of "void flash_fwd_kernel<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax>(Flash_fwd_params) [with Kernel_traits=Flash_fwd_kernel_traits<224, 64, 64, 4, false, false, cutlass::half_t, Flash_kernel_traits<224, 64, 64, 4, cutlass::half_t>>, Is_dropout=true, Is_causal=false, Is_even_N=true, Is_even_K=true, Return_softmax=false]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(34): here
            instantiation of "void run_flash_fwd<Kernel_traits,Is_dropout,Is_causal>(Flash_fwd_params &, cudaStream_t) [with Kernel_traits=Flash_fwd_kernel_traits<224, 64, 64, 4, false, false, cutlass::half_t, Flash_kernel_traits<224, 64, 64, 4, cutlass::half_t>>, Is_dropout=true, Is_causal=false]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(208): here
            instantiation of "void run_mha_fwd_hdim224<T>(Flash_fwd_params &, cudaStream_t) [with T=cutlass::half_t]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_hdim224_fp16_sm80.cu(8): here

/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_kernel.h(140): warning: variable "MMA_M" was declared but never referenced
          detected during:
            instantiation of "void flash::compute_attn_1rowblock<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &, int, int, int) [with Kernel_traits=Flash_fwd_kernel_traits<224, 64, 64, 4, false, false, cutlass::half_t, Flash_kernel_traits<224, 64, 64, 4, cutlass::half_t>>, Is_dropout=true, Is_causal=false, Is_even_N=true, Is_even_K=false, Return_softmax=true, Params=Flash_fwd_params]" 
(588): here
            instantiation of "void flash::compute_attn<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &) [with Kernel_traits=Flash_fwd_kernel_traits<224, 64, 64, 4, false, false, cutlass::half_t, Flash_kernel_traits<224, 64, 64, 4, cutlass::half_t>>, Is_dropout=true, Is_causal=false, Is_even_N=true, Is_even_K=false, Return_softmax=true, Params=Flash_fwd_params]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(15): here
            instantiation of "void flash_fwd_kernel<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax>(Flash_fwd_params) [with Kernel_traits=Flash_fwd_kernel_traits<224, 64, 64, 4, false, false, cutlass::half_t, Flash_kernel_traits<224, 64, 64, 4, cutlass::half_t>>, Is_dropout=true, Is_causal=false, Is_even_N=true, Is_even_K=false, Return_softmax=true]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(34): here
            instantiation of "void run_flash_fwd<Kernel_traits,Is_dropout,Is_causal>(Flash_fwd_params &, cudaStream_t) [with Kernel_traits=Flash_fwd_kernel_traits<224, 64, 64, 4, false, false, cutlass::half_t, Flash_kernel_traits<224, 64, 64, 4, cutlass::half_t>>, Is_dropout=true, Is_causal=false]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(208): here
            instantiation of "void run_mha_fwd_hdim224<T>(Flash_fwd_params &, cudaStream_t) [with T=cutlass::half_t]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_hdim224_fp16_sm80.cu(8): here

/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_kernel.h(275): warning: variable "tQrQ" was declared but never referenced
          detected during:
            instantiation of "void flash::compute_attn_1rowblock<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &, int, int, int) [with Kernel_traits=Flash_fwd_kernel_traits<224, 64, 64, 4, false, false, cutlass::half_t, Flash_kernel_traits<224, 64, 64, 4, cutlass::half_t>>, Is_dropout=true, Is_causal=false, Is_even_N=true, Is_even_K=false, Return_softmax=true, Params=Flash_fwd_params]" 
(588): here
            instantiation of "void flash::compute_attn<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &) [with Kernel_traits=Flash_fwd_kernel_traits<224, 64, 64, 4, false, false, cutlass::half_t, Flash_kernel_traits<224, 64, 64, 4, cutlass::half_t>>, Is_dropout=true, Is_causal=false, Is_even_N=true, Is_even_K=false, Return_softmax=true, Params=Flash_fwd_params]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(15): here
            instantiation of "void flash_fwd_kernel<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax>(Flash_fwd_params) [with Kernel_traits=Flash_fwd_kernel_traits<224, 64, 64, 4, false, false, cutlass::half_t, Flash_kernel_traits<224, 64, 64, 4, cutlass::half_t>>, Is_dropout=true, Is_causal=false, Is_even_N=true, Is_even_K=false, Return_softmax=true]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(34): here
            instantiation of "void run_flash_fwd<Kernel_traits,Is_dropout,Is_causal>(Flash_fwd_params &, cudaStream_t) [with Kernel_traits=Flash_fwd_kernel_traits<224, 64, 64, 4, false, false, cutlass::half_t, Flash_kernel_traits<224, 64, 64, 4, cutlass::half_t>>, Is_dropout=true, Is_causal=false]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(208): here
            instantiation of "void run_mha_fwd_hdim224<T>(Flash_fwd_params &, cudaStream_t) [with T=cutlass::half_t]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_hdim224_fp16_sm80.cu(8): here

/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_kernel.h(140): warning: variable "MMA_M" was declared but never referenced
          detected during:
            instantiation of "void flash::compute_attn_1rowblock<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &, int, int, int) [with Kernel_traits=Flash_fwd_kernel_traits<224, 64, 64, 4, false, false, cutlass::half_t, Flash_kernel_traits<224, 64, 64, 4, cutlass::half_t>>, Is_dropout=true, Is_causal=false, Is_even_N=true, Is_even_K=false, Return_softmax=false, Params=Flash_fwd_params]" 
(588): here
            instantiation of "void flash::compute_attn<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &) [with Kernel_traits=Flash_fwd_kernel_traits<224, 64, 64, 4, false, false, cutlass::half_t, Flash_kernel_traits<224, 64, 64, 4, cutlass::half_t>>, Is_dropout=true, Is_causal=false, Is_even_N=true, Is_even_K=false, Return_softmax=false, Params=Flash_fwd_params]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(15): here
            instantiation of "void flash_fwd_kernel<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax>(Flash_fwd_params) [with Kernel_traits=Flash_fwd_kernel_traits<224, 64, 64, 4, false, false, cutlass::half_t, Flash_kernel_traits<224, 64, 64, 4, cutlass::half_t>>, Is_dropout=true, Is_causal=false, Is_even_N=true, Is_even_K=false, Return_softmax=false]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(34): here
            instantiation of "void run_flash_fwd<Kernel_traits,Is_dropout,Is_causal>(Flash_fwd_params &, cudaStream_t) [with Kernel_traits=Flash_fwd_kernel_traits<224, 64, 64, 4, false, false, cutlass::half_t, Flash_kernel_traits<224, 64, 64, 4, cutlass::half_t>>, Is_dropout=true, Is_causal=false]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(208): here
            instantiation of "void run_mha_fwd_hdim224<T>(Flash_fwd_params &, cudaStream_t) [with T=cutlass::half_t]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_hdim224_fp16_sm80.cu(8): here

/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_kernel.h(275): warning: variable "tQrQ" was declared but never referenced
          detected during:
            instantiation of "void flash::compute_attn_1rowblock<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &, int, int, int) [with Kernel_traits=Flash_fwd_kernel_traits<224, 64, 64, 4, false, false, cutlass::half_t, Flash_kernel_traits<224, 64, 64, 4, cutlass::half_t>>, Is_dropout=true, Is_causal=false, Is_even_N=true, Is_even_K=false, Return_softmax=false, Params=Flash_fwd_params]" 
(588): here
            instantiation of "void flash::compute_attn<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &) [with Kernel_traits=Flash_fwd_kernel_traits<224, 64, 64, 4, false, false, cutlass::half_t, Flash_kernel_traits<224, 64, 64, 4, cutlass::half_t>>, Is_dropout=true, Is_causal=false, Is_even_N=true, Is_even_K=false, Return_softmax=false, Params=Flash_fwd_params]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(15): here
            instantiation of "void flash_fwd_kernel<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax>(Flash_fwd_params) [with Kernel_traits=Flash_fwd_kernel_traits<224, 64, 64, 4, false, false, cutlass::half_t, Flash_kernel_traits<224, 64, 64, 4, cutlass::half_t>>, Is_dropout=true, Is_causal=false, Is_even_N=true, Is_even_K=false, Return_softmax=false]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(34): here
            instantiation of "void run_flash_fwd<Kernel_traits,Is_dropout,Is_causal>(Flash_fwd_params &, cudaStream_t) [with Kernel_traits=Flash_fwd_kernel_traits<224, 64, 64, 4, false, false, cutlass::half_t, Flash_kernel_traits<224, 64, 64, 4, cutlass::half_t>>, Is_dropout=true, Is_causal=false]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(208): here
            instantiation of "void run_mha_fwd_hdim224<T>(Flash_fwd_params &, cudaStream_t) [with T=cutlass::half_t]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_hdim224_fp16_sm80.cu(8): here

/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_kernel.h(140): warning: variable "MMA_M" was declared but never referenced
          detected during:
            instantiation of "void flash::compute_attn_1rowblock<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &, int, int, int) [with Kernel_traits=Flash_fwd_kernel_traits<224, 64, 64, 4, false, false, cutlass::half_t, Flash_kernel_traits<224, 64, 64, 4, cutlass::half_t>>, Is_dropout=true, Is_causal=false, Is_even_N=false, Is_even_K=true, Return_softmax=true, Params=Flash_fwd_params]" 
(588): here
            instantiation of "void flash::compute_attn<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &) [with Kernel_traits=Flash_fwd_kernel_traits<224, 64, 64, 4, false, false, cutlass::half_t, Flash_kernel_traits<224, 64, 64, 4, cutlass::half_t>>, Is_dropout=true, Is_causal=false, Is_even_N=false, Is_even_K=true, Return_softmax=true, Params=Flash_fwd_params]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(15): here
            instantiation of "void flash_fwd_kernel<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax>(Flash_fwd_params) [with Kernel_traits=Flash_fwd_kernel_traits<224, 64, 64, 4, false, false, cutlass::half_t, Flash_kernel_traits<224, 64, 64, 4, cutlass::half_t>>, Is_dropout=true, Is_causal=false, Is_even_N=false, Is_even_K=true, Return_softmax=true]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(34): here
            instantiation of "void run_flash_fwd<Kernel_traits,Is_dropout,Is_causal>(Flash_fwd_params &, cudaStream_t) [with Kernel_traits=Flash_fwd_kernel_traits<224, 64, 64, 4, false, false, cutlass::half_t, Flash_kernel_traits<224, 64, 64, 4, cutlass::half_t>>, Is_dropout=true, Is_causal=false]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(208): here
            instantiation of "void run_mha_fwd_hdim224<T>(Flash_fwd_params &, cudaStream_t) [with T=cutlass::half_t]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_hdim224_fp16_sm80.cu(8): here

/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_kernel.h(275): warning: variable "tQrQ" was declared but never referenced
          detected during:
            instantiation of "void flash::compute_attn_1rowblock<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &, int, int, int) [with Kernel_traits=Flash_fwd_kernel_traits<224, 64, 64, 4, false, false, cutlass::half_t, Flash_kernel_traits<224, 64, 64, 4, cutlass::half_t>>, Is_dropout=true, Is_causal=false, Is_even_N=false, Is_even_K=true, Return_softmax=true, Params=Flash_fwd_params]" 
(588): here
            instantiation of "void flash::compute_attn<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &) [with Kernel_traits=Flash_fwd_kernel_traits<224, 64, 64, 4, false, false, cutlass::half_t, Flash_kernel_traits<224, 64, 64, 4, cutlass::half_t>>, Is_dropout=true, Is_causal=false, Is_even_N=false, Is_even_K=true, Return_softmax=true, Params=Flash_fwd_params]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(15): here
            instantiation of "void flash_fwd_kernel<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax>(Flash_fwd_params) [with Kernel_traits=Flash_fwd_kernel_traits<224, 64, 64, 4, false, false, cutlass::half_t, Flash_kernel_traits<224, 64, 64, 4, cutlass::half_t>>, Is_dropout=true, Is_causal=false, Is_even_N=false, Is_even_K=true, Return_softmax=true]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(34): here
            instantiation of "void run_flash_fwd<Kernel_traits,Is_dropout,Is_causal>(Flash_fwd_params &, cudaStream_t) [with Kernel_traits=Flash_fwd_kernel_traits<224, 64, 64, 4, false, false, cutlass::half_t, Flash_kernel_traits<224, 64, 64, 4, cutlass::half_t>>, Is_dropout=true, Is_causal=false]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(208): here
            instantiation of "void run_mha_fwd_hdim224<T>(Flash_fwd_params &, cudaStream_t) [with T=cutlass::half_t]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_hdim224_fp16_sm80.cu(8): here

/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_kernel.h(140): warning: variable "MMA_M" was declared but never referenced
          detected during:
            instantiation of "void flash::compute_attn_1rowblock<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &, int, int, int) [with Kernel_traits=Flash_fwd_kernel_traits<224, 64, 64, 4, false, false, cutlass::half_t, Flash_kernel_traits<224, 64, 64, 4, cutlass::half_t>>, Is_dropout=true, Is_causal=false, Is_even_N=false, Is_even_K=true, Return_softmax=false, Params=Flash_fwd_params]" 
(588): here
            instantiation of "void flash::compute_attn<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &) [with Kernel_traits=Flash_fwd_kernel_traits<224, 64, 64, 4, false, false, cutlass::half_t, Flash_kernel_traits<224, 64, 64, 4, cutlass::half_t>>, Is_dropout=true, Is_causal=false, Is_even_N=false, Is_even_K=true, Return_softmax=false, Params=Flash_fwd_params]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(15): here
            instantiation of "void flash_fwd_kernel<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax>(Flash_fwd_params) [with Kernel_traits=Flash_fwd_kernel_traits<224, 64, 64, 4, false, false, cutlass::half_t, Flash_kernel_traits<224, 64, 64, 4, cutlass::half_t>>, Is_dropout=true, Is_causal=false, Is_even_N=false, Is_even_K=true, Return_softmax=false]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(34): here
            instantiation of "void run_flash_fwd<Kernel_traits,Is_dropout,Is_causal>(Flash_fwd_params &, cudaStream_t) [with Kernel_traits=Flash_fwd_kernel_traits<224, 64, 64, 4, false, false, cutlass::half_t, Flash_kernel_traits<224, 64, 64, 4, cutlass::half_t>>, Is_dropout=true, Is_causal=false]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(208): here
            instantiation of "void run_mha_fwd_hdim224<T>(Flash_fwd_params &, cudaStream_t) [with T=cutlass::half_t]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_hdim224_fp16_sm80.cu(8): here

/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_kernel.h(275): warning: variable "tQrQ" was declared but never referenced
          detected during:
            instantiation of "void flash::compute_attn_1rowblock<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &, int, int, int) [with Kernel_traits=Flash_fwd_kernel_traits<224, 64, 64, 4, false, false, cutlass::half_t, Flash_kernel_traits<224, 64, 64, 4, cutlass::half_t>>, Is_dropout=true, Is_causal=false, Is_even_N=false, Is_even_K=true, Return_softmax=false, Params=Flash_fwd_params]" 
(588): here
            instantiation of "void flash::compute_attn<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &) [with Kernel_traits=Flash_fwd_kernel_traits<224, 64, 64, 4, false, false, cutlass::half_t, Flash_kernel_traits<224, 64, 64, 4, cutlass::half_t>>, Is_dropout=true, Is_causal=false, Is_even_N=false, Is_even_K=true, Return_softmax=false, Params=Flash_fwd_params]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(15): here
            instantiation of "void flash_fwd_kernel<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax>(Flash_fwd_params) [with Kernel_traits=Flash_fwd_kernel_traits<224, 64, 64, 4, false, false, cutlass::half_t, Flash_kernel_traits<224, 64, 64, 4, cutlass::half_t>>, Is_dropout=true, Is_causal=false, Is_even_N=false, Is_even_K=true, Return_softmax=false]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(34): here
            instantiation of "void run_flash_fwd<Kernel_traits,Is_dropout,Is_causal>(Flash_fwd_params &, cudaStream_t) [with Kernel_traits=Flash_fwd_kernel_traits<224, 64, 64, 4, false, false, cutlass::half_t, Flash_kernel_traits<224, 64, 64, 4, cutlass::half_t>>, Is_dropout=true, Is_causal=false]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(208): here
            instantiation of "void run_mha_fwd_hdim224<T>(Flash_fwd_params &, cudaStream_t) [with T=cutlass::half_t]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_hdim224_fp16_sm80.cu(8): here

/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_kernel.h(140): warning: variable "MMA_M" was declared but never referenced
          detected during:
            instantiation of "void flash::compute_attn_1rowblock<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &, int, int, int) [with Kernel_traits=Flash_fwd_kernel_traits<224, 64, 64, 4, false, false, cutlass::half_t, Flash_kernel_traits<224, 64, 64, 4, cutlass::half_t>>, Is_dropout=true, Is_causal=false, Is_even_N=false, Is_even_K=false, Return_softmax=true, Params=Flash_fwd_params]" 
(588): here
            instantiation of "void flash::compute_attn<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &) [with Kernel_traits=Flash_fwd_kernel_traits<224, 64, 64, 4, false, false, cutlass::half_t, Flash_kernel_traits<224, 64, 64, 4, cutlass::half_t>>, Is_dropout=true, Is_causal=false, Is_even_N=false, Is_even_K=false, Return_softmax=true, Params=Flash_fwd_params]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(15): here
            instantiation of "void flash_fwd_kernel<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax>(Flash_fwd_params) [with Kernel_traits=Flash_fwd_kernel_traits<224, 64, 64, 4, false, false, cutlass::half_t, Flash_kernel_traits<224, 64, 64, 4, cutlass::half_t>>, Is_dropout=true, Is_causal=false, Is_even_N=false, Is_even_K=false, Return_softmax=true]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(34): here
            instantiation of "void run_flash_fwd<Kernel_traits,Is_dropout,Is_causal>(Flash_fwd_params &, cudaStream_t) [with Kernel_traits=Flash_fwd_kernel_traits<224, 64, 64, 4, false, false, cutlass::half_t, Flash_kernel_traits<224, 64, 64, 4, cutlass::half_t>>, Is_dropout=true, Is_causal=false]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(208): here
            instantiation of "void run_mha_fwd_hdim224<T>(Flash_fwd_params &, cudaStream_t) [with T=cutlass::half_t]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_hdim224_fp16_sm80.cu(8): here

/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_kernel.h(275): warning: variable "tQrQ" was declared but never referenced
          detected during:
            instantiation of "void flash::compute_attn_1rowblock<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &, int, int, int) [with Kernel_traits=Flash_fwd_kernel_traits<224, 64, 64, 4, false, false, cutlass::half_t, Flash_kernel_traits<224, 64, 64, 4, cutlass::half_t>>, Is_dropout=true, Is_causal=false, Is_even_N=false, Is_even_K=false, Return_softmax=true, Params=Flash_fwd_params]" 
(588): here
            instantiation of "void flash::compute_attn<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &) [with Kernel_traits=Flash_fwd_kernel_traits<224, 64, 64, 4, false, false, cutlass::half_t, Flash_kernel_traits<224, 64, 64, 4, cutlass::half_t>>, Is_dropout=true, Is_causal=false, Is_even_N=false, Is_even_K=false, Return_softmax=true, Params=Flash_fwd_params]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(15): here
            instantiation of "void flash_fwd_kernel<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax>(Flash_fwd_params) [with Kernel_traits=Flash_fwd_kernel_traits<224, 64, 64, 4, false, false, cutlass::half_t, Flash_kernel_traits<224, 64, 64, 4, cutlass::half_t>>, Is_dropout=true, Is_causal=false, Is_even_N=false, Is_even_K=false, Return_softmax=true]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(34): here
            instantiation of "void run_flash_fwd<Kernel_traits,Is_dropout,Is_causal>(Flash_fwd_params &, cudaStream_t) [with Kernel_traits=Flash_fwd_kernel_traits<224, 64, 64, 4, false, false, cutlass::half_t, Flash_kernel_traits<224, 64, 64, 4, cutlass::half_t>>, Is_dropout=true, Is_causal=false]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(208): here
            instantiation of "void run_mha_fwd_hdim224<T>(Flash_fwd_params &, cudaStream_t) [with T=cutlass::half_t]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_hdim224_fp16_sm80.cu(8): here

/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_kernel.h(140): warning: variable "MMA_M" was declared but never referenced
          detected during:
            instantiation of "void flash::compute_attn_1rowblock<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &, int, int, int) [with Kernel_traits=Flash_fwd_kernel_traits<224, 64, 64, 4, false, false, cutlass::half_t, Flash_kernel_traits<224, 64, 64, 4, cutlass::half_t>>, Is_dropout=true, Is_causal=false, Is_even_N=false, Is_even_K=false, Return_softmax=false, Params=Flash_fwd_params]" 
(588): here
            instantiation of "void flash::compute_attn<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &) [with Kernel_traits=Flash_fwd_kernel_traits<224, 64, 64, 4, false, false, cutlass::half_t, Flash_kernel_traits<224, 64, 64, 4, cutlass::half_t>>, Is_dropout=true, Is_causal=false, Is_even_N=false, Is_even_K=false, Return_softmax=false, Params=Flash_fwd_params]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(15): here
            instantiation of "void flash_fwd_kernel<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax>(Flash_fwd_params) [with Kernel_traits=Flash_fwd_kernel_traits<224, 64, 64, 4, false, false, cutlass::half_t, Flash_kernel_traits<224, 64, 64, 4, cutlass::half_t>>, Is_dropout=true, Is_causal=false, Is_even_N=false, Is_even_K=false, Return_softmax=false]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(34): here
            instantiation of "void run_flash_fwd<Kernel_traits,Is_dropout,Is_causal>(Flash_fwd_params &, cudaStream_t) [with Kernel_traits=Flash_fwd_kernel_traits<224, 64, 64, 4, false, false, cutlass::half_t, Flash_kernel_traits<224, 64, 64, 4, cutlass::half_t>>, Is_dropout=true, Is_causal=false]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(208): here
            instantiation of "void run_mha_fwd_hdim224<T>(Flash_fwd_params &, cudaStream_t) [with T=cutlass::half_t]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_hdim224_fp16_sm80.cu(8): here

/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_kernel.h(275): warning: variable "tQrQ" was declared but never referenced
          detected during:
            instantiation of "void flash::compute_attn_1rowblock<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &, int, int, int) [with Kernel_traits=Flash_fwd_kernel_traits<224, 64, 64, 4, false, false, cutlass::half_t, Flash_kernel_traits<224, 64, 64, 4, cutlass::half_t>>, Is_dropout=true, Is_causal=false, Is_even_N=false, Is_even_K=false, Return_softmax=false, Params=Flash_fwd_params]" 
(588): here
            instantiation of "void flash::compute_attn<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &) [with Kernel_traits=Flash_fwd_kernel_traits<224, 64, 64, 4, false, false, cutlass::half_t, Flash_kernel_traits<224, 64, 64, 4, cutlass::half_t>>, Is_dropout=true, Is_causal=false, Is_even_N=false, Is_even_K=false, Return_softmax=false, Params=Flash_fwd_params]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(15): here
            instantiation of "void flash_fwd_kernel<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax>(Flash_fwd_params) [with Kernel_traits=Flash_fwd_kernel_traits<224, 64, 64, 4, false, false, cutlass::half_t, Flash_kernel_traits<224, 64, 64, 4, cutlass::half_t>>, Is_dropout=true, Is_causal=false, Is_even_N=false, Is_even_K=false, Return_softmax=false]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(34): here
            instantiation of "void run_flash_fwd<Kernel_traits,Is_dropout,Is_causal>(Flash_fwd_params &, cudaStream_t) [with Kernel_traits=Flash_fwd_kernel_traits<224, 64, 64, 4, false, false, cutlass::half_t, Flash_kernel_traits<224, 64, 64, 4, cutlass::half_t>>, Is_dropout=true, Is_causal=false]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(208): here
            instantiation of "void run_mha_fwd_hdim224<T>(Flash_fwd_params &, cudaStream_t) [with T=cutlass::half_t]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_hdim224_fp16_sm80.cu(8): here

/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_kernel.h(140): warning: variable "MMA_M" was declared but never referenced
          detected during:
            instantiation of "void flash::compute_attn_1rowblock<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &, int, int, int) [with Kernel_traits=Flash_fwd_kernel_traits<224, 128, 64, 8, false, false, cutlass::half_t, Flash_kernel_traits<224, 128, 64, 8, cutlass::half_t>>, Is_dropout=false, Is_causal=true, Is_even_N=true, Is_even_K=true, Return_softmax=false, Params=Flash_fwd_params]" 
(588): here
            instantiation of "void flash::compute_attn<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &) [with Kernel_traits=Flash_fwd_kernel_traits<224, 128, 64, 8, false, false, cutlass::half_t, Flash_kernel_traits<224, 128, 64, 8, cutlass::half_t>>, Is_dropout=false, Is_causal=true, Is_even_N=true, Is_even_K=true, Return_softmax=false, Params=Flash_fwd_params]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(15): here
            instantiation of "void flash_fwd_kernel<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax>(Flash_fwd_params) [with Kernel_traits=Flash_fwd_kernel_traits<224, 128, 64, 8, false, false, cutlass::half_t, Flash_kernel_traits<224, 128, 64, 8, cutlass::half_t>>, Is_dropout=false, Is_causal=true, Is_even_N=true, Is_even_K=true, Return_softmax=false]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(34): here
            instantiation of "void run_flash_fwd<Kernel_traits,Is_dropout,Is_causal>(Flash_fwd_params &, cudaStream_t) [with Kernel_traits=Flash_fwd_kernel_traits<224, 128, 64, 8, false, false, cutlass::half_t, Flash_kernel_traits<224, 128, 64, 8, cutlass::half_t>>, Is_dropout=false, Is_causal=true]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(208): here
            instantiation of "void run_mha_fwd_hdim224<T>(Flash_fwd_params &, cudaStream_t) [with T=cutlass::half_t]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_hdim224_fp16_sm80.cu(8): here

/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_kernel.h(275): warning: variable "tQrQ" was declared but never referenced
          detected during:
            instantiation of "void flash::compute_attn_1rowblock<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &, int, int, int) [with Kernel_traits=Flash_fwd_kernel_traits<224, 128, 64, 8, false, false, cutlass::half_t, Flash_kernel_traits<224, 128, 64, 8, cutlass::half_t>>, Is_dropout=false, Is_causal=true, Is_even_N=true, Is_even_K=true, Return_softmax=false, Params=Flash_fwd_params]" 
(588): here
            instantiation of "void flash::compute_attn<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &) [with Kernel_traits=Flash_fwd_kernel_traits<224, 128, 64, 8, false, false, cutlass::half_t, Flash_kernel_traits<224, 128, 64, 8, cutlass::half_t>>, Is_dropout=false, Is_causal=true, Is_even_N=true, Is_even_K=true, Return_softmax=false, Params=Flash_fwd_params]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(15): here
            instantiation of "void flash_fwd_kernel<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax>(Flash_fwd_params) [with Kernel_traits=Flash_fwd_kernel_traits<224, 128, 64, 8, false, false, cutlass::half_t, Flash_kernel_traits<224, 128, 64, 8, cutlass::half_t>>, Is_dropout=false, Is_causal=true, Is_even_N=true, Is_even_K=true, Return_softmax=false]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(34): here
            instantiation of "void run_flash_fwd<Kernel_traits,Is_dropout,Is_causal>(Flash_fwd_params &, cudaStream_t) [with Kernel_traits=Flash_fwd_kernel_traits<224, 128, 64, 8, false, false, cutlass::half_t, Flash_kernel_traits<224, 128, 64, 8, cutlass::half_t>>, Is_dropout=false, Is_causal=true]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(208): here
            instantiation of "void run_mha_fwd_hdim224<T>(Flash_fwd_params &, cudaStream_t) [with T=cutlass::half_t]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_hdim224_fp16_sm80.cu(8): here

/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_kernel.h(140): warning: variable "MMA_M" was declared but never referenced
          detected during:
            instantiation of "void flash::compute_attn_1rowblock<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &, int, int, int) [with Kernel_traits=Flash_fwd_kernel_traits<224, 128, 64, 8, false, false, cutlass::half_t, Flash_kernel_traits<224, 128, 64, 8, cutlass::half_t>>, Is_dropout=false, Is_causal=true, Is_even_N=true, Is_even_K=false, Return_softmax=false, Params=Flash_fwd_params]" 
(588): here
            instantiation of "void flash::compute_attn<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &) [with Kernel_traits=Flash_fwd_kernel_traits<224, 128, 64, 8, false, false, cutlass::half_t, Flash_kernel_traits<224, 128, 64, 8, cutlass::half_t>>, Is_dropout=false, Is_causal=true, Is_even_N=true, Is_even_K=false, Return_softmax=false, Params=Flash_fwd_params]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(15): here
            instantiation of "void flash_fwd_kernel<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax>(Flash_fwd_params) [with Kernel_traits=Flash_fwd_kernel_traits<224, 128, 64, 8, false, false, cutlass::half_t, Flash_kernel_traits<224, 128, 64, 8, cutlass::half_t>>, Is_dropout=false, Is_causal=true, Is_even_N=true, Is_even_K=false, Return_softmax=false]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(34): here
            instantiation of "void run_flash_fwd<Kernel_traits,Is_dropout,Is_causal>(Flash_fwd_params &, cudaStream_t) [with Kernel_traits=Flash_fwd_kernel_traits<224, 128, 64, 8, false, false, cutlass::half_t, Flash_kernel_traits<224, 128, 64, 8, cutlass::half_t>>, Is_dropout=false, Is_causal=true]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(208): here
            instantiation of "void run_mha_fwd_hdim224<T>(Flash_fwd_params &, cudaStream_t) [with T=cutlass::half_t]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_hdim224_fp16_sm80.cu(8): here

/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_kernel.h(275): warning: variable "tQrQ" was declared but never referenced
          detected during:
            instantiation of "void flash::compute_attn_1rowblock<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &, int, int, int) [with Kernel_traits=Flash_fwd_kernel_traits<224, 128, 64, 8, false, false, cutlass::half_t, Flash_kernel_traits<224, 128, 64, 8, cutlass::half_t>>, Is_dropout=false, Is_causal=true, Is_even_N=true, Is_even_K=false, Return_softmax=false, Params=Flash_fwd_params]" 
(588): here
            instantiation of "void flash::compute_attn<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &) [with Kernel_traits=Flash_fwd_kernel_traits<224, 128, 64, 8, false, false, cutlass::half_t, Flash_kernel_traits<224, 128, 64, 8, cutlass::half_t>>, Is_dropout=false, Is_causal=true, Is_even_N=true, Is_even_K=false, Return_softmax=false, Params=Flash_fwd_params]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(15): here
            instantiation of "void flash_fwd_kernel<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax>(Flash_fwd_params) [with Kernel_traits=Flash_fwd_kernel_traits<224, 128, 64, 8, false, false, cutlass::half_t, Flash_kernel_traits<224, 128, 64, 8, cutlass::half_t>>, Is_dropout=false, Is_causal=true, Is_even_N=true, Is_even_K=false, Return_softmax=false]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(34): here
            instantiation of "void run_flash_fwd<Kernel_traits,Is_dropout,Is_causal>(Flash_fwd_params &, cudaStream_t) [with Kernel_traits=Flash_fwd_kernel_traits<224, 128, 64, 8, false, false, cutlass::half_t, Flash_kernel_traits<224, 128, 64, 8, cutlass::half_t>>, Is_dropout=false, Is_causal=true]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(208): here
            instantiation of "void run_mha_fwd_hdim224<T>(Flash_fwd_params &, cudaStream_t) [with T=cutlass::half_t]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_hdim224_fp16_sm80.cu(8): here

/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_kernel.h(140): warning: variable "MMA_M" was declared but never referenced
          detected during:
            instantiation of "void flash::compute_attn_1rowblock<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &, int, int, int) [with Kernel_traits=Flash_fwd_kernel_traits<224, 128, 64, 8, false, false, cutlass::half_t, Flash_kernel_traits<224, 128, 64, 8, cutlass::half_t>>, Is_dropout=false, Is_causal=true, Is_even_N=false, Is_even_K=true, Return_softmax=false, Params=Flash_fwd_params]" 
(588): here
            instantiation of "void flash::compute_attn<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &) [with Kernel_traits=Flash_fwd_kernel_traits<224, 128, 64, 8, false, false, cutlass::half_t, Flash_kernel_traits<224, 128, 64, 8, cutlass::half_t>>, Is_dropout=false, Is_causal=true, Is_even_N=false, Is_even_K=true, Return_softmax=false, Params=Flash_fwd_params]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(15): here
            instantiation of "void flash_fwd_kernel<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax>(Flash_fwd_params) [with Kernel_traits=Flash_fwd_kernel_traits<224, 128, 64, 8, false, false, cutlass::half_t, Flash_kernel_traits<224, 128, 64, 8, cutlass::half_t>>, Is_dropout=false, Is_causal=true, Is_even_N=false, Is_even_K=true, Return_softmax=false]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(34): here
            instantiation of "void run_flash_fwd<Kernel_traits,Is_dropout,Is_causal>(Flash_fwd_params &, cudaStream_t) [with Kernel_traits=Flash_fwd_kernel_traits<224, 128, 64, 8, false, false, cutlass::half_t, Flash_kernel_traits<224, 128, 64, 8, cutlass::half_t>>, Is_dropout=false, Is_causal=true]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(208): here
            instantiation of "void run_mha_fwd_hdim224<T>(Flash_fwd_params &, cudaStream_t) [with T=cutlass::half_t]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_hdim224_fp16_sm80.cu(8): here

/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_kernel.h(275): warning: variable "tQrQ" was declared but never referenced
          detected during:
            instantiation of "void flash::compute_attn_1rowblock<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &, int, int, int) [with Kernel_traits=Flash_fwd_kernel_traits<224, 128, 64, 8, false, false, cutlass::half_t, Flash_kernel_traits<224, 128, 64, 8, cutlass::half_t>>, Is_dropout=false, Is_causal=true, Is_even_N=false, Is_even_K=true, Return_softmax=false, Params=Flash_fwd_params]" 
(588): here
            instantiation of "void flash::compute_attn<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &) [with Kernel_traits=Flash_fwd_kernel_traits<224, 128, 64, 8, false, false, cutlass::half_t, Flash_kernel_traits<224, 128, 64, 8, cutlass::half_t>>, Is_dropout=false, Is_causal=true, Is_even_N=false, Is_even_K=true, Return_softmax=false, Params=Flash_fwd_params]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(15): here
            instantiation of "void flash_fwd_kernel<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax>(Flash_fwd_params) [with Kernel_traits=Flash_fwd_kernel_traits<224, 128, 64, 8, false, false, cutlass::half_t, Flash_kernel_traits<224, 128, 64, 8, cutlass::half_t>>, Is_dropout=false, Is_causal=true, Is_even_N=false, Is_even_K=true, Return_softmax=false]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(34): here
            instantiation of "void run_flash_fwd<Kernel_traits,Is_dropout,Is_causal>(Flash_fwd_params &, cudaStream_t) [with Kernel_traits=Flash_fwd_kernel_traits<224, 128, 64, 8, false, false, cutlass::half_t, Flash_kernel_traits<224, 128, 64, 8, cutlass::half_t>>, Is_dropout=false, Is_causal=true]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(208): here
            instantiation of "void run_mha_fwd_hdim224<T>(Flash_fwd_params &, cudaStream_t) [with T=cutlass::half_t]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_hdim224_fp16_sm80.cu(8): here

/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_kernel.h(140): warning: variable "MMA_M" was declared but never referenced
          detected during:
            instantiation of "void flash::compute_attn_1rowblock<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &, int, int, int) [with Kernel_traits=Flash_fwd_kernel_traits<224, 128, 64, 8, false, false, cutlass::half_t, Flash_kernel_traits<224, 128, 64, 8, cutlass::half_t>>, Is_dropout=false, Is_causal=true, Is_even_N=false, Is_even_K=false, Return_softmax=false, Params=Flash_fwd_params]" 
(588): here
            instantiation of "void flash::compute_attn<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &) [with Kernel_traits=Flash_fwd_kernel_traits<224, 128, 64, 8, false, false, cutlass::half_t, Flash_kernel_traits<224, 128, 64, 8, cutlass::half_t>>, Is_dropout=false, Is_causal=true, Is_even_N=false, Is_even_K=false, Return_softmax=false, Params=Flash_fwd_params]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(15): here
            instantiation of "void flash_fwd_kernel<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax>(Flash_fwd_params) [with Kernel_traits=Flash_fwd_kernel_traits<224, 128, 64, 8, false, false, cutlass::half_t, Flash_kernel_traits<224, 128, 64, 8, cutlass::half_t>>, Is_dropout=false, Is_causal=true, Is_even_N=false, Is_even_K=false, Return_softmax=false]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(34): here
            instantiation of "void run_flash_fwd<Kernel_traits,Is_dropout,Is_causal>(Flash_fwd_params &, cudaStream_t) [with Kernel_traits=Flash_fwd_kernel_traits<224, 128, 64, 8, false, false, cutlass::half_t, Flash_kernel_traits<224, 128, 64, 8, cutlass::half_t>>, Is_dropout=false, Is_causal=true]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(208): here
            instantiation of "void run_mha_fwd_hdim224<T>(Flash_fwd_params &, cudaStream_t) [with T=cutlass::half_t]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_hdim224_fp16_sm80.cu(8): here

/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_kernel.h(275): warning: variable "tQrQ" was declared but never referenced
          detected during:
            instantiation of "void flash::compute_attn_1rowblock<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &, int, int, int) [with Kernel_traits=Flash_fwd_kernel_traits<224, 128, 64, 8, false, false, cutlass::half_t, Flash_kernel_traits<224, 128, 64, 8, cutlass::half_t>>, Is_dropout=false, Is_causal=true, Is_even_N=false, Is_even_K=false, Return_softmax=false, Params=Flash_fwd_params]" 
(588): here
            instantiation of "void flash::compute_attn<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &) [with Kernel_traits=Flash_fwd_kernel_traits<224, 128, 64, 8, false, false, cutlass::half_t, Flash_kernel_traits<224, 128, 64, 8, cutlass::half_t>>, Is_dropout=false, Is_causal=true, Is_even_N=false, Is_even_K=false, Return_softmax=false, Params=Flash_fwd_params]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(15): here
            instantiation of "void flash_fwd_kernel<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax>(Flash_fwd_params) [with Kernel_traits=Flash_fwd_kernel_traits<224, 128, 64, 8, false, false, cutlass::half_t, Flash_kernel_traits<224, 128, 64, 8, cutlass::half_t>>, Is_dropout=false, Is_causal=true, Is_even_N=false, Is_even_K=false, Return_softmax=false]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(34): here
            instantiation of "void run_flash_fwd<Kernel_traits,Is_dropout,Is_causal>(Flash_fwd_params &, cudaStream_t) [with Kernel_traits=Flash_fwd_kernel_traits<224, 128, 64, 8, false, false, cutlass::half_t, Flash_kernel_traits<224, 128, 64, 8, cutlass::half_t>>, Is_dropout=false, Is_causal=true]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(208): here
            instantiation of "void run_mha_fwd_hdim224<T>(Flash_fwd_params &, cudaStream_t) [with T=cutlass::half_t]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_hdim224_fp16_sm80.cu(8): here

/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_kernel.h(140): warning: variable "MMA_M" was declared but never referenced
          detected during:
            instantiation of "void flash::compute_attn_1rowblock<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &, int, int, int) [with Kernel_traits=Flash_fwd_kernel_traits<224, 64, 64, 4, false, false, cutlass::half_t, Flash_kernel_traits<224, 64, 64, 4, cutlass::half_t>>, Is_dropout=false, Is_causal=true, Is_even_N=true, Is_even_K=true, Return_softmax=false, Params=Flash_fwd_params]" 
(588): here
            instantiation of "void flash::compute_attn<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &) [with Kernel_traits=Flash_fwd_kernel_traits<224, 64, 64, 4, false, false, cutlass::half_t, Flash_kernel_traits<224, 64, 64, 4, cutlass::half_t>>, Is_dropout=false, Is_causal=true, Is_even_N=true, Is_even_K=true, Return_softmax=false, Params=Flash_fwd_params]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(15): here
            instantiation of "void flash_fwd_kernel<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax>(Flash_fwd_params) [with Kernel_traits=Flash_fwd_kernel_traits<224, 64, 64, 4, false, false, cutlass::half_t, Flash_kernel_traits<224, 64, 64, 4, cutlass::half_t>>, Is_dropout=false, Is_causal=true, Is_even_N=true, Is_even_K=true, Return_softmax=false]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(34): here
            instantiation of "void run_flash_fwd<Kernel_traits,Is_dropout,Is_causal>(Flash_fwd_params &, cudaStream_t) [with Kernel_traits=Flash_fwd_kernel_traits<224, 64, 64, 4, false, false, cutlass::half_t, Flash_kernel_traits<224, 64, 64, 4, cutlass::half_t>>, Is_dropout=false, Is_causal=true]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(208): here
            instantiation of "void run_mha_fwd_hdim224<T>(Flash_fwd_params &, cudaStream_t) [with T=cutlass::half_t]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_hdim224_fp16_sm80.cu(8): here

/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_kernel.h(275): warning: variable "tQrQ" was declared but never referenced
          detected during:
            instantiation of "void flash::compute_attn_1rowblock<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &, int, int, int) [with Kernel_traits=Flash_fwd_kernel_traits<224, 64, 64, 4, false, false, cutlass::half_t, Flash_kernel_traits<224, 64, 64, 4, cutlass::half_t>>, Is_dropout=false, Is_causal=true, Is_even_N=true, Is_even_K=true, Return_softmax=false, Params=Flash_fwd_params]" 
(588): here
            instantiation of "void flash::compute_attn<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &) [with Kernel_traits=Flash_fwd_kernel_traits<224, 64, 64, 4, false, false, cutlass::half_t, Flash_kernel_traits<224, 64, 64, 4, cutlass::half_t>>, Is_dropout=false, Is_causal=true, Is_even_N=true, Is_even_K=true, Return_softmax=false, Params=Flash_fwd_params]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(15): here
            instantiation of "void flash_fwd_kernel<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax>(Flash_fwd_params) [with Kernel_traits=Flash_fwd_kernel_traits<224, 64, 64, 4, false, false, cutlass::half_t, Flash_kernel_traits<224, 64, 64, 4, cutlass::half_t>>, Is_dropout=false, Is_causal=true, Is_even_N=true, Is_even_K=true, Return_softmax=false]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(34): here
            instantiation of "void run_flash_fwd<Kernel_traits,Is_dropout,Is_causal>(Flash_fwd_params &, cudaStream_t) [with Kernel_traits=Flash_fwd_kernel_traits<224, 64, 64, 4, false, false, cutlass::half_t, Flash_kernel_traits<224, 64, 64, 4, cutlass::half_t>>, Is_dropout=false, Is_causal=true]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(208): here
            instantiation of "void run_mha_fwd_hdim224<T>(Flash_fwd_params &, cudaStream_t) [with T=cutlass::half_t]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_hdim224_fp16_sm80.cu(8): here

/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_kernel.h(140): warning: variable "MMA_M" was declared but never referenced
          detected during:
            instantiation of "void flash::compute_attn_1rowblock<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &, int, int, int) [with Kernel_traits=Flash_fwd_kernel_traits<224, 64, 64, 4, false, false, cutlass::half_t, Flash_kernel_traits<224, 64, 64, 4, cutlass::half_t>>, Is_dropout=false, Is_causal=true, Is_even_N=true, Is_even_K=false, Return_softmax=false, Params=Flash_fwd_params]" 
(588): here
            instantiation of "void flash::compute_attn<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &) [with Kernel_traits=Flash_fwd_kernel_traits<224, 64, 64, 4, false, false, cutlass::half_t, Flash_kernel_traits<224, 64, 64, 4, cutlass::half_t>>, Is_dropout=false, Is_causal=true, Is_even_N=true, Is_even_K=false, Return_softmax=false, Params=Flash_fwd_params]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(15): here
            instantiation of "void flash_fwd_kernel<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax>(Flash_fwd_params) [with Kernel_traits=Flash_fwd_kernel_traits<224, 64, 64, 4, false, false, cutlass::half_t, Flash_kernel_traits<224, 64, 64, 4, cutlass::half_t>>, Is_dropout=false, Is_causal=true, Is_even_N=true, Is_even_K=false, Return_softmax=false]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(34): here
            instantiation of "void run_flash_fwd<Kernel_traits,Is_dropout,Is_causal>(Flash_fwd_params &, cudaStream_t) [with Kernel_traits=Flash_fwd_kernel_traits<224, 64, 64, 4, false, false, cutlass::half_t, Flash_kernel_traits<224, 64, 64, 4, cutlass::half_t>>, Is_dropout=false, Is_causal=true]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(208): here
            instantiation of "void run_mha_fwd_hdim224<T>(Flash_fwd_params &, cudaStream_t) [with T=cutlass::half_t]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_hdim224_fp16_sm80.cu(8): here

/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_kernel.h(275): warning: variable "tQrQ" was declared but never referenced
          detected during:
            instantiation of "void flash::compute_attn_1rowblock<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &, int, int, int) [with Kernel_traits=Flash_fwd_kernel_traits<224, 64, 64, 4, false, false, cutlass::half_t, Flash_kernel_traits<224, 64, 64, 4, cutlass::half_t>>, Is_dropout=false, Is_causal=true, Is_even_N=true, Is_even_K=false, Return_softmax=false, Params=Flash_fwd_params]" 
(588): here
            instantiation of "void flash::compute_attn<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &) [with Kernel_traits=Flash_fwd_kernel_traits<224, 64, 64, 4, false, false, cutlass::half_t, Flash_kernel_traits<224, 64, 64, 4, cutlass::half_t>>, Is_dropout=false, Is_causal=true, Is_even_N=true, Is_even_K=false, Return_softmax=false, Params=Flash_fwd_params]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(15): here
            instantiation of "void flash_fwd_kernel<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax>(Flash_fwd_params) [with Kernel_traits=Flash_fwd_kernel_traits<224, 64, 64, 4, false, false, cutlass::half_t, Flash_kernel_traits<224, 64, 64, 4, cutlass::half_t>>, Is_dropout=false, Is_causal=true, Is_even_N=true, Is_even_K=false, Return_softmax=false]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(34): here
            instantiation of "void run_flash_fwd<Kernel_traits,Is_dropout,Is_causal>(Flash_fwd_params &, cudaStream_t) [with Kernel_traits=Flash_fwd_kernel_traits<224, 64, 64, 4, false, false, cutlass::half_t, Flash_kernel_traits<224, 64, 64, 4, cutlass::half_t>>, Is_dropout=false, Is_causal=true]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(208): here
            instantiation of "void run_mha_fwd_hdim224<T>(Flash_fwd_params &, cudaStream_t) [with T=cutlass::half_t]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_hdim224_fp16_sm80.cu(8): here

/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_kernel.h(140): warning: variable "MMA_M" was declared but never referenced
          detected during:
            instantiation of "void flash::compute_attn_1rowblock<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &, int, int, int) [with Kernel_traits=Flash_fwd_kernel_traits<224, 64, 64, 4, false, false, cutlass::half_t, Flash_kernel_traits<224, 64, 64, 4, cutlass::half_t>>, Is_dropout=false, Is_causal=true, Is_even_N=false, Is_even_K=true, Return_softmax=false, Params=Flash_fwd_params]" 
(588): here
            instantiation of "void flash::compute_attn<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &) [with Kernel_traits=Flash_fwd_kernel_traits<224, 64, 64, 4, false, false, cutlass::half_t, Flash_kernel_traits<224, 64, 64, 4, cutlass::half_t>>, Is_dropout=false, Is_causal=true, Is_even_N=false, Is_even_K=true, Return_softmax=false, Params=Flash_fwd_params]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(15): here
            instantiation of "void flash_fwd_kernel<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax>(Flash_fwd_params) [with Kernel_traits=Flash_fwd_kernel_traits<224, 64, 64, 4, false, false, cutlass::half_t, Flash_kernel_traits<224, 64, 64, 4, cutlass::half_t>>, Is_dropout=false, Is_causal=true, Is_even_N=false, Is_even_K=true, Return_softmax=false]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(34): here
            instantiation of "void run_flash_fwd<Kernel_traits,Is_dropout,Is_causal>(Flash_fwd_params &, cudaStream_t) [with Kernel_traits=Flash_fwd_kernel_traits<224, 64, 64, 4, false, false, cutlass::half_t, Flash_kernel_traits<224, 64, 64, 4, cutlass::half_t>>, Is_dropout=false, Is_causal=true]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(208): here
            instantiation of "void run_mha_fwd_hdim224<T>(Flash_fwd_params &, cudaStream_t) [with T=cutlass::half_t]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_hdim224_fp16_sm80.cu(8): here

/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_kernel.h(275): warning: variable "tQrQ" was declared but never referenced
          detected during:
            instantiation of "void flash::compute_attn_1rowblock<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &, int, int, int) [with Kernel_traits=Flash_fwd_kernel_traits<224, 64, 64, 4, false, false, cutlass::half_t, Flash_kernel_traits<224, 64, 64, 4, cutlass::half_t>>, Is_dropout=false, Is_causal=true, Is_even_N=false, Is_even_K=true, Return_softmax=false, Params=Flash_fwd_params]" 
(588): here
            instantiation of "void flash::compute_attn<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &) [with Kernel_traits=Flash_fwd_kernel_traits<224, 64, 64, 4, false, false, cutlass::half_t, Flash_kernel_traits<224, 64, 64, 4, cutlass::half_t>>, Is_dropout=false, Is_causal=true, Is_even_N=false, Is_even_K=true, Return_softmax=false, Params=Flash_fwd_params]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(15): here
            instantiation of "void flash_fwd_kernel<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax>(Flash_fwd_params) [with Kernel_traits=Flash_fwd_kernel_traits<224, 64, 64, 4, false, false, cutlass::half_t, Flash_kernel_traits<224, 64, 64, 4, cutlass::half_t>>, Is_dropout=false, Is_causal=true, Is_even_N=false, Is_even_K=true, Return_softmax=false]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(34): here
            instantiation of "void run_flash_fwd<Kernel_traits,Is_dropout,Is_causal>(Flash_fwd_params &, cudaStream_t) [with Kernel_traits=Flash_fwd_kernel_traits<224, 64, 64, 4, false, false, cutlass::half_t, Flash_kernel_traits<224, 64, 64, 4, cutlass::half_t>>, Is_dropout=false, Is_causal=true]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(208): here
            instantiation of "void run_mha_fwd_hdim224<T>(Flash_fwd_params &, cudaStream_t) [with T=cutlass::half_t]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_hdim224_fp16_sm80.cu(8): here

/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_kernel.h(140): warning: variable "MMA_M" was declared but never referenced
          detected during:
            instantiation of "void flash::compute_attn_1rowblock<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &, int, int, int) [with Kernel_traits=Flash_fwd_kernel_traits<224, 64, 64, 4, false, false, cutlass::half_t, Flash_kernel_traits<224, 64, 64, 4, cutlass::half_t>>, Is_dropout=false, Is_causal=true, Is_even_N=false, Is_even_K=false, Return_softmax=false, Params=Flash_fwd_params]" 
(588): here
            instantiation of "void flash::compute_attn<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &) [with Kernel_traits=Flash_fwd_kernel_traits<224, 64, 64, 4, false, false, cutlass::half_t, Flash_kernel_traits<224, 64, 64, 4, cutlass::half_t>>, Is_dropout=false, Is_causal=true, Is_even_N=false, Is_even_K=false, Return_softmax=false, Params=Flash_fwd_params]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(15): here
            instantiation of "void flash_fwd_kernel<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax>(Flash_fwd_params) [with Kernel_traits=Flash_fwd_kernel_traits<224, 64, 64, 4, false, false, cutlass::half_t, Flash_kernel_traits<224, 64, 64, 4, cutlass::half_t>>, Is_dropout=false, Is_causal=true, Is_even_N=false, Is_even_K=false, Return_softmax=false]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(34): here
            instantiation of "void run_flash_fwd<Kernel_traits,Is_dropout,Is_causal>(Flash_fwd_params &, cudaStream_t) [with Kernel_traits=Flash_fwd_kernel_traits<224, 64, 64, 4, false, false, cutlass::half_t, Flash_kernel_traits<224, 64, 64, 4, cutlass::half_t>>, Is_dropout=false, Is_causal=true]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(208): here
            instantiation of "void run_mha_fwd_hdim224<T>(Flash_fwd_params &, cudaStream_t) [with T=cutlass::half_t]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_hdim224_fp16_sm80.cu(8): here

/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_kernel.h(275): warning: variable "tQrQ" was declared but never referenced
          detected during:
            instantiation of "void flash::compute_attn_1rowblock<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &, int, int, int) [with Kernel_traits=Flash_fwd_kernel_traits<224, 64, 64, 4, false, false, cutlass::half_t, Flash_kernel_traits<224, 64, 64, 4, cutlass::half_t>>, Is_dropout=false, Is_causal=true, Is_even_N=false, Is_even_K=false, Return_softmax=false, Params=Flash_fwd_params]" 
(588): here
            instantiation of "void flash::compute_attn<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &) [with Kernel_traits=Flash_fwd_kernel_traits<224, 64, 64, 4, false, false, cutlass::half_t, Flash_kernel_traits<224, 64, 64, 4, cutlass::half_t>>, Is_dropout=false, Is_causal=true, Is_even_N=false, Is_even_K=false, Return_softmax=false, Params=Flash_fwd_params]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(15): here
            instantiation of "void flash_fwd_kernel<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax>(Flash_fwd_params) [with Kernel_traits=Flash_fwd_kernel_traits<224, 64, 64, 4, false, false, cutlass::half_t, Flash_kernel_traits<224, 64, 64, 4, cutlass::half_t>>, Is_dropout=false, Is_causal=true, Is_even_N=false, Is_even_K=false, Return_softmax=false]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(34): here
            instantiation of "void run_flash_fwd<Kernel_traits,Is_dropout,Is_causal>(Flash_fwd_params &, cudaStream_t) [with Kernel_traits=Flash_fwd_kernel_traits<224, 64, 64, 4, false, false, cutlass::half_t, Flash_kernel_traits<224, 64, 64, 4, cutlass::half_t>>, Is_dropout=false, Is_causal=true]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(208): here
            instantiation of "void run_mha_fwd_hdim224<T>(Flash_fwd_params &, cudaStream_t) [with T=cutlass::half_t]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_hdim224_fp16_sm80.cu(8): here

/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_kernel.h(140): warning: variable "MMA_M" was declared but never referenced
          detected during:
            instantiation of "void flash::compute_attn_1rowblock<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &, int, int, int) [with Kernel_traits=Flash_fwd_kernel_traits<224, 128, 64, 8, false, false, cutlass::half_t, Flash_kernel_traits<224, 128, 64, 8, cutlass::half_t>>, Is_dropout=false, Is_causal=false, Is_even_N=true, Is_even_K=true, Return_softmax=false, Params=Flash_fwd_params]" 
(588): here
            instantiation of "void flash::compute_attn<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &) [with Kernel_traits=Flash_fwd_kernel_traits<224, 128, 64, 8, false, false, cutlass::half_t, Flash_kernel_traits<224, 128, 64, 8, cutlass::half_t>>, Is_dropout=false, Is_causal=false, Is_even_N=true, Is_even_K=true, Return_softmax=false, Params=Flash_fwd_params]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(15): here
            instantiation of "void flash_fwd_kernel<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax>(Flash_fwd_params) [with Kernel_traits=Flash_fwd_kernel_traits<224, 128, 64, 8, false, false, cutlass::half_t, Flash_kernel_traits<224, 128, 64, 8, cutlass::half_t>>, Is_dropout=false, Is_causal=false, Is_even_N=true, Is_even_K=true, Return_softmax=false]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(34): here
            instantiation of "void run_flash_fwd<Kernel_traits,Is_dropout,Is_causal>(Flash_fwd_params &, cudaStream_t) [with Kernel_traits=Flash_fwd_kernel_traits<224, 128, 64, 8, false, false, cutlass::half_t, Flash_kernel_traits<224, 128, 64, 8, cutlass::half_t>>, Is_dropout=false, Is_causal=false]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(208): here
            instantiation of "void run_mha_fwd_hdim224<T>(Flash_fwd_params &, cudaStream_t) [with T=cutlass::half_t]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_hdim224_fp16_sm80.cu(8): here

/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_kernel.h(275): warning: variable "tQrQ" was declared but never referenced
          detected during:
            instantiation of "void flash::compute_attn_1rowblock<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &, int, int, int) [with Kernel_traits=Flash_fwd_kernel_traits<224, 128, 64, 8, false, false, cutlass::half_t, Flash_kernel_traits<224, 128, 64, 8, cutlass::half_t>>, Is_dropout=false, Is_causal=false, Is_even_N=true, Is_even_K=true, Return_softmax=false, Params=Flash_fwd_params]" 
(588): here
            instantiation of "void flash::compute_attn<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &) [with Kernel_traits=Flash_fwd_kernel_traits<224, 128, 64, 8, false, false, cutlass::half_t, Flash_kernel_traits<224, 128, 64, 8, cutlass::half_t>>, Is_dropout=false, Is_causal=false, Is_even_N=true, Is_even_K=true, Return_softmax=false, Params=Flash_fwd_params]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(15): here
            instantiation of "void flash_fwd_kernel<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax>(Flash_fwd_params) [with Kernel_traits=Flash_fwd_kernel_traits<224, 128, 64, 8, false, false, cutlass::half_t, Flash_kernel_traits<224, 128, 64, 8, cutlass::half_t>>, Is_dropout=false, Is_causal=false, Is_even_N=true, Is_even_K=true, Return_softmax=false]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(34): here
            instantiation of "void run_flash_fwd<Kernel_traits,Is_dropout,Is_causal>(Flash_fwd_params &, cudaStream_t) [with Kernel_traits=Flash_fwd_kernel_traits<224, 128, 64, 8, false, false, cutlass::half_t, Flash_kernel_traits<224, 128, 64, 8, cutlass::half_t>>, Is_dropout=false, Is_causal=false]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(208): here
            instantiation of "void run_mha_fwd_hdim224<T>(Flash_fwd_params &, cudaStream_t) [with T=cutlass::half_t]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_hdim224_fp16_sm80.cu(8): here

/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_kernel.h(140): warning: variable "MMA_M" was declared but never referenced
          detected during:
            instantiation of "void flash::compute_attn_1rowblock<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &, int, int, int) [with Kernel_traits=Flash_fwd_kernel_traits<224, 128, 64, 8, false, false, cutlass::half_t, Flash_kernel_traits<224, 128, 64, 8, cutlass::half_t>>, Is_dropout=false, Is_causal=false, Is_even_N=true, Is_even_K=false, Return_softmax=false, Params=Flash_fwd_params]" 
(588): here
            instantiation of "void flash::compute_attn<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &) [with Kernel_traits=Flash_fwd_kernel_traits<224, 128, 64, 8, false, false, cutlass::half_t, Flash_kernel_traits<224, 128, 64, 8, cutlass::half_t>>, Is_dropout=false, Is_causal=false, Is_even_N=true, Is_even_K=false, Return_softmax=false, Params=Flash_fwd_params]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(15): here
            instantiation of "void flash_fwd_kernel<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax>(Flash_fwd_params) [with Kernel_traits=Flash_fwd_kernel_traits<224, 128, 64, 8, false, false, cutlass::half_t, Flash_kernel_traits<224, 128, 64, 8, cutlass::half_t>>, Is_dropout=false, Is_causal=false, Is_even_N=true, Is_even_K=false, Return_softmax=false]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(34): here
            instantiation of "void run_flash_fwd<Kernel_traits,Is_dropout,Is_causal>(Flash_fwd_params &, cudaStream_t) [with Kernel_traits=Flash_fwd_kernel_traits<224, 128, 64, 8, false, false, cutlass::half_t, Flash_kernel_traits<224, 128, 64, 8, cutlass::half_t>>, Is_dropout=false, Is_causal=false]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(208): here
            instantiation of "void run_mha_fwd_hdim224<T>(Flash_fwd_params &, cudaStream_t) [with T=cutlass::half_t]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_hdim224_fp16_sm80.cu(8): here

/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_kernel.h(275): warning: variable "tQrQ" was declared but never referenced
          detected during:
            instantiation of "void flash::compute_attn_1rowblock<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &, int, int, int) [with Kernel_traits=Flash_fwd_kernel_traits<224, 128, 64, 8, false, false, cutlass::half_t, Flash_kernel_traits<224, 128, 64, 8, cutlass::half_t>>, Is_dropout=false, Is_causal=false, Is_even_N=true, Is_even_K=false, Return_softmax=false, Params=Flash_fwd_params]" 
(588): here
            instantiation of "void flash::compute_attn<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &) [with Kernel_traits=Flash_fwd_kernel_traits<224, 128, 64, 8, false, false, cutlass::half_t, Flash_kernel_traits<224, 128, 64, 8, cutlass::half_t>>, Is_dropout=false, Is_causal=false, Is_even_N=true, Is_even_K=false, Return_softmax=false, Params=Flash_fwd_params]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(15): here
            instantiation of "void flash_fwd_kernel<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax>(Flash_fwd_params) [with Kernel_traits=Flash_fwd_kernel_traits<224, 128, 64, 8, false, false, cutlass::half_t, Flash_kernel_traits<224, 128, 64, 8, cutlass::half_t>>, Is_dropout=false, Is_causal=false, Is_even_N=true, Is_even_K=false, Return_softmax=false]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(34): here
            instantiation of "void run_flash_fwd<Kernel_traits,Is_dropout,Is_causal>(Flash_fwd_params &, cudaStream_t) [with Kernel_traits=Flash_fwd_kernel_traits<224, 128, 64, 8, false, false, cutlass::half_t, Flash_kernel_traits<224, 128, 64, 8, cutlass::half_t>>, Is_dropout=false, Is_causal=false]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(208): here
            instantiation of "void run_mha_fwd_hdim224<T>(Flash_fwd_params &, cudaStream_t) [with T=cutlass::half_t]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_hdim224_fp16_sm80.cu(8): here

/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_kernel.h(140): warning: variable "MMA_M" was declared but never referenced
          detected during:
            instantiation of "void flash::compute_attn_1rowblock<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &, int, int, int) [with Kernel_traits=Flash_fwd_kernel_traits<224, 128, 64, 8, false, false, cutlass::half_t, Flash_kernel_traits<224, 128, 64, 8, cutlass::half_t>>, Is_dropout=false, Is_causal=false, Is_even_N=false, Is_even_K=true, Return_softmax=false, Params=Flash_fwd_params]" 
(588): here
            instantiation of "void flash::compute_attn<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &) [with Kernel_traits=Flash_fwd_kernel_traits<224, 128, 64, 8, false, false, cutlass::half_t, Flash_kernel_traits<224, 128, 64, 8, cutlass::half_t>>, Is_dropout=false, Is_causal=false, Is_even_N=false, Is_even_K=true, Return_softmax=false, Params=Flash_fwd_params]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(15): here
            instantiation of "void flash_fwd_kernel<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax>(Flash_fwd_params) [with Kernel_traits=Flash_fwd_kernel_traits<224, 128, 64, 8, false, false, cutlass::half_t, Flash_kernel_traits<224, 128, 64, 8, cutlass::half_t>>, Is_dropout=false, Is_causal=false, Is_even_N=false, Is_even_K=true, Return_softmax=false]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(34): here
            instantiation of "void run_flash_fwd<Kernel_traits,Is_dropout,Is_causal>(Flash_fwd_params &, cudaStream_t) [with Kernel_traits=Flash_fwd_kernel_traits<224, 128, 64, 8, false, false, cutlass::half_t, Flash_kernel_traits<224, 128, 64, 8, cutlass::half_t>>, Is_dropout=false, Is_causal=false]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(208): here
            instantiation of "void run_mha_fwd_hdim224<T>(Flash_fwd_params &, cudaStream_t) [with T=cutlass::half_t]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_hdim224_fp16_sm80.cu(8): here

/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_kernel.h(275): warning: variable "tQrQ" was declared but never referenced
          detected during:
            instantiation of "void flash::compute_attn_1rowblock<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &, int, int, int) [with Kernel_traits=Flash_fwd_kernel_traits<224, 128, 64, 8, false, false, cutlass::half_t, Flash_kernel_traits<224, 128, 64, 8, cutlass::half_t>>, Is_dropout=false, Is_causal=false, Is_even_N=false, Is_even_K=true, Return_softmax=false, Params=Flash_fwd_params]" 
(588): here
            instantiation of "void flash::compute_attn<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &) [with Kernel_traits=Flash_fwd_kernel_traits<224, 128, 64, 8, false, false, cutlass::half_t, Flash_kernel_traits<224, 128, 64, 8, cutlass::half_t>>, Is_dropout=false, Is_causal=false, Is_even_N=false, Is_even_K=true, Return_softmax=false, Params=Flash_fwd_params]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(15): here
            instantiation of "void flash_fwd_kernel<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax>(Flash_fwd_params) [with Kernel_traits=Flash_fwd_kernel_traits<224, 128, 64, 8, false, false, cutlass::half_t, Flash_kernel_traits<224, 128, 64, 8, cutlass::half_t>>, Is_dropout=false, Is_causal=false, Is_even_N=false, Is_even_K=true, Return_softmax=false]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(34): here
            instantiation of "void run_flash_fwd<Kernel_traits,Is_dropout,Is_causal>(Flash_fwd_params &, cudaStream_t) [with Kernel_traits=Flash_fwd_kernel_traits<224, 128, 64, 8, false, false, cutlass::half_t, Flash_kernel_traits<224, 128, 64, 8, cutlass::half_t>>, Is_dropout=false, Is_causal=false]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(208): here
            instantiation of "void run_mha_fwd_hdim224<T>(Flash_fwd_params &, cudaStream_t) [with T=cutlass::half_t]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_hdim224_fp16_sm80.cu(8): here

/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_kernel.h(140): warning: variable "MMA_M" was declared but never referenced
          detected during:
            instantiation of "void flash::compute_attn_1rowblock<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &, int, int, int) [with Kernel_traits=Flash_fwd_kernel_traits<224, 128, 64, 8, false, false, cutlass::half_t, Flash_kernel_traits<224, 128, 64, 8, cutlass::half_t>>, Is_dropout=false, Is_causal=false, Is_even_N=false, Is_even_K=false, Return_softmax=false, Params=Flash_fwd_params]" 
(588): here
            instantiation of "void flash::compute_attn<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &) [with Kernel_traits=Flash_fwd_kernel_traits<224, 128, 64, 8, false, false, cutlass::half_t, Flash_kernel_traits<224, 128, 64, 8, cutlass::half_t>>, Is_dropout=false, Is_causal=false, Is_even_N=false, Is_even_K=false, Return_softmax=false, Params=Flash_fwd_params]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(15): here
            instantiation of "void flash_fwd_kernel<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax>(Flash_fwd_params) [with Kernel_traits=Flash_fwd_kernel_traits<224, 128, 64, 8, false, false, cutlass::half_t, Flash_kernel_traits<224, 128, 64, 8, cutlass::half_t>>, Is_dropout=false, Is_causal=false, Is_even_N=false, Is_even_K=false, Return_softmax=false]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(34): here
            instantiation of "void run_flash_fwd<Kernel_traits,Is_dropout,Is_causal>(Flash_fwd_params &, cudaStream_t) [with Kernel_traits=Flash_fwd_kernel_traits<224, 128, 64, 8, false, false, cutlass::half_t, Flash_kernel_traits<224, 128, 64, 8, cutlass::half_t>>, Is_dropout=false, Is_causal=false]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(208): here
            instantiation of "void run_mha_fwd_hdim224<T>(Flash_fwd_params &, cudaStream_t) [with T=cutlass::half_t]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_hdim224_fp16_sm80.cu(8): here

/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_kernel.h(275): warning: variable "tQrQ" was declared but never referenced
          detected during:
            instantiation of "void flash::compute_attn_1rowblock<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &, int, int, int) [with Kernel_traits=Flash_fwd_kernel_traits<224, 128, 64, 8, false, false, cutlass::half_t, Flash_kernel_traits<224, 128, 64, 8, cutlass::half_t>>, Is_dropout=false, Is_causal=false, Is_even_N=false, Is_even_K=false, Return_softmax=false, Params=Flash_fwd_params]" 
(588): here
            instantiation of "void flash::compute_attn<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &) [with Kernel_traits=Flash_fwd_kernel_traits<224, 128, 64, 8, false, false, cutlass::half_t, Flash_kernel_traits<224, 128, 64, 8, cutlass::half_t>>, Is_dropout=false, Is_causal=false, Is_even_N=false, Is_even_K=false, Return_softmax=false, Params=Flash_fwd_params]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(15): here
            instantiation of "void flash_fwd_kernel<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax>(Flash_fwd_params) [with Kernel_traits=Flash_fwd_kernel_traits<224, 128, 64, 8, false, false, cutlass::half_t, Flash_kernel_traits<224, 128, 64, 8, cutlass::half_t>>, Is_dropout=false, Is_causal=false, Is_even_N=false, Is_even_K=false, Return_softmax=false]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(34): here
            instantiation of "void run_flash_fwd<Kernel_traits,Is_dropout,Is_causal>(Flash_fwd_params &, cudaStream_t) [with Kernel_traits=Flash_fwd_kernel_traits<224, 128, 64, 8, false, false, cutlass::half_t, Flash_kernel_traits<224, 128, 64, 8, cutlass::half_t>>, Is_dropout=false, Is_causal=false]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(208): here
            instantiation of "void run_mha_fwd_hdim224<T>(Flash_fwd_params &, cudaStream_t) [with T=cutlass::half_t]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_hdim224_fp16_sm80.cu(8): here

/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_kernel.h(140): warning: variable "MMA_M" was declared but never referenced
          detected during:
            instantiation of "void flash::compute_attn_1rowblock<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &, int, int, int) [with Kernel_traits=Flash_fwd_kernel_traits<224, 64, 64, 4, false, false, cutlass::half_t, Flash_kernel_traits<224, 64, 64, 4, cutlass::half_t>>, Is_dropout=false, Is_causal=false, Is_even_N=true, Is_even_K=true, Return_softmax=false, Params=Flash_fwd_params]" 
(588): here
            instantiation of "void flash::compute_attn<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &) [with Kernel_traits=Flash_fwd_kernel_traits<224, 64, 64, 4, false, false, cutlass::half_t, Flash_kernel_traits<224, 64, 64, 4, cutlass::half_t>>, Is_dropout=false, Is_causal=false, Is_even_N=true, Is_even_K=true, Return_softmax=false, Params=Flash_fwd_params]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(15): here
            instantiation of "void flash_fwd_kernel<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax>(Flash_fwd_params) [with Kernel_traits=Flash_fwd_kernel_traits<224, 64, 64, 4, false, false, cutlass::half_t, Flash_kernel_traits<224, 64, 64, 4, cutlass::half_t>>, Is_dropout=false, Is_causal=false, Is_even_N=true, Is_even_K=true, Return_softmax=false]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(34): here
            instantiation of "void run_flash_fwd<Kernel_traits,Is_dropout,Is_causal>(Flash_fwd_params &, cudaStream_t) [with Kernel_traits=Flash_fwd_kernel_traits<224, 64, 64, 4, false, false, cutlass::half_t, Flash_kernel_traits<224, 64, 64, 4, cutlass::half_t>>, Is_dropout=false, Is_causal=false]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(208): here
            instantiation of "void run_mha_fwd_hdim224<T>(Flash_fwd_params &, cudaStream_t) [with T=cutlass::half_t]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_hdim224_fp16_sm80.cu(8): here

/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_kernel.h(275): warning: variable "tQrQ" was declared but never referenced
          detected during:
            instantiation of "void flash::compute_attn_1rowblock<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &, int, int, int) [with Kernel_traits=Flash_fwd_kernel_traits<224, 64, 64, 4, false, false, cutlass::half_t, Flash_kernel_traits<224, 64, 64, 4, cutlass::half_t>>, Is_dropout=false, Is_causal=false, Is_even_N=true, Is_even_K=true, Return_softmax=false, Params=Flash_fwd_params]" 
(588): here
            instantiation of "void flash::compute_attn<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &) [with Kernel_traits=Flash_fwd_kernel_traits<224, 64, 64, 4, false, false, cutlass::half_t, Flash_kernel_traits<224, 64, 64, 4, cutlass::half_t>>, Is_dropout=false, Is_causal=false, Is_even_N=true, Is_even_K=true, Return_softmax=false, Params=Flash_fwd_params]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(15): here
            instantiation of "void flash_fwd_kernel<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax>(Flash_fwd_params) [with Kernel_traits=Flash_fwd_kernel_traits<224, 64, 64, 4, false, false, cutlass::half_t, Flash_kernel_traits<224, 64, 64, 4, cutlass::half_t>>, Is_dropout=false, Is_causal=false, Is_even_N=true, Is_even_K=true, Return_softmax=false]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(34): here
            instantiation of "void run_flash_fwd<Kernel_traits,Is_dropout,Is_causal>(Flash_fwd_params &, cudaStream_t) [with Kernel_traits=Flash_fwd_kernel_traits<224, 64, 64, 4, false, false, cutlass::half_t, Flash_kernel_traits<224, 64, 64, 4, cutlass::half_t>>, Is_dropout=false, Is_causal=false]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(208): here
            instantiation of "void run_mha_fwd_hdim224<T>(Flash_fwd_params &, cudaStream_t) [with T=cutlass::half_t]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_hdim224_fp16_sm80.cu(8): here

/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_kernel.h(140): warning: variable "MMA_M" was declared but never referenced
          detected during:
            instantiation of "void flash::compute_attn_1rowblock<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &, int, int, int) [with Kernel_traits=Flash_fwd_kernel_traits<224, 64, 64, 4, false, false, cutlass::half_t, Flash_kernel_traits<224, 64, 64, 4, cutlass::half_t>>, Is_dropout=false, Is_causal=false, Is_even_N=true, Is_even_K=false, Return_softmax=false, Params=Flash_fwd_params]" 
(588): here
            instantiation of "void flash::compute_attn<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &) [with Kernel_traits=Flash_fwd_kernel_traits<224, 64, 64, 4, false, false, cutlass::half_t, Flash_kernel_traits<224, 64, 64, 4, cutlass::half_t>>, Is_dropout=false, Is_causal=false, Is_even_N=true, Is_even_K=false, Return_softmax=false, Params=Flash_fwd_params]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(15): here
            instantiation of "void flash_fwd_kernel<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax>(Flash_fwd_params) [with Kernel_traits=Flash_fwd_kernel_traits<224, 64, 64, 4, false, false, cutlass::half_t, Flash_kernel_traits<224, 64, 64, 4, cutlass::half_t>>, Is_dropout=false, Is_causal=false, Is_even_N=true, Is_even_K=false, Return_softmax=false]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(34): here
            instantiation of "void run_flash_fwd<Kernel_traits,Is_dropout,Is_causal>(Flash_fwd_params &, cudaStream_t) [with Kernel_traits=Flash_fwd_kernel_traits<224, 64, 64, 4, false, false, cutlass::half_t, Flash_kernel_traits<224, 64, 64, 4, cutlass::half_t>>, Is_dropout=false, Is_causal=false]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(208): here
            instantiation of "void run_mha_fwd_hdim224<T>(Flash_fwd_params &, cudaStream_t) [with T=cutlass::half_t]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_hdim224_fp16_sm80.cu(8): here

/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_kernel.h(275): warning: variable "tQrQ" was declared but never referenced
          detected during:
            instantiation of "void flash::compute_attn_1rowblock<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &, int, int, int) [with Kernel_traits=Flash_fwd_kernel_traits<224, 64, 64, 4, false, false, cutlass::half_t, Flash_kernel_traits<224, 64, 64, 4, cutlass::half_t>>, Is_dropout=false, Is_causal=false, Is_even_N=true, Is_even_K=false, Return_softmax=false, Params=Flash_fwd_params]" 
(588): here
            instantiation of "void flash::compute_attn<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &) [with Kernel_traits=Flash_fwd_kernel_traits<224, 64, 64, 4, false, false, cutlass::half_t, Flash_kernel_traits<224, 64, 64, 4, cutlass::half_t>>, Is_dropout=false, Is_causal=false, Is_even_N=true, Is_even_K=false, Return_softmax=false, Params=Flash_fwd_params]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(15): here
            instantiation of "void flash_fwd_kernel<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax>(Flash_fwd_params) [with Kernel_traits=Flash_fwd_kernel_traits<224, 64, 64, 4, false, false, cutlass::half_t, Flash_kernel_traits<224, 64, 64, 4, cutlass::half_t>>, Is_dropout=false, Is_causal=false, Is_even_N=true, Is_even_K=false, Return_softmax=false]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(34): here
            instantiation of "void run_flash_fwd<Kernel_traits,Is_dropout,Is_causal>(Flash_fwd_params &, cudaStream_t) [with Kernel_traits=Flash_fwd_kernel_traits<224, 64, 64, 4, false, false, cutlass::half_t, Flash_kernel_traits<224, 64, 64, 4, cutlass::half_t>>, Is_dropout=false, Is_causal=false]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(208): here
            instantiation of "void run_mha_fwd_hdim224<T>(Flash_fwd_params &, cudaStream_t) [with T=cutlass::half_t]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_hdim224_fp16_sm80.cu(8): here

/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_kernel.h(140): warning: variable "MMA_M" was declared but never referenced
          detected during:
            instantiation of "void flash::compute_attn_1rowblock<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &, int, int, int) [with Kernel_traits=Flash_fwd_kernel_traits<224, 64, 64, 4, false, false, cutlass::half_t, Flash_kernel_traits<224, 64, 64, 4, cutlass::half_t>>, Is_dropout=false, Is_causal=false, Is_even_N=false, Is_even_K=true, Return_softmax=false, Params=Flash_fwd_params]" 
(588): here
            instantiation of "void flash::compute_attn<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &) [with Kernel_traits=Flash_fwd_kernel_traits<224, 64, 64, 4, false, false, cutlass::half_t, Flash_kernel_traits<224, 64, 64, 4, cutlass::half_t>>, Is_dropout=false, Is_causal=false, Is_even_N=false, Is_even_K=true, Return_softmax=false, Params=Flash_fwd_params]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(15): here
            instantiation of "void flash_fwd_kernel<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax>(Flash_fwd_params) [with Kernel_traits=Flash_fwd_kernel_traits<224, 64, 64, 4, false, false, cutlass::half_t, Flash_kernel_traits<224, 64, 64, 4, cutlass::half_t>>, Is_dropout=false, Is_causal=false, Is_even_N=false, Is_even_K=true, Return_softmax=false]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(34): here
            instantiation of "void run_flash_fwd<Kernel_traits,Is_dropout,Is_causal>(Flash_fwd_params &, cudaStream_t) [with Kernel_traits=Flash_fwd_kernel_traits<224, 64, 64, 4, false, false, cutlass::half_t, Flash_kernel_traits<224, 64, 64, 4, cutlass::half_t>>, Is_dropout=false, Is_causal=false]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(208): here
            instantiation of "void run_mha_fwd_hdim224<T>(Flash_fwd_params &, cudaStream_t) [with T=cutlass::half_t]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_hdim224_fp16_sm80.cu(8): here

/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_kernel.h(275): warning: variable "tQrQ" was declared but never referenced
          detected during:
            instantiation of "void flash::compute_attn_1rowblock<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &, int, int, int) [with Kernel_traits=Flash_fwd_kernel_traits<224, 64, 64, 4, false, false, cutlass::half_t, Flash_kernel_traits<224, 64, 64, 4, cutlass::half_t>>, Is_dropout=false, Is_causal=false, Is_even_N=false, Is_even_K=true, Return_softmax=false, Params=Flash_fwd_params]" 
(588): here
            instantiation of "void flash::compute_attn<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &) [with Kernel_traits=Flash_fwd_kernel_traits<224, 64, 64, 4, false, false, cutlass::half_t, Flash_kernel_traits<224, 64, 64, 4, cutlass::half_t>>, Is_dropout=false, Is_causal=false, Is_even_N=false, Is_even_K=true, Return_softmax=false, Params=Flash_fwd_params]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(15): here
            instantiation of "void flash_fwd_kernel<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax>(Flash_fwd_params) [with Kernel_traits=Flash_fwd_kernel_traits<224, 64, 64, 4, false, false, cutlass::half_t, Flash_kernel_traits<224, 64, 64, 4, cutlass::half_t>>, Is_dropout=false, Is_causal=false, Is_even_N=false, Is_even_K=true, Return_softmax=false]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(34): here
            instantiation of "void run_flash_fwd<Kernel_traits,Is_dropout,Is_causal>(Flash_fwd_params &, cudaStream_t) [with Kernel_traits=Flash_fwd_kernel_traits<224, 64, 64, 4, false, false, cutlass::half_t, Flash_kernel_traits<224, 64, 64, 4, cutlass::half_t>>, Is_dropout=false, Is_causal=false]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(208): here
            instantiation of "void run_mha_fwd_hdim224<T>(Flash_fwd_params &, cudaStream_t) [with T=cutlass::half_t]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_hdim224_fp16_sm80.cu(8): here

/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_kernel.h(140): warning: variable "MMA_M" was declared but never referenced
          detected during:
            instantiation of "void flash::compute_attn_1rowblock<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &, int, int, int) [with Kernel_traits=Flash_fwd_kernel_traits<224, 64, 64, 4, false, false, cutlass::half_t, Flash_kernel_traits<224, 64, 64, 4, cutlass::half_t>>, Is_dropout=false, Is_causal=false, Is_even_N=false, Is_even_K=false, Return_softmax=false, Params=Flash_fwd_params]" 
(588): here
            instantiation of "void flash::compute_attn<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &) [with Kernel_traits=Flash_fwd_kernel_traits<224, 64, 64, 4, false, false, cutlass::half_t, Flash_kernel_traits<224, 64, 64, 4, cutlass::half_t>>, Is_dropout=false, Is_causal=false, Is_even_N=false, Is_even_K=false, Return_softmax=false, Params=Flash_fwd_params]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(15): here
            instantiation of "void flash_fwd_kernel<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax>(Flash_fwd_params) [with Kernel_traits=Flash_fwd_kernel_traits<224, 64, 64, 4, false, false, cutlass::half_t, Flash_kernel_traits<224, 64, 64, 4, cutlass::half_t>>, Is_dropout=false, Is_causal=false, Is_even_N=false, Is_even_K=false, Return_softmax=false]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(34): here
            instantiation of "void run_flash_fwd<Kernel_traits,Is_dropout,Is_causal>(Flash_fwd_params &, cudaStream_t) [with Kernel_traits=Flash_fwd_kernel_traits<224, 64, 64, 4, false, false, cutlass::half_t, Flash_kernel_traits<224, 64, 64, 4, cutlass::half_t>>, Is_dropout=false, Is_causal=false]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(208): here
            instantiation of "void run_mha_fwd_hdim224<T>(Flash_fwd_params &, cudaStream_t) [with T=cutlass::half_t]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_hdim224_fp16_sm80.cu(8): here

/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_kernel.h(275): warning: variable "tQrQ" was declared but never referenced
          detected during:
            instantiation of "void flash::compute_attn_1rowblock<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &, int, int, int) [with Kernel_traits=Flash_fwd_kernel_traits<224, 64, 64, 4, false, false, cutlass::half_t, Flash_kernel_traits<224, 64, 64, 4, cutlass::half_t>>, Is_dropout=false, Is_causal=false, Is_even_N=false, Is_even_K=false, Return_softmax=false, Params=Flash_fwd_params]" 
(588): here
            instantiation of "void flash::compute_attn<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &) [with Kernel_traits=Flash_fwd_kernel_traits<224, 64, 64, 4, false, false, cutlass::half_t, Flash_kernel_traits<224, 64, 64, 4, cutlass::half_t>>, Is_dropout=false, Is_causal=false, Is_even_N=false, Is_even_K=false, Return_softmax=false, Params=Flash_fwd_params]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(15): here
            instantiation of "void flash_fwd_kernel<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax>(Flash_fwd_params) [with Kernel_traits=Flash_fwd_kernel_traits<224, 64, 64, 4, false, false, cutlass::half_t, Flash_kernel_traits<224, 64, 64, 4, cutlass::half_t>>, Is_dropout=false, Is_causal=false, Is_even_N=false, Is_even_K=false, Return_softmax=false]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(34): here
            instantiation of "void run_flash_fwd<Kernel_traits,Is_dropout,Is_causal>(Flash_fwd_params &, cudaStream_t) [with Kernel_traits=Flash_fwd_kernel_traits<224, 64, 64, 4, false, false, cutlass::half_t, Flash_kernel_traits<224, 64, 64, 4, cutlass::half_t>>, Is_dropout=false, Is_causal=false]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(208): here
            instantiation of "void run_mha_fwd_hdim224<T>(Flash_fwd_params &, cudaStream_t) [with T=cutlass::half_t]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_hdim224_fp16_sm80.cu(8): here

ptxas info    : 2 bytes gmem
ptxas info    : Compiling entry function '_Z16flash_fwd_kernelI23Flash_fwd_kernel_traitsILi224ELi64ELi64ELi4ELb0ELb0EN7cutlass6half_tE19Flash_kernel_traitsILi224ELi64ELi64ELi4ES2_EELb0ELb0ELb0ELb0ELb0EEv16Flash_fwd_params' for 'sm_80'
ptxas info    : Function properties for _Z16flash_fwd_kernelI23Flash_fwd_kernel_traitsILi224ELi64ELi64ELi4ELb0ELb0EN7cutlass6half_tE19Flash_kernel_traitsILi224ELi64ELi64ELi4ES2_EELb0ELb0ELb0ELb0ELb0EEv16Flash_fwd_params
    0 bytes stack frame, 0 bytes spill stores, 0 bytes spill loads
ptxas info    : Used 218 registers, 584 bytes cmem[0]
ptxas info    : Compiling entry function '_Z16flash_fwd_kernelI23Flash_fwd_kernel_traitsILi224ELi64ELi64ELi4ELb0ELb0EN7cutlass6half_tE19Flash_kernel_traitsILi224ELi64ELi64ELi4ES2_EELb0ELb0ELb0ELb1ELb0EEv16Flash_fwd_params' for 'sm_80'
ptxas info    : Function properties for _Z16flash_fwd_kernelI23Flash_fwd_kernel_traitsILi224ELi64ELi64ELi4ELb0ELb0EN7cutlass6half_tE19Flash_kernel_traitsILi224ELi64ELi64ELi4ES2_EELb0ELb0ELb0ELb1ELb0EEv16Flash_fwd_params
    0 bytes stack frame, 0 bytes spill stores, 0 bytes spill loads
ptxas info    : Used 211 registers, 584 bytes cmem[0]
ptxas info    : Compiling entry function '_Z16flash_fwd_kernelI23Flash_fwd_kernel_traitsILi224ELi64ELi64ELi4ELb0ELb0EN7cutlass6half_tE19Flash_kernel_traitsILi224ELi64ELi64ELi4ES2_EELb0ELb0ELb1ELb0ELb0EEv16Flash_fwd_params' for 'sm_80'
ptxas info    : Function properties for _Z16flash_fwd_kernelI23Flash_fwd_kernel_traitsILi224ELi64ELi64ELi4ELb0ELb0EN7cutlass6half_tE19Flash_kernel_traitsILi224ELi64ELi64ELi4ES2_EELb0ELb0ELb1ELb0ELb0EEv16Flash_fwd_params
    0 bytes stack frame, 0 bytes spill stores, 0 bytes spill loads
ptxas info    : Used 215 registers, 584 bytes cmem[0]
ptxas info    : Compiling entry function '_Z16flash_fwd_kernelI23Flash_fwd_kernel_traitsILi224ELi64ELi64ELi4ELb0ELb0EN7cutlass6half_tE19Flash_kernel_traitsILi224ELi64ELi64ELi4ES2_EELb0ELb0ELb1ELb1ELb0EEv16Flash_fwd_params' for 'sm_80'
ptxas info    : Function properties for _Z16flash_fwd_kernelI23Flash_fwd_kernel_traitsILi224ELi64ELi64ELi4ELb0ELb0EN7cutlass6half_tE19Flash_kernel_traitsILi224ELi64ELi64ELi4ES2_EELb0ELb0ELb1ELb1ELb0EEv16Flash_fwd_params
    0 bytes stack frame, 0 bytes spill stores, 0 bytes spill loads
ptxas info    : Used 211 registers, 584 bytes cmem[0]
ptxas info    : Compiling entry function '_Z16flash_fwd_kernelI23Flash_fwd_kernel_traitsILi224ELi128ELi64ELi8ELb0ELb0EN7cutlass6half_tE19Flash_kernel_traitsILi224ELi128ELi64ELi8ES2_EELb0ELb0ELb0ELb0ELb0EEv16Flash_fwd_params' for 'sm_80'
ptxas info    : Function properties for _Z16flash_fwd_kernelI23Flash_fwd_kernel_traitsILi224ELi128ELi64ELi8ELb0ELb0EN7cutlass6half_tE19Flash_kernel_traitsILi224ELi128ELi64ELi8ES2_EELb0ELb0ELb0ELb0ELb0EEv16Flash_fwd_params
    0 bytes stack frame, 0 bytes spill stores, 0 bytes spill loads
ptxas info    : Used 221 registers, 584 bytes cmem[0]
ptxas info    : Compiling entry function '_Z16flash_fwd_kernelI23Flash_fwd_kernel_traitsILi224ELi128ELi64ELi8ELb0ELb0EN7cutlass6half_tE19Flash_kernel_traitsILi224ELi128ELi64ELi8ES2_EELb0ELb0ELb0ELb1ELb0EEv16Flash_fwd_params' for 'sm_80'
ptxas info    : Function properties for _Z16flash_fwd_kernelI23Flash_fwd_kernel_traitsILi224ELi128ELi64ELi8ELb0ELb0EN7cutlass6half_tE19Flash_kernel_traitsILi224ELi128ELi64ELi8ES2_EELb0ELb0ELb0ELb1ELb0EEv16Flash_fwd_params
    0 bytes stack frame, 0 bytes spill stores, 0 bytes spill loads
ptxas info    : Used 214 registers, 584 bytes cmem[0]
ptxas info    : Compiling entry function '_Z16flash_fwd_kernelI23Flash_fwd_kernel_traitsILi224ELi128ELi64ELi8ELb0ELb0EN7cutlass6half_tE19Flash_kernel_traitsILi224ELi128ELi64ELi8ES2_EELb0ELb0ELb1ELb0ELb0EEv16Flash_fwd_params' for 'sm_80'
ptxas info    : Function properties for _Z16flash_fwd_kernelI23Flash_fwd_kernel_traitsILi224ELi128ELi64ELi8ELb0ELb0EN7cutlass6half_tE19Flash_kernel_traitsILi224ELi128ELi64ELi8ES2_EELb0ELb0ELb1ELb0ELb0EEv16Flash_fwd_params
    0 bytes stack frame, 0 bytes spill stores, 0 bytes spill loads
ptxas info    : Used 218 registers, 584 bytes cmem[0]
ptxas info    : Compiling entry function '_Z16flash_fwd_kernelI23Flash_fwd_kernel_traitsILi224ELi128ELi64ELi8ELb0ELb0EN7cutlass6half_tE19Flash_kernel_traitsILi224ELi128ELi64ELi8ES2_EELb0ELb0ELb1ELb1ELb0EEv16Flash_fwd_params' for 'sm_80'
ptxas info    : Function properties for _Z16flash_fwd_kernelI23Flash_fwd_kernel_traitsILi224ELi128ELi64ELi8ELb0ELb0EN7cutlass6half_tE19Flash_kernel_traitsILi224ELi128ELi64ELi8ES2_EELb0ELb0ELb1ELb1ELb0EEv16Flash_fwd_params
    0 bytes stack frame, 0 bytes spill stores, 0 bytes spill loads
ptxas info    : Used 212 registers, 584 bytes cmem[0]
ptxas info    : Compiling entry function '_Z16flash_fwd_kernelI23Flash_fwd_kernel_traitsILi224ELi64ELi64ELi4ELb0ELb0EN7cutlass6half_tE19Flash_kernel_traitsILi224ELi64ELi64ELi4ES2_EELb0ELb1ELb0ELb0ELb0EEv16Flash_fwd_params' for 'sm_80'
ptxas info    : Function properties for _Z16flash_fwd_kernelI23Flash_fwd_kernel_traitsILi224ELi64ELi64ELi4ELb0ELb0EN7cutlass6half_tE19Flash_kernel_traitsILi224ELi64ELi64ELi4ES2_EELb0ELb1ELb0ELb0ELb0EEv16Flash_fwd_params
    0 bytes stack frame, 0 bytes spill stores, 0 bytes spill loads
ptxas info    : Used 219 registers, 584 bytes cmem[0]
ptxas info    : Compiling entry function '_Z16flash_fwd_kernelI23Flash_fwd_kernel_traitsILi224ELi64ELi64ELi4ELb0ELb0EN7cutlass6half_tE19Flash_kernel_traitsILi224ELi64ELi64ELi4ES2_EELb0ELb1ELb0ELb1ELb0EEv16Flash_fwd_params' for 'sm_80'
ptxas info    : Function properties for _Z16flash_fwd_kernelI23Flash_fwd_kernel_traitsILi224ELi64ELi64ELi4ELb0ELb0EN7cutlass6half_tE19Flash_kernel_traitsILi224ELi64ELi64ELi4ES2_EELb0ELb1ELb0ELb1ELb0EEv16Flash_fwd_params
    0 bytes stack frame, 0 bytes spill stores, 0 bytes spill loads
ptxas info    : Used 212 registers, 584 bytes cmem[0]
ptxas info    : Compiling entry function '_Z16flash_fwd_kernelI23Flash_fwd_kernel_traitsILi224ELi64ELi64ELi4ELb0ELb0EN7cutlass6half_tE19Flash_kernel_traitsILi224ELi64ELi64ELi4ES2_EELb0ELb1ELb1ELb0ELb0EEv16Flash_fwd_params' for 'sm_80'
ptxas info    : Function properties for _Z16flash_fwd_kernelI23Flash_fwd_kernel_traitsILi224ELi64ELi64ELi4ELb0ELb0EN7cutlass6half_tE19Flash_kernel_traitsILi224ELi64ELi64ELi4ES2_EELb0ELb1ELb1ELb0ELb0EEv16Flash_fwd_params
    0 bytes stack frame, 0 bytes spill stores, 0 bytes spill loads
ptxas info    : Used 230 registers, 584 bytes cmem[0]
ptxas info    : Compiling entry function '_Z16flash_fwd_kernelI23Flash_fwd_kernel_traitsILi224ELi64ELi64ELi4ELb0ELb0EN7cutlass6half_tE19Flash_kernel_traitsILi224ELi64ELi64ELi4ES2_EELb0ELb1ELb1ELb1ELb0EEv16Flash_fwd_params' for 'sm_80'
ptxas info    : Function properties for _Z16flash_fwd_kernelI23Flash_fwd_kernel_traitsILi224ELi64ELi64ELi4ELb0ELb0EN7cutlass6half_tE19Flash_kernel_traitsILi224ELi64ELi64ELi4ES2_EELb0ELb1ELb1ELb1ELb0EEv16Flash_fwd_params
    0 bytes stack frame, 0 bytes spill stores, 0 bytes spill loads
ptxas info    : Used 222 registers, 584 bytes cmem[0]
ptxas info    : Compiling entry function '_Z16flash_fwd_kernelI23Flash_fwd_kernel_traitsILi224ELi128ELi64ELi8ELb0ELb0EN7cutlass6half_tE19Flash_kernel_traitsILi224ELi128ELi64ELi8ES2_EELb0ELb1ELb0ELb0ELb0EEv16Flash_fwd_params' for 'sm_80'
ptxas info    : Function properties for _Z16flash_fwd_kernelI23Flash_fwd_kernel_traitsILi224ELi128ELi64ELi8ELb0ELb0EN7cutlass6half_tE19Flash_kernel_traitsILi224ELi128ELi64ELi8ES2_EELb0ELb1ELb0ELb0ELb0EEv16Flash_fwd_params
    0 bytes stack frame, 0 bytes spill stores, 0 bytes spill loads
ptxas info    : Used 234 registers, 584 bytes cmem[0]
ptxas info    : Compiling entry function '_Z16flash_fwd_kernelI23Flash_fwd_kernel_traitsILi224ELi128ELi64ELi8ELb0ELb0EN7cutlass6half_tE19Flash_kernel_traitsILi224ELi128ELi64ELi8ES2_EELb0ELb1ELb0ELb1ELb0EEv16Flash_fwd_params' for 'sm_80'
ptxas info    : Function properties for _Z16flash_fwd_kernelI23Flash_fwd_kernel_traitsILi224ELi128ELi64ELi8ELb0ELb0EN7cutlass6half_tE19Flash_kernel_traitsILi224ELi128ELi64ELi8ES2_EELb0ELb1ELb0ELb1ELb0EEv16Flash_fwd_params
    0 bytes stack frame, 0 bytes spill stores, 0 bytes spill loads
ptxas info    : Used 226 registers, 584 bytes cmem[0]
ptxas info    : Compiling entry function '_Z16flash_fwd_kernelI23Flash_fwd_kernel_traitsILi224ELi128ELi64ELi8ELb0ELb0EN7cutlass6half_tE19Flash_kernel_traitsILi224ELi128ELi64ELi8ES2_EELb0ELb1ELb1ELb0ELb0EEv16Flash_fwd_params' for 'sm_80'
ptxas info    : Function properties for _Z16flash_fwd_kernelI23Flash_fwd_kernel_traitsILi224ELi128ELi64ELi8ELb0ELb0EN7cutlass6half_tE19Flash_kernel_traitsILi224ELi128ELi64ELi8ES2_EELb0ELb1ELb1ELb0ELb0EEv16Flash_fwd_params
    48 bytes stack frame, 80 bytes spill stores, 72 bytes spill loads
ptxas info    : Used 255 registers, 584 bytes cmem[0]
ptxas info    : Compiling entry function '_Z16flash_fwd_kernelI23Flash_fwd_kernel_traitsILi224ELi128ELi64ELi8ELb0ELb0EN7cutlass6half_tE19Flash_kernel_traitsILi224ELi128ELi64ELi8ES2_EELb0ELb1ELb1ELb1ELb0EEv16Flash_fwd_params' for 'sm_80'
ptxas info    : Function properties for _Z16flash_fwd_kernelI23Flash_fwd_kernel_traitsILi224ELi128ELi64ELi8ELb0ELb0EN7cutlass6half_tE19Flash_kernel_traitsILi224ELi128ELi64ELi8ES2_EELb0ELb1ELb1ELb1ELb0EEv16Flash_fwd_params
    0 bytes stack frame, 0 bytes spill stores, 0 bytes spill loads
ptxas info    : Used 228 registers, 584 bytes cmem[0]
ptxas info    : Compiling entry function '_Z16flash_fwd_kernelI23Flash_fwd_kernel_traitsILi224ELi64ELi64ELi4ELb0ELb0EN7cutlass6half_tE19Flash_kernel_traitsILi224ELi64ELi64ELi4ES2_EELb1ELb0ELb0ELb0ELb0EEv16Flash_fwd_params' for 'sm_80'
ptxas info    : Function properties for _Z16flash_fwd_kernelI23Flash_fwd_kernel_traitsILi224ELi64ELi64ELi4ELb0ELb0EN7cutlass6half_tE19Flash_kernel_traitsILi224ELi64ELi64ELi4ES2_EELb1ELb0ELb0ELb0ELb0EEv16Flash_fwd_params
    0 bytes stack frame, 0 bytes spill stores, 0 bytes spill loads
ptxas info    : Used 230 registers, 584 bytes cmem[0]
ptxas info    : Compiling entry function '_Z16flash_fwd_kernelI23Flash_fwd_kernel_traitsILi224ELi64ELi64ELi4ELb0ELb0EN7cutlass6half_tE19Flash_kernel_traitsILi224ELi64ELi64ELi4ES2_EELb1ELb0ELb0ELb0ELb1EEv16Flash_fwd_params' for 'sm_80'
ptxas info    : Function properties for _Z16flash_fwd_kernelI23Flash_fwd_kernel_traitsILi224ELi64ELi64ELi4ELb0ELb0EN7cutlass6half_tE19Flash_kernel_traitsILi224ELi64ELi64ELi4ES2_EELb1ELb0ELb0ELb0ELb1EEv16Flash_fwd_params
    48 bytes stack frame, 60 bytes spill stores, 52 bytes spill loads
ptxas info    : Used 255 registers, 584 bytes cmem[0]
ptxas info    : Compiling entry function '_Z16flash_fwd_kernelI23Flash_fwd_kernel_traitsILi224ELi64ELi64ELi4ELb0ELb0EN7cutlass6half_tE19Flash_kernel_traitsILi224ELi64ELi64ELi4ES2_EELb1ELb0ELb0ELb1ELb0EEv16Flash_fwd_params' for 'sm_80'
ptxas info    : Function properties for _Z16flash_fwd_kernelI23Flash_fwd_kernel_traitsILi224ELi64ELi64ELi4ELb0ELb0EN7cutlass6half_tE19Flash_kernel_traitsILi224ELi64ELi64ELi4ES2_EELb1ELb0ELb0ELb1ELb0EEv16Flash_fwd_params
    0 bytes stack frame, 0 bytes spill stores, 0 bytes spill loads
ptxas info    : Used 222 registers, 584 bytes cmem[0]
ptxas info    : Compiling entry function '_Z16flash_fwd_kernelI23Flash_fwd_kernel_traitsILi224ELi64ELi64ELi4ELb0ELb0EN7cutlass6half_tE19Flash_kernel_traitsILi224ELi64ELi64ELi4ES2_EELb1ELb0ELb0ELb1ELb1EEv16Flash_fwd_params' for 'sm_80'
ptxas info    : Function properties for _Z16flash_fwd_kernelI23Flash_fwd_kernel_traitsILi224ELi64ELi64ELi4ELb0ELb0EN7cutlass6half_tE19Flash_kernel_traitsILi224ELi64ELi64ELi4ES2_EELb1ELb0ELb0ELb1ELb1EEv16Flash_fwd_params
    40 bytes stack frame, 68 bytes spill stores, 44 bytes spill loads
ptxas info    : Used 255 registers, 584 bytes cmem[0]
ptxas info    : Compiling entry function '_Z16flash_fwd_kernelI23Flash_fwd_kernel_traitsILi224ELi64ELi64ELi4ELb0ELb0EN7cutlass6half_tE19Flash_kernel_traitsILi224ELi64ELi64ELi4ES2_EELb1ELb0ELb1ELb0ELb0EEv16Flash_fwd_params' for 'sm_80'
ptxas info    : Function properties for _Z16flash_fwd_kernelI23Flash_fwd_kernel_traitsILi224ELi64ELi64ELi4ELb0ELb0EN7cutlass6half_tE19Flash_kernel_traitsILi224ELi64ELi64ELi4ES2_EELb1ELb0ELb1ELb0ELb0EEv16Flash_fwd_params
    0 bytes stack frame, 0 bytes spill stores, 0 bytes spill loads
ptxas info    : Used 228 registers, 584 bytes cmem[0]
ptxas info    : Compiling entry function '_Z16flash_fwd_kernelI23Flash_fwd_kernel_traitsILi224ELi64ELi64ELi4ELb0ELb0EN7cutlass6half_tE19Flash_kernel_traitsILi224ELi64ELi64ELi4ES2_EELb1ELb0ELb1ELb0ELb1EEv16Flash_fwd_params' for 'sm_80'
ptxas info    : Function properties for _Z16flash_fwd_kernelI23Flash_fwd_kernel_traitsILi224ELi64ELi64ELi4ELb0ELb0EN7cutlass6half_tE19Flash_kernel_traitsILi224ELi64ELi64ELi4ES2_EELb1ELb0ELb1ELb0ELb1EEv16Flash_fwd_params
    64 bytes stack frame, 68 bytes spill stores, 60 bytes spill loads
ptxas info    : Used 255 registers, 584 bytes cmem[0]
ptxas info    : Compiling entry function '_Z16flash_fwd_kernelI23Flash_fwd_kernel_traitsILi224ELi64ELi64ELi4ELb0ELb0EN7cutlass6half_tE19Flash_kernel_traitsILi224ELi64ELi64ELi4ES2_EELb1ELb0ELb1ELb1ELb0EEv16Flash_fwd_params' for 'sm_80'
ptxas info    : Function properties for _Z16flash_fwd_kernelI23Flash_fwd_kernel_traitsILi224ELi64ELi64ELi4ELb0ELb0EN7cutlass6half_tE19Flash_kernel_traitsILi224ELi64ELi64ELi4ES2_EELb1ELb0ELb1ELb1ELb0EEv16Flash_fwd_params
    0 bytes stack frame, 0 bytes spill stores, 0 bytes spill loads
ptxas info    : Used 222 registers, 584 bytes cmem[0]
ptxas info    : Compiling entry function '_Z16flash_fwd_kernelI23Flash_fwd_kernel_traitsILi224ELi64ELi64ELi4ELb0ELb0EN7cutlass6half_tE19Flash_kernel_traitsILi224ELi64ELi64ELi4ES2_EELb1ELb0ELb1ELb1ELb1EEv16Flash_fwd_params' for 'sm_80'
ptxas info    : Function properties for _Z16flash_fwd_kernelI23Flash_fwd_kernel_traitsILi224ELi64ELi64ELi4ELb0ELb0EN7cutlass6half_tE19Flash_kernel_traitsILi224ELi64ELi64ELi4ES2_EELb1ELb0ELb1ELb1ELb1EEv16Flash_fwd_params
    48 bytes stack frame, 62 bytes spill stores, 54 bytes spill loads
ptxas info    : Used 255 registers, 584 bytes cmem[0]
ptxas info    : Compiling entry function '_Z16flash_fwd_kernelI23Flash_fwd_kernel_traitsILi224ELi128ELi64ELi8ELb0ELb0EN7cutlass6half_tE19Flash_kernel_traitsILi224ELi128ELi64ELi8ES2_EELb1ELb0ELb0ELb0ELb0EEv16Flash_fwd_params' for 'sm_80'
ptxas info    : Function properties for _Z16flash_fwd_kernelI23Flash_fwd_kernel_traitsILi224ELi128ELi64ELi8ELb0ELb0EN7cutlass6half_tE19Flash_kernel_traitsILi224ELi128ELi64ELi8ES2_EELb1ELb0ELb0ELb0ELb0EEv16Flash_fwd_params
    0 bytes stack frame, 0 bytes spill stores, 0 bytes spill loads
ptxas info    : Used 234 registers, 584 bytes cmem[0]
ptxas info    : Compiling entry function '_Z16flash_fwd_kernelI23Flash_fwd_kernel_traitsILi224ELi128ELi64ELi8ELb0ELb0EN7cutlass6half_tE19Flash_kernel_traitsILi224ELi128ELi64ELi8ES2_EELb1ELb0ELb0ELb0ELb1EEv16Flash_fwd_params' for 'sm_80'
ptxas info    : Function properties for _Z16flash_fwd_kernelI23Flash_fwd_kernel_traitsILi224ELi128ELi64ELi8ELb0ELb0EN7cutlass6half_tE19Flash_kernel_traitsILi224ELi128ELi64ELi8ES2_EELb1ELb0ELb0ELb0ELb1EEv16Flash_fwd_params
    48 bytes stack frame, 44 bytes spill stores, 44 bytes spill loads
ptxas info    : Used 255 registers, 584 bytes cmem[0]
ptxas info    : Compiling entry function '_Z16flash_fwd_kernelI23Flash_fwd_kernel_traitsILi224ELi128ELi64ELi8ELb0ELb0EN7cutlass6half_tE19Flash_kernel_traitsILi224ELi128ELi64ELi8ES2_EELb1ELb0ELb0ELb1ELb0EEv16Flash_fwd_params' for 'sm_80'
ptxas info    : Function properties for _Z16flash_fwd_kernelI23Flash_fwd_kernel_traitsILi224ELi128ELi64ELi8ELb0ELb0EN7cutlass6half_tE19Flash_kernel_traitsILi224ELi128ELi64ELi8ES2_EELb1ELb0ELb0ELb1ELb0EEv16Flash_fwd_params
    0 bytes stack frame, 0 bytes spill stores, 0 bytes spill loads
ptxas info    : Used 226 registers, 584 bytes cmem[0]
ptxas info    : Compiling entry function '_Z16flash_fwd_kernelI23Flash_fwd_kernel_traitsILi224ELi128ELi64ELi8ELb0ELb0EN7cutlass6half_tE19Flash_kernel_traitsILi224ELi128ELi64ELi8ES2_EELb1ELb0ELb0ELb1ELb1EEv16Flash_fwd_params' for 'sm_80'
ptxas info    : Function properties for _Z16flash_fwd_kernelI23Flash_fwd_kernel_traitsILi224ELi128ELi64ELi8ELb0ELb0EN7cutlass6half_tE19Flash_kernel_traitsILi224ELi128ELi64ELi8ES2_EELb1ELb0ELb0ELb1ELb1EEv16Flash_fwd_params
    40 bytes stack frame, 72 bytes spill stores, 48 bytes spill loads
ptxas info    : Used 255 registers, 584 bytes cmem[0]
ptxas info    : Compiling entry function '_Z16flash_fwd_kernelI23Flash_fwd_kernel_traitsILi224ELi128ELi64ELi8ELb0ELb0EN7cutlass6half_tE19Flash_kernel_traitsILi224ELi128ELi64ELi8ES2_EELb1ELb0ELb1ELb0ELb0EEv16Flash_fwd_params' for 'sm_80'
ptxas info    : Function properties for _Z16flash_fwd_kernelI23Flash_fwd_kernel_traitsILi224ELi128ELi64ELi8ELb0ELb0EN7cutlass6half_tE19Flash_kernel_traitsILi224ELi128ELi64ELi8ES2_EELb1ELb0ELb1ELb0ELb0EEv16Flash_fwd_params
    0 bytes stack frame, 0 bytes spill stores, 0 bytes spill loads
ptxas info    : Used 234 registers, 584 bytes cmem[0]
ptxas info    : Compiling entry function '_Z16flash_fwd_kernelI23Flash_fwd_kernel_traitsILi224ELi128ELi64ELi8ELb0ELb0EN7cutlass6half_tE19Flash_kernel_traitsILi224ELi128ELi64ELi8ES2_EELb1ELb0ELb1ELb0ELb1EEv16Flash_fwd_params' for 'sm_80'
ptxas info    : Function properties for _Z16flash_fwd_kernelI23Flash_fwd_kernel_traitsILi224ELi128ELi64ELi8ELb0ELb0EN7cutlass6half_tE19Flash_kernel_traitsILi224ELi128ELi64ELi8ES2_EELb1ELb0ELb1ELb0ELb1EEv16Flash_fwd_params
    64 bytes stack frame, 56 bytes spill stores, 56 bytes spill loads
ptxas info    : Used 255 registers, 584 bytes cmem[0]
ptxas info    : Compiling entry function '_Z16flash_fwd_kernelI23Flash_fwd_kernel_traitsILi224ELi128ELi64ELi8ELb0ELb0EN7cutlass6half_tE19Flash_kernel_traitsILi224ELi128ELi64ELi8ES2_EELb1ELb0ELb1ELb1ELb0EEv16Flash_fwd_params' for 'sm_80'
ptxas info    : Function properties for _Z16flash_fwd_kernelI23Flash_fwd_kernel_traitsILi224ELi128ELi64ELi8ELb0ELb0EN7cutlass6half_tE19Flash_kernel_traitsILi224ELi128ELi64ELi8ES2_EELb1ELb0ELb1ELb1ELb0EEv16Flash_fwd_params
    0 bytes stack frame, 0 bytes spill stores, 0 bytes spill loads
ptxas info    : Used 222 registers, 584 bytes cmem[0]
ptxas info    : Compiling entry function '_Z16flash_fwd_kernelI23Flash_fwd_kernel_traitsILi224ELi128ELi64ELi8ELb0ELb0EN7cutlass6half_tE19Flash_kernel_traitsILi224ELi128ELi64ELi8ES2_EELb1ELb0ELb1ELb1ELb1EEv16Flash_fwd_params' for 'sm_80'
ptxas info    : Function properties for _Z16flash_fwd_kernelI23Flash_fwd_kernel_traitsILi224ELi128ELi64ELi8ELb0ELb0EN7cutlass6half_tE19Flash_kernel_traitsILi224ELi128ELi64ELi8ES2_EELb1ELb0ELb1ELb1ELb1EEv16Flash_fwd_params
    104 bytes stack frame, 100 bytes spill stores, 92 bytes spill loads
ptxas info    : Used 255 registers, 584 bytes cmem[0]
ptxas info    : Compiling entry function '_Z16flash_fwd_kernelI23Flash_fwd_kernel_traitsILi224ELi64ELi64ELi4ELb0ELb0EN7cutlass6half_tE19Flash_kernel_traitsILi224ELi64ELi64ELi4ES2_EELb1ELb1ELb0ELb0ELb0EEv16Flash_fwd_params' for 'sm_80'
ptxas info    : Function properties for _Z16flash_fwd_kernelI23Flash_fwd_kernel_traitsILi224ELi64ELi64ELi4ELb0ELb0EN7cutlass6half_tE19Flash_kernel_traitsILi224ELi64ELi64ELi4ES2_EELb1ELb1ELb0ELb0ELb0EEv16Flash_fwd_params
    0 bytes stack frame, 0 bytes spill stores, 0 bytes spill loads
ptxas info    : Used 232 registers, 584 bytes cmem[0]
ptxas info    : Compiling entry function '_Z16flash_fwd_kernelI23Flash_fwd_kernel_traitsILi224ELi64ELi64ELi4ELb0ELb0EN7cutlass6half_tE19Flash_kernel_traitsILi224ELi64ELi64ELi4ES2_EELb1ELb1ELb0ELb0ELb1EEv16Flash_fwd_params' for 'sm_80'
ptxas info    : Function properties for _Z16flash_fwd_kernelI23Flash_fwd_kernel_traitsILi224ELi64ELi64ELi4ELb0ELb0EN7cutlass6half_tE19Flash_kernel_traitsILi224ELi64ELi64ELi4ES2_EELb1ELb1ELb0ELb0ELb1EEv16Flash_fwd_params
    48 bytes stack frame, 62 bytes spill stores, 54 bytes spill loads
ptxas info    : Used 255 registers, 584 bytes cmem[0]
ptxas info    : Compiling entry function '_Z16flash_fwd_kernelI23Flash_fwd_kernel_traitsILi224ELi64ELi64ELi4ELb0ELb0EN7cutlass6half_tE19Flash_kernel_traitsILi224ELi64ELi64ELi4ES2_EELb1ELb1ELb0ELb1ELb0EEv16Flash_fwd_params' for 'sm_80'
ptxas info    : Function properties for _Z16flash_fwd_kernelI23Flash_fwd_kernel_traitsILi224ELi64ELi64ELi4ELb0ELb0EN7cutlass6half_tE19Flash_kernel_traitsILi224ELi64ELi64ELi4ES2_EELb1ELb1ELb0ELb1ELb0EEv16Flash_fwd_params
    0 bytes stack frame, 0 bytes spill stores, 0 bytes spill loads
ptxas info    : Used 222 registers, 584 bytes cmem[0]
ptxas info    : Compiling entry function '_Z16flash_fwd_kernelI23Flash_fwd_kernel_traitsILi224ELi64ELi64ELi4ELb0ELb0EN7cutlass6half_tE19Flash_kernel_traitsILi224ELi64ELi64ELi4ES2_EELb1ELb1ELb0ELb1ELb1EEv16Flash_fwd_params' for 'sm_80'
ptxas info    : Function properties for _Z16flash_fwd_kernelI23Flash_fwd_kernel_traitsILi224ELi64ELi64ELi4ELb0ELb0EN7cutlass6half_tE19Flash_kernel_traitsILi224ELi64ELi64ELi4ES2_EELb1ELb1ELb0ELb1ELb1EEv16Flash_fwd_params
    32 bytes stack frame, 56 bytes spill stores, 32 bytes spill loads
ptxas info    : Used 255 registers, 584 bytes cmem[0]
ptxas info    : Compiling entry function '_Z16flash_fwd_kernelI23Flash_fwd_kernel_traitsILi224ELi64ELi64ELi4ELb0ELb0EN7cutlass6half_tE19Flash_kernel_traitsILi224ELi64ELi64ELi4ES2_EELb1ELb1ELb1ELb0ELb0EEv16Flash_fwd_params' for 'sm_80'
ptxas info    : Function properties for _Z16flash_fwd_kernelI23Flash_fwd_kernel_traitsILi224ELi64ELi64ELi4ELb0ELb0EN7cutlass6half_tE19Flash_kernel_traitsILi224ELi64ELi64ELi4ES2_EELb1ELb1ELb1ELb0ELb0EEv16Flash_fwd_params
    0 bytes stack frame, 0 bytes spill stores, 0 bytes spill loads
ptxas info    : Used 238 registers, 584 bytes cmem[0]
ptxas info    : Compiling entry function '_Z16flash_fwd_kernelI23Flash_fwd_kernel_traitsILi224ELi64ELi64ELi4ELb0ELb0EN7cutlass6half_tE19Flash_kernel_traitsILi224ELi64ELi64ELi4ES2_EELb1ELb1ELb1ELb0ELb1EEv16Flash_fwd_params' for 'sm_80'
ptxas info    : Function properties for _Z16flash_fwd_kernelI23Flash_fwd_kernel_traitsILi224ELi64ELi64ELi4ELb0ELb0EN7cutlass6half_tE19Flash_kernel_traitsILi224ELi64ELi64ELi4ES2_EELb1ELb1ELb1ELb0ELb1EEv16Flash_fwd_params
    64 bytes stack frame, 70 bytes spill stores, 62 bytes spill loads
ptxas info    : Used 255 registers, 584 bytes cmem[0]
ptxas info    : Compiling entry function '_Z16flash_fwd_kernelI23Flash_fwd_kernel_traitsILi224ELi64ELi64ELi4ELb0ELb0EN7cutlass6half_tE19Flash_kernel_traitsILi224ELi64ELi64ELi4ES2_EELb1ELb1ELb1ELb1ELb0EEv16Flash_fwd_params' for 'sm_80'
ptxas info    : Function properties for _Z16flash_fwd_kernelI23Flash_fwd_kernel_traitsILi224ELi64ELi64ELi4ELb0ELb0EN7cutlass6half_tE19Flash_kernel_traitsILi224ELi64ELi64ELi4ES2_EELb1ELb1ELb1ELb1ELb0EEv16Flash_fwd_params
    0 bytes stack frame, 0 bytes spill stores, 0 bytes spill loads
ptxas info    : Used 232 registers, 584 bytes cmem[0]
ptxas info    : Compiling entry function '_Z16flash_fwd_kernelI23Flash_fwd_kernel_traitsILi224ELi64ELi64ELi4ELb0ELb0EN7cutlass6half_tE19Flash_kernel_traitsILi224ELi64ELi64ELi4ES2_EELb1ELb1ELb1ELb1ELb1EEv16Flash_fwd_params' for 'sm_80'
ptxas info    : Function properties for _Z16flash_fwd_kernelI23Flash_fwd_kernel_traitsILi224ELi64ELi64ELi4ELb0ELb0EN7cutlass6half_tE19Flash_kernel_traitsILi224ELi64ELi64ELi4ES2_EELb1ELb1ELb1ELb1ELb1EEv16Flash_fwd_params
    32 bytes stack frame, 40 bytes spill stores, 32 bytes spill loads
ptxas info    : Used 255 registers, 584 bytes cmem[0]
ptxas info    : Compiling entry function '_Z16flash_fwd_kernelI23Flash_fwd_kernel_traitsILi224ELi128ELi64ELi8ELb0ELb0EN7cutlass6half_tE19Flash_kernel_traitsILi224ELi128ELi64ELi8ES2_EELb1ELb1ELb0ELb0ELb0EEv16Flash_fwd_params' for 'sm_80'
ptxas info    : Function properties for _Z16flash_fwd_kernelI23Flash_fwd_kernel_traitsILi224ELi128ELi64ELi8ELb0ELb0EN7cutlass6half_tE19Flash_kernel_traitsILi224ELi128ELi64ELi8ES2_EELb1ELb1ELb0ELb0ELb0EEv16Flash_fwd_params
    72 bytes stack frame, 72 bytes spill stores, 72 bytes spill loads
ptxas info    : Used 255 registers, 584 bytes cmem[0]
ptxas info    : Compiling entry function '_Z16flash_fwd_kernelI23Flash_fwd_kernel_traitsILi224ELi128ELi64ELi8ELb0ELb0EN7cutlass6half_tE19Flash_kernel_traitsILi224ELi128ELi64ELi8ES2_EELb1ELb1ELb0ELb0ELb1EEv16Flash_fwd_params' for 'sm_80'
ptxas info    : Function properties for _Z16flash_fwd_kernelI23Flash_fwd_kernel_traitsILi224ELi128ELi64ELi8ELb0ELb0EN7cutlass6half_tE19Flash_kernel_traitsILi224ELi128ELi64ELi8ES2_EELb1ELb1ELb0ELb0ELb1EEv16Flash_fwd_params
    80 bytes stack frame, 106 bytes spill stores, 110 bytes spill loads
ptxas info    : Used 255 registers, 584 bytes cmem[0]
ptxas info    : Compiling entry function '_Z16flash_fwd_kernelI23Flash_fwd_kernel_traitsILi224ELi128ELi64ELi8ELb0ELb0EN7cutlass6half_tE19Flash_kernel_traitsILi224ELi128ELi64ELi8ES2_EELb1ELb1ELb0ELb1ELb0EEv16Flash_fwd_params' for 'sm_80'
ptxas info    : Function properties for _Z16flash_fwd_kernelI23Flash_fwd_kernel_traitsILi224ELi128ELi64ELi8ELb0ELb0EN7cutlass6half_tE19Flash_kernel_traitsILi224ELi128ELi64ELi8ES2_EELb1ELb1ELb0ELb1ELb0EEv16Flash_fwd_params
    0 bytes stack frame, 0 bytes spill stores, 0 bytes spill loads
ptxas info    : Used 251 registers, 584 bytes cmem[0]
ptxas info    : Compiling entry function '_Z16flash_fwd_kernelI23Flash_fwd_kernel_traitsILi224ELi128ELi64ELi8ELb0ELb0EN7cutlass6half_tE19Flash_kernel_traitsILi224ELi128ELi64ELi8ES2_EELb1ELb1ELb0ELb1ELb1EEv16Flash_fwd_params' for 'sm_80'
ptxas info    : Function properties for _Z16flash_fwd_kernelI23Flash_fwd_kernel_traitsILi224ELi128ELi64ELi8ELb0ELb0EN7cutlass6half_tE19Flash_kernel_traitsILi224ELi128ELi64ELi8ES2_EELb1ELb1ELb0ELb1ELb1EEv16Flash_fwd_params
    88 bytes stack frame, 60 bytes spill stores, 78 bytes spill loads
ptxas info    : Used 255 registers, 584 bytes cmem[0]
ptxas info    : Compiling entry function '_Z16flash_fwd_kernelI23Flash_fwd_kernel_traitsILi224ELi128ELi64ELi8ELb0ELb0EN7cutlass6half_tE19Flash_kernel_traitsILi224ELi128ELi64ELi8ES2_EELb1ELb1ELb1ELb0ELb0EEv16Flash_fwd_params' for 'sm_80'
ptxas info    : Function properties for _Z16flash_fwd_kernelI23Flash_fwd_kernel_traitsILi224ELi128ELi64ELi8ELb0ELb0EN7cutlass6half_tE19Flash_kernel_traitsILi224ELi128ELi64ELi8ES2_EELb1ELb1ELb1ELb0ELb0EEv16Flash_fwd_params
    72 bytes stack frame, 84 bytes spill stores, 84 bytes spill loads
ptxas info    : Used 255 registers, 584 bytes cmem[0]
ptxas info    : Compiling entry function '_Z16flash_fwd_kernelI23Flash_fwd_kernel_traitsILi224ELi128ELi64ELi8ELb0ELb0EN7cutlass6half_tE19Flash_kernel_traitsILi224ELi128ELi64ELi8ES2_EELb1ELb1ELb1ELb0ELb1EEv16Flash_fwd_params' for 'sm_80'
ptxas info    : Function properties for _Z16flash_fwd_kernelI23Flash_fwd_kernel_traitsILi224ELi128ELi64ELi8ELb0ELb0EN7cutlass6half_tE19Flash_kernel_traitsILi224ELi128ELi64ELi8ES2_EELb1ELb1ELb1ELb0ELb1EEv16Flash_fwd_params
    56 bytes stack frame, 68 bytes spill stores, 60 bytes spill loads
ptxas info    : Used 255 registers, 584 bytes cmem[0]
ptxas info    : Compiling entry function '_Z16flash_fwd_kernelI23Flash_fwd_kernel_traitsILi224ELi128ELi64ELi8ELb0ELb0EN7cutlass6half_tE19Flash_kernel_traitsILi224ELi128ELi64ELi8ES2_EELb1ELb1ELb1ELb1ELb0EEv16Flash_fwd_params' for 'sm_80'
ptxas info    : Function properties for _Z16flash_fwd_kernelI23Flash_fwd_kernel_traitsILi224ELi128ELi64ELi8ELb0ELb0EN7cutlass6half_tE19Flash_kernel_traitsILi224ELi128ELi64ELi8ES2_EELb1ELb1ELb1ELb1ELb0EEv16Flash_fwd_params
    0 bytes stack frame, 0 bytes spill stores, 0 bytes spill loads
ptxas info    : Used 235 registers, 584 bytes cmem[0]
ptxas info    : Compiling entry function '_Z16flash_fwd_kernelI23Flash_fwd_kernel_traitsILi224ELi128ELi64ELi8ELb0ELb0EN7cutlass6half_tE19Flash_kernel_traitsILi224ELi128ELi64ELi8ES2_EELb1ELb1ELb1ELb1ELb1EEv16Flash_fwd_params' for 'sm_80'
ptxas info    : Function properties for _Z16flash_fwd_kernelI23Flash_fwd_kernel_traitsILi224ELi128ELi64ELi8ELb0ELb0EN7cutlass6half_tE19Flash_kernel_traitsILi224ELi128ELi64ELi8ES2_EELb1ELb1ELb1ELb1ELb1EEv16Flash_fwd_params
    88 bytes stack frame, 64 bytes spill stores, 94 bytes spill loads
ptxas info    : Used 255 registers, 584 bytes cmem[0]
/anaconda/envs/flashattention/lib/python3.9/site-packages/torch/include/c10/core/SymInt.h(84): warning: integer conversion resulted in a change of sign

[20/36] /usr/local/cuda-11.4/bin/nvcc  -I/home/v-feiychen/flash-attention/csrc/flash_attn -I/home/v-feiychen/flash-attention/csrc/flash_attn/src -I/home/v-feiychen/flash-attention/csrc/cutlass/include -I/anaconda/envs/flashattention/lib/python3.9/site-packages/torch/include -I/anaconda/envs/flashattention/lib/python3.9/site-packages/torch/include/torch/csrc/api/include -I/anaconda/envs/flashattention/lib/python3.9/site-packages/torch/include/TH -I/anaconda/envs/flashattention/lib/python3.9/site-packages/torch/include/THC -I/usr/local/cuda-11.4/include -I/anaconda/envs/flashattention/include/python3.9 -c -c /home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_hdim224_bf16_sm80.cu -o /home/v-feiychen/flash-attention/build/temp.linux-x86_64-cpython-39/csrc/flash_attn/src/flash_fwd_hdim224_bf16_sm80.o -D__CUDA_NO_HALF_OPERATORS__ -D__CUDA_NO_HALF_CONVERSIONS__ -D__CUDA_NO_BFLOAT16_CONVERSIONS__ -D__CUDA_NO_HALF2_OPERATORS__ --expt-relaxed-constexpr --compiler-options ''"'"'-fPIC'"'"'' -O3 -std=c++17 -U__CUDA_NO_HALF_OPERATORS__ -U__CUDA_NO_HALF_CONVERSIONS__ -U__CUDA_NO_HALF2_OPERATORS__ -U__CUDA_NO_BFLOAT16_CONVERSIONS__ --expt-relaxed-constexpr --expt-extended-lambda --use_fast_math --ptxas-options=-v -lineinfo -gencode arch=compute_80,code=sm_80 --threads 4 -DTORCH_API_INCLUDE_EXTENSION_H '-DPYBIND11_COMPILER_TYPE="_gcc"' '-DPYBIND11_STDLIB="_libstdcpp"' '-DPYBIND11_BUILD_ABI="_cxxabi1011"' -DTORCH_EXTENSION_NAME=flash_attn_2_cuda -D_GLIBCXX_USE_CXX11_ABI=0
/anaconda/envs/flashattention/lib/python3.9/site-packages/torch/include/c10/core/SymInt.h(84): warning: integer conversion resulted in a change of sign

/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_kernel.h(140): warning: variable "MMA_M" was declared but never referenced
          detected during:
            instantiation of "void flash::compute_attn_1rowblock<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &, int, int, int) [with Kernel_traits=Flash_fwd_kernel_traits<224, 128, 64, 8, false, false, cutlass::bfloat16_t, Flash_kernel_traits<224, 128, 64, 8, cutlass::bfloat16_t>>, Is_dropout=true, Is_causal=true, Is_even_N=true, Is_even_K=true, Return_softmax=true, Params=Flash_fwd_params]" 
(588): here
            instantiation of "void flash::compute_attn<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &) [with Kernel_traits=Flash_fwd_kernel_traits<224, 128, 64, 8, false, false, cutlass::bfloat16_t, Flash_kernel_traits<224, 128, 64, 8, cutlass::bfloat16_t>>, Is_dropout=true, Is_causal=true, Is_even_N=true, Is_even_K=true, Return_softmax=true, Params=Flash_fwd_params]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(15): here
            instantiation of "void flash_fwd_kernel<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax>(Flash_fwd_params) [with Kernel_traits=Flash_fwd_kernel_traits<224, 128, 64, 8, false, false, cutlass::bfloat16_t, Flash_kernel_traits<224, 128, 64, 8, cutlass::bfloat16_t>>, Is_dropout=true, Is_causal=true, Is_even_N=true, Is_even_K=true, Return_softmax=true]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(34): here
            instantiation of "void run_flash_fwd<Kernel_traits,Is_dropout,Is_causal>(Flash_fwd_params &, cudaStream_t) [with Kernel_traits=Flash_fwd_kernel_traits<224, 128, 64, 8, false, false, cutlass::bfloat16_t, Flash_kernel_traits<224, 128, 64, 8, cutlass::bfloat16_t>>, Is_dropout=true, Is_causal=true]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(208): here
            instantiation of "void run_mha_fwd_hdim224<T>(Flash_fwd_params &, cudaStream_t) [with T=cutlass::bfloat16_t]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_hdim224_bf16_sm80.cu(8): here

/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_kernel.h(275): warning: variable "tQrQ" was declared but never referenced
          detected during:
            instantiation of "void flash::compute_attn_1rowblock<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &, int, int, int) [with Kernel_traits=Flash_fwd_kernel_traits<224, 128, 64, 8, false, false, cutlass::bfloat16_t, Flash_kernel_traits<224, 128, 64, 8, cutlass::bfloat16_t>>, Is_dropout=true, Is_causal=true, Is_even_N=true, Is_even_K=true, Return_softmax=true, Params=Flash_fwd_params]" 
(588): here
            instantiation of "void flash::compute_attn<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &) [with Kernel_traits=Flash_fwd_kernel_traits<224, 128, 64, 8, false, false, cutlass::bfloat16_t, Flash_kernel_traits<224, 128, 64, 8, cutlass::bfloat16_t>>, Is_dropout=true, Is_causal=true, Is_even_N=true, Is_even_K=true, Return_softmax=true, Params=Flash_fwd_params]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(15): here
            instantiation of "void flash_fwd_kernel<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax>(Flash_fwd_params) [with Kernel_traits=Flash_fwd_kernel_traits<224, 128, 64, 8, false, false, cutlass::bfloat16_t, Flash_kernel_traits<224, 128, 64, 8, cutlass::bfloat16_t>>, Is_dropout=true, Is_causal=true, Is_even_N=true, Is_even_K=true, Return_softmax=true]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(34): here
            instantiation of "void run_flash_fwd<Kernel_traits,Is_dropout,Is_causal>(Flash_fwd_params &, cudaStream_t) [with Kernel_traits=Flash_fwd_kernel_traits<224, 128, 64, 8, false, false, cutlass::bfloat16_t, Flash_kernel_traits<224, 128, 64, 8, cutlass::bfloat16_t>>, Is_dropout=true, Is_causal=true]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(208): here
            instantiation of "void run_mha_fwd_hdim224<T>(Flash_fwd_params &, cudaStream_t) [with T=cutlass::bfloat16_t]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_hdim224_bf16_sm80.cu(8): here

/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_kernel.h(140): warning: variable "MMA_M" was declared but never referenced
          detected during:
            instantiation of "void flash::compute_attn_1rowblock<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &, int, int, int) [with Kernel_traits=Flash_fwd_kernel_traits<224, 128, 64, 8, false, false, cutlass::bfloat16_t, Flash_kernel_traits<224, 128, 64, 8, cutlass::bfloat16_t>>, Is_dropout=true, Is_causal=true, Is_even_N=true, Is_even_K=true, Return_softmax=false, Params=Flash_fwd_params]" 
(588): here
            instantiation of "void flash::compute_attn<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &) [with Kernel_traits=Flash_fwd_kernel_traits<224, 128, 64, 8, false, false, cutlass::bfloat16_t, Flash_kernel_traits<224, 128, 64, 8, cutlass::bfloat16_t>>, Is_dropout=true, Is_causal=true, Is_even_N=true, Is_even_K=true, Return_softmax=false, Params=Flash_fwd_params]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(15): here
            instantiation of "void flash_fwd_kernel<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax>(Flash_fwd_params) [with Kernel_traits=Flash_fwd_kernel_traits<224, 128, 64, 8, false, false, cutlass::bfloat16_t, Flash_kernel_traits<224, 128, 64, 8, cutlass::bfloat16_t>>, Is_dropout=true, Is_causal=true, Is_even_N=true, Is_even_K=true, Return_softmax=false]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(34): here
            instantiation of "void run_flash_fwd<Kernel_traits,Is_dropout,Is_causal>(Flash_fwd_params &, cudaStream_t) [with Kernel_traits=Flash_fwd_kernel_traits<224, 128, 64, 8, false, false, cutlass::bfloat16_t, Flash_kernel_traits<224, 128, 64, 8, cutlass::bfloat16_t>>, Is_dropout=true, Is_causal=true]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(208): here
            instantiation of "void run_mha_fwd_hdim224<T>(Flash_fwd_params &, cudaStream_t) [with T=cutlass::bfloat16_t]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_hdim224_bf16_sm80.cu(8): here

/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_kernel.h(275): warning: variable "tQrQ" was declared but never referenced
          detected during:
            instantiation of "void flash::compute_attn_1rowblock<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &, int, int, int) [with Kernel_traits=Flash_fwd_kernel_traits<224, 128, 64, 8, false, false, cutlass::bfloat16_t, Flash_kernel_traits<224, 128, 64, 8, cutlass::bfloat16_t>>, Is_dropout=true, Is_causal=true, Is_even_N=true, Is_even_K=true, Return_softmax=false, Params=Flash_fwd_params]" 
(588): here
            instantiation of "void flash::compute_attn<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &) [with Kernel_traits=Flash_fwd_kernel_traits<224, 128, 64, 8, false, false, cutlass::bfloat16_t, Flash_kernel_traits<224, 128, 64, 8, cutlass::bfloat16_t>>, Is_dropout=true, Is_causal=true, Is_even_N=true, Is_even_K=true, Return_softmax=false, Params=Flash_fwd_params]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(15): here
            instantiation of "void flash_fwd_kernel<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax>(Flash_fwd_params) [with Kernel_traits=Flash_fwd_kernel_traits<224, 128, 64, 8, false, false, cutlass::bfloat16_t, Flash_kernel_traits<224, 128, 64, 8, cutlass::bfloat16_t>>, Is_dropout=true, Is_causal=true, Is_even_N=true, Is_even_K=true, Return_softmax=false]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(34): here
            instantiation of "void run_flash_fwd<Kernel_traits,Is_dropout,Is_causal>(Flash_fwd_params &, cudaStream_t) [with Kernel_traits=Flash_fwd_kernel_traits<224, 128, 64, 8, false, false, cutlass::bfloat16_t, Flash_kernel_traits<224, 128, 64, 8, cutlass::bfloat16_t>>, Is_dropout=true, Is_causal=true]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(208): here
            instantiation of "void run_mha_fwd_hdim224<T>(Flash_fwd_params &, cudaStream_t) [with T=cutlass::bfloat16_t]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_hdim224_bf16_sm80.cu(8): here

/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_kernel.h(140): warning: variable "MMA_M" was declared but never referenced
          detected during:
            instantiation of "void flash::compute_attn_1rowblock<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &, int, int, int) [with Kernel_traits=Flash_fwd_kernel_traits<224, 128, 64, 8, false, false, cutlass::bfloat16_t, Flash_kernel_traits<224, 128, 64, 8, cutlass::bfloat16_t>>, Is_dropout=true, Is_causal=true, Is_even_N=true, Is_even_K=false, Return_softmax=true, Params=Flash_fwd_params]" 
(588): here
            instantiation of "void flash::compute_attn<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &) [with Kernel_traits=Flash_fwd_kernel_traits<224, 128, 64, 8, false, false, cutlass::bfloat16_t, Flash_kernel_traits<224, 128, 64, 8, cutlass::bfloat16_t>>, Is_dropout=true, Is_causal=true, Is_even_N=true, Is_even_K=false, Return_softmax=true, Params=Flash_fwd_params]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(15): here
            instantiation of "void flash_fwd_kernel<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax>(Flash_fwd_params) [with Kernel_traits=Flash_fwd_kernel_traits<224, 128, 64, 8, false, false, cutlass::bfloat16_t, Flash_kernel_traits<224, 128, 64, 8, cutlass::bfloat16_t>>, Is_dropout=true, Is_causal=true, Is_even_N=true, Is_even_K=false, Return_softmax=true]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(34): here
            instantiation of "void run_flash_fwd<Kernel_traits,Is_dropout,Is_causal>(Flash_fwd_params &, cudaStream_t) [with Kernel_traits=Flash_fwd_kernel_traits<224, 128, 64, 8, false, false, cutlass::bfloat16_t, Flash_kernel_traits<224, 128, 64, 8, cutlass::bfloat16_t>>, Is_dropout=true, Is_causal=true]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(208): here
            instantiation of "void run_mha_fwd_hdim224<T>(Flash_fwd_params &, cudaStream_t) [with T=cutlass::bfloat16_t]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_hdim224_bf16_sm80.cu(8): here

/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_kernel.h(275): warning: variable "tQrQ" was declared but never referenced
          detected during:
            instantiation of "void flash::compute_attn_1rowblock<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &, int, int, int) [with Kernel_traits=Flash_fwd_kernel_traits<224, 128, 64, 8, false, false, cutlass::bfloat16_t, Flash_kernel_traits<224, 128, 64, 8, cutlass::bfloat16_t>>, Is_dropout=true, Is_causal=true, Is_even_N=true, Is_even_K=false, Return_softmax=true, Params=Flash_fwd_params]" 
(588): here
            instantiation of "void flash::compute_attn<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &) [with Kernel_traits=Flash_fwd_kernel_traits<224, 128, 64, 8, false, false, cutlass::bfloat16_t, Flash_kernel_traits<224, 128, 64, 8, cutlass::bfloat16_t>>, Is_dropout=true, Is_causal=true, Is_even_N=true, Is_even_K=false, Return_softmax=true, Params=Flash_fwd_params]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(15): here
            instantiation of "void flash_fwd_kernel<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax>(Flash_fwd_params) [with Kernel_traits=Flash_fwd_kernel_traits<224, 128, 64, 8, false, false, cutlass::bfloat16_t, Flash_kernel_traits<224, 128, 64, 8, cutlass::bfloat16_t>>, Is_dropout=true, Is_causal=true, Is_even_N=true, Is_even_K=false, Return_softmax=true]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(34): here
            instantiation of "void run_flash_fwd<Kernel_traits,Is_dropout,Is_causal>(Flash_fwd_params &, cudaStream_t) [with Kernel_traits=Flash_fwd_kernel_traits<224, 128, 64, 8, false, false, cutlass::bfloat16_t, Flash_kernel_traits<224, 128, 64, 8, cutlass::bfloat16_t>>, Is_dropout=true, Is_causal=true]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(208): here
            instantiation of "void run_mha_fwd_hdim224<T>(Flash_fwd_params &, cudaStream_t) [with T=cutlass::bfloat16_t]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_hdim224_bf16_sm80.cu(8): here

/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_kernel.h(140): warning: variable "MMA_M" was declared but never referenced
          detected during:
            instantiation of "void flash::compute_attn_1rowblock<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &, int, int, int) [with Kernel_traits=Flash_fwd_kernel_traits<224, 128, 64, 8, false, false, cutlass::bfloat16_t, Flash_kernel_traits<224, 128, 64, 8, cutlass::bfloat16_t>>, Is_dropout=true, Is_causal=true, Is_even_N=true, Is_even_K=false, Return_softmax=false, Params=Flash_fwd_params]" 
(588): here
            instantiation of "void flash::compute_attn<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &) [with Kernel_traits=Flash_fwd_kernel_traits<224, 128, 64, 8, false, false, cutlass::bfloat16_t, Flash_kernel_traits<224, 128, 64, 8, cutlass::bfloat16_t>>, Is_dropout=true, Is_causal=true, Is_even_N=true, Is_even_K=false, Return_softmax=false, Params=Flash_fwd_params]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(15): here
            instantiation of "void flash_fwd_kernel<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax>(Flash_fwd_params) [with Kernel_traits=Flash_fwd_kernel_traits<224, 128, 64, 8, false, false, cutlass::bfloat16_t, Flash_kernel_traits<224, 128, 64, 8, cutlass::bfloat16_t>>, Is_dropout=true, Is_causal=true, Is_even_N=true, Is_even_K=false, Return_softmax=false]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(34): here
            instantiation of "void run_flash_fwd<Kernel_traits,Is_dropout,Is_causal>(Flash_fwd_params &, cudaStream_t) [with Kernel_traits=Flash_fwd_kernel_traits<224, 128, 64, 8, false, false, cutlass::bfloat16_t, Flash_kernel_traits<224, 128, 64, 8, cutlass::bfloat16_t>>, Is_dropout=true, Is_causal=true]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(208): here
            instantiation of "void run_mha_fwd_hdim224<T>(Flash_fwd_params &, cudaStream_t) [with T=cutlass::bfloat16_t]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_hdim224_bf16_sm80.cu(8): here

/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_kernel.h(275): warning: variable "tQrQ" was declared but never referenced
          detected during:
            instantiation of "void flash::compute_attn_1rowblock<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &, int, int, int) [with Kernel_traits=Flash_fwd_kernel_traits<224, 128, 64, 8, false, false, cutlass::bfloat16_t, Flash_kernel_traits<224, 128, 64, 8, cutlass::bfloat16_t>>, Is_dropout=true, Is_causal=true, Is_even_N=true, Is_even_K=false, Return_softmax=false, Params=Flash_fwd_params]" 
(588): here
            instantiation of "void flash::compute_attn<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &) [with Kernel_traits=Flash_fwd_kernel_traits<224, 128, 64, 8, false, false, cutlass::bfloat16_t, Flash_kernel_traits<224, 128, 64, 8, cutlass::bfloat16_t>>, Is_dropout=true, Is_causal=true, Is_even_N=true, Is_even_K=false, Return_softmax=false, Params=Flash_fwd_params]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(15): here
            instantiation of "void flash_fwd_kernel<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax>(Flash_fwd_params) [with Kernel_traits=Flash_fwd_kernel_traits<224, 128, 64, 8, false, false, cutlass::bfloat16_t, Flash_kernel_traits<224, 128, 64, 8, cutlass::bfloat16_t>>, Is_dropout=true, Is_causal=true, Is_even_N=true, Is_even_K=false, Return_softmax=false]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(34): here
            instantiation of "void run_flash_fwd<Kernel_traits,Is_dropout,Is_causal>(Flash_fwd_params &, cudaStream_t) [with Kernel_traits=Flash_fwd_kernel_traits<224, 128, 64, 8, false, false, cutlass::bfloat16_t, Flash_kernel_traits<224, 128, 64, 8, cutlass::bfloat16_t>>, Is_dropout=true, Is_causal=true]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(208): here
            instantiation of "void run_mha_fwd_hdim224<T>(Flash_fwd_params &, cudaStream_t) [with T=cutlass::bfloat16_t]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_hdim224_bf16_sm80.cu(8): here

/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_kernel.h(140): warning: variable "MMA_M" was declared but never referenced
          detected during:
            instantiation of "void flash::compute_attn_1rowblock<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &, int, int, int) [with Kernel_traits=Flash_fwd_kernel_traits<224, 128, 64, 8, false, false, cutlass::bfloat16_t, Flash_kernel_traits<224, 128, 64, 8, cutlass::bfloat16_t>>, Is_dropout=true, Is_causal=true, Is_even_N=false, Is_even_K=true, Return_softmax=true, Params=Flash_fwd_params]" 
(588): here
            instantiation of "void flash::compute_attn<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &) [with Kernel_traits=Flash_fwd_kernel_traits<224, 128, 64, 8, false, false, cutlass::bfloat16_t, Flash_kernel_traits<224, 128, 64, 8, cutlass::bfloat16_t>>, Is_dropout=true, Is_causal=true, Is_even_N=false, Is_even_K=true, Return_softmax=true, Params=Flash_fwd_params]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(15): here
            instantiation of "void flash_fwd_kernel<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax>(Flash_fwd_params) [with Kernel_traits=Flash_fwd_kernel_traits<224, 128, 64, 8, false, false, cutlass::bfloat16_t, Flash_kernel_traits<224, 128, 64, 8, cutlass::bfloat16_t>>, Is_dropout=true, Is_causal=true, Is_even_N=false, Is_even_K=true, Return_softmax=true]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(34): here
            instantiation of "void run_flash_fwd<Kernel_traits,Is_dropout,Is_causal>(Flash_fwd_params &, cudaStream_t) [with Kernel_traits=Flash_fwd_kernel_traits<224, 128, 64, 8, false, false, cutlass::bfloat16_t, Flash_kernel_traits<224, 128, 64, 8, cutlass::bfloat16_t>>, Is_dropout=true, Is_causal=true]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(208): here
            instantiation of "void run_mha_fwd_hdim224<T>(Flash_fwd_params &, cudaStream_t) [with T=cutlass::bfloat16_t]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_hdim224_bf16_sm80.cu(8): here

/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_kernel.h(275): warning: variable "tQrQ" was declared but never referenced
          detected during:
            instantiation of "void flash::compute_attn_1rowblock<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &, int, int, int) [with Kernel_traits=Flash_fwd_kernel_traits<224, 128, 64, 8, false, false, cutlass::bfloat16_t, Flash_kernel_traits<224, 128, 64, 8, cutlass::bfloat16_t>>, Is_dropout=true, Is_causal=true, Is_even_N=false, Is_even_K=true, Return_softmax=true, Params=Flash_fwd_params]" 
(588): here
            instantiation of "void flash::compute_attn<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &) [with Kernel_traits=Flash_fwd_kernel_traits<224, 128, 64, 8, false, false, cutlass::bfloat16_t, Flash_kernel_traits<224, 128, 64, 8, cutlass::bfloat16_t>>, Is_dropout=true, Is_causal=true, Is_even_N=false, Is_even_K=true, Return_softmax=true, Params=Flash_fwd_params]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(15): here
            instantiation of "void flash_fwd_kernel<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax>(Flash_fwd_params) [with Kernel_traits=Flash_fwd_kernel_traits<224, 128, 64, 8, false, false, cutlass::bfloat16_t, Flash_kernel_traits<224, 128, 64, 8, cutlass::bfloat16_t>>, Is_dropout=true, Is_causal=true, Is_even_N=false, Is_even_K=true, Return_softmax=true]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(34): here
            instantiation of "void run_flash_fwd<Kernel_traits,Is_dropout,Is_causal>(Flash_fwd_params &, cudaStream_t) [with Kernel_traits=Flash_fwd_kernel_traits<224, 128, 64, 8, false, false, cutlass::bfloat16_t, Flash_kernel_traits<224, 128, 64, 8, cutlass::bfloat16_t>>, Is_dropout=true, Is_causal=true]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(208): here
            instantiation of "void run_mha_fwd_hdim224<T>(Flash_fwd_params &, cudaStream_t) [with T=cutlass::bfloat16_t]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_hdim224_bf16_sm80.cu(8): here

/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_kernel.h(140): warning: variable "MMA_M" was declared but never referenced
          detected during:
            instantiation of "void flash::compute_attn_1rowblock<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &, int, int, int) [with Kernel_traits=Flash_fwd_kernel_traits<224, 128, 64, 8, false, false, cutlass::bfloat16_t, Flash_kernel_traits<224, 128, 64, 8, cutlass::bfloat16_t>>, Is_dropout=true, Is_causal=true, Is_even_N=false, Is_even_K=true, Return_softmax=false, Params=Flash_fwd_params]" 
(588): here
            instantiation of "void flash::compute_attn<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &) [with Kernel_traits=Flash_fwd_kernel_traits<224, 128, 64, 8, false, false, cutlass::bfloat16_t, Flash_kernel_traits<224, 128, 64, 8, cutlass::bfloat16_t>>, Is_dropout=true, Is_causal=true, Is_even_N=false, Is_even_K=true, Return_softmax=false, Params=Flash_fwd_params]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(15): here
            instantiation of "void flash_fwd_kernel<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax>(Flash_fwd_params) [with Kernel_traits=Flash_fwd_kernel_traits<224, 128, 64, 8, false, false, cutlass::bfloat16_t, Flash_kernel_traits<224, 128, 64, 8, cutlass::bfloat16_t>>, Is_dropout=true, Is_causal=true, Is_even_N=false, Is_even_K=true, Return_softmax=false]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(34): here
            instantiation of "void run_flash_fwd<Kernel_traits,Is_dropout,Is_causal>(Flash_fwd_params &, cudaStream_t) [with Kernel_traits=Flash_fwd_kernel_traits<224, 128, 64, 8, false, false, cutlass::bfloat16_t, Flash_kernel_traits<224, 128, 64, 8, cutlass::bfloat16_t>>, Is_dropout=true, Is_causal=true]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(208): here
            instantiation of "void run_mha_fwd_hdim224<T>(Flash_fwd_params &, cudaStream_t) [with T=cutlass::bfloat16_t]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_hdim224_bf16_sm80.cu(8): here

/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_kernel.h(275): warning: variable "tQrQ" was declared but never referenced
          detected during:
            instantiation of "void flash::compute_attn_1rowblock<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &, int, int, int) [with Kernel_traits=Flash_fwd_kernel_traits<224, 128, 64, 8, false, false, cutlass::bfloat16_t, Flash_kernel_traits<224, 128, 64, 8, cutlass::bfloat16_t>>, Is_dropout=true, Is_causal=true, Is_even_N=false, Is_even_K=true, Return_softmax=false, Params=Flash_fwd_params]" 
(588): here
            instantiation of "void flash::compute_attn<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &) [with Kernel_traits=Flash_fwd_kernel_traits<224, 128, 64, 8, false, false, cutlass::bfloat16_t, Flash_kernel_traits<224, 128, 64, 8, cutlass::bfloat16_t>>, Is_dropout=true, Is_causal=true, Is_even_N=false, Is_even_K=true, Return_softmax=false, Params=Flash_fwd_params]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(15): here
            instantiation of "void flash_fwd_kernel<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax>(Flash_fwd_params) [with Kernel_traits=Flash_fwd_kernel_traits<224, 128, 64, 8, false, false, cutlass::bfloat16_t, Flash_kernel_traits<224, 128, 64, 8, cutlass::bfloat16_t>>, Is_dropout=true, Is_causal=true, Is_even_N=false, Is_even_K=true, Return_softmax=false]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(34): here
            instantiation of "void run_flash_fwd<Kernel_traits,Is_dropout,Is_causal>(Flash_fwd_params &, cudaStream_t) [with Kernel_traits=Flash_fwd_kernel_traits<224, 128, 64, 8, false, false, cutlass::bfloat16_t, Flash_kernel_traits<224, 128, 64, 8, cutlass::bfloat16_t>>, Is_dropout=true, Is_causal=true]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(208): here
            instantiation of "void run_mha_fwd_hdim224<T>(Flash_fwd_params &, cudaStream_t) [with T=cutlass::bfloat16_t]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_hdim224_bf16_sm80.cu(8): here

/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_kernel.h(140): warning: variable "MMA_M" was declared but never referenced
          detected during:
            instantiation of "void flash::compute_attn_1rowblock<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &, int, int, int) [with Kernel_traits=Flash_fwd_kernel_traits<224, 128, 64, 8, false, false, cutlass::bfloat16_t, Flash_kernel_traits<224, 128, 64, 8, cutlass::bfloat16_t>>, Is_dropout=true, Is_causal=true, Is_even_N=false, Is_even_K=false, Return_softmax=true, Params=Flash_fwd_params]" 
(588): here
            instantiation of "void flash::compute_attn<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &) [with Kernel_traits=Flash_fwd_kernel_traits<224, 128, 64, 8, false, false, cutlass::bfloat16_t, Flash_kernel_traits<224, 128, 64, 8, cutlass::bfloat16_t>>, Is_dropout=true, Is_causal=true, Is_even_N=false, Is_even_K=false, Return_softmax=true, Params=Flash_fwd_params]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(15): here
            instantiation of "void flash_fwd_kernel<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax>(Flash_fwd_params) [with Kernel_traits=Flash_fwd_kernel_traits<224, 128, 64, 8, false, false, cutlass::bfloat16_t, Flash_kernel_traits<224, 128, 64, 8, cutlass::bfloat16_t>>, Is_dropout=true, Is_causal=true, Is_even_N=false, Is_even_K=false, Return_softmax=true]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(34): here
            instantiation of "void run_flash_fwd<Kernel_traits,Is_dropout,Is_causal>(Flash_fwd_params &, cudaStream_t) [with Kernel_traits=Flash_fwd_kernel_traits<224, 128, 64, 8, false, false, cutlass::bfloat16_t, Flash_kernel_traits<224, 128, 64, 8, cutlass::bfloat16_t>>, Is_dropout=true, Is_causal=true]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(208): here
            instantiation of "void run_mha_fwd_hdim224<T>(Flash_fwd_params &, cudaStream_t) [with T=cutlass::bfloat16_t]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_hdim224_bf16_sm80.cu(8): here

/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_kernel.h(275): warning: variable "tQrQ" was declared but never referenced
          detected during:
            instantiation of "void flash::compute_attn_1rowblock<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &, int, int, int) [with Kernel_traits=Flash_fwd_kernel_traits<224, 128, 64, 8, false, false, cutlass::bfloat16_t, Flash_kernel_traits<224, 128, 64, 8, cutlass::bfloat16_t>>, Is_dropout=true, Is_causal=true, Is_even_N=false, Is_even_K=false, Return_softmax=true, Params=Flash_fwd_params]" 
(588): here
            instantiation of "void flash::compute_attn<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &) [with Kernel_traits=Flash_fwd_kernel_traits<224, 128, 64, 8, false, false, cutlass::bfloat16_t, Flash_kernel_traits<224, 128, 64, 8, cutlass::bfloat16_t>>, Is_dropout=true, Is_causal=true, Is_even_N=false, Is_even_K=false, Return_softmax=true, Params=Flash_fwd_params]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(15): here
            instantiation of "void flash_fwd_kernel<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax>(Flash_fwd_params) [with Kernel_traits=Flash_fwd_kernel_traits<224, 128, 64, 8, false, false, cutlass::bfloat16_t, Flash_kernel_traits<224, 128, 64, 8, cutlass::bfloat16_t>>, Is_dropout=true, Is_causal=true, Is_even_N=false, Is_even_K=false, Return_softmax=true]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(34): here
            instantiation of "void run_flash_fwd<Kernel_traits,Is_dropout,Is_causal>(Flash_fwd_params &, cudaStream_t) [with Kernel_traits=Flash_fwd_kernel_traits<224, 128, 64, 8, false, false, cutlass::bfloat16_t, Flash_kernel_traits<224, 128, 64, 8, cutlass::bfloat16_t>>, Is_dropout=true, Is_causal=true]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(208): here
            instantiation of "void run_mha_fwd_hdim224<T>(Flash_fwd_params &, cudaStream_t) [with T=cutlass::bfloat16_t]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_hdim224_bf16_sm80.cu(8): here

/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_kernel.h(140): warning: variable "MMA_M" was declared but never referenced
          detected during:
            instantiation of "void flash::compute_attn_1rowblock<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &, int, int, int) [with Kernel_traits=Flash_fwd_kernel_traits<224, 128, 64, 8, false, false, cutlass::bfloat16_t, Flash_kernel_traits<224, 128, 64, 8, cutlass::bfloat16_t>>, Is_dropout=true, Is_causal=true, Is_even_N=false, Is_even_K=false, Return_softmax=false, Params=Flash_fwd_params]" 
(588): here
            instantiation of "void flash::compute_attn<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &) [with Kernel_traits=Flash_fwd_kernel_traits<224, 128, 64, 8, false, false, cutlass::bfloat16_t, Flash_kernel_traits<224, 128, 64, 8, cutlass::bfloat16_t>>, Is_dropout=true, Is_causal=true, Is_even_N=false, Is_even_K=false, Return_softmax=false, Params=Flash_fwd_params]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(15): here
            instantiation of "void flash_fwd_kernel<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax>(Flash_fwd_params) [with Kernel_traits=Flash_fwd_kernel_traits<224, 128, 64, 8, false, false, cutlass::bfloat16_t, Flash_kernel_traits<224, 128, 64, 8, cutlass::bfloat16_t>>, Is_dropout=true, Is_causal=true, Is_even_N=false, Is_even_K=false, Return_softmax=false]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(34): here
            instantiation of "void run_flash_fwd<Kernel_traits,Is_dropout,Is_causal>(Flash_fwd_params &, cudaStream_t) [with Kernel_traits=Flash_fwd_kernel_traits<224, 128, 64, 8, false, false, cutlass::bfloat16_t, Flash_kernel_traits<224, 128, 64, 8, cutlass::bfloat16_t>>, Is_dropout=true, Is_causal=true]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(208): here
            instantiation of "void run_mha_fwd_hdim224<T>(Flash_fwd_params &, cudaStream_t) [with T=cutlass::bfloat16_t]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_hdim224_bf16_sm80.cu(8): here

/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_kernel.h(275): warning: variable "tQrQ" was declared but never referenced
          detected during:
            instantiation of "void flash::compute_attn_1rowblock<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &, int, int, int) [with Kernel_traits=Flash_fwd_kernel_traits<224, 128, 64, 8, false, false, cutlass::bfloat16_t, Flash_kernel_traits<224, 128, 64, 8, cutlass::bfloat16_t>>, Is_dropout=true, Is_causal=true, Is_even_N=false, Is_even_K=false, Return_softmax=false, Params=Flash_fwd_params]" 
(588): here
            instantiation of "void flash::compute_attn<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &) [with Kernel_traits=Flash_fwd_kernel_traits<224, 128, 64, 8, false, false, cutlass::bfloat16_t, Flash_kernel_traits<224, 128, 64, 8, cutlass::bfloat16_t>>, Is_dropout=true, Is_causal=true, Is_even_N=false, Is_even_K=false, Return_softmax=false, Params=Flash_fwd_params]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(15): here
            instantiation of "void flash_fwd_kernel<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax>(Flash_fwd_params) [with Kernel_traits=Flash_fwd_kernel_traits<224, 128, 64, 8, false, false, cutlass::bfloat16_t, Flash_kernel_traits<224, 128, 64, 8, cutlass::bfloat16_t>>, Is_dropout=true, Is_causal=true, Is_even_N=false, Is_even_K=false, Return_softmax=false]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(34): here
            instantiation of "void run_flash_fwd<Kernel_traits,Is_dropout,Is_causal>(Flash_fwd_params &, cudaStream_t) [with Kernel_traits=Flash_fwd_kernel_traits<224, 128, 64, 8, false, false, cutlass::bfloat16_t, Flash_kernel_traits<224, 128, 64, 8, cutlass::bfloat16_t>>, Is_dropout=true, Is_causal=true]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(208): here
            instantiation of "void run_mha_fwd_hdim224<T>(Flash_fwd_params &, cudaStream_t) [with T=cutlass::bfloat16_t]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_hdim224_bf16_sm80.cu(8): here

/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_kernel.h(140): warning: variable "MMA_M" was declared but never referenced
          detected during:
            instantiation of "void flash::compute_attn_1rowblock<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &, int, int, int) [with Kernel_traits=Flash_fwd_kernel_traits<224, 64, 64, 4, false, false, cutlass::bfloat16_t, Flash_kernel_traits<224, 64, 64, 4, cutlass::bfloat16_t>>, Is_dropout=true, Is_causal=true, Is_even_N=true, Is_even_K=true, Return_softmax=true, Params=Flash_fwd_params]" 
(588): here
            instantiation of "void flash::compute_attn<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &) [with Kernel_traits=Flash_fwd_kernel_traits<224, 64, 64, 4, false, false, cutlass::bfloat16_t, Flash_kernel_traits<224, 64, 64, 4, cutlass::bfloat16_t>>, Is_dropout=true, Is_causal=true, Is_even_N=true, Is_even_K=true, Return_softmax=true, Params=Flash_fwd_params]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(15): here
            instantiation of "void flash_fwd_kernel<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax>(Flash_fwd_params) [with Kernel_traits=Flash_fwd_kernel_traits<224, 64, 64, 4, false, false, cutlass::bfloat16_t, Flash_kernel_traits<224, 64, 64, 4, cutlass::bfloat16_t>>, Is_dropout=true, Is_causal=true, Is_even_N=true, Is_even_K=true, Return_softmax=true]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(34): here
            instantiation of "void run_flash_fwd<Kernel_traits,Is_dropout,Is_causal>(Flash_fwd_params &, cudaStream_t) [with Kernel_traits=Flash_fwd_kernel_traits<224, 64, 64, 4, false, false, cutlass::bfloat16_t, Flash_kernel_traits<224, 64, 64, 4, cutlass::bfloat16_t>>, Is_dropout=true, Is_causal=true]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(208): here
            instantiation of "void run_mha_fwd_hdim224<T>(Flash_fwd_params &, cudaStream_t) [with T=cutlass::bfloat16_t]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_hdim224_bf16_sm80.cu(8): here

/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_kernel.h(275): warning: variable "tQrQ" was declared but never referenced
          detected during:
            instantiation of "void flash::compute_attn_1rowblock<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &, int, int, int) [with Kernel_traits=Flash_fwd_kernel_traits<224, 64, 64, 4, false, false, cutlass::bfloat16_t, Flash_kernel_traits<224, 64, 64, 4, cutlass::bfloat16_t>>, Is_dropout=true, Is_causal=true, Is_even_N=true, Is_even_K=true, Return_softmax=true, Params=Flash_fwd_params]" 
(588): here
            instantiation of "void flash::compute_attn<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &) [with Kernel_traits=Flash_fwd_kernel_traits<224, 64, 64, 4, false, false, cutlass::bfloat16_t, Flash_kernel_traits<224, 64, 64, 4, cutlass::bfloat16_t>>, Is_dropout=true, Is_causal=true, Is_even_N=true, Is_even_K=true, Return_softmax=true, Params=Flash_fwd_params]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(15): here
            instantiation of "void flash_fwd_kernel<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax>(Flash_fwd_params) [with Kernel_traits=Flash_fwd_kernel_traits<224, 64, 64, 4, false, false, cutlass::bfloat16_t, Flash_kernel_traits<224, 64, 64, 4, cutlass::bfloat16_t>>, Is_dropout=true, Is_causal=true, Is_even_N=true, Is_even_K=true, Return_softmax=true]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(34): here
            instantiation of "void run_flash_fwd<Kernel_traits,Is_dropout,Is_causal>(Flash_fwd_params &, cudaStream_t) [with Kernel_traits=Flash_fwd_kernel_traits<224, 64, 64, 4, false, false, cutlass::bfloat16_t, Flash_kernel_traits<224, 64, 64, 4, cutlass::bfloat16_t>>, Is_dropout=true, Is_causal=true]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(208): here
            instantiation of "void run_mha_fwd_hdim224<T>(Flash_fwd_params &, cudaStream_t) [with T=cutlass::bfloat16_t]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_hdim224_bf16_sm80.cu(8): here

/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_kernel.h(140): warning: variable "MMA_M" was declared but never referenced
          detected during:
            instantiation of "void flash::compute_attn_1rowblock<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &, int, int, int) [with Kernel_traits=Flash_fwd_kernel_traits<224, 64, 64, 4, false, false, cutlass::bfloat16_t, Flash_kernel_traits<224, 64, 64, 4, cutlass::bfloat16_t>>, Is_dropout=true, Is_causal=true, Is_even_N=true, Is_even_K=true, Return_softmax=false, Params=Flash_fwd_params]" 
(588): here
            instantiation of "void flash::compute_attn<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &) [with Kernel_traits=Flash_fwd_kernel_traits<224, 64, 64, 4, false, false, cutlass::bfloat16_t, Flash_kernel_traits<224, 64, 64, 4, cutlass::bfloat16_t>>, Is_dropout=true, Is_causal=true, Is_even_N=true, Is_even_K=true, Return_softmax=false, Params=Flash_fwd_params]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(15): here
            instantiation of "void flash_fwd_kernel<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax>(Flash_fwd_params) [with Kernel_traits=Flash_fwd_kernel_traits<224, 64, 64, 4, false, false, cutlass::bfloat16_t, Flash_kernel_traits<224, 64, 64, 4, cutlass::bfloat16_t>>, Is_dropout=true, Is_causal=true, Is_even_N=true, Is_even_K=true, Return_softmax=false]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(34): here
            instantiation of "void run_flash_fwd<Kernel_traits,Is_dropout,Is_causal>(Flash_fwd_params &, cudaStream_t) [with Kernel_traits=Flash_fwd_kernel_traits<224, 64, 64, 4, false, false, cutlass::bfloat16_t, Flash_kernel_traits<224, 64, 64, 4, cutlass::bfloat16_t>>, Is_dropout=true, Is_causal=true]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(208): here
            instantiation of "void run_mha_fwd_hdim224<T>(Flash_fwd_params &, cudaStream_t) [with T=cutlass::bfloat16_t]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_hdim224_bf16_sm80.cu(8): here

/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_kernel.h(275): warning: variable "tQrQ" was declared but never referenced
          detected during:
            instantiation of "void flash::compute_attn_1rowblock<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &, int, int, int) [with Kernel_traits=Flash_fwd_kernel_traits<224, 64, 64, 4, false, false, cutlass::bfloat16_t, Flash_kernel_traits<224, 64, 64, 4, cutlass::bfloat16_t>>, Is_dropout=true, Is_causal=true, Is_even_N=true, Is_even_K=true, Return_softmax=false, Params=Flash_fwd_params]" 
(588): here
            instantiation of "void flash::compute_attn<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &) [with Kernel_traits=Flash_fwd_kernel_traits<224, 64, 64, 4, false, false, cutlass::bfloat16_t, Flash_kernel_traits<224, 64, 64, 4, cutlass::bfloat16_t>>, Is_dropout=true, Is_causal=true, Is_even_N=true, Is_even_K=true, Return_softmax=false, Params=Flash_fwd_params]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(15): here
            instantiation of "void flash_fwd_kernel<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax>(Flash_fwd_params) [with Kernel_traits=Flash_fwd_kernel_traits<224, 64, 64, 4, false, false, cutlass::bfloat16_t, Flash_kernel_traits<224, 64, 64, 4, cutlass::bfloat16_t>>, Is_dropout=true, Is_causal=true, Is_even_N=true, Is_even_K=true, Return_softmax=false]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(34): here
            instantiation of "void run_flash_fwd<Kernel_traits,Is_dropout,Is_causal>(Flash_fwd_params &, cudaStream_t) [with Kernel_traits=Flash_fwd_kernel_traits<224, 64, 64, 4, false, false, cutlass::bfloat16_t, Flash_kernel_traits<224, 64, 64, 4, cutlass::bfloat16_t>>, Is_dropout=true, Is_causal=true]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(208): here
            instantiation of "void run_mha_fwd_hdim224<T>(Flash_fwd_params &, cudaStream_t) [with T=cutlass::bfloat16_t]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_hdim224_bf16_sm80.cu(8): here

/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_kernel.h(140): warning: variable "MMA_M" was declared but never referenced
          detected during:
            instantiation of "void flash::compute_attn_1rowblock<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &, int, int, int) [with Kernel_traits=Flash_fwd_kernel_traits<224, 64, 64, 4, false, false, cutlass::bfloat16_t, Flash_kernel_traits<224, 64, 64, 4, cutlass::bfloat16_t>>, Is_dropout=true, Is_causal=true, Is_even_N=true, Is_even_K=false, Return_softmax=true, Params=Flash_fwd_params]" 
(588): here
            instantiation of "void flash::compute_attn<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &) [with Kernel_traits=Flash_fwd_kernel_traits<224, 64, 64, 4, false, false, cutlass::bfloat16_t, Flash_kernel_traits<224, 64, 64, 4, cutlass::bfloat16_t>>, Is_dropout=true, Is_causal=true, Is_even_N=true, Is_even_K=false, Return_softmax=true, Params=Flash_fwd_params]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(15): here
            instantiation of "void flash_fwd_kernel<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax>(Flash_fwd_params) [with Kernel_traits=Flash_fwd_kernel_traits<224, 64, 64, 4, false, false, cutlass::bfloat16_t, Flash_kernel_traits<224, 64, 64, 4, cutlass::bfloat16_t>>, Is_dropout=true, Is_causal=true, Is_even_N=true, Is_even_K=false, Return_softmax=true]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(34): here
            instantiation of "void run_flash_fwd<Kernel_traits,Is_dropout,Is_causal>(Flash_fwd_params &, cudaStream_t) [with Kernel_traits=Flash_fwd_kernel_traits<224, 64, 64, 4, false, false, cutlass::bfloat16_t, Flash_kernel_traits<224, 64, 64, 4, cutlass::bfloat16_t>>, Is_dropout=true, Is_causal=true]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(208): here
            instantiation of "void run_mha_fwd_hdim224<T>(Flash_fwd_params &, cudaStream_t) [with T=cutlass::bfloat16_t]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_hdim224_bf16_sm80.cu(8): here

/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_kernel.h(275): warning: variable "tQrQ" was declared but never referenced
          detected during:
            instantiation of "void flash::compute_attn_1rowblock<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &, int, int, int) [with Kernel_traits=Flash_fwd_kernel_traits<224, 64, 64, 4, false, false, cutlass::bfloat16_t, Flash_kernel_traits<224, 64, 64, 4, cutlass::bfloat16_t>>, Is_dropout=true, Is_causal=true, Is_even_N=true, Is_even_K=false, Return_softmax=true, Params=Flash_fwd_params]" 
(588): here
            instantiation of "void flash::compute_attn<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &) [with Kernel_traits=Flash_fwd_kernel_traits<224, 64, 64, 4, false, false, cutlass::bfloat16_t, Flash_kernel_traits<224, 64, 64, 4, cutlass::bfloat16_t>>, Is_dropout=true, Is_causal=true, Is_even_N=true, Is_even_K=false, Return_softmax=true, Params=Flash_fwd_params]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(15): here
            instantiation of "void flash_fwd_kernel<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax>(Flash_fwd_params) [with Kernel_traits=Flash_fwd_kernel_traits<224, 64, 64, 4, false, false, cutlass::bfloat16_t, Flash_kernel_traits<224, 64, 64, 4, cutlass::bfloat16_t>>, Is_dropout=true, Is_causal=true, Is_even_N=true, Is_even_K=false, Return_softmax=true]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(34): here
            instantiation of "void run_flash_fwd<Kernel_traits,Is_dropout,Is_causal>(Flash_fwd_params &, cudaStream_t) [with Kernel_traits=Flash_fwd_kernel_traits<224, 64, 64, 4, false, false, cutlass::bfloat16_t, Flash_kernel_traits<224, 64, 64, 4, cutlass::bfloat16_t>>, Is_dropout=true, Is_causal=true]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(208): here
            instantiation of "void run_mha_fwd_hdim224<T>(Flash_fwd_params &, cudaStream_t) [with T=cutlass::bfloat16_t]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_hdim224_bf16_sm80.cu(8): here

/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_kernel.h(140): warning: variable "MMA_M" was declared but never referenced
          detected during:
            instantiation of "void flash::compute_attn_1rowblock<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &, int, int, int) [with Kernel_traits=Flash_fwd_kernel_traits<224, 64, 64, 4, false, false, cutlass::bfloat16_t, Flash_kernel_traits<224, 64, 64, 4, cutlass::bfloat16_t>>, Is_dropout=true, Is_causal=true, Is_even_N=true, Is_even_K=false, Return_softmax=false, Params=Flash_fwd_params]" 
(588): here
            instantiation of "void flash::compute_attn<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &) [with Kernel_traits=Flash_fwd_kernel_traits<224, 64, 64, 4, false, false, cutlass::bfloat16_t, Flash_kernel_traits<224, 64, 64, 4, cutlass::bfloat16_t>>, Is_dropout=true, Is_causal=true, Is_even_N=true, Is_even_K=false, Return_softmax=false, Params=Flash_fwd_params]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(15): here
            instantiation of "void flash_fwd_kernel<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax>(Flash_fwd_params) [with Kernel_traits=Flash_fwd_kernel_traits<224, 64, 64, 4, false, false, cutlass::bfloat16_t, Flash_kernel_traits<224, 64, 64, 4, cutlass::bfloat16_t>>, Is_dropout=true, Is_causal=true, Is_even_N=true, Is_even_K=false, Return_softmax=false]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(34): here
            instantiation of "void run_flash_fwd<Kernel_traits,Is_dropout,Is_causal>(Flash_fwd_params &, cudaStream_t) [with Kernel_traits=Flash_fwd_kernel_traits<224, 64, 64, 4, false, false, cutlass::bfloat16_t, Flash_kernel_traits<224, 64, 64, 4, cutlass::bfloat16_t>>, Is_dropout=true, Is_causal=true]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(208): here
            instantiation of "void run_mha_fwd_hdim224<T>(Flash_fwd_params &, cudaStream_t) [with T=cutlass::bfloat16_t]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_hdim224_bf16_sm80.cu(8): here

/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_kernel.h(275): warning: variable "tQrQ" was declared but never referenced
          detected during:
            instantiation of "void flash::compute_attn_1rowblock<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &, int, int, int) [with Kernel_traits=Flash_fwd_kernel_traits<224, 64, 64, 4, false, false, cutlass::bfloat16_t, Flash_kernel_traits<224, 64, 64, 4, cutlass::bfloat16_t>>, Is_dropout=true, Is_causal=true, Is_even_N=true, Is_even_K=false, Return_softmax=false, Params=Flash_fwd_params]" 
(588): here
            instantiation of "void flash::compute_attn<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &) [with Kernel_traits=Flash_fwd_kernel_traits<224, 64, 64, 4, false, false, cutlass::bfloat16_t, Flash_kernel_traits<224, 64, 64, 4, cutlass::bfloat16_t>>, Is_dropout=true, Is_causal=true, Is_even_N=true, Is_even_K=false, Return_softmax=false, Params=Flash_fwd_params]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(15): here
            instantiation of "void flash_fwd_kernel<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax>(Flash_fwd_params) [with Kernel_traits=Flash_fwd_kernel_traits<224, 64, 64, 4, false, false, cutlass::bfloat16_t, Flash_kernel_traits<224, 64, 64, 4, cutlass::bfloat16_t>>, Is_dropout=true, Is_causal=true, Is_even_N=true, Is_even_K=false, Return_softmax=false]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(34): here
            instantiation of "void run_flash_fwd<Kernel_traits,Is_dropout,Is_causal>(Flash_fwd_params &, cudaStream_t) [with Kernel_traits=Flash_fwd_kernel_traits<224, 64, 64, 4, false, false, cutlass::bfloat16_t, Flash_kernel_traits<224, 64, 64, 4, cutlass::bfloat16_t>>, Is_dropout=true, Is_causal=true]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(208): here
            instantiation of "void run_mha_fwd_hdim224<T>(Flash_fwd_params &, cudaStream_t) [with T=cutlass::bfloat16_t]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_hdim224_bf16_sm80.cu(8): here

/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_kernel.h(140): warning: variable "MMA_M" was declared but never referenced
          detected during:
            instantiation of "void flash::compute_attn_1rowblock<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &, int, int, int) [with Kernel_traits=Flash_fwd_kernel_traits<224, 64, 64, 4, false, false, cutlass::bfloat16_t, Flash_kernel_traits<224, 64, 64, 4, cutlass::bfloat16_t>>, Is_dropout=true, Is_causal=true, Is_even_N=false, Is_even_K=true, Return_softmax=true, Params=Flash_fwd_params]" 
(588): here
            instantiation of "void flash::compute_attn<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &) [with Kernel_traits=Flash_fwd_kernel_traits<224, 64, 64, 4, false, false, cutlass::bfloat16_t, Flash_kernel_traits<224, 64, 64, 4, cutlass::bfloat16_t>>, Is_dropout=true, Is_causal=true, Is_even_N=false, Is_even_K=true, Return_softmax=true, Params=Flash_fwd_params]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(15): here
            instantiation of "void flash_fwd_kernel<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax>(Flash_fwd_params) [with Kernel_traits=Flash_fwd_kernel_traits<224, 64, 64, 4, false, false, cutlass::bfloat16_t, Flash_kernel_traits<224, 64, 64, 4, cutlass::bfloat16_t>>, Is_dropout=true, Is_causal=true, Is_even_N=false, Is_even_K=true, Return_softmax=true]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(34): here
            instantiation of "void run_flash_fwd<Kernel_traits,Is_dropout,Is_causal>(Flash_fwd_params &, cudaStream_t) [with Kernel_traits=Flash_fwd_kernel_traits<224, 64, 64, 4, false, false, cutlass::bfloat16_t, Flash_kernel_traits<224, 64, 64, 4, cutlass::bfloat16_t>>, Is_dropout=true, Is_causal=true]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(208): here
            instantiation of "void run_mha_fwd_hdim224<T>(Flash_fwd_params &, cudaStream_t) [with T=cutlass::bfloat16_t]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_hdim224_bf16_sm80.cu(8): here

/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_kernel.h(275): warning: variable "tQrQ" was declared but never referenced
          detected during:
            instantiation of "void flash::compute_attn_1rowblock<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &, int, int, int) [with Kernel_traits=Flash_fwd_kernel_traits<224, 64, 64, 4, false, false, cutlass::bfloat16_t, Flash_kernel_traits<224, 64, 64, 4, cutlass::bfloat16_t>>, Is_dropout=true, Is_causal=true, Is_even_N=false, Is_even_K=true, Return_softmax=true, Params=Flash_fwd_params]" 
(588): here
            instantiation of "void flash::compute_attn<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &) [with Kernel_traits=Flash_fwd_kernel_traits<224, 64, 64, 4, false, false, cutlass::bfloat16_t, Flash_kernel_traits<224, 64, 64, 4, cutlass::bfloat16_t>>, Is_dropout=true, Is_causal=true, Is_even_N=false, Is_even_K=true, Return_softmax=true, Params=Flash_fwd_params]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(15): here
            instantiation of "void flash_fwd_kernel<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax>(Flash_fwd_params) [with Kernel_traits=Flash_fwd_kernel_traits<224, 64, 64, 4, false, false, cutlass::bfloat16_t, Flash_kernel_traits<224, 64, 64, 4, cutlass::bfloat16_t>>, Is_dropout=true, Is_causal=true, Is_even_N=false, Is_even_K=true, Return_softmax=true]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(34): here
            instantiation of "void run_flash_fwd<Kernel_traits,Is_dropout,Is_causal>(Flash_fwd_params &, cudaStream_t) [with Kernel_traits=Flash_fwd_kernel_traits<224, 64, 64, 4, false, false, cutlass::bfloat16_t, Flash_kernel_traits<224, 64, 64, 4, cutlass::bfloat16_t>>, Is_dropout=true, Is_causal=true]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(208): here
            instantiation of "void run_mha_fwd_hdim224<T>(Flash_fwd_params &, cudaStream_t) [with T=cutlass::bfloat16_t]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_hdim224_bf16_sm80.cu(8): here

/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_kernel.h(140): warning: variable "MMA_M" was declared but never referenced
          detected during:
            instantiation of "void flash::compute_attn_1rowblock<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &, int, int, int) [with Kernel_traits=Flash_fwd_kernel_traits<224, 64, 64, 4, false, false, cutlass::bfloat16_t, Flash_kernel_traits<224, 64, 64, 4, cutlass::bfloat16_t>>, Is_dropout=true, Is_causal=true, Is_even_N=false, Is_even_K=true, Return_softmax=false, Params=Flash_fwd_params]" 
(588): here
            instantiation of "void flash::compute_attn<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &) [with Kernel_traits=Flash_fwd_kernel_traits<224, 64, 64, 4, false, false, cutlass::bfloat16_t, Flash_kernel_traits<224, 64, 64, 4, cutlass::bfloat16_t>>, Is_dropout=true, Is_causal=true, Is_even_N=false, Is_even_K=true, Return_softmax=false, Params=Flash_fwd_params]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(15): here
            instantiation of "void flash_fwd_kernel<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax>(Flash_fwd_params) [with Kernel_traits=Flash_fwd_kernel_traits<224, 64, 64, 4, false, false, cutlass::bfloat16_t, Flash_kernel_traits<224, 64, 64, 4, cutlass::bfloat16_t>>, Is_dropout=true, Is_causal=true, Is_even_N=false, Is_even_K=true, Return_softmax=false]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(34): here
            instantiation of "void run_flash_fwd<Kernel_traits,Is_dropout,Is_causal>(Flash_fwd_params &, cudaStream_t) [with Kernel_traits=Flash_fwd_kernel_traits<224, 64, 64, 4, false, false, cutlass::bfloat16_t, Flash_kernel_traits<224, 64, 64, 4, cutlass::bfloat16_t>>, Is_dropout=true, Is_causal=true]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(208): here
            instantiation of "void run_mha_fwd_hdim224<T>(Flash_fwd_params &, cudaStream_t) [with T=cutlass::bfloat16_t]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_hdim224_bf16_sm80.cu(8): here

/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_kernel.h(275): warning: variable "tQrQ" was declared but never referenced
          detected during:
            instantiation of "void flash::compute_attn_1rowblock<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &, int, int, int) [with Kernel_traits=Flash_fwd_kernel_traits<224, 64, 64, 4, false, false, cutlass::bfloat16_t, Flash_kernel_traits<224, 64, 64, 4, cutlass::bfloat16_t>>, Is_dropout=true, Is_causal=true, Is_even_N=false, Is_even_K=true, Return_softmax=false, Params=Flash_fwd_params]" 
(588): here
            instantiation of "void flash::compute_attn<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &) [with Kernel_traits=Flash_fwd_kernel_traits<224, 64, 64, 4, false, false, cutlass::bfloat16_t, Flash_kernel_traits<224, 64, 64, 4, cutlass::bfloat16_t>>, Is_dropout=true, Is_causal=true, Is_even_N=false, Is_even_K=true, Return_softmax=false, Params=Flash_fwd_params]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(15): here
            instantiation of "void flash_fwd_kernel<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax>(Flash_fwd_params) [with Kernel_traits=Flash_fwd_kernel_traits<224, 64, 64, 4, false, false, cutlass::bfloat16_t, Flash_kernel_traits<224, 64, 64, 4, cutlass::bfloat16_t>>, Is_dropout=true, Is_causal=true, Is_even_N=false, Is_even_K=true, Return_softmax=false]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(34): here
            instantiation of "void run_flash_fwd<Kernel_traits,Is_dropout,Is_causal>(Flash_fwd_params &, cudaStream_t) [with Kernel_traits=Flash_fwd_kernel_traits<224, 64, 64, 4, false, false, cutlass::bfloat16_t, Flash_kernel_traits<224, 64, 64, 4, cutlass::bfloat16_t>>, Is_dropout=true, Is_causal=true]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(208): here
            instantiation of "void run_mha_fwd_hdim224<T>(Flash_fwd_params &, cudaStream_t) [with T=cutlass::bfloat16_t]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_hdim224_bf16_sm80.cu(8): here

/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_kernel.h(140): warning: variable "MMA_M" was declared but never referenced
          detected during:
            instantiation of "void flash::compute_attn_1rowblock<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &, int, int, int) [with Kernel_traits=Flash_fwd_kernel_traits<224, 64, 64, 4, false, false, cutlass::bfloat16_t, Flash_kernel_traits<224, 64, 64, 4, cutlass::bfloat16_t>>, Is_dropout=true, Is_causal=true, Is_even_N=false, Is_even_K=false, Return_softmax=true, Params=Flash_fwd_params]" 
(588): here
            instantiation of "void flash::compute_attn<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &) [with Kernel_traits=Flash_fwd_kernel_traits<224, 64, 64, 4, false, false, cutlass::bfloat16_t, Flash_kernel_traits<224, 64, 64, 4, cutlass::bfloat16_t>>, Is_dropout=true, Is_causal=true, Is_even_N=false, Is_even_K=false, Return_softmax=true, Params=Flash_fwd_params]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(15): here
            instantiation of "void flash_fwd_kernel<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax>(Flash_fwd_params) [with Kernel_traits=Flash_fwd_kernel_traits<224, 64, 64, 4, false, false, cutlass::bfloat16_t, Flash_kernel_traits<224, 64, 64, 4, cutlass::bfloat16_t>>, Is_dropout=true, Is_causal=true, Is_even_N=false, Is_even_K=false, Return_softmax=true]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(34): here
            instantiation of "void run_flash_fwd<Kernel_traits,Is_dropout,Is_causal>(Flash_fwd_params &, cudaStream_t) [with Kernel_traits=Flash_fwd_kernel_traits<224, 64, 64, 4, false, false, cutlass::bfloat16_t, Flash_kernel_traits<224, 64, 64, 4, cutlass::bfloat16_t>>, Is_dropout=true, Is_causal=true]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(208): here
            instantiation of "void run_mha_fwd_hdim224<T>(Flash_fwd_params &, cudaStream_t) [with T=cutlass::bfloat16_t]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_hdim224_bf16_sm80.cu(8): here

/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_kernel.h(275): warning: variable "tQrQ" was declared but never referenced
          detected during:
            instantiation of "void flash::compute_attn_1rowblock<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &, int, int, int) [with Kernel_traits=Flash_fwd_kernel_traits<224, 64, 64, 4, false, false, cutlass::bfloat16_t, Flash_kernel_traits<224, 64, 64, 4, cutlass::bfloat16_t>>, Is_dropout=true, Is_causal=true, Is_even_N=false, Is_even_K=false, Return_softmax=true, Params=Flash_fwd_params]" 
(588): here
            instantiation of "void flash::compute_attn<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &) [with Kernel_traits=Flash_fwd_kernel_traits<224, 64, 64, 4, false, false, cutlass::bfloat16_t, Flash_kernel_traits<224, 64, 64, 4, cutlass::bfloat16_t>>, Is_dropout=true, Is_causal=true, Is_even_N=false, Is_even_K=false, Return_softmax=true, Params=Flash_fwd_params]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(15): here
            instantiation of "void flash_fwd_kernel<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax>(Flash_fwd_params) [with Kernel_traits=Flash_fwd_kernel_traits<224, 64, 64, 4, false, false, cutlass::bfloat16_t, Flash_kernel_traits<224, 64, 64, 4, cutlass::bfloat16_t>>, Is_dropout=true, Is_causal=true, Is_even_N=false, Is_even_K=false, Return_softmax=true]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(34): here
            instantiation of "void run_flash_fwd<Kernel_traits,Is_dropout,Is_causal>(Flash_fwd_params &, cudaStream_t) [with Kernel_traits=Flash_fwd_kernel_traits<224, 64, 64, 4, false, false, cutlass::bfloat16_t, Flash_kernel_traits<224, 64, 64, 4, cutlass::bfloat16_t>>, Is_dropout=true, Is_causal=true]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(208): here
            instantiation of "void run_mha_fwd_hdim224<T>(Flash_fwd_params &, cudaStream_t) [with T=cutlass::bfloat16_t]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_hdim224_bf16_sm80.cu(8): here

/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_kernel.h(140): warning: variable "MMA_M" was declared but never referenced
          detected during:
            instantiation of "void flash::compute_attn_1rowblock<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &, int, int, int) [with Kernel_traits=Flash_fwd_kernel_traits<224, 64, 64, 4, false, false, cutlass::bfloat16_t, Flash_kernel_traits<224, 64, 64, 4, cutlass::bfloat16_t>>, Is_dropout=true, Is_causal=true, Is_even_N=false, Is_even_K=false, Return_softmax=false, Params=Flash_fwd_params]" 
(588): here
            instantiation of "void flash::compute_attn<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &) [with Kernel_traits=Flash_fwd_kernel_traits<224, 64, 64, 4, false, false, cutlass::bfloat16_t, Flash_kernel_traits<224, 64, 64, 4, cutlass::bfloat16_t>>, Is_dropout=true, Is_causal=true, Is_even_N=false, Is_even_K=false, Return_softmax=false, Params=Flash_fwd_params]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(15): here
            instantiation of "void flash_fwd_kernel<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax>(Flash_fwd_params) [with Kernel_traits=Flash_fwd_kernel_traits<224, 64, 64, 4, false, false, cutlass::bfloat16_t, Flash_kernel_traits<224, 64, 64, 4, cutlass::bfloat16_t>>, Is_dropout=true, Is_causal=true, Is_even_N=false, Is_even_K=false, Return_softmax=false]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(34): here
            instantiation of "void run_flash_fwd<Kernel_traits,Is_dropout,Is_causal>(Flash_fwd_params &, cudaStream_t) [with Kernel_traits=Flash_fwd_kernel_traits<224, 64, 64, 4, false, false, cutlass::bfloat16_t, Flash_kernel_traits<224, 64, 64, 4, cutlass::bfloat16_t>>, Is_dropout=true, Is_causal=true]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(208): here
            instantiation of "void run_mha_fwd_hdim224<T>(Flash_fwd_params &, cudaStream_t) [with T=cutlass::bfloat16_t]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_hdim224_bf16_sm80.cu(8): here

/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_kernel.h(275): warning: variable "tQrQ" was declared but never referenced
          detected during:
            instantiation of "void flash::compute_attn_1rowblock<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &, int, int, int) [with Kernel_traits=Flash_fwd_kernel_traits<224, 64, 64, 4, false, false, cutlass::bfloat16_t, Flash_kernel_traits<224, 64, 64, 4, cutlass::bfloat16_t>>, Is_dropout=true, Is_causal=true, Is_even_N=false, Is_even_K=false, Return_softmax=false, Params=Flash_fwd_params]" 
(588): here
            instantiation of "void flash::compute_attn<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &) [with Kernel_traits=Flash_fwd_kernel_traits<224, 64, 64, 4, false, false, cutlass::bfloat16_t, Flash_kernel_traits<224, 64, 64, 4, cutlass::bfloat16_t>>, Is_dropout=true, Is_causal=true, Is_even_N=false, Is_even_K=false, Return_softmax=false, Params=Flash_fwd_params]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(15): here
            instantiation of "void flash_fwd_kernel<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax>(Flash_fwd_params) [with Kernel_traits=Flash_fwd_kernel_traits<224, 64, 64, 4, false, false, cutlass::bfloat16_t, Flash_kernel_traits<224, 64, 64, 4, cutlass::bfloat16_t>>, Is_dropout=true, Is_causal=true, Is_even_N=false, Is_even_K=false, Return_softmax=false]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(34): here
            instantiation of "void run_flash_fwd<Kernel_traits,Is_dropout,Is_causal>(Flash_fwd_params &, cudaStream_t) [with Kernel_traits=Flash_fwd_kernel_traits<224, 64, 64, 4, false, false, cutlass::bfloat16_t, Flash_kernel_traits<224, 64, 64, 4, cutlass::bfloat16_t>>, Is_dropout=true, Is_causal=true]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(208): here
            instantiation of "void run_mha_fwd_hdim224<T>(Flash_fwd_params &, cudaStream_t) [with T=cutlass::bfloat16_t]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_hdim224_bf16_sm80.cu(8): here

/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_kernel.h(140): warning: variable "MMA_M" was declared but never referenced
          detected during:
            instantiation of "void flash::compute_attn_1rowblock<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &, int, int, int) [with Kernel_traits=Flash_fwd_kernel_traits<224, 128, 64, 8, false, false, cutlass::bfloat16_t, Flash_kernel_traits<224, 128, 64, 8, cutlass::bfloat16_t>>, Is_dropout=true, Is_causal=false, Is_even_N=true, Is_even_K=true, Return_softmax=true, Params=Flash_fwd_params]" 
(588): here
            instantiation of "void flash::compute_attn<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &) [with Kernel_traits=Flash_fwd_kernel_traits<224, 128, 64, 8, false, false, cutlass::bfloat16_t, Flash_kernel_traits<224, 128, 64, 8, cutlass::bfloat16_t>>, Is_dropout=true, Is_causal=false, Is_even_N=true, Is_even_K=true, Return_softmax=true, Params=Flash_fwd_params]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(15): here
            instantiation of "void flash_fwd_kernel<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax>(Flash_fwd_params) [with Kernel_traits=Flash_fwd_kernel_traits<224, 128, 64, 8, false, false, cutlass::bfloat16_t, Flash_kernel_traits<224, 128, 64, 8, cutlass::bfloat16_t>>, Is_dropout=true, Is_causal=false, Is_even_N=true, Is_even_K=true, Return_softmax=true]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(34): here
            instantiation of "void run_flash_fwd<Kernel_traits,Is_dropout,Is_causal>(Flash_fwd_params &, cudaStream_t) [with Kernel_traits=Flash_fwd_kernel_traits<224, 128, 64, 8, false, false, cutlass::bfloat16_t, Flash_kernel_traits<224, 128, 64, 8, cutlass::bfloat16_t>>, Is_dropout=true, Is_causal=false]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(208): here
            instantiation of "void run_mha_fwd_hdim224<T>(Flash_fwd_params &, cudaStream_t) [with T=cutlass::bfloat16_t]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_hdim224_bf16_sm80.cu(8): here

/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_kernel.h(275): warning: variable "tQrQ" was declared but never referenced
          detected during:
            instantiation of "void flash::compute_attn_1rowblock<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &, int, int, int) [with Kernel_traits=Flash_fwd_kernel_traits<224, 128, 64, 8, false, false, cutlass::bfloat16_t, Flash_kernel_traits<224, 128, 64, 8, cutlass::bfloat16_t>>, Is_dropout=true, Is_causal=false, Is_even_N=true, Is_even_K=true, Return_softmax=true, Params=Flash_fwd_params]" 
(588): here
            instantiation of "void flash::compute_attn<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &) [with Kernel_traits=Flash_fwd_kernel_traits<224, 128, 64, 8, false, false, cutlass::bfloat16_t, Flash_kernel_traits<224, 128, 64, 8, cutlass::bfloat16_t>>, Is_dropout=true, Is_causal=false, Is_even_N=true, Is_even_K=true, Return_softmax=true, Params=Flash_fwd_params]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(15): here
            instantiation of "void flash_fwd_kernel<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax>(Flash_fwd_params) [with Kernel_traits=Flash_fwd_kernel_traits<224, 128, 64, 8, false, false, cutlass::bfloat16_t, Flash_kernel_traits<224, 128, 64, 8, cutlass::bfloat16_t>>, Is_dropout=true, Is_causal=false, Is_even_N=true, Is_even_K=true, Return_softmax=true]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(34): here
            instantiation of "void run_flash_fwd<Kernel_traits,Is_dropout,Is_causal>(Flash_fwd_params &, cudaStream_t) [with Kernel_traits=Flash_fwd_kernel_traits<224, 128, 64, 8, false, false, cutlass::bfloat16_t, Flash_kernel_traits<224, 128, 64, 8, cutlass::bfloat16_t>>, Is_dropout=true, Is_causal=false]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(208): here
            instantiation of "void run_mha_fwd_hdim224<T>(Flash_fwd_params &, cudaStream_t) [with T=cutlass::bfloat16_t]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_hdim224_bf16_sm80.cu(8): here

/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_kernel.h(140): warning: variable "MMA_M" was declared but never referenced
          detected during:
            instantiation of "void flash::compute_attn_1rowblock<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &, int, int, int) [with Kernel_traits=Flash_fwd_kernel_traits<224, 128, 64, 8, false, false, cutlass::bfloat16_t, Flash_kernel_traits<224, 128, 64, 8, cutlass::bfloat16_t>>, Is_dropout=true, Is_causal=false, Is_even_N=true, Is_even_K=true, Return_softmax=false, Params=Flash_fwd_params]" 
(588): here
            instantiation of "void flash::compute_attn<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &) [with Kernel_traits=Flash_fwd_kernel_traits<224, 128, 64, 8, false, false, cutlass::bfloat16_t, Flash_kernel_traits<224, 128, 64, 8, cutlass::bfloat16_t>>, Is_dropout=true, Is_causal=false, Is_even_N=true, Is_even_K=true, Return_softmax=false, Params=Flash_fwd_params]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(15): here
            instantiation of "void flash_fwd_kernel<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax>(Flash_fwd_params) [with Kernel_traits=Flash_fwd_kernel_traits<224, 128, 64, 8, false, false, cutlass::bfloat16_t, Flash_kernel_traits<224, 128, 64, 8, cutlass::bfloat16_t>>, Is_dropout=true, Is_causal=false, Is_even_N=true, Is_even_K=true, Return_softmax=false]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(34): here
            instantiation of "void run_flash_fwd<Kernel_traits,Is_dropout,Is_causal>(Flash_fwd_params &, cudaStream_t) [with Kernel_traits=Flash_fwd_kernel_traits<224, 128, 64, 8, false, false, cutlass::bfloat16_t, Flash_kernel_traits<224, 128, 64, 8, cutlass::bfloat16_t>>, Is_dropout=true, Is_causal=false]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(208): here
            instantiation of "void run_mha_fwd_hdim224<T>(Flash_fwd_params &, cudaStream_t) [with T=cutlass::bfloat16_t]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_hdim224_bf16_sm80.cu(8): here

/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_kernel.h(275): warning: variable "tQrQ" was declared but never referenced
          detected during:
            instantiation of "void flash::compute_attn_1rowblock<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &, int, int, int) [with Kernel_traits=Flash_fwd_kernel_traits<224, 128, 64, 8, false, false, cutlass::bfloat16_t, Flash_kernel_traits<224, 128, 64, 8, cutlass::bfloat16_t>>, Is_dropout=true, Is_causal=false, Is_even_N=true, Is_even_K=true, Return_softmax=false, Params=Flash_fwd_params]" 
(588): here
            instantiation of "void flash::compute_attn<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &) [with Kernel_traits=Flash_fwd_kernel_traits<224, 128, 64, 8, false, false, cutlass::bfloat16_t, Flash_kernel_traits<224, 128, 64, 8, cutlass::bfloat16_t>>, Is_dropout=true, Is_causal=false, Is_even_N=true, Is_even_K=true, Return_softmax=false, Params=Flash_fwd_params]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(15): here
            instantiation of "void flash_fwd_kernel<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax>(Flash_fwd_params) [with Kernel_traits=Flash_fwd_kernel_traits<224, 128, 64, 8, false, false, cutlass::bfloat16_t, Flash_kernel_traits<224, 128, 64, 8, cutlass::bfloat16_t>>, Is_dropout=true, Is_causal=false, Is_even_N=true, Is_even_K=true, Return_softmax=false]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(34): here
            instantiation of "void run_flash_fwd<Kernel_traits,Is_dropout,Is_causal>(Flash_fwd_params &, cudaStream_t) [with Kernel_traits=Flash_fwd_kernel_traits<224, 128, 64, 8, false, false, cutlass::bfloat16_t, Flash_kernel_traits<224, 128, 64, 8, cutlass::bfloat16_t>>, Is_dropout=true, Is_causal=false]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(208): here
            instantiation of "void run_mha_fwd_hdim224<T>(Flash_fwd_params &, cudaStream_t) [with T=cutlass::bfloat16_t]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_hdim224_bf16_sm80.cu(8): here

/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_kernel.h(140): warning: variable "MMA_M" was declared but never referenced
          detected during:
            instantiation of "void flash::compute_attn_1rowblock<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &, int, int, int) [with Kernel_traits=Flash_fwd_kernel_traits<224, 128, 64, 8, false, false, cutlass::bfloat16_t, Flash_kernel_traits<224, 128, 64, 8, cutlass::bfloat16_t>>, Is_dropout=true, Is_causal=false, Is_even_N=true, Is_even_K=false, Return_softmax=true, Params=Flash_fwd_params]" 
(588): here
            instantiation of "void flash::compute_attn<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &) [with Kernel_traits=Flash_fwd_kernel_traits<224, 128, 64, 8, false, false, cutlass::bfloat16_t, Flash_kernel_traits<224, 128, 64, 8, cutlass::bfloat16_t>>, Is_dropout=true, Is_causal=false, Is_even_N=true, Is_even_K=false, Return_softmax=true, Params=Flash_fwd_params]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(15): here
            instantiation of "void flash_fwd_kernel<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax>(Flash_fwd_params) [with Kernel_traits=Flash_fwd_kernel_traits<224, 128, 64, 8, false, false, cutlass::bfloat16_t, Flash_kernel_traits<224, 128, 64, 8, cutlass::bfloat16_t>>, Is_dropout=true, Is_causal=false, Is_even_N=true, Is_even_K=false, Return_softmax=true]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(34): here
            instantiation of "void run_flash_fwd<Kernel_traits,Is_dropout,Is_causal>(Flash_fwd_params &, cudaStream_t) [with Kernel_traits=Flash_fwd_kernel_traits<224, 128, 64, 8, false, false, cutlass::bfloat16_t, Flash_kernel_traits<224, 128, 64, 8, cutlass::bfloat16_t>>, Is_dropout=true, Is_causal=false]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(208): here
            instantiation of "void run_mha_fwd_hdim224<T>(Flash_fwd_params &, cudaStream_t) [with T=cutlass::bfloat16_t]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_hdim224_bf16_sm80.cu(8): here

/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_kernel.h(275): warning: variable "tQrQ" was declared but never referenced
          detected during:
            instantiation of "void flash::compute_attn_1rowblock<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &, int, int, int) [with Kernel_traits=Flash_fwd_kernel_traits<224, 128, 64, 8, false, false, cutlass::bfloat16_t, Flash_kernel_traits<224, 128, 64, 8, cutlass::bfloat16_t>>, Is_dropout=true, Is_causal=false, Is_even_N=true, Is_even_K=false, Return_softmax=true, Params=Flash_fwd_params]" 
(588): here
            instantiation of "void flash::compute_attn<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &) [with Kernel_traits=Flash_fwd_kernel_traits<224, 128, 64, 8, false, false, cutlass::bfloat16_t, Flash_kernel_traits<224, 128, 64, 8, cutlass::bfloat16_t>>, Is_dropout=true, Is_causal=false, Is_even_N=true, Is_even_K=false, Return_softmax=true, Params=Flash_fwd_params]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(15): here
            instantiation of "void flash_fwd_kernel<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax>(Flash_fwd_params) [with Kernel_traits=Flash_fwd_kernel_traits<224, 128, 64, 8, false, false, cutlass::bfloat16_t, Flash_kernel_traits<224, 128, 64, 8, cutlass::bfloat16_t>>, Is_dropout=true, Is_causal=false, Is_even_N=true, Is_even_K=false, Return_softmax=true]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(34): here
            instantiation of "void run_flash_fwd<Kernel_traits,Is_dropout,Is_causal>(Flash_fwd_params &, cudaStream_t) [with Kernel_traits=Flash_fwd_kernel_traits<224, 128, 64, 8, false, false, cutlass::bfloat16_t, Flash_kernel_traits<224, 128, 64, 8, cutlass::bfloat16_t>>, Is_dropout=true, Is_causal=false]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(208): here
            instantiation of "void run_mha_fwd_hdim224<T>(Flash_fwd_params &, cudaStream_t) [with T=cutlass::bfloat16_t]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_hdim224_bf16_sm80.cu(8): here

/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_kernel.h(140): warning: variable "MMA_M" was declared but never referenced
          detected during:
            instantiation of "void flash::compute_attn_1rowblock<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &, int, int, int) [with Kernel_traits=Flash_fwd_kernel_traits<224, 128, 64, 8, false, false, cutlass::bfloat16_t, Flash_kernel_traits<224, 128, 64, 8, cutlass::bfloat16_t>>, Is_dropout=true, Is_causal=false, Is_even_N=true, Is_even_K=false, Return_softmax=false, Params=Flash_fwd_params]" 
(588): here
            instantiation of "void flash::compute_attn<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &) [with Kernel_traits=Flash_fwd_kernel_traits<224, 128, 64, 8, false, false, cutlass::bfloat16_t, Flash_kernel_traits<224, 128, 64, 8, cutlass::bfloat16_t>>, Is_dropout=true, Is_causal=false, Is_even_N=true, Is_even_K=false, Return_softmax=false, Params=Flash_fwd_params]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(15): here
            instantiation of "void flash_fwd_kernel<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax>(Flash_fwd_params) [with Kernel_traits=Flash_fwd_kernel_traits<224, 128, 64, 8, false, false, cutlass::bfloat16_t, Flash_kernel_traits<224, 128, 64, 8, cutlass::bfloat16_t>>, Is_dropout=true, Is_causal=false, Is_even_N=true, Is_even_K=false, Return_softmax=false]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(34): here
            instantiation of "void run_flash_fwd<Kernel_traits,Is_dropout,Is_causal>(Flash_fwd_params &, cudaStream_t) [with Kernel_traits=Flash_fwd_kernel_traits<224, 128, 64, 8, false, false, cutlass::bfloat16_t, Flash_kernel_traits<224, 128, 64, 8, cutlass::bfloat16_t>>, Is_dropout=true, Is_causal=false]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(208): here
            instantiation of "void run_mha_fwd_hdim224<T>(Flash_fwd_params &, cudaStream_t) [with T=cutlass::bfloat16_t]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_hdim224_bf16_sm80.cu(8): here

/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_kernel.h(275): warning: variable "tQrQ" was declared but never referenced
          detected during:
            instantiation of "void flash::compute_attn_1rowblock<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &, int, int, int) [with Kernel_traits=Flash_fwd_kernel_traits<224, 128, 64, 8, false, false, cutlass::bfloat16_t, Flash_kernel_traits<224, 128, 64, 8, cutlass::bfloat16_t>>, Is_dropout=true, Is_causal=false, Is_even_N=true, Is_even_K=false, Return_softmax=false, Params=Flash_fwd_params]" 
(588): here
            instantiation of "void flash::compute_attn<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &) [with Kernel_traits=Flash_fwd_kernel_traits<224, 128, 64, 8, false, false, cutlass::bfloat16_t, Flash_kernel_traits<224, 128, 64, 8, cutlass::bfloat16_t>>, Is_dropout=true, Is_causal=false, Is_even_N=true, Is_even_K=false, Return_softmax=false, Params=Flash_fwd_params]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(15): here
            instantiation of "void flash_fwd_kernel<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax>(Flash_fwd_params) [with Kernel_traits=Flash_fwd_kernel_traits<224, 128, 64, 8, false, false, cutlass::bfloat16_t, Flash_kernel_traits<224, 128, 64, 8, cutlass::bfloat16_t>>, Is_dropout=true, Is_causal=false, Is_even_N=true, Is_even_K=false, Return_softmax=false]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(34): here
            instantiation of "void run_flash_fwd<Kernel_traits,Is_dropout,Is_causal>(Flash_fwd_params &, cudaStream_t) [with Kernel_traits=Flash_fwd_kernel_traits<224, 128, 64, 8, false, false, cutlass::bfloat16_t, Flash_kernel_traits<224, 128, 64, 8, cutlass::bfloat16_t>>, Is_dropout=true, Is_causal=false]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(208): here
            instantiation of "void run_mha_fwd_hdim224<T>(Flash_fwd_params &, cudaStream_t) [with T=cutlass::bfloat16_t]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_hdim224_bf16_sm80.cu(8): here

/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_kernel.h(140): warning: variable "MMA_M" was declared but never referenced
          detected during:
            instantiation of "void flash::compute_attn_1rowblock<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &, int, int, int) [with Kernel_traits=Flash_fwd_kernel_traits<224, 128, 64, 8, false, false, cutlass::bfloat16_t, Flash_kernel_traits<224, 128, 64, 8, cutlass::bfloat16_t>>, Is_dropout=true, Is_causal=false, Is_even_N=false, Is_even_K=true, Return_softmax=true, Params=Flash_fwd_params]" 
(588): here
            instantiation of "void flash::compute_attn<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &) [with Kernel_traits=Flash_fwd_kernel_traits<224, 128, 64, 8, false, false, cutlass::bfloat16_t, Flash_kernel_traits<224, 128, 64, 8, cutlass::bfloat16_t>>, Is_dropout=true, Is_causal=false, Is_even_N=false, Is_even_K=true, Return_softmax=true, Params=Flash_fwd_params]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(15): here
            instantiation of "void flash_fwd_kernel<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax>(Flash_fwd_params) [with Kernel_traits=Flash_fwd_kernel_traits<224, 128, 64, 8, false, false, cutlass::bfloat16_t, Flash_kernel_traits<224, 128, 64, 8, cutlass::bfloat16_t>>, Is_dropout=true, Is_causal=false, Is_even_N=false, Is_even_K=true, Return_softmax=true]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(34): here
            instantiation of "void run_flash_fwd<Kernel_traits,Is_dropout,Is_causal>(Flash_fwd_params &, cudaStream_t) [with Kernel_traits=Flash_fwd_kernel_traits<224, 128, 64, 8, false, false, cutlass::bfloat16_t, Flash_kernel_traits<224, 128, 64, 8, cutlass::bfloat16_t>>, Is_dropout=true, Is_causal=false]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(208): here
            instantiation of "void run_mha_fwd_hdim224<T>(Flash_fwd_params &, cudaStream_t) [with T=cutlass::bfloat16_t]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_hdim224_bf16_sm80.cu(8): here

/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_kernel.h(275): warning: variable "tQrQ" was declared but never referenced
          detected during:
            instantiation of "void flash::compute_attn_1rowblock<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &, int, int, int) [with Kernel_traits=Flash_fwd_kernel_traits<224, 128, 64, 8, false, false, cutlass::bfloat16_t, Flash_kernel_traits<224, 128, 64, 8, cutlass::bfloat16_t>>, Is_dropout=true, Is_causal=false, Is_even_N=false, Is_even_K=true, Return_softmax=true, Params=Flash_fwd_params]" 
(588): here
            instantiation of "void flash::compute_attn<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &) [with Kernel_traits=Flash_fwd_kernel_traits<224, 128, 64, 8, false, false, cutlass::bfloat16_t, Flash_kernel_traits<224, 128, 64, 8, cutlass::bfloat16_t>>, Is_dropout=true, Is_causal=false, Is_even_N=false, Is_even_K=true, Return_softmax=true, Params=Flash_fwd_params]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(15): here
            instantiation of "void flash_fwd_kernel<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax>(Flash_fwd_params) [with Kernel_traits=Flash_fwd_kernel_traits<224, 128, 64, 8, false, false, cutlass::bfloat16_t, Flash_kernel_traits<224, 128, 64, 8, cutlass::bfloat16_t>>, Is_dropout=true, Is_causal=false, Is_even_N=false, Is_even_K=true, Return_softmax=true]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(34): here
            instantiation of "void run_flash_fwd<Kernel_traits,Is_dropout,Is_causal>(Flash_fwd_params &, cudaStream_t) [with Kernel_traits=Flash_fwd_kernel_traits<224, 128, 64, 8, false, false, cutlass::bfloat16_t, Flash_kernel_traits<224, 128, 64, 8, cutlass::bfloat16_t>>, Is_dropout=true, Is_causal=false]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(208): here
            instantiation of "void run_mha_fwd_hdim224<T>(Flash_fwd_params &, cudaStream_t) [with T=cutlass::bfloat16_t]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_hdim224_bf16_sm80.cu(8): here

/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_kernel.h(140): warning: variable "MMA_M" was declared but never referenced
          detected during:
            instantiation of "void flash::compute_attn_1rowblock<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &, int, int, int) [with Kernel_traits=Flash_fwd_kernel_traits<224, 128, 64, 8, false, false, cutlass::bfloat16_t, Flash_kernel_traits<224, 128, 64, 8, cutlass::bfloat16_t>>, Is_dropout=true, Is_causal=false, Is_even_N=false, Is_even_K=true, Return_softmax=false, Params=Flash_fwd_params]" 
(588): here
            instantiation of "void flash::compute_attn<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &) [with Kernel_traits=Flash_fwd_kernel_traits<224, 128, 64, 8, false, false, cutlass::bfloat16_t, Flash_kernel_traits<224, 128, 64, 8, cutlass::bfloat16_t>>, Is_dropout=true, Is_causal=false, Is_even_N=false, Is_even_K=true, Return_softmax=false, Params=Flash_fwd_params]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(15): here
            instantiation of "void flash_fwd_kernel<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax>(Flash_fwd_params) [with Kernel_traits=Flash_fwd_kernel_traits<224, 128, 64, 8, false, false, cutlass::bfloat16_t, Flash_kernel_traits<224, 128, 64, 8, cutlass::bfloat16_t>>, Is_dropout=true, Is_causal=false, Is_even_N=false, Is_even_K=true, Return_softmax=false]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(34): here
            instantiation of "void run_flash_fwd<Kernel_traits,Is_dropout,Is_causal>(Flash_fwd_params &, cudaStream_t) [with Kernel_traits=Flash_fwd_kernel_traits<224, 128, 64, 8, false, false, cutlass::bfloat16_t, Flash_kernel_traits<224, 128, 64, 8, cutlass::bfloat16_t>>, Is_dropout=true, Is_causal=false]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(208): here
            instantiation of "void run_mha_fwd_hdim224<T>(Flash_fwd_params &, cudaStream_t) [with T=cutlass::bfloat16_t]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_hdim224_bf16_sm80.cu(8): here

/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_kernel.h(275): warning: variable "tQrQ" was declared but never referenced
          detected during:
            instantiation of "void flash::compute_attn_1rowblock<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &, int, int, int) [with Kernel_traits=Flash_fwd_kernel_traits<224, 128, 64, 8, false, false, cutlass::bfloat16_t, Flash_kernel_traits<224, 128, 64, 8, cutlass::bfloat16_t>>, Is_dropout=true, Is_causal=false, Is_even_N=false, Is_even_K=true, Return_softmax=false, Params=Flash_fwd_params]" 
(588): here
            instantiation of "void flash::compute_attn<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &) [with Kernel_traits=Flash_fwd_kernel_traits<224, 128, 64, 8, false, false, cutlass::bfloat16_t, Flash_kernel_traits<224, 128, 64, 8, cutlass::bfloat16_t>>, Is_dropout=true, Is_causal=false, Is_even_N=false, Is_even_K=true, Return_softmax=false, Params=Flash_fwd_params]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(15): here
            instantiation of "void flash_fwd_kernel<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax>(Flash_fwd_params) [with Kernel_traits=Flash_fwd_kernel_traits<224, 128, 64, 8, false, false, cutlass::bfloat16_t, Flash_kernel_traits<224, 128, 64, 8, cutlass::bfloat16_t>>, Is_dropout=true, Is_causal=false, Is_even_N=false, Is_even_K=true, Return_softmax=false]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(34): here
            instantiation of "void run_flash_fwd<Kernel_traits,Is_dropout,Is_causal>(Flash_fwd_params &, cudaStream_t) [with Kernel_traits=Flash_fwd_kernel_traits<224, 128, 64, 8, false, false, cutlass::bfloat16_t, Flash_kernel_traits<224, 128, 64, 8, cutlass::bfloat16_t>>, Is_dropout=true, Is_causal=false]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(208): here
            instantiation of "void run_mha_fwd_hdim224<T>(Flash_fwd_params &, cudaStream_t) [with T=cutlass::bfloat16_t]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_hdim224_bf16_sm80.cu(8): here

/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_kernel.h(140): warning: variable "MMA_M" was declared but never referenced
          detected during:
            instantiation of "void flash::compute_attn_1rowblock<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &, int, int, int) [with Kernel_traits=Flash_fwd_kernel_traits<224, 128, 64, 8, false, false, cutlass::bfloat16_t, Flash_kernel_traits<224, 128, 64, 8, cutlass::bfloat16_t>>, Is_dropout=true, Is_causal=false, Is_even_N=false, Is_even_K=false, Return_softmax=true, Params=Flash_fwd_params]" 
(588): here
            instantiation of "void flash::compute_attn<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &) [with Kernel_traits=Flash_fwd_kernel_traits<224, 128, 64, 8, false, false, cutlass::bfloat16_t, Flash_kernel_traits<224, 128, 64, 8, cutlass::bfloat16_t>>, Is_dropout=true, Is_causal=false, Is_even_N=false, Is_even_K=false, Return_softmax=true, Params=Flash_fwd_params]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(15): here
            instantiation of "void flash_fwd_kernel<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax>(Flash_fwd_params) [with Kernel_traits=Flash_fwd_kernel_traits<224, 128, 64, 8, false, false, cutlass::bfloat16_t, Flash_kernel_traits<224, 128, 64, 8, cutlass::bfloat16_t>>, Is_dropout=true, Is_causal=false, Is_even_N=false, Is_even_K=false, Return_softmax=true]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(34): here
            instantiation of "void run_flash_fwd<Kernel_traits,Is_dropout,Is_causal>(Flash_fwd_params &, cudaStream_t) [with Kernel_traits=Flash_fwd_kernel_traits<224, 128, 64, 8, false, false, cutlass::bfloat16_t, Flash_kernel_traits<224, 128, 64, 8, cutlass::bfloat16_t>>, Is_dropout=true, Is_causal=false]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(208): here
            instantiation of "void run_mha_fwd_hdim224<T>(Flash_fwd_params &, cudaStream_t) [with T=cutlass::bfloat16_t]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_hdim224_bf16_sm80.cu(8): here

/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_kernel.h(275): warning: variable "tQrQ" was declared but never referenced
          detected during:
            instantiation of "void flash::compute_attn_1rowblock<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &, int, int, int) [with Kernel_traits=Flash_fwd_kernel_traits<224, 128, 64, 8, false, false, cutlass::bfloat16_t, Flash_kernel_traits<224, 128, 64, 8, cutlass::bfloat16_t>>, Is_dropout=true, Is_causal=false, Is_even_N=false, Is_even_K=false, Return_softmax=true, Params=Flash_fwd_params]" 
(588): here
            instantiation of "void flash::compute_attn<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &) [with Kernel_traits=Flash_fwd_kernel_traits<224, 128, 64, 8, false, false, cutlass::bfloat16_t, Flash_kernel_traits<224, 128, 64, 8, cutlass::bfloat16_t>>, Is_dropout=true, Is_causal=false, Is_even_N=false, Is_even_K=false, Return_softmax=true, Params=Flash_fwd_params]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(15): here
            instantiation of "void flash_fwd_kernel<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax>(Flash_fwd_params) [with Kernel_traits=Flash_fwd_kernel_traits<224, 128, 64, 8, false, false, cutlass::bfloat16_t, Flash_kernel_traits<224, 128, 64, 8, cutlass::bfloat16_t>>, Is_dropout=true, Is_causal=false, Is_even_N=false, Is_even_K=false, Return_softmax=true]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(34): here
            instantiation of "void run_flash_fwd<Kernel_traits,Is_dropout,Is_causal>(Flash_fwd_params &, cudaStream_t) [with Kernel_traits=Flash_fwd_kernel_traits<224, 128, 64, 8, false, false, cutlass::bfloat16_t, Flash_kernel_traits<224, 128, 64, 8, cutlass::bfloat16_t>>, Is_dropout=true, Is_causal=false]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(208): here
            instantiation of "void run_mha_fwd_hdim224<T>(Flash_fwd_params &, cudaStream_t) [with T=cutlass::bfloat16_t]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_hdim224_bf16_sm80.cu(8): here

/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_kernel.h(140): warning: variable "MMA_M" was declared but never referenced
          detected during:
            instantiation of "void flash::compute_attn_1rowblock<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &, int, int, int) [with Kernel_traits=Flash_fwd_kernel_traits<224, 128, 64, 8, false, false, cutlass::bfloat16_t, Flash_kernel_traits<224, 128, 64, 8, cutlass::bfloat16_t>>, Is_dropout=true, Is_causal=false, Is_even_N=false, Is_even_K=false, Return_softmax=false, Params=Flash_fwd_params]" 
(588): here
            instantiation of "void flash::compute_attn<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &) [with Kernel_traits=Flash_fwd_kernel_traits<224, 128, 64, 8, false, false, cutlass::bfloat16_t, Flash_kernel_traits<224, 128, 64, 8, cutlass::bfloat16_t>>, Is_dropout=true, Is_causal=false, Is_even_N=false, Is_even_K=false, Return_softmax=false, Params=Flash_fwd_params]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(15): here
            instantiation of "void flash_fwd_kernel<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax>(Flash_fwd_params) [with Kernel_traits=Flash_fwd_kernel_traits<224, 128, 64, 8, false, false, cutlass::bfloat16_t, Flash_kernel_traits<224, 128, 64, 8, cutlass::bfloat16_t>>, Is_dropout=true, Is_causal=false, Is_even_N=false, Is_even_K=false, Return_softmax=false]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(34): here
            instantiation of "void run_flash_fwd<Kernel_traits,Is_dropout,Is_causal>(Flash_fwd_params &, cudaStream_t) [with Kernel_traits=Flash_fwd_kernel_traits<224, 128, 64, 8, false, false, cutlass::bfloat16_t, Flash_kernel_traits<224, 128, 64, 8, cutlass::bfloat16_t>>, Is_dropout=true, Is_causal=false]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(208): here
            instantiation of "void run_mha_fwd_hdim224<T>(Flash_fwd_params &, cudaStream_t) [with T=cutlass::bfloat16_t]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_hdim224_bf16_sm80.cu(8): here

/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_kernel.h(275): warning: variable "tQrQ" was declared but never referenced
          detected during:
            instantiation of "void flash::compute_attn_1rowblock<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &, int, int, int) [with Kernel_traits=Flash_fwd_kernel_traits<224, 128, 64, 8, false, false, cutlass::bfloat16_t, Flash_kernel_traits<224, 128, 64, 8, cutlass::bfloat16_t>>, Is_dropout=true, Is_causal=false, Is_even_N=false, Is_even_K=false, Return_softmax=false, Params=Flash_fwd_params]" 
(588): here
            instantiation of "void flash::compute_attn<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &) [with Kernel_traits=Flash_fwd_kernel_traits<224, 128, 64, 8, false, false, cutlass::bfloat16_t, Flash_kernel_traits<224, 128, 64, 8, cutlass::bfloat16_t>>, Is_dropout=true, Is_causal=false, Is_even_N=false, Is_even_K=false, Return_softmax=false, Params=Flash_fwd_params]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(15): here
            instantiation of "void flash_fwd_kernel<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax>(Flash_fwd_params) [with Kernel_traits=Flash_fwd_kernel_traits<224, 128, 64, 8, false, false, cutlass::bfloat16_t, Flash_kernel_traits<224, 128, 64, 8, cutlass::bfloat16_t>>, Is_dropout=true, Is_causal=false, Is_even_N=false, Is_even_K=false, Return_softmax=false]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(34): here
            instantiation of "void run_flash_fwd<Kernel_traits,Is_dropout,Is_causal>(Flash_fwd_params &, cudaStream_t) [with Kernel_traits=Flash_fwd_kernel_traits<224, 128, 64, 8, false, false, cutlass::bfloat16_t, Flash_kernel_traits<224, 128, 64, 8, cutlass::bfloat16_t>>, Is_dropout=true, Is_causal=false]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(208): here
            instantiation of "void run_mha_fwd_hdim224<T>(Flash_fwd_params &, cudaStream_t) [with T=cutlass::bfloat16_t]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_hdim224_bf16_sm80.cu(8): here

/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_kernel.h(140): warning: variable "MMA_M" was declared but never referenced
          detected during:
            instantiation of "void flash::compute_attn_1rowblock<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &, int, int, int) [with Kernel_traits=Flash_fwd_kernel_traits<224, 64, 64, 4, false, false, cutlass::bfloat16_t, Flash_kernel_traits<224, 64, 64, 4, cutlass::bfloat16_t>>, Is_dropout=true, Is_causal=false, Is_even_N=true, Is_even_K=true, Return_softmax=true, Params=Flash_fwd_params]" 
(588): here
            instantiation of "void flash::compute_attn<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &) [with Kernel_traits=Flash_fwd_kernel_traits<224, 64, 64, 4, false, false, cutlass::bfloat16_t, Flash_kernel_traits<224, 64, 64, 4, cutlass::bfloat16_t>>, Is_dropout=true, Is_causal=false, Is_even_N=true, Is_even_K=true, Return_softmax=true, Params=Flash_fwd_params]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(15): here
            instantiation of "void flash_fwd_kernel<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax>(Flash_fwd_params) [with Kernel_traits=Flash_fwd_kernel_traits<224, 64, 64, 4, false, false, cutlass::bfloat16_t, Flash_kernel_traits<224, 64, 64, 4, cutlass::bfloat16_t>>, Is_dropout=true, Is_causal=false, Is_even_N=true, Is_even_K=true, Return_softmax=true]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(34): here
            instantiation of "void run_flash_fwd<Kernel_traits,Is_dropout,Is_causal>(Flash_fwd_params &, cudaStream_t) [with Kernel_traits=Flash_fwd_kernel_traits<224, 64, 64, 4, false, false, cutlass::bfloat16_t, Flash_kernel_traits<224, 64, 64, 4, cutlass::bfloat16_t>>, Is_dropout=true, Is_causal=false]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(208): here
            instantiation of "void run_mha_fwd_hdim224<T>(Flash_fwd_params &, cudaStream_t) [with T=cutlass::bfloat16_t]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_hdim224_bf16_sm80.cu(8): here

/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_kernel.h(275): warning: variable "tQrQ" was declared but never referenced
          detected during:
            instantiation of "void flash::compute_attn_1rowblock<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &, int, int, int) [with Kernel_traits=Flash_fwd_kernel_traits<224, 64, 64, 4, false, false, cutlass::bfloat16_t, Flash_kernel_traits<224, 64, 64, 4, cutlass::bfloat16_t>>, Is_dropout=true, Is_causal=false, Is_even_N=true, Is_even_K=true, Return_softmax=true, Params=Flash_fwd_params]" 
(588): here
            instantiation of "void flash::compute_attn<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &) [with Kernel_traits=Flash_fwd_kernel_traits<224, 64, 64, 4, false, false, cutlass::bfloat16_t, Flash_kernel_traits<224, 64, 64, 4, cutlass::bfloat16_t>>, Is_dropout=true, Is_causal=false, Is_even_N=true, Is_even_K=true, Return_softmax=true, Params=Flash_fwd_params]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(15): here
            instantiation of "void flash_fwd_kernel<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax>(Flash_fwd_params) [with Kernel_traits=Flash_fwd_kernel_traits<224, 64, 64, 4, false, false, cutlass::bfloat16_t, Flash_kernel_traits<224, 64, 64, 4, cutlass::bfloat16_t>>, Is_dropout=true, Is_causal=false, Is_even_N=true, Is_even_K=true, Return_softmax=true]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(34): here
            instantiation of "void run_flash_fwd<Kernel_traits,Is_dropout,Is_causal>(Flash_fwd_params &, cudaStream_t) [with Kernel_traits=Flash_fwd_kernel_traits<224, 64, 64, 4, false, false, cutlass::bfloat16_t, Flash_kernel_traits<224, 64, 64, 4, cutlass::bfloat16_t>>, Is_dropout=true, Is_causal=false]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(208): here
            instantiation of "void run_mha_fwd_hdim224<T>(Flash_fwd_params &, cudaStream_t) [with T=cutlass::bfloat16_t]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_hdim224_bf16_sm80.cu(8): here

/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_kernel.h(140): warning: variable "MMA_M" was declared but never referenced
          detected during:
            instantiation of "void flash::compute_attn_1rowblock<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &, int, int, int) [with Kernel_traits=Flash_fwd_kernel_traits<224, 64, 64, 4, false, false, cutlass::bfloat16_t, Flash_kernel_traits<224, 64, 64, 4, cutlass::bfloat16_t>>, Is_dropout=true, Is_causal=false, Is_even_N=true, Is_even_K=true, Return_softmax=false, Params=Flash_fwd_params]" 
(588): here
            instantiation of "void flash::compute_attn<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &) [with Kernel_traits=Flash_fwd_kernel_traits<224, 64, 64, 4, false, false, cutlass::bfloat16_t, Flash_kernel_traits<224, 64, 64, 4, cutlass::bfloat16_t>>, Is_dropout=true, Is_causal=false, Is_even_N=true, Is_even_K=true, Return_softmax=false, Params=Flash_fwd_params]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(15): here
            instantiation of "void flash_fwd_kernel<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax>(Flash_fwd_params) [with Kernel_traits=Flash_fwd_kernel_traits<224, 64, 64, 4, false, false, cutlass::bfloat16_t, Flash_kernel_traits<224, 64, 64, 4, cutlass::bfloat16_t>>, Is_dropout=true, Is_causal=false, Is_even_N=true, Is_even_K=true, Return_softmax=false]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(34): here
            instantiation of "void run_flash_fwd<Kernel_traits,Is_dropout,Is_causal>(Flash_fwd_params &, cudaStream_t) [with Kernel_traits=Flash_fwd_kernel_traits<224, 64, 64, 4, false, false, cutlass::bfloat16_t, Flash_kernel_traits<224, 64, 64, 4, cutlass::bfloat16_t>>, Is_dropout=true, Is_causal=false]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(208): here
            instantiation of "void run_mha_fwd_hdim224<T>(Flash_fwd_params &, cudaStream_t) [with T=cutlass::bfloat16_t]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_hdim224_bf16_sm80.cu(8): here

/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_kernel.h(275): warning: variable "tQrQ" was declared but never referenced
          detected during:
            instantiation of "void flash::compute_attn_1rowblock<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &, int, int, int) [with Kernel_traits=Flash_fwd_kernel_traits<224, 64, 64, 4, false, false, cutlass::bfloat16_t, Flash_kernel_traits<224, 64, 64, 4, cutlass::bfloat16_t>>, Is_dropout=true, Is_causal=false, Is_even_N=true, Is_even_K=true, Return_softmax=false, Params=Flash_fwd_params]" 
(588): here
            instantiation of "void flash::compute_attn<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &) [with Kernel_traits=Flash_fwd_kernel_traits<224, 64, 64, 4, false, false, cutlass::bfloat16_t, Flash_kernel_traits<224, 64, 64, 4, cutlass::bfloat16_t>>, Is_dropout=true, Is_causal=false, Is_even_N=true, Is_even_K=true, Return_softmax=false, Params=Flash_fwd_params]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(15): here
            instantiation of "void flash_fwd_kernel<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax>(Flash_fwd_params) [with Kernel_traits=Flash_fwd_kernel_traits<224, 64, 64, 4, false, false, cutlass::bfloat16_t, Flash_kernel_traits<224, 64, 64, 4, cutlass::bfloat16_t>>, Is_dropout=true, Is_causal=false, Is_even_N=true, Is_even_K=true, Return_softmax=false]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(34): here
            instantiation of "void run_flash_fwd<Kernel_traits,Is_dropout,Is_causal>(Flash_fwd_params &, cudaStream_t) [with Kernel_traits=Flash_fwd_kernel_traits<224, 64, 64, 4, false, false, cutlass::bfloat16_t, Flash_kernel_traits<224, 64, 64, 4, cutlass::bfloat16_t>>, Is_dropout=true, Is_causal=false]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(208): here
            instantiation of "void run_mha_fwd_hdim224<T>(Flash_fwd_params &, cudaStream_t) [with T=cutlass::bfloat16_t]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_hdim224_bf16_sm80.cu(8): here

/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_kernel.h(140): warning: variable "MMA_M" was declared but never referenced
          detected during:
            instantiation of "void flash::compute_attn_1rowblock<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &, int, int, int) [with Kernel_traits=Flash_fwd_kernel_traits<224, 64, 64, 4, false, false, cutlass::bfloat16_t, Flash_kernel_traits<224, 64, 64, 4, cutlass::bfloat16_t>>, Is_dropout=true, Is_causal=false, Is_even_N=true, Is_even_K=false, Return_softmax=true, Params=Flash_fwd_params]" 
(588): here
            instantiation of "void flash::compute_attn<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &) [with Kernel_traits=Flash_fwd_kernel_traits<224, 64, 64, 4, false, false, cutlass::bfloat16_t, Flash_kernel_traits<224, 64, 64, 4, cutlass::bfloat16_t>>, Is_dropout=true, Is_causal=false, Is_even_N=true, Is_even_K=false, Return_softmax=true, Params=Flash_fwd_params]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(15): here
            instantiation of "void flash_fwd_kernel<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax>(Flash_fwd_params) [with Kernel_traits=Flash_fwd_kernel_traits<224, 64, 64, 4, false, false, cutlass::bfloat16_t, Flash_kernel_traits<224, 64, 64, 4, cutlass::bfloat16_t>>, Is_dropout=true, Is_causal=false, Is_even_N=true, Is_even_K=false, Return_softmax=true]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(34): here
            instantiation of "void run_flash_fwd<Kernel_traits,Is_dropout,Is_causal>(Flash_fwd_params &, cudaStream_t) [with Kernel_traits=Flash_fwd_kernel_traits<224, 64, 64, 4, false, false, cutlass::bfloat16_t, Flash_kernel_traits<224, 64, 64, 4, cutlass::bfloat16_t>>, Is_dropout=true, Is_causal=false]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(208): here
            instantiation of "void run_mha_fwd_hdim224<T>(Flash_fwd_params &, cudaStream_t) [with T=cutlass::bfloat16_t]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_hdim224_bf16_sm80.cu(8): here

/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_kernel.h(275): warning: variable "tQrQ" was declared but never referenced
          detected during:
            instantiation of "void flash::compute_attn_1rowblock<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &, int, int, int) [with Kernel_traits=Flash_fwd_kernel_traits<224, 64, 64, 4, false, false, cutlass::bfloat16_t, Flash_kernel_traits<224, 64, 64, 4, cutlass::bfloat16_t>>, Is_dropout=true, Is_causal=false, Is_even_N=true, Is_even_K=false, Return_softmax=true, Params=Flash_fwd_params]" 
(588): here
            instantiation of "void flash::compute_attn<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &) [with Kernel_traits=Flash_fwd_kernel_traits<224, 64, 64, 4, false, false, cutlass::bfloat16_t, Flash_kernel_traits<224, 64, 64, 4, cutlass::bfloat16_t>>, Is_dropout=true, Is_causal=false, Is_even_N=true, Is_even_K=false, Return_softmax=true, Params=Flash_fwd_params]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(15): here
            instantiation of "void flash_fwd_kernel<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax>(Flash_fwd_params) [with Kernel_traits=Flash_fwd_kernel_traits<224, 64, 64, 4, false, false, cutlass::bfloat16_t, Flash_kernel_traits<224, 64, 64, 4, cutlass::bfloat16_t>>, Is_dropout=true, Is_causal=false, Is_even_N=true, Is_even_K=false, Return_softmax=true]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(34): here
            instantiation of "void run_flash_fwd<Kernel_traits,Is_dropout,Is_causal>(Flash_fwd_params &, cudaStream_t) [with Kernel_traits=Flash_fwd_kernel_traits<224, 64, 64, 4, false, false, cutlass::bfloat16_t, Flash_kernel_traits<224, 64, 64, 4, cutlass::bfloat16_t>>, Is_dropout=true, Is_causal=false]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(208): here
            instantiation of "void run_mha_fwd_hdim224<T>(Flash_fwd_params &, cudaStream_t) [with T=cutlass::bfloat16_t]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_hdim224_bf16_sm80.cu(8): here

/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_kernel.h(140): warning: variable "MMA_M" was declared but never referenced
          detected during:
            instantiation of "void flash::compute_attn_1rowblock<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &, int, int, int) [with Kernel_traits=Flash_fwd_kernel_traits<224, 64, 64, 4, false, false, cutlass::bfloat16_t, Flash_kernel_traits<224, 64, 64, 4, cutlass::bfloat16_t>>, Is_dropout=true, Is_causal=false, Is_even_N=true, Is_even_K=false, Return_softmax=false, Params=Flash_fwd_params]" 
(588): here
            instantiation of "void flash::compute_attn<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &) [with Kernel_traits=Flash_fwd_kernel_traits<224, 64, 64, 4, false, false, cutlass::bfloat16_t, Flash_kernel_traits<224, 64, 64, 4, cutlass::bfloat16_t>>, Is_dropout=true, Is_causal=false, Is_even_N=true, Is_even_K=false, Return_softmax=false, Params=Flash_fwd_params]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(15): here
            instantiation of "void flash_fwd_kernel<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax>(Flash_fwd_params) [with Kernel_traits=Flash_fwd_kernel_traits<224, 64, 64, 4, false, false, cutlass::bfloat16_t, Flash_kernel_traits<224, 64, 64, 4, cutlass::bfloat16_t>>, Is_dropout=true, Is_causal=false, Is_even_N=true, Is_even_K=false, Return_softmax=false]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(34): here
            instantiation of "void run_flash_fwd<Kernel_traits,Is_dropout,Is_causal>(Flash_fwd_params &, cudaStream_t) [with Kernel_traits=Flash_fwd_kernel_traits<224, 64, 64, 4, false, false, cutlass::bfloat16_t, Flash_kernel_traits<224, 64, 64, 4, cutlass::bfloat16_t>>, Is_dropout=true, Is_causal=false]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(208): here
            instantiation of "void run_mha_fwd_hdim224<T>(Flash_fwd_params &, cudaStream_t) [with T=cutlass::bfloat16_t]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_hdim224_bf16_sm80.cu(8): here

/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_kernel.h(275): warning: variable "tQrQ" was declared but never referenced
          detected during:
            instantiation of "void flash::compute_attn_1rowblock<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &, int, int, int) [with Kernel_traits=Flash_fwd_kernel_traits<224, 64, 64, 4, false, false, cutlass::bfloat16_t, Flash_kernel_traits<224, 64, 64, 4, cutlass::bfloat16_t>>, Is_dropout=true, Is_causal=false, Is_even_N=true, Is_even_K=false, Return_softmax=false, Params=Flash_fwd_params]" 
(588): here
            instantiation of "void flash::compute_attn<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &) [with Kernel_traits=Flash_fwd_kernel_traits<224, 64, 64, 4, false, false, cutlass::bfloat16_t, Flash_kernel_traits<224, 64, 64, 4, cutlass::bfloat16_t>>, Is_dropout=true, Is_causal=false, Is_even_N=true, Is_even_K=false, Return_softmax=false, Params=Flash_fwd_params]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(15): here
            instantiation of "void flash_fwd_kernel<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax>(Flash_fwd_params) [with Kernel_traits=Flash_fwd_kernel_traits<224, 64, 64, 4, false, false, cutlass::bfloat16_t, Flash_kernel_traits<224, 64, 64, 4, cutlass::bfloat16_t>>, Is_dropout=true, Is_causal=false, Is_even_N=true, Is_even_K=false, Return_softmax=false]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(34): here
            instantiation of "void run_flash_fwd<Kernel_traits,Is_dropout,Is_causal>(Flash_fwd_params &, cudaStream_t) [with Kernel_traits=Flash_fwd_kernel_traits<224, 64, 64, 4, false, false, cutlass::bfloat16_t, Flash_kernel_traits<224, 64, 64, 4, cutlass::bfloat16_t>>, Is_dropout=true, Is_causal=false]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(208): here
            instantiation of "void run_mha_fwd_hdim224<T>(Flash_fwd_params &, cudaStream_t) [with T=cutlass::bfloat16_t]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_hdim224_bf16_sm80.cu(8): here

/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_kernel.h(140): warning: variable "MMA_M" was declared but never referenced
          detected during:
            instantiation of "void flash::compute_attn_1rowblock<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &, int, int, int) [with Kernel_traits=Flash_fwd_kernel_traits<224, 64, 64, 4, false, false, cutlass::bfloat16_t, Flash_kernel_traits<224, 64, 64, 4, cutlass::bfloat16_t>>, Is_dropout=true, Is_causal=false, Is_even_N=false, Is_even_K=true, Return_softmax=true, Params=Flash_fwd_params]" 
(588): here
            instantiation of "void flash::compute_attn<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &) [with Kernel_traits=Flash_fwd_kernel_traits<224, 64, 64, 4, false, false, cutlass::bfloat16_t, Flash_kernel_traits<224, 64, 64, 4, cutlass::bfloat16_t>>, Is_dropout=true, Is_causal=false, Is_even_N=false, Is_even_K=true, Return_softmax=true, Params=Flash_fwd_params]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(15): here
            instantiation of "void flash_fwd_kernel<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax>(Flash_fwd_params) [with Kernel_traits=Flash_fwd_kernel_traits<224, 64, 64, 4, false, false, cutlass::bfloat16_t, Flash_kernel_traits<224, 64, 64, 4, cutlass::bfloat16_t>>, Is_dropout=true, Is_causal=false, Is_even_N=false, Is_even_K=true, Return_softmax=true]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(34): here
            instantiation of "void run_flash_fwd<Kernel_traits,Is_dropout,Is_causal>(Flash_fwd_params &, cudaStream_t) [with Kernel_traits=Flash_fwd_kernel_traits<224, 64, 64, 4, false, false, cutlass::bfloat16_t, Flash_kernel_traits<224, 64, 64, 4, cutlass::bfloat16_t>>, Is_dropout=true, Is_causal=false]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(208): here
            instantiation of "void run_mha_fwd_hdim224<T>(Flash_fwd_params &, cudaStream_t) [with T=cutlass::bfloat16_t]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_hdim224_bf16_sm80.cu(8): here

/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_kernel.h(275): warning: variable "tQrQ" was declared but never referenced
          detected during:
            instantiation of "void flash::compute_attn_1rowblock<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &, int, int, int) [with Kernel_traits=Flash_fwd_kernel_traits<224, 64, 64, 4, false, false, cutlass::bfloat16_t, Flash_kernel_traits<224, 64, 64, 4, cutlass::bfloat16_t>>, Is_dropout=true, Is_causal=false, Is_even_N=false, Is_even_K=true, Return_softmax=true, Params=Flash_fwd_params]" 
(588): here
            instantiation of "void flash::compute_attn<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &) [with Kernel_traits=Flash_fwd_kernel_traits<224, 64, 64, 4, false, false, cutlass::bfloat16_t, Flash_kernel_traits<224, 64, 64, 4, cutlass::bfloat16_t>>, Is_dropout=true, Is_causal=false, Is_even_N=false, Is_even_K=true, Return_softmax=true, Params=Flash_fwd_params]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(15): here
            instantiation of "void flash_fwd_kernel<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax>(Flash_fwd_params) [with Kernel_traits=Flash_fwd_kernel_traits<224, 64, 64, 4, false, false, cutlass::bfloat16_t, Flash_kernel_traits<224, 64, 64, 4, cutlass::bfloat16_t>>, Is_dropout=true, Is_causal=false, Is_even_N=false, Is_even_K=true, Return_softmax=true]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(34): here
            instantiation of "void run_flash_fwd<Kernel_traits,Is_dropout,Is_causal>(Flash_fwd_params &, cudaStream_t) [with Kernel_traits=Flash_fwd_kernel_traits<224, 64, 64, 4, false, false, cutlass::bfloat16_t, Flash_kernel_traits<224, 64, 64, 4, cutlass::bfloat16_t>>, Is_dropout=true, Is_causal=false]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(208): here
            instantiation of "void run_mha_fwd_hdim224<T>(Flash_fwd_params &, cudaStream_t) [with T=cutlass::bfloat16_t]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_hdim224_bf16_sm80.cu(8): here

/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_kernel.h(140): warning: variable "MMA_M" was declared but never referenced
          detected during:
            instantiation of "void flash::compute_attn_1rowblock<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &, int, int, int) [with Kernel_traits=Flash_fwd_kernel_traits<224, 64, 64, 4, false, false, cutlass::bfloat16_t, Flash_kernel_traits<224, 64, 64, 4, cutlass::bfloat16_t>>, Is_dropout=true, Is_causal=false, Is_even_N=false, Is_even_K=true, Return_softmax=false, Params=Flash_fwd_params]" 
(588): here
            instantiation of "void flash::compute_attn<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &) [with Kernel_traits=Flash_fwd_kernel_traits<224, 64, 64, 4, false, false, cutlass::bfloat16_t, Flash_kernel_traits<224, 64, 64, 4, cutlass::bfloat16_t>>, Is_dropout=true, Is_causal=false, Is_even_N=false, Is_even_K=true, Return_softmax=false, Params=Flash_fwd_params]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(15): here
            instantiation of "void flash_fwd_kernel<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax>(Flash_fwd_params) [with Kernel_traits=Flash_fwd_kernel_traits<224, 64, 64, 4, false, false, cutlass::bfloat16_t, Flash_kernel_traits<224, 64, 64, 4, cutlass::bfloat16_t>>, Is_dropout=true, Is_causal=false, Is_even_N=false, Is_even_K=true, Return_softmax=false]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(34): here
            instantiation of "void run_flash_fwd<Kernel_traits,Is_dropout,Is_causal>(Flash_fwd_params &, cudaStream_t) [with Kernel_traits=Flash_fwd_kernel_traits<224, 64, 64, 4, false, false, cutlass::bfloat16_t, Flash_kernel_traits<224, 64, 64, 4, cutlass::bfloat16_t>>, Is_dropout=true, Is_causal=false]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(208): here
            instantiation of "void run_mha_fwd_hdim224<T>(Flash_fwd_params &, cudaStream_t) [with T=cutlass::bfloat16_t]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_hdim224_bf16_sm80.cu(8): here

/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_kernel.h(275): warning: variable "tQrQ" was declared but never referenced
          detected during:
            instantiation of "void flash::compute_attn_1rowblock<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &, int, int, int) [with Kernel_traits=Flash_fwd_kernel_traits<224, 64, 64, 4, false, false, cutlass::bfloat16_t, Flash_kernel_traits<224, 64, 64, 4, cutlass::bfloat16_t>>, Is_dropout=true, Is_causal=false, Is_even_N=false, Is_even_K=true, Return_softmax=false, Params=Flash_fwd_params]" 
(588): here
            instantiation of "void flash::compute_attn<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &) [with Kernel_traits=Flash_fwd_kernel_traits<224, 64, 64, 4, false, false, cutlass::bfloat16_t, Flash_kernel_traits<224, 64, 64, 4, cutlass::bfloat16_t>>, Is_dropout=true, Is_causal=false, Is_even_N=false, Is_even_K=true, Return_softmax=false, Params=Flash_fwd_params]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(15): here
            instantiation of "void flash_fwd_kernel<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax>(Flash_fwd_params) [with Kernel_traits=Flash_fwd_kernel_traits<224, 64, 64, 4, false, false, cutlass::bfloat16_t, Flash_kernel_traits<224, 64, 64, 4, cutlass::bfloat16_t>>, Is_dropout=true, Is_causal=false, Is_even_N=false, Is_even_K=true, Return_softmax=false]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(34): here
            instantiation of "void run_flash_fwd<Kernel_traits,Is_dropout,Is_causal>(Flash_fwd_params &, cudaStream_t) [with Kernel_traits=Flash_fwd_kernel_traits<224, 64, 64, 4, false, false, cutlass::bfloat16_t, Flash_kernel_traits<224, 64, 64, 4, cutlass::bfloat16_t>>, Is_dropout=true, Is_causal=false]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(208): here
            instantiation of "void run_mha_fwd_hdim224<T>(Flash_fwd_params &, cudaStream_t) [with T=cutlass::bfloat16_t]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_hdim224_bf16_sm80.cu(8): here

/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_kernel.h(140): warning: variable "MMA_M" was declared but never referenced
          detected during:
            instantiation of "void flash::compute_attn_1rowblock<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &, int, int, int) [with Kernel_traits=Flash_fwd_kernel_traits<224, 64, 64, 4, false, false, cutlass::bfloat16_t, Flash_kernel_traits<224, 64, 64, 4, cutlass::bfloat16_t>>, Is_dropout=true, Is_causal=false, Is_even_N=false, Is_even_K=false, Return_softmax=true, Params=Flash_fwd_params]" 
(588): here
            instantiation of "void flash::compute_attn<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &) [with Kernel_traits=Flash_fwd_kernel_traits<224, 64, 64, 4, false, false, cutlass::bfloat16_t, Flash_kernel_traits<224, 64, 64, 4, cutlass::bfloat16_t>>, Is_dropout=true, Is_causal=false, Is_even_N=false, Is_even_K=false, Return_softmax=true, Params=Flash_fwd_params]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(15): here
            instantiation of "void flash_fwd_kernel<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax>(Flash_fwd_params) [with Kernel_traits=Flash_fwd_kernel_traits<224, 64, 64, 4, false, false, cutlass::bfloat16_t, Flash_kernel_traits<224, 64, 64, 4, cutlass::bfloat16_t>>, Is_dropout=true, Is_causal=false, Is_even_N=false, Is_even_K=false, Return_softmax=true]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(34): here
            instantiation of "void run_flash_fwd<Kernel_traits,Is_dropout,Is_causal>(Flash_fwd_params &, cudaStream_t) [with Kernel_traits=Flash_fwd_kernel_traits<224, 64, 64, 4, false, false, cutlass::bfloat16_t, Flash_kernel_traits<224, 64, 64, 4, cutlass::bfloat16_t>>, Is_dropout=true, Is_causal=false]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(208): here
            instantiation of "void run_mha_fwd_hdim224<T>(Flash_fwd_params &, cudaStream_t) [with T=cutlass::bfloat16_t]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_hdim224_bf16_sm80.cu(8): here

/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_kernel.h(275): warning: variable "tQrQ" was declared but never referenced
          detected during:
            instantiation of "void flash::compute_attn_1rowblock<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &, int, int, int) [with Kernel_traits=Flash_fwd_kernel_traits<224, 64, 64, 4, false, false, cutlass::bfloat16_t, Flash_kernel_traits<224, 64, 64, 4, cutlass::bfloat16_t>>, Is_dropout=true, Is_causal=false, Is_even_N=false, Is_even_K=false, Return_softmax=true, Params=Flash_fwd_params]" 
(588): here
            instantiation of "void flash::compute_attn<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &) [with Kernel_traits=Flash_fwd_kernel_traits<224, 64, 64, 4, false, false, cutlass::bfloat16_t, Flash_kernel_traits<224, 64, 64, 4, cutlass::bfloat16_t>>, Is_dropout=true, Is_causal=false, Is_even_N=false, Is_even_K=false, Return_softmax=true, Params=Flash_fwd_params]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(15): here
            instantiation of "void flash_fwd_kernel<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax>(Flash_fwd_params) [with Kernel_traits=Flash_fwd_kernel_traits<224, 64, 64, 4, false, false, cutlass::bfloat16_t, Flash_kernel_traits<224, 64, 64, 4, cutlass::bfloat16_t>>, Is_dropout=true, Is_causal=false, Is_even_N=false, Is_even_K=false, Return_softmax=true]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(34): here
            instantiation of "void run_flash_fwd<Kernel_traits,Is_dropout,Is_causal>(Flash_fwd_params &, cudaStream_t) [with Kernel_traits=Flash_fwd_kernel_traits<224, 64, 64, 4, false, false, cutlass::bfloat16_t, Flash_kernel_traits<224, 64, 64, 4, cutlass::bfloat16_t>>, Is_dropout=true, Is_causal=false]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(208): here
            instantiation of "void run_mha_fwd_hdim224<T>(Flash_fwd_params &, cudaStream_t) [with T=cutlass::bfloat16_t]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_hdim224_bf16_sm80.cu(8): here

/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_kernel.h(140): warning: variable "MMA_M" was declared but never referenced
          detected during:
            instantiation of "void flash::compute_attn_1rowblock<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &, int, int, int) [with Kernel_traits=Flash_fwd_kernel_traits<224, 64, 64, 4, false, false, cutlass::bfloat16_t, Flash_kernel_traits<224, 64, 64, 4, cutlass::bfloat16_t>>, Is_dropout=true, Is_causal=false, Is_even_N=false, Is_even_K=false, Return_softmax=false, Params=Flash_fwd_params]" 
(588): here
            instantiation of "void flash::compute_attn<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &) [with Kernel_traits=Flash_fwd_kernel_traits<224, 64, 64, 4, false, false, cutlass::bfloat16_t, Flash_kernel_traits<224, 64, 64, 4, cutlass::bfloat16_t>>, Is_dropout=true, Is_causal=false, Is_even_N=false, Is_even_K=false, Return_softmax=false, Params=Flash_fwd_params]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(15): here
            instantiation of "void flash_fwd_kernel<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax>(Flash_fwd_params) [with Kernel_traits=Flash_fwd_kernel_traits<224, 64, 64, 4, false, false, cutlass::bfloat16_t, Flash_kernel_traits<224, 64, 64, 4, cutlass::bfloat16_t>>, Is_dropout=true, Is_causal=false, Is_even_N=false, Is_even_K=false, Return_softmax=false]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(34): here
            instantiation of "void run_flash_fwd<Kernel_traits,Is_dropout,Is_causal>(Flash_fwd_params &, cudaStream_t) [with Kernel_traits=Flash_fwd_kernel_traits<224, 64, 64, 4, false, false, cutlass::bfloat16_t, Flash_kernel_traits<224, 64, 64, 4, cutlass::bfloat16_t>>, Is_dropout=true, Is_causal=false]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(208): here
            instantiation of "void run_mha_fwd_hdim224<T>(Flash_fwd_params &, cudaStream_t) [with T=cutlass::bfloat16_t]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_hdim224_bf16_sm80.cu(8): here

/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_kernel.h(275): warning: variable "tQrQ" was declared but never referenced
          detected during:
            instantiation of "void flash::compute_attn_1rowblock<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &, int, int, int) [with Kernel_traits=Flash_fwd_kernel_traits<224, 64, 64, 4, false, false, cutlass::bfloat16_t, Flash_kernel_traits<224, 64, 64, 4, cutlass::bfloat16_t>>, Is_dropout=true, Is_causal=false, Is_even_N=false, Is_even_K=false, Return_softmax=false, Params=Flash_fwd_params]" 
(588): here
            instantiation of "void flash::compute_attn<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &) [with Kernel_traits=Flash_fwd_kernel_traits<224, 64, 64, 4, false, false, cutlass::bfloat16_t, Flash_kernel_traits<224, 64, 64, 4, cutlass::bfloat16_t>>, Is_dropout=true, Is_causal=false, Is_even_N=false, Is_even_K=false, Return_softmax=false, Params=Flash_fwd_params]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(15): here
            instantiation of "void flash_fwd_kernel<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax>(Flash_fwd_params) [with Kernel_traits=Flash_fwd_kernel_traits<224, 64, 64, 4, false, false, cutlass::bfloat16_t, Flash_kernel_traits<224, 64, 64, 4, cutlass::bfloat16_t>>, Is_dropout=true, Is_causal=false, Is_even_N=false, Is_even_K=false, Return_softmax=false]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(34): here
            instantiation of "void run_flash_fwd<Kernel_traits,Is_dropout,Is_causal>(Flash_fwd_params &, cudaStream_t) [with Kernel_traits=Flash_fwd_kernel_traits<224, 64, 64, 4, false, false, cutlass::bfloat16_t, Flash_kernel_traits<224, 64, 64, 4, cutlass::bfloat16_t>>, Is_dropout=true, Is_causal=false]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(208): here
            instantiation of "void run_mha_fwd_hdim224<T>(Flash_fwd_params &, cudaStream_t) [with T=cutlass::bfloat16_t]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_hdim224_bf16_sm80.cu(8): here

/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_kernel.h(140): warning: variable "MMA_M" was declared but never referenced
          detected during:
            instantiation of "void flash::compute_attn_1rowblock<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &, int, int, int) [with Kernel_traits=Flash_fwd_kernel_traits<224, 128, 64, 8, false, false, cutlass::bfloat16_t, Flash_kernel_traits<224, 128, 64, 8, cutlass::bfloat16_t>>, Is_dropout=false, Is_causal=true, Is_even_N=true, Is_even_K=true, Return_softmax=false, Params=Flash_fwd_params]" 
(588): here
            instantiation of "void flash::compute_attn<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &) [with Kernel_traits=Flash_fwd_kernel_traits<224, 128, 64, 8, false, false, cutlass::bfloat16_t, Flash_kernel_traits<224, 128, 64, 8, cutlass::bfloat16_t>>, Is_dropout=false, Is_causal=true, Is_even_N=true, Is_even_K=true, Return_softmax=false, Params=Flash_fwd_params]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(15): here
            instantiation of "void flash_fwd_kernel<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax>(Flash_fwd_params) [with Kernel_traits=Flash_fwd_kernel_traits<224, 128, 64, 8, false, false, cutlass::bfloat16_t, Flash_kernel_traits<224, 128, 64, 8, cutlass::bfloat16_t>>, Is_dropout=false, Is_causal=true, Is_even_N=true, Is_even_K=true, Return_softmax=false]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(34): here
            instantiation of "void run_flash_fwd<Kernel_traits,Is_dropout,Is_causal>(Flash_fwd_params &, cudaStream_t) [with Kernel_traits=Flash_fwd_kernel_traits<224, 128, 64, 8, false, false, cutlass::bfloat16_t, Flash_kernel_traits<224, 128, 64, 8, cutlass::bfloat16_t>>, Is_dropout=false, Is_causal=true]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(208): here
            instantiation of "void run_mha_fwd_hdim224<T>(Flash_fwd_params &, cudaStream_t) [with T=cutlass::bfloat16_t]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_hdim224_bf16_sm80.cu(8): here

/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_kernel.h(275): warning: variable "tQrQ" was declared but never referenced
          detected during:
            instantiation of "void flash::compute_attn_1rowblock<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &, int, int, int) [with Kernel_traits=Flash_fwd_kernel_traits<224, 128, 64, 8, false, false, cutlass::bfloat16_t, Flash_kernel_traits<224, 128, 64, 8, cutlass::bfloat16_t>>, Is_dropout=false, Is_causal=true, Is_even_N=true, Is_even_K=true, Return_softmax=false, Params=Flash_fwd_params]" 
(588): here
            instantiation of "void flash::compute_attn<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &) [with Kernel_traits=Flash_fwd_kernel_traits<224, 128, 64, 8, false, false, cutlass::bfloat16_t, Flash_kernel_traits<224, 128, 64, 8, cutlass::bfloat16_t>>, Is_dropout=false, Is_causal=true, Is_even_N=true, Is_even_K=true, Return_softmax=false, Params=Flash_fwd_params]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(15): here
            instantiation of "void flash_fwd_kernel<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax>(Flash_fwd_params) [with Kernel_traits=Flash_fwd_kernel_traits<224, 128, 64, 8, false, false, cutlass::bfloat16_t, Flash_kernel_traits<224, 128, 64, 8, cutlass::bfloat16_t>>, Is_dropout=false, Is_causal=true, Is_even_N=true, Is_even_K=true, Return_softmax=false]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(34): here
            instantiation of "void run_flash_fwd<Kernel_traits,Is_dropout,Is_causal>(Flash_fwd_params &, cudaStream_t) [with Kernel_traits=Flash_fwd_kernel_traits<224, 128, 64, 8, false, false, cutlass::bfloat16_t, Flash_kernel_traits<224, 128, 64, 8, cutlass::bfloat16_t>>, Is_dropout=false, Is_causal=true]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(208): here
            instantiation of "void run_mha_fwd_hdim224<T>(Flash_fwd_params &, cudaStream_t) [with T=cutlass::bfloat16_t]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_hdim224_bf16_sm80.cu(8): here

/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_kernel.h(140): warning: variable "MMA_M" was declared but never referenced
          detected during:
            instantiation of "void flash::compute_attn_1rowblock<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &, int, int, int) [with Kernel_traits=Flash_fwd_kernel_traits<224, 128, 64, 8, false, false, cutlass::bfloat16_t, Flash_kernel_traits<224, 128, 64, 8, cutlass::bfloat16_t>>, Is_dropout=false, Is_causal=true, Is_even_N=true, Is_even_K=false, Return_softmax=false, Params=Flash_fwd_params]" 
(588): here
            instantiation of "void flash::compute_attn<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &) [with Kernel_traits=Flash_fwd_kernel_traits<224, 128, 64, 8, false, false, cutlass::bfloat16_t, Flash_kernel_traits<224, 128, 64, 8, cutlass::bfloat16_t>>, Is_dropout=false, Is_causal=true, Is_even_N=true, Is_even_K=false, Return_softmax=false, Params=Flash_fwd_params]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(15): here
            instantiation of "void flash_fwd_kernel<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax>(Flash_fwd_params) [with Kernel_traits=Flash_fwd_kernel_traits<224, 128, 64, 8, false, false, cutlass::bfloat16_t, Flash_kernel_traits<224, 128, 64, 8, cutlass::bfloat16_t>>, Is_dropout=false, Is_causal=true, Is_even_N=true, Is_even_K=false, Return_softmax=false]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(34): here
            instantiation of "void run_flash_fwd<Kernel_traits,Is_dropout,Is_causal>(Flash_fwd_params &, cudaStream_t) [with Kernel_traits=Flash_fwd_kernel_traits<224, 128, 64, 8, false, false, cutlass::bfloat16_t, Flash_kernel_traits<224, 128, 64, 8, cutlass::bfloat16_t>>, Is_dropout=false, Is_causal=true]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(208): here
            instantiation of "void run_mha_fwd_hdim224<T>(Flash_fwd_params &, cudaStream_t) [with T=cutlass::bfloat16_t]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_hdim224_bf16_sm80.cu(8): here

/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_kernel.h(275): warning: variable "tQrQ" was declared but never referenced
          detected during:
            instantiation of "void flash::compute_attn_1rowblock<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &, int, int, int) [with Kernel_traits=Flash_fwd_kernel_traits<224, 128, 64, 8, false, false, cutlass::bfloat16_t, Flash_kernel_traits<224, 128, 64, 8, cutlass::bfloat16_t>>, Is_dropout=false, Is_causal=true, Is_even_N=true, Is_even_K=false, Return_softmax=false, Params=Flash_fwd_params]" 
(588): here
            instantiation of "void flash::compute_attn<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &) [with Kernel_traits=Flash_fwd_kernel_traits<224, 128, 64, 8, false, false, cutlass::bfloat16_t, Flash_kernel_traits<224, 128, 64, 8, cutlass::bfloat16_t>>, Is_dropout=false, Is_causal=true, Is_even_N=true, Is_even_K=false, Return_softmax=false, Params=Flash_fwd_params]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(15): here
            instantiation of "void flash_fwd_kernel<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax>(Flash_fwd_params) [with Kernel_traits=Flash_fwd_kernel_traits<224, 128, 64, 8, false, false, cutlass::bfloat16_t, Flash_kernel_traits<224, 128, 64, 8, cutlass::bfloat16_t>>, Is_dropout=false, Is_causal=true, Is_even_N=true, Is_even_K=false, Return_softmax=false]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(34): here
            instantiation of "void run_flash_fwd<Kernel_traits,Is_dropout,Is_causal>(Flash_fwd_params &, cudaStream_t) [with Kernel_traits=Flash_fwd_kernel_traits<224, 128, 64, 8, false, false, cutlass::bfloat16_t, Flash_kernel_traits<224, 128, 64, 8, cutlass::bfloat16_t>>, Is_dropout=false, Is_causal=true]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(208): here
            instantiation of "void run_mha_fwd_hdim224<T>(Flash_fwd_params &, cudaStream_t) [with T=cutlass::bfloat16_t]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_hdim224_bf16_sm80.cu(8): here

/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_kernel.h(140): warning: variable "MMA_M" was declared but never referenced
          detected during:
            instantiation of "void flash::compute_attn_1rowblock<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &, int, int, int) [with Kernel_traits=Flash_fwd_kernel_traits<224, 128, 64, 8, false, false, cutlass::bfloat16_t, Flash_kernel_traits<224, 128, 64, 8, cutlass::bfloat16_t>>, Is_dropout=false, Is_causal=true, Is_even_N=false, Is_even_K=true, Return_softmax=false, Params=Flash_fwd_params]" 
(588): here
            instantiation of "void flash::compute_attn<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &) [with Kernel_traits=Flash_fwd_kernel_traits<224, 128, 64, 8, false, false, cutlass::bfloat16_t, Flash_kernel_traits<224, 128, 64, 8, cutlass::bfloat16_t>>, Is_dropout=false, Is_causal=true, Is_even_N=false, Is_even_K=true, Return_softmax=false, Params=Flash_fwd_params]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(15): here
            instantiation of "void flash_fwd_kernel<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax>(Flash_fwd_params) [with Kernel_traits=Flash_fwd_kernel_traits<224, 128, 64, 8, false, false, cutlass::bfloat16_t, Flash_kernel_traits<224, 128, 64, 8, cutlass::bfloat16_t>>, Is_dropout=false, Is_causal=true, Is_even_N=false, Is_even_K=true, Return_softmax=false]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(34): here
            instantiation of "void run_flash_fwd<Kernel_traits,Is_dropout,Is_causal>(Flash_fwd_params &, cudaStream_t) [with Kernel_traits=Flash_fwd_kernel_traits<224, 128, 64, 8, false, false, cutlass::bfloat16_t, Flash_kernel_traits<224, 128, 64, 8, cutlass::bfloat16_t>>, Is_dropout=false, Is_causal=true]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(208): here
            instantiation of "void run_mha_fwd_hdim224<T>(Flash_fwd_params &, cudaStream_t) [with T=cutlass::bfloat16_t]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_hdim224_bf16_sm80.cu(8): here

/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_kernel.h(275): warning: variable "tQrQ" was declared but never referenced
          detected during:
            instantiation of "void flash::compute_attn_1rowblock<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &, int, int, int) [with Kernel_traits=Flash_fwd_kernel_traits<224, 128, 64, 8, false, false, cutlass::bfloat16_t, Flash_kernel_traits<224, 128, 64, 8, cutlass::bfloat16_t>>, Is_dropout=false, Is_causal=true, Is_even_N=false, Is_even_K=true, Return_softmax=false, Params=Flash_fwd_params]" 
(588): here
            instantiation of "void flash::compute_attn<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &) [with Kernel_traits=Flash_fwd_kernel_traits<224, 128, 64, 8, false, false, cutlass::bfloat16_t, Flash_kernel_traits<224, 128, 64, 8, cutlass::bfloat16_t>>, Is_dropout=false, Is_causal=true, Is_even_N=false, Is_even_K=true, Return_softmax=false, Params=Flash_fwd_params]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(15): here
            instantiation of "void flash_fwd_kernel<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax>(Flash_fwd_params) [with Kernel_traits=Flash_fwd_kernel_traits<224, 128, 64, 8, false, false, cutlass::bfloat16_t, Flash_kernel_traits<224, 128, 64, 8, cutlass::bfloat16_t>>, Is_dropout=false, Is_causal=true, Is_even_N=false, Is_even_K=true, Return_softmax=false]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(34): here
            instantiation of "void run_flash_fwd<Kernel_traits,Is_dropout,Is_causal>(Flash_fwd_params &, cudaStream_t) [with Kernel_traits=Flash_fwd_kernel_traits<224, 128, 64, 8, false, false, cutlass::bfloat16_t, Flash_kernel_traits<224, 128, 64, 8, cutlass::bfloat16_t>>, Is_dropout=false, Is_causal=true]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(208): here
            instantiation of "void run_mha_fwd_hdim224<T>(Flash_fwd_params &, cudaStream_t) [with T=cutlass::bfloat16_t]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_hdim224_bf16_sm80.cu(8): here

/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_kernel.h(140): warning: variable "MMA_M" was declared but never referenced
          detected during:
            instantiation of "void flash::compute_attn_1rowblock<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &, int, int, int) [with Kernel_traits=Flash_fwd_kernel_traits<224, 128, 64, 8, false, false, cutlass::bfloat16_t, Flash_kernel_traits<224, 128, 64, 8, cutlass::bfloat16_t>>, Is_dropout=false, Is_causal=true, Is_even_N=false, Is_even_K=false, Return_softmax=false, Params=Flash_fwd_params]" 
(588): here
            instantiation of "void flash::compute_attn<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &) [with Kernel_traits=Flash_fwd_kernel_traits<224, 128, 64, 8, false, false, cutlass::bfloat16_t, Flash_kernel_traits<224, 128, 64, 8, cutlass::bfloat16_t>>, Is_dropout=false, Is_causal=true, Is_even_N=false, Is_even_K=false, Return_softmax=false, Params=Flash_fwd_params]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(15): here
            instantiation of "void flash_fwd_kernel<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax>(Flash_fwd_params) [with Kernel_traits=Flash_fwd_kernel_traits<224, 128, 64, 8, false, false, cutlass::bfloat16_t, Flash_kernel_traits<224, 128, 64, 8, cutlass::bfloat16_t>>, Is_dropout=false, Is_causal=true, Is_even_N=false, Is_even_K=false, Return_softmax=false]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(34): here
            instantiation of "void run_flash_fwd<Kernel_traits,Is_dropout,Is_causal>(Flash_fwd_params &, cudaStream_t) [with Kernel_traits=Flash_fwd_kernel_traits<224, 128, 64, 8, false, false, cutlass::bfloat16_t, Flash_kernel_traits<224, 128, 64, 8, cutlass::bfloat16_t>>, Is_dropout=false, Is_causal=true]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(208): here
            instantiation of "void run_mha_fwd_hdim224<T>(Flash_fwd_params &, cudaStream_t) [with T=cutlass::bfloat16_t]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_hdim224_bf16_sm80.cu(8): here

/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_kernel.h(275): warning: variable "tQrQ" was declared but never referenced
          detected during:
            instantiation of "void flash::compute_attn_1rowblock<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &, int, int, int) [with Kernel_traits=Flash_fwd_kernel_traits<224, 128, 64, 8, false, false, cutlass::bfloat16_t, Flash_kernel_traits<224, 128, 64, 8, cutlass::bfloat16_t>>, Is_dropout=false, Is_causal=true, Is_even_N=false, Is_even_K=false, Return_softmax=false, Params=Flash_fwd_params]" 
(588): here
            instantiation of "void flash::compute_attn<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &) [with Kernel_traits=Flash_fwd_kernel_traits<224, 128, 64, 8, false, false, cutlass::bfloat16_t, Flash_kernel_traits<224, 128, 64, 8, cutlass::bfloat16_t>>, Is_dropout=false, Is_causal=true, Is_even_N=false, Is_even_K=false, Return_softmax=false, Params=Flash_fwd_params]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(15): here
            instantiation of "void flash_fwd_kernel<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax>(Flash_fwd_params) [with Kernel_traits=Flash_fwd_kernel_traits<224, 128, 64, 8, false, false, cutlass::bfloat16_t, Flash_kernel_traits<224, 128, 64, 8, cutlass::bfloat16_t>>, Is_dropout=false, Is_causal=true, Is_even_N=false, Is_even_K=false, Return_softmax=false]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(34): here
            instantiation of "void run_flash_fwd<Kernel_traits,Is_dropout,Is_causal>(Flash_fwd_params &, cudaStream_t) [with Kernel_traits=Flash_fwd_kernel_traits<224, 128, 64, 8, false, false, cutlass::bfloat16_t, Flash_kernel_traits<224, 128, 64, 8, cutlass::bfloat16_t>>, Is_dropout=false, Is_causal=true]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(208): here
            instantiation of "void run_mha_fwd_hdim224<T>(Flash_fwd_params &, cudaStream_t) [with T=cutlass::bfloat16_t]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_hdim224_bf16_sm80.cu(8): here

/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_kernel.h(140): warning: variable "MMA_M" was declared but never referenced
          detected during:
            instantiation of "void flash::compute_attn_1rowblock<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &, int, int, int) [with Kernel_traits=Flash_fwd_kernel_traits<224, 64, 64, 4, false, false, cutlass::bfloat16_t, Flash_kernel_traits<224, 64, 64, 4, cutlass::bfloat16_t>>, Is_dropout=false, Is_causal=true, Is_even_N=true, Is_even_K=true, Return_softmax=false, Params=Flash_fwd_params]" 
(588): here
            instantiation of "void flash::compute_attn<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &) [with Kernel_traits=Flash_fwd_kernel_traits<224, 64, 64, 4, false, false, cutlass::bfloat16_t, Flash_kernel_traits<224, 64, 64, 4, cutlass::bfloat16_t>>, Is_dropout=false, Is_causal=true, Is_even_N=true, Is_even_K=true, Return_softmax=false, Params=Flash_fwd_params]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(15): here
            instantiation of "void flash_fwd_kernel<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax>(Flash_fwd_params) [with Kernel_traits=Flash_fwd_kernel_traits<224, 64, 64, 4, false, false, cutlass::bfloat16_t, Flash_kernel_traits<224, 64, 64, 4, cutlass::bfloat16_t>>, Is_dropout=false, Is_causal=true, Is_even_N=true, Is_even_K=true, Return_softmax=false]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(34): here
            instantiation of "void run_flash_fwd<Kernel_traits,Is_dropout,Is_causal>(Flash_fwd_params &, cudaStream_t) [with Kernel_traits=Flash_fwd_kernel_traits<224, 64, 64, 4, false, false, cutlass::bfloat16_t, Flash_kernel_traits<224, 64, 64, 4, cutlass::bfloat16_t>>, Is_dropout=false, Is_causal=true]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(208): here
            instantiation of "void run_mha_fwd_hdim224<T>(Flash_fwd_params &, cudaStream_t) [with T=cutlass::bfloat16_t]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_hdim224_bf16_sm80.cu(8): here

/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_kernel.h(275): warning: variable "tQrQ" was declared but never referenced
          detected during:
            instantiation of "void flash::compute_attn_1rowblock<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &, int, int, int) [with Kernel_traits=Flash_fwd_kernel_traits<224, 64, 64, 4, false, false, cutlass::bfloat16_t, Flash_kernel_traits<224, 64, 64, 4, cutlass::bfloat16_t>>, Is_dropout=false, Is_causal=true, Is_even_N=true, Is_even_K=true, Return_softmax=false, Params=Flash_fwd_params]" 
(588): here
            instantiation of "void flash::compute_attn<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &) [with Kernel_traits=Flash_fwd_kernel_traits<224, 64, 64, 4, false, false, cutlass::bfloat16_t, Flash_kernel_traits<224, 64, 64, 4, cutlass::bfloat16_t>>, Is_dropout=false, Is_causal=true, Is_even_N=true, Is_even_K=true, Return_softmax=false, Params=Flash_fwd_params]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(15): here
            instantiation of "void flash_fwd_kernel<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax>(Flash_fwd_params) [with Kernel_traits=Flash_fwd_kernel_traits<224, 64, 64, 4, false, false, cutlass::bfloat16_t, Flash_kernel_traits<224, 64, 64, 4, cutlass::bfloat16_t>>, Is_dropout=false, Is_causal=true, Is_even_N=true, Is_even_K=true, Return_softmax=false]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(34): here
            instantiation of "void run_flash_fwd<Kernel_traits,Is_dropout,Is_causal>(Flash_fwd_params &, cudaStream_t) [with Kernel_traits=Flash_fwd_kernel_traits<224, 64, 64, 4, false, false, cutlass::bfloat16_t, Flash_kernel_traits<224, 64, 64, 4, cutlass::bfloat16_t>>, Is_dropout=false, Is_causal=true]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(208): here
            instantiation of "void run_mha_fwd_hdim224<T>(Flash_fwd_params &, cudaStream_t) [with T=cutlass::bfloat16_t]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_hdim224_bf16_sm80.cu(8): here

/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_kernel.h(140): warning: variable "MMA_M" was declared but never referenced
          detected during:
            instantiation of "void flash::compute_attn_1rowblock<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &, int, int, int) [with Kernel_traits=Flash_fwd_kernel_traits<224, 64, 64, 4, false, false, cutlass::bfloat16_t, Flash_kernel_traits<224, 64, 64, 4, cutlass::bfloat16_t>>, Is_dropout=false, Is_causal=true, Is_even_N=true, Is_even_K=false, Return_softmax=false, Params=Flash_fwd_params]" 
(588): here
            instantiation of "void flash::compute_attn<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &) [with Kernel_traits=Flash_fwd_kernel_traits<224, 64, 64, 4, false, false, cutlass::bfloat16_t, Flash_kernel_traits<224, 64, 64, 4, cutlass::bfloat16_t>>, Is_dropout=false, Is_causal=true, Is_even_N=true, Is_even_K=false, Return_softmax=false, Params=Flash_fwd_params]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(15): here
            instantiation of "void flash_fwd_kernel<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax>(Flash_fwd_params) [with Kernel_traits=Flash_fwd_kernel_traits<224, 64, 64, 4, false, false, cutlass::bfloat16_t, Flash_kernel_traits<224, 64, 64, 4, cutlass::bfloat16_t>>, Is_dropout=false, Is_causal=true, Is_even_N=true, Is_even_K=false, Return_softmax=false]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(34): here
            instantiation of "void run_flash_fwd<Kernel_traits,Is_dropout,Is_causal>(Flash_fwd_params &, cudaStream_t) [with Kernel_traits=Flash_fwd_kernel_traits<224, 64, 64, 4, false, false, cutlass::bfloat16_t, Flash_kernel_traits<224, 64, 64, 4, cutlass::bfloat16_t>>, Is_dropout=false, Is_causal=true]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(208): here
            instantiation of "void run_mha_fwd_hdim224<T>(Flash_fwd_params &, cudaStream_t) [with T=cutlass::bfloat16_t]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_hdim224_bf16_sm80.cu(8): here

/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_kernel.h(275): warning: variable "tQrQ" was declared but never referenced
          detected during:
            instantiation of "void flash::compute_attn_1rowblock<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &, int, int, int) [with Kernel_traits=Flash_fwd_kernel_traits<224, 64, 64, 4, false, false, cutlass::bfloat16_t, Flash_kernel_traits<224, 64, 64, 4, cutlass::bfloat16_t>>, Is_dropout=false, Is_causal=true, Is_even_N=true, Is_even_K=false, Return_softmax=false, Params=Flash_fwd_params]" 
(588): here
            instantiation of "void flash::compute_attn<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &) [with Kernel_traits=Flash_fwd_kernel_traits<224, 64, 64, 4, false, false, cutlass::bfloat16_t, Flash_kernel_traits<224, 64, 64, 4, cutlass::bfloat16_t>>, Is_dropout=false, Is_causal=true, Is_even_N=true, Is_even_K=false, Return_softmax=false, Params=Flash_fwd_params]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(15): here
            instantiation of "void flash_fwd_kernel<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax>(Flash_fwd_params) [with Kernel_traits=Flash_fwd_kernel_traits<224, 64, 64, 4, false, false, cutlass::bfloat16_t, Flash_kernel_traits<224, 64, 64, 4, cutlass::bfloat16_t>>, Is_dropout=false, Is_causal=true, Is_even_N=true, Is_even_K=false, Return_softmax=false]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(34): here
            instantiation of "void run_flash_fwd<Kernel_traits,Is_dropout,Is_causal>(Flash_fwd_params &, cudaStream_t) [with Kernel_traits=Flash_fwd_kernel_traits<224, 64, 64, 4, false, false, cutlass::bfloat16_t, Flash_kernel_traits<224, 64, 64, 4, cutlass::bfloat16_t>>, Is_dropout=false, Is_causal=true]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(208): here
            instantiation of "void run_mha_fwd_hdim224<T>(Flash_fwd_params &, cudaStream_t) [with T=cutlass::bfloat16_t]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_hdim224_bf16_sm80.cu(8): here

/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_kernel.h(140): warning: variable "MMA_M" was declared but never referenced
          detected during:
            instantiation of "void flash::compute_attn_1rowblock<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &, int, int, int) [with Kernel_traits=Flash_fwd_kernel_traits<224, 64, 64, 4, false, false, cutlass::bfloat16_t, Flash_kernel_traits<224, 64, 64, 4, cutlass::bfloat16_t>>, Is_dropout=false, Is_causal=true, Is_even_N=false, Is_even_K=true, Return_softmax=false, Params=Flash_fwd_params]" 
(588): here
            instantiation of "void flash::compute_attn<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &) [with Kernel_traits=Flash_fwd_kernel_traits<224, 64, 64, 4, false, false, cutlass::bfloat16_t, Flash_kernel_traits<224, 64, 64, 4, cutlass::bfloat16_t>>, Is_dropout=false, Is_causal=true, Is_even_N=false, Is_even_K=true, Return_softmax=false, Params=Flash_fwd_params]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(15): here
            instantiation of "void flash_fwd_kernel<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax>(Flash_fwd_params) [with Kernel_traits=Flash_fwd_kernel_traits<224, 64, 64, 4, false, false, cutlass::bfloat16_t, Flash_kernel_traits<224, 64, 64, 4, cutlass::bfloat16_t>>, Is_dropout=false, Is_causal=true, Is_even_N=false, Is_even_K=true, Return_softmax=false]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(34): here
            instantiation of "void run_flash_fwd<Kernel_traits,Is_dropout,Is_causal>(Flash_fwd_params &, cudaStream_t) [with Kernel_traits=Flash_fwd_kernel_traits<224, 64, 64, 4, false, false, cutlass::bfloat16_t, Flash_kernel_traits<224, 64, 64, 4, cutlass::bfloat16_t>>, Is_dropout=false, Is_causal=true]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(208): here
            instantiation of "void run_mha_fwd_hdim224<T>(Flash_fwd_params &, cudaStream_t) [with T=cutlass::bfloat16_t]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_hdim224_bf16_sm80.cu(8): here

/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_kernel.h(275): warning: variable "tQrQ" was declared but never referenced
          detected during:
            instantiation of "void flash::compute_attn_1rowblock<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &, int, int, int) [with Kernel_traits=Flash_fwd_kernel_traits<224, 64, 64, 4, false, false, cutlass::bfloat16_t, Flash_kernel_traits<224, 64, 64, 4, cutlass::bfloat16_t>>, Is_dropout=false, Is_causal=true, Is_even_N=false, Is_even_K=true, Return_softmax=false, Params=Flash_fwd_params]" 
(588): here
            instantiation of "void flash::compute_attn<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &) [with Kernel_traits=Flash_fwd_kernel_traits<224, 64, 64, 4, false, false, cutlass::bfloat16_t, Flash_kernel_traits<224, 64, 64, 4, cutlass::bfloat16_t>>, Is_dropout=false, Is_causal=true, Is_even_N=false, Is_even_K=true, Return_softmax=false, Params=Flash_fwd_params]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(15): here
            instantiation of "void flash_fwd_kernel<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax>(Flash_fwd_params) [with Kernel_traits=Flash_fwd_kernel_traits<224, 64, 64, 4, false, false, cutlass::bfloat16_t, Flash_kernel_traits<224, 64, 64, 4, cutlass::bfloat16_t>>, Is_dropout=false, Is_causal=true, Is_even_N=false, Is_even_K=true, Return_softmax=false]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(34): here
            instantiation of "void run_flash_fwd<Kernel_traits,Is_dropout,Is_causal>(Flash_fwd_params &, cudaStream_t) [with Kernel_traits=Flash_fwd_kernel_traits<224, 64, 64, 4, false, false, cutlass::bfloat16_t, Flash_kernel_traits<224, 64, 64, 4, cutlass::bfloat16_t>>, Is_dropout=false, Is_causal=true]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(208): here
            instantiation of "void run_mha_fwd_hdim224<T>(Flash_fwd_params &, cudaStream_t) [with T=cutlass::bfloat16_t]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_hdim224_bf16_sm80.cu(8): here

/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_kernel.h(140): warning: variable "MMA_M" was declared but never referenced
          detected during:
            instantiation of "void flash::compute_attn_1rowblock<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &, int, int, int) [with Kernel_traits=Flash_fwd_kernel_traits<224, 64, 64, 4, false, false, cutlass::bfloat16_t, Flash_kernel_traits<224, 64, 64, 4, cutlass::bfloat16_t>>, Is_dropout=false, Is_causal=true, Is_even_N=false, Is_even_K=false, Return_softmax=false, Params=Flash_fwd_params]" 
(588): here
            instantiation of "void flash::compute_attn<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &) [with Kernel_traits=Flash_fwd_kernel_traits<224, 64, 64, 4, false, false, cutlass::bfloat16_t, Flash_kernel_traits<224, 64, 64, 4, cutlass::bfloat16_t>>, Is_dropout=false, Is_causal=true, Is_even_N=false, Is_even_K=false, Return_softmax=false, Params=Flash_fwd_params]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(15): here
            instantiation of "void flash_fwd_kernel<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax>(Flash_fwd_params) [with Kernel_traits=Flash_fwd_kernel_traits<224, 64, 64, 4, false, false, cutlass::bfloat16_t, Flash_kernel_traits<224, 64, 64, 4, cutlass::bfloat16_t>>, Is_dropout=false, Is_causal=true, Is_even_N=false, Is_even_K=false, Return_softmax=false]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(34): here
            instantiation of "void run_flash_fwd<Kernel_traits,Is_dropout,Is_causal>(Flash_fwd_params &, cudaStream_t) [with Kernel_traits=Flash_fwd_kernel_traits<224, 64, 64, 4, false, false, cutlass::bfloat16_t, Flash_kernel_traits<224, 64, 64, 4, cutlass::bfloat16_t>>, Is_dropout=false, Is_causal=true]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(208): here
            instantiation of "void run_mha_fwd_hdim224<T>(Flash_fwd_params &, cudaStream_t) [with T=cutlass::bfloat16_t]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_hdim224_bf16_sm80.cu(8): here

/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_kernel.h(275): warning: variable "tQrQ" was declared but never referenced
          detected during:
            instantiation of "void flash::compute_attn_1rowblock<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &, int, int, int) [with Kernel_traits=Flash_fwd_kernel_traits<224, 64, 64, 4, false, false, cutlass::bfloat16_t, Flash_kernel_traits<224, 64, 64, 4, cutlass::bfloat16_t>>, Is_dropout=false, Is_causal=true, Is_even_N=false, Is_even_K=false, Return_softmax=false, Params=Flash_fwd_params]" 
(588): here
            instantiation of "void flash::compute_attn<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &) [with Kernel_traits=Flash_fwd_kernel_traits<224, 64, 64, 4, false, false, cutlass::bfloat16_t, Flash_kernel_traits<224, 64, 64, 4, cutlass::bfloat16_t>>, Is_dropout=false, Is_causal=true, Is_even_N=false, Is_even_K=false, Return_softmax=false, Params=Flash_fwd_params]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(15): here
            instantiation of "void flash_fwd_kernel<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax>(Flash_fwd_params) [with Kernel_traits=Flash_fwd_kernel_traits<224, 64, 64, 4, false, false, cutlass::bfloat16_t, Flash_kernel_traits<224, 64, 64, 4, cutlass::bfloat16_t>>, Is_dropout=false, Is_causal=true, Is_even_N=false, Is_even_K=false, Return_softmax=false]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(34): here
            instantiation of "void run_flash_fwd<Kernel_traits,Is_dropout,Is_causal>(Flash_fwd_params &, cudaStream_t) [with Kernel_traits=Flash_fwd_kernel_traits<224, 64, 64, 4, false, false, cutlass::bfloat16_t, Flash_kernel_traits<224, 64, 64, 4, cutlass::bfloat16_t>>, Is_dropout=false, Is_causal=true]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(208): here
            instantiation of "void run_mha_fwd_hdim224<T>(Flash_fwd_params &, cudaStream_t) [with T=cutlass::bfloat16_t]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_hdim224_bf16_sm80.cu(8): here

/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_kernel.h(140): warning: variable "MMA_M" was declared but never referenced
          detected during:
            instantiation of "void flash::compute_attn_1rowblock<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &, int, int, int) [with Kernel_traits=Flash_fwd_kernel_traits<224, 128, 64, 8, false, false, cutlass::bfloat16_t, Flash_kernel_traits<224, 128, 64, 8, cutlass::bfloat16_t>>, Is_dropout=false, Is_causal=false, Is_even_N=true, Is_even_K=true, Return_softmax=false, Params=Flash_fwd_params]" 
(588): here
            instantiation of "void flash::compute_attn<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &) [with Kernel_traits=Flash_fwd_kernel_traits<224, 128, 64, 8, false, false, cutlass::bfloat16_t, Flash_kernel_traits<224, 128, 64, 8, cutlass::bfloat16_t>>, Is_dropout=false, Is_causal=false, Is_even_N=true, Is_even_K=true, Return_softmax=false, Params=Flash_fwd_params]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(15): here
            instantiation of "void flash_fwd_kernel<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax>(Flash_fwd_params) [with Kernel_traits=Flash_fwd_kernel_traits<224, 128, 64, 8, false, false, cutlass::bfloat16_t, Flash_kernel_traits<224, 128, 64, 8, cutlass::bfloat16_t>>, Is_dropout=false, Is_causal=false, Is_even_N=true, Is_even_K=true, Return_softmax=false]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(34): here
            instantiation of "void run_flash_fwd<Kernel_traits,Is_dropout,Is_causal>(Flash_fwd_params &, cudaStream_t) [with Kernel_traits=Flash_fwd_kernel_traits<224, 128, 64, 8, false, false, cutlass::bfloat16_t, Flash_kernel_traits<224, 128, 64, 8, cutlass::bfloat16_t>>, Is_dropout=false, Is_causal=false]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(208): here
            instantiation of "void run_mha_fwd_hdim224<T>(Flash_fwd_params &, cudaStream_t) [with T=cutlass::bfloat16_t]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_hdim224_bf16_sm80.cu(8): here

/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_kernel.h(275): warning: variable "tQrQ" was declared but never referenced
          detected during:
            instantiation of "void flash::compute_attn_1rowblock<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &, int, int, int) [with Kernel_traits=Flash_fwd_kernel_traits<224, 128, 64, 8, false, false, cutlass::bfloat16_t, Flash_kernel_traits<224, 128, 64, 8, cutlass::bfloat16_t>>, Is_dropout=false, Is_causal=false, Is_even_N=true, Is_even_K=true, Return_softmax=false, Params=Flash_fwd_params]" 
(588): here
            instantiation of "void flash::compute_attn<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &) [with Kernel_traits=Flash_fwd_kernel_traits<224, 128, 64, 8, false, false, cutlass::bfloat16_t, Flash_kernel_traits<224, 128, 64, 8, cutlass::bfloat16_t>>, Is_dropout=false, Is_causal=false, Is_even_N=true, Is_even_K=true, Return_softmax=false, Params=Flash_fwd_params]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(15): here
            instantiation of "void flash_fwd_kernel<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax>(Flash_fwd_params) [with Kernel_traits=Flash_fwd_kernel_traits<224, 128, 64, 8, false, false, cutlass::bfloat16_t, Flash_kernel_traits<224, 128, 64, 8, cutlass::bfloat16_t>>, Is_dropout=false, Is_causal=false, Is_even_N=true, Is_even_K=true, Return_softmax=false]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(34): here
            instantiation of "void run_flash_fwd<Kernel_traits,Is_dropout,Is_causal>(Flash_fwd_params &, cudaStream_t) [with Kernel_traits=Flash_fwd_kernel_traits<224, 128, 64, 8, false, false, cutlass::bfloat16_t, Flash_kernel_traits<224, 128, 64, 8, cutlass::bfloat16_t>>, Is_dropout=false, Is_causal=false]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(208): here
            instantiation of "void run_mha_fwd_hdim224<T>(Flash_fwd_params &, cudaStream_t) [with T=cutlass::bfloat16_t]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_hdim224_bf16_sm80.cu(8): here

/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_kernel.h(140): warning: variable "MMA_M" was declared but never referenced
          detected during:
            instantiation of "void flash::compute_attn_1rowblock<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &, int, int, int) [with Kernel_traits=Flash_fwd_kernel_traits<224, 128, 64, 8, false, false, cutlass::bfloat16_t, Flash_kernel_traits<224, 128, 64, 8, cutlass::bfloat16_t>>, Is_dropout=false, Is_causal=false, Is_even_N=true, Is_even_K=false, Return_softmax=false, Params=Flash_fwd_params]" 
(588): here
            instantiation of "void flash::compute_attn<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &) [with Kernel_traits=Flash_fwd_kernel_traits<224, 128, 64, 8, false, false, cutlass::bfloat16_t, Flash_kernel_traits<224, 128, 64, 8, cutlass::bfloat16_t>>, Is_dropout=false, Is_causal=false, Is_even_N=true, Is_even_K=false, Return_softmax=false, Params=Flash_fwd_params]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(15): here
            instantiation of "void flash_fwd_kernel<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax>(Flash_fwd_params) [with Kernel_traits=Flash_fwd_kernel_traits<224, 128, 64, 8, false, false, cutlass::bfloat16_t, Flash_kernel_traits<224, 128, 64, 8, cutlass::bfloat16_t>>, Is_dropout=false, Is_causal=false, Is_even_N=true, Is_even_K=false, Return_softmax=false]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(34): here
            instantiation of "void run_flash_fwd<Kernel_traits,Is_dropout,Is_causal>(Flash_fwd_params &, cudaStream_t) [with Kernel_traits=Flash_fwd_kernel_traits<224, 128, 64, 8, false, false, cutlass::bfloat16_t, Flash_kernel_traits<224, 128, 64, 8, cutlass::bfloat16_t>>, Is_dropout=false, Is_causal=false]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(208): here
            instantiation of "void run_mha_fwd_hdim224<T>(Flash_fwd_params &, cudaStream_t) [with T=cutlass::bfloat16_t]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_hdim224_bf16_sm80.cu(8): here

/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_kernel.h(275): warning: variable "tQrQ" was declared but never referenced
          detected during:
            instantiation of "void flash::compute_attn_1rowblock<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &, int, int, int) [with Kernel_traits=Flash_fwd_kernel_traits<224, 128, 64, 8, false, false, cutlass::bfloat16_t, Flash_kernel_traits<224, 128, 64, 8, cutlass::bfloat16_t>>, Is_dropout=false, Is_causal=false, Is_even_N=true, Is_even_K=false, Return_softmax=false, Params=Flash_fwd_params]" 
(588): here
            instantiation of "void flash::compute_attn<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &) [with Kernel_traits=Flash_fwd_kernel_traits<224, 128, 64, 8, false, false, cutlass::bfloat16_t, Flash_kernel_traits<224, 128, 64, 8, cutlass::bfloat16_t>>, Is_dropout=false, Is_causal=false, Is_even_N=true, Is_even_K=false, Return_softmax=false, Params=Flash_fwd_params]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(15): here
            instantiation of "void flash_fwd_kernel<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax>(Flash_fwd_params) [with Kernel_traits=Flash_fwd_kernel_traits<224, 128, 64, 8, false, false, cutlass::bfloat16_t, Flash_kernel_traits<224, 128, 64, 8, cutlass::bfloat16_t>>, Is_dropout=false, Is_causal=false, Is_even_N=true, Is_even_K=false, Return_softmax=false]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(34): here
            instantiation of "void run_flash_fwd<Kernel_traits,Is_dropout,Is_causal>(Flash_fwd_params &, cudaStream_t) [with Kernel_traits=Flash_fwd_kernel_traits<224, 128, 64, 8, false, false, cutlass::bfloat16_t, Flash_kernel_traits<224, 128, 64, 8, cutlass::bfloat16_t>>, Is_dropout=false, Is_causal=false]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(208): here
            instantiation of "void run_mha_fwd_hdim224<T>(Flash_fwd_params &, cudaStream_t) [with T=cutlass::bfloat16_t]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_hdim224_bf16_sm80.cu(8): here

/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_kernel.h(140): warning: variable "MMA_M" was declared but never referenced
          detected during:
            instantiation of "void flash::compute_attn_1rowblock<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &, int, int, int) [with Kernel_traits=Flash_fwd_kernel_traits<224, 128, 64, 8, false, false, cutlass::bfloat16_t, Flash_kernel_traits<224, 128, 64, 8, cutlass::bfloat16_t>>, Is_dropout=false, Is_causal=false, Is_even_N=false, Is_even_K=true, Return_softmax=false, Params=Flash_fwd_params]" 
(588): here
            instantiation of "void flash::compute_attn<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &) [with Kernel_traits=Flash_fwd_kernel_traits<224, 128, 64, 8, false, false, cutlass::bfloat16_t, Flash_kernel_traits<224, 128, 64, 8, cutlass::bfloat16_t>>, Is_dropout=false, Is_causal=false, Is_even_N=false, Is_even_K=true, Return_softmax=false, Params=Flash_fwd_params]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(15): here
            instantiation of "void flash_fwd_kernel<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax>(Flash_fwd_params) [with Kernel_traits=Flash_fwd_kernel_traits<224, 128, 64, 8, false, false, cutlass::bfloat16_t, Flash_kernel_traits<224, 128, 64, 8, cutlass::bfloat16_t>>, Is_dropout=false, Is_causal=false, Is_even_N=false, Is_even_K=true, Return_softmax=false]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(34): here
            instantiation of "void run_flash_fwd<Kernel_traits,Is_dropout,Is_causal>(Flash_fwd_params &, cudaStream_t) [with Kernel_traits=Flash_fwd_kernel_traits<224, 128, 64, 8, false, false, cutlass::bfloat16_t, Flash_kernel_traits<224, 128, 64, 8, cutlass::bfloat16_t>>, Is_dropout=false, Is_causal=false]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(208): here
            instantiation of "void run_mha_fwd_hdim224<T>(Flash_fwd_params &, cudaStream_t) [with T=cutlass::bfloat16_t]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_hdim224_bf16_sm80.cu(8): here

/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_kernel.h(275): warning: variable "tQrQ" was declared but never referenced
          detected during:
            instantiation of "void flash::compute_attn_1rowblock<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &, int, int, int) [with Kernel_traits=Flash_fwd_kernel_traits<224, 128, 64, 8, false, false, cutlass::bfloat16_t, Flash_kernel_traits<224, 128, 64, 8, cutlass::bfloat16_t>>, Is_dropout=false, Is_causal=false, Is_even_N=false, Is_even_K=true, Return_softmax=false, Params=Flash_fwd_params]" 
(588): here
            instantiation of "void flash::compute_attn<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &) [with Kernel_traits=Flash_fwd_kernel_traits<224, 128, 64, 8, false, false, cutlass::bfloat16_t, Flash_kernel_traits<224, 128, 64, 8, cutlass::bfloat16_t>>, Is_dropout=false, Is_causal=false, Is_even_N=false, Is_even_K=true, Return_softmax=false, Params=Flash_fwd_params]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(15): here
            instantiation of "void flash_fwd_kernel<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax>(Flash_fwd_params) [with Kernel_traits=Flash_fwd_kernel_traits<224, 128, 64, 8, false, false, cutlass::bfloat16_t, Flash_kernel_traits<224, 128, 64, 8, cutlass::bfloat16_t>>, Is_dropout=false, Is_causal=false, Is_even_N=false, Is_even_K=true, Return_softmax=false]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(34): here
            instantiation of "void run_flash_fwd<Kernel_traits,Is_dropout,Is_causal>(Flash_fwd_params &, cudaStream_t) [with Kernel_traits=Flash_fwd_kernel_traits<224, 128, 64, 8, false, false, cutlass::bfloat16_t, Flash_kernel_traits<224, 128, 64, 8, cutlass::bfloat16_t>>, Is_dropout=false, Is_causal=false]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(208): here
            instantiation of "void run_mha_fwd_hdim224<T>(Flash_fwd_params &, cudaStream_t) [with T=cutlass::bfloat16_t]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_hdim224_bf16_sm80.cu(8): here

/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_kernel.h(140): warning: variable "MMA_M" was declared but never referenced
          detected during:
            instantiation of "void flash::compute_attn_1rowblock<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &, int, int, int) [with Kernel_traits=Flash_fwd_kernel_traits<224, 128, 64, 8, false, false, cutlass::bfloat16_t, Flash_kernel_traits<224, 128, 64, 8, cutlass::bfloat16_t>>, Is_dropout=false, Is_causal=false, Is_even_N=false, Is_even_K=false, Return_softmax=false, Params=Flash_fwd_params]" 
(588): here
            instantiation of "void flash::compute_attn<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &) [with Kernel_traits=Flash_fwd_kernel_traits<224, 128, 64, 8, false, false, cutlass::bfloat16_t, Flash_kernel_traits<224, 128, 64, 8, cutlass::bfloat16_t>>, Is_dropout=false, Is_causal=false, Is_even_N=false, Is_even_K=false, Return_softmax=false, Params=Flash_fwd_params]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(15): here
            instantiation of "void flash_fwd_kernel<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax>(Flash_fwd_params) [with Kernel_traits=Flash_fwd_kernel_traits<224, 128, 64, 8, false, false, cutlass::bfloat16_t, Flash_kernel_traits<224, 128, 64, 8, cutlass::bfloat16_t>>, Is_dropout=false, Is_causal=false, Is_even_N=false, Is_even_K=false, Return_softmax=false]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(34): here
            instantiation of "void run_flash_fwd<Kernel_traits,Is_dropout,Is_causal>(Flash_fwd_params &, cudaStream_t) [with Kernel_traits=Flash_fwd_kernel_traits<224, 128, 64, 8, false, false, cutlass::bfloat16_t, Flash_kernel_traits<224, 128, 64, 8, cutlass::bfloat16_t>>, Is_dropout=false, Is_causal=false]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(208): here
            instantiation of "void run_mha_fwd_hdim224<T>(Flash_fwd_params &, cudaStream_t) [with T=cutlass::bfloat16_t]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_hdim224_bf16_sm80.cu(8): here

/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_kernel.h(275): warning: variable "tQrQ" was declared but never referenced
          detected during:
            instantiation of "void flash::compute_attn_1rowblock<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &, int, int, int) [with Kernel_traits=Flash_fwd_kernel_traits<224, 128, 64, 8, false, false, cutlass::bfloat16_t, Flash_kernel_traits<224, 128, 64, 8, cutlass::bfloat16_t>>, Is_dropout=false, Is_causal=false, Is_even_N=false, Is_even_K=false, Return_softmax=false, Params=Flash_fwd_params]" 
(588): here
            instantiation of "void flash::compute_attn<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &) [with Kernel_traits=Flash_fwd_kernel_traits<224, 128, 64, 8, false, false, cutlass::bfloat16_t, Flash_kernel_traits<224, 128, 64, 8, cutlass::bfloat16_t>>, Is_dropout=false, Is_causal=false, Is_even_N=false, Is_even_K=false, Return_softmax=false, Params=Flash_fwd_params]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(15): here
            instantiation of "void flash_fwd_kernel<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax>(Flash_fwd_params) [with Kernel_traits=Flash_fwd_kernel_traits<224, 128, 64, 8, false, false, cutlass::bfloat16_t, Flash_kernel_traits<224, 128, 64, 8, cutlass::bfloat16_t>>, Is_dropout=false, Is_causal=false, Is_even_N=false, Is_even_K=false, Return_softmax=false]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(34): here
            instantiation of "void run_flash_fwd<Kernel_traits,Is_dropout,Is_causal>(Flash_fwd_params &, cudaStream_t) [with Kernel_traits=Flash_fwd_kernel_traits<224, 128, 64, 8, false, false, cutlass::bfloat16_t, Flash_kernel_traits<224, 128, 64, 8, cutlass::bfloat16_t>>, Is_dropout=false, Is_causal=false]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(208): here
            instantiation of "void run_mha_fwd_hdim224<T>(Flash_fwd_params &, cudaStream_t) [with T=cutlass::bfloat16_t]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_hdim224_bf16_sm80.cu(8): here

/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_kernel.h(140): warning: variable "MMA_M" was declared but never referenced
          detected during:
            instantiation of "void flash::compute_attn_1rowblock<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &, int, int, int) [with Kernel_traits=Flash_fwd_kernel_traits<224, 64, 64, 4, false, false, cutlass::bfloat16_t, Flash_kernel_traits<224, 64, 64, 4, cutlass::bfloat16_t>>, Is_dropout=false, Is_causal=false, Is_even_N=true, Is_even_K=true, Return_softmax=false, Params=Flash_fwd_params]" 
(588): here
            instantiation of "void flash::compute_attn<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &) [with Kernel_traits=Flash_fwd_kernel_traits<224, 64, 64, 4, false, false, cutlass::bfloat16_t, Flash_kernel_traits<224, 64, 64, 4, cutlass::bfloat16_t>>, Is_dropout=false, Is_causal=false, Is_even_N=true, Is_even_K=true, Return_softmax=false, Params=Flash_fwd_params]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(15): here
            instantiation of "void flash_fwd_kernel<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax>(Flash_fwd_params) [with Kernel_traits=Flash_fwd_kernel_traits<224, 64, 64, 4, false, false, cutlass::bfloat16_t, Flash_kernel_traits<224, 64, 64, 4, cutlass::bfloat16_t>>, Is_dropout=false, Is_causal=false, Is_even_N=true, Is_even_K=true, Return_softmax=false]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(34): here
            instantiation of "void run_flash_fwd<Kernel_traits,Is_dropout,Is_causal>(Flash_fwd_params &, cudaStream_t) [with Kernel_traits=Flash_fwd_kernel_traits<224, 64, 64, 4, false, false, cutlass::bfloat16_t, Flash_kernel_traits<224, 64, 64, 4, cutlass::bfloat16_t>>, Is_dropout=false, Is_causal=false]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(208): here
            instantiation of "void run_mha_fwd_hdim224<T>(Flash_fwd_params &, cudaStream_t) [with T=cutlass::bfloat16_t]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_hdim224_bf16_sm80.cu(8): here

/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_kernel.h(275): warning: variable "tQrQ" was declared but never referenced
          detected during:
            instantiation of "void flash::compute_attn_1rowblock<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &, int, int, int) [with Kernel_traits=Flash_fwd_kernel_traits<224, 64, 64, 4, false, false, cutlass::bfloat16_t, Flash_kernel_traits<224, 64, 64, 4, cutlass::bfloat16_t>>, Is_dropout=false, Is_causal=false, Is_even_N=true, Is_even_K=true, Return_softmax=false, Params=Flash_fwd_params]" 
(588): here
            instantiation of "void flash::compute_attn<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &) [with Kernel_traits=Flash_fwd_kernel_traits<224, 64, 64, 4, false, false, cutlass::bfloat16_t, Flash_kernel_traits<224, 64, 64, 4, cutlass::bfloat16_t>>, Is_dropout=false, Is_causal=false, Is_even_N=true, Is_even_K=true, Return_softmax=false, Params=Flash_fwd_params]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(15): here
            instantiation of "void flash_fwd_kernel<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax>(Flash_fwd_params) [with Kernel_traits=Flash_fwd_kernel_traits<224, 64, 64, 4, false, false, cutlass::bfloat16_t, Flash_kernel_traits<224, 64, 64, 4, cutlass::bfloat16_t>>, Is_dropout=false, Is_causal=false, Is_even_N=true, Is_even_K=true, Return_softmax=false]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(34): here
            instantiation of "void run_flash_fwd<Kernel_traits,Is_dropout,Is_causal>(Flash_fwd_params &, cudaStream_t) [with Kernel_traits=Flash_fwd_kernel_traits<224, 64, 64, 4, false, false, cutlass::bfloat16_t, Flash_kernel_traits<224, 64, 64, 4, cutlass::bfloat16_t>>, Is_dropout=false, Is_causal=false]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(208): here
            instantiation of "void run_mha_fwd_hdim224<T>(Flash_fwd_params &, cudaStream_t) [with T=cutlass::bfloat16_t]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_hdim224_bf16_sm80.cu(8): here

/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_kernel.h(140): warning: variable "MMA_M" was declared but never referenced
          detected during:
            instantiation of "void flash::compute_attn_1rowblock<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &, int, int, int) [with Kernel_traits=Flash_fwd_kernel_traits<224, 64, 64, 4, false, false, cutlass::bfloat16_t, Flash_kernel_traits<224, 64, 64, 4, cutlass::bfloat16_t>>, Is_dropout=false, Is_causal=false, Is_even_N=true, Is_even_K=false, Return_softmax=false, Params=Flash_fwd_params]" 
(588): here
            instantiation of "void flash::compute_attn<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &) [with Kernel_traits=Flash_fwd_kernel_traits<224, 64, 64, 4, false, false, cutlass::bfloat16_t, Flash_kernel_traits<224, 64, 64, 4, cutlass::bfloat16_t>>, Is_dropout=false, Is_causal=false, Is_even_N=true, Is_even_K=false, Return_softmax=false, Params=Flash_fwd_params]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(15): here
            instantiation of "void flash_fwd_kernel<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax>(Flash_fwd_params) [with Kernel_traits=Flash_fwd_kernel_traits<224, 64, 64, 4, false, false, cutlass::bfloat16_t, Flash_kernel_traits<224, 64, 64, 4, cutlass::bfloat16_t>>, Is_dropout=false, Is_causal=false, Is_even_N=true, Is_even_K=false, Return_softmax=false]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(34): here
            instantiation of "void run_flash_fwd<Kernel_traits,Is_dropout,Is_causal>(Flash_fwd_params &, cudaStream_t) [with Kernel_traits=Flash_fwd_kernel_traits<224, 64, 64, 4, false, false, cutlass::bfloat16_t, Flash_kernel_traits<224, 64, 64, 4, cutlass::bfloat16_t>>, Is_dropout=false, Is_causal=false]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(208): here
            instantiation of "void run_mha_fwd_hdim224<T>(Flash_fwd_params &, cudaStream_t) [with T=cutlass::bfloat16_t]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_hdim224_bf16_sm80.cu(8): here

/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_kernel.h(275): warning: variable "tQrQ" was declared but never referenced
          detected during:
            instantiation of "void flash::compute_attn_1rowblock<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &, int, int, int) [with Kernel_traits=Flash_fwd_kernel_traits<224, 64, 64, 4, false, false, cutlass::bfloat16_t, Flash_kernel_traits<224, 64, 64, 4, cutlass::bfloat16_t>>, Is_dropout=false, Is_causal=false, Is_even_N=true, Is_even_K=false, Return_softmax=false, Params=Flash_fwd_params]" 
(588): here
            instantiation of "void flash::compute_attn<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &) [with Kernel_traits=Flash_fwd_kernel_traits<224, 64, 64, 4, false, false, cutlass::bfloat16_t, Flash_kernel_traits<224, 64, 64, 4, cutlass::bfloat16_t>>, Is_dropout=false, Is_causal=false, Is_even_N=true, Is_even_K=false, Return_softmax=false, Params=Flash_fwd_params]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(15): here
            instantiation of "void flash_fwd_kernel<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax>(Flash_fwd_params) [with Kernel_traits=Flash_fwd_kernel_traits<224, 64, 64, 4, false, false, cutlass::bfloat16_t, Flash_kernel_traits<224, 64, 64, 4, cutlass::bfloat16_t>>, Is_dropout=false, Is_causal=false, Is_even_N=true, Is_even_K=false, Return_softmax=false]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(34): here
            instantiation of "void run_flash_fwd<Kernel_traits,Is_dropout,Is_causal>(Flash_fwd_params &, cudaStream_t) [with Kernel_traits=Flash_fwd_kernel_traits<224, 64, 64, 4, false, false, cutlass::bfloat16_t, Flash_kernel_traits<224, 64, 64, 4, cutlass::bfloat16_t>>, Is_dropout=false, Is_causal=false]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(208): here
            instantiation of "void run_mha_fwd_hdim224<T>(Flash_fwd_params &, cudaStream_t) [with T=cutlass::bfloat16_t]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_hdim224_bf16_sm80.cu(8): here

/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_kernel.h(140): warning: variable "MMA_M" was declared but never referenced
          detected during:
            instantiation of "void flash::compute_attn_1rowblock<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &, int, int, int) [with Kernel_traits=Flash_fwd_kernel_traits<224, 64, 64, 4, false, false, cutlass::bfloat16_t, Flash_kernel_traits<224, 64, 64, 4, cutlass::bfloat16_t>>, Is_dropout=false, Is_causal=false, Is_even_N=false, Is_even_K=true, Return_softmax=false, Params=Flash_fwd_params]" 
(588): here
            instantiation of "void flash::compute_attn<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &) [with Kernel_traits=Flash_fwd_kernel_traits<224, 64, 64, 4, false, false, cutlass::bfloat16_t, Flash_kernel_traits<224, 64, 64, 4, cutlass::bfloat16_t>>, Is_dropout=false, Is_causal=false, Is_even_N=false, Is_even_K=true, Return_softmax=false, Params=Flash_fwd_params]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(15): here
            instantiation of "void flash_fwd_kernel<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax>(Flash_fwd_params) [with Kernel_traits=Flash_fwd_kernel_traits<224, 64, 64, 4, false, false, cutlass::bfloat16_t, Flash_kernel_traits<224, 64, 64, 4, cutlass::bfloat16_t>>, Is_dropout=false, Is_causal=false, Is_even_N=false, Is_even_K=true, Return_softmax=false]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(34): here
            instantiation of "void run_flash_fwd<Kernel_traits,Is_dropout,Is_causal>(Flash_fwd_params &, cudaStream_t) [with Kernel_traits=Flash_fwd_kernel_traits<224, 64, 64, 4, false, false, cutlass::bfloat16_t, Flash_kernel_traits<224, 64, 64, 4, cutlass::bfloat16_t>>, Is_dropout=false, Is_causal=false]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(208): here
            instantiation of "void run_mha_fwd_hdim224<T>(Flash_fwd_params &, cudaStream_t) [with T=cutlass::bfloat16_t]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_hdim224_bf16_sm80.cu(8): here

/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_kernel.h(275): warning: variable "tQrQ" was declared but never referenced
          detected during:
            instantiation of "void flash::compute_attn_1rowblock<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &, int, int, int) [with Kernel_traits=Flash_fwd_kernel_traits<224, 64, 64, 4, false, false, cutlass::bfloat16_t, Flash_kernel_traits<224, 64, 64, 4, cutlass::bfloat16_t>>, Is_dropout=false, Is_causal=false, Is_even_N=false, Is_even_K=true, Return_softmax=false, Params=Flash_fwd_params]" 
(588): here
            instantiation of "void flash::compute_attn<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &) [with Kernel_traits=Flash_fwd_kernel_traits<224, 64, 64, 4, false, false, cutlass::bfloat16_t, Flash_kernel_traits<224, 64, 64, 4, cutlass::bfloat16_t>>, Is_dropout=false, Is_causal=false, Is_even_N=false, Is_even_K=true, Return_softmax=false, Params=Flash_fwd_params]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(15): here
            instantiation of "void flash_fwd_kernel<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax>(Flash_fwd_params) [with Kernel_traits=Flash_fwd_kernel_traits<224, 64, 64, 4, false, false, cutlass::bfloat16_t, Flash_kernel_traits<224, 64, 64, 4, cutlass::bfloat16_t>>, Is_dropout=false, Is_causal=false, Is_even_N=false, Is_even_K=true, Return_softmax=false]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(34): here
            instantiation of "void run_flash_fwd<Kernel_traits,Is_dropout,Is_causal>(Flash_fwd_params &, cudaStream_t) [with Kernel_traits=Flash_fwd_kernel_traits<224, 64, 64, 4, false, false, cutlass::bfloat16_t, Flash_kernel_traits<224, 64, 64, 4, cutlass::bfloat16_t>>, Is_dropout=false, Is_causal=false]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(208): here
            instantiation of "void run_mha_fwd_hdim224<T>(Flash_fwd_params &, cudaStream_t) [with T=cutlass::bfloat16_t]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_hdim224_bf16_sm80.cu(8): here

/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_kernel.h(140): warning: variable "MMA_M" was declared but never referenced
          detected during:
            instantiation of "void flash::compute_attn_1rowblock<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &, int, int, int) [with Kernel_traits=Flash_fwd_kernel_traits<224, 64, 64, 4, false, false, cutlass::bfloat16_t, Flash_kernel_traits<224, 64, 64, 4, cutlass::bfloat16_t>>, Is_dropout=false, Is_causal=false, Is_even_N=false, Is_even_K=false, Return_softmax=false, Params=Flash_fwd_params]" 
(588): here
            instantiation of "void flash::compute_attn<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &) [with Kernel_traits=Flash_fwd_kernel_traits<224, 64, 64, 4, false, false, cutlass::bfloat16_t, Flash_kernel_traits<224, 64, 64, 4, cutlass::bfloat16_t>>, Is_dropout=false, Is_causal=false, Is_even_N=false, Is_even_K=false, Return_softmax=false, Params=Flash_fwd_params]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(15): here
            instantiation of "void flash_fwd_kernel<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax>(Flash_fwd_params) [with Kernel_traits=Flash_fwd_kernel_traits<224, 64, 64, 4, false, false, cutlass::bfloat16_t, Flash_kernel_traits<224, 64, 64, 4, cutlass::bfloat16_t>>, Is_dropout=false, Is_causal=false, Is_even_N=false, Is_even_K=false, Return_softmax=false]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(34): here
            instantiation of "void run_flash_fwd<Kernel_traits,Is_dropout,Is_causal>(Flash_fwd_params &, cudaStream_t) [with Kernel_traits=Flash_fwd_kernel_traits<224, 64, 64, 4, false, false, cutlass::bfloat16_t, Flash_kernel_traits<224, 64, 64, 4, cutlass::bfloat16_t>>, Is_dropout=false, Is_causal=false]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(208): here
            instantiation of "void run_mha_fwd_hdim224<T>(Flash_fwd_params &, cudaStream_t) [with T=cutlass::bfloat16_t]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_hdim224_bf16_sm80.cu(8): here

/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_kernel.h(275): warning: variable "tQrQ" was declared but never referenced
          detected during:
            instantiation of "void flash::compute_attn_1rowblock<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &, int, int, int) [with Kernel_traits=Flash_fwd_kernel_traits<224, 64, 64, 4, false, false, cutlass::bfloat16_t, Flash_kernel_traits<224, 64, 64, 4, cutlass::bfloat16_t>>, Is_dropout=false, Is_causal=false, Is_even_N=false, Is_even_K=false, Return_softmax=false, Params=Flash_fwd_params]" 
(588): here
            instantiation of "void flash::compute_attn<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &) [with Kernel_traits=Flash_fwd_kernel_traits<224, 64, 64, 4, false, false, cutlass::bfloat16_t, Flash_kernel_traits<224, 64, 64, 4, cutlass::bfloat16_t>>, Is_dropout=false, Is_causal=false, Is_even_N=false, Is_even_K=false, Return_softmax=false, Params=Flash_fwd_params]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(15): here
            instantiation of "void flash_fwd_kernel<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax>(Flash_fwd_params) [with Kernel_traits=Flash_fwd_kernel_traits<224, 64, 64, 4, false, false, cutlass::bfloat16_t, Flash_kernel_traits<224, 64, 64, 4, cutlass::bfloat16_t>>, Is_dropout=false, Is_causal=false, Is_even_N=false, Is_even_K=false, Return_softmax=false]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(34): here
            instantiation of "void run_flash_fwd<Kernel_traits,Is_dropout,Is_causal>(Flash_fwd_params &, cudaStream_t) [with Kernel_traits=Flash_fwd_kernel_traits<224, 64, 64, 4, false, false, cutlass::bfloat16_t, Flash_kernel_traits<224, 64, 64, 4, cutlass::bfloat16_t>>, Is_dropout=false, Is_causal=false]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(208): here
            instantiation of "void run_mha_fwd_hdim224<T>(Flash_fwd_params &, cudaStream_t) [with T=cutlass::bfloat16_t]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_hdim224_bf16_sm80.cu(8): here

ptxas info    : 2 bytes gmem
ptxas info    : Compiling entry function '_Z16flash_fwd_kernelI23Flash_fwd_kernel_traitsILi224ELi64ELi64ELi4ELb0ELb0EN7cutlass10bfloat16_tE19Flash_kernel_traitsILi224ELi64ELi64ELi4ES2_EELb0ELb0ELb0ELb0ELb0EEv16Flash_fwd_params' for 'sm_80'
ptxas info    : Function properties for _Z16flash_fwd_kernelI23Flash_fwd_kernel_traitsILi224ELi64ELi64ELi4ELb0ELb0EN7cutlass10bfloat16_tE19Flash_kernel_traitsILi224ELi64ELi64ELi4ES2_EELb0ELb0ELb0ELb0ELb0EEv16Flash_fwd_params
    0 bytes stack frame, 0 bytes spill stores, 0 bytes spill loads
ptxas info    : Used 218 registers, 584 bytes cmem[0]
ptxas info    : Compiling entry function '_Z16flash_fwd_kernelI23Flash_fwd_kernel_traitsILi224ELi64ELi64ELi4ELb0ELb0EN7cutlass10bfloat16_tE19Flash_kernel_traitsILi224ELi64ELi64ELi4ES2_EELb0ELb0ELb0ELb1ELb0EEv16Flash_fwd_params' for 'sm_80'
ptxas info    : Function properties for _Z16flash_fwd_kernelI23Flash_fwd_kernel_traitsILi224ELi64ELi64ELi4ELb0ELb0EN7cutlass10bfloat16_tE19Flash_kernel_traitsILi224ELi64ELi64ELi4ES2_EELb0ELb0ELb0ELb1ELb0EEv16Flash_fwd_params
    0 bytes stack frame, 0 bytes spill stores, 0 bytes spill loads
ptxas info    : Used 211 registers, 584 bytes cmem[0]
ptxas info    : Compiling entry function '_Z16flash_fwd_kernelI23Flash_fwd_kernel_traitsILi224ELi64ELi64ELi4ELb0ELb0EN7cutlass10bfloat16_tE19Flash_kernel_traitsILi224ELi64ELi64ELi4ES2_EELb0ELb0ELb1ELb0ELb0EEv16Flash_fwd_params' for 'sm_80'
ptxas info    : Function properties for _Z16flash_fwd_kernelI23Flash_fwd_kernel_traitsILi224ELi64ELi64ELi4ELb0ELb0EN7cutlass10bfloat16_tE19Flash_kernel_traitsILi224ELi64ELi64ELi4ES2_EELb0ELb0ELb1ELb0ELb0EEv16Flash_fwd_params
    0 bytes stack frame, 0 bytes spill stores, 0 bytes spill loads
ptxas info    : Used 218 registers, 584 bytes cmem[0]
ptxas info    : Compiling entry function '_Z16flash_fwd_kernelI23Flash_fwd_kernel_traitsILi224ELi64ELi64ELi4ELb0ELb0EN7cutlass10bfloat16_tE19Flash_kernel_traitsILi224ELi64ELi64ELi4ES2_EELb0ELb0ELb1ELb1ELb0EEv16Flash_fwd_params' for 'sm_80'
ptxas info    : Function properties for _Z16flash_fwd_kernelI23Flash_fwd_kernel_traitsILi224ELi64ELi64ELi4ELb0ELb0EN7cutlass10bfloat16_tE19Flash_kernel_traitsILi224ELi64ELi64ELi4ES2_EELb0ELb0ELb1ELb1ELb0EEv16Flash_fwd_params
    0 bytes stack frame, 0 bytes spill stores, 0 bytes spill loads
ptxas info    : Used 211 registers, 584 bytes cmem[0]
ptxas info    : Compiling entry function '_Z16flash_fwd_kernelI23Flash_fwd_kernel_traitsILi224ELi128ELi64ELi8ELb0ELb0EN7cutlass10bfloat16_tE19Flash_kernel_traitsILi224ELi128ELi64ELi8ES2_EELb0ELb0ELb0ELb0ELb0EEv16Flash_fwd_params' for 'sm_80'
ptxas info    : Function properties for _Z16flash_fwd_kernelI23Flash_fwd_kernel_traitsILi224ELi128ELi64ELi8ELb0ELb0EN7cutlass10bfloat16_tE19Flash_kernel_traitsILi224ELi128ELi64ELi8ES2_EELb0ELb0ELb0ELb0ELb0EEv16Flash_fwd_params
    0 bytes stack frame, 0 bytes spill stores, 0 bytes spill loads
ptxas info    : Used 221 registers, 584 bytes cmem[0]
ptxas info    : Compiling entry function '_Z16flash_fwd_kernelI23Flash_fwd_kernel_traitsILi224ELi128ELi64ELi8ELb0ELb0EN7cutlass10bfloat16_tE19Flash_kernel_traitsILi224ELi128ELi64ELi8ES2_EELb0ELb0ELb0ELb1ELb0EEv16Flash_fwd_params' for 'sm_80'
ptxas info    : Function properties for _Z16flash_fwd_kernelI23Flash_fwd_kernel_traitsILi224ELi128ELi64ELi8ELb0ELb0EN7cutlass10bfloat16_tE19Flash_kernel_traitsILi224ELi128ELi64ELi8ES2_EELb0ELb0ELb0ELb1ELb0EEv16Flash_fwd_params
    0 bytes stack frame, 0 bytes spill stores, 0 bytes spill loads
ptxas info    : Used 214 registers, 584 bytes cmem[0]
ptxas info    : Compiling entry function '_Z16flash_fwd_kernelI23Flash_fwd_kernel_traitsILi224ELi128ELi64ELi8ELb0ELb0EN7cutlass10bfloat16_tE19Flash_kernel_traitsILi224ELi128ELi64ELi8ES2_EELb0ELb0ELb1ELb0ELb0EEv16Flash_fwd_params' for 'sm_80'
ptxas info    : Function properties for _Z16flash_fwd_kernelI23Flash_fwd_kernel_traitsILi224ELi128ELi64ELi8ELb0ELb0EN7cutlass10bfloat16_tE19Flash_kernel_traitsILi224ELi128ELi64ELi8ES2_EELb0ELb0ELb1ELb0ELb0EEv16Flash_fwd_params
    0 bytes stack frame, 0 bytes spill stores, 0 bytes spill loads
ptxas info    : Used 218 registers, 584 bytes cmem[0]
ptxas info    : Compiling entry function '_Z16flash_fwd_kernelI23Flash_fwd_kernel_traitsILi224ELi128ELi64ELi8ELb0ELb0EN7cutlass10bfloat16_tE19Flash_kernel_traitsILi224ELi128ELi64ELi8ES2_EELb0ELb0ELb1ELb1ELb0EEv16Flash_fwd_params' for 'sm_80'
ptxas info    : Function properties for _Z16flash_fwd_kernelI23Flash_fwd_kernel_traitsILi224ELi128ELi64ELi8ELb0ELb0EN7cutlass10bfloat16_tE19Flash_kernel_traitsILi224ELi128ELi64ELi8ES2_EELb0ELb0ELb1ELb1ELb0EEv16Flash_fwd_params
    0 bytes stack frame, 0 bytes spill stores, 0 bytes spill loads
ptxas info    : Used 212 registers, 584 bytes cmem[0]
ptxas info    : Compiling entry function '_Z16flash_fwd_kernelI23Flash_fwd_kernel_traitsILi224ELi64ELi64ELi4ELb0ELb0EN7cutlass10bfloat16_tE19Flash_kernel_traitsILi224ELi64ELi64ELi4ES2_EELb0ELb1ELb0ELb0ELb0EEv16Flash_fwd_params' for 'sm_80'
ptxas info    : Function properties for _Z16flash_fwd_kernelI23Flash_fwd_kernel_traitsILi224ELi64ELi64ELi4ELb0ELb0EN7cutlass10bfloat16_tE19Flash_kernel_traitsILi224ELi64ELi64ELi4ES2_EELb0ELb1ELb0ELb0ELb0EEv16Flash_fwd_params
    0 bytes stack frame, 0 bytes spill stores, 0 bytes spill loads
ptxas info    : Used 219 registers, 584 bytes cmem[0]
ptxas info    : Compiling entry function '_Z16flash_fwd_kernelI23Flash_fwd_kernel_traitsILi224ELi64ELi64ELi4ELb0ELb0EN7cutlass10bfloat16_tE19Flash_kernel_traitsILi224ELi64ELi64ELi4ES2_EELb0ELb1ELb0ELb1ELb0EEv16Flash_fwd_params' for 'sm_80'
ptxas info    : Function properties for _Z16flash_fwd_kernelI23Flash_fwd_kernel_traitsILi224ELi64ELi64ELi4ELb0ELb0EN7cutlass10bfloat16_tE19Flash_kernel_traitsILi224ELi64ELi64ELi4ES2_EELb0ELb1ELb0ELb1ELb0EEv16Flash_fwd_params
    0 bytes stack frame, 0 bytes spill stores, 0 bytes spill loads
ptxas info    : Used 212 registers, 584 bytes cmem[0]
ptxas info    : Compiling entry function '_Z16flash_fwd_kernelI23Flash_fwd_kernel_traitsILi224ELi64ELi64ELi4ELb0ELb0EN7cutlass10bfloat16_tE19Flash_kernel_traitsILi224ELi64ELi64ELi4ES2_EELb0ELb1ELb1ELb0ELb0EEv16Flash_fwd_params' for 'sm_80'
ptxas info    : Function properties for _Z16flash_fwd_kernelI23Flash_fwd_kernel_traitsILi224ELi64ELi64ELi4ELb0ELb0EN7cutlass10bfloat16_tE19Flash_kernel_traitsILi224ELi64ELi64ELi4ES2_EELb0ELb1ELb1ELb0ELb0EEv16Flash_fwd_params
    0 bytes stack frame, 0 bytes spill stores, 0 bytes spill loads
ptxas info    : Used 230 registers, 584 bytes cmem[0]
ptxas info    : Compiling entry function '_Z16flash_fwd_kernelI23Flash_fwd_kernel_traitsILi224ELi64ELi64ELi4ELb0ELb0EN7cutlass10bfloat16_tE19Flash_kernel_traitsILi224ELi64ELi64ELi4ES2_EELb0ELb1ELb1ELb1ELb0EEv16Flash_fwd_params' for 'sm_80'
ptxas info    : Function properties for _Z16flash_fwd_kernelI23Flash_fwd_kernel_traitsILi224ELi64ELi64ELi4ELb0ELb0EN7cutlass10bfloat16_tE19Flash_kernel_traitsILi224ELi64ELi64ELi4ES2_EELb0ELb1ELb1ELb1ELb0EEv16Flash_fwd_params
    0 bytes stack frame, 0 bytes spill stores, 0 bytes spill loads
ptxas info    : Used 222 registers, 584 bytes cmem[0]
ptxas info    : Compiling entry function '_Z16flash_fwd_kernelI23Flash_fwd_kernel_traitsILi224ELi128ELi64ELi8ELb0ELb0EN7cutlass10bfloat16_tE19Flash_kernel_traitsILi224ELi128ELi64ELi8ES2_EELb0ELb1ELb0ELb0ELb0EEv16Flash_fwd_params' for 'sm_80'
ptxas info    : Function properties for _Z16flash_fwd_kernelI23Flash_fwd_kernel_traitsILi224ELi128ELi64ELi8ELb0ELb0EN7cutlass10bfloat16_tE19Flash_kernel_traitsILi224ELi128ELi64ELi8ES2_EELb0ELb1ELb0ELb0ELb0EEv16Flash_fwd_params
    0 bytes stack frame, 0 bytes spill stores, 0 bytes spill loads
ptxas info    : Used 234 registers, 584 bytes cmem[0]
ptxas info    : Compiling entry function '_Z16flash_fwd_kernelI23Flash_fwd_kernel_traitsILi224ELi128ELi64ELi8ELb0ELb0EN7cutlass10bfloat16_tE19Flash_kernel_traitsILi224ELi128ELi64ELi8ES2_EELb0ELb1ELb0ELb1ELb0EEv16Flash_fwd_params' for 'sm_80'
ptxas info    : Function properties for _Z16flash_fwd_kernelI23Flash_fwd_kernel_traitsILi224ELi128ELi64ELi8ELb0ELb0EN7cutlass10bfloat16_tE19Flash_kernel_traitsILi224ELi128ELi64ELi8ES2_EELb0ELb1ELb0ELb1ELb0EEv16Flash_fwd_params
    0 bytes stack frame, 0 bytes spill stores, 0 bytes spill loads
ptxas info    : Used 226 registers, 584 bytes cmem[0]
ptxas info    : Compiling entry function '_Z16flash_fwd_kernelI23Flash_fwd_kernel_traitsILi224ELi128ELi64ELi8ELb0ELb0EN7cutlass10bfloat16_tE19Flash_kernel_traitsILi224ELi128ELi64ELi8ES2_EELb0ELb1ELb1ELb0ELb0EEv16Flash_fwd_params' for 'sm_80'
ptxas info    : Function properties for _Z16flash_fwd_kernelI23Flash_fwd_kernel_traitsILi224ELi128ELi64ELi8ELb0ELb0EN7cutlass10bfloat16_tE19Flash_kernel_traitsILi224ELi128ELi64ELi8ES2_EELb0ELb1ELb1ELb0ELb0EEv16Flash_fwd_params
    48 bytes stack frame, 80 bytes spill stores, 72 bytes spill loads
ptxas info    : Used 255 registers, 584 bytes cmem[0]
ptxas info    : Compiling entry function '_Z16flash_fwd_kernelI23Flash_fwd_kernel_traitsILi224ELi128ELi64ELi8ELb0ELb0EN7cutlass10bfloat16_tE19Flash_kernel_traitsILi224ELi128ELi64ELi8ES2_EELb0ELb1ELb1ELb1ELb0EEv16Flash_fwd_params' for 'sm_80'
ptxas info    : Function properties for _Z16flash_fwd_kernelI23Flash_fwd_kernel_traitsILi224ELi128ELi64ELi8ELb0ELb0EN7cutlass10bfloat16_tE19Flash_kernel_traitsILi224ELi128ELi64ELi8ES2_EELb0ELb1ELb1ELb1ELb0EEv16Flash_fwd_params
    0 bytes stack frame, 0 bytes spill stores, 0 bytes spill loads
ptxas info    : Used 228 registers, 584 bytes cmem[0]
ptxas info    : Compiling entry function '_Z16flash_fwd_kernelI23Flash_fwd_kernel_traitsILi224ELi64ELi64ELi4ELb0ELb0EN7cutlass10bfloat16_tE19Flash_kernel_traitsILi224ELi64ELi64ELi4ES2_EELb1ELb0ELb0ELb0ELb0EEv16Flash_fwd_params' for 'sm_80'
ptxas info    : Function properties for _Z16flash_fwd_kernelI23Flash_fwd_kernel_traitsILi224ELi64ELi64ELi4ELb0ELb0EN7cutlass10bfloat16_tE19Flash_kernel_traitsILi224ELi64ELi64ELi4ES2_EELb1ELb0ELb0ELb0ELb0EEv16Flash_fwd_params
    0 bytes stack frame, 0 bytes spill stores, 0 bytes spill loads
ptxas info    : Used 230 registers, 584 bytes cmem[0]
ptxas info    : Compiling entry function '_Z16flash_fwd_kernelI23Flash_fwd_kernel_traitsILi224ELi64ELi64ELi4ELb0ELb0EN7cutlass10bfloat16_tE19Flash_kernel_traitsILi224ELi64ELi64ELi4ES2_EELb1ELb0ELb0ELb0ELb1EEv16Flash_fwd_params' for 'sm_80'
ptxas info    : Function properties for _Z16flash_fwd_kernelI23Flash_fwd_kernel_traitsILi224ELi64ELi64ELi4ELb0ELb0EN7cutlass10bfloat16_tE19Flash_kernel_traitsILi224ELi64ELi64ELi4ES2_EELb1ELb0ELb0ELb0ELb1EEv16Flash_fwd_params
    8 bytes stack frame, 8 bytes spill stores, 8 bytes spill loads
ptxas info    : Used 255 registers, 584 bytes cmem[0]
ptxas info    : Compiling entry function '_Z16flash_fwd_kernelI23Flash_fwd_kernel_traitsILi224ELi64ELi64ELi4ELb0ELb0EN7cutlass10bfloat16_tE19Flash_kernel_traitsILi224ELi64ELi64ELi4ES2_EELb1ELb0ELb0ELb1ELb0EEv16Flash_fwd_params' for 'sm_80'
ptxas info    : Function properties for _Z16flash_fwd_kernelI23Flash_fwd_kernel_traitsILi224ELi64ELi64ELi4ELb0ELb0EN7cutlass10bfloat16_tE19Flash_kernel_traitsILi224ELi64ELi64ELi4ES2_EELb1ELb0ELb0ELb1ELb0EEv16Flash_fwd_params
    0 bytes stack frame, 0 bytes spill stores, 0 bytes spill loads
ptxas info    : Used 222 registers, 584 bytes cmem[0]
ptxas info    : Compiling entry function '_Z16flash_fwd_kernelI23Flash_fwd_kernel_traitsILi224ELi64ELi64ELi4ELb0ELb0EN7cutlass10bfloat16_tE19Flash_kernel_traitsILi224ELi64ELi64ELi4ES2_EELb1ELb0ELb0ELb1ELb1EEv16Flash_fwd_params' for 'sm_80'
ptxas info    : Function properties for _Z16flash_fwd_kernelI23Flash_fwd_kernel_traitsILi224ELi64ELi64ELi4ELb0ELb0EN7cutlass10bfloat16_tE19Flash_kernel_traitsILi224ELi64ELi64ELi4ES2_EELb1ELb0ELb0ELb1ELb1EEv16Flash_fwd_params
    0 bytes stack frame, 0 bytes spill stores, 0 bytes spill loads
ptxas info    : Used 248 registers, 584 bytes cmem[0]
ptxas info    : Compiling entry function '_Z16flash_fwd_kernelI23Flash_fwd_kernel_traitsILi224ELi64ELi64ELi4ELb0ELb0EN7cutlass10bfloat16_tE19Flash_kernel_traitsILi224ELi64ELi64ELi4ES2_EELb1ELb0ELb1ELb0ELb0EEv16Flash_fwd_params' for 'sm_80'
ptxas info    : Function properties for _Z16flash_fwd_kernelI23Flash_fwd_kernel_traitsILi224ELi64ELi64ELi4ELb0ELb0EN7cutlass10bfloat16_tE19Flash_kernel_traitsILi224ELi64ELi64ELi4ES2_EELb1ELb0ELb1ELb0ELb0EEv16Flash_fwd_params
    0 bytes stack frame, 0 bytes spill stores, 0 bytes spill loads
ptxas info    : Used 228 registers, 584 bytes cmem[0]
ptxas info    : Compiling entry function '_Z16flash_fwd_kernelI23Flash_fwd_kernel_traitsILi224ELi64ELi64ELi4ELb0ELb0EN7cutlass10bfloat16_tE19Flash_kernel_traitsILi224ELi64ELi64ELi4ES2_EELb1ELb0ELb1ELb0ELb1EEv16Flash_fwd_params' for 'sm_80'
ptxas info    : Function properties for _Z16flash_fwd_kernelI23Flash_fwd_kernel_traitsILi224ELi64ELi64ELi4ELb0ELb0EN7cutlass10bfloat16_tE19Flash_kernel_traitsILi224ELi64ELi64ELi4ES2_EELb1ELb0ELb1ELb0ELb1EEv16Flash_fwd_params
    24 bytes stack frame, 24 bytes spill stores, 24 bytes spill loads
ptxas info    : Used 255 registers, 584 bytes cmem[0]
ptxas info    : Compiling entry function '_Z16flash_fwd_kernelI23Flash_fwd_kernel_traitsILi224ELi64ELi64ELi4ELb0ELb0EN7cutlass10bfloat16_tE19Flash_kernel_traitsILi224ELi64ELi64ELi4ES2_EELb1ELb0ELb1ELb1ELb0EEv16Flash_fwd_params' for 'sm_80'
ptxas info    : Function properties for _Z16flash_fwd_kernelI23Flash_fwd_kernel_traitsILi224ELi64ELi64ELi4ELb0ELb0EN7cutlass10bfloat16_tE19Flash_kernel_traitsILi224ELi64ELi64ELi4ES2_EELb1ELb0ELb1ELb1ELb0EEv16Flash_fwd_params
    0 bytes stack frame, 0 bytes spill stores, 0 bytes spill loads
ptxas info    : Used 222 registers, 584 bytes cmem[0]
ptxas info    : Compiling entry function '_Z16flash_fwd_kernelI23Flash_fwd_kernel_traitsILi224ELi64ELi64ELi4ELb0ELb0EN7cutlass10bfloat16_tE19Flash_kernel_traitsILi224ELi64ELi64ELi4ES2_EELb1ELb0ELb1ELb1ELb1EEv16Flash_fwd_params' for 'sm_80'
ptxas info    : Function properties for _Z16flash_fwd_kernelI23Flash_fwd_kernel_traitsILi224ELi64ELi64ELi4ELb0ELb0EN7cutlass10bfloat16_tE19Flash_kernel_traitsILi224ELi64ELi64ELi4ES2_EELb1ELb0ELb1ELb1ELb1EEv16Flash_fwd_params
    0 bytes stack frame, 0 bytes spill stores, 0 bytes spill loads
ptxas info    : Used 242 registers, 584 bytes cmem[0]
ptxas info    : Compiling entry function '_Z16flash_fwd_kernelI23Flash_fwd_kernel_traitsILi224ELi128ELi64ELi8ELb0ELb0EN7cutlass10bfloat16_tE19Flash_kernel_traitsILi224ELi128ELi64ELi8ES2_EELb1ELb0ELb0ELb0ELb0EEv16Flash_fwd_params' for 'sm_80'
ptxas info    : Function properties for _Z16flash_fwd_kernelI23Flash_fwd_kernel_traitsILi224ELi128ELi64ELi8ELb0ELb0EN7cutlass10bfloat16_tE19Flash_kernel_traitsILi224ELi128ELi64ELi8ES2_EELb1ELb0ELb0ELb0ELb0EEv16Flash_fwd_params
    0 bytes stack frame, 0 bytes spill stores, 0 bytes spill loads
ptxas info    : Used 234 registers, 584 bytes cmem[0]
ptxas info    : Compiling entry function '_Z16flash_fwd_kernelI23Flash_fwd_kernel_traitsILi224ELi128ELi64ELi8ELb0ELb0EN7cutlass10bfloat16_tE19Flash_kernel_traitsILi224ELi128ELi64ELi8ES2_EELb1ELb0ELb0ELb0ELb1EEv16Flash_fwd_params' for 'sm_80'
ptxas info    : Function properties for _Z16flash_fwd_kernelI23Flash_fwd_kernel_traitsILi224ELi128ELi64ELi8ELb0ELb0EN7cutlass10bfloat16_tE19Flash_kernel_traitsILi224ELi128ELi64ELi8ES2_EELb1ELb0ELb0ELb0ELb1EEv16Flash_fwd_params
    8 bytes stack frame, 8 bytes spill stores, 8 bytes spill loads
ptxas info    : Used 255 registers, 584 bytes cmem[0]
ptxas info    : Compiling entry function '_Z16flash_fwd_kernelI23Flash_fwd_kernel_traitsILi224ELi128ELi64ELi8ELb0ELb0EN7cutlass10bfloat16_tE19Flash_kernel_traitsILi224ELi128ELi64ELi8ES2_EELb1ELb0ELb0ELb1ELb0EEv16Flash_fwd_params' for 'sm_80'
ptxas info    : Function properties for _Z16flash_fwd_kernelI23Flash_fwd_kernel_traitsILi224ELi128ELi64ELi8ELb0ELb0EN7cutlass10bfloat16_tE19Flash_kernel_traitsILi224ELi128ELi64ELi8ES2_EELb1ELb0ELb0ELb1ELb0EEv16Flash_fwd_params
    0 bytes stack frame, 0 bytes spill stores, 0 bytes spill loads
ptxas info    : Used 226 registers, 584 bytes cmem[0]
ptxas info    : Compiling entry function '_Z16flash_fwd_kernelI23Flash_fwd_kernel_traitsILi224ELi128ELi64ELi8ELb0ELb0EN7cutlass10bfloat16_tE19Flash_kernel_traitsILi224ELi128ELi64ELi8ES2_EELb1ELb0ELb0ELb1ELb1EEv16Flash_fwd_params' for 'sm_80'
ptxas info    : Function properties for _Z16flash_fwd_kernelI23Flash_fwd_kernel_traitsILi224ELi128ELi64ELi8ELb0ELb0EN7cutlass10bfloat16_tE19Flash_kernel_traitsILi224ELi128ELi64ELi8ES2_EELb1ELb0ELb0ELb1ELb1EEv16Flash_fwd_params
    0 bytes stack frame, 0 bytes spill stores, 0 bytes spill loads
ptxas info    : Used 248 registers, 584 bytes cmem[0]
ptxas info    : Compiling entry function '_Z16flash_fwd_kernelI23Flash_fwd_kernel_traitsILi224ELi128ELi64ELi8ELb0ELb0EN7cutlass10bfloat16_tE19Flash_kernel_traitsILi224ELi128ELi64ELi8ES2_EELb1ELb0ELb1ELb0ELb0EEv16Flash_fwd_params' for 'sm_80'
ptxas info    : Function properties for _Z16flash_fwd_kernelI23Flash_fwd_kernel_traitsILi224ELi128ELi64ELi8ELb0ELb0EN7cutlass10bfloat16_tE19Flash_kernel_traitsILi224ELi128ELi64ELi8ES2_EELb1ELb0ELb1ELb0ELb0EEv16Flash_fwd_params
    0 bytes stack frame, 0 bytes spill stores, 0 bytes spill loads
ptxas info    : Used 234 registers, 584 bytes cmem[0]
ptxas info    : Compiling entry function '_Z16flash_fwd_kernelI23Flash_fwd_kernel_traitsILi224ELi128ELi64ELi8ELb0ELb0EN7cutlass10bfloat16_tE19Flash_kernel_traitsILi224ELi128ELi64ELi8ES2_EELb1ELb0ELb1ELb0ELb1EEv16Flash_fwd_params' for 'sm_80'
ptxas info    : Function properties for _Z16flash_fwd_kernelI23Flash_fwd_kernel_traitsILi224ELi128ELi64ELi8ELb0ELb0EN7cutlass10bfloat16_tE19Flash_kernel_traitsILi224ELi128ELi64ELi8ES2_EELb1ELb0ELb1ELb0ELb1EEv16Flash_fwd_params
    24 bytes stack frame, 24 bytes spill stores, 20 bytes spill loads
ptxas info    : Used 255 registers, 584 bytes cmem[0]
ptxas info    : Compiling entry function '_Z16flash_fwd_kernelI23Flash_fwd_kernel_traitsILi224ELi128ELi64ELi8ELb0ELb0EN7cutlass10bfloat16_tE19Flash_kernel_traitsILi224ELi128ELi64ELi8ES2_EELb1ELb0ELb1ELb1ELb0EEv16Flash_fwd_params' for 'sm_80'
ptxas info    : Function properties for _Z16flash_fwd_kernelI23Flash_fwd_kernel_traitsILi224ELi128ELi64ELi8ELb0ELb0EN7cutlass10bfloat16_tE19Flash_kernel_traitsILi224ELi128ELi64ELi8ES2_EELb1ELb0ELb1ELb1ELb0EEv16Flash_fwd_params
    0 bytes stack frame, 0 bytes spill stores, 0 bytes spill loads
ptxas info    : Used 222 registers, 584 bytes cmem[0]
ptxas info    : Compiling entry function '_Z16flash_fwd_kernelI23Flash_fwd_kernel_traitsILi224ELi128ELi64ELi8ELb0ELb0EN7cutlass10bfloat16_tE19Flash_kernel_traitsILi224ELi128ELi64ELi8ES2_EELb1ELb0ELb1ELb1ELb1EEv16Flash_fwd_params' for 'sm_80'
ptxas info    : Function properties for _Z16flash_fwd_kernelI23Flash_fwd_kernel_traitsILi224ELi128ELi64ELi8ELb0ELb0EN7cutlass10bfloat16_tE19Flash_kernel_traitsILi224ELi128ELi64ELi8ES2_EELb1ELb0ELb1ELb1ELb1EEv16Flash_fwd_params
    0 bytes stack frame, 0 bytes spill stores, 0 bytes spill loads
ptxas info    : Used 242 registers, 584 bytes cmem[0]
ptxas info    : Compiling entry function '_Z16flash_fwd_kernelI23Flash_fwd_kernel_traitsILi224ELi64ELi64ELi4ELb0ELb0EN7cutlass10bfloat16_tE19Flash_kernel_traitsILi224ELi64ELi64ELi4ES2_EELb1ELb1ELb0ELb0ELb0EEv16Flash_fwd_params' for 'sm_80'
ptxas info    : Function properties for _Z16flash_fwd_kernelI23Flash_fwd_kernel_traitsILi224ELi64ELi64ELi4ELb0ELb0EN7cutlass10bfloat16_tE19Flash_kernel_traitsILi224ELi64ELi64ELi4ES2_EELb1ELb1ELb0ELb0ELb0EEv16Flash_fwd_params
    0 bytes stack frame, 0 bytes spill stores, 0 bytes spill loads
ptxas info    : Used 232 registers, 584 bytes cmem[0]
ptxas info    : Compiling entry function '_Z16flash_fwd_kernelI23Flash_fwd_kernel_traitsILi224ELi64ELi64ELi4ELb0ELb0EN7cutlass10bfloat16_tE19Flash_kernel_traitsILi224ELi64ELi64ELi4ES2_EELb1ELb1ELb0ELb0ELb1EEv16Flash_fwd_params' for 'sm_80'
ptxas info    : Function properties for _Z16flash_fwd_kernelI23Flash_fwd_kernel_traitsILi224ELi64ELi64ELi4ELb0ELb0EN7cutlass10bfloat16_tE19Flash_kernel_traitsILi224ELi64ELi64ELi4ES2_EELb1ELb1ELb0ELb0ELb1EEv16Flash_fwd_params
    8 bytes stack frame, 8 bytes spill stores, 8 bytes spill loads
ptxas info    : Used 255 registers, 584 bytes cmem[0]
ptxas info    : Compiling entry function '_Z16flash_fwd_kernelI23Flash_fwd_kernel_traitsILi224ELi64ELi64ELi4ELb0ELb0EN7cutlass10bfloat16_tE19Flash_kernel_traitsILi224ELi64ELi64ELi4ES2_EELb1ELb1ELb0ELb1ELb0EEv16Flash_fwd_params' for 'sm_80'
ptxas info    : Function properties for _Z16flash_fwd_kernelI23Flash_fwd_kernel_traitsILi224ELi64ELi64ELi4ELb0ELb0EN7cutlass10bfloat16_tE19Flash_kernel_traitsILi224ELi64ELi64ELi4ES2_EELb1ELb1ELb0ELb1ELb0EEv16Flash_fwd_params
    0 bytes stack frame, 0 bytes spill stores, 0 bytes spill loads
ptxas info    : Used 222 registers, 584 bytes cmem[0]
ptxas info    : Compiling entry function '_Z16flash_fwd_kernelI23Flash_fwd_kernel_traitsILi224ELi64ELi64ELi4ELb0ELb0EN7cutlass10bfloat16_tE19Flash_kernel_traitsILi224ELi64ELi64ELi4ES2_EELb1ELb1ELb0ELb1ELb1EEv16Flash_fwd_params' for 'sm_80'
ptxas info    : Function properties for _Z16flash_fwd_kernelI23Flash_fwd_kernel_traitsILi224ELi64ELi64ELi4ELb0ELb0EN7cutlass10bfloat16_tE19Flash_kernel_traitsILi224ELi64ELi64ELi4ES2_EELb1ELb1ELb0ELb1ELb1EEv16Flash_fwd_params
    0 bytes stack frame, 0 bytes spill stores, 0 bytes spill loads
ptxas info    : Used 248 registers, 584 bytes cmem[0]
ptxas info    : Compiling entry function '_Z16flash_fwd_kernelI23Flash_fwd_kernel_traitsILi224ELi64ELi64ELi4ELb0ELb0EN7cutlass10bfloat16_tE19Flash_kernel_traitsILi224ELi64ELi64ELi4ES2_EELb1ELb1ELb1ELb0ELb0EEv16Flash_fwd_params' for 'sm_80'
ptxas info    : Function properties for _Z16flash_fwd_kernelI23Flash_fwd_kernel_traitsILi224ELi64ELi64ELi4ELb0ELb0EN7cutlass10bfloat16_tE19Flash_kernel_traitsILi224ELi64ELi64ELi4ES2_EELb1ELb1ELb1ELb0ELb0EEv16Flash_fwd_params
    0 bytes stack frame, 0 bytes spill stores, 0 bytes spill loads
ptxas info    : Used 238 registers, 584 bytes cmem[0]
ptxas info    : Compiling entry function '_Z16flash_fwd_kernelI23Flash_fwd_kernel_traitsILi224ELi64ELi64ELi4ELb0ELb0EN7cutlass10bfloat16_tE19Flash_kernel_traitsILi224ELi64ELi64ELi4ES2_EELb1ELb1ELb1ELb0ELb1EEv16Flash_fwd_params' for 'sm_80'
ptxas info    : Function properties for _Z16flash_fwd_kernelI23Flash_fwd_kernel_traitsILi224ELi64ELi64ELi4ELb0ELb0EN7cutlass10bfloat16_tE19Flash_kernel_traitsILi224ELi64ELi64ELi4ES2_EELb1ELb1ELb1ELb0ELb1EEv16Flash_fwd_params
    24 bytes stack frame, 20 bytes spill stores, 20 bytes spill loads
ptxas info    : Used 255 registers, 584 bytes cmem[0]
ptxas info    : Compiling entry function '_Z16flash_fwd_kernelI23Flash_fwd_kernel_traitsILi224ELi64ELi64ELi4ELb0ELb0EN7cutlass10bfloat16_tE19Flash_kernel_traitsILi224ELi64ELi64ELi4ES2_EELb1ELb1ELb1ELb1ELb0EEv16Flash_fwd_params' for 'sm_80'
ptxas info    : Function properties for _Z16flash_fwd_kernelI23Flash_fwd_kernel_traitsILi224ELi64ELi64ELi4ELb0ELb0EN7cutlass10bfloat16_tE19Flash_kernel_traitsILi224ELi64ELi64ELi4ES2_EELb1ELb1ELb1ELb1ELb0EEv16Flash_fwd_params
    0 bytes stack frame, 0 bytes spill stores, 0 bytes spill loads
ptxas info    : Used 232 registers, 584 bytes cmem[0]
ptxas info    : Compiling entry function '_Z16flash_fwd_kernelI23Flash_fwd_kernel_traitsILi224ELi64ELi64ELi4ELb0ELb0EN7cutlass10bfloat16_tE19Flash_kernel_traitsILi224ELi64ELi64ELi4ES2_EELb1ELb1ELb1ELb1ELb1EEv16Flash_fwd_params' for 'sm_80'
ptxas info    : Function properties for _Z16flash_fwd_kernelI23Flash_fwd_kernel_traitsILi224ELi64ELi64ELi4ELb0ELb0EN7cutlass10bfloat16_tE19Flash_kernel_traitsILi224ELi64ELi64ELi4ES2_EELb1ELb1ELb1ELb1ELb1EEv16Flash_fwd_params
    0 bytes stack frame, 0 bytes spill stores, 0 bytes spill loads
ptxas info    : Used 254 registers, 584 bytes cmem[0]
ptxas info    : Compiling entry function '_Z16flash_fwd_kernelI23Flash_fwd_kernel_traitsILi224ELi128ELi64ELi8ELb0ELb0EN7cutlass10bfloat16_tE19Flash_kernel_traitsILi224ELi128ELi64ELi8ES2_EELb1ELb1ELb0ELb0ELb0EEv16Flash_fwd_params' for 'sm_80'
ptxas info    : Function properties for _Z16flash_fwd_kernelI23Flash_fwd_kernel_traitsILi224ELi128ELi64ELi8ELb0ELb0EN7cutlass10bfloat16_tE19Flash_kernel_traitsILi224ELi128ELi64ELi8ES2_EELb1ELb1ELb0ELb0ELb0EEv16Flash_fwd_params
    72 bytes stack frame, 72 bytes spill stores, 72 bytes spill loads
ptxas info    : Used 255 registers, 584 bytes cmem[0]
ptxas info    : Compiling entry function '_Z16flash_fwd_kernelI23Flash_fwd_kernel_traitsILi224ELi128ELi64ELi8ELb0ELb0EN7cutlass10bfloat16_tE19Flash_kernel_traitsILi224ELi128ELi64ELi8ES2_EELb1ELb1ELb0ELb0ELb1EEv16Flash_fwd_params' for 'sm_80'
ptxas info    : Function properties for _Z16flash_fwd_kernelI23Flash_fwd_kernel_traitsILi224ELi128ELi64ELi8ELb0ELb0EN7cutlass10bfloat16_tE19Flash_kernel_traitsILi224ELi128ELi64ELi8ES2_EELb1ELb1ELb0ELb0ELb1EEv16Flash_fwd_params
    0 bytes stack frame, 0 bytes spill stores, 0 bytes spill loads
ptxas info    : Used 255 registers, 584 bytes cmem[0]
ptxas info    : Compiling entry function '_Z16flash_fwd_kernelI23Flash_fwd_kernel_traitsILi224ELi128ELi64ELi8ELb0ELb0EN7cutlass10bfloat16_tE19Flash_kernel_traitsILi224ELi128ELi64ELi8ES2_EELb1ELb1ELb0ELb1ELb0EEv16Flash_fwd_params' for 'sm_80'
ptxas info    : Function properties for _Z16flash_fwd_kernelI23Flash_fwd_kernel_traitsILi224ELi128ELi64ELi8ELb0ELb0EN7cutlass10bfloat16_tE19Flash_kernel_traitsILi224ELi128ELi64ELi8ES2_EELb1ELb1ELb0ELb1ELb0EEv16Flash_fwd_params
    0 bytes stack frame, 0 bytes spill stores, 0 bytes spill loads
ptxas info    : Used 251 registers, 584 bytes cmem[0]
ptxas info    : Compiling entry function '_Z16flash_fwd_kernelI23Flash_fwd_kernel_traitsILi224ELi128ELi64ELi8ELb0ELb0EN7cutlass10bfloat16_tE19Flash_kernel_traitsILi224ELi128ELi64ELi8ES2_EELb1ELb1ELb0ELb1ELb1EEv16Flash_fwd_params' for 'sm_80'
ptxas info    : Function properties for _Z16flash_fwd_kernelI23Flash_fwd_kernel_traitsILi224ELi128ELi64ELi8ELb0ELb0EN7cutlass10bfloat16_tE19Flash_kernel_traitsILi224ELi128ELi64ELi8ES2_EELb1ELb1ELb0ELb1ELb1EEv16Flash_fwd_params
    0 bytes stack frame, 0 bytes spill stores, 0 bytes spill loads
ptxas info    : Used 252 registers, 584 bytes cmem[0]
ptxas info    : Compiling entry function '_Z16flash_fwd_kernelI23Flash_fwd_kernel_traitsILi224ELi128ELi64ELi8ELb0ELb0EN7cutlass10bfloat16_tE19Flash_kernel_traitsILi224ELi128ELi64ELi8ES2_EELb1ELb1ELb1ELb0ELb0EEv16Flash_fwd_params' for 'sm_80'
ptxas info    : Function properties for _Z16flash_fwd_kernelI23Flash_fwd_kernel_traitsILi224ELi128ELi64ELi8ELb0ELb0EN7cutlass10bfloat16_tE19Flash_kernel_traitsILi224ELi128ELi64ELi8ES2_EELb1ELb1ELb1ELb0ELb0EEv16Flash_fwd_params
    72 bytes stack frame, 84 bytes spill stores, 84 bytes spill loads
ptxas info    : Used 255 registers, 584 bytes cmem[0]
ptxas info    : Compiling entry function '_Z16flash_fwd_kernelI23Flash_fwd_kernel_traitsILi224ELi128ELi64ELi8ELb0ELb0EN7cutlass10bfloat16_tE19Flash_kernel_traitsILi224ELi128ELi64ELi8ES2_EELb1ELb1ELb1ELb0ELb1EEv16Flash_fwd_params' for 'sm_80'
ptxas info    : Function properties for _Z16flash_fwd_kernelI23Flash_fwd_kernel_traitsILi224ELi128ELi64ELi8ELb0ELb0EN7cutlass10bfloat16_tE19Flash_kernel_traitsILi224ELi128ELi64ELi8ES2_EELb1ELb1ELb1ELb0ELb1EEv16Flash_fwd_params
    72 bytes stack frame, 88 bytes spill stores, 88 bytes spill loads
ptxas info    : Used 255 registers, 584 bytes cmem[0]
ptxas info    : Compiling entry function '_Z16flash_fwd_kernelI23Flash_fwd_kernel_traitsILi224ELi128ELi64ELi8ELb0ELb0EN7cutlass10bfloat16_tE19Flash_kernel_traitsILi224ELi128ELi64ELi8ES2_EELb1ELb1ELb1ELb1ELb0EEv16Flash_fwd_params' for 'sm_80'
ptxas info    : Function properties for _Z16flash_fwd_kernelI23Flash_fwd_kernel_traitsILi224ELi128ELi64ELi8ELb0ELb0EN7cutlass10bfloat16_tE19Flash_kernel_traitsILi224ELi128ELi64ELi8ES2_EELb1ELb1ELb1ELb1ELb0EEv16Flash_fwd_params
    0 bytes stack frame, 0 bytes spill stores, 0 bytes spill loads
ptxas info    : Used 235 registers, 584 bytes cmem[0]
ptxas info    : Compiling entry function '_Z16flash_fwd_kernelI23Flash_fwd_kernel_traitsILi224ELi128ELi64ELi8ELb0ELb0EN7cutlass10bfloat16_tE19Flash_kernel_traitsILi224ELi128ELi64ELi8ES2_EELb1ELb1ELb1ELb1ELb1EEv16Flash_fwd_params' for 'sm_80'
ptxas info    : Function properties for _Z16flash_fwd_kernelI23Flash_fwd_kernel_traitsILi224ELi128ELi64ELi8ELb0ELb0EN7cutlass10bfloat16_tE19Flash_kernel_traitsILi224ELi128ELi64ELi8ES2_EELb1ELb1ELb1ELb1ELb1EEv16Flash_fwd_params
    0 bytes stack frame, 0 bytes spill stores, 0 bytes spill loads
ptxas info    : Used 250 registers, 584 bytes cmem[0]
/anaconda/envs/flashattention/lib/python3.9/site-packages/torch/include/c10/core/SymInt.h(84): warning: integer conversion resulted in a change of sign

[21/36] /usr/local/cuda-11.4/bin/nvcc  -I/home/v-feiychen/flash-attention/csrc/flash_attn -I/home/v-feiychen/flash-attention/csrc/flash_attn/src -I/home/v-feiychen/flash-attention/csrc/cutlass/include -I/anaconda/envs/flashattention/lib/python3.9/site-packages/torch/include -I/anaconda/envs/flashattention/lib/python3.9/site-packages/torch/include/torch/csrc/api/include -I/anaconda/envs/flashattention/lib/python3.9/site-packages/torch/include/TH -I/anaconda/envs/flashattention/lib/python3.9/site-packages/torch/include/THC -I/usr/local/cuda-11.4/include -I/anaconda/envs/flashattention/include/python3.9 -c -c /home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_hdim160_fp16_sm80.cu -o /home/v-feiychen/flash-attention/build/temp.linux-x86_64-cpython-39/csrc/flash_attn/src/flash_fwd_hdim160_fp16_sm80.o -D__CUDA_NO_HALF_OPERATORS__ -D__CUDA_NO_HALF_CONVERSIONS__ -D__CUDA_NO_BFLOAT16_CONVERSIONS__ -D__CUDA_NO_HALF2_OPERATORS__ --expt-relaxed-constexpr --compiler-options ''"'"'-fPIC'"'"'' -O3 -std=c++17 -U__CUDA_NO_HALF_OPERATORS__ -U__CUDA_NO_HALF_CONVERSIONS__ -U__CUDA_NO_HALF2_OPERATORS__ -U__CUDA_NO_BFLOAT16_CONVERSIONS__ --expt-relaxed-constexpr --expt-extended-lambda --use_fast_math --ptxas-options=-v -lineinfo -gencode arch=compute_80,code=sm_80 --threads 4 -DTORCH_API_INCLUDE_EXTENSION_H '-DPYBIND11_COMPILER_TYPE="_gcc"' '-DPYBIND11_STDLIB="_libstdcpp"' '-DPYBIND11_BUILD_ABI="_cxxabi1011"' -DTORCH_EXTENSION_NAME=flash_attn_2_cuda -D_GLIBCXX_USE_CXX11_ABI=0
/anaconda/envs/flashattention/lib/python3.9/site-packages/torch/include/c10/core/SymInt.h(84): warning: integer conversion resulted in a change of sign

/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_kernel.h(140): warning: variable "MMA_M" was declared but never referenced
          detected during:
            instantiation of "void flash::compute_attn_1rowblock<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &, int, int, int) [with Kernel_traits=Flash_fwd_kernel_traits<160, 64, 64, 4, false, false, cutlass::half_t, Flash_kernel_traits<160, 64, 64, 4, cutlass::half_t>>, Is_dropout=true, Is_causal=true, Is_even_N=true, Is_even_K=true, Return_softmax=true, Params=Flash_fwd_params]" 
(588): here
            instantiation of "void flash::compute_attn<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &) [with Kernel_traits=Flash_fwd_kernel_traits<160, 64, 64, 4, false, false, cutlass::half_t, Flash_kernel_traits<160, 64, 64, 4, cutlass::half_t>>, Is_dropout=true, Is_causal=true, Is_even_N=true, Is_even_K=true, Return_softmax=true, Params=Flash_fwd_params]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(15): here
            instantiation of "void flash_fwd_kernel<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax>(Flash_fwd_params) [with Kernel_traits=Flash_fwd_kernel_traits<160, 64, 64, 4, false, false, cutlass::half_t, Flash_kernel_traits<160, 64, 64, 4, cutlass::half_t>>, Is_dropout=true, Is_causal=true, Is_even_N=true, Is_even_K=true, Return_softmax=true]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(34): here
            instantiation of "void run_flash_fwd<Kernel_traits,Is_dropout,Is_causal>(Flash_fwd_params &, cudaStream_t) [with Kernel_traits=Flash_fwd_kernel_traits<160, 64, 64, 4, false, false, cutlass::half_t, Flash_kernel_traits<160, 64, 64, 4, cutlass::half_t>>, Is_dropout=true, Is_causal=true]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(155): here
            instantiation of "void run_mha_fwd_hdim160<T>(Flash_fwd_params &, cudaStream_t) [with T=cutlass::half_t]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_hdim160_fp16_sm80.cu(26): here

/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_kernel.h(275): warning: variable "tQrQ" was declared but never referenced
          detected during:
            instantiation of "void flash::compute_attn_1rowblock<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &, int, int, int) [with Kernel_traits=Flash_fwd_kernel_traits<160, 64, 64, 4, false, false, cutlass::half_t, Flash_kernel_traits<160, 64, 64, 4, cutlass::half_t>>, Is_dropout=true, Is_causal=true, Is_even_N=true, Is_even_K=true, Return_softmax=true, Params=Flash_fwd_params]" 
(588): here
            instantiation of "void flash::compute_attn<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &) [with Kernel_traits=Flash_fwd_kernel_traits<160, 64, 64, 4, false, false, cutlass::half_t, Flash_kernel_traits<160, 64, 64, 4, cutlass::half_t>>, Is_dropout=true, Is_causal=true, Is_even_N=true, Is_even_K=true, Return_softmax=true, Params=Flash_fwd_params]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(15): here
            instantiation of "void flash_fwd_kernel<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax>(Flash_fwd_params) [with Kernel_traits=Flash_fwd_kernel_traits<160, 64, 64, 4, false, false, cutlass::half_t, Flash_kernel_traits<160, 64, 64, 4, cutlass::half_t>>, Is_dropout=true, Is_causal=true, Is_even_N=true, Is_even_K=true, Return_softmax=true]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(34): here
            instantiation of "void run_flash_fwd<Kernel_traits,Is_dropout,Is_causal>(Flash_fwd_params &, cudaStream_t) [with Kernel_traits=Flash_fwd_kernel_traits<160, 64, 64, 4, false, false, cutlass::half_t, Flash_kernel_traits<160, 64, 64, 4, cutlass::half_t>>, Is_dropout=true, Is_causal=true]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(155): here
            instantiation of "void run_mha_fwd_hdim160<T>(Flash_fwd_params &, cudaStream_t) [with T=cutlass::half_t]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_hdim160_fp16_sm80.cu(26): here

/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_kernel.h(140): warning: variable "MMA_M" was declared but never referenced
          detected during:
            instantiation of "void flash::compute_attn_1rowblock<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &, int, int, int) [with Kernel_traits=Flash_fwd_kernel_traits<160, 64, 64, 4, false, false, cutlass::half_t, Flash_kernel_traits<160, 64, 64, 4, cutlass::half_t>>, Is_dropout=true, Is_causal=true, Is_even_N=true, Is_even_K=true, Return_softmax=false, Params=Flash_fwd_params]" 
(588): here
            instantiation of "void flash::compute_attn<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &) [with Kernel_traits=Flash_fwd_kernel_traits<160, 64, 64, 4, false, false, cutlass::half_t, Flash_kernel_traits<160, 64, 64, 4, cutlass::half_t>>, Is_dropout=true, Is_causal=true, Is_even_N=true, Is_even_K=true, Return_softmax=false, Params=Flash_fwd_params]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(15): here
            instantiation of "void flash_fwd_kernel<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax>(Flash_fwd_params) [with Kernel_traits=Flash_fwd_kernel_traits<160, 64, 64, 4, false, false, cutlass::half_t, Flash_kernel_traits<160, 64, 64, 4, cutlass::half_t>>, Is_dropout=true, Is_causal=true, Is_even_N=true, Is_even_K=true, Return_softmax=false]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(34): here
            instantiation of "void run_flash_fwd<Kernel_traits,Is_dropout,Is_causal>(Flash_fwd_params &, cudaStream_t) [with Kernel_traits=Flash_fwd_kernel_traits<160, 64, 64, 4, false, false, cutlass::half_t, Flash_kernel_traits<160, 64, 64, 4, cutlass::half_t>>, Is_dropout=true, Is_causal=true]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(155): here
            instantiation of "void run_mha_fwd_hdim160<T>(Flash_fwd_params &, cudaStream_t) [with T=cutlass::half_t]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_hdim160_fp16_sm80.cu(26): here

/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_kernel.h(275): warning: variable "tQrQ" was declared but never referenced
          detected during:
            instantiation of "void flash::compute_attn_1rowblock<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &, int, int, int) [with Kernel_traits=Flash_fwd_kernel_traits<160, 64, 64, 4, false, false, cutlass::half_t, Flash_kernel_traits<160, 64, 64, 4, cutlass::half_t>>, Is_dropout=true, Is_causal=true, Is_even_N=true, Is_even_K=true, Return_softmax=false, Params=Flash_fwd_params]" 
(588): here
            instantiation of "void flash::compute_attn<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &) [with Kernel_traits=Flash_fwd_kernel_traits<160, 64, 64, 4, false, false, cutlass::half_t, Flash_kernel_traits<160, 64, 64, 4, cutlass::half_t>>, Is_dropout=true, Is_causal=true, Is_even_N=true, Is_even_K=true, Return_softmax=false, Params=Flash_fwd_params]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(15): here
            instantiation of "void flash_fwd_kernel<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax>(Flash_fwd_params) [with Kernel_traits=Flash_fwd_kernel_traits<160, 64, 64, 4, false, false, cutlass::half_t, Flash_kernel_traits<160, 64, 64, 4, cutlass::half_t>>, Is_dropout=true, Is_causal=true, Is_even_N=true, Is_even_K=true, Return_softmax=false]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(34): here
            instantiation of "void run_flash_fwd<Kernel_traits,Is_dropout,Is_causal>(Flash_fwd_params &, cudaStream_t) [with Kernel_traits=Flash_fwd_kernel_traits<160, 64, 64, 4, false, false, cutlass::half_t, Flash_kernel_traits<160, 64, 64, 4, cutlass::half_t>>, Is_dropout=true, Is_causal=true]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(155): here
            instantiation of "void run_mha_fwd_hdim160<T>(Flash_fwd_params &, cudaStream_t) [with T=cutlass::half_t]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_hdim160_fp16_sm80.cu(26): here

/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_kernel.h(140): warning: variable "MMA_M" was declared but never referenced
          detected during:
            instantiation of "void flash::compute_attn_1rowblock<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &, int, int, int) [with Kernel_traits=Flash_fwd_kernel_traits<160, 64, 64, 4, false, false, cutlass::half_t, Flash_kernel_traits<160, 64, 64, 4, cutlass::half_t>>, Is_dropout=true, Is_causal=true, Is_even_N=true, Is_even_K=false, Return_softmax=true, Params=Flash_fwd_params]" 
(588): here
            instantiation of "void flash::compute_attn<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &) [with Kernel_traits=Flash_fwd_kernel_traits<160, 64, 64, 4, false, false, cutlass::half_t, Flash_kernel_traits<160, 64, 64, 4, cutlass::half_t>>, Is_dropout=true, Is_causal=true, Is_even_N=true, Is_even_K=false, Return_softmax=true, Params=Flash_fwd_params]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(15): here
            instantiation of "void flash_fwd_kernel<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax>(Flash_fwd_params) [with Kernel_traits=Flash_fwd_kernel_traits<160, 64, 64, 4, false, false, cutlass::half_t, Flash_kernel_traits<160, 64, 64, 4, cutlass::half_t>>, Is_dropout=true, Is_causal=true, Is_even_N=true, Is_even_K=false, Return_softmax=true]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(34): here
            instantiation of "void run_flash_fwd<Kernel_traits,Is_dropout,Is_causal>(Flash_fwd_params &, cudaStream_t) [with Kernel_traits=Flash_fwd_kernel_traits<160, 64, 64, 4, false, false, cutlass::half_t, Flash_kernel_traits<160, 64, 64, 4, cutlass::half_t>>, Is_dropout=true, Is_causal=true]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(155): here
            instantiation of "void run_mha_fwd_hdim160<T>(Flash_fwd_params &, cudaStream_t) [with T=cutlass::half_t]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_hdim160_fp16_sm80.cu(26): here

/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_kernel.h(275): warning: variable "tQrQ" was declared but never referenced
          detected during:
            instantiation of "void flash::compute_attn_1rowblock<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &, int, int, int) [with Kernel_traits=Flash_fwd_kernel_traits<160, 64, 64, 4, false, false, cutlass::half_t, Flash_kernel_traits<160, 64, 64, 4, cutlass::half_t>>, Is_dropout=true, Is_causal=true, Is_even_N=true, Is_even_K=false, Return_softmax=true, Params=Flash_fwd_params]" 
(588): here
            instantiation of "void flash::compute_attn<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &) [with Kernel_traits=Flash_fwd_kernel_traits<160, 64, 64, 4, false, false, cutlass::half_t, Flash_kernel_traits<160, 64, 64, 4, cutlass::half_t>>, Is_dropout=true, Is_causal=true, Is_even_N=true, Is_even_K=false, Return_softmax=true, Params=Flash_fwd_params]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(15): here
            instantiation of "void flash_fwd_kernel<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax>(Flash_fwd_params) [with Kernel_traits=Flash_fwd_kernel_traits<160, 64, 64, 4, false, false, cutlass::half_t, Flash_kernel_traits<160, 64, 64, 4, cutlass::half_t>>, Is_dropout=true, Is_causal=true, Is_even_N=true, Is_even_K=false, Return_softmax=true]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(34): here
            instantiation of "void run_flash_fwd<Kernel_traits,Is_dropout,Is_causal>(Flash_fwd_params &, cudaStream_t) [with Kernel_traits=Flash_fwd_kernel_traits<160, 64, 64, 4, false, false, cutlass::half_t, Flash_kernel_traits<160, 64, 64, 4, cutlass::half_t>>, Is_dropout=true, Is_causal=true]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(155): here
            instantiation of "void run_mha_fwd_hdim160<T>(Flash_fwd_params &, cudaStream_t) [with T=cutlass::half_t]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_hdim160_fp16_sm80.cu(26): here

/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_kernel.h(140): warning: variable "MMA_M" was declared but never referenced
          detected during:
            instantiation of "void flash::compute_attn_1rowblock<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &, int, int, int) [with Kernel_traits=Flash_fwd_kernel_traits<160, 64, 64, 4, false, false, cutlass::half_t, Flash_kernel_traits<160, 64, 64, 4, cutlass::half_t>>, Is_dropout=true, Is_causal=true, Is_even_N=true, Is_even_K=false, Return_softmax=false, Params=Flash_fwd_params]" 
(588): here
            instantiation of "void flash::compute_attn<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &) [with Kernel_traits=Flash_fwd_kernel_traits<160, 64, 64, 4, false, false, cutlass::half_t, Flash_kernel_traits<160, 64, 64, 4, cutlass::half_t>>, Is_dropout=true, Is_causal=true, Is_even_N=true, Is_even_K=false, Return_softmax=false, Params=Flash_fwd_params]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(15): here
            instantiation of "void flash_fwd_kernel<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax>(Flash_fwd_params) [with Kernel_traits=Flash_fwd_kernel_traits<160, 64, 64, 4, false, false, cutlass::half_t, Flash_kernel_traits<160, 64, 64, 4, cutlass::half_t>>, Is_dropout=true, Is_causal=true, Is_even_N=true, Is_even_K=false, Return_softmax=false]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(34): here
            instantiation of "void run_flash_fwd<Kernel_traits,Is_dropout,Is_causal>(Flash_fwd_params &, cudaStream_t) [with Kernel_traits=Flash_fwd_kernel_traits<160, 64, 64, 4, false, false, cutlass::half_t, Flash_kernel_traits<160, 64, 64, 4, cutlass::half_t>>, Is_dropout=true, Is_causal=true]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(155): here
            instantiation of "void run_mha_fwd_hdim160<T>(Flash_fwd_params &, cudaStream_t) [with T=cutlass::half_t]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_hdim160_fp16_sm80.cu(26): here

/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_kernel.h(275): warning: variable "tQrQ" was declared but never referenced
          detected during:
            instantiation of "void flash::compute_attn_1rowblock<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &, int, int, int) [with Kernel_traits=Flash_fwd_kernel_traits<160, 64, 64, 4, false, false, cutlass::half_t, Flash_kernel_traits<160, 64, 64, 4, cutlass::half_t>>, Is_dropout=true, Is_causal=true, Is_even_N=true, Is_even_K=false, Return_softmax=false, Params=Flash_fwd_params]" 
(588): here
            instantiation of "void flash::compute_attn<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &) [with Kernel_traits=Flash_fwd_kernel_traits<160, 64, 64, 4, false, false, cutlass::half_t, Flash_kernel_traits<160, 64, 64, 4, cutlass::half_t>>, Is_dropout=true, Is_causal=true, Is_even_N=true, Is_even_K=false, Return_softmax=false, Params=Flash_fwd_params]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(15): here
            instantiation of "void flash_fwd_kernel<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax>(Flash_fwd_params) [with Kernel_traits=Flash_fwd_kernel_traits<160, 64, 64, 4, false, false, cutlass::half_t, Flash_kernel_traits<160, 64, 64, 4, cutlass::half_t>>, Is_dropout=true, Is_causal=true, Is_even_N=true, Is_even_K=false, Return_softmax=false]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(34): here
            instantiation of "void run_flash_fwd<Kernel_traits,Is_dropout,Is_causal>(Flash_fwd_params &, cudaStream_t) [with Kernel_traits=Flash_fwd_kernel_traits<160, 64, 64, 4, false, false, cutlass::half_t, Flash_kernel_traits<160, 64, 64, 4, cutlass::half_t>>, Is_dropout=true, Is_causal=true]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(155): here
            instantiation of "void run_mha_fwd_hdim160<T>(Flash_fwd_params &, cudaStream_t) [with T=cutlass::half_t]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_hdim160_fp16_sm80.cu(26): here

/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_kernel.h(140): warning: variable "MMA_M" was declared but never referenced
          detected during:
            instantiation of "void flash::compute_attn_1rowblock<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &, int, int, int) [with Kernel_traits=Flash_fwd_kernel_traits<160, 64, 64, 4, false, false, cutlass::half_t, Flash_kernel_traits<160, 64, 64, 4, cutlass::half_t>>, Is_dropout=true, Is_causal=true, Is_even_N=false, Is_even_K=true, Return_softmax=true, Params=Flash_fwd_params]" 
(588): here
            instantiation of "void flash::compute_attn<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &) [with Kernel_traits=Flash_fwd_kernel_traits<160, 64, 64, 4, false, false, cutlass::half_t, Flash_kernel_traits<160, 64, 64, 4, cutlass::half_t>>, Is_dropout=true, Is_causal=true, Is_even_N=false, Is_even_K=true, Return_softmax=true, Params=Flash_fwd_params]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(15): here
            instantiation of "void flash_fwd_kernel<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax>(Flash_fwd_params) [with Kernel_traits=Flash_fwd_kernel_traits<160, 64, 64, 4, false, false, cutlass::half_t, Flash_kernel_traits<160, 64, 64, 4, cutlass::half_t>>, Is_dropout=true, Is_causal=true, Is_even_N=false, Is_even_K=true, Return_softmax=true]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(34): here
            instantiation of "void run_flash_fwd<Kernel_traits,Is_dropout,Is_causal>(Flash_fwd_params &, cudaStream_t) [with Kernel_traits=Flash_fwd_kernel_traits<160, 64, 64, 4, false, false, cutlass::half_t, Flash_kernel_traits<160, 64, 64, 4, cutlass::half_t>>, Is_dropout=true, Is_causal=true]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(155): here
            instantiation of "void run_mha_fwd_hdim160<T>(Flash_fwd_params &, cudaStream_t) [with T=cutlass::half_t]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_hdim160_fp16_sm80.cu(26): here

/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_kernel.h(275): warning: variable "tQrQ" was declared but never referenced
          detected during:
            instantiation of "void flash::compute_attn_1rowblock<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &, int, int, int) [with Kernel_traits=Flash_fwd_kernel_traits<160, 64, 64, 4, false, false, cutlass::half_t, Flash_kernel_traits<160, 64, 64, 4, cutlass::half_t>>, Is_dropout=true, Is_causal=true, Is_even_N=false, Is_even_K=true, Return_softmax=true, Params=Flash_fwd_params]" 
(588): here
            instantiation of "void flash::compute_attn<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &) [with Kernel_traits=Flash_fwd_kernel_traits<160, 64, 64, 4, false, false, cutlass::half_t, Flash_kernel_traits<160, 64, 64, 4, cutlass::half_t>>, Is_dropout=true, Is_causal=true, Is_even_N=false, Is_even_K=true, Return_softmax=true, Params=Flash_fwd_params]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(15): here
            instantiation of "void flash_fwd_kernel<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax>(Flash_fwd_params) [with Kernel_traits=Flash_fwd_kernel_traits<160, 64, 64, 4, false, false, cutlass::half_t, Flash_kernel_traits<160, 64, 64, 4, cutlass::half_t>>, Is_dropout=true, Is_causal=true, Is_even_N=false, Is_even_K=true, Return_softmax=true]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(34): here
            instantiation of "void run_flash_fwd<Kernel_traits,Is_dropout,Is_causal>(Flash_fwd_params &, cudaStream_t) [with Kernel_traits=Flash_fwd_kernel_traits<160, 64, 64, 4, false, false, cutlass::half_t, Flash_kernel_traits<160, 64, 64, 4, cutlass::half_t>>, Is_dropout=true, Is_causal=true]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(155): here
            instantiation of "void run_mha_fwd_hdim160<T>(Flash_fwd_params &, cudaStream_t) [with T=cutlass::half_t]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_hdim160_fp16_sm80.cu(26): here

/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_kernel.h(140): warning: variable "MMA_M" was declared but never referenced
          detected during:
            instantiation of "void flash::compute_attn_1rowblock<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &, int, int, int) [with Kernel_traits=Flash_fwd_kernel_traits<160, 64, 64, 4, false, false, cutlass::half_t, Flash_kernel_traits<160, 64, 64, 4, cutlass::half_t>>, Is_dropout=true, Is_causal=true, Is_even_N=false, Is_even_K=true, Return_softmax=false, Params=Flash_fwd_params]" 
(588): here
            instantiation of "void flash::compute_attn<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &) [with Kernel_traits=Flash_fwd_kernel_traits<160, 64, 64, 4, false, false, cutlass::half_t, Flash_kernel_traits<160, 64, 64, 4, cutlass::half_t>>, Is_dropout=true, Is_causal=true, Is_even_N=false, Is_even_K=true, Return_softmax=false, Params=Flash_fwd_params]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(15): here
            instantiation of "void flash_fwd_kernel<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax>(Flash_fwd_params) [with Kernel_traits=Flash_fwd_kernel_traits<160, 64, 64, 4, false, false, cutlass::half_t, Flash_kernel_traits<160, 64, 64, 4, cutlass::half_t>>, Is_dropout=true, Is_causal=true, Is_even_N=false, Is_even_K=true, Return_softmax=false]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(34): here
            instantiation of "void run_flash_fwd<Kernel_traits,Is_dropout,Is_causal>(Flash_fwd_params &, cudaStream_t) [with Kernel_traits=Flash_fwd_kernel_traits<160, 64, 64, 4, false, false, cutlass::half_t, Flash_kernel_traits<160, 64, 64, 4, cutlass::half_t>>, Is_dropout=true, Is_causal=true]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(155): here
            instantiation of "void run_mha_fwd_hdim160<T>(Flash_fwd_params &, cudaStream_t) [with T=cutlass::half_t]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_hdim160_fp16_sm80.cu(26): here

/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_kernel.h(275): warning: variable "tQrQ" was declared but never referenced
          detected during:
            instantiation of "void flash::compute_attn_1rowblock<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &, int, int, int) [with Kernel_traits=Flash_fwd_kernel_traits<160, 64, 64, 4, false, false, cutlass::half_t, Flash_kernel_traits<160, 64, 64, 4, cutlass::half_t>>, Is_dropout=true, Is_causal=true, Is_even_N=false, Is_even_K=true, Return_softmax=false, Params=Flash_fwd_params]" 
(588): here
            instantiation of "void flash::compute_attn<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &) [with Kernel_traits=Flash_fwd_kernel_traits<160, 64, 64, 4, false, false, cutlass::half_t, Flash_kernel_traits<160, 64, 64, 4, cutlass::half_t>>, Is_dropout=true, Is_causal=true, Is_even_N=false, Is_even_K=true, Return_softmax=false, Params=Flash_fwd_params]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(15): here
            instantiation of "void flash_fwd_kernel<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax>(Flash_fwd_params) [with Kernel_traits=Flash_fwd_kernel_traits<160, 64, 64, 4, false, false, cutlass::half_t, Flash_kernel_traits<160, 64, 64, 4, cutlass::half_t>>, Is_dropout=true, Is_causal=true, Is_even_N=false, Is_even_K=true, Return_softmax=false]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(34): here
            instantiation of "void run_flash_fwd<Kernel_traits,Is_dropout,Is_causal>(Flash_fwd_params &, cudaStream_t) [with Kernel_traits=Flash_fwd_kernel_traits<160, 64, 64, 4, false, false, cutlass::half_t, Flash_kernel_traits<160, 64, 64, 4, cutlass::half_t>>, Is_dropout=true, Is_causal=true]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(155): here
            instantiation of "void run_mha_fwd_hdim160<T>(Flash_fwd_params &, cudaStream_t) [with T=cutlass::half_t]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_hdim160_fp16_sm80.cu(26): here

/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_kernel.h(140): warning: variable "MMA_M" was declared but never referenced
          detected during:
            instantiation of "void flash::compute_attn_1rowblock<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &, int, int, int) [with Kernel_traits=Flash_fwd_kernel_traits<160, 64, 64, 4, false, false, cutlass::half_t, Flash_kernel_traits<160, 64, 64, 4, cutlass::half_t>>, Is_dropout=true, Is_causal=true, Is_even_N=false, Is_even_K=false, Return_softmax=true, Params=Flash_fwd_params]" 
(588): here
            instantiation of "void flash::compute_attn<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &) [with Kernel_traits=Flash_fwd_kernel_traits<160, 64, 64, 4, false, false, cutlass::half_t, Flash_kernel_traits<160, 64, 64, 4, cutlass::half_t>>, Is_dropout=true, Is_causal=true, Is_even_N=false, Is_even_K=false, Return_softmax=true, Params=Flash_fwd_params]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(15): here
            instantiation of "void flash_fwd_kernel<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax>(Flash_fwd_params) [with Kernel_traits=Flash_fwd_kernel_traits<160, 64, 64, 4, false, false, cutlass::half_t, Flash_kernel_traits<160, 64, 64, 4, cutlass::half_t>>, Is_dropout=true, Is_causal=true, Is_even_N=false, Is_even_K=false, Return_softmax=true]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(34): here
            instantiation of "void run_flash_fwd<Kernel_traits,Is_dropout,Is_causal>(Flash_fwd_params &, cudaStream_t) [with Kernel_traits=Flash_fwd_kernel_traits<160, 64, 64, 4, false, false, cutlass::half_t, Flash_kernel_traits<160, 64, 64, 4, cutlass::half_t>>, Is_dropout=true, Is_causal=true]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(155): here
            instantiation of "void run_mha_fwd_hdim160<T>(Flash_fwd_params &, cudaStream_t) [with T=cutlass::half_t]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_hdim160_fp16_sm80.cu(26): here

/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_kernel.h(275): warning: variable "tQrQ" was declared but never referenced
          detected during:
            instantiation of "void flash::compute_attn_1rowblock<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &, int, int, int) [with Kernel_traits=Flash_fwd_kernel_traits<160, 64, 64, 4, false, false, cutlass::half_t, Flash_kernel_traits<160, 64, 64, 4, cutlass::half_t>>, Is_dropout=true, Is_causal=true, Is_even_N=false, Is_even_K=false, Return_softmax=true, Params=Flash_fwd_params]" 
(588): here
            instantiation of "void flash::compute_attn<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &) [with Kernel_traits=Flash_fwd_kernel_traits<160, 64, 64, 4, false, false, cutlass::half_t, Flash_kernel_traits<160, 64, 64, 4, cutlass::half_t>>, Is_dropout=true, Is_causal=true, Is_even_N=false, Is_even_K=false, Return_softmax=true, Params=Flash_fwd_params]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(15): here
            instantiation of "void flash_fwd_kernel<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax>(Flash_fwd_params) [with Kernel_traits=Flash_fwd_kernel_traits<160, 64, 64, 4, false, false, cutlass::half_t, Flash_kernel_traits<160, 64, 64, 4, cutlass::half_t>>, Is_dropout=true, Is_causal=true, Is_even_N=false, Is_even_K=false, Return_softmax=true]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(34): here
            instantiation of "void run_flash_fwd<Kernel_traits,Is_dropout,Is_causal>(Flash_fwd_params &, cudaStream_t) [with Kernel_traits=Flash_fwd_kernel_traits<160, 64, 64, 4, false, false, cutlass::half_t, Flash_kernel_traits<160, 64, 64, 4, cutlass::half_t>>, Is_dropout=true, Is_causal=true]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(155): here
            instantiation of "void run_mha_fwd_hdim160<T>(Flash_fwd_params &, cudaStream_t) [with T=cutlass::half_t]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_hdim160_fp16_sm80.cu(26): here

/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_kernel.h(140): warning: variable "MMA_M" was declared but never referenced
          detected during:
            instantiation of "void flash::compute_attn_1rowblock<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &, int, int, int) [with Kernel_traits=Flash_fwd_kernel_traits<160, 64, 64, 4, false, false, cutlass::half_t, Flash_kernel_traits<160, 64, 64, 4, cutlass::half_t>>, Is_dropout=true, Is_causal=true, Is_even_N=false, Is_even_K=false, Return_softmax=false, Params=Flash_fwd_params]" 
(588): here
            instantiation of "void flash::compute_attn<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &) [with Kernel_traits=Flash_fwd_kernel_traits<160, 64, 64, 4, false, false, cutlass::half_t, Flash_kernel_traits<160, 64, 64, 4, cutlass::half_t>>, Is_dropout=true, Is_causal=true, Is_even_N=false, Is_even_K=false, Return_softmax=false, Params=Flash_fwd_params]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(15): here
            instantiation of "void flash_fwd_kernel<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax>(Flash_fwd_params) [with Kernel_traits=Flash_fwd_kernel_traits<160, 64, 64, 4, false, false, cutlass::half_t, Flash_kernel_traits<160, 64, 64, 4, cutlass::half_t>>, Is_dropout=true, Is_causal=true, Is_even_N=false, Is_even_K=false, Return_softmax=false]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(34): here
            instantiation of "void run_flash_fwd<Kernel_traits,Is_dropout,Is_causal>(Flash_fwd_params &, cudaStream_t) [with Kernel_traits=Flash_fwd_kernel_traits<160, 64, 64, 4, false, false, cutlass::half_t, Flash_kernel_traits<160, 64, 64, 4, cutlass::half_t>>, Is_dropout=true, Is_causal=true]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(155): here
            instantiation of "void run_mha_fwd_hdim160<T>(Flash_fwd_params &, cudaStream_t) [with T=cutlass::half_t]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_hdim160_fp16_sm80.cu(26): here

/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_kernel.h(275): warning: variable "tQrQ" was declared but never referenced
          detected during:
            instantiation of "void flash::compute_attn_1rowblock<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &, int, int, int) [with Kernel_traits=Flash_fwd_kernel_traits<160, 64, 64, 4, false, false, cutlass::half_t, Flash_kernel_traits<160, 64, 64, 4, cutlass::half_t>>, Is_dropout=true, Is_causal=true, Is_even_N=false, Is_even_K=false, Return_softmax=false, Params=Flash_fwd_params]" 
(588): here
            instantiation of "void flash::compute_attn<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &) [with Kernel_traits=Flash_fwd_kernel_traits<160, 64, 64, 4, false, false, cutlass::half_t, Flash_kernel_traits<160, 64, 64, 4, cutlass::half_t>>, Is_dropout=true, Is_causal=true, Is_even_N=false, Is_even_K=false, Return_softmax=false, Params=Flash_fwd_params]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(15): here
            instantiation of "void flash_fwd_kernel<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax>(Flash_fwd_params) [with Kernel_traits=Flash_fwd_kernel_traits<160, 64, 64, 4, false, false, cutlass::half_t, Flash_kernel_traits<160, 64, 64, 4, cutlass::half_t>>, Is_dropout=true, Is_causal=true, Is_even_N=false, Is_even_K=false, Return_softmax=false]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(34): here
            instantiation of "void run_flash_fwd<Kernel_traits,Is_dropout,Is_causal>(Flash_fwd_params &, cudaStream_t) [with Kernel_traits=Flash_fwd_kernel_traits<160, 64, 64, 4, false, false, cutlass::half_t, Flash_kernel_traits<160, 64, 64, 4, cutlass::half_t>>, Is_dropout=true, Is_causal=true]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(155): here
            instantiation of "void run_mha_fwd_hdim160<T>(Flash_fwd_params &, cudaStream_t) [with T=cutlass::half_t]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_hdim160_fp16_sm80.cu(26): here

/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_kernel.h(140): warning: variable "MMA_M" was declared but never referenced
          detected during:
            instantiation of "void flash::compute_attn_1rowblock<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &, int, int, int) [with Kernel_traits=Flash_fwd_kernel_traits<160, 128, 32, 4, false, false, cutlass::half_t, Flash_kernel_traits<160, 128, 32, 4, cutlass::half_t>>, Is_dropout=true, Is_causal=true, Is_even_N=true, Is_even_K=true, Return_softmax=true, Params=Flash_fwd_params]" 
(588): here
            instantiation of "void flash::compute_attn<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &) [with Kernel_traits=Flash_fwd_kernel_traits<160, 128, 32, 4, false, false, cutlass::half_t, Flash_kernel_traits<160, 128, 32, 4, cutlass::half_t>>, Is_dropout=true, Is_causal=true, Is_even_N=true, Is_even_K=true, Return_softmax=true, Params=Flash_fwd_params]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(15): here
            instantiation of "void flash_fwd_kernel<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax>(Flash_fwd_params) [with Kernel_traits=Flash_fwd_kernel_traits<160, 128, 32, 4, false, false, cutlass::half_t, Flash_kernel_traits<160, 128, 32, 4, cutlass::half_t>>, Is_dropout=true, Is_causal=true, Is_even_N=true, Is_even_K=true, Return_softmax=true]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(34): here
            instantiation of "void run_flash_fwd<Kernel_traits,Is_dropout,Is_causal>(Flash_fwd_params &, cudaStream_t) [with Kernel_traits=Flash_fwd_kernel_traits<160, 128, 32, 4, false, false, cutlass::half_t, Flash_kernel_traits<160, 128, 32, 4, cutlass::half_t>>, Is_dropout=true, Is_causal=true]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(155): here
            instantiation of "void run_mha_fwd_hdim160<T>(Flash_fwd_params &, cudaStream_t) [with T=cutlass::half_t]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_hdim160_fp16_sm80.cu(26): here

/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_kernel.h(275): warning: variable "tQrQ" was declared but never referenced
          detected during:
            instantiation of "void flash::compute_attn_1rowblock<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &, int, int, int) [with Kernel_traits=Flash_fwd_kernel_traits<160, 128, 32, 4, false, false, cutlass::half_t, Flash_kernel_traits<160, 128, 32, 4, cutlass::half_t>>, Is_dropout=true, Is_causal=true, Is_even_N=true, Is_even_K=true, Return_softmax=true, Params=Flash_fwd_params]" 
(588): here
            instantiation of "void flash::compute_attn<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &) [with Kernel_traits=Flash_fwd_kernel_traits<160, 128, 32, 4, false, false, cutlass::half_t, Flash_kernel_traits<160, 128, 32, 4, cutlass::half_t>>, Is_dropout=true, Is_causal=true, Is_even_N=true, Is_even_K=true, Return_softmax=true, Params=Flash_fwd_params]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(15): here
            instantiation of "void flash_fwd_kernel<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax>(Flash_fwd_params) [with Kernel_traits=Flash_fwd_kernel_traits<160, 128, 32, 4, false, false, cutlass::half_t, Flash_kernel_traits<160, 128, 32, 4, cutlass::half_t>>, Is_dropout=true, Is_causal=true, Is_even_N=true, Is_even_K=true, Return_softmax=true]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(34): here
            instantiation of "void run_flash_fwd<Kernel_traits,Is_dropout,Is_causal>(Flash_fwd_params &, cudaStream_t) [with Kernel_traits=Flash_fwd_kernel_traits<160, 128, 32, 4, false, false, cutlass::half_t, Flash_kernel_traits<160, 128, 32, 4, cutlass::half_t>>, Is_dropout=true, Is_causal=true]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(155): here
            instantiation of "void run_mha_fwd_hdim160<T>(Flash_fwd_params &, cudaStream_t) [with T=cutlass::half_t]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_hdim160_fp16_sm80.cu(26): here

/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_kernel.h(140): warning: variable "MMA_M" was declared but never referenced
          detected during:
            instantiation of "void flash::compute_attn_1rowblock<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &, int, int, int) [with Kernel_traits=Flash_fwd_kernel_traits<160, 128, 32, 4, false, false, cutlass::half_t, Flash_kernel_traits<160, 128, 32, 4, cutlass::half_t>>, Is_dropout=true, Is_causal=true, Is_even_N=true, Is_even_K=true, Return_softmax=false, Params=Flash_fwd_params]" 
(588): here
            instantiation of "void flash::compute_attn<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &) [with Kernel_traits=Flash_fwd_kernel_traits<160, 128, 32, 4, false, false, cutlass::half_t, Flash_kernel_traits<160, 128, 32, 4, cutlass::half_t>>, Is_dropout=true, Is_causal=true, Is_even_N=true, Is_even_K=true, Return_softmax=false, Params=Flash_fwd_params]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(15): here
            instantiation of "void flash_fwd_kernel<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax>(Flash_fwd_params) [with Kernel_traits=Flash_fwd_kernel_traits<160, 128, 32, 4, false, false, cutlass::half_t, Flash_kernel_traits<160, 128, 32, 4, cutlass::half_t>>, Is_dropout=true, Is_causal=true, Is_even_N=true, Is_even_K=true, Return_softmax=false]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(34): here
            instantiation of "void run_flash_fwd<Kernel_traits,Is_dropout,Is_causal>(Flash_fwd_params &, cudaStream_t) [with Kernel_traits=Flash_fwd_kernel_traits<160, 128, 32, 4, false, false, cutlass::half_t, Flash_kernel_traits<160, 128, 32, 4, cutlass::half_t>>, Is_dropout=true, Is_causal=true]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(155): here
            instantiation of "void run_mha_fwd_hdim160<T>(Flash_fwd_params &, cudaStream_t) [with T=cutlass::half_t]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_hdim160_fp16_sm80.cu(26): here

/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_kernel.h(275): warning: variable "tQrQ" was declared but never referenced
          detected during:
            instantiation of "void flash::compute_attn_1rowblock<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &, int, int, int) [with Kernel_traits=Flash_fwd_kernel_traits<160, 128, 32, 4, false, false, cutlass::half_t, Flash_kernel_traits<160, 128, 32, 4, cutlass::half_t>>, Is_dropout=true, Is_causal=true, Is_even_N=true, Is_even_K=true, Return_softmax=false, Params=Flash_fwd_params]" 
(588): here
            instantiation of "void flash::compute_attn<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &) [with Kernel_traits=Flash_fwd_kernel_traits<160, 128, 32, 4, false, false, cutlass::half_t, Flash_kernel_traits<160, 128, 32, 4, cutlass::half_t>>, Is_dropout=true, Is_causal=true, Is_even_N=true, Is_even_K=true, Return_softmax=false, Params=Flash_fwd_params]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(15): here
            instantiation of "void flash_fwd_kernel<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax>(Flash_fwd_params) [with Kernel_traits=Flash_fwd_kernel_traits<160, 128, 32, 4, false, false, cutlass::half_t, Flash_kernel_traits<160, 128, 32, 4, cutlass::half_t>>, Is_dropout=true, Is_causal=true, Is_even_N=true, Is_even_K=true, Return_softmax=false]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(34): here
            instantiation of "void run_flash_fwd<Kernel_traits,Is_dropout,Is_causal>(Flash_fwd_params &, cudaStream_t) [with Kernel_traits=Flash_fwd_kernel_traits<160, 128, 32, 4, false, false, cutlass::half_t, Flash_kernel_traits<160, 128, 32, 4, cutlass::half_t>>, Is_dropout=true, Is_causal=true]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(155): here
            instantiation of "void run_mha_fwd_hdim160<T>(Flash_fwd_params &, cudaStream_t) [with T=cutlass::half_t]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_hdim160_fp16_sm80.cu(26): here

/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_kernel.h(140): warning: variable "MMA_M" was declared but never referenced
          detected during:
            instantiation of "void flash::compute_attn_1rowblock<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &, int, int, int) [with Kernel_traits=Flash_fwd_kernel_traits<160, 128, 32, 4, false, false, cutlass::half_t, Flash_kernel_traits<160, 128, 32, 4, cutlass::half_t>>, Is_dropout=true, Is_causal=true, Is_even_N=true, Is_even_K=false, Return_softmax=true, Params=Flash_fwd_params]" 
(588): here
            instantiation of "void flash::compute_attn<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &) [with Kernel_traits=Flash_fwd_kernel_traits<160, 128, 32, 4, false, false, cutlass::half_t, Flash_kernel_traits<160, 128, 32, 4, cutlass::half_t>>, Is_dropout=true, Is_causal=true, Is_even_N=true, Is_even_K=false, Return_softmax=true, Params=Flash_fwd_params]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(15): here
            instantiation of "void flash_fwd_kernel<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax>(Flash_fwd_params) [with Kernel_traits=Flash_fwd_kernel_traits<160, 128, 32, 4, false, false, cutlass::half_t, Flash_kernel_traits<160, 128, 32, 4, cutlass::half_t>>, Is_dropout=true, Is_causal=true, Is_even_N=true, Is_even_K=false, Return_softmax=true]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(34): here
            instantiation of "void run_flash_fwd<Kernel_traits,Is_dropout,Is_causal>(Flash_fwd_params &, cudaStream_t) [with Kernel_traits=Flash_fwd_kernel_traits<160, 128, 32, 4, false, false, cutlass::half_t, Flash_kernel_traits<160, 128, 32, 4, cutlass::half_t>>, Is_dropout=true, Is_causal=true]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(155): here
            instantiation of "void run_mha_fwd_hdim160<T>(Flash_fwd_params &, cudaStream_t) [with T=cutlass::half_t]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_hdim160_fp16_sm80.cu(26): here

/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_kernel.h(275): warning: variable "tQrQ" was declared but never referenced
          detected during:
            instantiation of "void flash::compute_attn_1rowblock<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &, int, int, int) [with Kernel_traits=Flash_fwd_kernel_traits<160, 128, 32, 4, false, false, cutlass::half_t, Flash_kernel_traits<160, 128, 32, 4, cutlass::half_t>>, Is_dropout=true, Is_causal=true, Is_even_N=true, Is_even_K=false, Return_softmax=true, Params=Flash_fwd_params]" 
(588): here
            instantiation of "void flash::compute_attn<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &) [with Kernel_traits=Flash_fwd_kernel_traits<160, 128, 32, 4, false, false, cutlass::half_t, Flash_kernel_traits<160, 128, 32, 4, cutlass::half_t>>, Is_dropout=true, Is_causal=true, Is_even_N=true, Is_even_K=false, Return_softmax=true, Params=Flash_fwd_params]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(15): here
            instantiation of "void flash_fwd_kernel<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax>(Flash_fwd_params) [with Kernel_traits=Flash_fwd_kernel_traits<160, 128, 32, 4, false, false, cutlass::half_t, Flash_kernel_traits<160, 128, 32, 4, cutlass::half_t>>, Is_dropout=true, Is_causal=true, Is_even_N=true, Is_even_K=false, Return_softmax=true]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(34): here
            instantiation of "void run_flash_fwd<Kernel_traits,Is_dropout,Is_causal>(Flash_fwd_params &, cudaStream_t) [with Kernel_traits=Flash_fwd_kernel_traits<160, 128, 32, 4, false, false, cutlass::half_t, Flash_kernel_traits<160, 128, 32, 4, cutlass::half_t>>, Is_dropout=true, Is_causal=true]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(155): here
            instantiation of "void run_mha_fwd_hdim160<T>(Flash_fwd_params &, cudaStream_t) [with T=cutlass::half_t]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_hdim160_fp16_sm80.cu(26): here

/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_kernel.h(140): warning: variable "MMA_M" was declared but never referenced
          detected during:
            instantiation of "void flash::compute_attn_1rowblock<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &, int, int, int) [with Kernel_traits=Flash_fwd_kernel_traits<160, 128, 32, 4, false, false, cutlass::half_t, Flash_kernel_traits<160, 128, 32, 4, cutlass::half_t>>, Is_dropout=true, Is_causal=true, Is_even_N=true, Is_even_K=false, Return_softmax=false, Params=Flash_fwd_params]" 
(588): here
            instantiation of "void flash::compute_attn<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &) [with Kernel_traits=Flash_fwd_kernel_traits<160, 128, 32, 4, false, false, cutlass::half_t, Flash_kernel_traits<160, 128, 32, 4, cutlass::half_t>>, Is_dropout=true, Is_causal=true, Is_even_N=true, Is_even_K=false, Return_softmax=false, Params=Flash_fwd_params]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(15): here
            instantiation of "void flash_fwd_kernel<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax>(Flash_fwd_params) [with Kernel_traits=Flash_fwd_kernel_traits<160, 128, 32, 4, false, false, cutlass::half_t, Flash_kernel_traits<160, 128, 32, 4, cutlass::half_t>>, Is_dropout=true, Is_causal=true, Is_even_N=true, Is_even_K=false, Return_softmax=false]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(34): here
            instantiation of "void run_flash_fwd<Kernel_traits,Is_dropout,Is_causal>(Flash_fwd_params &, cudaStream_t) [with Kernel_traits=Flash_fwd_kernel_traits<160, 128, 32, 4, false, false, cutlass::half_t, Flash_kernel_traits<160, 128, 32, 4, cutlass::half_t>>, Is_dropout=true, Is_causal=true]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(155): here
            instantiation of "void run_mha_fwd_hdim160<T>(Flash_fwd_params &, cudaStream_t) [with T=cutlass::half_t]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_hdim160_fp16_sm80.cu(26): here

/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_kernel.h(275): warning: variable "tQrQ" was declared but never referenced
          detected during:
            instantiation of "void flash::compute_attn_1rowblock<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &, int, int, int) [with Kernel_traits=Flash_fwd_kernel_traits<160, 128, 32, 4, false, false, cutlass::half_t, Flash_kernel_traits<160, 128, 32, 4, cutlass::half_t>>, Is_dropout=true, Is_causal=true, Is_even_N=true, Is_even_K=false, Return_softmax=false, Params=Flash_fwd_params]" 
(588): here
            instantiation of "void flash::compute_attn<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &) [with Kernel_traits=Flash_fwd_kernel_traits<160, 128, 32, 4, false, false, cutlass::half_t, Flash_kernel_traits<160, 128, 32, 4, cutlass::half_t>>, Is_dropout=true, Is_causal=true, Is_even_N=true, Is_even_K=false, Return_softmax=false, Params=Flash_fwd_params]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(15): here
            instantiation of "void flash_fwd_kernel<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax>(Flash_fwd_params) [with Kernel_traits=Flash_fwd_kernel_traits<160, 128, 32, 4, false, false, cutlass::half_t, Flash_kernel_traits<160, 128, 32, 4, cutlass::half_t>>, Is_dropout=true, Is_causal=true, Is_even_N=true, Is_even_K=false, Return_softmax=false]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(34): here
            instantiation of "void run_flash_fwd<Kernel_traits,Is_dropout,Is_causal>(Flash_fwd_params &, cudaStream_t) [with Kernel_traits=Flash_fwd_kernel_traits<160, 128, 32, 4, false, false, cutlass::half_t, Flash_kernel_traits<160, 128, 32, 4, cutlass::half_t>>, Is_dropout=true, Is_causal=true]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(155): here
            instantiation of "void run_mha_fwd_hdim160<T>(Flash_fwd_params &, cudaStream_t) [with T=cutlass::half_t]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_hdim160_fp16_sm80.cu(26): here

/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_kernel.h(140): warning: variable "MMA_M" was declared but never referenced
          detected during:
            instantiation of "void flash::compute_attn_1rowblock<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &, int, int, int) [with Kernel_traits=Flash_fwd_kernel_traits<160, 128, 32, 4, false, false, cutlass::half_t, Flash_kernel_traits<160, 128, 32, 4, cutlass::half_t>>, Is_dropout=true, Is_causal=true, Is_even_N=false, Is_even_K=true, Return_softmax=true, Params=Flash_fwd_params]" 
(588): here
            instantiation of "void flash::compute_attn<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &) [with Kernel_traits=Flash_fwd_kernel_traits<160, 128, 32, 4, false, false, cutlass::half_t, Flash_kernel_traits<160, 128, 32, 4, cutlass::half_t>>, Is_dropout=true, Is_causal=true, Is_even_N=false, Is_even_K=true, Return_softmax=true, Params=Flash_fwd_params]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(15): here
            instantiation of "void flash_fwd_kernel<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax>(Flash_fwd_params) [with Kernel_traits=Flash_fwd_kernel_traits<160, 128, 32, 4, false, false, cutlass::half_t, Flash_kernel_traits<160, 128, 32, 4, cutlass::half_t>>, Is_dropout=true, Is_causal=true, Is_even_N=false, Is_even_K=true, Return_softmax=true]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(34): here
            instantiation of "void run_flash_fwd<Kernel_traits,Is_dropout,Is_causal>(Flash_fwd_params &, cudaStream_t) [with Kernel_traits=Flash_fwd_kernel_traits<160, 128, 32, 4, false, false, cutlass::half_t, Flash_kernel_traits<160, 128, 32, 4, cutlass::half_t>>, Is_dropout=true, Is_causal=true]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(155): here
            instantiation of "void run_mha_fwd_hdim160<T>(Flash_fwd_params &, cudaStream_t) [with T=cutlass::half_t]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_hdim160_fp16_sm80.cu(26): here

/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_kernel.h(275): warning: variable "tQrQ" was declared but never referenced
          detected during:
            instantiation of "void flash::compute_attn_1rowblock<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &, int, int, int) [with Kernel_traits=Flash_fwd_kernel_traits<160, 128, 32, 4, false, false, cutlass::half_t, Flash_kernel_traits<160, 128, 32, 4, cutlass::half_t>>, Is_dropout=true, Is_causal=true, Is_even_N=false, Is_even_K=true, Return_softmax=true, Params=Flash_fwd_params]" 
(588): here
            instantiation of "void flash::compute_attn<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &) [with Kernel_traits=Flash_fwd_kernel_traits<160, 128, 32, 4, false, false, cutlass::half_t, Flash_kernel_traits<160, 128, 32, 4, cutlass::half_t>>, Is_dropout=true, Is_causal=true, Is_even_N=false, Is_even_K=true, Return_softmax=true, Params=Flash_fwd_params]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(15): here
            instantiation of "void flash_fwd_kernel<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax>(Flash_fwd_params) [with Kernel_traits=Flash_fwd_kernel_traits<160, 128, 32, 4, false, false, cutlass::half_t, Flash_kernel_traits<160, 128, 32, 4, cutlass::half_t>>, Is_dropout=true, Is_causal=true, Is_even_N=false, Is_even_K=true, Return_softmax=true]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(34): here
            instantiation of "void run_flash_fwd<Kernel_traits,Is_dropout,Is_causal>(Flash_fwd_params &, cudaStream_t) [with Kernel_traits=Flash_fwd_kernel_traits<160, 128, 32, 4, false, false, cutlass::half_t, Flash_kernel_traits<160, 128, 32, 4, cutlass::half_t>>, Is_dropout=true, Is_causal=true]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(155): here
            instantiation of "void run_mha_fwd_hdim160<T>(Flash_fwd_params &, cudaStream_t) [with T=cutlass::half_t]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_hdim160_fp16_sm80.cu(26): here

/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_kernel.h(140): warning: variable "MMA_M" was declared but never referenced
          detected during:
            instantiation of "void flash::compute_attn_1rowblock<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &, int, int, int) [with Kernel_traits=Flash_fwd_kernel_traits<160, 128, 32, 4, false, false, cutlass::half_t, Flash_kernel_traits<160, 128, 32, 4, cutlass::half_t>>, Is_dropout=true, Is_causal=true, Is_even_N=false, Is_even_K=true, Return_softmax=false, Params=Flash_fwd_params]" 
(588): here
            instantiation of "void flash::compute_attn<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &) [with Kernel_traits=Flash_fwd_kernel_traits<160, 128, 32, 4, false, false, cutlass::half_t, Flash_kernel_traits<160, 128, 32, 4, cutlass::half_t>>, Is_dropout=true, Is_causal=true, Is_even_N=false, Is_even_K=true, Return_softmax=false, Params=Flash_fwd_params]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(15): here
            instantiation of "void flash_fwd_kernel<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax>(Flash_fwd_params) [with Kernel_traits=Flash_fwd_kernel_traits<160, 128, 32, 4, false, false, cutlass::half_t, Flash_kernel_traits<160, 128, 32, 4, cutlass::half_t>>, Is_dropout=true, Is_causal=true, Is_even_N=false, Is_even_K=true, Return_softmax=false]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(34): here
            instantiation of "void run_flash_fwd<Kernel_traits,Is_dropout,Is_causal>(Flash_fwd_params &, cudaStream_t) [with Kernel_traits=Flash_fwd_kernel_traits<160, 128, 32, 4, false, false, cutlass::half_t, Flash_kernel_traits<160, 128, 32, 4, cutlass::half_t>>, Is_dropout=true, Is_causal=true]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(155): here
            instantiation of "void run_mha_fwd_hdim160<T>(Flash_fwd_params &, cudaStream_t) [with T=cutlass::half_t]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_hdim160_fp16_sm80.cu(26): here

/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_kernel.h(275): warning: variable "tQrQ" was declared but never referenced
          detected during:
            instantiation of "void flash::compute_attn_1rowblock<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &, int, int, int) [with Kernel_traits=Flash_fwd_kernel_traits<160, 128, 32, 4, false, false, cutlass::half_t, Flash_kernel_traits<160, 128, 32, 4, cutlass::half_t>>, Is_dropout=true, Is_causal=true, Is_even_N=false, Is_even_K=true, Return_softmax=false, Params=Flash_fwd_params]" 
(588): here
            instantiation of "void flash::compute_attn<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &) [with Kernel_traits=Flash_fwd_kernel_traits<160, 128, 32, 4, false, false, cutlass::half_t, Flash_kernel_traits<160, 128, 32, 4, cutlass::half_t>>, Is_dropout=true, Is_causal=true, Is_even_N=false, Is_even_K=true, Return_softmax=false, Params=Flash_fwd_params]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(15): here
            instantiation of "void flash_fwd_kernel<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax>(Flash_fwd_params) [with Kernel_traits=Flash_fwd_kernel_traits<160, 128, 32, 4, false, false, cutlass::half_t, Flash_kernel_traits<160, 128, 32, 4, cutlass::half_t>>, Is_dropout=true, Is_causal=true, Is_even_N=false, Is_even_K=true, Return_softmax=false]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(34): here
            instantiation of "void run_flash_fwd<Kernel_traits,Is_dropout,Is_causal>(Flash_fwd_params &, cudaStream_t) [with Kernel_traits=Flash_fwd_kernel_traits<160, 128, 32, 4, false, false, cutlass::half_t, Flash_kernel_traits<160, 128, 32, 4, cutlass::half_t>>, Is_dropout=true, Is_causal=true]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(155): here
            instantiation of "void run_mha_fwd_hdim160<T>(Flash_fwd_params &, cudaStream_t) [with T=cutlass::half_t]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_hdim160_fp16_sm80.cu(26): here

/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_kernel.h(140): warning: variable "MMA_M" was declared but never referenced
          detected during:
            instantiation of "void flash::compute_attn_1rowblock<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &, int, int, int) [with Kernel_traits=Flash_fwd_kernel_traits<160, 128, 32, 4, false, false, cutlass::half_t, Flash_kernel_traits<160, 128, 32, 4, cutlass::half_t>>, Is_dropout=true, Is_causal=true, Is_even_N=false, Is_even_K=false, Return_softmax=true, Params=Flash_fwd_params]" 
(588): here
            instantiation of "void flash::compute_attn<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &) [with Kernel_traits=Flash_fwd_kernel_traits<160, 128, 32, 4, false, false, cutlass::half_t, Flash_kernel_traits<160, 128, 32, 4, cutlass::half_t>>, Is_dropout=true, Is_causal=true, Is_even_N=false, Is_even_K=false, Return_softmax=true, Params=Flash_fwd_params]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(15): here
            instantiation of "void flash_fwd_kernel<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax>(Flash_fwd_params) [with Kernel_traits=Flash_fwd_kernel_traits<160, 128, 32, 4, false, false, cutlass::half_t, Flash_kernel_traits<160, 128, 32, 4, cutlass::half_t>>, Is_dropout=true, Is_causal=true, Is_even_N=false, Is_even_K=false, Return_softmax=true]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(34): here
            instantiation of "void run_flash_fwd<Kernel_traits,Is_dropout,Is_causal>(Flash_fwd_params &, cudaStream_t) [with Kernel_traits=Flash_fwd_kernel_traits<160, 128, 32, 4, false, false, cutlass::half_t, Flash_kernel_traits<160, 128, 32, 4, cutlass::half_t>>, Is_dropout=true, Is_causal=true]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(155): here
            instantiation of "void run_mha_fwd_hdim160<T>(Flash_fwd_params &, cudaStream_t) [with T=cutlass::half_t]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_hdim160_fp16_sm80.cu(26): here

/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_kernel.h(275): warning: variable "tQrQ" was declared but never referenced
          detected during:
            instantiation of "void flash::compute_attn_1rowblock<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &, int, int, int) [with Kernel_traits=Flash_fwd_kernel_traits<160, 128, 32, 4, false, false, cutlass::half_t, Flash_kernel_traits<160, 128, 32, 4, cutlass::half_t>>, Is_dropout=true, Is_causal=true, Is_even_N=false, Is_even_K=false, Return_softmax=true, Params=Flash_fwd_params]" 
(588): here
            instantiation of "void flash::compute_attn<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &) [with Kernel_traits=Flash_fwd_kernel_traits<160, 128, 32, 4, false, false, cutlass::half_t, Flash_kernel_traits<160, 128, 32, 4, cutlass::half_t>>, Is_dropout=true, Is_causal=true, Is_even_N=false, Is_even_K=false, Return_softmax=true, Params=Flash_fwd_params]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(15): here
            instantiation of "void flash_fwd_kernel<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax>(Flash_fwd_params) [with Kernel_traits=Flash_fwd_kernel_traits<160, 128, 32, 4, false, false, cutlass::half_t, Flash_kernel_traits<160, 128, 32, 4, cutlass::half_t>>, Is_dropout=true, Is_causal=true, Is_even_N=false, Is_even_K=false, Return_softmax=true]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(34): here
            instantiation of "void run_flash_fwd<Kernel_traits,Is_dropout,Is_causal>(Flash_fwd_params &, cudaStream_t) [with Kernel_traits=Flash_fwd_kernel_traits<160, 128, 32, 4, false, false, cutlass::half_t, Flash_kernel_traits<160, 128, 32, 4, cutlass::half_t>>, Is_dropout=true, Is_causal=true]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(155): here
            instantiation of "void run_mha_fwd_hdim160<T>(Flash_fwd_params &, cudaStream_t) [with T=cutlass::half_t]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_hdim160_fp16_sm80.cu(26): here

/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_kernel.h(140): warning: variable "MMA_M" was declared but never referenced
          detected during:
            instantiation of "void flash::compute_attn_1rowblock<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &, int, int, int) [with Kernel_traits=Flash_fwd_kernel_traits<160, 128, 32, 4, false, false, cutlass::half_t, Flash_kernel_traits<160, 128, 32, 4, cutlass::half_t>>, Is_dropout=true, Is_causal=true, Is_even_N=false, Is_even_K=false, Return_softmax=false, Params=Flash_fwd_params]" 
(588): here
            instantiation of "void flash::compute_attn<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &) [with Kernel_traits=Flash_fwd_kernel_traits<160, 128, 32, 4, false, false, cutlass::half_t, Flash_kernel_traits<160, 128, 32, 4, cutlass::half_t>>, Is_dropout=true, Is_causal=true, Is_even_N=false, Is_even_K=false, Return_softmax=false, Params=Flash_fwd_params]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(15): here
            instantiation of "void flash_fwd_kernel<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax>(Flash_fwd_params) [with Kernel_traits=Flash_fwd_kernel_traits<160, 128, 32, 4, false, false, cutlass::half_t, Flash_kernel_traits<160, 128, 32, 4, cutlass::half_t>>, Is_dropout=true, Is_causal=true, Is_even_N=false, Is_even_K=false, Return_softmax=false]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(34): here
            instantiation of "void run_flash_fwd<Kernel_traits,Is_dropout,Is_causal>(Flash_fwd_params &, cudaStream_t) [with Kernel_traits=Flash_fwd_kernel_traits<160, 128, 32, 4, false, false, cutlass::half_t, Flash_kernel_traits<160, 128, 32, 4, cutlass::half_t>>, Is_dropout=true, Is_causal=true]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(155): here
            instantiation of "void run_mha_fwd_hdim160<T>(Flash_fwd_params &, cudaStream_t) [with T=cutlass::half_t]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_hdim160_fp16_sm80.cu(26): here

/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_kernel.h(275): warning: variable "tQrQ" was declared but never referenced
          detected during:
            instantiation of "void flash::compute_attn_1rowblock<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &, int, int, int) [with Kernel_traits=Flash_fwd_kernel_traits<160, 128, 32, 4, false, false, cutlass::half_t, Flash_kernel_traits<160, 128, 32, 4, cutlass::half_t>>, Is_dropout=true, Is_causal=true, Is_even_N=false, Is_even_K=false, Return_softmax=false, Params=Flash_fwd_params]" 
(588): here
            instantiation of "void flash::compute_attn<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &) [with Kernel_traits=Flash_fwd_kernel_traits<160, 128, 32, 4, false, false, cutlass::half_t, Flash_kernel_traits<160, 128, 32, 4, cutlass::half_t>>, Is_dropout=true, Is_causal=true, Is_even_N=false, Is_even_K=false, Return_softmax=false, Params=Flash_fwd_params]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(15): here
            instantiation of "void flash_fwd_kernel<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax>(Flash_fwd_params) [with Kernel_traits=Flash_fwd_kernel_traits<160, 128, 32, 4, false, false, cutlass::half_t, Flash_kernel_traits<160, 128, 32, 4, cutlass::half_t>>, Is_dropout=true, Is_causal=true, Is_even_N=false, Is_even_K=false, Return_softmax=false]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(34): here
            instantiation of "void run_flash_fwd<Kernel_traits,Is_dropout,Is_causal>(Flash_fwd_params &, cudaStream_t) [with Kernel_traits=Flash_fwd_kernel_traits<160, 128, 32, 4, false, false, cutlass::half_t, Flash_kernel_traits<160, 128, 32, 4, cutlass::half_t>>, Is_dropout=true, Is_causal=true]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(155): here
            instantiation of "void run_mha_fwd_hdim160<T>(Flash_fwd_params &, cudaStream_t) [with T=cutlass::half_t]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_hdim160_fp16_sm80.cu(26): here

/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_kernel.h(140): warning: variable "MMA_M" was declared but never referenced
          detected during:
            instantiation of "void flash::compute_attn_1rowblock<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &, int, int, int) [with Kernel_traits=Flash_fwd_kernel_traits<160, 128, 64, 8, false, false, cutlass::half_t, Flash_kernel_traits<160, 128, 64, 8, cutlass::half_t>>, Is_dropout=true, Is_causal=false, Is_even_N=true, Is_even_K=true, Return_softmax=true, Params=Flash_fwd_params]" 
(588): here
            instantiation of "void flash::compute_attn<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &) [with Kernel_traits=Flash_fwd_kernel_traits<160, 128, 64, 8, false, false, cutlass::half_t, Flash_kernel_traits<160, 128, 64, 8, cutlass::half_t>>, Is_dropout=true, Is_causal=false, Is_even_N=true, Is_even_K=true, Return_softmax=true, Params=Flash_fwd_params]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(15): here
            instantiation of "void flash_fwd_kernel<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax>(Flash_fwd_params) [with Kernel_traits=Flash_fwd_kernel_traits<160, 128, 64, 8, false, false, cutlass::half_t, Flash_kernel_traits<160, 128, 64, 8, cutlass::half_t>>, Is_dropout=true, Is_causal=false, Is_even_N=true, Is_even_K=true, Return_softmax=true]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(34): here
            instantiation of "void run_flash_fwd<Kernel_traits,Is_dropout,Is_causal>(Flash_fwd_params &, cudaStream_t) [with Kernel_traits=Flash_fwd_kernel_traits<160, 128, 64, 8, false, false, cutlass::half_t, Flash_kernel_traits<160, 128, 64, 8, cutlass::half_t>>, Is_dropout=true, Is_causal=false]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(155): here
            instantiation of "void run_mha_fwd_hdim160<T>(Flash_fwd_params &, cudaStream_t) [with T=cutlass::half_t]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_hdim160_fp16_sm80.cu(26): here

/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_kernel.h(275): warning: variable "tQrQ" was declared but never referenced
          detected during:
            instantiation of "void flash::compute_attn_1rowblock<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &, int, int, int) [with Kernel_traits=Flash_fwd_kernel_traits<160, 128, 64, 8, false, false, cutlass::half_t, Flash_kernel_traits<160, 128, 64, 8, cutlass::half_t>>, Is_dropout=true, Is_causal=false, Is_even_N=true, Is_even_K=true, Return_softmax=true, Params=Flash_fwd_params]" 
(588): here
            instantiation of "void flash::compute_attn<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &) [with Kernel_traits=Flash_fwd_kernel_traits<160, 128, 64, 8, false, false, cutlass::half_t, Flash_kernel_traits<160, 128, 64, 8, cutlass::half_t>>, Is_dropout=true, Is_causal=false, Is_even_N=true, Is_even_K=true, Return_softmax=true, Params=Flash_fwd_params]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(15): here
            instantiation of "void flash_fwd_kernel<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax>(Flash_fwd_params) [with Kernel_traits=Flash_fwd_kernel_traits<160, 128, 64, 8, false, false, cutlass::half_t, Flash_kernel_traits<160, 128, 64, 8, cutlass::half_t>>, Is_dropout=true, Is_causal=false, Is_even_N=true, Is_even_K=true, Return_softmax=true]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(34): here
            instantiation of "void run_flash_fwd<Kernel_traits,Is_dropout,Is_causal>(Flash_fwd_params &, cudaStream_t) [with Kernel_traits=Flash_fwd_kernel_traits<160, 128, 64, 8, false, false, cutlass::half_t, Flash_kernel_traits<160, 128, 64, 8, cutlass::half_t>>, Is_dropout=true, Is_causal=false]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(155): here
            instantiation of "void run_mha_fwd_hdim160<T>(Flash_fwd_params &, cudaStream_t) [with T=cutlass::half_t]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_hdim160_fp16_sm80.cu(26): here

/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_kernel.h(140): warning: variable "MMA_M" was declared but never referenced
          detected during:
            instantiation of "void flash::compute_attn_1rowblock<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &, int, int, int) [with Kernel_traits=Flash_fwd_kernel_traits<160, 128, 64, 8, false, false, cutlass::half_t, Flash_kernel_traits<160, 128, 64, 8, cutlass::half_t>>, Is_dropout=true, Is_causal=false, Is_even_N=true, Is_even_K=true, Return_softmax=false, Params=Flash_fwd_params]" 
(588): here
            instantiation of "void flash::compute_attn<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &) [with Kernel_traits=Flash_fwd_kernel_traits<160, 128, 64, 8, false, false, cutlass::half_t, Flash_kernel_traits<160, 128, 64, 8, cutlass::half_t>>, Is_dropout=true, Is_causal=false, Is_even_N=true, Is_even_K=true, Return_softmax=false, Params=Flash_fwd_params]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(15): here
            instantiation of "void flash_fwd_kernel<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax>(Flash_fwd_params) [with Kernel_traits=Flash_fwd_kernel_traits<160, 128, 64, 8, false, false, cutlass::half_t, Flash_kernel_traits<160, 128, 64, 8, cutlass::half_t>>, Is_dropout=true, Is_causal=false, Is_even_N=true, Is_even_K=true, Return_softmax=false]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(34): here
            instantiation of "void run_flash_fwd<Kernel_traits,Is_dropout,Is_causal>(Flash_fwd_params &, cudaStream_t) [with Kernel_traits=Flash_fwd_kernel_traits<160, 128, 64, 8, false, false, cutlass::half_t, Flash_kernel_traits<160, 128, 64, 8, cutlass::half_t>>, Is_dropout=true, Is_causal=false]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(155): here
            instantiation of "void run_mha_fwd_hdim160<T>(Flash_fwd_params &, cudaStream_t) [with T=cutlass::half_t]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_hdim160_fp16_sm80.cu(26): here

/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_kernel.h(275): warning: variable "tQrQ" was declared but never referenced
          detected during:
            instantiation of "void flash::compute_attn_1rowblock<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &, int, int, int) [with Kernel_traits=Flash_fwd_kernel_traits<160, 128, 64, 8, false, false, cutlass::half_t, Flash_kernel_traits<160, 128, 64, 8, cutlass::half_t>>, Is_dropout=true, Is_causal=false, Is_even_N=true, Is_even_K=true, Return_softmax=false, Params=Flash_fwd_params]" 
(588): here
            instantiation of "void flash::compute_attn<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &) [with Kernel_traits=Flash_fwd_kernel_traits<160, 128, 64, 8, false, false, cutlass::half_t, Flash_kernel_traits<160, 128, 64, 8, cutlass::half_t>>, Is_dropout=true, Is_causal=false, Is_even_N=true, Is_even_K=true, Return_softmax=false, Params=Flash_fwd_params]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(15): here
            instantiation of "void flash_fwd_kernel<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax>(Flash_fwd_params) [with Kernel_traits=Flash_fwd_kernel_traits<160, 128, 64, 8, false, false, cutlass::half_t, Flash_kernel_traits<160, 128, 64, 8, cutlass::half_t>>, Is_dropout=true, Is_causal=false, Is_even_N=true, Is_even_K=true, Return_softmax=false]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(34): here
            instantiation of "void run_flash_fwd<Kernel_traits,Is_dropout,Is_causal>(Flash_fwd_params &, cudaStream_t) [with Kernel_traits=Flash_fwd_kernel_traits<160, 128, 64, 8, false, false, cutlass::half_t, Flash_kernel_traits<160, 128, 64, 8, cutlass::half_t>>, Is_dropout=true, Is_causal=false]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(155): here
            instantiation of "void run_mha_fwd_hdim160<T>(Flash_fwd_params &, cudaStream_t) [with T=cutlass::half_t]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_hdim160_fp16_sm80.cu(26): here

/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_kernel.h(140): warning: variable "MMA_M" was declared but never referenced
          detected during:
            instantiation of "void flash::compute_attn_1rowblock<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &, int, int, int) [with Kernel_traits=Flash_fwd_kernel_traits<160, 128, 64, 8, false, false, cutlass::half_t, Flash_kernel_traits<160, 128, 64, 8, cutlass::half_t>>, Is_dropout=true, Is_causal=false, Is_even_N=true, Is_even_K=false, Return_softmax=true, Params=Flash_fwd_params]" 
(588): here
            instantiation of "void flash::compute_attn<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &) [with Kernel_traits=Flash_fwd_kernel_traits<160, 128, 64, 8, false, false, cutlass::half_t, Flash_kernel_traits<160, 128, 64, 8, cutlass::half_t>>, Is_dropout=true, Is_causal=false, Is_even_N=true, Is_even_K=false, Return_softmax=true, Params=Flash_fwd_params]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(15): here
            instantiation of "void flash_fwd_kernel<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax>(Flash_fwd_params) [with Kernel_traits=Flash_fwd_kernel_traits<160, 128, 64, 8, false, false, cutlass::half_t, Flash_kernel_traits<160, 128, 64, 8, cutlass::half_t>>, Is_dropout=true, Is_causal=false, Is_even_N=true, Is_even_K=false, Return_softmax=true]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(34): here
            instantiation of "void run_flash_fwd<Kernel_traits,Is_dropout,Is_causal>(Flash_fwd_params &, cudaStream_t) [with Kernel_traits=Flash_fwd_kernel_traits<160, 128, 64, 8, false, false, cutlass::half_t, Flash_kernel_traits<160, 128, 64, 8, cutlass::half_t>>, Is_dropout=true, Is_causal=false]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(155): here
            instantiation of "void run_mha_fwd_hdim160<T>(Flash_fwd_params &, cudaStream_t) [with T=cutlass::half_t]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_hdim160_fp16_sm80.cu(26): here

/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_kernel.h(275): warning: variable "tQrQ" was declared but never referenced
          detected during:
            instantiation of "void flash::compute_attn_1rowblock<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &, int, int, int) [with Kernel_traits=Flash_fwd_kernel_traits<160, 128, 64, 8, false, false, cutlass::half_t, Flash_kernel_traits<160, 128, 64, 8, cutlass::half_t>>, Is_dropout=true, Is_causal=false, Is_even_N=true, Is_even_K=false, Return_softmax=true, Params=Flash_fwd_params]" 
(588): here
            instantiation of "void flash::compute_attn<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &) [with Kernel_traits=Flash_fwd_kernel_traits<160, 128, 64, 8, false, false, cutlass::half_t, Flash_kernel_traits<160, 128, 64, 8, cutlass::half_t>>, Is_dropout=true, Is_causal=false, Is_even_N=true, Is_even_K=false, Return_softmax=true, Params=Flash_fwd_params]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(15): here
            instantiation of "void flash_fwd_kernel<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax>(Flash_fwd_params) [with Kernel_traits=Flash_fwd_kernel_traits<160, 128, 64, 8, false, false, cutlass::half_t, Flash_kernel_traits<160, 128, 64, 8, cutlass::half_t>>, Is_dropout=true, Is_causal=false, Is_even_N=true, Is_even_K=false, Return_softmax=true]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(34): here
            instantiation of "void run_flash_fwd<Kernel_traits,Is_dropout,Is_causal>(Flash_fwd_params &, cudaStream_t) [with Kernel_traits=Flash_fwd_kernel_traits<160, 128, 64, 8, false, false, cutlass::half_t, Flash_kernel_traits<160, 128, 64, 8, cutlass::half_t>>, Is_dropout=true, Is_causal=false]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(155): here
            instantiation of "void run_mha_fwd_hdim160<T>(Flash_fwd_params &, cudaStream_t) [with T=cutlass::half_t]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_hdim160_fp16_sm80.cu(26): here

/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_kernel.h(140): warning: variable "MMA_M" was declared but never referenced
          detected during:
            instantiation of "void flash::compute_attn_1rowblock<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &, int, int, int) [with Kernel_traits=Flash_fwd_kernel_traits<160, 128, 64, 8, false, false, cutlass::half_t, Flash_kernel_traits<160, 128, 64, 8, cutlass::half_t>>, Is_dropout=true, Is_causal=false, Is_even_N=true, Is_even_K=false, Return_softmax=false, Params=Flash_fwd_params]" 
(588): here
            instantiation of "void flash::compute_attn<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &) [with Kernel_traits=Flash_fwd_kernel_traits<160, 128, 64, 8, false, false, cutlass::half_t, Flash_kernel_traits<160, 128, 64, 8, cutlass::half_t>>, Is_dropout=true, Is_causal=false, Is_even_N=true, Is_even_K=false, Return_softmax=false, Params=Flash_fwd_params]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(15): here
            instantiation of "void flash_fwd_kernel<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax>(Flash_fwd_params) [with Kernel_traits=Flash_fwd_kernel_traits<160, 128, 64, 8, false, false, cutlass::half_t, Flash_kernel_traits<160, 128, 64, 8, cutlass::half_t>>, Is_dropout=true, Is_causal=false, Is_even_N=true, Is_even_K=false, Return_softmax=false]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(34): here
            instantiation of "void run_flash_fwd<Kernel_traits,Is_dropout,Is_causal>(Flash_fwd_params &, cudaStream_t) [with Kernel_traits=Flash_fwd_kernel_traits<160, 128, 64, 8, false, false, cutlass::half_t, Flash_kernel_traits<160, 128, 64, 8, cutlass::half_t>>, Is_dropout=true, Is_causal=false]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(155): here
            instantiation of "void run_mha_fwd_hdim160<T>(Flash_fwd_params &, cudaStream_t) [with T=cutlass::half_t]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_hdim160_fp16_sm80.cu(26): here

/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_kernel.h(275): warning: variable "tQrQ" was declared but never referenced
          detected during:
            instantiation of "void flash::compute_attn_1rowblock<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &, int, int, int) [with Kernel_traits=Flash_fwd_kernel_traits<160, 128, 64, 8, false, false, cutlass::half_t, Flash_kernel_traits<160, 128, 64, 8, cutlass::half_t>>, Is_dropout=true, Is_causal=false, Is_even_N=true, Is_even_K=false, Return_softmax=false, Params=Flash_fwd_params]" 
(588): here
            instantiation of "void flash::compute_attn<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &) [with Kernel_traits=Flash_fwd_kernel_traits<160, 128, 64, 8, false, false, cutlass::half_t, Flash_kernel_traits<160, 128, 64, 8, cutlass::half_t>>, Is_dropout=true, Is_causal=false, Is_even_N=true, Is_even_K=false, Return_softmax=false, Params=Flash_fwd_params]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(15): here
            instantiation of "void flash_fwd_kernel<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax>(Flash_fwd_params) [with Kernel_traits=Flash_fwd_kernel_traits<160, 128, 64, 8, false, false, cutlass::half_t, Flash_kernel_traits<160, 128, 64, 8, cutlass::half_t>>, Is_dropout=true, Is_causal=false, Is_even_N=true, Is_even_K=false, Return_softmax=false]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(34): here
            instantiation of "void run_flash_fwd<Kernel_traits,Is_dropout,Is_causal>(Flash_fwd_params &, cudaStream_t) [with Kernel_traits=Flash_fwd_kernel_traits<160, 128, 64, 8, false, false, cutlass::half_t, Flash_kernel_traits<160, 128, 64, 8, cutlass::half_t>>, Is_dropout=true, Is_causal=false]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(155): here
            instantiation of "void run_mha_fwd_hdim160<T>(Flash_fwd_params &, cudaStream_t) [with T=cutlass::half_t]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_hdim160_fp16_sm80.cu(26): here

/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_kernel.h(140): warning: variable "MMA_M" was declared but never referenced
          detected during:
            instantiation of "void flash::compute_attn_1rowblock<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &, int, int, int) [with Kernel_traits=Flash_fwd_kernel_traits<160, 128, 64, 8, false, false, cutlass::half_t, Flash_kernel_traits<160, 128, 64, 8, cutlass::half_t>>, Is_dropout=true, Is_causal=false, Is_even_N=false, Is_even_K=true, Return_softmax=true, Params=Flash_fwd_params]" 
(588): here
            instantiation of "void flash::compute_attn<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &) [with Kernel_traits=Flash_fwd_kernel_traits<160, 128, 64, 8, false, false, cutlass::half_t, Flash_kernel_traits<160, 128, 64, 8, cutlass::half_t>>, Is_dropout=true, Is_causal=false, Is_even_N=false, Is_even_K=true, Return_softmax=true, Params=Flash_fwd_params]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(15): here
            instantiation of "void flash_fwd_kernel<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax>(Flash_fwd_params) [with Kernel_traits=Flash_fwd_kernel_traits<160, 128, 64, 8, false, false, cutlass::half_t, Flash_kernel_traits<160, 128, 64, 8, cutlass::half_t>>, Is_dropout=true, Is_causal=false, Is_even_N=false, Is_even_K=true, Return_softmax=true]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(34): here
            instantiation of "void run_flash_fwd<Kernel_traits,Is_dropout,Is_causal>(Flash_fwd_params &, cudaStream_t) [with Kernel_traits=Flash_fwd_kernel_traits<160, 128, 64, 8, false, false, cutlass::half_t, Flash_kernel_traits<160, 128, 64, 8, cutlass::half_t>>, Is_dropout=true, Is_causal=false]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(155): here
            instantiation of "void run_mha_fwd_hdim160<T>(Flash_fwd_params &, cudaStream_t) [with T=cutlass::half_t]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_hdim160_fp16_sm80.cu(26): here

/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_kernel.h(275): warning: variable "tQrQ" was declared but never referenced
          detected during:
            instantiation of "void flash::compute_attn_1rowblock<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &, int, int, int) [with Kernel_traits=Flash_fwd_kernel_traits<160, 128, 64, 8, false, false, cutlass::half_t, Flash_kernel_traits<160, 128, 64, 8, cutlass::half_t>>, Is_dropout=true, Is_causal=false, Is_even_N=false, Is_even_K=true, Return_softmax=true, Params=Flash_fwd_params]" 
(588): here
            instantiation of "void flash::compute_attn<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &) [with Kernel_traits=Flash_fwd_kernel_traits<160, 128, 64, 8, false, false, cutlass::half_t, Flash_kernel_traits<160, 128, 64, 8, cutlass::half_t>>, Is_dropout=true, Is_causal=false, Is_even_N=false, Is_even_K=true, Return_softmax=true, Params=Flash_fwd_params]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(15): here
            instantiation of "void flash_fwd_kernel<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax>(Flash_fwd_params) [with Kernel_traits=Flash_fwd_kernel_traits<160, 128, 64, 8, false, false, cutlass::half_t, Flash_kernel_traits<160, 128, 64, 8, cutlass::half_t>>, Is_dropout=true, Is_causal=false, Is_even_N=false, Is_even_K=true, Return_softmax=true]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(34): here
            instantiation of "void run_flash_fwd<Kernel_traits,Is_dropout,Is_causal>(Flash_fwd_params &, cudaStream_t) [with Kernel_traits=Flash_fwd_kernel_traits<160, 128, 64, 8, false, false, cutlass::half_t, Flash_kernel_traits<160, 128, 64, 8, cutlass::half_t>>, Is_dropout=true, Is_causal=false]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(155): here
            instantiation of "void run_mha_fwd_hdim160<T>(Flash_fwd_params &, cudaStream_t) [with T=cutlass::half_t]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_hdim160_fp16_sm80.cu(26): here

/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_kernel.h(140): warning: variable "MMA_M" was declared but never referenced
          detected during:
            instantiation of "void flash::compute_attn_1rowblock<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &, int, int, int) [with Kernel_traits=Flash_fwd_kernel_traits<160, 128, 64, 8, false, false, cutlass::half_t, Flash_kernel_traits<160, 128, 64, 8, cutlass::half_t>>, Is_dropout=true, Is_causal=false, Is_even_N=false, Is_even_K=true, Return_softmax=false, Params=Flash_fwd_params]" 
(588): here
            instantiation of "void flash::compute_attn<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &) [with Kernel_traits=Flash_fwd_kernel_traits<160, 128, 64, 8, false, false, cutlass::half_t, Flash_kernel_traits<160, 128, 64, 8, cutlass::half_t>>, Is_dropout=true, Is_causal=false, Is_even_N=false, Is_even_K=true, Return_softmax=false, Params=Flash_fwd_params]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(15): here
            instantiation of "void flash_fwd_kernel<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax>(Flash_fwd_params) [with Kernel_traits=Flash_fwd_kernel_traits<160, 128, 64, 8, false, false, cutlass::half_t, Flash_kernel_traits<160, 128, 64, 8, cutlass::half_t>>, Is_dropout=true, Is_causal=false, Is_even_N=false, Is_even_K=true, Return_softmax=false]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(34): here
            instantiation of "void run_flash_fwd<Kernel_traits,Is_dropout,Is_causal>(Flash_fwd_params &, cudaStream_t) [with Kernel_traits=Flash_fwd_kernel_traits<160, 128, 64, 8, false, false, cutlass::half_t, Flash_kernel_traits<160, 128, 64, 8, cutlass::half_t>>, Is_dropout=true, Is_causal=false]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(155): here
            instantiation of "void run_mha_fwd_hdim160<T>(Flash_fwd_params &, cudaStream_t) [with T=cutlass::half_t]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_hdim160_fp16_sm80.cu(26): here

/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_kernel.h(275): warning: variable "tQrQ" was declared but never referenced
          detected during:
            instantiation of "void flash::compute_attn_1rowblock<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &, int, int, int) [with Kernel_traits=Flash_fwd_kernel_traits<160, 128, 64, 8, false, false, cutlass::half_t, Flash_kernel_traits<160, 128, 64, 8, cutlass::half_t>>, Is_dropout=true, Is_causal=false, Is_even_N=false, Is_even_K=true, Return_softmax=false, Params=Flash_fwd_params]" 
(588): here
            instantiation of "void flash::compute_attn<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &) [with Kernel_traits=Flash_fwd_kernel_traits<160, 128, 64, 8, false, false, cutlass::half_t, Flash_kernel_traits<160, 128, 64, 8, cutlass::half_t>>, Is_dropout=true, Is_causal=false, Is_even_N=false, Is_even_K=true, Return_softmax=false, Params=Flash_fwd_params]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(15): here
            instantiation of "void flash_fwd_kernel<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax>(Flash_fwd_params) [with Kernel_traits=Flash_fwd_kernel_traits<160, 128, 64, 8, false, false, cutlass::half_t, Flash_kernel_traits<160, 128, 64, 8, cutlass::half_t>>, Is_dropout=true, Is_causal=false, Is_even_N=false, Is_even_K=true, Return_softmax=false]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(34): here
            instantiation of "void run_flash_fwd<Kernel_traits,Is_dropout,Is_causal>(Flash_fwd_params &, cudaStream_t) [with Kernel_traits=Flash_fwd_kernel_traits<160, 128, 64, 8, false, false, cutlass::half_t, Flash_kernel_traits<160, 128, 64, 8, cutlass::half_t>>, Is_dropout=true, Is_causal=false]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(155): here
            instantiation of "void run_mha_fwd_hdim160<T>(Flash_fwd_params &, cudaStream_t) [with T=cutlass::half_t]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_hdim160_fp16_sm80.cu(26): here

/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_kernel.h(140): warning: variable "MMA_M" was declared but never referenced
          detected during:
            instantiation of "void flash::compute_attn_1rowblock<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &, int, int, int) [with Kernel_traits=Flash_fwd_kernel_traits<160, 128, 64, 8, false, false, cutlass::half_t, Flash_kernel_traits<160, 128, 64, 8, cutlass::half_t>>, Is_dropout=true, Is_causal=false, Is_even_N=false, Is_even_K=false, Return_softmax=true, Params=Flash_fwd_params]" 
(588): here
            instantiation of "void flash::compute_attn<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &) [with Kernel_traits=Flash_fwd_kernel_traits<160, 128, 64, 8, false, false, cutlass::half_t, Flash_kernel_traits<160, 128, 64, 8, cutlass::half_t>>, Is_dropout=true, Is_causal=false, Is_even_N=false, Is_even_K=false, Return_softmax=true, Params=Flash_fwd_params]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(15): here
            instantiation of "void flash_fwd_kernel<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax>(Flash_fwd_params) [with Kernel_traits=Flash_fwd_kernel_traits<160, 128, 64, 8, false, false, cutlass::half_t, Flash_kernel_traits<160, 128, 64, 8, cutlass::half_t>>, Is_dropout=true, Is_causal=false, Is_even_N=false, Is_even_K=false, Return_softmax=true]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(34): here
            instantiation of "void run_flash_fwd<Kernel_traits,Is_dropout,Is_causal>(Flash_fwd_params &, cudaStream_t) [with Kernel_traits=Flash_fwd_kernel_traits<160, 128, 64, 8, false, false, cutlass::half_t, Flash_kernel_traits<160, 128, 64, 8, cutlass::half_t>>, Is_dropout=true, Is_causal=false]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(155): here
            instantiation of "void run_mha_fwd_hdim160<T>(Flash_fwd_params &, cudaStream_t) [with T=cutlass::half_t]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_hdim160_fp16_sm80.cu(26): here

/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_kernel.h(275): warning: variable "tQrQ" was declared but never referenced
          detected during:
            instantiation of "void flash::compute_attn_1rowblock<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &, int, int, int) [with Kernel_traits=Flash_fwd_kernel_traits<160, 128, 64, 8, false, false, cutlass::half_t, Flash_kernel_traits<160, 128, 64, 8, cutlass::half_t>>, Is_dropout=true, Is_causal=false, Is_even_N=false, Is_even_K=false, Return_softmax=true, Params=Flash_fwd_params]" 
(588): here
            instantiation of "void flash::compute_attn<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &) [with Kernel_traits=Flash_fwd_kernel_traits<160, 128, 64, 8, false, false, cutlass::half_t, Flash_kernel_traits<160, 128, 64, 8, cutlass::half_t>>, Is_dropout=true, Is_causal=false, Is_even_N=false, Is_even_K=false, Return_softmax=true, Params=Flash_fwd_params]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(15): here
            instantiation of "void flash_fwd_kernel<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax>(Flash_fwd_params) [with Kernel_traits=Flash_fwd_kernel_traits<160, 128, 64, 8, false, false, cutlass::half_t, Flash_kernel_traits<160, 128, 64, 8, cutlass::half_t>>, Is_dropout=true, Is_causal=false, Is_even_N=false, Is_even_K=false, Return_softmax=true]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(34): here
            instantiation of "void run_flash_fwd<Kernel_traits,Is_dropout,Is_causal>(Flash_fwd_params &, cudaStream_t) [with Kernel_traits=Flash_fwd_kernel_traits<160, 128, 64, 8, false, false, cutlass::half_t, Flash_kernel_traits<160, 128, 64, 8, cutlass::half_t>>, Is_dropout=true, Is_causal=false]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(155): here
            instantiation of "void run_mha_fwd_hdim160<T>(Flash_fwd_params &, cudaStream_t) [with T=cutlass::half_t]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_hdim160_fp16_sm80.cu(26): here

/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_kernel.h(140): warning: variable "MMA_M" was declared but never referenced
          detected during:
            instantiation of "void flash::compute_attn_1rowblock<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &, int, int, int) [with Kernel_traits=Flash_fwd_kernel_traits<160, 128, 64, 8, false, false, cutlass::half_t, Flash_kernel_traits<160, 128, 64, 8, cutlass::half_t>>, Is_dropout=true, Is_causal=false, Is_even_N=false, Is_even_K=false, Return_softmax=false, Params=Flash_fwd_params]" 
(588): here
            instantiation of "void flash::compute_attn<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &) [with Kernel_traits=Flash_fwd_kernel_traits<160, 128, 64, 8, false, false, cutlass::half_t, Flash_kernel_traits<160, 128, 64, 8, cutlass::half_t>>, Is_dropout=true, Is_causal=false, Is_even_N=false, Is_even_K=false, Return_softmax=false, Params=Flash_fwd_params]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(15): here
            instantiation of "void flash_fwd_kernel<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax>(Flash_fwd_params) [with Kernel_traits=Flash_fwd_kernel_traits<160, 128, 64, 8, false, false, cutlass::half_t, Flash_kernel_traits<160, 128, 64, 8, cutlass::half_t>>, Is_dropout=true, Is_causal=false, Is_even_N=false, Is_even_K=false, Return_softmax=false]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(34): here
            instantiation of "void run_flash_fwd<Kernel_traits,Is_dropout,Is_causal>(Flash_fwd_params &, cudaStream_t) [with Kernel_traits=Flash_fwd_kernel_traits<160, 128, 64, 8, false, false, cutlass::half_t, Flash_kernel_traits<160, 128, 64, 8, cutlass::half_t>>, Is_dropout=true, Is_causal=false]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(155): here
            instantiation of "void run_mha_fwd_hdim160<T>(Flash_fwd_params &, cudaStream_t) [with T=cutlass::half_t]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_hdim160_fp16_sm80.cu(26): here

/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_kernel.h(275): warning: variable "tQrQ" was declared but never referenced
          detected during:
            instantiation of "void flash::compute_attn_1rowblock<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &, int, int, int) [with Kernel_traits=Flash_fwd_kernel_traits<160, 128, 64, 8, false, false, cutlass::half_t, Flash_kernel_traits<160, 128, 64, 8, cutlass::half_t>>, Is_dropout=true, Is_causal=false, Is_even_N=false, Is_even_K=false, Return_softmax=false, Params=Flash_fwd_params]" 
(588): here
            instantiation of "void flash::compute_attn<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &) [with Kernel_traits=Flash_fwd_kernel_traits<160, 128, 64, 8, false, false, cutlass::half_t, Flash_kernel_traits<160, 128, 64, 8, cutlass::half_t>>, Is_dropout=true, Is_causal=false, Is_even_N=false, Is_even_K=false, Return_softmax=false, Params=Flash_fwd_params]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(15): here
            instantiation of "void flash_fwd_kernel<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax>(Flash_fwd_params) [with Kernel_traits=Flash_fwd_kernel_traits<160, 128, 64, 8, false, false, cutlass::half_t, Flash_kernel_traits<160, 128, 64, 8, cutlass::half_t>>, Is_dropout=true, Is_causal=false, Is_even_N=false, Is_even_K=false, Return_softmax=false]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(34): here
            instantiation of "void run_flash_fwd<Kernel_traits,Is_dropout,Is_causal>(Flash_fwd_params &, cudaStream_t) [with Kernel_traits=Flash_fwd_kernel_traits<160, 128, 64, 8, false, false, cutlass::half_t, Flash_kernel_traits<160, 128, 64, 8, cutlass::half_t>>, Is_dropout=true, Is_causal=false]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(155): here
            instantiation of "void run_mha_fwd_hdim160<T>(Flash_fwd_params &, cudaStream_t) [with T=cutlass::half_t]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_hdim160_fp16_sm80.cu(26): here

/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_kernel.h(140): warning: variable "MMA_M" was declared but never referenced
          detected during:
            instantiation of "void flash::compute_attn_1rowblock<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &, int, int, int) [with Kernel_traits=Flash_fwd_kernel_traits<160, 128, 32, 4, false, false, cutlass::half_t, Flash_kernel_traits<160, 128, 32, 4, cutlass::half_t>>, Is_dropout=true, Is_causal=false, Is_even_N=true, Is_even_K=true, Return_softmax=true, Params=Flash_fwd_params]" 
(588): here
            instantiation of "void flash::compute_attn<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &) [with Kernel_traits=Flash_fwd_kernel_traits<160, 128, 32, 4, false, false, cutlass::half_t, Flash_kernel_traits<160, 128, 32, 4, cutlass::half_t>>, Is_dropout=true, Is_causal=false, Is_even_N=true, Is_even_K=true, Return_softmax=true, Params=Flash_fwd_params]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(15): here
            instantiation of "void flash_fwd_kernel<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax>(Flash_fwd_params) [with Kernel_traits=Flash_fwd_kernel_traits<160, 128, 32, 4, false, false, cutlass::half_t, Flash_kernel_traits<160, 128, 32, 4, cutlass::half_t>>, Is_dropout=true, Is_causal=false, Is_even_N=true, Is_even_K=true, Return_softmax=true]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(34): here
            instantiation of "void run_flash_fwd<Kernel_traits,Is_dropout,Is_causal>(Flash_fwd_params &, cudaStream_t) [with Kernel_traits=Flash_fwd_kernel_traits<160, 128, 32, 4, false, false, cutlass::half_t, Flash_kernel_traits<160, 128, 32, 4, cutlass::half_t>>, Is_dropout=true, Is_causal=false]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(155): here
            instantiation of "void run_mha_fwd_hdim160<T>(Flash_fwd_params &, cudaStream_t) [with T=cutlass::half_t]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_hdim160_fp16_sm80.cu(26): here

/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_kernel.h(275): warning: variable "tQrQ" was declared but never referenced
          detected during:
            instantiation of "void flash::compute_attn_1rowblock<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &, int, int, int) [with Kernel_traits=Flash_fwd_kernel_traits<160, 128, 32, 4, false, false, cutlass::half_t, Flash_kernel_traits<160, 128, 32, 4, cutlass::half_t>>, Is_dropout=true, Is_causal=false, Is_even_N=true, Is_even_K=true, Return_softmax=true, Params=Flash_fwd_params]" 
(588): here
            instantiation of "void flash::compute_attn<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &) [with Kernel_traits=Flash_fwd_kernel_traits<160, 128, 32, 4, false, false, cutlass::half_t, Flash_kernel_traits<160, 128, 32, 4, cutlass::half_t>>, Is_dropout=true, Is_causal=false, Is_even_N=true, Is_even_K=true, Return_softmax=true, Params=Flash_fwd_params]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(15): here
            instantiation of "void flash_fwd_kernel<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax>(Flash_fwd_params) [with Kernel_traits=Flash_fwd_kernel_traits<160, 128, 32, 4, false, false, cutlass::half_t, Flash_kernel_traits<160, 128, 32, 4, cutlass::half_t>>, Is_dropout=true, Is_causal=false, Is_even_N=true, Is_even_K=true, Return_softmax=true]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(34): here
            instantiation of "void run_flash_fwd<Kernel_traits,Is_dropout,Is_causal>(Flash_fwd_params &, cudaStream_t) [with Kernel_traits=Flash_fwd_kernel_traits<160, 128, 32, 4, false, false, cutlass::half_t, Flash_kernel_traits<160, 128, 32, 4, cutlass::half_t>>, Is_dropout=true, Is_causal=false]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(155): here
            instantiation of "void run_mha_fwd_hdim160<T>(Flash_fwd_params &, cudaStream_t) [with T=cutlass::half_t]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_hdim160_fp16_sm80.cu(26): here

/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_kernel.h(140): warning: variable "MMA_M" was declared but never referenced
          detected during:
            instantiation of "void flash::compute_attn_1rowblock<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &, int, int, int) [with Kernel_traits=Flash_fwd_kernel_traits<160, 128, 32, 4, false, false, cutlass::half_t, Flash_kernel_traits<160, 128, 32, 4, cutlass::half_t>>, Is_dropout=true, Is_causal=false, Is_even_N=true, Is_even_K=true, Return_softmax=false, Params=Flash_fwd_params]" 
(588): here
            instantiation of "void flash::compute_attn<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &) [with Kernel_traits=Flash_fwd_kernel_traits<160, 128, 32, 4, false, false, cutlass::half_t, Flash_kernel_traits<160, 128, 32, 4, cutlass::half_t>>, Is_dropout=true, Is_causal=false, Is_even_N=true, Is_even_K=true, Return_softmax=false, Params=Flash_fwd_params]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(15): here
            instantiation of "void flash_fwd_kernel<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax>(Flash_fwd_params) [with Kernel_traits=Flash_fwd_kernel_traits<160, 128, 32, 4, false, false, cutlass::half_t, Flash_kernel_traits<160, 128, 32, 4, cutlass::half_t>>, Is_dropout=true, Is_causal=false, Is_even_N=true, Is_even_K=true, Return_softmax=false]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(34): here
            instantiation of "void run_flash_fwd<Kernel_traits,Is_dropout,Is_causal>(Flash_fwd_params &, cudaStream_t) [with Kernel_traits=Flash_fwd_kernel_traits<160, 128, 32, 4, false, false, cutlass::half_t, Flash_kernel_traits<160, 128, 32, 4, cutlass::half_t>>, Is_dropout=true, Is_causal=false]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(155): here
            instantiation of "void run_mha_fwd_hdim160<T>(Flash_fwd_params &, cudaStream_t) [with T=cutlass::half_t]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_hdim160_fp16_sm80.cu(26): here

/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_kernel.h(275): warning: variable "tQrQ" was declared but never referenced
          detected during:
            instantiation of "void flash::compute_attn_1rowblock<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &, int, int, int) [with Kernel_traits=Flash_fwd_kernel_traits<160, 128, 32, 4, false, false, cutlass::half_t, Flash_kernel_traits<160, 128, 32, 4, cutlass::half_t>>, Is_dropout=true, Is_causal=false, Is_even_N=true, Is_even_K=true, Return_softmax=false, Params=Flash_fwd_params]" 
(588): here
            instantiation of "void flash::compute_attn<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &) [with Kernel_traits=Flash_fwd_kernel_traits<160, 128, 32, 4, false, false, cutlass::half_t, Flash_kernel_traits<160, 128, 32, 4, cutlass::half_t>>, Is_dropout=true, Is_causal=false, Is_even_N=true, Is_even_K=true, Return_softmax=false, Params=Flash_fwd_params]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(15): here
            instantiation of "void flash_fwd_kernel<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax>(Flash_fwd_params) [with Kernel_traits=Flash_fwd_kernel_traits<160, 128, 32, 4, false, false, cutlass::half_t, Flash_kernel_traits<160, 128, 32, 4, cutlass::half_t>>, Is_dropout=true, Is_causal=false, Is_even_N=true, Is_even_K=true, Return_softmax=false]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(34): here
            instantiation of "void run_flash_fwd<Kernel_traits,Is_dropout,Is_causal>(Flash_fwd_params &, cudaStream_t) [with Kernel_traits=Flash_fwd_kernel_traits<160, 128, 32, 4, false, false, cutlass::half_t, Flash_kernel_traits<160, 128, 32, 4, cutlass::half_t>>, Is_dropout=true, Is_causal=false]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(155): here
            instantiation of "void run_mha_fwd_hdim160<T>(Flash_fwd_params &, cudaStream_t) [with T=cutlass::half_t]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_hdim160_fp16_sm80.cu(26): here

/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_kernel.h(140): warning: variable "MMA_M" was declared but never referenced
          detected during:
            instantiation of "void flash::compute_attn_1rowblock<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &, int, int, int) [with Kernel_traits=Flash_fwd_kernel_traits<160, 128, 32, 4, false, false, cutlass::half_t, Flash_kernel_traits<160, 128, 32, 4, cutlass::half_t>>, Is_dropout=true, Is_causal=false, Is_even_N=true, Is_even_K=false, Return_softmax=true, Params=Flash_fwd_params]" 
(588): here
            instantiation of "void flash::compute_attn<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &) [with Kernel_traits=Flash_fwd_kernel_traits<160, 128, 32, 4, false, false, cutlass::half_t, Flash_kernel_traits<160, 128, 32, 4, cutlass::half_t>>, Is_dropout=true, Is_causal=false, Is_even_N=true, Is_even_K=false, Return_softmax=true, Params=Flash_fwd_params]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(15): here
            instantiation of "void flash_fwd_kernel<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax>(Flash_fwd_params) [with Kernel_traits=Flash_fwd_kernel_traits<160, 128, 32, 4, false, false, cutlass::half_t, Flash_kernel_traits<160, 128, 32, 4, cutlass::half_t>>, Is_dropout=true, Is_causal=false, Is_even_N=true, Is_even_K=false, Return_softmax=true]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(34): here
            instantiation of "void run_flash_fwd<Kernel_traits,Is_dropout,Is_causal>(Flash_fwd_params &, cudaStream_t) [with Kernel_traits=Flash_fwd_kernel_traits<160, 128, 32, 4, false, false, cutlass::half_t, Flash_kernel_traits<160, 128, 32, 4, cutlass::half_t>>, Is_dropout=true, Is_causal=false]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(155): here
            instantiation of "void run_mha_fwd_hdim160<T>(Flash_fwd_params &, cudaStream_t) [with T=cutlass::half_t]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_hdim160_fp16_sm80.cu(26): here

/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_kernel.h(275): warning: variable "tQrQ" was declared but never referenced
          detected during:
            instantiation of "void flash::compute_attn_1rowblock<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &, int, int, int) [with Kernel_traits=Flash_fwd_kernel_traits<160, 128, 32, 4, false, false, cutlass::half_t, Flash_kernel_traits<160, 128, 32, 4, cutlass::half_t>>, Is_dropout=true, Is_causal=false, Is_even_N=true, Is_even_K=false, Return_softmax=true, Params=Flash_fwd_params]" 
(588): here
            instantiation of "void flash::compute_attn<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &) [with Kernel_traits=Flash_fwd_kernel_traits<160, 128, 32, 4, false, false, cutlass::half_t, Flash_kernel_traits<160, 128, 32, 4, cutlass::half_t>>, Is_dropout=true, Is_causal=false, Is_even_N=true, Is_even_K=false, Return_softmax=true, Params=Flash_fwd_params]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(15): here
            instantiation of "void flash_fwd_kernel<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax>(Flash_fwd_params) [with Kernel_traits=Flash_fwd_kernel_traits<160, 128, 32, 4, false, false, cutlass::half_t, Flash_kernel_traits<160, 128, 32, 4, cutlass::half_t>>, Is_dropout=true, Is_causal=false, Is_even_N=true, Is_even_K=false, Return_softmax=true]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(34): here
            instantiation of "void run_flash_fwd<Kernel_traits,Is_dropout,Is_causal>(Flash_fwd_params &, cudaStream_t) [with Kernel_traits=Flash_fwd_kernel_traits<160, 128, 32, 4, false, false, cutlass::half_t, Flash_kernel_traits<160, 128, 32, 4, cutlass::half_t>>, Is_dropout=true, Is_causal=false]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(155): here
            instantiation of "void run_mha_fwd_hdim160<T>(Flash_fwd_params &, cudaStream_t) [with T=cutlass::half_t]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_hdim160_fp16_sm80.cu(26): here

/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_kernel.h(140): warning: variable "MMA_M" was declared but never referenced
          detected during:
            instantiation of "void flash::compute_attn_1rowblock<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &, int, int, int) [with Kernel_traits=Flash_fwd_kernel_traits<160, 128, 32, 4, false, false, cutlass::half_t, Flash_kernel_traits<160, 128, 32, 4, cutlass::half_t>>, Is_dropout=true, Is_causal=false, Is_even_N=true, Is_even_K=false, Return_softmax=false, Params=Flash_fwd_params]" 
(588): here
            instantiation of "void flash::compute_attn<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &) [with Kernel_traits=Flash_fwd_kernel_traits<160, 128, 32, 4, false, false, cutlass::half_t, Flash_kernel_traits<160, 128, 32, 4, cutlass::half_t>>, Is_dropout=true, Is_causal=false, Is_even_N=true, Is_even_K=false, Return_softmax=false, Params=Flash_fwd_params]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(15): here
            instantiation of "void flash_fwd_kernel<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax>(Flash_fwd_params) [with Kernel_traits=Flash_fwd_kernel_traits<160, 128, 32, 4, false, false, cutlass::half_t, Flash_kernel_traits<160, 128, 32, 4, cutlass::half_t>>, Is_dropout=true, Is_causal=false, Is_even_N=true, Is_even_K=false, Return_softmax=false]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(34): here
            instantiation of "void run_flash_fwd<Kernel_traits,Is_dropout,Is_causal>(Flash_fwd_params &, cudaStream_t) [with Kernel_traits=Flash_fwd_kernel_traits<160, 128, 32, 4, false, false, cutlass::half_t, Flash_kernel_traits<160, 128, 32, 4, cutlass::half_t>>, Is_dropout=true, Is_causal=false]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(155): here
            instantiation of "void run_mha_fwd_hdim160<T>(Flash_fwd_params &, cudaStream_t) [with T=cutlass::half_t]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_hdim160_fp16_sm80.cu(26): here

/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_kernel.h(275): warning: variable "tQrQ" was declared but never referenced
          detected during:
            instantiation of "void flash::compute_attn_1rowblock<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &, int, int, int) [with Kernel_traits=Flash_fwd_kernel_traits<160, 128, 32, 4, false, false, cutlass::half_t, Flash_kernel_traits<160, 128, 32, 4, cutlass::half_t>>, Is_dropout=true, Is_causal=false, Is_even_N=true, Is_even_K=false, Return_softmax=false, Params=Flash_fwd_params]" 
(588): here
            instantiation of "void flash::compute_attn<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &) [with Kernel_traits=Flash_fwd_kernel_traits<160, 128, 32, 4, false, false, cutlass::half_t, Flash_kernel_traits<160, 128, 32, 4, cutlass::half_t>>, Is_dropout=true, Is_causal=false, Is_even_N=true, Is_even_K=false, Return_softmax=false, Params=Flash_fwd_params]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(15): here
            instantiation of "void flash_fwd_kernel<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax>(Flash_fwd_params) [with Kernel_traits=Flash_fwd_kernel_traits<160, 128, 32, 4, false, false, cutlass::half_t, Flash_kernel_traits<160, 128, 32, 4, cutlass::half_t>>, Is_dropout=true, Is_causal=false, Is_even_N=true, Is_even_K=false, Return_softmax=false]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(34): here
            instantiation of "void run_flash_fwd<Kernel_traits,Is_dropout,Is_causal>(Flash_fwd_params &, cudaStream_t) [with Kernel_traits=Flash_fwd_kernel_traits<160, 128, 32, 4, false, false, cutlass::half_t, Flash_kernel_traits<160, 128, 32, 4, cutlass::half_t>>, Is_dropout=true, Is_causal=false]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(155): here
            instantiation of "void run_mha_fwd_hdim160<T>(Flash_fwd_params &, cudaStream_t) [with T=cutlass::half_t]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_hdim160_fp16_sm80.cu(26): here

/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_kernel.h(140): warning: variable "MMA_M" was declared but never referenced
          detected during:
            instantiation of "void flash::compute_attn_1rowblock<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &, int, int, int) [with Kernel_traits=Flash_fwd_kernel_traits<160, 128, 32, 4, false, false, cutlass::half_t, Flash_kernel_traits<160, 128, 32, 4, cutlass::half_t>>, Is_dropout=true, Is_causal=false, Is_even_N=false, Is_even_K=true, Return_softmax=true, Params=Flash_fwd_params]" 
(588): here
            instantiation of "void flash::compute_attn<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &) [with Kernel_traits=Flash_fwd_kernel_traits<160, 128, 32, 4, false, false, cutlass::half_t, Flash_kernel_traits<160, 128, 32, 4, cutlass::half_t>>, Is_dropout=true, Is_causal=false, Is_even_N=false, Is_even_K=true, Return_softmax=true, Params=Flash_fwd_params]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(15): here
            instantiation of "void flash_fwd_kernel<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax>(Flash_fwd_params) [with Kernel_traits=Flash_fwd_kernel_traits<160, 128, 32, 4, false, false, cutlass::half_t, Flash_kernel_traits<160, 128, 32, 4, cutlass::half_t>>, Is_dropout=true, Is_causal=false, Is_even_N=false, Is_even_K=true, Return_softmax=true]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(34): here
            instantiation of "void run_flash_fwd<Kernel_traits,Is_dropout,Is_causal>(Flash_fwd_params &, cudaStream_t) [with Kernel_traits=Flash_fwd_kernel_traits<160, 128, 32, 4, false, false, cutlass::half_t, Flash_kernel_traits<160, 128, 32, 4, cutlass::half_t>>, Is_dropout=true, Is_causal=false]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(155): here
            instantiation of "void run_mha_fwd_hdim160<T>(Flash_fwd_params &, cudaStream_t) [with T=cutlass::half_t]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_hdim160_fp16_sm80.cu(26): here

/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_kernel.h(275): warning: variable "tQrQ" was declared but never referenced
          detected during:
            instantiation of "void flash::compute_attn_1rowblock<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &, int, int, int) [with Kernel_traits=Flash_fwd_kernel_traits<160, 128, 32, 4, false, false, cutlass::half_t, Flash_kernel_traits<160, 128, 32, 4, cutlass::half_t>>, Is_dropout=true, Is_causal=false, Is_even_N=false, Is_even_K=true, Return_softmax=true, Params=Flash_fwd_params]" 
(588): here
            instantiation of "void flash::compute_attn<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &) [with Kernel_traits=Flash_fwd_kernel_traits<160, 128, 32, 4, false, false, cutlass::half_t, Flash_kernel_traits<160, 128, 32, 4, cutlass::half_t>>, Is_dropout=true, Is_causal=false, Is_even_N=false, Is_even_K=true, Return_softmax=true, Params=Flash_fwd_params]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(15): here
            instantiation of "void flash_fwd_kernel<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax>(Flash_fwd_params) [with Kernel_traits=Flash_fwd_kernel_traits<160, 128, 32, 4, false, false, cutlass::half_t, Flash_kernel_traits<160, 128, 32, 4, cutlass::half_t>>, Is_dropout=true, Is_causal=false, Is_even_N=false, Is_even_K=true, Return_softmax=true]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(34): here
            instantiation of "void run_flash_fwd<Kernel_traits,Is_dropout,Is_causal>(Flash_fwd_params &, cudaStream_t) [with Kernel_traits=Flash_fwd_kernel_traits<160, 128, 32, 4, false, false, cutlass::half_t, Flash_kernel_traits<160, 128, 32, 4, cutlass::half_t>>, Is_dropout=true, Is_causal=false]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(155): here
            instantiation of "void run_mha_fwd_hdim160<T>(Flash_fwd_params &, cudaStream_t) [with T=cutlass::half_t]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_hdim160_fp16_sm80.cu(26): here

/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_kernel.h(140): warning: variable "MMA_M" was declared but never referenced
          detected during:
            instantiation of "void flash::compute_attn_1rowblock<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &, int, int, int) [with Kernel_traits=Flash_fwd_kernel_traits<160, 128, 32, 4, false, false, cutlass::half_t, Flash_kernel_traits<160, 128, 32, 4, cutlass::half_t>>, Is_dropout=true, Is_causal=false, Is_even_N=false, Is_even_K=true, Return_softmax=false, Params=Flash_fwd_params]" 
(588): here
            instantiation of "void flash::compute_attn<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &) [with Kernel_traits=Flash_fwd_kernel_traits<160, 128, 32, 4, false, false, cutlass::half_t, Flash_kernel_traits<160, 128, 32, 4, cutlass::half_t>>, Is_dropout=true, Is_causal=false, Is_even_N=false, Is_even_K=true, Return_softmax=false, Params=Flash_fwd_params]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(15): here
            instantiation of "void flash_fwd_kernel<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax>(Flash_fwd_params) [with Kernel_traits=Flash_fwd_kernel_traits<160, 128, 32, 4, false, false, cutlass::half_t, Flash_kernel_traits<160, 128, 32, 4, cutlass::half_t>>, Is_dropout=true, Is_causal=false, Is_even_N=false, Is_even_K=true, Return_softmax=false]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(34): here
            instantiation of "void run_flash_fwd<Kernel_traits,Is_dropout,Is_causal>(Flash_fwd_params &, cudaStream_t) [with Kernel_traits=Flash_fwd_kernel_traits<160, 128, 32, 4, false, false, cutlass::half_t, Flash_kernel_traits<160, 128, 32, 4, cutlass::half_t>>, Is_dropout=true, Is_causal=false]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(155): here
            instantiation of "void run_mha_fwd_hdim160<T>(Flash_fwd_params &, cudaStream_t) [with T=cutlass::half_t]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_hdim160_fp16_sm80.cu(26): here

/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_kernel.h(275): warning: variable "tQrQ" was declared but never referenced
          detected during:
            instantiation of "void flash::compute_attn_1rowblock<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &, int, int, int) [with Kernel_traits=Flash_fwd_kernel_traits<160, 128, 32, 4, false, false, cutlass::half_t, Flash_kernel_traits<160, 128, 32, 4, cutlass::half_t>>, Is_dropout=true, Is_causal=false, Is_even_N=false, Is_even_K=true, Return_softmax=false, Params=Flash_fwd_params]" 
(588): here
            instantiation of "void flash::compute_attn<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &) [with Kernel_traits=Flash_fwd_kernel_traits<160, 128, 32, 4, false, false, cutlass::half_t, Flash_kernel_traits<160, 128, 32, 4, cutlass::half_t>>, Is_dropout=true, Is_causal=false, Is_even_N=false, Is_even_K=true, Return_softmax=false, Params=Flash_fwd_params]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(15): here
            instantiation of "void flash_fwd_kernel<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax>(Flash_fwd_params) [with Kernel_traits=Flash_fwd_kernel_traits<160, 128, 32, 4, false, false, cutlass::half_t, Flash_kernel_traits<160, 128, 32, 4, cutlass::half_t>>, Is_dropout=true, Is_causal=false, Is_even_N=false, Is_even_K=true, Return_softmax=false]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(34): here
            instantiation of "void run_flash_fwd<Kernel_traits,Is_dropout,Is_causal>(Flash_fwd_params &, cudaStream_t) [with Kernel_traits=Flash_fwd_kernel_traits<160, 128, 32, 4, false, false, cutlass::half_t, Flash_kernel_traits<160, 128, 32, 4, cutlass::half_t>>, Is_dropout=true, Is_causal=false]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(155): here
            instantiation of "void run_mha_fwd_hdim160<T>(Flash_fwd_params &, cudaStream_t) [with T=cutlass::half_t]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_hdim160_fp16_sm80.cu(26): here

/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_kernel.h(140): warning: variable "MMA_M" was declared but never referenced
          detected during:
            instantiation of "void flash::compute_attn_1rowblock<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &, int, int, int) [with Kernel_traits=Flash_fwd_kernel_traits<160, 128, 32, 4, false, false, cutlass::half_t, Flash_kernel_traits<160, 128, 32, 4, cutlass::half_t>>, Is_dropout=true, Is_causal=false, Is_even_N=false, Is_even_K=false, Return_softmax=true, Params=Flash_fwd_params]" 
(588): here
            instantiation of "void flash::compute_attn<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &) [with Kernel_traits=Flash_fwd_kernel_traits<160, 128, 32, 4, false, false, cutlass::half_t, Flash_kernel_traits<160, 128, 32, 4, cutlass::half_t>>, Is_dropout=true, Is_causal=false, Is_even_N=false, Is_even_K=false, Return_softmax=true, Params=Flash_fwd_params]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(15): here
            instantiation of "void flash_fwd_kernel<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax>(Flash_fwd_params) [with Kernel_traits=Flash_fwd_kernel_traits<160, 128, 32, 4, false, false, cutlass::half_t, Flash_kernel_traits<160, 128, 32, 4, cutlass::half_t>>, Is_dropout=true, Is_causal=false, Is_even_N=false, Is_even_K=false, Return_softmax=true]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(34): here
            instantiation of "void run_flash_fwd<Kernel_traits,Is_dropout,Is_causal>(Flash_fwd_params &, cudaStream_t) [with Kernel_traits=Flash_fwd_kernel_traits<160, 128, 32, 4, false, false, cutlass::half_t, Flash_kernel_traits<160, 128, 32, 4, cutlass::half_t>>, Is_dropout=true, Is_causal=false]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(155): here
            instantiation of "void run_mha_fwd_hdim160<T>(Flash_fwd_params &, cudaStream_t) [with T=cutlass::half_t]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_hdim160_fp16_sm80.cu(26): here

/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_kernel.h(275): warning: variable "tQrQ" was declared but never referenced
          detected during:
            instantiation of "void flash::compute_attn_1rowblock<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &, int, int, int) [with Kernel_traits=Flash_fwd_kernel_traits<160, 128, 32, 4, false, false, cutlass::half_t, Flash_kernel_traits<160, 128, 32, 4, cutlass::half_t>>, Is_dropout=true, Is_causal=false, Is_even_N=false, Is_even_K=false, Return_softmax=true, Params=Flash_fwd_params]" 
(588): here
            instantiation of "void flash::compute_attn<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &) [with Kernel_traits=Flash_fwd_kernel_traits<160, 128, 32, 4, false, false, cutlass::half_t, Flash_kernel_traits<160, 128, 32, 4, cutlass::half_t>>, Is_dropout=true, Is_causal=false, Is_even_N=false, Is_even_K=false, Return_softmax=true, Params=Flash_fwd_params]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(15): here
            instantiation of "void flash_fwd_kernel<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax>(Flash_fwd_params) [with Kernel_traits=Flash_fwd_kernel_traits<160, 128, 32, 4, false, false, cutlass::half_t, Flash_kernel_traits<160, 128, 32, 4, cutlass::half_t>>, Is_dropout=true, Is_causal=false, Is_even_N=false, Is_even_K=false, Return_softmax=true]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(34): here
            instantiation of "void run_flash_fwd<Kernel_traits,Is_dropout,Is_causal>(Flash_fwd_params &, cudaStream_t) [with Kernel_traits=Flash_fwd_kernel_traits<160, 128, 32, 4, false, false, cutlass::half_t, Flash_kernel_traits<160, 128, 32, 4, cutlass::half_t>>, Is_dropout=true, Is_causal=false]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(155): here
            instantiation of "void run_mha_fwd_hdim160<T>(Flash_fwd_params &, cudaStream_t) [with T=cutlass::half_t]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_hdim160_fp16_sm80.cu(26): here

/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_kernel.h(140): warning: variable "MMA_M" was declared but never referenced
          detected during:
            instantiation of "void flash::compute_attn_1rowblock<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &, int, int, int) [with Kernel_traits=Flash_fwd_kernel_traits<160, 128, 32, 4, false, false, cutlass::half_t, Flash_kernel_traits<160, 128, 32, 4, cutlass::half_t>>, Is_dropout=true, Is_causal=false, Is_even_N=false, Is_even_K=false, Return_softmax=false, Params=Flash_fwd_params]" 
(588): here
            instantiation of "void flash::compute_attn<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &) [with Kernel_traits=Flash_fwd_kernel_traits<160, 128, 32, 4, false, false, cutlass::half_t, Flash_kernel_traits<160, 128, 32, 4, cutlass::half_t>>, Is_dropout=true, Is_causal=false, Is_even_N=false, Is_even_K=false, Return_softmax=false, Params=Flash_fwd_params]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(15): here
            instantiation of "void flash_fwd_kernel<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax>(Flash_fwd_params) [with Kernel_traits=Flash_fwd_kernel_traits<160, 128, 32, 4, false, false, cutlass::half_t, Flash_kernel_traits<160, 128, 32, 4, cutlass::half_t>>, Is_dropout=true, Is_causal=false, Is_even_N=false, Is_even_K=false, Return_softmax=false]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(34): here
            instantiation of "void run_flash_fwd<Kernel_traits,Is_dropout,Is_causal>(Flash_fwd_params &, cudaStream_t) [with Kernel_traits=Flash_fwd_kernel_traits<160, 128, 32, 4, false, false, cutlass::half_t, Flash_kernel_traits<160, 128, 32, 4, cutlass::half_t>>, Is_dropout=true, Is_causal=false]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(155): here
            instantiation of "void run_mha_fwd_hdim160<T>(Flash_fwd_params &, cudaStream_t) [with T=cutlass::half_t]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_hdim160_fp16_sm80.cu(26): here

/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_kernel.h(275): warning: variable "tQrQ" was declared but never referenced
          detected during:
            instantiation of "void flash::compute_attn_1rowblock<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &, int, int, int) [with Kernel_traits=Flash_fwd_kernel_traits<160, 128, 32, 4, false, false, cutlass::half_t, Flash_kernel_traits<160, 128, 32, 4, cutlass::half_t>>, Is_dropout=true, Is_causal=false, Is_even_N=false, Is_even_K=false, Return_softmax=false, Params=Flash_fwd_params]" 
(588): here
            instantiation of "void flash::compute_attn<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &) [with Kernel_traits=Flash_fwd_kernel_traits<160, 128, 32, 4, false, false, cutlass::half_t, Flash_kernel_traits<160, 128, 32, 4, cutlass::half_t>>, Is_dropout=true, Is_causal=false, Is_even_N=false, Is_even_K=false, Return_softmax=false, Params=Flash_fwd_params]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(15): here
            instantiation of "void flash_fwd_kernel<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax>(Flash_fwd_params) [with Kernel_traits=Flash_fwd_kernel_traits<160, 128, 32, 4, false, false, cutlass::half_t, Flash_kernel_traits<160, 128, 32, 4, cutlass::half_t>>, Is_dropout=true, Is_causal=false, Is_even_N=false, Is_even_K=false, Return_softmax=false]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(34): here
            instantiation of "void run_flash_fwd<Kernel_traits,Is_dropout,Is_causal>(Flash_fwd_params &, cudaStream_t) [with Kernel_traits=Flash_fwd_kernel_traits<160, 128, 32, 4, false, false, cutlass::half_t, Flash_kernel_traits<160, 128, 32, 4, cutlass::half_t>>, Is_dropout=true, Is_causal=false]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(155): here
            instantiation of "void run_mha_fwd_hdim160<T>(Flash_fwd_params &, cudaStream_t) [with T=cutlass::half_t]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_hdim160_fp16_sm80.cu(26): here

/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_kernel.h(140): warning: variable "MMA_M" was declared but never referenced
          detected during:
            instantiation of "void flash::compute_attn_1rowblock<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &, int, int, int) [with Kernel_traits=Flash_fwd_kernel_traits<160, 64, 64, 4, false, false, cutlass::half_t, Flash_kernel_traits<160, 64, 64, 4, cutlass::half_t>>, Is_dropout=false, Is_causal=true, Is_even_N=true, Is_even_K=true, Return_softmax=false, Params=Flash_fwd_params]" 
(588): here
            instantiation of "void flash::compute_attn<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &) [with Kernel_traits=Flash_fwd_kernel_traits<160, 64, 64, 4, false, false, cutlass::half_t, Flash_kernel_traits<160, 64, 64, 4, cutlass::half_t>>, Is_dropout=false, Is_causal=true, Is_even_N=true, Is_even_K=true, Return_softmax=false, Params=Flash_fwd_params]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(15): here
            instantiation of "void flash_fwd_kernel<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax>(Flash_fwd_params) [with Kernel_traits=Flash_fwd_kernel_traits<160, 64, 64, 4, false, false, cutlass::half_t, Flash_kernel_traits<160, 64, 64, 4, cutlass::half_t>>, Is_dropout=false, Is_causal=true, Is_even_N=true, Is_even_K=true, Return_softmax=false]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(34): here
            instantiation of "void run_flash_fwd<Kernel_traits,Is_dropout,Is_causal>(Flash_fwd_params &, cudaStream_t) [with Kernel_traits=Flash_fwd_kernel_traits<160, 64, 64, 4, false, false, cutlass::half_t, Flash_kernel_traits<160, 64, 64, 4, cutlass::half_t>>, Is_dropout=false, Is_causal=true]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(155): here
            instantiation of "void run_mha_fwd_hdim160<T>(Flash_fwd_params &, cudaStream_t) [with T=cutlass::half_t]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_hdim160_fp16_sm80.cu(26): here

/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_kernel.h(275): warning: variable "tQrQ" was declared but never referenced
          detected during:
            instantiation of "void flash::compute_attn_1rowblock<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &, int, int, int) [with Kernel_traits=Flash_fwd_kernel_traits<160, 64, 64, 4, false, false, cutlass::half_t, Flash_kernel_traits<160, 64, 64, 4, cutlass::half_t>>, Is_dropout=false, Is_causal=true, Is_even_N=true, Is_even_K=true, Return_softmax=false, Params=Flash_fwd_params]" 
(588): here
            instantiation of "void flash::compute_attn<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &) [with Kernel_traits=Flash_fwd_kernel_traits<160, 64, 64, 4, false, false, cutlass::half_t, Flash_kernel_traits<160, 64, 64, 4, cutlass::half_t>>, Is_dropout=false, Is_causal=true, Is_even_N=true, Is_even_K=true, Return_softmax=false, Params=Flash_fwd_params]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(15): here
            instantiation of "void flash_fwd_kernel<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax>(Flash_fwd_params) [with Kernel_traits=Flash_fwd_kernel_traits<160, 64, 64, 4, false, false, cutlass::half_t, Flash_kernel_traits<160, 64, 64, 4, cutlass::half_t>>, Is_dropout=false, Is_causal=true, Is_even_N=true, Is_even_K=true, Return_softmax=false]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(34): here
            instantiation of "void run_flash_fwd<Kernel_traits,Is_dropout,Is_causal>(Flash_fwd_params &, cudaStream_t) [with Kernel_traits=Flash_fwd_kernel_traits<160, 64, 64, 4, false, false, cutlass::half_t, Flash_kernel_traits<160, 64, 64, 4, cutlass::half_t>>, Is_dropout=false, Is_causal=true]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(155): here
            instantiation of "void run_mha_fwd_hdim160<T>(Flash_fwd_params &, cudaStream_t) [with T=cutlass::half_t]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_hdim160_fp16_sm80.cu(26): here

/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_kernel.h(140): warning: variable "MMA_M" was declared but never referenced
          detected during:
            instantiation of "void flash::compute_attn_1rowblock<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &, int, int, int) [with Kernel_traits=Flash_fwd_kernel_traits<160, 64, 64, 4, false, false, cutlass::half_t, Flash_kernel_traits<160, 64, 64, 4, cutlass::half_t>>, Is_dropout=false, Is_causal=true, Is_even_N=true, Is_even_K=false, Return_softmax=false, Params=Flash_fwd_params]" 
(588): here
            instantiation of "void flash::compute_attn<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &) [with Kernel_traits=Flash_fwd_kernel_traits<160, 64, 64, 4, false, false, cutlass::half_t, Flash_kernel_traits<160, 64, 64, 4, cutlass::half_t>>, Is_dropout=false, Is_causal=true, Is_even_N=true, Is_even_K=false, Return_softmax=false, Params=Flash_fwd_params]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(15): here
            instantiation of "void flash_fwd_kernel<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax>(Flash_fwd_params) [with Kernel_traits=Flash_fwd_kernel_traits<160, 64, 64, 4, false, false, cutlass::half_t, Flash_kernel_traits<160, 64, 64, 4, cutlass::half_t>>, Is_dropout=false, Is_causal=true, Is_even_N=true, Is_even_K=false, Return_softmax=false]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(34): here
            instantiation of "void run_flash_fwd<Kernel_traits,Is_dropout,Is_causal>(Flash_fwd_params &, cudaStream_t) [with Kernel_traits=Flash_fwd_kernel_traits<160, 64, 64, 4, false, false, cutlass::half_t, Flash_kernel_traits<160, 64, 64, 4, cutlass::half_t>>, Is_dropout=false, Is_causal=true]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(155): here
            instantiation of "void run_mha_fwd_hdim160<T>(Flash_fwd_params &, cudaStream_t) [with T=cutlass::half_t]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_hdim160_fp16_sm80.cu(26): here

/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_kernel.h(275): warning: variable "tQrQ" was declared but never referenced
          detected during:
            instantiation of "void flash::compute_attn_1rowblock<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &, int, int, int) [with Kernel_traits=Flash_fwd_kernel_traits<160, 64, 64, 4, false, false, cutlass::half_t, Flash_kernel_traits<160, 64, 64, 4, cutlass::half_t>>, Is_dropout=false, Is_causal=true, Is_even_N=true, Is_even_K=false, Return_softmax=false, Params=Flash_fwd_params]" 
(588): here
            instantiation of "void flash::compute_attn<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &) [with Kernel_traits=Flash_fwd_kernel_traits<160, 64, 64, 4, false, false, cutlass::half_t, Flash_kernel_traits<160, 64, 64, 4, cutlass::half_t>>, Is_dropout=false, Is_causal=true, Is_even_N=true, Is_even_K=false, Return_softmax=false, Params=Flash_fwd_params]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(15): here
            instantiation of "void flash_fwd_kernel<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax>(Flash_fwd_params) [with Kernel_traits=Flash_fwd_kernel_traits<160, 64, 64, 4, false, false, cutlass::half_t, Flash_kernel_traits<160, 64, 64, 4, cutlass::half_t>>, Is_dropout=false, Is_causal=true, Is_even_N=true, Is_even_K=false, Return_softmax=false]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(34): here
            instantiation of "void run_flash_fwd<Kernel_traits,Is_dropout,Is_causal>(Flash_fwd_params &, cudaStream_t) [with Kernel_traits=Flash_fwd_kernel_traits<160, 64, 64, 4, false, false, cutlass::half_t, Flash_kernel_traits<160, 64, 64, 4, cutlass::half_t>>, Is_dropout=false, Is_causal=true]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(155): here
            instantiation of "void run_mha_fwd_hdim160<T>(Flash_fwd_params &, cudaStream_t) [with T=cutlass::half_t]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_hdim160_fp16_sm80.cu(26): here

/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_kernel.h(140): warning: variable "MMA_M" was declared but never referenced
          detected during:
            instantiation of "void flash::compute_attn_1rowblock<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &, int, int, int) [with Kernel_traits=Flash_fwd_kernel_traits<160, 64, 64, 4, false, false, cutlass::half_t, Flash_kernel_traits<160, 64, 64, 4, cutlass::half_t>>, Is_dropout=false, Is_causal=true, Is_even_N=false, Is_even_K=true, Return_softmax=false, Params=Flash_fwd_params]" 
(588): here
            instantiation of "void flash::compute_attn<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &) [with Kernel_traits=Flash_fwd_kernel_traits<160, 64, 64, 4, false, false, cutlass::half_t, Flash_kernel_traits<160, 64, 64, 4, cutlass::half_t>>, Is_dropout=false, Is_causal=true, Is_even_N=false, Is_even_K=true, Return_softmax=false, Params=Flash_fwd_params]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(15): here
            instantiation of "void flash_fwd_kernel<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax>(Flash_fwd_params) [with Kernel_traits=Flash_fwd_kernel_traits<160, 64, 64, 4, false, false, cutlass::half_t, Flash_kernel_traits<160, 64, 64, 4, cutlass::half_t>>, Is_dropout=false, Is_causal=true, Is_even_N=false, Is_even_K=true, Return_softmax=false]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(34): here
            instantiation of "void run_flash_fwd<Kernel_traits,Is_dropout,Is_causal>(Flash_fwd_params &, cudaStream_t) [with Kernel_traits=Flash_fwd_kernel_traits<160, 64, 64, 4, false, false, cutlass::half_t, Flash_kernel_traits<160, 64, 64, 4, cutlass::half_t>>, Is_dropout=false, Is_causal=true]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(155): here
            instantiation of "void run_mha_fwd_hdim160<T>(Flash_fwd_params &, cudaStream_t) [with T=cutlass::half_t]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_hdim160_fp16_sm80.cu(26): here

/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_kernel.h(275): warning: variable "tQrQ" was declared but never referenced
          detected during:
            instantiation of "void flash::compute_attn_1rowblock<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &, int, int, int) [with Kernel_traits=Flash_fwd_kernel_traits<160, 64, 64, 4, false, false, cutlass::half_t, Flash_kernel_traits<160, 64, 64, 4, cutlass::half_t>>, Is_dropout=false, Is_causal=true, Is_even_N=false, Is_even_K=true, Return_softmax=false, Params=Flash_fwd_params]" 
(588): here
            instantiation of "void flash::compute_attn<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &) [with Kernel_traits=Flash_fwd_kernel_traits<160, 64, 64, 4, false, false, cutlass::half_t, Flash_kernel_traits<160, 64, 64, 4, cutlass::half_t>>, Is_dropout=false, Is_causal=true, Is_even_N=false, Is_even_K=true, Return_softmax=false, Params=Flash_fwd_params]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(15): here
            instantiation of "void flash_fwd_kernel<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax>(Flash_fwd_params) [with Kernel_traits=Flash_fwd_kernel_traits<160, 64, 64, 4, false, false, cutlass::half_t, Flash_kernel_traits<160, 64, 64, 4, cutlass::half_t>>, Is_dropout=false, Is_causal=true, Is_even_N=false, Is_even_K=true, Return_softmax=false]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(34): here
            instantiation of "void run_flash_fwd<Kernel_traits,Is_dropout,Is_causal>(Flash_fwd_params &, cudaStream_t) [with Kernel_traits=Flash_fwd_kernel_traits<160, 64, 64, 4, false, false, cutlass::half_t, Flash_kernel_traits<160, 64, 64, 4, cutlass::half_t>>, Is_dropout=false, Is_causal=true]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(155): here
            instantiation of "void run_mha_fwd_hdim160<T>(Flash_fwd_params &, cudaStream_t) [with T=cutlass::half_t]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_hdim160_fp16_sm80.cu(26): here

/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_kernel.h(140): warning: variable "MMA_M" was declared but never referenced
          detected during:
            instantiation of "void flash::compute_attn_1rowblock<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &, int, int, int) [with Kernel_traits=Flash_fwd_kernel_traits<160, 64, 64, 4, false, false, cutlass::half_t, Flash_kernel_traits<160, 64, 64, 4, cutlass::half_t>>, Is_dropout=false, Is_causal=true, Is_even_N=false, Is_even_K=false, Return_softmax=false, Params=Flash_fwd_params]" 
(588): here
            instantiation of "void flash::compute_attn<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &) [with Kernel_traits=Flash_fwd_kernel_traits<160, 64, 64, 4, false, false, cutlass::half_t, Flash_kernel_traits<160, 64, 64, 4, cutlass::half_t>>, Is_dropout=false, Is_causal=true, Is_even_N=false, Is_even_K=false, Return_softmax=false, Params=Flash_fwd_params]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(15): here
            instantiation of "void flash_fwd_kernel<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax>(Flash_fwd_params) [with Kernel_traits=Flash_fwd_kernel_traits<160, 64, 64, 4, false, false, cutlass::half_t, Flash_kernel_traits<160, 64, 64, 4, cutlass::half_t>>, Is_dropout=false, Is_causal=true, Is_even_N=false, Is_even_K=false, Return_softmax=false]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(34): here
            instantiation of "void run_flash_fwd<Kernel_traits,Is_dropout,Is_causal>(Flash_fwd_params &, cudaStream_t) [with Kernel_traits=Flash_fwd_kernel_traits<160, 64, 64, 4, false, false, cutlass::half_t, Flash_kernel_traits<160, 64, 64, 4, cutlass::half_t>>, Is_dropout=false, Is_causal=true]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(155): here
            instantiation of "void run_mha_fwd_hdim160<T>(Flash_fwd_params &, cudaStream_t) [with T=cutlass::half_t]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_hdim160_fp16_sm80.cu(26): here

/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_kernel.h(275): warning: variable "tQrQ" was declared but never referenced
          detected during:
            instantiation of "void flash::compute_attn_1rowblock<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &, int, int, int) [with Kernel_traits=Flash_fwd_kernel_traits<160, 64, 64, 4, false, false, cutlass::half_t, Flash_kernel_traits<160, 64, 64, 4, cutlass::half_t>>, Is_dropout=false, Is_causal=true, Is_even_N=false, Is_even_K=false, Return_softmax=false, Params=Flash_fwd_params]" 
(588): here
            instantiation of "void flash::compute_attn<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &) [with Kernel_traits=Flash_fwd_kernel_traits<160, 64, 64, 4, false, false, cutlass::half_t, Flash_kernel_traits<160, 64, 64, 4, cutlass::half_t>>, Is_dropout=false, Is_causal=true, Is_even_N=false, Is_even_K=false, Return_softmax=false, Params=Flash_fwd_params]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(15): here
            instantiation of "void flash_fwd_kernel<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax>(Flash_fwd_params) [with Kernel_traits=Flash_fwd_kernel_traits<160, 64, 64, 4, false, false, cutlass::half_t, Flash_kernel_traits<160, 64, 64, 4, cutlass::half_t>>, Is_dropout=false, Is_causal=true, Is_even_N=false, Is_even_K=false, Return_softmax=false]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(34): here
            instantiation of "void run_flash_fwd<Kernel_traits,Is_dropout,Is_causal>(Flash_fwd_params &, cudaStream_t) [with Kernel_traits=Flash_fwd_kernel_traits<160, 64, 64, 4, false, false, cutlass::half_t, Flash_kernel_traits<160, 64, 64, 4, cutlass::half_t>>, Is_dropout=false, Is_causal=true]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(155): here
            instantiation of "void run_mha_fwd_hdim160<T>(Flash_fwd_params &, cudaStream_t) [with T=cutlass::half_t]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_hdim160_fp16_sm80.cu(26): here

/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_kernel.h(140): warning: variable "MMA_M" was declared but never referenced
          detected during:
            instantiation of "void flash::compute_attn_1rowblock<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &, int, int, int) [with Kernel_traits=Flash_fwd_kernel_traits<160, 128, 32, 4, false, false, cutlass::half_t, Flash_kernel_traits<160, 128, 32, 4, cutlass::half_t>>, Is_dropout=false, Is_causal=true, Is_even_N=true, Is_even_K=true, Return_softmax=false, Params=Flash_fwd_params]" 
(588): here
            instantiation of "void flash::compute_attn<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &) [with Kernel_traits=Flash_fwd_kernel_traits<160, 128, 32, 4, false, false, cutlass::half_t, Flash_kernel_traits<160, 128, 32, 4, cutlass::half_t>>, Is_dropout=false, Is_causal=true, Is_even_N=true, Is_even_K=true, Return_softmax=false, Params=Flash_fwd_params]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(15): here
            instantiation of "void flash_fwd_kernel<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax>(Flash_fwd_params) [with Kernel_traits=Flash_fwd_kernel_traits<160, 128, 32, 4, false, false, cutlass::half_t, Flash_kernel_traits<160, 128, 32, 4, cutlass::half_t>>, Is_dropout=false, Is_causal=true, Is_even_N=true, Is_even_K=true, Return_softmax=false]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(34): here
            instantiation of "void run_flash_fwd<Kernel_traits,Is_dropout,Is_causal>(Flash_fwd_params &, cudaStream_t) [with Kernel_traits=Flash_fwd_kernel_traits<160, 128, 32, 4, false, false, cutlass::half_t, Flash_kernel_traits<160, 128, 32, 4, cutlass::half_t>>, Is_dropout=false, Is_causal=true]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(155): here
            instantiation of "void run_mha_fwd_hdim160<T>(Flash_fwd_params &, cudaStream_t) [with T=cutlass::half_t]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_hdim160_fp16_sm80.cu(26): here

/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_kernel.h(275): warning: variable "tQrQ" was declared but never referenced
          detected during:
            instantiation of "void flash::compute_attn_1rowblock<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &, int, int, int) [with Kernel_traits=Flash_fwd_kernel_traits<160, 128, 32, 4, false, false, cutlass::half_t, Flash_kernel_traits<160, 128, 32, 4, cutlass::half_t>>, Is_dropout=false, Is_causal=true, Is_even_N=true, Is_even_K=true, Return_softmax=false, Params=Flash_fwd_params]" 
(588): here
            instantiation of "void flash::compute_attn<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &) [with Kernel_traits=Flash_fwd_kernel_traits<160, 128, 32, 4, false, false, cutlass::half_t, Flash_kernel_traits<160, 128, 32, 4, cutlass::half_t>>, Is_dropout=false, Is_causal=true, Is_even_N=true, Is_even_K=true, Return_softmax=false, Params=Flash_fwd_params]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(15): here
            instantiation of "void flash_fwd_kernel<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax>(Flash_fwd_params) [with Kernel_traits=Flash_fwd_kernel_traits<160, 128, 32, 4, false, false, cutlass::half_t, Flash_kernel_traits<160, 128, 32, 4, cutlass::half_t>>, Is_dropout=false, Is_causal=true, Is_even_N=true, Is_even_K=true, Return_softmax=false]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(34): here
            instantiation of "void run_flash_fwd<Kernel_traits,Is_dropout,Is_causal>(Flash_fwd_params &, cudaStream_t) [with Kernel_traits=Flash_fwd_kernel_traits<160, 128, 32, 4, false, false, cutlass::half_t, Flash_kernel_traits<160, 128, 32, 4, cutlass::half_t>>, Is_dropout=false, Is_causal=true]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(155): here
            instantiation of "void run_mha_fwd_hdim160<T>(Flash_fwd_params &, cudaStream_t) [with T=cutlass::half_t]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_hdim160_fp16_sm80.cu(26): here

/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_kernel.h(140): warning: variable "MMA_M" was declared but never referenced
          detected during:
            instantiation of "void flash::compute_attn_1rowblock<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &, int, int, int) [with Kernel_traits=Flash_fwd_kernel_traits<160, 128, 32, 4, false, false, cutlass::half_t, Flash_kernel_traits<160, 128, 32, 4, cutlass::half_t>>, Is_dropout=false, Is_causal=true, Is_even_N=true, Is_even_K=false, Return_softmax=false, Params=Flash_fwd_params]" 
(588): here
            instantiation of "void flash::compute_attn<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &) [with Kernel_traits=Flash_fwd_kernel_traits<160, 128, 32, 4, false, false, cutlass::half_t, Flash_kernel_traits<160, 128, 32, 4, cutlass::half_t>>, Is_dropout=false, Is_causal=true, Is_even_N=true, Is_even_K=false, Return_softmax=false, Params=Flash_fwd_params]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(15): here
            instantiation of "void flash_fwd_kernel<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax>(Flash_fwd_params) [with Kernel_traits=Flash_fwd_kernel_traits<160, 128, 32, 4, false, false, cutlass::half_t, Flash_kernel_traits<160, 128, 32, 4, cutlass::half_t>>, Is_dropout=false, Is_causal=true, Is_even_N=true, Is_even_K=false, Return_softmax=false]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(34): here
            instantiation of "void run_flash_fwd<Kernel_traits,Is_dropout,Is_causal>(Flash_fwd_params &, cudaStream_t) [with Kernel_traits=Flash_fwd_kernel_traits<160, 128, 32, 4, false, false, cutlass::half_t, Flash_kernel_traits<160, 128, 32, 4, cutlass::half_t>>, Is_dropout=false, Is_causal=true]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(155): here
            instantiation of "void run_mha_fwd_hdim160<T>(Flash_fwd_params &, cudaStream_t) [with T=cutlass::half_t]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_hdim160_fp16_sm80.cu(26): here

/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_kernel.h(275): warning: variable "tQrQ" was declared but never referenced
          detected during:
            instantiation of "void flash::compute_attn_1rowblock<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &, int, int, int) [with Kernel_traits=Flash_fwd_kernel_traits<160, 128, 32, 4, false, false, cutlass::half_t, Flash_kernel_traits<160, 128, 32, 4, cutlass::half_t>>, Is_dropout=false, Is_causal=true, Is_even_N=true, Is_even_K=false, Return_softmax=false, Params=Flash_fwd_params]" 
(588): here
            instantiation of "void flash::compute_attn<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &) [with Kernel_traits=Flash_fwd_kernel_traits<160, 128, 32, 4, false, false, cutlass::half_t, Flash_kernel_traits<160, 128, 32, 4, cutlass::half_t>>, Is_dropout=false, Is_causal=true, Is_even_N=true, Is_even_K=false, Return_softmax=false, Params=Flash_fwd_params]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(15): here
            instantiation of "void flash_fwd_kernel<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax>(Flash_fwd_params) [with Kernel_traits=Flash_fwd_kernel_traits<160, 128, 32, 4, false, false, cutlass::half_t, Flash_kernel_traits<160, 128, 32, 4, cutlass::half_t>>, Is_dropout=false, Is_causal=true, Is_even_N=true, Is_even_K=false, Return_softmax=false]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(34): here
            instantiation of "void run_flash_fwd<Kernel_traits,Is_dropout,Is_causal>(Flash_fwd_params &, cudaStream_t) [with Kernel_traits=Flash_fwd_kernel_traits<160, 128, 32, 4, false, false, cutlass::half_t, Flash_kernel_traits<160, 128, 32, 4, cutlass::half_t>>, Is_dropout=false, Is_causal=true]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(155): here
            instantiation of "void run_mha_fwd_hdim160<T>(Flash_fwd_params &, cudaStream_t) [with T=cutlass::half_t]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_hdim160_fp16_sm80.cu(26): here

/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_kernel.h(140): warning: variable "MMA_M" was declared but never referenced
          detected during:
            instantiation of "void flash::compute_attn_1rowblock<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &, int, int, int) [with Kernel_traits=Flash_fwd_kernel_traits<160, 128, 32, 4, false, false, cutlass::half_t, Flash_kernel_traits<160, 128, 32, 4, cutlass::half_t>>, Is_dropout=false, Is_causal=true, Is_even_N=false, Is_even_K=true, Return_softmax=false, Params=Flash_fwd_params]" 
(588): here
            instantiation of "void flash::compute_attn<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &) [with Kernel_traits=Flash_fwd_kernel_traits<160, 128, 32, 4, false, false, cutlass::half_t, Flash_kernel_traits<160, 128, 32, 4, cutlass::half_t>>, Is_dropout=false, Is_causal=true, Is_even_N=false, Is_even_K=true, Return_softmax=false, Params=Flash_fwd_params]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(15): here
            instantiation of "void flash_fwd_kernel<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax>(Flash_fwd_params) [with Kernel_traits=Flash_fwd_kernel_traits<160, 128, 32, 4, false, false, cutlass::half_t, Flash_kernel_traits<160, 128, 32, 4, cutlass::half_t>>, Is_dropout=false, Is_causal=true, Is_even_N=false, Is_even_K=true, Return_softmax=false]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(34): here
            instantiation of "void run_flash_fwd<Kernel_traits,Is_dropout,Is_causal>(Flash_fwd_params &, cudaStream_t) [with Kernel_traits=Flash_fwd_kernel_traits<160, 128, 32, 4, false, false, cutlass::half_t, Flash_kernel_traits<160, 128, 32, 4, cutlass::half_t>>, Is_dropout=false, Is_causal=true]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(155): here
            instantiation of "void run_mha_fwd_hdim160<T>(Flash_fwd_params &, cudaStream_t) [with T=cutlass::half_t]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_hdim160_fp16_sm80.cu(26): here

/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_kernel.h(275): warning: variable "tQrQ" was declared but never referenced
          detected during:
            instantiation of "void flash::compute_attn_1rowblock<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &, int, int, int) [with Kernel_traits=Flash_fwd_kernel_traits<160, 128, 32, 4, false, false, cutlass::half_t, Flash_kernel_traits<160, 128, 32, 4, cutlass::half_t>>, Is_dropout=false, Is_causal=true, Is_even_N=false, Is_even_K=true, Return_softmax=false, Params=Flash_fwd_params]" 
(588): here
            instantiation of "void flash::compute_attn<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &) [with Kernel_traits=Flash_fwd_kernel_traits<160, 128, 32, 4, false, false, cutlass::half_t, Flash_kernel_traits<160, 128, 32, 4, cutlass::half_t>>, Is_dropout=false, Is_causal=true, Is_even_N=false, Is_even_K=true, Return_softmax=false, Params=Flash_fwd_params]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(15): here
            instantiation of "void flash_fwd_kernel<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax>(Flash_fwd_params) [with Kernel_traits=Flash_fwd_kernel_traits<160, 128, 32, 4, false, false, cutlass::half_t, Flash_kernel_traits<160, 128, 32, 4, cutlass::half_t>>, Is_dropout=false, Is_causal=true, Is_even_N=false, Is_even_K=true, Return_softmax=false]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(34): here
            instantiation of "void run_flash_fwd<Kernel_traits,Is_dropout,Is_causal>(Flash_fwd_params &, cudaStream_t) [with Kernel_traits=Flash_fwd_kernel_traits<160, 128, 32, 4, false, false, cutlass::half_t, Flash_kernel_traits<160, 128, 32, 4, cutlass::half_t>>, Is_dropout=false, Is_causal=true]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(155): here
            instantiation of "void run_mha_fwd_hdim160<T>(Flash_fwd_params &, cudaStream_t) [with T=cutlass::half_t]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_hdim160_fp16_sm80.cu(26): here

/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_kernel.h(140): warning: variable "MMA_M" was declared but never referenced
          detected during:
            instantiation of "void flash::compute_attn_1rowblock<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &, int, int, int) [with Kernel_traits=Flash_fwd_kernel_traits<160, 128, 32, 4, false, false, cutlass::half_t, Flash_kernel_traits<160, 128, 32, 4, cutlass::half_t>>, Is_dropout=false, Is_causal=true, Is_even_N=false, Is_even_K=false, Return_softmax=false, Params=Flash_fwd_params]" 
(588): here
            instantiation of "void flash::compute_attn<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &) [with Kernel_traits=Flash_fwd_kernel_traits<160, 128, 32, 4, false, false, cutlass::half_t, Flash_kernel_traits<160, 128, 32, 4, cutlass::half_t>>, Is_dropout=false, Is_causal=true, Is_even_N=false, Is_even_K=false, Return_softmax=false, Params=Flash_fwd_params]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(15): here
            instantiation of "void flash_fwd_kernel<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax>(Flash_fwd_params) [with Kernel_traits=Flash_fwd_kernel_traits<160, 128, 32, 4, false, false, cutlass::half_t, Flash_kernel_traits<160, 128, 32, 4, cutlass::half_t>>, Is_dropout=false, Is_causal=true, Is_even_N=false, Is_even_K=false, Return_softmax=false]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(34): here
            instantiation of "void run_flash_fwd<Kernel_traits,Is_dropout,Is_causal>(Flash_fwd_params &, cudaStream_t) [with Kernel_traits=Flash_fwd_kernel_traits<160, 128, 32, 4, false, false, cutlass::half_t, Flash_kernel_traits<160, 128, 32, 4, cutlass::half_t>>, Is_dropout=false, Is_causal=true]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(155): here
            instantiation of "void run_mha_fwd_hdim160<T>(Flash_fwd_params &, cudaStream_t) [with T=cutlass::half_t]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_hdim160_fp16_sm80.cu(26): here

/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_kernel.h(275): warning: variable "tQrQ" was declared but never referenced
          detected during:
            instantiation of "void flash::compute_attn_1rowblock<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &, int, int, int) [with Kernel_traits=Flash_fwd_kernel_traits<160, 128, 32, 4, false, false, cutlass::half_t, Flash_kernel_traits<160, 128, 32, 4, cutlass::half_t>>, Is_dropout=false, Is_causal=true, Is_even_N=false, Is_even_K=false, Return_softmax=false, Params=Flash_fwd_params]" 
(588): here
            instantiation of "void flash::compute_attn<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &) [with Kernel_traits=Flash_fwd_kernel_traits<160, 128, 32, 4, false, false, cutlass::half_t, Flash_kernel_traits<160, 128, 32, 4, cutlass::half_t>>, Is_dropout=false, Is_causal=true, Is_even_N=false, Is_even_K=false, Return_softmax=false, Params=Flash_fwd_params]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(15): here
            instantiation of "void flash_fwd_kernel<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax>(Flash_fwd_params) [with Kernel_traits=Flash_fwd_kernel_traits<160, 128, 32, 4, false, false, cutlass::half_t, Flash_kernel_traits<160, 128, 32, 4, cutlass::half_t>>, Is_dropout=false, Is_causal=true, Is_even_N=false, Is_even_K=false, Return_softmax=false]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(34): here
            instantiation of "void run_flash_fwd<Kernel_traits,Is_dropout,Is_causal>(Flash_fwd_params &, cudaStream_t) [with Kernel_traits=Flash_fwd_kernel_traits<160, 128, 32, 4, false, false, cutlass::half_t, Flash_kernel_traits<160, 128, 32, 4, cutlass::half_t>>, Is_dropout=false, Is_causal=true]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(155): here
            instantiation of "void run_mha_fwd_hdim160<T>(Flash_fwd_params &, cudaStream_t) [with T=cutlass::half_t]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_hdim160_fp16_sm80.cu(26): here

/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_kernel.h(140): warning: variable "MMA_M" was declared but never referenced
          detected during:
            instantiation of "void flash::compute_attn_1rowblock<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &, int, int, int) [with Kernel_traits=Flash_fwd_kernel_traits<160, 128, 64, 8, false, false, cutlass::half_t, Flash_kernel_traits<160, 128, 64, 8, cutlass::half_t>>, Is_dropout=false, Is_causal=false, Is_even_N=true, Is_even_K=true, Return_softmax=false, Params=Flash_fwd_params]" 
(588): here
            instantiation of "void flash::compute_attn<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &) [with Kernel_traits=Flash_fwd_kernel_traits<160, 128, 64, 8, false, false, cutlass::half_t, Flash_kernel_traits<160, 128, 64, 8, cutlass::half_t>>, Is_dropout=false, Is_causal=false, Is_even_N=true, Is_even_K=true, Return_softmax=false, Params=Flash_fwd_params]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(15): here
            instantiation of "void flash_fwd_kernel<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax>(Flash_fwd_params) [with Kernel_traits=Flash_fwd_kernel_traits<160, 128, 64, 8, false, false, cutlass::half_t, Flash_kernel_traits<160, 128, 64, 8, cutlass::half_t>>, Is_dropout=false, Is_causal=false, Is_even_N=true, Is_even_K=true, Return_softmax=false]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(34): here
            instantiation of "void run_flash_fwd<Kernel_traits,Is_dropout,Is_causal>(Flash_fwd_params &, cudaStream_t) [with Kernel_traits=Flash_fwd_kernel_traits<160, 128, 64, 8, false, false, cutlass::half_t, Flash_kernel_traits<160, 128, 64, 8, cutlass::half_t>>, Is_dropout=false, Is_causal=false]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(155): here
            instantiation of "void run_mha_fwd_hdim160<T>(Flash_fwd_params &, cudaStream_t) [with T=cutlass::half_t]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_hdim160_fp16_sm80.cu(26): here

/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_kernel.h(275): warning: variable "tQrQ" was declared but never referenced
          detected during:
            instantiation of "void flash::compute_attn_1rowblock<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &, int, int, int) [with Kernel_traits=Flash_fwd_kernel_traits<160, 128, 64, 8, false, false, cutlass::half_t, Flash_kernel_traits<160, 128, 64, 8, cutlass::half_t>>, Is_dropout=false, Is_causal=false, Is_even_N=true, Is_even_K=true, Return_softmax=false, Params=Flash_fwd_params]" 
(588): here
            instantiation of "void flash::compute_attn<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &) [with Kernel_traits=Flash_fwd_kernel_traits<160, 128, 64, 8, false, false, cutlass::half_t, Flash_kernel_traits<160, 128, 64, 8, cutlass::half_t>>, Is_dropout=false, Is_causal=false, Is_even_N=true, Is_even_K=true, Return_softmax=false, Params=Flash_fwd_params]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(15): here
            instantiation of "void flash_fwd_kernel<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax>(Flash_fwd_params) [with Kernel_traits=Flash_fwd_kernel_traits<160, 128, 64, 8, false, false, cutlass::half_t, Flash_kernel_traits<160, 128, 64, 8, cutlass::half_t>>, Is_dropout=false, Is_causal=false, Is_even_N=true, Is_even_K=true, Return_softmax=false]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(34): here
            instantiation of "void run_flash_fwd<Kernel_traits,Is_dropout,Is_causal>(Flash_fwd_params &, cudaStream_t) [with Kernel_traits=Flash_fwd_kernel_traits<160, 128, 64, 8, false, false, cutlass::half_t, Flash_kernel_traits<160, 128, 64, 8, cutlass::half_t>>, Is_dropout=false, Is_causal=false]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(155): here
            instantiation of "void run_mha_fwd_hdim160<T>(Flash_fwd_params &, cudaStream_t) [with T=cutlass::half_t]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_hdim160_fp16_sm80.cu(26): here

/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_kernel.h(140): warning: variable "MMA_M" was declared but never referenced
          detected during:
            instantiation of "void flash::compute_attn_1rowblock<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &, int, int, int) [with Kernel_traits=Flash_fwd_kernel_traits<160, 128, 64, 8, false, false, cutlass::half_t, Flash_kernel_traits<160, 128, 64, 8, cutlass::half_t>>, Is_dropout=false, Is_causal=false, Is_even_N=true, Is_even_K=false, Return_softmax=false, Params=Flash_fwd_params]" 
(588): here
            instantiation of "void flash::compute_attn<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &) [with Kernel_traits=Flash_fwd_kernel_traits<160, 128, 64, 8, false, false, cutlass::half_t, Flash_kernel_traits<160, 128, 64, 8, cutlass::half_t>>, Is_dropout=false, Is_causal=false, Is_even_N=true, Is_even_K=false, Return_softmax=false, Params=Flash_fwd_params]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(15): here
            instantiation of "void flash_fwd_kernel<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax>(Flash_fwd_params) [with Kernel_traits=Flash_fwd_kernel_traits<160, 128, 64, 8, false, false, cutlass::half_t, Flash_kernel_traits<160, 128, 64, 8, cutlass::half_t>>, Is_dropout=false, Is_causal=false, Is_even_N=true, Is_even_K=false, Return_softmax=false]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(34): here
            instantiation of "void run_flash_fwd<Kernel_traits,Is_dropout,Is_causal>(Flash_fwd_params &, cudaStream_t) [with Kernel_traits=Flash_fwd_kernel_traits<160, 128, 64, 8, false, false, cutlass::half_t, Flash_kernel_traits<160, 128, 64, 8, cutlass::half_t>>, Is_dropout=false, Is_causal=false]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(155): here
            instantiation of "void run_mha_fwd_hdim160<T>(Flash_fwd_params &, cudaStream_t) [with T=cutlass::half_t]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_hdim160_fp16_sm80.cu(26): here

/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_kernel.h(275): warning: variable "tQrQ" was declared but never referenced
          detected during:
            instantiation of "void flash::compute_attn_1rowblock<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &, int, int, int) [with Kernel_traits=Flash_fwd_kernel_traits<160, 128, 64, 8, false, false, cutlass::half_t, Flash_kernel_traits<160, 128, 64, 8, cutlass::half_t>>, Is_dropout=false, Is_causal=false, Is_even_N=true, Is_even_K=false, Return_softmax=false, Params=Flash_fwd_params]" 
(588): here
            instantiation of "void flash::compute_attn<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &) [with Kernel_traits=Flash_fwd_kernel_traits<160, 128, 64, 8, false, false, cutlass::half_t, Flash_kernel_traits<160, 128, 64, 8, cutlass::half_t>>, Is_dropout=false, Is_causal=false, Is_even_N=true, Is_even_K=false, Return_softmax=false, Params=Flash_fwd_params]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(15): here
            instantiation of "void flash_fwd_kernel<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax>(Flash_fwd_params) [with Kernel_traits=Flash_fwd_kernel_traits<160, 128, 64, 8, false, false, cutlass::half_t, Flash_kernel_traits<160, 128, 64, 8, cutlass::half_t>>, Is_dropout=false, Is_causal=false, Is_even_N=true, Is_even_K=false, Return_softmax=false]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(34): here
            instantiation of "void run_flash_fwd<Kernel_traits,Is_dropout,Is_causal>(Flash_fwd_params &, cudaStream_t) [with Kernel_traits=Flash_fwd_kernel_traits<160, 128, 64, 8, false, false, cutlass::half_t, Flash_kernel_traits<160, 128, 64, 8, cutlass::half_t>>, Is_dropout=false, Is_causal=false]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(155): here
            instantiation of "void run_mha_fwd_hdim160<T>(Flash_fwd_params &, cudaStream_t) [with T=cutlass::half_t]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_hdim160_fp16_sm80.cu(26): here

/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_kernel.h(140): warning: variable "MMA_M" was declared but never referenced
          detected during:
            instantiation of "void flash::compute_attn_1rowblock<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &, int, int, int) [with Kernel_traits=Flash_fwd_kernel_traits<160, 128, 64, 8, false, false, cutlass::half_t, Flash_kernel_traits<160, 128, 64, 8, cutlass::half_t>>, Is_dropout=false, Is_causal=false, Is_even_N=false, Is_even_K=true, Return_softmax=false, Params=Flash_fwd_params]" 
(588): here
            instantiation of "void flash::compute_attn<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &) [with Kernel_traits=Flash_fwd_kernel_traits<160, 128, 64, 8, false, false, cutlass::half_t, Flash_kernel_traits<160, 128, 64, 8, cutlass::half_t>>, Is_dropout=false, Is_causal=false, Is_even_N=false, Is_even_K=true, Return_softmax=false, Params=Flash_fwd_params]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(15): here
            instantiation of "void flash_fwd_kernel<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax>(Flash_fwd_params) [with Kernel_traits=Flash_fwd_kernel_traits<160, 128, 64, 8, false, false, cutlass::half_t, Flash_kernel_traits<160, 128, 64, 8, cutlass::half_t>>, Is_dropout=false, Is_causal=false, Is_even_N=false, Is_even_K=true, Return_softmax=false]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(34): here
            instantiation of "void run_flash_fwd<Kernel_traits,Is_dropout,Is_causal>(Flash_fwd_params &, cudaStream_t) [with Kernel_traits=Flash_fwd_kernel_traits<160, 128, 64, 8, false, false, cutlass::half_t, Flash_kernel_traits<160, 128, 64, 8, cutlass::half_t>>, Is_dropout=false, Is_causal=false]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(155): here
            instantiation of "void run_mha_fwd_hdim160<T>(Flash_fwd_params &, cudaStream_t) [with T=cutlass::half_t]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_hdim160_fp16_sm80.cu(26): here

/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_kernel.h(275): warning: variable "tQrQ" was declared but never referenced
          detected during:
            instantiation of "void flash::compute_attn_1rowblock<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &, int, int, int) [with Kernel_traits=Flash_fwd_kernel_traits<160, 128, 64, 8, false, false, cutlass::half_t, Flash_kernel_traits<160, 128, 64, 8, cutlass::half_t>>, Is_dropout=false, Is_causal=false, Is_even_N=false, Is_even_K=true, Return_softmax=false, Params=Flash_fwd_params]" 
(588): here
            instantiation of "void flash::compute_attn<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &) [with Kernel_traits=Flash_fwd_kernel_traits<160, 128, 64, 8, false, false, cutlass::half_t, Flash_kernel_traits<160, 128, 64, 8, cutlass::half_t>>, Is_dropout=false, Is_causal=false, Is_even_N=false, Is_even_K=true, Return_softmax=false, Params=Flash_fwd_params]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(15): here
            instantiation of "void flash_fwd_kernel<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax>(Flash_fwd_params) [with Kernel_traits=Flash_fwd_kernel_traits<160, 128, 64, 8, false, false, cutlass::half_t, Flash_kernel_traits<160, 128, 64, 8, cutlass::half_t>>, Is_dropout=false, Is_causal=false, Is_even_N=false, Is_even_K=true, Return_softmax=false]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(34): here
            instantiation of "void run_flash_fwd<Kernel_traits,Is_dropout,Is_causal>(Flash_fwd_params &, cudaStream_t) [with Kernel_traits=Flash_fwd_kernel_traits<160, 128, 64, 8, false, false, cutlass::half_t, Flash_kernel_traits<160, 128, 64, 8, cutlass::half_t>>, Is_dropout=false, Is_causal=false]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(155): here
            instantiation of "void run_mha_fwd_hdim160<T>(Flash_fwd_params &, cudaStream_t) [with T=cutlass::half_t]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_hdim160_fp16_sm80.cu(26): here

/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_kernel.h(140): warning: variable "MMA_M" was declared but never referenced
          detected during:
            instantiation of "void flash::compute_attn_1rowblock<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &, int, int, int) [with Kernel_traits=Flash_fwd_kernel_traits<160, 128, 64, 8, false, false, cutlass::half_t, Flash_kernel_traits<160, 128, 64, 8, cutlass::half_t>>, Is_dropout=false, Is_causal=false, Is_even_N=false, Is_even_K=false, Return_softmax=false, Params=Flash_fwd_params]" 
(588): here
            instantiation of "void flash::compute_attn<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &) [with Kernel_traits=Flash_fwd_kernel_traits<160, 128, 64, 8, false, false, cutlass::half_t, Flash_kernel_traits<160, 128, 64, 8, cutlass::half_t>>, Is_dropout=false, Is_causal=false, Is_even_N=false, Is_even_K=false, Return_softmax=false, Params=Flash_fwd_params]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(15): here
            instantiation of "void flash_fwd_kernel<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax>(Flash_fwd_params) [with Kernel_traits=Flash_fwd_kernel_traits<160, 128, 64, 8, false, false, cutlass::half_t, Flash_kernel_traits<160, 128, 64, 8, cutlass::half_t>>, Is_dropout=false, Is_causal=false, Is_even_N=false, Is_even_K=false, Return_softmax=false]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(34): here
            instantiation of "void run_flash_fwd<Kernel_traits,Is_dropout,Is_causal>(Flash_fwd_params &, cudaStream_t) [with Kernel_traits=Flash_fwd_kernel_traits<160, 128, 64, 8, false, false, cutlass::half_t, Flash_kernel_traits<160, 128, 64, 8, cutlass::half_t>>, Is_dropout=false, Is_causal=false]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(155): here
            instantiation of "void run_mha_fwd_hdim160<T>(Flash_fwd_params &, cudaStream_t) [with T=cutlass::half_t]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_hdim160_fp16_sm80.cu(26): here

/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_kernel.h(275): warning: variable "tQrQ" was declared but never referenced
          detected during:
            instantiation of "void flash::compute_attn_1rowblock<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &, int, int, int) [with Kernel_traits=Flash_fwd_kernel_traits<160, 128, 64, 8, false, false, cutlass::half_t, Flash_kernel_traits<160, 128, 64, 8, cutlass::half_t>>, Is_dropout=false, Is_causal=false, Is_even_N=false, Is_even_K=false, Return_softmax=false, Params=Flash_fwd_params]" 
(588): here
            instantiation of "void flash::compute_attn<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &) [with Kernel_traits=Flash_fwd_kernel_traits<160, 128, 64, 8, false, false, cutlass::half_t, Flash_kernel_traits<160, 128, 64, 8, cutlass::half_t>>, Is_dropout=false, Is_causal=false, Is_even_N=false, Is_even_K=false, Return_softmax=false, Params=Flash_fwd_params]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(15): here
            instantiation of "void flash_fwd_kernel<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax>(Flash_fwd_params) [with Kernel_traits=Flash_fwd_kernel_traits<160, 128, 64, 8, false, false, cutlass::half_t, Flash_kernel_traits<160, 128, 64, 8, cutlass::half_t>>, Is_dropout=false, Is_causal=false, Is_even_N=false, Is_even_K=false, Return_softmax=false]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(34): here
            instantiation of "void run_flash_fwd<Kernel_traits,Is_dropout,Is_causal>(Flash_fwd_params &, cudaStream_t) [with Kernel_traits=Flash_fwd_kernel_traits<160, 128, 64, 8, false, false, cutlass::half_t, Flash_kernel_traits<160, 128, 64, 8, cutlass::half_t>>, Is_dropout=false, Is_causal=false]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(155): here
            instantiation of "void run_mha_fwd_hdim160<T>(Flash_fwd_params &, cudaStream_t) [with T=cutlass::half_t]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_hdim160_fp16_sm80.cu(26): here

/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_kernel.h(140): warning: variable "MMA_M" was declared but never referenced
          detected during:
            instantiation of "void flash::compute_attn_1rowblock<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &, int, int, int) [with Kernel_traits=Flash_fwd_kernel_traits<160, 128, 32, 4, false, false, cutlass::half_t, Flash_kernel_traits<160, 128, 32, 4, cutlass::half_t>>, Is_dropout=false, Is_causal=false, Is_even_N=true, Is_even_K=true, Return_softmax=false, Params=Flash_fwd_params]" 
(588): here
            instantiation of "void flash::compute_attn<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &) [with Kernel_traits=Flash_fwd_kernel_traits<160, 128, 32, 4, false, false, cutlass::half_t, Flash_kernel_traits<160, 128, 32, 4, cutlass::half_t>>, Is_dropout=false, Is_causal=false, Is_even_N=true, Is_even_K=true, Return_softmax=false, Params=Flash_fwd_params]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(15): here
            instantiation of "void flash_fwd_kernel<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax>(Flash_fwd_params) [with Kernel_traits=Flash_fwd_kernel_traits<160, 128, 32, 4, false, false, cutlass::half_t, Flash_kernel_traits<160, 128, 32, 4, cutlass::half_t>>, Is_dropout=false, Is_causal=false, Is_even_N=true, Is_even_K=true, Return_softmax=false]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(34): here
            instantiation of "void run_flash_fwd<Kernel_traits,Is_dropout,Is_causal>(Flash_fwd_params &, cudaStream_t) [with Kernel_traits=Flash_fwd_kernel_traits<160, 128, 32, 4, false, false, cutlass::half_t, Flash_kernel_traits<160, 128, 32, 4, cutlass::half_t>>, Is_dropout=false, Is_causal=false]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(155): here
            instantiation of "void run_mha_fwd_hdim160<T>(Flash_fwd_params &, cudaStream_t) [with T=cutlass::half_t]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_hdim160_fp16_sm80.cu(26): here

/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_kernel.h(275): warning: variable "tQrQ" was declared but never referenced
          detected during:
            instantiation of "void flash::compute_attn_1rowblock<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &, int, int, int) [with Kernel_traits=Flash_fwd_kernel_traits<160, 128, 32, 4, false, false, cutlass::half_t, Flash_kernel_traits<160, 128, 32, 4, cutlass::half_t>>, Is_dropout=false, Is_causal=false, Is_even_N=true, Is_even_K=true, Return_softmax=false, Params=Flash_fwd_params]" 
(588): here
            instantiation of "void flash::compute_attn<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &) [with Kernel_traits=Flash_fwd_kernel_traits<160, 128, 32, 4, false, false, cutlass::half_t, Flash_kernel_traits<160, 128, 32, 4, cutlass::half_t>>, Is_dropout=false, Is_causal=false, Is_even_N=true, Is_even_K=true, Return_softmax=false, Params=Flash_fwd_params]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(15): here
            instantiation of "void flash_fwd_kernel<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax>(Flash_fwd_params) [with Kernel_traits=Flash_fwd_kernel_traits<160, 128, 32, 4, false, false, cutlass::half_t, Flash_kernel_traits<160, 128, 32, 4, cutlass::half_t>>, Is_dropout=false, Is_causal=false, Is_even_N=true, Is_even_K=true, Return_softmax=false]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(34): here
            instantiation of "void run_flash_fwd<Kernel_traits,Is_dropout,Is_causal>(Flash_fwd_params &, cudaStream_t) [with Kernel_traits=Flash_fwd_kernel_traits<160, 128, 32, 4, false, false, cutlass::half_t, Flash_kernel_traits<160, 128, 32, 4, cutlass::half_t>>, Is_dropout=false, Is_causal=false]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(155): here
            instantiation of "void run_mha_fwd_hdim160<T>(Flash_fwd_params &, cudaStream_t) [with T=cutlass::half_t]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_hdim160_fp16_sm80.cu(26): here

/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_kernel.h(140): warning: variable "MMA_M" was declared but never referenced
          detected during:
            instantiation of "void flash::compute_attn_1rowblock<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &, int, int, int) [with Kernel_traits=Flash_fwd_kernel_traits<160, 128, 32, 4, false, false, cutlass::half_t, Flash_kernel_traits<160, 128, 32, 4, cutlass::half_t>>, Is_dropout=false, Is_causal=false, Is_even_N=true, Is_even_K=false, Return_softmax=false, Params=Flash_fwd_params]" 
(588): here
            instantiation of "void flash::compute_attn<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &) [with Kernel_traits=Flash_fwd_kernel_traits<160, 128, 32, 4, false, false, cutlass::half_t, Flash_kernel_traits<160, 128, 32, 4, cutlass::half_t>>, Is_dropout=false, Is_causal=false, Is_even_N=true, Is_even_K=false, Return_softmax=false, Params=Flash_fwd_params]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(15): here
            instantiation of "void flash_fwd_kernel<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax>(Flash_fwd_params) [with Kernel_traits=Flash_fwd_kernel_traits<160, 128, 32, 4, false, false, cutlass::half_t, Flash_kernel_traits<160, 128, 32, 4, cutlass::half_t>>, Is_dropout=false, Is_causal=false, Is_even_N=true, Is_even_K=false, Return_softmax=false]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(34): here
            instantiation of "void run_flash_fwd<Kernel_traits,Is_dropout,Is_causal>(Flash_fwd_params &, cudaStream_t) [with Kernel_traits=Flash_fwd_kernel_traits<160, 128, 32, 4, false, false, cutlass::half_t, Flash_kernel_traits<160, 128, 32, 4, cutlass::half_t>>, Is_dropout=false, Is_causal=false]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(155): here
            instantiation of "void run_mha_fwd_hdim160<T>(Flash_fwd_params &, cudaStream_t) [with T=cutlass::half_t]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_hdim160_fp16_sm80.cu(26): here

/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_kernel.h(275): warning: variable "tQrQ" was declared but never referenced
          detected during:
            instantiation of "void flash::compute_attn_1rowblock<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &, int, int, int) [with Kernel_traits=Flash_fwd_kernel_traits<160, 128, 32, 4, false, false, cutlass::half_t, Flash_kernel_traits<160, 128, 32, 4, cutlass::half_t>>, Is_dropout=false, Is_causal=false, Is_even_N=true, Is_even_K=false, Return_softmax=false, Params=Flash_fwd_params]" 
(588): here
            instantiation of "void flash::compute_attn<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &) [with Kernel_traits=Flash_fwd_kernel_traits<160, 128, 32, 4, false, false, cutlass::half_t, Flash_kernel_traits<160, 128, 32, 4, cutlass::half_t>>, Is_dropout=false, Is_causal=false, Is_even_N=true, Is_even_K=false, Return_softmax=false, Params=Flash_fwd_params]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(15): here
            instantiation of "void flash_fwd_kernel<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax>(Flash_fwd_params) [with Kernel_traits=Flash_fwd_kernel_traits<160, 128, 32, 4, false, false, cutlass::half_t, Flash_kernel_traits<160, 128, 32, 4, cutlass::half_t>>, Is_dropout=false, Is_causal=false, Is_even_N=true, Is_even_K=false, Return_softmax=false]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(34): here
            instantiation of "void run_flash_fwd<Kernel_traits,Is_dropout,Is_causal>(Flash_fwd_params &, cudaStream_t) [with Kernel_traits=Flash_fwd_kernel_traits<160, 128, 32, 4, false, false, cutlass::half_t, Flash_kernel_traits<160, 128, 32, 4, cutlass::half_t>>, Is_dropout=false, Is_causal=false]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(155): here
            instantiation of "void run_mha_fwd_hdim160<T>(Flash_fwd_params &, cudaStream_t) [with T=cutlass::half_t]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_hdim160_fp16_sm80.cu(26): here

/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_kernel.h(140): warning: variable "MMA_M" was declared but never referenced
          detected during:
            instantiation of "void flash::compute_attn_1rowblock<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &, int, int, int) [with Kernel_traits=Flash_fwd_kernel_traits<160, 128, 32, 4, false, false, cutlass::half_t, Flash_kernel_traits<160, 128, 32, 4, cutlass::half_t>>, Is_dropout=false, Is_causal=false, Is_even_N=false, Is_even_K=true, Return_softmax=false, Params=Flash_fwd_params]" 
(588): here
            instantiation of "void flash::compute_attn<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &) [with Kernel_traits=Flash_fwd_kernel_traits<160, 128, 32, 4, false, false, cutlass::half_t, Flash_kernel_traits<160, 128, 32, 4, cutlass::half_t>>, Is_dropout=false, Is_causal=false, Is_even_N=false, Is_even_K=true, Return_softmax=false, Params=Flash_fwd_params]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(15): here
            instantiation of "void flash_fwd_kernel<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax>(Flash_fwd_params) [with Kernel_traits=Flash_fwd_kernel_traits<160, 128, 32, 4, false, false, cutlass::half_t, Flash_kernel_traits<160, 128, 32, 4, cutlass::half_t>>, Is_dropout=false, Is_causal=false, Is_even_N=false, Is_even_K=true, Return_softmax=false]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(34): here
            instantiation of "void run_flash_fwd<Kernel_traits,Is_dropout,Is_causal>(Flash_fwd_params &, cudaStream_t) [with Kernel_traits=Flash_fwd_kernel_traits<160, 128, 32, 4, false, false, cutlass::half_t, Flash_kernel_traits<160, 128, 32, 4, cutlass::half_t>>, Is_dropout=false, Is_causal=false]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(155): here
            instantiation of "void run_mha_fwd_hdim160<T>(Flash_fwd_params &, cudaStream_t) [with T=cutlass::half_t]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_hdim160_fp16_sm80.cu(26): here

/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_kernel.h(275): warning: variable "tQrQ" was declared but never referenced
          detected during:
            instantiation of "void flash::compute_attn_1rowblock<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &, int, int, int) [with Kernel_traits=Flash_fwd_kernel_traits<160, 128, 32, 4, false, false, cutlass::half_t, Flash_kernel_traits<160, 128, 32, 4, cutlass::half_t>>, Is_dropout=false, Is_causal=false, Is_even_N=false, Is_even_K=true, Return_softmax=false, Params=Flash_fwd_params]" 
(588): here
            instantiation of "void flash::compute_attn<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &) [with Kernel_traits=Flash_fwd_kernel_traits<160, 128, 32, 4, false, false, cutlass::half_t, Flash_kernel_traits<160, 128, 32, 4, cutlass::half_t>>, Is_dropout=false, Is_causal=false, Is_even_N=false, Is_even_K=true, Return_softmax=false, Params=Flash_fwd_params]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(15): here
            instantiation of "void flash_fwd_kernel<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax>(Flash_fwd_params) [with Kernel_traits=Flash_fwd_kernel_traits<160, 128, 32, 4, false, false, cutlass::half_t, Flash_kernel_traits<160, 128, 32, 4, cutlass::half_t>>, Is_dropout=false, Is_causal=false, Is_even_N=false, Is_even_K=true, Return_softmax=false]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(34): here
            instantiation of "void run_flash_fwd<Kernel_traits,Is_dropout,Is_causal>(Flash_fwd_params &, cudaStream_t) [with Kernel_traits=Flash_fwd_kernel_traits<160, 128, 32, 4, false, false, cutlass::half_t, Flash_kernel_traits<160, 128, 32, 4, cutlass::half_t>>, Is_dropout=false, Is_causal=false]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(155): here
            instantiation of "void run_mha_fwd_hdim160<T>(Flash_fwd_params &, cudaStream_t) [with T=cutlass::half_t]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_hdim160_fp16_sm80.cu(26): here

/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_kernel.h(140): warning: variable "MMA_M" was declared but never referenced
          detected during:
            instantiation of "void flash::compute_attn_1rowblock<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &, int, int, int) [with Kernel_traits=Flash_fwd_kernel_traits<160, 128, 32, 4, false, false, cutlass::half_t, Flash_kernel_traits<160, 128, 32, 4, cutlass::half_t>>, Is_dropout=false, Is_causal=false, Is_even_N=false, Is_even_K=false, Return_softmax=false, Params=Flash_fwd_params]" 
(588): here
            instantiation of "void flash::compute_attn<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &) [with Kernel_traits=Flash_fwd_kernel_traits<160, 128, 32, 4, false, false, cutlass::half_t, Flash_kernel_traits<160, 128, 32, 4, cutlass::half_t>>, Is_dropout=false, Is_causal=false, Is_even_N=false, Is_even_K=false, Return_softmax=false, Params=Flash_fwd_params]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(15): here
            instantiation of "void flash_fwd_kernel<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax>(Flash_fwd_params) [with Kernel_traits=Flash_fwd_kernel_traits<160, 128, 32, 4, false, false, cutlass::half_t, Flash_kernel_traits<160, 128, 32, 4, cutlass::half_t>>, Is_dropout=false, Is_causal=false, Is_even_N=false, Is_even_K=false, Return_softmax=false]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(34): here
            instantiation of "void run_flash_fwd<Kernel_traits,Is_dropout,Is_causal>(Flash_fwd_params &, cudaStream_t) [with Kernel_traits=Flash_fwd_kernel_traits<160, 128, 32, 4, false, false, cutlass::half_t, Flash_kernel_traits<160, 128, 32, 4, cutlass::half_t>>, Is_dropout=false, Is_causal=false]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(155): here
            instantiation of "void run_mha_fwd_hdim160<T>(Flash_fwd_params &, cudaStream_t) [with T=cutlass::half_t]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_hdim160_fp16_sm80.cu(26): here

/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_kernel.h(275): warning: variable "tQrQ" was declared but never referenced
          detected during:
            instantiation of "void flash::compute_attn_1rowblock<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &, int, int, int) [with Kernel_traits=Flash_fwd_kernel_traits<160, 128, 32, 4, false, false, cutlass::half_t, Flash_kernel_traits<160, 128, 32, 4, cutlass::half_t>>, Is_dropout=false, Is_causal=false, Is_even_N=false, Is_even_K=false, Return_softmax=false, Params=Flash_fwd_params]" 
(588): here
            instantiation of "void flash::compute_attn<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &) [with Kernel_traits=Flash_fwd_kernel_traits<160, 128, 32, 4, false, false, cutlass::half_t, Flash_kernel_traits<160, 128, 32, 4, cutlass::half_t>>, Is_dropout=false, Is_causal=false, Is_even_N=false, Is_even_K=false, Return_softmax=false, Params=Flash_fwd_params]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(15): here
            instantiation of "void flash_fwd_kernel<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax>(Flash_fwd_params) [with Kernel_traits=Flash_fwd_kernel_traits<160, 128, 32, 4, false, false, cutlass::half_t, Flash_kernel_traits<160, 128, 32, 4, cutlass::half_t>>, Is_dropout=false, Is_causal=false, Is_even_N=false, Is_even_K=false, Return_softmax=false]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(34): here
            instantiation of "void run_flash_fwd<Kernel_traits,Is_dropout,Is_causal>(Flash_fwd_params &, cudaStream_t) [with Kernel_traits=Flash_fwd_kernel_traits<160, 128, 32, 4, false, false, cutlass::half_t, Flash_kernel_traits<160, 128, 32, 4, cutlass::half_t>>, Is_dropout=false, Is_causal=false]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(155): here
            instantiation of "void run_mha_fwd_hdim160<T>(Flash_fwd_params &, cudaStream_t) [with T=cutlass::half_t]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_hdim160_fp16_sm80.cu(26): here

ptxas info    : 2 bytes gmem
ptxas info    : Compiling entry function '_Z16flash_fwd_kernelI23Flash_fwd_kernel_traitsILi160ELi128ELi32ELi4ELb0ELb0EN7cutlass6half_tE19Flash_kernel_traitsILi160ELi128ELi32ELi4ES2_EELb0ELb0ELb0ELb0ELb0EEv16Flash_fwd_params' for 'sm_80'
ptxas info    : Function properties for _Z16flash_fwd_kernelI23Flash_fwd_kernel_traitsILi160ELi128ELi32ELi4ELb0ELb0EN7cutlass6half_tE19Flash_kernel_traitsILi160ELi128ELi32ELi4ES2_EELb0ELb0ELb0ELb0ELb0EEv16Flash_fwd_params
    32 bytes stack frame, 48 bytes spill stores, 48 bytes spill loads
ptxas info    : Used 255 registers, 584 bytes cmem[0]
ptxas info    : Compiling entry function '_Z16flash_fwd_kernelI23Flash_fwd_kernel_traitsILi160ELi128ELi32ELi4ELb0ELb0EN7cutlass6half_tE19Flash_kernel_traitsILi160ELi128ELi32ELi4ES2_EELb0ELb0ELb0ELb1ELb0EEv16Flash_fwd_params' for 'sm_80'
ptxas info    : Function properties for _Z16flash_fwd_kernelI23Flash_fwd_kernel_traitsILi160ELi128ELi32ELi4ELb0ELb0EN7cutlass6half_tE19Flash_kernel_traitsILi160ELi128ELi32ELi4ES2_EELb0ELb0ELb0ELb1ELb0EEv16Flash_fwd_params
    48 bytes stack frame, 80 bytes spill stores, 64 bytes spill loads
ptxas info    : Used 255 registers, 584 bytes cmem[0]
ptxas info    : Compiling entry function '_Z16flash_fwd_kernelI23Flash_fwd_kernel_traitsILi160ELi128ELi32ELi4ELb0ELb0EN7cutlass6half_tE19Flash_kernel_traitsILi160ELi128ELi32ELi4ES2_EELb0ELb0ELb1ELb0ELb0EEv16Flash_fwd_params' for 'sm_80'
ptxas info    : Function properties for _Z16flash_fwd_kernelI23Flash_fwd_kernel_traitsILi160ELi128ELi32ELi4ELb0ELb0EN7cutlass6half_tE19Flash_kernel_traitsILi160ELi128ELi32ELi4ES2_EELb0ELb0ELb1ELb0ELb0EEv16Flash_fwd_params
    24 bytes stack frame, 36 bytes spill stores, 36 bytes spill loads
ptxas info    : Used 255 registers, 584 bytes cmem[0]
ptxas info    : Compiling entry function '_Z16flash_fwd_kernelI23Flash_fwd_kernel_traitsILi160ELi128ELi32ELi4ELb0ELb0EN7cutlass6half_tE19Flash_kernel_traitsILi160ELi128ELi32ELi4ES2_EELb0ELb0ELb1ELb1ELb0EEv16Flash_fwd_params' for 'sm_80'
ptxas info    : Function properties for _Z16flash_fwd_kernelI23Flash_fwd_kernel_traitsILi160ELi128ELi32ELi4ELb0ELb0EN7cutlass6half_tE19Flash_kernel_traitsILi160ELi128ELi32ELi4ES2_EELb0ELb0ELb1ELb1ELb0EEv16Flash_fwd_params
    24 bytes stack frame, 40 bytes spill stores, 40 bytes spill loads
ptxas info    : Used 255 registers, 584 bytes cmem[0]
ptxas info    : Compiling entry function '_Z16flash_fwd_kernelI23Flash_fwd_kernel_traitsILi160ELi128ELi64ELi8ELb0ELb0EN7cutlass6half_tE19Flash_kernel_traitsILi160ELi128ELi64ELi8ES2_EELb0ELb0ELb0ELb0ELb0EEv16Flash_fwd_params' for 'sm_80'
ptxas info    : Function properties for _Z16flash_fwd_kernelI23Flash_fwd_kernel_traitsILi160ELi128ELi64ELi8ELb0ELb0EN7cutlass6half_tE19Flash_kernel_traitsILi160ELi128ELi64ELi8ES2_EELb0ELb0ELb0ELb0ELb0EEv16Flash_fwd_params
    0 bytes stack frame, 0 bytes spill stores, 0 bytes spill loads
ptxas info    : Used 165 registers, 584 bytes cmem[0]
ptxas info    : Compiling entry function '_Z16flash_fwd_kernelI23Flash_fwd_kernel_traitsILi160ELi128ELi64ELi8ELb0ELb0EN7cutlass6half_tE19Flash_kernel_traitsILi160ELi128ELi64ELi8ES2_EELb0ELb0ELb0ELb1ELb0EEv16Flash_fwd_params' for 'sm_80'
ptxas info    : Function properties for _Z16flash_fwd_kernelI23Flash_fwd_kernel_traitsILi160ELi128ELi64ELi8ELb0ELb0EN7cutlass6half_tE19Flash_kernel_traitsILi160ELi128ELi64ELi8ES2_EELb0ELb0ELb0ELb1ELb0EEv16Flash_fwd_params
    0 bytes stack frame, 0 bytes spill stores, 0 bytes spill loads
ptxas info    : Used 163 registers, 584 bytes cmem[0]
ptxas info    : Compiling entry function '_Z16flash_fwd_kernelI23Flash_fwd_kernel_traitsILi160ELi128ELi64ELi8ELb0ELb0EN7cutlass6half_tE19Flash_kernel_traitsILi160ELi128ELi64ELi8ES2_EELb0ELb0ELb1ELb0ELb0EEv16Flash_fwd_params' for 'sm_80'
ptxas info    : Function properties for _Z16flash_fwd_kernelI23Flash_fwd_kernel_traitsILi160ELi128ELi64ELi8ELb0ELb0EN7cutlass6half_tE19Flash_kernel_traitsILi160ELi128ELi64ELi8ES2_EELb0ELb0ELb1ELb0ELb0EEv16Flash_fwd_params
    0 bytes stack frame, 0 bytes spill stores, 0 bytes spill loads
ptxas info    : Used 171 registers, 584 bytes cmem[0]
ptxas info    : Compiling entry function '_Z16flash_fwd_kernelI23Flash_fwd_kernel_traitsILi160ELi128ELi64ELi8ELb0ELb0EN7cutlass6half_tE19Flash_kernel_traitsILi160ELi128ELi64ELi8ES2_EELb0ELb0ELb1ELb1ELb0EEv16Flash_fwd_params' for 'sm_80'
ptxas info    : Function properties for _Z16flash_fwd_kernelI23Flash_fwd_kernel_traitsILi160ELi128ELi64ELi8ELb0ELb0EN7cutlass6half_tE19Flash_kernel_traitsILi160ELi128ELi64ELi8ES2_EELb0ELb0ELb1ELb1ELb0EEv16Flash_fwd_params
    0 bytes stack frame, 0 bytes spill stores, 0 bytes spill loads
ptxas info    : Used 170 registers, 584 bytes cmem[0]
ptxas info    : Compiling entry function '_Z16flash_fwd_kernelI23Flash_fwd_kernel_traitsILi160ELi128ELi32ELi4ELb0ELb0EN7cutlass6half_tE19Flash_kernel_traitsILi160ELi128ELi32ELi4ES2_EELb0ELb1ELb0ELb0ELb0EEv16Flash_fwd_params' for 'sm_80'
ptxas info    : Function properties for _Z16flash_fwd_kernelI23Flash_fwd_kernel_traitsILi160ELi128ELi32ELi4ELb0ELb0EN7cutlass6half_tE19Flash_kernel_traitsILi160ELi128ELi32ELi4ES2_EELb0ELb1ELb0ELb0ELb0EEv16Flash_fwd_params
    56 bytes stack frame, 132 bytes spill stores, 112 bytes spill loads
ptxas info    : Used 255 registers, 584 bytes cmem[0]
ptxas info    : Compiling entry function '_Z16flash_fwd_kernelI23Flash_fwd_kernel_traitsILi160ELi128ELi32ELi4ELb0ELb0EN7cutlass6half_tE19Flash_kernel_traitsILi160ELi128ELi32ELi4ES2_EELb0ELb1ELb0ELb1ELb0EEv16Flash_fwd_params' for 'sm_80'
ptxas info    : Function properties for _Z16flash_fwd_kernelI23Flash_fwd_kernel_traitsILi160ELi128ELi32ELi4ELb0ELb0EN7cutlass6half_tE19Flash_kernel_traitsILi160ELi128ELi32ELi4ES2_EELb0ELb1ELb0ELb1ELb0EEv16Flash_fwd_params
    40 bytes stack frame, 156 bytes spill stores, 96 bytes spill loads
ptxas info    : Used 255 registers, 584 bytes cmem[0]
ptxas info    : Compiling entry function '_Z16flash_fwd_kernelI23Flash_fwd_kernel_traitsILi160ELi128ELi32ELi4ELb0ELb0EN7cutlass6half_tE19Flash_kernel_traitsILi160ELi128ELi32ELi4ES2_EELb0ELb1ELb1ELb0ELb0EEv16Flash_fwd_params' for 'sm_80'
ptxas info    : Function properties for _Z16flash_fwd_kernelI23Flash_fwd_kernel_traitsILi160ELi128ELi32ELi4ELb0ELb0EN7cutlass6half_tE19Flash_kernel_traitsILi160ELi128ELi32ELi4ES2_EELb0ELb1ELb1ELb0ELb0EEv16Flash_fwd_params
    24 bytes stack frame, 84 bytes spill stores, 44 bytes spill loads
ptxas info    : Used 255 registers, 584 bytes cmem[0]
ptxas info    : Compiling entry function '_Z16flash_fwd_kernelI23Flash_fwd_kernel_traitsILi160ELi128ELi32ELi4ELb0ELb0EN7cutlass6half_tE19Flash_kernel_traitsILi160ELi128ELi32ELi4ES2_EELb0ELb1ELb1ELb1ELb0EEv16Flash_fwd_params' for 'sm_80'
ptxas info    : Function properties for _Z16flash_fwd_kernelI23Flash_fwd_kernel_traitsILi160ELi128ELi32ELi4ELb0ELb0EN7cutlass6half_tE19Flash_kernel_traitsILi160ELi128ELi32ELi4ES2_EELb0ELb1ELb1ELb1ELb0EEv16Flash_fwd_params
    56 bytes stack frame, 176 bytes spill stores, 124 bytes spill loads
ptxas info    : Used 255 registers, 584 bytes cmem[0]
ptxas info    : Compiling entry function '_Z16flash_fwd_kernelI23Flash_fwd_kernel_traitsILi160ELi64ELi64ELi4ELb0ELb0EN7cutlass6half_tE19Flash_kernel_traitsILi160ELi64ELi64ELi4ES2_EELb0ELb1ELb0ELb0ELb0EEv16Flash_fwd_params' for 'sm_80'
ptxas info    : Function properties for _Z16flash_fwd_kernelI23Flash_fwd_kernel_traitsILi160ELi64ELi64ELi4ELb0ELb0EN7cutlass6half_tE19Flash_kernel_traitsILi160ELi64ELi64ELi4ES2_EELb0ELb1ELb0ELb0ELb0EEv16Flash_fwd_params
    0 bytes stack frame, 0 bytes spill stores, 0 bytes spill loads
ptxas info    : Used 171 registers, 584 bytes cmem[0]
ptxas info    : Compiling entry function '_Z16flash_fwd_kernelI23Flash_fwd_kernel_traitsILi160ELi64ELi64ELi4ELb0ELb0EN7cutlass6half_tE19Flash_kernel_traitsILi160ELi64ELi64ELi4ES2_EELb0ELb1ELb0ELb1ELb0EEv16Flash_fwd_params' for 'sm_80'
ptxas info    : Function properties for _Z16flash_fwd_kernelI23Flash_fwd_kernel_traitsILi160ELi64ELi64ELi4ELb0ELb0EN7cutlass6half_tE19Flash_kernel_traitsILi160ELi64ELi64ELi4ES2_EELb0ELb1ELb0ELb1ELb0EEv16Flash_fwd_params
    0 bytes stack frame, 0 bytes spill stores, 0 bytes spill loads
ptxas info    : Used 167 registers, 584 bytes cmem[0]
ptxas info    : Compiling entry function '_Z16flash_fwd_kernelI23Flash_fwd_kernel_traitsILi160ELi64ELi64ELi4ELb0ELb0EN7cutlass6half_tE19Flash_kernel_traitsILi160ELi64ELi64ELi4ES2_EELb0ELb1ELb1ELb0ELb0EEv16Flash_fwd_params' for 'sm_80'
ptxas info    : Function properties for _Z16flash_fwd_kernelI23Flash_fwd_kernel_traitsILi160ELi64ELi64ELi4ELb0ELb0EN7cutlass6half_tE19Flash_kernel_traitsILi160ELi64ELi64ELi4ES2_EELb0ELb1ELb1ELb0ELb0EEv16Flash_fwd_params
    0 bytes stack frame, 0 bytes spill stores, 0 bytes spill loads
ptxas info    : Used 168 registers, 584 bytes cmem[0]
ptxas info    : Compiling entry function '_Z16flash_fwd_kernelI23Flash_fwd_kernel_traitsILi160ELi64ELi64ELi4ELb0ELb0EN7cutlass6half_tE19Flash_kernel_traitsILi160ELi64ELi64ELi4ES2_EELb0ELb1ELb1ELb1ELb0EEv16Flash_fwd_params' for 'sm_80'
ptxas info    : Function properties for _Z16flash_fwd_kernelI23Flash_fwd_kernel_traitsILi160ELi64ELi64ELi4ELb0ELb0EN7cutlass6half_tE19Flash_kernel_traitsILi160ELi64ELi64ELi4ES2_EELb0ELb1ELb1ELb1ELb0EEv16Flash_fwd_params
    0 bytes stack frame, 0 bytes spill stores, 0 bytes spill loads
ptxas info    : Used 172 registers, 584 bytes cmem[0]
ptxas info    : Compiling entry function '_Z16flash_fwd_kernelI23Flash_fwd_kernel_traitsILi160ELi128ELi32ELi4ELb0ELb0EN7cutlass6half_tE19Flash_kernel_traitsILi160ELi128ELi32ELi4ES2_EELb1ELb0ELb0ELb0ELb0EEv16Flash_fwd_params' for 'sm_80'
ptxas info    : Function properties for _Z16flash_fwd_kernelI23Flash_fwd_kernel_traitsILi160ELi128ELi32ELi4ELb0ELb0EN7cutlass6half_tE19Flash_kernel_traitsILi160ELi128ELi32ELi4ES2_EELb1ELb0ELb0ELb0ELb0EEv16Flash_fwd_params
    32 bytes stack frame, 32 bytes spill stores, 32 bytes spill loads
ptxas info    : Used 255 registers, 584 bytes cmem[0]
ptxas info    : Compiling entry function '_Z16flash_fwd_kernelI23Flash_fwd_kernel_traitsILi160ELi128ELi32ELi4ELb0ELb0EN7cutlass6half_tE19Flash_kernel_traitsILi160ELi128ELi32ELi4ES2_EELb1ELb0ELb0ELb0ELb1EEv16Flash_fwd_params' for 'sm_80'
ptxas info    : Function properties for _Z16flash_fwd_kernelI23Flash_fwd_kernel_traitsILi160ELi128ELi32ELi4ELb0ELb0EN7cutlass6half_tE19Flash_kernel_traitsILi160ELi128ELi32ELi4ES2_EELb1ELb0ELb0ELb0ELb1EEv16Flash_fwd_params
    144 bytes stack frame, 106 bytes spill stores, 106 bytes spill loads
ptxas info    : Used 255 registers, 584 bytes cmem[0]
ptxas info    : Compiling entry function '_Z16flash_fwd_kernelI23Flash_fwd_kernel_traitsILi160ELi128ELi32ELi4ELb0ELb0EN7cutlass6half_tE19Flash_kernel_traitsILi160ELi128ELi32ELi4ES2_EELb1ELb0ELb0ELb1ELb0EEv16Flash_fwd_params' for 'sm_80'
ptxas info    : Function properties for _Z16flash_fwd_kernelI23Flash_fwd_kernel_traitsILi160ELi128ELi32ELi4ELb0ELb0EN7cutlass6half_tE19Flash_kernel_traitsILi160ELi128ELi32ELi4ES2_EELb1ELb0ELb0ELb1ELb0EEv16Flash_fwd_params
    32 bytes stack frame, 32 bytes spill stores, 32 bytes spill loads
ptxas info    : Used 255 registers, 584 bytes cmem[0]
ptxas info    : Compiling entry function '_Z16flash_fwd_kernelI23Flash_fwd_kernel_traitsILi160ELi128ELi32ELi4ELb0ELb0EN7cutlass6half_tE19Flash_kernel_traitsILi160ELi128ELi32ELi4ES2_EELb1ELb0ELb0ELb1ELb1EEv16Flash_fwd_params' for 'sm_80'
ptxas info    : Function properties for _Z16flash_fwd_kernelI23Flash_fwd_kernel_traitsILi160ELi128ELi32ELi4ELb0ELb0EN7cutlass6half_tE19Flash_kernel_traitsILi160ELi128ELi32ELi4ES2_EELb1ELb0ELb0ELb1ELb1EEv16Flash_fwd_params
    128 bytes stack frame, 94 bytes spill stores, 100 bytes spill loads
ptxas info    : Used 255 registers, 584 bytes cmem[0]
ptxas info    : Compiling entry function '_Z16flash_fwd_kernelI23Flash_fwd_kernel_traitsILi160ELi128ELi32ELi4ELb0ELb0EN7cutlass6half_tE19Flash_kernel_traitsILi160ELi128ELi32ELi4ES2_EELb1ELb0ELb1ELb0ELb0EEv16Flash_fwd_params' for 'sm_80'
ptxas info    : Function properties for _Z16flash_fwd_kernelI23Flash_fwd_kernel_traitsILi160ELi128ELi32ELi4ELb0ELb0EN7cutlass6half_tE19Flash_kernel_traitsILi160ELi128ELi32ELi4ES2_EELb1ELb0ELb1ELb0ELb0EEv16Flash_fwd_params
    32 bytes stack frame, 32 bytes spill stores, 32 bytes spill loads
ptxas info    : Used 255 registers, 584 bytes cmem[0]
ptxas info    : Compiling entry function '_Z16flash_fwd_kernelI23Flash_fwd_kernel_traitsILi160ELi128ELi32ELi4ELb0ELb0EN7cutlass6half_tE19Flash_kernel_traitsILi160ELi128ELi32ELi4ES2_EELb1ELb0ELb1ELb0ELb1EEv16Flash_fwd_params' for 'sm_80'
ptxas info    : Function properties for _Z16flash_fwd_kernelI23Flash_fwd_kernel_traitsILi160ELi128ELi32ELi4ELb0ELb0EN7cutlass6half_tE19Flash_kernel_traitsILi160ELi128ELi32ELi4ES2_EELb1ELb0ELb1ELb0ELb1EEv16Flash_fwd_params
    168 bytes stack frame, 120 bytes spill stores, 120 bytes spill loads
ptxas info    : Used 255 registers, 584 bytes cmem[0]
ptxas info    : Compiling entry function '_Z16flash_fwd_kernelI23Flash_fwd_kernel_traitsILi160ELi128ELi32ELi4ELb0ELb0EN7cutlass6half_tE19Flash_kernel_traitsILi160ELi128ELi32ELi4ES2_EELb1ELb0ELb1ELb1ELb0EEv16Flash_fwd_params' for 'sm_80'
ptxas info    : Function properties for _Z16flash_fwd_kernelI23Flash_fwd_kernel_traitsILi160ELi128ELi32ELi4ELb0ELb0EN7cutlass6half_tE19Flash_kernel_traitsILi160ELi128ELi32ELi4ES2_EELb1ELb0ELb1ELb1ELb0EEv16Flash_fwd_params
    16 bytes stack frame, 16 bytes spill stores, 16 bytes spill loads
ptxas info    : Used 255 registers, 584 bytes cmem[0]
ptxas info    : Compiling entry function '_Z16flash_fwd_kernelI23Flash_fwd_kernel_traitsILi160ELi128ELi32ELi4ELb0ELb0EN7cutlass6half_tE19Flash_kernel_traitsILi160ELi128ELi32ELi4ES2_EELb1ELb0ELb1ELb1ELb1EEv16Flash_fwd_params' for 'sm_80'
ptxas info    : Function properties for _Z16flash_fwd_kernelI23Flash_fwd_kernel_traitsILi160ELi128ELi32ELi4ELb0ELb0EN7cutlass6half_tE19Flash_kernel_traitsILi160ELi128ELi32ELi4ES2_EELb1ELb0ELb1ELb1ELb1EEv16Flash_fwd_params
    120 bytes stack frame, 82 bytes spill stores, 98 bytes spill loads
ptxas info    : Used 255 registers, 584 bytes cmem[0]
ptxas info    : Compiling entry function '_Z16flash_fwd_kernelI23Flash_fwd_kernel_traitsILi160ELi128ELi64ELi8ELb0ELb0EN7cutlass6half_tE19Flash_kernel_traitsILi160ELi128ELi64ELi8ES2_EELb1ELb0ELb0ELb0ELb0EEv16Flash_fwd_params' for 'sm_80'
ptxas info    : Function properties for _Z16flash_fwd_kernelI23Flash_fwd_kernel_traitsILi160ELi128ELi64ELi8ELb0ELb0EN7cutlass6half_tE19Flash_kernel_traitsILi160ELi128ELi64ELi8ES2_EELb1ELb0ELb0ELb0ELb0EEv16Flash_fwd_params
    0 bytes stack frame, 0 bytes spill stores, 0 bytes spill loads
ptxas info    : Used 176 registers, 584 bytes cmem[0]
ptxas info    : Compiling entry function '_Z16flash_fwd_kernelI23Flash_fwd_kernel_traitsILi160ELi128ELi64ELi8ELb0ELb0EN7cutlass6half_tE19Flash_kernel_traitsILi160ELi128ELi64ELi8ES2_EELb1ELb0ELb0ELb0ELb1EEv16Flash_fwd_params' for 'sm_80'
ptxas info    : Function properties for _Z16flash_fwd_kernelI23Flash_fwd_kernel_traitsILi160ELi128ELi64ELi8ELb0ELb0EN7cutlass6half_tE19Flash_kernel_traitsILi160ELi128ELi64ELi8ES2_EELb1ELb0ELb0ELb0ELb1EEv16Flash_fwd_params
    0 bytes stack frame, 0 bytes spill stores, 0 bytes spill loads
ptxas info    : Used 252 registers, 584 bytes cmem[0]
ptxas info    : Compiling entry function '_Z16flash_fwd_kernelI23Flash_fwd_kernel_traitsILi160ELi128ELi64ELi8ELb0ELb0EN7cutlass6half_tE19Flash_kernel_traitsILi160ELi128ELi64ELi8ES2_EELb1ELb0ELb0ELb1ELb0EEv16Flash_fwd_params' for 'sm_80'
ptxas info    : Function properties for _Z16flash_fwd_kernelI23Flash_fwd_kernel_traitsILi160ELi128ELi64ELi8ELb0ELb0EN7cutlass6half_tE19Flash_kernel_traitsILi160ELi128ELi64ELi8ES2_EELb1ELb0ELb0ELb1ELb0EEv16Flash_fwd_params
    0 bytes stack frame, 0 bytes spill stores, 0 bytes spill loads
ptxas info    : Used 176 registers, 584 bytes cmem[0]
ptxas info    : Compiling entry function '_Z16flash_fwd_kernelI23Flash_fwd_kernel_traitsILi160ELi128ELi64ELi8ELb0ELb0EN7cutlass6half_tE19Flash_kernel_traitsILi160ELi128ELi64ELi8ES2_EELb1ELb0ELb0ELb1ELb1EEv16Flash_fwd_params' for 'sm_80'
ptxas info    : Function properties for _Z16flash_fwd_kernelI23Flash_fwd_kernel_traitsILi160ELi128ELi64ELi8ELb0ELb0EN7cutlass6half_tE19Flash_kernel_traitsILi160ELi128ELi64ELi8ES2_EELb1ELb0ELb0ELb1ELb1EEv16Flash_fwd_params
    0 bytes stack frame, 0 bytes spill stores, 0 bytes spill loads
ptxas info    : Used 253 registers, 584 bytes cmem[0]
ptxas info    : Compiling entry function '_Z16flash_fwd_kernelI23Flash_fwd_kernel_traitsILi160ELi128ELi64ELi8ELb0ELb0EN7cutlass6half_tE19Flash_kernel_traitsILi160ELi128ELi64ELi8ES2_EELb1ELb0ELb1ELb0ELb0EEv16Flash_fwd_params' for 'sm_80'
ptxas info    : Function properties for _Z16flash_fwd_kernelI23Flash_fwd_kernel_traitsILi160ELi128ELi64ELi8ELb0ELb0EN7cutlass6half_tE19Flash_kernel_traitsILi160ELi128ELi64ELi8ES2_EELb1ELb0ELb1ELb0ELb0EEv16Flash_fwd_params
    0 bytes stack frame, 0 bytes spill stores, 0 bytes spill loads
ptxas info    : Used 172 registers, 584 bytes cmem[0]
ptxas info    : Compiling entry function '_Z16flash_fwd_kernelI23Flash_fwd_kernel_traitsILi160ELi128ELi64ELi8ELb0ELb0EN7cutlass6half_tE19Flash_kernel_traitsILi160ELi128ELi64ELi8ES2_EELb1ELb0ELb1ELb0ELb1EEv16Flash_fwd_params' for 'sm_80'
ptxas info    : Function properties for _Z16flash_fwd_kernelI23Flash_fwd_kernel_traitsILi160ELi128ELi64ELi8ELb0ELb0EN7cutlass6half_tE19Flash_kernel_traitsILi160ELi128ELi64ELi8ES2_EELb1ELb0ELb1ELb0ELb1EEv16Flash_fwd_params
    0 bytes stack frame, 0 bytes spill stores, 0 bytes spill loads
ptxas info    : Used 250 registers, 584 bytes cmem[0]
ptxas info    : Compiling entry function '_Z16flash_fwd_kernelI23Flash_fwd_kernel_traitsILi160ELi128ELi64ELi8ELb0ELb0EN7cutlass6half_tE19Flash_kernel_traitsILi160ELi128ELi64ELi8ES2_EELb1ELb0ELb1ELb1ELb0EEv16Flash_fwd_params' for 'sm_80'
ptxas info    : Function properties for _Z16flash_fwd_kernelI23Flash_fwd_kernel_traitsILi160ELi128ELi64ELi8ELb0ELb0EN7cutlass6half_tE19Flash_kernel_traitsILi160ELi128ELi64ELi8ES2_EELb1ELb0ELb1ELb1ELb0EEv16Flash_fwd_params
    0 bytes stack frame, 0 bytes spill stores, 0 bytes spill loads
ptxas info    : Used 168 registers, 584 bytes cmem[0]
ptxas info    : Compiling entry function '_Z16flash_fwd_kernelI23Flash_fwd_kernel_traitsILi160ELi128ELi64ELi8ELb0ELb0EN7cutlass6half_tE19Flash_kernel_traitsILi160ELi128ELi64ELi8ES2_EELb1ELb0ELb1ELb1ELb1EEv16Flash_fwd_params' for 'sm_80'
ptxas info    : Function properties for _Z16flash_fwd_kernelI23Flash_fwd_kernel_traitsILi160ELi128ELi64ELi8ELb0ELb0EN7cutlass6half_tE19Flash_kernel_traitsILi160ELi128ELi64ELi8ES2_EELb1ELb0ELb1ELb1ELb1EEv16Flash_fwd_params
    0 bytes stack frame, 0 bytes spill stores, 0 bytes spill loads
ptxas info    : Used 247 registers, 584 bytes cmem[0]
ptxas info    : Compiling entry function '_Z16flash_fwd_kernelI23Flash_fwd_kernel_traitsILi160ELi128ELi32ELi4ELb0ELb0EN7cutlass6half_tE19Flash_kernel_traitsILi160ELi128ELi32ELi4ES2_EELb1ELb1ELb0ELb0ELb0EEv16Flash_fwd_params' for 'sm_80'
ptxas info    : Function properties for _Z16flash_fwd_kernelI23Flash_fwd_kernel_traitsILi160ELi128ELi32ELi4ELb0ELb0EN7cutlass6half_tE19Flash_kernel_traitsILi160ELi128ELi32ELi4ES2_EELb1ELb1ELb0ELb0ELb0EEv16Flash_fwd_params
    152 bytes stack frame, 204 bytes spill stores, 268 bytes spill loads
ptxas info    : Used 255 registers, 584 bytes cmem[0]
ptxas info    : Compiling entry function '_Z16flash_fwd_kernelI23Flash_fwd_kernel_traitsILi160ELi128ELi32ELi4ELb0ELb0EN7cutlass6half_tE19Flash_kernel_traitsILi160ELi128ELi32ELi4ES2_EELb1ELb1ELb0ELb0ELb1EEv16Flash_fwd_params' for 'sm_80'
ptxas info    : Function properties for _Z16flash_fwd_kernelI23Flash_fwd_kernel_traitsILi160ELi128ELi32ELi4ELb0ELb0EN7cutlass6half_tE19Flash_kernel_traitsILi160ELi128ELi32ELi4ES2_EELb1ELb1ELb0ELb0ELb1EEv16Flash_fwd_params
    256 bytes stack frame, 428 bytes spill stores, 588 bytes spill loads
ptxas info    : Used 255 registers, 584 bytes cmem[0]
ptxas info    : Compiling entry function '_Z16flash_fwd_kernelI23Flash_fwd_kernel_traitsILi160ELi128ELi32ELi4ELb0ELb0EN7cutlass6half_tE19Flash_kernel_traitsILi160ELi128ELi32ELi4ES2_EELb1ELb1ELb0ELb1ELb0EEv16Flash_fwd_params' for 'sm_80'
ptxas info    : Function properties for _Z16flash_fwd_kernelI23Flash_fwd_kernel_traitsILi160ELi128ELi32ELi4ELb0ELb0EN7cutlass6half_tE19Flash_kernel_traitsILi160ELi128ELi32ELi4ES2_EELb1ELb1ELb0ELb1ELb0EEv16Flash_fwd_params
    112 bytes stack frame, 252 bytes spill stores, 228 bytes spill loads
ptxas info    : Used 255 registers, 584 bytes cmem[0]
ptxas info    : Compiling entry function '_Z16flash_fwd_kernelI23Flash_fwd_kernel_traitsILi160ELi128ELi32ELi4ELb0ELb0EN7cutlass6half_tE19Flash_kernel_traitsILi160ELi128ELi32ELi4ES2_EELb1ELb1ELb0ELb1ELb1EEv16Flash_fwd_params' for 'sm_80'
ptxas info    : Function properties for _Z16flash_fwd_kernelI23Flash_fwd_kernel_traitsILi160ELi128ELi32ELi4ELb0ELb0EN7cutlass6half_tE19Flash_kernel_traitsILi160ELi128ELi32ELi4ES2_EELb1ELb1ELb0ELb1ELb1EEv16Flash_fwd_params
    264 bytes stack frame, 422 bytes spill stores, 554 bytes spill loads
ptxas info    : Used 255 registers, 584 bytes cmem[0]
ptxas info    : Compiling entry function '_Z16flash_fwd_kernelI23Flash_fwd_kernel_traitsILi160ELi128ELi32ELi4ELb0ELb0EN7cutlass6half_tE19Flash_kernel_traitsILi160ELi128ELi32ELi4ES2_EELb1ELb1ELb1ELb0ELb0EEv16Flash_fwd_params' for 'sm_80'
ptxas info    : Function properties for _Z16flash_fwd_kernelI23Flash_fwd_kernel_traitsILi160ELi128ELi32ELi4ELb0ELb0EN7cutlass6half_tE19Flash_kernel_traitsILi160ELi128ELi32ELi4ES2_EELb1ELb1ELb1ELb0ELb0EEv16Flash_fwd_params
    88 bytes stack frame, 144 bytes spill stores, 168 bytes spill loads
ptxas info    : Used 255 registers, 584 bytes cmem[0]
ptxas info    : Compiling entry function '_Z16flash_fwd_kernelI23Flash_fwd_kernel_traitsILi160ELi128ELi32ELi4ELb0ELb0EN7cutlass6half_tE19Flash_kernel_traitsILi160ELi128ELi32ELi4ES2_EELb1ELb1ELb1ELb0ELb1EEv16Flash_fwd_params' for 'sm_80'
ptxas info    : Function properties for _Z16flash_fwd_kernelI23Flash_fwd_kernel_traitsILi160ELi128ELi32ELi4ELb0ELb0EN7cutlass6half_tE19Flash_kernel_traitsILi160ELi128ELi32ELi4ES2_EELb1ELb1ELb1ELb0ELb1EEv16Flash_fwd_params
    208 bytes stack frame, 366 bytes spill stores, 488 bytes spill loads
ptxas info    : Used 255 registers, 584 bytes cmem[0]
ptxas info    : Compiling entry function '_Z16flash_fwd_kernelI23Flash_fwd_kernel_traitsILi160ELi128ELi32ELi4ELb0ELb0EN7cutlass6half_tE19Flash_kernel_traitsILi160ELi128ELi32ELi4ES2_EELb1ELb1ELb1ELb1ELb0EEv16Flash_fwd_params' for 'sm_80'
ptxas info    : Function properties for _Z16flash_fwd_kernelI23Flash_fwd_kernel_traitsILi160ELi128ELi32ELi4ELb0ELb0EN7cutlass6half_tE19Flash_kernel_traitsILi160ELi128ELi32ELi4ES2_EELb1ELb1ELb1ELb1ELb0EEv16Flash_fwd_params
    120 bytes stack frame, 196 bytes spill stores, 236 bytes spill loads
ptxas info    : Used 255 registers, 584 bytes cmem[0]
ptxas info    : Compiling entry function '_Z16flash_fwd_kernelI23Flash_fwd_kernel_traitsILi160ELi128ELi32ELi4ELb0ELb0EN7cutlass6half_tE19Flash_kernel_traitsILi160ELi128ELi32ELi4ES2_EELb1ELb1ELb1ELb1ELb1EEv16Flash_fwd_params' for 'sm_80'
ptxas info    : Function properties for _Z16flash_fwd_kernelI23Flash_fwd_kernel_traitsILi160ELi128ELi32ELi4ELb0ELb0EN7cutlass6half_tE19Flash_kernel_traitsILi160ELi128ELi32ELi4ES2_EELb1ELb1ELb1ELb1ELb1EEv16Flash_fwd_params
    288 bytes stack frame, 508 bytes spill stores, 648 bytes spill loads
ptxas info    : Used 255 registers, 584 bytes cmem[0]
ptxas info    : Compiling entry function '_Z16flash_fwd_kernelI23Flash_fwd_kernel_traitsILi160ELi64ELi64ELi4ELb0ELb0EN7cutlass6half_tE19Flash_kernel_traitsILi160ELi64ELi64ELi4ES2_EELb1ELb1ELb0ELb0ELb0EEv16Flash_fwd_params' for 'sm_80'
ptxas info    : Function properties for _Z16flash_fwd_kernelI23Flash_fwd_kernel_traitsILi160ELi64ELi64ELi4ELb0ELb0EN7cutlass6half_tE19Flash_kernel_traitsILi160ELi64ELi64ELi4ES2_EELb1ELb1ELb0ELb0ELb0EEv16Flash_fwd_params
    0 bytes stack frame, 0 bytes spill stores, 0 bytes spill loads
ptxas info    : Used 176 registers, 584 bytes cmem[0]
ptxas info    : Compiling entry function '_Z16flash_fwd_kernelI23Flash_fwd_kernel_traitsILi160ELi64ELi64ELi4ELb0ELb0EN7cutlass6half_tE19Flash_kernel_traitsILi160ELi64ELi64ELi4ES2_EELb1ELb1ELb0ELb0ELb1EEv16Flash_fwd_params' for 'sm_80'
ptxas info    : Function properties for _Z16flash_fwd_kernelI23Flash_fwd_kernel_traitsILi160ELi64ELi64ELi4ELb0ELb0EN7cutlass6half_tE19Flash_kernel_traitsILi160ELi64ELi64ELi4ES2_EELb1ELb1ELb0ELb0ELb1EEv16Flash_fwd_params
    0 bytes stack frame, 0 bytes spill stores, 0 bytes spill loads
ptxas info    : Used 252 registers, 584 bytes cmem[0]
ptxas info    : Compiling entry function '_Z16flash_fwd_kernelI23Flash_fwd_kernel_traitsILi160ELi64ELi64ELi4ELb0ELb0EN7cutlass6half_tE19Flash_kernel_traitsILi160ELi64ELi64ELi4ES2_EELb1ELb1ELb0ELb1ELb0EEv16Flash_fwd_params' for 'sm_80'
ptxas info    : Function properties for _Z16flash_fwd_kernelI23Flash_fwd_kernel_traitsILi160ELi64ELi64ELi4ELb0ELb0EN7cutlass6half_tE19Flash_kernel_traitsILi160ELi64ELi64ELi4ES2_EELb1ELb1ELb0ELb1ELb0EEv16Flash_fwd_params
    0 bytes stack frame, 0 bytes spill stores, 0 bytes spill loads
ptxas info    : Used 182 registers, 584 bytes cmem[0]
ptxas info    : Compiling entry function '_Z16flash_fwd_kernelI23Flash_fwd_kernel_traitsILi160ELi64ELi64ELi4ELb0ELb0EN7cutlass6half_tE19Flash_kernel_traitsILi160ELi64ELi64ELi4ES2_EELb1ELb1ELb0ELb1ELb1EEv16Flash_fwd_params' for 'sm_80'
ptxas info    : Function properties for _Z16flash_fwd_kernelI23Flash_fwd_kernel_traitsILi160ELi64ELi64ELi4ELb0ELb0EN7cutlass6half_tE19Flash_kernel_traitsILi160ELi64ELi64ELi4ES2_EELb1ELb1ELb0ELb1ELb1EEv16Flash_fwd_params
    0 bytes stack frame, 0 bytes spill stores, 0 bytes spill loads
ptxas info    : Used 253 registers, 584 bytes cmem[0]
ptxas info    : Compiling entry function '_Z16flash_fwd_kernelI23Flash_fwd_kernel_traitsILi160ELi64ELi64ELi4ELb0ELb0EN7cutlass6half_tE19Flash_kernel_traitsILi160ELi64ELi64ELi4ES2_EELb1ELb1ELb1ELb0ELb0EEv16Flash_fwd_params' for 'sm_80'
ptxas info    : Function properties for _Z16flash_fwd_kernelI23Flash_fwd_kernel_traitsILi160ELi64ELi64ELi4ELb0ELb0EN7cutlass6half_tE19Flash_kernel_traitsILi160ELi64ELi64ELi4ES2_EELb1ELb1ELb1ELb0ELb0EEv16Flash_fwd_params
    0 bytes stack frame, 0 bytes spill stores, 0 bytes spill loads
ptxas info    : Used 182 registers, 584 bytes cmem[0]
ptxas info    : Compiling entry function '_Z16flash_fwd_kernelI23Flash_fwd_kernel_traitsILi160ELi64ELi64ELi4ELb0ELb0EN7cutlass6half_tE19Flash_kernel_traitsILi160ELi64ELi64ELi4ES2_EELb1ELb1ELb1ELb0ELb1EEv16Flash_fwd_params' for 'sm_80'
ptxas info    : Function properties for _Z16flash_fwd_kernelI23Flash_fwd_kernel_traitsILi160ELi64ELi64ELi4ELb0ELb0EN7cutlass6half_tE19Flash_kernel_traitsILi160ELi64ELi64ELi4ES2_EELb1ELb1ELb1ELb0ELb1EEv16Flash_fwd_params
    0 bytes stack frame, 0 bytes spill stores, 0 bytes spill loads
ptxas info    : Used 250 registers, 584 bytes cmem[0]
ptxas info    : Compiling entry function '_Z16flash_fwd_kernelI23Flash_fwd_kernel_traitsILi160ELi64ELi64ELi4ELb0ELb0EN7cutlass6half_tE19Flash_kernel_traitsILi160ELi64ELi64ELi4ES2_EELb1ELb1ELb1ELb1ELb0EEv16Flash_fwd_params' for 'sm_80'
ptxas info    : Function properties for _Z16flash_fwd_kernelI23Flash_fwd_kernel_traitsILi160ELi64ELi64ELi4ELb0ELb0EN7cutlass6half_tE19Flash_kernel_traitsILi160ELi64ELi64ELi4ES2_EELb1ELb1ELb1ELb1ELb0EEv16Flash_fwd_params
    0 bytes stack frame, 0 bytes spill stores, 0 bytes spill loads
ptxas info    : Used 188 registers, 584 bytes cmem[0]
ptxas info    : Compiling entry function '_Z16flash_fwd_kernelI23Flash_fwd_kernel_traitsILi160ELi64ELi64ELi4ELb0ELb0EN7cutlass6half_tE19Flash_kernel_traitsILi160ELi64ELi64ELi4ES2_EELb1ELb1ELb1ELb1ELb1EEv16Flash_fwd_params' for 'sm_80'
ptxas info    : Function properties for _Z16flash_fwd_kernelI23Flash_fwd_kernel_traitsILi160ELi64ELi64ELi4ELb0ELb0EN7cutlass6half_tE19Flash_kernel_traitsILi160ELi64ELi64ELi4ES2_EELb1ELb1ELb1ELb1ELb1EEv16Flash_fwd_params
    0 bytes stack frame, 0 bytes spill stores, 0 bytes spill loads
ptxas info    : Used 253 registers, 584 bytes cmem[0]
/anaconda/envs/flashattention/lib/python3.9/site-packages/torch/include/c10/core/SymInt.h(84): warning: integer conversion resulted in a change of sign

[22/36] /usr/local/cuda-11.4/bin/nvcc  -I/home/v-feiychen/flash-attention/csrc/flash_attn -I/home/v-feiychen/flash-attention/csrc/flash_attn/src -I/home/v-feiychen/flash-attention/csrc/cutlass/include -I/anaconda/envs/flashattention/lib/python3.9/site-packages/torch/include -I/anaconda/envs/flashattention/lib/python3.9/site-packages/torch/include/torch/csrc/api/include -I/anaconda/envs/flashattention/lib/python3.9/site-packages/torch/include/TH -I/anaconda/envs/flashattention/lib/python3.9/site-packages/torch/include/THC -I/usr/local/cuda-11.4/include -I/anaconda/envs/flashattention/include/python3.9 -c -c /home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_bwd_hdim128_bf16_sm80.cu -o /home/v-feiychen/flash-attention/build/temp.linux-x86_64-cpython-39/csrc/flash_attn/src/flash_bwd_hdim128_bf16_sm80.o -D__CUDA_NO_HALF_OPERATORS__ -D__CUDA_NO_HALF_CONVERSIONS__ -D__CUDA_NO_BFLOAT16_CONVERSIONS__ -D__CUDA_NO_HALF2_OPERATORS__ --expt-relaxed-constexpr --compiler-options ''"'"'-fPIC'"'"'' -O3 -std=c++17 -U__CUDA_NO_HALF_OPERATORS__ -U__CUDA_NO_HALF_CONVERSIONS__ -U__CUDA_NO_HALF2_OPERATORS__ -U__CUDA_NO_BFLOAT16_CONVERSIONS__ --expt-relaxed-constexpr --expt-extended-lambda --use_fast_math --ptxas-options=-v -lineinfo -gencode arch=compute_80,code=sm_80 --threads 4 -DTORCH_API_INCLUDE_EXTENSION_H '-DPYBIND11_COMPILER_TYPE="_gcc"' '-DPYBIND11_STDLIB="_libstdcpp"' '-DPYBIND11_BUILD_ABI="_cxxabi1011"' -DTORCH_EXTENSION_NAME=flash_attn_2_cuda -D_GLIBCXX_USE_CXX11_ABI=0
/anaconda/envs/flashattention/lib/python3.9/site-packages/torch/include/c10/core/SymInt.h(84): warning: integer conversion resulted in a change of sign

ptxas info    : 2 bytes gmem
ptxas info    : Compiling entry function '_Z44flash_bwd_dq_dk_dv_loop_seqk_parallel_kernelI23Flash_bwd_kernel_traitsILi128ELi64ELi64ELi8ELi4ELi2ELi2ELb1ELb0EN7cutlass10bfloat16_tE19Flash_kernel_traitsILi128ELi64ELi64ELi8ES2_EELb0ELb0ELb0ELb0EEv16Flash_bwd_params' for 'sm_80'
ptxas info    : Function properties for _Z44flash_bwd_dq_dk_dv_loop_seqk_parallel_kernelI23Flash_bwd_kernel_traitsILi128ELi64ELi64ELi8ELi4ELi2ELi2ELb1ELb0EN7cutlass10bfloat16_tE19Flash_kernel_traitsILi128ELi64ELi64ELi8ES2_EELb0ELb0ELb0ELb0EEv16Flash_bwd_params
    0 bytes stack frame, 0 bytes spill stores, 0 bytes spill loads
ptxas info    : Used 242 registers, 696 bytes cmem[0]
ptxas info    : Compiling entry function '_Z44flash_bwd_dq_dk_dv_loop_seqk_parallel_kernelI23Flash_bwd_kernel_traitsILi128ELi64ELi64ELi8ELi4ELi2ELi2ELb1ELb0EN7cutlass10bfloat16_tE19Flash_kernel_traitsILi128ELi64ELi64ELi8ES2_EELb0ELb0ELb0ELb1EEv16Flash_bwd_params' for 'sm_80'
ptxas info    : Function properties for _Z44flash_bwd_dq_dk_dv_loop_seqk_parallel_kernelI23Flash_bwd_kernel_traitsILi128ELi64ELi64ELi8ELi4ELi2ELi2ELb1ELb0EN7cutlass10bfloat16_tE19Flash_kernel_traitsILi128ELi64ELi64ELi8ES2_EELb0ELb0ELb0ELb1EEv16Flash_bwd_params
    0 bytes stack frame, 0 bytes spill stores, 0 bytes spill loads
ptxas info    : Used 241 registers, 696 bytes cmem[0]
ptxas info    : Compiling entry function '_Z44flash_bwd_dq_dk_dv_loop_seqk_parallel_kernelI23Flash_bwd_kernel_traitsILi128ELi64ELi64ELi8ELi4ELi2ELi2ELb1ELb0EN7cutlass10bfloat16_tE19Flash_kernel_traitsILi128ELi64ELi64ELi8ES2_EELb0ELb0ELb1ELb0EEv16Flash_bwd_params' for 'sm_80'
ptxas info    : Function properties for _Z44flash_bwd_dq_dk_dv_loop_seqk_parallel_kernelI23Flash_bwd_kernel_traitsILi128ELi64ELi64ELi8ELi4ELi2ELi2ELb1ELb0EN7cutlass10bfloat16_tE19Flash_kernel_traitsILi128ELi64ELi64ELi8ES2_EELb0ELb0ELb1ELb0EEv16Flash_bwd_params
    0 bytes stack frame, 0 bytes spill stores, 0 bytes spill loads
ptxas info    : Used 239 registers, 696 bytes cmem[0]
ptxas info    : Compiling entry function '_Z44flash_bwd_dq_dk_dv_loop_seqk_parallel_kernelI23Flash_bwd_kernel_traitsILi128ELi64ELi64ELi8ELi4ELi2ELi2ELb1ELb0EN7cutlass10bfloat16_tE19Flash_kernel_traitsILi128ELi64ELi64ELi8ES2_EELb0ELb0ELb1ELb1EEv16Flash_bwd_params' for 'sm_80'
ptxas info    : Function properties for _Z44flash_bwd_dq_dk_dv_loop_seqk_parallel_kernelI23Flash_bwd_kernel_traitsILi128ELi64ELi64ELi8ELi4ELi2ELi2ELb1ELb0EN7cutlass10bfloat16_tE19Flash_kernel_traitsILi128ELi64ELi64ELi8ES2_EELb0ELb0ELb1ELb1EEv16Flash_bwd_params
    0 bytes stack frame, 0 bytes spill stores, 0 bytes spill loads
ptxas info    : Used 236 registers, 696 bytes cmem[0]
ptxas info    : Compiling entry function '_Z44flash_bwd_dq_dk_dv_loop_seqk_parallel_kernelI23Flash_bwd_kernel_traitsILi128ELi64ELi64ELi8ELi4ELi2ELi2ELb1ELb0EN7cutlass10bfloat16_tE19Flash_kernel_traitsILi128ELi64ELi64ELi8ES2_EELb0ELb1ELb0ELb0EEv16Flash_bwd_params' for 'sm_80'
ptxas info    : Function properties for _Z44flash_bwd_dq_dk_dv_loop_seqk_parallel_kernelI23Flash_bwd_kernel_traitsILi128ELi64ELi64ELi8ELi4ELi2ELi2ELb1ELb0EN7cutlass10bfloat16_tE19Flash_kernel_traitsILi128ELi64ELi64ELi8ES2_EELb0ELb1ELb0ELb0EEv16Flash_bwd_params
    0 bytes stack frame, 0 bytes spill stores, 0 bytes spill loads
ptxas info    : Used 240 registers, 696 bytes cmem[0]
ptxas info    : Compiling entry function '_Z44flash_bwd_dq_dk_dv_loop_seqk_parallel_kernelI23Flash_bwd_kernel_traitsILi128ELi64ELi64ELi8ELi4ELi2ELi2ELb1ELb0EN7cutlass10bfloat16_tE19Flash_kernel_traitsILi128ELi64ELi64ELi8ES2_EELb0ELb1ELb0ELb1EEv16Flash_bwd_params' for 'sm_80'
ptxas info    : Function properties for _Z44flash_bwd_dq_dk_dv_loop_seqk_parallel_kernelI23Flash_bwd_kernel_traitsILi128ELi64ELi64ELi8ELi4ELi2ELi2ELb1ELb0EN7cutlass10bfloat16_tE19Flash_kernel_traitsILi128ELi64ELi64ELi8ES2_EELb0ELb1ELb0ELb1EEv16Flash_bwd_params
    0 bytes stack frame, 0 bytes spill stores, 0 bytes spill loads
ptxas info    : Used 240 registers, 696 bytes cmem[0]
ptxas info    : Compiling entry function '_Z44flash_bwd_dq_dk_dv_loop_seqk_parallel_kernelI23Flash_bwd_kernel_traitsILi128ELi64ELi64ELi8ELi4ELi2ELi2ELb1ELb0EN7cutlass10bfloat16_tE19Flash_kernel_traitsILi128ELi64ELi64ELi8ES2_EELb0ELb1ELb1ELb0EEv16Flash_bwd_params' for 'sm_80'
ptxas info    : Function properties for _Z44flash_bwd_dq_dk_dv_loop_seqk_parallel_kernelI23Flash_bwd_kernel_traitsILi128ELi64ELi64ELi8ELi4ELi2ELi2ELb1ELb0EN7cutlass10bfloat16_tE19Flash_kernel_traitsILi128ELi64ELi64ELi8ES2_EELb0ELb1ELb1ELb0EEv16Flash_bwd_params
    0 bytes stack frame, 0 bytes spill stores, 0 bytes spill loads
ptxas info    : Used 238 registers, 696 bytes cmem[0]
ptxas info    : Compiling entry function '_Z44flash_bwd_dq_dk_dv_loop_seqk_parallel_kernelI23Flash_bwd_kernel_traitsILi128ELi64ELi64ELi8ELi4ELi2ELi2ELb1ELb0EN7cutlass10bfloat16_tE19Flash_kernel_traitsILi128ELi64ELi64ELi8ES2_EELb0ELb1ELb1ELb1EEv16Flash_bwd_params' for 'sm_80'
ptxas info    : Function properties for _Z44flash_bwd_dq_dk_dv_loop_seqk_parallel_kernelI23Flash_bwd_kernel_traitsILi128ELi64ELi64ELi8ELi4ELi2ELi2ELb1ELb0EN7cutlass10bfloat16_tE19Flash_kernel_traitsILi128ELi64ELi64ELi8ES2_EELb0ELb1ELb1ELb1EEv16Flash_bwd_params
    0 bytes stack frame, 0 bytes spill stores, 0 bytes spill loads
ptxas info    : Used 240 registers, 696 bytes cmem[0]
ptxas info    : Compiling entry function '_Z44flash_bwd_dq_dk_dv_loop_seqk_parallel_kernelI23Flash_bwd_kernel_traitsILi128ELi64ELi128ELi8ELi2ELi4ELi2ELb0ELb0EN7cutlass10bfloat16_tE19Flash_kernel_traitsILi128ELi64ELi128ELi8ES2_EELb0ELb0ELb0ELb0EEv16Flash_bwd_params' for 'sm_80'
ptxas info    : Function properties for _Z44flash_bwd_dq_dk_dv_loop_seqk_parallel_kernelI23Flash_bwd_kernel_traitsILi128ELi64ELi128ELi8ELi2ELi4ELi2ELb0ELb0EN7cutlass10bfloat16_tE19Flash_kernel_traitsILi128ELi64ELi128ELi8ES2_EELb0ELb0ELb0ELb0EEv16Flash_bwd_params
    136 bytes stack frame, 164 bytes spill stores, 152 bytes spill loads
ptxas info    : Used 255 registers, 696 bytes cmem[0]
ptxas info    : Compiling entry function '_Z44flash_bwd_dq_dk_dv_loop_seqk_parallel_kernelI23Flash_bwd_kernel_traitsILi128ELi64ELi128ELi8ELi2ELi4ELi2ELb0ELb0EN7cutlass10bfloat16_tE19Flash_kernel_traitsILi128ELi64ELi128ELi8ES2_EELb0ELb0ELb0ELb1EEv16Flash_bwd_params' for 'sm_80'
ptxas info    : Function properties for _Z44flash_bwd_dq_dk_dv_loop_seqk_parallel_kernelI23Flash_bwd_kernel_traitsILi128ELi64ELi128ELi8ELi2ELi4ELi2ELb0ELb0EN7cutlass10bfloat16_tE19Flash_kernel_traitsILi128ELi64ELi128ELi8ES2_EELb0ELb0ELb0ELb1EEv16Flash_bwd_params
    136 bytes stack frame, 168 bytes spill stores, 180 bytes spill loads
ptxas info    : Used 255 registers, 696 bytes cmem[0]
ptxas info    : Compiling entry function '_Z44flash_bwd_dq_dk_dv_loop_seqk_parallel_kernelI23Flash_bwd_kernel_traitsILi128ELi64ELi128ELi8ELi2ELi4ELi2ELb0ELb0EN7cutlass10bfloat16_tE19Flash_kernel_traitsILi128ELi64ELi128ELi8ES2_EELb0ELb0ELb1ELb0EEv16Flash_bwd_params' for 'sm_80'
ptxas info    : Function properties for _Z44flash_bwd_dq_dk_dv_loop_seqk_parallel_kernelI23Flash_bwd_kernel_traitsILi128ELi64ELi128ELi8ELi2ELi4ELi2ELb0ELb0EN7cutlass10bfloat16_tE19Flash_kernel_traitsILi128ELi64ELi128ELi8ES2_EELb0ELb0ELb1ELb0EEv16Flash_bwd_params
    120 bytes stack frame, 156 bytes spill stores, 168 bytes spill loads
ptxas info    : Used 255 registers, 696 bytes cmem[0]
ptxas info    : Compiling entry function '_Z44flash_bwd_dq_dk_dv_loop_seqk_parallel_kernelI23Flash_bwd_kernel_traitsILi128ELi64ELi128ELi8ELi2ELi4ELi2ELb0ELb0EN7cutlass10bfloat16_tE19Flash_kernel_traitsILi128ELi64ELi128ELi8ES2_EELb0ELb0ELb1ELb1EEv16Flash_bwd_params' for 'sm_80'
ptxas info    : Function properties for _Z44flash_bwd_dq_dk_dv_loop_seqk_parallel_kernelI23Flash_bwd_kernel_traitsILi128ELi64ELi128ELi8ELi2ELi4ELi2ELb0ELb0EN7cutlass10bfloat16_tE19Flash_kernel_traitsILi128ELi64ELi128ELi8ES2_EELb0ELb0ELb1ELb1EEv16Flash_bwd_params
    136 bytes stack frame, 160 bytes spill stores, 176 bytes spill loads
ptxas info    : Used 255 registers, 696 bytes cmem[0]
ptxas info    : Compiling entry function '_Z44flash_bwd_dq_dk_dv_loop_seqk_parallel_kernelI23Flash_bwd_kernel_traitsILi128ELi64ELi128ELi8ELi2ELi4ELi2ELb0ELb0EN7cutlass10bfloat16_tE19Flash_kernel_traitsILi128ELi64ELi128ELi8ES2_EELb0ELb1ELb0ELb0EEv16Flash_bwd_params' for 'sm_80'
ptxas info    : Function properties for _Z44flash_bwd_dq_dk_dv_loop_seqk_parallel_kernelI23Flash_bwd_kernel_traitsILi128ELi64ELi128ELi8ELi2ELi4ELi2ELb0ELb0EN7cutlass10bfloat16_tE19Flash_kernel_traitsILi128ELi64ELi128ELi8ES2_EELb0ELb1ELb0ELb0EEv16Flash_bwd_params
    120 bytes stack frame, 144 bytes spill stores, 132 bytes spill loads
ptxas info    : Used 255 registers, 696 bytes cmem[0]
ptxas info    : Compiling entry function '_Z44flash_bwd_dq_dk_dv_loop_seqk_parallel_kernelI23Flash_bwd_kernel_traitsILi128ELi64ELi128ELi8ELi2ELi4ELi2ELb0ELb0EN7cutlass10bfloat16_tE19Flash_kernel_traitsILi128ELi64ELi128ELi8ES2_EELb0ELb1ELb0ELb1EEv16Flash_bwd_params' for 'sm_80'
ptxas info    : Function properties for _Z44flash_bwd_dq_dk_dv_loop_seqk_parallel_kernelI23Flash_bwd_kernel_traitsILi128ELi64ELi128ELi8ELi2ELi4ELi2ELb0ELb0EN7cutlass10bfloat16_tE19Flash_kernel_traitsILi128ELi64ELi128ELi8ES2_EELb0ELb1ELb0ELb1EEv16Flash_bwd_params
    144 bytes stack frame, 168 bytes spill stores, 180 bytes spill loads
ptxas info    : Used 255 registers, 696 bytes cmem[0]
ptxas info    : Compiling entry function '_Z44flash_bwd_dq_dk_dv_loop_seqk_parallel_kernelI23Flash_bwd_kernel_traitsILi128ELi64ELi128ELi8ELi2ELi4ELi2ELb0ELb0EN7cutlass10bfloat16_tE19Flash_kernel_traitsILi128ELi64ELi128ELi8ES2_EELb0ELb1ELb1ELb0EEv16Flash_bwd_params' for 'sm_80'
ptxas info    : Function properties for _Z44flash_bwd_dq_dk_dv_loop_seqk_parallel_kernelI23Flash_bwd_kernel_traitsILi128ELi64ELi128ELi8ELi2ELi4ELi2ELb0ELb0EN7cutlass10bfloat16_tE19Flash_kernel_traitsILi128ELi64ELi128ELi8ES2_EELb0ELb1ELb1ELb0EEv16Flash_bwd_params
    136 bytes stack frame, 156 bytes spill stores, 152 bytes spill loads
ptxas info    : Used 255 registers, 696 bytes cmem[0]
ptxas info    : Compiling entry function '_Z44flash_bwd_dq_dk_dv_loop_seqk_parallel_kernelI23Flash_bwd_kernel_traitsILi128ELi64ELi128ELi8ELi2ELi4ELi2ELb0ELb0EN7cutlass10bfloat16_tE19Flash_kernel_traitsILi128ELi64ELi128ELi8ES2_EELb0ELb1ELb1ELb1EEv16Flash_bwd_params' for 'sm_80'
ptxas info    : Function properties for _Z44flash_bwd_dq_dk_dv_loop_seqk_parallel_kernelI23Flash_bwd_kernel_traitsILi128ELi64ELi128ELi8ELi2ELi4ELi2ELb0ELb0EN7cutlass10bfloat16_tE19Flash_kernel_traitsILi128ELi64ELi128ELi8ES2_EELb0ELb1ELb1ELb1EEv16Flash_bwd_params
    144 bytes stack frame, 164 bytes spill stores, 156 bytes spill loads
ptxas info    : Used 255 registers, 696 bytes cmem[0]
ptxas info    : Compiling entry function '_Z27flash_bwd_convert_dq_kernelI23Flash_bwd_kernel_traitsILi128ELi64ELi64ELi8ELi4ELi2ELi2ELb1ELb0EN7cutlass10bfloat16_tE19Flash_kernel_traitsILi128ELi64ELi64ELi8ES2_EEEv16Flash_bwd_params' for 'sm_80'
ptxas info    : Function properties for _Z27flash_bwd_convert_dq_kernelI23Flash_bwd_kernel_traitsILi128ELi64ELi64ELi8ELi4ELi2ELi2ELb1ELb0EN7cutlass10bfloat16_tE19Flash_kernel_traitsILi128ELi64ELi64ELi8ES2_EEEv16Flash_bwd_params
    0 bytes stack frame, 0 bytes spill stores, 0 bytes spill loads
ptxas info    : Used 32 registers, 696 bytes cmem[0]
ptxas info    : Compiling entry function '_Z44flash_bwd_dq_dk_dv_loop_seqk_parallel_kernelI23Flash_bwd_kernel_traitsILi128ELi64ELi64ELi8ELi4ELi2ELi2ELb1ELb0EN7cutlass10bfloat16_tE19Flash_kernel_traitsILi128ELi64ELi64ELi8ES2_EELb1ELb0ELb0ELb0EEv16Flash_bwd_params' for 'sm_80'
ptxas info    : Function properties for _Z44flash_bwd_dq_dk_dv_loop_seqk_parallel_kernelI23Flash_bwd_kernel_traitsILi128ELi64ELi64ELi8ELi4ELi2ELi2ELb1ELb0EN7cutlass10bfloat16_tE19Flash_kernel_traitsILi128ELi64ELi64ELi8ES2_EELb1ELb0ELb0ELb0EEv16Flash_bwd_params
    0 bytes stack frame, 0 bytes spill stores, 0 bytes spill loads
ptxas info    : Used 239 registers, 696 bytes cmem[0]
ptxas info    : Compiling entry function '_Z44flash_bwd_dq_dk_dv_loop_seqk_parallel_kernelI23Flash_bwd_kernel_traitsILi128ELi64ELi64ELi8ELi4ELi2ELi2ELb1ELb0EN7cutlass10bfloat16_tE19Flash_kernel_traitsILi128ELi64ELi64ELi8ES2_EELb1ELb0ELb0ELb1EEv16Flash_bwd_params' for 'sm_80'
ptxas info    : Function properties for _Z44flash_bwd_dq_dk_dv_loop_seqk_parallel_kernelI23Flash_bwd_kernel_traitsILi128ELi64ELi64ELi8ELi4ELi2ELi2ELb1ELb0EN7cutlass10bfloat16_tE19Flash_kernel_traitsILi128ELi64ELi64ELi8ES2_EELb1ELb0ELb0ELb1EEv16Flash_bwd_params
    0 bytes stack frame, 0 bytes spill stores, 0 bytes spill loads
ptxas info    : Used 242 registers, 696 bytes cmem[0]
ptxas info    : Compiling entry function '_Z44flash_bwd_dq_dk_dv_loop_seqk_parallel_kernelI23Flash_bwd_kernel_traitsILi128ELi64ELi64ELi8ELi4ELi2ELi2ELb1ELb0EN7cutlass10bfloat16_tE19Flash_kernel_traitsILi128ELi64ELi64ELi8ES2_EELb1ELb0ELb1ELb0EEv16Flash_bwd_params' for 'sm_80'
ptxas info    : Function properties for _Z44flash_bwd_dq_dk_dv_loop_seqk_parallel_kernelI23Flash_bwd_kernel_traitsILi128ELi64ELi64ELi8ELi4ELi2ELi2ELb1ELb0EN7cutlass10bfloat16_tE19Flash_kernel_traitsILi128ELi64ELi64ELi8ES2_EELb1ELb0ELb1ELb0EEv16Flash_bwd_params
    0 bytes stack frame, 0 bytes spill stores, 0 bytes spill loads
ptxas info    : Used 240 registers, 696 bytes cmem[0]
ptxas info    : Compiling entry function '_Z44flash_bwd_dq_dk_dv_loop_seqk_parallel_kernelI23Flash_bwd_kernel_traitsILi128ELi64ELi64ELi8ELi4ELi2ELi2ELb1ELb0EN7cutlass10bfloat16_tE19Flash_kernel_traitsILi128ELi64ELi64ELi8ES2_EELb1ELb0ELb1ELb1EEv16Flash_bwd_params' for 'sm_80'
ptxas info    : Function properties for _Z44flash_bwd_dq_dk_dv_loop_seqk_parallel_kernelI23Flash_bwd_kernel_traitsILi128ELi64ELi64ELi8ELi4ELi2ELi2ELb1ELb0EN7cutlass10bfloat16_tE19Flash_kernel_traitsILi128ELi64ELi64ELi8ES2_EELb1ELb0ELb1ELb1EEv16Flash_bwd_params
    0 bytes stack frame, 0 bytes spill stores, 0 bytes spill loads
ptxas info    : Used 240 registers, 696 bytes cmem[0]
ptxas info    : Compiling entry function '_Z44flash_bwd_dq_dk_dv_loop_seqk_parallel_kernelI23Flash_bwd_kernel_traitsILi128ELi64ELi64ELi8ELi4ELi2ELi2ELb1ELb0EN7cutlass10bfloat16_tE19Flash_kernel_traitsILi128ELi64ELi64ELi8ES2_EELb1ELb1ELb0ELb0EEv16Flash_bwd_params' for 'sm_80'
ptxas info    : Function properties for _Z44flash_bwd_dq_dk_dv_loop_seqk_parallel_kernelI23Flash_bwd_kernel_traitsILi128ELi64ELi64ELi8ELi4ELi2ELi2ELb1ELb0EN7cutlass10bfloat16_tE19Flash_kernel_traitsILi128ELi64ELi64ELi8ES2_EELb1ELb1ELb0ELb0EEv16Flash_bwd_params
    0 bytes stack frame, 0 bytes spill stores, 0 bytes spill loads
ptxas info    : Used 244 registers, 696 bytes cmem[0]
ptxas info    : Compiling entry function '_Z44flash_bwd_dq_dk_dv_loop_seqk_parallel_kernelI23Flash_bwd_kernel_traitsILi128ELi64ELi64ELi8ELi4ELi2ELi2ELb1ELb0EN7cutlass10bfloat16_tE19Flash_kernel_traitsILi128ELi64ELi64ELi8ES2_EELb1ELb1ELb0ELb1EEv16Flash_bwd_params' for 'sm_80'
ptxas info    : Function properties for _Z44flash_bwd_dq_dk_dv_loop_seqk_parallel_kernelI23Flash_bwd_kernel_traitsILi128ELi64ELi64ELi8ELi4ELi2ELi2ELb1ELb0EN7cutlass10bfloat16_tE19Flash_kernel_traitsILi128ELi64ELi64ELi8ES2_EELb1ELb1ELb0ELb1EEv16Flash_bwd_params
    0 bytes stack frame, 0 bytes spill stores, 0 bytes spill loads
ptxas info    : Used 241 registers, 696 bytes cmem[0]
ptxas info    : Compiling entry function '_Z44flash_bwd_dq_dk_dv_loop_seqk_parallel_kernelI23Flash_bwd_kernel_traitsILi128ELi64ELi64ELi8ELi4ELi2ELi2ELb1ELb0EN7cutlass10bfloat16_tE19Flash_kernel_traitsILi128ELi64ELi64ELi8ES2_EELb1ELb1ELb1ELb0EEv16Flash_bwd_params' for 'sm_80'
ptxas info    : Function properties for _Z44flash_bwd_dq_dk_dv_loop_seqk_parallel_kernelI23Flash_bwd_kernel_traitsILi128ELi64ELi64ELi8ELi4ELi2ELi2ELb1ELb0EN7cutlass10bfloat16_tE19Flash_kernel_traitsILi128ELi64ELi64ELi8ES2_EELb1ELb1ELb1ELb0EEv16Flash_bwd_params
    0 bytes stack frame, 0 bytes spill stores, 0 bytes spill loads
ptxas info    : Used 239 registers, 696 bytes cmem[0]
ptxas info    : Compiling entry function '_Z44flash_bwd_dq_dk_dv_loop_seqk_parallel_kernelI23Flash_bwd_kernel_traitsILi128ELi64ELi64ELi8ELi4ELi2ELi2ELb1ELb0EN7cutlass10bfloat16_tE19Flash_kernel_traitsILi128ELi64ELi64ELi8ES2_EELb1ELb1ELb1ELb1EEv16Flash_bwd_params' for 'sm_80'
ptxas info    : Function properties for _Z44flash_bwd_dq_dk_dv_loop_seqk_parallel_kernelI23Flash_bwd_kernel_traitsILi128ELi64ELi64ELi8ELi4ELi2ELi2ELb1ELb0EN7cutlass10bfloat16_tE19Flash_kernel_traitsILi128ELi64ELi64ELi8ES2_EELb1ELb1ELb1ELb1EEv16Flash_bwd_params
    0 bytes stack frame, 0 bytes spill stores, 0 bytes spill loads
ptxas info    : Used 240 registers, 696 bytes cmem[0]
ptxas info    : Compiling entry function '_Z25flash_bwd_dot_do_o_kernelILb1E23Flash_bwd_kernel_traitsILi128ELi64ELi64ELi8ELi4ELi2ELi2ELb1ELb0EN7cutlass10bfloat16_tE19Flash_kernel_traitsILi128ELi64ELi64ELi8ES2_EEEv16Flash_bwd_params' for 'sm_80'
ptxas info    : Function properties for _Z25flash_bwd_dot_do_o_kernelILb1E23Flash_bwd_kernel_traitsILi128ELi64ELi64ELi8ELi4ELi2ELi2ELb1ELb0EN7cutlass10bfloat16_tE19Flash_kernel_traitsILi128ELi64ELi64ELi8ES2_EEEv16Flash_bwd_params
    0 bytes stack frame, 0 bytes spill stores, 0 bytes spill loads
ptxas info    : Used 47 registers, 696 bytes cmem[0]
ptxas info    : Compiling entry function '_Z27flash_bwd_convert_dq_kernelI23Flash_bwd_kernel_traitsILi128ELi64ELi128ELi8ELi2ELi4ELi2ELb0ELb0EN7cutlass10bfloat16_tE19Flash_kernel_traitsILi128ELi64ELi128ELi8ES2_EEEv16Flash_bwd_params' for 'sm_80'
ptxas info    : Function properties for _Z27flash_bwd_convert_dq_kernelI23Flash_bwd_kernel_traitsILi128ELi64ELi128ELi8ELi2ELi4ELi2ELb0ELb0EN7cutlass10bfloat16_tE19Flash_kernel_traitsILi128ELi64ELi128ELi8ES2_EEEv16Flash_bwd_params
    0 bytes stack frame, 0 bytes spill stores, 0 bytes spill loads
ptxas info    : Used 32 registers, 696 bytes cmem[0]
ptxas info    : Compiling entry function '_Z44flash_bwd_dq_dk_dv_loop_seqk_parallel_kernelI23Flash_bwd_kernel_traitsILi128ELi64ELi128ELi8ELi2ELi4ELi2ELb0ELb0EN7cutlass10bfloat16_tE19Flash_kernel_traitsILi128ELi64ELi128ELi8ES2_EELb1ELb0ELb0ELb0EEv16Flash_bwd_params' for 'sm_80'
ptxas info    : Function properties for _Z44flash_bwd_dq_dk_dv_loop_seqk_parallel_kernelI23Flash_bwd_kernel_traitsILi128ELi64ELi128ELi8ELi2ELi4ELi2ELb0ELb0EN7cutlass10bfloat16_tE19Flash_kernel_traitsILi128ELi64ELi128ELi8ES2_EELb1ELb0ELb0ELb0EEv16Flash_bwd_params
    136 bytes stack frame, 168 bytes spill stores, 184 bytes spill loads
ptxas info    : Used 255 registers, 696 bytes cmem[0]
ptxas info    : Compiling entry function '_Z44flash_bwd_dq_dk_dv_loop_seqk_parallel_kernelI23Flash_bwd_kernel_traitsILi128ELi64ELi128ELi8ELi2ELi4ELi2ELb0ELb0EN7cutlass10bfloat16_tE19Flash_kernel_traitsILi128ELi64ELi128ELi8ES2_EELb1ELb0ELb0ELb1EEv16Flash_bwd_params' for 'sm_80'
ptxas info    : Function properties for _Z44flash_bwd_dq_dk_dv_loop_seqk_parallel_kernelI23Flash_bwd_kernel_traitsILi128ELi64ELi128ELi8ELi2ELi4ELi2ELb0ELb0EN7cutlass10bfloat16_tE19Flash_kernel_traitsILi128ELi64ELi128ELi8ES2_EELb1ELb0ELb0ELb1EEv16Flash_bwd_params
    128 bytes stack frame, 160 bytes spill stores, 176 bytes spill loads
ptxas info    : Used 255 registers, 696 bytes cmem[0]
ptxas info    : Compiling entry function '_Z44flash_bwd_dq_dk_dv_loop_seqk_parallel_kernelI23Flash_bwd_kernel_traitsILi128ELi64ELi128ELi8ELi2ELi4ELi2ELb0ELb0EN7cutlass10bfloat16_tE19Flash_kernel_traitsILi128ELi64ELi128ELi8ES2_EELb1ELb0ELb1ELb0EEv16Flash_bwd_params' for 'sm_80'
ptxas info    : Function properties for _Z44flash_bwd_dq_dk_dv_loop_seqk_parallel_kernelI23Flash_bwd_kernel_traitsILi128ELi64ELi128ELi8ELi2ELi4ELi2ELb0ELb0EN7cutlass10bfloat16_tE19Flash_kernel_traitsILi128ELi64ELi128ELi8ES2_EELb1ELb0ELb1ELb0EEv16Flash_bwd_params
    112 bytes stack frame, 140 bytes spill stores, 164 bytes spill loads
ptxas info    : Used 255 registers, 696 bytes cmem[0]
ptxas info    : Compiling entry function '_Z44flash_bwd_dq_dk_dv_loop_seqk_parallel_kernelI23Flash_bwd_kernel_traitsILi128ELi64ELi128ELi8ELi2ELi4ELi2ELb0ELb0EN7cutlass10bfloat16_tE19Flash_kernel_traitsILi128ELi64ELi128ELi8ES2_EELb1ELb0ELb1ELb1EEv16Flash_bwd_params' for 'sm_80'
ptxas info    : Function properties for _Z44flash_bwd_dq_dk_dv_loop_seqk_parallel_kernelI23Flash_bwd_kernel_traitsILi128ELi64ELi128ELi8ELi2ELi4ELi2ELb0ELb0EN7cutlass10bfloat16_tE19Flash_kernel_traitsILi128ELi64ELi128ELi8ES2_EELb1ELb0ELb1ELb1EEv16Flash_bwd_params
    0 bytes stack frame, 0 bytes spill stores, 0 bytes spill loads
ptxas info    : Used 255 registers, 696 bytes cmem[0]
ptxas info    : Compiling entry function '_Z44flash_bwd_dq_dk_dv_loop_seqk_parallel_kernelI23Flash_bwd_kernel_traitsILi128ELi64ELi128ELi8ELi2ELi4ELi2ELb0ELb0EN7cutlass10bfloat16_tE19Flash_kernel_traitsILi128ELi64ELi128ELi8ES2_EELb1ELb1ELb0ELb0EEv16Flash_bwd_params' for 'sm_80'
ptxas info    : Function properties for _Z44flash_bwd_dq_dk_dv_loop_seqk_parallel_kernelI23Flash_bwd_kernel_traitsILi128ELi64ELi128ELi8ELi2ELi4ELi2ELb0ELb0EN7cutlass10bfloat16_tE19Flash_kernel_traitsILi128ELi64ELi128ELi8ES2_EELb1ELb1ELb0ELb0EEv16Flash_bwd_params
    24 bytes stack frame, 24 bytes spill stores, 24 bytes spill loads
ptxas info    : Used 255 registers, 696 bytes cmem[0]
ptxas info    : Compiling entry function '_Z44flash_bwd_dq_dk_dv_loop_seqk_parallel_kernelI23Flash_bwd_kernel_traitsILi128ELi64ELi128ELi8ELi2ELi4ELi2ELb0ELb0EN7cutlass10bfloat16_tE19Flash_kernel_traitsILi128ELi64ELi128ELi8ES2_EELb1ELb1ELb0ELb1EEv16Flash_bwd_params' for 'sm_80'
ptxas info    : Function properties for _Z44flash_bwd_dq_dk_dv_loop_seqk_parallel_kernelI23Flash_bwd_kernel_traitsILi128ELi64ELi128ELi8ELi2ELi4ELi2ELb0ELb0EN7cutlass10bfloat16_tE19Flash_kernel_traitsILi128ELi64ELi128ELi8ES2_EELb1ELb1ELb0ELb1EEv16Flash_bwd_params
    16 bytes stack frame, 16 bytes spill stores, 16 bytes spill loads
ptxas info    : Used 255 registers, 696 bytes cmem[0]
ptxas info    : Compiling entry function '_Z44flash_bwd_dq_dk_dv_loop_seqk_parallel_kernelI23Flash_bwd_kernel_traitsILi128ELi64ELi128ELi8ELi2ELi4ELi2ELb0ELb0EN7cutlass10bfloat16_tE19Flash_kernel_traitsILi128ELi64ELi128ELi8ES2_EELb1ELb1ELb1ELb0EEv16Flash_bwd_params' for 'sm_80'
ptxas info    : Function properties for _Z44flash_bwd_dq_dk_dv_loop_seqk_parallel_kernelI23Flash_bwd_kernel_traitsILi128ELi64ELi128ELi8ELi2ELi4ELi2ELb0ELb0EN7cutlass10bfloat16_tE19Flash_kernel_traitsILi128ELi64ELi128ELi8ES2_EELb1ELb1ELb1ELb0EEv16Flash_bwd_params
    40 bytes stack frame, 48 bytes spill stores, 40 bytes spill loads
ptxas info    : Used 255 registers, 696 bytes cmem[0]
ptxas info    : Compiling entry function '_Z44flash_bwd_dq_dk_dv_loop_seqk_parallel_kernelI23Flash_bwd_kernel_traitsILi128ELi64ELi128ELi8ELi2ELi4ELi2ELb0ELb0EN7cutlass10bfloat16_tE19Flash_kernel_traitsILi128ELi64ELi128ELi8ES2_EELb1ELb1ELb1ELb1EEv16Flash_bwd_params' for 'sm_80'
ptxas info    : Function properties for _Z44flash_bwd_dq_dk_dv_loop_seqk_parallel_kernelI23Flash_bwd_kernel_traitsILi128ELi64ELi128ELi8ELi2ELi4ELi2ELb0ELb0EN7cutlass10bfloat16_tE19Flash_kernel_traitsILi128ELi64ELi128ELi8ES2_EELb1ELb1ELb1ELb1EEv16Flash_bwd_params
    16 bytes stack frame, 12 bytes spill stores, 12 bytes spill loads
ptxas info    : Used 255 registers, 696 bytes cmem[0]
ptxas info    : Compiling entry function '_Z25flash_bwd_dot_do_o_kernelILb1E23Flash_bwd_kernel_traitsILi128ELi64ELi128ELi8ELi2ELi4ELi2ELb0ELb0EN7cutlass10bfloat16_tE19Flash_kernel_traitsILi128ELi64ELi128ELi8ES2_EEEv16Flash_bwd_params' for 'sm_80'
ptxas info    : Function properties for _Z25flash_bwd_dot_do_o_kernelILb1E23Flash_bwd_kernel_traitsILi128ELi64ELi128ELi8ELi2ELi4ELi2ELb0ELb0EN7cutlass10bfloat16_tE19Flash_kernel_traitsILi128ELi64ELi128ELi8ES2_EEEv16Flash_bwd_params
    0 bytes stack frame, 0 bytes spill stores, 0 bytes spill loads
ptxas info    : Used 47 registers, 696 bytes cmem[0]
/anaconda/envs/flashattention/lib/python3.9/site-packages/torch/include/c10/core/SymInt.h(84): warning: integer conversion resulted in a change of sign

[23/36] /usr/local/cuda-11.4/bin/nvcc  -I/home/v-feiychen/flash-attention/csrc/flash_attn -I/home/v-feiychen/flash-attention/csrc/flash_attn/src -I/home/v-feiychen/flash-attention/csrc/cutlass/include -I/anaconda/envs/flashattention/lib/python3.9/site-packages/torch/include -I/anaconda/envs/flashattention/lib/python3.9/site-packages/torch/include/torch/csrc/api/include -I/anaconda/envs/flashattention/lib/python3.9/site-packages/torch/include/TH -I/anaconda/envs/flashattention/lib/python3.9/site-packages/torch/include/THC -I/usr/local/cuda-11.4/include -I/anaconda/envs/flashattention/include/python3.9 -c -c /home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_bwd_hdim64_bf16_sm80.cu -o /home/v-feiychen/flash-attention/build/temp.linux-x86_64-cpython-39/csrc/flash_attn/src/flash_bwd_hdim64_bf16_sm80.o -D__CUDA_NO_HALF_OPERATORS__ -D__CUDA_NO_HALF_CONVERSIONS__ -D__CUDA_NO_BFLOAT16_CONVERSIONS__ -D__CUDA_NO_HALF2_OPERATORS__ --expt-relaxed-constexpr --compiler-options ''"'"'-fPIC'"'"'' -O3 -std=c++17 -U__CUDA_NO_HALF_OPERATORS__ -U__CUDA_NO_HALF_CONVERSIONS__ -U__CUDA_NO_HALF2_OPERATORS__ -U__CUDA_NO_BFLOAT16_CONVERSIONS__ --expt-relaxed-constexpr --expt-extended-lambda --use_fast_math --ptxas-options=-v -lineinfo -gencode arch=compute_80,code=sm_80 --threads 4 -DTORCH_API_INCLUDE_EXTENSION_H '-DPYBIND11_COMPILER_TYPE="_gcc"' '-DPYBIND11_STDLIB="_libstdcpp"' '-DPYBIND11_BUILD_ABI="_cxxabi1011"' -DTORCH_EXTENSION_NAME=flash_attn_2_cuda -D_GLIBCXX_USE_CXX11_ABI=0
/anaconda/envs/flashattention/lib/python3.9/site-packages/torch/include/c10/core/SymInt.h(84): warning: integer conversion resulted in a change of sign

ptxas info    : 2 bytes gmem
ptxas info    : Compiling entry function '_Z44flash_bwd_dq_dk_dv_loop_seqk_parallel_kernelI23Flash_bwd_kernel_traitsILi64ELi64ELi128ELi8ELi2ELi4ELi4ELb1ELb0EN7cutlass10bfloat16_tE19Flash_kernel_traitsILi64ELi64ELi128ELi8ES2_EELb0ELb0ELb0ELb0EEv16Flash_bwd_params' for 'sm_80'
ptxas info    : Function properties for _Z44flash_bwd_dq_dk_dv_loop_seqk_parallel_kernelI23Flash_bwd_kernel_traitsILi64ELi64ELi128ELi8ELi2ELi4ELi4ELb1ELb0EN7cutlass10bfloat16_tE19Flash_kernel_traitsILi64ELi64ELi128ELi8ES2_EELb0ELb0ELb0ELb0EEv16Flash_bwd_params
    0 bytes stack frame, 0 bytes spill stores, 0 bytes spill loads
ptxas info    : Used 230 registers, 696 bytes cmem[0]
ptxas info    : Compiling entry function '_Z44flash_bwd_dq_dk_dv_loop_seqk_parallel_kernelI23Flash_bwd_kernel_traitsILi64ELi64ELi128ELi8ELi2ELi4ELi4ELb1ELb0EN7cutlass10bfloat16_tE19Flash_kernel_traitsILi64ELi64ELi128ELi8ES2_EELb0ELb0ELb0ELb1EEv16Flash_bwd_params' for 'sm_80'
ptxas info    : Function properties for _Z44flash_bwd_dq_dk_dv_loop_seqk_parallel_kernelI23Flash_bwd_kernel_traitsILi64ELi64ELi128ELi8ELi2ELi4ELi4ELb1ELb0EN7cutlass10bfloat16_tE19Flash_kernel_traitsILi64ELi64ELi128ELi8ES2_EELb0ELb0ELb0ELb1EEv16Flash_bwd_params
    0 bytes stack frame, 0 bytes spill stores, 0 bytes spill loads
ptxas info    : Used 232 registers, 696 bytes cmem[0]
ptxas info    : Compiling entry function '_Z44flash_bwd_dq_dk_dv_loop_seqk_parallel_kernelI23Flash_bwd_kernel_traitsILi64ELi64ELi128ELi8ELi2ELi4ELi4ELb1ELb0EN7cutlass10bfloat16_tE19Flash_kernel_traitsILi64ELi64ELi128ELi8ES2_EELb0ELb0ELb1ELb0EEv16Flash_bwd_params' for 'sm_80'
ptxas info    : Function properties for _Z44flash_bwd_dq_dk_dv_loop_seqk_parallel_kernelI23Flash_bwd_kernel_traitsILi64ELi64ELi128ELi8ELi2ELi4ELi4ELb1ELb0EN7cutlass10bfloat16_tE19Flash_kernel_traitsILi64ELi64ELi128ELi8ES2_EELb0ELb0ELb1ELb0EEv16Flash_bwd_params
    0 bytes stack frame, 0 bytes spill stores, 0 bytes spill loads
ptxas info    : Used 220 registers, 696 bytes cmem[0]
ptxas info    : Compiling entry function '_Z44flash_bwd_dq_dk_dv_loop_seqk_parallel_kernelI23Flash_bwd_kernel_traitsILi64ELi64ELi128ELi8ELi2ELi4ELi4ELb1ELb0EN7cutlass10bfloat16_tE19Flash_kernel_traitsILi64ELi64ELi128ELi8ES2_EELb0ELb0ELb1ELb1EEv16Flash_bwd_params' for 'sm_80'
ptxas info    : Function properties for _Z44flash_bwd_dq_dk_dv_loop_seqk_parallel_kernelI23Flash_bwd_kernel_traitsILi64ELi64ELi128ELi8ELi2ELi4ELi4ELb1ELb0EN7cutlass10bfloat16_tE19Flash_kernel_traitsILi64ELi64ELi128ELi8ES2_EELb0ELb0ELb1ELb1EEv16Flash_bwd_params
    0 bytes stack frame, 0 bytes spill stores, 0 bytes spill loads
ptxas info    : Used 221 registers, 696 bytes cmem[0]
ptxas info    : Compiling entry function '_Z44flash_bwd_dq_dk_dv_loop_seqk_parallel_kernelI23Flash_bwd_kernel_traitsILi64ELi64ELi128ELi8ELi2ELi4ELi4ELb1ELb0EN7cutlass10bfloat16_tE19Flash_kernel_traitsILi64ELi64ELi128ELi8ES2_EELb0ELb1ELb0ELb0EEv16Flash_bwd_params' for 'sm_80'
ptxas info    : Function properties for _Z44flash_bwd_dq_dk_dv_loop_seqk_parallel_kernelI23Flash_bwd_kernel_traitsILi64ELi64ELi128ELi8ELi2ELi4ELi4ELb1ELb0EN7cutlass10bfloat16_tE19Flash_kernel_traitsILi64ELi64ELi128ELi8ES2_EELb0ELb1ELb0ELb0EEv16Flash_bwd_params
    0 bytes stack frame, 0 bytes spill stores, 0 bytes spill loads
ptxas info    : Used 231 registers, 696 bytes cmem[0]
ptxas info    : Compiling entry function '_Z44flash_bwd_dq_dk_dv_loop_seqk_parallel_kernelI23Flash_bwd_kernel_traitsILi64ELi64ELi128ELi8ELi2ELi4ELi4ELb1ELb0EN7cutlass10bfloat16_tE19Flash_kernel_traitsILi64ELi64ELi128ELi8ES2_EELb0ELb1ELb0ELb1EEv16Flash_bwd_params' for 'sm_80'
ptxas info    : Function properties for _Z44flash_bwd_dq_dk_dv_loop_seqk_parallel_kernelI23Flash_bwd_kernel_traitsILi64ELi64ELi128ELi8ELi2ELi4ELi4ELb1ELb0EN7cutlass10bfloat16_tE19Flash_kernel_traitsILi64ELi64ELi128ELi8ES2_EELb0ELb1ELb0ELb1EEv16Flash_bwd_params
    0 bytes stack frame, 0 bytes spill stores, 0 bytes spill loads
ptxas info    : Used 229 registers, 696 bytes cmem[0]
ptxas info    : Compiling entry function '_Z44flash_bwd_dq_dk_dv_loop_seqk_parallel_kernelI23Flash_bwd_kernel_traitsILi64ELi64ELi128ELi8ELi2ELi4ELi4ELb1ELb0EN7cutlass10bfloat16_tE19Flash_kernel_traitsILi64ELi64ELi128ELi8ES2_EELb0ELb1ELb1ELb0EEv16Flash_bwd_params' for 'sm_80'
ptxas info    : Function properties for _Z44flash_bwd_dq_dk_dv_loop_seqk_parallel_kernelI23Flash_bwd_kernel_traitsILi64ELi64ELi128ELi8ELi2ELi4ELi4ELb1ELb0EN7cutlass10bfloat16_tE19Flash_kernel_traitsILi64ELi64ELi128ELi8ES2_EELb0ELb1ELb1ELb0EEv16Flash_bwd_params
    0 bytes stack frame, 0 bytes spill stores, 0 bytes spill loads
ptxas info    : Used 225 registers, 696 bytes cmem[0]
ptxas info    : Compiling entry function '_Z44flash_bwd_dq_dk_dv_loop_seqk_parallel_kernelI23Flash_bwd_kernel_traitsILi64ELi64ELi128ELi8ELi2ELi4ELi4ELb1ELb0EN7cutlass10bfloat16_tE19Flash_kernel_traitsILi64ELi64ELi128ELi8ES2_EELb0ELb1ELb1ELb1EEv16Flash_bwd_params' for 'sm_80'
ptxas info    : Function properties for _Z44flash_bwd_dq_dk_dv_loop_seqk_parallel_kernelI23Flash_bwd_kernel_traitsILi64ELi64ELi128ELi8ELi2ELi4ELi4ELb1ELb0EN7cutlass10bfloat16_tE19Flash_kernel_traitsILi64ELi64ELi128ELi8ES2_EELb0ELb1ELb1ELb1EEv16Flash_bwd_params
    0 bytes stack frame, 0 bytes spill stores, 0 bytes spill loads
ptxas info    : Used 230 registers, 696 bytes cmem[0]
ptxas info    : Compiling entry function '_Z44flash_bwd_dq_dk_dv_loop_seqk_parallel_kernelI23Flash_bwd_kernel_traitsILi64ELi128ELi128ELi8ELi4ELi4ELi4ELb0ELb0EN7cutlass10bfloat16_tE19Flash_kernel_traitsILi64ELi128ELi128ELi8ES2_EELb0ELb0ELb0ELb0EEv16Flash_bwd_params' for 'sm_80'
ptxas info    : Function properties for _Z44flash_bwd_dq_dk_dv_loop_seqk_parallel_kernelI23Flash_bwd_kernel_traitsILi64ELi128ELi128ELi8ELi4ELi4ELi4ELb0ELb0EN7cutlass10bfloat16_tE19Flash_kernel_traitsILi64ELi128ELi128ELi8ES2_EELb0ELb0ELb0ELb0EEv16Flash_bwd_params
    32 bytes stack frame, 44 bytes spill stores, 32 bytes spill loads
ptxas info    : Used 255 registers, 696 bytes cmem[0]
ptxas info    : Compiling entry function '_Z44flash_bwd_dq_dk_dv_loop_seqk_parallel_kernelI23Flash_bwd_kernel_traitsILi64ELi128ELi128ELi8ELi4ELi4ELi4ELb0ELb0EN7cutlass10bfloat16_tE19Flash_kernel_traitsILi64ELi128ELi128ELi8ES2_EELb0ELb0ELb0ELb1EEv16Flash_bwd_params' for 'sm_80'
ptxas info    : Function properties for _Z44flash_bwd_dq_dk_dv_loop_seqk_parallel_kernelI23Flash_bwd_kernel_traitsILi64ELi128ELi128ELi8ELi4ELi4ELi4ELb0ELb0EN7cutlass10bfloat16_tE19Flash_kernel_traitsILi64ELi128ELi128ELi8ES2_EELb0ELb0ELb0ELb1EEv16Flash_bwd_params
    32 bytes stack frame, 36 bytes spill stores, 28 bytes spill loads
ptxas info    : Used 255 registers, 696 bytes cmem[0]
ptxas info    : Compiling entry function '_Z44flash_bwd_dq_dk_dv_loop_seqk_parallel_kernelI23Flash_bwd_kernel_traitsILi64ELi128ELi128ELi8ELi4ELi4ELi4ELb0ELb0EN7cutlass10bfloat16_tE19Flash_kernel_traitsILi64ELi128ELi128ELi8ES2_EELb0ELb0ELb1ELb0EEv16Flash_bwd_params' for 'sm_80'
ptxas info    : Function properties for _Z44flash_bwd_dq_dk_dv_loop_seqk_parallel_kernelI23Flash_bwd_kernel_traitsILi64ELi128ELi128ELi8ELi4ELi4ELi4ELb0ELb0EN7cutlass10bfloat16_tE19Flash_kernel_traitsILi64ELi128ELi128ELi8ES2_EELb0ELb0ELb1ELb0EEv16Flash_bwd_params
    40 bytes stack frame, 64 bytes spill stores, 60 bytes spill loads
ptxas info    : Used 255 registers, 696 bytes cmem[0]
ptxas info    : Compiling entry function '_Z44flash_bwd_dq_dk_dv_loop_seqk_parallel_kernelI23Flash_bwd_kernel_traitsILi64ELi128ELi128ELi8ELi4ELi4ELi4ELb0ELb0EN7cutlass10bfloat16_tE19Flash_kernel_traitsILi64ELi128ELi128ELi8ES2_EELb0ELb0ELb1ELb1EEv16Flash_bwd_params' for 'sm_80'
ptxas info    : Function properties for _Z44flash_bwd_dq_dk_dv_loop_seqk_parallel_kernelI23Flash_bwd_kernel_traitsILi64ELi128ELi128ELi8ELi4ELi4ELi4ELb0ELb0EN7cutlass10bfloat16_tE19Flash_kernel_traitsILi64ELi128ELi128ELi8ES2_EELb0ELb0ELb1ELb1EEv16Flash_bwd_params
    40 bytes stack frame, 64 bytes spill stores, 64 bytes spill loads
ptxas info    : Used 255 registers, 696 bytes cmem[0]
ptxas info    : Compiling entry function '_Z44flash_bwd_dq_dk_dv_loop_seqk_parallel_kernelI23Flash_bwd_kernel_traitsILi64ELi128ELi128ELi8ELi4ELi4ELi4ELb0ELb0EN7cutlass10bfloat16_tE19Flash_kernel_traitsILi64ELi128ELi128ELi8ES2_EELb0ELb1ELb0ELb0EEv16Flash_bwd_params' for 'sm_80'
ptxas info    : Function properties for _Z44flash_bwd_dq_dk_dv_loop_seqk_parallel_kernelI23Flash_bwd_kernel_traitsILi64ELi128ELi128ELi8ELi4ELi4ELi4ELb0ELb0EN7cutlass10bfloat16_tE19Flash_kernel_traitsILi64ELi128ELi128ELi8ES2_EELb0ELb1ELb0ELb0EEv16Flash_bwd_params
    40 bytes stack frame, 40 bytes spill stores, 40 bytes spill loads
ptxas info    : Used 255 registers, 696 bytes cmem[0]
ptxas info    : Compiling entry function '_Z44flash_bwd_dq_dk_dv_loop_seqk_parallel_kernelI23Flash_bwd_kernel_traitsILi64ELi128ELi128ELi8ELi4ELi4ELi4ELb0ELb0EN7cutlass10bfloat16_tE19Flash_kernel_traitsILi64ELi128ELi128ELi8ES2_EELb0ELb1ELb0ELb1EEv16Flash_bwd_params' for 'sm_80'
ptxas info    : Function properties for _Z44flash_bwd_dq_dk_dv_loop_seqk_parallel_kernelI23Flash_bwd_kernel_traitsILi64ELi128ELi128ELi8ELi4ELi4ELi4ELb0ELb0EN7cutlass10bfloat16_tE19Flash_kernel_traitsILi64ELi128ELi128ELi8ES2_EELb0ELb1ELb0ELb1EEv16Flash_bwd_params
    48 bytes stack frame, 44 bytes spill stores, 44 bytes spill loads
ptxas info    : Used 255 registers, 696 bytes cmem[0]
ptxas info    : Compiling entry function '_Z44flash_bwd_dq_dk_dv_loop_seqk_parallel_kernelI23Flash_bwd_kernel_traitsILi64ELi128ELi128ELi8ELi4ELi4ELi4ELb0ELb0EN7cutlass10bfloat16_tE19Flash_kernel_traitsILi64ELi128ELi128ELi8ES2_EELb0ELb1ELb1ELb0EEv16Flash_bwd_params' for 'sm_80'
ptxas info    : Function properties for _Z44flash_bwd_dq_dk_dv_loop_seqk_parallel_kernelI23Flash_bwd_kernel_traitsILi64ELi128ELi128ELi8ELi4ELi4ELi4ELb0ELb0EN7cutlass10bfloat16_tE19Flash_kernel_traitsILi64ELi128ELi128ELi8ES2_EELb0ELb1ELb1ELb0EEv16Flash_bwd_params
    32 bytes stack frame, 36 bytes spill stores, 28 bytes spill loads
ptxas info    : Used 255 registers, 696 bytes cmem[0]
ptxas info    : Compiling entry function '_Z44flash_bwd_dq_dk_dv_loop_seqk_parallel_kernelI23Flash_bwd_kernel_traitsILi64ELi128ELi128ELi8ELi4ELi4ELi4ELb0ELb0EN7cutlass10bfloat16_tE19Flash_kernel_traitsILi64ELi128ELi128ELi8ES2_EELb0ELb1ELb1ELb1EEv16Flash_bwd_params' for 'sm_80'
ptxas info    : Function properties for _Z44flash_bwd_dq_dk_dv_loop_seqk_parallel_kernelI23Flash_bwd_kernel_traitsILi64ELi128ELi128ELi8ELi4ELi4ELi4ELb0ELb0EN7cutlass10bfloat16_tE19Flash_kernel_traitsILi64ELi128ELi128ELi8ES2_EELb0ELb1ELb1ELb1EEv16Flash_bwd_params
    0 bytes stack frame, 0 bytes spill stores, 0 bytes spill loads
ptxas info    : Used 255 registers, 696 bytes cmem[0]
ptxas info    : Compiling entry function '_Z27flash_bwd_convert_dq_kernelI23Flash_bwd_kernel_traitsILi64ELi64ELi128ELi8ELi2ELi4ELi4ELb1ELb0EN7cutlass10bfloat16_tE19Flash_kernel_traitsILi64ELi64ELi128ELi8ES2_EEEv16Flash_bwd_params' for 'sm_80'
ptxas info    : Function properties for _Z27flash_bwd_convert_dq_kernelI23Flash_bwd_kernel_traitsILi64ELi64ELi128ELi8ELi2ELi4ELi4ELb1ELb0EN7cutlass10bfloat16_tE19Flash_kernel_traitsILi64ELi64ELi128ELi8ES2_EEEv16Flash_bwd_params
    0 bytes stack frame, 0 bytes spill stores, 0 bytes spill loads
ptxas info    : Used 32 registers, 696 bytes cmem[0]
ptxas info    : Compiling entry function '_Z44flash_bwd_dq_dk_dv_loop_seqk_parallel_kernelI23Flash_bwd_kernel_traitsILi64ELi64ELi128ELi8ELi2ELi4ELi4ELb1ELb0EN7cutlass10bfloat16_tE19Flash_kernel_traitsILi64ELi64ELi128ELi8ES2_EELb1ELb0ELb0ELb0EEv16Flash_bwd_params' for 'sm_80'
ptxas info    : Function properties for _Z44flash_bwd_dq_dk_dv_loop_seqk_parallel_kernelI23Flash_bwd_kernel_traitsILi64ELi64ELi128ELi8ELi2ELi4ELi4ELb1ELb0EN7cutlass10bfloat16_tE19Flash_kernel_traitsILi64ELi64ELi128ELi8ES2_EELb1ELb0ELb0ELb0EEv16Flash_bwd_params
    0 bytes stack frame, 0 bytes spill stores, 0 bytes spill loads
ptxas info    : Used 237 registers, 696 bytes cmem[0]
ptxas info    : Compiling entry function '_Z44flash_bwd_dq_dk_dv_loop_seqk_parallel_kernelI23Flash_bwd_kernel_traitsILi64ELi64ELi128ELi8ELi2ELi4ELi4ELb1ELb0EN7cutlass10bfloat16_tE19Flash_kernel_traitsILi64ELi64ELi128ELi8ES2_EELb1ELb0ELb0ELb1EEv16Flash_bwd_params' for 'sm_80'
ptxas info    : Function properties for _Z44flash_bwd_dq_dk_dv_loop_seqk_parallel_kernelI23Flash_bwd_kernel_traitsILi64ELi64ELi128ELi8ELi2ELi4ELi4ELb1ELb0EN7cutlass10bfloat16_tE19Flash_kernel_traitsILi64ELi64ELi128ELi8ES2_EELb1ELb0ELb0ELb1EEv16Flash_bwd_params
    0 bytes stack frame, 0 bytes spill stores, 0 bytes spill loads
ptxas info    : Used 237 registers, 696 bytes cmem[0]
ptxas info    : Compiling entry function '_Z44flash_bwd_dq_dk_dv_loop_seqk_parallel_kernelI23Flash_bwd_kernel_traitsILi64ELi64ELi128ELi8ELi2ELi4ELi4ELb1ELb0EN7cutlass10bfloat16_tE19Flash_kernel_traitsILi64ELi64ELi128ELi8ES2_EELb1ELb0ELb1ELb0EEv16Flash_bwd_params' for 'sm_80'
ptxas info    : Function properties for _Z44flash_bwd_dq_dk_dv_loop_seqk_parallel_kernelI23Flash_bwd_kernel_traitsILi64ELi64ELi128ELi8ELi2ELi4ELi4ELb1ELb0EN7cutlass10bfloat16_tE19Flash_kernel_traitsILi64ELi64ELi128ELi8ES2_EELb1ELb0ELb1ELb0EEv16Flash_bwd_params
    0 bytes stack frame, 0 bytes spill stores, 0 bytes spill loads
ptxas info    : Used 222 registers, 696 bytes cmem[0]
ptxas info    : Compiling entry function '_Z44flash_bwd_dq_dk_dv_loop_seqk_parallel_kernelI23Flash_bwd_kernel_traitsILi64ELi64ELi128ELi8ELi2ELi4ELi4ELb1ELb0EN7cutlass10bfloat16_tE19Flash_kernel_traitsILi64ELi64ELi128ELi8ES2_EELb1ELb0ELb1ELb1EEv16Flash_bwd_params' for 'sm_80'
ptxas info    : Function properties for _Z44flash_bwd_dq_dk_dv_loop_seqk_parallel_kernelI23Flash_bwd_kernel_traitsILi64ELi64ELi128ELi8ELi2ELi4ELi4ELb1ELb0EN7cutlass10bfloat16_tE19Flash_kernel_traitsILi64ELi64ELi128ELi8ES2_EELb1ELb0ELb1ELb1EEv16Flash_bwd_params
    0 bytes stack frame, 0 bytes spill stores, 0 bytes spill loads
ptxas info    : Used 226 registers, 696 bytes cmem[0]
ptxas info    : Compiling entry function '_Z44flash_bwd_dq_dk_dv_loop_seqk_parallel_kernelI23Flash_bwd_kernel_traitsILi64ELi64ELi128ELi8ELi2ELi4ELi4ELb1ELb0EN7cutlass10bfloat16_tE19Flash_kernel_traitsILi64ELi64ELi128ELi8ES2_EELb1ELb1ELb0ELb0EEv16Flash_bwd_params' for 'sm_80'
ptxas info    : Function properties for _Z44flash_bwd_dq_dk_dv_loop_seqk_parallel_kernelI23Flash_bwd_kernel_traitsILi64ELi64ELi128ELi8ELi2ELi4ELi4ELb1ELb0EN7cutlass10bfloat16_tE19Flash_kernel_traitsILi64ELi64ELi128ELi8ES2_EELb1ELb1ELb0ELb0EEv16Flash_bwd_params
    0 bytes stack frame, 0 bytes spill stores, 0 bytes spill loads
ptxas info    : Used 236 registers, 696 bytes cmem[0]
ptxas info    : Compiling entry function '_Z44flash_bwd_dq_dk_dv_loop_seqk_parallel_kernelI23Flash_bwd_kernel_traitsILi64ELi64ELi128ELi8ELi2ELi4ELi4ELb1ELb0EN7cutlass10bfloat16_tE19Flash_kernel_traitsILi64ELi64ELi128ELi8ES2_EELb1ELb1ELb0ELb1EEv16Flash_bwd_params' for 'sm_80'
ptxas info    : Function properties for _Z44flash_bwd_dq_dk_dv_loop_seqk_parallel_kernelI23Flash_bwd_kernel_traitsILi64ELi64ELi128ELi8ELi2ELi4ELi4ELb1ELb0EN7cutlass10bfloat16_tE19Flash_kernel_traitsILi64ELi64ELi128ELi8ES2_EELb1ELb1ELb0ELb1EEv16Flash_bwd_params
    0 bytes stack frame, 0 bytes spill stores, 0 bytes spill loads
ptxas info    : Used 238 registers, 696 bytes cmem[0]
ptxas info    : Compiling entry function '_Z44flash_bwd_dq_dk_dv_loop_seqk_parallel_kernelI23Flash_bwd_kernel_traitsILi64ELi64ELi128ELi8ELi2ELi4ELi4ELb1ELb0EN7cutlass10bfloat16_tE19Flash_kernel_traitsILi64ELi64ELi128ELi8ES2_EELb1ELb1ELb1ELb0EEv16Flash_bwd_params' for 'sm_80'
ptxas info    : Function properties for _Z44flash_bwd_dq_dk_dv_loop_seqk_parallel_kernelI23Flash_bwd_kernel_traitsILi64ELi64ELi128ELi8ELi2ELi4ELi4ELb1ELb0EN7cutlass10bfloat16_tE19Flash_kernel_traitsILi64ELi64ELi128ELi8ES2_EELb1ELb1ELb1ELb0EEv16Flash_bwd_params
    0 bytes stack frame, 0 bytes spill stores, 0 bytes spill loads
ptxas info    : Used 234 registers, 696 bytes cmem[0]
ptxas info    : Compiling entry function '_Z44flash_bwd_dq_dk_dv_loop_seqk_parallel_kernelI23Flash_bwd_kernel_traitsILi64ELi64ELi128ELi8ELi2ELi4ELi4ELb1ELb0EN7cutlass10bfloat16_tE19Flash_kernel_traitsILi64ELi64ELi128ELi8ES2_EELb1ELb1ELb1ELb1EEv16Flash_bwd_params' for 'sm_80'
ptxas info    : Function properties for _Z44flash_bwd_dq_dk_dv_loop_seqk_parallel_kernelI23Flash_bwd_kernel_traitsILi64ELi64ELi128ELi8ELi2ELi4ELi4ELb1ELb0EN7cutlass10bfloat16_tE19Flash_kernel_traitsILi64ELi64ELi128ELi8ES2_EELb1ELb1ELb1ELb1EEv16Flash_bwd_params
    0 bytes stack frame, 0 bytes spill stores, 0 bytes spill loads
ptxas info    : Used 239 registers, 696 bytes cmem[0]
ptxas info    : Compiling entry function '_Z25flash_bwd_dot_do_o_kernelILb1E23Flash_bwd_kernel_traitsILi64ELi64ELi128ELi8ELi2ELi4ELi4ELb1ELb0EN7cutlass10bfloat16_tE19Flash_kernel_traitsILi64ELi64ELi128ELi8ES2_EEEv16Flash_bwd_params' for 'sm_80'
ptxas info    : Function properties for _Z25flash_bwd_dot_do_o_kernelILb1E23Flash_bwd_kernel_traitsILi64ELi64ELi128ELi8ELi2ELi4ELi4ELb1ELb0EN7cutlass10bfloat16_tE19Flash_kernel_traitsILi64ELi64ELi128ELi8ES2_EEEv16Flash_bwd_params
    0 bytes stack frame, 0 bytes spill stores, 0 bytes spill loads
ptxas info    : Used 30 registers, 696 bytes cmem[0]
ptxas info    : Compiling entry function '_Z27flash_bwd_convert_dq_kernelI23Flash_bwd_kernel_traitsILi64ELi128ELi128ELi8ELi4ELi4ELi4ELb0ELb0EN7cutlass10bfloat16_tE19Flash_kernel_traitsILi64ELi128ELi128ELi8ES2_EEEv16Flash_bwd_params' for 'sm_80'
ptxas info    : Function properties for _Z27flash_bwd_convert_dq_kernelI23Flash_bwd_kernel_traitsILi64ELi128ELi128ELi8ELi4ELi4ELi4ELb0ELb0EN7cutlass10bfloat16_tE19Flash_kernel_traitsILi64ELi128ELi128ELi8ES2_EEEv16Flash_bwd_params
    0 bytes stack frame, 0 bytes spill stores, 0 bytes spill loads
ptxas info    : Used 32 registers, 696 bytes cmem[0]
ptxas info    : Compiling entry function '_Z44flash_bwd_dq_dk_dv_loop_seqk_parallel_kernelI23Flash_bwd_kernel_traitsILi64ELi128ELi128ELi8ELi4ELi4ELi4ELb0ELb0EN7cutlass10bfloat16_tE19Flash_kernel_traitsILi64ELi128ELi128ELi8ES2_EELb1ELb0ELb0ELb0EEv16Flash_bwd_params' for 'sm_80'
ptxas info    : Function properties for _Z44flash_bwd_dq_dk_dv_loop_seqk_parallel_kernelI23Flash_bwd_kernel_traitsILi64ELi128ELi128ELi8ELi4ELi4ELi4ELb0ELb0EN7cutlass10bfloat16_tE19Flash_kernel_traitsILi64ELi128ELi128ELi8ES2_EELb1ELb0ELb0ELb0EEv16Flash_bwd_params
    48 bytes stack frame, 48 bytes spill stores, 44 bytes spill loads
ptxas info    : Used 255 registers, 696 bytes cmem[0]
ptxas info    : Compiling entry function '_Z44flash_bwd_dq_dk_dv_loop_seqk_parallel_kernelI23Flash_bwd_kernel_traitsILi64ELi128ELi128ELi8ELi4ELi4ELi4ELb0ELb0EN7cutlass10bfloat16_tE19Flash_kernel_traitsILi64ELi128ELi128ELi8ES2_EELb1ELb0ELb0ELb1EEv16Flash_bwd_params' for 'sm_80'
ptxas info    : Function properties for _Z44flash_bwd_dq_dk_dv_loop_seqk_parallel_kernelI23Flash_bwd_kernel_traitsILi64ELi128ELi128ELi8ELi4ELi4ELi4ELb0ELb0EN7cutlass10bfloat16_tE19Flash_kernel_traitsILi64ELi128ELi128ELi8ES2_EELb1ELb0ELb0ELb1EEv16Flash_bwd_params
    56 bytes stack frame, 72 bytes spill stores, 72 bytes spill loads
ptxas info    : Used 255 registers, 696 bytes cmem[0]
ptxas info    : Compiling entry function '_Z44flash_bwd_dq_dk_dv_loop_seqk_parallel_kernelI23Flash_bwd_kernel_traitsILi64ELi128ELi128ELi8ELi4ELi4ELi4ELb0ELb0EN7cutlass10bfloat16_tE19Flash_kernel_traitsILi64ELi128ELi128ELi8ES2_EELb1ELb0ELb1ELb0EEv16Flash_bwd_params' for 'sm_80'
ptxas info    : Function properties for _Z44flash_bwd_dq_dk_dv_loop_seqk_parallel_kernelI23Flash_bwd_kernel_traitsILi64ELi128ELi128ELi8ELi4ELi4ELi4ELb0ELb0EN7cutlass10bfloat16_tE19Flash_kernel_traitsILi64ELi128ELi128ELi8ES2_EELb1ELb0ELb1ELb0EEv16Flash_bwd_params
    56 bytes stack frame, 68 bytes spill stores, 72 bytes spill loads
ptxas info    : Used 255 registers, 696 bytes cmem[0]
ptxas info    : Compiling entry function '_Z44flash_bwd_dq_dk_dv_loop_seqk_parallel_kernelI23Flash_bwd_kernel_traitsILi64ELi128ELi128ELi8ELi4ELi4ELi4ELb0ELb0EN7cutlass10bfloat16_tE19Flash_kernel_traitsILi64ELi128ELi128ELi8ES2_EELb1ELb0ELb1ELb1EEv16Flash_bwd_params' for 'sm_80'
ptxas info    : Function properties for _Z44flash_bwd_dq_dk_dv_loop_seqk_parallel_kernelI23Flash_bwd_kernel_traitsILi64ELi128ELi128ELi8ELi4ELi4ELi4ELb0ELb0EN7cutlass10bfloat16_tE19Flash_kernel_traitsILi64ELi128ELi128ELi8ES2_EELb1ELb0ELb1ELb1EEv16Flash_bwd_params
    56 bytes stack frame, 72 bytes spill stores, 80 bytes spill loads
ptxas info    : Used 255 registers, 696 bytes cmem[0]
ptxas info    : Compiling entry function '_Z44flash_bwd_dq_dk_dv_loop_seqk_parallel_kernelI23Flash_bwd_kernel_traitsILi64ELi128ELi128ELi8ELi4ELi4ELi4ELb0ELb0EN7cutlass10bfloat16_tE19Flash_kernel_traitsILi64ELi128ELi128ELi8ES2_EELb1ELb1ELb0ELb0EEv16Flash_bwd_params' for 'sm_80'
ptxas info    : Function properties for _Z44flash_bwd_dq_dk_dv_loop_seqk_parallel_kernelI23Flash_bwd_kernel_traitsILi64ELi128ELi128ELi8ELi4ELi4ELi4ELb0ELb0EN7cutlass10bfloat16_tE19Flash_kernel_traitsILi64ELi128ELi128ELi8ES2_EELb1ELb1ELb0ELb0EEv16Flash_bwd_params
    24 bytes stack frame, 20 bytes spill stores, 20 bytes spill loads
ptxas info    : Used 255 registers, 696 bytes cmem[0]
ptxas info    : Compiling entry function '_Z44flash_bwd_dq_dk_dv_loop_seqk_parallel_kernelI23Flash_bwd_kernel_traitsILi64ELi128ELi128ELi8ELi4ELi4ELi4ELb0ELb0EN7cutlass10bfloat16_tE19Flash_kernel_traitsILi64ELi128ELi128ELi8ES2_EELb1ELb1ELb0ELb1EEv16Flash_bwd_params' for 'sm_80'
ptxas info    : Function properties for _Z44flash_bwd_dq_dk_dv_loop_seqk_parallel_kernelI23Flash_bwd_kernel_traitsILi64ELi128ELi128ELi8ELi4ELi4ELi4ELb0ELb0EN7cutlass10bfloat16_tE19Flash_kernel_traitsILi64ELi128ELi128ELi8ES2_EELb1ELb1ELb0ELb1EEv16Flash_bwd_params
    32 bytes stack frame, 28 bytes spill stores, 28 bytes spill loads
ptxas info    : Used 255 registers, 696 bytes cmem[0]
ptxas info    : Compiling entry function '_Z44flash_bwd_dq_dk_dv_loop_seqk_parallel_kernelI23Flash_bwd_kernel_traitsILi64ELi128ELi128ELi8ELi4ELi4ELi4ELb0ELb0EN7cutlass10bfloat16_tE19Flash_kernel_traitsILi64ELi128ELi128ELi8ES2_EELb1ELb1ELb1ELb0EEv16Flash_bwd_params' for 'sm_80'
ptxas info    : Function properties for _Z44flash_bwd_dq_dk_dv_loop_seqk_parallel_kernelI23Flash_bwd_kernel_traitsILi64ELi128ELi128ELi8ELi4ELi4ELi4ELb0ELb0EN7cutlass10bfloat16_tE19Flash_kernel_traitsILi64ELi128ELi128ELi8ES2_EELb1ELb1ELb1ELb0EEv16Flash_bwd_params
    48 bytes stack frame, 60 bytes spill stores, 56 bytes spill loads
ptxas info    : Used 255 registers, 696 bytes cmem[0]
ptxas info    : Compiling entry function '_Z44flash_bwd_dq_dk_dv_loop_seqk_parallel_kernelI23Flash_bwd_kernel_traitsILi64ELi128ELi128ELi8ELi4ELi4ELi4ELb0ELb0EN7cutlass10bfloat16_tE19Flash_kernel_traitsILi64ELi128ELi128ELi8ES2_EELb1ELb1ELb1ELb1EEv16Flash_bwd_params' for 'sm_80'
ptxas info    : Function properties for _Z44flash_bwd_dq_dk_dv_loop_seqk_parallel_kernelI23Flash_bwd_kernel_traitsILi64ELi128ELi128ELi8ELi4ELi4ELi4ELb0ELb0EN7cutlass10bfloat16_tE19Flash_kernel_traitsILi64ELi128ELi128ELi8ES2_EELb1ELb1ELb1ELb1EEv16Flash_bwd_params
    16 bytes stack frame, 16 bytes spill stores, 16 bytes spill loads
ptxas info    : Used 255 registers, 696 bytes cmem[0]
ptxas info    : Compiling entry function '_Z25flash_bwd_dot_do_o_kernelILb1E23Flash_bwd_kernel_traitsILi64ELi128ELi128ELi8ELi4ELi4ELi4ELb0ELb0EN7cutlass10bfloat16_tE19Flash_kernel_traitsILi64ELi128ELi128ELi8ES2_EEEv16Flash_bwd_params' for 'sm_80'
ptxas info    : Function properties for _Z25flash_bwd_dot_do_o_kernelILb1E23Flash_bwd_kernel_traitsILi64ELi128ELi128ELi8ELi4ELi4ELi4ELb0ELb0EN7cutlass10bfloat16_tE19Flash_kernel_traitsILi64ELi128ELi128ELi8ES2_EEEv16Flash_bwd_params
    0 bytes stack frame, 0 bytes spill stores, 0 bytes spill loads
ptxas info    : Used 45 registers, 696 bytes cmem[0]
/anaconda/envs/flashattention/lib/python3.9/site-packages/torch/include/c10/core/SymInt.h(84): warning: integer conversion resulted in a change of sign

[24/36] /usr/local/cuda-11.4/bin/nvcc  -I/home/v-feiychen/flash-attention/csrc/flash_attn -I/home/v-feiychen/flash-attention/csrc/flash_attn/src -I/home/v-feiychen/flash-attention/csrc/cutlass/include -I/anaconda/envs/flashattention/lib/python3.9/site-packages/torch/include -I/anaconda/envs/flashattention/lib/python3.9/site-packages/torch/include/torch/csrc/api/include -I/anaconda/envs/flashattention/lib/python3.9/site-packages/torch/include/TH -I/anaconda/envs/flashattention/lib/python3.9/site-packages/torch/include/THC -I/usr/local/cuda-11.4/include -I/anaconda/envs/flashattention/include/python3.9 -c -c /home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_bwd_hdim64_fp16_sm80.cu -o /home/v-feiychen/flash-attention/build/temp.linux-x86_64-cpython-39/csrc/flash_attn/src/flash_bwd_hdim64_fp16_sm80.o -D__CUDA_NO_HALF_OPERATORS__ -D__CUDA_NO_HALF_CONVERSIONS__ -D__CUDA_NO_BFLOAT16_CONVERSIONS__ -D__CUDA_NO_HALF2_OPERATORS__ --expt-relaxed-constexpr --compiler-options ''"'"'-fPIC'"'"'' -O3 -std=c++17 -U__CUDA_NO_HALF_OPERATORS__ -U__CUDA_NO_HALF_CONVERSIONS__ -U__CUDA_NO_HALF2_OPERATORS__ -U__CUDA_NO_BFLOAT16_CONVERSIONS__ --expt-relaxed-constexpr --expt-extended-lambda --use_fast_math --ptxas-options=-v -lineinfo -gencode arch=compute_80,code=sm_80 --threads 4 -DTORCH_API_INCLUDE_EXTENSION_H '-DPYBIND11_COMPILER_TYPE="_gcc"' '-DPYBIND11_STDLIB="_libstdcpp"' '-DPYBIND11_BUILD_ABI="_cxxabi1011"' -DTORCH_EXTENSION_NAME=flash_attn_2_cuda -D_GLIBCXX_USE_CXX11_ABI=0
/anaconda/envs/flashattention/lib/python3.9/site-packages/torch/include/c10/core/SymInt.h(84): warning: integer conversion resulted in a change of sign

ptxas info    : 2 bytes gmem
ptxas info    : Compiling entry function '_Z44flash_bwd_dq_dk_dv_loop_seqk_parallel_kernelI23Flash_bwd_kernel_traitsILi64ELi64ELi128ELi8ELi2ELi4ELi4ELb1ELb0EN7cutlass6half_tE19Flash_kernel_traitsILi64ELi64ELi128ELi8ES2_EELb0ELb0ELb0ELb0EEv16Flash_bwd_params' for 'sm_80'
ptxas info    : Function properties for _Z44flash_bwd_dq_dk_dv_loop_seqk_parallel_kernelI23Flash_bwd_kernel_traitsILi64ELi64ELi128ELi8ELi2ELi4ELi4ELb1ELb0EN7cutlass6half_tE19Flash_kernel_traitsILi64ELi64ELi128ELi8ES2_EELb0ELb0ELb0ELb0EEv16Flash_bwd_params
    0 bytes stack frame, 0 bytes spill stores, 0 bytes spill loads
ptxas info    : Used 230 registers, 696 bytes cmem[0]
ptxas info    : Compiling entry function '_Z44flash_bwd_dq_dk_dv_loop_seqk_parallel_kernelI23Flash_bwd_kernel_traitsILi64ELi64ELi128ELi8ELi2ELi4ELi4ELb1ELb0EN7cutlass6half_tE19Flash_kernel_traitsILi64ELi64ELi128ELi8ES2_EELb0ELb0ELb0ELb1EEv16Flash_bwd_params' for 'sm_80'
ptxas info    : Function properties for _Z44flash_bwd_dq_dk_dv_loop_seqk_parallel_kernelI23Flash_bwd_kernel_traitsILi64ELi64ELi128ELi8ELi2ELi4ELi4ELb1ELb0EN7cutlass6half_tE19Flash_kernel_traitsILi64ELi64ELi128ELi8ES2_EELb0ELb0ELb0ELb1EEv16Flash_bwd_params
    0 bytes stack frame, 0 bytes spill stores, 0 bytes spill loads
ptxas info    : Used 232 registers, 696 bytes cmem[0]
ptxas info    : Compiling entry function '_Z44flash_bwd_dq_dk_dv_loop_seqk_parallel_kernelI23Flash_bwd_kernel_traitsILi64ELi64ELi128ELi8ELi2ELi4ELi4ELb1ELb0EN7cutlass6half_tE19Flash_kernel_traitsILi64ELi64ELi128ELi8ES2_EELb0ELb0ELb1ELb0EEv16Flash_bwd_params' for 'sm_80'
ptxas info    : Function properties for _Z44flash_bwd_dq_dk_dv_loop_seqk_parallel_kernelI23Flash_bwd_kernel_traitsILi64ELi64ELi128ELi8ELi2ELi4ELi4ELb1ELb0EN7cutlass6half_tE19Flash_kernel_traitsILi64ELi64ELi128ELi8ES2_EELb0ELb0ELb1ELb0EEv16Flash_bwd_params
    0 bytes stack frame, 0 bytes spill stores, 0 bytes spill loads
ptxas info    : Used 220 registers, 696 bytes cmem[0]
ptxas info    : Compiling entry function '_Z44flash_bwd_dq_dk_dv_loop_seqk_parallel_kernelI23Flash_bwd_kernel_traitsILi64ELi64ELi128ELi8ELi2ELi4ELi4ELb1ELb0EN7cutlass6half_tE19Flash_kernel_traitsILi64ELi64ELi128ELi8ES2_EELb0ELb0ELb1ELb1EEv16Flash_bwd_params' for 'sm_80'
ptxas info    : Function properties for _Z44flash_bwd_dq_dk_dv_loop_seqk_parallel_kernelI23Flash_bwd_kernel_traitsILi64ELi64ELi128ELi8ELi2ELi4ELi4ELb1ELb0EN7cutlass6half_tE19Flash_kernel_traitsILi64ELi64ELi128ELi8ES2_EELb0ELb0ELb1ELb1EEv16Flash_bwd_params
    0 bytes stack frame, 0 bytes spill stores, 0 bytes spill loads
ptxas info    : Used 221 registers, 696 bytes cmem[0]
ptxas info    : Compiling entry function '_Z44flash_bwd_dq_dk_dv_loop_seqk_parallel_kernelI23Flash_bwd_kernel_traitsILi64ELi64ELi128ELi8ELi2ELi4ELi4ELb1ELb0EN7cutlass6half_tE19Flash_kernel_traitsILi64ELi64ELi128ELi8ES2_EELb0ELb1ELb0ELb0EEv16Flash_bwd_params' for 'sm_80'
ptxas info    : Function properties for _Z44flash_bwd_dq_dk_dv_loop_seqk_parallel_kernelI23Flash_bwd_kernel_traitsILi64ELi64ELi128ELi8ELi2ELi4ELi4ELb1ELb0EN7cutlass6half_tE19Flash_kernel_traitsILi64ELi64ELi128ELi8ES2_EELb0ELb1ELb0ELb0EEv16Flash_bwd_params
    0 bytes stack frame, 0 bytes spill stores, 0 bytes spill loads
ptxas info    : Used 231 registers, 696 bytes cmem[0]
ptxas info    : Compiling entry function '_Z44flash_bwd_dq_dk_dv_loop_seqk_parallel_kernelI23Flash_bwd_kernel_traitsILi64ELi64ELi128ELi8ELi2ELi4ELi4ELb1ELb0EN7cutlass6half_tE19Flash_kernel_traitsILi64ELi64ELi128ELi8ES2_EELb0ELb1ELb0ELb1EEv16Flash_bwd_params' for 'sm_80'
ptxas info    : Function properties for _Z44flash_bwd_dq_dk_dv_loop_seqk_parallel_kernelI23Flash_bwd_kernel_traitsILi64ELi64ELi128ELi8ELi2ELi4ELi4ELb1ELb0EN7cutlass6half_tE19Flash_kernel_traitsILi64ELi64ELi128ELi8ES2_EELb0ELb1ELb0ELb1EEv16Flash_bwd_params
    0 bytes stack frame, 0 bytes spill stores, 0 bytes spill loads
ptxas info    : Used 229 registers, 696 bytes cmem[0]
ptxas info    : Compiling entry function '_Z44flash_bwd_dq_dk_dv_loop_seqk_parallel_kernelI23Flash_bwd_kernel_traitsILi64ELi64ELi128ELi8ELi2ELi4ELi4ELb1ELb0EN7cutlass6half_tE19Flash_kernel_traitsILi64ELi64ELi128ELi8ES2_EELb0ELb1ELb1ELb0EEv16Flash_bwd_params' for 'sm_80'
ptxas info    : Function properties for _Z44flash_bwd_dq_dk_dv_loop_seqk_parallel_kernelI23Flash_bwd_kernel_traitsILi64ELi64ELi128ELi8ELi2ELi4ELi4ELb1ELb0EN7cutlass6half_tE19Flash_kernel_traitsILi64ELi64ELi128ELi8ES2_EELb0ELb1ELb1ELb0EEv16Flash_bwd_params
    0 bytes stack frame, 0 bytes spill stores, 0 bytes spill loads
ptxas info    : Used 225 registers, 696 bytes cmem[0]
ptxas info    : Compiling entry function '_Z44flash_bwd_dq_dk_dv_loop_seqk_parallel_kernelI23Flash_bwd_kernel_traitsILi64ELi64ELi128ELi8ELi2ELi4ELi4ELb1ELb0EN7cutlass6half_tE19Flash_kernel_traitsILi64ELi64ELi128ELi8ES2_EELb0ELb1ELb1ELb1EEv16Flash_bwd_params' for 'sm_80'
ptxas info    : Function properties for _Z44flash_bwd_dq_dk_dv_loop_seqk_parallel_kernelI23Flash_bwd_kernel_traitsILi64ELi64ELi128ELi8ELi2ELi4ELi4ELb1ELb0EN7cutlass6half_tE19Flash_kernel_traitsILi64ELi64ELi128ELi8ES2_EELb0ELb1ELb1ELb1EEv16Flash_bwd_params
    0 bytes stack frame, 0 bytes spill stores, 0 bytes spill loads
ptxas info    : Used 230 registers, 696 bytes cmem[0]
ptxas info    : Compiling entry function '_Z44flash_bwd_dq_dk_dv_loop_seqk_parallel_kernelI23Flash_bwd_kernel_traitsILi64ELi128ELi128ELi8ELi4ELi4ELi4ELb0ELb0EN7cutlass6half_tE19Flash_kernel_traitsILi64ELi128ELi128ELi8ES2_EELb0ELb0ELb0ELb0EEv16Flash_bwd_params' for 'sm_80'
ptxas info    : Function properties for _Z44flash_bwd_dq_dk_dv_loop_seqk_parallel_kernelI23Flash_bwd_kernel_traitsILi64ELi128ELi128ELi8ELi4ELi4ELi4ELb0ELb0EN7cutlass6half_tE19Flash_kernel_traitsILi64ELi128ELi128ELi8ES2_EELb0ELb0ELb0ELb0EEv16Flash_bwd_params
    32 bytes stack frame, 44 bytes spill stores, 32 bytes spill loads
ptxas info    : Used 255 registers, 696 bytes cmem[0]
ptxas info    : Compiling entry function '_Z44flash_bwd_dq_dk_dv_loop_seqk_parallel_kernelI23Flash_bwd_kernel_traitsILi64ELi128ELi128ELi8ELi4ELi4ELi4ELb0ELb0EN7cutlass6half_tE19Flash_kernel_traitsILi64ELi128ELi128ELi8ES2_EELb0ELb0ELb0ELb1EEv16Flash_bwd_params' for 'sm_80'
ptxas info    : Function properties for _Z44flash_bwd_dq_dk_dv_loop_seqk_parallel_kernelI23Flash_bwd_kernel_traitsILi64ELi128ELi128ELi8ELi4ELi4ELi4ELb0ELb0EN7cutlass6half_tE19Flash_kernel_traitsILi64ELi128ELi128ELi8ES2_EELb0ELb0ELb0ELb1EEv16Flash_bwd_params
    32 bytes stack frame, 36 bytes spill stores, 28 bytes spill loads
ptxas info    : Used 255 registers, 696 bytes cmem[0]
ptxas info    : Compiling entry function '_Z44flash_bwd_dq_dk_dv_loop_seqk_parallel_kernelI23Flash_bwd_kernel_traitsILi64ELi128ELi128ELi8ELi4ELi4ELi4ELb0ELb0EN7cutlass6half_tE19Flash_kernel_traitsILi64ELi128ELi128ELi8ES2_EELb0ELb0ELb1ELb0EEv16Flash_bwd_params' for 'sm_80'
ptxas info    : Function properties for _Z44flash_bwd_dq_dk_dv_loop_seqk_parallel_kernelI23Flash_bwd_kernel_traitsILi64ELi128ELi128ELi8ELi4ELi4ELi4ELb0ELb0EN7cutlass6half_tE19Flash_kernel_traitsILi64ELi128ELi128ELi8ES2_EELb0ELb0ELb1ELb0EEv16Flash_bwd_params
    40 bytes stack frame, 64 bytes spill stores, 60 bytes spill loads
ptxas info    : Used 255 registers, 696 bytes cmem[0]
ptxas info    : Compiling entry function '_Z44flash_bwd_dq_dk_dv_loop_seqk_parallel_kernelI23Flash_bwd_kernel_traitsILi64ELi128ELi128ELi8ELi4ELi4ELi4ELb0ELb0EN7cutlass6half_tE19Flash_kernel_traitsILi64ELi128ELi128ELi8ES2_EELb0ELb0ELb1ELb1EEv16Flash_bwd_params' for 'sm_80'
ptxas info    : Function properties for _Z44flash_bwd_dq_dk_dv_loop_seqk_parallel_kernelI23Flash_bwd_kernel_traitsILi64ELi128ELi128ELi8ELi4ELi4ELi4ELb0ELb0EN7cutlass6half_tE19Flash_kernel_traitsILi64ELi128ELi128ELi8ES2_EELb0ELb0ELb1ELb1EEv16Flash_bwd_params
    40 bytes stack frame, 64 bytes spill stores, 64 bytes spill loads
ptxas info    : Used 255 registers, 696 bytes cmem[0]
ptxas info    : Compiling entry function '_Z44flash_bwd_dq_dk_dv_loop_seqk_parallel_kernelI23Flash_bwd_kernel_traitsILi64ELi128ELi128ELi8ELi4ELi4ELi4ELb0ELb0EN7cutlass6half_tE19Flash_kernel_traitsILi64ELi128ELi128ELi8ES2_EELb0ELb1ELb0ELb0EEv16Flash_bwd_params' for 'sm_80'
ptxas info    : Function properties for _Z44flash_bwd_dq_dk_dv_loop_seqk_parallel_kernelI23Flash_bwd_kernel_traitsILi64ELi128ELi128ELi8ELi4ELi4ELi4ELb0ELb0EN7cutlass6half_tE19Flash_kernel_traitsILi64ELi128ELi128ELi8ES2_EELb0ELb1ELb0ELb0EEv16Flash_bwd_params
    40 bytes stack frame, 40 bytes spill stores, 40 bytes spill loads
ptxas info    : Used 255 registers, 696 bytes cmem[0]
ptxas info    : Compiling entry function '_Z44flash_bwd_dq_dk_dv_loop_seqk_parallel_kernelI23Flash_bwd_kernel_traitsILi64ELi128ELi128ELi8ELi4ELi4ELi4ELb0ELb0EN7cutlass6half_tE19Flash_kernel_traitsILi64ELi128ELi128ELi8ES2_EELb0ELb1ELb0ELb1EEv16Flash_bwd_params' for 'sm_80'
ptxas info    : Function properties for _Z44flash_bwd_dq_dk_dv_loop_seqk_parallel_kernelI23Flash_bwd_kernel_traitsILi64ELi128ELi128ELi8ELi4ELi4ELi4ELb0ELb0EN7cutlass6half_tE19Flash_kernel_traitsILi64ELi128ELi128ELi8ES2_EELb0ELb1ELb0ELb1EEv16Flash_bwd_params
    48 bytes stack frame, 44 bytes spill stores, 44 bytes spill loads
ptxas info    : Used 255 registers, 696 bytes cmem[0]
ptxas info    : Compiling entry function '_Z44flash_bwd_dq_dk_dv_loop_seqk_parallel_kernelI23Flash_bwd_kernel_traitsILi64ELi128ELi128ELi8ELi4ELi4ELi4ELb0ELb0EN7cutlass6half_tE19Flash_kernel_traitsILi64ELi128ELi128ELi8ES2_EELb0ELb1ELb1ELb0EEv16Flash_bwd_params' for 'sm_80'
ptxas info    : Function properties for _Z44flash_bwd_dq_dk_dv_loop_seqk_parallel_kernelI23Flash_bwd_kernel_traitsILi64ELi128ELi128ELi8ELi4ELi4ELi4ELb0ELb0EN7cutlass6half_tE19Flash_kernel_traitsILi64ELi128ELi128ELi8ES2_EELb0ELb1ELb1ELb0EEv16Flash_bwd_params
    32 bytes stack frame, 36 bytes spill stores, 28 bytes spill loads
ptxas info    : Used 255 registers, 696 bytes cmem[0]
ptxas info    : Compiling entry function '_Z44flash_bwd_dq_dk_dv_loop_seqk_parallel_kernelI23Flash_bwd_kernel_traitsILi64ELi128ELi128ELi8ELi4ELi4ELi4ELb0ELb0EN7cutlass6half_tE19Flash_kernel_traitsILi64ELi128ELi128ELi8ES2_EELb0ELb1ELb1ELb1EEv16Flash_bwd_params' for 'sm_80'
ptxas info    : Function properties for _Z44flash_bwd_dq_dk_dv_loop_seqk_parallel_kernelI23Flash_bwd_kernel_traitsILi64ELi128ELi128ELi8ELi4ELi4ELi4ELb0ELb0EN7cutlass6half_tE19Flash_kernel_traitsILi64ELi128ELi128ELi8ES2_EELb0ELb1ELb1ELb1EEv16Flash_bwd_params
    0 bytes stack frame, 0 bytes spill stores, 0 bytes spill loads
ptxas info    : Used 255 registers, 696 bytes cmem[0]
ptxas info    : Compiling entry function '_Z27flash_bwd_convert_dq_kernelI23Flash_bwd_kernel_traitsILi64ELi64ELi128ELi8ELi2ELi4ELi4ELb1ELb0EN7cutlass6half_tE19Flash_kernel_traitsILi64ELi64ELi128ELi8ES2_EEEv16Flash_bwd_params' for 'sm_80'
ptxas info    : Function properties for _Z27flash_bwd_convert_dq_kernelI23Flash_bwd_kernel_traitsILi64ELi64ELi128ELi8ELi2ELi4ELi4ELb1ELb0EN7cutlass6half_tE19Flash_kernel_traitsILi64ELi64ELi128ELi8ES2_EEEv16Flash_bwd_params
    0 bytes stack frame, 0 bytes spill stores, 0 bytes spill loads
ptxas info    : Used 32 registers, 696 bytes cmem[0]
ptxas info    : Compiling entry function '_Z44flash_bwd_dq_dk_dv_loop_seqk_parallel_kernelI23Flash_bwd_kernel_traitsILi64ELi64ELi128ELi8ELi2ELi4ELi4ELb1ELb0EN7cutlass6half_tE19Flash_kernel_traitsILi64ELi64ELi128ELi8ES2_EELb1ELb0ELb0ELb0EEv16Flash_bwd_params' for 'sm_80'
ptxas info    : Function properties for _Z44flash_bwd_dq_dk_dv_loop_seqk_parallel_kernelI23Flash_bwd_kernel_traitsILi64ELi64ELi128ELi8ELi2ELi4ELi4ELb1ELb0EN7cutlass6half_tE19Flash_kernel_traitsILi64ELi64ELi128ELi8ES2_EELb1ELb0ELb0ELb0EEv16Flash_bwd_params
    0 bytes stack frame, 0 bytes spill stores, 0 bytes spill loads
ptxas info    : Used 237 registers, 696 bytes cmem[0]
ptxas info    : Compiling entry function '_Z44flash_bwd_dq_dk_dv_loop_seqk_parallel_kernelI23Flash_bwd_kernel_traitsILi64ELi64ELi128ELi8ELi2ELi4ELi4ELb1ELb0EN7cutlass6half_tE19Flash_kernel_traitsILi64ELi64ELi128ELi8ES2_EELb1ELb0ELb0ELb1EEv16Flash_bwd_params' for 'sm_80'
ptxas info    : Function properties for _Z44flash_bwd_dq_dk_dv_loop_seqk_parallel_kernelI23Flash_bwd_kernel_traitsILi64ELi64ELi128ELi8ELi2ELi4ELi4ELb1ELb0EN7cutlass6half_tE19Flash_kernel_traitsILi64ELi64ELi128ELi8ES2_EELb1ELb0ELb0ELb1EEv16Flash_bwd_params
    0 bytes stack frame, 0 bytes spill stores, 0 bytes spill loads
ptxas info    : Used 237 registers, 696 bytes cmem[0]
ptxas info    : Compiling entry function '_Z44flash_bwd_dq_dk_dv_loop_seqk_parallel_kernelI23Flash_bwd_kernel_traitsILi64ELi64ELi128ELi8ELi2ELi4ELi4ELb1ELb0EN7cutlass6half_tE19Flash_kernel_traitsILi64ELi64ELi128ELi8ES2_EELb1ELb0ELb1ELb0EEv16Flash_bwd_params' for 'sm_80'
ptxas info    : Function properties for _Z44flash_bwd_dq_dk_dv_loop_seqk_parallel_kernelI23Flash_bwd_kernel_traitsILi64ELi64ELi128ELi8ELi2ELi4ELi4ELb1ELb0EN7cutlass6half_tE19Flash_kernel_traitsILi64ELi64ELi128ELi8ES2_EELb1ELb0ELb1ELb0EEv16Flash_bwd_params
    0 bytes stack frame, 0 bytes spill stores, 0 bytes spill loads
ptxas info    : Used 222 registers, 696 bytes cmem[0]
ptxas info    : Compiling entry function '_Z44flash_bwd_dq_dk_dv_loop_seqk_parallel_kernelI23Flash_bwd_kernel_traitsILi64ELi64ELi128ELi8ELi2ELi4ELi4ELb1ELb0EN7cutlass6half_tE19Flash_kernel_traitsILi64ELi64ELi128ELi8ES2_EELb1ELb0ELb1ELb1EEv16Flash_bwd_params' for 'sm_80'
ptxas info    : Function properties for _Z44flash_bwd_dq_dk_dv_loop_seqk_parallel_kernelI23Flash_bwd_kernel_traitsILi64ELi64ELi128ELi8ELi2ELi4ELi4ELb1ELb0EN7cutlass6half_tE19Flash_kernel_traitsILi64ELi64ELi128ELi8ES2_EELb1ELb0ELb1ELb1EEv16Flash_bwd_params
    0 bytes stack frame, 0 bytes spill stores, 0 bytes spill loads
ptxas info    : Used 226 registers, 696 bytes cmem[0]
ptxas info    : Compiling entry function '_Z44flash_bwd_dq_dk_dv_loop_seqk_parallel_kernelI23Flash_bwd_kernel_traitsILi64ELi64ELi128ELi8ELi2ELi4ELi4ELb1ELb0EN7cutlass6half_tE19Flash_kernel_traitsILi64ELi64ELi128ELi8ES2_EELb1ELb1ELb0ELb0EEv16Flash_bwd_params' for 'sm_80'
ptxas info    : Function properties for _Z44flash_bwd_dq_dk_dv_loop_seqk_parallel_kernelI23Flash_bwd_kernel_traitsILi64ELi64ELi128ELi8ELi2ELi4ELi4ELb1ELb0EN7cutlass6half_tE19Flash_kernel_traitsILi64ELi64ELi128ELi8ES2_EELb1ELb1ELb0ELb0EEv16Flash_bwd_params
    0 bytes stack frame, 0 bytes spill stores, 0 bytes spill loads
ptxas info    : Used 236 registers, 696 bytes cmem[0]
ptxas info    : Compiling entry function '_Z44flash_bwd_dq_dk_dv_loop_seqk_parallel_kernelI23Flash_bwd_kernel_traitsILi64ELi64ELi128ELi8ELi2ELi4ELi4ELb1ELb0EN7cutlass6half_tE19Flash_kernel_traitsILi64ELi64ELi128ELi8ES2_EELb1ELb1ELb0ELb1EEv16Flash_bwd_params' for 'sm_80'
ptxas info    : Function properties for _Z44flash_bwd_dq_dk_dv_loop_seqk_parallel_kernelI23Flash_bwd_kernel_traitsILi64ELi64ELi128ELi8ELi2ELi4ELi4ELb1ELb0EN7cutlass6half_tE19Flash_kernel_traitsILi64ELi64ELi128ELi8ES2_EELb1ELb1ELb0ELb1EEv16Flash_bwd_params
    0 bytes stack frame, 0 bytes spill stores, 0 bytes spill loads
ptxas info    : Used 238 registers, 696 bytes cmem[0]
ptxas info    : Compiling entry function '_Z44flash_bwd_dq_dk_dv_loop_seqk_parallel_kernelI23Flash_bwd_kernel_traitsILi64ELi64ELi128ELi8ELi2ELi4ELi4ELb1ELb0EN7cutlass6half_tE19Flash_kernel_traitsILi64ELi64ELi128ELi8ES2_EELb1ELb1ELb1ELb0EEv16Flash_bwd_params' for 'sm_80'
ptxas info    : Function properties for _Z44flash_bwd_dq_dk_dv_loop_seqk_parallel_kernelI23Flash_bwd_kernel_traitsILi64ELi64ELi128ELi8ELi2ELi4ELi4ELb1ELb0EN7cutlass6half_tE19Flash_kernel_traitsILi64ELi64ELi128ELi8ES2_EELb1ELb1ELb1ELb0EEv16Flash_bwd_params
    0 bytes stack frame, 0 bytes spill stores, 0 bytes spill loads
ptxas info    : Used 234 registers, 696 bytes cmem[0]
ptxas info    : Compiling entry function '_Z44flash_bwd_dq_dk_dv_loop_seqk_parallel_kernelI23Flash_bwd_kernel_traitsILi64ELi64ELi128ELi8ELi2ELi4ELi4ELb1ELb0EN7cutlass6half_tE19Flash_kernel_traitsILi64ELi64ELi128ELi8ES2_EELb1ELb1ELb1ELb1EEv16Flash_bwd_params' for 'sm_80'
ptxas info    : Function properties for _Z44flash_bwd_dq_dk_dv_loop_seqk_parallel_kernelI23Flash_bwd_kernel_traitsILi64ELi64ELi128ELi8ELi2ELi4ELi4ELb1ELb0EN7cutlass6half_tE19Flash_kernel_traitsILi64ELi64ELi128ELi8ES2_EELb1ELb1ELb1ELb1EEv16Flash_bwd_params
    0 bytes stack frame, 0 bytes spill stores, 0 bytes spill loads
ptxas info    : Used 239 registers, 696 bytes cmem[0]
ptxas info    : Compiling entry function '_Z25flash_bwd_dot_do_o_kernelILb1E23Flash_bwd_kernel_traitsILi64ELi64ELi128ELi8ELi2ELi4ELi4ELb1ELb0EN7cutlass6half_tE19Flash_kernel_traitsILi64ELi64ELi128ELi8ES2_EEEv16Flash_bwd_params' for 'sm_80'
ptxas info    : Function properties for _Z25flash_bwd_dot_do_o_kernelILb1E23Flash_bwd_kernel_traitsILi64ELi64ELi128ELi8ELi2ELi4ELi4ELb1ELb0EN7cutlass6half_tE19Flash_kernel_traitsILi64ELi64ELi128ELi8ES2_EEEv16Flash_bwd_params
    0 bytes stack frame, 0 bytes spill stores, 0 bytes spill loads
ptxas info    : Used 31 registers, 696 bytes cmem[0]
ptxas info    : Compiling entry function '_Z27flash_bwd_convert_dq_kernelI23Flash_bwd_kernel_traitsILi64ELi128ELi128ELi8ELi4ELi4ELi4ELb0ELb0EN7cutlass6half_tE19Flash_kernel_traitsILi64ELi128ELi128ELi8ES2_EEEv16Flash_bwd_params' for 'sm_80'
ptxas info    : Function properties for _Z27flash_bwd_convert_dq_kernelI23Flash_bwd_kernel_traitsILi64ELi128ELi128ELi8ELi4ELi4ELi4ELb0ELb0EN7cutlass6half_tE19Flash_kernel_traitsILi64ELi128ELi128ELi8ES2_EEEv16Flash_bwd_params
    0 bytes stack frame, 0 bytes spill stores, 0 bytes spill loads
ptxas info    : Used 32 registers, 696 bytes cmem[0]
ptxas info    : Compiling entry function '_Z44flash_bwd_dq_dk_dv_loop_seqk_parallel_kernelI23Flash_bwd_kernel_traitsILi64ELi128ELi128ELi8ELi4ELi4ELi4ELb0ELb0EN7cutlass6half_tE19Flash_kernel_traitsILi64ELi128ELi128ELi8ES2_EELb1ELb0ELb0ELb0EEv16Flash_bwd_params' for 'sm_80'
ptxas info    : Function properties for _Z44flash_bwd_dq_dk_dv_loop_seqk_parallel_kernelI23Flash_bwd_kernel_traitsILi64ELi128ELi128ELi8ELi4ELi4ELi4ELb0ELb0EN7cutlass6half_tE19Flash_kernel_traitsILi64ELi128ELi128ELi8ES2_EELb1ELb0ELb0ELb0EEv16Flash_bwd_params
    48 bytes stack frame, 48 bytes spill stores, 44 bytes spill loads
ptxas info    : Used 255 registers, 696 bytes cmem[0]
ptxas info    : Compiling entry function '_Z44flash_bwd_dq_dk_dv_loop_seqk_parallel_kernelI23Flash_bwd_kernel_traitsILi64ELi128ELi128ELi8ELi4ELi4ELi4ELb0ELb0EN7cutlass6half_tE19Flash_kernel_traitsILi64ELi128ELi128ELi8ES2_EELb1ELb0ELb0ELb1EEv16Flash_bwd_params' for 'sm_80'
ptxas info    : Function properties for _Z44flash_bwd_dq_dk_dv_loop_seqk_parallel_kernelI23Flash_bwd_kernel_traitsILi64ELi128ELi128ELi8ELi4ELi4ELi4ELb0ELb0EN7cutlass6half_tE19Flash_kernel_traitsILi64ELi128ELi128ELi8ES2_EELb1ELb0ELb0ELb1EEv16Flash_bwd_params
    56 bytes stack frame, 72 bytes spill stores, 72 bytes spill loads
ptxas info    : Used 255 registers, 696 bytes cmem[0]
ptxas info    : Compiling entry function '_Z44flash_bwd_dq_dk_dv_loop_seqk_parallel_kernelI23Flash_bwd_kernel_traitsILi64ELi128ELi128ELi8ELi4ELi4ELi4ELb0ELb0EN7cutlass6half_tE19Flash_kernel_traitsILi64ELi128ELi128ELi8ES2_EELb1ELb0ELb1ELb0EEv16Flash_bwd_params' for 'sm_80'
ptxas info    : Function properties for _Z44flash_bwd_dq_dk_dv_loop_seqk_parallel_kernelI23Flash_bwd_kernel_traitsILi64ELi128ELi128ELi8ELi4ELi4ELi4ELb0ELb0EN7cutlass6half_tE19Flash_kernel_traitsILi64ELi128ELi128ELi8ES2_EELb1ELb0ELb1ELb0EEv16Flash_bwd_params
    56 bytes stack frame, 68 bytes spill stores, 72 bytes spill loads
ptxas info    : Used 255 registers, 696 bytes cmem[0]
ptxas info    : Compiling entry function '_Z44flash_bwd_dq_dk_dv_loop_seqk_parallel_kernelI23Flash_bwd_kernel_traitsILi64ELi128ELi128ELi8ELi4ELi4ELi4ELb0ELb0EN7cutlass6half_tE19Flash_kernel_traitsILi64ELi128ELi128ELi8ES2_EELb1ELb0ELb1ELb1EEv16Flash_bwd_params' for 'sm_80'
ptxas info    : Function properties for _Z44flash_bwd_dq_dk_dv_loop_seqk_parallel_kernelI23Flash_bwd_kernel_traitsILi64ELi128ELi128ELi8ELi4ELi4ELi4ELb0ELb0EN7cutlass6half_tE19Flash_kernel_traitsILi64ELi128ELi128ELi8ES2_EELb1ELb0ELb1ELb1EEv16Flash_bwd_params
    56 bytes stack frame, 72 bytes spill stores, 80 bytes spill loads
ptxas info    : Used 255 registers, 696 bytes cmem[0]
ptxas info    : Compiling entry function '_Z44flash_bwd_dq_dk_dv_loop_seqk_parallel_kernelI23Flash_bwd_kernel_traitsILi64ELi128ELi128ELi8ELi4ELi4ELi4ELb0ELb0EN7cutlass6half_tE19Flash_kernel_traitsILi64ELi128ELi128ELi8ES2_EELb1ELb1ELb0ELb0EEv16Flash_bwd_params' for 'sm_80'
ptxas info    : Function properties for _Z44flash_bwd_dq_dk_dv_loop_seqk_parallel_kernelI23Flash_bwd_kernel_traitsILi64ELi128ELi128ELi8ELi4ELi4ELi4ELb0ELb0EN7cutlass6half_tE19Flash_kernel_traitsILi64ELi128ELi128ELi8ES2_EELb1ELb1ELb0ELb0EEv16Flash_bwd_params
    24 bytes stack frame, 20 bytes spill stores, 20 bytes spill loads
ptxas info    : Used 255 registers, 696 bytes cmem[0]
ptxas info    : Compiling entry function '_Z44flash_bwd_dq_dk_dv_loop_seqk_parallel_kernelI23Flash_bwd_kernel_traitsILi64ELi128ELi128ELi8ELi4ELi4ELi4ELb0ELb0EN7cutlass6half_tE19Flash_kernel_traitsILi64ELi128ELi128ELi8ES2_EELb1ELb1ELb0ELb1EEv16Flash_bwd_params' for 'sm_80'
ptxas info    : Function properties for _Z44flash_bwd_dq_dk_dv_loop_seqk_parallel_kernelI23Flash_bwd_kernel_traitsILi64ELi128ELi128ELi8ELi4ELi4ELi4ELb0ELb0EN7cutlass6half_tE19Flash_kernel_traitsILi64ELi128ELi128ELi8ES2_EELb1ELb1ELb0ELb1EEv16Flash_bwd_params
    32 bytes stack frame, 28 bytes spill stores, 28 bytes spill loads
ptxas info    : Used 255 registers, 696 bytes cmem[0]
ptxas info    : Compiling entry function '_Z44flash_bwd_dq_dk_dv_loop_seqk_parallel_kernelI23Flash_bwd_kernel_traitsILi64ELi128ELi128ELi8ELi4ELi4ELi4ELb0ELb0EN7cutlass6half_tE19Flash_kernel_traitsILi64ELi128ELi128ELi8ES2_EELb1ELb1ELb1ELb0EEv16Flash_bwd_params' for 'sm_80'
ptxas info    : Function properties for _Z44flash_bwd_dq_dk_dv_loop_seqk_parallel_kernelI23Flash_bwd_kernel_traitsILi64ELi128ELi128ELi8ELi4ELi4ELi4ELb0ELb0EN7cutlass6half_tE19Flash_kernel_traitsILi64ELi128ELi128ELi8ES2_EELb1ELb1ELb1ELb0EEv16Flash_bwd_params
    48 bytes stack frame, 60 bytes spill stores, 56 bytes spill loads
ptxas info    : Used 255 registers, 696 bytes cmem[0]
ptxas info    : Compiling entry function '_Z44flash_bwd_dq_dk_dv_loop_seqk_parallel_kernelI23Flash_bwd_kernel_traitsILi64ELi128ELi128ELi8ELi4ELi4ELi4ELb0ELb0EN7cutlass6half_tE19Flash_kernel_traitsILi64ELi128ELi128ELi8ES2_EELb1ELb1ELb1ELb1EEv16Flash_bwd_params' for 'sm_80'
ptxas info    : Function properties for _Z44flash_bwd_dq_dk_dv_loop_seqk_parallel_kernelI23Flash_bwd_kernel_traitsILi64ELi128ELi128ELi8ELi4ELi4ELi4ELb0ELb0EN7cutlass6half_tE19Flash_kernel_traitsILi64ELi128ELi128ELi8ES2_EELb1ELb1ELb1ELb1EEv16Flash_bwd_params
    16 bytes stack frame, 16 bytes spill stores, 16 bytes spill loads
ptxas info    : Used 255 registers, 696 bytes cmem[0]
ptxas info    : Compiling entry function '_Z25flash_bwd_dot_do_o_kernelILb1E23Flash_bwd_kernel_traitsILi64ELi128ELi128ELi8ELi4ELi4ELi4ELb0ELb0EN7cutlass6half_tE19Flash_kernel_traitsILi64ELi128ELi128ELi8ES2_EEEv16Flash_bwd_params' for 'sm_80'
ptxas info    : Function properties for _Z25flash_bwd_dot_do_o_kernelILb1E23Flash_bwd_kernel_traitsILi64ELi128ELi128ELi8ELi4ELi4ELi4ELb0ELb0EN7cutlass6half_tE19Flash_kernel_traitsILi64ELi128ELi128ELi8ES2_EEEv16Flash_bwd_params
    0 bytes stack frame, 0 bytes spill stores, 0 bytes spill loads
ptxas info    : Used 46 registers, 696 bytes cmem[0]
/anaconda/envs/flashattention/lib/python3.9/site-packages/torch/include/c10/core/SymInt.h(84): warning: integer conversion resulted in a change of sign

[25/36] /usr/local/cuda-11.4/bin/nvcc  -I/home/v-feiychen/flash-attention/csrc/flash_attn -I/home/v-feiychen/flash-attention/csrc/flash_attn/src -I/home/v-feiychen/flash-attention/csrc/cutlass/include -I/anaconda/envs/flashattention/lib/python3.9/site-packages/torch/include -I/anaconda/envs/flashattention/lib/python3.9/site-packages/torch/include/torch/csrc/api/include -I/anaconda/envs/flashattention/lib/python3.9/site-packages/torch/include/TH -I/anaconda/envs/flashattention/lib/python3.9/site-packages/torch/include/THC -I/usr/local/cuda-11.4/include -I/anaconda/envs/flashattention/include/python3.9 -c -c /home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_hdim160_bf16_sm80.cu -o /home/v-feiychen/flash-attention/build/temp.linux-x86_64-cpython-39/csrc/flash_attn/src/flash_fwd_hdim160_bf16_sm80.o -D__CUDA_NO_HALF_OPERATORS__ -D__CUDA_NO_HALF_CONVERSIONS__ -D__CUDA_NO_BFLOAT16_CONVERSIONS__ -D__CUDA_NO_HALF2_OPERATORS__ --expt-relaxed-constexpr --compiler-options ''"'"'-fPIC'"'"'' -O3 -std=c++17 -U__CUDA_NO_HALF_OPERATORS__ -U__CUDA_NO_HALF_CONVERSIONS__ -U__CUDA_NO_HALF2_OPERATORS__ -U__CUDA_NO_BFLOAT16_CONVERSIONS__ --expt-relaxed-constexpr --expt-extended-lambda --use_fast_math --ptxas-options=-v -lineinfo -gencode arch=compute_80,code=sm_80 --threads 4 -DTORCH_API_INCLUDE_EXTENSION_H '-DPYBIND11_COMPILER_TYPE="_gcc"' '-DPYBIND11_STDLIB="_libstdcpp"' '-DPYBIND11_BUILD_ABI="_cxxabi1011"' -DTORCH_EXTENSION_NAME=flash_attn_2_cuda -D_GLIBCXX_USE_CXX11_ABI=0
/anaconda/envs/flashattention/lib/python3.9/site-packages/torch/include/c10/core/SymInt.h(84): warning: integer conversion resulted in a change of sign

/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_kernel.h(140): warning: variable "MMA_M" was declared but never referenced
          detected during:
            instantiation of "void flash::compute_attn_1rowblock<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &, int, int, int) [with Kernel_traits=Flash_fwd_kernel_traits<160, 64, 64, 4, false, false, cutlass::bfloat16_t, Flash_kernel_traits<160, 64, 64, 4, cutlass::bfloat16_t>>, Is_dropout=true, Is_causal=true, Is_even_N=true, Is_even_K=true, Return_softmax=true, Params=Flash_fwd_params]" 
(588): here
            instantiation of "void flash::compute_attn<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &) [with Kernel_traits=Flash_fwd_kernel_traits<160, 64, 64, 4, false, false, cutlass::bfloat16_t, Flash_kernel_traits<160, 64, 64, 4, cutlass::bfloat16_t>>, Is_dropout=true, Is_causal=true, Is_even_N=true, Is_even_K=true, Return_softmax=true, Params=Flash_fwd_params]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(15): here
            instantiation of "void flash_fwd_kernel<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax>(Flash_fwd_params) [with Kernel_traits=Flash_fwd_kernel_traits<160, 64, 64, 4, false, false, cutlass::bfloat16_t, Flash_kernel_traits<160, 64, 64, 4, cutlass::bfloat16_t>>, Is_dropout=true, Is_causal=true, Is_even_N=true, Is_even_K=true, Return_softmax=true]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(34): here
            instantiation of "void run_flash_fwd<Kernel_traits,Is_dropout,Is_causal>(Flash_fwd_params &, cudaStream_t) [with Kernel_traits=Flash_fwd_kernel_traits<160, 64, 64, 4, false, false, cutlass::bfloat16_t, Flash_kernel_traits<160, 64, 64, 4, cutlass::bfloat16_t>>, Is_dropout=true, Is_causal=true]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(155): here
            instantiation of "void run_mha_fwd_hdim160<T>(Flash_fwd_params &, cudaStream_t) [with T=cutlass::bfloat16_t]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_hdim160_bf16_sm80.cu(16): here

/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_kernel.h(275): warning: variable "tQrQ" was declared but never referenced
          detected during:
            instantiation of "void flash::compute_attn_1rowblock<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &, int, int, int) [with Kernel_traits=Flash_fwd_kernel_traits<160, 64, 64, 4, false, false, cutlass::bfloat16_t, Flash_kernel_traits<160, 64, 64, 4, cutlass::bfloat16_t>>, Is_dropout=true, Is_causal=true, Is_even_N=true, Is_even_K=true, Return_softmax=true, Params=Flash_fwd_params]" 
(588): here
            instantiation of "void flash::compute_attn<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &) [with Kernel_traits=Flash_fwd_kernel_traits<160, 64, 64, 4, false, false, cutlass::bfloat16_t, Flash_kernel_traits<160, 64, 64, 4, cutlass::bfloat16_t>>, Is_dropout=true, Is_causal=true, Is_even_N=true, Is_even_K=true, Return_softmax=true, Params=Flash_fwd_params]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(15): here
            instantiation of "void flash_fwd_kernel<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax>(Flash_fwd_params) [with Kernel_traits=Flash_fwd_kernel_traits<160, 64, 64, 4, false, false, cutlass::bfloat16_t, Flash_kernel_traits<160, 64, 64, 4, cutlass::bfloat16_t>>, Is_dropout=true, Is_causal=true, Is_even_N=true, Is_even_K=true, Return_softmax=true]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(34): here
            instantiation of "void run_flash_fwd<Kernel_traits,Is_dropout,Is_causal>(Flash_fwd_params &, cudaStream_t) [with Kernel_traits=Flash_fwd_kernel_traits<160, 64, 64, 4, false, false, cutlass::bfloat16_t, Flash_kernel_traits<160, 64, 64, 4, cutlass::bfloat16_t>>, Is_dropout=true, Is_causal=true]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(155): here
            instantiation of "void run_mha_fwd_hdim160<T>(Flash_fwd_params &, cudaStream_t) [with T=cutlass::bfloat16_t]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_hdim160_bf16_sm80.cu(16): here

/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_kernel.h(140): warning: variable "MMA_M" was declared but never referenced
          detected during:
            instantiation of "void flash::compute_attn_1rowblock<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &, int, int, int) [with Kernel_traits=Flash_fwd_kernel_traits<160, 64, 64, 4, false, false, cutlass::bfloat16_t, Flash_kernel_traits<160, 64, 64, 4, cutlass::bfloat16_t>>, Is_dropout=true, Is_causal=true, Is_even_N=true, Is_even_K=true, Return_softmax=false, Params=Flash_fwd_params]" 
(588): here
            instantiation of "void flash::compute_attn<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &) [with Kernel_traits=Flash_fwd_kernel_traits<160, 64, 64, 4, false, false, cutlass::bfloat16_t, Flash_kernel_traits<160, 64, 64, 4, cutlass::bfloat16_t>>, Is_dropout=true, Is_causal=true, Is_even_N=true, Is_even_K=true, Return_softmax=false, Params=Flash_fwd_params]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(15): here
            instantiation of "void flash_fwd_kernel<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax>(Flash_fwd_params) [with Kernel_traits=Flash_fwd_kernel_traits<160, 64, 64, 4, false, false, cutlass::bfloat16_t, Flash_kernel_traits<160, 64, 64, 4, cutlass::bfloat16_t>>, Is_dropout=true, Is_causal=true, Is_even_N=true, Is_even_K=true, Return_softmax=false]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(34): here
            instantiation of "void run_flash_fwd<Kernel_traits,Is_dropout,Is_causal>(Flash_fwd_params &, cudaStream_t) [with Kernel_traits=Flash_fwd_kernel_traits<160, 64, 64, 4, false, false, cutlass::bfloat16_t, Flash_kernel_traits<160, 64, 64, 4, cutlass::bfloat16_t>>, Is_dropout=true, Is_causal=true]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(155): here
            instantiation of "void run_mha_fwd_hdim160<T>(Flash_fwd_params &, cudaStream_t) [with T=cutlass::bfloat16_t]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_hdim160_bf16_sm80.cu(16): here

/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_kernel.h(275): warning: variable "tQrQ" was declared but never referenced
          detected during:
            instantiation of "void flash::compute_attn_1rowblock<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &, int, int, int) [with Kernel_traits=Flash_fwd_kernel_traits<160, 64, 64, 4, false, false, cutlass::bfloat16_t, Flash_kernel_traits<160, 64, 64, 4, cutlass::bfloat16_t>>, Is_dropout=true, Is_causal=true, Is_even_N=true, Is_even_K=true, Return_softmax=false, Params=Flash_fwd_params]" 
(588): here
            instantiation of "void flash::compute_attn<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &) [with Kernel_traits=Flash_fwd_kernel_traits<160, 64, 64, 4, false, false, cutlass::bfloat16_t, Flash_kernel_traits<160, 64, 64, 4, cutlass::bfloat16_t>>, Is_dropout=true, Is_causal=true, Is_even_N=true, Is_even_K=true, Return_softmax=false, Params=Flash_fwd_params]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(15): here
            instantiation of "void flash_fwd_kernel<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax>(Flash_fwd_params) [with Kernel_traits=Flash_fwd_kernel_traits<160, 64, 64, 4, false, false, cutlass::bfloat16_t, Flash_kernel_traits<160, 64, 64, 4, cutlass::bfloat16_t>>, Is_dropout=true, Is_causal=true, Is_even_N=true, Is_even_K=true, Return_softmax=false]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(34): here
            instantiation of "void run_flash_fwd<Kernel_traits,Is_dropout,Is_causal>(Flash_fwd_params &, cudaStream_t) [with Kernel_traits=Flash_fwd_kernel_traits<160, 64, 64, 4, false, false, cutlass::bfloat16_t, Flash_kernel_traits<160, 64, 64, 4, cutlass::bfloat16_t>>, Is_dropout=true, Is_causal=true]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(155): here
            instantiation of "void run_mha_fwd_hdim160<T>(Flash_fwd_params &, cudaStream_t) [with T=cutlass::bfloat16_t]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_hdim160_bf16_sm80.cu(16): here

/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_kernel.h(140): warning: variable "MMA_M" was declared but never referenced
          detected during:
            instantiation of "void flash::compute_attn_1rowblock<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &, int, int, int) [with Kernel_traits=Flash_fwd_kernel_traits<160, 64, 64, 4, false, false, cutlass::bfloat16_t, Flash_kernel_traits<160, 64, 64, 4, cutlass::bfloat16_t>>, Is_dropout=true, Is_causal=true, Is_even_N=true, Is_even_K=false, Return_softmax=true, Params=Flash_fwd_params]" 
(588): here
            instantiation of "void flash::compute_attn<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &) [with Kernel_traits=Flash_fwd_kernel_traits<160, 64, 64, 4, false, false, cutlass::bfloat16_t, Flash_kernel_traits<160, 64, 64, 4, cutlass::bfloat16_t>>, Is_dropout=true, Is_causal=true, Is_even_N=true, Is_even_K=false, Return_softmax=true, Params=Flash_fwd_params]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(15): here
            instantiation of "void flash_fwd_kernel<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax>(Flash_fwd_params) [with Kernel_traits=Flash_fwd_kernel_traits<160, 64, 64, 4, false, false, cutlass::bfloat16_t, Flash_kernel_traits<160, 64, 64, 4, cutlass::bfloat16_t>>, Is_dropout=true, Is_causal=true, Is_even_N=true, Is_even_K=false, Return_softmax=true]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(34): here
            instantiation of "void run_flash_fwd<Kernel_traits,Is_dropout,Is_causal>(Flash_fwd_params &, cudaStream_t) [with Kernel_traits=Flash_fwd_kernel_traits<160, 64, 64, 4, false, false, cutlass::bfloat16_t, Flash_kernel_traits<160, 64, 64, 4, cutlass::bfloat16_t>>, Is_dropout=true, Is_causal=true]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(155): here
            instantiation of "void run_mha_fwd_hdim160<T>(Flash_fwd_params &, cudaStream_t) [with T=cutlass::bfloat16_t]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_hdim160_bf16_sm80.cu(16): here

/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_kernel.h(275): warning: variable "tQrQ" was declared but never referenced
          detected during:
            instantiation of "void flash::compute_attn_1rowblock<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &, int, int, int) [with Kernel_traits=Flash_fwd_kernel_traits<160, 64, 64, 4, false, false, cutlass::bfloat16_t, Flash_kernel_traits<160, 64, 64, 4, cutlass::bfloat16_t>>, Is_dropout=true, Is_causal=true, Is_even_N=true, Is_even_K=false, Return_softmax=true, Params=Flash_fwd_params]" 
(588): here
            instantiation of "void flash::compute_attn<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &) [with Kernel_traits=Flash_fwd_kernel_traits<160, 64, 64, 4, false, false, cutlass::bfloat16_t, Flash_kernel_traits<160, 64, 64, 4, cutlass::bfloat16_t>>, Is_dropout=true, Is_causal=true, Is_even_N=true, Is_even_K=false, Return_softmax=true, Params=Flash_fwd_params]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(15): here
            instantiation of "void flash_fwd_kernel<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax>(Flash_fwd_params) [with Kernel_traits=Flash_fwd_kernel_traits<160, 64, 64, 4, false, false, cutlass::bfloat16_t, Flash_kernel_traits<160, 64, 64, 4, cutlass::bfloat16_t>>, Is_dropout=true, Is_causal=true, Is_even_N=true, Is_even_K=false, Return_softmax=true]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(34): here
            instantiation of "void run_flash_fwd<Kernel_traits,Is_dropout,Is_causal>(Flash_fwd_params &, cudaStream_t) [with Kernel_traits=Flash_fwd_kernel_traits<160, 64, 64, 4, false, false, cutlass::bfloat16_t, Flash_kernel_traits<160, 64, 64, 4, cutlass::bfloat16_t>>, Is_dropout=true, Is_causal=true]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(155): here
            instantiation of "void run_mha_fwd_hdim160<T>(Flash_fwd_params &, cudaStream_t) [with T=cutlass::bfloat16_t]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_hdim160_bf16_sm80.cu(16): here

/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_kernel.h(140): warning: variable "MMA_M" was declared but never referenced
          detected during:
            instantiation of "void flash::compute_attn_1rowblock<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &, int, int, int) [with Kernel_traits=Flash_fwd_kernel_traits<160, 64, 64, 4, false, false, cutlass::bfloat16_t, Flash_kernel_traits<160, 64, 64, 4, cutlass::bfloat16_t>>, Is_dropout=true, Is_causal=true, Is_even_N=true, Is_even_K=false, Return_softmax=false, Params=Flash_fwd_params]" 
(588): here
            instantiation of "void flash::compute_attn<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &) [with Kernel_traits=Flash_fwd_kernel_traits<160, 64, 64, 4, false, false, cutlass::bfloat16_t, Flash_kernel_traits<160, 64, 64, 4, cutlass::bfloat16_t>>, Is_dropout=true, Is_causal=true, Is_even_N=true, Is_even_K=false, Return_softmax=false, Params=Flash_fwd_params]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(15): here
            instantiation of "void flash_fwd_kernel<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax>(Flash_fwd_params) [with Kernel_traits=Flash_fwd_kernel_traits<160, 64, 64, 4, false, false, cutlass::bfloat16_t, Flash_kernel_traits<160, 64, 64, 4, cutlass::bfloat16_t>>, Is_dropout=true, Is_causal=true, Is_even_N=true, Is_even_K=false, Return_softmax=false]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(34): here
            instantiation of "void run_flash_fwd<Kernel_traits,Is_dropout,Is_causal>(Flash_fwd_params &, cudaStream_t) [with Kernel_traits=Flash_fwd_kernel_traits<160, 64, 64, 4, false, false, cutlass::bfloat16_t, Flash_kernel_traits<160, 64, 64, 4, cutlass::bfloat16_t>>, Is_dropout=true, Is_causal=true]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(155): here
            instantiation of "void run_mha_fwd_hdim160<T>(Flash_fwd_params &, cudaStream_t) [with T=cutlass::bfloat16_t]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_hdim160_bf16_sm80.cu(16): here

/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_kernel.h(275): warning: variable "tQrQ" was declared but never referenced
          detected during:
            instantiation of "void flash::compute_attn_1rowblock<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &, int, int, int) [with Kernel_traits=Flash_fwd_kernel_traits<160, 64, 64, 4, false, false, cutlass::bfloat16_t, Flash_kernel_traits<160, 64, 64, 4, cutlass::bfloat16_t>>, Is_dropout=true, Is_causal=true, Is_even_N=true, Is_even_K=false, Return_softmax=false, Params=Flash_fwd_params]" 
(588): here
            instantiation of "void flash::compute_attn<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &) [with Kernel_traits=Flash_fwd_kernel_traits<160, 64, 64, 4, false, false, cutlass::bfloat16_t, Flash_kernel_traits<160, 64, 64, 4, cutlass::bfloat16_t>>, Is_dropout=true, Is_causal=true, Is_even_N=true, Is_even_K=false, Return_softmax=false, Params=Flash_fwd_params]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(15): here
            instantiation of "void flash_fwd_kernel<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax>(Flash_fwd_params) [with Kernel_traits=Flash_fwd_kernel_traits<160, 64, 64, 4, false, false, cutlass::bfloat16_t, Flash_kernel_traits<160, 64, 64, 4, cutlass::bfloat16_t>>, Is_dropout=true, Is_causal=true, Is_even_N=true, Is_even_K=false, Return_softmax=false]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(34): here
            instantiation of "void run_flash_fwd<Kernel_traits,Is_dropout,Is_causal>(Flash_fwd_params &, cudaStream_t) [with Kernel_traits=Flash_fwd_kernel_traits<160, 64, 64, 4, false, false, cutlass::bfloat16_t, Flash_kernel_traits<160, 64, 64, 4, cutlass::bfloat16_t>>, Is_dropout=true, Is_causal=true]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(155): here
            instantiation of "void run_mha_fwd_hdim160<T>(Flash_fwd_params &, cudaStream_t) [with T=cutlass::bfloat16_t]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_hdim160_bf16_sm80.cu(16): here

/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_kernel.h(140): warning: variable "MMA_M" was declared but never referenced
          detected during:
            instantiation of "void flash::compute_attn_1rowblock<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &, int, int, int) [with Kernel_traits=Flash_fwd_kernel_traits<160, 64, 64, 4, false, false, cutlass::bfloat16_t, Flash_kernel_traits<160, 64, 64, 4, cutlass::bfloat16_t>>, Is_dropout=true, Is_causal=true, Is_even_N=false, Is_even_K=true, Return_softmax=true, Params=Flash_fwd_params]" 
(588): here
            instantiation of "void flash::compute_attn<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &) [with Kernel_traits=Flash_fwd_kernel_traits<160, 64, 64, 4, false, false, cutlass::bfloat16_t, Flash_kernel_traits<160, 64, 64, 4, cutlass::bfloat16_t>>, Is_dropout=true, Is_causal=true, Is_even_N=false, Is_even_K=true, Return_softmax=true, Params=Flash_fwd_params]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(15): here
            instantiation of "void flash_fwd_kernel<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax>(Flash_fwd_params) [with Kernel_traits=Flash_fwd_kernel_traits<160, 64, 64, 4, false, false, cutlass::bfloat16_t, Flash_kernel_traits<160, 64, 64, 4, cutlass::bfloat16_t>>, Is_dropout=true, Is_causal=true, Is_even_N=false, Is_even_K=true, Return_softmax=true]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(34): here
            instantiation of "void run_flash_fwd<Kernel_traits,Is_dropout,Is_causal>(Flash_fwd_params &, cudaStream_t) [with Kernel_traits=Flash_fwd_kernel_traits<160, 64, 64, 4, false, false, cutlass::bfloat16_t, Flash_kernel_traits<160, 64, 64, 4, cutlass::bfloat16_t>>, Is_dropout=true, Is_causal=true]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(155): here
            instantiation of "void run_mha_fwd_hdim160<T>(Flash_fwd_params &, cudaStream_t) [with T=cutlass::bfloat16_t]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_hdim160_bf16_sm80.cu(16): here

/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_kernel.h(275): warning: variable "tQrQ" was declared but never referenced
          detected during:
            instantiation of "void flash::compute_attn_1rowblock<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &, int, int, int) [with Kernel_traits=Flash_fwd_kernel_traits<160, 64, 64, 4, false, false, cutlass::bfloat16_t, Flash_kernel_traits<160, 64, 64, 4, cutlass::bfloat16_t>>, Is_dropout=true, Is_causal=true, Is_even_N=false, Is_even_K=true, Return_softmax=true, Params=Flash_fwd_params]" 
(588): here
            instantiation of "void flash::compute_attn<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &) [with Kernel_traits=Flash_fwd_kernel_traits<160, 64, 64, 4, false, false, cutlass::bfloat16_t, Flash_kernel_traits<160, 64, 64, 4, cutlass::bfloat16_t>>, Is_dropout=true, Is_causal=true, Is_even_N=false, Is_even_K=true, Return_softmax=true, Params=Flash_fwd_params]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(15): here
            instantiation of "void flash_fwd_kernel<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax>(Flash_fwd_params) [with Kernel_traits=Flash_fwd_kernel_traits<160, 64, 64, 4, false, false, cutlass::bfloat16_t, Flash_kernel_traits<160, 64, 64, 4, cutlass::bfloat16_t>>, Is_dropout=true, Is_causal=true, Is_even_N=false, Is_even_K=true, Return_softmax=true]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(34): here
            instantiation of "void run_flash_fwd<Kernel_traits,Is_dropout,Is_causal>(Flash_fwd_params &, cudaStream_t) [with Kernel_traits=Flash_fwd_kernel_traits<160, 64, 64, 4, false, false, cutlass::bfloat16_t, Flash_kernel_traits<160, 64, 64, 4, cutlass::bfloat16_t>>, Is_dropout=true, Is_causal=true]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(155): here
            instantiation of "void run_mha_fwd_hdim160<T>(Flash_fwd_params &, cudaStream_t) [with T=cutlass::bfloat16_t]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_hdim160_bf16_sm80.cu(16): here

/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_kernel.h(140): warning: variable "MMA_M" was declared but never referenced
          detected during:
            instantiation of "void flash::compute_attn_1rowblock<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &, int, int, int) [with Kernel_traits=Flash_fwd_kernel_traits<160, 64, 64, 4, false, false, cutlass::bfloat16_t, Flash_kernel_traits<160, 64, 64, 4, cutlass::bfloat16_t>>, Is_dropout=true, Is_causal=true, Is_even_N=false, Is_even_K=true, Return_softmax=false, Params=Flash_fwd_params]" 
(588): here
            instantiation of "void flash::compute_attn<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &) [with Kernel_traits=Flash_fwd_kernel_traits<160, 64, 64, 4, false, false, cutlass::bfloat16_t, Flash_kernel_traits<160, 64, 64, 4, cutlass::bfloat16_t>>, Is_dropout=true, Is_causal=true, Is_even_N=false, Is_even_K=true, Return_softmax=false, Params=Flash_fwd_params]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(15): here
            instantiation of "void flash_fwd_kernel<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax>(Flash_fwd_params) [with Kernel_traits=Flash_fwd_kernel_traits<160, 64, 64, 4, false, false, cutlass::bfloat16_t, Flash_kernel_traits<160, 64, 64, 4, cutlass::bfloat16_t>>, Is_dropout=true, Is_causal=true, Is_even_N=false, Is_even_K=true, Return_softmax=false]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(34): here
            instantiation of "void run_flash_fwd<Kernel_traits,Is_dropout,Is_causal>(Flash_fwd_params &, cudaStream_t) [with Kernel_traits=Flash_fwd_kernel_traits<160, 64, 64, 4, false, false, cutlass::bfloat16_t, Flash_kernel_traits<160, 64, 64, 4, cutlass::bfloat16_t>>, Is_dropout=true, Is_causal=true]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(155): here
            instantiation of "void run_mha_fwd_hdim160<T>(Flash_fwd_params &, cudaStream_t) [with T=cutlass::bfloat16_t]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_hdim160_bf16_sm80.cu(16): here

/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_kernel.h(275): warning: variable "tQrQ" was declared but never referenced
          detected during:
            instantiation of "void flash::compute_attn_1rowblock<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &, int, int, int) [with Kernel_traits=Flash_fwd_kernel_traits<160, 64, 64, 4, false, false, cutlass::bfloat16_t, Flash_kernel_traits<160, 64, 64, 4, cutlass::bfloat16_t>>, Is_dropout=true, Is_causal=true, Is_even_N=false, Is_even_K=true, Return_softmax=false, Params=Flash_fwd_params]" 
(588): here
            instantiation of "void flash::compute_attn<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &) [with Kernel_traits=Flash_fwd_kernel_traits<160, 64, 64, 4, false, false, cutlass::bfloat16_t, Flash_kernel_traits<160, 64, 64, 4, cutlass::bfloat16_t>>, Is_dropout=true, Is_causal=true, Is_even_N=false, Is_even_K=true, Return_softmax=false, Params=Flash_fwd_params]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(15): here
            instantiation of "void flash_fwd_kernel<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax>(Flash_fwd_params) [with Kernel_traits=Flash_fwd_kernel_traits<160, 64, 64, 4, false, false, cutlass::bfloat16_t, Flash_kernel_traits<160, 64, 64, 4, cutlass::bfloat16_t>>, Is_dropout=true, Is_causal=true, Is_even_N=false, Is_even_K=true, Return_softmax=false]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(34): here
            instantiation of "void run_flash_fwd<Kernel_traits,Is_dropout,Is_causal>(Flash_fwd_params &, cudaStream_t) [with Kernel_traits=Flash_fwd_kernel_traits<160, 64, 64, 4, false, false, cutlass::bfloat16_t, Flash_kernel_traits<160, 64, 64, 4, cutlass::bfloat16_t>>, Is_dropout=true, Is_causal=true]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(155): here
            instantiation of "void run_mha_fwd_hdim160<T>(Flash_fwd_params &, cudaStream_t) [with T=cutlass::bfloat16_t]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_hdim160_bf16_sm80.cu(16): here

/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_kernel.h(140): warning: variable "MMA_M" was declared but never referenced
          detected during:
            instantiation of "void flash::compute_attn_1rowblock<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &, int, int, int) [with Kernel_traits=Flash_fwd_kernel_traits<160, 64, 64, 4, false, false, cutlass::bfloat16_t, Flash_kernel_traits<160, 64, 64, 4, cutlass::bfloat16_t>>, Is_dropout=true, Is_causal=true, Is_even_N=false, Is_even_K=false, Return_softmax=true, Params=Flash_fwd_params]" 
(588): here
            instantiation of "void flash::compute_attn<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &) [with Kernel_traits=Flash_fwd_kernel_traits<160, 64, 64, 4, false, false, cutlass::bfloat16_t, Flash_kernel_traits<160, 64, 64, 4, cutlass::bfloat16_t>>, Is_dropout=true, Is_causal=true, Is_even_N=false, Is_even_K=false, Return_softmax=true, Params=Flash_fwd_params]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(15): here
            instantiation of "void flash_fwd_kernel<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax>(Flash_fwd_params) [with Kernel_traits=Flash_fwd_kernel_traits<160, 64, 64, 4, false, false, cutlass::bfloat16_t, Flash_kernel_traits<160, 64, 64, 4, cutlass::bfloat16_t>>, Is_dropout=true, Is_causal=true, Is_even_N=false, Is_even_K=false, Return_softmax=true]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(34): here
            instantiation of "void run_flash_fwd<Kernel_traits,Is_dropout,Is_causal>(Flash_fwd_params &, cudaStream_t) [with Kernel_traits=Flash_fwd_kernel_traits<160, 64, 64, 4, false, false, cutlass::bfloat16_t, Flash_kernel_traits<160, 64, 64, 4, cutlass::bfloat16_t>>, Is_dropout=true, Is_causal=true]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(155): here
            instantiation of "void run_mha_fwd_hdim160<T>(Flash_fwd_params &, cudaStream_t) [with T=cutlass::bfloat16_t]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_hdim160_bf16_sm80.cu(16): here

/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_kernel.h(275): warning: variable "tQrQ" was declared but never referenced
          detected during:
            instantiation of "void flash::compute_attn_1rowblock<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &, int, int, int) [with Kernel_traits=Flash_fwd_kernel_traits<160, 64, 64, 4, false, false, cutlass::bfloat16_t, Flash_kernel_traits<160, 64, 64, 4, cutlass::bfloat16_t>>, Is_dropout=true, Is_causal=true, Is_even_N=false, Is_even_K=false, Return_softmax=true, Params=Flash_fwd_params]" 
(588): here
            instantiation of "void flash::compute_attn<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &) [with Kernel_traits=Flash_fwd_kernel_traits<160, 64, 64, 4, false, false, cutlass::bfloat16_t, Flash_kernel_traits<160, 64, 64, 4, cutlass::bfloat16_t>>, Is_dropout=true, Is_causal=true, Is_even_N=false, Is_even_K=false, Return_softmax=true, Params=Flash_fwd_params]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(15): here
            instantiation of "void flash_fwd_kernel<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax>(Flash_fwd_params) [with Kernel_traits=Flash_fwd_kernel_traits<160, 64, 64, 4, false, false, cutlass::bfloat16_t, Flash_kernel_traits<160, 64, 64, 4, cutlass::bfloat16_t>>, Is_dropout=true, Is_causal=true, Is_even_N=false, Is_even_K=false, Return_softmax=true]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(34): here
            instantiation of "void run_flash_fwd<Kernel_traits,Is_dropout,Is_causal>(Flash_fwd_params &, cudaStream_t) [with Kernel_traits=Flash_fwd_kernel_traits<160, 64, 64, 4, false, false, cutlass::bfloat16_t, Flash_kernel_traits<160, 64, 64, 4, cutlass::bfloat16_t>>, Is_dropout=true, Is_causal=true]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(155): here
            instantiation of "void run_mha_fwd_hdim160<T>(Flash_fwd_params &, cudaStream_t) [with T=cutlass::bfloat16_t]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_hdim160_bf16_sm80.cu(16): here

/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_kernel.h(140): warning: variable "MMA_M" was declared but never referenced
          detected during:
            instantiation of "void flash::compute_attn_1rowblock<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &, int, int, int) [with Kernel_traits=Flash_fwd_kernel_traits<160, 64, 64, 4, false, false, cutlass::bfloat16_t, Flash_kernel_traits<160, 64, 64, 4, cutlass::bfloat16_t>>, Is_dropout=true, Is_causal=true, Is_even_N=false, Is_even_K=false, Return_softmax=false, Params=Flash_fwd_params]" 
(588): here
            instantiation of "void flash::compute_attn<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &) [with Kernel_traits=Flash_fwd_kernel_traits<160, 64, 64, 4, false, false, cutlass::bfloat16_t, Flash_kernel_traits<160, 64, 64, 4, cutlass::bfloat16_t>>, Is_dropout=true, Is_causal=true, Is_even_N=false, Is_even_K=false, Return_softmax=false, Params=Flash_fwd_params]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(15): here
            instantiation of "void flash_fwd_kernel<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax>(Flash_fwd_params) [with Kernel_traits=Flash_fwd_kernel_traits<160, 64, 64, 4, false, false, cutlass::bfloat16_t, Flash_kernel_traits<160, 64, 64, 4, cutlass::bfloat16_t>>, Is_dropout=true, Is_causal=true, Is_even_N=false, Is_even_K=false, Return_softmax=false]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(34): here
            instantiation of "void run_flash_fwd<Kernel_traits,Is_dropout,Is_causal>(Flash_fwd_params &, cudaStream_t) [with Kernel_traits=Flash_fwd_kernel_traits<160, 64, 64, 4, false, false, cutlass::bfloat16_t, Flash_kernel_traits<160, 64, 64, 4, cutlass::bfloat16_t>>, Is_dropout=true, Is_causal=true]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(155): here
            instantiation of "void run_mha_fwd_hdim160<T>(Flash_fwd_params &, cudaStream_t) [with T=cutlass::bfloat16_t]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_hdim160_bf16_sm80.cu(16): here

/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_kernel.h(275): warning: variable "tQrQ" was declared but never referenced
          detected during:
            instantiation of "void flash::compute_attn_1rowblock<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &, int, int, int) [with Kernel_traits=Flash_fwd_kernel_traits<160, 64, 64, 4, false, false, cutlass::bfloat16_t, Flash_kernel_traits<160, 64, 64, 4, cutlass::bfloat16_t>>, Is_dropout=true, Is_causal=true, Is_even_N=false, Is_even_K=false, Return_softmax=false, Params=Flash_fwd_params]" 
(588): here
            instantiation of "void flash::compute_attn<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &) [with Kernel_traits=Flash_fwd_kernel_traits<160, 64, 64, 4, false, false, cutlass::bfloat16_t, Flash_kernel_traits<160, 64, 64, 4, cutlass::bfloat16_t>>, Is_dropout=true, Is_causal=true, Is_even_N=false, Is_even_K=false, Return_softmax=false, Params=Flash_fwd_params]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(15): here
            instantiation of "void flash_fwd_kernel<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax>(Flash_fwd_params) [with Kernel_traits=Flash_fwd_kernel_traits<160, 64, 64, 4, false, false, cutlass::bfloat16_t, Flash_kernel_traits<160, 64, 64, 4, cutlass::bfloat16_t>>, Is_dropout=true, Is_causal=true, Is_even_N=false, Is_even_K=false, Return_softmax=false]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(34): here
            instantiation of "void run_flash_fwd<Kernel_traits,Is_dropout,Is_causal>(Flash_fwd_params &, cudaStream_t) [with Kernel_traits=Flash_fwd_kernel_traits<160, 64, 64, 4, false, false, cutlass::bfloat16_t, Flash_kernel_traits<160, 64, 64, 4, cutlass::bfloat16_t>>, Is_dropout=true, Is_causal=true]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(155): here
            instantiation of "void run_mha_fwd_hdim160<T>(Flash_fwd_params &, cudaStream_t) [with T=cutlass::bfloat16_t]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_hdim160_bf16_sm80.cu(16): here

/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_kernel.h(140): warning: variable "MMA_M" was declared but never referenced
          detected during:
            instantiation of "void flash::compute_attn_1rowblock<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &, int, int, int) [with Kernel_traits=Flash_fwd_kernel_traits<160, 128, 32, 4, false, false, cutlass::bfloat16_t, Flash_kernel_traits<160, 128, 32, 4, cutlass::bfloat16_t>>, Is_dropout=true, Is_causal=true, Is_even_N=true, Is_even_K=true, Return_softmax=true, Params=Flash_fwd_params]" 
(588): here
            instantiation of "void flash::compute_attn<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &) [with Kernel_traits=Flash_fwd_kernel_traits<160, 128, 32, 4, false, false, cutlass::bfloat16_t, Flash_kernel_traits<160, 128, 32, 4, cutlass::bfloat16_t>>, Is_dropout=true, Is_causal=true, Is_even_N=true, Is_even_K=true, Return_softmax=true, Params=Flash_fwd_params]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(15): here
            instantiation of "void flash_fwd_kernel<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax>(Flash_fwd_params) [with Kernel_traits=Flash_fwd_kernel_traits<160, 128, 32, 4, false, false, cutlass::bfloat16_t, Flash_kernel_traits<160, 128, 32, 4, cutlass::bfloat16_t>>, Is_dropout=true, Is_causal=true, Is_even_N=true, Is_even_K=true, Return_softmax=true]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(34): here
            instantiation of "void run_flash_fwd<Kernel_traits,Is_dropout,Is_causal>(Flash_fwd_params &, cudaStream_t) [with Kernel_traits=Flash_fwd_kernel_traits<160, 128, 32, 4, false, false, cutlass::bfloat16_t, Flash_kernel_traits<160, 128, 32, 4, cutlass::bfloat16_t>>, Is_dropout=true, Is_causal=true]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(155): here
            instantiation of "void run_mha_fwd_hdim160<T>(Flash_fwd_params &, cudaStream_t) [with T=cutlass::bfloat16_t]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_hdim160_bf16_sm80.cu(16): here

/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_kernel.h(275): warning: variable "tQrQ" was declared but never referenced
          detected during:
            instantiation of "void flash::compute_attn_1rowblock<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &, int, int, int) [with Kernel_traits=Flash_fwd_kernel_traits<160, 128, 32, 4, false, false, cutlass::bfloat16_t, Flash_kernel_traits<160, 128, 32, 4, cutlass::bfloat16_t>>, Is_dropout=true, Is_causal=true, Is_even_N=true, Is_even_K=true, Return_softmax=true, Params=Flash_fwd_params]" 
(588): here
            instantiation of "void flash::compute_attn<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &) [with Kernel_traits=Flash_fwd_kernel_traits<160, 128, 32, 4, false, false, cutlass::bfloat16_t, Flash_kernel_traits<160, 128, 32, 4, cutlass::bfloat16_t>>, Is_dropout=true, Is_causal=true, Is_even_N=true, Is_even_K=true, Return_softmax=true, Params=Flash_fwd_params]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(15): here
            instantiation of "void flash_fwd_kernel<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax>(Flash_fwd_params) [with Kernel_traits=Flash_fwd_kernel_traits<160, 128, 32, 4, false, false, cutlass::bfloat16_t, Flash_kernel_traits<160, 128, 32, 4, cutlass::bfloat16_t>>, Is_dropout=true, Is_causal=true, Is_even_N=true, Is_even_K=true, Return_softmax=true]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(34): here
            instantiation of "void run_flash_fwd<Kernel_traits,Is_dropout,Is_causal>(Flash_fwd_params &, cudaStream_t) [with Kernel_traits=Flash_fwd_kernel_traits<160, 128, 32, 4, false, false, cutlass::bfloat16_t, Flash_kernel_traits<160, 128, 32, 4, cutlass::bfloat16_t>>, Is_dropout=true, Is_causal=true]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(155): here
            instantiation of "void run_mha_fwd_hdim160<T>(Flash_fwd_params &, cudaStream_t) [with T=cutlass::bfloat16_t]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_hdim160_bf16_sm80.cu(16): here

/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_kernel.h(140): warning: variable "MMA_M" was declared but never referenced
          detected during:
            instantiation of "void flash::compute_attn_1rowblock<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &, int, int, int) [with Kernel_traits=Flash_fwd_kernel_traits<160, 128, 32, 4, false, false, cutlass::bfloat16_t, Flash_kernel_traits<160, 128, 32, 4, cutlass::bfloat16_t>>, Is_dropout=true, Is_causal=true, Is_even_N=true, Is_even_K=true, Return_softmax=false, Params=Flash_fwd_params]" 
(588): here
            instantiation of "void flash::compute_attn<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &) [with Kernel_traits=Flash_fwd_kernel_traits<160, 128, 32, 4, false, false, cutlass::bfloat16_t, Flash_kernel_traits<160, 128, 32, 4, cutlass::bfloat16_t>>, Is_dropout=true, Is_causal=true, Is_even_N=true, Is_even_K=true, Return_softmax=false, Params=Flash_fwd_params]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(15): here
            instantiation of "void flash_fwd_kernel<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax>(Flash_fwd_params) [with Kernel_traits=Flash_fwd_kernel_traits<160, 128, 32, 4, false, false, cutlass::bfloat16_t, Flash_kernel_traits<160, 128, 32, 4, cutlass::bfloat16_t>>, Is_dropout=true, Is_causal=true, Is_even_N=true, Is_even_K=true, Return_softmax=false]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(34): here
            instantiation of "void run_flash_fwd<Kernel_traits,Is_dropout,Is_causal>(Flash_fwd_params &, cudaStream_t) [with Kernel_traits=Flash_fwd_kernel_traits<160, 128, 32, 4, false, false, cutlass::bfloat16_t, Flash_kernel_traits<160, 128, 32, 4, cutlass::bfloat16_t>>, Is_dropout=true, Is_causal=true]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(155): here
            instantiation of "void run_mha_fwd_hdim160<T>(Flash_fwd_params &, cudaStream_t) [with T=cutlass::bfloat16_t]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_hdim160_bf16_sm80.cu(16): here

/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_kernel.h(275): warning: variable "tQrQ" was declared but never referenced
          detected during:
            instantiation of "void flash::compute_attn_1rowblock<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &, int, int, int) [with Kernel_traits=Flash_fwd_kernel_traits<160, 128, 32, 4, false, false, cutlass::bfloat16_t, Flash_kernel_traits<160, 128, 32, 4, cutlass::bfloat16_t>>, Is_dropout=true, Is_causal=true, Is_even_N=true, Is_even_K=true, Return_softmax=false, Params=Flash_fwd_params]" 
(588): here
            instantiation of "void flash::compute_attn<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &) [with Kernel_traits=Flash_fwd_kernel_traits<160, 128, 32, 4, false, false, cutlass::bfloat16_t, Flash_kernel_traits<160, 128, 32, 4, cutlass::bfloat16_t>>, Is_dropout=true, Is_causal=true, Is_even_N=true, Is_even_K=true, Return_softmax=false, Params=Flash_fwd_params]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(15): here
            instantiation of "void flash_fwd_kernel<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax>(Flash_fwd_params) [with Kernel_traits=Flash_fwd_kernel_traits<160, 128, 32, 4, false, false, cutlass::bfloat16_t, Flash_kernel_traits<160, 128, 32, 4, cutlass::bfloat16_t>>, Is_dropout=true, Is_causal=true, Is_even_N=true, Is_even_K=true, Return_softmax=false]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(34): here
            instantiation of "void run_flash_fwd<Kernel_traits,Is_dropout,Is_causal>(Flash_fwd_params &, cudaStream_t) [with Kernel_traits=Flash_fwd_kernel_traits<160, 128, 32, 4, false, false, cutlass::bfloat16_t, Flash_kernel_traits<160, 128, 32, 4, cutlass::bfloat16_t>>, Is_dropout=true, Is_causal=true]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(155): here
            instantiation of "void run_mha_fwd_hdim160<T>(Flash_fwd_params &, cudaStream_t) [with T=cutlass::bfloat16_t]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_hdim160_bf16_sm80.cu(16): here

/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_kernel.h(140): warning: variable "MMA_M" was declared but never referenced
          detected during:
            instantiation of "void flash::compute_attn_1rowblock<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &, int, int, int) [with Kernel_traits=Flash_fwd_kernel_traits<160, 128, 32, 4, false, false, cutlass::bfloat16_t, Flash_kernel_traits<160, 128, 32, 4, cutlass::bfloat16_t>>, Is_dropout=true, Is_causal=true, Is_even_N=true, Is_even_K=false, Return_softmax=true, Params=Flash_fwd_params]" 
(588): here
            instantiation of "void flash::compute_attn<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &) [with Kernel_traits=Flash_fwd_kernel_traits<160, 128, 32, 4, false, false, cutlass::bfloat16_t, Flash_kernel_traits<160, 128, 32, 4, cutlass::bfloat16_t>>, Is_dropout=true, Is_causal=true, Is_even_N=true, Is_even_K=false, Return_softmax=true, Params=Flash_fwd_params]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(15): here
            instantiation of "void flash_fwd_kernel<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax>(Flash_fwd_params) [with Kernel_traits=Flash_fwd_kernel_traits<160, 128, 32, 4, false, false, cutlass::bfloat16_t, Flash_kernel_traits<160, 128, 32, 4, cutlass::bfloat16_t>>, Is_dropout=true, Is_causal=true, Is_even_N=true, Is_even_K=false, Return_softmax=true]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(34): here
            instantiation of "void run_flash_fwd<Kernel_traits,Is_dropout,Is_causal>(Flash_fwd_params &, cudaStream_t) [with Kernel_traits=Flash_fwd_kernel_traits<160, 128, 32, 4, false, false, cutlass::bfloat16_t, Flash_kernel_traits<160, 128, 32, 4, cutlass::bfloat16_t>>, Is_dropout=true, Is_causal=true]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(155): here
            instantiation of "void run_mha_fwd_hdim160<T>(Flash_fwd_params &, cudaStream_t) [with T=cutlass::bfloat16_t]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_hdim160_bf16_sm80.cu(16): here

/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_kernel.h(275): warning: variable "tQrQ" was declared but never referenced
          detected during:
            instantiation of "void flash::compute_attn_1rowblock<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &, int, int, int) [with Kernel_traits=Flash_fwd_kernel_traits<160, 128, 32, 4, false, false, cutlass::bfloat16_t, Flash_kernel_traits<160, 128, 32, 4, cutlass::bfloat16_t>>, Is_dropout=true, Is_causal=true, Is_even_N=true, Is_even_K=false, Return_softmax=true, Params=Flash_fwd_params]" 
(588): here
            instantiation of "void flash::compute_attn<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &) [with Kernel_traits=Flash_fwd_kernel_traits<160, 128, 32, 4, false, false, cutlass::bfloat16_t, Flash_kernel_traits<160, 128, 32, 4, cutlass::bfloat16_t>>, Is_dropout=true, Is_causal=true, Is_even_N=true, Is_even_K=false, Return_softmax=true, Params=Flash_fwd_params]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(15): here
            instantiation of "void flash_fwd_kernel<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax>(Flash_fwd_params) [with Kernel_traits=Flash_fwd_kernel_traits<160, 128, 32, 4, false, false, cutlass::bfloat16_t, Flash_kernel_traits<160, 128, 32, 4, cutlass::bfloat16_t>>, Is_dropout=true, Is_causal=true, Is_even_N=true, Is_even_K=false, Return_softmax=true]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(34): here
            instantiation of "void run_flash_fwd<Kernel_traits,Is_dropout,Is_causal>(Flash_fwd_params &, cudaStream_t) [with Kernel_traits=Flash_fwd_kernel_traits<160, 128, 32, 4, false, false, cutlass::bfloat16_t, Flash_kernel_traits<160, 128, 32, 4, cutlass::bfloat16_t>>, Is_dropout=true, Is_causal=true]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(155): here
            instantiation of "void run_mha_fwd_hdim160<T>(Flash_fwd_params &, cudaStream_t) [with T=cutlass::bfloat16_t]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_hdim160_bf16_sm80.cu(16): here

/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_kernel.h(140): warning: variable "MMA_M" was declared but never referenced
          detected during:
            instantiation of "void flash::compute_attn_1rowblock<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &, int, int, int) [with Kernel_traits=Flash_fwd_kernel_traits<160, 128, 32, 4, false, false, cutlass::bfloat16_t, Flash_kernel_traits<160, 128, 32, 4, cutlass::bfloat16_t>>, Is_dropout=true, Is_causal=true, Is_even_N=true, Is_even_K=false, Return_softmax=false, Params=Flash_fwd_params]" 
(588): here
            instantiation of "void flash::compute_attn<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &) [with Kernel_traits=Flash_fwd_kernel_traits<160, 128, 32, 4, false, false, cutlass::bfloat16_t, Flash_kernel_traits<160, 128, 32, 4, cutlass::bfloat16_t>>, Is_dropout=true, Is_causal=true, Is_even_N=true, Is_even_K=false, Return_softmax=false, Params=Flash_fwd_params]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(15): here
            instantiation of "void flash_fwd_kernel<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax>(Flash_fwd_params) [with Kernel_traits=Flash_fwd_kernel_traits<160, 128, 32, 4, false, false, cutlass::bfloat16_t, Flash_kernel_traits<160, 128, 32, 4, cutlass::bfloat16_t>>, Is_dropout=true, Is_causal=true, Is_even_N=true, Is_even_K=false, Return_softmax=false]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(34): here
            instantiation of "void run_flash_fwd<Kernel_traits,Is_dropout,Is_causal>(Flash_fwd_params &, cudaStream_t) [with Kernel_traits=Flash_fwd_kernel_traits<160, 128, 32, 4, false, false, cutlass::bfloat16_t, Flash_kernel_traits<160, 128, 32, 4, cutlass::bfloat16_t>>, Is_dropout=true, Is_causal=true]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(155): here
            instantiation of "void run_mha_fwd_hdim160<T>(Flash_fwd_params &, cudaStream_t) [with T=cutlass::bfloat16_t]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_hdim160_bf16_sm80.cu(16): here

/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_kernel.h(275): warning: variable "tQrQ" was declared but never referenced
          detected during:
            instantiation of "void flash::compute_attn_1rowblock<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &, int, int, int) [with Kernel_traits=Flash_fwd_kernel_traits<160, 128, 32, 4, false, false, cutlass::bfloat16_t, Flash_kernel_traits<160, 128, 32, 4, cutlass::bfloat16_t>>, Is_dropout=true, Is_causal=true, Is_even_N=true, Is_even_K=false, Return_softmax=false, Params=Flash_fwd_params]" 
(588): here
            instantiation of "void flash::compute_attn<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &) [with Kernel_traits=Flash_fwd_kernel_traits<160, 128, 32, 4, false, false, cutlass::bfloat16_t, Flash_kernel_traits<160, 128, 32, 4, cutlass::bfloat16_t>>, Is_dropout=true, Is_causal=true, Is_even_N=true, Is_even_K=false, Return_softmax=false, Params=Flash_fwd_params]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(15): here
            instantiation of "void flash_fwd_kernel<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax>(Flash_fwd_params) [with Kernel_traits=Flash_fwd_kernel_traits<160, 128, 32, 4, false, false, cutlass::bfloat16_t, Flash_kernel_traits<160, 128, 32, 4, cutlass::bfloat16_t>>, Is_dropout=true, Is_causal=true, Is_even_N=true, Is_even_K=false, Return_softmax=false]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(34): here
            instantiation of "void run_flash_fwd<Kernel_traits,Is_dropout,Is_causal>(Flash_fwd_params &, cudaStream_t) [with Kernel_traits=Flash_fwd_kernel_traits<160, 128, 32, 4, false, false, cutlass::bfloat16_t, Flash_kernel_traits<160, 128, 32, 4, cutlass::bfloat16_t>>, Is_dropout=true, Is_causal=true]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(155): here
            instantiation of "void run_mha_fwd_hdim160<T>(Flash_fwd_params &, cudaStream_t) [with T=cutlass::bfloat16_t]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_hdim160_bf16_sm80.cu(16): here

/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_kernel.h(140): warning: variable "MMA_M" was declared but never referenced
          detected during:
            instantiation of "void flash::compute_attn_1rowblock<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &, int, int, int) [with Kernel_traits=Flash_fwd_kernel_traits<160, 128, 32, 4, false, false, cutlass::bfloat16_t, Flash_kernel_traits<160, 128, 32, 4, cutlass::bfloat16_t>>, Is_dropout=true, Is_causal=true, Is_even_N=false, Is_even_K=true, Return_softmax=true, Params=Flash_fwd_params]" 
(588): here
            instantiation of "void flash::compute_attn<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &) [with Kernel_traits=Flash_fwd_kernel_traits<160, 128, 32, 4, false, false, cutlass::bfloat16_t, Flash_kernel_traits<160, 128, 32, 4, cutlass::bfloat16_t>>, Is_dropout=true, Is_causal=true, Is_even_N=false, Is_even_K=true, Return_softmax=true, Params=Flash_fwd_params]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(15): here
            instantiation of "void flash_fwd_kernel<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax>(Flash_fwd_params) [with Kernel_traits=Flash_fwd_kernel_traits<160, 128, 32, 4, false, false, cutlass::bfloat16_t, Flash_kernel_traits<160, 128, 32, 4, cutlass::bfloat16_t>>, Is_dropout=true, Is_causal=true, Is_even_N=false, Is_even_K=true, Return_softmax=true]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(34): here
            instantiation of "void run_flash_fwd<Kernel_traits,Is_dropout,Is_causal>(Flash_fwd_params &, cudaStream_t) [with Kernel_traits=Flash_fwd_kernel_traits<160, 128, 32, 4, false, false, cutlass::bfloat16_t, Flash_kernel_traits<160, 128, 32, 4, cutlass::bfloat16_t>>, Is_dropout=true, Is_causal=true]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(155): here
            instantiation of "void run_mha_fwd_hdim160<T>(Flash_fwd_params &, cudaStream_t) [with T=cutlass::bfloat16_t]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_hdim160_bf16_sm80.cu(16): here

/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_kernel.h(275): warning: variable "tQrQ" was declared but never referenced
          detected during:
            instantiation of "void flash::compute_attn_1rowblock<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &, int, int, int) [with Kernel_traits=Flash_fwd_kernel_traits<160, 128, 32, 4, false, false, cutlass::bfloat16_t, Flash_kernel_traits<160, 128, 32, 4, cutlass::bfloat16_t>>, Is_dropout=true, Is_causal=true, Is_even_N=false, Is_even_K=true, Return_softmax=true, Params=Flash_fwd_params]" 
(588): here
            instantiation of "void flash::compute_attn<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &) [with Kernel_traits=Flash_fwd_kernel_traits<160, 128, 32, 4, false, false, cutlass::bfloat16_t, Flash_kernel_traits<160, 128, 32, 4, cutlass::bfloat16_t>>, Is_dropout=true, Is_causal=true, Is_even_N=false, Is_even_K=true, Return_softmax=true, Params=Flash_fwd_params]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(15): here
            instantiation of "void flash_fwd_kernel<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax>(Flash_fwd_params) [with Kernel_traits=Flash_fwd_kernel_traits<160, 128, 32, 4, false, false, cutlass::bfloat16_t, Flash_kernel_traits<160, 128, 32, 4, cutlass::bfloat16_t>>, Is_dropout=true, Is_causal=true, Is_even_N=false, Is_even_K=true, Return_softmax=true]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(34): here
            instantiation of "void run_flash_fwd<Kernel_traits,Is_dropout,Is_causal>(Flash_fwd_params &, cudaStream_t) [with Kernel_traits=Flash_fwd_kernel_traits<160, 128, 32, 4, false, false, cutlass::bfloat16_t, Flash_kernel_traits<160, 128, 32, 4, cutlass::bfloat16_t>>, Is_dropout=true, Is_causal=true]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(155): here
            instantiation of "void run_mha_fwd_hdim160<T>(Flash_fwd_params &, cudaStream_t) [with T=cutlass::bfloat16_t]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_hdim160_bf16_sm80.cu(16): here

/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_kernel.h(140): warning: variable "MMA_M" was declared but never referenced
          detected during:
            instantiation of "void flash::compute_attn_1rowblock<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &, int, int, int) [with Kernel_traits=Flash_fwd_kernel_traits<160, 128, 32, 4, false, false, cutlass::bfloat16_t, Flash_kernel_traits<160, 128, 32, 4, cutlass::bfloat16_t>>, Is_dropout=true, Is_causal=true, Is_even_N=false, Is_even_K=true, Return_softmax=false, Params=Flash_fwd_params]" 
(588): here
            instantiation of "void flash::compute_attn<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &) [with Kernel_traits=Flash_fwd_kernel_traits<160, 128, 32, 4, false, false, cutlass::bfloat16_t, Flash_kernel_traits<160, 128, 32, 4, cutlass::bfloat16_t>>, Is_dropout=true, Is_causal=true, Is_even_N=false, Is_even_K=true, Return_softmax=false, Params=Flash_fwd_params]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(15): here
            instantiation of "void flash_fwd_kernel<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax>(Flash_fwd_params) [with Kernel_traits=Flash_fwd_kernel_traits<160, 128, 32, 4, false, false, cutlass::bfloat16_t, Flash_kernel_traits<160, 128, 32, 4, cutlass::bfloat16_t>>, Is_dropout=true, Is_causal=true, Is_even_N=false, Is_even_K=true, Return_softmax=false]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(34): here
            instantiation of "void run_flash_fwd<Kernel_traits,Is_dropout,Is_causal>(Flash_fwd_params &, cudaStream_t) [with Kernel_traits=Flash_fwd_kernel_traits<160, 128, 32, 4, false, false, cutlass::bfloat16_t, Flash_kernel_traits<160, 128, 32, 4, cutlass::bfloat16_t>>, Is_dropout=true, Is_causal=true]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(155): here
            instantiation of "void run_mha_fwd_hdim160<T>(Flash_fwd_params &, cudaStream_t) [with T=cutlass::bfloat16_t]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_hdim160_bf16_sm80.cu(16): here

/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_kernel.h(275): warning: variable "tQrQ" was declared but never referenced
          detected during:
            instantiation of "void flash::compute_attn_1rowblock<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &, int, int, int) [with Kernel_traits=Flash_fwd_kernel_traits<160, 128, 32, 4, false, false, cutlass::bfloat16_t, Flash_kernel_traits<160, 128, 32, 4, cutlass::bfloat16_t>>, Is_dropout=true, Is_causal=true, Is_even_N=false, Is_even_K=true, Return_softmax=false, Params=Flash_fwd_params]" 
(588): here
            instantiation of "void flash::compute_attn<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &) [with Kernel_traits=Flash_fwd_kernel_traits<160, 128, 32, 4, false, false, cutlass::bfloat16_t, Flash_kernel_traits<160, 128, 32, 4, cutlass::bfloat16_t>>, Is_dropout=true, Is_causal=true, Is_even_N=false, Is_even_K=true, Return_softmax=false, Params=Flash_fwd_params]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(15): here
            instantiation of "void flash_fwd_kernel<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax>(Flash_fwd_params) [with Kernel_traits=Flash_fwd_kernel_traits<160, 128, 32, 4, false, false, cutlass::bfloat16_t, Flash_kernel_traits<160, 128, 32, 4, cutlass::bfloat16_t>>, Is_dropout=true, Is_causal=true, Is_even_N=false, Is_even_K=true, Return_softmax=false]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(34): here
            instantiation of "void run_flash_fwd<Kernel_traits,Is_dropout,Is_causal>(Flash_fwd_params &, cudaStream_t) [with Kernel_traits=Flash_fwd_kernel_traits<160, 128, 32, 4, false, false, cutlass::bfloat16_t, Flash_kernel_traits<160, 128, 32, 4, cutlass::bfloat16_t>>, Is_dropout=true, Is_causal=true]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(155): here
            instantiation of "void run_mha_fwd_hdim160<T>(Flash_fwd_params &, cudaStream_t) [with T=cutlass::bfloat16_t]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_hdim160_bf16_sm80.cu(16): here

/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_kernel.h(140): warning: variable "MMA_M" was declared but never referenced
          detected during:
            instantiation of "void flash::compute_attn_1rowblock<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &, int, int, int) [with Kernel_traits=Flash_fwd_kernel_traits<160, 128, 32, 4, false, false, cutlass::bfloat16_t, Flash_kernel_traits<160, 128, 32, 4, cutlass::bfloat16_t>>, Is_dropout=true, Is_causal=true, Is_even_N=false, Is_even_K=false, Return_softmax=true, Params=Flash_fwd_params]" 
(588): here
            instantiation of "void flash::compute_attn<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &) [with Kernel_traits=Flash_fwd_kernel_traits<160, 128, 32, 4, false, false, cutlass::bfloat16_t, Flash_kernel_traits<160, 128, 32, 4, cutlass::bfloat16_t>>, Is_dropout=true, Is_causal=true, Is_even_N=false, Is_even_K=false, Return_softmax=true, Params=Flash_fwd_params]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(15): here
            instantiation of "void flash_fwd_kernel<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax>(Flash_fwd_params) [with Kernel_traits=Flash_fwd_kernel_traits<160, 128, 32, 4, false, false, cutlass::bfloat16_t, Flash_kernel_traits<160, 128, 32, 4, cutlass::bfloat16_t>>, Is_dropout=true, Is_causal=true, Is_even_N=false, Is_even_K=false, Return_softmax=true]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(34): here
            instantiation of "void run_flash_fwd<Kernel_traits,Is_dropout,Is_causal>(Flash_fwd_params &, cudaStream_t) [with Kernel_traits=Flash_fwd_kernel_traits<160, 128, 32, 4, false, false, cutlass::bfloat16_t, Flash_kernel_traits<160, 128, 32, 4, cutlass::bfloat16_t>>, Is_dropout=true, Is_causal=true]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(155): here
            instantiation of "void run_mha_fwd_hdim160<T>(Flash_fwd_params &, cudaStream_t) [with T=cutlass::bfloat16_t]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_hdim160_bf16_sm80.cu(16): here

/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_kernel.h(275): warning: variable "tQrQ" was declared but never referenced
          detected during:
            instantiation of "void flash::compute_attn_1rowblock<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &, int, int, int) [with Kernel_traits=Flash_fwd_kernel_traits<160, 128, 32, 4, false, false, cutlass::bfloat16_t, Flash_kernel_traits<160, 128, 32, 4, cutlass::bfloat16_t>>, Is_dropout=true, Is_causal=true, Is_even_N=false, Is_even_K=false, Return_softmax=true, Params=Flash_fwd_params]" 
(588): here
            instantiation of "void flash::compute_attn<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &) [with Kernel_traits=Flash_fwd_kernel_traits<160, 128, 32, 4, false, false, cutlass::bfloat16_t, Flash_kernel_traits<160, 128, 32, 4, cutlass::bfloat16_t>>, Is_dropout=true, Is_causal=true, Is_even_N=false, Is_even_K=false, Return_softmax=true, Params=Flash_fwd_params]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(15): here
            instantiation of "void flash_fwd_kernel<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax>(Flash_fwd_params) [with Kernel_traits=Flash_fwd_kernel_traits<160, 128, 32, 4, false, false, cutlass::bfloat16_t, Flash_kernel_traits<160, 128, 32, 4, cutlass::bfloat16_t>>, Is_dropout=true, Is_causal=true, Is_even_N=false, Is_even_K=false, Return_softmax=true]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(34): here
            instantiation of "void run_flash_fwd<Kernel_traits,Is_dropout,Is_causal>(Flash_fwd_params &, cudaStream_t) [with Kernel_traits=Flash_fwd_kernel_traits<160, 128, 32, 4, false, false, cutlass::bfloat16_t, Flash_kernel_traits<160, 128, 32, 4, cutlass::bfloat16_t>>, Is_dropout=true, Is_causal=true]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(155): here
            instantiation of "void run_mha_fwd_hdim160<T>(Flash_fwd_params &, cudaStream_t) [with T=cutlass::bfloat16_t]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_hdim160_bf16_sm80.cu(16): here

/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_kernel.h(140): warning: variable "MMA_M" was declared but never referenced
          detected during:
            instantiation of "void flash::compute_attn_1rowblock<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &, int, int, int) [with Kernel_traits=Flash_fwd_kernel_traits<160, 128, 32, 4, false, false, cutlass::bfloat16_t, Flash_kernel_traits<160, 128, 32, 4, cutlass::bfloat16_t>>, Is_dropout=true, Is_causal=true, Is_even_N=false, Is_even_K=false, Return_softmax=false, Params=Flash_fwd_params]" 
(588): here
            instantiation of "void flash::compute_attn<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &) [with Kernel_traits=Flash_fwd_kernel_traits<160, 128, 32, 4, false, false, cutlass::bfloat16_t, Flash_kernel_traits<160, 128, 32, 4, cutlass::bfloat16_t>>, Is_dropout=true, Is_causal=true, Is_even_N=false, Is_even_K=false, Return_softmax=false, Params=Flash_fwd_params]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(15): here
            instantiation of "void flash_fwd_kernel<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax>(Flash_fwd_params) [with Kernel_traits=Flash_fwd_kernel_traits<160, 128, 32, 4, false, false, cutlass::bfloat16_t, Flash_kernel_traits<160, 128, 32, 4, cutlass::bfloat16_t>>, Is_dropout=true, Is_causal=true, Is_even_N=false, Is_even_K=false, Return_softmax=false]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(34): here
            instantiation of "void run_flash_fwd<Kernel_traits,Is_dropout,Is_causal>(Flash_fwd_params &, cudaStream_t) [with Kernel_traits=Flash_fwd_kernel_traits<160, 128, 32, 4, false, false, cutlass::bfloat16_t, Flash_kernel_traits<160, 128, 32, 4, cutlass::bfloat16_t>>, Is_dropout=true, Is_causal=true]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(155): here
            instantiation of "void run_mha_fwd_hdim160<T>(Flash_fwd_params &, cudaStream_t) [with T=cutlass::bfloat16_t]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_hdim160_bf16_sm80.cu(16): here

/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_kernel.h(275): warning: variable "tQrQ" was declared but never referenced
          detected during:
            instantiation of "void flash::compute_attn_1rowblock<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &, int, int, int) [with Kernel_traits=Flash_fwd_kernel_traits<160, 128, 32, 4, false, false, cutlass::bfloat16_t, Flash_kernel_traits<160, 128, 32, 4, cutlass::bfloat16_t>>, Is_dropout=true, Is_causal=true, Is_even_N=false, Is_even_K=false, Return_softmax=false, Params=Flash_fwd_params]" 
(588): here
            instantiation of "void flash::compute_attn<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &) [with Kernel_traits=Flash_fwd_kernel_traits<160, 128, 32, 4, false, false, cutlass::bfloat16_t, Flash_kernel_traits<160, 128, 32, 4, cutlass::bfloat16_t>>, Is_dropout=true, Is_causal=true, Is_even_N=false, Is_even_K=false, Return_softmax=false, Params=Flash_fwd_params]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(15): here
            instantiation of "void flash_fwd_kernel<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax>(Flash_fwd_params) [with Kernel_traits=Flash_fwd_kernel_traits<160, 128, 32, 4, false, false, cutlass::bfloat16_t, Flash_kernel_traits<160, 128, 32, 4, cutlass::bfloat16_t>>, Is_dropout=true, Is_causal=true, Is_even_N=false, Is_even_K=false, Return_softmax=false]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(34): here
            instantiation of "void run_flash_fwd<Kernel_traits,Is_dropout,Is_causal>(Flash_fwd_params &, cudaStream_t) [with Kernel_traits=Flash_fwd_kernel_traits<160, 128, 32, 4, false, false, cutlass::bfloat16_t, Flash_kernel_traits<160, 128, 32, 4, cutlass::bfloat16_t>>, Is_dropout=true, Is_causal=true]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(155): here
            instantiation of "void run_mha_fwd_hdim160<T>(Flash_fwd_params &, cudaStream_t) [with T=cutlass::bfloat16_t]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_hdim160_bf16_sm80.cu(16): here

/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_kernel.h(140): warning: variable "MMA_M" was declared but never referenced
          detected during:
            instantiation of "void flash::compute_attn_1rowblock<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &, int, int, int) [with Kernel_traits=Flash_fwd_kernel_traits<160, 128, 64, 8, false, false, cutlass::bfloat16_t, Flash_kernel_traits<160, 128, 64, 8, cutlass::bfloat16_t>>, Is_dropout=true, Is_causal=false, Is_even_N=true, Is_even_K=true, Return_softmax=true, Params=Flash_fwd_params]" 
(588): here
            instantiation of "void flash::compute_attn<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &) [with Kernel_traits=Flash_fwd_kernel_traits<160, 128, 64, 8, false, false, cutlass::bfloat16_t, Flash_kernel_traits<160, 128, 64, 8, cutlass::bfloat16_t>>, Is_dropout=true, Is_causal=false, Is_even_N=true, Is_even_K=true, Return_softmax=true, Params=Flash_fwd_params]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(15): here
            instantiation of "void flash_fwd_kernel<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax>(Flash_fwd_params) [with Kernel_traits=Flash_fwd_kernel_traits<160, 128, 64, 8, false, false, cutlass::bfloat16_t, Flash_kernel_traits<160, 128, 64, 8, cutlass::bfloat16_t>>, Is_dropout=true, Is_causal=false, Is_even_N=true, Is_even_K=true, Return_softmax=true]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(34): here
            instantiation of "void run_flash_fwd<Kernel_traits,Is_dropout,Is_causal>(Flash_fwd_params &, cudaStream_t) [with Kernel_traits=Flash_fwd_kernel_traits<160, 128, 64, 8, false, false, cutlass::bfloat16_t, Flash_kernel_traits<160, 128, 64, 8, cutlass::bfloat16_t>>, Is_dropout=true, Is_causal=false]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(155): here
            instantiation of "void run_mha_fwd_hdim160<T>(Flash_fwd_params &, cudaStream_t) [with T=cutlass::bfloat16_t]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_hdim160_bf16_sm80.cu(16): here

/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_kernel.h(275): warning: variable "tQrQ" was declared but never referenced
          detected during:
            instantiation of "void flash::compute_attn_1rowblock<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &, int, int, int) [with Kernel_traits=Flash_fwd_kernel_traits<160, 128, 64, 8, false, false, cutlass::bfloat16_t, Flash_kernel_traits<160, 128, 64, 8, cutlass::bfloat16_t>>, Is_dropout=true, Is_causal=false, Is_even_N=true, Is_even_K=true, Return_softmax=true, Params=Flash_fwd_params]" 
(588): here
            instantiation of "void flash::compute_attn<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &) [with Kernel_traits=Flash_fwd_kernel_traits<160, 128, 64, 8, false, false, cutlass::bfloat16_t, Flash_kernel_traits<160, 128, 64, 8, cutlass::bfloat16_t>>, Is_dropout=true, Is_causal=false, Is_even_N=true, Is_even_K=true, Return_softmax=true, Params=Flash_fwd_params]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(15): here
            instantiation of "void flash_fwd_kernel<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax>(Flash_fwd_params) [with Kernel_traits=Flash_fwd_kernel_traits<160, 128, 64, 8, false, false, cutlass::bfloat16_t, Flash_kernel_traits<160, 128, 64, 8, cutlass::bfloat16_t>>, Is_dropout=true, Is_causal=false, Is_even_N=true, Is_even_K=true, Return_softmax=true]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(34): here
            instantiation of "void run_flash_fwd<Kernel_traits,Is_dropout,Is_causal>(Flash_fwd_params &, cudaStream_t) [with Kernel_traits=Flash_fwd_kernel_traits<160, 128, 64, 8, false, false, cutlass::bfloat16_t, Flash_kernel_traits<160, 128, 64, 8, cutlass::bfloat16_t>>, Is_dropout=true, Is_causal=false]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(155): here
            instantiation of "void run_mha_fwd_hdim160<T>(Flash_fwd_params &, cudaStream_t) [with T=cutlass::bfloat16_t]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_hdim160_bf16_sm80.cu(16): here

/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_kernel.h(140): warning: variable "MMA_M" was declared but never referenced
          detected during:
            instantiation of "void flash::compute_attn_1rowblock<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &, int, int, int) [with Kernel_traits=Flash_fwd_kernel_traits<160, 128, 64, 8, false, false, cutlass::bfloat16_t, Flash_kernel_traits<160, 128, 64, 8, cutlass::bfloat16_t>>, Is_dropout=true, Is_causal=false, Is_even_N=true, Is_even_K=true, Return_softmax=false, Params=Flash_fwd_params]" 
(588): here
            instantiation of "void flash::compute_attn<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &) [with Kernel_traits=Flash_fwd_kernel_traits<160, 128, 64, 8, false, false, cutlass::bfloat16_t, Flash_kernel_traits<160, 128, 64, 8, cutlass::bfloat16_t>>, Is_dropout=true, Is_causal=false, Is_even_N=true, Is_even_K=true, Return_softmax=false, Params=Flash_fwd_params]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(15): here
            instantiation of "void flash_fwd_kernel<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax>(Flash_fwd_params) [with Kernel_traits=Flash_fwd_kernel_traits<160, 128, 64, 8, false, false, cutlass::bfloat16_t, Flash_kernel_traits<160, 128, 64, 8, cutlass::bfloat16_t>>, Is_dropout=true, Is_causal=false, Is_even_N=true, Is_even_K=true, Return_softmax=false]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(34): here
            instantiation of "void run_flash_fwd<Kernel_traits,Is_dropout,Is_causal>(Flash_fwd_params &, cudaStream_t) [with Kernel_traits=Flash_fwd_kernel_traits<160, 128, 64, 8, false, false, cutlass::bfloat16_t, Flash_kernel_traits<160, 128, 64, 8, cutlass::bfloat16_t>>, Is_dropout=true, Is_causal=false]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(155): here
            instantiation of "void run_mha_fwd_hdim160<T>(Flash_fwd_params &, cudaStream_t) [with T=cutlass::bfloat16_t]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_hdim160_bf16_sm80.cu(16): here

/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_kernel.h(275): warning: variable "tQrQ" was declared but never referenced
          detected during:
            instantiation of "void flash::compute_attn_1rowblock<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &, int, int, int) [with Kernel_traits=Flash_fwd_kernel_traits<160, 128, 64, 8, false, false, cutlass::bfloat16_t, Flash_kernel_traits<160, 128, 64, 8, cutlass::bfloat16_t>>, Is_dropout=true, Is_causal=false, Is_even_N=true, Is_even_K=true, Return_softmax=false, Params=Flash_fwd_params]" 
(588): here
            instantiation of "void flash::compute_attn<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &) [with Kernel_traits=Flash_fwd_kernel_traits<160, 128, 64, 8, false, false, cutlass::bfloat16_t, Flash_kernel_traits<160, 128, 64, 8, cutlass::bfloat16_t>>, Is_dropout=true, Is_causal=false, Is_even_N=true, Is_even_K=true, Return_softmax=false, Params=Flash_fwd_params]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(15): here
            instantiation of "void flash_fwd_kernel<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax>(Flash_fwd_params) [with Kernel_traits=Flash_fwd_kernel_traits<160, 128, 64, 8, false, false, cutlass::bfloat16_t, Flash_kernel_traits<160, 128, 64, 8, cutlass::bfloat16_t>>, Is_dropout=true, Is_causal=false, Is_even_N=true, Is_even_K=true, Return_softmax=false]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(34): here
            instantiation of "void run_flash_fwd<Kernel_traits,Is_dropout,Is_causal>(Flash_fwd_params &, cudaStream_t) [with Kernel_traits=Flash_fwd_kernel_traits<160, 128, 64, 8, false, false, cutlass::bfloat16_t, Flash_kernel_traits<160, 128, 64, 8, cutlass::bfloat16_t>>, Is_dropout=true, Is_causal=false]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(155): here
            instantiation of "void run_mha_fwd_hdim160<T>(Flash_fwd_params &, cudaStream_t) [with T=cutlass::bfloat16_t]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_hdim160_bf16_sm80.cu(16): here

/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_kernel.h(140): warning: variable "MMA_M" was declared but never referenced
          detected during:
            instantiation of "void flash::compute_attn_1rowblock<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &, int, int, int) [with Kernel_traits=Flash_fwd_kernel_traits<160, 128, 64, 8, false, false, cutlass::bfloat16_t, Flash_kernel_traits<160, 128, 64, 8, cutlass::bfloat16_t>>, Is_dropout=true, Is_causal=false, Is_even_N=true, Is_even_K=false, Return_softmax=true, Params=Flash_fwd_params]" 
(588): here
            instantiation of "void flash::compute_attn<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &) [with Kernel_traits=Flash_fwd_kernel_traits<160, 128, 64, 8, false, false, cutlass::bfloat16_t, Flash_kernel_traits<160, 128, 64, 8, cutlass::bfloat16_t>>, Is_dropout=true, Is_causal=false, Is_even_N=true, Is_even_K=false, Return_softmax=true, Params=Flash_fwd_params]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(15): here
            instantiation of "void flash_fwd_kernel<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax>(Flash_fwd_params) [with Kernel_traits=Flash_fwd_kernel_traits<160, 128, 64, 8, false, false, cutlass::bfloat16_t, Flash_kernel_traits<160, 128, 64, 8, cutlass::bfloat16_t>>, Is_dropout=true, Is_causal=false, Is_even_N=true, Is_even_K=false, Return_softmax=true]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(34): here
            instantiation of "void run_flash_fwd<Kernel_traits,Is_dropout,Is_causal>(Flash_fwd_params &, cudaStream_t) [with Kernel_traits=Flash_fwd_kernel_traits<160, 128, 64, 8, false, false, cutlass::bfloat16_t, Flash_kernel_traits<160, 128, 64, 8, cutlass::bfloat16_t>>, Is_dropout=true, Is_causal=false]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(155): here
            instantiation of "void run_mha_fwd_hdim160<T>(Flash_fwd_params &, cudaStream_t) [with T=cutlass::bfloat16_t]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_hdim160_bf16_sm80.cu(16): here

/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_kernel.h(275): warning: variable "tQrQ" was declared but never referenced
          detected during:
            instantiation of "void flash::compute_attn_1rowblock<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &, int, int, int) [with Kernel_traits=Flash_fwd_kernel_traits<160, 128, 64, 8, false, false, cutlass::bfloat16_t, Flash_kernel_traits<160, 128, 64, 8, cutlass::bfloat16_t>>, Is_dropout=true, Is_causal=false, Is_even_N=true, Is_even_K=false, Return_softmax=true, Params=Flash_fwd_params]" 
(588): here
            instantiation of "void flash::compute_attn<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &) [with Kernel_traits=Flash_fwd_kernel_traits<160, 128, 64, 8, false, false, cutlass::bfloat16_t, Flash_kernel_traits<160, 128, 64, 8, cutlass::bfloat16_t>>, Is_dropout=true, Is_causal=false, Is_even_N=true, Is_even_K=false, Return_softmax=true, Params=Flash_fwd_params]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(15): here
            instantiation of "void flash_fwd_kernel<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax>(Flash_fwd_params) [with Kernel_traits=Flash_fwd_kernel_traits<160, 128, 64, 8, false, false, cutlass::bfloat16_t, Flash_kernel_traits<160, 128, 64, 8, cutlass::bfloat16_t>>, Is_dropout=true, Is_causal=false, Is_even_N=true, Is_even_K=false, Return_softmax=true]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(34): here
            instantiation of "void run_flash_fwd<Kernel_traits,Is_dropout,Is_causal>(Flash_fwd_params &, cudaStream_t) [with Kernel_traits=Flash_fwd_kernel_traits<160, 128, 64, 8, false, false, cutlass::bfloat16_t, Flash_kernel_traits<160, 128, 64, 8, cutlass::bfloat16_t>>, Is_dropout=true, Is_causal=false]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(155): here
            instantiation of "void run_mha_fwd_hdim160<T>(Flash_fwd_params &, cudaStream_t) [with T=cutlass::bfloat16_t]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_hdim160_bf16_sm80.cu(16): here

/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_kernel.h(140): warning: variable "MMA_M" was declared but never referenced
          detected during:
            instantiation of "void flash::compute_attn_1rowblock<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &, int, int, int) [with Kernel_traits=Flash_fwd_kernel_traits<160, 128, 64, 8, false, false, cutlass::bfloat16_t, Flash_kernel_traits<160, 128, 64, 8, cutlass::bfloat16_t>>, Is_dropout=true, Is_causal=false, Is_even_N=true, Is_even_K=false, Return_softmax=false, Params=Flash_fwd_params]" 
(588): here
            instantiation of "void flash::compute_attn<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &) [with Kernel_traits=Flash_fwd_kernel_traits<160, 128, 64, 8, false, false, cutlass::bfloat16_t, Flash_kernel_traits<160, 128, 64, 8, cutlass::bfloat16_t>>, Is_dropout=true, Is_causal=false, Is_even_N=true, Is_even_K=false, Return_softmax=false, Params=Flash_fwd_params]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(15): here
            instantiation of "void flash_fwd_kernel<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax>(Flash_fwd_params) [with Kernel_traits=Flash_fwd_kernel_traits<160, 128, 64, 8, false, false, cutlass::bfloat16_t, Flash_kernel_traits<160, 128, 64, 8, cutlass::bfloat16_t>>, Is_dropout=true, Is_causal=false, Is_even_N=true, Is_even_K=false, Return_softmax=false]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(34): here
            instantiation of "void run_flash_fwd<Kernel_traits,Is_dropout,Is_causal>(Flash_fwd_params &, cudaStream_t) [with Kernel_traits=Flash_fwd_kernel_traits<160, 128, 64, 8, false, false, cutlass::bfloat16_t, Flash_kernel_traits<160, 128, 64, 8, cutlass::bfloat16_t>>, Is_dropout=true, Is_causal=false]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(155): here
            instantiation of "void run_mha_fwd_hdim160<T>(Flash_fwd_params &, cudaStream_t) [with T=cutlass::bfloat16_t]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_hdim160_bf16_sm80.cu(16): here

/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_kernel.h(275): warning: variable "tQrQ" was declared but never referenced
          detected during:
            instantiation of "void flash::compute_attn_1rowblock<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &, int, int, int) [with Kernel_traits=Flash_fwd_kernel_traits<160, 128, 64, 8, false, false, cutlass::bfloat16_t, Flash_kernel_traits<160, 128, 64, 8, cutlass::bfloat16_t>>, Is_dropout=true, Is_causal=false, Is_even_N=true, Is_even_K=false, Return_softmax=false, Params=Flash_fwd_params]" 
(588): here
            instantiation of "void flash::compute_attn<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &) [with Kernel_traits=Flash_fwd_kernel_traits<160, 128, 64, 8, false, false, cutlass::bfloat16_t, Flash_kernel_traits<160, 128, 64, 8, cutlass::bfloat16_t>>, Is_dropout=true, Is_causal=false, Is_even_N=true, Is_even_K=false, Return_softmax=false, Params=Flash_fwd_params]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(15): here
            instantiation of "void flash_fwd_kernel<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax>(Flash_fwd_params) [with Kernel_traits=Flash_fwd_kernel_traits<160, 128, 64, 8, false, false, cutlass::bfloat16_t, Flash_kernel_traits<160, 128, 64, 8, cutlass::bfloat16_t>>, Is_dropout=true, Is_causal=false, Is_even_N=true, Is_even_K=false, Return_softmax=false]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(34): here
            instantiation of "void run_flash_fwd<Kernel_traits,Is_dropout,Is_causal>(Flash_fwd_params &, cudaStream_t) [with Kernel_traits=Flash_fwd_kernel_traits<160, 128, 64, 8, false, false, cutlass::bfloat16_t, Flash_kernel_traits<160, 128, 64, 8, cutlass::bfloat16_t>>, Is_dropout=true, Is_causal=false]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(155): here
            instantiation of "void run_mha_fwd_hdim160<T>(Flash_fwd_params &, cudaStream_t) [with T=cutlass::bfloat16_t]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_hdim160_bf16_sm80.cu(16): here

/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_kernel.h(140): warning: variable "MMA_M" was declared but never referenced
          detected during:
            instantiation of "void flash::compute_attn_1rowblock<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &, int, int, int) [with Kernel_traits=Flash_fwd_kernel_traits<160, 128, 64, 8, false, false, cutlass::bfloat16_t, Flash_kernel_traits<160, 128, 64, 8, cutlass::bfloat16_t>>, Is_dropout=true, Is_causal=false, Is_even_N=false, Is_even_K=true, Return_softmax=true, Params=Flash_fwd_params]" 
(588): here
            instantiation of "void flash::compute_attn<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &) [with Kernel_traits=Flash_fwd_kernel_traits<160, 128, 64, 8, false, false, cutlass::bfloat16_t, Flash_kernel_traits<160, 128, 64, 8, cutlass::bfloat16_t>>, Is_dropout=true, Is_causal=false, Is_even_N=false, Is_even_K=true, Return_softmax=true, Params=Flash_fwd_params]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(15): here
            instantiation of "void flash_fwd_kernel<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax>(Flash_fwd_params) [with Kernel_traits=Flash_fwd_kernel_traits<160, 128, 64, 8, false, false, cutlass::bfloat16_t, Flash_kernel_traits<160, 128, 64, 8, cutlass::bfloat16_t>>, Is_dropout=true, Is_causal=false, Is_even_N=false, Is_even_K=true, Return_softmax=true]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(34): here
            instantiation of "void run_flash_fwd<Kernel_traits,Is_dropout,Is_causal>(Flash_fwd_params &, cudaStream_t) [with Kernel_traits=Flash_fwd_kernel_traits<160, 128, 64, 8, false, false, cutlass::bfloat16_t, Flash_kernel_traits<160, 128, 64, 8, cutlass::bfloat16_t>>, Is_dropout=true, Is_causal=false]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(155): here
            instantiation of "void run_mha_fwd_hdim160<T>(Flash_fwd_params &, cudaStream_t) [with T=cutlass::bfloat16_t]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_hdim160_bf16_sm80.cu(16): here

/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_kernel.h(275): warning: variable "tQrQ" was declared but never referenced
          detected during:
            instantiation of "void flash::compute_attn_1rowblock<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &, int, int, int) [with Kernel_traits=Flash_fwd_kernel_traits<160, 128, 64, 8, false, false, cutlass::bfloat16_t, Flash_kernel_traits<160, 128, 64, 8, cutlass::bfloat16_t>>, Is_dropout=true, Is_causal=false, Is_even_N=false, Is_even_K=true, Return_softmax=true, Params=Flash_fwd_params]" 
(588): here
            instantiation of "void flash::compute_attn<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &) [with Kernel_traits=Flash_fwd_kernel_traits<160, 128, 64, 8, false, false, cutlass::bfloat16_t, Flash_kernel_traits<160, 128, 64, 8, cutlass::bfloat16_t>>, Is_dropout=true, Is_causal=false, Is_even_N=false, Is_even_K=true, Return_softmax=true, Params=Flash_fwd_params]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(15): here
            instantiation of "void flash_fwd_kernel<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax>(Flash_fwd_params) [with Kernel_traits=Flash_fwd_kernel_traits<160, 128, 64, 8, false, false, cutlass::bfloat16_t, Flash_kernel_traits<160, 128, 64, 8, cutlass::bfloat16_t>>, Is_dropout=true, Is_causal=false, Is_even_N=false, Is_even_K=true, Return_softmax=true]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(34): here
            instantiation of "void run_flash_fwd<Kernel_traits,Is_dropout,Is_causal>(Flash_fwd_params &, cudaStream_t) [with Kernel_traits=Flash_fwd_kernel_traits<160, 128, 64, 8, false, false, cutlass::bfloat16_t, Flash_kernel_traits<160, 128, 64, 8, cutlass::bfloat16_t>>, Is_dropout=true, Is_causal=false]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(155): here
            instantiation of "void run_mha_fwd_hdim160<T>(Flash_fwd_params &, cudaStream_t) [with T=cutlass::bfloat16_t]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_hdim160_bf16_sm80.cu(16): here

/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_kernel.h(140): warning: variable "MMA_M" was declared but never referenced
          detected during:
            instantiation of "void flash::compute_attn_1rowblock<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &, int, int, int) [with Kernel_traits=Flash_fwd_kernel_traits<160, 128, 64, 8, false, false, cutlass::bfloat16_t, Flash_kernel_traits<160, 128, 64, 8, cutlass::bfloat16_t>>, Is_dropout=true, Is_causal=false, Is_even_N=false, Is_even_K=true, Return_softmax=false, Params=Flash_fwd_params]" 
(588): here
            instantiation of "void flash::compute_attn<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &) [with Kernel_traits=Flash_fwd_kernel_traits<160, 128, 64, 8, false, false, cutlass::bfloat16_t, Flash_kernel_traits<160, 128, 64, 8, cutlass::bfloat16_t>>, Is_dropout=true, Is_causal=false, Is_even_N=false, Is_even_K=true, Return_softmax=false, Params=Flash_fwd_params]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(15): here
            instantiation of "void flash_fwd_kernel<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax>(Flash_fwd_params) [with Kernel_traits=Flash_fwd_kernel_traits<160, 128, 64, 8, false, false, cutlass::bfloat16_t, Flash_kernel_traits<160, 128, 64, 8, cutlass::bfloat16_t>>, Is_dropout=true, Is_causal=false, Is_even_N=false, Is_even_K=true, Return_softmax=false]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(34): here
            instantiation of "void run_flash_fwd<Kernel_traits,Is_dropout,Is_causal>(Flash_fwd_params &, cudaStream_t) [with Kernel_traits=Flash_fwd_kernel_traits<160, 128, 64, 8, false, false, cutlass::bfloat16_t, Flash_kernel_traits<160, 128, 64, 8, cutlass::bfloat16_t>>, Is_dropout=true, Is_causal=false]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(155): here
            instantiation of "void run_mha_fwd_hdim160<T>(Flash_fwd_params &, cudaStream_t) [with T=cutlass::bfloat16_t]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_hdim160_bf16_sm80.cu(16): here

/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_kernel.h(275): warning: variable "tQrQ" was declared but never referenced
          detected during:
            instantiation of "void flash::compute_attn_1rowblock<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &, int, int, int) [with Kernel_traits=Flash_fwd_kernel_traits<160, 128, 64, 8, false, false, cutlass::bfloat16_t, Flash_kernel_traits<160, 128, 64, 8, cutlass::bfloat16_t>>, Is_dropout=true, Is_causal=false, Is_even_N=false, Is_even_K=true, Return_softmax=false, Params=Flash_fwd_params]" 
(588): here
            instantiation of "void flash::compute_attn<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &) [with Kernel_traits=Flash_fwd_kernel_traits<160, 128, 64, 8, false, false, cutlass::bfloat16_t, Flash_kernel_traits<160, 128, 64, 8, cutlass::bfloat16_t>>, Is_dropout=true, Is_causal=false, Is_even_N=false, Is_even_K=true, Return_softmax=false, Params=Flash_fwd_params]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(15): here
            instantiation of "void flash_fwd_kernel<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax>(Flash_fwd_params) [with Kernel_traits=Flash_fwd_kernel_traits<160, 128, 64, 8, false, false, cutlass::bfloat16_t, Flash_kernel_traits<160, 128, 64, 8, cutlass::bfloat16_t>>, Is_dropout=true, Is_causal=false, Is_even_N=false, Is_even_K=true, Return_softmax=false]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(34): here
            instantiation of "void run_flash_fwd<Kernel_traits,Is_dropout,Is_causal>(Flash_fwd_params &, cudaStream_t) [with Kernel_traits=Flash_fwd_kernel_traits<160, 128, 64, 8, false, false, cutlass::bfloat16_t, Flash_kernel_traits<160, 128, 64, 8, cutlass::bfloat16_t>>, Is_dropout=true, Is_causal=false]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(155): here
            instantiation of "void run_mha_fwd_hdim160<T>(Flash_fwd_params &, cudaStream_t) [with T=cutlass::bfloat16_t]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_hdim160_bf16_sm80.cu(16): here

/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_kernel.h(140): warning: variable "MMA_M" was declared but never referenced
          detected during:
            instantiation of "void flash::compute_attn_1rowblock<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &, int, int, int) [with Kernel_traits=Flash_fwd_kernel_traits<160, 128, 64, 8, false, false, cutlass::bfloat16_t, Flash_kernel_traits<160, 128, 64, 8, cutlass::bfloat16_t>>, Is_dropout=true, Is_causal=false, Is_even_N=false, Is_even_K=false, Return_softmax=true, Params=Flash_fwd_params]" 
(588): here
            instantiation of "void flash::compute_attn<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &) [with Kernel_traits=Flash_fwd_kernel_traits<160, 128, 64, 8, false, false, cutlass::bfloat16_t, Flash_kernel_traits<160, 128, 64, 8, cutlass::bfloat16_t>>, Is_dropout=true, Is_causal=false, Is_even_N=false, Is_even_K=false, Return_softmax=true, Params=Flash_fwd_params]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(15): here
            instantiation of "void flash_fwd_kernel<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax>(Flash_fwd_params) [with Kernel_traits=Flash_fwd_kernel_traits<160, 128, 64, 8, false, false, cutlass::bfloat16_t, Flash_kernel_traits<160, 128, 64, 8, cutlass::bfloat16_t>>, Is_dropout=true, Is_causal=false, Is_even_N=false, Is_even_K=false, Return_softmax=true]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(34): here
            instantiation of "void run_flash_fwd<Kernel_traits,Is_dropout,Is_causal>(Flash_fwd_params &, cudaStream_t) [with Kernel_traits=Flash_fwd_kernel_traits<160, 128, 64, 8, false, false, cutlass::bfloat16_t, Flash_kernel_traits<160, 128, 64, 8, cutlass::bfloat16_t>>, Is_dropout=true, Is_causal=false]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(155): here
            instantiation of "void run_mha_fwd_hdim160<T>(Flash_fwd_params &, cudaStream_t) [with T=cutlass::bfloat16_t]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_hdim160_bf16_sm80.cu(16): here

/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_kernel.h(275): warning: variable "tQrQ" was declared but never referenced
          detected during:
            instantiation of "void flash::compute_attn_1rowblock<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &, int, int, int) [with Kernel_traits=Flash_fwd_kernel_traits<160, 128, 64, 8, false, false, cutlass::bfloat16_t, Flash_kernel_traits<160, 128, 64, 8, cutlass::bfloat16_t>>, Is_dropout=true, Is_causal=false, Is_even_N=false, Is_even_K=false, Return_softmax=true, Params=Flash_fwd_params]" 
(588): here
            instantiation of "void flash::compute_attn<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &) [with Kernel_traits=Flash_fwd_kernel_traits<160, 128, 64, 8, false, false, cutlass::bfloat16_t, Flash_kernel_traits<160, 128, 64, 8, cutlass::bfloat16_t>>, Is_dropout=true, Is_causal=false, Is_even_N=false, Is_even_K=false, Return_softmax=true, Params=Flash_fwd_params]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(15): here
            instantiation of "void flash_fwd_kernel<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax>(Flash_fwd_params) [with Kernel_traits=Flash_fwd_kernel_traits<160, 128, 64, 8, false, false, cutlass::bfloat16_t, Flash_kernel_traits<160, 128, 64, 8, cutlass::bfloat16_t>>, Is_dropout=true, Is_causal=false, Is_even_N=false, Is_even_K=false, Return_softmax=true]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(34): here
            instantiation of "void run_flash_fwd<Kernel_traits,Is_dropout,Is_causal>(Flash_fwd_params &, cudaStream_t) [with Kernel_traits=Flash_fwd_kernel_traits<160, 128, 64, 8, false, false, cutlass::bfloat16_t, Flash_kernel_traits<160, 128, 64, 8, cutlass::bfloat16_t>>, Is_dropout=true, Is_causal=false]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(155): here
            instantiation of "void run_mha_fwd_hdim160<T>(Flash_fwd_params &, cudaStream_t) [with T=cutlass::bfloat16_t]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_hdim160_bf16_sm80.cu(16): here

/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_kernel.h(140): warning: variable "MMA_M" was declared but never referenced
          detected during:
            instantiation of "void flash::compute_attn_1rowblock<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &, int, int, int) [with Kernel_traits=Flash_fwd_kernel_traits<160, 128, 64, 8, false, false, cutlass::bfloat16_t, Flash_kernel_traits<160, 128, 64, 8, cutlass::bfloat16_t>>, Is_dropout=true, Is_causal=false, Is_even_N=false, Is_even_K=false, Return_softmax=false, Params=Flash_fwd_params]" 
(588): here
            instantiation of "void flash::compute_attn<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &) [with Kernel_traits=Flash_fwd_kernel_traits<160, 128, 64, 8, false, false, cutlass::bfloat16_t, Flash_kernel_traits<160, 128, 64, 8, cutlass::bfloat16_t>>, Is_dropout=true, Is_causal=false, Is_even_N=false, Is_even_K=false, Return_softmax=false, Params=Flash_fwd_params]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(15): here
            instantiation of "void flash_fwd_kernel<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax>(Flash_fwd_params) [with Kernel_traits=Flash_fwd_kernel_traits<160, 128, 64, 8, false, false, cutlass::bfloat16_t, Flash_kernel_traits<160, 128, 64, 8, cutlass::bfloat16_t>>, Is_dropout=true, Is_causal=false, Is_even_N=false, Is_even_K=false, Return_softmax=false]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(34): here
            instantiation of "void run_flash_fwd<Kernel_traits,Is_dropout,Is_causal>(Flash_fwd_params &, cudaStream_t) [with Kernel_traits=Flash_fwd_kernel_traits<160, 128, 64, 8, false, false, cutlass::bfloat16_t, Flash_kernel_traits<160, 128, 64, 8, cutlass::bfloat16_t>>, Is_dropout=true, Is_causal=false]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(155): here
            instantiation of "void run_mha_fwd_hdim160<T>(Flash_fwd_params &, cudaStream_t) [with T=cutlass::bfloat16_t]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_hdim160_bf16_sm80.cu(16): here

/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_kernel.h(275): warning: variable "tQrQ" was declared but never referenced
          detected during:
            instantiation of "void flash::compute_attn_1rowblock<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &, int, int, int) [with Kernel_traits=Flash_fwd_kernel_traits<160, 128, 64, 8, false, false, cutlass::bfloat16_t, Flash_kernel_traits<160, 128, 64, 8, cutlass::bfloat16_t>>, Is_dropout=true, Is_causal=false, Is_even_N=false, Is_even_K=false, Return_softmax=false, Params=Flash_fwd_params]" 
(588): here
            instantiation of "void flash::compute_attn<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &) [with Kernel_traits=Flash_fwd_kernel_traits<160, 128, 64, 8, false, false, cutlass::bfloat16_t, Flash_kernel_traits<160, 128, 64, 8, cutlass::bfloat16_t>>, Is_dropout=true, Is_causal=false, Is_even_N=false, Is_even_K=false, Return_softmax=false, Params=Flash_fwd_params]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(15): here
            instantiation of "void flash_fwd_kernel<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax>(Flash_fwd_params) [with Kernel_traits=Flash_fwd_kernel_traits<160, 128, 64, 8, false, false, cutlass::bfloat16_t, Flash_kernel_traits<160, 128, 64, 8, cutlass::bfloat16_t>>, Is_dropout=true, Is_causal=false, Is_even_N=false, Is_even_K=false, Return_softmax=false]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(34): here
            instantiation of "void run_flash_fwd<Kernel_traits,Is_dropout,Is_causal>(Flash_fwd_params &, cudaStream_t) [with Kernel_traits=Flash_fwd_kernel_traits<160, 128, 64, 8, false, false, cutlass::bfloat16_t, Flash_kernel_traits<160, 128, 64, 8, cutlass::bfloat16_t>>, Is_dropout=true, Is_causal=false]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(155): here
            instantiation of "void run_mha_fwd_hdim160<T>(Flash_fwd_params &, cudaStream_t) [with T=cutlass::bfloat16_t]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_hdim160_bf16_sm80.cu(16): here

/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_kernel.h(140): warning: variable "MMA_M" was declared but never referenced
          detected during:
            instantiation of "void flash::compute_attn_1rowblock<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &, int, int, int) [with Kernel_traits=Flash_fwd_kernel_traits<160, 128, 32, 4, false, false, cutlass::bfloat16_t, Flash_kernel_traits<160, 128, 32, 4, cutlass::bfloat16_t>>, Is_dropout=true, Is_causal=false, Is_even_N=true, Is_even_K=true, Return_softmax=true, Params=Flash_fwd_params]" 
(588): here
            instantiation of "void flash::compute_attn<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &) [with Kernel_traits=Flash_fwd_kernel_traits<160, 128, 32, 4, false, false, cutlass::bfloat16_t, Flash_kernel_traits<160, 128, 32, 4, cutlass::bfloat16_t>>, Is_dropout=true, Is_causal=false, Is_even_N=true, Is_even_K=true, Return_softmax=true, Params=Flash_fwd_params]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(15): here
            instantiation of "void flash_fwd_kernel<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax>(Flash_fwd_params) [with Kernel_traits=Flash_fwd_kernel_traits<160, 128, 32, 4, false, false, cutlass::bfloat16_t, Flash_kernel_traits<160, 128, 32, 4, cutlass::bfloat16_t>>, Is_dropout=true, Is_causal=false, Is_even_N=true, Is_even_K=true, Return_softmax=true]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(34): here
            instantiation of "void run_flash_fwd<Kernel_traits,Is_dropout,Is_causal>(Flash_fwd_params &, cudaStream_t) [with Kernel_traits=Flash_fwd_kernel_traits<160, 128, 32, 4, false, false, cutlass::bfloat16_t, Flash_kernel_traits<160, 128, 32, 4, cutlass::bfloat16_t>>, Is_dropout=true, Is_causal=false]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(155): here
            instantiation of "void run_mha_fwd_hdim160<T>(Flash_fwd_params &, cudaStream_t) [with T=cutlass::bfloat16_t]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_hdim160_bf16_sm80.cu(16): here

/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_kernel.h(275): warning: variable "tQrQ" was declared but never referenced
          detected during:
            instantiation of "void flash::compute_attn_1rowblock<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &, int, int, int) [with Kernel_traits=Flash_fwd_kernel_traits<160, 128, 32, 4, false, false, cutlass::bfloat16_t, Flash_kernel_traits<160, 128, 32, 4, cutlass::bfloat16_t>>, Is_dropout=true, Is_causal=false, Is_even_N=true, Is_even_K=true, Return_softmax=true, Params=Flash_fwd_params]" 
(588): here
            instantiation of "void flash::compute_attn<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &) [with Kernel_traits=Flash_fwd_kernel_traits<160, 128, 32, 4, false, false, cutlass::bfloat16_t, Flash_kernel_traits<160, 128, 32, 4, cutlass::bfloat16_t>>, Is_dropout=true, Is_causal=false, Is_even_N=true, Is_even_K=true, Return_softmax=true, Params=Flash_fwd_params]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(15): here
            instantiation of "void flash_fwd_kernel<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax>(Flash_fwd_params) [with Kernel_traits=Flash_fwd_kernel_traits<160, 128, 32, 4, false, false, cutlass::bfloat16_t, Flash_kernel_traits<160, 128, 32, 4, cutlass::bfloat16_t>>, Is_dropout=true, Is_causal=false, Is_even_N=true, Is_even_K=true, Return_softmax=true]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(34): here
            instantiation of "void run_flash_fwd<Kernel_traits,Is_dropout,Is_causal>(Flash_fwd_params &, cudaStream_t) [with Kernel_traits=Flash_fwd_kernel_traits<160, 128, 32, 4, false, false, cutlass::bfloat16_t, Flash_kernel_traits<160, 128, 32, 4, cutlass::bfloat16_t>>, Is_dropout=true, Is_causal=false]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(155): here
            instantiation of "void run_mha_fwd_hdim160<T>(Flash_fwd_params &, cudaStream_t) [with T=cutlass::bfloat16_t]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_hdim160_bf16_sm80.cu(16): here

/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_kernel.h(140): warning: variable "MMA_M" was declared but never referenced
          detected during:
            instantiation of "void flash::compute_attn_1rowblock<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &, int, int, int) [with Kernel_traits=Flash_fwd_kernel_traits<160, 128, 32, 4, false, false, cutlass::bfloat16_t, Flash_kernel_traits<160, 128, 32, 4, cutlass::bfloat16_t>>, Is_dropout=true, Is_causal=false, Is_even_N=true, Is_even_K=true, Return_softmax=false, Params=Flash_fwd_params]" 
(588): here
            instantiation of "void flash::compute_attn<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &) [with Kernel_traits=Flash_fwd_kernel_traits<160, 128, 32, 4, false, false, cutlass::bfloat16_t, Flash_kernel_traits<160, 128, 32, 4, cutlass::bfloat16_t>>, Is_dropout=true, Is_causal=false, Is_even_N=true, Is_even_K=true, Return_softmax=false, Params=Flash_fwd_params]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(15): here
            instantiation of "void flash_fwd_kernel<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax>(Flash_fwd_params) [with Kernel_traits=Flash_fwd_kernel_traits<160, 128, 32, 4, false, false, cutlass::bfloat16_t, Flash_kernel_traits<160, 128, 32, 4, cutlass::bfloat16_t>>, Is_dropout=true, Is_causal=false, Is_even_N=true, Is_even_K=true, Return_softmax=false]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(34): here
            instantiation of "void run_flash_fwd<Kernel_traits,Is_dropout,Is_causal>(Flash_fwd_params &, cudaStream_t) [with Kernel_traits=Flash_fwd_kernel_traits<160, 128, 32, 4, false, false, cutlass::bfloat16_t, Flash_kernel_traits<160, 128, 32, 4, cutlass::bfloat16_t>>, Is_dropout=true, Is_causal=false]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(155): here
            instantiation of "void run_mha_fwd_hdim160<T>(Flash_fwd_params &, cudaStream_t) [with T=cutlass::bfloat16_t]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_hdim160_bf16_sm80.cu(16): here

/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_kernel.h(275): warning: variable "tQrQ" was declared but never referenced
          detected during:
            instantiation of "void flash::compute_attn_1rowblock<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &, int, int, int) [with Kernel_traits=Flash_fwd_kernel_traits<160, 128, 32, 4, false, false, cutlass::bfloat16_t, Flash_kernel_traits<160, 128, 32, 4, cutlass::bfloat16_t>>, Is_dropout=true, Is_causal=false, Is_even_N=true, Is_even_K=true, Return_softmax=false, Params=Flash_fwd_params]" 
(588): here
            instantiation of "void flash::compute_attn<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &) [with Kernel_traits=Flash_fwd_kernel_traits<160, 128, 32, 4, false, false, cutlass::bfloat16_t, Flash_kernel_traits<160, 128, 32, 4, cutlass::bfloat16_t>>, Is_dropout=true, Is_causal=false, Is_even_N=true, Is_even_K=true, Return_softmax=false, Params=Flash_fwd_params]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(15): here
            instantiation of "void flash_fwd_kernel<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax>(Flash_fwd_params) [with Kernel_traits=Flash_fwd_kernel_traits<160, 128, 32, 4, false, false, cutlass::bfloat16_t, Flash_kernel_traits<160, 128, 32, 4, cutlass::bfloat16_t>>, Is_dropout=true, Is_causal=false, Is_even_N=true, Is_even_K=true, Return_softmax=false]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(34): here
            instantiation of "void run_flash_fwd<Kernel_traits,Is_dropout,Is_causal>(Flash_fwd_params &, cudaStream_t) [with Kernel_traits=Flash_fwd_kernel_traits<160, 128, 32, 4, false, false, cutlass::bfloat16_t, Flash_kernel_traits<160, 128, 32, 4, cutlass::bfloat16_t>>, Is_dropout=true, Is_causal=false]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(155): here
            instantiation of "void run_mha_fwd_hdim160<T>(Flash_fwd_params &, cudaStream_t) [with T=cutlass::bfloat16_t]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_hdim160_bf16_sm80.cu(16): here

/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_kernel.h(140): warning: variable "MMA_M" was declared but never referenced
          detected during:
            instantiation of "void flash::compute_attn_1rowblock<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &, int, int, int) [with Kernel_traits=Flash_fwd_kernel_traits<160, 128, 32, 4, false, false, cutlass::bfloat16_t, Flash_kernel_traits<160, 128, 32, 4, cutlass::bfloat16_t>>, Is_dropout=true, Is_causal=false, Is_even_N=true, Is_even_K=false, Return_softmax=true, Params=Flash_fwd_params]" 
(588): here
            instantiation of "void flash::compute_attn<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &) [with Kernel_traits=Flash_fwd_kernel_traits<160, 128, 32, 4, false, false, cutlass::bfloat16_t, Flash_kernel_traits<160, 128, 32, 4, cutlass::bfloat16_t>>, Is_dropout=true, Is_causal=false, Is_even_N=true, Is_even_K=false, Return_softmax=true, Params=Flash_fwd_params]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(15): here
            instantiation of "void flash_fwd_kernel<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax>(Flash_fwd_params) [with Kernel_traits=Flash_fwd_kernel_traits<160, 128, 32, 4, false, false, cutlass::bfloat16_t, Flash_kernel_traits<160, 128, 32, 4, cutlass::bfloat16_t>>, Is_dropout=true, Is_causal=false, Is_even_N=true, Is_even_K=false, Return_softmax=true]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(34): here
            instantiation of "void run_flash_fwd<Kernel_traits,Is_dropout,Is_causal>(Flash_fwd_params &, cudaStream_t) [with Kernel_traits=Flash_fwd_kernel_traits<160, 128, 32, 4, false, false, cutlass::bfloat16_t, Flash_kernel_traits<160, 128, 32, 4, cutlass::bfloat16_t>>, Is_dropout=true, Is_causal=false]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(155): here
            instantiation of "void run_mha_fwd_hdim160<T>(Flash_fwd_params &, cudaStream_t) [with T=cutlass::bfloat16_t]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_hdim160_bf16_sm80.cu(16): here

/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_kernel.h(275): warning: variable "tQrQ" was declared but never referenced
          detected during:
            instantiation of "void flash::compute_attn_1rowblock<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &, int, int, int) [with Kernel_traits=Flash_fwd_kernel_traits<160, 128, 32, 4, false, false, cutlass::bfloat16_t, Flash_kernel_traits<160, 128, 32, 4, cutlass::bfloat16_t>>, Is_dropout=true, Is_causal=false, Is_even_N=true, Is_even_K=false, Return_softmax=true, Params=Flash_fwd_params]" 
(588): here
            instantiation of "void flash::compute_attn<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &) [with Kernel_traits=Flash_fwd_kernel_traits<160, 128, 32, 4, false, false, cutlass::bfloat16_t, Flash_kernel_traits<160, 128, 32, 4, cutlass::bfloat16_t>>, Is_dropout=true, Is_causal=false, Is_even_N=true, Is_even_K=false, Return_softmax=true, Params=Flash_fwd_params]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(15): here
            instantiation of "void flash_fwd_kernel<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax>(Flash_fwd_params) [with Kernel_traits=Flash_fwd_kernel_traits<160, 128, 32, 4, false, false, cutlass::bfloat16_t, Flash_kernel_traits<160, 128, 32, 4, cutlass::bfloat16_t>>, Is_dropout=true, Is_causal=false, Is_even_N=true, Is_even_K=false, Return_softmax=true]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(34): here
            instantiation of "void run_flash_fwd<Kernel_traits,Is_dropout,Is_causal>(Flash_fwd_params &, cudaStream_t) [with Kernel_traits=Flash_fwd_kernel_traits<160, 128, 32, 4, false, false, cutlass::bfloat16_t, Flash_kernel_traits<160, 128, 32, 4, cutlass::bfloat16_t>>, Is_dropout=true, Is_causal=false]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(155): here
            instantiation of "void run_mha_fwd_hdim160<T>(Flash_fwd_params &, cudaStream_t) [with T=cutlass::bfloat16_t]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_hdim160_bf16_sm80.cu(16): here

/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_kernel.h(140): warning: variable "MMA_M" was declared but never referenced
          detected during:
            instantiation of "void flash::compute_attn_1rowblock<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &, int, int, int) [with Kernel_traits=Flash_fwd_kernel_traits<160, 128, 32, 4, false, false, cutlass::bfloat16_t, Flash_kernel_traits<160, 128, 32, 4, cutlass::bfloat16_t>>, Is_dropout=true, Is_causal=false, Is_even_N=true, Is_even_K=false, Return_softmax=false, Params=Flash_fwd_params]" 
(588): here
            instantiation of "void flash::compute_attn<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &) [with Kernel_traits=Flash_fwd_kernel_traits<160, 128, 32, 4, false, false, cutlass::bfloat16_t, Flash_kernel_traits<160, 128, 32, 4, cutlass::bfloat16_t>>, Is_dropout=true, Is_causal=false, Is_even_N=true, Is_even_K=false, Return_softmax=false, Params=Flash_fwd_params]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(15): here
            instantiation of "void flash_fwd_kernel<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax>(Flash_fwd_params) [with Kernel_traits=Flash_fwd_kernel_traits<160, 128, 32, 4, false, false, cutlass::bfloat16_t, Flash_kernel_traits<160, 128, 32, 4, cutlass::bfloat16_t>>, Is_dropout=true, Is_causal=false, Is_even_N=true, Is_even_K=false, Return_softmax=false]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(34): here
            instantiation of "void run_flash_fwd<Kernel_traits,Is_dropout,Is_causal>(Flash_fwd_params &, cudaStream_t) [with Kernel_traits=Flash_fwd_kernel_traits<160, 128, 32, 4, false, false, cutlass::bfloat16_t, Flash_kernel_traits<160, 128, 32, 4, cutlass::bfloat16_t>>, Is_dropout=true, Is_causal=false]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(155): here
            instantiation of "void run_mha_fwd_hdim160<T>(Flash_fwd_params &, cudaStream_t) [with T=cutlass::bfloat16_t]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_hdim160_bf16_sm80.cu(16): here

/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_kernel.h(275): warning: variable "tQrQ" was declared but never referenced
          detected during:
            instantiation of "void flash::compute_attn_1rowblock<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &, int, int, int) [with Kernel_traits=Flash_fwd_kernel_traits<160, 128, 32, 4, false, false, cutlass::bfloat16_t, Flash_kernel_traits<160, 128, 32, 4, cutlass::bfloat16_t>>, Is_dropout=true, Is_causal=false, Is_even_N=true, Is_even_K=false, Return_softmax=false, Params=Flash_fwd_params]" 
(588): here
            instantiation of "void flash::compute_attn<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &) [with Kernel_traits=Flash_fwd_kernel_traits<160, 128, 32, 4, false, false, cutlass::bfloat16_t, Flash_kernel_traits<160, 128, 32, 4, cutlass::bfloat16_t>>, Is_dropout=true, Is_causal=false, Is_even_N=true, Is_even_K=false, Return_softmax=false, Params=Flash_fwd_params]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(15): here
            instantiation of "void flash_fwd_kernel<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax>(Flash_fwd_params) [with Kernel_traits=Flash_fwd_kernel_traits<160, 128, 32, 4, false, false, cutlass::bfloat16_t, Flash_kernel_traits<160, 128, 32, 4, cutlass::bfloat16_t>>, Is_dropout=true, Is_causal=false, Is_even_N=true, Is_even_K=false, Return_softmax=false]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(34): here
            instantiation of "void run_flash_fwd<Kernel_traits,Is_dropout,Is_causal>(Flash_fwd_params &, cudaStream_t) [with Kernel_traits=Flash_fwd_kernel_traits<160, 128, 32, 4, false, false, cutlass::bfloat16_t, Flash_kernel_traits<160, 128, 32, 4, cutlass::bfloat16_t>>, Is_dropout=true, Is_causal=false]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(155): here
            instantiation of "void run_mha_fwd_hdim160<T>(Flash_fwd_params &, cudaStream_t) [with T=cutlass::bfloat16_t]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_hdim160_bf16_sm80.cu(16): here

/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_kernel.h(140): warning: variable "MMA_M" was declared but never referenced
          detected during:
            instantiation of "void flash::compute_attn_1rowblock<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &, int, int, int) [with Kernel_traits=Flash_fwd_kernel_traits<160, 128, 32, 4, false, false, cutlass::bfloat16_t, Flash_kernel_traits<160, 128, 32, 4, cutlass::bfloat16_t>>, Is_dropout=true, Is_causal=false, Is_even_N=false, Is_even_K=true, Return_softmax=true, Params=Flash_fwd_params]" 
(588): here
            instantiation of "void flash::compute_attn<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &) [with Kernel_traits=Flash_fwd_kernel_traits<160, 128, 32, 4, false, false, cutlass::bfloat16_t, Flash_kernel_traits<160, 128, 32, 4, cutlass::bfloat16_t>>, Is_dropout=true, Is_causal=false, Is_even_N=false, Is_even_K=true, Return_softmax=true, Params=Flash_fwd_params]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(15): here
            instantiation of "void flash_fwd_kernel<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax>(Flash_fwd_params) [with Kernel_traits=Flash_fwd_kernel_traits<160, 128, 32, 4, false, false, cutlass::bfloat16_t, Flash_kernel_traits<160, 128, 32, 4, cutlass::bfloat16_t>>, Is_dropout=true, Is_causal=false, Is_even_N=false, Is_even_K=true, Return_softmax=true]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(34): here
            instantiation of "void run_flash_fwd<Kernel_traits,Is_dropout,Is_causal>(Flash_fwd_params &, cudaStream_t) [with Kernel_traits=Flash_fwd_kernel_traits<160, 128, 32, 4, false, false, cutlass::bfloat16_t, Flash_kernel_traits<160, 128, 32, 4, cutlass::bfloat16_t>>, Is_dropout=true, Is_causal=false]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(155): here
            instantiation of "void run_mha_fwd_hdim160<T>(Flash_fwd_params &, cudaStream_t) [with T=cutlass::bfloat16_t]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_hdim160_bf16_sm80.cu(16): here

/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_kernel.h(275): warning: variable "tQrQ" was declared but never referenced
          detected during:
            instantiation of "void flash::compute_attn_1rowblock<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &, int, int, int) [with Kernel_traits=Flash_fwd_kernel_traits<160, 128, 32, 4, false, false, cutlass::bfloat16_t, Flash_kernel_traits<160, 128, 32, 4, cutlass::bfloat16_t>>, Is_dropout=true, Is_causal=false, Is_even_N=false, Is_even_K=true, Return_softmax=true, Params=Flash_fwd_params]" 
(588): here
            instantiation of "void flash::compute_attn<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &) [with Kernel_traits=Flash_fwd_kernel_traits<160, 128, 32, 4, false, false, cutlass::bfloat16_t, Flash_kernel_traits<160, 128, 32, 4, cutlass::bfloat16_t>>, Is_dropout=true, Is_causal=false, Is_even_N=false, Is_even_K=true, Return_softmax=true, Params=Flash_fwd_params]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(15): here
            instantiation of "void flash_fwd_kernel<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax>(Flash_fwd_params) [with Kernel_traits=Flash_fwd_kernel_traits<160, 128, 32, 4, false, false, cutlass::bfloat16_t, Flash_kernel_traits<160, 128, 32, 4, cutlass::bfloat16_t>>, Is_dropout=true, Is_causal=false, Is_even_N=false, Is_even_K=true, Return_softmax=true]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(34): here
            instantiation of "void run_flash_fwd<Kernel_traits,Is_dropout,Is_causal>(Flash_fwd_params &, cudaStream_t) [with Kernel_traits=Flash_fwd_kernel_traits<160, 128, 32, 4, false, false, cutlass::bfloat16_t, Flash_kernel_traits<160, 128, 32, 4, cutlass::bfloat16_t>>, Is_dropout=true, Is_causal=false]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(155): here
            instantiation of "void run_mha_fwd_hdim160<T>(Flash_fwd_params &, cudaStream_t) [with T=cutlass::bfloat16_t]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_hdim160_bf16_sm80.cu(16): here

/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_kernel.h(140): warning: variable "MMA_M" was declared but never referenced
          detected during:
            instantiation of "void flash::compute_attn_1rowblock<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &, int, int, int) [with Kernel_traits=Flash_fwd_kernel_traits<160, 128, 32, 4, false, false, cutlass::bfloat16_t, Flash_kernel_traits<160, 128, 32, 4, cutlass::bfloat16_t>>, Is_dropout=true, Is_causal=false, Is_even_N=false, Is_even_K=true, Return_softmax=false, Params=Flash_fwd_params]" 
(588): here
            instantiation of "void flash::compute_attn<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &) [with Kernel_traits=Flash_fwd_kernel_traits<160, 128, 32, 4, false, false, cutlass::bfloat16_t, Flash_kernel_traits<160, 128, 32, 4, cutlass::bfloat16_t>>, Is_dropout=true, Is_causal=false, Is_even_N=false, Is_even_K=true, Return_softmax=false, Params=Flash_fwd_params]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(15): here
            instantiation of "void flash_fwd_kernel<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax>(Flash_fwd_params) [with Kernel_traits=Flash_fwd_kernel_traits<160, 128, 32, 4, false, false, cutlass::bfloat16_t, Flash_kernel_traits<160, 128, 32, 4, cutlass::bfloat16_t>>, Is_dropout=true, Is_causal=false, Is_even_N=false, Is_even_K=true, Return_softmax=false]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(34): here
            instantiation of "void run_flash_fwd<Kernel_traits,Is_dropout,Is_causal>(Flash_fwd_params &, cudaStream_t) [with Kernel_traits=Flash_fwd_kernel_traits<160, 128, 32, 4, false, false, cutlass::bfloat16_t, Flash_kernel_traits<160, 128, 32, 4, cutlass::bfloat16_t>>, Is_dropout=true, Is_causal=false]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(155): here
            instantiation of "void run_mha_fwd_hdim160<T>(Flash_fwd_params &, cudaStream_t) [with T=cutlass::bfloat16_t]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_hdim160_bf16_sm80.cu(16): here

/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_kernel.h(275): warning: variable "tQrQ" was declared but never referenced
          detected during:
            instantiation of "void flash::compute_attn_1rowblock<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &, int, int, int) [with Kernel_traits=Flash_fwd_kernel_traits<160, 128, 32, 4, false, false, cutlass::bfloat16_t, Flash_kernel_traits<160, 128, 32, 4, cutlass::bfloat16_t>>, Is_dropout=true, Is_causal=false, Is_even_N=false, Is_even_K=true, Return_softmax=false, Params=Flash_fwd_params]" 
(588): here
            instantiation of "void flash::compute_attn<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &) [with Kernel_traits=Flash_fwd_kernel_traits<160, 128, 32, 4, false, false, cutlass::bfloat16_t, Flash_kernel_traits<160, 128, 32, 4, cutlass::bfloat16_t>>, Is_dropout=true, Is_causal=false, Is_even_N=false, Is_even_K=true, Return_softmax=false, Params=Flash_fwd_params]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(15): here
            instantiation of "void flash_fwd_kernel<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax>(Flash_fwd_params) [with Kernel_traits=Flash_fwd_kernel_traits<160, 128, 32, 4, false, false, cutlass::bfloat16_t, Flash_kernel_traits<160, 128, 32, 4, cutlass::bfloat16_t>>, Is_dropout=true, Is_causal=false, Is_even_N=false, Is_even_K=true, Return_softmax=false]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(34): here
            instantiation of "void run_flash_fwd<Kernel_traits,Is_dropout,Is_causal>(Flash_fwd_params &, cudaStream_t) [with Kernel_traits=Flash_fwd_kernel_traits<160, 128, 32, 4, false, false, cutlass::bfloat16_t, Flash_kernel_traits<160, 128, 32, 4, cutlass::bfloat16_t>>, Is_dropout=true, Is_causal=false]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(155): here
            instantiation of "void run_mha_fwd_hdim160<T>(Flash_fwd_params &, cudaStream_t) [with T=cutlass::bfloat16_t]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_hdim160_bf16_sm80.cu(16): here

/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_kernel.h(140): warning: variable "MMA_M" was declared but never referenced
          detected during:
            instantiation of "void flash::compute_attn_1rowblock<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &, int, int, int) [with Kernel_traits=Flash_fwd_kernel_traits<160, 128, 32, 4, false, false, cutlass::bfloat16_t, Flash_kernel_traits<160, 128, 32, 4, cutlass::bfloat16_t>>, Is_dropout=true, Is_causal=false, Is_even_N=false, Is_even_K=false, Return_softmax=true, Params=Flash_fwd_params]" 
(588): here
            instantiation of "void flash::compute_attn<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &) [with Kernel_traits=Flash_fwd_kernel_traits<160, 128, 32, 4, false, false, cutlass::bfloat16_t, Flash_kernel_traits<160, 128, 32, 4, cutlass::bfloat16_t>>, Is_dropout=true, Is_causal=false, Is_even_N=false, Is_even_K=false, Return_softmax=true, Params=Flash_fwd_params]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(15): here
            instantiation of "void flash_fwd_kernel<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax>(Flash_fwd_params) [with Kernel_traits=Flash_fwd_kernel_traits<160, 128, 32, 4, false, false, cutlass::bfloat16_t, Flash_kernel_traits<160, 128, 32, 4, cutlass::bfloat16_t>>, Is_dropout=true, Is_causal=false, Is_even_N=false, Is_even_K=false, Return_softmax=true]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(34): here
            instantiation of "void run_flash_fwd<Kernel_traits,Is_dropout,Is_causal>(Flash_fwd_params &, cudaStream_t) [with Kernel_traits=Flash_fwd_kernel_traits<160, 128, 32, 4, false, false, cutlass::bfloat16_t, Flash_kernel_traits<160, 128, 32, 4, cutlass::bfloat16_t>>, Is_dropout=true, Is_causal=false]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(155): here
            instantiation of "void run_mha_fwd_hdim160<T>(Flash_fwd_params &, cudaStream_t) [with T=cutlass::bfloat16_t]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_hdim160_bf16_sm80.cu(16): here

/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_kernel.h(275): warning: variable "tQrQ" was declared but never referenced
          detected during:
            instantiation of "void flash::compute_attn_1rowblock<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &, int, int, int) [with Kernel_traits=Flash_fwd_kernel_traits<160, 128, 32, 4, false, false, cutlass::bfloat16_t, Flash_kernel_traits<160, 128, 32, 4, cutlass::bfloat16_t>>, Is_dropout=true, Is_causal=false, Is_even_N=false, Is_even_K=false, Return_softmax=true, Params=Flash_fwd_params]" 
(588): here
            instantiation of "void flash::compute_attn<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &) [with Kernel_traits=Flash_fwd_kernel_traits<160, 128, 32, 4, false, false, cutlass::bfloat16_t, Flash_kernel_traits<160, 128, 32, 4, cutlass::bfloat16_t>>, Is_dropout=true, Is_causal=false, Is_even_N=false, Is_even_K=false, Return_softmax=true, Params=Flash_fwd_params]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(15): here
            instantiation of "void flash_fwd_kernel<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax>(Flash_fwd_params) [with Kernel_traits=Flash_fwd_kernel_traits<160, 128, 32, 4, false, false, cutlass::bfloat16_t, Flash_kernel_traits<160, 128, 32, 4, cutlass::bfloat16_t>>, Is_dropout=true, Is_causal=false, Is_even_N=false, Is_even_K=false, Return_softmax=true]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(34): here
            instantiation of "void run_flash_fwd<Kernel_traits,Is_dropout,Is_causal>(Flash_fwd_params &, cudaStream_t) [with Kernel_traits=Flash_fwd_kernel_traits<160, 128, 32, 4, false, false, cutlass::bfloat16_t, Flash_kernel_traits<160, 128, 32, 4, cutlass::bfloat16_t>>, Is_dropout=true, Is_causal=false]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(155): here
            instantiation of "void run_mha_fwd_hdim160<T>(Flash_fwd_params &, cudaStream_t) [with T=cutlass::bfloat16_t]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_hdim160_bf16_sm80.cu(16): here

/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_kernel.h(140): warning: variable "MMA_M" was declared but never referenced
          detected during:
            instantiation of "void flash::compute_attn_1rowblock<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &, int, int, int) [with Kernel_traits=Flash_fwd_kernel_traits<160, 128, 32, 4, false, false, cutlass::bfloat16_t, Flash_kernel_traits<160, 128, 32, 4, cutlass::bfloat16_t>>, Is_dropout=true, Is_causal=false, Is_even_N=false, Is_even_K=false, Return_softmax=false, Params=Flash_fwd_params]" 
(588): here
            instantiation of "void flash::compute_attn<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &) [with Kernel_traits=Flash_fwd_kernel_traits<160, 128, 32, 4, false, false, cutlass::bfloat16_t, Flash_kernel_traits<160, 128, 32, 4, cutlass::bfloat16_t>>, Is_dropout=true, Is_causal=false, Is_even_N=false, Is_even_K=false, Return_softmax=false, Params=Flash_fwd_params]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(15): here
            instantiation of "void flash_fwd_kernel<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax>(Flash_fwd_params) [with Kernel_traits=Flash_fwd_kernel_traits<160, 128, 32, 4, false, false, cutlass::bfloat16_t, Flash_kernel_traits<160, 128, 32, 4, cutlass::bfloat16_t>>, Is_dropout=true, Is_causal=false, Is_even_N=false, Is_even_K=false, Return_softmax=false]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(34): here
            instantiation of "void run_flash_fwd<Kernel_traits,Is_dropout,Is_causal>(Flash_fwd_params &, cudaStream_t) [with Kernel_traits=Flash_fwd_kernel_traits<160, 128, 32, 4, false, false, cutlass::bfloat16_t, Flash_kernel_traits<160, 128, 32, 4, cutlass::bfloat16_t>>, Is_dropout=true, Is_causal=false]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(155): here
            instantiation of "void run_mha_fwd_hdim160<T>(Flash_fwd_params &, cudaStream_t) [with T=cutlass::bfloat16_t]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_hdim160_bf16_sm80.cu(16): here

/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_kernel.h(275): warning: variable "tQrQ" was declared but never referenced
          detected during:
            instantiation of "void flash::compute_attn_1rowblock<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &, int, int, int) [with Kernel_traits=Flash_fwd_kernel_traits<160, 128, 32, 4, false, false, cutlass::bfloat16_t, Flash_kernel_traits<160, 128, 32, 4, cutlass::bfloat16_t>>, Is_dropout=true, Is_causal=false, Is_even_N=false, Is_even_K=false, Return_softmax=false, Params=Flash_fwd_params]" 
(588): here
            instantiation of "void flash::compute_attn<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &) [with Kernel_traits=Flash_fwd_kernel_traits<160, 128, 32, 4, false, false, cutlass::bfloat16_t, Flash_kernel_traits<160, 128, 32, 4, cutlass::bfloat16_t>>, Is_dropout=true, Is_causal=false, Is_even_N=false, Is_even_K=false, Return_softmax=false, Params=Flash_fwd_params]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(15): here
            instantiation of "void flash_fwd_kernel<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax>(Flash_fwd_params) [with Kernel_traits=Flash_fwd_kernel_traits<160, 128, 32, 4, false, false, cutlass::bfloat16_t, Flash_kernel_traits<160, 128, 32, 4, cutlass::bfloat16_t>>, Is_dropout=true, Is_causal=false, Is_even_N=false, Is_even_K=false, Return_softmax=false]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(34): here
            instantiation of "void run_flash_fwd<Kernel_traits,Is_dropout,Is_causal>(Flash_fwd_params &, cudaStream_t) [with Kernel_traits=Flash_fwd_kernel_traits<160, 128, 32, 4, false, false, cutlass::bfloat16_t, Flash_kernel_traits<160, 128, 32, 4, cutlass::bfloat16_t>>, Is_dropout=true, Is_causal=false]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(155): here
            instantiation of "void run_mha_fwd_hdim160<T>(Flash_fwd_params &, cudaStream_t) [with T=cutlass::bfloat16_t]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_hdim160_bf16_sm80.cu(16): here

/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_kernel.h(140): warning: variable "MMA_M" was declared but never referenced
          detected during:
            instantiation of "void flash::compute_attn_1rowblock<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &, int, int, int) [with Kernel_traits=Flash_fwd_kernel_traits<160, 64, 64, 4, false, false, cutlass::bfloat16_t, Flash_kernel_traits<160, 64, 64, 4, cutlass::bfloat16_t>>, Is_dropout=false, Is_causal=true, Is_even_N=true, Is_even_K=true, Return_softmax=false, Params=Flash_fwd_params]" 
(588): here
            instantiation of "void flash::compute_attn<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &) [with Kernel_traits=Flash_fwd_kernel_traits<160, 64, 64, 4, false, false, cutlass::bfloat16_t, Flash_kernel_traits<160, 64, 64, 4, cutlass::bfloat16_t>>, Is_dropout=false, Is_causal=true, Is_even_N=true, Is_even_K=true, Return_softmax=false, Params=Flash_fwd_params]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(15): here
            instantiation of "void flash_fwd_kernel<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax>(Flash_fwd_params) [with Kernel_traits=Flash_fwd_kernel_traits<160, 64, 64, 4, false, false, cutlass::bfloat16_t, Flash_kernel_traits<160, 64, 64, 4, cutlass::bfloat16_t>>, Is_dropout=false, Is_causal=true, Is_even_N=true, Is_even_K=true, Return_softmax=false]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(34): here
            instantiation of "void run_flash_fwd<Kernel_traits,Is_dropout,Is_causal>(Flash_fwd_params &, cudaStream_t) [with Kernel_traits=Flash_fwd_kernel_traits<160, 64, 64, 4, false, false, cutlass::bfloat16_t, Flash_kernel_traits<160, 64, 64, 4, cutlass::bfloat16_t>>, Is_dropout=false, Is_causal=true]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(155): here
            instantiation of "void run_mha_fwd_hdim160<T>(Flash_fwd_params &, cudaStream_t) [with T=cutlass::bfloat16_t]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_hdim160_bf16_sm80.cu(16): here

/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_kernel.h(275): warning: variable "tQrQ" was declared but never referenced
          detected during:
            instantiation of "void flash::compute_attn_1rowblock<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &, int, int, int) [with Kernel_traits=Flash_fwd_kernel_traits<160, 64, 64, 4, false, false, cutlass::bfloat16_t, Flash_kernel_traits<160, 64, 64, 4, cutlass::bfloat16_t>>, Is_dropout=false, Is_causal=true, Is_even_N=true, Is_even_K=true, Return_softmax=false, Params=Flash_fwd_params]" 
(588): here
            instantiation of "void flash::compute_attn<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &) [with Kernel_traits=Flash_fwd_kernel_traits<160, 64, 64, 4, false, false, cutlass::bfloat16_t, Flash_kernel_traits<160, 64, 64, 4, cutlass::bfloat16_t>>, Is_dropout=false, Is_causal=true, Is_even_N=true, Is_even_K=true, Return_softmax=false, Params=Flash_fwd_params]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(15): here
            instantiation of "void flash_fwd_kernel<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax>(Flash_fwd_params) [with Kernel_traits=Flash_fwd_kernel_traits<160, 64, 64, 4, false, false, cutlass::bfloat16_t, Flash_kernel_traits<160, 64, 64, 4, cutlass::bfloat16_t>>, Is_dropout=false, Is_causal=true, Is_even_N=true, Is_even_K=true, Return_softmax=false]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(34): here
            instantiation of "void run_flash_fwd<Kernel_traits,Is_dropout,Is_causal>(Flash_fwd_params &, cudaStream_t) [with Kernel_traits=Flash_fwd_kernel_traits<160, 64, 64, 4, false, false, cutlass::bfloat16_t, Flash_kernel_traits<160, 64, 64, 4, cutlass::bfloat16_t>>, Is_dropout=false, Is_causal=true]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(155): here
            instantiation of "void run_mha_fwd_hdim160<T>(Flash_fwd_params &, cudaStream_t) [with T=cutlass::bfloat16_t]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_hdim160_bf16_sm80.cu(16): here

/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_kernel.h(140): warning: variable "MMA_M" was declared but never referenced
          detected during:
            instantiation of "void flash::compute_attn_1rowblock<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &, int, int, int) [with Kernel_traits=Flash_fwd_kernel_traits<160, 64, 64, 4, false, false, cutlass::bfloat16_t, Flash_kernel_traits<160, 64, 64, 4, cutlass::bfloat16_t>>, Is_dropout=false, Is_causal=true, Is_even_N=true, Is_even_K=false, Return_softmax=false, Params=Flash_fwd_params]" 
(588): here
            instantiation of "void flash::compute_attn<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &) [with Kernel_traits=Flash_fwd_kernel_traits<160, 64, 64, 4, false, false, cutlass::bfloat16_t, Flash_kernel_traits<160, 64, 64, 4, cutlass::bfloat16_t>>, Is_dropout=false, Is_causal=true, Is_even_N=true, Is_even_K=false, Return_softmax=false, Params=Flash_fwd_params]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(15): here
            instantiation of "void flash_fwd_kernel<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax>(Flash_fwd_params) [with Kernel_traits=Flash_fwd_kernel_traits<160, 64, 64, 4, false, false, cutlass::bfloat16_t, Flash_kernel_traits<160, 64, 64, 4, cutlass::bfloat16_t>>, Is_dropout=false, Is_causal=true, Is_even_N=true, Is_even_K=false, Return_softmax=false]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(34): here
            instantiation of "void run_flash_fwd<Kernel_traits,Is_dropout,Is_causal>(Flash_fwd_params &, cudaStream_t) [with Kernel_traits=Flash_fwd_kernel_traits<160, 64, 64, 4, false, false, cutlass::bfloat16_t, Flash_kernel_traits<160, 64, 64, 4, cutlass::bfloat16_t>>, Is_dropout=false, Is_causal=true]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(155): here
            instantiation of "void run_mha_fwd_hdim160<T>(Flash_fwd_params &, cudaStream_t) [with T=cutlass::bfloat16_t]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_hdim160_bf16_sm80.cu(16): here

/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_kernel.h(275): warning: variable "tQrQ" was declared but never referenced
          detected during:
            instantiation of "void flash::compute_attn_1rowblock<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &, int, int, int) [with Kernel_traits=Flash_fwd_kernel_traits<160, 64, 64, 4, false, false, cutlass::bfloat16_t, Flash_kernel_traits<160, 64, 64, 4, cutlass::bfloat16_t>>, Is_dropout=false, Is_causal=true, Is_even_N=true, Is_even_K=false, Return_softmax=false, Params=Flash_fwd_params]" 
(588): here
            instantiation of "void flash::compute_attn<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &) [with Kernel_traits=Flash_fwd_kernel_traits<160, 64, 64, 4, false, false, cutlass::bfloat16_t, Flash_kernel_traits<160, 64, 64, 4, cutlass::bfloat16_t>>, Is_dropout=false, Is_causal=true, Is_even_N=true, Is_even_K=false, Return_softmax=false, Params=Flash_fwd_params]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(15): here
            instantiation of "void flash_fwd_kernel<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax>(Flash_fwd_params) [with Kernel_traits=Flash_fwd_kernel_traits<160, 64, 64, 4, false, false, cutlass::bfloat16_t, Flash_kernel_traits<160, 64, 64, 4, cutlass::bfloat16_t>>, Is_dropout=false, Is_causal=true, Is_even_N=true, Is_even_K=false, Return_softmax=false]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(34): here
            instantiation of "void run_flash_fwd<Kernel_traits,Is_dropout,Is_causal>(Flash_fwd_params &, cudaStream_t) [with Kernel_traits=Flash_fwd_kernel_traits<160, 64, 64, 4, false, false, cutlass::bfloat16_t, Flash_kernel_traits<160, 64, 64, 4, cutlass::bfloat16_t>>, Is_dropout=false, Is_causal=true]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(155): here
            instantiation of "void run_mha_fwd_hdim160<T>(Flash_fwd_params &, cudaStream_t) [with T=cutlass::bfloat16_t]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_hdim160_bf16_sm80.cu(16): here

/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_kernel.h(140): warning: variable "MMA_M" was declared but never referenced
          detected during:
            instantiation of "void flash::compute_attn_1rowblock<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &, int, int, int) [with Kernel_traits=Flash_fwd_kernel_traits<160, 64, 64, 4, false, false, cutlass::bfloat16_t, Flash_kernel_traits<160, 64, 64, 4, cutlass::bfloat16_t>>, Is_dropout=false, Is_causal=true, Is_even_N=false, Is_even_K=true, Return_softmax=false, Params=Flash_fwd_params]" 
(588): here
            instantiation of "void flash::compute_attn<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &) [with Kernel_traits=Flash_fwd_kernel_traits<160, 64, 64, 4, false, false, cutlass::bfloat16_t, Flash_kernel_traits<160, 64, 64, 4, cutlass::bfloat16_t>>, Is_dropout=false, Is_causal=true, Is_even_N=false, Is_even_K=true, Return_softmax=false, Params=Flash_fwd_params]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(15): here
            instantiation of "void flash_fwd_kernel<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax>(Flash_fwd_params) [with Kernel_traits=Flash_fwd_kernel_traits<160, 64, 64, 4, false, false, cutlass::bfloat16_t, Flash_kernel_traits<160, 64, 64, 4, cutlass::bfloat16_t>>, Is_dropout=false, Is_causal=true, Is_even_N=false, Is_even_K=true, Return_softmax=false]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(34): here
            instantiation of "void run_flash_fwd<Kernel_traits,Is_dropout,Is_causal>(Flash_fwd_params &, cudaStream_t) [with Kernel_traits=Flash_fwd_kernel_traits<160, 64, 64, 4, false, false, cutlass::bfloat16_t, Flash_kernel_traits<160, 64, 64, 4, cutlass::bfloat16_t>>, Is_dropout=false, Is_causal=true]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(155): here
            instantiation of "void run_mha_fwd_hdim160<T>(Flash_fwd_params &, cudaStream_t) [with T=cutlass::bfloat16_t]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_hdim160_bf16_sm80.cu(16): here

/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_kernel.h(275): warning: variable "tQrQ" was declared but never referenced
          detected during:
            instantiation of "void flash::compute_attn_1rowblock<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &, int, int, int) [with Kernel_traits=Flash_fwd_kernel_traits<160, 64, 64, 4, false, false, cutlass::bfloat16_t, Flash_kernel_traits<160, 64, 64, 4, cutlass::bfloat16_t>>, Is_dropout=false, Is_causal=true, Is_even_N=false, Is_even_K=true, Return_softmax=false, Params=Flash_fwd_params]" 
(588): here
            instantiation of "void flash::compute_attn<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &) [with Kernel_traits=Flash_fwd_kernel_traits<160, 64, 64, 4, false, false, cutlass::bfloat16_t, Flash_kernel_traits<160, 64, 64, 4, cutlass::bfloat16_t>>, Is_dropout=false, Is_causal=true, Is_even_N=false, Is_even_K=true, Return_softmax=false, Params=Flash_fwd_params]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(15): here
            instantiation of "void flash_fwd_kernel<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax>(Flash_fwd_params) [with Kernel_traits=Flash_fwd_kernel_traits<160, 64, 64, 4, false, false, cutlass::bfloat16_t, Flash_kernel_traits<160, 64, 64, 4, cutlass::bfloat16_t>>, Is_dropout=false, Is_causal=true, Is_even_N=false, Is_even_K=true, Return_softmax=false]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(34): here
            instantiation of "void run_flash_fwd<Kernel_traits,Is_dropout,Is_causal>(Flash_fwd_params &, cudaStream_t) [with Kernel_traits=Flash_fwd_kernel_traits<160, 64, 64, 4, false, false, cutlass::bfloat16_t, Flash_kernel_traits<160, 64, 64, 4, cutlass::bfloat16_t>>, Is_dropout=false, Is_causal=true]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(155): here
            instantiation of "void run_mha_fwd_hdim160<T>(Flash_fwd_params &, cudaStream_t) [with T=cutlass::bfloat16_t]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_hdim160_bf16_sm80.cu(16): here

/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_kernel.h(140): warning: variable "MMA_M" was declared but never referenced
          detected during:
            instantiation of "void flash::compute_attn_1rowblock<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &, int, int, int) [with Kernel_traits=Flash_fwd_kernel_traits<160, 64, 64, 4, false, false, cutlass::bfloat16_t, Flash_kernel_traits<160, 64, 64, 4, cutlass::bfloat16_t>>, Is_dropout=false, Is_causal=true, Is_even_N=false, Is_even_K=false, Return_softmax=false, Params=Flash_fwd_params]" 
(588): here
            instantiation of "void flash::compute_attn<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &) [with Kernel_traits=Flash_fwd_kernel_traits<160, 64, 64, 4, false, false, cutlass::bfloat16_t, Flash_kernel_traits<160, 64, 64, 4, cutlass::bfloat16_t>>, Is_dropout=false, Is_causal=true, Is_even_N=false, Is_even_K=false, Return_softmax=false, Params=Flash_fwd_params]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(15): here
            instantiation of "void flash_fwd_kernel<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax>(Flash_fwd_params) [with Kernel_traits=Flash_fwd_kernel_traits<160, 64, 64, 4, false, false, cutlass::bfloat16_t, Flash_kernel_traits<160, 64, 64, 4, cutlass::bfloat16_t>>, Is_dropout=false, Is_causal=true, Is_even_N=false, Is_even_K=false, Return_softmax=false]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(34): here
            instantiation of "void run_flash_fwd<Kernel_traits,Is_dropout,Is_causal>(Flash_fwd_params &, cudaStream_t) [with Kernel_traits=Flash_fwd_kernel_traits<160, 64, 64, 4, false, false, cutlass::bfloat16_t, Flash_kernel_traits<160, 64, 64, 4, cutlass::bfloat16_t>>, Is_dropout=false, Is_causal=true]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(155): here
            instantiation of "void run_mha_fwd_hdim160<T>(Flash_fwd_params &, cudaStream_t) [with T=cutlass::bfloat16_t]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_hdim160_bf16_sm80.cu(16): here

/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_kernel.h(275): warning: variable "tQrQ" was declared but never referenced
          detected during:
            instantiation of "void flash::compute_attn_1rowblock<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &, int, int, int) [with Kernel_traits=Flash_fwd_kernel_traits<160, 64, 64, 4, false, false, cutlass::bfloat16_t, Flash_kernel_traits<160, 64, 64, 4, cutlass::bfloat16_t>>, Is_dropout=false, Is_causal=true, Is_even_N=false, Is_even_K=false, Return_softmax=false, Params=Flash_fwd_params]" 
(588): here
            instantiation of "void flash::compute_attn<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &) [with Kernel_traits=Flash_fwd_kernel_traits<160, 64, 64, 4, false, false, cutlass::bfloat16_t, Flash_kernel_traits<160, 64, 64, 4, cutlass::bfloat16_t>>, Is_dropout=false, Is_causal=true, Is_even_N=false, Is_even_K=false, Return_softmax=false, Params=Flash_fwd_params]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(15): here
            instantiation of "void flash_fwd_kernel<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax>(Flash_fwd_params) [with Kernel_traits=Flash_fwd_kernel_traits<160, 64, 64, 4, false, false, cutlass::bfloat16_t, Flash_kernel_traits<160, 64, 64, 4, cutlass::bfloat16_t>>, Is_dropout=false, Is_causal=true, Is_even_N=false, Is_even_K=false, Return_softmax=false]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(34): here
            instantiation of "void run_flash_fwd<Kernel_traits,Is_dropout,Is_causal>(Flash_fwd_params &, cudaStream_t) [with Kernel_traits=Flash_fwd_kernel_traits<160, 64, 64, 4, false, false, cutlass::bfloat16_t, Flash_kernel_traits<160, 64, 64, 4, cutlass::bfloat16_t>>, Is_dropout=false, Is_causal=true]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(155): here
            instantiation of "void run_mha_fwd_hdim160<T>(Flash_fwd_params &, cudaStream_t) [with T=cutlass::bfloat16_t]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_hdim160_bf16_sm80.cu(16): here

/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_kernel.h(140): warning: variable "MMA_M" was declared but never referenced
          detected during:
            instantiation of "void flash::compute_attn_1rowblock<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &, int, int, int) [with Kernel_traits=Flash_fwd_kernel_traits<160, 128, 32, 4, false, false, cutlass::bfloat16_t, Flash_kernel_traits<160, 128, 32, 4, cutlass::bfloat16_t>>, Is_dropout=false, Is_causal=true, Is_even_N=true, Is_even_K=true, Return_softmax=false, Params=Flash_fwd_params]" 
(588): here
            instantiation of "void flash::compute_attn<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &) [with Kernel_traits=Flash_fwd_kernel_traits<160, 128, 32, 4, false, false, cutlass::bfloat16_t, Flash_kernel_traits<160, 128, 32, 4, cutlass::bfloat16_t>>, Is_dropout=false, Is_causal=true, Is_even_N=true, Is_even_K=true, Return_softmax=false, Params=Flash_fwd_params]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(15): here
            instantiation of "void flash_fwd_kernel<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax>(Flash_fwd_params) [with Kernel_traits=Flash_fwd_kernel_traits<160, 128, 32, 4, false, false, cutlass::bfloat16_t, Flash_kernel_traits<160, 128, 32, 4, cutlass::bfloat16_t>>, Is_dropout=false, Is_causal=true, Is_even_N=true, Is_even_K=true, Return_softmax=false]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(34): here
            instantiation of "void run_flash_fwd<Kernel_traits,Is_dropout,Is_causal>(Flash_fwd_params &, cudaStream_t) [with Kernel_traits=Flash_fwd_kernel_traits<160, 128, 32, 4, false, false, cutlass::bfloat16_t, Flash_kernel_traits<160, 128, 32, 4, cutlass::bfloat16_t>>, Is_dropout=false, Is_causal=true]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(155): here
            instantiation of "void run_mha_fwd_hdim160<T>(Flash_fwd_params &, cudaStream_t) [with T=cutlass::bfloat16_t]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_hdim160_bf16_sm80.cu(16): here

/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_kernel.h(275): warning: variable "tQrQ" was declared but never referenced
          detected during:
            instantiation of "void flash::compute_attn_1rowblock<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &, int, int, int) [with Kernel_traits=Flash_fwd_kernel_traits<160, 128, 32, 4, false, false, cutlass::bfloat16_t, Flash_kernel_traits<160, 128, 32, 4, cutlass::bfloat16_t>>, Is_dropout=false, Is_causal=true, Is_even_N=true, Is_even_K=true, Return_softmax=false, Params=Flash_fwd_params]" 
(588): here
            instantiation of "void flash::compute_attn<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &) [with Kernel_traits=Flash_fwd_kernel_traits<160, 128, 32, 4, false, false, cutlass::bfloat16_t, Flash_kernel_traits<160, 128, 32, 4, cutlass::bfloat16_t>>, Is_dropout=false, Is_causal=true, Is_even_N=true, Is_even_K=true, Return_softmax=false, Params=Flash_fwd_params]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(15): here
            instantiation of "void flash_fwd_kernel<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax>(Flash_fwd_params) [with Kernel_traits=Flash_fwd_kernel_traits<160, 128, 32, 4, false, false, cutlass::bfloat16_t, Flash_kernel_traits<160, 128, 32, 4, cutlass::bfloat16_t>>, Is_dropout=false, Is_causal=true, Is_even_N=true, Is_even_K=true, Return_softmax=false]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(34): here
            instantiation of "void run_flash_fwd<Kernel_traits,Is_dropout,Is_causal>(Flash_fwd_params &, cudaStream_t) [with Kernel_traits=Flash_fwd_kernel_traits<160, 128, 32, 4, false, false, cutlass::bfloat16_t, Flash_kernel_traits<160, 128, 32, 4, cutlass::bfloat16_t>>, Is_dropout=false, Is_causal=true]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(155): here
            instantiation of "void run_mha_fwd_hdim160<T>(Flash_fwd_params &, cudaStream_t) [with T=cutlass::bfloat16_t]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_hdim160_bf16_sm80.cu(16): here

/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_kernel.h(140): warning: variable "MMA_M" was declared but never referenced
          detected during:
            instantiation of "void flash::compute_attn_1rowblock<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &, int, int, int) [with Kernel_traits=Flash_fwd_kernel_traits<160, 128, 32, 4, false, false, cutlass::bfloat16_t, Flash_kernel_traits<160, 128, 32, 4, cutlass::bfloat16_t>>, Is_dropout=false, Is_causal=true, Is_even_N=true, Is_even_K=false, Return_softmax=false, Params=Flash_fwd_params]" 
(588): here
            instantiation of "void flash::compute_attn<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &) [with Kernel_traits=Flash_fwd_kernel_traits<160, 128, 32, 4, false, false, cutlass::bfloat16_t, Flash_kernel_traits<160, 128, 32, 4, cutlass::bfloat16_t>>, Is_dropout=false, Is_causal=true, Is_even_N=true, Is_even_K=false, Return_softmax=false, Params=Flash_fwd_params]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(15): here
            instantiation of "void flash_fwd_kernel<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax>(Flash_fwd_params) [with Kernel_traits=Flash_fwd_kernel_traits<160, 128, 32, 4, false, false, cutlass::bfloat16_t, Flash_kernel_traits<160, 128, 32, 4, cutlass::bfloat16_t>>, Is_dropout=false, Is_causal=true, Is_even_N=true, Is_even_K=false, Return_softmax=false]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(34): here
            instantiation of "void run_flash_fwd<Kernel_traits,Is_dropout,Is_causal>(Flash_fwd_params &, cudaStream_t) [with Kernel_traits=Flash_fwd_kernel_traits<160, 128, 32, 4, false, false, cutlass::bfloat16_t, Flash_kernel_traits<160, 128, 32, 4, cutlass::bfloat16_t>>, Is_dropout=false, Is_causal=true]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(155): here
            instantiation of "void run_mha_fwd_hdim160<T>(Flash_fwd_params &, cudaStream_t) [with T=cutlass::bfloat16_t]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_hdim160_bf16_sm80.cu(16): here

/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_kernel.h(275): warning: variable "tQrQ" was declared but never referenced
          detected during:
            instantiation of "void flash::compute_attn_1rowblock<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &, int, int, int) [with Kernel_traits=Flash_fwd_kernel_traits<160, 128, 32, 4, false, false, cutlass::bfloat16_t, Flash_kernel_traits<160, 128, 32, 4, cutlass::bfloat16_t>>, Is_dropout=false, Is_causal=true, Is_even_N=true, Is_even_K=false, Return_softmax=false, Params=Flash_fwd_params]" 
(588): here
            instantiation of "void flash::compute_attn<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &) [with Kernel_traits=Flash_fwd_kernel_traits<160, 128, 32, 4, false, false, cutlass::bfloat16_t, Flash_kernel_traits<160, 128, 32, 4, cutlass::bfloat16_t>>, Is_dropout=false, Is_causal=true, Is_even_N=true, Is_even_K=false, Return_softmax=false, Params=Flash_fwd_params]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(15): here
            instantiation of "void flash_fwd_kernel<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax>(Flash_fwd_params) [with Kernel_traits=Flash_fwd_kernel_traits<160, 128, 32, 4, false, false, cutlass::bfloat16_t, Flash_kernel_traits<160, 128, 32, 4, cutlass::bfloat16_t>>, Is_dropout=false, Is_causal=true, Is_even_N=true, Is_even_K=false, Return_softmax=false]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(34): here
            instantiation of "void run_flash_fwd<Kernel_traits,Is_dropout,Is_causal>(Flash_fwd_params &, cudaStream_t) [with Kernel_traits=Flash_fwd_kernel_traits<160, 128, 32, 4, false, false, cutlass::bfloat16_t, Flash_kernel_traits<160, 128, 32, 4, cutlass::bfloat16_t>>, Is_dropout=false, Is_causal=true]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(155): here
            instantiation of "void run_mha_fwd_hdim160<T>(Flash_fwd_params &, cudaStream_t) [with T=cutlass::bfloat16_t]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_hdim160_bf16_sm80.cu(16): here

/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_kernel.h(140): warning: variable "MMA_M" was declared but never referenced
          detected during:
            instantiation of "void flash::compute_attn_1rowblock<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &, int, int, int) [with Kernel_traits=Flash_fwd_kernel_traits<160, 128, 32, 4, false, false, cutlass::bfloat16_t, Flash_kernel_traits<160, 128, 32, 4, cutlass::bfloat16_t>>, Is_dropout=false, Is_causal=true, Is_even_N=false, Is_even_K=true, Return_softmax=false, Params=Flash_fwd_params]" 
(588): here
            instantiation of "void flash::compute_attn<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &) [with Kernel_traits=Flash_fwd_kernel_traits<160, 128, 32, 4, false, false, cutlass::bfloat16_t, Flash_kernel_traits<160, 128, 32, 4, cutlass::bfloat16_t>>, Is_dropout=false, Is_causal=true, Is_even_N=false, Is_even_K=true, Return_softmax=false, Params=Flash_fwd_params]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(15): here
            instantiation of "void flash_fwd_kernel<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax>(Flash_fwd_params) [with Kernel_traits=Flash_fwd_kernel_traits<160, 128, 32, 4, false, false, cutlass::bfloat16_t, Flash_kernel_traits<160, 128, 32, 4, cutlass::bfloat16_t>>, Is_dropout=false, Is_causal=true, Is_even_N=false, Is_even_K=true, Return_softmax=false]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(34): here
            instantiation of "void run_flash_fwd<Kernel_traits,Is_dropout,Is_causal>(Flash_fwd_params &, cudaStream_t) [with Kernel_traits=Flash_fwd_kernel_traits<160, 128, 32, 4, false, false, cutlass::bfloat16_t, Flash_kernel_traits<160, 128, 32, 4, cutlass::bfloat16_t>>, Is_dropout=false, Is_causal=true]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(155): here
            instantiation of "void run_mha_fwd_hdim160<T>(Flash_fwd_params &, cudaStream_t) [with T=cutlass::bfloat16_t]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_hdim160_bf16_sm80.cu(16): here

/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_kernel.h(275): warning: variable "tQrQ" was declared but never referenced
          detected during:
            instantiation of "void flash::compute_attn_1rowblock<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &, int, int, int) [with Kernel_traits=Flash_fwd_kernel_traits<160, 128, 32, 4, false, false, cutlass::bfloat16_t, Flash_kernel_traits<160, 128, 32, 4, cutlass::bfloat16_t>>, Is_dropout=false, Is_causal=true, Is_even_N=false, Is_even_K=true, Return_softmax=false, Params=Flash_fwd_params]" 
(588): here
            instantiation of "void flash::compute_attn<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &) [with Kernel_traits=Flash_fwd_kernel_traits<160, 128, 32, 4, false, false, cutlass::bfloat16_t, Flash_kernel_traits<160, 128, 32, 4, cutlass::bfloat16_t>>, Is_dropout=false, Is_causal=true, Is_even_N=false, Is_even_K=true, Return_softmax=false, Params=Flash_fwd_params]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(15): here
            instantiation of "void flash_fwd_kernel<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax>(Flash_fwd_params) [with Kernel_traits=Flash_fwd_kernel_traits<160, 128, 32, 4, false, false, cutlass::bfloat16_t, Flash_kernel_traits<160, 128, 32, 4, cutlass::bfloat16_t>>, Is_dropout=false, Is_causal=true, Is_even_N=false, Is_even_K=true, Return_softmax=false]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(34): here
            instantiation of "void run_flash_fwd<Kernel_traits,Is_dropout,Is_causal>(Flash_fwd_params &, cudaStream_t) [with Kernel_traits=Flash_fwd_kernel_traits<160, 128, 32, 4, false, false, cutlass::bfloat16_t, Flash_kernel_traits<160, 128, 32, 4, cutlass::bfloat16_t>>, Is_dropout=false, Is_causal=true]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(155): here
            instantiation of "void run_mha_fwd_hdim160<T>(Flash_fwd_params &, cudaStream_t) [with T=cutlass::bfloat16_t]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_hdim160_bf16_sm80.cu(16): here

/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_kernel.h(140): warning: variable "MMA_M" was declared but never referenced
          detected during:
            instantiation of "void flash::compute_attn_1rowblock<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &, int, int, int) [with Kernel_traits=Flash_fwd_kernel_traits<160, 128, 32, 4, false, false, cutlass::bfloat16_t, Flash_kernel_traits<160, 128, 32, 4, cutlass::bfloat16_t>>, Is_dropout=false, Is_causal=true, Is_even_N=false, Is_even_K=false, Return_softmax=false, Params=Flash_fwd_params]" 
(588): here
            instantiation of "void flash::compute_attn<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &) [with Kernel_traits=Flash_fwd_kernel_traits<160, 128, 32, 4, false, false, cutlass::bfloat16_t, Flash_kernel_traits<160, 128, 32, 4, cutlass::bfloat16_t>>, Is_dropout=false, Is_causal=true, Is_even_N=false, Is_even_K=false, Return_softmax=false, Params=Flash_fwd_params]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(15): here
            instantiation of "void flash_fwd_kernel<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax>(Flash_fwd_params) [with Kernel_traits=Flash_fwd_kernel_traits<160, 128, 32, 4, false, false, cutlass::bfloat16_t, Flash_kernel_traits<160, 128, 32, 4, cutlass::bfloat16_t>>, Is_dropout=false, Is_causal=true, Is_even_N=false, Is_even_K=false, Return_softmax=false]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(34): here
            instantiation of "void run_flash_fwd<Kernel_traits,Is_dropout,Is_causal>(Flash_fwd_params &, cudaStream_t) [with Kernel_traits=Flash_fwd_kernel_traits<160, 128, 32, 4, false, false, cutlass::bfloat16_t, Flash_kernel_traits<160, 128, 32, 4, cutlass::bfloat16_t>>, Is_dropout=false, Is_causal=true]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(155): here
            instantiation of "void run_mha_fwd_hdim160<T>(Flash_fwd_params &, cudaStream_t) [with T=cutlass::bfloat16_t]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_hdim160_bf16_sm80.cu(16): here

/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_kernel.h(275): warning: variable "tQrQ" was declared but never referenced
          detected during:
            instantiation of "void flash::compute_attn_1rowblock<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &, int, int, int) [with Kernel_traits=Flash_fwd_kernel_traits<160, 128, 32, 4, false, false, cutlass::bfloat16_t, Flash_kernel_traits<160, 128, 32, 4, cutlass::bfloat16_t>>, Is_dropout=false, Is_causal=true, Is_even_N=false, Is_even_K=false, Return_softmax=false, Params=Flash_fwd_params]" 
(588): here
            instantiation of "void flash::compute_attn<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &) [with Kernel_traits=Flash_fwd_kernel_traits<160, 128, 32, 4, false, false, cutlass::bfloat16_t, Flash_kernel_traits<160, 128, 32, 4, cutlass::bfloat16_t>>, Is_dropout=false, Is_causal=true, Is_even_N=false, Is_even_K=false, Return_softmax=false, Params=Flash_fwd_params]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(15): here
            instantiation of "void flash_fwd_kernel<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax>(Flash_fwd_params) [with Kernel_traits=Flash_fwd_kernel_traits<160, 128, 32, 4, false, false, cutlass::bfloat16_t, Flash_kernel_traits<160, 128, 32, 4, cutlass::bfloat16_t>>, Is_dropout=false, Is_causal=true, Is_even_N=false, Is_even_K=false, Return_softmax=false]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(34): here
            instantiation of "void run_flash_fwd<Kernel_traits,Is_dropout,Is_causal>(Flash_fwd_params &, cudaStream_t) [with Kernel_traits=Flash_fwd_kernel_traits<160, 128, 32, 4, false, false, cutlass::bfloat16_t, Flash_kernel_traits<160, 128, 32, 4, cutlass::bfloat16_t>>, Is_dropout=false, Is_causal=true]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(155): here
            instantiation of "void run_mha_fwd_hdim160<T>(Flash_fwd_params &, cudaStream_t) [with T=cutlass::bfloat16_t]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_hdim160_bf16_sm80.cu(16): here

/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_kernel.h(140): warning: variable "MMA_M" was declared but never referenced
          detected during:
            instantiation of "void flash::compute_attn_1rowblock<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &, int, int, int) [with Kernel_traits=Flash_fwd_kernel_traits<160, 128, 64, 8, false, false, cutlass::bfloat16_t, Flash_kernel_traits<160, 128, 64, 8, cutlass::bfloat16_t>>, Is_dropout=false, Is_causal=false, Is_even_N=true, Is_even_K=true, Return_softmax=false, Params=Flash_fwd_params]" 
(588): here
            instantiation of "void flash::compute_attn<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &) [with Kernel_traits=Flash_fwd_kernel_traits<160, 128, 64, 8, false, false, cutlass::bfloat16_t, Flash_kernel_traits<160, 128, 64, 8, cutlass::bfloat16_t>>, Is_dropout=false, Is_causal=false, Is_even_N=true, Is_even_K=true, Return_softmax=false, Params=Flash_fwd_params]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(15): here
            instantiation of "void flash_fwd_kernel<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax>(Flash_fwd_params) [with Kernel_traits=Flash_fwd_kernel_traits<160, 128, 64, 8, false, false, cutlass::bfloat16_t, Flash_kernel_traits<160, 128, 64, 8, cutlass::bfloat16_t>>, Is_dropout=false, Is_causal=false, Is_even_N=true, Is_even_K=true, Return_softmax=false]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(34): here
            instantiation of "void run_flash_fwd<Kernel_traits,Is_dropout,Is_causal>(Flash_fwd_params &, cudaStream_t) [with Kernel_traits=Flash_fwd_kernel_traits<160, 128, 64, 8, false, false, cutlass::bfloat16_t, Flash_kernel_traits<160, 128, 64, 8, cutlass::bfloat16_t>>, Is_dropout=false, Is_causal=false]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(155): here
            instantiation of "void run_mha_fwd_hdim160<T>(Flash_fwd_params &, cudaStream_t) [with T=cutlass::bfloat16_t]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_hdim160_bf16_sm80.cu(16): here

/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_kernel.h(275): warning: variable "tQrQ" was declared but never referenced
          detected during:
            instantiation of "void flash::compute_attn_1rowblock<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &, int, int, int) [with Kernel_traits=Flash_fwd_kernel_traits<160, 128, 64, 8, false, false, cutlass::bfloat16_t, Flash_kernel_traits<160, 128, 64, 8, cutlass::bfloat16_t>>, Is_dropout=false, Is_causal=false, Is_even_N=true, Is_even_K=true, Return_softmax=false, Params=Flash_fwd_params]" 
(588): here
            instantiation of "void flash::compute_attn<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &) [with Kernel_traits=Flash_fwd_kernel_traits<160, 128, 64, 8, false, false, cutlass::bfloat16_t, Flash_kernel_traits<160, 128, 64, 8, cutlass::bfloat16_t>>, Is_dropout=false, Is_causal=false, Is_even_N=true, Is_even_K=true, Return_softmax=false, Params=Flash_fwd_params]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(15): here
            instantiation of "void flash_fwd_kernel<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax>(Flash_fwd_params) [with Kernel_traits=Flash_fwd_kernel_traits<160, 128, 64, 8, false, false, cutlass::bfloat16_t, Flash_kernel_traits<160, 128, 64, 8, cutlass::bfloat16_t>>, Is_dropout=false, Is_causal=false, Is_even_N=true, Is_even_K=true, Return_softmax=false]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(34): here
            instantiation of "void run_flash_fwd<Kernel_traits,Is_dropout,Is_causal>(Flash_fwd_params &, cudaStream_t) [with Kernel_traits=Flash_fwd_kernel_traits<160, 128, 64, 8, false, false, cutlass::bfloat16_t, Flash_kernel_traits<160, 128, 64, 8, cutlass::bfloat16_t>>, Is_dropout=false, Is_causal=false]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(155): here
            instantiation of "void run_mha_fwd_hdim160<T>(Flash_fwd_params &, cudaStream_t) [with T=cutlass::bfloat16_t]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_hdim160_bf16_sm80.cu(16): here

/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_kernel.h(140): warning: variable "MMA_M" was declared but never referenced
          detected during:
            instantiation of "void flash::compute_attn_1rowblock<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &, int, int, int) [with Kernel_traits=Flash_fwd_kernel_traits<160, 128, 64, 8, false, false, cutlass::bfloat16_t, Flash_kernel_traits<160, 128, 64, 8, cutlass::bfloat16_t>>, Is_dropout=false, Is_causal=false, Is_even_N=true, Is_even_K=false, Return_softmax=false, Params=Flash_fwd_params]" 
(588): here
            instantiation of "void flash::compute_attn<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &) [with Kernel_traits=Flash_fwd_kernel_traits<160, 128, 64, 8, false, false, cutlass::bfloat16_t, Flash_kernel_traits<160, 128, 64, 8, cutlass::bfloat16_t>>, Is_dropout=false, Is_causal=false, Is_even_N=true, Is_even_K=false, Return_softmax=false, Params=Flash_fwd_params]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(15): here
            instantiation of "void flash_fwd_kernel<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax>(Flash_fwd_params) [with Kernel_traits=Flash_fwd_kernel_traits<160, 128, 64, 8, false, false, cutlass::bfloat16_t, Flash_kernel_traits<160, 128, 64, 8, cutlass::bfloat16_t>>, Is_dropout=false, Is_causal=false, Is_even_N=true, Is_even_K=false, Return_softmax=false]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(34): here
            instantiation of "void run_flash_fwd<Kernel_traits,Is_dropout,Is_causal>(Flash_fwd_params &, cudaStream_t) [with Kernel_traits=Flash_fwd_kernel_traits<160, 128, 64, 8, false, false, cutlass::bfloat16_t, Flash_kernel_traits<160, 128, 64, 8, cutlass::bfloat16_t>>, Is_dropout=false, Is_causal=false]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(155): here
            instantiation of "void run_mha_fwd_hdim160<T>(Flash_fwd_params &, cudaStream_t) [with T=cutlass::bfloat16_t]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_hdim160_bf16_sm80.cu(16): here

/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_kernel.h(275): warning: variable "tQrQ" was declared but never referenced
          detected during:
            instantiation of "void flash::compute_attn_1rowblock<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &, int, int, int) [with Kernel_traits=Flash_fwd_kernel_traits<160, 128, 64, 8, false, false, cutlass::bfloat16_t, Flash_kernel_traits<160, 128, 64, 8, cutlass::bfloat16_t>>, Is_dropout=false, Is_causal=false, Is_even_N=true, Is_even_K=false, Return_softmax=false, Params=Flash_fwd_params]" 
(588): here
            instantiation of "void flash::compute_attn<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &) [with Kernel_traits=Flash_fwd_kernel_traits<160, 128, 64, 8, false, false, cutlass::bfloat16_t, Flash_kernel_traits<160, 128, 64, 8, cutlass::bfloat16_t>>, Is_dropout=false, Is_causal=false, Is_even_N=true, Is_even_K=false, Return_softmax=false, Params=Flash_fwd_params]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(15): here
            instantiation of "void flash_fwd_kernel<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax>(Flash_fwd_params) [with Kernel_traits=Flash_fwd_kernel_traits<160, 128, 64, 8, false, false, cutlass::bfloat16_t, Flash_kernel_traits<160, 128, 64, 8, cutlass::bfloat16_t>>, Is_dropout=false, Is_causal=false, Is_even_N=true, Is_even_K=false, Return_softmax=false]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(34): here
            instantiation of "void run_flash_fwd<Kernel_traits,Is_dropout,Is_causal>(Flash_fwd_params &, cudaStream_t) [with Kernel_traits=Flash_fwd_kernel_traits<160, 128, 64, 8, false, false, cutlass::bfloat16_t, Flash_kernel_traits<160, 128, 64, 8, cutlass::bfloat16_t>>, Is_dropout=false, Is_causal=false]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(155): here
            instantiation of "void run_mha_fwd_hdim160<T>(Flash_fwd_params &, cudaStream_t) [with T=cutlass::bfloat16_t]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_hdim160_bf16_sm80.cu(16): here

/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_kernel.h(140): warning: variable "MMA_M" was declared but never referenced
          detected during:
            instantiation of "void flash::compute_attn_1rowblock<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &, int, int, int) [with Kernel_traits=Flash_fwd_kernel_traits<160, 128, 64, 8, false, false, cutlass::bfloat16_t, Flash_kernel_traits<160, 128, 64, 8, cutlass::bfloat16_t>>, Is_dropout=false, Is_causal=false, Is_even_N=false, Is_even_K=true, Return_softmax=false, Params=Flash_fwd_params]" 
(588): here
            instantiation of "void flash::compute_attn<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &) [with Kernel_traits=Flash_fwd_kernel_traits<160, 128, 64, 8, false, false, cutlass::bfloat16_t, Flash_kernel_traits<160, 128, 64, 8, cutlass::bfloat16_t>>, Is_dropout=false, Is_causal=false, Is_even_N=false, Is_even_K=true, Return_softmax=false, Params=Flash_fwd_params]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(15): here
            instantiation of "void flash_fwd_kernel<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax>(Flash_fwd_params) [with Kernel_traits=Flash_fwd_kernel_traits<160, 128, 64, 8, false, false, cutlass::bfloat16_t, Flash_kernel_traits<160, 128, 64, 8, cutlass::bfloat16_t>>, Is_dropout=false, Is_causal=false, Is_even_N=false, Is_even_K=true, Return_softmax=false]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(34): here
            instantiation of "void run_flash_fwd<Kernel_traits,Is_dropout,Is_causal>(Flash_fwd_params &, cudaStream_t) [with Kernel_traits=Flash_fwd_kernel_traits<160, 128, 64, 8, false, false, cutlass::bfloat16_t, Flash_kernel_traits<160, 128, 64, 8, cutlass::bfloat16_t>>, Is_dropout=false, Is_causal=false]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(155): here
            instantiation of "void run_mha_fwd_hdim160<T>(Flash_fwd_params &, cudaStream_t) [with T=cutlass::bfloat16_t]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_hdim160_bf16_sm80.cu(16): here

/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_kernel.h(275): warning: variable "tQrQ" was declared but never referenced
          detected during:
            instantiation of "void flash::compute_attn_1rowblock<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &, int, int, int) [with Kernel_traits=Flash_fwd_kernel_traits<160, 128, 64, 8, false, false, cutlass::bfloat16_t, Flash_kernel_traits<160, 128, 64, 8, cutlass::bfloat16_t>>, Is_dropout=false, Is_causal=false, Is_even_N=false, Is_even_K=true, Return_softmax=false, Params=Flash_fwd_params]" 
(588): here
            instantiation of "void flash::compute_attn<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &) [with Kernel_traits=Flash_fwd_kernel_traits<160, 128, 64, 8, false, false, cutlass::bfloat16_t, Flash_kernel_traits<160, 128, 64, 8, cutlass::bfloat16_t>>, Is_dropout=false, Is_causal=false, Is_even_N=false, Is_even_K=true, Return_softmax=false, Params=Flash_fwd_params]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(15): here
            instantiation of "void flash_fwd_kernel<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax>(Flash_fwd_params) [with Kernel_traits=Flash_fwd_kernel_traits<160, 128, 64, 8, false, false, cutlass::bfloat16_t, Flash_kernel_traits<160, 128, 64, 8, cutlass::bfloat16_t>>, Is_dropout=false, Is_causal=false, Is_even_N=false, Is_even_K=true, Return_softmax=false]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(34): here
            instantiation of "void run_flash_fwd<Kernel_traits,Is_dropout,Is_causal>(Flash_fwd_params &, cudaStream_t) [with Kernel_traits=Flash_fwd_kernel_traits<160, 128, 64, 8, false, false, cutlass::bfloat16_t, Flash_kernel_traits<160, 128, 64, 8, cutlass::bfloat16_t>>, Is_dropout=false, Is_causal=false]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(155): here
            instantiation of "void run_mha_fwd_hdim160<T>(Flash_fwd_params &, cudaStream_t) [with T=cutlass::bfloat16_t]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_hdim160_bf16_sm80.cu(16): here

/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_kernel.h(140): warning: variable "MMA_M" was declared but never referenced
          detected during:
            instantiation of "void flash::compute_attn_1rowblock<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &, int, int, int) [with Kernel_traits=Flash_fwd_kernel_traits<160, 128, 64, 8, false, false, cutlass::bfloat16_t, Flash_kernel_traits<160, 128, 64, 8, cutlass::bfloat16_t>>, Is_dropout=false, Is_causal=false, Is_even_N=false, Is_even_K=false, Return_softmax=false, Params=Flash_fwd_params]" 
(588): here
            instantiation of "void flash::compute_attn<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &) [with Kernel_traits=Flash_fwd_kernel_traits<160, 128, 64, 8, false, false, cutlass::bfloat16_t, Flash_kernel_traits<160, 128, 64, 8, cutlass::bfloat16_t>>, Is_dropout=false, Is_causal=false, Is_even_N=false, Is_even_K=false, Return_softmax=false, Params=Flash_fwd_params]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(15): here
            instantiation of "void flash_fwd_kernel<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax>(Flash_fwd_params) [with Kernel_traits=Flash_fwd_kernel_traits<160, 128, 64, 8, false, false, cutlass::bfloat16_t, Flash_kernel_traits<160, 128, 64, 8, cutlass::bfloat16_t>>, Is_dropout=false, Is_causal=false, Is_even_N=false, Is_even_K=false, Return_softmax=false]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(34): here
            instantiation of "void run_flash_fwd<Kernel_traits,Is_dropout,Is_causal>(Flash_fwd_params &, cudaStream_t) [with Kernel_traits=Flash_fwd_kernel_traits<160, 128, 64, 8, false, false, cutlass::bfloat16_t, Flash_kernel_traits<160, 128, 64, 8, cutlass::bfloat16_t>>, Is_dropout=false, Is_causal=false]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(155): here
            instantiation of "void run_mha_fwd_hdim160<T>(Flash_fwd_params &, cudaStream_t) [with T=cutlass::bfloat16_t]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_hdim160_bf16_sm80.cu(16): here

/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_kernel.h(275): warning: variable "tQrQ" was declared but never referenced
          detected during:
            instantiation of "void flash::compute_attn_1rowblock<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &, int, int, int) [with Kernel_traits=Flash_fwd_kernel_traits<160, 128, 64, 8, false, false, cutlass::bfloat16_t, Flash_kernel_traits<160, 128, 64, 8, cutlass::bfloat16_t>>, Is_dropout=false, Is_causal=false, Is_even_N=false, Is_even_K=false, Return_softmax=false, Params=Flash_fwd_params]" 
(588): here
            instantiation of "void flash::compute_attn<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &) [with Kernel_traits=Flash_fwd_kernel_traits<160, 128, 64, 8, false, false, cutlass::bfloat16_t, Flash_kernel_traits<160, 128, 64, 8, cutlass::bfloat16_t>>, Is_dropout=false, Is_causal=false, Is_even_N=false, Is_even_K=false, Return_softmax=false, Params=Flash_fwd_params]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(15): here
            instantiation of "void flash_fwd_kernel<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax>(Flash_fwd_params) [with Kernel_traits=Flash_fwd_kernel_traits<160, 128, 64, 8, false, false, cutlass::bfloat16_t, Flash_kernel_traits<160, 128, 64, 8, cutlass::bfloat16_t>>, Is_dropout=false, Is_causal=false, Is_even_N=false, Is_even_K=false, Return_softmax=false]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(34): here
            instantiation of "void run_flash_fwd<Kernel_traits,Is_dropout,Is_causal>(Flash_fwd_params &, cudaStream_t) [with Kernel_traits=Flash_fwd_kernel_traits<160, 128, 64, 8, false, false, cutlass::bfloat16_t, Flash_kernel_traits<160, 128, 64, 8, cutlass::bfloat16_t>>, Is_dropout=false, Is_causal=false]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(155): here
            instantiation of "void run_mha_fwd_hdim160<T>(Flash_fwd_params &, cudaStream_t) [with T=cutlass::bfloat16_t]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_hdim160_bf16_sm80.cu(16): here

/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_kernel.h(140): warning: variable "MMA_M" was declared but never referenced
          detected during:
            instantiation of "void flash::compute_attn_1rowblock<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &, int, int, int) [with Kernel_traits=Flash_fwd_kernel_traits<160, 128, 32, 4, false, false, cutlass::bfloat16_t, Flash_kernel_traits<160, 128, 32, 4, cutlass::bfloat16_t>>, Is_dropout=false, Is_causal=false, Is_even_N=true, Is_even_K=true, Return_softmax=false, Params=Flash_fwd_params]" 
(588): here
            instantiation of "void flash::compute_attn<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &) [with Kernel_traits=Flash_fwd_kernel_traits<160, 128, 32, 4, false, false, cutlass::bfloat16_t, Flash_kernel_traits<160, 128, 32, 4, cutlass::bfloat16_t>>, Is_dropout=false, Is_causal=false, Is_even_N=true, Is_even_K=true, Return_softmax=false, Params=Flash_fwd_params]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(15): here
            instantiation of "void flash_fwd_kernel<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax>(Flash_fwd_params) [with Kernel_traits=Flash_fwd_kernel_traits<160, 128, 32, 4, false, false, cutlass::bfloat16_t, Flash_kernel_traits<160, 128, 32, 4, cutlass::bfloat16_t>>, Is_dropout=false, Is_causal=false, Is_even_N=true, Is_even_K=true, Return_softmax=false]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(34): here
            instantiation of "void run_flash_fwd<Kernel_traits,Is_dropout,Is_causal>(Flash_fwd_params &, cudaStream_t) [with Kernel_traits=Flash_fwd_kernel_traits<160, 128, 32, 4, false, false, cutlass::bfloat16_t, Flash_kernel_traits<160, 128, 32, 4, cutlass::bfloat16_t>>, Is_dropout=false, Is_causal=false]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(155): here
            instantiation of "void run_mha_fwd_hdim160<T>(Flash_fwd_params &, cudaStream_t) [with T=cutlass::bfloat16_t]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_hdim160_bf16_sm80.cu(16): here

/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_kernel.h(275): warning: variable "tQrQ" was declared but never referenced
          detected during:
            instantiation of "void flash::compute_attn_1rowblock<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &, int, int, int) [with Kernel_traits=Flash_fwd_kernel_traits<160, 128, 32, 4, false, false, cutlass::bfloat16_t, Flash_kernel_traits<160, 128, 32, 4, cutlass::bfloat16_t>>, Is_dropout=false, Is_causal=false, Is_even_N=true, Is_even_K=true, Return_softmax=false, Params=Flash_fwd_params]" 
(588): here
            instantiation of "void flash::compute_attn<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &) [with Kernel_traits=Flash_fwd_kernel_traits<160, 128, 32, 4, false, false, cutlass::bfloat16_t, Flash_kernel_traits<160, 128, 32, 4, cutlass::bfloat16_t>>, Is_dropout=false, Is_causal=false, Is_even_N=true, Is_even_K=true, Return_softmax=false, Params=Flash_fwd_params]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(15): here
            instantiation of "void flash_fwd_kernel<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax>(Flash_fwd_params) [with Kernel_traits=Flash_fwd_kernel_traits<160, 128, 32, 4, false, false, cutlass::bfloat16_t, Flash_kernel_traits<160, 128, 32, 4, cutlass::bfloat16_t>>, Is_dropout=false, Is_causal=false, Is_even_N=true, Is_even_K=true, Return_softmax=false]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(34): here
            instantiation of "void run_flash_fwd<Kernel_traits,Is_dropout,Is_causal>(Flash_fwd_params &, cudaStream_t) [with Kernel_traits=Flash_fwd_kernel_traits<160, 128, 32, 4, false, false, cutlass::bfloat16_t, Flash_kernel_traits<160, 128, 32, 4, cutlass::bfloat16_t>>, Is_dropout=false, Is_causal=false]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(155): here
            instantiation of "void run_mha_fwd_hdim160<T>(Flash_fwd_params &, cudaStream_t) [with T=cutlass::bfloat16_t]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_hdim160_bf16_sm80.cu(16): here

/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_kernel.h(140): warning: variable "MMA_M" was declared but never referenced
          detected during:
            instantiation of "void flash::compute_attn_1rowblock<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &, int, int, int) [with Kernel_traits=Flash_fwd_kernel_traits<160, 128, 32, 4, false, false, cutlass::bfloat16_t, Flash_kernel_traits<160, 128, 32, 4, cutlass::bfloat16_t>>, Is_dropout=false, Is_causal=false, Is_even_N=true, Is_even_K=false, Return_softmax=false, Params=Flash_fwd_params]" 
(588): here
            instantiation of "void flash::compute_attn<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &) [with Kernel_traits=Flash_fwd_kernel_traits<160, 128, 32, 4, false, false, cutlass::bfloat16_t, Flash_kernel_traits<160, 128, 32, 4, cutlass::bfloat16_t>>, Is_dropout=false, Is_causal=false, Is_even_N=true, Is_even_K=false, Return_softmax=false, Params=Flash_fwd_params]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(15): here
            instantiation of "void flash_fwd_kernel<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax>(Flash_fwd_params) [with Kernel_traits=Flash_fwd_kernel_traits<160, 128, 32, 4, false, false, cutlass::bfloat16_t, Flash_kernel_traits<160, 128, 32, 4, cutlass::bfloat16_t>>, Is_dropout=false, Is_causal=false, Is_even_N=true, Is_even_K=false, Return_softmax=false]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(34): here
            instantiation of "void run_flash_fwd<Kernel_traits,Is_dropout,Is_causal>(Flash_fwd_params &, cudaStream_t) [with Kernel_traits=Flash_fwd_kernel_traits<160, 128, 32, 4, false, false, cutlass::bfloat16_t, Flash_kernel_traits<160, 128, 32, 4, cutlass::bfloat16_t>>, Is_dropout=false, Is_causal=false]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(155): here
            instantiation of "void run_mha_fwd_hdim160<T>(Flash_fwd_params &, cudaStream_t) [with T=cutlass::bfloat16_t]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_hdim160_bf16_sm80.cu(16): here

/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_kernel.h(275): warning: variable "tQrQ" was declared but never referenced
          detected during:
            instantiation of "void flash::compute_attn_1rowblock<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &, int, int, int) [with Kernel_traits=Flash_fwd_kernel_traits<160, 128, 32, 4, false, false, cutlass::bfloat16_t, Flash_kernel_traits<160, 128, 32, 4, cutlass::bfloat16_t>>, Is_dropout=false, Is_causal=false, Is_even_N=true, Is_even_K=false, Return_softmax=false, Params=Flash_fwd_params]" 
(588): here
            instantiation of "void flash::compute_attn<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &) [with Kernel_traits=Flash_fwd_kernel_traits<160, 128, 32, 4, false, false, cutlass::bfloat16_t, Flash_kernel_traits<160, 128, 32, 4, cutlass::bfloat16_t>>, Is_dropout=false, Is_causal=false, Is_even_N=true, Is_even_K=false, Return_softmax=false, Params=Flash_fwd_params]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(15): here
            instantiation of "void flash_fwd_kernel<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax>(Flash_fwd_params) [with Kernel_traits=Flash_fwd_kernel_traits<160, 128, 32, 4, false, false, cutlass::bfloat16_t, Flash_kernel_traits<160, 128, 32, 4, cutlass::bfloat16_t>>, Is_dropout=false, Is_causal=false, Is_even_N=true, Is_even_K=false, Return_softmax=false]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(34): here
            instantiation of "void run_flash_fwd<Kernel_traits,Is_dropout,Is_causal>(Flash_fwd_params &, cudaStream_t) [with Kernel_traits=Flash_fwd_kernel_traits<160, 128, 32, 4, false, false, cutlass::bfloat16_t, Flash_kernel_traits<160, 128, 32, 4, cutlass::bfloat16_t>>, Is_dropout=false, Is_causal=false]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(155): here
            instantiation of "void run_mha_fwd_hdim160<T>(Flash_fwd_params &, cudaStream_t) [with T=cutlass::bfloat16_t]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_hdim160_bf16_sm80.cu(16): here

/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_kernel.h(140): warning: variable "MMA_M" was declared but never referenced
          detected during:
            instantiation of "void flash::compute_attn_1rowblock<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &, int, int, int) [with Kernel_traits=Flash_fwd_kernel_traits<160, 128, 32, 4, false, false, cutlass::bfloat16_t, Flash_kernel_traits<160, 128, 32, 4, cutlass::bfloat16_t>>, Is_dropout=false, Is_causal=false, Is_even_N=false, Is_even_K=true, Return_softmax=false, Params=Flash_fwd_params]" 
(588): here
            instantiation of "void flash::compute_attn<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &) [with Kernel_traits=Flash_fwd_kernel_traits<160, 128, 32, 4, false, false, cutlass::bfloat16_t, Flash_kernel_traits<160, 128, 32, 4, cutlass::bfloat16_t>>, Is_dropout=false, Is_causal=false, Is_even_N=false, Is_even_K=true, Return_softmax=false, Params=Flash_fwd_params]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(15): here
            instantiation of "void flash_fwd_kernel<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax>(Flash_fwd_params) [with Kernel_traits=Flash_fwd_kernel_traits<160, 128, 32, 4, false, false, cutlass::bfloat16_t, Flash_kernel_traits<160, 128, 32, 4, cutlass::bfloat16_t>>, Is_dropout=false, Is_causal=false, Is_even_N=false, Is_even_K=true, Return_softmax=false]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(34): here
            instantiation of "void run_flash_fwd<Kernel_traits,Is_dropout,Is_causal>(Flash_fwd_params &, cudaStream_t) [with Kernel_traits=Flash_fwd_kernel_traits<160, 128, 32, 4, false, false, cutlass::bfloat16_t, Flash_kernel_traits<160, 128, 32, 4, cutlass::bfloat16_t>>, Is_dropout=false, Is_causal=false]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(155): here
            instantiation of "void run_mha_fwd_hdim160<T>(Flash_fwd_params &, cudaStream_t) [with T=cutlass::bfloat16_t]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_hdim160_bf16_sm80.cu(16): here

/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_kernel.h(275): warning: variable "tQrQ" was declared but never referenced
          detected during:
            instantiation of "void flash::compute_attn_1rowblock<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &, int, int, int) [with Kernel_traits=Flash_fwd_kernel_traits<160, 128, 32, 4, false, false, cutlass::bfloat16_t, Flash_kernel_traits<160, 128, 32, 4, cutlass::bfloat16_t>>, Is_dropout=false, Is_causal=false, Is_even_N=false, Is_even_K=true, Return_softmax=false, Params=Flash_fwd_params]" 
(588): here
            instantiation of "void flash::compute_attn<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &) [with Kernel_traits=Flash_fwd_kernel_traits<160, 128, 32, 4, false, false, cutlass::bfloat16_t, Flash_kernel_traits<160, 128, 32, 4, cutlass::bfloat16_t>>, Is_dropout=false, Is_causal=false, Is_even_N=false, Is_even_K=true, Return_softmax=false, Params=Flash_fwd_params]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(15): here
            instantiation of "void flash_fwd_kernel<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax>(Flash_fwd_params) [with Kernel_traits=Flash_fwd_kernel_traits<160, 128, 32, 4, false, false, cutlass::bfloat16_t, Flash_kernel_traits<160, 128, 32, 4, cutlass::bfloat16_t>>, Is_dropout=false, Is_causal=false, Is_even_N=false, Is_even_K=true, Return_softmax=false]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(34): here
            instantiation of "void run_flash_fwd<Kernel_traits,Is_dropout,Is_causal>(Flash_fwd_params &, cudaStream_t) [with Kernel_traits=Flash_fwd_kernel_traits<160, 128, 32, 4, false, false, cutlass::bfloat16_t, Flash_kernel_traits<160, 128, 32, 4, cutlass::bfloat16_t>>, Is_dropout=false, Is_causal=false]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(155): here
            instantiation of "void run_mha_fwd_hdim160<T>(Flash_fwd_params &, cudaStream_t) [with T=cutlass::bfloat16_t]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_hdim160_bf16_sm80.cu(16): here

/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_kernel.h(140): warning: variable "MMA_M" was declared but never referenced
          detected during:
            instantiation of "void flash::compute_attn_1rowblock<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &, int, int, int) [with Kernel_traits=Flash_fwd_kernel_traits<160, 128, 32, 4, false, false, cutlass::bfloat16_t, Flash_kernel_traits<160, 128, 32, 4, cutlass::bfloat16_t>>, Is_dropout=false, Is_causal=false, Is_even_N=false, Is_even_K=false, Return_softmax=false, Params=Flash_fwd_params]" 
(588): here
            instantiation of "void flash::compute_attn<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &) [with Kernel_traits=Flash_fwd_kernel_traits<160, 128, 32, 4, false, false, cutlass::bfloat16_t, Flash_kernel_traits<160, 128, 32, 4, cutlass::bfloat16_t>>, Is_dropout=false, Is_causal=false, Is_even_N=false, Is_even_K=false, Return_softmax=false, Params=Flash_fwd_params]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(15): here
            instantiation of "void flash_fwd_kernel<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax>(Flash_fwd_params) [with Kernel_traits=Flash_fwd_kernel_traits<160, 128, 32, 4, false, false, cutlass::bfloat16_t, Flash_kernel_traits<160, 128, 32, 4, cutlass::bfloat16_t>>, Is_dropout=false, Is_causal=false, Is_even_N=false, Is_even_K=false, Return_softmax=false]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(34): here
            instantiation of "void run_flash_fwd<Kernel_traits,Is_dropout,Is_causal>(Flash_fwd_params &, cudaStream_t) [with Kernel_traits=Flash_fwd_kernel_traits<160, 128, 32, 4, false, false, cutlass::bfloat16_t, Flash_kernel_traits<160, 128, 32, 4, cutlass::bfloat16_t>>, Is_dropout=false, Is_causal=false]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(155): here
            instantiation of "void run_mha_fwd_hdim160<T>(Flash_fwd_params &, cudaStream_t) [with T=cutlass::bfloat16_t]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_hdim160_bf16_sm80.cu(16): here

/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_kernel.h(275): warning: variable "tQrQ" was declared but never referenced
          detected during:
            instantiation of "void flash::compute_attn_1rowblock<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &, int, int, int) [with Kernel_traits=Flash_fwd_kernel_traits<160, 128, 32, 4, false, false, cutlass::bfloat16_t, Flash_kernel_traits<160, 128, 32, 4, cutlass::bfloat16_t>>, Is_dropout=false, Is_causal=false, Is_even_N=false, Is_even_K=false, Return_softmax=false, Params=Flash_fwd_params]" 
(588): here
            instantiation of "void flash::compute_attn<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &) [with Kernel_traits=Flash_fwd_kernel_traits<160, 128, 32, 4, false, false, cutlass::bfloat16_t, Flash_kernel_traits<160, 128, 32, 4, cutlass::bfloat16_t>>, Is_dropout=false, Is_causal=false, Is_even_N=false, Is_even_K=false, Return_softmax=false, Params=Flash_fwd_params]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(15): here
            instantiation of "void flash_fwd_kernel<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax>(Flash_fwd_params) [with Kernel_traits=Flash_fwd_kernel_traits<160, 128, 32, 4, false, false, cutlass::bfloat16_t, Flash_kernel_traits<160, 128, 32, 4, cutlass::bfloat16_t>>, Is_dropout=false, Is_causal=false, Is_even_N=false, Is_even_K=false, Return_softmax=false]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(34): here
            instantiation of "void run_flash_fwd<Kernel_traits,Is_dropout,Is_causal>(Flash_fwd_params &, cudaStream_t) [with Kernel_traits=Flash_fwd_kernel_traits<160, 128, 32, 4, false, false, cutlass::bfloat16_t, Flash_kernel_traits<160, 128, 32, 4, cutlass::bfloat16_t>>, Is_dropout=false, Is_causal=false]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(155): here
            instantiation of "void run_mha_fwd_hdim160<T>(Flash_fwd_params &, cudaStream_t) [with T=cutlass::bfloat16_t]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_hdim160_bf16_sm80.cu(16): here

ptxas info    : 2 bytes gmem
ptxas info    : Compiling entry function '_Z16flash_fwd_kernelI23Flash_fwd_kernel_traitsILi160ELi128ELi32ELi4ELb0ELb0EN7cutlass10bfloat16_tE19Flash_kernel_traitsILi160ELi128ELi32ELi4ES2_EELb0ELb0ELb0ELb0ELb0EEv16Flash_fwd_params' for 'sm_80'
ptxas info    : Function properties for _Z16flash_fwd_kernelI23Flash_fwd_kernel_traitsILi160ELi128ELi32ELi4ELb0ELb0EN7cutlass10bfloat16_tE19Flash_kernel_traitsILi160ELi128ELi32ELi4ES2_EELb0ELb0ELb0ELb0ELb0EEv16Flash_fwd_params
    32 bytes stack frame, 48 bytes spill stores, 48 bytes spill loads
ptxas info    : Used 255 registers, 584 bytes cmem[0]
ptxas info    : Compiling entry function '_Z16flash_fwd_kernelI23Flash_fwd_kernel_traitsILi160ELi128ELi32ELi4ELb0ELb0EN7cutlass10bfloat16_tE19Flash_kernel_traitsILi160ELi128ELi32ELi4ES2_EELb0ELb0ELb0ELb1ELb0EEv16Flash_fwd_params' for 'sm_80'
ptxas info    : Function properties for _Z16flash_fwd_kernelI23Flash_fwd_kernel_traitsILi160ELi128ELi32ELi4ELb0ELb0EN7cutlass10bfloat16_tE19Flash_kernel_traitsILi160ELi128ELi32ELi4ES2_EELb0ELb0ELb0ELb1ELb0EEv16Flash_fwd_params
    48 bytes stack frame, 80 bytes spill stores, 64 bytes spill loads
ptxas info    : Used 255 registers, 584 bytes cmem[0]
ptxas info    : Compiling entry function '_Z16flash_fwd_kernelI23Flash_fwd_kernel_traitsILi160ELi128ELi32ELi4ELb0ELb0EN7cutlass10bfloat16_tE19Flash_kernel_traitsILi160ELi128ELi32ELi4ES2_EELb0ELb0ELb1ELb0ELb0EEv16Flash_fwd_params' for 'sm_80'
ptxas info    : Function properties for _Z16flash_fwd_kernelI23Flash_fwd_kernel_traitsILi160ELi128ELi32ELi4ELb0ELb0EN7cutlass10bfloat16_tE19Flash_kernel_traitsILi160ELi128ELi32ELi4ES2_EELb0ELb0ELb1ELb0ELb0EEv16Flash_fwd_params
    24 bytes stack frame, 36 bytes spill stores, 36 bytes spill loads
ptxas info    : Used 255 registers, 584 bytes cmem[0]
ptxas info    : Compiling entry function '_Z16flash_fwd_kernelI23Flash_fwd_kernel_traitsILi160ELi128ELi32ELi4ELb0ELb0EN7cutlass10bfloat16_tE19Flash_kernel_traitsILi160ELi128ELi32ELi4ES2_EELb0ELb0ELb1ELb1ELb0EEv16Flash_fwd_params' for 'sm_80'
ptxas info    : Function properties for _Z16flash_fwd_kernelI23Flash_fwd_kernel_traitsILi160ELi128ELi32ELi4ELb0ELb0EN7cutlass10bfloat16_tE19Flash_kernel_traitsILi160ELi128ELi32ELi4ES2_EELb0ELb0ELb1ELb1ELb0EEv16Flash_fwd_params
    24 bytes stack frame, 40 bytes spill stores, 40 bytes spill loads
ptxas info    : Used 255 registers, 584 bytes cmem[0]
ptxas info    : Compiling entry function '_Z16flash_fwd_kernelI23Flash_fwd_kernel_traitsILi160ELi128ELi64ELi8ELb0ELb0EN7cutlass10bfloat16_tE19Flash_kernel_traitsILi160ELi128ELi64ELi8ES2_EELb0ELb0ELb0ELb0ELb0EEv16Flash_fwd_params' for 'sm_80'
ptxas info    : Function properties for _Z16flash_fwd_kernelI23Flash_fwd_kernel_traitsILi160ELi128ELi64ELi8ELb0ELb0EN7cutlass10bfloat16_tE19Flash_kernel_traitsILi160ELi128ELi64ELi8ES2_EELb0ELb0ELb0ELb0ELb0EEv16Flash_fwd_params
    0 bytes stack frame, 0 bytes spill stores, 0 bytes spill loads
ptxas info    : Used 165 registers, 584 bytes cmem[0]
ptxas info    : Compiling entry function '_Z16flash_fwd_kernelI23Flash_fwd_kernel_traitsILi160ELi128ELi64ELi8ELb0ELb0EN7cutlass10bfloat16_tE19Flash_kernel_traitsILi160ELi128ELi64ELi8ES2_EELb0ELb0ELb0ELb1ELb0EEv16Flash_fwd_params' for 'sm_80'
ptxas info    : Function properties for _Z16flash_fwd_kernelI23Flash_fwd_kernel_traitsILi160ELi128ELi64ELi8ELb0ELb0EN7cutlass10bfloat16_tE19Flash_kernel_traitsILi160ELi128ELi64ELi8ES2_EELb0ELb0ELb0ELb1ELb0EEv16Flash_fwd_params
    0 bytes stack frame, 0 bytes spill stores, 0 bytes spill loads
ptxas info    : Used 163 registers, 584 bytes cmem[0]
ptxas info    : Compiling entry function '_Z16flash_fwd_kernelI23Flash_fwd_kernel_traitsILi160ELi128ELi64ELi8ELb0ELb0EN7cutlass10bfloat16_tE19Flash_kernel_traitsILi160ELi128ELi64ELi8ES2_EELb0ELb0ELb1ELb0ELb0EEv16Flash_fwd_params' for 'sm_80'
ptxas info    : Function properties for _Z16flash_fwd_kernelI23Flash_fwd_kernel_traitsILi160ELi128ELi64ELi8ELb0ELb0EN7cutlass10bfloat16_tE19Flash_kernel_traitsILi160ELi128ELi64ELi8ES2_EELb0ELb0ELb1ELb0ELb0EEv16Flash_fwd_params
    0 bytes stack frame, 0 bytes spill stores, 0 bytes spill loads
ptxas info    : Used 171 registers, 584 bytes cmem[0]
ptxas info    : Compiling entry function '_Z16flash_fwd_kernelI23Flash_fwd_kernel_traitsILi160ELi128ELi64ELi8ELb0ELb0EN7cutlass10bfloat16_tE19Flash_kernel_traitsILi160ELi128ELi64ELi8ES2_EELb0ELb0ELb1ELb1ELb0EEv16Flash_fwd_params' for 'sm_80'
ptxas info    : Function properties for _Z16flash_fwd_kernelI23Flash_fwd_kernel_traitsILi160ELi128ELi64ELi8ELb0ELb0EN7cutlass10bfloat16_tE19Flash_kernel_traitsILi160ELi128ELi64ELi8ES2_EELb0ELb0ELb1ELb1ELb0EEv16Flash_fwd_params
    0 bytes stack frame, 0 bytes spill stores, 0 bytes spill loads
ptxas info    : Used 170 registers, 584 bytes cmem[0]
ptxas info    : Compiling entry function '_Z16flash_fwd_kernelI23Flash_fwd_kernel_traitsILi160ELi128ELi32ELi4ELb0ELb0EN7cutlass10bfloat16_tE19Flash_kernel_traitsILi160ELi128ELi32ELi4ES2_EELb0ELb1ELb0ELb0ELb0EEv16Flash_fwd_params' for 'sm_80'
ptxas info    : Function properties for _Z16flash_fwd_kernelI23Flash_fwd_kernel_traitsILi160ELi128ELi32ELi4ELb0ELb0EN7cutlass10bfloat16_tE19Flash_kernel_traitsILi160ELi128ELi32ELi4ES2_EELb0ELb1ELb0ELb0ELb0EEv16Flash_fwd_params
    56 bytes stack frame, 132 bytes spill stores, 112 bytes spill loads
ptxas info    : Used 255 registers, 584 bytes cmem[0]
ptxas info    : Compiling entry function '_Z16flash_fwd_kernelI23Flash_fwd_kernel_traitsILi160ELi128ELi32ELi4ELb0ELb0EN7cutlass10bfloat16_tE19Flash_kernel_traitsILi160ELi128ELi32ELi4ES2_EELb0ELb1ELb0ELb1ELb0EEv16Flash_fwd_params' for 'sm_80'
ptxas info    : Function properties for _Z16flash_fwd_kernelI23Flash_fwd_kernel_traitsILi160ELi128ELi32ELi4ELb0ELb0EN7cutlass10bfloat16_tE19Flash_kernel_traitsILi160ELi128ELi32ELi4ES2_EELb0ELb1ELb0ELb1ELb0EEv16Flash_fwd_params
    40 bytes stack frame, 156 bytes spill stores, 96 bytes spill loads
ptxas info    : Used 255 registers, 584 bytes cmem[0]
ptxas info    : Compiling entry function '_Z16flash_fwd_kernelI23Flash_fwd_kernel_traitsILi160ELi128ELi32ELi4ELb0ELb0EN7cutlass10bfloat16_tE19Flash_kernel_traitsILi160ELi128ELi32ELi4ES2_EELb0ELb1ELb1ELb0ELb0EEv16Flash_fwd_params' for 'sm_80'
ptxas info    : Function properties for _Z16flash_fwd_kernelI23Flash_fwd_kernel_traitsILi160ELi128ELi32ELi4ELb0ELb0EN7cutlass10bfloat16_tE19Flash_kernel_traitsILi160ELi128ELi32ELi4ES2_EELb0ELb1ELb1ELb0ELb0EEv16Flash_fwd_params
    24 bytes stack frame, 84 bytes spill stores, 44 bytes spill loads
ptxas info    : Used 255 registers, 584 bytes cmem[0]
ptxas info    : Compiling entry function '_Z16flash_fwd_kernelI23Flash_fwd_kernel_traitsILi160ELi128ELi32ELi4ELb0ELb0EN7cutlass10bfloat16_tE19Flash_kernel_traitsILi160ELi128ELi32ELi4ES2_EELb0ELb1ELb1ELb1ELb0EEv16Flash_fwd_params' for 'sm_80'
ptxas info    : Function properties for _Z16flash_fwd_kernelI23Flash_fwd_kernel_traitsILi160ELi128ELi32ELi4ELb0ELb0EN7cutlass10bfloat16_tE19Flash_kernel_traitsILi160ELi128ELi32ELi4ES2_EELb0ELb1ELb1ELb1ELb0EEv16Flash_fwd_params
    56 bytes stack frame, 176 bytes spill stores, 124 bytes spill loads
ptxas info    : Used 255 registers, 584 bytes cmem[0]
ptxas info    : Compiling entry function '_Z16flash_fwd_kernelI23Flash_fwd_kernel_traitsILi160ELi64ELi64ELi4ELb0ELb0EN7cutlass10bfloat16_tE19Flash_kernel_traitsILi160ELi64ELi64ELi4ES2_EELb0ELb1ELb0ELb0ELb0EEv16Flash_fwd_params' for 'sm_80'
ptxas info    : Function properties for _Z16flash_fwd_kernelI23Flash_fwd_kernel_traitsILi160ELi64ELi64ELi4ELb0ELb0EN7cutlass10bfloat16_tE19Flash_kernel_traitsILi160ELi64ELi64ELi4ES2_EELb0ELb1ELb0ELb0ELb0EEv16Flash_fwd_params
    0 bytes stack frame, 0 bytes spill stores, 0 bytes spill loads
ptxas info    : Used 171 registers, 584 bytes cmem[0]
ptxas info    : Compiling entry function '_Z16flash_fwd_kernelI23Flash_fwd_kernel_traitsILi160ELi64ELi64ELi4ELb0ELb0EN7cutlass10bfloat16_tE19Flash_kernel_traitsILi160ELi64ELi64ELi4ES2_EELb0ELb1ELb0ELb1ELb0EEv16Flash_fwd_params' for 'sm_80'
ptxas info    : Function properties for _Z16flash_fwd_kernelI23Flash_fwd_kernel_traitsILi160ELi64ELi64ELi4ELb0ELb0EN7cutlass10bfloat16_tE19Flash_kernel_traitsILi160ELi64ELi64ELi4ES2_EELb0ELb1ELb0ELb1ELb0EEv16Flash_fwd_params
    0 bytes stack frame, 0 bytes spill stores, 0 bytes spill loads
ptxas info    : Used 167 registers, 584 bytes cmem[0]
ptxas info    : Compiling entry function '_Z16flash_fwd_kernelI23Flash_fwd_kernel_traitsILi160ELi64ELi64ELi4ELb0ELb0EN7cutlass10bfloat16_tE19Flash_kernel_traitsILi160ELi64ELi64ELi4ES2_EELb0ELb1ELb1ELb0ELb0EEv16Flash_fwd_params' for 'sm_80'
ptxas info    : Function properties for _Z16flash_fwd_kernelI23Flash_fwd_kernel_traitsILi160ELi64ELi64ELi4ELb0ELb0EN7cutlass10bfloat16_tE19Flash_kernel_traitsILi160ELi64ELi64ELi4ES2_EELb0ELb1ELb1ELb0ELb0EEv16Flash_fwd_params
    0 bytes stack frame, 0 bytes spill stores, 0 bytes spill loads
ptxas info    : Used 168 registers, 584 bytes cmem[0]
ptxas info    : Compiling entry function '_Z16flash_fwd_kernelI23Flash_fwd_kernel_traitsILi160ELi64ELi64ELi4ELb0ELb0EN7cutlass10bfloat16_tE19Flash_kernel_traitsILi160ELi64ELi64ELi4ES2_EELb0ELb1ELb1ELb1ELb0EEv16Flash_fwd_params' for 'sm_80'
ptxas info    : Function properties for _Z16flash_fwd_kernelI23Flash_fwd_kernel_traitsILi160ELi64ELi64ELi4ELb0ELb0EN7cutlass10bfloat16_tE19Flash_kernel_traitsILi160ELi64ELi64ELi4ES2_EELb0ELb1ELb1ELb1ELb0EEv16Flash_fwd_params
    0 bytes stack frame, 0 bytes spill stores, 0 bytes spill loads
ptxas info    : Used 172 registers, 584 bytes cmem[0]
ptxas info    : Compiling entry function '_Z16flash_fwd_kernelI23Flash_fwd_kernel_traitsILi160ELi128ELi32ELi4ELb0ELb0EN7cutlass10bfloat16_tE19Flash_kernel_traitsILi160ELi128ELi32ELi4ES2_EELb1ELb0ELb0ELb0ELb0EEv16Flash_fwd_params' for 'sm_80'
ptxas info    : Function properties for _Z16flash_fwd_kernelI23Flash_fwd_kernel_traitsILi160ELi128ELi32ELi4ELb0ELb0EN7cutlass10bfloat16_tE19Flash_kernel_traitsILi160ELi128ELi32ELi4ES2_EELb1ELb0ELb0ELb0ELb0EEv16Flash_fwd_params
    32 bytes stack frame, 32 bytes spill stores, 32 bytes spill loads
ptxas info    : Used 255 registers, 584 bytes cmem[0]
ptxas info    : Compiling entry function '_Z16flash_fwd_kernelI23Flash_fwd_kernel_traitsILi160ELi128ELi32ELi4ELb0ELb0EN7cutlass10bfloat16_tE19Flash_kernel_traitsILi160ELi128ELi32ELi4ES2_EELb1ELb0ELb0ELb0ELb1EEv16Flash_fwd_params' for 'sm_80'
ptxas info    : Function properties for _Z16flash_fwd_kernelI23Flash_fwd_kernel_traitsILi160ELi128ELi32ELi4ELb0ELb0EN7cutlass10bfloat16_tE19Flash_kernel_traitsILi160ELi128ELi32ELi4ES2_EELb1ELb0ELb0ELb0ELb1EEv16Flash_fwd_params
    32 bytes stack frame, 36 bytes spill stores, 36 bytes spill loads
ptxas info    : Used 255 registers, 584 bytes cmem[0]
ptxas info    : Compiling entry function '_Z16flash_fwd_kernelI23Flash_fwd_kernel_traitsILi160ELi128ELi32ELi4ELb0ELb0EN7cutlass10bfloat16_tE19Flash_kernel_traitsILi160ELi128ELi32ELi4ES2_EELb1ELb0ELb0ELb1ELb0EEv16Flash_fwd_params' for 'sm_80'
ptxas info    : Function properties for _Z16flash_fwd_kernelI23Flash_fwd_kernel_traitsILi160ELi128ELi32ELi4ELb0ELb0EN7cutlass10bfloat16_tE19Flash_kernel_traitsILi160ELi128ELi32ELi4ES2_EELb1ELb0ELb0ELb1ELb0EEv16Flash_fwd_params
    32 bytes stack frame, 32 bytes spill stores, 32 bytes spill loads
ptxas info    : Used 255 registers, 584 bytes cmem[0]
ptxas info    : Compiling entry function '_Z16flash_fwd_kernelI23Flash_fwd_kernel_traitsILi160ELi128ELi32ELi4ELb0ELb0EN7cutlass10bfloat16_tE19Flash_kernel_traitsILi160ELi128ELi32ELi4ES2_EELb1ELb0ELb0ELb1ELb1EEv16Flash_fwd_params' for 'sm_80'
ptxas info    : Function properties for _Z16flash_fwd_kernelI23Flash_fwd_kernel_traitsILi160ELi128ELi32ELi4ELb0ELb0EN7cutlass10bfloat16_tE19Flash_kernel_traitsILi160ELi128ELi32ELi4ES2_EELb1ELb0ELb0ELb1ELb1EEv16Flash_fwd_params
    32 bytes stack frame, 32 bytes spill stores, 32 bytes spill loads
ptxas info    : Used 255 registers, 584 bytes cmem[0]
ptxas info    : Compiling entry function '_Z16flash_fwd_kernelI23Flash_fwd_kernel_traitsILi160ELi128ELi32ELi4ELb0ELb0EN7cutlass10bfloat16_tE19Flash_kernel_traitsILi160ELi128ELi32ELi4ES2_EELb1ELb0ELb1ELb0ELb0EEv16Flash_fwd_params' for 'sm_80'
ptxas info    : Function properties for _Z16flash_fwd_kernelI23Flash_fwd_kernel_traitsILi160ELi128ELi32ELi4ELb0ELb0EN7cutlass10bfloat16_tE19Flash_kernel_traitsILi160ELi128ELi32ELi4ES2_EELb1ELb0ELb1ELb0ELb0EEv16Flash_fwd_params
    32 bytes stack frame, 32 bytes spill stores, 32 bytes spill loads
ptxas info    : Used 255 registers, 584 bytes cmem[0]
ptxas info    : Compiling entry function '_Z16flash_fwd_kernelI23Flash_fwd_kernel_traitsILi160ELi128ELi32ELi4ELb0ELb0EN7cutlass10bfloat16_tE19Flash_kernel_traitsILi160ELi128ELi32ELi4ES2_EELb1ELb0ELb1ELb0ELb1EEv16Flash_fwd_params' for 'sm_80'
ptxas info    : Function properties for _Z16flash_fwd_kernelI23Flash_fwd_kernel_traitsILi160ELi128ELi32ELi4ELb0ELb0EN7cutlass10bfloat16_tE19Flash_kernel_traitsILi160ELi128ELi32ELi4ES2_EELb1ELb0ELb1ELb0ELb1EEv16Flash_fwd_params
    8 bytes stack frame, 8 bytes spill stores, 8 bytes spill loads
ptxas info    : Used 255 registers, 584 bytes cmem[0]
ptxas info    : Compiling entry function '_Z16flash_fwd_kernelI23Flash_fwd_kernel_traitsILi160ELi128ELi32ELi4ELb0ELb0EN7cutlass10bfloat16_tE19Flash_kernel_traitsILi160ELi128ELi32ELi4ES2_EELb1ELb0ELb1ELb1ELb0EEv16Flash_fwd_params' for 'sm_80'
ptxas info    : Function properties for _Z16flash_fwd_kernelI23Flash_fwd_kernel_traitsILi160ELi128ELi32ELi4ELb0ELb0EN7cutlass10bfloat16_tE19Flash_kernel_traitsILi160ELi128ELi32ELi4ES2_EELb1ELb0ELb1ELb1ELb0EEv16Flash_fwd_params
    16 bytes stack frame, 16 bytes spill stores, 16 bytes spill loads
ptxas info    : Used 255 registers, 584 bytes cmem[0]
ptxas info    : Compiling entry function '_Z16flash_fwd_kernelI23Flash_fwd_kernel_traitsILi160ELi128ELi32ELi4ELb0ELb0EN7cutlass10bfloat16_tE19Flash_kernel_traitsILi160ELi128ELi32ELi4ES2_EELb1ELb0ELb1ELb1ELb1EEv16Flash_fwd_params' for 'sm_80'
ptxas info    : Function properties for _Z16flash_fwd_kernelI23Flash_fwd_kernel_traitsILi160ELi128ELi32ELi4ELb0ELb0EN7cutlass10bfloat16_tE19Flash_kernel_traitsILi160ELi128ELi32ELi4ES2_EELb1ELb0ELb1ELb1ELb1EEv16Flash_fwd_params
    8 bytes stack frame, 8 bytes spill stores, 8 bytes spill loads
ptxas info    : Used 255 registers, 584 bytes cmem[0]
ptxas info    : Compiling entry function '_Z16flash_fwd_kernelI23Flash_fwd_kernel_traitsILi160ELi128ELi64ELi8ELb0ELb0EN7cutlass10bfloat16_tE19Flash_kernel_traitsILi160ELi128ELi64ELi8ES2_EELb1ELb0ELb0ELb0ELb0EEv16Flash_fwd_params' for 'sm_80'
ptxas info    : Function properties for _Z16flash_fwd_kernelI23Flash_fwd_kernel_traitsILi160ELi128ELi64ELi8ELb0ELb0EN7cutlass10bfloat16_tE19Flash_kernel_traitsILi160ELi128ELi64ELi8ES2_EELb1ELb0ELb0ELb0ELb0EEv16Flash_fwd_params
    0 bytes stack frame, 0 bytes spill stores, 0 bytes spill loads
ptxas info    : Used 176 registers, 584 bytes cmem[0]
ptxas info    : Compiling entry function '_Z16flash_fwd_kernelI23Flash_fwd_kernel_traitsILi160ELi128ELi64ELi8ELb0ELb0EN7cutlass10bfloat16_tE19Flash_kernel_traitsILi160ELi128ELi64ELi8ES2_EELb1ELb0ELb0ELb0ELb1EEv16Flash_fwd_params' for 'sm_80'
ptxas info    : Function properties for _Z16flash_fwd_kernelI23Flash_fwd_kernel_traitsILi160ELi128ELi64ELi8ELb0ELb0EN7cutlass10bfloat16_tE19Flash_kernel_traitsILi160ELi128ELi64ELi8ES2_EELb1ELb0ELb0ELb0ELb1EEv16Flash_fwd_params
    0 bytes stack frame, 0 bytes spill stores, 0 bytes spill loads
ptxas info    : Used 176 registers, 584 bytes cmem[0]
ptxas info    : Compiling entry function '_Z16flash_fwd_kernelI23Flash_fwd_kernel_traitsILi160ELi128ELi64ELi8ELb0ELb0EN7cutlass10bfloat16_tE19Flash_kernel_traitsILi160ELi128ELi64ELi8ES2_EELb1ELb0ELb0ELb1ELb0EEv16Flash_fwd_params' for 'sm_80'
ptxas info    : Function properties for _Z16flash_fwd_kernelI23Flash_fwd_kernel_traitsILi160ELi128ELi64ELi8ELb0ELb0EN7cutlass10bfloat16_tE19Flash_kernel_traitsILi160ELi128ELi64ELi8ES2_EELb1ELb0ELb0ELb1ELb0EEv16Flash_fwd_params
    0 bytes stack frame, 0 bytes spill stores, 0 bytes spill loads
ptxas info    : Used 176 registers, 584 bytes cmem[0]
ptxas info    : Compiling entry function '_Z16flash_fwd_kernelI23Flash_fwd_kernel_traitsILi160ELi128ELi64ELi8ELb0ELb0EN7cutlass10bfloat16_tE19Flash_kernel_traitsILi160ELi128ELi64ELi8ES2_EELb1ELb0ELb0ELb1ELb1EEv16Flash_fwd_params' for 'sm_80'
ptxas info    : Function properties for _Z16flash_fwd_kernelI23Flash_fwd_kernel_traitsILi160ELi128ELi64ELi8ELb0ELb0EN7cutlass10bfloat16_tE19Flash_kernel_traitsILi160ELi128ELi64ELi8ES2_EELb1ELb0ELb0ELb1ELb1EEv16Flash_fwd_params
    0 bytes stack frame, 0 bytes spill stores, 0 bytes spill loads
ptxas info    : Used 174 registers, 584 bytes cmem[0]
ptxas info    : Compiling entry function '_Z16flash_fwd_kernelI23Flash_fwd_kernel_traitsILi160ELi128ELi64ELi8ELb0ELb0EN7cutlass10bfloat16_tE19Flash_kernel_traitsILi160ELi128ELi64ELi8ES2_EELb1ELb0ELb1ELb0ELb0EEv16Flash_fwd_params' for 'sm_80'
ptxas info    : Function properties for _Z16flash_fwd_kernelI23Flash_fwd_kernel_traitsILi160ELi128ELi64ELi8ELb0ELb0EN7cutlass10bfloat16_tE19Flash_kernel_traitsILi160ELi128ELi64ELi8ES2_EELb1ELb0ELb1ELb0ELb0EEv16Flash_fwd_params
    0 bytes stack frame, 0 bytes spill stores, 0 bytes spill loads
ptxas info    : Used 172 registers, 584 bytes cmem[0]
ptxas info    : Compiling entry function '_Z16flash_fwd_kernelI23Flash_fwd_kernel_traitsILi160ELi128ELi64ELi8ELb0ELb0EN7cutlass10bfloat16_tE19Flash_kernel_traitsILi160ELi128ELi64ELi8ES2_EELb1ELb0ELb1ELb0ELb1EEv16Flash_fwd_params' for 'sm_80'
ptxas info    : Function properties for _Z16flash_fwd_kernelI23Flash_fwd_kernel_traitsILi160ELi128ELi64ELi8ELb0ELb0EN7cutlass10bfloat16_tE19Flash_kernel_traitsILi160ELi128ELi64ELi8ES2_EELb1ELb0ELb1ELb0ELb1EEv16Flash_fwd_params
    0 bytes stack frame, 0 bytes spill stores, 0 bytes spill loads
ptxas info    : Used 176 registers, 584 bytes cmem[0]
ptxas info    : Compiling entry function '_Z16flash_fwd_kernelI23Flash_fwd_kernel_traitsILi160ELi128ELi64ELi8ELb0ELb0EN7cutlass10bfloat16_tE19Flash_kernel_traitsILi160ELi128ELi64ELi8ES2_EELb1ELb0ELb1ELb1ELb0EEv16Flash_fwd_params' for 'sm_80'
ptxas info    : Function properties for _Z16flash_fwd_kernelI23Flash_fwd_kernel_traitsILi160ELi128ELi64ELi8ELb0ELb0EN7cutlass10bfloat16_tE19Flash_kernel_traitsILi160ELi128ELi64ELi8ES2_EELb1ELb0ELb1ELb1ELb0EEv16Flash_fwd_params
    0 bytes stack frame, 0 bytes spill stores, 0 bytes spill loads
ptxas info    : Used 168 registers, 584 bytes cmem[0]
ptxas info    : Compiling entry function '_Z16flash_fwd_kernelI23Flash_fwd_kernel_traitsILi160ELi128ELi64ELi8ELb0ELb0EN7cutlass10bfloat16_tE19Flash_kernel_traitsILi160ELi128ELi64ELi8ES2_EELb1ELb0ELb1ELb1ELb1EEv16Flash_fwd_params' for 'sm_80'
ptxas info    : Function properties for _Z16flash_fwd_kernelI23Flash_fwd_kernel_traitsILi160ELi128ELi64ELi8ELb0ELb0EN7cutlass10bfloat16_tE19Flash_kernel_traitsILi160ELi128ELi64ELi8ES2_EELb1ELb0ELb1ELb1ELb1EEv16Flash_fwd_params
    0 bytes stack frame, 0 bytes spill stores, 0 bytes spill loads
ptxas info    : Used 176 registers, 584 bytes cmem[0]
ptxas info    : Compiling entry function '_Z16flash_fwd_kernelI23Flash_fwd_kernel_traitsILi160ELi128ELi32ELi4ELb0ELb0EN7cutlass10bfloat16_tE19Flash_kernel_traitsILi160ELi128ELi32ELi4ES2_EELb1ELb1ELb0ELb0ELb0EEv16Flash_fwd_params' for 'sm_80'
ptxas info    : Function properties for _Z16flash_fwd_kernelI23Flash_fwd_kernel_traitsILi160ELi128ELi32ELi4ELb0ELb0EN7cutlass10bfloat16_tE19Flash_kernel_traitsILi160ELi128ELi32ELi4ES2_EELb1ELb1ELb0ELb0ELb0EEv16Flash_fwd_params
    152 bytes stack frame, 204 bytes spill stores, 268 bytes spill loads
ptxas info    : Used 255 registers, 584 bytes cmem[0]
ptxas info    : Compiling entry function '_Z16flash_fwd_kernelI23Flash_fwd_kernel_traitsILi160ELi128ELi32ELi4ELb0ELb0EN7cutlass10bfloat16_tE19Flash_kernel_traitsILi160ELi128ELi32ELi4ES2_EELb1ELb1ELb0ELb0ELb1EEv16Flash_fwd_params' for 'sm_80'
ptxas info    : Function properties for _Z16flash_fwd_kernelI23Flash_fwd_kernel_traitsILi160ELi128ELi32ELi4ELb0ELb0EN7cutlass10bfloat16_tE19Flash_kernel_traitsILi160ELi128ELi32ELi4ES2_EELb1ELb1ELb0ELb0ELb1EEv16Flash_fwd_params
    232 bytes stack frame, 332 bytes spill stores, 440 bytes spill loads
ptxas info    : Used 255 registers, 584 bytes cmem[0]
ptxas info    : Compiling entry function '_Z16flash_fwd_kernelI23Flash_fwd_kernel_traitsILi160ELi128ELi32ELi4ELb0ELb0EN7cutlass10bfloat16_tE19Flash_kernel_traitsILi160ELi128ELi32ELi4ES2_EELb1ELb1ELb0ELb1ELb0EEv16Flash_fwd_params' for 'sm_80'
ptxas info    : Function properties for _Z16flash_fwd_kernelI23Flash_fwd_kernel_traitsILi160ELi128ELi32ELi4ELb0ELb0EN7cutlass10bfloat16_tE19Flash_kernel_traitsILi160ELi128ELi32ELi4ES2_EELb1ELb1ELb0ELb1ELb0EEv16Flash_fwd_params
    112 bytes stack frame, 252 bytes spill stores, 228 bytes spill loads
ptxas info    : Used 255 registers, 584 bytes cmem[0]
ptxas info    : Compiling entry function '_Z16flash_fwd_kernelI23Flash_fwd_kernel_traitsILi160ELi128ELi32ELi4ELb0ELb0EN7cutlass10bfloat16_tE19Flash_kernel_traitsILi160ELi128ELi32ELi4ES2_EELb1ELb1ELb0ELb1ELb1EEv16Flash_fwd_params' for 'sm_80'
ptxas info    : Function properties for _Z16flash_fwd_kernelI23Flash_fwd_kernel_traitsILi160ELi128ELi32ELi4ELb0ELb0EN7cutlass10bfloat16_tE19Flash_kernel_traitsILi160ELi128ELi32ELi4ES2_EELb1ELb1ELb0ELb1ELb1EEv16Flash_fwd_params
    216 bytes stack frame, 344 bytes spill stores, 360 bytes spill loads
ptxas info    : Used 255 registers, 584 bytes cmem[0]
ptxas info    : Compiling entry function '_Z16flash_fwd_kernelI23Flash_fwd_kernel_traitsILi160ELi128ELi32ELi4ELb0ELb0EN7cutlass10bfloat16_tE19Flash_kernel_traitsILi160ELi128ELi32ELi4ES2_EELb1ELb1ELb1ELb0ELb0EEv16Flash_fwd_params' for 'sm_80'
ptxas info    : Function properties for _Z16flash_fwd_kernelI23Flash_fwd_kernel_traitsILi160ELi128ELi32ELi4ELb0ELb0EN7cutlass10bfloat16_tE19Flash_kernel_traitsILi160ELi128ELi32ELi4ES2_EELb1ELb1ELb1ELb0ELb0EEv16Flash_fwd_params
    88 bytes stack frame, 144 bytes spill stores, 168 bytes spill loads
ptxas info    : Used 255 registers, 584 bytes cmem[0]
ptxas info    : Compiling entry function '_Z16flash_fwd_kernelI23Flash_fwd_kernel_traitsILi160ELi128ELi32ELi4ELb0ELb0EN7cutlass10bfloat16_tE19Flash_kernel_traitsILi160ELi128ELi32ELi4ES2_EELb1ELb1ELb1ELb0ELb1EEv16Flash_fwd_params' for 'sm_80'
ptxas info    : Function properties for _Z16flash_fwd_kernelI23Flash_fwd_kernel_traitsILi160ELi128ELi32ELi4ELb0ELb0EN7cutlass10bfloat16_tE19Flash_kernel_traitsILi160ELi128ELi32ELi4ES2_EELb1ELb1ELb1ELb0ELb1EEv16Flash_fwd_params
    160 bytes stack frame, 240 bytes spill stores, 324 bytes spill loads
ptxas info    : Used 255 registers, 584 bytes cmem[0]
ptxas info    : Compiling entry function '_Z16flash_fwd_kernelI23Flash_fwd_kernel_traitsILi160ELi128ELi32ELi4ELb0ELb0EN7cutlass10bfloat16_tE19Flash_kernel_traitsILi160ELi128ELi32ELi4ES2_EELb1ELb1ELb1ELb1ELb0EEv16Flash_fwd_params' for 'sm_80'
ptxas info    : Function properties for _Z16flash_fwd_kernelI23Flash_fwd_kernel_traitsILi160ELi128ELi32ELi4ELb0ELb0EN7cutlass10bfloat16_tE19Flash_kernel_traitsILi160ELi128ELi32ELi4ES2_EELb1ELb1ELb1ELb1ELb0EEv16Flash_fwd_params
    120 bytes stack frame, 196 bytes spill stores, 236 bytes spill loads
ptxas info    : Used 255 registers, 584 bytes cmem[0]
ptxas info    : Compiling entry function '_Z16flash_fwd_kernelI23Flash_fwd_kernel_traitsILi160ELi128ELi32ELi4ELb0ELb0EN7cutlass10bfloat16_tE19Flash_kernel_traitsILi160ELi128ELi32ELi4ES2_EELb1ELb1ELb1ELb1ELb1EEv16Flash_fwd_params' for 'sm_80'
ptxas info    : Function properties for _Z16flash_fwd_kernelI23Flash_fwd_kernel_traitsILi160ELi128ELi32ELi4ELb0ELb0EN7cutlass10bfloat16_tE19Flash_kernel_traitsILi160ELi128ELi32ELi4ES2_EELb1ELb1ELb1ELb1ELb1EEv16Flash_fwd_params
    208 bytes stack frame, 296 bytes spill stores, 376 bytes spill loads
ptxas info    : Used 255 registers, 584 bytes cmem[0]
ptxas info    : Compiling entry function '_Z16flash_fwd_kernelI23Flash_fwd_kernel_traitsILi160ELi64ELi64ELi4ELb0ELb0EN7cutlass10bfloat16_tE19Flash_kernel_traitsILi160ELi64ELi64ELi4ES2_EELb1ELb1ELb0ELb0ELb0EEv16Flash_fwd_params' for 'sm_80'
ptxas info    : Function properties for _Z16flash_fwd_kernelI23Flash_fwd_kernel_traitsILi160ELi64ELi64ELi4ELb0ELb0EN7cutlass10bfloat16_tE19Flash_kernel_traitsILi160ELi64ELi64ELi4ES2_EELb1ELb1ELb0ELb0ELb0EEv16Flash_fwd_params
    0 bytes stack frame, 0 bytes spill stores, 0 bytes spill loads
ptxas info    : Used 176 registers, 584 bytes cmem[0]
ptxas info    : Compiling entry function '_Z16flash_fwd_kernelI23Flash_fwd_kernel_traitsILi160ELi64ELi64ELi4ELb0ELb0EN7cutlass10bfloat16_tE19Flash_kernel_traitsILi160ELi64ELi64ELi4ES2_EELb1ELb1ELb0ELb0ELb1EEv16Flash_fwd_params' for 'sm_80'
ptxas info    : Function properties for _Z16flash_fwd_kernelI23Flash_fwd_kernel_traitsILi160ELi64ELi64ELi4ELb0ELb0EN7cutlass10bfloat16_tE19Flash_kernel_traitsILi160ELi64ELi64ELi4ES2_EELb1ELb1ELb0ELb0ELb1EEv16Flash_fwd_params
    0 bytes stack frame, 0 bytes spill stores, 0 bytes spill loads
ptxas info    : Used 176 registers, 584 bytes cmem[0]
ptxas info    : Compiling entry function '_Z16flash_fwd_kernelI23Flash_fwd_kernel_traitsILi160ELi64ELi64ELi4ELb0ELb0EN7cutlass10bfloat16_tE19Flash_kernel_traitsILi160ELi64ELi64ELi4ES2_EELb1ELb1ELb0ELb1ELb0EEv16Flash_fwd_params' for 'sm_80'
ptxas info    : Function properties for _Z16flash_fwd_kernelI23Flash_fwd_kernel_traitsILi160ELi64ELi64ELi4ELb0ELb0EN7cutlass10bfloat16_tE19Flash_kernel_traitsILi160ELi64ELi64ELi4ES2_EELb1ELb1ELb0ELb1ELb0EEv16Flash_fwd_params
    0 bytes stack frame, 0 bytes spill stores, 0 bytes spill loads
ptxas info    : Used 182 registers, 584 bytes cmem[0]
ptxas info    : Compiling entry function '_Z16flash_fwd_kernelI23Flash_fwd_kernel_traitsILi160ELi64ELi64ELi4ELb0ELb0EN7cutlass10bfloat16_tE19Flash_kernel_traitsILi160ELi64ELi64ELi4ES2_EELb1ELb1ELb0ELb1ELb1EEv16Flash_fwd_params' for 'sm_80'
ptxas info    : Function properties for _Z16flash_fwd_kernelI23Flash_fwd_kernel_traitsILi160ELi64ELi64ELi4ELb0ELb0EN7cutlass10bfloat16_tE19Flash_kernel_traitsILi160ELi64ELi64ELi4ES2_EELb1ELb1ELb0ELb1ELb1EEv16Flash_fwd_params
    0 bytes stack frame, 0 bytes spill stores, 0 bytes spill loads
ptxas info    : Used 174 registers, 584 bytes cmem[0]
ptxas info    : Compiling entry function '_Z16flash_fwd_kernelI23Flash_fwd_kernel_traitsILi160ELi64ELi64ELi4ELb0ELb0EN7cutlass10bfloat16_tE19Flash_kernel_traitsILi160ELi64ELi64ELi4ES2_EELb1ELb1ELb1ELb0ELb0EEv16Flash_fwd_params' for 'sm_80'
ptxas info    : Function properties for _Z16flash_fwd_kernelI23Flash_fwd_kernel_traitsILi160ELi64ELi64ELi4ELb0ELb0EN7cutlass10bfloat16_tE19Flash_kernel_traitsILi160ELi64ELi64ELi4ES2_EELb1ELb1ELb1ELb0ELb0EEv16Flash_fwd_params
    0 bytes stack frame, 0 bytes spill stores, 0 bytes spill loads
ptxas info    : Used 182 registers, 584 bytes cmem[0]
ptxas info    : Compiling entry function '_Z16flash_fwd_kernelI23Flash_fwd_kernel_traitsILi160ELi64ELi64ELi4ELb0ELb0EN7cutlass10bfloat16_tE19Flash_kernel_traitsILi160ELi64ELi64ELi4ES2_EELb1ELb1ELb1ELb0ELb1EEv16Flash_fwd_params' for 'sm_80'
ptxas info    : Function properties for _Z16flash_fwd_kernelI23Flash_fwd_kernel_traitsILi160ELi64ELi64ELi4ELb0ELb0EN7cutlass10bfloat16_tE19Flash_kernel_traitsILi160ELi64ELi64ELi4ES2_EELb1ELb1ELb1ELb0ELb1EEv16Flash_fwd_params
    0 bytes stack frame, 0 bytes spill stores, 0 bytes spill loads
ptxas info    : Used 178 registers, 584 bytes cmem[0]
ptxas info    : Compiling entry function '_Z16flash_fwd_kernelI23Flash_fwd_kernel_traitsILi160ELi64ELi64ELi4ELb0ELb0EN7cutlass10bfloat16_tE19Flash_kernel_traitsILi160ELi64ELi64ELi4ES2_EELb1ELb1ELb1ELb1ELb0EEv16Flash_fwd_params' for 'sm_80'
ptxas info    : Function properties for _Z16flash_fwd_kernelI23Flash_fwd_kernel_traitsILi160ELi64ELi64ELi4ELb0ELb0EN7cutlass10bfloat16_tE19Flash_kernel_traitsILi160ELi64ELi64ELi4ES2_EELb1ELb1ELb1ELb1ELb0EEv16Flash_fwd_params
    0 bytes stack frame, 0 bytes spill stores, 0 bytes spill loads
ptxas info    : Used 188 registers, 584 bytes cmem[0]
ptxas info    : Compiling entry function '_Z16flash_fwd_kernelI23Flash_fwd_kernel_traitsILi160ELi64ELi64ELi4ELb0ELb0EN7cutlass10bfloat16_tE19Flash_kernel_traitsILi160ELi64ELi64ELi4ES2_EELb1ELb1ELb1ELb1ELb1EEv16Flash_fwd_params' for 'sm_80'
ptxas info    : Function properties for _Z16flash_fwd_kernelI23Flash_fwd_kernel_traitsILi160ELi64ELi64ELi4ELb0ELb0EN7cutlass10bfloat16_tE19Flash_kernel_traitsILi160ELi64ELi64ELi4ES2_EELb1ELb1ELb1ELb1ELb1EEv16Flash_fwd_params
    0 bytes stack frame, 0 bytes spill stores, 0 bytes spill loads
ptxas info    : Used 174 registers, 584 bytes cmem[0]
/anaconda/envs/flashattention/lib/python3.9/site-packages/torch/include/c10/core/SymInt.h(84): warning: integer conversion resulted in a change of sign

[26/36] /usr/local/cuda-11.4/bin/nvcc  -I/home/v-feiychen/flash-attention/csrc/flash_attn -I/home/v-feiychen/flash-attention/csrc/flash_attn/src -I/home/v-feiychen/flash-attention/csrc/cutlass/include -I/anaconda/envs/flashattention/lib/python3.9/site-packages/torch/include -I/anaconda/envs/flashattention/lib/python3.9/site-packages/torch/include/torch/csrc/api/include -I/anaconda/envs/flashattention/lib/python3.9/site-packages/torch/include/TH -I/anaconda/envs/flashattention/lib/python3.9/site-packages/torch/include/THC -I/usr/local/cuda-11.4/include -I/anaconda/envs/flashattention/include/python3.9 -c -c /home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_bwd_hdim128_fp16_sm80.cu -o /home/v-feiychen/flash-attention/build/temp.linux-x86_64-cpython-39/csrc/flash_attn/src/flash_bwd_hdim128_fp16_sm80.o -D__CUDA_NO_HALF_OPERATORS__ -D__CUDA_NO_HALF_CONVERSIONS__ -D__CUDA_NO_BFLOAT16_CONVERSIONS__ -D__CUDA_NO_HALF2_OPERATORS__ --expt-relaxed-constexpr --compiler-options ''"'"'-fPIC'"'"'' -O3 -std=c++17 -U__CUDA_NO_HALF_OPERATORS__ -U__CUDA_NO_HALF_CONVERSIONS__ -U__CUDA_NO_HALF2_OPERATORS__ -U__CUDA_NO_BFLOAT16_CONVERSIONS__ --expt-relaxed-constexpr --expt-extended-lambda --use_fast_math --ptxas-options=-v -lineinfo -gencode arch=compute_80,code=sm_80 --threads 4 -DTORCH_API_INCLUDE_EXTENSION_H '-DPYBIND11_COMPILER_TYPE="_gcc"' '-DPYBIND11_STDLIB="_libstdcpp"' '-DPYBIND11_BUILD_ABI="_cxxabi1011"' -DTORCH_EXTENSION_NAME=flash_attn_2_cuda -D_GLIBCXX_USE_CXX11_ABI=0
/anaconda/envs/flashattention/lib/python3.9/site-packages/torch/include/c10/core/SymInt.h(84): warning: integer conversion resulted in a change of sign

ptxas info    : 2 bytes gmem
ptxas info    : Compiling entry function '_Z44flash_bwd_dq_dk_dv_loop_seqk_parallel_kernelI23Flash_bwd_kernel_traitsILi128ELi64ELi64ELi8ELi4ELi2ELi2ELb1ELb0EN7cutlass6half_tE19Flash_kernel_traitsILi128ELi64ELi64ELi8ES2_EELb0ELb0ELb0ELb0EEv16Flash_bwd_params' for 'sm_80'
ptxas info    : Function properties for _Z44flash_bwd_dq_dk_dv_loop_seqk_parallel_kernelI23Flash_bwd_kernel_traitsILi128ELi64ELi64ELi8ELi4ELi2ELi2ELb1ELb0EN7cutlass6half_tE19Flash_kernel_traitsILi128ELi64ELi64ELi8ES2_EELb0ELb0ELb0ELb0EEv16Flash_bwd_params
    0 bytes stack frame, 0 bytes spill stores, 0 bytes spill loads
ptxas info    : Used 242 registers, 696 bytes cmem[0]
ptxas info    : Compiling entry function '_Z44flash_bwd_dq_dk_dv_loop_seqk_parallel_kernelI23Flash_bwd_kernel_traitsILi128ELi64ELi64ELi8ELi4ELi2ELi2ELb1ELb0EN7cutlass6half_tE19Flash_kernel_traitsILi128ELi64ELi64ELi8ES2_EELb0ELb0ELb0ELb1EEv16Flash_bwd_params' for 'sm_80'
ptxas info    : Function properties for _Z44flash_bwd_dq_dk_dv_loop_seqk_parallel_kernelI23Flash_bwd_kernel_traitsILi128ELi64ELi64ELi8ELi4ELi2ELi2ELb1ELb0EN7cutlass6half_tE19Flash_kernel_traitsILi128ELi64ELi64ELi8ES2_EELb0ELb0ELb0ELb1EEv16Flash_bwd_params
    0 bytes stack frame, 0 bytes spill stores, 0 bytes spill loads
ptxas info    : Used 241 registers, 696 bytes cmem[0]
ptxas info    : Compiling entry function '_Z44flash_bwd_dq_dk_dv_loop_seqk_parallel_kernelI23Flash_bwd_kernel_traitsILi128ELi64ELi64ELi8ELi4ELi2ELi2ELb1ELb0EN7cutlass6half_tE19Flash_kernel_traitsILi128ELi64ELi64ELi8ES2_EELb0ELb0ELb1ELb0EEv16Flash_bwd_params' for 'sm_80'
ptxas info    : Function properties for _Z44flash_bwd_dq_dk_dv_loop_seqk_parallel_kernelI23Flash_bwd_kernel_traitsILi128ELi64ELi64ELi8ELi4ELi2ELi2ELb1ELb0EN7cutlass6half_tE19Flash_kernel_traitsILi128ELi64ELi64ELi8ES2_EELb0ELb0ELb1ELb0EEv16Flash_bwd_params
    0 bytes stack frame, 0 bytes spill stores, 0 bytes spill loads
ptxas info    : Used 239 registers, 696 bytes cmem[0]
ptxas info    : Compiling entry function '_Z44flash_bwd_dq_dk_dv_loop_seqk_parallel_kernelI23Flash_bwd_kernel_traitsILi128ELi64ELi64ELi8ELi4ELi2ELi2ELb1ELb0EN7cutlass6half_tE19Flash_kernel_traitsILi128ELi64ELi64ELi8ES2_EELb0ELb0ELb1ELb1EEv16Flash_bwd_params' for 'sm_80'
ptxas info    : Function properties for _Z44flash_bwd_dq_dk_dv_loop_seqk_parallel_kernelI23Flash_bwd_kernel_traitsILi128ELi64ELi64ELi8ELi4ELi2ELi2ELb1ELb0EN7cutlass6half_tE19Flash_kernel_traitsILi128ELi64ELi64ELi8ES2_EELb0ELb0ELb1ELb1EEv16Flash_bwd_params
    0 bytes stack frame, 0 bytes spill stores, 0 bytes spill loads
ptxas info    : Used 236 registers, 696 bytes cmem[0]
ptxas info    : Compiling entry function '_Z44flash_bwd_dq_dk_dv_loop_seqk_parallel_kernelI23Flash_bwd_kernel_traitsILi128ELi64ELi64ELi8ELi4ELi2ELi2ELb1ELb0EN7cutlass6half_tE19Flash_kernel_traitsILi128ELi64ELi64ELi8ES2_EELb0ELb1ELb0ELb0EEv16Flash_bwd_params' for 'sm_80'
ptxas info    : Function properties for _Z44flash_bwd_dq_dk_dv_loop_seqk_parallel_kernelI23Flash_bwd_kernel_traitsILi128ELi64ELi64ELi8ELi4ELi2ELi2ELb1ELb0EN7cutlass6half_tE19Flash_kernel_traitsILi128ELi64ELi64ELi8ES2_EELb0ELb1ELb0ELb0EEv16Flash_bwd_params
    0 bytes stack frame, 0 bytes spill stores, 0 bytes spill loads
ptxas info    : Used 240 registers, 696 bytes cmem[0]
ptxas info    : Compiling entry function '_Z44flash_bwd_dq_dk_dv_loop_seqk_parallel_kernelI23Flash_bwd_kernel_traitsILi128ELi64ELi64ELi8ELi4ELi2ELi2ELb1ELb0EN7cutlass6half_tE19Flash_kernel_traitsILi128ELi64ELi64ELi8ES2_EELb0ELb1ELb0ELb1EEv16Flash_bwd_params' for 'sm_80'
ptxas info    : Function properties for _Z44flash_bwd_dq_dk_dv_loop_seqk_parallel_kernelI23Flash_bwd_kernel_traitsILi128ELi64ELi64ELi8ELi4ELi2ELi2ELb1ELb0EN7cutlass6half_tE19Flash_kernel_traitsILi128ELi64ELi64ELi8ES2_EELb0ELb1ELb0ELb1EEv16Flash_bwd_params
    0 bytes stack frame, 0 bytes spill stores, 0 bytes spill loads
ptxas info    : Used 240 registers, 696 bytes cmem[0]
ptxas info    : Compiling entry function '_Z44flash_bwd_dq_dk_dv_loop_seqk_parallel_kernelI23Flash_bwd_kernel_traitsILi128ELi64ELi64ELi8ELi4ELi2ELi2ELb1ELb0EN7cutlass6half_tE19Flash_kernel_traitsILi128ELi64ELi64ELi8ES2_EELb0ELb1ELb1ELb0EEv16Flash_bwd_params' for 'sm_80'
ptxas info    : Function properties for _Z44flash_bwd_dq_dk_dv_loop_seqk_parallel_kernelI23Flash_bwd_kernel_traitsILi128ELi64ELi64ELi8ELi4ELi2ELi2ELb1ELb0EN7cutlass6half_tE19Flash_kernel_traitsILi128ELi64ELi64ELi8ES2_EELb0ELb1ELb1ELb0EEv16Flash_bwd_params
    0 bytes stack frame, 0 bytes spill stores, 0 bytes spill loads
ptxas info    : Used 238 registers, 696 bytes cmem[0]
ptxas info    : Compiling entry function '_Z44flash_bwd_dq_dk_dv_loop_seqk_parallel_kernelI23Flash_bwd_kernel_traitsILi128ELi64ELi64ELi8ELi4ELi2ELi2ELb1ELb0EN7cutlass6half_tE19Flash_kernel_traitsILi128ELi64ELi64ELi8ES2_EELb0ELb1ELb1ELb1EEv16Flash_bwd_params' for 'sm_80'
ptxas info    : Function properties for _Z44flash_bwd_dq_dk_dv_loop_seqk_parallel_kernelI23Flash_bwd_kernel_traitsILi128ELi64ELi64ELi8ELi4ELi2ELi2ELb1ELb0EN7cutlass6half_tE19Flash_kernel_traitsILi128ELi64ELi64ELi8ES2_EELb0ELb1ELb1ELb1EEv16Flash_bwd_params
    0 bytes stack frame, 0 bytes spill stores, 0 bytes spill loads
ptxas info    : Used 240 registers, 696 bytes cmem[0]
ptxas info    : Compiling entry function '_Z44flash_bwd_dq_dk_dv_loop_seqk_parallel_kernelI23Flash_bwd_kernel_traitsILi128ELi64ELi128ELi8ELi2ELi4ELi2ELb0ELb0EN7cutlass6half_tE19Flash_kernel_traitsILi128ELi64ELi128ELi8ES2_EELb0ELb0ELb0ELb0EEv16Flash_bwd_params' for 'sm_80'
ptxas info    : Function properties for _Z44flash_bwd_dq_dk_dv_loop_seqk_parallel_kernelI23Flash_bwd_kernel_traitsILi128ELi64ELi128ELi8ELi2ELi4ELi2ELb0ELb0EN7cutlass6half_tE19Flash_kernel_traitsILi128ELi64ELi128ELi8ES2_EELb0ELb0ELb0ELb0EEv16Flash_bwd_params
    136 bytes stack frame, 164 bytes spill stores, 152 bytes spill loads
ptxas info    : Used 255 registers, 696 bytes cmem[0]
ptxas info    : Compiling entry function '_Z44flash_bwd_dq_dk_dv_loop_seqk_parallel_kernelI23Flash_bwd_kernel_traitsILi128ELi64ELi128ELi8ELi2ELi4ELi2ELb0ELb0EN7cutlass6half_tE19Flash_kernel_traitsILi128ELi64ELi128ELi8ES2_EELb0ELb0ELb0ELb1EEv16Flash_bwd_params' for 'sm_80'
ptxas info    : Function properties for _Z44flash_bwd_dq_dk_dv_loop_seqk_parallel_kernelI23Flash_bwd_kernel_traitsILi128ELi64ELi128ELi8ELi2ELi4ELi2ELb0ELb0EN7cutlass6half_tE19Flash_kernel_traitsILi128ELi64ELi128ELi8ES2_EELb0ELb0ELb0ELb1EEv16Flash_bwd_params
    136 bytes stack frame, 168 bytes spill stores, 180 bytes spill loads
ptxas info    : Used 255 registers, 696 bytes cmem[0]
ptxas info    : Compiling entry function '_Z44flash_bwd_dq_dk_dv_loop_seqk_parallel_kernelI23Flash_bwd_kernel_traitsILi128ELi64ELi128ELi8ELi2ELi4ELi2ELb0ELb0EN7cutlass6half_tE19Flash_kernel_traitsILi128ELi64ELi128ELi8ES2_EELb0ELb0ELb1ELb0EEv16Flash_bwd_params' for 'sm_80'
ptxas info    : Function properties for _Z44flash_bwd_dq_dk_dv_loop_seqk_parallel_kernelI23Flash_bwd_kernel_traitsILi128ELi64ELi128ELi8ELi2ELi4ELi2ELb0ELb0EN7cutlass6half_tE19Flash_kernel_traitsILi128ELi64ELi128ELi8ES2_EELb0ELb0ELb1ELb0EEv16Flash_bwd_params
    120 bytes stack frame, 156 bytes spill stores, 168 bytes spill loads
ptxas info    : Used 255 registers, 696 bytes cmem[0]
ptxas info    : Compiling entry function '_Z44flash_bwd_dq_dk_dv_loop_seqk_parallel_kernelI23Flash_bwd_kernel_traitsILi128ELi64ELi128ELi8ELi2ELi4ELi2ELb0ELb0EN7cutlass6half_tE19Flash_kernel_traitsILi128ELi64ELi128ELi8ES2_EELb0ELb0ELb1ELb1EEv16Flash_bwd_params' for 'sm_80'
ptxas info    : Function properties for _Z44flash_bwd_dq_dk_dv_loop_seqk_parallel_kernelI23Flash_bwd_kernel_traitsILi128ELi64ELi128ELi8ELi2ELi4ELi2ELb0ELb0EN7cutlass6half_tE19Flash_kernel_traitsILi128ELi64ELi128ELi8ES2_EELb0ELb0ELb1ELb1EEv16Flash_bwd_params
    136 bytes stack frame, 160 bytes spill stores, 176 bytes spill loads
ptxas info    : Used 255 registers, 696 bytes cmem[0]
ptxas info    : Compiling entry function '_Z44flash_bwd_dq_dk_dv_loop_seqk_parallel_kernelI23Flash_bwd_kernel_traitsILi128ELi64ELi128ELi8ELi2ELi4ELi2ELb0ELb0EN7cutlass6half_tE19Flash_kernel_traitsILi128ELi64ELi128ELi8ES2_EELb0ELb1ELb0ELb0EEv16Flash_bwd_params' for 'sm_80'
ptxas info    : Function properties for _Z44flash_bwd_dq_dk_dv_loop_seqk_parallel_kernelI23Flash_bwd_kernel_traitsILi128ELi64ELi128ELi8ELi2ELi4ELi2ELb0ELb0EN7cutlass6half_tE19Flash_kernel_traitsILi128ELi64ELi128ELi8ES2_EELb0ELb1ELb0ELb0EEv16Flash_bwd_params
    120 bytes stack frame, 144 bytes spill stores, 132 bytes spill loads
ptxas info    : Used 255 registers, 696 bytes cmem[0]
ptxas info    : Compiling entry function '_Z44flash_bwd_dq_dk_dv_loop_seqk_parallel_kernelI23Flash_bwd_kernel_traitsILi128ELi64ELi128ELi8ELi2ELi4ELi2ELb0ELb0EN7cutlass6half_tE19Flash_kernel_traitsILi128ELi64ELi128ELi8ES2_EELb0ELb1ELb0ELb1EEv16Flash_bwd_params' for 'sm_80'
ptxas info    : Function properties for _Z44flash_bwd_dq_dk_dv_loop_seqk_parallel_kernelI23Flash_bwd_kernel_traitsILi128ELi64ELi128ELi8ELi2ELi4ELi2ELb0ELb0EN7cutlass6half_tE19Flash_kernel_traitsILi128ELi64ELi128ELi8ES2_EELb0ELb1ELb0ELb1EEv16Flash_bwd_params
    144 bytes stack frame, 168 bytes spill stores, 180 bytes spill loads
ptxas info    : Used 255 registers, 696 bytes cmem[0]
ptxas info    : Compiling entry function '_Z44flash_bwd_dq_dk_dv_loop_seqk_parallel_kernelI23Flash_bwd_kernel_traitsILi128ELi64ELi128ELi8ELi2ELi4ELi2ELb0ELb0EN7cutlass6half_tE19Flash_kernel_traitsILi128ELi64ELi128ELi8ES2_EELb0ELb1ELb1ELb0EEv16Flash_bwd_params' for 'sm_80'
ptxas info    : Function properties for _Z44flash_bwd_dq_dk_dv_loop_seqk_parallel_kernelI23Flash_bwd_kernel_traitsILi128ELi64ELi128ELi8ELi2ELi4ELi2ELb0ELb0EN7cutlass6half_tE19Flash_kernel_traitsILi128ELi64ELi128ELi8ES2_EELb0ELb1ELb1ELb0EEv16Flash_bwd_params
    136 bytes stack frame, 156 bytes spill stores, 152 bytes spill loads
ptxas info    : Used 255 registers, 696 bytes cmem[0]
ptxas info    : Compiling entry function '_Z44flash_bwd_dq_dk_dv_loop_seqk_parallel_kernelI23Flash_bwd_kernel_traitsILi128ELi64ELi128ELi8ELi2ELi4ELi2ELb0ELb0EN7cutlass6half_tE19Flash_kernel_traitsILi128ELi64ELi128ELi8ES2_EELb0ELb1ELb1ELb1EEv16Flash_bwd_params' for 'sm_80'
ptxas info    : Function properties for _Z44flash_bwd_dq_dk_dv_loop_seqk_parallel_kernelI23Flash_bwd_kernel_traitsILi128ELi64ELi128ELi8ELi2ELi4ELi2ELb0ELb0EN7cutlass6half_tE19Flash_kernel_traitsILi128ELi64ELi128ELi8ES2_EELb0ELb1ELb1ELb1EEv16Flash_bwd_params
    144 bytes stack frame, 164 bytes spill stores, 156 bytes spill loads
ptxas info    : Used 255 registers, 696 bytes cmem[0]
ptxas info    : Compiling entry function '_Z27flash_bwd_convert_dq_kernelI23Flash_bwd_kernel_traitsILi128ELi64ELi64ELi8ELi4ELi2ELi2ELb1ELb0EN7cutlass6half_tE19Flash_kernel_traitsILi128ELi64ELi64ELi8ES2_EEEv16Flash_bwd_params' for 'sm_80'
ptxas info    : Function properties for _Z27flash_bwd_convert_dq_kernelI23Flash_bwd_kernel_traitsILi128ELi64ELi64ELi8ELi4ELi2ELi2ELb1ELb0EN7cutlass6half_tE19Flash_kernel_traitsILi128ELi64ELi64ELi8ES2_EEEv16Flash_bwd_params
    0 bytes stack frame, 0 bytes spill stores, 0 bytes spill loads
ptxas info    : Used 32 registers, 696 bytes cmem[0]
ptxas info    : Compiling entry function '_Z44flash_bwd_dq_dk_dv_loop_seqk_parallel_kernelI23Flash_bwd_kernel_traitsILi128ELi64ELi64ELi8ELi4ELi2ELi2ELb1ELb0EN7cutlass6half_tE19Flash_kernel_traitsILi128ELi64ELi64ELi8ES2_EELb1ELb0ELb0ELb0EEv16Flash_bwd_params' for 'sm_80'
ptxas info    : Function properties for _Z44flash_bwd_dq_dk_dv_loop_seqk_parallel_kernelI23Flash_bwd_kernel_traitsILi128ELi64ELi64ELi8ELi4ELi2ELi2ELb1ELb0EN7cutlass6half_tE19Flash_kernel_traitsILi128ELi64ELi64ELi8ES2_EELb1ELb0ELb0ELb0EEv16Flash_bwd_params
    0 bytes stack frame, 0 bytes spill stores, 0 bytes spill loads
ptxas info    : Used 239 registers, 696 bytes cmem[0]
ptxas info    : Compiling entry function '_Z44flash_bwd_dq_dk_dv_loop_seqk_parallel_kernelI23Flash_bwd_kernel_traitsILi128ELi64ELi64ELi8ELi4ELi2ELi2ELb1ELb0EN7cutlass6half_tE19Flash_kernel_traitsILi128ELi64ELi64ELi8ES2_EELb1ELb0ELb0ELb1EEv16Flash_bwd_params' for 'sm_80'
ptxas info    : Function properties for _Z44flash_bwd_dq_dk_dv_loop_seqk_parallel_kernelI23Flash_bwd_kernel_traitsILi128ELi64ELi64ELi8ELi4ELi2ELi2ELb1ELb0EN7cutlass6half_tE19Flash_kernel_traitsILi128ELi64ELi64ELi8ES2_EELb1ELb0ELb0ELb1EEv16Flash_bwd_params
    0 bytes stack frame, 0 bytes spill stores, 0 bytes spill loads
ptxas info    : Used 242 registers, 696 bytes cmem[0]
ptxas info    : Compiling entry function '_Z44flash_bwd_dq_dk_dv_loop_seqk_parallel_kernelI23Flash_bwd_kernel_traitsILi128ELi64ELi64ELi8ELi4ELi2ELi2ELb1ELb0EN7cutlass6half_tE19Flash_kernel_traitsILi128ELi64ELi64ELi8ES2_EELb1ELb0ELb1ELb0EEv16Flash_bwd_params' for 'sm_80'
ptxas info    : Function properties for _Z44flash_bwd_dq_dk_dv_loop_seqk_parallel_kernelI23Flash_bwd_kernel_traitsILi128ELi64ELi64ELi8ELi4ELi2ELi2ELb1ELb0EN7cutlass6half_tE19Flash_kernel_traitsILi128ELi64ELi64ELi8ES2_EELb1ELb0ELb1ELb0EEv16Flash_bwd_params
    0 bytes stack frame, 0 bytes spill stores, 0 bytes spill loads
ptxas info    : Used 240 registers, 696 bytes cmem[0]
ptxas info    : Compiling entry function '_Z44flash_bwd_dq_dk_dv_loop_seqk_parallel_kernelI23Flash_bwd_kernel_traitsILi128ELi64ELi64ELi8ELi4ELi2ELi2ELb1ELb0EN7cutlass6half_tE19Flash_kernel_traitsILi128ELi64ELi64ELi8ES2_EELb1ELb0ELb1ELb1EEv16Flash_bwd_params' for 'sm_80'
ptxas info    : Function properties for _Z44flash_bwd_dq_dk_dv_loop_seqk_parallel_kernelI23Flash_bwd_kernel_traitsILi128ELi64ELi64ELi8ELi4ELi2ELi2ELb1ELb0EN7cutlass6half_tE19Flash_kernel_traitsILi128ELi64ELi64ELi8ES2_EELb1ELb0ELb1ELb1EEv16Flash_bwd_params
    0 bytes stack frame, 0 bytes spill stores, 0 bytes spill loads
ptxas info    : Used 240 registers, 696 bytes cmem[0]
ptxas info    : Compiling entry function '_Z44flash_bwd_dq_dk_dv_loop_seqk_parallel_kernelI23Flash_bwd_kernel_traitsILi128ELi64ELi64ELi8ELi4ELi2ELi2ELb1ELb0EN7cutlass6half_tE19Flash_kernel_traitsILi128ELi64ELi64ELi8ES2_EELb1ELb1ELb0ELb0EEv16Flash_bwd_params' for 'sm_80'
ptxas info    : Function properties for _Z44flash_bwd_dq_dk_dv_loop_seqk_parallel_kernelI23Flash_bwd_kernel_traitsILi128ELi64ELi64ELi8ELi4ELi2ELi2ELb1ELb0EN7cutlass6half_tE19Flash_kernel_traitsILi128ELi64ELi64ELi8ES2_EELb1ELb1ELb0ELb0EEv16Flash_bwd_params
    0 bytes stack frame, 0 bytes spill stores, 0 bytes spill loads
ptxas info    : Used 244 registers, 696 bytes cmem[0]
ptxas info    : Compiling entry function '_Z44flash_bwd_dq_dk_dv_loop_seqk_parallel_kernelI23Flash_bwd_kernel_traitsILi128ELi64ELi64ELi8ELi4ELi2ELi2ELb1ELb0EN7cutlass6half_tE19Flash_kernel_traitsILi128ELi64ELi64ELi8ES2_EELb1ELb1ELb0ELb1EEv16Flash_bwd_params' for 'sm_80'
ptxas info    : Function properties for _Z44flash_bwd_dq_dk_dv_loop_seqk_parallel_kernelI23Flash_bwd_kernel_traitsILi128ELi64ELi64ELi8ELi4ELi2ELi2ELb1ELb0EN7cutlass6half_tE19Flash_kernel_traitsILi128ELi64ELi64ELi8ES2_EELb1ELb1ELb0ELb1EEv16Flash_bwd_params
    0 bytes stack frame, 0 bytes spill stores, 0 bytes spill loads
ptxas info    : Used 241 registers, 696 bytes cmem[0]
ptxas info    : Compiling entry function '_Z44flash_bwd_dq_dk_dv_loop_seqk_parallel_kernelI23Flash_bwd_kernel_traitsILi128ELi64ELi64ELi8ELi4ELi2ELi2ELb1ELb0EN7cutlass6half_tE19Flash_kernel_traitsILi128ELi64ELi64ELi8ES2_EELb1ELb1ELb1ELb0EEv16Flash_bwd_params' for 'sm_80'
ptxas info    : Function properties for _Z44flash_bwd_dq_dk_dv_loop_seqk_parallel_kernelI23Flash_bwd_kernel_traitsILi128ELi64ELi64ELi8ELi4ELi2ELi2ELb1ELb0EN7cutlass6half_tE19Flash_kernel_traitsILi128ELi64ELi64ELi8ES2_EELb1ELb1ELb1ELb0EEv16Flash_bwd_params
    0 bytes stack frame, 0 bytes spill stores, 0 bytes spill loads
ptxas info    : Used 239 registers, 696 bytes cmem[0]
ptxas info    : Compiling entry function '_Z44flash_bwd_dq_dk_dv_loop_seqk_parallel_kernelI23Flash_bwd_kernel_traitsILi128ELi64ELi64ELi8ELi4ELi2ELi2ELb1ELb0EN7cutlass6half_tE19Flash_kernel_traitsILi128ELi64ELi64ELi8ES2_EELb1ELb1ELb1ELb1EEv16Flash_bwd_params' for 'sm_80'
ptxas info    : Function properties for _Z44flash_bwd_dq_dk_dv_loop_seqk_parallel_kernelI23Flash_bwd_kernel_traitsILi128ELi64ELi64ELi8ELi4ELi2ELi2ELb1ELb0EN7cutlass6half_tE19Flash_kernel_traitsILi128ELi64ELi64ELi8ES2_EELb1ELb1ELb1ELb1EEv16Flash_bwd_params
    0 bytes stack frame, 0 bytes spill stores, 0 bytes spill loads
ptxas info    : Used 240 registers, 696 bytes cmem[0]
ptxas info    : Compiling entry function '_Z25flash_bwd_dot_do_o_kernelILb1E23Flash_bwd_kernel_traitsILi128ELi64ELi64ELi8ELi4ELi2ELi2ELb1ELb0EN7cutlass6half_tE19Flash_kernel_traitsILi128ELi64ELi64ELi8ES2_EEEv16Flash_bwd_params' for 'sm_80'
ptxas info    : Function properties for _Z25flash_bwd_dot_do_o_kernelILb1E23Flash_bwd_kernel_traitsILi128ELi64ELi64ELi8ELi4ELi2ELi2ELb1ELb0EN7cutlass6half_tE19Flash_kernel_traitsILi128ELi64ELi64ELi8ES2_EEEv16Flash_bwd_params
    0 bytes stack frame, 0 bytes spill stores, 0 bytes spill loads
ptxas info    : Used 47 registers, 696 bytes cmem[0]
ptxas info    : Compiling entry function '_Z27flash_bwd_convert_dq_kernelI23Flash_bwd_kernel_traitsILi128ELi64ELi128ELi8ELi2ELi4ELi2ELb0ELb0EN7cutlass6half_tE19Flash_kernel_traitsILi128ELi64ELi128ELi8ES2_EEEv16Flash_bwd_params' for 'sm_80'
ptxas info    : Function properties for _Z27flash_bwd_convert_dq_kernelI23Flash_bwd_kernel_traitsILi128ELi64ELi128ELi8ELi2ELi4ELi2ELb0ELb0EN7cutlass6half_tE19Flash_kernel_traitsILi128ELi64ELi128ELi8ES2_EEEv16Flash_bwd_params
    0 bytes stack frame, 0 bytes spill stores, 0 bytes spill loads
ptxas info    : Used 32 registers, 696 bytes cmem[0]
ptxas info    : Compiling entry function '_Z44flash_bwd_dq_dk_dv_loop_seqk_parallel_kernelI23Flash_bwd_kernel_traitsILi128ELi64ELi128ELi8ELi2ELi4ELi2ELb0ELb0EN7cutlass6half_tE19Flash_kernel_traitsILi128ELi64ELi128ELi8ES2_EELb1ELb0ELb0ELb0EEv16Flash_bwd_params' for 'sm_80'
ptxas info    : Function properties for _Z44flash_bwd_dq_dk_dv_loop_seqk_parallel_kernelI23Flash_bwd_kernel_traitsILi128ELi64ELi128ELi8ELi2ELi4ELi2ELb0ELb0EN7cutlass6half_tE19Flash_kernel_traitsILi128ELi64ELi128ELi8ES2_EELb1ELb0ELb0ELb0EEv16Flash_bwd_params
    136 bytes stack frame, 168 bytes spill stores, 184 bytes spill loads
ptxas info    : Used 255 registers, 696 bytes cmem[0]
ptxas info    : Compiling entry function '_Z44flash_bwd_dq_dk_dv_loop_seqk_parallel_kernelI23Flash_bwd_kernel_traitsILi128ELi64ELi128ELi8ELi2ELi4ELi2ELb0ELb0EN7cutlass6half_tE19Flash_kernel_traitsILi128ELi64ELi128ELi8ES2_EELb1ELb0ELb0ELb1EEv16Flash_bwd_params' for 'sm_80'
ptxas info    : Function properties for _Z44flash_bwd_dq_dk_dv_loop_seqk_parallel_kernelI23Flash_bwd_kernel_traitsILi128ELi64ELi128ELi8ELi2ELi4ELi2ELb0ELb0EN7cutlass6half_tE19Flash_kernel_traitsILi128ELi64ELi128ELi8ES2_EELb1ELb0ELb0ELb1EEv16Flash_bwd_params
    128 bytes stack frame, 160 bytes spill stores, 176 bytes spill loads
ptxas info    : Used 255 registers, 696 bytes cmem[0]
ptxas info    : Compiling entry function '_Z44flash_bwd_dq_dk_dv_loop_seqk_parallel_kernelI23Flash_bwd_kernel_traitsILi128ELi64ELi128ELi8ELi2ELi4ELi2ELb0ELb0EN7cutlass6half_tE19Flash_kernel_traitsILi128ELi64ELi128ELi8ES2_EELb1ELb0ELb1ELb0EEv16Flash_bwd_params' for 'sm_80'
ptxas info    : Function properties for _Z44flash_bwd_dq_dk_dv_loop_seqk_parallel_kernelI23Flash_bwd_kernel_traitsILi128ELi64ELi128ELi8ELi2ELi4ELi2ELb0ELb0EN7cutlass6half_tE19Flash_kernel_traitsILi128ELi64ELi128ELi8ES2_EELb1ELb0ELb1ELb0EEv16Flash_bwd_params
    112 bytes stack frame, 140 bytes spill stores, 164 bytes spill loads
ptxas info    : Used 255 registers, 696 bytes cmem[0]
ptxas info    : Compiling entry function '_Z44flash_bwd_dq_dk_dv_loop_seqk_parallel_kernelI23Flash_bwd_kernel_traitsILi128ELi64ELi128ELi8ELi2ELi4ELi2ELb0ELb0EN7cutlass6half_tE19Flash_kernel_traitsILi128ELi64ELi128ELi8ES2_EELb1ELb0ELb1ELb1EEv16Flash_bwd_params' for 'sm_80'
ptxas info    : Function properties for _Z44flash_bwd_dq_dk_dv_loop_seqk_parallel_kernelI23Flash_bwd_kernel_traitsILi128ELi64ELi128ELi8ELi2ELi4ELi2ELb0ELb0EN7cutlass6half_tE19Flash_kernel_traitsILi128ELi64ELi128ELi8ES2_EELb1ELb0ELb1ELb1EEv16Flash_bwd_params
    0 bytes stack frame, 0 bytes spill stores, 0 bytes spill loads
ptxas info    : Used 255 registers, 696 bytes cmem[0]
ptxas info    : Compiling entry function '_Z44flash_bwd_dq_dk_dv_loop_seqk_parallel_kernelI23Flash_bwd_kernel_traitsILi128ELi64ELi128ELi8ELi2ELi4ELi2ELb0ELb0EN7cutlass6half_tE19Flash_kernel_traitsILi128ELi64ELi128ELi8ES2_EELb1ELb1ELb0ELb0EEv16Flash_bwd_params' for 'sm_80'
ptxas info    : Function properties for _Z44flash_bwd_dq_dk_dv_loop_seqk_parallel_kernelI23Flash_bwd_kernel_traitsILi128ELi64ELi128ELi8ELi2ELi4ELi2ELb0ELb0EN7cutlass6half_tE19Flash_kernel_traitsILi128ELi64ELi128ELi8ES2_EELb1ELb1ELb0ELb0EEv16Flash_bwd_params
    24 bytes stack frame, 24 bytes spill stores, 24 bytes spill loads
ptxas info    : Used 255 registers, 696 bytes cmem[0]
ptxas info    : Compiling entry function '_Z44flash_bwd_dq_dk_dv_loop_seqk_parallel_kernelI23Flash_bwd_kernel_traitsILi128ELi64ELi128ELi8ELi2ELi4ELi2ELb0ELb0EN7cutlass6half_tE19Flash_kernel_traitsILi128ELi64ELi128ELi8ES2_EELb1ELb1ELb0ELb1EEv16Flash_bwd_params' for 'sm_80'
ptxas info    : Function properties for _Z44flash_bwd_dq_dk_dv_loop_seqk_parallel_kernelI23Flash_bwd_kernel_traitsILi128ELi64ELi128ELi8ELi2ELi4ELi2ELb0ELb0EN7cutlass6half_tE19Flash_kernel_traitsILi128ELi64ELi128ELi8ES2_EELb1ELb1ELb0ELb1EEv16Flash_bwd_params
    16 bytes stack frame, 16 bytes spill stores, 16 bytes spill loads
ptxas info    : Used 255 registers, 696 bytes cmem[0]
ptxas info    : Compiling entry function '_Z44flash_bwd_dq_dk_dv_loop_seqk_parallel_kernelI23Flash_bwd_kernel_traitsILi128ELi64ELi128ELi8ELi2ELi4ELi2ELb0ELb0EN7cutlass6half_tE19Flash_kernel_traitsILi128ELi64ELi128ELi8ES2_EELb1ELb1ELb1ELb0EEv16Flash_bwd_params' for 'sm_80'
ptxas info    : Function properties for _Z44flash_bwd_dq_dk_dv_loop_seqk_parallel_kernelI23Flash_bwd_kernel_traitsILi128ELi64ELi128ELi8ELi2ELi4ELi2ELb0ELb0EN7cutlass6half_tE19Flash_kernel_traitsILi128ELi64ELi128ELi8ES2_EELb1ELb1ELb1ELb0EEv16Flash_bwd_params
    40 bytes stack frame, 48 bytes spill stores, 40 bytes spill loads
ptxas info    : Used 255 registers, 696 bytes cmem[0]
ptxas info    : Compiling entry function '_Z44flash_bwd_dq_dk_dv_loop_seqk_parallel_kernelI23Flash_bwd_kernel_traitsILi128ELi64ELi128ELi8ELi2ELi4ELi2ELb0ELb0EN7cutlass6half_tE19Flash_kernel_traitsILi128ELi64ELi128ELi8ES2_EELb1ELb1ELb1ELb1EEv16Flash_bwd_params' for 'sm_80'
ptxas info    : Function properties for _Z44flash_bwd_dq_dk_dv_loop_seqk_parallel_kernelI23Flash_bwd_kernel_traitsILi128ELi64ELi128ELi8ELi2ELi4ELi2ELb0ELb0EN7cutlass6half_tE19Flash_kernel_traitsILi128ELi64ELi128ELi8ES2_EELb1ELb1ELb1ELb1EEv16Flash_bwd_params
    16 bytes stack frame, 12 bytes spill stores, 12 bytes spill loads
ptxas info    : Used 255 registers, 696 bytes cmem[0]
ptxas info    : Compiling entry function '_Z25flash_bwd_dot_do_o_kernelILb1E23Flash_bwd_kernel_traitsILi128ELi64ELi128ELi8ELi2ELi4ELi2ELb0ELb0EN7cutlass6half_tE19Flash_kernel_traitsILi128ELi64ELi128ELi8ES2_EEEv16Flash_bwd_params' for 'sm_80'
ptxas info    : Function properties for _Z25flash_bwd_dot_do_o_kernelILb1E23Flash_bwd_kernel_traitsILi128ELi64ELi128ELi8ELi2ELi4ELi2ELb0ELb0EN7cutlass6half_tE19Flash_kernel_traitsILi128ELi64ELi128ELi8ES2_EEEv16Flash_bwd_params
    0 bytes stack frame, 0 bytes spill stores, 0 bytes spill loads
ptxas info    : Used 47 registers, 696 bytes cmem[0]
/anaconda/envs/flashattention/lib/python3.9/site-packages/torch/include/c10/core/SymInt.h(84): warning: integer conversion resulted in a change of sign

ninja: build stopped: subcommand failed.
