

torch.__version__  = 1.12.1


running install
running bdist_egg
running egg_info
writing flash_attn.egg-info/PKG-INFO
writing dependency_links to flash_attn.egg-info/dependency_links.txt
writing requirements to flash_attn.egg-info/requires.txt
writing top-level names to flash_attn.egg-info/top_level.txt
reading manifest file 'flash_attn.egg-info/SOURCES.txt'
reading manifest template 'MANIFEST.in'
adding license file 'LICENSE'
adding license file 'AUTHORS'
writing manifest file 'flash_attn.egg-info/SOURCES.txt'
installing library code to build/bdist.linux-x86_64/egg
running install_lib
running build_py
running build_ext
building 'flash_attn_2_cuda' extension
Emitting ninja build file /home/v-feiychen/flash-attention/build/temp.linux-x86_64-cpython-39/build.ninja...
Compiling objects...
Allowing ninja to set a default number of workers... (overridable by setting the environment variable MAX_JOBS=N)
[1/13] /usr/local/cuda-11.4/bin/nvcc  -I/home/v-feiychen/flash-attention/csrc/flash_attn -I/home/v-feiychen/flash-attention/csrc/flash_attn/src -I/home/v-feiychen/flash-attention/csrc/cutlass/include -I/anaconda/envs/flashattention/lib/python3.9/site-packages/torch/include -I/anaconda/envs/flashattention/lib/python3.9/site-packages/torch/include/torch/csrc/api/include -I/anaconda/envs/flashattention/lib/python3.9/site-packages/torch/include/TH -I/anaconda/envs/flashattention/lib/python3.9/site-packages/torch/include/THC -I/usr/local/cuda-11.4/include -I/anaconda/envs/flashattention/include/python3.9 -c -c /home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_bwd_hdim512_bf16_sm80.cu -o /home/v-feiychen/flash-attention/build/temp.linux-x86_64-cpython-39/csrc/flash_attn/src/flash_bwd_hdim512_bf16_sm80.o -D__CUDA_NO_HALF_OPERATORS__ -D__CUDA_NO_HALF_CONVERSIONS__ -D__CUDA_NO_BFLOAT16_CONVERSIONS__ -D__CUDA_NO_HALF2_OPERATORS__ --expt-relaxed-constexpr --compiler-options ''"'"'-fPIC'"'"'' -O3 -std=c++17 -U__CUDA_NO_HALF_OPERATORS__ -U__CUDA_NO_HALF_CONVERSIONS__ -U__CUDA_NO_HALF2_OPERATORS__ -U__CUDA_NO_BFLOAT16_CONVERSIONS__ --expt-relaxed-constexpr --expt-extended-lambda --use_fast_math --ptxas-options=-v -lineinfo -gencode arch=compute_80,code=sm_80 --threads 4 -DTORCH_API_INCLUDE_EXTENSION_H '-DPYBIND11_COMPILER_TYPE="_gcc"' '-DPYBIND11_STDLIB="_libstdcpp"' '-DPYBIND11_BUILD_ABI="_cxxabi1011"' -DTORCH_EXTENSION_NAME=flash_attn_2_cuda -D_GLIBCXX_USE_CXX11_ABI=0
FAILED: /home/v-feiychen/flash-attention/build/temp.linux-x86_64-cpython-39/csrc/flash_attn/src/flash_bwd_hdim512_bf16_sm80.o 
/usr/local/cuda-11.4/bin/nvcc  -I/home/v-feiychen/flash-attention/csrc/flash_attn -I/home/v-feiychen/flash-attention/csrc/flash_attn/src -I/home/v-feiychen/flash-attention/csrc/cutlass/include -I/anaconda/envs/flashattention/lib/python3.9/site-packages/torch/include -I/anaconda/envs/flashattention/lib/python3.9/site-packages/torch/include/torch/csrc/api/include -I/anaconda/envs/flashattention/lib/python3.9/site-packages/torch/include/TH -I/anaconda/envs/flashattention/lib/python3.9/site-packages/torch/include/THC -I/usr/local/cuda-11.4/include -I/anaconda/envs/flashattention/include/python3.9 -c -c /home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_bwd_hdim512_bf16_sm80.cu -o /home/v-feiychen/flash-attention/build/temp.linux-x86_64-cpython-39/csrc/flash_attn/src/flash_bwd_hdim512_bf16_sm80.o -D__CUDA_NO_HALF_OPERATORS__ -D__CUDA_NO_HALF_CONVERSIONS__ -D__CUDA_NO_BFLOAT16_CONVERSIONS__ -D__CUDA_NO_HALF2_OPERATORS__ --expt-relaxed-constexpr --compiler-options ''"'"'-fPIC'"'"'' -O3 -std=c++17 -U__CUDA_NO_HALF_OPERATORS__ -U__CUDA_NO_HALF_CONVERSIONS__ -U__CUDA_NO_HALF2_OPERATORS__ -U__CUDA_NO_BFLOAT16_CONVERSIONS__ --expt-relaxed-constexpr --expt-extended-lambda --use_fast_math --ptxas-options=-v -lineinfo -gencode arch=compute_80,code=sm_80 --threads 4 -DTORCH_API_INCLUDE_EXTENSION_H '-DPYBIND11_COMPILER_TYPE="_gcc"' '-DPYBIND11_STDLIB="_libstdcpp"' '-DPYBIND11_BUILD_ABI="_cxxabi1011"' -DTORCH_EXTENSION_NAME=flash_attn_2_cuda -D_GLIBCXX_USE_CXX11_ABI=0
/anaconda/envs/flashattention/lib/python3.9/site-packages/torch/include/c10/core/SymInt.h(84): warning: integer conversion resulted in a change of sign

/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_bwd_kernel.h(1004): error: static assertion failed
          detected during:
            instantiation of "void flash::compute_dq_dk_dv_1colblock<Kernel_traits,Is_dropout,Is_causal,Is_even_MN,Is_even_K,Is_first,Is_last,Seq_parallel,Params>(const Params &, int, int, int) [with Kernel_traits=Flash_bwd_kernel_traits<512, 32, 64, 4, 2, 2, 2, false, false, cutlass::bfloat16_t, Flash_kernel_traits<512, 32, 64, 4, cutlass::bfloat16_t>>, Is_dropout=true, Is_causal=true, Is_even_MN=true, Is_even_K=true, Is_first=false, Is_last=false, Seq_parallel=true, Params=Flash_bwd_params]" 
(1591): here
            instantiation of "void flash::compute_dq_dk_dv_seqk_parallel<Kernel_traits,Is_dropout,Is_causal,Is_even_MN,Is_even_K,Params>(const Params &) [with Kernel_traits=Flash_bwd_kernel_traits<512, 32, 64, 4, 2, 2, 2, false, false, cutlass::bfloat16_t, Flash_kernel_traits<512, 32, 64, 4, cutlass::bfloat16_t>>, Is_dropout=true, Is_causal=true, Is_even_MN=true, Is_even_K=true, Params=Flash_bwd_params]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_bwd_launch_template.h(28): here
            instantiation of "void flash_bwd_dq_dk_dv_loop_seqk_parallel_kernel<Kernel_traits,Is_dropout,Is_causal,Is_even_MN,Is_even_K>(Flash_bwd_params) [with Kernel_traits=Flash_bwd_kernel_traits<512, 32, 64, 4, 2, 2, 2, false, false, cutlass::bfloat16_t, Flash_kernel_traits<512, 32, 64, 4, cutlass::bfloat16_t>>, Is_dropout=true, Is_causal=true, Is_even_MN=true, Is_even_K=true]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_bwd_launch_template.h(62): here
            instantiation of "void run_flash_bwd_seqk_parallel<Kernel_traits,Is_dropout>(Flash_bwd_params &, cudaStream_t, __nv_bool) [with Kernel_traits=Flash_bwd_kernel_traits<512, 32, 64, 4, 2, 2, 2, false, false, cutlass::bfloat16_t, Flash_kernel_traits<512, 32, 64, 4, cutlass::bfloat16_t>>, Is_dropout=true]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_bwd_launch_template.h(134): here
            instantiation of "void run_flash_bwd<Kernel_traits,Is_dropout>(Flash_bwd_params &, cudaStream_t, __nv_bool) [with Kernel_traits=Flash_bwd_kernel_traits<512, 32, 64, 4, 2, 2, 2, false, false, cutlass::bfloat16_t, Flash_kernel_traits<512, 32, 64, 4, cutlass::bfloat16_t>>, Is_dropout=true]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_bwd_launch_template.h(365): here
            instantiation of "void run_mha_bwd_hdim512<T>(Flash_bwd_params &, cudaStream_t, __nv_bool) [with T=cutlass::bfloat16_t]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_bwd_hdim512_bf16_sm80.cu(9): here

/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_bwd_kernel.h(1004): error: static assertion failed
          detected during:
            instantiation of "void flash::compute_dq_dk_dv_1colblock<Kernel_traits,Is_dropout,Is_causal,Is_even_MN,Is_even_K,Is_first,Is_last,Seq_parallel,Params>(const Params &, int, int, int) [with Kernel_traits=Flash_bwd_kernel_traits<512, 32, 64, 4, 2, 2, 2, false, false, cutlass::bfloat16_t, Flash_kernel_traits<512, 32, 64, 4, cutlass::bfloat16_t>>, Is_dropout=true, Is_causal=true, Is_even_MN=true, Is_even_K=false, Is_first=false, Is_last=false, Seq_parallel=true, Params=Flash_bwd_params]" 
(1591): here
            instantiation of "void flash::compute_dq_dk_dv_seqk_parallel<Kernel_traits,Is_dropout,Is_causal,Is_even_MN,Is_even_K,Params>(const Params &) [with Kernel_traits=Flash_bwd_kernel_traits<512, 32, 64, 4, 2, 2, 2, false, false, cutlass::bfloat16_t, Flash_kernel_traits<512, 32, 64, 4, cutlass::bfloat16_t>>, Is_dropout=true, Is_causal=true, Is_even_MN=true, Is_even_K=false, Params=Flash_bwd_params]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_bwd_launch_template.h(28): here
            instantiation of "void flash_bwd_dq_dk_dv_loop_seqk_parallel_kernel<Kernel_traits,Is_dropout,Is_causal,Is_even_MN,Is_even_K>(Flash_bwd_params) [with Kernel_traits=Flash_bwd_kernel_traits<512, 32, 64, 4, 2, 2, 2, false, false, cutlass::bfloat16_t, Flash_kernel_traits<512, 32, 64, 4, cutlass::bfloat16_t>>, Is_dropout=true, Is_causal=true, Is_even_MN=true, Is_even_K=false]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_bwd_launch_template.h(62): here
            instantiation of "void run_flash_bwd_seqk_parallel<Kernel_traits,Is_dropout>(Flash_bwd_params &, cudaStream_t, __nv_bool) [with Kernel_traits=Flash_bwd_kernel_traits<512, 32, 64, 4, 2, 2, 2, false, false, cutlass::bfloat16_t, Flash_kernel_traits<512, 32, 64, 4, cutlass::bfloat16_t>>, Is_dropout=true]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_bwd_launch_template.h(134): here
            instantiation of "void run_flash_bwd<Kernel_traits,Is_dropout>(Flash_bwd_params &, cudaStream_t, __nv_bool) [with Kernel_traits=Flash_bwd_kernel_traits<512, 32, 64, 4, 2, 2, 2, false, false, cutlass::bfloat16_t, Flash_kernel_traits<512, 32, 64, 4, cutlass::bfloat16_t>>, Is_dropout=true]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_bwd_launch_template.h(365): here
            instantiation of "void run_mha_bwd_hdim512<T>(Flash_bwd_params &, cudaStream_t, __nv_bool) [with T=cutlass::bfloat16_t]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_bwd_hdim512_bf16_sm80.cu(9): here

/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_bwd_kernel.h(1004): error: static assertion failed
          detected during:
            instantiation of "void flash::compute_dq_dk_dv_1colblock<Kernel_traits,Is_dropout,Is_causal,Is_even_MN,Is_even_K,Is_first,Is_last,Seq_parallel,Params>(const Params &, int, int, int) [with Kernel_traits=Flash_bwd_kernel_traits<512, 32, 64, 4, 2, 2, 2, false, false, cutlass::bfloat16_t, Flash_kernel_traits<512, 32, 64, 4, cutlass::bfloat16_t>>, Is_dropout=true, Is_causal=true, Is_even_MN=false, Is_even_K=true, Is_first=false, Is_last=false, Seq_parallel=true, Params=Flash_bwd_params]" 
(1591): here
            instantiation of "void flash::compute_dq_dk_dv_seqk_parallel<Kernel_traits,Is_dropout,Is_causal,Is_even_MN,Is_even_K,Params>(const Params &) [with Kernel_traits=Flash_bwd_kernel_traits<512, 32, 64, 4, 2, 2, 2, false, false, cutlass::bfloat16_t, Flash_kernel_traits<512, 32, 64, 4, cutlass::bfloat16_t>>, Is_dropout=true, Is_causal=true, Is_even_MN=false, Is_even_K=true, Params=Flash_bwd_params]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_bwd_launch_template.h(28): here
            instantiation of "void flash_bwd_dq_dk_dv_loop_seqk_parallel_kernel<Kernel_traits,Is_dropout,Is_causal,Is_even_MN,Is_even_K>(Flash_bwd_params) [with Kernel_traits=Flash_bwd_kernel_traits<512, 32, 64, 4, 2, 2, 2, false, false, cutlass::bfloat16_t, Flash_kernel_traits<512, 32, 64, 4, cutlass::bfloat16_t>>, Is_dropout=true, Is_causal=true, Is_even_MN=false, Is_even_K=true]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_bwd_launch_template.h(62): here
            instantiation of "void run_flash_bwd_seqk_parallel<Kernel_traits,Is_dropout>(Flash_bwd_params &, cudaStream_t, __nv_bool) [with Kernel_traits=Flash_bwd_kernel_traits<512, 32, 64, 4, 2, 2, 2, false, false, cutlass::bfloat16_t, Flash_kernel_traits<512, 32, 64, 4, cutlass::bfloat16_t>>, Is_dropout=true]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_bwd_launch_template.h(134): here
            instantiation of "void run_flash_bwd<Kernel_traits,Is_dropout>(Flash_bwd_params &, cudaStream_t, __nv_bool) [with Kernel_traits=Flash_bwd_kernel_traits<512, 32, 64, 4, 2, 2, 2, false, false, cutlass::bfloat16_t, Flash_kernel_traits<512, 32, 64, 4, cutlass::bfloat16_t>>, Is_dropout=true]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_bwd_launch_template.h(365): here
            instantiation of "void run_mha_bwd_hdim512<T>(Flash_bwd_params &, cudaStream_t, __nv_bool) [with T=cutlass::bfloat16_t]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_bwd_hdim512_bf16_sm80.cu(9): here

/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_bwd_kernel.h(1004): error: static assertion failed
          detected during:
            instantiation of "void flash::compute_dq_dk_dv_1colblock<Kernel_traits,Is_dropout,Is_causal,Is_even_MN,Is_even_K,Is_first,Is_last,Seq_parallel,Params>(const Params &, int, int, int) [with Kernel_traits=Flash_bwd_kernel_traits<512, 32, 64, 4, 2, 2, 2, false, false, cutlass::bfloat16_t, Flash_kernel_traits<512, 32, 64, 4, cutlass::bfloat16_t>>, Is_dropout=true, Is_causal=true, Is_even_MN=false, Is_even_K=false, Is_first=false, Is_last=false, Seq_parallel=true, Params=Flash_bwd_params]" 
(1591): here
            instantiation of "void flash::compute_dq_dk_dv_seqk_parallel<Kernel_traits,Is_dropout,Is_causal,Is_even_MN,Is_even_K,Params>(const Params &) [with Kernel_traits=Flash_bwd_kernel_traits<512, 32, 64, 4, 2, 2, 2, false, false, cutlass::bfloat16_t, Flash_kernel_traits<512, 32, 64, 4, cutlass::bfloat16_t>>, Is_dropout=true, Is_causal=true, Is_even_MN=false, Is_even_K=false, Params=Flash_bwd_params]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_bwd_launch_template.h(28): here
            instantiation of "void flash_bwd_dq_dk_dv_loop_seqk_parallel_kernel<Kernel_traits,Is_dropout,Is_causal,Is_even_MN,Is_even_K>(Flash_bwd_params) [with Kernel_traits=Flash_bwd_kernel_traits<512, 32, 64, 4, 2, 2, 2, false, false, cutlass::bfloat16_t, Flash_kernel_traits<512, 32, 64, 4, cutlass::bfloat16_t>>, Is_dropout=true, Is_causal=true, Is_even_MN=false, Is_even_K=false]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_bwd_launch_template.h(62): here
            instantiation of "void run_flash_bwd_seqk_parallel<Kernel_traits,Is_dropout>(Flash_bwd_params &, cudaStream_t, __nv_bool) [with Kernel_traits=Flash_bwd_kernel_traits<512, 32, 64, 4, 2, 2, 2, false, false, cutlass::bfloat16_t, Flash_kernel_traits<512, 32, 64, 4, cutlass::bfloat16_t>>, Is_dropout=true]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_bwd_launch_template.h(134): here
            instantiation of "void run_flash_bwd<Kernel_traits,Is_dropout>(Flash_bwd_params &, cudaStream_t, __nv_bool) [with Kernel_traits=Flash_bwd_kernel_traits<512, 32, 64, 4, 2, 2, 2, false, false, cutlass::bfloat16_t, Flash_kernel_traits<512, 32, 64, 4, cutlass::bfloat16_t>>, Is_dropout=true]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_bwd_launch_template.h(365): here
            instantiation of "void run_mha_bwd_hdim512<T>(Flash_bwd_params &, cudaStream_t, __nv_bool) [with T=cutlass::bfloat16_t]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_bwd_hdim512_bf16_sm80.cu(9): here

/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_bwd_kernel.h(1004): error: static assertion failed
          detected during:
            instantiation of "void flash::compute_dq_dk_dv_1colblock<Kernel_traits,Is_dropout,Is_causal,Is_even_MN,Is_even_K,Is_first,Is_last,Seq_parallel,Params>(const Params &, int, int, int) [with Kernel_traits=Flash_bwd_kernel_traits<512, 32, 64, 4, 2, 2, 2, false, false, cutlass::bfloat16_t, Flash_kernel_traits<512, 32, 64, 4, cutlass::bfloat16_t>>, Is_dropout=true, Is_causal=false, Is_even_MN=true, Is_even_K=true, Is_first=false, Is_last=false, Seq_parallel=true, Params=Flash_bwd_params]" 
(1591): here
            instantiation of "void flash::compute_dq_dk_dv_seqk_parallel<Kernel_traits,Is_dropout,Is_causal,Is_even_MN,Is_even_K,Params>(const Params &) [with Kernel_traits=Flash_bwd_kernel_traits<512, 32, 64, 4, 2, 2, 2, false, false, cutlass::bfloat16_t, Flash_kernel_traits<512, 32, 64, 4, cutlass::bfloat16_t>>, Is_dropout=true, Is_causal=false, Is_even_MN=true, Is_even_K=true, Params=Flash_bwd_params]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_bwd_launch_template.h(28): here
            instantiation of "void flash_bwd_dq_dk_dv_loop_seqk_parallel_kernel<Kernel_traits,Is_dropout,Is_causal,Is_even_MN,Is_even_K>(Flash_bwd_params) [with Kernel_traits=Flash_bwd_kernel_traits<512, 32, 64, 4, 2, 2, 2, false, false, cutlass::bfloat16_t, Flash_kernel_traits<512, 32, 64, 4, cutlass::bfloat16_t>>, Is_dropout=true, Is_causal=false, Is_even_MN=true, Is_even_K=true]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_bwd_launch_template.h(62): here
            instantiation of "void run_flash_bwd_seqk_parallel<Kernel_traits,Is_dropout>(Flash_bwd_params &, cudaStream_t, __nv_bool) [with Kernel_traits=Flash_bwd_kernel_traits<512, 32, 64, 4, 2, 2, 2, false, false, cutlass::bfloat16_t, Flash_kernel_traits<512, 32, 64, 4, cutlass::bfloat16_t>>, Is_dropout=true]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_bwd_launch_template.h(134): here
            instantiation of "void run_flash_bwd<Kernel_traits,Is_dropout>(Flash_bwd_params &, cudaStream_t, __nv_bool) [with Kernel_traits=Flash_bwd_kernel_traits<512, 32, 64, 4, 2, 2, 2, false, false, cutlass::bfloat16_t, Flash_kernel_traits<512, 32, 64, 4, cutlass::bfloat16_t>>, Is_dropout=true]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_bwd_launch_template.h(365): here
            instantiation of "void run_mha_bwd_hdim512<T>(Flash_bwd_params &, cudaStream_t, __nv_bool) [with T=cutlass::bfloat16_t]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_bwd_hdim512_bf16_sm80.cu(9): here

/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_bwd_kernel.h(1004): error: static assertion failed
          detected during:
            instantiation of "void flash::compute_dq_dk_dv_1colblock<Kernel_traits,Is_dropout,Is_causal,Is_even_MN,Is_even_K,Is_first,Is_last,Seq_parallel,Params>(const Params &, int, int, int) [with Kernel_traits=Flash_bwd_kernel_traits<512, 32, 64, 4, 2, 2, 2, false, false, cutlass::bfloat16_t, Flash_kernel_traits<512, 32, 64, 4, cutlass::bfloat16_t>>, Is_dropout=true, Is_causal=false, Is_even_MN=true, Is_even_K=false, Is_first=false, Is_last=false, Seq_parallel=true, Params=Flash_bwd_params]" 
(1591): here
            instantiation of "void flash::compute_dq_dk_dv_seqk_parallel<Kernel_traits,Is_dropout,Is_causal,Is_even_MN,Is_even_K,Params>(const Params &) [with Kernel_traits=Flash_bwd_kernel_traits<512, 32, 64, 4, 2, 2, 2, false, false, cutlass::bfloat16_t, Flash_kernel_traits<512, 32, 64, 4, cutlass::bfloat16_t>>, Is_dropout=true, Is_causal=false, Is_even_MN=true, Is_even_K=false, Params=Flash_bwd_params]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_bwd_launch_template.h(28): here
            instantiation of "void flash_bwd_dq_dk_dv_loop_seqk_parallel_kernel<Kernel_traits,Is_dropout,Is_causal,Is_even_MN,Is_even_K>(Flash_bwd_params) [with Kernel_traits=Flash_bwd_kernel_traits<512, 32, 64, 4, 2, 2, 2, false, false, cutlass::bfloat16_t, Flash_kernel_traits<512, 32, 64, 4, cutlass::bfloat16_t>>, Is_dropout=true, Is_causal=false, Is_even_MN=true, Is_even_K=false]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_bwd_launch_template.h(62): here
            instantiation of "void run_flash_bwd_seqk_parallel<Kernel_traits,Is_dropout>(Flash_bwd_params &, cudaStream_t, __nv_bool) [with Kernel_traits=Flash_bwd_kernel_traits<512, 32, 64, 4, 2, 2, 2, false, false, cutlass::bfloat16_t, Flash_kernel_traits<512, 32, 64, 4, cutlass::bfloat16_t>>, Is_dropout=true]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_bwd_launch_template.h(134): here
            instantiation of "void run_flash_bwd<Kernel_traits,Is_dropout>(Flash_bwd_params &, cudaStream_t, __nv_bool) [with Kernel_traits=Flash_bwd_kernel_traits<512, 32, 64, 4, 2, 2, 2, false, false, cutlass::bfloat16_t, Flash_kernel_traits<512, 32, 64, 4, cutlass::bfloat16_t>>, Is_dropout=true]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_bwd_launch_template.h(365): here
            instantiation of "void run_mha_bwd_hdim512<T>(Flash_bwd_params &, cudaStream_t, __nv_bool) [with T=cutlass::bfloat16_t]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_bwd_hdim512_bf16_sm80.cu(9): here

/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_bwd_kernel.h(1004): error: static assertion failed
          detected during:
            instantiation of "void flash::compute_dq_dk_dv_1colblock<Kernel_traits,Is_dropout,Is_causal,Is_even_MN,Is_even_K,Is_first,Is_last,Seq_parallel,Params>(const Params &, int, int, int) [with Kernel_traits=Flash_bwd_kernel_traits<512, 32, 64, 4, 2, 2, 2, false, false, cutlass::bfloat16_t, Flash_kernel_traits<512, 32, 64, 4, cutlass::bfloat16_t>>, Is_dropout=true, Is_causal=false, Is_even_MN=false, Is_even_K=true, Is_first=false, Is_last=false, Seq_parallel=true, Params=Flash_bwd_params]" 
(1591): here
            instantiation of "void flash::compute_dq_dk_dv_seqk_parallel<Kernel_traits,Is_dropout,Is_causal,Is_even_MN,Is_even_K,Params>(const Params &) [with Kernel_traits=Flash_bwd_kernel_traits<512, 32, 64, 4, 2, 2, 2, false, false, cutlass::bfloat16_t, Flash_kernel_traits<512, 32, 64, 4, cutlass::bfloat16_t>>, Is_dropout=true, Is_causal=false, Is_even_MN=false, Is_even_K=true, Params=Flash_bwd_params]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_bwd_launch_template.h(28): here
            instantiation of "void flash_bwd_dq_dk_dv_loop_seqk_parallel_kernel<Kernel_traits,Is_dropout,Is_causal,Is_even_MN,Is_even_K>(Flash_bwd_params) [with Kernel_traits=Flash_bwd_kernel_traits<512, 32, 64, 4, 2, 2, 2, false, false, cutlass::bfloat16_t, Flash_kernel_traits<512, 32, 64, 4, cutlass::bfloat16_t>>, Is_dropout=true, Is_causal=false, Is_even_MN=false, Is_even_K=true]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_bwd_launch_template.h(62): here
            instantiation of "void run_flash_bwd_seqk_parallel<Kernel_traits,Is_dropout>(Flash_bwd_params &, cudaStream_t, __nv_bool) [with Kernel_traits=Flash_bwd_kernel_traits<512, 32, 64, 4, 2, 2, 2, false, false, cutlass::bfloat16_t, Flash_kernel_traits<512, 32, 64, 4, cutlass::bfloat16_t>>, Is_dropout=true]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_bwd_launch_template.h(134): here
            instantiation of "void run_flash_bwd<Kernel_traits,Is_dropout>(Flash_bwd_params &, cudaStream_t, __nv_bool) [with Kernel_traits=Flash_bwd_kernel_traits<512, 32, 64, 4, 2, 2, 2, false, false, cutlass::bfloat16_t, Flash_kernel_traits<512, 32, 64, 4, cutlass::bfloat16_t>>, Is_dropout=true]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_bwd_launch_template.h(365): here
            instantiation of "void run_mha_bwd_hdim512<T>(Flash_bwd_params &, cudaStream_t, __nv_bool) [with T=cutlass::bfloat16_t]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_bwd_hdim512_bf16_sm80.cu(9): here

/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_bwd_kernel.h(1004): error: static assertion failed
          detected during:
            instantiation of "void flash::compute_dq_dk_dv_1colblock<Kernel_traits,Is_dropout,Is_causal,Is_even_MN,Is_even_K,Is_first,Is_last,Seq_parallel,Params>(const Params &, int, int, int) [with Kernel_traits=Flash_bwd_kernel_traits<512, 32, 64, 4, 2, 2, 2, false, false, cutlass::bfloat16_t, Flash_kernel_traits<512, 32, 64, 4, cutlass::bfloat16_t>>, Is_dropout=true, Is_causal=false, Is_even_MN=false, Is_even_K=false, Is_first=false, Is_last=false, Seq_parallel=true, Params=Flash_bwd_params]" 
(1591): here
            instantiation of "void flash::compute_dq_dk_dv_seqk_parallel<Kernel_traits,Is_dropout,Is_causal,Is_even_MN,Is_even_K,Params>(const Params &) [with Kernel_traits=Flash_bwd_kernel_traits<512, 32, 64, 4, 2, 2, 2, false, false, cutlass::bfloat16_t, Flash_kernel_traits<512, 32, 64, 4, cutlass::bfloat16_t>>, Is_dropout=true, Is_causal=false, Is_even_MN=false, Is_even_K=false, Params=Flash_bwd_params]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_bwd_launch_template.h(28): here
            instantiation of "void flash_bwd_dq_dk_dv_loop_seqk_parallel_kernel<Kernel_traits,Is_dropout,Is_causal,Is_even_MN,Is_even_K>(Flash_bwd_params) [with Kernel_traits=Flash_bwd_kernel_traits<512, 32, 64, 4, 2, 2, 2, false, false, cutlass::bfloat16_t, Flash_kernel_traits<512, 32, 64, 4, cutlass::bfloat16_t>>, Is_dropout=true, Is_causal=false, Is_even_MN=false, Is_even_K=false]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_bwd_launch_template.h(62): here
            instantiation of "void run_flash_bwd_seqk_parallel<Kernel_traits,Is_dropout>(Flash_bwd_params &, cudaStream_t, __nv_bool) [with Kernel_traits=Flash_bwd_kernel_traits<512, 32, 64, 4, 2, 2, 2, false, false, cutlass::bfloat16_t, Flash_kernel_traits<512, 32, 64, 4, cutlass::bfloat16_t>>, Is_dropout=true]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_bwd_launch_template.h(134): here
            instantiation of "void run_flash_bwd<Kernel_traits,Is_dropout>(Flash_bwd_params &, cudaStream_t, __nv_bool) [with Kernel_traits=Flash_bwd_kernel_traits<512, 32, 64, 4, 2, 2, 2, false, false, cutlass::bfloat16_t, Flash_kernel_traits<512, 32, 64, 4, cutlass::bfloat16_t>>, Is_dropout=true]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_bwd_launch_template.h(365): here
            instantiation of "void run_mha_bwd_hdim512<T>(Flash_bwd_params &, cudaStream_t, __nv_bool) [with T=cutlass::bfloat16_t]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_bwd_hdim512_bf16_sm80.cu(9): here

/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_bwd_kernel.h(287): error: static assertion failed
          detected during:
            instantiation of "void flash::convert_dQ<Kernel_traits,Params>(const Params &) [with Kernel_traits=Flash_bwd_kernel_traits<512, 32, 64, 4, 2, 2, 2, false, false, cutlass::bfloat16_t, Flash_kernel_traits<512, 32, 64, 4, cutlass::bfloat16_t>>, Params=Flash_bwd_params]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_bwd_launch_template.h(38): here
            instantiation of "void flash_bwd_convert_dq_kernel<Kernel_traits>(Flash_bwd_params) [with Kernel_traits=Flash_bwd_kernel_traits<512, 32, 64, 4, 2, 2, 2, false, false, cutlass::bfloat16_t, Flash_kernel_traits<512, 32, 64, 4, cutlass::bfloat16_t>>]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_bwd_launch_template.h(77): here
            instantiation of "void run_flash_bwd_seqk_parallel<Kernel_traits,Is_dropout>(Flash_bwd_params &, cudaStream_t, __nv_bool) [with Kernel_traits=Flash_bwd_kernel_traits<512, 32, 64, 4, 2, 2, 2, false, false, cutlass::bfloat16_t, Flash_kernel_traits<512, 32, 64, 4, cutlass::bfloat16_t>>, Is_dropout=true]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_bwd_launch_template.h(134): here
            instantiation of "void run_flash_bwd<Kernel_traits,Is_dropout>(Flash_bwd_params &, cudaStream_t, __nv_bool) [with Kernel_traits=Flash_bwd_kernel_traits<512, 32, 64, 4, 2, 2, 2, false, false, cutlass::bfloat16_t, Flash_kernel_traits<512, 32, 64, 4, cutlass::bfloat16_t>>, Is_dropout=true]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_bwd_launch_template.h(365): here
            instantiation of "void run_mha_bwd_hdim512<T>(Flash_bwd_params &, cudaStream_t, __nv_bool) [with T=cutlass::bfloat16_t]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_bwd_hdim512_bf16_sm80.cu(9): here

/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_bwd_kernel.h(1004): error: static assertion failed
          detected during:
            instantiation of "void flash::compute_dq_dk_dv_1colblock<Kernel_traits,Is_dropout,Is_causal,Is_even_MN,Is_even_K,Is_first,Is_last,Seq_parallel,Params>(const Params &, int, int, int) [with Kernel_traits=Flash_bwd_kernel_traits<512, 32, 64, 4, 2, 2, 2, false, true, cutlass::bfloat16_t, Flash_kernel_traits<512, 32, 64, 4, cutlass::bfloat16_t>>, Is_dropout=true, Is_causal=true, Is_even_MN=true, Is_even_K=true, Is_first=false, Is_last=false, Seq_parallel=true, Params=Flash_bwd_params]" 
(1591): here
            instantiation of "void flash::compute_dq_dk_dv_seqk_parallel<Kernel_traits,Is_dropout,Is_causal,Is_even_MN,Is_even_K,Params>(const Params &) [with Kernel_traits=Flash_bwd_kernel_traits<512, 32, 64, 4, 2, 2, 2, false, true, cutlass::bfloat16_t, Flash_kernel_traits<512, 32, 64, 4, cutlass::bfloat16_t>>, Is_dropout=true, Is_causal=true, Is_even_MN=true, Is_even_K=true, Params=Flash_bwd_params]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_bwd_launch_template.h(28): here
            instantiation of "void flash_bwd_dq_dk_dv_loop_seqk_parallel_kernel<Kernel_traits,Is_dropout,Is_causal,Is_even_MN,Is_even_K>(Flash_bwd_params) [with Kernel_traits=Flash_bwd_kernel_traits<512, 32, 64, 4, 2, 2, 2, false, true, cutlass::bfloat16_t, Flash_kernel_traits<512, 32, 64, 4, cutlass::bfloat16_t>>, Is_dropout=true, Is_causal=true, Is_even_MN=true, Is_even_K=true]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_bwd_launch_template.h(62): here
            instantiation of "void run_flash_bwd_seqk_parallel<Kernel_traits,Is_dropout>(Flash_bwd_params &, cudaStream_t, __nv_bool) [with Kernel_traits=Flash_bwd_kernel_traits<512, 32, 64, 4, 2, 2, 2, false, true, cutlass::bfloat16_t, Flash_kernel_traits<512, 32, 64, 4, cutlass::bfloat16_t>>, Is_dropout=true]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_bwd_launch_template.h(134): here
            instantiation of "void run_flash_bwd<Kernel_traits,Is_dropout>(Flash_bwd_params &, cudaStream_t, __nv_bool) [with Kernel_traits=Flash_bwd_kernel_traits<512, 32, 64, 4, 2, 2, 2, false, true, cutlass::bfloat16_t, Flash_kernel_traits<512, 32, 64, 4, cutlass::bfloat16_t>>, Is_dropout=true]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_bwd_launch_template.h(365): here
            instantiation of "void run_mha_bwd_hdim512<T>(Flash_bwd_params &, cudaStream_t, __nv_bool) [with T=cutlass::bfloat16_t]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_bwd_hdim512_bf16_sm80.cu(9): here

/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_bwd_kernel.h(1004): error: static assertion failed
          detected during:
            instantiation of "void flash::compute_dq_dk_dv_1colblock<Kernel_traits,Is_dropout,Is_causal,Is_even_MN,Is_even_K,Is_first,Is_last,Seq_parallel,Params>(const Params &, int, int, int) [with Kernel_traits=Flash_bwd_kernel_traits<512, 32, 64, 4, 2, 2, 2, false, true, cutlass::bfloat16_t, Flash_kernel_traits<512, 32, 64, 4, cutlass::bfloat16_t>>, Is_dropout=true, Is_causal=true, Is_even_MN=true, Is_even_K=false, Is_first=false, Is_last=false, Seq_parallel=true, Params=Flash_bwd_params]" 
(1591): here
            instantiation of "void flash::compute_dq_dk_dv_seqk_parallel<Kernel_traits,Is_dropout,Is_causal,Is_even_MN,Is_even_K,Params>(const Params &) [with Kernel_traits=Flash_bwd_kernel_traits<512, 32, 64, 4, 2, 2, 2, false, true, cutlass::bfloat16_t, Flash_kernel_traits<512, 32, 64, 4, cutlass::bfloat16_t>>, Is_dropout=true, Is_causal=true, Is_even_MN=true, Is_even_K=false, Params=Flash_bwd_params]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_bwd_launch_template.h(28): here
            instantiation of "void flash_bwd_dq_dk_dv_loop_seqk_parallel_kernel<Kernel_traits,Is_dropout,Is_causal,Is_even_MN,Is_even_K>(Flash_bwd_params) [with Kernel_traits=Flash_bwd_kernel_traits<512, 32, 64, 4, 2, 2, 2, false, true, cutlass::bfloat16_t, Flash_kernel_traits<512, 32, 64, 4, cutlass::bfloat16_t>>, Is_dropout=true, Is_causal=true, Is_even_MN=true, Is_even_K=false]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_bwd_launch_template.h(62): here
            instantiation of "void run_flash_bwd_seqk_parallel<Kernel_traits,Is_dropout>(Flash_bwd_params &, cudaStream_t, __nv_bool) [with Kernel_traits=Flash_bwd_kernel_traits<512, 32, 64, 4, 2, 2, 2, false, true, cutlass::bfloat16_t, Flash_kernel_traits<512, 32, 64, 4, cutlass::bfloat16_t>>, Is_dropout=true]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_bwd_launch_template.h(134): here
            instantiation of "void run_flash_bwd<Kernel_traits,Is_dropout>(Flash_bwd_params &, cudaStream_t, __nv_bool) [with Kernel_traits=Flash_bwd_kernel_traits<512, 32, 64, 4, 2, 2, 2, false, true, cutlass::bfloat16_t, Flash_kernel_traits<512, 32, 64, 4, cutlass::bfloat16_t>>, Is_dropout=true]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_bwd_launch_template.h(365): here
            instantiation of "void run_mha_bwd_hdim512<T>(Flash_bwd_params &, cudaStream_t, __nv_bool) [with T=cutlass::bfloat16_t]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_bwd_hdim512_bf16_sm80.cu(9): here

/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_bwd_kernel.h(1004): error: static assertion failed
          detected during:
            instantiation of "void flash::compute_dq_dk_dv_1colblock<Kernel_traits,Is_dropout,Is_causal,Is_even_MN,Is_even_K,Is_first,Is_last,Seq_parallel,Params>(const Params &, int, int, int) [with Kernel_traits=Flash_bwd_kernel_traits<512, 32, 64, 4, 2, 2, 2, false, true, cutlass::bfloat16_t, Flash_kernel_traits<512, 32, 64, 4, cutlass::bfloat16_t>>, Is_dropout=true, Is_causal=true, Is_even_MN=false, Is_even_K=true, Is_first=false, Is_last=false, Seq_parallel=true, Params=Flash_bwd_params]" 
(1591): here
            instantiation of "void flash::compute_dq_dk_dv_seqk_parallel<Kernel_traits,Is_dropout,Is_causal,Is_even_MN,Is_even_K,Params>(const Params &) [with Kernel_traits=Flash_bwd_kernel_traits<512, 32, 64, 4, 2, 2, 2, false, true, cutlass::bfloat16_t, Flash_kernel_traits<512, 32, 64, 4, cutlass::bfloat16_t>>, Is_dropout=true, Is_causal=true, Is_even_MN=false, Is_even_K=true, Params=Flash_bwd_params]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_bwd_launch_template.h(28): here
            instantiation of "void flash_bwd_dq_dk_dv_loop_seqk_parallel_kernel<Kernel_traits,Is_dropout,Is_causal,Is_even_MN,Is_even_K>(Flash_bwd_params) [with Kernel_traits=Flash_bwd_kernel_traits<512, 32, 64, 4, 2, 2, 2, false, true, cutlass::bfloat16_t, Flash_kernel_traits<512, 32, 64, 4, cutlass::bfloat16_t>>, Is_dropout=true, Is_causal=true, Is_even_MN=false, Is_even_K=true]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_bwd_launch_template.h(62): here
            instantiation of "void run_flash_bwd_seqk_parallel<Kernel_traits,Is_dropout>(Flash_bwd_params &, cudaStream_t, __nv_bool) [with Kernel_traits=Flash_bwd_kernel_traits<512, 32, 64, 4, 2, 2, 2, false, true, cutlass::bfloat16_t, Flash_kernel_traits<512, 32, 64, 4, cutlass::bfloat16_t>>, Is_dropout=true]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_bwd_launch_template.h(134): here
            instantiation of "void run_flash_bwd<Kernel_traits,Is_dropout>(Flash_bwd_params &, cudaStream_t, __nv_bool) [with Kernel_traits=Flash_bwd_kernel_traits<512, 32, 64, 4, 2, 2, 2, false, true, cutlass::bfloat16_t, Flash_kernel_traits<512, 32, 64, 4, cutlass::bfloat16_t>>, Is_dropout=true]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_bwd_launch_template.h(365): here
            instantiation of "void run_mha_bwd_hdim512<T>(Flash_bwd_params &, cudaStream_t, __nv_bool) [with T=cutlass::bfloat16_t]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_bwd_hdim512_bf16_sm80.cu(9): here

/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_bwd_kernel.h(1004): error: static assertion failed
          detected during:
            instantiation of "void flash::compute_dq_dk_dv_1colblock<Kernel_traits,Is_dropout,Is_causal,Is_even_MN,Is_even_K,Is_first,Is_last,Seq_parallel,Params>(const Params &, int, int, int) [with Kernel_traits=Flash_bwd_kernel_traits<512, 32, 64, 4, 2, 2, 2, false, true, cutlass::bfloat16_t, Flash_kernel_traits<512, 32, 64, 4, cutlass::bfloat16_t>>, Is_dropout=true, Is_causal=true, Is_even_MN=false, Is_even_K=false, Is_first=false, Is_last=false, Seq_parallel=true, Params=Flash_bwd_params]" 
(1591): here
            instantiation of "void flash::compute_dq_dk_dv_seqk_parallel<Kernel_traits,Is_dropout,Is_causal,Is_even_MN,Is_even_K,Params>(const Params &) [with Kernel_traits=Flash_bwd_kernel_traits<512, 32, 64, 4, 2, 2, 2, false, true, cutlass::bfloat16_t, Flash_kernel_traits<512, 32, 64, 4, cutlass::bfloat16_t>>, Is_dropout=true, Is_causal=true, Is_even_MN=false, Is_even_K=false, Params=Flash_bwd_params]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_bwd_launch_template.h(28): here
            instantiation of "void flash_bwd_dq_dk_dv_loop_seqk_parallel_kernel<Kernel_traits,Is_dropout,Is_causal,Is_even_MN,Is_even_K>(Flash_bwd_params) [with Kernel_traits=Flash_bwd_kernel_traits<512, 32, 64, 4, 2, 2, 2, false, true, cutlass::bfloat16_t, Flash_kernel_traits<512, 32, 64, 4, cutlass::bfloat16_t>>, Is_dropout=true, Is_causal=true, Is_even_MN=false, Is_even_K=false]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_bwd_launch_template.h(62): here
            instantiation of "void run_flash_bwd_seqk_parallel<Kernel_traits,Is_dropout>(Flash_bwd_params &, cudaStream_t, __nv_bool) [with Kernel_traits=Flash_bwd_kernel_traits<512, 32, 64, 4, 2, 2, 2, false, true, cutlass::bfloat16_t, Flash_kernel_traits<512, 32, 64, 4, cutlass::bfloat16_t>>, Is_dropout=true]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_bwd_launch_template.h(134): here
            instantiation of "void run_flash_bwd<Kernel_traits,Is_dropout>(Flash_bwd_params &, cudaStream_t, __nv_bool) [with Kernel_traits=Flash_bwd_kernel_traits<512, 32, 64, 4, 2, 2, 2, false, true, cutlass::bfloat16_t, Flash_kernel_traits<512, 32, 64, 4, cutlass::bfloat16_t>>, Is_dropout=true]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_bwd_launch_template.h(365): here
            instantiation of "void run_mha_bwd_hdim512<T>(Flash_bwd_params &, cudaStream_t, __nv_bool) [with T=cutlass::bfloat16_t]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_bwd_hdim512_bf16_sm80.cu(9): here

/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_bwd_kernel.h(1004): error: static assertion failed
          detected during:
            instantiation of "void flash::compute_dq_dk_dv_1colblock<Kernel_traits,Is_dropout,Is_causal,Is_even_MN,Is_even_K,Is_first,Is_last,Seq_parallel,Params>(const Params &, int, int, int) [with Kernel_traits=Flash_bwd_kernel_traits<512, 32, 64, 4, 2, 2, 2, false, true, cutlass::bfloat16_t, Flash_kernel_traits<512, 32, 64, 4, cutlass::bfloat16_t>>, Is_dropout=true, Is_causal=false, Is_even_MN=true, Is_even_K=true, Is_first=false, Is_last=false, Seq_parallel=true, Params=Flash_bwd_params]" 
(1591): here
            instantiation of "void flash::compute_dq_dk_dv_seqk_parallel<Kernel_traits,Is_dropout,Is_causal,Is_even_MN,Is_even_K,Params>(const Params &) [with Kernel_traits=Flash_bwd_kernel_traits<512, 32, 64, 4, 2, 2, 2, false, true, cutlass::bfloat16_t, Flash_kernel_traits<512, 32, 64, 4, cutlass::bfloat16_t>>, Is_dropout=true, Is_causal=false, Is_even_MN=true, Is_even_K=true, Params=Flash_bwd_params]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_bwd_launch_template.h(28): here
            instantiation of "void flash_bwd_dq_dk_dv_loop_seqk_parallel_kernel<Kernel_traits,Is_dropout,Is_causal,Is_even_MN,Is_even_K>(Flash_bwd_params) [with Kernel_traits=Flash_bwd_kernel_traits<512, 32, 64, 4, 2, 2, 2, false, true, cutlass::bfloat16_t, Flash_kernel_traits<512, 32, 64, 4, cutlass::bfloat16_t>>, Is_dropout=true, Is_causal=false, Is_even_MN=true, Is_even_K=true]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_bwd_launch_template.h(62): here
            instantiation of "void run_flash_bwd_seqk_parallel<Kernel_traits,Is_dropout>(Flash_bwd_params &, cudaStream_t, __nv_bool) [with Kernel_traits=Flash_bwd_kernel_traits<512, 32, 64, 4, 2, 2, 2, false, true, cutlass::bfloat16_t, Flash_kernel_traits<512, 32, 64, 4, cutlass::bfloat16_t>>, Is_dropout=true]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_bwd_launch_template.h(134): here
            instantiation of "void run_flash_bwd<Kernel_traits,Is_dropout>(Flash_bwd_params &, cudaStream_t, __nv_bool) [with Kernel_traits=Flash_bwd_kernel_traits<512, 32, 64, 4, 2, 2, 2, false, true, cutlass::bfloat16_t, Flash_kernel_traits<512, 32, 64, 4, cutlass::bfloat16_t>>, Is_dropout=true]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_bwd_launch_template.h(365): here
            instantiation of "void run_mha_bwd_hdim512<T>(Flash_bwd_params &, cudaStream_t, __nv_bool) [with T=cutlass::bfloat16_t]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_bwd_hdim512_bf16_sm80.cu(9): here

/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_bwd_kernel.h(1004): error: static assertion failed
          detected during:
            instantiation of "void flash::compute_dq_dk_dv_1colblock<Kernel_traits,Is_dropout,Is_causal,Is_even_MN,Is_even_K,Is_first,Is_last,Seq_parallel,Params>(const Params &, int, int, int) [with Kernel_traits=Flash_bwd_kernel_traits<512, 32, 64, 4, 2, 2, 2, false, true, cutlass::bfloat16_t, Flash_kernel_traits<512, 32, 64, 4, cutlass::bfloat16_t>>, Is_dropout=true, Is_causal=false, Is_even_MN=true, Is_even_K=false, Is_first=false, Is_last=false, Seq_parallel=true, Params=Flash_bwd_params]" 
(1591): here
            instantiation of "void flash::compute_dq_dk_dv_seqk_parallel<Kernel_traits,Is_dropout,Is_causal,Is_even_MN,Is_even_K,Params>(const Params &) [with Kernel_traits=Flash_bwd_kernel_traits<512, 32, 64, 4, 2, 2, 2, false, true, cutlass::bfloat16_t, Flash_kernel_traits<512, 32, 64, 4, cutlass::bfloat16_t>>, Is_dropout=true, Is_causal=false, Is_even_MN=true, Is_even_K=false, Params=Flash_bwd_params]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_bwd_launch_template.h(28): here
            instantiation of "void flash_bwd_dq_dk_dv_loop_seqk_parallel_kernel<Kernel_traits,Is_dropout,Is_causal,Is_even_MN,Is_even_K>(Flash_bwd_params) [with Kernel_traits=Flash_bwd_kernel_traits<512, 32, 64, 4, 2, 2, 2, false, true, cutlass::bfloat16_t, Flash_kernel_traits<512, 32, 64, 4, cutlass::bfloat16_t>>, Is_dropout=true, Is_causal=false, Is_even_MN=true, Is_even_K=false]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_bwd_launch_template.h(62): here
            instantiation of "void run_flash_bwd_seqk_parallel<Kernel_traits,Is_dropout>(Flash_bwd_params &, cudaStream_t, __nv_bool) [with Kernel_traits=Flash_bwd_kernel_traits<512, 32, 64, 4, 2, 2, 2, false, true, cutlass::bfloat16_t, Flash_kernel_traits<512, 32, 64, 4, cutlass::bfloat16_t>>, Is_dropout=true]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_bwd_launch_template.h(134): here
            instantiation of "void run_flash_bwd<Kernel_traits,Is_dropout>(Flash_bwd_params &, cudaStream_t, __nv_bool) [with Kernel_traits=Flash_bwd_kernel_traits<512, 32, 64, 4, 2, 2, 2, false, true, cutlass::bfloat16_t, Flash_kernel_traits<512, 32, 64, 4, cutlass::bfloat16_t>>, Is_dropout=true]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_bwd_launch_template.h(365): here
            instantiation of "void run_mha_bwd_hdim512<T>(Flash_bwd_params &, cudaStream_t, __nv_bool) [with T=cutlass::bfloat16_t]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_bwd_hdim512_bf16_sm80.cu(9): here

/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_bwd_kernel.h(1004): error: static assertion failed
          detected during:
            instantiation of "void flash::compute_dq_dk_dv_1colblock<Kernel_traits,Is_dropout,Is_causal,Is_even_MN,Is_even_K,Is_first,Is_last,Seq_parallel,Params>(const Params &, int, int, int) [with Kernel_traits=Flash_bwd_kernel_traits<512, 32, 64, 4, 2, 2, 2, false, true, cutlass::bfloat16_t, Flash_kernel_traits<512, 32, 64, 4, cutlass::bfloat16_t>>, Is_dropout=true, Is_causal=false, Is_even_MN=false, Is_even_K=true, Is_first=false, Is_last=false, Seq_parallel=true, Params=Flash_bwd_params]" 
(1591): here
            instantiation of "void flash::compute_dq_dk_dv_seqk_parallel<Kernel_traits,Is_dropout,Is_causal,Is_even_MN,Is_even_K,Params>(const Params &) [with Kernel_traits=Flash_bwd_kernel_traits<512, 32, 64, 4, 2, 2, 2, false, true, cutlass::bfloat16_t, Flash_kernel_traits<512, 32, 64, 4, cutlass::bfloat16_t>>, Is_dropout=true, Is_causal=false, Is_even_MN=false, Is_even_K=true, Params=Flash_bwd_params]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_bwd_launch_template.h(28): here
            instantiation of "void flash_bwd_dq_dk_dv_loop_seqk_parallel_kernel<Kernel_traits,Is_dropout,Is_causal,Is_even_MN,Is_even_K>(Flash_bwd_params) [with Kernel_traits=Flash_bwd_kernel_traits<512, 32, 64, 4, 2, 2, 2, false, true, cutlass::bfloat16_t, Flash_kernel_traits<512, 32, 64, 4, cutlass::bfloat16_t>>, Is_dropout=true, Is_causal=false, Is_even_MN=false, Is_even_K=true]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_bwd_launch_template.h(62): here
            instantiation of "void run_flash_bwd_seqk_parallel<Kernel_traits,Is_dropout>(Flash_bwd_params &, cudaStream_t, __nv_bool) [with Kernel_traits=Flash_bwd_kernel_traits<512, 32, 64, 4, 2, 2, 2, false, true, cutlass::bfloat16_t, Flash_kernel_traits<512, 32, 64, 4, cutlass::bfloat16_t>>, Is_dropout=true]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_bwd_launch_template.h(134): here
            instantiation of "void run_flash_bwd<Kernel_traits,Is_dropout>(Flash_bwd_params &, cudaStream_t, __nv_bool) [with Kernel_traits=Flash_bwd_kernel_traits<512, 32, 64, 4, 2, 2, 2, false, true, cutlass::bfloat16_t, Flash_kernel_traits<512, 32, 64, 4, cutlass::bfloat16_t>>, Is_dropout=true]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_bwd_launch_template.h(365): here
            instantiation of "void run_mha_bwd_hdim512<T>(Flash_bwd_params &, cudaStream_t, __nv_bool) [with T=cutlass::bfloat16_t]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_bwd_hdim512_bf16_sm80.cu(9): here

/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_bwd_kernel.h(1004): error: static assertion failed
          detected during:
            instantiation of "void flash::compute_dq_dk_dv_1colblock<Kernel_traits,Is_dropout,Is_causal,Is_even_MN,Is_even_K,Is_first,Is_last,Seq_parallel,Params>(const Params &, int, int, int) [with Kernel_traits=Flash_bwd_kernel_traits<512, 32, 64, 4, 2, 2, 2, false, true, cutlass::bfloat16_t, Flash_kernel_traits<512, 32, 64, 4, cutlass::bfloat16_t>>, Is_dropout=true, Is_causal=false, Is_even_MN=false, Is_even_K=false, Is_first=false, Is_last=false, Seq_parallel=true, Params=Flash_bwd_params]" 
(1591): here
            instantiation of "void flash::compute_dq_dk_dv_seqk_parallel<Kernel_traits,Is_dropout,Is_causal,Is_even_MN,Is_even_K,Params>(const Params &) [with Kernel_traits=Flash_bwd_kernel_traits<512, 32, 64, 4, 2, 2, 2, false, true, cutlass::bfloat16_t, Flash_kernel_traits<512, 32, 64, 4, cutlass::bfloat16_t>>, Is_dropout=true, Is_causal=false, Is_even_MN=false, Is_even_K=false, Params=Flash_bwd_params]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_bwd_launch_template.h(28): here
            instantiation of "void flash_bwd_dq_dk_dv_loop_seqk_parallel_kernel<Kernel_traits,Is_dropout,Is_causal,Is_even_MN,Is_even_K>(Flash_bwd_params) [with Kernel_traits=Flash_bwd_kernel_traits<512, 32, 64, 4, 2, 2, 2, false, true, cutlass::bfloat16_t, Flash_kernel_traits<512, 32, 64, 4, cutlass::bfloat16_t>>, Is_dropout=true, Is_causal=false, Is_even_MN=false, Is_even_K=false]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_bwd_launch_template.h(62): here
            instantiation of "void run_flash_bwd_seqk_parallel<Kernel_traits,Is_dropout>(Flash_bwd_params &, cudaStream_t, __nv_bool) [with Kernel_traits=Flash_bwd_kernel_traits<512, 32, 64, 4, 2, 2, 2, false, true, cutlass::bfloat16_t, Flash_kernel_traits<512, 32, 64, 4, cutlass::bfloat16_t>>, Is_dropout=true]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_bwd_launch_template.h(134): here
            instantiation of "void run_flash_bwd<Kernel_traits,Is_dropout>(Flash_bwd_params &, cudaStream_t, __nv_bool) [with Kernel_traits=Flash_bwd_kernel_traits<512, 32, 64, 4, 2, 2, 2, false, true, cutlass::bfloat16_t, Flash_kernel_traits<512, 32, 64, 4, cutlass::bfloat16_t>>, Is_dropout=true]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_bwd_launch_template.h(365): here
            instantiation of "void run_mha_bwd_hdim512<T>(Flash_bwd_params &, cudaStream_t, __nv_bool) [with T=cutlass::bfloat16_t]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_bwd_hdim512_bf16_sm80.cu(9): here

/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_bwd_kernel.h(287): error: static assertion failed
          detected during:
            instantiation of "void flash::convert_dQ<Kernel_traits,Params>(const Params &) [with Kernel_traits=Flash_bwd_kernel_traits<512, 32, 64, 4, 2, 2, 2, false, true, cutlass::bfloat16_t, Flash_kernel_traits<512, 32, 64, 4, cutlass::bfloat16_t>>, Params=Flash_bwd_params]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_bwd_launch_template.h(38): here
            instantiation of "void flash_bwd_convert_dq_kernel<Kernel_traits>(Flash_bwd_params) [with Kernel_traits=Flash_bwd_kernel_traits<512, 32, 64, 4, 2, 2, 2, false, true, cutlass::bfloat16_t, Flash_kernel_traits<512, 32, 64, 4, cutlass::bfloat16_t>>]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_bwd_launch_template.h(77): here
            instantiation of "void run_flash_bwd_seqk_parallel<Kernel_traits,Is_dropout>(Flash_bwd_params &, cudaStream_t, __nv_bool) [with Kernel_traits=Flash_bwd_kernel_traits<512, 32, 64, 4, 2, 2, 2, false, true, cutlass::bfloat16_t, Flash_kernel_traits<512, 32, 64, 4, cutlass::bfloat16_t>>, Is_dropout=true]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_bwd_launch_template.h(134): here
            instantiation of "void run_flash_bwd<Kernel_traits,Is_dropout>(Flash_bwd_params &, cudaStream_t, __nv_bool) [with Kernel_traits=Flash_bwd_kernel_traits<512, 32, 64, 4, 2, 2, 2, false, true, cutlass::bfloat16_t, Flash_kernel_traits<512, 32, 64, 4, cutlass::bfloat16_t>>, Is_dropout=true]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_bwd_launch_template.h(365): here
            instantiation of "void run_mha_bwd_hdim512<T>(Flash_bwd_params &, cudaStream_t, __nv_bool) [with T=cutlass::bfloat16_t]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_bwd_hdim512_bf16_sm80.cu(9): here

/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_bwd_kernel.h(1004): error: static assertion failed
          detected during:
            instantiation of "void flash::compute_dq_dk_dv_1colblock<Kernel_traits,Is_dropout,Is_causal,Is_even_MN,Is_even_K,Is_first,Is_last,Seq_parallel,Params>(const Params &, int, int, int) [with Kernel_traits=Flash_bwd_kernel_traits<512, 32, 64, 4, 2, 2, 2, false, false, cutlass::bfloat16_t, Flash_kernel_traits<512, 32, 64, 4, cutlass::bfloat16_t>>, Is_dropout=false, Is_causal=true, Is_even_MN=true, Is_even_K=true, Is_first=false, Is_last=false, Seq_parallel=true, Params=Flash_bwd_params]" 
(1591): here
            instantiation of "void flash::compute_dq_dk_dv_seqk_parallel<Kernel_traits,Is_dropout,Is_causal,Is_even_MN,Is_even_K,Params>(const Params &) [with Kernel_traits=Flash_bwd_kernel_traits<512, 32, 64, 4, 2, 2, 2, false, false, cutlass::bfloat16_t, Flash_kernel_traits<512, 32, 64, 4, cutlass::bfloat16_t>>, Is_dropout=false, Is_causal=true, Is_even_MN=true, Is_even_K=true, Params=Flash_bwd_params]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_bwd_launch_template.h(28): here
            instantiation of "void flash_bwd_dq_dk_dv_loop_seqk_parallel_kernel<Kernel_traits,Is_dropout,Is_causal,Is_even_MN,Is_even_K>(Flash_bwd_params) [with Kernel_traits=Flash_bwd_kernel_traits<512, 32, 64, 4, 2, 2, 2, false, false, cutlass::bfloat16_t, Flash_kernel_traits<512, 32, 64, 4, cutlass::bfloat16_t>>, Is_dropout=false, Is_causal=true, Is_even_MN=true, Is_even_K=true]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_bwd_launch_template.h(62): here
            instantiation of "void run_flash_bwd_seqk_parallel<Kernel_traits,Is_dropout>(Flash_bwd_params &, cudaStream_t, __nv_bool) [with Kernel_traits=Flash_bwd_kernel_traits<512, 32, 64, 4, 2, 2, 2, false, false, cutlass::bfloat16_t, Flash_kernel_traits<512, 32, 64, 4, cutlass::bfloat16_t>>, Is_dropout=false]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_bwd_launch_template.h(134): here
            instantiation of "void run_flash_bwd<Kernel_traits,Is_dropout>(Flash_bwd_params &, cudaStream_t, __nv_bool) [with Kernel_traits=Flash_bwd_kernel_traits<512, 32, 64, 4, 2, 2, 2, false, false, cutlass::bfloat16_t, Flash_kernel_traits<512, 32, 64, 4, cutlass::bfloat16_t>>, Is_dropout=false]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_bwd_launch_template.h(365): here
            instantiation of "void run_mha_bwd_hdim512<T>(Flash_bwd_params &, cudaStream_t, __nv_bool) [with T=cutlass::bfloat16_t]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_bwd_hdim512_bf16_sm80.cu(9): here

/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_bwd_kernel.h(1004): error: static assertion failed
          detected during:
            instantiation of "void flash::compute_dq_dk_dv_1colblock<Kernel_traits,Is_dropout,Is_causal,Is_even_MN,Is_even_K,Is_first,Is_last,Seq_parallel,Params>(const Params &, int, int, int) [with Kernel_traits=Flash_bwd_kernel_traits<512, 32, 64, 4, 2, 2, 2, false, false, cutlass::bfloat16_t, Flash_kernel_traits<512, 32, 64, 4, cutlass::bfloat16_t>>, Is_dropout=false, Is_causal=true, Is_even_MN=true, Is_even_K=false, Is_first=false, Is_last=false, Seq_parallel=true, Params=Flash_bwd_params]" 
(1591): here
            instantiation of "void flash::compute_dq_dk_dv_seqk_parallel<Kernel_traits,Is_dropout,Is_causal,Is_even_MN,Is_even_K,Params>(const Params &) [with Kernel_traits=Flash_bwd_kernel_traits<512, 32, 64, 4, 2, 2, 2, false, false, cutlass::bfloat16_t, Flash_kernel_traits<512, 32, 64, 4, cutlass::bfloat16_t>>, Is_dropout=false, Is_causal=true, Is_even_MN=true, Is_even_K=false, Params=Flash_bwd_params]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_bwd_launch_template.h(28): here
            instantiation of "void flash_bwd_dq_dk_dv_loop_seqk_parallel_kernel<Kernel_traits,Is_dropout,Is_causal,Is_even_MN,Is_even_K>(Flash_bwd_params) [with Kernel_traits=Flash_bwd_kernel_traits<512, 32, 64, 4, 2, 2, 2, false, false, cutlass::bfloat16_t, Flash_kernel_traits<512, 32, 64, 4, cutlass::bfloat16_t>>, Is_dropout=false, Is_causal=true, Is_even_MN=true, Is_even_K=false]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_bwd_launch_template.h(62): here
            instantiation of "void run_flash_bwd_seqk_parallel<Kernel_traits,Is_dropout>(Flash_bwd_params &, cudaStream_t, __nv_bool) [with Kernel_traits=Flash_bwd_kernel_traits<512, 32, 64, 4, 2, 2, 2, false, false, cutlass::bfloat16_t, Flash_kernel_traits<512, 32, 64, 4, cutlass::bfloat16_t>>, Is_dropout=false]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_bwd_launch_template.h(134): here
            instantiation of "void run_flash_bwd<Kernel_traits,Is_dropout>(Flash_bwd_params &, cudaStream_t, __nv_bool) [with Kernel_traits=Flash_bwd_kernel_traits<512, 32, 64, 4, 2, 2, 2, false, false, cutlass::bfloat16_t, Flash_kernel_traits<512, 32, 64, 4, cutlass::bfloat16_t>>, Is_dropout=false]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_bwd_launch_template.h(365): here
            instantiation of "void run_mha_bwd_hdim512<T>(Flash_bwd_params &, cudaStream_t, __nv_bool) [with T=cutlass::bfloat16_t]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_bwd_hdim512_bf16_sm80.cu(9): here

/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_bwd_kernel.h(1004): error: static assertion failed
          detected during:
            instantiation of "void flash::compute_dq_dk_dv_1colblock<Kernel_traits,Is_dropout,Is_causal,Is_even_MN,Is_even_K,Is_first,Is_last,Seq_parallel,Params>(const Params &, int, int, int) [with Kernel_traits=Flash_bwd_kernel_traits<512, 32, 64, 4, 2, 2, 2, false, false, cutlass::bfloat16_t, Flash_kernel_traits<512, 32, 64, 4, cutlass::bfloat16_t>>, Is_dropout=false, Is_causal=true, Is_even_MN=false, Is_even_K=true, Is_first=false, Is_last=false, Seq_parallel=true, Params=Flash_bwd_params]" 
(1591): here
            instantiation of "void flash::compute_dq_dk_dv_seqk_parallel<Kernel_traits,Is_dropout,Is_causal,Is_even_MN,Is_even_K,Params>(const Params &) [with Kernel_traits=Flash_bwd_kernel_traits<512, 32, 64, 4, 2, 2, 2, false, false, cutlass::bfloat16_t, Flash_kernel_traits<512, 32, 64, 4, cutlass::bfloat16_t>>, Is_dropout=false, Is_causal=true, Is_even_MN=false, Is_even_K=true, Params=Flash_bwd_params]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_bwd_launch_template.h(28): here
            instantiation of "void flash_bwd_dq_dk_dv_loop_seqk_parallel_kernel<Kernel_traits,Is_dropout,Is_causal,Is_even_MN,Is_even_K>(Flash_bwd_params) [with Kernel_traits=Flash_bwd_kernel_traits<512, 32, 64, 4, 2, 2, 2, false, false, cutlass::bfloat16_t, Flash_kernel_traits<512, 32, 64, 4, cutlass::bfloat16_t>>, Is_dropout=false, Is_causal=true, Is_even_MN=false, Is_even_K=true]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_bwd_launch_template.h(62): here
            instantiation of "void run_flash_bwd_seqk_parallel<Kernel_traits,Is_dropout>(Flash_bwd_params &, cudaStream_t, __nv_bool) [with Kernel_traits=Flash_bwd_kernel_traits<512, 32, 64, 4, 2, 2, 2, false, false, cutlass::bfloat16_t, Flash_kernel_traits<512, 32, 64, 4, cutlass::bfloat16_t>>, Is_dropout=false]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_bwd_launch_template.h(134): here
            instantiation of "void run_flash_bwd<Kernel_traits,Is_dropout>(Flash_bwd_params &, cudaStream_t, __nv_bool) [with Kernel_traits=Flash_bwd_kernel_traits<512, 32, 64, 4, 2, 2, 2, false, false, cutlass::bfloat16_t, Flash_kernel_traits<512, 32, 64, 4, cutlass::bfloat16_t>>, Is_dropout=false]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_bwd_launch_template.h(365): here
            instantiation of "void run_mha_bwd_hdim512<T>(Flash_bwd_params &, cudaStream_t, __nv_bool) [with T=cutlass::bfloat16_t]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_bwd_hdim512_bf16_sm80.cu(9): here

/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_bwd_kernel.h(1004): error: static assertion failed
          detected during:
            instantiation of "void flash::compute_dq_dk_dv_1colblock<Kernel_traits,Is_dropout,Is_causal,Is_even_MN,Is_even_K,Is_first,Is_last,Seq_parallel,Params>(const Params &, int, int, int) [with Kernel_traits=Flash_bwd_kernel_traits<512, 32, 64, 4, 2, 2, 2, false, false, cutlass::bfloat16_t, Flash_kernel_traits<512, 32, 64, 4, cutlass::bfloat16_t>>, Is_dropout=false, Is_causal=true, Is_even_MN=false, Is_even_K=false, Is_first=false, Is_last=false, Seq_parallel=true, Params=Flash_bwd_params]" 
(1591): here
            instantiation of "void flash::compute_dq_dk_dv_seqk_parallel<Kernel_traits,Is_dropout,Is_causal,Is_even_MN,Is_even_K,Params>(const Params &) [with Kernel_traits=Flash_bwd_kernel_traits<512, 32, 64, 4, 2, 2, 2, false, false, cutlass::bfloat16_t, Flash_kernel_traits<512, 32, 64, 4, cutlass::bfloat16_t>>, Is_dropout=false, Is_causal=true, Is_even_MN=false, Is_even_K=false, Params=Flash_bwd_params]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_bwd_launch_template.h(28): here
            instantiation of "void flash_bwd_dq_dk_dv_loop_seqk_parallel_kernel<Kernel_traits,Is_dropout,Is_causal,Is_even_MN,Is_even_K>(Flash_bwd_params) [with Kernel_traits=Flash_bwd_kernel_traits<512, 32, 64, 4, 2, 2, 2, false, false, cutlass::bfloat16_t, Flash_kernel_traits<512, 32, 64, 4, cutlass::bfloat16_t>>, Is_dropout=false, Is_causal=true, Is_even_MN=false, Is_even_K=false]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_bwd_launch_template.h(62): here
            instantiation of "void run_flash_bwd_seqk_parallel<Kernel_traits,Is_dropout>(Flash_bwd_params &, cudaStream_t, __nv_bool) [with Kernel_traits=Flash_bwd_kernel_traits<512, 32, 64, 4, 2, 2, 2, false, false, cutlass::bfloat16_t, Flash_kernel_traits<512, 32, 64, 4, cutlass::bfloat16_t>>, Is_dropout=false]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_bwd_launch_template.h(134): here
            instantiation of "void run_flash_bwd<Kernel_traits,Is_dropout>(Flash_bwd_params &, cudaStream_t, __nv_bool) [with Kernel_traits=Flash_bwd_kernel_traits<512, 32, 64, 4, 2, 2, 2, false, false, cutlass::bfloat16_t, Flash_kernel_traits<512, 32, 64, 4, cutlass::bfloat16_t>>, Is_dropout=false]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_bwd_launch_template.h(365): here
            instantiation of "void run_mha_bwd_hdim512<T>(Flash_bwd_params &, cudaStream_t, __nv_bool) [with T=cutlass::bfloat16_t]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_bwd_hdim512_bf16_sm80.cu(9): here

/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_bwd_kernel.h(1004): error: static assertion failed
          detected during:
            instantiation of "void flash::compute_dq_dk_dv_1colblock<Kernel_traits,Is_dropout,Is_causal,Is_even_MN,Is_even_K,Is_first,Is_last,Seq_parallel,Params>(const Params &, int, int, int) [with Kernel_traits=Flash_bwd_kernel_traits<512, 32, 64, 4, 2, 2, 2, false, false, cutlass::bfloat16_t, Flash_kernel_traits<512, 32, 64, 4, cutlass::bfloat16_t>>, Is_dropout=false, Is_causal=false, Is_even_MN=true, Is_even_K=true, Is_first=false, Is_last=false, Seq_parallel=true, Params=Flash_bwd_params]" 
(1591): here
            instantiation of "void flash::compute_dq_dk_dv_seqk_parallel<Kernel_traits,Is_dropout,Is_causal,Is_even_MN,Is_even_K,Params>(const Params &) [with Kernel_traits=Flash_bwd_kernel_traits<512, 32, 64, 4, 2, 2, 2, false, false, cutlass::bfloat16_t, Flash_kernel_traits<512, 32, 64, 4, cutlass::bfloat16_t>>, Is_dropout=false, Is_causal=false, Is_even_MN=true, Is_even_K=true, Params=Flash_bwd_params]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_bwd_launch_template.h(28): here
            instantiation of "void flash_bwd_dq_dk_dv_loop_seqk_parallel_kernel<Kernel_traits,Is_dropout,Is_causal,Is_even_MN,Is_even_K>(Flash_bwd_params) [with Kernel_traits=Flash_bwd_kernel_traits<512, 32, 64, 4, 2, 2, 2, false, false, cutlass::bfloat16_t, Flash_kernel_traits<512, 32, 64, 4, cutlass::bfloat16_t>>, Is_dropout=false, Is_causal=false, Is_even_MN=true, Is_even_K=true]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_bwd_launch_template.h(62): here
            instantiation of "void run_flash_bwd_seqk_parallel<Kernel_traits,Is_dropout>(Flash_bwd_params &, cudaStream_t, __nv_bool) [with Kernel_traits=Flash_bwd_kernel_traits<512, 32, 64, 4, 2, 2, 2, false, false, cutlass::bfloat16_t, Flash_kernel_traits<512, 32, 64, 4, cutlass::bfloat16_t>>, Is_dropout=false]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_bwd_launch_template.h(134): here
            instantiation of "void run_flash_bwd<Kernel_traits,Is_dropout>(Flash_bwd_params &, cudaStream_t, __nv_bool) [with Kernel_traits=Flash_bwd_kernel_traits<512, 32, 64, 4, 2, 2, 2, false, false, cutlass::bfloat16_t, Flash_kernel_traits<512, 32, 64, 4, cutlass::bfloat16_t>>, Is_dropout=false]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_bwd_launch_template.h(365): here
            instantiation of "void run_mha_bwd_hdim512<T>(Flash_bwd_params &, cudaStream_t, __nv_bool) [with T=cutlass::bfloat16_t]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_bwd_hdim512_bf16_sm80.cu(9): here

/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_bwd_kernel.h(1004): error: static assertion failed
          detected during:
            instantiation of "void flash::compute_dq_dk_dv_1colblock<Kernel_traits,Is_dropout,Is_causal,Is_even_MN,Is_even_K,Is_first,Is_last,Seq_parallel,Params>(const Params &, int, int, int) [with Kernel_traits=Flash_bwd_kernel_traits<512, 32, 64, 4, 2, 2, 2, false, false, cutlass::bfloat16_t, Flash_kernel_traits<512, 32, 64, 4, cutlass::bfloat16_t>>, Is_dropout=false, Is_causal=false, Is_even_MN=true, Is_even_K=false, Is_first=false, Is_last=false, Seq_parallel=true, Params=Flash_bwd_params]" 
(1591): here
            instantiation of "void flash::compute_dq_dk_dv_seqk_parallel<Kernel_traits,Is_dropout,Is_causal,Is_even_MN,Is_even_K,Params>(const Params &) [with Kernel_traits=Flash_bwd_kernel_traits<512, 32, 64, 4, 2, 2, 2, false, false, cutlass::bfloat16_t, Flash_kernel_traits<512, 32, 64, 4, cutlass::bfloat16_t>>, Is_dropout=false, Is_causal=false, Is_even_MN=true, Is_even_K=false, Params=Flash_bwd_params]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_bwd_launch_template.h(28): here
            instantiation of "void flash_bwd_dq_dk_dv_loop_seqk_parallel_kernel<Kernel_traits,Is_dropout,Is_causal,Is_even_MN,Is_even_K>(Flash_bwd_params) [with Kernel_traits=Flash_bwd_kernel_traits<512, 32, 64, 4, 2, 2, 2, false, false, cutlass::bfloat16_t, Flash_kernel_traits<512, 32, 64, 4, cutlass::bfloat16_t>>, Is_dropout=false, Is_causal=false, Is_even_MN=true, Is_even_K=false]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_bwd_launch_template.h(62): here
            instantiation of "void run_flash_bwd_seqk_parallel<Kernel_traits,Is_dropout>(Flash_bwd_params &, cudaStream_t, __nv_bool) [with Kernel_traits=Flash_bwd_kernel_traits<512, 32, 64, 4, 2, 2, 2, false, false, cutlass::bfloat16_t, Flash_kernel_traits<512, 32, 64, 4, cutlass::bfloat16_t>>, Is_dropout=false]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_bwd_launch_template.h(134): here
            instantiation of "void run_flash_bwd<Kernel_traits,Is_dropout>(Flash_bwd_params &, cudaStream_t, __nv_bool) [with Kernel_traits=Flash_bwd_kernel_traits<512, 32, 64, 4, 2, 2, 2, false, false, cutlass::bfloat16_t, Flash_kernel_traits<512, 32, 64, 4, cutlass::bfloat16_t>>, Is_dropout=false]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_bwd_launch_template.h(365): here
            instantiation of "void run_mha_bwd_hdim512<T>(Flash_bwd_params &, cudaStream_t, __nv_bool) [with T=cutlass::bfloat16_t]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_bwd_hdim512_bf16_sm80.cu(9): here

/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_bwd_kernel.h(1004): error: static assertion failed
          detected during:
            instantiation of "void flash::compute_dq_dk_dv_1colblock<Kernel_traits,Is_dropout,Is_causal,Is_even_MN,Is_even_K,Is_first,Is_last,Seq_parallel,Params>(const Params &, int, int, int) [with Kernel_traits=Flash_bwd_kernel_traits<512, 32, 64, 4, 2, 2, 2, false, false, cutlass::bfloat16_t, Flash_kernel_traits<512, 32, 64, 4, cutlass::bfloat16_t>>, Is_dropout=false, Is_causal=false, Is_even_MN=false, Is_even_K=true, Is_first=false, Is_last=false, Seq_parallel=true, Params=Flash_bwd_params]" 
(1591): here
            instantiation of "void flash::compute_dq_dk_dv_seqk_parallel<Kernel_traits,Is_dropout,Is_causal,Is_even_MN,Is_even_K,Params>(const Params &) [with Kernel_traits=Flash_bwd_kernel_traits<512, 32, 64, 4, 2, 2, 2, false, false, cutlass::bfloat16_t, Flash_kernel_traits<512, 32, 64, 4, cutlass::bfloat16_t>>, Is_dropout=false, Is_causal=false, Is_even_MN=false, Is_even_K=true, Params=Flash_bwd_params]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_bwd_launch_template.h(28): here
            instantiation of "void flash_bwd_dq_dk_dv_loop_seqk_parallel_kernel<Kernel_traits,Is_dropout,Is_causal,Is_even_MN,Is_even_K>(Flash_bwd_params) [with Kernel_traits=Flash_bwd_kernel_traits<512, 32, 64, 4, 2, 2, 2, false, false, cutlass::bfloat16_t, Flash_kernel_traits<512, 32, 64, 4, cutlass::bfloat16_t>>, Is_dropout=false, Is_causal=false, Is_even_MN=false, Is_even_K=true]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_bwd_launch_template.h(62): here
            instantiation of "void run_flash_bwd_seqk_parallel<Kernel_traits,Is_dropout>(Flash_bwd_params &, cudaStream_t, __nv_bool) [with Kernel_traits=Flash_bwd_kernel_traits<512, 32, 64, 4, 2, 2, 2, false, false, cutlass::bfloat16_t, Flash_kernel_traits<512, 32, 64, 4, cutlass::bfloat16_t>>, Is_dropout=false]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_bwd_launch_template.h(134): here
            instantiation of "void run_flash_bwd<Kernel_traits,Is_dropout>(Flash_bwd_params &, cudaStream_t, __nv_bool) [with Kernel_traits=Flash_bwd_kernel_traits<512, 32, 64, 4, 2, 2, 2, false, false, cutlass::bfloat16_t, Flash_kernel_traits<512, 32, 64, 4, cutlass::bfloat16_t>>, Is_dropout=false]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_bwd_launch_template.h(365): here
            instantiation of "void run_mha_bwd_hdim512<T>(Flash_bwd_params &, cudaStream_t, __nv_bool) [with T=cutlass::bfloat16_t]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_bwd_hdim512_bf16_sm80.cu(9): here

/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_bwd_kernel.h(1004): error: static assertion failed
          detected during:
            instantiation of "void flash::compute_dq_dk_dv_1colblock<Kernel_traits,Is_dropout,Is_causal,Is_even_MN,Is_even_K,Is_first,Is_last,Seq_parallel,Params>(const Params &, int, int, int) [with Kernel_traits=Flash_bwd_kernel_traits<512, 32, 64, 4, 2, 2, 2, false, false, cutlass::bfloat16_t, Flash_kernel_traits<512, 32, 64, 4, cutlass::bfloat16_t>>, Is_dropout=false, Is_causal=false, Is_even_MN=false, Is_even_K=false, Is_first=false, Is_last=false, Seq_parallel=true, Params=Flash_bwd_params]" 
(1591): here
            instantiation of "void flash::compute_dq_dk_dv_seqk_parallel<Kernel_traits,Is_dropout,Is_causal,Is_even_MN,Is_even_K,Params>(const Params &) [with Kernel_traits=Flash_bwd_kernel_traits<512, 32, 64, 4, 2, 2, 2, false, false, cutlass::bfloat16_t, Flash_kernel_traits<512, 32, 64, 4, cutlass::bfloat16_t>>, Is_dropout=false, Is_causal=false, Is_even_MN=false, Is_even_K=false, Params=Flash_bwd_params]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_bwd_launch_template.h(28): here
            instantiation of "void flash_bwd_dq_dk_dv_loop_seqk_parallel_kernel<Kernel_traits,Is_dropout,Is_causal,Is_even_MN,Is_even_K>(Flash_bwd_params) [with Kernel_traits=Flash_bwd_kernel_traits<512, 32, 64, 4, 2, 2, 2, false, false, cutlass::bfloat16_t, Flash_kernel_traits<512, 32, 64, 4, cutlass::bfloat16_t>>, Is_dropout=false, Is_causal=false, Is_even_MN=false, Is_even_K=false]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_bwd_launch_template.h(62): here
            instantiation of "void run_flash_bwd_seqk_parallel<Kernel_traits,Is_dropout>(Flash_bwd_params &, cudaStream_t, __nv_bool) [with Kernel_traits=Flash_bwd_kernel_traits<512, 32, 64, 4, 2, 2, 2, false, false, cutlass::bfloat16_t, Flash_kernel_traits<512, 32, 64, 4, cutlass::bfloat16_t>>, Is_dropout=false]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_bwd_launch_template.h(134): here
            instantiation of "void run_flash_bwd<Kernel_traits,Is_dropout>(Flash_bwd_params &, cudaStream_t, __nv_bool) [with Kernel_traits=Flash_bwd_kernel_traits<512, 32, 64, 4, 2, 2, 2, false, false, cutlass::bfloat16_t, Flash_kernel_traits<512, 32, 64, 4, cutlass::bfloat16_t>>, Is_dropout=false]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_bwd_launch_template.h(365): here
            instantiation of "void run_mha_bwd_hdim512<T>(Flash_bwd_params &, cudaStream_t, __nv_bool) [with T=cutlass::bfloat16_t]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_bwd_hdim512_bf16_sm80.cu(9): here

/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_bwd_kernel.h(1004): error: static assertion failed
          detected during:
            instantiation of "void flash::compute_dq_dk_dv_1colblock<Kernel_traits,Is_dropout,Is_causal,Is_even_MN,Is_even_K,Is_first,Is_last,Seq_parallel,Params>(const Params &, int, int, int) [with Kernel_traits=Flash_bwd_kernel_traits<512, 32, 64, 4, 2, 2, 2, false, true, cutlass::bfloat16_t, Flash_kernel_traits<512, 32, 64, 4, cutlass::bfloat16_t>>, Is_dropout=false, Is_causal=true, Is_even_MN=true, Is_even_K=true, Is_first=false, Is_last=false, Seq_parallel=true, Params=Flash_bwd_params]" 
(1591): here
            instantiation of "void flash::compute_dq_dk_dv_seqk_parallel<Kernel_traits,Is_dropout,Is_causal,Is_even_MN,Is_even_K,Params>(const Params &) [with Kernel_traits=Flash_bwd_kernel_traits<512, 32, 64, 4, 2, 2, 2, false, true, cutlass::bfloat16_t, Flash_kernel_traits<512, 32, 64, 4, cutlass::bfloat16_t>>, Is_dropout=false, Is_causal=true, Is_even_MN=true, Is_even_K=true, Params=Flash_bwd_params]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_bwd_launch_template.h(28): here
            instantiation of "void flash_bwd_dq_dk_dv_loop_seqk_parallel_kernel<Kernel_traits,Is_dropout,Is_causal,Is_even_MN,Is_even_K>(Flash_bwd_params) [with Kernel_traits=Flash_bwd_kernel_traits<512, 32, 64, 4, 2, 2, 2, false, true, cutlass::bfloat16_t, Flash_kernel_traits<512, 32, 64, 4, cutlass::bfloat16_t>>, Is_dropout=false, Is_causal=true, Is_even_MN=true, Is_even_K=true]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_bwd_launch_template.h(62): here
            instantiation of "void run_flash_bwd_seqk_parallel<Kernel_traits,Is_dropout>(Flash_bwd_params &, cudaStream_t, __nv_bool) [with Kernel_traits=Flash_bwd_kernel_traits<512, 32, 64, 4, 2, 2, 2, false, true, cutlass::bfloat16_t, Flash_kernel_traits<512, 32, 64, 4, cutlass::bfloat16_t>>, Is_dropout=false]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_bwd_launch_template.h(134): here
            instantiation of "void run_flash_bwd<Kernel_traits,Is_dropout>(Flash_bwd_params &, cudaStream_t, __nv_bool) [with Kernel_traits=Flash_bwd_kernel_traits<512, 32, 64, 4, 2, 2, 2, false, true, cutlass::bfloat16_t, Flash_kernel_traits<512, 32, 64, 4, cutlass::bfloat16_t>>, Is_dropout=false]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_bwd_launch_template.h(365): here
            instantiation of "void run_mha_bwd_hdim512<T>(Flash_bwd_params &, cudaStream_t, __nv_bool) [with T=cutlass::bfloat16_t]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_bwd_hdim512_bf16_sm80.cu(9): here

/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_bwd_kernel.h(1004): error: static assertion failed
          detected during:
            instantiation of "void flash::compute_dq_dk_dv_1colblock<Kernel_traits,Is_dropout,Is_causal,Is_even_MN,Is_even_K,Is_first,Is_last,Seq_parallel,Params>(const Params &, int, int, int) [with Kernel_traits=Flash_bwd_kernel_traits<512, 32, 64, 4, 2, 2, 2, false, true, cutlass::bfloat16_t, Flash_kernel_traits<512, 32, 64, 4, cutlass::bfloat16_t>>, Is_dropout=false, Is_causal=true, Is_even_MN=true, Is_even_K=false, Is_first=false, Is_last=false, Seq_parallel=true, Params=Flash_bwd_params]" 
(1591): here
            instantiation of "void flash::compute_dq_dk_dv_seqk_parallel<Kernel_traits,Is_dropout,Is_causal,Is_even_MN,Is_even_K,Params>(const Params &) [with Kernel_traits=Flash_bwd_kernel_traits<512, 32, 64, 4, 2, 2, 2, false, true, cutlass::bfloat16_t, Flash_kernel_traits<512, 32, 64, 4, cutlass::bfloat16_t>>, Is_dropout=false, Is_causal=true, Is_even_MN=true, Is_even_K=false, Params=Flash_bwd_params]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_bwd_launch_template.h(28): here
            instantiation of "void flash_bwd_dq_dk_dv_loop_seqk_parallel_kernel<Kernel_traits,Is_dropout,Is_causal,Is_even_MN,Is_even_K>(Flash_bwd_params) [with Kernel_traits=Flash_bwd_kernel_traits<512, 32, 64, 4, 2, 2, 2, false, true, cutlass::bfloat16_t, Flash_kernel_traits<512, 32, 64, 4, cutlass::bfloat16_t>>, Is_dropout=false, Is_causal=true, Is_even_MN=true, Is_even_K=false]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_bwd_launch_template.h(62): here
            instantiation of "void run_flash_bwd_seqk_parallel<Kernel_traits,Is_dropout>(Flash_bwd_params &, cudaStream_t, __nv_bool) [with Kernel_traits=Flash_bwd_kernel_traits<512, 32, 64, 4, 2, 2, 2, false, true, cutlass::bfloat16_t, Flash_kernel_traits<512, 32, 64, 4, cutlass::bfloat16_t>>, Is_dropout=false]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_bwd_launch_template.h(134): here
            instantiation of "void run_flash_bwd<Kernel_traits,Is_dropout>(Flash_bwd_params &, cudaStream_t, __nv_bool) [with Kernel_traits=Flash_bwd_kernel_traits<512, 32, 64, 4, 2, 2, 2, false, true, cutlass::bfloat16_t, Flash_kernel_traits<512, 32, 64, 4, cutlass::bfloat16_t>>, Is_dropout=false]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_bwd_launch_template.h(365): here
            instantiation of "void run_mha_bwd_hdim512<T>(Flash_bwd_params &, cudaStream_t, __nv_bool) [with T=cutlass::bfloat16_t]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_bwd_hdim512_bf16_sm80.cu(9): here

/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_bwd_kernel.h(1004): error: static assertion failed
          detected during:
            instantiation of "void flash::compute_dq_dk_dv_1colblock<Kernel_traits,Is_dropout,Is_causal,Is_even_MN,Is_even_K,Is_first,Is_last,Seq_parallel,Params>(const Params &, int, int, int) [with Kernel_traits=Flash_bwd_kernel_traits<512, 32, 64, 4, 2, 2, 2, false, true, cutlass::bfloat16_t, Flash_kernel_traits<512, 32, 64, 4, cutlass::bfloat16_t>>, Is_dropout=false, Is_causal=true, Is_even_MN=false, Is_even_K=true, Is_first=false, Is_last=false, Seq_parallel=true, Params=Flash_bwd_params]" 
(1591): here
            instantiation of "void flash::compute_dq_dk_dv_seqk_parallel<Kernel_traits,Is_dropout,Is_causal,Is_even_MN,Is_even_K,Params>(const Params &) [with Kernel_traits=Flash_bwd_kernel_traits<512, 32, 64, 4, 2, 2, 2, false, true, cutlass::bfloat16_t, Flash_kernel_traits<512, 32, 64, 4, cutlass::bfloat16_t>>, Is_dropout=false, Is_causal=true, Is_even_MN=false, Is_even_K=true, Params=Flash_bwd_params]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_bwd_launch_template.h(28): here
            instantiation of "void flash_bwd_dq_dk_dv_loop_seqk_parallel_kernel<Kernel_traits,Is_dropout,Is_causal,Is_even_MN,Is_even_K>(Flash_bwd_params) [with Kernel_traits=Flash_bwd_kernel_traits<512, 32, 64, 4, 2, 2, 2, false, true, cutlass::bfloat16_t, Flash_kernel_traits<512, 32, 64, 4, cutlass::bfloat16_t>>, Is_dropout=false, Is_causal=true, Is_even_MN=false, Is_even_K=true]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_bwd_launch_template.h(62): here
            instantiation of "void run_flash_bwd_seqk_parallel<Kernel_traits,Is_dropout>(Flash_bwd_params &, cudaStream_t, __nv_bool) [with Kernel_traits=Flash_bwd_kernel_traits<512, 32, 64, 4, 2, 2, 2, false, true, cutlass::bfloat16_t, Flash_kernel_traits<512, 32, 64, 4, cutlass::bfloat16_t>>, Is_dropout=false]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_bwd_launch_template.h(134): here
            instantiation of "void run_flash_bwd<Kernel_traits,Is_dropout>(Flash_bwd_params &, cudaStream_t, __nv_bool) [with Kernel_traits=Flash_bwd_kernel_traits<512, 32, 64, 4, 2, 2, 2, false, true, cutlass::bfloat16_t, Flash_kernel_traits<512, 32, 64, 4, cutlass::bfloat16_t>>, Is_dropout=false]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_bwd_launch_template.h(365): here
            instantiation of "void run_mha_bwd_hdim512<T>(Flash_bwd_params &, cudaStream_t, __nv_bool) [with T=cutlass::bfloat16_t]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_bwd_hdim512_bf16_sm80.cu(9): here

/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_bwd_kernel.h(1004): error: static assertion failed
          detected during:
            instantiation of "void flash::compute_dq_dk_dv_1colblock<Kernel_traits,Is_dropout,Is_causal,Is_even_MN,Is_even_K,Is_first,Is_last,Seq_parallel,Params>(const Params &, int, int, int) [with Kernel_traits=Flash_bwd_kernel_traits<512, 32, 64, 4, 2, 2, 2, false, true, cutlass::bfloat16_t, Flash_kernel_traits<512, 32, 64, 4, cutlass::bfloat16_t>>, Is_dropout=false, Is_causal=true, Is_even_MN=false, Is_even_K=false, Is_first=false, Is_last=false, Seq_parallel=true, Params=Flash_bwd_params]" 
(1591): here
            instantiation of "void flash::compute_dq_dk_dv_seqk_parallel<Kernel_traits,Is_dropout,Is_causal,Is_even_MN,Is_even_K,Params>(const Params &) [with Kernel_traits=Flash_bwd_kernel_traits<512, 32, 64, 4, 2, 2, 2, false, true, cutlass::bfloat16_t, Flash_kernel_traits<512, 32, 64, 4, cutlass::bfloat16_t>>, Is_dropout=false, Is_causal=true, Is_even_MN=false, Is_even_K=false, Params=Flash_bwd_params]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_bwd_launch_template.h(28): here
            instantiation of "void flash_bwd_dq_dk_dv_loop_seqk_parallel_kernel<Kernel_traits,Is_dropout,Is_causal,Is_even_MN,Is_even_K>(Flash_bwd_params) [with Kernel_traits=Flash_bwd_kernel_traits<512, 32, 64, 4, 2, 2, 2, false, true, cutlass::bfloat16_t, Flash_kernel_traits<512, 32, 64, 4, cutlass::bfloat16_t>>, Is_dropout=false, Is_causal=true, Is_even_MN=false, Is_even_K=false]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_bwd_launch_template.h(62): here
            instantiation of "void run_flash_bwd_seqk_parallel<Kernel_traits,Is_dropout>(Flash_bwd_params &, cudaStream_t, __nv_bool) [with Kernel_traits=Flash_bwd_kernel_traits<512, 32, 64, 4, 2, 2, 2, false, true, cutlass::bfloat16_t, Flash_kernel_traits<512, 32, 64, 4, cutlass::bfloat16_t>>, Is_dropout=false]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_bwd_launch_template.h(134): here
            instantiation of "void run_flash_bwd<Kernel_traits,Is_dropout>(Flash_bwd_params &, cudaStream_t, __nv_bool) [with Kernel_traits=Flash_bwd_kernel_traits<512, 32, 64, 4, 2, 2, 2, false, true, cutlass::bfloat16_t, Flash_kernel_traits<512, 32, 64, 4, cutlass::bfloat16_t>>, Is_dropout=false]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_bwd_launch_template.h(365): here
            instantiation of "void run_mha_bwd_hdim512<T>(Flash_bwd_params &, cudaStream_t, __nv_bool) [with T=cutlass::bfloat16_t]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_bwd_hdim512_bf16_sm80.cu(9): here

/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_bwd_kernel.h(1004): error: static assertion failed
          detected during:
            instantiation of "void flash::compute_dq_dk_dv_1colblock<Kernel_traits,Is_dropout,Is_causal,Is_even_MN,Is_even_K,Is_first,Is_last,Seq_parallel,Params>(const Params &, int, int, int) [with Kernel_traits=Flash_bwd_kernel_traits<512, 32, 64, 4, 2, 2, 2, false, true, cutlass::bfloat16_t, Flash_kernel_traits<512, 32, 64, 4, cutlass::bfloat16_t>>, Is_dropout=false, Is_causal=false, Is_even_MN=true, Is_even_K=true, Is_first=false, Is_last=false, Seq_parallel=true, Params=Flash_bwd_params]" 
(1591): here
            instantiation of "void flash::compute_dq_dk_dv_seqk_parallel<Kernel_traits,Is_dropout,Is_causal,Is_even_MN,Is_even_K,Params>(const Params &) [with Kernel_traits=Flash_bwd_kernel_traits<512, 32, 64, 4, 2, 2, 2, false, true, cutlass::bfloat16_t, Flash_kernel_traits<512, 32, 64, 4, cutlass::bfloat16_t>>, Is_dropout=false, Is_causal=false, Is_even_MN=true, Is_even_K=true, Params=Flash_bwd_params]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_bwd_launch_template.h(28): here
            instantiation of "void flash_bwd_dq_dk_dv_loop_seqk_parallel_kernel<Kernel_traits,Is_dropout,Is_causal,Is_even_MN,Is_even_K>(Flash_bwd_params) [with Kernel_traits=Flash_bwd_kernel_traits<512, 32, 64, 4, 2, 2, 2, false, true, cutlass::bfloat16_t, Flash_kernel_traits<512, 32, 64, 4, cutlass::bfloat16_t>>, Is_dropout=false, Is_causal=false, Is_even_MN=true, Is_even_K=true]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_bwd_launch_template.h(62): here
            instantiation of "void run_flash_bwd_seqk_parallel<Kernel_traits,Is_dropout>(Flash_bwd_params &, cudaStream_t, __nv_bool) [with Kernel_traits=Flash_bwd_kernel_traits<512, 32, 64, 4, 2, 2, 2, false, true, cutlass::bfloat16_t, Flash_kernel_traits<512, 32, 64, 4, cutlass::bfloat16_t>>, Is_dropout=false]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_bwd_launch_template.h(134): here
            instantiation of "void run_flash_bwd<Kernel_traits,Is_dropout>(Flash_bwd_params &, cudaStream_t, __nv_bool) [with Kernel_traits=Flash_bwd_kernel_traits<512, 32, 64, 4, 2, 2, 2, false, true, cutlass::bfloat16_t, Flash_kernel_traits<512, 32, 64, 4, cutlass::bfloat16_t>>, Is_dropout=false]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_bwd_launch_template.h(365): here
            instantiation of "void run_mha_bwd_hdim512<T>(Flash_bwd_params &, cudaStream_t, __nv_bool) [with T=cutlass::bfloat16_t]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_bwd_hdim512_bf16_sm80.cu(9): here

/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_bwd_kernel.h(1004): error: static assertion failed
          detected during:
            instantiation of "void flash::compute_dq_dk_dv_1colblock<Kernel_traits,Is_dropout,Is_causal,Is_even_MN,Is_even_K,Is_first,Is_last,Seq_parallel,Params>(const Params &, int, int, int) [with Kernel_traits=Flash_bwd_kernel_traits<512, 32, 64, 4, 2, 2, 2, false, true, cutlass::bfloat16_t, Flash_kernel_traits<512, 32, 64, 4, cutlass::bfloat16_t>>, Is_dropout=false, Is_causal=false, Is_even_MN=true, Is_even_K=false, Is_first=false, Is_last=false, Seq_parallel=true, Params=Flash_bwd_params]" 
(1591): here
            instantiation of "void flash::compute_dq_dk_dv_seqk_parallel<Kernel_traits,Is_dropout,Is_causal,Is_even_MN,Is_even_K,Params>(const Params &) [with Kernel_traits=Flash_bwd_kernel_traits<512, 32, 64, 4, 2, 2, 2, false, true, cutlass::bfloat16_t, Flash_kernel_traits<512, 32, 64, 4, cutlass::bfloat16_t>>, Is_dropout=false, Is_causal=false, Is_even_MN=true, Is_even_K=false, Params=Flash_bwd_params]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_bwd_launch_template.h(28): here
            instantiation of "void flash_bwd_dq_dk_dv_loop_seqk_parallel_kernel<Kernel_traits,Is_dropout,Is_causal,Is_even_MN,Is_even_K>(Flash_bwd_params) [with Kernel_traits=Flash_bwd_kernel_traits<512, 32, 64, 4, 2, 2, 2, false, true, cutlass::bfloat16_t, Flash_kernel_traits<512, 32, 64, 4, cutlass::bfloat16_t>>, Is_dropout=false, Is_causal=false, Is_even_MN=true, Is_even_K=false]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_bwd_launch_template.h(62): here
            instantiation of "void run_flash_bwd_seqk_parallel<Kernel_traits,Is_dropout>(Flash_bwd_params &, cudaStream_t, __nv_bool) [with Kernel_traits=Flash_bwd_kernel_traits<512, 32, 64, 4, 2, 2, 2, false, true, cutlass::bfloat16_t, Flash_kernel_traits<512, 32, 64, 4, cutlass::bfloat16_t>>, Is_dropout=false]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_bwd_launch_template.h(134): here
            instantiation of "void run_flash_bwd<Kernel_traits,Is_dropout>(Flash_bwd_params &, cudaStream_t, __nv_bool) [with Kernel_traits=Flash_bwd_kernel_traits<512, 32, 64, 4, 2, 2, 2, false, true, cutlass::bfloat16_t, Flash_kernel_traits<512, 32, 64, 4, cutlass::bfloat16_t>>, Is_dropout=false]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_bwd_launch_template.h(365): here
            instantiation of "void run_mha_bwd_hdim512<T>(Flash_bwd_params &, cudaStream_t, __nv_bool) [with T=cutlass::bfloat16_t]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_bwd_hdim512_bf16_sm80.cu(9): here

/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_bwd_kernel.h(1004): error: static assertion failed
          detected during:
            instantiation of "void flash::compute_dq_dk_dv_1colblock<Kernel_traits,Is_dropout,Is_causal,Is_even_MN,Is_even_K,Is_first,Is_last,Seq_parallel,Params>(const Params &, int, int, int) [with Kernel_traits=Flash_bwd_kernel_traits<512, 32, 64, 4, 2, 2, 2, false, true, cutlass::bfloat16_t, Flash_kernel_traits<512, 32, 64, 4, cutlass::bfloat16_t>>, Is_dropout=false, Is_causal=false, Is_even_MN=false, Is_even_K=true, Is_first=false, Is_last=false, Seq_parallel=true, Params=Flash_bwd_params]" 
(1591): here
            instantiation of "void flash::compute_dq_dk_dv_seqk_parallel<Kernel_traits,Is_dropout,Is_causal,Is_even_MN,Is_even_K,Params>(const Params &) [with Kernel_traits=Flash_bwd_kernel_traits<512, 32, 64, 4, 2, 2, 2, false, true, cutlass::bfloat16_t, Flash_kernel_traits<512, 32, 64, 4, cutlass::bfloat16_t>>, Is_dropout=false, Is_causal=false, Is_even_MN=false, Is_even_K=true, Params=Flash_bwd_params]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_bwd_launch_template.h(28): here
            instantiation of "void flash_bwd_dq_dk_dv_loop_seqk_parallel_kernel<Kernel_traits,Is_dropout,Is_causal,Is_even_MN,Is_even_K>(Flash_bwd_params) [with Kernel_traits=Flash_bwd_kernel_traits<512, 32, 64, 4, 2, 2, 2, false, true, cutlass::bfloat16_t, Flash_kernel_traits<512, 32, 64, 4, cutlass::bfloat16_t>>, Is_dropout=false, Is_causal=false, Is_even_MN=false, Is_even_K=true]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_bwd_launch_template.h(62): here
            instantiation of "void run_flash_bwd_seqk_parallel<Kernel_traits,Is_dropout>(Flash_bwd_params &, cudaStream_t, __nv_bool) [with Kernel_traits=Flash_bwd_kernel_traits<512, 32, 64, 4, 2, 2, 2, false, true, cutlass::bfloat16_t, Flash_kernel_traits<512, 32, 64, 4, cutlass::bfloat16_t>>, Is_dropout=false]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_bwd_launch_template.h(134): here
            instantiation of "void run_flash_bwd<Kernel_traits,Is_dropout>(Flash_bwd_params &, cudaStream_t, __nv_bool) [with Kernel_traits=Flash_bwd_kernel_traits<512, 32, 64, 4, 2, 2, 2, false, true, cutlass::bfloat16_t, Flash_kernel_traits<512, 32, 64, 4, cutlass::bfloat16_t>>, Is_dropout=false]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_bwd_launch_template.h(365): here
            instantiation of "void run_mha_bwd_hdim512<T>(Flash_bwd_params &, cudaStream_t, __nv_bool) [with T=cutlass::bfloat16_t]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_bwd_hdim512_bf16_sm80.cu(9): here

/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_bwd_kernel.h(1004): error: static assertion failed
          detected during:
            instantiation of "void flash::compute_dq_dk_dv_1colblock<Kernel_traits,Is_dropout,Is_causal,Is_even_MN,Is_even_K,Is_first,Is_last,Seq_parallel,Params>(const Params &, int, int, int) [with Kernel_traits=Flash_bwd_kernel_traits<512, 32, 64, 4, 2, 2, 2, false, true, cutlass::bfloat16_t, Flash_kernel_traits<512, 32, 64, 4, cutlass::bfloat16_t>>, Is_dropout=false, Is_causal=false, Is_even_MN=false, Is_even_K=false, Is_first=false, Is_last=false, Seq_parallel=true, Params=Flash_bwd_params]" 
(1591): here
            instantiation of "void flash::compute_dq_dk_dv_seqk_parallel<Kernel_traits,Is_dropout,Is_causal,Is_even_MN,Is_even_K,Params>(const Params &) [with Kernel_traits=Flash_bwd_kernel_traits<512, 32, 64, 4, 2, 2, 2, false, true, cutlass::bfloat16_t, Flash_kernel_traits<512, 32, 64, 4, cutlass::bfloat16_t>>, Is_dropout=false, Is_causal=false, Is_even_MN=false, Is_even_K=false, Params=Flash_bwd_params]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_bwd_launch_template.h(28): here
            instantiation of "void flash_bwd_dq_dk_dv_loop_seqk_parallel_kernel<Kernel_traits,Is_dropout,Is_causal,Is_even_MN,Is_even_K>(Flash_bwd_params) [with Kernel_traits=Flash_bwd_kernel_traits<512, 32, 64, 4, 2, 2, 2, false, true, cutlass::bfloat16_t, Flash_kernel_traits<512, 32, 64, 4, cutlass::bfloat16_t>>, Is_dropout=false, Is_causal=false, Is_even_MN=false, Is_even_K=false]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_bwd_launch_template.h(62): here
            instantiation of "void run_flash_bwd_seqk_parallel<Kernel_traits,Is_dropout>(Flash_bwd_params &, cudaStream_t, __nv_bool) [with Kernel_traits=Flash_bwd_kernel_traits<512, 32, 64, 4, 2, 2, 2, false, true, cutlass::bfloat16_t, Flash_kernel_traits<512, 32, 64, 4, cutlass::bfloat16_t>>, Is_dropout=false]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_bwd_launch_template.h(134): here
            instantiation of "void run_flash_bwd<Kernel_traits,Is_dropout>(Flash_bwd_params &, cudaStream_t, __nv_bool) [with Kernel_traits=Flash_bwd_kernel_traits<512, 32, 64, 4, 2, 2, 2, false, true, cutlass::bfloat16_t, Flash_kernel_traits<512, 32, 64, 4, cutlass::bfloat16_t>>, Is_dropout=false]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_bwd_launch_template.h(365): here
            instantiation of "void run_mha_bwd_hdim512<T>(Flash_bwd_params &, cudaStream_t, __nv_bool) [with T=cutlass::bfloat16_t]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_bwd_hdim512_bf16_sm80.cu(9): here

34 errors detected in the compilation of "/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_bwd_hdim512_bf16_sm80.cu".
[2/13] /usr/local/cuda-11.4/bin/nvcc  -I/home/v-feiychen/flash-attention/csrc/flash_attn -I/home/v-feiychen/flash-attention/csrc/flash_attn/src -I/home/v-feiychen/flash-attention/csrc/cutlass/include -I/anaconda/envs/flashattention/lib/python3.9/site-packages/torch/include -I/anaconda/envs/flashattention/lib/python3.9/site-packages/torch/include/torch/csrc/api/include -I/anaconda/envs/flashattention/lib/python3.9/site-packages/torch/include/TH -I/anaconda/envs/flashattention/lib/python3.9/site-packages/torch/include/THC -I/usr/local/cuda-11.4/include -I/anaconda/envs/flashattention/include/python3.9 -c -c /home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_bwd_hdim512_fp16_sm80.cu -o /home/v-feiychen/flash-attention/build/temp.linux-x86_64-cpython-39/csrc/flash_attn/src/flash_bwd_hdim512_fp16_sm80.o -D__CUDA_NO_HALF_OPERATORS__ -D__CUDA_NO_HALF_CONVERSIONS__ -D__CUDA_NO_BFLOAT16_CONVERSIONS__ -D__CUDA_NO_HALF2_OPERATORS__ --expt-relaxed-constexpr --compiler-options ''"'"'-fPIC'"'"'' -O3 -std=c++17 -U__CUDA_NO_HALF_OPERATORS__ -U__CUDA_NO_HALF_CONVERSIONS__ -U__CUDA_NO_HALF2_OPERATORS__ -U__CUDA_NO_BFLOAT16_CONVERSIONS__ --expt-relaxed-constexpr --expt-extended-lambda --use_fast_math --ptxas-options=-v -lineinfo -gencode arch=compute_80,code=sm_80 --threads 4 -DTORCH_API_INCLUDE_EXTENSION_H '-DPYBIND11_COMPILER_TYPE="_gcc"' '-DPYBIND11_STDLIB="_libstdcpp"' '-DPYBIND11_BUILD_ABI="_cxxabi1011"' -DTORCH_EXTENSION_NAME=flash_attn_2_cuda -D_GLIBCXX_USE_CXX11_ABI=0
FAILED: /home/v-feiychen/flash-attention/build/temp.linux-x86_64-cpython-39/csrc/flash_attn/src/flash_bwd_hdim512_fp16_sm80.o 
/usr/local/cuda-11.4/bin/nvcc  -I/home/v-feiychen/flash-attention/csrc/flash_attn -I/home/v-feiychen/flash-attention/csrc/flash_attn/src -I/home/v-feiychen/flash-attention/csrc/cutlass/include -I/anaconda/envs/flashattention/lib/python3.9/site-packages/torch/include -I/anaconda/envs/flashattention/lib/python3.9/site-packages/torch/include/torch/csrc/api/include -I/anaconda/envs/flashattention/lib/python3.9/site-packages/torch/include/TH -I/anaconda/envs/flashattention/lib/python3.9/site-packages/torch/include/THC -I/usr/local/cuda-11.4/include -I/anaconda/envs/flashattention/include/python3.9 -c -c /home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_bwd_hdim512_fp16_sm80.cu -o /home/v-feiychen/flash-attention/build/temp.linux-x86_64-cpython-39/csrc/flash_attn/src/flash_bwd_hdim512_fp16_sm80.o -D__CUDA_NO_HALF_OPERATORS__ -D__CUDA_NO_HALF_CONVERSIONS__ -D__CUDA_NO_BFLOAT16_CONVERSIONS__ -D__CUDA_NO_HALF2_OPERATORS__ --expt-relaxed-constexpr --compiler-options ''"'"'-fPIC'"'"'' -O3 -std=c++17 -U__CUDA_NO_HALF_OPERATORS__ -U__CUDA_NO_HALF_CONVERSIONS__ -U__CUDA_NO_HALF2_OPERATORS__ -U__CUDA_NO_BFLOAT16_CONVERSIONS__ --expt-relaxed-constexpr --expt-extended-lambda --use_fast_math --ptxas-options=-v -lineinfo -gencode arch=compute_80,code=sm_80 --threads 4 -DTORCH_API_INCLUDE_EXTENSION_H '-DPYBIND11_COMPILER_TYPE="_gcc"' '-DPYBIND11_STDLIB="_libstdcpp"' '-DPYBIND11_BUILD_ABI="_cxxabi1011"' -DTORCH_EXTENSION_NAME=flash_attn_2_cuda -D_GLIBCXX_USE_CXX11_ABI=0
/anaconda/envs/flashattention/lib/python3.9/site-packages/torch/include/c10/core/SymInt.h(84): warning: integer conversion resulted in a change of sign

/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_bwd_kernel.h(1004): error: static assertion failed
          detected during:
            instantiation of "void flash::compute_dq_dk_dv_1colblock<Kernel_traits,Is_dropout,Is_causal,Is_even_MN,Is_even_K,Is_first,Is_last,Seq_parallel,Params>(const Params &, int, int, int) [with Kernel_traits=Flash_bwd_kernel_traits<512, 32, 64, 4, 2, 2, 2, false, false, cutlass::half_t, Flash_kernel_traits<512, 32, 64, 4, cutlass::half_t>>, Is_dropout=true, Is_causal=true, Is_even_MN=true, Is_even_K=true, Is_first=false, Is_last=false, Seq_parallel=true, Params=Flash_bwd_params]" 
(1591): here
            instantiation of "void flash::compute_dq_dk_dv_seqk_parallel<Kernel_traits,Is_dropout,Is_causal,Is_even_MN,Is_even_K,Params>(const Params &) [with Kernel_traits=Flash_bwd_kernel_traits<512, 32, 64, 4, 2, 2, 2, false, false, cutlass::half_t, Flash_kernel_traits<512, 32, 64, 4, cutlass::half_t>>, Is_dropout=true, Is_causal=true, Is_even_MN=true, Is_even_K=true, Params=Flash_bwd_params]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_bwd_launch_template.h(28): here
            instantiation of "void flash_bwd_dq_dk_dv_loop_seqk_parallel_kernel<Kernel_traits,Is_dropout,Is_causal,Is_even_MN,Is_even_K>(Flash_bwd_params) [with Kernel_traits=Flash_bwd_kernel_traits<512, 32, 64, 4, 2, 2, 2, false, false, cutlass::half_t, Flash_kernel_traits<512, 32, 64, 4, cutlass::half_t>>, Is_dropout=true, Is_causal=true, Is_even_MN=true, Is_even_K=true]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_bwd_launch_template.h(62): here
            instantiation of "void run_flash_bwd_seqk_parallel<Kernel_traits,Is_dropout>(Flash_bwd_params &, cudaStream_t, __nv_bool) [with Kernel_traits=Flash_bwd_kernel_traits<512, 32, 64, 4, 2, 2, 2, false, false, cutlass::half_t, Flash_kernel_traits<512, 32, 64, 4, cutlass::half_t>>, Is_dropout=true]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_bwd_launch_template.h(134): here
            instantiation of "void run_flash_bwd<Kernel_traits,Is_dropout>(Flash_bwd_params &, cudaStream_t, __nv_bool) [with Kernel_traits=Flash_bwd_kernel_traits<512, 32, 64, 4, 2, 2, 2, false, false, cutlass::half_t, Flash_kernel_traits<512, 32, 64, 4, cutlass::half_t>>, Is_dropout=true]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_bwd_launch_template.h(365): here
            instantiation of "void run_mha_bwd_hdim512<T>(Flash_bwd_params &, cudaStream_t, __nv_bool) [with T=cutlass::half_t]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_bwd_hdim512_fp16_sm80.cu(9): here

/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_bwd_kernel.h(1004): error: static assertion failed
          detected during:
            instantiation of "void flash::compute_dq_dk_dv_1colblock<Kernel_traits,Is_dropout,Is_causal,Is_even_MN,Is_even_K,Is_first,Is_last,Seq_parallel,Params>(const Params &, int, int, int) [with Kernel_traits=Flash_bwd_kernel_traits<512, 32, 64, 4, 2, 2, 2, false, false, cutlass::half_t, Flash_kernel_traits<512, 32, 64, 4, cutlass::half_t>>, Is_dropout=true, Is_causal=true, Is_even_MN=true, Is_even_K=false, Is_first=false, Is_last=false, Seq_parallel=true, Params=Flash_bwd_params]" 
(1591): here
            instantiation of "void flash::compute_dq_dk_dv_seqk_parallel<Kernel_traits,Is_dropout,Is_causal,Is_even_MN,Is_even_K,Params>(const Params &) [with Kernel_traits=Flash_bwd_kernel_traits<512, 32, 64, 4, 2, 2, 2, false, false, cutlass::half_t, Flash_kernel_traits<512, 32, 64, 4, cutlass::half_t>>, Is_dropout=true, Is_causal=true, Is_even_MN=true, Is_even_K=false, Params=Flash_bwd_params]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_bwd_launch_template.h(28): here
            instantiation of "void flash_bwd_dq_dk_dv_loop_seqk_parallel_kernel<Kernel_traits,Is_dropout,Is_causal,Is_even_MN,Is_even_K>(Flash_bwd_params) [with Kernel_traits=Flash_bwd_kernel_traits<512, 32, 64, 4, 2, 2, 2, false, false, cutlass::half_t, Flash_kernel_traits<512, 32, 64, 4, cutlass::half_t>>, Is_dropout=true, Is_causal=true, Is_even_MN=true, Is_even_K=false]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_bwd_launch_template.h(62): here
            instantiation of "void run_flash_bwd_seqk_parallel<Kernel_traits,Is_dropout>(Flash_bwd_params &, cudaStream_t, __nv_bool) [with Kernel_traits=Flash_bwd_kernel_traits<512, 32, 64, 4, 2, 2, 2, false, false, cutlass::half_t, Flash_kernel_traits<512, 32, 64, 4, cutlass::half_t>>, Is_dropout=true]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_bwd_launch_template.h(134): here
            instantiation of "void run_flash_bwd<Kernel_traits,Is_dropout>(Flash_bwd_params &, cudaStream_t, __nv_bool) [with Kernel_traits=Flash_bwd_kernel_traits<512, 32, 64, 4, 2, 2, 2, false, false, cutlass::half_t, Flash_kernel_traits<512, 32, 64, 4, cutlass::half_t>>, Is_dropout=true]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_bwd_launch_template.h(365): here
            instantiation of "void run_mha_bwd_hdim512<T>(Flash_bwd_params &, cudaStream_t, __nv_bool) [with T=cutlass::half_t]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_bwd_hdim512_fp16_sm80.cu(9): here

/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_bwd_kernel.h(1004): error: static assertion failed
          detected during:
            instantiation of "void flash::compute_dq_dk_dv_1colblock<Kernel_traits,Is_dropout,Is_causal,Is_even_MN,Is_even_K,Is_first,Is_last,Seq_parallel,Params>(const Params &, int, int, int) [with Kernel_traits=Flash_bwd_kernel_traits<512, 32, 64, 4, 2, 2, 2, false, false, cutlass::half_t, Flash_kernel_traits<512, 32, 64, 4, cutlass::half_t>>, Is_dropout=true, Is_causal=true, Is_even_MN=false, Is_even_K=true, Is_first=false, Is_last=false, Seq_parallel=true, Params=Flash_bwd_params]" 
(1591): here
            instantiation of "void flash::compute_dq_dk_dv_seqk_parallel<Kernel_traits,Is_dropout,Is_causal,Is_even_MN,Is_even_K,Params>(const Params &) [with Kernel_traits=Flash_bwd_kernel_traits<512, 32, 64, 4, 2, 2, 2, false, false, cutlass::half_t, Flash_kernel_traits<512, 32, 64, 4, cutlass::half_t>>, Is_dropout=true, Is_causal=true, Is_even_MN=false, Is_even_K=true, Params=Flash_bwd_params]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_bwd_launch_template.h(28): here
            instantiation of "void flash_bwd_dq_dk_dv_loop_seqk_parallel_kernel<Kernel_traits,Is_dropout,Is_causal,Is_even_MN,Is_even_K>(Flash_bwd_params) [with Kernel_traits=Flash_bwd_kernel_traits<512, 32, 64, 4, 2, 2, 2, false, false, cutlass::half_t, Flash_kernel_traits<512, 32, 64, 4, cutlass::half_t>>, Is_dropout=true, Is_causal=true, Is_even_MN=false, Is_even_K=true]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_bwd_launch_template.h(62): here
            instantiation of "void run_flash_bwd_seqk_parallel<Kernel_traits,Is_dropout>(Flash_bwd_params &, cudaStream_t, __nv_bool) [with Kernel_traits=Flash_bwd_kernel_traits<512, 32, 64, 4, 2, 2, 2, false, false, cutlass::half_t, Flash_kernel_traits<512, 32, 64, 4, cutlass::half_t>>, Is_dropout=true]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_bwd_launch_template.h(134): here
            instantiation of "void run_flash_bwd<Kernel_traits,Is_dropout>(Flash_bwd_params &, cudaStream_t, __nv_bool) [with Kernel_traits=Flash_bwd_kernel_traits<512, 32, 64, 4, 2, 2, 2, false, false, cutlass::half_t, Flash_kernel_traits<512, 32, 64, 4, cutlass::half_t>>, Is_dropout=true]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_bwd_launch_template.h(365): here
            instantiation of "void run_mha_bwd_hdim512<T>(Flash_bwd_params &, cudaStream_t, __nv_bool) [with T=cutlass::half_t]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_bwd_hdim512_fp16_sm80.cu(9): here

/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_bwd_kernel.h(1004): error: static assertion failed
          detected during:
            instantiation of "void flash::compute_dq_dk_dv_1colblock<Kernel_traits,Is_dropout,Is_causal,Is_even_MN,Is_even_K,Is_first,Is_last,Seq_parallel,Params>(const Params &, int, int, int) [with Kernel_traits=Flash_bwd_kernel_traits<512, 32, 64, 4, 2, 2, 2, false, false, cutlass::half_t, Flash_kernel_traits<512, 32, 64, 4, cutlass::half_t>>, Is_dropout=true, Is_causal=true, Is_even_MN=false, Is_even_K=false, Is_first=false, Is_last=false, Seq_parallel=true, Params=Flash_bwd_params]" 
(1591): here
            instantiation of "void flash::compute_dq_dk_dv_seqk_parallel<Kernel_traits,Is_dropout,Is_causal,Is_even_MN,Is_even_K,Params>(const Params &) [with Kernel_traits=Flash_bwd_kernel_traits<512, 32, 64, 4, 2, 2, 2, false, false, cutlass::half_t, Flash_kernel_traits<512, 32, 64, 4, cutlass::half_t>>, Is_dropout=true, Is_causal=true, Is_even_MN=false, Is_even_K=false, Params=Flash_bwd_params]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_bwd_launch_template.h(28): here
            instantiation of "void flash_bwd_dq_dk_dv_loop_seqk_parallel_kernel<Kernel_traits,Is_dropout,Is_causal,Is_even_MN,Is_even_K>(Flash_bwd_params) [with Kernel_traits=Flash_bwd_kernel_traits<512, 32, 64, 4, 2, 2, 2, false, false, cutlass::half_t, Flash_kernel_traits<512, 32, 64, 4, cutlass::half_t>>, Is_dropout=true, Is_causal=true, Is_even_MN=false, Is_even_K=false]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_bwd_launch_template.h(62): here
            instantiation of "void run_flash_bwd_seqk_parallel<Kernel_traits,Is_dropout>(Flash_bwd_params &, cudaStream_t, __nv_bool) [with Kernel_traits=Flash_bwd_kernel_traits<512, 32, 64, 4, 2, 2, 2, false, false, cutlass::half_t, Flash_kernel_traits<512, 32, 64, 4, cutlass::half_t>>, Is_dropout=true]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_bwd_launch_template.h(134): here
            instantiation of "void run_flash_bwd<Kernel_traits,Is_dropout>(Flash_bwd_params &, cudaStream_t, __nv_bool) [with Kernel_traits=Flash_bwd_kernel_traits<512, 32, 64, 4, 2, 2, 2, false, false, cutlass::half_t, Flash_kernel_traits<512, 32, 64, 4, cutlass::half_t>>, Is_dropout=true]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_bwd_launch_template.h(365): here
            instantiation of "void run_mha_bwd_hdim512<T>(Flash_bwd_params &, cudaStream_t, __nv_bool) [with T=cutlass::half_t]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_bwd_hdim512_fp16_sm80.cu(9): here

/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_bwd_kernel.h(1004): error: static assertion failed
          detected during:
            instantiation of "void flash::compute_dq_dk_dv_1colblock<Kernel_traits,Is_dropout,Is_causal,Is_even_MN,Is_even_K,Is_first,Is_last,Seq_parallel,Params>(const Params &, int, int, int) [with Kernel_traits=Flash_bwd_kernel_traits<512, 32, 64, 4, 2, 2, 2, false, false, cutlass::half_t, Flash_kernel_traits<512, 32, 64, 4, cutlass::half_t>>, Is_dropout=true, Is_causal=false, Is_even_MN=true, Is_even_K=true, Is_first=false, Is_last=false, Seq_parallel=true, Params=Flash_bwd_params]" 
(1591): here
            instantiation of "void flash::compute_dq_dk_dv_seqk_parallel<Kernel_traits,Is_dropout,Is_causal,Is_even_MN,Is_even_K,Params>(const Params &) [with Kernel_traits=Flash_bwd_kernel_traits<512, 32, 64, 4, 2, 2, 2, false, false, cutlass::half_t, Flash_kernel_traits<512, 32, 64, 4, cutlass::half_t>>, Is_dropout=true, Is_causal=false, Is_even_MN=true, Is_even_K=true, Params=Flash_bwd_params]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_bwd_launch_template.h(28): here
            instantiation of "void flash_bwd_dq_dk_dv_loop_seqk_parallel_kernel<Kernel_traits,Is_dropout,Is_causal,Is_even_MN,Is_even_K>(Flash_bwd_params) [with Kernel_traits=Flash_bwd_kernel_traits<512, 32, 64, 4, 2, 2, 2, false, false, cutlass::half_t, Flash_kernel_traits<512, 32, 64, 4, cutlass::half_t>>, Is_dropout=true, Is_causal=false, Is_even_MN=true, Is_even_K=true]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_bwd_launch_template.h(62): here
            instantiation of "void run_flash_bwd_seqk_parallel<Kernel_traits,Is_dropout>(Flash_bwd_params &, cudaStream_t, __nv_bool) [with Kernel_traits=Flash_bwd_kernel_traits<512, 32, 64, 4, 2, 2, 2, false, false, cutlass::half_t, Flash_kernel_traits<512, 32, 64, 4, cutlass::half_t>>, Is_dropout=true]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_bwd_launch_template.h(134): here
            instantiation of "void run_flash_bwd<Kernel_traits,Is_dropout>(Flash_bwd_params &, cudaStream_t, __nv_bool) [with Kernel_traits=Flash_bwd_kernel_traits<512, 32, 64, 4, 2, 2, 2, false, false, cutlass::half_t, Flash_kernel_traits<512, 32, 64, 4, cutlass::half_t>>, Is_dropout=true]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_bwd_launch_template.h(365): here
            instantiation of "void run_mha_bwd_hdim512<T>(Flash_bwd_params &, cudaStream_t, __nv_bool) [with T=cutlass::half_t]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_bwd_hdim512_fp16_sm80.cu(9): here

/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_bwd_kernel.h(1004): error: static assertion failed
          detected during:
            instantiation of "void flash::compute_dq_dk_dv_1colblock<Kernel_traits,Is_dropout,Is_causal,Is_even_MN,Is_even_K,Is_first,Is_last,Seq_parallel,Params>(const Params &, int, int, int) [with Kernel_traits=Flash_bwd_kernel_traits<512, 32, 64, 4, 2, 2, 2, false, false, cutlass::half_t, Flash_kernel_traits<512, 32, 64, 4, cutlass::half_t>>, Is_dropout=true, Is_causal=false, Is_even_MN=true, Is_even_K=false, Is_first=false, Is_last=false, Seq_parallel=true, Params=Flash_bwd_params]" 
(1591): here
            instantiation of "void flash::compute_dq_dk_dv_seqk_parallel<Kernel_traits,Is_dropout,Is_causal,Is_even_MN,Is_even_K,Params>(const Params &) [with Kernel_traits=Flash_bwd_kernel_traits<512, 32, 64, 4, 2, 2, 2, false, false, cutlass::half_t, Flash_kernel_traits<512, 32, 64, 4, cutlass::half_t>>, Is_dropout=true, Is_causal=false, Is_even_MN=true, Is_even_K=false, Params=Flash_bwd_params]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_bwd_launch_template.h(28): here
            instantiation of "void flash_bwd_dq_dk_dv_loop_seqk_parallel_kernel<Kernel_traits,Is_dropout,Is_causal,Is_even_MN,Is_even_K>(Flash_bwd_params) [with Kernel_traits=Flash_bwd_kernel_traits<512, 32, 64, 4, 2, 2, 2, false, false, cutlass::half_t, Flash_kernel_traits<512, 32, 64, 4, cutlass::half_t>>, Is_dropout=true, Is_causal=false, Is_even_MN=true, Is_even_K=false]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_bwd_launch_template.h(62): here
            instantiation of "void run_flash_bwd_seqk_parallel<Kernel_traits,Is_dropout>(Flash_bwd_params &, cudaStream_t, __nv_bool) [with Kernel_traits=Flash_bwd_kernel_traits<512, 32, 64, 4, 2, 2, 2, false, false, cutlass::half_t, Flash_kernel_traits<512, 32, 64, 4, cutlass::half_t>>, Is_dropout=true]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_bwd_launch_template.h(134): here
            instantiation of "void run_flash_bwd<Kernel_traits,Is_dropout>(Flash_bwd_params &, cudaStream_t, __nv_bool) [with Kernel_traits=Flash_bwd_kernel_traits<512, 32, 64, 4, 2, 2, 2, false, false, cutlass::half_t, Flash_kernel_traits<512, 32, 64, 4, cutlass::half_t>>, Is_dropout=true]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_bwd_launch_template.h(365): here
            instantiation of "void run_mha_bwd_hdim512<T>(Flash_bwd_params &, cudaStream_t, __nv_bool) [with T=cutlass::half_t]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_bwd_hdim512_fp16_sm80.cu(9): here

/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_bwd_kernel.h(1004): error: static assertion failed
          detected during:
            instantiation of "void flash::compute_dq_dk_dv_1colblock<Kernel_traits,Is_dropout,Is_causal,Is_even_MN,Is_even_K,Is_first,Is_last,Seq_parallel,Params>(const Params &, int, int, int) [with Kernel_traits=Flash_bwd_kernel_traits<512, 32, 64, 4, 2, 2, 2, false, false, cutlass::half_t, Flash_kernel_traits<512, 32, 64, 4, cutlass::half_t>>, Is_dropout=true, Is_causal=false, Is_even_MN=false, Is_even_K=true, Is_first=false, Is_last=false, Seq_parallel=true, Params=Flash_bwd_params]" 
(1591): here
            instantiation of "void flash::compute_dq_dk_dv_seqk_parallel<Kernel_traits,Is_dropout,Is_causal,Is_even_MN,Is_even_K,Params>(const Params &) [with Kernel_traits=Flash_bwd_kernel_traits<512, 32, 64, 4, 2, 2, 2, false, false, cutlass::half_t, Flash_kernel_traits<512, 32, 64, 4, cutlass::half_t>>, Is_dropout=true, Is_causal=false, Is_even_MN=false, Is_even_K=true, Params=Flash_bwd_params]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_bwd_launch_template.h(28): here
            instantiation of "void flash_bwd_dq_dk_dv_loop_seqk_parallel_kernel<Kernel_traits,Is_dropout,Is_causal,Is_even_MN,Is_even_K>(Flash_bwd_params) [with Kernel_traits=Flash_bwd_kernel_traits<512, 32, 64, 4, 2, 2, 2, false, false, cutlass::half_t, Flash_kernel_traits<512, 32, 64, 4, cutlass::half_t>>, Is_dropout=true, Is_causal=false, Is_even_MN=false, Is_even_K=true]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_bwd_launch_template.h(62): here
            instantiation of "void run_flash_bwd_seqk_parallel<Kernel_traits,Is_dropout>(Flash_bwd_params &, cudaStream_t, __nv_bool) [with Kernel_traits=Flash_bwd_kernel_traits<512, 32, 64, 4, 2, 2, 2, false, false, cutlass::half_t, Flash_kernel_traits<512, 32, 64, 4, cutlass::half_t>>, Is_dropout=true]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_bwd_launch_template.h(134): here
            instantiation of "void run_flash_bwd<Kernel_traits,Is_dropout>(Flash_bwd_params &, cudaStream_t, __nv_bool) [with Kernel_traits=Flash_bwd_kernel_traits<512, 32, 64, 4, 2, 2, 2, false, false, cutlass::half_t, Flash_kernel_traits<512, 32, 64, 4, cutlass::half_t>>, Is_dropout=true]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_bwd_launch_template.h(365): here
            instantiation of "void run_mha_bwd_hdim512<T>(Flash_bwd_params &, cudaStream_t, __nv_bool) [with T=cutlass::half_t]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_bwd_hdim512_fp16_sm80.cu(9): here

/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_bwd_kernel.h(1004): error: static assertion failed
          detected during:
            instantiation of "void flash::compute_dq_dk_dv_1colblock<Kernel_traits,Is_dropout,Is_causal,Is_even_MN,Is_even_K,Is_first,Is_last,Seq_parallel,Params>(const Params &, int, int, int) [with Kernel_traits=Flash_bwd_kernel_traits<512, 32, 64, 4, 2, 2, 2, false, false, cutlass::half_t, Flash_kernel_traits<512, 32, 64, 4, cutlass::half_t>>, Is_dropout=true, Is_causal=false, Is_even_MN=false, Is_even_K=false, Is_first=false, Is_last=false, Seq_parallel=true, Params=Flash_bwd_params]" 
(1591): here
            instantiation of "void flash::compute_dq_dk_dv_seqk_parallel<Kernel_traits,Is_dropout,Is_causal,Is_even_MN,Is_even_K,Params>(const Params &) [with Kernel_traits=Flash_bwd_kernel_traits<512, 32, 64, 4, 2, 2, 2, false, false, cutlass::half_t, Flash_kernel_traits<512, 32, 64, 4, cutlass::half_t>>, Is_dropout=true, Is_causal=false, Is_even_MN=false, Is_even_K=false, Params=Flash_bwd_params]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_bwd_launch_template.h(28): here
            instantiation of "void flash_bwd_dq_dk_dv_loop_seqk_parallel_kernel<Kernel_traits,Is_dropout,Is_causal,Is_even_MN,Is_even_K>(Flash_bwd_params) [with Kernel_traits=Flash_bwd_kernel_traits<512, 32, 64, 4, 2, 2, 2, false, false, cutlass::half_t, Flash_kernel_traits<512, 32, 64, 4, cutlass::half_t>>, Is_dropout=true, Is_causal=false, Is_even_MN=false, Is_even_K=false]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_bwd_launch_template.h(62): here
            instantiation of "void run_flash_bwd_seqk_parallel<Kernel_traits,Is_dropout>(Flash_bwd_params &, cudaStream_t, __nv_bool) [with Kernel_traits=Flash_bwd_kernel_traits<512, 32, 64, 4, 2, 2, 2, false, false, cutlass::half_t, Flash_kernel_traits<512, 32, 64, 4, cutlass::half_t>>, Is_dropout=true]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_bwd_launch_template.h(134): here
            instantiation of "void run_flash_bwd<Kernel_traits,Is_dropout>(Flash_bwd_params &, cudaStream_t, __nv_bool) [with Kernel_traits=Flash_bwd_kernel_traits<512, 32, 64, 4, 2, 2, 2, false, false, cutlass::half_t, Flash_kernel_traits<512, 32, 64, 4, cutlass::half_t>>, Is_dropout=true]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_bwd_launch_template.h(365): here
            instantiation of "void run_mha_bwd_hdim512<T>(Flash_bwd_params &, cudaStream_t, __nv_bool) [with T=cutlass::half_t]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_bwd_hdim512_fp16_sm80.cu(9): here

/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_bwd_kernel.h(287): error: static assertion failed
          detected during:
            instantiation of "void flash::convert_dQ<Kernel_traits,Params>(const Params &) [with Kernel_traits=Flash_bwd_kernel_traits<512, 32, 64, 4, 2, 2, 2, false, false, cutlass::half_t, Flash_kernel_traits<512, 32, 64, 4, cutlass::half_t>>, Params=Flash_bwd_params]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_bwd_launch_template.h(38): here
            instantiation of "void flash_bwd_convert_dq_kernel<Kernel_traits>(Flash_bwd_params) [with Kernel_traits=Flash_bwd_kernel_traits<512, 32, 64, 4, 2, 2, 2, false, false, cutlass::half_t, Flash_kernel_traits<512, 32, 64, 4, cutlass::half_t>>]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_bwd_launch_template.h(77): here
            instantiation of "void run_flash_bwd_seqk_parallel<Kernel_traits,Is_dropout>(Flash_bwd_params &, cudaStream_t, __nv_bool) [with Kernel_traits=Flash_bwd_kernel_traits<512, 32, 64, 4, 2, 2, 2, false, false, cutlass::half_t, Flash_kernel_traits<512, 32, 64, 4, cutlass::half_t>>, Is_dropout=true]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_bwd_launch_template.h(134): here
            instantiation of "void run_flash_bwd<Kernel_traits,Is_dropout>(Flash_bwd_params &, cudaStream_t, __nv_bool) [with Kernel_traits=Flash_bwd_kernel_traits<512, 32, 64, 4, 2, 2, 2, false, false, cutlass::half_t, Flash_kernel_traits<512, 32, 64, 4, cutlass::half_t>>, Is_dropout=true]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_bwd_launch_template.h(365): here
            instantiation of "void run_mha_bwd_hdim512<T>(Flash_bwd_params &, cudaStream_t, __nv_bool) [with T=cutlass::half_t]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_bwd_hdim512_fp16_sm80.cu(9): here

/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_bwd_kernel.h(1004): error: static assertion failed
          detected during:
            instantiation of "void flash::compute_dq_dk_dv_1colblock<Kernel_traits,Is_dropout,Is_causal,Is_even_MN,Is_even_K,Is_first,Is_last,Seq_parallel,Params>(const Params &, int, int, int) [with Kernel_traits=Flash_bwd_kernel_traits<512, 32, 64, 4, 2, 2, 2, false, true, cutlass::half_t, Flash_kernel_traits<512, 32, 64, 4, cutlass::half_t>>, Is_dropout=true, Is_causal=true, Is_even_MN=true, Is_even_K=true, Is_first=false, Is_last=false, Seq_parallel=true, Params=Flash_bwd_params]" 
(1591): here
            instantiation of "void flash::compute_dq_dk_dv_seqk_parallel<Kernel_traits,Is_dropout,Is_causal,Is_even_MN,Is_even_K,Params>(const Params &) [with Kernel_traits=Flash_bwd_kernel_traits<512, 32, 64, 4, 2, 2, 2, false, true, cutlass::half_t, Flash_kernel_traits<512, 32, 64, 4, cutlass::half_t>>, Is_dropout=true, Is_causal=true, Is_even_MN=true, Is_even_K=true, Params=Flash_bwd_params]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_bwd_launch_template.h(28): here
            instantiation of "void flash_bwd_dq_dk_dv_loop_seqk_parallel_kernel<Kernel_traits,Is_dropout,Is_causal,Is_even_MN,Is_even_K>(Flash_bwd_params) [with Kernel_traits=Flash_bwd_kernel_traits<512, 32, 64, 4, 2, 2, 2, false, true, cutlass::half_t, Flash_kernel_traits<512, 32, 64, 4, cutlass::half_t>>, Is_dropout=true, Is_causal=true, Is_even_MN=true, Is_even_K=true]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_bwd_launch_template.h(62): here
            instantiation of "void run_flash_bwd_seqk_parallel<Kernel_traits,Is_dropout>(Flash_bwd_params &, cudaStream_t, __nv_bool) [with Kernel_traits=Flash_bwd_kernel_traits<512, 32, 64, 4, 2, 2, 2, false, true, cutlass::half_t, Flash_kernel_traits<512, 32, 64, 4, cutlass::half_t>>, Is_dropout=true]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_bwd_launch_template.h(134): here
            instantiation of "void run_flash_bwd<Kernel_traits,Is_dropout>(Flash_bwd_params &, cudaStream_t, __nv_bool) [with Kernel_traits=Flash_bwd_kernel_traits<512, 32, 64, 4, 2, 2, 2, false, true, cutlass::half_t, Flash_kernel_traits<512, 32, 64, 4, cutlass::half_t>>, Is_dropout=true]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_bwd_launch_template.h(365): here
            instantiation of "void run_mha_bwd_hdim512<T>(Flash_bwd_params &, cudaStream_t, __nv_bool) [with T=cutlass::half_t]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_bwd_hdim512_fp16_sm80.cu(9): here

/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_bwd_kernel.h(1004): error: static assertion failed
          detected during:
            instantiation of "void flash::compute_dq_dk_dv_1colblock<Kernel_traits,Is_dropout,Is_causal,Is_even_MN,Is_even_K,Is_first,Is_last,Seq_parallel,Params>(const Params &, int, int, int) [with Kernel_traits=Flash_bwd_kernel_traits<512, 32, 64, 4, 2, 2, 2, false, true, cutlass::half_t, Flash_kernel_traits<512, 32, 64, 4, cutlass::half_t>>, Is_dropout=true, Is_causal=true, Is_even_MN=true, Is_even_K=false, Is_first=false, Is_last=false, Seq_parallel=true, Params=Flash_bwd_params]" 
(1591): here
            instantiation of "void flash::compute_dq_dk_dv_seqk_parallel<Kernel_traits,Is_dropout,Is_causal,Is_even_MN,Is_even_K,Params>(const Params &) [with Kernel_traits=Flash_bwd_kernel_traits<512, 32, 64, 4, 2, 2, 2, false, true, cutlass::half_t, Flash_kernel_traits<512, 32, 64, 4, cutlass::half_t>>, Is_dropout=true, Is_causal=true, Is_even_MN=true, Is_even_K=false, Params=Flash_bwd_params]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_bwd_launch_template.h(28): here
            instantiation of "void flash_bwd_dq_dk_dv_loop_seqk_parallel_kernel<Kernel_traits,Is_dropout,Is_causal,Is_even_MN,Is_even_K>(Flash_bwd_params) [with Kernel_traits=Flash_bwd_kernel_traits<512, 32, 64, 4, 2, 2, 2, false, true, cutlass::half_t, Flash_kernel_traits<512, 32, 64, 4, cutlass::half_t>>, Is_dropout=true, Is_causal=true, Is_even_MN=true, Is_even_K=false]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_bwd_launch_template.h(62): here
            instantiation of "void run_flash_bwd_seqk_parallel<Kernel_traits,Is_dropout>(Flash_bwd_params &, cudaStream_t, __nv_bool) [with Kernel_traits=Flash_bwd_kernel_traits<512, 32, 64, 4, 2, 2, 2, false, true, cutlass::half_t, Flash_kernel_traits<512, 32, 64, 4, cutlass::half_t>>, Is_dropout=true]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_bwd_launch_template.h(134): here
            instantiation of "void run_flash_bwd<Kernel_traits,Is_dropout>(Flash_bwd_params &, cudaStream_t, __nv_bool) [with Kernel_traits=Flash_bwd_kernel_traits<512, 32, 64, 4, 2, 2, 2, false, true, cutlass::half_t, Flash_kernel_traits<512, 32, 64, 4, cutlass::half_t>>, Is_dropout=true]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_bwd_launch_template.h(365): here
            instantiation of "void run_mha_bwd_hdim512<T>(Flash_bwd_params &, cudaStream_t, __nv_bool) [with T=cutlass::half_t]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_bwd_hdim512_fp16_sm80.cu(9): here

/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_bwd_kernel.h(1004): error: static assertion failed
          detected during:
            instantiation of "void flash::compute_dq_dk_dv_1colblock<Kernel_traits,Is_dropout,Is_causal,Is_even_MN,Is_even_K,Is_first,Is_last,Seq_parallel,Params>(const Params &, int, int, int) [with Kernel_traits=Flash_bwd_kernel_traits<512, 32, 64, 4, 2, 2, 2, false, true, cutlass::half_t, Flash_kernel_traits<512, 32, 64, 4, cutlass::half_t>>, Is_dropout=true, Is_causal=true, Is_even_MN=false, Is_even_K=true, Is_first=false, Is_last=false, Seq_parallel=true, Params=Flash_bwd_params]" 
(1591): here
            instantiation of "void flash::compute_dq_dk_dv_seqk_parallel<Kernel_traits,Is_dropout,Is_causal,Is_even_MN,Is_even_K,Params>(const Params &) [with Kernel_traits=Flash_bwd_kernel_traits<512, 32, 64, 4, 2, 2, 2, false, true, cutlass::half_t, Flash_kernel_traits<512, 32, 64, 4, cutlass::half_t>>, Is_dropout=true, Is_causal=true, Is_even_MN=false, Is_even_K=true, Params=Flash_bwd_params]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_bwd_launch_template.h(28): here
            instantiation of "void flash_bwd_dq_dk_dv_loop_seqk_parallel_kernel<Kernel_traits,Is_dropout,Is_causal,Is_even_MN,Is_even_K>(Flash_bwd_params) [with Kernel_traits=Flash_bwd_kernel_traits<512, 32, 64, 4, 2, 2, 2, false, true, cutlass::half_t, Flash_kernel_traits<512, 32, 64, 4, cutlass::half_t>>, Is_dropout=true, Is_causal=true, Is_even_MN=false, Is_even_K=true]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_bwd_launch_template.h(62): here
            instantiation of "void run_flash_bwd_seqk_parallel<Kernel_traits,Is_dropout>(Flash_bwd_params &, cudaStream_t, __nv_bool) [with Kernel_traits=Flash_bwd_kernel_traits<512, 32, 64, 4, 2, 2, 2, false, true, cutlass::half_t, Flash_kernel_traits<512, 32, 64, 4, cutlass::half_t>>, Is_dropout=true]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_bwd_launch_template.h(134): here
            instantiation of "void run_flash_bwd<Kernel_traits,Is_dropout>(Flash_bwd_params &, cudaStream_t, __nv_bool) [with Kernel_traits=Flash_bwd_kernel_traits<512, 32, 64, 4, 2, 2, 2, false, true, cutlass::half_t, Flash_kernel_traits<512, 32, 64, 4, cutlass::half_t>>, Is_dropout=true]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_bwd_launch_template.h(365): here
            instantiation of "void run_mha_bwd_hdim512<T>(Flash_bwd_params &, cudaStream_t, __nv_bool) [with T=cutlass::half_t]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_bwd_hdim512_fp16_sm80.cu(9): here

/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_bwd_kernel.h(1004): error: static assertion failed
          detected during:
            instantiation of "void flash::compute_dq_dk_dv_1colblock<Kernel_traits,Is_dropout,Is_causal,Is_even_MN,Is_even_K,Is_first,Is_last,Seq_parallel,Params>(const Params &, int, int, int) [with Kernel_traits=Flash_bwd_kernel_traits<512, 32, 64, 4, 2, 2, 2, false, true, cutlass::half_t, Flash_kernel_traits<512, 32, 64, 4, cutlass::half_t>>, Is_dropout=true, Is_causal=true, Is_even_MN=false, Is_even_K=false, Is_first=false, Is_last=false, Seq_parallel=true, Params=Flash_bwd_params]" 
(1591): here
            instantiation of "void flash::compute_dq_dk_dv_seqk_parallel<Kernel_traits,Is_dropout,Is_causal,Is_even_MN,Is_even_K,Params>(const Params &) [with Kernel_traits=Flash_bwd_kernel_traits<512, 32, 64, 4, 2, 2, 2, false, true, cutlass::half_t, Flash_kernel_traits<512, 32, 64, 4, cutlass::half_t>>, Is_dropout=true, Is_causal=true, Is_even_MN=false, Is_even_K=false, Params=Flash_bwd_params]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_bwd_launch_template.h(28): here
            instantiation of "void flash_bwd_dq_dk_dv_loop_seqk_parallel_kernel<Kernel_traits,Is_dropout,Is_causal,Is_even_MN,Is_even_K>(Flash_bwd_params) [with Kernel_traits=Flash_bwd_kernel_traits<512, 32, 64, 4, 2, 2, 2, false, true, cutlass::half_t, Flash_kernel_traits<512, 32, 64, 4, cutlass::half_t>>, Is_dropout=true, Is_causal=true, Is_even_MN=false, Is_even_K=false]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_bwd_launch_template.h(62): here
            instantiation of "void run_flash_bwd_seqk_parallel<Kernel_traits,Is_dropout>(Flash_bwd_params &, cudaStream_t, __nv_bool) [with Kernel_traits=Flash_bwd_kernel_traits<512, 32, 64, 4, 2, 2, 2, false, true, cutlass::half_t, Flash_kernel_traits<512, 32, 64, 4, cutlass::half_t>>, Is_dropout=true]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_bwd_launch_template.h(134): here
            instantiation of "void run_flash_bwd<Kernel_traits,Is_dropout>(Flash_bwd_params &, cudaStream_t, __nv_bool) [with Kernel_traits=Flash_bwd_kernel_traits<512, 32, 64, 4, 2, 2, 2, false, true, cutlass::half_t, Flash_kernel_traits<512, 32, 64, 4, cutlass::half_t>>, Is_dropout=true]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_bwd_launch_template.h(365): here
            instantiation of "void run_mha_bwd_hdim512<T>(Flash_bwd_params &, cudaStream_t, __nv_bool) [with T=cutlass::half_t]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_bwd_hdim512_fp16_sm80.cu(9): here

/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_bwd_kernel.h(1004): error: static assertion failed
          detected during:
            instantiation of "void flash::compute_dq_dk_dv_1colblock<Kernel_traits,Is_dropout,Is_causal,Is_even_MN,Is_even_K,Is_first,Is_last,Seq_parallel,Params>(const Params &, int, int, int) [with Kernel_traits=Flash_bwd_kernel_traits<512, 32, 64, 4, 2, 2, 2, false, true, cutlass::half_t, Flash_kernel_traits<512, 32, 64, 4, cutlass::half_t>>, Is_dropout=true, Is_causal=false, Is_even_MN=true, Is_even_K=true, Is_first=false, Is_last=false, Seq_parallel=true, Params=Flash_bwd_params]" 
(1591): here
            instantiation of "void flash::compute_dq_dk_dv_seqk_parallel<Kernel_traits,Is_dropout,Is_causal,Is_even_MN,Is_even_K,Params>(const Params &) [with Kernel_traits=Flash_bwd_kernel_traits<512, 32, 64, 4, 2, 2, 2, false, true, cutlass::half_t, Flash_kernel_traits<512, 32, 64, 4, cutlass::half_t>>, Is_dropout=true, Is_causal=false, Is_even_MN=true, Is_even_K=true, Params=Flash_bwd_params]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_bwd_launch_template.h(28): here
            instantiation of "void flash_bwd_dq_dk_dv_loop_seqk_parallel_kernel<Kernel_traits,Is_dropout,Is_causal,Is_even_MN,Is_even_K>(Flash_bwd_params) [with Kernel_traits=Flash_bwd_kernel_traits<512, 32, 64, 4, 2, 2, 2, false, true, cutlass::half_t, Flash_kernel_traits<512, 32, 64, 4, cutlass::half_t>>, Is_dropout=true, Is_causal=false, Is_even_MN=true, Is_even_K=true]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_bwd_launch_template.h(62): here
            instantiation of "void run_flash_bwd_seqk_parallel<Kernel_traits,Is_dropout>(Flash_bwd_params &, cudaStream_t, __nv_bool) [with Kernel_traits=Flash_bwd_kernel_traits<512, 32, 64, 4, 2, 2, 2, false, true, cutlass::half_t, Flash_kernel_traits<512, 32, 64, 4, cutlass::half_t>>, Is_dropout=true]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_bwd_launch_template.h(134): here
            instantiation of "void run_flash_bwd<Kernel_traits,Is_dropout>(Flash_bwd_params &, cudaStream_t, __nv_bool) [with Kernel_traits=Flash_bwd_kernel_traits<512, 32, 64, 4, 2, 2, 2, false, true, cutlass::half_t, Flash_kernel_traits<512, 32, 64, 4, cutlass::half_t>>, Is_dropout=true]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_bwd_launch_template.h(365): here
            instantiation of "void run_mha_bwd_hdim512<T>(Flash_bwd_params &, cudaStream_t, __nv_bool) [with T=cutlass::half_t]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_bwd_hdim512_fp16_sm80.cu(9): here

/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_bwd_kernel.h(1004): error: static assertion failed
          detected during:
            instantiation of "void flash::compute_dq_dk_dv_1colblock<Kernel_traits,Is_dropout,Is_causal,Is_even_MN,Is_even_K,Is_first,Is_last,Seq_parallel,Params>(const Params &, int, int, int) [with Kernel_traits=Flash_bwd_kernel_traits<512, 32, 64, 4, 2, 2, 2, false, true, cutlass::half_t, Flash_kernel_traits<512, 32, 64, 4, cutlass::half_t>>, Is_dropout=true, Is_causal=false, Is_even_MN=true, Is_even_K=false, Is_first=false, Is_last=false, Seq_parallel=true, Params=Flash_bwd_params]" 
(1591): here
            instantiation of "void flash::compute_dq_dk_dv_seqk_parallel<Kernel_traits,Is_dropout,Is_causal,Is_even_MN,Is_even_K,Params>(const Params &) [with Kernel_traits=Flash_bwd_kernel_traits<512, 32, 64, 4, 2, 2, 2, false, true, cutlass::half_t, Flash_kernel_traits<512, 32, 64, 4, cutlass::half_t>>, Is_dropout=true, Is_causal=false, Is_even_MN=true, Is_even_K=false, Params=Flash_bwd_params]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_bwd_launch_template.h(28): here
            instantiation of "void flash_bwd_dq_dk_dv_loop_seqk_parallel_kernel<Kernel_traits,Is_dropout,Is_causal,Is_even_MN,Is_even_K>(Flash_bwd_params) [with Kernel_traits=Flash_bwd_kernel_traits<512, 32, 64, 4, 2, 2, 2, false, true, cutlass::half_t, Flash_kernel_traits<512, 32, 64, 4, cutlass::half_t>>, Is_dropout=true, Is_causal=false, Is_even_MN=true, Is_even_K=false]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_bwd_launch_template.h(62): here
            instantiation of "void run_flash_bwd_seqk_parallel<Kernel_traits,Is_dropout>(Flash_bwd_params &, cudaStream_t, __nv_bool) [with Kernel_traits=Flash_bwd_kernel_traits<512, 32, 64, 4, 2, 2, 2, false, true, cutlass::half_t, Flash_kernel_traits<512, 32, 64, 4, cutlass::half_t>>, Is_dropout=true]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_bwd_launch_template.h(134): here
            instantiation of "void run_flash_bwd<Kernel_traits,Is_dropout>(Flash_bwd_params &, cudaStream_t, __nv_bool) [with Kernel_traits=Flash_bwd_kernel_traits<512, 32, 64, 4, 2, 2, 2, false, true, cutlass::half_t, Flash_kernel_traits<512, 32, 64, 4, cutlass::half_t>>, Is_dropout=true]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_bwd_launch_template.h(365): here
            instantiation of "void run_mha_bwd_hdim512<T>(Flash_bwd_params &, cudaStream_t, __nv_bool) [with T=cutlass::half_t]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_bwd_hdim512_fp16_sm80.cu(9): here

/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_bwd_kernel.h(1004): error: static assertion failed
          detected during:
            instantiation of "void flash::compute_dq_dk_dv_1colblock<Kernel_traits,Is_dropout,Is_causal,Is_even_MN,Is_even_K,Is_first,Is_last,Seq_parallel,Params>(const Params &, int, int, int) [with Kernel_traits=Flash_bwd_kernel_traits<512, 32, 64, 4, 2, 2, 2, false, true, cutlass::half_t, Flash_kernel_traits<512, 32, 64, 4, cutlass::half_t>>, Is_dropout=true, Is_causal=false, Is_even_MN=false, Is_even_K=true, Is_first=false, Is_last=false, Seq_parallel=true, Params=Flash_bwd_params]" 
(1591): here
            instantiation of "void flash::compute_dq_dk_dv_seqk_parallel<Kernel_traits,Is_dropout,Is_causal,Is_even_MN,Is_even_K,Params>(const Params &) [with Kernel_traits=Flash_bwd_kernel_traits<512, 32, 64, 4, 2, 2, 2, false, true, cutlass::half_t, Flash_kernel_traits<512, 32, 64, 4, cutlass::half_t>>, Is_dropout=true, Is_causal=false, Is_even_MN=false, Is_even_K=true, Params=Flash_bwd_params]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_bwd_launch_template.h(28): here
            instantiation of "void flash_bwd_dq_dk_dv_loop_seqk_parallel_kernel<Kernel_traits,Is_dropout,Is_causal,Is_even_MN,Is_even_K>(Flash_bwd_params) [with Kernel_traits=Flash_bwd_kernel_traits<512, 32, 64, 4, 2, 2, 2, false, true, cutlass::half_t, Flash_kernel_traits<512, 32, 64, 4, cutlass::half_t>>, Is_dropout=true, Is_causal=false, Is_even_MN=false, Is_even_K=true]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_bwd_launch_template.h(62): here
            instantiation of "void run_flash_bwd_seqk_parallel<Kernel_traits,Is_dropout>(Flash_bwd_params &, cudaStream_t, __nv_bool) [with Kernel_traits=Flash_bwd_kernel_traits<512, 32, 64, 4, 2, 2, 2, false, true, cutlass::half_t, Flash_kernel_traits<512, 32, 64, 4, cutlass::half_t>>, Is_dropout=true]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_bwd_launch_template.h(134): here
            instantiation of "void run_flash_bwd<Kernel_traits,Is_dropout>(Flash_bwd_params &, cudaStream_t, __nv_bool) [with Kernel_traits=Flash_bwd_kernel_traits<512, 32, 64, 4, 2, 2, 2, false, true, cutlass::half_t, Flash_kernel_traits<512, 32, 64, 4, cutlass::half_t>>, Is_dropout=true]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_bwd_launch_template.h(365): here
            instantiation of "void run_mha_bwd_hdim512<T>(Flash_bwd_params &, cudaStream_t, __nv_bool) [with T=cutlass::half_t]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_bwd_hdim512_fp16_sm80.cu(9): here

/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_bwd_kernel.h(1004): error: static assertion failed
          detected during:
            instantiation of "void flash::compute_dq_dk_dv_1colblock<Kernel_traits,Is_dropout,Is_causal,Is_even_MN,Is_even_K,Is_first,Is_last,Seq_parallel,Params>(const Params &, int, int, int) [with Kernel_traits=Flash_bwd_kernel_traits<512, 32, 64, 4, 2, 2, 2, false, true, cutlass::half_t, Flash_kernel_traits<512, 32, 64, 4, cutlass::half_t>>, Is_dropout=true, Is_causal=false, Is_even_MN=false, Is_even_K=false, Is_first=false, Is_last=false, Seq_parallel=true, Params=Flash_bwd_params]" 
(1591): here
            instantiation of "void flash::compute_dq_dk_dv_seqk_parallel<Kernel_traits,Is_dropout,Is_causal,Is_even_MN,Is_even_K,Params>(const Params &) [with Kernel_traits=Flash_bwd_kernel_traits<512, 32, 64, 4, 2, 2, 2, false, true, cutlass::half_t, Flash_kernel_traits<512, 32, 64, 4, cutlass::half_t>>, Is_dropout=true, Is_causal=false, Is_even_MN=false, Is_even_K=false, Params=Flash_bwd_params]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_bwd_launch_template.h(28): here
            instantiation of "void flash_bwd_dq_dk_dv_loop_seqk_parallel_kernel<Kernel_traits,Is_dropout,Is_causal,Is_even_MN,Is_even_K>(Flash_bwd_params) [with Kernel_traits=Flash_bwd_kernel_traits<512, 32, 64, 4, 2, 2, 2, false, true, cutlass::half_t, Flash_kernel_traits<512, 32, 64, 4, cutlass::half_t>>, Is_dropout=true, Is_causal=false, Is_even_MN=false, Is_even_K=false]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_bwd_launch_template.h(62): here
            instantiation of "void run_flash_bwd_seqk_parallel<Kernel_traits,Is_dropout>(Flash_bwd_params &, cudaStream_t, __nv_bool) [with Kernel_traits=Flash_bwd_kernel_traits<512, 32, 64, 4, 2, 2, 2, false, true, cutlass::half_t, Flash_kernel_traits<512, 32, 64, 4, cutlass::half_t>>, Is_dropout=true]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_bwd_launch_template.h(134): here
            instantiation of "void run_flash_bwd<Kernel_traits,Is_dropout>(Flash_bwd_params &, cudaStream_t, __nv_bool) [with Kernel_traits=Flash_bwd_kernel_traits<512, 32, 64, 4, 2, 2, 2, false, true, cutlass::half_t, Flash_kernel_traits<512, 32, 64, 4, cutlass::half_t>>, Is_dropout=true]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_bwd_launch_template.h(365): here
            instantiation of "void run_mha_bwd_hdim512<T>(Flash_bwd_params &, cudaStream_t, __nv_bool) [with T=cutlass::half_t]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_bwd_hdim512_fp16_sm80.cu(9): here

/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_bwd_kernel.h(287): error: static assertion failed
          detected during:
            instantiation of "void flash::convert_dQ<Kernel_traits,Params>(const Params &) [with Kernel_traits=Flash_bwd_kernel_traits<512, 32, 64, 4, 2, 2, 2, false, true, cutlass::half_t, Flash_kernel_traits<512, 32, 64, 4, cutlass::half_t>>, Params=Flash_bwd_params]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_bwd_launch_template.h(38): here
            instantiation of "void flash_bwd_convert_dq_kernel<Kernel_traits>(Flash_bwd_params) [with Kernel_traits=Flash_bwd_kernel_traits<512, 32, 64, 4, 2, 2, 2, false, true, cutlass::half_t, Flash_kernel_traits<512, 32, 64, 4, cutlass::half_t>>]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_bwd_launch_template.h(77): here
            instantiation of "void run_flash_bwd_seqk_parallel<Kernel_traits,Is_dropout>(Flash_bwd_params &, cudaStream_t, __nv_bool) [with Kernel_traits=Flash_bwd_kernel_traits<512, 32, 64, 4, 2, 2, 2, false, true, cutlass::half_t, Flash_kernel_traits<512, 32, 64, 4, cutlass::half_t>>, Is_dropout=true]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_bwd_launch_template.h(134): here
            instantiation of "void run_flash_bwd<Kernel_traits,Is_dropout>(Flash_bwd_params &, cudaStream_t, __nv_bool) [with Kernel_traits=Flash_bwd_kernel_traits<512, 32, 64, 4, 2, 2, 2, false, true, cutlass::half_t, Flash_kernel_traits<512, 32, 64, 4, cutlass::half_t>>, Is_dropout=true]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_bwd_launch_template.h(365): here
            instantiation of "void run_mha_bwd_hdim512<T>(Flash_bwd_params &, cudaStream_t, __nv_bool) [with T=cutlass::half_t]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_bwd_hdim512_fp16_sm80.cu(9): here

/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_bwd_kernel.h(1004): error: static assertion failed
          detected during:
            instantiation of "void flash::compute_dq_dk_dv_1colblock<Kernel_traits,Is_dropout,Is_causal,Is_even_MN,Is_even_K,Is_first,Is_last,Seq_parallel,Params>(const Params &, int, int, int) [with Kernel_traits=Flash_bwd_kernel_traits<512, 32, 64, 4, 2, 2, 2, false, false, cutlass::half_t, Flash_kernel_traits<512, 32, 64, 4, cutlass::half_t>>, Is_dropout=false, Is_causal=true, Is_even_MN=true, Is_even_K=true, Is_first=false, Is_last=false, Seq_parallel=true, Params=Flash_bwd_params]" 
(1591): here
            instantiation of "void flash::compute_dq_dk_dv_seqk_parallel<Kernel_traits,Is_dropout,Is_causal,Is_even_MN,Is_even_K,Params>(const Params &) [with Kernel_traits=Flash_bwd_kernel_traits<512, 32, 64, 4, 2, 2, 2, false, false, cutlass::half_t, Flash_kernel_traits<512, 32, 64, 4, cutlass::half_t>>, Is_dropout=false, Is_causal=true, Is_even_MN=true, Is_even_K=true, Params=Flash_bwd_params]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_bwd_launch_template.h(28): here
            instantiation of "void flash_bwd_dq_dk_dv_loop_seqk_parallel_kernel<Kernel_traits,Is_dropout,Is_causal,Is_even_MN,Is_even_K>(Flash_bwd_params) [with Kernel_traits=Flash_bwd_kernel_traits<512, 32, 64, 4, 2, 2, 2, false, false, cutlass::half_t, Flash_kernel_traits<512, 32, 64, 4, cutlass::half_t>>, Is_dropout=false, Is_causal=true, Is_even_MN=true, Is_even_K=true]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_bwd_launch_template.h(62): here
            instantiation of "void run_flash_bwd_seqk_parallel<Kernel_traits,Is_dropout>(Flash_bwd_params &, cudaStream_t, __nv_bool) [with Kernel_traits=Flash_bwd_kernel_traits<512, 32, 64, 4, 2, 2, 2, false, false, cutlass::half_t, Flash_kernel_traits<512, 32, 64, 4, cutlass::half_t>>, Is_dropout=false]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_bwd_launch_template.h(134): here
            instantiation of "void run_flash_bwd<Kernel_traits,Is_dropout>(Flash_bwd_params &, cudaStream_t, __nv_bool) [with Kernel_traits=Flash_bwd_kernel_traits<512, 32, 64, 4, 2, 2, 2, false, false, cutlass::half_t, Flash_kernel_traits<512, 32, 64, 4, cutlass::half_t>>, Is_dropout=false]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_bwd_launch_template.h(365): here
            instantiation of "void run_mha_bwd_hdim512<T>(Flash_bwd_params &, cudaStream_t, __nv_bool) [with T=cutlass::half_t]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_bwd_hdim512_fp16_sm80.cu(9): here

/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_bwd_kernel.h(1004): error: static assertion failed
          detected during:
            instantiation of "void flash::compute_dq_dk_dv_1colblock<Kernel_traits,Is_dropout,Is_causal,Is_even_MN,Is_even_K,Is_first,Is_last,Seq_parallel,Params>(const Params &, int, int, int) [with Kernel_traits=Flash_bwd_kernel_traits<512, 32, 64, 4, 2, 2, 2, false, false, cutlass::half_t, Flash_kernel_traits<512, 32, 64, 4, cutlass::half_t>>, Is_dropout=false, Is_causal=true, Is_even_MN=true, Is_even_K=false, Is_first=false, Is_last=false, Seq_parallel=true, Params=Flash_bwd_params]" 
(1591): here
            instantiation of "void flash::compute_dq_dk_dv_seqk_parallel<Kernel_traits,Is_dropout,Is_causal,Is_even_MN,Is_even_K,Params>(const Params &) [with Kernel_traits=Flash_bwd_kernel_traits<512, 32, 64, 4, 2, 2, 2, false, false, cutlass::half_t, Flash_kernel_traits<512, 32, 64, 4, cutlass::half_t>>, Is_dropout=false, Is_causal=true, Is_even_MN=true, Is_even_K=false, Params=Flash_bwd_params]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_bwd_launch_template.h(28): here
            instantiation of "void flash_bwd_dq_dk_dv_loop_seqk_parallel_kernel<Kernel_traits,Is_dropout,Is_causal,Is_even_MN,Is_even_K>(Flash_bwd_params) [with Kernel_traits=Flash_bwd_kernel_traits<512, 32, 64, 4, 2, 2, 2, false, false, cutlass::half_t, Flash_kernel_traits<512, 32, 64, 4, cutlass::half_t>>, Is_dropout=false, Is_causal=true, Is_even_MN=true, Is_even_K=false]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_bwd_launch_template.h(62): here
            instantiation of "void run_flash_bwd_seqk_parallel<Kernel_traits,Is_dropout>(Flash_bwd_params &, cudaStream_t, __nv_bool) [with Kernel_traits=Flash_bwd_kernel_traits<512, 32, 64, 4, 2, 2, 2, false, false, cutlass::half_t, Flash_kernel_traits<512, 32, 64, 4, cutlass::half_t>>, Is_dropout=false]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_bwd_launch_template.h(134): here
            instantiation of "void run_flash_bwd<Kernel_traits,Is_dropout>(Flash_bwd_params &, cudaStream_t, __nv_bool) [with Kernel_traits=Flash_bwd_kernel_traits<512, 32, 64, 4, 2, 2, 2, false, false, cutlass::half_t, Flash_kernel_traits<512, 32, 64, 4, cutlass::half_t>>, Is_dropout=false]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_bwd_launch_template.h(365): here
            instantiation of "void run_mha_bwd_hdim512<T>(Flash_bwd_params &, cudaStream_t, __nv_bool) [with T=cutlass::half_t]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_bwd_hdim512_fp16_sm80.cu(9): here

/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_bwd_kernel.h(1004): error: static assertion failed
          detected during:
            instantiation of "void flash::compute_dq_dk_dv_1colblock<Kernel_traits,Is_dropout,Is_causal,Is_even_MN,Is_even_K,Is_first,Is_last,Seq_parallel,Params>(const Params &, int, int, int) [with Kernel_traits=Flash_bwd_kernel_traits<512, 32, 64, 4, 2, 2, 2, false, false, cutlass::half_t, Flash_kernel_traits<512, 32, 64, 4, cutlass::half_t>>, Is_dropout=false, Is_causal=true, Is_even_MN=false, Is_even_K=true, Is_first=false, Is_last=false, Seq_parallel=true, Params=Flash_bwd_params]" 
(1591): here
            instantiation of "void flash::compute_dq_dk_dv_seqk_parallel<Kernel_traits,Is_dropout,Is_causal,Is_even_MN,Is_even_K,Params>(const Params &) [with Kernel_traits=Flash_bwd_kernel_traits<512, 32, 64, 4, 2, 2, 2, false, false, cutlass::half_t, Flash_kernel_traits<512, 32, 64, 4, cutlass::half_t>>, Is_dropout=false, Is_causal=true, Is_even_MN=false, Is_even_K=true, Params=Flash_bwd_params]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_bwd_launch_template.h(28): here
            instantiation of "void flash_bwd_dq_dk_dv_loop_seqk_parallel_kernel<Kernel_traits,Is_dropout,Is_causal,Is_even_MN,Is_even_K>(Flash_bwd_params) [with Kernel_traits=Flash_bwd_kernel_traits<512, 32, 64, 4, 2, 2, 2, false, false, cutlass::half_t, Flash_kernel_traits<512, 32, 64, 4, cutlass::half_t>>, Is_dropout=false, Is_causal=true, Is_even_MN=false, Is_even_K=true]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_bwd_launch_template.h(62): here
            instantiation of "void run_flash_bwd_seqk_parallel<Kernel_traits,Is_dropout>(Flash_bwd_params &, cudaStream_t, __nv_bool) [with Kernel_traits=Flash_bwd_kernel_traits<512, 32, 64, 4, 2, 2, 2, false, false, cutlass::half_t, Flash_kernel_traits<512, 32, 64, 4, cutlass::half_t>>, Is_dropout=false]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_bwd_launch_template.h(134): here
            instantiation of "void run_flash_bwd<Kernel_traits,Is_dropout>(Flash_bwd_params &, cudaStream_t, __nv_bool) [with Kernel_traits=Flash_bwd_kernel_traits<512, 32, 64, 4, 2, 2, 2, false, false, cutlass::half_t, Flash_kernel_traits<512, 32, 64, 4, cutlass::half_t>>, Is_dropout=false]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_bwd_launch_template.h(365): here
            instantiation of "void run_mha_bwd_hdim512<T>(Flash_bwd_params &, cudaStream_t, __nv_bool) [with T=cutlass::half_t]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_bwd_hdim512_fp16_sm80.cu(9): here

/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_bwd_kernel.h(1004): error: static assertion failed
          detected during:
            instantiation of "void flash::compute_dq_dk_dv_1colblock<Kernel_traits,Is_dropout,Is_causal,Is_even_MN,Is_even_K,Is_first,Is_last,Seq_parallel,Params>(const Params &, int, int, int) [with Kernel_traits=Flash_bwd_kernel_traits<512, 32, 64, 4, 2, 2, 2, false, false, cutlass::half_t, Flash_kernel_traits<512, 32, 64, 4, cutlass::half_t>>, Is_dropout=false, Is_causal=true, Is_even_MN=false, Is_even_K=false, Is_first=false, Is_last=false, Seq_parallel=true, Params=Flash_bwd_params]" 
(1591): here
            instantiation of "void flash::compute_dq_dk_dv_seqk_parallel<Kernel_traits,Is_dropout,Is_causal,Is_even_MN,Is_even_K,Params>(const Params &) [with Kernel_traits=Flash_bwd_kernel_traits<512, 32, 64, 4, 2, 2, 2, false, false, cutlass::half_t, Flash_kernel_traits<512, 32, 64, 4, cutlass::half_t>>, Is_dropout=false, Is_causal=true, Is_even_MN=false, Is_even_K=false, Params=Flash_bwd_params]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_bwd_launch_template.h(28): here
            instantiation of "void flash_bwd_dq_dk_dv_loop_seqk_parallel_kernel<Kernel_traits,Is_dropout,Is_causal,Is_even_MN,Is_even_K>(Flash_bwd_params) [with Kernel_traits=Flash_bwd_kernel_traits<512, 32, 64, 4, 2, 2, 2, false, false, cutlass::half_t, Flash_kernel_traits<512, 32, 64, 4, cutlass::half_t>>, Is_dropout=false, Is_causal=true, Is_even_MN=false, Is_even_K=false]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_bwd_launch_template.h(62): here
            instantiation of "void run_flash_bwd_seqk_parallel<Kernel_traits,Is_dropout>(Flash_bwd_params &, cudaStream_t, __nv_bool) [with Kernel_traits=Flash_bwd_kernel_traits<512, 32, 64, 4, 2, 2, 2, false, false, cutlass::half_t, Flash_kernel_traits<512, 32, 64, 4, cutlass::half_t>>, Is_dropout=false]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_bwd_launch_template.h(134): here
            instantiation of "void run_flash_bwd<Kernel_traits,Is_dropout>(Flash_bwd_params &, cudaStream_t, __nv_bool) [with Kernel_traits=Flash_bwd_kernel_traits<512, 32, 64, 4, 2, 2, 2, false, false, cutlass::half_t, Flash_kernel_traits<512, 32, 64, 4, cutlass::half_t>>, Is_dropout=false]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_bwd_launch_template.h(365): here
            instantiation of "void run_mha_bwd_hdim512<T>(Flash_bwd_params &, cudaStream_t, __nv_bool) [with T=cutlass::half_t]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_bwd_hdim512_fp16_sm80.cu(9): here

/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_bwd_kernel.h(1004): error: static assertion failed
          detected during:
            instantiation of "void flash::compute_dq_dk_dv_1colblock<Kernel_traits,Is_dropout,Is_causal,Is_even_MN,Is_even_K,Is_first,Is_last,Seq_parallel,Params>(const Params &, int, int, int) [with Kernel_traits=Flash_bwd_kernel_traits<512, 32, 64, 4, 2, 2, 2, false, false, cutlass::half_t, Flash_kernel_traits<512, 32, 64, 4, cutlass::half_t>>, Is_dropout=false, Is_causal=false, Is_even_MN=true, Is_even_K=true, Is_first=false, Is_last=false, Seq_parallel=true, Params=Flash_bwd_params]" 
(1591): here
            instantiation of "void flash::compute_dq_dk_dv_seqk_parallel<Kernel_traits,Is_dropout,Is_causal,Is_even_MN,Is_even_K,Params>(const Params &) [with Kernel_traits=Flash_bwd_kernel_traits<512, 32, 64, 4, 2, 2, 2, false, false, cutlass::half_t, Flash_kernel_traits<512, 32, 64, 4, cutlass::half_t>>, Is_dropout=false, Is_causal=false, Is_even_MN=true, Is_even_K=true, Params=Flash_bwd_params]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_bwd_launch_template.h(28): here
            instantiation of "void flash_bwd_dq_dk_dv_loop_seqk_parallel_kernel<Kernel_traits,Is_dropout,Is_causal,Is_even_MN,Is_even_K>(Flash_bwd_params) [with Kernel_traits=Flash_bwd_kernel_traits<512, 32, 64, 4, 2, 2, 2, false, false, cutlass::half_t, Flash_kernel_traits<512, 32, 64, 4, cutlass::half_t>>, Is_dropout=false, Is_causal=false, Is_even_MN=true, Is_even_K=true]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_bwd_launch_template.h(62): here
            instantiation of "void run_flash_bwd_seqk_parallel<Kernel_traits,Is_dropout>(Flash_bwd_params &, cudaStream_t, __nv_bool) [with Kernel_traits=Flash_bwd_kernel_traits<512, 32, 64, 4, 2, 2, 2, false, false, cutlass::half_t, Flash_kernel_traits<512, 32, 64, 4, cutlass::half_t>>, Is_dropout=false]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_bwd_launch_template.h(134): here
            instantiation of "void run_flash_bwd<Kernel_traits,Is_dropout>(Flash_bwd_params &, cudaStream_t, __nv_bool) [with Kernel_traits=Flash_bwd_kernel_traits<512, 32, 64, 4, 2, 2, 2, false, false, cutlass::half_t, Flash_kernel_traits<512, 32, 64, 4, cutlass::half_t>>, Is_dropout=false]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_bwd_launch_template.h(365): here
            instantiation of "void run_mha_bwd_hdim512<T>(Flash_bwd_params &, cudaStream_t, __nv_bool) [with T=cutlass::half_t]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_bwd_hdim512_fp16_sm80.cu(9): here

/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_bwd_kernel.h(1004): error: static assertion failed
          detected during:
            instantiation of "void flash::compute_dq_dk_dv_1colblock<Kernel_traits,Is_dropout,Is_causal,Is_even_MN,Is_even_K,Is_first,Is_last,Seq_parallel,Params>(const Params &, int, int, int) [with Kernel_traits=Flash_bwd_kernel_traits<512, 32, 64, 4, 2, 2, 2, false, false, cutlass::half_t, Flash_kernel_traits<512, 32, 64, 4, cutlass::half_t>>, Is_dropout=false, Is_causal=false, Is_even_MN=true, Is_even_K=false, Is_first=false, Is_last=false, Seq_parallel=true, Params=Flash_bwd_params]" 
(1591): here
            instantiation of "void flash::compute_dq_dk_dv_seqk_parallel<Kernel_traits,Is_dropout,Is_causal,Is_even_MN,Is_even_K,Params>(const Params &) [with Kernel_traits=Flash_bwd_kernel_traits<512, 32, 64, 4, 2, 2, 2, false, false, cutlass::half_t, Flash_kernel_traits<512, 32, 64, 4, cutlass::half_t>>, Is_dropout=false, Is_causal=false, Is_even_MN=true, Is_even_K=false, Params=Flash_bwd_params]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_bwd_launch_template.h(28): here
            instantiation of "void flash_bwd_dq_dk_dv_loop_seqk_parallel_kernel<Kernel_traits,Is_dropout,Is_causal,Is_even_MN,Is_even_K>(Flash_bwd_params) [with Kernel_traits=Flash_bwd_kernel_traits<512, 32, 64, 4, 2, 2, 2, false, false, cutlass::half_t, Flash_kernel_traits<512, 32, 64, 4, cutlass::half_t>>, Is_dropout=false, Is_causal=false, Is_even_MN=true, Is_even_K=false]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_bwd_launch_template.h(62): here
            instantiation of "void run_flash_bwd_seqk_parallel<Kernel_traits,Is_dropout>(Flash_bwd_params &, cudaStream_t, __nv_bool) [with Kernel_traits=Flash_bwd_kernel_traits<512, 32, 64, 4, 2, 2, 2, false, false, cutlass::half_t, Flash_kernel_traits<512, 32, 64, 4, cutlass::half_t>>, Is_dropout=false]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_bwd_launch_template.h(134): here
            instantiation of "void run_flash_bwd<Kernel_traits,Is_dropout>(Flash_bwd_params &, cudaStream_t, __nv_bool) [with Kernel_traits=Flash_bwd_kernel_traits<512, 32, 64, 4, 2, 2, 2, false, false, cutlass::half_t, Flash_kernel_traits<512, 32, 64, 4, cutlass::half_t>>, Is_dropout=false]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_bwd_launch_template.h(365): here
            instantiation of "void run_mha_bwd_hdim512<T>(Flash_bwd_params &, cudaStream_t, __nv_bool) [with T=cutlass::half_t]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_bwd_hdim512_fp16_sm80.cu(9): here

/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_bwd_kernel.h(1004): error: static assertion failed
          detected during:
            instantiation of "void flash::compute_dq_dk_dv_1colblock<Kernel_traits,Is_dropout,Is_causal,Is_even_MN,Is_even_K,Is_first,Is_last,Seq_parallel,Params>(const Params &, int, int, int) [with Kernel_traits=Flash_bwd_kernel_traits<512, 32, 64, 4, 2, 2, 2, false, false, cutlass::half_t, Flash_kernel_traits<512, 32, 64, 4, cutlass::half_t>>, Is_dropout=false, Is_causal=false, Is_even_MN=false, Is_even_K=true, Is_first=false, Is_last=false, Seq_parallel=true, Params=Flash_bwd_params]" 
(1591): here
            instantiation of "void flash::compute_dq_dk_dv_seqk_parallel<Kernel_traits,Is_dropout,Is_causal,Is_even_MN,Is_even_K,Params>(const Params &) [with Kernel_traits=Flash_bwd_kernel_traits<512, 32, 64, 4, 2, 2, 2, false, false, cutlass::half_t, Flash_kernel_traits<512, 32, 64, 4, cutlass::half_t>>, Is_dropout=false, Is_causal=false, Is_even_MN=false, Is_even_K=true, Params=Flash_bwd_params]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_bwd_launch_template.h(28): here
            instantiation of "void flash_bwd_dq_dk_dv_loop_seqk_parallel_kernel<Kernel_traits,Is_dropout,Is_causal,Is_even_MN,Is_even_K>(Flash_bwd_params) [with Kernel_traits=Flash_bwd_kernel_traits<512, 32, 64, 4, 2, 2, 2, false, false, cutlass::half_t, Flash_kernel_traits<512, 32, 64, 4, cutlass::half_t>>, Is_dropout=false, Is_causal=false, Is_even_MN=false, Is_even_K=true]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_bwd_launch_template.h(62): here
            instantiation of "void run_flash_bwd_seqk_parallel<Kernel_traits,Is_dropout>(Flash_bwd_params &, cudaStream_t, __nv_bool) [with Kernel_traits=Flash_bwd_kernel_traits<512, 32, 64, 4, 2, 2, 2, false, false, cutlass::half_t, Flash_kernel_traits<512, 32, 64, 4, cutlass::half_t>>, Is_dropout=false]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_bwd_launch_template.h(134): here
            instantiation of "void run_flash_bwd<Kernel_traits,Is_dropout>(Flash_bwd_params &, cudaStream_t, __nv_bool) [with Kernel_traits=Flash_bwd_kernel_traits<512, 32, 64, 4, 2, 2, 2, false, false, cutlass::half_t, Flash_kernel_traits<512, 32, 64, 4, cutlass::half_t>>, Is_dropout=false]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_bwd_launch_template.h(365): here
            instantiation of "void run_mha_bwd_hdim512<T>(Flash_bwd_params &, cudaStream_t, __nv_bool) [with T=cutlass::half_t]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_bwd_hdim512_fp16_sm80.cu(9): here

/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_bwd_kernel.h(1004): error: static assertion failed
          detected during:
            instantiation of "void flash::compute_dq_dk_dv_1colblock<Kernel_traits,Is_dropout,Is_causal,Is_even_MN,Is_even_K,Is_first,Is_last,Seq_parallel,Params>(const Params &, int, int, int) [with Kernel_traits=Flash_bwd_kernel_traits<512, 32, 64, 4, 2, 2, 2, false, false, cutlass::half_t, Flash_kernel_traits<512, 32, 64, 4, cutlass::half_t>>, Is_dropout=false, Is_causal=false, Is_even_MN=false, Is_even_K=false, Is_first=false, Is_last=false, Seq_parallel=true, Params=Flash_bwd_params]" 
(1591): here
            instantiation of "void flash::compute_dq_dk_dv_seqk_parallel<Kernel_traits,Is_dropout,Is_causal,Is_even_MN,Is_even_K,Params>(const Params &) [with Kernel_traits=Flash_bwd_kernel_traits<512, 32, 64, 4, 2, 2, 2, false, false, cutlass::half_t, Flash_kernel_traits<512, 32, 64, 4, cutlass::half_t>>, Is_dropout=false, Is_causal=false, Is_even_MN=false, Is_even_K=false, Params=Flash_bwd_params]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_bwd_launch_template.h(28): here
            instantiation of "void flash_bwd_dq_dk_dv_loop_seqk_parallel_kernel<Kernel_traits,Is_dropout,Is_causal,Is_even_MN,Is_even_K>(Flash_bwd_params) [with Kernel_traits=Flash_bwd_kernel_traits<512, 32, 64, 4, 2, 2, 2, false, false, cutlass::half_t, Flash_kernel_traits<512, 32, 64, 4, cutlass::half_t>>, Is_dropout=false, Is_causal=false, Is_even_MN=false, Is_even_K=false]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_bwd_launch_template.h(62): here
            instantiation of "void run_flash_bwd_seqk_parallel<Kernel_traits,Is_dropout>(Flash_bwd_params &, cudaStream_t, __nv_bool) [with Kernel_traits=Flash_bwd_kernel_traits<512, 32, 64, 4, 2, 2, 2, false, false, cutlass::half_t, Flash_kernel_traits<512, 32, 64, 4, cutlass::half_t>>, Is_dropout=false]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_bwd_launch_template.h(134): here
            instantiation of "void run_flash_bwd<Kernel_traits,Is_dropout>(Flash_bwd_params &, cudaStream_t, __nv_bool) [with Kernel_traits=Flash_bwd_kernel_traits<512, 32, 64, 4, 2, 2, 2, false, false, cutlass::half_t, Flash_kernel_traits<512, 32, 64, 4, cutlass::half_t>>, Is_dropout=false]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_bwd_launch_template.h(365): here
            instantiation of "void run_mha_bwd_hdim512<T>(Flash_bwd_params &, cudaStream_t, __nv_bool) [with T=cutlass::half_t]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_bwd_hdim512_fp16_sm80.cu(9): here

/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_bwd_kernel.h(1004): error: static assertion failed
          detected during:
            instantiation of "void flash::compute_dq_dk_dv_1colblock<Kernel_traits,Is_dropout,Is_causal,Is_even_MN,Is_even_K,Is_first,Is_last,Seq_parallel,Params>(const Params &, int, int, int) [with Kernel_traits=Flash_bwd_kernel_traits<512, 32, 64, 4, 2, 2, 2, false, true, cutlass::half_t, Flash_kernel_traits<512, 32, 64, 4, cutlass::half_t>>, Is_dropout=false, Is_causal=true, Is_even_MN=true, Is_even_K=true, Is_first=false, Is_last=false, Seq_parallel=true, Params=Flash_bwd_params]" 
(1591): here
            instantiation of "void flash::compute_dq_dk_dv_seqk_parallel<Kernel_traits,Is_dropout,Is_causal,Is_even_MN,Is_even_K,Params>(const Params &) [with Kernel_traits=Flash_bwd_kernel_traits<512, 32, 64, 4, 2, 2, 2, false, true, cutlass::half_t, Flash_kernel_traits<512, 32, 64, 4, cutlass::half_t>>, Is_dropout=false, Is_causal=true, Is_even_MN=true, Is_even_K=true, Params=Flash_bwd_params]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_bwd_launch_template.h(28): here
            instantiation of "void flash_bwd_dq_dk_dv_loop_seqk_parallel_kernel<Kernel_traits,Is_dropout,Is_causal,Is_even_MN,Is_even_K>(Flash_bwd_params) [with Kernel_traits=Flash_bwd_kernel_traits<512, 32, 64, 4, 2, 2, 2, false, true, cutlass::half_t, Flash_kernel_traits<512, 32, 64, 4, cutlass::half_t>>, Is_dropout=false, Is_causal=true, Is_even_MN=true, Is_even_K=true]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_bwd_launch_template.h(62): here
            instantiation of "void run_flash_bwd_seqk_parallel<Kernel_traits,Is_dropout>(Flash_bwd_params &, cudaStream_t, __nv_bool) [with Kernel_traits=Flash_bwd_kernel_traits<512, 32, 64, 4, 2, 2, 2, false, true, cutlass::half_t, Flash_kernel_traits<512, 32, 64, 4, cutlass::half_t>>, Is_dropout=false]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_bwd_launch_template.h(134): here
            instantiation of "void run_flash_bwd<Kernel_traits,Is_dropout>(Flash_bwd_params &, cudaStream_t, __nv_bool) [with Kernel_traits=Flash_bwd_kernel_traits<512, 32, 64, 4, 2, 2, 2, false, true, cutlass::half_t, Flash_kernel_traits<512, 32, 64, 4, cutlass::half_t>>, Is_dropout=false]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_bwd_launch_template.h(365): here
            instantiation of "void run_mha_bwd_hdim512<T>(Flash_bwd_params &, cudaStream_t, __nv_bool) [with T=cutlass::half_t]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_bwd_hdim512_fp16_sm80.cu(9): here

/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_bwd_kernel.h(1004): error: static assertion failed
          detected during:
            instantiation of "void flash::compute_dq_dk_dv_1colblock<Kernel_traits,Is_dropout,Is_causal,Is_even_MN,Is_even_K,Is_first,Is_last,Seq_parallel,Params>(const Params &, int, int, int) [with Kernel_traits=Flash_bwd_kernel_traits<512, 32, 64, 4, 2, 2, 2, false, true, cutlass::half_t, Flash_kernel_traits<512, 32, 64, 4, cutlass::half_t>>, Is_dropout=false, Is_causal=true, Is_even_MN=true, Is_even_K=false, Is_first=false, Is_last=false, Seq_parallel=true, Params=Flash_bwd_params]" 
(1591): here
            instantiation of "void flash::compute_dq_dk_dv_seqk_parallel<Kernel_traits,Is_dropout,Is_causal,Is_even_MN,Is_even_K,Params>(const Params &) [with Kernel_traits=Flash_bwd_kernel_traits<512, 32, 64, 4, 2, 2, 2, false, true, cutlass::half_t, Flash_kernel_traits<512, 32, 64, 4, cutlass::half_t>>, Is_dropout=false, Is_causal=true, Is_even_MN=true, Is_even_K=false, Params=Flash_bwd_params]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_bwd_launch_template.h(28): here
            instantiation of "void flash_bwd_dq_dk_dv_loop_seqk_parallel_kernel<Kernel_traits,Is_dropout,Is_causal,Is_even_MN,Is_even_K>(Flash_bwd_params) [with Kernel_traits=Flash_bwd_kernel_traits<512, 32, 64, 4, 2, 2, 2, false, true, cutlass::half_t, Flash_kernel_traits<512, 32, 64, 4, cutlass::half_t>>, Is_dropout=false, Is_causal=true, Is_even_MN=true, Is_even_K=false]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_bwd_launch_template.h(62): here
            instantiation of "void run_flash_bwd_seqk_parallel<Kernel_traits,Is_dropout>(Flash_bwd_params &, cudaStream_t, __nv_bool) [with Kernel_traits=Flash_bwd_kernel_traits<512, 32, 64, 4, 2, 2, 2, false, true, cutlass::half_t, Flash_kernel_traits<512, 32, 64, 4, cutlass::half_t>>, Is_dropout=false]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_bwd_launch_template.h(134): here
            instantiation of "void run_flash_bwd<Kernel_traits,Is_dropout>(Flash_bwd_params &, cudaStream_t, __nv_bool) [with Kernel_traits=Flash_bwd_kernel_traits<512, 32, 64, 4, 2, 2, 2, false, true, cutlass::half_t, Flash_kernel_traits<512, 32, 64, 4, cutlass::half_t>>, Is_dropout=false]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_bwd_launch_template.h(365): here
            instantiation of "void run_mha_bwd_hdim512<T>(Flash_bwd_params &, cudaStream_t, __nv_bool) [with T=cutlass::half_t]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_bwd_hdim512_fp16_sm80.cu(9): here

/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_bwd_kernel.h(1004): error: static assertion failed
          detected during:
            instantiation of "void flash::compute_dq_dk_dv_1colblock<Kernel_traits,Is_dropout,Is_causal,Is_even_MN,Is_even_K,Is_first,Is_last,Seq_parallel,Params>(const Params &, int, int, int) [with Kernel_traits=Flash_bwd_kernel_traits<512, 32, 64, 4, 2, 2, 2, false, true, cutlass::half_t, Flash_kernel_traits<512, 32, 64, 4, cutlass::half_t>>, Is_dropout=false, Is_causal=true, Is_even_MN=false, Is_even_K=true, Is_first=false, Is_last=false, Seq_parallel=true, Params=Flash_bwd_params]" 
(1591): here
            instantiation of "void flash::compute_dq_dk_dv_seqk_parallel<Kernel_traits,Is_dropout,Is_causal,Is_even_MN,Is_even_K,Params>(const Params &) [with Kernel_traits=Flash_bwd_kernel_traits<512, 32, 64, 4, 2, 2, 2, false, true, cutlass::half_t, Flash_kernel_traits<512, 32, 64, 4, cutlass::half_t>>, Is_dropout=false, Is_causal=true, Is_even_MN=false, Is_even_K=true, Params=Flash_bwd_params]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_bwd_launch_template.h(28): here
            instantiation of "void flash_bwd_dq_dk_dv_loop_seqk_parallel_kernel<Kernel_traits,Is_dropout,Is_causal,Is_even_MN,Is_even_K>(Flash_bwd_params) [with Kernel_traits=Flash_bwd_kernel_traits<512, 32, 64, 4, 2, 2, 2, false, true, cutlass::half_t, Flash_kernel_traits<512, 32, 64, 4, cutlass::half_t>>, Is_dropout=false, Is_causal=true, Is_even_MN=false, Is_even_K=true]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_bwd_launch_template.h(62): here
            instantiation of "void run_flash_bwd_seqk_parallel<Kernel_traits,Is_dropout>(Flash_bwd_params &, cudaStream_t, __nv_bool) [with Kernel_traits=Flash_bwd_kernel_traits<512, 32, 64, 4, 2, 2, 2, false, true, cutlass::half_t, Flash_kernel_traits<512, 32, 64, 4, cutlass::half_t>>, Is_dropout=false]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_bwd_launch_template.h(134): here
            instantiation of "void run_flash_bwd<Kernel_traits,Is_dropout>(Flash_bwd_params &, cudaStream_t, __nv_bool) [with Kernel_traits=Flash_bwd_kernel_traits<512, 32, 64, 4, 2, 2, 2, false, true, cutlass::half_t, Flash_kernel_traits<512, 32, 64, 4, cutlass::half_t>>, Is_dropout=false]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_bwd_launch_template.h(365): here
            instantiation of "void run_mha_bwd_hdim512<T>(Flash_bwd_params &, cudaStream_t, __nv_bool) [with T=cutlass::half_t]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_bwd_hdim512_fp16_sm80.cu(9): here

/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_bwd_kernel.h(1004): error: static assertion failed
          detected during:
            instantiation of "void flash::compute_dq_dk_dv_1colblock<Kernel_traits,Is_dropout,Is_causal,Is_even_MN,Is_even_K,Is_first,Is_last,Seq_parallel,Params>(const Params &, int, int, int) [with Kernel_traits=Flash_bwd_kernel_traits<512, 32, 64, 4, 2, 2, 2, false, true, cutlass::half_t, Flash_kernel_traits<512, 32, 64, 4, cutlass::half_t>>, Is_dropout=false, Is_causal=true, Is_even_MN=false, Is_even_K=false, Is_first=false, Is_last=false, Seq_parallel=true, Params=Flash_bwd_params]" 
(1591): here
            instantiation of "void flash::compute_dq_dk_dv_seqk_parallel<Kernel_traits,Is_dropout,Is_causal,Is_even_MN,Is_even_K,Params>(const Params &) [with Kernel_traits=Flash_bwd_kernel_traits<512, 32, 64, 4, 2, 2, 2, false, true, cutlass::half_t, Flash_kernel_traits<512, 32, 64, 4, cutlass::half_t>>, Is_dropout=false, Is_causal=true, Is_even_MN=false, Is_even_K=false, Params=Flash_bwd_params]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_bwd_launch_template.h(28): here
            instantiation of "void flash_bwd_dq_dk_dv_loop_seqk_parallel_kernel<Kernel_traits,Is_dropout,Is_causal,Is_even_MN,Is_even_K>(Flash_bwd_params) [with Kernel_traits=Flash_bwd_kernel_traits<512, 32, 64, 4, 2, 2, 2, false, true, cutlass::half_t, Flash_kernel_traits<512, 32, 64, 4, cutlass::half_t>>, Is_dropout=false, Is_causal=true, Is_even_MN=false, Is_even_K=false]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_bwd_launch_template.h(62): here
            instantiation of "void run_flash_bwd_seqk_parallel<Kernel_traits,Is_dropout>(Flash_bwd_params &, cudaStream_t, __nv_bool) [with Kernel_traits=Flash_bwd_kernel_traits<512, 32, 64, 4, 2, 2, 2, false, true, cutlass::half_t, Flash_kernel_traits<512, 32, 64, 4, cutlass::half_t>>, Is_dropout=false]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_bwd_launch_template.h(134): here
            instantiation of "void run_flash_bwd<Kernel_traits,Is_dropout>(Flash_bwd_params &, cudaStream_t, __nv_bool) [with Kernel_traits=Flash_bwd_kernel_traits<512, 32, 64, 4, 2, 2, 2, false, true, cutlass::half_t, Flash_kernel_traits<512, 32, 64, 4, cutlass::half_t>>, Is_dropout=false]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_bwd_launch_template.h(365): here
            instantiation of "void run_mha_bwd_hdim512<T>(Flash_bwd_params &, cudaStream_t, __nv_bool) [with T=cutlass::half_t]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_bwd_hdim512_fp16_sm80.cu(9): here

/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_bwd_kernel.h(1004): error: static assertion failed
          detected during:
            instantiation of "void flash::compute_dq_dk_dv_1colblock<Kernel_traits,Is_dropout,Is_causal,Is_even_MN,Is_even_K,Is_first,Is_last,Seq_parallel,Params>(const Params &, int, int, int) [with Kernel_traits=Flash_bwd_kernel_traits<512, 32, 64, 4, 2, 2, 2, false, true, cutlass::half_t, Flash_kernel_traits<512, 32, 64, 4, cutlass::half_t>>, Is_dropout=false, Is_causal=false, Is_even_MN=true, Is_even_K=true, Is_first=false, Is_last=false, Seq_parallel=true, Params=Flash_bwd_params]" 
(1591): here
            instantiation of "void flash::compute_dq_dk_dv_seqk_parallel<Kernel_traits,Is_dropout,Is_causal,Is_even_MN,Is_even_K,Params>(const Params &) [with Kernel_traits=Flash_bwd_kernel_traits<512, 32, 64, 4, 2, 2, 2, false, true, cutlass::half_t, Flash_kernel_traits<512, 32, 64, 4, cutlass::half_t>>, Is_dropout=false, Is_causal=false, Is_even_MN=true, Is_even_K=true, Params=Flash_bwd_params]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_bwd_launch_template.h(28): here
            instantiation of "void flash_bwd_dq_dk_dv_loop_seqk_parallel_kernel<Kernel_traits,Is_dropout,Is_causal,Is_even_MN,Is_even_K>(Flash_bwd_params) [with Kernel_traits=Flash_bwd_kernel_traits<512, 32, 64, 4, 2, 2, 2, false, true, cutlass::half_t, Flash_kernel_traits<512, 32, 64, 4, cutlass::half_t>>, Is_dropout=false, Is_causal=false, Is_even_MN=true, Is_even_K=true]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_bwd_launch_template.h(62): here
            instantiation of "void run_flash_bwd_seqk_parallel<Kernel_traits,Is_dropout>(Flash_bwd_params &, cudaStream_t, __nv_bool) [with Kernel_traits=Flash_bwd_kernel_traits<512, 32, 64, 4, 2, 2, 2, false, true, cutlass::half_t, Flash_kernel_traits<512, 32, 64, 4, cutlass::half_t>>, Is_dropout=false]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_bwd_launch_template.h(134): here
            instantiation of "void run_flash_bwd<Kernel_traits,Is_dropout>(Flash_bwd_params &, cudaStream_t, __nv_bool) [with Kernel_traits=Flash_bwd_kernel_traits<512, 32, 64, 4, 2, 2, 2, false, true, cutlass::half_t, Flash_kernel_traits<512, 32, 64, 4, cutlass::half_t>>, Is_dropout=false]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_bwd_launch_template.h(365): here
            instantiation of "void run_mha_bwd_hdim512<T>(Flash_bwd_params &, cudaStream_t, __nv_bool) [with T=cutlass::half_t]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_bwd_hdim512_fp16_sm80.cu(9): here

/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_bwd_kernel.h(1004): error: static assertion failed
          detected during:
            instantiation of "void flash::compute_dq_dk_dv_1colblock<Kernel_traits,Is_dropout,Is_causal,Is_even_MN,Is_even_K,Is_first,Is_last,Seq_parallel,Params>(const Params &, int, int, int) [with Kernel_traits=Flash_bwd_kernel_traits<512, 32, 64, 4, 2, 2, 2, false, true, cutlass::half_t, Flash_kernel_traits<512, 32, 64, 4, cutlass::half_t>>, Is_dropout=false, Is_causal=false, Is_even_MN=true, Is_even_K=false, Is_first=false, Is_last=false, Seq_parallel=true, Params=Flash_bwd_params]" 
(1591): here
            instantiation of "void flash::compute_dq_dk_dv_seqk_parallel<Kernel_traits,Is_dropout,Is_causal,Is_even_MN,Is_even_K,Params>(const Params &) [with Kernel_traits=Flash_bwd_kernel_traits<512, 32, 64, 4, 2, 2, 2, false, true, cutlass::half_t, Flash_kernel_traits<512, 32, 64, 4, cutlass::half_t>>, Is_dropout=false, Is_causal=false, Is_even_MN=true, Is_even_K=false, Params=Flash_bwd_params]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_bwd_launch_template.h(28): here
            instantiation of "void flash_bwd_dq_dk_dv_loop_seqk_parallel_kernel<Kernel_traits,Is_dropout,Is_causal,Is_even_MN,Is_even_K>(Flash_bwd_params) [with Kernel_traits=Flash_bwd_kernel_traits<512, 32, 64, 4, 2, 2, 2, false, true, cutlass::half_t, Flash_kernel_traits<512, 32, 64, 4, cutlass::half_t>>, Is_dropout=false, Is_causal=false, Is_even_MN=true, Is_even_K=false]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_bwd_launch_template.h(62): here
            instantiation of "void run_flash_bwd_seqk_parallel<Kernel_traits,Is_dropout>(Flash_bwd_params &, cudaStream_t, __nv_bool) [with Kernel_traits=Flash_bwd_kernel_traits<512, 32, 64, 4, 2, 2, 2, false, true, cutlass::half_t, Flash_kernel_traits<512, 32, 64, 4, cutlass::half_t>>, Is_dropout=false]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_bwd_launch_template.h(134): here
            instantiation of "void run_flash_bwd<Kernel_traits,Is_dropout>(Flash_bwd_params &, cudaStream_t, __nv_bool) [with Kernel_traits=Flash_bwd_kernel_traits<512, 32, 64, 4, 2, 2, 2, false, true, cutlass::half_t, Flash_kernel_traits<512, 32, 64, 4, cutlass::half_t>>, Is_dropout=false]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_bwd_launch_template.h(365): here
            instantiation of "void run_mha_bwd_hdim512<T>(Flash_bwd_params &, cudaStream_t, __nv_bool) [with T=cutlass::half_t]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_bwd_hdim512_fp16_sm80.cu(9): here

/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_bwd_kernel.h(1004): error: static assertion failed
          detected during:
            instantiation of "void flash::compute_dq_dk_dv_1colblock<Kernel_traits,Is_dropout,Is_causal,Is_even_MN,Is_even_K,Is_first,Is_last,Seq_parallel,Params>(const Params &, int, int, int) [with Kernel_traits=Flash_bwd_kernel_traits<512, 32, 64, 4, 2, 2, 2, false, true, cutlass::half_t, Flash_kernel_traits<512, 32, 64, 4, cutlass::half_t>>, Is_dropout=false, Is_causal=false, Is_even_MN=false, Is_even_K=true, Is_first=false, Is_last=false, Seq_parallel=true, Params=Flash_bwd_params]" 
(1591): here
            instantiation of "void flash::compute_dq_dk_dv_seqk_parallel<Kernel_traits,Is_dropout,Is_causal,Is_even_MN,Is_even_K,Params>(const Params &) [with Kernel_traits=Flash_bwd_kernel_traits<512, 32, 64, 4, 2, 2, 2, false, true, cutlass::half_t, Flash_kernel_traits<512, 32, 64, 4, cutlass::half_t>>, Is_dropout=false, Is_causal=false, Is_even_MN=false, Is_even_K=true, Params=Flash_bwd_params]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_bwd_launch_template.h(28): here
            instantiation of "void flash_bwd_dq_dk_dv_loop_seqk_parallel_kernel<Kernel_traits,Is_dropout,Is_causal,Is_even_MN,Is_even_K>(Flash_bwd_params) [with Kernel_traits=Flash_bwd_kernel_traits<512, 32, 64, 4, 2, 2, 2, false, true, cutlass::half_t, Flash_kernel_traits<512, 32, 64, 4, cutlass::half_t>>, Is_dropout=false, Is_causal=false, Is_even_MN=false, Is_even_K=true]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_bwd_launch_template.h(62): here
            instantiation of "void run_flash_bwd_seqk_parallel<Kernel_traits,Is_dropout>(Flash_bwd_params &, cudaStream_t, __nv_bool) [with Kernel_traits=Flash_bwd_kernel_traits<512, 32, 64, 4, 2, 2, 2, false, true, cutlass::half_t, Flash_kernel_traits<512, 32, 64, 4, cutlass::half_t>>, Is_dropout=false]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_bwd_launch_template.h(134): here
            instantiation of "void run_flash_bwd<Kernel_traits,Is_dropout>(Flash_bwd_params &, cudaStream_t, __nv_bool) [with Kernel_traits=Flash_bwd_kernel_traits<512, 32, 64, 4, 2, 2, 2, false, true, cutlass::half_t, Flash_kernel_traits<512, 32, 64, 4, cutlass::half_t>>, Is_dropout=false]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_bwd_launch_template.h(365): here
            instantiation of "void run_mha_bwd_hdim512<T>(Flash_bwd_params &, cudaStream_t, __nv_bool) [with T=cutlass::half_t]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_bwd_hdim512_fp16_sm80.cu(9): here

/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_bwd_kernel.h(1004): error: static assertion failed
          detected during:
            instantiation of "void flash::compute_dq_dk_dv_1colblock<Kernel_traits,Is_dropout,Is_causal,Is_even_MN,Is_even_K,Is_first,Is_last,Seq_parallel,Params>(const Params &, int, int, int) [with Kernel_traits=Flash_bwd_kernel_traits<512, 32, 64, 4, 2, 2, 2, false, true, cutlass::half_t, Flash_kernel_traits<512, 32, 64, 4, cutlass::half_t>>, Is_dropout=false, Is_causal=false, Is_even_MN=false, Is_even_K=false, Is_first=false, Is_last=false, Seq_parallel=true, Params=Flash_bwd_params]" 
(1591): here
            instantiation of "void flash::compute_dq_dk_dv_seqk_parallel<Kernel_traits,Is_dropout,Is_causal,Is_even_MN,Is_even_K,Params>(const Params &) [with Kernel_traits=Flash_bwd_kernel_traits<512, 32, 64, 4, 2, 2, 2, false, true, cutlass::half_t, Flash_kernel_traits<512, 32, 64, 4, cutlass::half_t>>, Is_dropout=false, Is_causal=false, Is_even_MN=false, Is_even_K=false, Params=Flash_bwd_params]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_bwd_launch_template.h(28): here
            instantiation of "void flash_bwd_dq_dk_dv_loop_seqk_parallel_kernel<Kernel_traits,Is_dropout,Is_causal,Is_even_MN,Is_even_K>(Flash_bwd_params) [with Kernel_traits=Flash_bwd_kernel_traits<512, 32, 64, 4, 2, 2, 2, false, true, cutlass::half_t, Flash_kernel_traits<512, 32, 64, 4, cutlass::half_t>>, Is_dropout=false, Is_causal=false, Is_even_MN=false, Is_even_K=false]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_bwd_launch_template.h(62): here
            instantiation of "void run_flash_bwd_seqk_parallel<Kernel_traits,Is_dropout>(Flash_bwd_params &, cudaStream_t, __nv_bool) [with Kernel_traits=Flash_bwd_kernel_traits<512, 32, 64, 4, 2, 2, 2, false, true, cutlass::half_t, Flash_kernel_traits<512, 32, 64, 4, cutlass::half_t>>, Is_dropout=false]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_bwd_launch_template.h(134): here
            instantiation of "void run_flash_bwd<Kernel_traits,Is_dropout>(Flash_bwd_params &, cudaStream_t, __nv_bool) [with Kernel_traits=Flash_bwd_kernel_traits<512, 32, 64, 4, 2, 2, 2, false, true, cutlass::half_t, Flash_kernel_traits<512, 32, 64, 4, cutlass::half_t>>, Is_dropout=false]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_bwd_launch_template.h(365): here
            instantiation of "void run_mha_bwd_hdim512<T>(Flash_bwd_params &, cudaStream_t, __nv_bool) [with T=cutlass::half_t]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_bwd_hdim512_fp16_sm80.cu(9): here

34 errors detected in the compilation of "/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_bwd_hdim512_fp16_sm80.cu".
[3/13] /usr/local/cuda-11.4/bin/nvcc  -I/home/v-feiychen/flash-attention/csrc/flash_attn -I/home/v-feiychen/flash-attention/csrc/flash_attn/src -I/home/v-feiychen/flash-attention/csrc/cutlass/include -I/anaconda/envs/flashattention/lib/python3.9/site-packages/torch/include -I/anaconda/envs/flashattention/lib/python3.9/site-packages/torch/include/torch/csrc/api/include -I/anaconda/envs/flashattention/lib/python3.9/site-packages/torch/include/TH -I/anaconda/envs/flashattention/lib/python3.9/site-packages/torch/include/THC -I/usr/local/cuda-11.4/include -I/anaconda/envs/flashattention/include/python3.9 -c -c /home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_hdim32_bf16_sm80.cu -o /home/v-feiychen/flash-attention/build/temp.linux-x86_64-cpython-39/csrc/flash_attn/src/flash_fwd_hdim32_bf16_sm80.o -D__CUDA_NO_HALF_OPERATORS__ -D__CUDA_NO_HALF_CONVERSIONS__ -D__CUDA_NO_BFLOAT16_CONVERSIONS__ -D__CUDA_NO_HALF2_OPERATORS__ --expt-relaxed-constexpr --compiler-options ''"'"'-fPIC'"'"'' -O3 -std=c++17 -U__CUDA_NO_HALF_OPERATORS__ -U__CUDA_NO_HALF_CONVERSIONS__ -U__CUDA_NO_HALF2_OPERATORS__ -U__CUDA_NO_BFLOAT16_CONVERSIONS__ --expt-relaxed-constexpr --expt-extended-lambda --use_fast_math --ptxas-options=-v -lineinfo -gencode arch=compute_80,code=sm_80 --threads 4 -DTORCH_API_INCLUDE_EXTENSION_H '-DPYBIND11_COMPILER_TYPE="_gcc"' '-DPYBIND11_STDLIB="_libstdcpp"' '-DPYBIND11_BUILD_ABI="_cxxabi1011"' -DTORCH_EXTENSION_NAME=flash_attn_2_cuda -D_GLIBCXX_USE_CXX11_ABI=0
/anaconda/envs/flashattention/lib/python3.9/site-packages/torch/include/c10/core/SymInt.h(84): warning: integer conversion resulted in a change of sign

/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_kernel.h(140): warning: variable "MMA_M" was declared but never referenced
          detected during:
            instantiation of "void flash::compute_attn_1rowblock<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &, int, int, int) [with Kernel_traits=Flash_fwd_kernel_traits<32, 128, 128, 4, false, false, cutlass::bfloat16_t, Flash_kernel_traits<32, 128, 128, 4, cutlass::bfloat16_t>>, Is_dropout=true, Is_causal=true, Is_even_N=true, Is_even_K=true, Return_softmax=true, Params=Flash_fwd_params]" 
(588): here
            instantiation of "void flash::compute_attn<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &) [with Kernel_traits=Flash_fwd_kernel_traits<32, 128, 128, 4, false, false, cutlass::bfloat16_t, Flash_kernel_traits<32, 128, 128, 4, cutlass::bfloat16_t>>, Is_dropout=true, Is_causal=true, Is_even_N=true, Is_even_K=true, Return_softmax=true, Params=Flash_fwd_params]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(15): here
            instantiation of "void flash_fwd_kernel<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax>(Flash_fwd_params) [with Kernel_traits=Flash_fwd_kernel_traits<32, 128, 128, 4, false, false, cutlass::bfloat16_t, Flash_kernel_traits<32, 128, 128, 4, cutlass::bfloat16_t>>, Is_dropout=true, Is_causal=true, Is_even_N=true, Is_even_K=true, Return_softmax=true]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(34): here
            instantiation of "void run_flash_fwd<Kernel_traits,Is_dropout,Is_causal>(Flash_fwd_params &, cudaStream_t) [with Kernel_traits=Flash_fwd_kernel_traits<32, 128, 128, 4, false, false, cutlass::bfloat16_t, Flash_kernel_traits<32, 128, 128, 4, cutlass::bfloat16_t>>, Is_dropout=true, Is_causal=true]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(58): here
            instantiation of "void run_mha_fwd_hdim32<T>(Flash_fwd_params &, cudaStream_t) [with T=cutlass::bfloat16_t]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_hdim32_bf16_sm80.cu(9): here

/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_kernel.h(275): warning: variable "tQrQ" was declared but never referenced
          detected during:
            instantiation of "void flash::compute_attn_1rowblock<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &, int, int, int) [with Kernel_traits=Flash_fwd_kernel_traits<32, 128, 128, 4, false, false, cutlass::bfloat16_t, Flash_kernel_traits<32, 128, 128, 4, cutlass::bfloat16_t>>, Is_dropout=true, Is_causal=true, Is_even_N=true, Is_even_K=true, Return_softmax=true, Params=Flash_fwd_params]" 
(588): here
            instantiation of "void flash::compute_attn<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &) [with Kernel_traits=Flash_fwd_kernel_traits<32, 128, 128, 4, false, false, cutlass::bfloat16_t, Flash_kernel_traits<32, 128, 128, 4, cutlass::bfloat16_t>>, Is_dropout=true, Is_causal=true, Is_even_N=true, Is_even_K=true, Return_softmax=true, Params=Flash_fwd_params]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(15): here
            instantiation of "void flash_fwd_kernel<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax>(Flash_fwd_params) [with Kernel_traits=Flash_fwd_kernel_traits<32, 128, 128, 4, false, false, cutlass::bfloat16_t, Flash_kernel_traits<32, 128, 128, 4, cutlass::bfloat16_t>>, Is_dropout=true, Is_causal=true, Is_even_N=true, Is_even_K=true, Return_softmax=true]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(34): here
            instantiation of "void run_flash_fwd<Kernel_traits,Is_dropout,Is_causal>(Flash_fwd_params &, cudaStream_t) [with Kernel_traits=Flash_fwd_kernel_traits<32, 128, 128, 4, false, false, cutlass::bfloat16_t, Flash_kernel_traits<32, 128, 128, 4, cutlass::bfloat16_t>>, Is_dropout=true, Is_causal=true]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(58): here
            instantiation of "void run_mha_fwd_hdim32<T>(Flash_fwd_params &, cudaStream_t) [with T=cutlass::bfloat16_t]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_hdim32_bf16_sm80.cu(9): here

/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_kernel.h(140): warning: variable "MMA_M" was declared but never referenced
          detected during:
            instantiation of "void flash::compute_attn_1rowblock<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &, int, int, int) [with Kernel_traits=Flash_fwd_kernel_traits<32, 128, 128, 4, false, false, cutlass::bfloat16_t, Flash_kernel_traits<32, 128, 128, 4, cutlass::bfloat16_t>>, Is_dropout=true, Is_causal=true, Is_even_N=true, Is_even_K=true, Return_softmax=false, Params=Flash_fwd_params]" 
(588): here
            instantiation of "void flash::compute_attn<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &) [with Kernel_traits=Flash_fwd_kernel_traits<32, 128, 128, 4, false, false, cutlass::bfloat16_t, Flash_kernel_traits<32, 128, 128, 4, cutlass::bfloat16_t>>, Is_dropout=true, Is_causal=true, Is_even_N=true, Is_even_K=true, Return_softmax=false, Params=Flash_fwd_params]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(15): here
            instantiation of "void flash_fwd_kernel<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax>(Flash_fwd_params) [with Kernel_traits=Flash_fwd_kernel_traits<32, 128, 128, 4, false, false, cutlass::bfloat16_t, Flash_kernel_traits<32, 128, 128, 4, cutlass::bfloat16_t>>, Is_dropout=true, Is_causal=true, Is_even_N=true, Is_even_K=true, Return_softmax=false]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(34): here
            instantiation of "void run_flash_fwd<Kernel_traits,Is_dropout,Is_causal>(Flash_fwd_params &, cudaStream_t) [with Kernel_traits=Flash_fwd_kernel_traits<32, 128, 128, 4, false, false, cutlass::bfloat16_t, Flash_kernel_traits<32, 128, 128, 4, cutlass::bfloat16_t>>, Is_dropout=true, Is_causal=true]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(58): here
            instantiation of "void run_mha_fwd_hdim32<T>(Flash_fwd_params &, cudaStream_t) [with T=cutlass::bfloat16_t]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_hdim32_bf16_sm80.cu(9): here

/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_kernel.h(275): warning: variable "tQrQ" was declared but never referenced
          detected during:
            instantiation of "void flash::compute_attn_1rowblock<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &, int, int, int) [with Kernel_traits=Flash_fwd_kernel_traits<32, 128, 128, 4, false, false, cutlass::bfloat16_t, Flash_kernel_traits<32, 128, 128, 4, cutlass::bfloat16_t>>, Is_dropout=true, Is_causal=true, Is_even_N=true, Is_even_K=true, Return_softmax=false, Params=Flash_fwd_params]" 
(588): here
            instantiation of "void flash::compute_attn<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &) [with Kernel_traits=Flash_fwd_kernel_traits<32, 128, 128, 4, false, false, cutlass::bfloat16_t, Flash_kernel_traits<32, 128, 128, 4, cutlass::bfloat16_t>>, Is_dropout=true, Is_causal=true, Is_even_N=true, Is_even_K=true, Return_softmax=false, Params=Flash_fwd_params]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(15): here
            instantiation of "void flash_fwd_kernel<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax>(Flash_fwd_params) [with Kernel_traits=Flash_fwd_kernel_traits<32, 128, 128, 4, false, false, cutlass::bfloat16_t, Flash_kernel_traits<32, 128, 128, 4, cutlass::bfloat16_t>>, Is_dropout=true, Is_causal=true, Is_even_N=true, Is_even_K=true, Return_softmax=false]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(34): here
            instantiation of "void run_flash_fwd<Kernel_traits,Is_dropout,Is_causal>(Flash_fwd_params &, cudaStream_t) [with Kernel_traits=Flash_fwd_kernel_traits<32, 128, 128, 4, false, false, cutlass::bfloat16_t, Flash_kernel_traits<32, 128, 128, 4, cutlass::bfloat16_t>>, Is_dropout=true, Is_causal=true]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(58): here
            instantiation of "void run_mha_fwd_hdim32<T>(Flash_fwd_params &, cudaStream_t) [with T=cutlass::bfloat16_t]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_hdim32_bf16_sm80.cu(9): here

/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_kernel.h(140): warning: variable "MMA_M" was declared but never referenced
          detected during:
            instantiation of "void flash::compute_attn_1rowblock<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &, int, int, int) [with Kernel_traits=Flash_fwd_kernel_traits<32, 128, 128, 4, false, false, cutlass::bfloat16_t, Flash_kernel_traits<32, 128, 128, 4, cutlass::bfloat16_t>>, Is_dropout=true, Is_causal=true, Is_even_N=true, Is_even_K=false, Return_softmax=true, Params=Flash_fwd_params]" 
(588): here
            instantiation of "void flash::compute_attn<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &) [with Kernel_traits=Flash_fwd_kernel_traits<32, 128, 128, 4, false, false, cutlass::bfloat16_t, Flash_kernel_traits<32, 128, 128, 4, cutlass::bfloat16_t>>, Is_dropout=true, Is_causal=true, Is_even_N=true, Is_even_K=false, Return_softmax=true, Params=Flash_fwd_params]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(15): here
            instantiation of "void flash_fwd_kernel<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax>(Flash_fwd_params) [with Kernel_traits=Flash_fwd_kernel_traits<32, 128, 128, 4, false, false, cutlass::bfloat16_t, Flash_kernel_traits<32, 128, 128, 4, cutlass::bfloat16_t>>, Is_dropout=true, Is_causal=true, Is_even_N=true, Is_even_K=false, Return_softmax=true]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(34): here
            instantiation of "void run_flash_fwd<Kernel_traits,Is_dropout,Is_causal>(Flash_fwd_params &, cudaStream_t) [with Kernel_traits=Flash_fwd_kernel_traits<32, 128, 128, 4, false, false, cutlass::bfloat16_t, Flash_kernel_traits<32, 128, 128, 4, cutlass::bfloat16_t>>, Is_dropout=true, Is_causal=true]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(58): here
            instantiation of "void run_mha_fwd_hdim32<T>(Flash_fwd_params &, cudaStream_t) [with T=cutlass::bfloat16_t]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_hdim32_bf16_sm80.cu(9): here

/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_kernel.h(275): warning: variable "tQrQ" was declared but never referenced
          detected during:
            instantiation of "void flash::compute_attn_1rowblock<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &, int, int, int) [with Kernel_traits=Flash_fwd_kernel_traits<32, 128, 128, 4, false, false, cutlass::bfloat16_t, Flash_kernel_traits<32, 128, 128, 4, cutlass::bfloat16_t>>, Is_dropout=true, Is_causal=true, Is_even_N=true, Is_even_K=false, Return_softmax=true, Params=Flash_fwd_params]" 
(588): here
            instantiation of "void flash::compute_attn<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &) [with Kernel_traits=Flash_fwd_kernel_traits<32, 128, 128, 4, false, false, cutlass::bfloat16_t, Flash_kernel_traits<32, 128, 128, 4, cutlass::bfloat16_t>>, Is_dropout=true, Is_causal=true, Is_even_N=true, Is_even_K=false, Return_softmax=true, Params=Flash_fwd_params]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(15): here
            instantiation of "void flash_fwd_kernel<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax>(Flash_fwd_params) [with Kernel_traits=Flash_fwd_kernel_traits<32, 128, 128, 4, false, false, cutlass::bfloat16_t, Flash_kernel_traits<32, 128, 128, 4, cutlass::bfloat16_t>>, Is_dropout=true, Is_causal=true, Is_even_N=true, Is_even_K=false, Return_softmax=true]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(34): here
            instantiation of "void run_flash_fwd<Kernel_traits,Is_dropout,Is_causal>(Flash_fwd_params &, cudaStream_t) [with Kernel_traits=Flash_fwd_kernel_traits<32, 128, 128, 4, false, false, cutlass::bfloat16_t, Flash_kernel_traits<32, 128, 128, 4, cutlass::bfloat16_t>>, Is_dropout=true, Is_causal=true]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(58): here
            instantiation of "void run_mha_fwd_hdim32<T>(Flash_fwd_params &, cudaStream_t) [with T=cutlass::bfloat16_t]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_hdim32_bf16_sm80.cu(9): here

/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_kernel.h(140): warning: variable "MMA_M" was declared but never referenced
          detected during:
            instantiation of "void flash::compute_attn_1rowblock<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &, int, int, int) [with Kernel_traits=Flash_fwd_kernel_traits<32, 128, 128, 4, false, false, cutlass::bfloat16_t, Flash_kernel_traits<32, 128, 128, 4, cutlass::bfloat16_t>>, Is_dropout=true, Is_causal=true, Is_even_N=true, Is_even_K=false, Return_softmax=false, Params=Flash_fwd_params]" 
(588): here
            instantiation of "void flash::compute_attn<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &) [with Kernel_traits=Flash_fwd_kernel_traits<32, 128, 128, 4, false, false, cutlass::bfloat16_t, Flash_kernel_traits<32, 128, 128, 4, cutlass::bfloat16_t>>, Is_dropout=true, Is_causal=true, Is_even_N=true, Is_even_K=false, Return_softmax=false, Params=Flash_fwd_params]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(15): here
            instantiation of "void flash_fwd_kernel<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax>(Flash_fwd_params) [with Kernel_traits=Flash_fwd_kernel_traits<32, 128, 128, 4, false, false, cutlass::bfloat16_t, Flash_kernel_traits<32, 128, 128, 4, cutlass::bfloat16_t>>, Is_dropout=true, Is_causal=true, Is_even_N=true, Is_even_K=false, Return_softmax=false]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(34): here
            instantiation of "void run_flash_fwd<Kernel_traits,Is_dropout,Is_causal>(Flash_fwd_params &, cudaStream_t) [with Kernel_traits=Flash_fwd_kernel_traits<32, 128, 128, 4, false, false, cutlass::bfloat16_t, Flash_kernel_traits<32, 128, 128, 4, cutlass::bfloat16_t>>, Is_dropout=true, Is_causal=true]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(58): here
            instantiation of "void run_mha_fwd_hdim32<T>(Flash_fwd_params &, cudaStream_t) [with T=cutlass::bfloat16_t]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_hdim32_bf16_sm80.cu(9): here

/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_kernel.h(275): warning: variable "tQrQ" was declared but never referenced
          detected during:
            instantiation of "void flash::compute_attn_1rowblock<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &, int, int, int) [with Kernel_traits=Flash_fwd_kernel_traits<32, 128, 128, 4, false, false, cutlass::bfloat16_t, Flash_kernel_traits<32, 128, 128, 4, cutlass::bfloat16_t>>, Is_dropout=true, Is_causal=true, Is_even_N=true, Is_even_K=false, Return_softmax=false, Params=Flash_fwd_params]" 
(588): here
            instantiation of "void flash::compute_attn<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &) [with Kernel_traits=Flash_fwd_kernel_traits<32, 128, 128, 4, false, false, cutlass::bfloat16_t, Flash_kernel_traits<32, 128, 128, 4, cutlass::bfloat16_t>>, Is_dropout=true, Is_causal=true, Is_even_N=true, Is_even_K=false, Return_softmax=false, Params=Flash_fwd_params]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(15): here
            instantiation of "void flash_fwd_kernel<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax>(Flash_fwd_params) [with Kernel_traits=Flash_fwd_kernel_traits<32, 128, 128, 4, false, false, cutlass::bfloat16_t, Flash_kernel_traits<32, 128, 128, 4, cutlass::bfloat16_t>>, Is_dropout=true, Is_causal=true, Is_even_N=true, Is_even_K=false, Return_softmax=false]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(34): here
            instantiation of "void run_flash_fwd<Kernel_traits,Is_dropout,Is_causal>(Flash_fwd_params &, cudaStream_t) [with Kernel_traits=Flash_fwd_kernel_traits<32, 128, 128, 4, false, false, cutlass::bfloat16_t, Flash_kernel_traits<32, 128, 128, 4, cutlass::bfloat16_t>>, Is_dropout=true, Is_causal=true]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(58): here
            instantiation of "void run_mha_fwd_hdim32<T>(Flash_fwd_params &, cudaStream_t) [with T=cutlass::bfloat16_t]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_hdim32_bf16_sm80.cu(9): here

/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_kernel.h(140): warning: variable "MMA_M" was declared but never referenced
          detected during:
            instantiation of "void flash::compute_attn_1rowblock<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &, int, int, int) [with Kernel_traits=Flash_fwd_kernel_traits<32, 128, 128, 4, false, false, cutlass::bfloat16_t, Flash_kernel_traits<32, 128, 128, 4, cutlass::bfloat16_t>>, Is_dropout=true, Is_causal=true, Is_even_N=false, Is_even_K=true, Return_softmax=true, Params=Flash_fwd_params]" 
(588): here
            instantiation of "void flash::compute_attn<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &) [with Kernel_traits=Flash_fwd_kernel_traits<32, 128, 128, 4, false, false, cutlass::bfloat16_t, Flash_kernel_traits<32, 128, 128, 4, cutlass::bfloat16_t>>, Is_dropout=true, Is_causal=true, Is_even_N=false, Is_even_K=true, Return_softmax=true, Params=Flash_fwd_params]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(15): here
            instantiation of "void flash_fwd_kernel<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax>(Flash_fwd_params) [with Kernel_traits=Flash_fwd_kernel_traits<32, 128, 128, 4, false, false, cutlass::bfloat16_t, Flash_kernel_traits<32, 128, 128, 4, cutlass::bfloat16_t>>, Is_dropout=true, Is_causal=true, Is_even_N=false, Is_even_K=true, Return_softmax=true]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(34): here
            instantiation of "void run_flash_fwd<Kernel_traits,Is_dropout,Is_causal>(Flash_fwd_params &, cudaStream_t) [with Kernel_traits=Flash_fwd_kernel_traits<32, 128, 128, 4, false, false, cutlass::bfloat16_t, Flash_kernel_traits<32, 128, 128, 4, cutlass::bfloat16_t>>, Is_dropout=true, Is_causal=true]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(58): here
            instantiation of "void run_mha_fwd_hdim32<T>(Flash_fwd_params &, cudaStream_t) [with T=cutlass::bfloat16_t]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_hdim32_bf16_sm80.cu(9): here

/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_kernel.h(275): warning: variable "tQrQ" was declared but never referenced
          detected during:
            instantiation of "void flash::compute_attn_1rowblock<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &, int, int, int) [with Kernel_traits=Flash_fwd_kernel_traits<32, 128, 128, 4, false, false, cutlass::bfloat16_t, Flash_kernel_traits<32, 128, 128, 4, cutlass::bfloat16_t>>, Is_dropout=true, Is_causal=true, Is_even_N=false, Is_even_K=true, Return_softmax=true, Params=Flash_fwd_params]" 
(588): here
            instantiation of "void flash::compute_attn<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &) [with Kernel_traits=Flash_fwd_kernel_traits<32, 128, 128, 4, false, false, cutlass::bfloat16_t, Flash_kernel_traits<32, 128, 128, 4, cutlass::bfloat16_t>>, Is_dropout=true, Is_causal=true, Is_even_N=false, Is_even_K=true, Return_softmax=true, Params=Flash_fwd_params]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(15): here
            instantiation of "void flash_fwd_kernel<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax>(Flash_fwd_params) [with Kernel_traits=Flash_fwd_kernel_traits<32, 128, 128, 4, false, false, cutlass::bfloat16_t, Flash_kernel_traits<32, 128, 128, 4, cutlass::bfloat16_t>>, Is_dropout=true, Is_causal=true, Is_even_N=false, Is_even_K=true, Return_softmax=true]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(34): here
            instantiation of "void run_flash_fwd<Kernel_traits,Is_dropout,Is_causal>(Flash_fwd_params &, cudaStream_t) [with Kernel_traits=Flash_fwd_kernel_traits<32, 128, 128, 4, false, false, cutlass::bfloat16_t, Flash_kernel_traits<32, 128, 128, 4, cutlass::bfloat16_t>>, Is_dropout=true, Is_causal=true]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(58): here
            instantiation of "void run_mha_fwd_hdim32<T>(Flash_fwd_params &, cudaStream_t) [with T=cutlass::bfloat16_t]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_hdim32_bf16_sm80.cu(9): here

/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_kernel.h(140): warning: variable "MMA_M" was declared but never referenced
          detected during:
            instantiation of "void flash::compute_attn_1rowblock<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &, int, int, int) [with Kernel_traits=Flash_fwd_kernel_traits<32, 128, 128, 4, false, false, cutlass::bfloat16_t, Flash_kernel_traits<32, 128, 128, 4, cutlass::bfloat16_t>>, Is_dropout=true, Is_causal=true, Is_even_N=false, Is_even_K=true, Return_softmax=false, Params=Flash_fwd_params]" 
(588): here
            instantiation of "void flash::compute_attn<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &) [with Kernel_traits=Flash_fwd_kernel_traits<32, 128, 128, 4, false, false, cutlass::bfloat16_t, Flash_kernel_traits<32, 128, 128, 4, cutlass::bfloat16_t>>, Is_dropout=true, Is_causal=true, Is_even_N=false, Is_even_K=true, Return_softmax=false, Params=Flash_fwd_params]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(15): here
            instantiation of "void flash_fwd_kernel<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax>(Flash_fwd_params) [with Kernel_traits=Flash_fwd_kernel_traits<32, 128, 128, 4, false, false, cutlass::bfloat16_t, Flash_kernel_traits<32, 128, 128, 4, cutlass::bfloat16_t>>, Is_dropout=true, Is_causal=true, Is_even_N=false, Is_even_K=true, Return_softmax=false]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(34): here
            instantiation of "void run_flash_fwd<Kernel_traits,Is_dropout,Is_causal>(Flash_fwd_params &, cudaStream_t) [with Kernel_traits=Flash_fwd_kernel_traits<32, 128, 128, 4, false, false, cutlass::bfloat16_t, Flash_kernel_traits<32, 128, 128, 4, cutlass::bfloat16_t>>, Is_dropout=true, Is_causal=true]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(58): here
            instantiation of "void run_mha_fwd_hdim32<T>(Flash_fwd_params &, cudaStream_t) [with T=cutlass::bfloat16_t]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_hdim32_bf16_sm80.cu(9): here

/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_kernel.h(275): warning: variable "tQrQ" was declared but never referenced
          detected during:
            instantiation of "void flash::compute_attn_1rowblock<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &, int, int, int) [with Kernel_traits=Flash_fwd_kernel_traits<32, 128, 128, 4, false, false, cutlass::bfloat16_t, Flash_kernel_traits<32, 128, 128, 4, cutlass::bfloat16_t>>, Is_dropout=true, Is_causal=true, Is_even_N=false, Is_even_K=true, Return_softmax=false, Params=Flash_fwd_params]" 
(588): here
            instantiation of "void flash::compute_attn<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &) [with Kernel_traits=Flash_fwd_kernel_traits<32, 128, 128, 4, false, false, cutlass::bfloat16_t, Flash_kernel_traits<32, 128, 128, 4, cutlass::bfloat16_t>>, Is_dropout=true, Is_causal=true, Is_even_N=false, Is_even_K=true, Return_softmax=false, Params=Flash_fwd_params]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(15): here
            instantiation of "void flash_fwd_kernel<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax>(Flash_fwd_params) [with Kernel_traits=Flash_fwd_kernel_traits<32, 128, 128, 4, false, false, cutlass::bfloat16_t, Flash_kernel_traits<32, 128, 128, 4, cutlass::bfloat16_t>>, Is_dropout=true, Is_causal=true, Is_even_N=false, Is_even_K=true, Return_softmax=false]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(34): here
            instantiation of "void run_flash_fwd<Kernel_traits,Is_dropout,Is_causal>(Flash_fwd_params &, cudaStream_t) [with Kernel_traits=Flash_fwd_kernel_traits<32, 128, 128, 4, false, false, cutlass::bfloat16_t, Flash_kernel_traits<32, 128, 128, 4, cutlass::bfloat16_t>>, Is_dropout=true, Is_causal=true]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(58): here
            instantiation of "void run_mha_fwd_hdim32<T>(Flash_fwd_params &, cudaStream_t) [with T=cutlass::bfloat16_t]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_hdim32_bf16_sm80.cu(9): here

/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_kernel.h(140): warning: variable "MMA_M" was declared but never referenced
          detected during:
            instantiation of "void flash::compute_attn_1rowblock<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &, int, int, int) [with Kernel_traits=Flash_fwd_kernel_traits<32, 128, 128, 4, false, false, cutlass::bfloat16_t, Flash_kernel_traits<32, 128, 128, 4, cutlass::bfloat16_t>>, Is_dropout=true, Is_causal=true, Is_even_N=false, Is_even_K=false, Return_softmax=true, Params=Flash_fwd_params]" 
(588): here
            instantiation of "void flash::compute_attn<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &) [with Kernel_traits=Flash_fwd_kernel_traits<32, 128, 128, 4, false, false, cutlass::bfloat16_t, Flash_kernel_traits<32, 128, 128, 4, cutlass::bfloat16_t>>, Is_dropout=true, Is_causal=true, Is_even_N=false, Is_even_K=false, Return_softmax=true, Params=Flash_fwd_params]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(15): here
            instantiation of "void flash_fwd_kernel<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax>(Flash_fwd_params) [with Kernel_traits=Flash_fwd_kernel_traits<32, 128, 128, 4, false, false, cutlass::bfloat16_t, Flash_kernel_traits<32, 128, 128, 4, cutlass::bfloat16_t>>, Is_dropout=true, Is_causal=true, Is_even_N=false, Is_even_K=false, Return_softmax=true]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(34): here
            instantiation of "void run_flash_fwd<Kernel_traits,Is_dropout,Is_causal>(Flash_fwd_params &, cudaStream_t) [with Kernel_traits=Flash_fwd_kernel_traits<32, 128, 128, 4, false, false, cutlass::bfloat16_t, Flash_kernel_traits<32, 128, 128, 4, cutlass::bfloat16_t>>, Is_dropout=true, Is_causal=true]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(58): here
            instantiation of "void run_mha_fwd_hdim32<T>(Flash_fwd_params &, cudaStream_t) [with T=cutlass::bfloat16_t]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_hdim32_bf16_sm80.cu(9): here

/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_kernel.h(275): warning: variable "tQrQ" was declared but never referenced
          detected during:
            instantiation of "void flash::compute_attn_1rowblock<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &, int, int, int) [with Kernel_traits=Flash_fwd_kernel_traits<32, 128, 128, 4, false, false, cutlass::bfloat16_t, Flash_kernel_traits<32, 128, 128, 4, cutlass::bfloat16_t>>, Is_dropout=true, Is_causal=true, Is_even_N=false, Is_even_K=false, Return_softmax=true, Params=Flash_fwd_params]" 
(588): here
            instantiation of "void flash::compute_attn<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &) [with Kernel_traits=Flash_fwd_kernel_traits<32, 128, 128, 4, false, false, cutlass::bfloat16_t, Flash_kernel_traits<32, 128, 128, 4, cutlass::bfloat16_t>>, Is_dropout=true, Is_causal=true, Is_even_N=false, Is_even_K=false, Return_softmax=true, Params=Flash_fwd_params]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(15): here
            instantiation of "void flash_fwd_kernel<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax>(Flash_fwd_params) [with Kernel_traits=Flash_fwd_kernel_traits<32, 128, 128, 4, false, false, cutlass::bfloat16_t, Flash_kernel_traits<32, 128, 128, 4, cutlass::bfloat16_t>>, Is_dropout=true, Is_causal=true, Is_even_N=false, Is_even_K=false, Return_softmax=true]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(34): here
            instantiation of "void run_flash_fwd<Kernel_traits,Is_dropout,Is_causal>(Flash_fwd_params &, cudaStream_t) [with Kernel_traits=Flash_fwd_kernel_traits<32, 128, 128, 4, false, false, cutlass::bfloat16_t, Flash_kernel_traits<32, 128, 128, 4, cutlass::bfloat16_t>>, Is_dropout=true, Is_causal=true]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(58): here
            instantiation of "void run_mha_fwd_hdim32<T>(Flash_fwd_params &, cudaStream_t) [with T=cutlass::bfloat16_t]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_hdim32_bf16_sm80.cu(9): here

/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_kernel.h(140): warning: variable "MMA_M" was declared but never referenced
          detected during:
            instantiation of "void flash::compute_attn_1rowblock<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &, int, int, int) [with Kernel_traits=Flash_fwd_kernel_traits<32, 128, 128, 4, false, false, cutlass::bfloat16_t, Flash_kernel_traits<32, 128, 128, 4, cutlass::bfloat16_t>>, Is_dropout=true, Is_causal=true, Is_even_N=false, Is_even_K=false, Return_softmax=false, Params=Flash_fwd_params]" 
(588): here
            instantiation of "void flash::compute_attn<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &) [with Kernel_traits=Flash_fwd_kernel_traits<32, 128, 128, 4, false, false, cutlass::bfloat16_t, Flash_kernel_traits<32, 128, 128, 4, cutlass::bfloat16_t>>, Is_dropout=true, Is_causal=true, Is_even_N=false, Is_even_K=false, Return_softmax=false, Params=Flash_fwd_params]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(15): here
            instantiation of "void flash_fwd_kernel<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax>(Flash_fwd_params) [with Kernel_traits=Flash_fwd_kernel_traits<32, 128, 128, 4, false, false, cutlass::bfloat16_t, Flash_kernel_traits<32, 128, 128, 4, cutlass::bfloat16_t>>, Is_dropout=true, Is_causal=true, Is_even_N=false, Is_even_K=false, Return_softmax=false]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(34): here
            instantiation of "void run_flash_fwd<Kernel_traits,Is_dropout,Is_causal>(Flash_fwd_params &, cudaStream_t) [with Kernel_traits=Flash_fwd_kernel_traits<32, 128, 128, 4, false, false, cutlass::bfloat16_t, Flash_kernel_traits<32, 128, 128, 4, cutlass::bfloat16_t>>, Is_dropout=true, Is_causal=true]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(58): here
            instantiation of "void run_mha_fwd_hdim32<T>(Flash_fwd_params &, cudaStream_t) [with T=cutlass::bfloat16_t]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_hdim32_bf16_sm80.cu(9): here

/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_kernel.h(275): warning: variable "tQrQ" was declared but never referenced
          detected during:
            instantiation of "void flash::compute_attn_1rowblock<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &, int, int, int) [with Kernel_traits=Flash_fwd_kernel_traits<32, 128, 128, 4, false, false, cutlass::bfloat16_t, Flash_kernel_traits<32, 128, 128, 4, cutlass::bfloat16_t>>, Is_dropout=true, Is_causal=true, Is_even_N=false, Is_even_K=false, Return_softmax=false, Params=Flash_fwd_params]" 
(588): here
            instantiation of "void flash::compute_attn<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &) [with Kernel_traits=Flash_fwd_kernel_traits<32, 128, 128, 4, false, false, cutlass::bfloat16_t, Flash_kernel_traits<32, 128, 128, 4, cutlass::bfloat16_t>>, Is_dropout=true, Is_causal=true, Is_even_N=false, Is_even_K=false, Return_softmax=false, Params=Flash_fwd_params]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(15): here
            instantiation of "void flash_fwd_kernel<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax>(Flash_fwd_params) [with Kernel_traits=Flash_fwd_kernel_traits<32, 128, 128, 4, false, false, cutlass::bfloat16_t, Flash_kernel_traits<32, 128, 128, 4, cutlass::bfloat16_t>>, Is_dropout=true, Is_causal=true, Is_even_N=false, Is_even_K=false, Return_softmax=false]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(34): here
            instantiation of "void run_flash_fwd<Kernel_traits,Is_dropout,Is_causal>(Flash_fwd_params &, cudaStream_t) [with Kernel_traits=Flash_fwd_kernel_traits<32, 128, 128, 4, false, false, cutlass::bfloat16_t, Flash_kernel_traits<32, 128, 128, 4, cutlass::bfloat16_t>>, Is_dropout=true, Is_causal=true]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(58): here
            instantiation of "void run_mha_fwd_hdim32<T>(Flash_fwd_params &, cudaStream_t) [with T=cutlass::bfloat16_t]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_hdim32_bf16_sm80.cu(9): here

/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_kernel.h(140): warning: variable "MMA_M" was declared but never referenced
          detected during:
            instantiation of "void flash::compute_attn_1rowblock<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &, int, int, int) [with Kernel_traits=Flash_fwd_kernel_traits<32, 128, 128, 4, false, false, cutlass::bfloat16_t, Flash_kernel_traits<32, 128, 128, 4, cutlass::bfloat16_t>>, Is_dropout=true, Is_causal=false, Is_even_N=true, Is_even_K=true, Return_softmax=true, Params=Flash_fwd_params]" 
(588): here
            instantiation of "void flash::compute_attn<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &) [with Kernel_traits=Flash_fwd_kernel_traits<32, 128, 128, 4, false, false, cutlass::bfloat16_t, Flash_kernel_traits<32, 128, 128, 4, cutlass::bfloat16_t>>, Is_dropout=true, Is_causal=false, Is_even_N=true, Is_even_K=true, Return_softmax=true, Params=Flash_fwd_params]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(15): here
            instantiation of "void flash_fwd_kernel<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax>(Flash_fwd_params) [with Kernel_traits=Flash_fwd_kernel_traits<32, 128, 128, 4, false, false, cutlass::bfloat16_t, Flash_kernel_traits<32, 128, 128, 4, cutlass::bfloat16_t>>, Is_dropout=true, Is_causal=false, Is_even_N=true, Is_even_K=true, Return_softmax=true]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(34): here
            instantiation of "void run_flash_fwd<Kernel_traits,Is_dropout,Is_causal>(Flash_fwd_params &, cudaStream_t) [with Kernel_traits=Flash_fwd_kernel_traits<32, 128, 128, 4, false, false, cutlass::bfloat16_t, Flash_kernel_traits<32, 128, 128, 4, cutlass::bfloat16_t>>, Is_dropout=true, Is_causal=false]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(58): here
            instantiation of "void run_mha_fwd_hdim32<T>(Flash_fwd_params &, cudaStream_t) [with T=cutlass::bfloat16_t]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_hdim32_bf16_sm80.cu(9): here

/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_kernel.h(275): warning: variable "tQrQ" was declared but never referenced
          detected during:
            instantiation of "void flash::compute_attn_1rowblock<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &, int, int, int) [with Kernel_traits=Flash_fwd_kernel_traits<32, 128, 128, 4, false, false, cutlass::bfloat16_t, Flash_kernel_traits<32, 128, 128, 4, cutlass::bfloat16_t>>, Is_dropout=true, Is_causal=false, Is_even_N=true, Is_even_K=true, Return_softmax=true, Params=Flash_fwd_params]" 
(588): here
            instantiation of "void flash::compute_attn<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &) [with Kernel_traits=Flash_fwd_kernel_traits<32, 128, 128, 4, false, false, cutlass::bfloat16_t, Flash_kernel_traits<32, 128, 128, 4, cutlass::bfloat16_t>>, Is_dropout=true, Is_causal=false, Is_even_N=true, Is_even_K=true, Return_softmax=true, Params=Flash_fwd_params]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(15): here
            instantiation of "void flash_fwd_kernel<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax>(Flash_fwd_params) [with Kernel_traits=Flash_fwd_kernel_traits<32, 128, 128, 4, false, false, cutlass::bfloat16_t, Flash_kernel_traits<32, 128, 128, 4, cutlass::bfloat16_t>>, Is_dropout=true, Is_causal=false, Is_even_N=true, Is_even_K=true, Return_softmax=true]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(34): here
            instantiation of "void run_flash_fwd<Kernel_traits,Is_dropout,Is_causal>(Flash_fwd_params &, cudaStream_t) [with Kernel_traits=Flash_fwd_kernel_traits<32, 128, 128, 4, false, false, cutlass::bfloat16_t, Flash_kernel_traits<32, 128, 128, 4, cutlass::bfloat16_t>>, Is_dropout=true, Is_causal=false]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(58): here
            instantiation of "void run_mha_fwd_hdim32<T>(Flash_fwd_params &, cudaStream_t) [with T=cutlass::bfloat16_t]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_hdim32_bf16_sm80.cu(9): here

/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_kernel.h(140): warning: variable "MMA_M" was declared but never referenced
          detected during:
            instantiation of "void flash::compute_attn_1rowblock<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &, int, int, int) [with Kernel_traits=Flash_fwd_kernel_traits<32, 128, 128, 4, false, false, cutlass::bfloat16_t, Flash_kernel_traits<32, 128, 128, 4, cutlass::bfloat16_t>>, Is_dropout=true, Is_causal=false, Is_even_N=true, Is_even_K=true, Return_softmax=false, Params=Flash_fwd_params]" 
(588): here
            instantiation of "void flash::compute_attn<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &) [with Kernel_traits=Flash_fwd_kernel_traits<32, 128, 128, 4, false, false, cutlass::bfloat16_t, Flash_kernel_traits<32, 128, 128, 4, cutlass::bfloat16_t>>, Is_dropout=true, Is_causal=false, Is_even_N=true, Is_even_K=true, Return_softmax=false, Params=Flash_fwd_params]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(15): here
            instantiation of "void flash_fwd_kernel<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax>(Flash_fwd_params) [with Kernel_traits=Flash_fwd_kernel_traits<32, 128, 128, 4, false, false, cutlass::bfloat16_t, Flash_kernel_traits<32, 128, 128, 4, cutlass::bfloat16_t>>, Is_dropout=true, Is_causal=false, Is_even_N=true, Is_even_K=true, Return_softmax=false]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(34): here
            instantiation of "void run_flash_fwd<Kernel_traits,Is_dropout,Is_causal>(Flash_fwd_params &, cudaStream_t) [with Kernel_traits=Flash_fwd_kernel_traits<32, 128, 128, 4, false, false, cutlass::bfloat16_t, Flash_kernel_traits<32, 128, 128, 4, cutlass::bfloat16_t>>, Is_dropout=true, Is_causal=false]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(58): here
            instantiation of "void run_mha_fwd_hdim32<T>(Flash_fwd_params &, cudaStream_t) [with T=cutlass::bfloat16_t]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_hdim32_bf16_sm80.cu(9): here

/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_kernel.h(275): warning: variable "tQrQ" was declared but never referenced
          detected during:
            instantiation of "void flash::compute_attn_1rowblock<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &, int, int, int) [with Kernel_traits=Flash_fwd_kernel_traits<32, 128, 128, 4, false, false, cutlass::bfloat16_t, Flash_kernel_traits<32, 128, 128, 4, cutlass::bfloat16_t>>, Is_dropout=true, Is_causal=false, Is_even_N=true, Is_even_K=true, Return_softmax=false, Params=Flash_fwd_params]" 
(588): here
            instantiation of "void flash::compute_attn<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &) [with Kernel_traits=Flash_fwd_kernel_traits<32, 128, 128, 4, false, false, cutlass::bfloat16_t, Flash_kernel_traits<32, 128, 128, 4, cutlass::bfloat16_t>>, Is_dropout=true, Is_causal=false, Is_even_N=true, Is_even_K=true, Return_softmax=false, Params=Flash_fwd_params]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(15): here
            instantiation of "void flash_fwd_kernel<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax>(Flash_fwd_params) [with Kernel_traits=Flash_fwd_kernel_traits<32, 128, 128, 4, false, false, cutlass::bfloat16_t, Flash_kernel_traits<32, 128, 128, 4, cutlass::bfloat16_t>>, Is_dropout=true, Is_causal=false, Is_even_N=true, Is_even_K=true, Return_softmax=false]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(34): here
            instantiation of "void run_flash_fwd<Kernel_traits,Is_dropout,Is_causal>(Flash_fwd_params &, cudaStream_t) [with Kernel_traits=Flash_fwd_kernel_traits<32, 128, 128, 4, false, false, cutlass::bfloat16_t, Flash_kernel_traits<32, 128, 128, 4, cutlass::bfloat16_t>>, Is_dropout=true, Is_causal=false]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(58): here
            instantiation of "void run_mha_fwd_hdim32<T>(Flash_fwd_params &, cudaStream_t) [with T=cutlass::bfloat16_t]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_hdim32_bf16_sm80.cu(9): here

/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_kernel.h(140): warning: variable "MMA_M" was declared but never referenced
          detected during:
            instantiation of "void flash::compute_attn_1rowblock<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &, int, int, int) [with Kernel_traits=Flash_fwd_kernel_traits<32, 128, 128, 4, false, false, cutlass::bfloat16_t, Flash_kernel_traits<32, 128, 128, 4, cutlass::bfloat16_t>>, Is_dropout=true, Is_causal=false, Is_even_N=true, Is_even_K=false, Return_softmax=true, Params=Flash_fwd_params]" 
(588): here
            instantiation of "void flash::compute_attn<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &) [with Kernel_traits=Flash_fwd_kernel_traits<32, 128, 128, 4, false, false, cutlass::bfloat16_t, Flash_kernel_traits<32, 128, 128, 4, cutlass::bfloat16_t>>, Is_dropout=true, Is_causal=false, Is_even_N=true, Is_even_K=false, Return_softmax=true, Params=Flash_fwd_params]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(15): here
            instantiation of "void flash_fwd_kernel<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax>(Flash_fwd_params) [with Kernel_traits=Flash_fwd_kernel_traits<32, 128, 128, 4, false, false, cutlass::bfloat16_t, Flash_kernel_traits<32, 128, 128, 4, cutlass::bfloat16_t>>, Is_dropout=true, Is_causal=false, Is_even_N=true, Is_even_K=false, Return_softmax=true]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(34): here
            instantiation of "void run_flash_fwd<Kernel_traits,Is_dropout,Is_causal>(Flash_fwd_params &, cudaStream_t) [with Kernel_traits=Flash_fwd_kernel_traits<32, 128, 128, 4, false, false, cutlass::bfloat16_t, Flash_kernel_traits<32, 128, 128, 4, cutlass::bfloat16_t>>, Is_dropout=true, Is_causal=false]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(58): here
            instantiation of "void run_mha_fwd_hdim32<T>(Flash_fwd_params &, cudaStream_t) [with T=cutlass::bfloat16_t]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_hdim32_bf16_sm80.cu(9): here

/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_kernel.h(275): warning: variable "tQrQ" was declared but never referenced
          detected during:
            instantiation of "void flash::compute_attn_1rowblock<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &, int, int, int) [with Kernel_traits=Flash_fwd_kernel_traits<32, 128, 128, 4, false, false, cutlass::bfloat16_t, Flash_kernel_traits<32, 128, 128, 4, cutlass::bfloat16_t>>, Is_dropout=true, Is_causal=false, Is_even_N=true, Is_even_K=false, Return_softmax=true, Params=Flash_fwd_params]" 
(588): here
            instantiation of "void flash::compute_attn<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &) [with Kernel_traits=Flash_fwd_kernel_traits<32, 128, 128, 4, false, false, cutlass::bfloat16_t, Flash_kernel_traits<32, 128, 128, 4, cutlass::bfloat16_t>>, Is_dropout=true, Is_causal=false, Is_even_N=true, Is_even_K=false, Return_softmax=true, Params=Flash_fwd_params]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(15): here
            instantiation of "void flash_fwd_kernel<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax>(Flash_fwd_params) [with Kernel_traits=Flash_fwd_kernel_traits<32, 128, 128, 4, false, false, cutlass::bfloat16_t, Flash_kernel_traits<32, 128, 128, 4, cutlass::bfloat16_t>>, Is_dropout=true, Is_causal=false, Is_even_N=true, Is_even_K=false, Return_softmax=true]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(34): here
            instantiation of "void run_flash_fwd<Kernel_traits,Is_dropout,Is_causal>(Flash_fwd_params &, cudaStream_t) [with Kernel_traits=Flash_fwd_kernel_traits<32, 128, 128, 4, false, false, cutlass::bfloat16_t, Flash_kernel_traits<32, 128, 128, 4, cutlass::bfloat16_t>>, Is_dropout=true, Is_causal=false]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(58): here
            instantiation of "void run_mha_fwd_hdim32<T>(Flash_fwd_params &, cudaStream_t) [with T=cutlass::bfloat16_t]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_hdim32_bf16_sm80.cu(9): here

/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_kernel.h(140): warning: variable "MMA_M" was declared but never referenced
          detected during:
            instantiation of "void flash::compute_attn_1rowblock<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &, int, int, int) [with Kernel_traits=Flash_fwd_kernel_traits<32, 128, 128, 4, false, false, cutlass::bfloat16_t, Flash_kernel_traits<32, 128, 128, 4, cutlass::bfloat16_t>>, Is_dropout=true, Is_causal=false, Is_even_N=true, Is_even_K=false, Return_softmax=false, Params=Flash_fwd_params]" 
(588): here
            instantiation of "void flash::compute_attn<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &) [with Kernel_traits=Flash_fwd_kernel_traits<32, 128, 128, 4, false, false, cutlass::bfloat16_t, Flash_kernel_traits<32, 128, 128, 4, cutlass::bfloat16_t>>, Is_dropout=true, Is_causal=false, Is_even_N=true, Is_even_K=false, Return_softmax=false, Params=Flash_fwd_params]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(15): here
            instantiation of "void flash_fwd_kernel<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax>(Flash_fwd_params) [with Kernel_traits=Flash_fwd_kernel_traits<32, 128, 128, 4, false, false, cutlass::bfloat16_t, Flash_kernel_traits<32, 128, 128, 4, cutlass::bfloat16_t>>, Is_dropout=true, Is_causal=false, Is_even_N=true, Is_even_K=false, Return_softmax=false]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(34): here
            instantiation of "void run_flash_fwd<Kernel_traits,Is_dropout,Is_causal>(Flash_fwd_params &, cudaStream_t) [with Kernel_traits=Flash_fwd_kernel_traits<32, 128, 128, 4, false, false, cutlass::bfloat16_t, Flash_kernel_traits<32, 128, 128, 4, cutlass::bfloat16_t>>, Is_dropout=true, Is_causal=false]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(58): here
            instantiation of "void run_mha_fwd_hdim32<T>(Flash_fwd_params &, cudaStream_t) [with T=cutlass::bfloat16_t]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_hdim32_bf16_sm80.cu(9): here

/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_kernel.h(275): warning: variable "tQrQ" was declared but never referenced
          detected during:
            instantiation of "void flash::compute_attn_1rowblock<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &, int, int, int) [with Kernel_traits=Flash_fwd_kernel_traits<32, 128, 128, 4, false, false, cutlass::bfloat16_t, Flash_kernel_traits<32, 128, 128, 4, cutlass::bfloat16_t>>, Is_dropout=true, Is_causal=false, Is_even_N=true, Is_even_K=false, Return_softmax=false, Params=Flash_fwd_params]" 
(588): here
            instantiation of "void flash::compute_attn<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &) [with Kernel_traits=Flash_fwd_kernel_traits<32, 128, 128, 4, false, false, cutlass::bfloat16_t, Flash_kernel_traits<32, 128, 128, 4, cutlass::bfloat16_t>>, Is_dropout=true, Is_causal=false, Is_even_N=true, Is_even_K=false, Return_softmax=false, Params=Flash_fwd_params]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(15): here
            instantiation of "void flash_fwd_kernel<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax>(Flash_fwd_params) [with Kernel_traits=Flash_fwd_kernel_traits<32, 128, 128, 4, false, false, cutlass::bfloat16_t, Flash_kernel_traits<32, 128, 128, 4, cutlass::bfloat16_t>>, Is_dropout=true, Is_causal=false, Is_even_N=true, Is_even_K=false, Return_softmax=false]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(34): here
            instantiation of "void run_flash_fwd<Kernel_traits,Is_dropout,Is_causal>(Flash_fwd_params &, cudaStream_t) [with Kernel_traits=Flash_fwd_kernel_traits<32, 128, 128, 4, false, false, cutlass::bfloat16_t, Flash_kernel_traits<32, 128, 128, 4, cutlass::bfloat16_t>>, Is_dropout=true, Is_causal=false]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(58): here
            instantiation of "void run_mha_fwd_hdim32<T>(Flash_fwd_params &, cudaStream_t) [with T=cutlass::bfloat16_t]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_hdim32_bf16_sm80.cu(9): here

/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_kernel.h(140): warning: variable "MMA_M" was declared but never referenced
          detected during:
            instantiation of "void flash::compute_attn_1rowblock<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &, int, int, int) [with Kernel_traits=Flash_fwd_kernel_traits<32, 128, 128, 4, false, false, cutlass::bfloat16_t, Flash_kernel_traits<32, 128, 128, 4, cutlass::bfloat16_t>>, Is_dropout=true, Is_causal=false, Is_even_N=false, Is_even_K=true, Return_softmax=true, Params=Flash_fwd_params]" 
(588): here
            instantiation of "void flash::compute_attn<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &) [with Kernel_traits=Flash_fwd_kernel_traits<32, 128, 128, 4, false, false, cutlass::bfloat16_t, Flash_kernel_traits<32, 128, 128, 4, cutlass::bfloat16_t>>, Is_dropout=true, Is_causal=false, Is_even_N=false, Is_even_K=true, Return_softmax=true, Params=Flash_fwd_params]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(15): here
            instantiation of "void flash_fwd_kernel<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax>(Flash_fwd_params) [with Kernel_traits=Flash_fwd_kernel_traits<32, 128, 128, 4, false, false, cutlass::bfloat16_t, Flash_kernel_traits<32, 128, 128, 4, cutlass::bfloat16_t>>, Is_dropout=true, Is_causal=false, Is_even_N=false, Is_even_K=true, Return_softmax=true]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(34): here
            instantiation of "void run_flash_fwd<Kernel_traits,Is_dropout,Is_causal>(Flash_fwd_params &, cudaStream_t) [with Kernel_traits=Flash_fwd_kernel_traits<32, 128, 128, 4, false, false, cutlass::bfloat16_t, Flash_kernel_traits<32, 128, 128, 4, cutlass::bfloat16_t>>, Is_dropout=true, Is_causal=false]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(58): here
            instantiation of "void run_mha_fwd_hdim32<T>(Flash_fwd_params &, cudaStream_t) [with T=cutlass::bfloat16_t]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_hdim32_bf16_sm80.cu(9): here

/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_kernel.h(275): warning: variable "tQrQ" was declared but never referenced
          detected during:
            instantiation of "void flash::compute_attn_1rowblock<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &, int, int, int) [with Kernel_traits=Flash_fwd_kernel_traits<32, 128, 128, 4, false, false, cutlass::bfloat16_t, Flash_kernel_traits<32, 128, 128, 4, cutlass::bfloat16_t>>, Is_dropout=true, Is_causal=false, Is_even_N=false, Is_even_K=true, Return_softmax=true, Params=Flash_fwd_params]" 
(588): here
            instantiation of "void flash::compute_attn<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &) [with Kernel_traits=Flash_fwd_kernel_traits<32, 128, 128, 4, false, false, cutlass::bfloat16_t, Flash_kernel_traits<32, 128, 128, 4, cutlass::bfloat16_t>>, Is_dropout=true, Is_causal=false, Is_even_N=false, Is_even_K=true, Return_softmax=true, Params=Flash_fwd_params]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(15): here
            instantiation of "void flash_fwd_kernel<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax>(Flash_fwd_params) [with Kernel_traits=Flash_fwd_kernel_traits<32, 128, 128, 4, false, false, cutlass::bfloat16_t, Flash_kernel_traits<32, 128, 128, 4, cutlass::bfloat16_t>>, Is_dropout=true, Is_causal=false, Is_even_N=false, Is_even_K=true, Return_softmax=true]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(34): here
            instantiation of "void run_flash_fwd<Kernel_traits,Is_dropout,Is_causal>(Flash_fwd_params &, cudaStream_t) [with Kernel_traits=Flash_fwd_kernel_traits<32, 128, 128, 4, false, false, cutlass::bfloat16_t, Flash_kernel_traits<32, 128, 128, 4, cutlass::bfloat16_t>>, Is_dropout=true, Is_causal=false]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(58): here
            instantiation of "void run_mha_fwd_hdim32<T>(Flash_fwd_params &, cudaStream_t) [with T=cutlass::bfloat16_t]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_hdim32_bf16_sm80.cu(9): here

/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_kernel.h(140): warning: variable "MMA_M" was declared but never referenced
          detected during:
            instantiation of "void flash::compute_attn_1rowblock<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &, int, int, int) [with Kernel_traits=Flash_fwd_kernel_traits<32, 128, 128, 4, false, false, cutlass::bfloat16_t, Flash_kernel_traits<32, 128, 128, 4, cutlass::bfloat16_t>>, Is_dropout=true, Is_causal=false, Is_even_N=false, Is_even_K=true, Return_softmax=false, Params=Flash_fwd_params]" 
(588): here
            instantiation of "void flash::compute_attn<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &) [with Kernel_traits=Flash_fwd_kernel_traits<32, 128, 128, 4, false, false, cutlass::bfloat16_t, Flash_kernel_traits<32, 128, 128, 4, cutlass::bfloat16_t>>, Is_dropout=true, Is_causal=false, Is_even_N=false, Is_even_K=true, Return_softmax=false, Params=Flash_fwd_params]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(15): here
            instantiation of "void flash_fwd_kernel<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax>(Flash_fwd_params) [with Kernel_traits=Flash_fwd_kernel_traits<32, 128, 128, 4, false, false, cutlass::bfloat16_t, Flash_kernel_traits<32, 128, 128, 4, cutlass::bfloat16_t>>, Is_dropout=true, Is_causal=false, Is_even_N=false, Is_even_K=true, Return_softmax=false]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(34): here
            instantiation of "void run_flash_fwd<Kernel_traits,Is_dropout,Is_causal>(Flash_fwd_params &, cudaStream_t) [with Kernel_traits=Flash_fwd_kernel_traits<32, 128, 128, 4, false, false, cutlass::bfloat16_t, Flash_kernel_traits<32, 128, 128, 4, cutlass::bfloat16_t>>, Is_dropout=true, Is_causal=false]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(58): here
            instantiation of "void run_mha_fwd_hdim32<T>(Flash_fwd_params &, cudaStream_t) [with T=cutlass::bfloat16_t]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_hdim32_bf16_sm80.cu(9): here

/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_kernel.h(275): warning: variable "tQrQ" was declared but never referenced
          detected during:
            instantiation of "void flash::compute_attn_1rowblock<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &, int, int, int) [with Kernel_traits=Flash_fwd_kernel_traits<32, 128, 128, 4, false, false, cutlass::bfloat16_t, Flash_kernel_traits<32, 128, 128, 4, cutlass::bfloat16_t>>, Is_dropout=true, Is_causal=false, Is_even_N=false, Is_even_K=true, Return_softmax=false, Params=Flash_fwd_params]" 
(588): here
            instantiation of "void flash::compute_attn<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &) [with Kernel_traits=Flash_fwd_kernel_traits<32, 128, 128, 4, false, false, cutlass::bfloat16_t, Flash_kernel_traits<32, 128, 128, 4, cutlass::bfloat16_t>>, Is_dropout=true, Is_causal=false, Is_even_N=false, Is_even_K=true, Return_softmax=false, Params=Flash_fwd_params]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(15): here
            instantiation of "void flash_fwd_kernel<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax>(Flash_fwd_params) [with Kernel_traits=Flash_fwd_kernel_traits<32, 128, 128, 4, false, false, cutlass::bfloat16_t, Flash_kernel_traits<32, 128, 128, 4, cutlass::bfloat16_t>>, Is_dropout=true, Is_causal=false, Is_even_N=false, Is_even_K=true, Return_softmax=false]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(34): here
            instantiation of "void run_flash_fwd<Kernel_traits,Is_dropout,Is_causal>(Flash_fwd_params &, cudaStream_t) [with Kernel_traits=Flash_fwd_kernel_traits<32, 128, 128, 4, false, false, cutlass::bfloat16_t, Flash_kernel_traits<32, 128, 128, 4, cutlass::bfloat16_t>>, Is_dropout=true, Is_causal=false]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(58): here
            instantiation of "void run_mha_fwd_hdim32<T>(Flash_fwd_params &, cudaStream_t) [with T=cutlass::bfloat16_t]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_hdim32_bf16_sm80.cu(9): here

/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_kernel.h(140): warning: variable "MMA_M" was declared but never referenced
          detected during:
            instantiation of "void flash::compute_attn_1rowblock<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &, int, int, int) [with Kernel_traits=Flash_fwd_kernel_traits<32, 128, 128, 4, false, false, cutlass::bfloat16_t, Flash_kernel_traits<32, 128, 128, 4, cutlass::bfloat16_t>>, Is_dropout=true, Is_causal=false, Is_even_N=false, Is_even_K=false, Return_softmax=true, Params=Flash_fwd_params]" 
(588): here
            instantiation of "void flash::compute_attn<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &) [with Kernel_traits=Flash_fwd_kernel_traits<32, 128, 128, 4, false, false, cutlass::bfloat16_t, Flash_kernel_traits<32, 128, 128, 4, cutlass::bfloat16_t>>, Is_dropout=true, Is_causal=false, Is_even_N=false, Is_even_K=false, Return_softmax=true, Params=Flash_fwd_params]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(15): here
            instantiation of "void flash_fwd_kernel<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax>(Flash_fwd_params) [with Kernel_traits=Flash_fwd_kernel_traits<32, 128, 128, 4, false, false, cutlass::bfloat16_t, Flash_kernel_traits<32, 128, 128, 4, cutlass::bfloat16_t>>, Is_dropout=true, Is_causal=false, Is_even_N=false, Is_even_K=false, Return_softmax=true]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(34): here
            instantiation of "void run_flash_fwd<Kernel_traits,Is_dropout,Is_causal>(Flash_fwd_params &, cudaStream_t) [with Kernel_traits=Flash_fwd_kernel_traits<32, 128, 128, 4, false, false, cutlass::bfloat16_t, Flash_kernel_traits<32, 128, 128, 4, cutlass::bfloat16_t>>, Is_dropout=true, Is_causal=false]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(58): here
            instantiation of "void run_mha_fwd_hdim32<T>(Flash_fwd_params &, cudaStream_t) [with T=cutlass::bfloat16_t]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_hdim32_bf16_sm80.cu(9): here

/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_kernel.h(275): warning: variable "tQrQ" was declared but never referenced
          detected during:
            instantiation of "void flash::compute_attn_1rowblock<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &, int, int, int) [with Kernel_traits=Flash_fwd_kernel_traits<32, 128, 128, 4, false, false, cutlass::bfloat16_t, Flash_kernel_traits<32, 128, 128, 4, cutlass::bfloat16_t>>, Is_dropout=true, Is_causal=false, Is_even_N=false, Is_even_K=false, Return_softmax=true, Params=Flash_fwd_params]" 
(588): here
            instantiation of "void flash::compute_attn<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &) [with Kernel_traits=Flash_fwd_kernel_traits<32, 128, 128, 4, false, false, cutlass::bfloat16_t, Flash_kernel_traits<32, 128, 128, 4, cutlass::bfloat16_t>>, Is_dropout=true, Is_causal=false, Is_even_N=false, Is_even_K=false, Return_softmax=true, Params=Flash_fwd_params]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(15): here
            instantiation of "void flash_fwd_kernel<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax>(Flash_fwd_params) [with Kernel_traits=Flash_fwd_kernel_traits<32, 128, 128, 4, false, false, cutlass::bfloat16_t, Flash_kernel_traits<32, 128, 128, 4, cutlass::bfloat16_t>>, Is_dropout=true, Is_causal=false, Is_even_N=false, Is_even_K=false, Return_softmax=true]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(34): here
            instantiation of "void run_flash_fwd<Kernel_traits,Is_dropout,Is_causal>(Flash_fwd_params &, cudaStream_t) [with Kernel_traits=Flash_fwd_kernel_traits<32, 128, 128, 4, false, false, cutlass::bfloat16_t, Flash_kernel_traits<32, 128, 128, 4, cutlass::bfloat16_t>>, Is_dropout=true, Is_causal=false]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(58): here
            instantiation of "void run_mha_fwd_hdim32<T>(Flash_fwd_params &, cudaStream_t) [with T=cutlass::bfloat16_t]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_hdim32_bf16_sm80.cu(9): here

/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_kernel.h(140): warning: variable "MMA_M" was declared but never referenced
          detected during:
            instantiation of "void flash::compute_attn_1rowblock<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &, int, int, int) [with Kernel_traits=Flash_fwd_kernel_traits<32, 128, 128, 4, false, false, cutlass::bfloat16_t, Flash_kernel_traits<32, 128, 128, 4, cutlass::bfloat16_t>>, Is_dropout=true, Is_causal=false, Is_even_N=false, Is_even_K=false, Return_softmax=false, Params=Flash_fwd_params]" 
(588): here
            instantiation of "void flash::compute_attn<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &) [with Kernel_traits=Flash_fwd_kernel_traits<32, 128, 128, 4, false, false, cutlass::bfloat16_t, Flash_kernel_traits<32, 128, 128, 4, cutlass::bfloat16_t>>, Is_dropout=true, Is_causal=false, Is_even_N=false, Is_even_K=false, Return_softmax=false, Params=Flash_fwd_params]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(15): here
            instantiation of "void flash_fwd_kernel<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax>(Flash_fwd_params) [with Kernel_traits=Flash_fwd_kernel_traits<32, 128, 128, 4, false, false, cutlass::bfloat16_t, Flash_kernel_traits<32, 128, 128, 4, cutlass::bfloat16_t>>, Is_dropout=true, Is_causal=false, Is_even_N=false, Is_even_K=false, Return_softmax=false]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(34): here
            instantiation of "void run_flash_fwd<Kernel_traits,Is_dropout,Is_causal>(Flash_fwd_params &, cudaStream_t) [with Kernel_traits=Flash_fwd_kernel_traits<32, 128, 128, 4, false, false, cutlass::bfloat16_t, Flash_kernel_traits<32, 128, 128, 4, cutlass::bfloat16_t>>, Is_dropout=true, Is_causal=false]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(58): here
            instantiation of "void run_mha_fwd_hdim32<T>(Flash_fwd_params &, cudaStream_t) [with T=cutlass::bfloat16_t]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_hdim32_bf16_sm80.cu(9): here

/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_kernel.h(275): warning: variable "tQrQ" was declared but never referenced
          detected during:
            instantiation of "void flash::compute_attn_1rowblock<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &, int, int, int) [with Kernel_traits=Flash_fwd_kernel_traits<32, 128, 128, 4, false, false, cutlass::bfloat16_t, Flash_kernel_traits<32, 128, 128, 4, cutlass::bfloat16_t>>, Is_dropout=true, Is_causal=false, Is_even_N=false, Is_even_K=false, Return_softmax=false, Params=Flash_fwd_params]" 
(588): here
            instantiation of "void flash::compute_attn<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &) [with Kernel_traits=Flash_fwd_kernel_traits<32, 128, 128, 4, false, false, cutlass::bfloat16_t, Flash_kernel_traits<32, 128, 128, 4, cutlass::bfloat16_t>>, Is_dropout=true, Is_causal=false, Is_even_N=false, Is_even_K=false, Return_softmax=false, Params=Flash_fwd_params]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(15): here
            instantiation of "void flash_fwd_kernel<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax>(Flash_fwd_params) [with Kernel_traits=Flash_fwd_kernel_traits<32, 128, 128, 4, false, false, cutlass::bfloat16_t, Flash_kernel_traits<32, 128, 128, 4, cutlass::bfloat16_t>>, Is_dropout=true, Is_causal=false, Is_even_N=false, Is_even_K=false, Return_softmax=false]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(34): here
            instantiation of "void run_flash_fwd<Kernel_traits,Is_dropout,Is_causal>(Flash_fwd_params &, cudaStream_t) [with Kernel_traits=Flash_fwd_kernel_traits<32, 128, 128, 4, false, false, cutlass::bfloat16_t, Flash_kernel_traits<32, 128, 128, 4, cutlass::bfloat16_t>>, Is_dropout=true, Is_causal=false]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(58): here
            instantiation of "void run_mha_fwd_hdim32<T>(Flash_fwd_params &, cudaStream_t) [with T=cutlass::bfloat16_t]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_hdim32_bf16_sm80.cu(9): here

/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_kernel.h(140): warning: variable "MMA_M" was declared but never referenced
          detected during:
            instantiation of "void flash::compute_attn_1rowblock<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &, int, int, int) [with Kernel_traits=Flash_fwd_kernel_traits<32, 128, 128, 4, false, false, cutlass::bfloat16_t, Flash_kernel_traits<32, 128, 128, 4, cutlass::bfloat16_t>>, Is_dropout=false, Is_causal=true, Is_even_N=true, Is_even_K=true, Return_softmax=false, Params=Flash_fwd_params]" 
(588): here
            instantiation of "void flash::compute_attn<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &) [with Kernel_traits=Flash_fwd_kernel_traits<32, 128, 128, 4, false, false, cutlass::bfloat16_t, Flash_kernel_traits<32, 128, 128, 4, cutlass::bfloat16_t>>, Is_dropout=false, Is_causal=true, Is_even_N=true, Is_even_K=true, Return_softmax=false, Params=Flash_fwd_params]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(15): here
            instantiation of "void flash_fwd_kernel<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax>(Flash_fwd_params) [with Kernel_traits=Flash_fwd_kernel_traits<32, 128, 128, 4, false, false, cutlass::bfloat16_t, Flash_kernel_traits<32, 128, 128, 4, cutlass::bfloat16_t>>, Is_dropout=false, Is_causal=true, Is_even_N=true, Is_even_K=true, Return_softmax=false]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(34): here
            instantiation of "void run_flash_fwd<Kernel_traits,Is_dropout,Is_causal>(Flash_fwd_params &, cudaStream_t) [with Kernel_traits=Flash_fwd_kernel_traits<32, 128, 128, 4, false, false, cutlass::bfloat16_t, Flash_kernel_traits<32, 128, 128, 4, cutlass::bfloat16_t>>, Is_dropout=false, Is_causal=true]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(58): here
            instantiation of "void run_mha_fwd_hdim32<T>(Flash_fwd_params &, cudaStream_t) [with T=cutlass::bfloat16_t]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_hdim32_bf16_sm80.cu(9): here

/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_kernel.h(275): warning: variable "tQrQ" was declared but never referenced
          detected during:
            instantiation of "void flash::compute_attn_1rowblock<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &, int, int, int) [with Kernel_traits=Flash_fwd_kernel_traits<32, 128, 128, 4, false, false, cutlass::bfloat16_t, Flash_kernel_traits<32, 128, 128, 4, cutlass::bfloat16_t>>, Is_dropout=false, Is_causal=true, Is_even_N=true, Is_even_K=true, Return_softmax=false, Params=Flash_fwd_params]" 
(588): here
            instantiation of "void flash::compute_attn<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &) [with Kernel_traits=Flash_fwd_kernel_traits<32, 128, 128, 4, false, false, cutlass::bfloat16_t, Flash_kernel_traits<32, 128, 128, 4, cutlass::bfloat16_t>>, Is_dropout=false, Is_causal=true, Is_even_N=true, Is_even_K=true, Return_softmax=false, Params=Flash_fwd_params]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(15): here
            instantiation of "void flash_fwd_kernel<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax>(Flash_fwd_params) [with Kernel_traits=Flash_fwd_kernel_traits<32, 128, 128, 4, false, false, cutlass::bfloat16_t, Flash_kernel_traits<32, 128, 128, 4, cutlass::bfloat16_t>>, Is_dropout=false, Is_causal=true, Is_even_N=true, Is_even_K=true, Return_softmax=false]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(34): here
            instantiation of "void run_flash_fwd<Kernel_traits,Is_dropout,Is_causal>(Flash_fwd_params &, cudaStream_t) [with Kernel_traits=Flash_fwd_kernel_traits<32, 128, 128, 4, false, false, cutlass::bfloat16_t, Flash_kernel_traits<32, 128, 128, 4, cutlass::bfloat16_t>>, Is_dropout=false, Is_causal=true]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(58): here
            instantiation of "void run_mha_fwd_hdim32<T>(Flash_fwd_params &, cudaStream_t) [with T=cutlass::bfloat16_t]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_hdim32_bf16_sm80.cu(9): here

/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_kernel.h(140): warning: variable "MMA_M" was declared but never referenced
          detected during:
            instantiation of "void flash::compute_attn_1rowblock<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &, int, int, int) [with Kernel_traits=Flash_fwd_kernel_traits<32, 128, 128, 4, false, false, cutlass::bfloat16_t, Flash_kernel_traits<32, 128, 128, 4, cutlass::bfloat16_t>>, Is_dropout=false, Is_causal=true, Is_even_N=true, Is_even_K=false, Return_softmax=false, Params=Flash_fwd_params]" 
(588): here
            instantiation of "void flash::compute_attn<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &) [with Kernel_traits=Flash_fwd_kernel_traits<32, 128, 128, 4, false, false, cutlass::bfloat16_t, Flash_kernel_traits<32, 128, 128, 4, cutlass::bfloat16_t>>, Is_dropout=false, Is_causal=true, Is_even_N=true, Is_even_K=false, Return_softmax=false, Params=Flash_fwd_params]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(15): here
            instantiation of "void flash_fwd_kernel<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax>(Flash_fwd_params) [with Kernel_traits=Flash_fwd_kernel_traits<32, 128, 128, 4, false, false, cutlass::bfloat16_t, Flash_kernel_traits<32, 128, 128, 4, cutlass::bfloat16_t>>, Is_dropout=false, Is_causal=true, Is_even_N=true, Is_even_K=false, Return_softmax=false]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(34): here
            instantiation of "void run_flash_fwd<Kernel_traits,Is_dropout,Is_causal>(Flash_fwd_params &, cudaStream_t) [with Kernel_traits=Flash_fwd_kernel_traits<32, 128, 128, 4, false, false, cutlass::bfloat16_t, Flash_kernel_traits<32, 128, 128, 4, cutlass::bfloat16_t>>, Is_dropout=false, Is_causal=true]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(58): here
            instantiation of "void run_mha_fwd_hdim32<T>(Flash_fwd_params &, cudaStream_t) [with T=cutlass::bfloat16_t]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_hdim32_bf16_sm80.cu(9): here

/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_kernel.h(275): warning: variable "tQrQ" was declared but never referenced
          detected during:
            instantiation of "void flash::compute_attn_1rowblock<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &, int, int, int) [with Kernel_traits=Flash_fwd_kernel_traits<32, 128, 128, 4, false, false, cutlass::bfloat16_t, Flash_kernel_traits<32, 128, 128, 4, cutlass::bfloat16_t>>, Is_dropout=false, Is_causal=true, Is_even_N=true, Is_even_K=false, Return_softmax=false, Params=Flash_fwd_params]" 
(588): here
            instantiation of "void flash::compute_attn<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &) [with Kernel_traits=Flash_fwd_kernel_traits<32, 128, 128, 4, false, false, cutlass::bfloat16_t, Flash_kernel_traits<32, 128, 128, 4, cutlass::bfloat16_t>>, Is_dropout=false, Is_causal=true, Is_even_N=true, Is_even_K=false, Return_softmax=false, Params=Flash_fwd_params]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(15): here
            instantiation of "void flash_fwd_kernel<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax>(Flash_fwd_params) [with Kernel_traits=Flash_fwd_kernel_traits<32, 128, 128, 4, false, false, cutlass::bfloat16_t, Flash_kernel_traits<32, 128, 128, 4, cutlass::bfloat16_t>>, Is_dropout=false, Is_causal=true, Is_even_N=true, Is_even_K=false, Return_softmax=false]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(34): here
            instantiation of "void run_flash_fwd<Kernel_traits,Is_dropout,Is_causal>(Flash_fwd_params &, cudaStream_t) [with Kernel_traits=Flash_fwd_kernel_traits<32, 128, 128, 4, false, false, cutlass::bfloat16_t, Flash_kernel_traits<32, 128, 128, 4, cutlass::bfloat16_t>>, Is_dropout=false, Is_causal=true]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(58): here
            instantiation of "void run_mha_fwd_hdim32<T>(Flash_fwd_params &, cudaStream_t) [with T=cutlass::bfloat16_t]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_hdim32_bf16_sm80.cu(9): here

/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_kernel.h(140): warning: variable "MMA_M" was declared but never referenced
          detected during:
            instantiation of "void flash::compute_attn_1rowblock<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &, int, int, int) [with Kernel_traits=Flash_fwd_kernel_traits<32, 128, 128, 4, false, false, cutlass::bfloat16_t, Flash_kernel_traits<32, 128, 128, 4, cutlass::bfloat16_t>>, Is_dropout=false, Is_causal=true, Is_even_N=false, Is_even_K=true, Return_softmax=false, Params=Flash_fwd_params]" 
(588): here
            instantiation of "void flash::compute_attn<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &) [with Kernel_traits=Flash_fwd_kernel_traits<32, 128, 128, 4, false, false, cutlass::bfloat16_t, Flash_kernel_traits<32, 128, 128, 4, cutlass::bfloat16_t>>, Is_dropout=false, Is_causal=true, Is_even_N=false, Is_even_K=true, Return_softmax=false, Params=Flash_fwd_params]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(15): here
            instantiation of "void flash_fwd_kernel<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax>(Flash_fwd_params) [with Kernel_traits=Flash_fwd_kernel_traits<32, 128, 128, 4, false, false, cutlass::bfloat16_t, Flash_kernel_traits<32, 128, 128, 4, cutlass::bfloat16_t>>, Is_dropout=false, Is_causal=true, Is_even_N=false, Is_even_K=true, Return_softmax=false]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(34): here
            instantiation of "void run_flash_fwd<Kernel_traits,Is_dropout,Is_causal>(Flash_fwd_params &, cudaStream_t) [with Kernel_traits=Flash_fwd_kernel_traits<32, 128, 128, 4, false, false, cutlass::bfloat16_t, Flash_kernel_traits<32, 128, 128, 4, cutlass::bfloat16_t>>, Is_dropout=false, Is_causal=true]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(58): here
            instantiation of "void run_mha_fwd_hdim32<T>(Flash_fwd_params &, cudaStream_t) [with T=cutlass::bfloat16_t]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_hdim32_bf16_sm80.cu(9): here

/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_kernel.h(275): warning: variable "tQrQ" was declared but never referenced
          detected during:
            instantiation of "void flash::compute_attn_1rowblock<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &, int, int, int) [with Kernel_traits=Flash_fwd_kernel_traits<32, 128, 128, 4, false, false, cutlass::bfloat16_t, Flash_kernel_traits<32, 128, 128, 4, cutlass::bfloat16_t>>, Is_dropout=false, Is_causal=true, Is_even_N=false, Is_even_K=true, Return_softmax=false, Params=Flash_fwd_params]" 
(588): here
            instantiation of "void flash::compute_attn<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &) [with Kernel_traits=Flash_fwd_kernel_traits<32, 128, 128, 4, false, false, cutlass::bfloat16_t, Flash_kernel_traits<32, 128, 128, 4, cutlass::bfloat16_t>>, Is_dropout=false, Is_causal=true, Is_even_N=false, Is_even_K=true, Return_softmax=false, Params=Flash_fwd_params]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(15): here
            instantiation of "void flash_fwd_kernel<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax>(Flash_fwd_params) [with Kernel_traits=Flash_fwd_kernel_traits<32, 128, 128, 4, false, false, cutlass::bfloat16_t, Flash_kernel_traits<32, 128, 128, 4, cutlass::bfloat16_t>>, Is_dropout=false, Is_causal=true, Is_even_N=false, Is_even_K=true, Return_softmax=false]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(34): here
            instantiation of "void run_flash_fwd<Kernel_traits,Is_dropout,Is_causal>(Flash_fwd_params &, cudaStream_t) [with Kernel_traits=Flash_fwd_kernel_traits<32, 128, 128, 4, false, false, cutlass::bfloat16_t, Flash_kernel_traits<32, 128, 128, 4, cutlass::bfloat16_t>>, Is_dropout=false, Is_causal=true]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(58): here
            instantiation of "void run_mha_fwd_hdim32<T>(Flash_fwd_params &, cudaStream_t) [with T=cutlass::bfloat16_t]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_hdim32_bf16_sm80.cu(9): here

/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_kernel.h(140): warning: variable "MMA_M" was declared but never referenced
          detected during:
            instantiation of "void flash::compute_attn_1rowblock<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &, int, int, int) [with Kernel_traits=Flash_fwd_kernel_traits<32, 128, 128, 4, false, false, cutlass::bfloat16_t, Flash_kernel_traits<32, 128, 128, 4, cutlass::bfloat16_t>>, Is_dropout=false, Is_causal=true, Is_even_N=false, Is_even_K=false, Return_softmax=false, Params=Flash_fwd_params]" 
(588): here
            instantiation of "void flash::compute_attn<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &) [with Kernel_traits=Flash_fwd_kernel_traits<32, 128, 128, 4, false, false, cutlass::bfloat16_t, Flash_kernel_traits<32, 128, 128, 4, cutlass::bfloat16_t>>, Is_dropout=false, Is_causal=true, Is_even_N=false, Is_even_K=false, Return_softmax=false, Params=Flash_fwd_params]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(15): here
            instantiation of "void flash_fwd_kernel<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax>(Flash_fwd_params) [with Kernel_traits=Flash_fwd_kernel_traits<32, 128, 128, 4, false, false, cutlass::bfloat16_t, Flash_kernel_traits<32, 128, 128, 4, cutlass::bfloat16_t>>, Is_dropout=false, Is_causal=true, Is_even_N=false, Is_even_K=false, Return_softmax=false]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(34): here
            instantiation of "void run_flash_fwd<Kernel_traits,Is_dropout,Is_causal>(Flash_fwd_params &, cudaStream_t) [with Kernel_traits=Flash_fwd_kernel_traits<32, 128, 128, 4, false, false, cutlass::bfloat16_t, Flash_kernel_traits<32, 128, 128, 4, cutlass::bfloat16_t>>, Is_dropout=false, Is_causal=true]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(58): here
            instantiation of "void run_mha_fwd_hdim32<T>(Flash_fwd_params &, cudaStream_t) [with T=cutlass::bfloat16_t]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_hdim32_bf16_sm80.cu(9): here

/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_kernel.h(275): warning: variable "tQrQ" was declared but never referenced
          detected during:
            instantiation of "void flash::compute_attn_1rowblock<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &, int, int, int) [with Kernel_traits=Flash_fwd_kernel_traits<32, 128, 128, 4, false, false, cutlass::bfloat16_t, Flash_kernel_traits<32, 128, 128, 4, cutlass::bfloat16_t>>, Is_dropout=false, Is_causal=true, Is_even_N=false, Is_even_K=false, Return_softmax=false, Params=Flash_fwd_params]" 
(588): here
            instantiation of "void flash::compute_attn<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &) [with Kernel_traits=Flash_fwd_kernel_traits<32, 128, 128, 4, false, false, cutlass::bfloat16_t, Flash_kernel_traits<32, 128, 128, 4, cutlass::bfloat16_t>>, Is_dropout=false, Is_causal=true, Is_even_N=false, Is_even_K=false, Return_softmax=false, Params=Flash_fwd_params]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(15): here
            instantiation of "void flash_fwd_kernel<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax>(Flash_fwd_params) [with Kernel_traits=Flash_fwd_kernel_traits<32, 128, 128, 4, false, false, cutlass::bfloat16_t, Flash_kernel_traits<32, 128, 128, 4, cutlass::bfloat16_t>>, Is_dropout=false, Is_causal=true, Is_even_N=false, Is_even_K=false, Return_softmax=false]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(34): here
            instantiation of "void run_flash_fwd<Kernel_traits,Is_dropout,Is_causal>(Flash_fwd_params &, cudaStream_t) [with Kernel_traits=Flash_fwd_kernel_traits<32, 128, 128, 4, false, false, cutlass::bfloat16_t, Flash_kernel_traits<32, 128, 128, 4, cutlass::bfloat16_t>>, Is_dropout=false, Is_causal=true]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(58): here
            instantiation of "void run_mha_fwd_hdim32<T>(Flash_fwd_params &, cudaStream_t) [with T=cutlass::bfloat16_t]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_hdim32_bf16_sm80.cu(9): here

/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_kernel.h(140): warning: variable "MMA_M" was declared but never referenced
          detected during:
            instantiation of "void flash::compute_attn_1rowblock<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &, int, int, int) [with Kernel_traits=Flash_fwd_kernel_traits<32, 128, 128, 4, false, false, cutlass::bfloat16_t, Flash_kernel_traits<32, 128, 128, 4, cutlass::bfloat16_t>>, Is_dropout=false, Is_causal=false, Is_even_N=true, Is_even_K=true, Return_softmax=false, Params=Flash_fwd_params]" 
(588): here
            instantiation of "void flash::compute_attn<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &) [with Kernel_traits=Flash_fwd_kernel_traits<32, 128, 128, 4, false, false, cutlass::bfloat16_t, Flash_kernel_traits<32, 128, 128, 4, cutlass::bfloat16_t>>, Is_dropout=false, Is_causal=false, Is_even_N=true, Is_even_K=true, Return_softmax=false, Params=Flash_fwd_params]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(15): here
            instantiation of "void flash_fwd_kernel<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax>(Flash_fwd_params) [with Kernel_traits=Flash_fwd_kernel_traits<32, 128, 128, 4, false, false, cutlass::bfloat16_t, Flash_kernel_traits<32, 128, 128, 4, cutlass::bfloat16_t>>, Is_dropout=false, Is_causal=false, Is_even_N=true, Is_even_K=true, Return_softmax=false]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(34): here
            instantiation of "void run_flash_fwd<Kernel_traits,Is_dropout,Is_causal>(Flash_fwd_params &, cudaStream_t) [with Kernel_traits=Flash_fwd_kernel_traits<32, 128, 128, 4, false, false, cutlass::bfloat16_t, Flash_kernel_traits<32, 128, 128, 4, cutlass::bfloat16_t>>, Is_dropout=false, Is_causal=false]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(58): here
            instantiation of "void run_mha_fwd_hdim32<T>(Flash_fwd_params &, cudaStream_t) [with T=cutlass::bfloat16_t]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_hdim32_bf16_sm80.cu(9): here

/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_kernel.h(275): warning: variable "tQrQ" was declared but never referenced
          detected during:
            instantiation of "void flash::compute_attn_1rowblock<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &, int, int, int) [with Kernel_traits=Flash_fwd_kernel_traits<32, 128, 128, 4, false, false, cutlass::bfloat16_t, Flash_kernel_traits<32, 128, 128, 4, cutlass::bfloat16_t>>, Is_dropout=false, Is_causal=false, Is_even_N=true, Is_even_K=true, Return_softmax=false, Params=Flash_fwd_params]" 
(588): here
            instantiation of "void flash::compute_attn<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &) [with Kernel_traits=Flash_fwd_kernel_traits<32, 128, 128, 4, false, false, cutlass::bfloat16_t, Flash_kernel_traits<32, 128, 128, 4, cutlass::bfloat16_t>>, Is_dropout=false, Is_causal=false, Is_even_N=true, Is_even_K=true, Return_softmax=false, Params=Flash_fwd_params]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(15): here
            instantiation of "void flash_fwd_kernel<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax>(Flash_fwd_params) [with Kernel_traits=Flash_fwd_kernel_traits<32, 128, 128, 4, false, false, cutlass::bfloat16_t, Flash_kernel_traits<32, 128, 128, 4, cutlass::bfloat16_t>>, Is_dropout=false, Is_causal=false, Is_even_N=true, Is_even_K=true, Return_softmax=false]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(34): here
            instantiation of "void run_flash_fwd<Kernel_traits,Is_dropout,Is_causal>(Flash_fwd_params &, cudaStream_t) [with Kernel_traits=Flash_fwd_kernel_traits<32, 128, 128, 4, false, false, cutlass::bfloat16_t, Flash_kernel_traits<32, 128, 128, 4, cutlass::bfloat16_t>>, Is_dropout=false, Is_causal=false]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(58): here
            instantiation of "void run_mha_fwd_hdim32<T>(Flash_fwd_params &, cudaStream_t) [with T=cutlass::bfloat16_t]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_hdim32_bf16_sm80.cu(9): here

/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_kernel.h(140): warning: variable "MMA_M" was declared but never referenced
          detected during:
            instantiation of "void flash::compute_attn_1rowblock<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &, int, int, int) [with Kernel_traits=Flash_fwd_kernel_traits<32, 128, 128, 4, false, false, cutlass::bfloat16_t, Flash_kernel_traits<32, 128, 128, 4, cutlass::bfloat16_t>>, Is_dropout=false, Is_causal=false, Is_even_N=true, Is_even_K=false, Return_softmax=false, Params=Flash_fwd_params]" 
(588): here
            instantiation of "void flash::compute_attn<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &) [with Kernel_traits=Flash_fwd_kernel_traits<32, 128, 128, 4, false, false, cutlass::bfloat16_t, Flash_kernel_traits<32, 128, 128, 4, cutlass::bfloat16_t>>, Is_dropout=false, Is_causal=false, Is_even_N=true, Is_even_K=false, Return_softmax=false, Params=Flash_fwd_params]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(15): here
            instantiation of "void flash_fwd_kernel<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax>(Flash_fwd_params) [with Kernel_traits=Flash_fwd_kernel_traits<32, 128, 128, 4, false, false, cutlass::bfloat16_t, Flash_kernel_traits<32, 128, 128, 4, cutlass::bfloat16_t>>, Is_dropout=false, Is_causal=false, Is_even_N=true, Is_even_K=false, Return_softmax=false]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(34): here
            instantiation of "void run_flash_fwd<Kernel_traits,Is_dropout,Is_causal>(Flash_fwd_params &, cudaStream_t) [with Kernel_traits=Flash_fwd_kernel_traits<32, 128, 128, 4, false, false, cutlass::bfloat16_t, Flash_kernel_traits<32, 128, 128, 4, cutlass::bfloat16_t>>, Is_dropout=false, Is_causal=false]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(58): here
            instantiation of "void run_mha_fwd_hdim32<T>(Flash_fwd_params &, cudaStream_t) [with T=cutlass::bfloat16_t]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_hdim32_bf16_sm80.cu(9): here

/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_kernel.h(275): warning: variable "tQrQ" was declared but never referenced
          detected during:
            instantiation of "void flash::compute_attn_1rowblock<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &, int, int, int) [with Kernel_traits=Flash_fwd_kernel_traits<32, 128, 128, 4, false, false, cutlass::bfloat16_t, Flash_kernel_traits<32, 128, 128, 4, cutlass::bfloat16_t>>, Is_dropout=false, Is_causal=false, Is_even_N=true, Is_even_K=false, Return_softmax=false, Params=Flash_fwd_params]" 
(588): here
            instantiation of "void flash::compute_attn<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &) [with Kernel_traits=Flash_fwd_kernel_traits<32, 128, 128, 4, false, false, cutlass::bfloat16_t, Flash_kernel_traits<32, 128, 128, 4, cutlass::bfloat16_t>>, Is_dropout=false, Is_causal=false, Is_even_N=true, Is_even_K=false, Return_softmax=false, Params=Flash_fwd_params]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(15): here
            instantiation of "void flash_fwd_kernel<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax>(Flash_fwd_params) [with Kernel_traits=Flash_fwd_kernel_traits<32, 128, 128, 4, false, false, cutlass::bfloat16_t, Flash_kernel_traits<32, 128, 128, 4, cutlass::bfloat16_t>>, Is_dropout=false, Is_causal=false, Is_even_N=true, Is_even_K=false, Return_softmax=false]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(34): here
            instantiation of "void run_flash_fwd<Kernel_traits,Is_dropout,Is_causal>(Flash_fwd_params &, cudaStream_t) [with Kernel_traits=Flash_fwd_kernel_traits<32, 128, 128, 4, false, false, cutlass::bfloat16_t, Flash_kernel_traits<32, 128, 128, 4, cutlass::bfloat16_t>>, Is_dropout=false, Is_causal=false]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(58): here
            instantiation of "void run_mha_fwd_hdim32<T>(Flash_fwd_params &, cudaStream_t) [with T=cutlass::bfloat16_t]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_hdim32_bf16_sm80.cu(9): here

/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_kernel.h(140): warning: variable "MMA_M" was declared but never referenced
          detected during:
            instantiation of "void flash::compute_attn_1rowblock<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &, int, int, int) [with Kernel_traits=Flash_fwd_kernel_traits<32, 128, 128, 4, false, false, cutlass::bfloat16_t, Flash_kernel_traits<32, 128, 128, 4, cutlass::bfloat16_t>>, Is_dropout=false, Is_causal=false, Is_even_N=false, Is_even_K=true, Return_softmax=false, Params=Flash_fwd_params]" 
(588): here
            instantiation of "void flash::compute_attn<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &) [with Kernel_traits=Flash_fwd_kernel_traits<32, 128, 128, 4, false, false, cutlass::bfloat16_t, Flash_kernel_traits<32, 128, 128, 4, cutlass::bfloat16_t>>, Is_dropout=false, Is_causal=false, Is_even_N=false, Is_even_K=true, Return_softmax=false, Params=Flash_fwd_params]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(15): here
            instantiation of "void flash_fwd_kernel<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax>(Flash_fwd_params) [with Kernel_traits=Flash_fwd_kernel_traits<32, 128, 128, 4, false, false, cutlass::bfloat16_t, Flash_kernel_traits<32, 128, 128, 4, cutlass::bfloat16_t>>, Is_dropout=false, Is_causal=false, Is_even_N=false, Is_even_K=true, Return_softmax=false]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(34): here
            instantiation of "void run_flash_fwd<Kernel_traits,Is_dropout,Is_causal>(Flash_fwd_params &, cudaStream_t) [with Kernel_traits=Flash_fwd_kernel_traits<32, 128, 128, 4, false, false, cutlass::bfloat16_t, Flash_kernel_traits<32, 128, 128, 4, cutlass::bfloat16_t>>, Is_dropout=false, Is_causal=false]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(58): here
            instantiation of "void run_mha_fwd_hdim32<T>(Flash_fwd_params &, cudaStream_t) [with T=cutlass::bfloat16_t]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_hdim32_bf16_sm80.cu(9): here

/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_kernel.h(275): warning: variable "tQrQ" was declared but never referenced
          detected during:
            instantiation of "void flash::compute_attn_1rowblock<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &, int, int, int) [with Kernel_traits=Flash_fwd_kernel_traits<32, 128, 128, 4, false, false, cutlass::bfloat16_t, Flash_kernel_traits<32, 128, 128, 4, cutlass::bfloat16_t>>, Is_dropout=false, Is_causal=false, Is_even_N=false, Is_even_K=true, Return_softmax=false, Params=Flash_fwd_params]" 
(588): here
            instantiation of "void flash::compute_attn<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &) [with Kernel_traits=Flash_fwd_kernel_traits<32, 128, 128, 4, false, false, cutlass::bfloat16_t, Flash_kernel_traits<32, 128, 128, 4, cutlass::bfloat16_t>>, Is_dropout=false, Is_causal=false, Is_even_N=false, Is_even_K=true, Return_softmax=false, Params=Flash_fwd_params]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(15): here
            instantiation of "void flash_fwd_kernel<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax>(Flash_fwd_params) [with Kernel_traits=Flash_fwd_kernel_traits<32, 128, 128, 4, false, false, cutlass::bfloat16_t, Flash_kernel_traits<32, 128, 128, 4, cutlass::bfloat16_t>>, Is_dropout=false, Is_causal=false, Is_even_N=false, Is_even_K=true, Return_softmax=false]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(34): here
            instantiation of "void run_flash_fwd<Kernel_traits,Is_dropout,Is_causal>(Flash_fwd_params &, cudaStream_t) [with Kernel_traits=Flash_fwd_kernel_traits<32, 128, 128, 4, false, false, cutlass::bfloat16_t, Flash_kernel_traits<32, 128, 128, 4, cutlass::bfloat16_t>>, Is_dropout=false, Is_causal=false]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(58): here
            instantiation of "void run_mha_fwd_hdim32<T>(Flash_fwd_params &, cudaStream_t) [with T=cutlass::bfloat16_t]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_hdim32_bf16_sm80.cu(9): here

/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_kernel.h(140): warning: variable "MMA_M" was declared but never referenced
          detected during:
            instantiation of "void flash::compute_attn_1rowblock<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &, int, int, int) [with Kernel_traits=Flash_fwd_kernel_traits<32, 128, 128, 4, false, false, cutlass::bfloat16_t, Flash_kernel_traits<32, 128, 128, 4, cutlass::bfloat16_t>>, Is_dropout=false, Is_causal=false, Is_even_N=false, Is_even_K=false, Return_softmax=false, Params=Flash_fwd_params]" 
(588): here
            instantiation of "void flash::compute_attn<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &) [with Kernel_traits=Flash_fwd_kernel_traits<32, 128, 128, 4, false, false, cutlass::bfloat16_t, Flash_kernel_traits<32, 128, 128, 4, cutlass::bfloat16_t>>, Is_dropout=false, Is_causal=false, Is_even_N=false, Is_even_K=false, Return_softmax=false, Params=Flash_fwd_params]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(15): here
            instantiation of "void flash_fwd_kernel<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax>(Flash_fwd_params) [with Kernel_traits=Flash_fwd_kernel_traits<32, 128, 128, 4, false, false, cutlass::bfloat16_t, Flash_kernel_traits<32, 128, 128, 4, cutlass::bfloat16_t>>, Is_dropout=false, Is_causal=false, Is_even_N=false, Is_even_K=false, Return_softmax=false]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(34): here
            instantiation of "void run_flash_fwd<Kernel_traits,Is_dropout,Is_causal>(Flash_fwd_params &, cudaStream_t) [with Kernel_traits=Flash_fwd_kernel_traits<32, 128, 128, 4, false, false, cutlass::bfloat16_t, Flash_kernel_traits<32, 128, 128, 4, cutlass::bfloat16_t>>, Is_dropout=false, Is_causal=false]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(58): here
            instantiation of "void run_mha_fwd_hdim32<T>(Flash_fwd_params &, cudaStream_t) [with T=cutlass::bfloat16_t]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_hdim32_bf16_sm80.cu(9): here

/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_kernel.h(275): warning: variable "tQrQ" was declared but never referenced
          detected during:
            instantiation of "void flash::compute_attn_1rowblock<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &, int, int, int) [with Kernel_traits=Flash_fwd_kernel_traits<32, 128, 128, 4, false, false, cutlass::bfloat16_t, Flash_kernel_traits<32, 128, 128, 4, cutlass::bfloat16_t>>, Is_dropout=false, Is_causal=false, Is_even_N=false, Is_even_K=false, Return_softmax=false, Params=Flash_fwd_params]" 
(588): here
            instantiation of "void flash::compute_attn<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &) [with Kernel_traits=Flash_fwd_kernel_traits<32, 128, 128, 4, false, false, cutlass::bfloat16_t, Flash_kernel_traits<32, 128, 128, 4, cutlass::bfloat16_t>>, Is_dropout=false, Is_causal=false, Is_even_N=false, Is_even_K=false, Return_softmax=false, Params=Flash_fwd_params]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(15): here
            instantiation of "void flash_fwd_kernel<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax>(Flash_fwd_params) [with Kernel_traits=Flash_fwd_kernel_traits<32, 128, 128, 4, false, false, cutlass::bfloat16_t, Flash_kernel_traits<32, 128, 128, 4, cutlass::bfloat16_t>>, Is_dropout=false, Is_causal=false, Is_even_N=false, Is_even_K=false, Return_softmax=false]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(34): here
            instantiation of "void run_flash_fwd<Kernel_traits,Is_dropout,Is_causal>(Flash_fwd_params &, cudaStream_t) [with Kernel_traits=Flash_fwd_kernel_traits<32, 128, 128, 4, false, false, cutlass::bfloat16_t, Flash_kernel_traits<32, 128, 128, 4, cutlass::bfloat16_t>>, Is_dropout=false, Is_causal=false]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(58): here
            instantiation of "void run_mha_fwd_hdim32<T>(Flash_fwd_params &, cudaStream_t) [with T=cutlass::bfloat16_t]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_hdim32_bf16_sm80.cu(9): here

ptxas info    : 2 bytes gmem
ptxas info    : Compiling entry function '_Z16flash_fwd_kernelI23Flash_fwd_kernel_traitsILi32ELi128ELi128ELi4ELb0ELb0EN7cutlass10bfloat16_tE19Flash_kernel_traitsILi32ELi128ELi128ELi4ES2_EELb0ELb0ELb0ELb0ELb0EEv16Flash_fwd_params' for 'sm_80'
ptxas info    : Function properties for _Z16flash_fwd_kernelI23Flash_fwd_kernel_traitsILi32ELi128ELi128ELi4ELb0ELb0EN7cutlass10bfloat16_tE19Flash_kernel_traitsILi32ELi128ELi128ELi4ES2_EELb0ELb0ELb0ELb0ELb0EEv16Flash_fwd_params
    0 bytes stack frame, 0 bytes spill stores, 0 bytes spill loads
ptxas info    : Used 254 registers, 584 bytes cmem[0]
ptxas info    : Compiling entry function '_Z16flash_fwd_kernelI23Flash_fwd_kernel_traitsILi32ELi128ELi128ELi4ELb0ELb0EN7cutlass10bfloat16_tE19Flash_kernel_traitsILi32ELi128ELi128ELi4ES2_EELb0ELb0ELb0ELb1ELb0EEv16Flash_fwd_params' for 'sm_80'
ptxas info    : Function properties for _Z16flash_fwd_kernelI23Flash_fwd_kernel_traitsILi32ELi128ELi128ELi4ELb0ELb0EN7cutlass10bfloat16_tE19Flash_kernel_traitsILi32ELi128ELi128ELi4ES2_EELb0ELb0ELb0ELb1ELb0EEv16Flash_fwd_params
    0 bytes stack frame, 0 bytes spill stores, 0 bytes spill loads
ptxas info    : Used 254 registers, 584 bytes cmem[0]
ptxas info    : Compiling entry function '_Z16flash_fwd_kernelI23Flash_fwd_kernel_traitsILi32ELi128ELi128ELi4ELb0ELb0EN7cutlass10bfloat16_tE19Flash_kernel_traitsILi32ELi128ELi128ELi4ES2_EELb0ELb0ELb1ELb0ELb0EEv16Flash_fwd_params' for 'sm_80'
ptxas info    : Function properties for _Z16flash_fwd_kernelI23Flash_fwd_kernel_traitsILi32ELi128ELi128ELi4ELb0ELb0EN7cutlass10bfloat16_tE19Flash_kernel_traitsILi32ELi128ELi128ELi4ES2_EELb0ELb0ELb1ELb0ELb0EEv16Flash_fwd_params
    0 bytes stack frame, 0 bytes spill stores, 0 bytes spill loads
ptxas info    : Used 232 registers, 584 bytes cmem[0]
ptxas info    : Compiling entry function '_Z16flash_fwd_kernelI23Flash_fwd_kernel_traitsILi32ELi128ELi128ELi4ELb0ELb0EN7cutlass10bfloat16_tE19Flash_kernel_traitsILi32ELi128ELi128ELi4ES2_EELb0ELb0ELb1ELb1ELb0EEv16Flash_fwd_params' for 'sm_80'
ptxas info    : Function properties for _Z16flash_fwd_kernelI23Flash_fwd_kernel_traitsILi32ELi128ELi128ELi4ELb0ELb0EN7cutlass10bfloat16_tE19Flash_kernel_traitsILi32ELi128ELi128ELi4ES2_EELb0ELb0ELb1ELb1ELb0EEv16Flash_fwd_params
    0 bytes stack frame, 0 bytes spill stores, 0 bytes spill loads
ptxas info    : Used 236 registers, 584 bytes cmem[0]
ptxas info    : Compiling entry function '_Z16flash_fwd_kernelI23Flash_fwd_kernel_traitsILi32ELi128ELi128ELi4ELb0ELb0EN7cutlass10bfloat16_tE19Flash_kernel_traitsILi32ELi128ELi128ELi4ES2_EELb0ELb1ELb0ELb0ELb0EEv16Flash_fwd_params' for 'sm_80'
ptxas info    : Function properties for _Z16flash_fwd_kernelI23Flash_fwd_kernel_traitsILi32ELi128ELi128ELi4ELb0ELb0EN7cutlass10bfloat16_tE19Flash_kernel_traitsILi32ELi128ELi128ELi4ES2_EELb0ELb1ELb0ELb0ELb0EEv16Flash_fwd_params
    0 bytes stack frame, 0 bytes spill stores, 0 bytes spill loads
ptxas info    : Used 233 registers, 584 bytes cmem[0]
ptxas info    : Compiling entry function '_Z16flash_fwd_kernelI23Flash_fwd_kernel_traitsILi32ELi128ELi128ELi4ELb0ELb0EN7cutlass10bfloat16_tE19Flash_kernel_traitsILi32ELi128ELi128ELi4ES2_EELb0ELb1ELb0ELb1ELb0EEv16Flash_fwd_params' for 'sm_80'
ptxas info    : Function properties for _Z16flash_fwd_kernelI23Flash_fwd_kernel_traitsILi32ELi128ELi128ELi4ELb0ELb0EN7cutlass10bfloat16_tE19Flash_kernel_traitsILi32ELi128ELi128ELi4ES2_EELb0ELb1ELb0ELb1ELb0EEv16Flash_fwd_params
    0 bytes stack frame, 0 bytes spill stores, 0 bytes spill loads
ptxas info    : Used 254 registers, 584 bytes cmem[0]
ptxas info    : Compiling entry function '_Z16flash_fwd_kernelI23Flash_fwd_kernel_traitsILi32ELi128ELi128ELi4ELb0ELb0EN7cutlass10bfloat16_tE19Flash_kernel_traitsILi32ELi128ELi128ELi4ES2_EELb0ELb1ELb1ELb0ELb0EEv16Flash_fwd_params' for 'sm_80'
ptxas info    : Function properties for _Z16flash_fwd_kernelI23Flash_fwd_kernel_traitsILi32ELi128ELi128ELi4ELb0ELb0EN7cutlass10bfloat16_tE19Flash_kernel_traitsILi32ELi128ELi128ELi4ES2_EELb0ELb1ELb1ELb0ELb0EEv16Flash_fwd_params
    0 bytes stack frame, 0 bytes spill stores, 0 bytes spill loads
ptxas info    : Used 254 registers, 584 bytes cmem[0]
ptxas info    : Compiling entry function '_Z16flash_fwd_kernelI23Flash_fwd_kernel_traitsILi32ELi128ELi128ELi4ELb0ELb0EN7cutlass10bfloat16_tE19Flash_kernel_traitsILi32ELi128ELi128ELi4ES2_EELb0ELb1ELb1ELb1ELb0EEv16Flash_fwd_params' for 'sm_80'
ptxas info    : Function properties for _Z16flash_fwd_kernelI23Flash_fwd_kernel_traitsILi32ELi128ELi128ELi4ELb0ELb0EN7cutlass10bfloat16_tE19Flash_kernel_traitsILi32ELi128ELi128ELi4ES2_EELb0ELb1ELb1ELb1ELb0EEv16Flash_fwd_params
    0 bytes stack frame, 0 bytes spill stores, 0 bytes spill loads
ptxas info    : Used 255 registers, 584 bytes cmem[0]
ptxas info    : Compiling entry function '_Z16flash_fwd_kernelI23Flash_fwd_kernel_traitsILi32ELi128ELi128ELi4ELb0ELb0EN7cutlass10bfloat16_tE19Flash_kernel_traitsILi32ELi128ELi128ELi4ES2_EELb1ELb0ELb0ELb0ELb0EEv16Flash_fwd_params' for 'sm_80'
ptxas info    : Function properties for _Z16flash_fwd_kernelI23Flash_fwd_kernel_traitsILi32ELi128ELi128ELi4ELb0ELb0EN7cutlass10bfloat16_tE19Flash_kernel_traitsILi32ELi128ELi128ELi4ES2_EELb1ELb0ELb0ELb0ELb0EEv16Flash_fwd_params
    0 bytes stack frame, 0 bytes spill stores, 0 bytes spill loads
ptxas info    : Used 255 registers, 584 bytes cmem[0]
ptxas info    : Compiling entry function '_Z16flash_fwd_kernelI23Flash_fwd_kernel_traitsILi32ELi128ELi128ELi4ELb0ELb0EN7cutlass10bfloat16_tE19Flash_kernel_traitsILi32ELi128ELi128ELi4ES2_EELb1ELb0ELb0ELb0ELb1EEv16Flash_fwd_params' for 'sm_80'
ptxas info    : Function properties for _Z16flash_fwd_kernelI23Flash_fwd_kernel_traitsILi32ELi128ELi128ELi4ELb0ELb0EN7cutlass10bfloat16_tE19Flash_kernel_traitsILi32ELi128ELi128ELi4ES2_EELb1ELb0ELb0ELb0ELb1EEv16Flash_fwd_params
    176 bytes stack frame, 192 bytes spill stores, 192 bytes spill loads
ptxas info    : Used 255 registers, 584 bytes cmem[0]
ptxas info    : Compiling entry function '_Z16flash_fwd_kernelI23Flash_fwd_kernel_traitsILi32ELi128ELi128ELi4ELb0ELb0EN7cutlass10bfloat16_tE19Flash_kernel_traitsILi32ELi128ELi128ELi4ES2_EELb1ELb0ELb0ELb1ELb0EEv16Flash_fwd_params' for 'sm_80'
ptxas info    : Function properties for _Z16flash_fwd_kernelI23Flash_fwd_kernel_traitsILi32ELi128ELi128ELi4ELb0ELb0EN7cutlass10bfloat16_tE19Flash_kernel_traitsILi32ELi128ELi128ELi4ES2_EELb1ELb0ELb0ELb1ELb0EEv16Flash_fwd_params
    24 bytes stack frame, 20 bytes spill stores, 20 bytes spill loads
ptxas info    : Used 255 registers, 584 bytes cmem[0]
ptxas info    : Compiling entry function '_Z16flash_fwd_kernelI23Flash_fwd_kernel_traitsILi32ELi128ELi128ELi4ELb0ELb0EN7cutlass10bfloat16_tE19Flash_kernel_traitsILi32ELi128ELi128ELi4ES2_EELb1ELb0ELb0ELb1ELb1EEv16Flash_fwd_params' for 'sm_80'
ptxas info    : Function properties for _Z16flash_fwd_kernelI23Flash_fwd_kernel_traitsILi32ELi128ELi128ELi4ELb0ELb0EN7cutlass10bfloat16_tE19Flash_kernel_traitsILi32ELi128ELi128ELi4ES2_EELb1ELb0ELb0ELb1ELb1EEv16Flash_fwd_params
    192 bytes stack frame, 212 bytes spill stores, 196 bytes spill loads
ptxas info    : Used 255 registers, 584 bytes cmem[0]
ptxas info    : Compiling entry function '_Z16flash_fwd_kernelI23Flash_fwd_kernel_traitsILi32ELi128ELi128ELi4ELb0ELb0EN7cutlass10bfloat16_tE19Flash_kernel_traitsILi32ELi128ELi128ELi4ES2_EELb1ELb0ELb1ELb0ELb0EEv16Flash_fwd_params' for 'sm_80'
ptxas info    : Function properties for _Z16flash_fwd_kernelI23Flash_fwd_kernel_traitsILi32ELi128ELi128ELi4ELb0ELb0EN7cutlass10bfloat16_tE19Flash_kernel_traitsILi32ELi128ELi128ELi4ES2_EELb1ELb0ELb1ELb0ELb0EEv16Flash_fwd_params
    48 bytes stack frame, 44 bytes spill stores, 44 bytes spill loads
ptxas info    : Used 255 registers, 584 bytes cmem[0]
ptxas info    : Compiling entry function '_Z16flash_fwd_kernelI23Flash_fwd_kernel_traitsILi32ELi128ELi128ELi4ELb0ELb0EN7cutlass10bfloat16_tE19Flash_kernel_traitsILi32ELi128ELi128ELi4ES2_EELb1ELb0ELb1ELb0ELb1EEv16Flash_fwd_params' for 'sm_80'
ptxas info    : Function properties for _Z16flash_fwd_kernelI23Flash_fwd_kernel_traitsILi32ELi128ELi128ELi4ELb0ELb0EN7cutlass10bfloat16_tE19Flash_kernel_traitsILi32ELi128ELi128ELi4ES2_EELb1ELb0ELb1ELb0ELb1EEv16Flash_fwd_params
    152 bytes stack frame, 164 bytes spill stores, 164 bytes spill loads
ptxas info    : Used 255 registers, 584 bytes cmem[0]
ptxas info    : Compiling entry function '_Z16flash_fwd_kernelI23Flash_fwd_kernel_traitsILi32ELi128ELi128ELi4ELb0ELb0EN7cutlass10bfloat16_tE19Flash_kernel_traitsILi32ELi128ELi128ELi4ES2_EELb1ELb0ELb1ELb1ELb0EEv16Flash_fwd_params' for 'sm_80'
ptxas info    : Function properties for _Z16flash_fwd_kernelI23Flash_fwd_kernel_traitsILi32ELi128ELi128ELi4ELb0ELb0EN7cutlass10bfloat16_tE19Flash_kernel_traitsILi32ELi128ELi128ELi4ES2_EELb1ELb0ELb1ELb1ELb0EEv16Flash_fwd_params
    0 bytes stack frame, 0 bytes spill stores, 0 bytes spill loads
ptxas info    : Used 255 registers, 584 bytes cmem[0]
ptxas info    : Compiling entry function '_Z16flash_fwd_kernelI23Flash_fwd_kernel_traitsILi32ELi128ELi128ELi4ELb0ELb0EN7cutlass10bfloat16_tE19Flash_kernel_traitsILi32ELi128ELi128ELi4ES2_EELb1ELb0ELb1ELb1ELb1EEv16Flash_fwd_params' for 'sm_80'
ptxas info    : Function properties for _Z16flash_fwd_kernelI23Flash_fwd_kernel_traitsILi32ELi128ELi128ELi4ELb0ELb0EN7cutlass10bfloat16_tE19Flash_kernel_traitsILi32ELi128ELi128ELi4ES2_EELb1ELb0ELb1ELb1ELb1EEv16Flash_fwd_params
    168 bytes stack frame, 170 bytes spill stores, 180 bytes spill loads
ptxas info    : Used 255 registers, 584 bytes cmem[0]
ptxas info    : Compiling entry function '_Z16flash_fwd_kernelI23Flash_fwd_kernel_traitsILi32ELi128ELi128ELi4ELb0ELb0EN7cutlass10bfloat16_tE19Flash_kernel_traitsILi32ELi128ELi128ELi4ES2_EELb1ELb1ELb0ELb0ELb0EEv16Flash_fwd_params' for 'sm_80'
ptxas info    : Function properties for _Z16flash_fwd_kernelI23Flash_fwd_kernel_traitsILi32ELi128ELi128ELi4ELb0ELb0EN7cutlass10bfloat16_tE19Flash_kernel_traitsILi32ELi128ELi128ELi4ES2_EELb1ELb1ELb0ELb0ELb0EEv16Flash_fwd_params
    0 bytes stack frame, 0 bytes spill stores, 0 bytes spill loads
ptxas info    : Used 252 registers, 584 bytes cmem[0]
ptxas info    : Compiling entry function '_Z16flash_fwd_kernelI23Flash_fwd_kernel_traitsILi32ELi128ELi128ELi4ELb0ELb0EN7cutlass10bfloat16_tE19Flash_kernel_traitsILi32ELi128ELi128ELi4ES2_EELb1ELb1ELb0ELb0ELb1EEv16Flash_fwd_params' for 'sm_80'
ptxas info    : Function properties for _Z16flash_fwd_kernelI23Flash_fwd_kernel_traitsILi32ELi128ELi128ELi4ELb0ELb0EN7cutlass10bfloat16_tE19Flash_kernel_traitsILi32ELi128ELi128ELi4ES2_EELb1ELb1ELb0ELb0ELb1EEv16Flash_fwd_params
    184 bytes stack frame, 204 bytes spill stores, 204 bytes spill loads
ptxas info    : Used 255 registers, 584 bytes cmem[0]
ptxas info    : Compiling entry function '_Z16flash_fwd_kernelI23Flash_fwd_kernel_traitsILi32ELi128ELi128ELi4ELb0ELb0EN7cutlass10bfloat16_tE19Flash_kernel_traitsILi32ELi128ELi128ELi4ES2_EELb1ELb1ELb0ELb1ELb0EEv16Flash_fwd_params' for 'sm_80'
ptxas info    : Function properties for _Z16flash_fwd_kernelI23Flash_fwd_kernel_traitsILi32ELi128ELi128ELi4ELb0ELb0EN7cutlass10bfloat16_tE19Flash_kernel_traitsILi32ELi128ELi128ELi4ES2_EELb1ELb1ELb0ELb1ELb0EEv16Flash_fwd_params
    24 bytes stack frame, 20 bytes spill stores, 20 bytes spill loads
ptxas info    : Used 255 registers, 584 bytes cmem[0]
ptxas info    : Compiling entry function '_Z16flash_fwd_kernelI23Flash_fwd_kernel_traitsILi32ELi128ELi128ELi4ELb0ELb0EN7cutlass10bfloat16_tE19Flash_kernel_traitsILi32ELi128ELi128ELi4ES2_EELb1ELb1ELb0ELb1ELb1EEv16Flash_fwd_params' for 'sm_80'
ptxas info    : Function properties for _Z16flash_fwd_kernelI23Flash_fwd_kernel_traitsILi32ELi128ELi128ELi4ELb0ELb0EN7cutlass10bfloat16_tE19Flash_kernel_traitsILi32ELi128ELi128ELi4ES2_EELb1ELb1ELb0ELb1ELb1EEv16Flash_fwd_params
    176 bytes stack frame, 212 bytes spill stores, 196 bytes spill loads
ptxas info    : Used 255 registers, 584 bytes cmem[0]
ptxas info    : Compiling entry function '_Z16flash_fwd_kernelI23Flash_fwd_kernel_traitsILi32ELi128ELi128ELi4ELb0ELb0EN7cutlass10bfloat16_tE19Flash_kernel_traitsILi32ELi128ELi128ELi4ES2_EELb1ELb1ELb1ELb0ELb0EEv16Flash_fwd_params' for 'sm_80'
ptxas info    : Function properties for _Z16flash_fwd_kernelI23Flash_fwd_kernel_traitsILi32ELi128ELi128ELi4ELb0ELb0EN7cutlass10bfloat16_tE19Flash_kernel_traitsILi32ELi128ELi128ELi4ES2_EELb1ELb1ELb1ELb0ELb0EEv16Flash_fwd_params
    48 bytes stack frame, 44 bytes spill stores, 44 bytes spill loads
ptxas info    : Used 255 registers, 584 bytes cmem[0]
ptxas info    : Compiling entry function '_Z16flash_fwd_kernelI23Flash_fwd_kernel_traitsILi32ELi128ELi128ELi4ELb0ELb0EN7cutlass10bfloat16_tE19Flash_kernel_traitsILi32ELi128ELi128ELi4ES2_EELb1ELb1ELb1ELb0ELb1EEv16Flash_fwd_params' for 'sm_80'
ptxas info    : Function properties for _Z16flash_fwd_kernelI23Flash_fwd_kernel_traitsILi32ELi128ELi128ELi4ELb0ELb0EN7cutlass10bfloat16_tE19Flash_kernel_traitsILi32ELi128ELi128ELi4ES2_EELb1ELb1ELb1ELb0ELb1EEv16Flash_fwd_params
    184 bytes stack frame, 196 bytes spill stores, 196 bytes spill loads
ptxas info    : Used 255 registers, 584 bytes cmem[0]
ptxas info    : Compiling entry function '_Z16flash_fwd_kernelI23Flash_fwd_kernel_traitsILi32ELi128ELi128ELi4ELb0ELb0EN7cutlass10bfloat16_tE19Flash_kernel_traitsILi32ELi128ELi128ELi4ES2_EELb1ELb1ELb1ELb1ELb0EEv16Flash_fwd_params' for 'sm_80'
ptxas info    : Function properties for _Z16flash_fwd_kernelI23Flash_fwd_kernel_traitsILi32ELi128ELi128ELi4ELb0ELb0EN7cutlass10bfloat16_tE19Flash_kernel_traitsILi32ELi128ELi128ELi4ES2_EELb1ELb1ELb1ELb1ELb0EEv16Flash_fwd_params
    8 bytes stack frame, 4 bytes spill stores, 4 bytes spill loads
ptxas info    : Used 255 registers, 584 bytes cmem[0]
ptxas info    : Compiling entry function '_Z16flash_fwd_kernelI23Flash_fwd_kernel_traitsILi32ELi128ELi128ELi4ELb0ELb0EN7cutlass10bfloat16_tE19Flash_kernel_traitsILi32ELi128ELi128ELi4ES2_EELb1ELb1ELb1ELb1ELb1EEv16Flash_fwd_params' for 'sm_80'
ptxas info    : Function properties for _Z16flash_fwd_kernelI23Flash_fwd_kernel_traitsILi32ELi128ELi128ELi4ELb0ELb0EN7cutlass10bfloat16_tE19Flash_kernel_traitsILi32ELi128ELi128ELi4ES2_EELb1ELb1ELb1ELb1ELb1EEv16Flash_fwd_params
    192 bytes stack frame, 212 bytes spill stores, 212 bytes spill loads
ptxas info    : Used 255 registers, 584 bytes cmem[0]
/anaconda/envs/flashattention/lib/python3.9/site-packages/torch/include/c10/core/SymInt.h(84): warning: integer conversion resulted in a change of sign

[4/13] /usr/local/cuda-11.4/bin/nvcc  -I/home/v-feiychen/flash-attention/csrc/flash_attn -I/home/v-feiychen/flash-attention/csrc/flash_attn/src -I/home/v-feiychen/flash-attention/csrc/cutlass/include -I/anaconda/envs/flashattention/lib/python3.9/site-packages/torch/include -I/anaconda/envs/flashattention/lib/python3.9/site-packages/torch/include/torch/csrc/api/include -I/anaconda/envs/flashattention/lib/python3.9/site-packages/torch/include/TH -I/anaconda/envs/flashattention/lib/python3.9/site-packages/torch/include/THC -I/usr/local/cuda-11.4/include -I/anaconda/envs/flashattention/include/python3.9 -c -c /home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_hdim32_fp16_sm80.cu -o /home/v-feiychen/flash-attention/build/temp.linux-x86_64-cpython-39/csrc/flash_attn/src/flash_fwd_hdim32_fp16_sm80.o -D__CUDA_NO_HALF_OPERATORS__ -D__CUDA_NO_HALF_CONVERSIONS__ -D__CUDA_NO_BFLOAT16_CONVERSIONS__ -D__CUDA_NO_HALF2_OPERATORS__ --expt-relaxed-constexpr --compiler-options ''"'"'-fPIC'"'"'' -O3 -std=c++17 -U__CUDA_NO_HALF_OPERATORS__ -U__CUDA_NO_HALF_CONVERSIONS__ -U__CUDA_NO_HALF2_OPERATORS__ -U__CUDA_NO_BFLOAT16_CONVERSIONS__ --expt-relaxed-constexpr --expt-extended-lambda --use_fast_math --ptxas-options=-v -lineinfo -gencode arch=compute_80,code=sm_80 --threads 4 -DTORCH_API_INCLUDE_EXTENSION_H '-DPYBIND11_COMPILER_TYPE="_gcc"' '-DPYBIND11_STDLIB="_libstdcpp"' '-DPYBIND11_BUILD_ABI="_cxxabi1011"' -DTORCH_EXTENSION_NAME=flash_attn_2_cuda -D_GLIBCXX_USE_CXX11_ABI=0
/anaconda/envs/flashattention/lib/python3.9/site-packages/torch/include/c10/core/SymInt.h(84): warning: integer conversion resulted in a change of sign

/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_kernel.h(140): warning: variable "MMA_M" was declared but never referenced
          detected during:
            instantiation of "void flash::compute_attn_1rowblock<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &, int, int, int) [with Kernel_traits=Flash_fwd_kernel_traits<32, 128, 128, 4, false, false, cutlass::half_t, Flash_kernel_traits<32, 128, 128, 4, cutlass::half_t>>, Is_dropout=true, Is_causal=true, Is_even_N=true, Is_even_K=true, Return_softmax=true, Params=Flash_fwd_params]" 
(588): here
            instantiation of "void flash::compute_attn<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &) [with Kernel_traits=Flash_fwd_kernel_traits<32, 128, 128, 4, false, false, cutlass::half_t, Flash_kernel_traits<32, 128, 128, 4, cutlass::half_t>>, Is_dropout=true, Is_causal=true, Is_even_N=true, Is_even_K=true, Return_softmax=true, Params=Flash_fwd_params]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(15): here
            instantiation of "void flash_fwd_kernel<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax>(Flash_fwd_params) [with Kernel_traits=Flash_fwd_kernel_traits<32, 128, 128, 4, false, false, cutlass::half_t, Flash_kernel_traits<32, 128, 128, 4, cutlass::half_t>>, Is_dropout=true, Is_causal=true, Is_even_N=true, Is_even_K=true, Return_softmax=true]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(34): here
            instantiation of "void run_flash_fwd<Kernel_traits,Is_dropout,Is_causal>(Flash_fwd_params &, cudaStream_t) [with Kernel_traits=Flash_fwd_kernel_traits<32, 128, 128, 4, false, false, cutlass::half_t, Flash_kernel_traits<32, 128, 128, 4, cutlass::half_t>>, Is_dropout=true, Is_causal=true]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(58): here
            instantiation of "void run_mha_fwd_hdim32<T>(Flash_fwd_params &, cudaStream_t) [with T=cutlass::half_t]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_hdim32_fp16_sm80.cu(22): here

/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_kernel.h(275): warning: variable "tQrQ" was declared but never referenced
          detected during:
            instantiation of "void flash::compute_attn_1rowblock<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &, int, int, int) [with Kernel_traits=Flash_fwd_kernel_traits<32, 128, 128, 4, false, false, cutlass::half_t, Flash_kernel_traits<32, 128, 128, 4, cutlass::half_t>>, Is_dropout=true, Is_causal=true, Is_even_N=true, Is_even_K=true, Return_softmax=true, Params=Flash_fwd_params]" 
(588): here
            instantiation of "void flash::compute_attn<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &) [with Kernel_traits=Flash_fwd_kernel_traits<32, 128, 128, 4, false, false, cutlass::half_t, Flash_kernel_traits<32, 128, 128, 4, cutlass::half_t>>, Is_dropout=true, Is_causal=true, Is_even_N=true, Is_even_K=true, Return_softmax=true, Params=Flash_fwd_params]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(15): here
            instantiation of "void flash_fwd_kernel<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax>(Flash_fwd_params) [with Kernel_traits=Flash_fwd_kernel_traits<32, 128, 128, 4, false, false, cutlass::half_t, Flash_kernel_traits<32, 128, 128, 4, cutlass::half_t>>, Is_dropout=true, Is_causal=true, Is_even_N=true, Is_even_K=true, Return_softmax=true]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(34): here
            instantiation of "void run_flash_fwd<Kernel_traits,Is_dropout,Is_causal>(Flash_fwd_params &, cudaStream_t) [with Kernel_traits=Flash_fwd_kernel_traits<32, 128, 128, 4, false, false, cutlass::half_t, Flash_kernel_traits<32, 128, 128, 4, cutlass::half_t>>, Is_dropout=true, Is_causal=true]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(58): here
            instantiation of "void run_mha_fwd_hdim32<T>(Flash_fwd_params &, cudaStream_t) [with T=cutlass::half_t]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_hdim32_fp16_sm80.cu(22): here

/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_kernel.h(140): warning: variable "MMA_M" was declared but never referenced
          detected during:
            instantiation of "void flash::compute_attn_1rowblock<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &, int, int, int) [with Kernel_traits=Flash_fwd_kernel_traits<32, 128, 128, 4, false, false, cutlass::half_t, Flash_kernel_traits<32, 128, 128, 4, cutlass::half_t>>, Is_dropout=true, Is_causal=true, Is_even_N=true, Is_even_K=true, Return_softmax=false, Params=Flash_fwd_params]" 
(588): here
            instantiation of "void flash::compute_attn<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &) [with Kernel_traits=Flash_fwd_kernel_traits<32, 128, 128, 4, false, false, cutlass::half_t, Flash_kernel_traits<32, 128, 128, 4, cutlass::half_t>>, Is_dropout=true, Is_causal=true, Is_even_N=true, Is_even_K=true, Return_softmax=false, Params=Flash_fwd_params]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(15): here
            instantiation of "void flash_fwd_kernel<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax>(Flash_fwd_params) [with Kernel_traits=Flash_fwd_kernel_traits<32, 128, 128, 4, false, false, cutlass::half_t, Flash_kernel_traits<32, 128, 128, 4, cutlass::half_t>>, Is_dropout=true, Is_causal=true, Is_even_N=true, Is_even_K=true, Return_softmax=false]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(34): here
            instantiation of "void run_flash_fwd<Kernel_traits,Is_dropout,Is_causal>(Flash_fwd_params &, cudaStream_t) [with Kernel_traits=Flash_fwd_kernel_traits<32, 128, 128, 4, false, false, cutlass::half_t, Flash_kernel_traits<32, 128, 128, 4, cutlass::half_t>>, Is_dropout=true, Is_causal=true]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(58): here
            instantiation of "void run_mha_fwd_hdim32<T>(Flash_fwd_params &, cudaStream_t) [with T=cutlass::half_t]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_hdim32_fp16_sm80.cu(22): here

/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_kernel.h(275): warning: variable "tQrQ" was declared but never referenced
          detected during:
            instantiation of "void flash::compute_attn_1rowblock<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &, int, int, int) [with Kernel_traits=Flash_fwd_kernel_traits<32, 128, 128, 4, false, false, cutlass::half_t, Flash_kernel_traits<32, 128, 128, 4, cutlass::half_t>>, Is_dropout=true, Is_causal=true, Is_even_N=true, Is_even_K=true, Return_softmax=false, Params=Flash_fwd_params]" 
(588): here
            instantiation of "void flash::compute_attn<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &) [with Kernel_traits=Flash_fwd_kernel_traits<32, 128, 128, 4, false, false, cutlass::half_t, Flash_kernel_traits<32, 128, 128, 4, cutlass::half_t>>, Is_dropout=true, Is_causal=true, Is_even_N=true, Is_even_K=true, Return_softmax=false, Params=Flash_fwd_params]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(15): here
            instantiation of "void flash_fwd_kernel<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax>(Flash_fwd_params) [with Kernel_traits=Flash_fwd_kernel_traits<32, 128, 128, 4, false, false, cutlass::half_t, Flash_kernel_traits<32, 128, 128, 4, cutlass::half_t>>, Is_dropout=true, Is_causal=true, Is_even_N=true, Is_even_K=true, Return_softmax=false]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(34): here
            instantiation of "void run_flash_fwd<Kernel_traits,Is_dropout,Is_causal>(Flash_fwd_params &, cudaStream_t) [with Kernel_traits=Flash_fwd_kernel_traits<32, 128, 128, 4, false, false, cutlass::half_t, Flash_kernel_traits<32, 128, 128, 4, cutlass::half_t>>, Is_dropout=true, Is_causal=true]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(58): here
            instantiation of "void run_mha_fwd_hdim32<T>(Flash_fwd_params &, cudaStream_t) [with T=cutlass::half_t]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_hdim32_fp16_sm80.cu(22): here

/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_kernel.h(140): warning: variable "MMA_M" was declared but never referenced
          detected during:
            instantiation of "void flash::compute_attn_1rowblock<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &, int, int, int) [with Kernel_traits=Flash_fwd_kernel_traits<32, 128, 128, 4, false, false, cutlass::half_t, Flash_kernel_traits<32, 128, 128, 4, cutlass::half_t>>, Is_dropout=true, Is_causal=true, Is_even_N=true, Is_even_K=false, Return_softmax=true, Params=Flash_fwd_params]" 
(588): here
            instantiation of "void flash::compute_attn<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &) [with Kernel_traits=Flash_fwd_kernel_traits<32, 128, 128, 4, false, false, cutlass::half_t, Flash_kernel_traits<32, 128, 128, 4, cutlass::half_t>>, Is_dropout=true, Is_causal=true, Is_even_N=true, Is_even_K=false, Return_softmax=true, Params=Flash_fwd_params]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(15): here
            instantiation of "void flash_fwd_kernel<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax>(Flash_fwd_params) [with Kernel_traits=Flash_fwd_kernel_traits<32, 128, 128, 4, false, false, cutlass::half_t, Flash_kernel_traits<32, 128, 128, 4, cutlass::half_t>>, Is_dropout=true, Is_causal=true, Is_even_N=true, Is_even_K=false, Return_softmax=true]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(34): here
            instantiation of "void run_flash_fwd<Kernel_traits,Is_dropout,Is_causal>(Flash_fwd_params &, cudaStream_t) [with Kernel_traits=Flash_fwd_kernel_traits<32, 128, 128, 4, false, false, cutlass::half_t, Flash_kernel_traits<32, 128, 128, 4, cutlass::half_t>>, Is_dropout=true, Is_causal=true]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(58): here
            instantiation of "void run_mha_fwd_hdim32<T>(Flash_fwd_params &, cudaStream_t) [with T=cutlass::half_t]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_hdim32_fp16_sm80.cu(22): here

/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_kernel.h(275): warning: variable "tQrQ" was declared but never referenced
          detected during:
            instantiation of "void flash::compute_attn_1rowblock<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &, int, int, int) [with Kernel_traits=Flash_fwd_kernel_traits<32, 128, 128, 4, false, false, cutlass::half_t, Flash_kernel_traits<32, 128, 128, 4, cutlass::half_t>>, Is_dropout=true, Is_causal=true, Is_even_N=true, Is_even_K=false, Return_softmax=true, Params=Flash_fwd_params]" 
(588): here
            instantiation of "void flash::compute_attn<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &) [with Kernel_traits=Flash_fwd_kernel_traits<32, 128, 128, 4, false, false, cutlass::half_t, Flash_kernel_traits<32, 128, 128, 4, cutlass::half_t>>, Is_dropout=true, Is_causal=true, Is_even_N=true, Is_even_K=false, Return_softmax=true, Params=Flash_fwd_params]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(15): here
            instantiation of "void flash_fwd_kernel<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax>(Flash_fwd_params) [with Kernel_traits=Flash_fwd_kernel_traits<32, 128, 128, 4, false, false, cutlass::half_t, Flash_kernel_traits<32, 128, 128, 4, cutlass::half_t>>, Is_dropout=true, Is_causal=true, Is_even_N=true, Is_even_K=false, Return_softmax=true]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(34): here
            instantiation of "void run_flash_fwd<Kernel_traits,Is_dropout,Is_causal>(Flash_fwd_params &, cudaStream_t) [with Kernel_traits=Flash_fwd_kernel_traits<32, 128, 128, 4, false, false, cutlass::half_t, Flash_kernel_traits<32, 128, 128, 4, cutlass::half_t>>, Is_dropout=true, Is_causal=true]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(58): here
            instantiation of "void run_mha_fwd_hdim32<T>(Flash_fwd_params &, cudaStream_t) [with T=cutlass::half_t]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_hdim32_fp16_sm80.cu(22): here

/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_kernel.h(140): warning: variable "MMA_M" was declared but never referenced
          detected during:
            instantiation of "void flash::compute_attn_1rowblock<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &, int, int, int) [with Kernel_traits=Flash_fwd_kernel_traits<32, 128, 128, 4, false, false, cutlass::half_t, Flash_kernel_traits<32, 128, 128, 4, cutlass::half_t>>, Is_dropout=true, Is_causal=true, Is_even_N=true, Is_even_K=false, Return_softmax=false, Params=Flash_fwd_params]" 
(588): here
            instantiation of "void flash::compute_attn<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &) [with Kernel_traits=Flash_fwd_kernel_traits<32, 128, 128, 4, false, false, cutlass::half_t, Flash_kernel_traits<32, 128, 128, 4, cutlass::half_t>>, Is_dropout=true, Is_causal=true, Is_even_N=true, Is_even_K=false, Return_softmax=false, Params=Flash_fwd_params]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(15): here
            instantiation of "void flash_fwd_kernel<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax>(Flash_fwd_params) [with Kernel_traits=Flash_fwd_kernel_traits<32, 128, 128, 4, false, false, cutlass::half_t, Flash_kernel_traits<32, 128, 128, 4, cutlass::half_t>>, Is_dropout=true, Is_causal=true, Is_even_N=true, Is_even_K=false, Return_softmax=false]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(34): here
            instantiation of "void run_flash_fwd<Kernel_traits,Is_dropout,Is_causal>(Flash_fwd_params &, cudaStream_t) [with Kernel_traits=Flash_fwd_kernel_traits<32, 128, 128, 4, false, false, cutlass::half_t, Flash_kernel_traits<32, 128, 128, 4, cutlass::half_t>>, Is_dropout=true, Is_causal=true]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(58): here
            instantiation of "void run_mha_fwd_hdim32<T>(Flash_fwd_params &, cudaStream_t) [with T=cutlass::half_t]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_hdim32_fp16_sm80.cu(22): here

/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_kernel.h(275): warning: variable "tQrQ" was declared but never referenced
          detected during:
            instantiation of "void flash::compute_attn_1rowblock<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &, int, int, int) [with Kernel_traits=Flash_fwd_kernel_traits<32, 128, 128, 4, false, false, cutlass::half_t, Flash_kernel_traits<32, 128, 128, 4, cutlass::half_t>>, Is_dropout=true, Is_causal=true, Is_even_N=true, Is_even_K=false, Return_softmax=false, Params=Flash_fwd_params]" 
(588): here
            instantiation of "void flash::compute_attn<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &) [with Kernel_traits=Flash_fwd_kernel_traits<32, 128, 128, 4, false, false, cutlass::half_t, Flash_kernel_traits<32, 128, 128, 4, cutlass::half_t>>, Is_dropout=true, Is_causal=true, Is_even_N=true, Is_even_K=false, Return_softmax=false, Params=Flash_fwd_params]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(15): here
            instantiation of "void flash_fwd_kernel<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax>(Flash_fwd_params) [with Kernel_traits=Flash_fwd_kernel_traits<32, 128, 128, 4, false, false, cutlass::half_t, Flash_kernel_traits<32, 128, 128, 4, cutlass::half_t>>, Is_dropout=true, Is_causal=true, Is_even_N=true, Is_even_K=false, Return_softmax=false]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(34): here
            instantiation of "void run_flash_fwd<Kernel_traits,Is_dropout,Is_causal>(Flash_fwd_params &, cudaStream_t) [with Kernel_traits=Flash_fwd_kernel_traits<32, 128, 128, 4, false, false, cutlass::half_t, Flash_kernel_traits<32, 128, 128, 4, cutlass::half_t>>, Is_dropout=true, Is_causal=true]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(58): here
            instantiation of "void run_mha_fwd_hdim32<T>(Flash_fwd_params &, cudaStream_t) [with T=cutlass::half_t]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_hdim32_fp16_sm80.cu(22): here

/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_kernel.h(140): warning: variable "MMA_M" was declared but never referenced
          detected during:
            instantiation of "void flash::compute_attn_1rowblock<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &, int, int, int) [with Kernel_traits=Flash_fwd_kernel_traits<32, 128, 128, 4, false, false, cutlass::half_t, Flash_kernel_traits<32, 128, 128, 4, cutlass::half_t>>, Is_dropout=true, Is_causal=true, Is_even_N=false, Is_even_K=true, Return_softmax=true, Params=Flash_fwd_params]" 
(588): here
            instantiation of "void flash::compute_attn<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &) [with Kernel_traits=Flash_fwd_kernel_traits<32, 128, 128, 4, false, false, cutlass::half_t, Flash_kernel_traits<32, 128, 128, 4, cutlass::half_t>>, Is_dropout=true, Is_causal=true, Is_even_N=false, Is_even_K=true, Return_softmax=true, Params=Flash_fwd_params]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(15): here
            instantiation of "void flash_fwd_kernel<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax>(Flash_fwd_params) [with Kernel_traits=Flash_fwd_kernel_traits<32, 128, 128, 4, false, false, cutlass::half_t, Flash_kernel_traits<32, 128, 128, 4, cutlass::half_t>>, Is_dropout=true, Is_causal=true, Is_even_N=false, Is_even_K=true, Return_softmax=true]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(34): here
            instantiation of "void run_flash_fwd<Kernel_traits,Is_dropout,Is_causal>(Flash_fwd_params &, cudaStream_t) [with Kernel_traits=Flash_fwd_kernel_traits<32, 128, 128, 4, false, false, cutlass::half_t, Flash_kernel_traits<32, 128, 128, 4, cutlass::half_t>>, Is_dropout=true, Is_causal=true]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(58): here
            instantiation of "void run_mha_fwd_hdim32<T>(Flash_fwd_params &, cudaStream_t) [with T=cutlass::half_t]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_hdim32_fp16_sm80.cu(22): here

/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_kernel.h(275): warning: variable "tQrQ" was declared but never referenced
          detected during:
            instantiation of "void flash::compute_attn_1rowblock<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &, int, int, int) [with Kernel_traits=Flash_fwd_kernel_traits<32, 128, 128, 4, false, false, cutlass::half_t, Flash_kernel_traits<32, 128, 128, 4, cutlass::half_t>>, Is_dropout=true, Is_causal=true, Is_even_N=false, Is_even_K=true, Return_softmax=true, Params=Flash_fwd_params]" 
(588): here
            instantiation of "void flash::compute_attn<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &) [with Kernel_traits=Flash_fwd_kernel_traits<32, 128, 128, 4, false, false, cutlass::half_t, Flash_kernel_traits<32, 128, 128, 4, cutlass::half_t>>, Is_dropout=true, Is_causal=true, Is_even_N=false, Is_even_K=true, Return_softmax=true, Params=Flash_fwd_params]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(15): here
            instantiation of "void flash_fwd_kernel<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax>(Flash_fwd_params) [with Kernel_traits=Flash_fwd_kernel_traits<32, 128, 128, 4, false, false, cutlass::half_t, Flash_kernel_traits<32, 128, 128, 4, cutlass::half_t>>, Is_dropout=true, Is_causal=true, Is_even_N=false, Is_even_K=true, Return_softmax=true]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(34): here
            instantiation of "void run_flash_fwd<Kernel_traits,Is_dropout,Is_causal>(Flash_fwd_params &, cudaStream_t) [with Kernel_traits=Flash_fwd_kernel_traits<32, 128, 128, 4, false, false, cutlass::half_t, Flash_kernel_traits<32, 128, 128, 4, cutlass::half_t>>, Is_dropout=true, Is_causal=true]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(58): here
            instantiation of "void run_mha_fwd_hdim32<T>(Flash_fwd_params &, cudaStream_t) [with T=cutlass::half_t]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_hdim32_fp16_sm80.cu(22): here

/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_kernel.h(140): warning: variable "MMA_M" was declared but never referenced
          detected during:
            instantiation of "void flash::compute_attn_1rowblock<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &, int, int, int) [with Kernel_traits=Flash_fwd_kernel_traits<32, 128, 128, 4, false, false, cutlass::half_t, Flash_kernel_traits<32, 128, 128, 4, cutlass::half_t>>, Is_dropout=true, Is_causal=true, Is_even_N=false, Is_even_K=true, Return_softmax=false, Params=Flash_fwd_params]" 
(588): here
            instantiation of "void flash::compute_attn<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &) [with Kernel_traits=Flash_fwd_kernel_traits<32, 128, 128, 4, false, false, cutlass::half_t, Flash_kernel_traits<32, 128, 128, 4, cutlass::half_t>>, Is_dropout=true, Is_causal=true, Is_even_N=false, Is_even_K=true, Return_softmax=false, Params=Flash_fwd_params]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(15): here
            instantiation of "void flash_fwd_kernel<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax>(Flash_fwd_params) [with Kernel_traits=Flash_fwd_kernel_traits<32, 128, 128, 4, false, false, cutlass::half_t, Flash_kernel_traits<32, 128, 128, 4, cutlass::half_t>>, Is_dropout=true, Is_causal=true, Is_even_N=false, Is_even_K=true, Return_softmax=false]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(34): here
            instantiation of "void run_flash_fwd<Kernel_traits,Is_dropout,Is_causal>(Flash_fwd_params &, cudaStream_t) [with Kernel_traits=Flash_fwd_kernel_traits<32, 128, 128, 4, false, false, cutlass::half_t, Flash_kernel_traits<32, 128, 128, 4, cutlass::half_t>>, Is_dropout=true, Is_causal=true]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(58): here
            instantiation of "void run_mha_fwd_hdim32<T>(Flash_fwd_params &, cudaStream_t) [with T=cutlass::half_t]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_hdim32_fp16_sm80.cu(22): here

/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_kernel.h(275): warning: variable "tQrQ" was declared but never referenced
          detected during:
            instantiation of "void flash::compute_attn_1rowblock<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &, int, int, int) [with Kernel_traits=Flash_fwd_kernel_traits<32, 128, 128, 4, false, false, cutlass::half_t, Flash_kernel_traits<32, 128, 128, 4, cutlass::half_t>>, Is_dropout=true, Is_causal=true, Is_even_N=false, Is_even_K=true, Return_softmax=false, Params=Flash_fwd_params]" 
(588): here
            instantiation of "void flash::compute_attn<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &) [with Kernel_traits=Flash_fwd_kernel_traits<32, 128, 128, 4, false, false, cutlass::half_t, Flash_kernel_traits<32, 128, 128, 4, cutlass::half_t>>, Is_dropout=true, Is_causal=true, Is_even_N=false, Is_even_K=true, Return_softmax=false, Params=Flash_fwd_params]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(15): here
            instantiation of "void flash_fwd_kernel<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax>(Flash_fwd_params) [with Kernel_traits=Flash_fwd_kernel_traits<32, 128, 128, 4, false, false, cutlass::half_t, Flash_kernel_traits<32, 128, 128, 4, cutlass::half_t>>, Is_dropout=true, Is_causal=true, Is_even_N=false, Is_even_K=true, Return_softmax=false]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(34): here
            instantiation of "void run_flash_fwd<Kernel_traits,Is_dropout,Is_causal>(Flash_fwd_params &, cudaStream_t) [with Kernel_traits=Flash_fwd_kernel_traits<32, 128, 128, 4, false, false, cutlass::half_t, Flash_kernel_traits<32, 128, 128, 4, cutlass::half_t>>, Is_dropout=true, Is_causal=true]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(58): here
            instantiation of "void run_mha_fwd_hdim32<T>(Flash_fwd_params &, cudaStream_t) [with T=cutlass::half_t]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_hdim32_fp16_sm80.cu(22): here

/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_kernel.h(140): warning: variable "MMA_M" was declared but never referenced
          detected during:
            instantiation of "void flash::compute_attn_1rowblock<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &, int, int, int) [with Kernel_traits=Flash_fwd_kernel_traits<32, 128, 128, 4, false, false, cutlass::half_t, Flash_kernel_traits<32, 128, 128, 4, cutlass::half_t>>, Is_dropout=true, Is_causal=true, Is_even_N=false, Is_even_K=false, Return_softmax=true, Params=Flash_fwd_params]" 
(588): here
            instantiation of "void flash::compute_attn<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &) [with Kernel_traits=Flash_fwd_kernel_traits<32, 128, 128, 4, false, false, cutlass::half_t, Flash_kernel_traits<32, 128, 128, 4, cutlass::half_t>>, Is_dropout=true, Is_causal=true, Is_even_N=false, Is_even_K=false, Return_softmax=true, Params=Flash_fwd_params]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(15): here
            instantiation of "void flash_fwd_kernel<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax>(Flash_fwd_params) [with Kernel_traits=Flash_fwd_kernel_traits<32, 128, 128, 4, false, false, cutlass::half_t, Flash_kernel_traits<32, 128, 128, 4, cutlass::half_t>>, Is_dropout=true, Is_causal=true, Is_even_N=false, Is_even_K=false, Return_softmax=true]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(34): here
            instantiation of "void run_flash_fwd<Kernel_traits,Is_dropout,Is_causal>(Flash_fwd_params &, cudaStream_t) [with Kernel_traits=Flash_fwd_kernel_traits<32, 128, 128, 4, false, false, cutlass::half_t, Flash_kernel_traits<32, 128, 128, 4, cutlass::half_t>>, Is_dropout=true, Is_causal=true]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(58): here
            instantiation of "void run_mha_fwd_hdim32<T>(Flash_fwd_params &, cudaStream_t) [with T=cutlass::half_t]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_hdim32_fp16_sm80.cu(22): here

/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_kernel.h(275): warning: variable "tQrQ" was declared but never referenced
          detected during:
            instantiation of "void flash::compute_attn_1rowblock<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &, int, int, int) [with Kernel_traits=Flash_fwd_kernel_traits<32, 128, 128, 4, false, false, cutlass::half_t, Flash_kernel_traits<32, 128, 128, 4, cutlass::half_t>>, Is_dropout=true, Is_causal=true, Is_even_N=false, Is_even_K=false, Return_softmax=true, Params=Flash_fwd_params]" 
(588): here
            instantiation of "void flash::compute_attn<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &) [with Kernel_traits=Flash_fwd_kernel_traits<32, 128, 128, 4, false, false, cutlass::half_t, Flash_kernel_traits<32, 128, 128, 4, cutlass::half_t>>, Is_dropout=true, Is_causal=true, Is_even_N=false, Is_even_K=false, Return_softmax=true, Params=Flash_fwd_params]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(15): here
            instantiation of "void flash_fwd_kernel<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax>(Flash_fwd_params) [with Kernel_traits=Flash_fwd_kernel_traits<32, 128, 128, 4, false, false, cutlass::half_t, Flash_kernel_traits<32, 128, 128, 4, cutlass::half_t>>, Is_dropout=true, Is_causal=true, Is_even_N=false, Is_even_K=false, Return_softmax=true]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(34): here
            instantiation of "void run_flash_fwd<Kernel_traits,Is_dropout,Is_causal>(Flash_fwd_params &, cudaStream_t) [with Kernel_traits=Flash_fwd_kernel_traits<32, 128, 128, 4, false, false, cutlass::half_t, Flash_kernel_traits<32, 128, 128, 4, cutlass::half_t>>, Is_dropout=true, Is_causal=true]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(58): here
            instantiation of "void run_mha_fwd_hdim32<T>(Flash_fwd_params &, cudaStream_t) [with T=cutlass::half_t]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_hdim32_fp16_sm80.cu(22): here

/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_kernel.h(140): warning: variable "MMA_M" was declared but never referenced
          detected during:
            instantiation of "void flash::compute_attn_1rowblock<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &, int, int, int) [with Kernel_traits=Flash_fwd_kernel_traits<32, 128, 128, 4, false, false, cutlass::half_t, Flash_kernel_traits<32, 128, 128, 4, cutlass::half_t>>, Is_dropout=true, Is_causal=true, Is_even_N=false, Is_even_K=false, Return_softmax=false, Params=Flash_fwd_params]" 
(588): here
            instantiation of "void flash::compute_attn<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &) [with Kernel_traits=Flash_fwd_kernel_traits<32, 128, 128, 4, false, false, cutlass::half_t, Flash_kernel_traits<32, 128, 128, 4, cutlass::half_t>>, Is_dropout=true, Is_causal=true, Is_even_N=false, Is_even_K=false, Return_softmax=false, Params=Flash_fwd_params]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(15): here
            instantiation of "void flash_fwd_kernel<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax>(Flash_fwd_params) [with Kernel_traits=Flash_fwd_kernel_traits<32, 128, 128, 4, false, false, cutlass::half_t, Flash_kernel_traits<32, 128, 128, 4, cutlass::half_t>>, Is_dropout=true, Is_causal=true, Is_even_N=false, Is_even_K=false, Return_softmax=false]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(34): here
            instantiation of "void run_flash_fwd<Kernel_traits,Is_dropout,Is_causal>(Flash_fwd_params &, cudaStream_t) [with Kernel_traits=Flash_fwd_kernel_traits<32, 128, 128, 4, false, false, cutlass::half_t, Flash_kernel_traits<32, 128, 128, 4, cutlass::half_t>>, Is_dropout=true, Is_causal=true]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(58): here
            instantiation of "void run_mha_fwd_hdim32<T>(Flash_fwd_params &, cudaStream_t) [with T=cutlass::half_t]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_hdim32_fp16_sm80.cu(22): here

/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_kernel.h(275): warning: variable "tQrQ" was declared but never referenced
          detected during:
            instantiation of "void flash::compute_attn_1rowblock<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &, int, int, int) [with Kernel_traits=Flash_fwd_kernel_traits<32, 128, 128, 4, false, false, cutlass::half_t, Flash_kernel_traits<32, 128, 128, 4, cutlass::half_t>>, Is_dropout=true, Is_causal=true, Is_even_N=false, Is_even_K=false, Return_softmax=false, Params=Flash_fwd_params]" 
(588): here
            instantiation of "void flash::compute_attn<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &) [with Kernel_traits=Flash_fwd_kernel_traits<32, 128, 128, 4, false, false, cutlass::half_t, Flash_kernel_traits<32, 128, 128, 4, cutlass::half_t>>, Is_dropout=true, Is_causal=true, Is_even_N=false, Is_even_K=false, Return_softmax=false, Params=Flash_fwd_params]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(15): here
            instantiation of "void flash_fwd_kernel<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax>(Flash_fwd_params) [with Kernel_traits=Flash_fwd_kernel_traits<32, 128, 128, 4, false, false, cutlass::half_t, Flash_kernel_traits<32, 128, 128, 4, cutlass::half_t>>, Is_dropout=true, Is_causal=true, Is_even_N=false, Is_even_K=false, Return_softmax=false]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(34): here
            instantiation of "void run_flash_fwd<Kernel_traits,Is_dropout,Is_causal>(Flash_fwd_params &, cudaStream_t) [with Kernel_traits=Flash_fwd_kernel_traits<32, 128, 128, 4, false, false, cutlass::half_t, Flash_kernel_traits<32, 128, 128, 4, cutlass::half_t>>, Is_dropout=true, Is_causal=true]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(58): here
            instantiation of "void run_mha_fwd_hdim32<T>(Flash_fwd_params &, cudaStream_t) [with T=cutlass::half_t]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_hdim32_fp16_sm80.cu(22): here

/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_kernel.h(140): warning: variable "MMA_M" was declared but never referenced
          detected during:
            instantiation of "void flash::compute_attn_1rowblock<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &, int, int, int) [with Kernel_traits=Flash_fwd_kernel_traits<32, 128, 128, 4, false, false, cutlass::half_t, Flash_kernel_traits<32, 128, 128, 4, cutlass::half_t>>, Is_dropout=true, Is_causal=false, Is_even_N=true, Is_even_K=true, Return_softmax=true, Params=Flash_fwd_params]" 
(588): here
            instantiation of "void flash::compute_attn<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &) [with Kernel_traits=Flash_fwd_kernel_traits<32, 128, 128, 4, false, false, cutlass::half_t, Flash_kernel_traits<32, 128, 128, 4, cutlass::half_t>>, Is_dropout=true, Is_causal=false, Is_even_N=true, Is_even_K=true, Return_softmax=true, Params=Flash_fwd_params]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(15): here
            instantiation of "void flash_fwd_kernel<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax>(Flash_fwd_params) [with Kernel_traits=Flash_fwd_kernel_traits<32, 128, 128, 4, false, false, cutlass::half_t, Flash_kernel_traits<32, 128, 128, 4, cutlass::half_t>>, Is_dropout=true, Is_causal=false, Is_even_N=true, Is_even_K=true, Return_softmax=true]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(34): here
            instantiation of "void run_flash_fwd<Kernel_traits,Is_dropout,Is_causal>(Flash_fwd_params &, cudaStream_t) [with Kernel_traits=Flash_fwd_kernel_traits<32, 128, 128, 4, false, false, cutlass::half_t, Flash_kernel_traits<32, 128, 128, 4, cutlass::half_t>>, Is_dropout=true, Is_causal=false]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(58): here
            instantiation of "void run_mha_fwd_hdim32<T>(Flash_fwd_params &, cudaStream_t) [with T=cutlass::half_t]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_hdim32_fp16_sm80.cu(22): here

/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_kernel.h(275): warning: variable "tQrQ" was declared but never referenced
          detected during:
            instantiation of "void flash::compute_attn_1rowblock<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &, int, int, int) [with Kernel_traits=Flash_fwd_kernel_traits<32, 128, 128, 4, false, false, cutlass::half_t, Flash_kernel_traits<32, 128, 128, 4, cutlass::half_t>>, Is_dropout=true, Is_causal=false, Is_even_N=true, Is_even_K=true, Return_softmax=true, Params=Flash_fwd_params]" 
(588): here
            instantiation of "void flash::compute_attn<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &) [with Kernel_traits=Flash_fwd_kernel_traits<32, 128, 128, 4, false, false, cutlass::half_t, Flash_kernel_traits<32, 128, 128, 4, cutlass::half_t>>, Is_dropout=true, Is_causal=false, Is_even_N=true, Is_even_K=true, Return_softmax=true, Params=Flash_fwd_params]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(15): here
            instantiation of "void flash_fwd_kernel<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax>(Flash_fwd_params) [with Kernel_traits=Flash_fwd_kernel_traits<32, 128, 128, 4, false, false, cutlass::half_t, Flash_kernel_traits<32, 128, 128, 4, cutlass::half_t>>, Is_dropout=true, Is_causal=false, Is_even_N=true, Is_even_K=true, Return_softmax=true]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(34): here
            instantiation of "void run_flash_fwd<Kernel_traits,Is_dropout,Is_causal>(Flash_fwd_params &, cudaStream_t) [with Kernel_traits=Flash_fwd_kernel_traits<32, 128, 128, 4, false, false, cutlass::half_t, Flash_kernel_traits<32, 128, 128, 4, cutlass::half_t>>, Is_dropout=true, Is_causal=false]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(58): here
            instantiation of "void run_mha_fwd_hdim32<T>(Flash_fwd_params &, cudaStream_t) [with T=cutlass::half_t]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_hdim32_fp16_sm80.cu(22): here

/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_kernel.h(140): warning: variable "MMA_M" was declared but never referenced
          detected during:
            instantiation of "void flash::compute_attn_1rowblock<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &, int, int, int) [with Kernel_traits=Flash_fwd_kernel_traits<32, 128, 128, 4, false, false, cutlass::half_t, Flash_kernel_traits<32, 128, 128, 4, cutlass::half_t>>, Is_dropout=true, Is_causal=false, Is_even_N=true, Is_even_K=true, Return_softmax=false, Params=Flash_fwd_params]" 
(588): here
            instantiation of "void flash::compute_attn<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &) [with Kernel_traits=Flash_fwd_kernel_traits<32, 128, 128, 4, false, false, cutlass::half_t, Flash_kernel_traits<32, 128, 128, 4, cutlass::half_t>>, Is_dropout=true, Is_causal=false, Is_even_N=true, Is_even_K=true, Return_softmax=false, Params=Flash_fwd_params]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(15): here
            instantiation of "void flash_fwd_kernel<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax>(Flash_fwd_params) [with Kernel_traits=Flash_fwd_kernel_traits<32, 128, 128, 4, false, false, cutlass::half_t, Flash_kernel_traits<32, 128, 128, 4, cutlass::half_t>>, Is_dropout=true, Is_causal=false, Is_even_N=true, Is_even_K=true, Return_softmax=false]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(34): here
            instantiation of "void run_flash_fwd<Kernel_traits,Is_dropout,Is_causal>(Flash_fwd_params &, cudaStream_t) [with Kernel_traits=Flash_fwd_kernel_traits<32, 128, 128, 4, false, false, cutlass::half_t, Flash_kernel_traits<32, 128, 128, 4, cutlass::half_t>>, Is_dropout=true, Is_causal=false]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(58): here
            instantiation of "void run_mha_fwd_hdim32<T>(Flash_fwd_params &, cudaStream_t) [with T=cutlass::half_t]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_hdim32_fp16_sm80.cu(22): here

/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_kernel.h(275): warning: variable "tQrQ" was declared but never referenced
          detected during:
            instantiation of "void flash::compute_attn_1rowblock<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &, int, int, int) [with Kernel_traits=Flash_fwd_kernel_traits<32, 128, 128, 4, false, false, cutlass::half_t, Flash_kernel_traits<32, 128, 128, 4, cutlass::half_t>>, Is_dropout=true, Is_causal=false, Is_even_N=true, Is_even_K=true, Return_softmax=false, Params=Flash_fwd_params]" 
(588): here
            instantiation of "void flash::compute_attn<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &) [with Kernel_traits=Flash_fwd_kernel_traits<32, 128, 128, 4, false, false, cutlass::half_t, Flash_kernel_traits<32, 128, 128, 4, cutlass::half_t>>, Is_dropout=true, Is_causal=false, Is_even_N=true, Is_even_K=true, Return_softmax=false, Params=Flash_fwd_params]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(15): here
            instantiation of "void flash_fwd_kernel<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax>(Flash_fwd_params) [with Kernel_traits=Flash_fwd_kernel_traits<32, 128, 128, 4, false, false, cutlass::half_t, Flash_kernel_traits<32, 128, 128, 4, cutlass::half_t>>, Is_dropout=true, Is_causal=false, Is_even_N=true, Is_even_K=true, Return_softmax=false]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(34): here
            instantiation of "void run_flash_fwd<Kernel_traits,Is_dropout,Is_causal>(Flash_fwd_params &, cudaStream_t) [with Kernel_traits=Flash_fwd_kernel_traits<32, 128, 128, 4, false, false, cutlass::half_t, Flash_kernel_traits<32, 128, 128, 4, cutlass::half_t>>, Is_dropout=true, Is_causal=false]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(58): here
            instantiation of "void run_mha_fwd_hdim32<T>(Flash_fwd_params &, cudaStream_t) [with T=cutlass::half_t]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_hdim32_fp16_sm80.cu(22): here

/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_kernel.h(140): warning: variable "MMA_M" was declared but never referenced
          detected during:
            instantiation of "void flash::compute_attn_1rowblock<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &, int, int, int) [with Kernel_traits=Flash_fwd_kernel_traits<32, 128, 128, 4, false, false, cutlass::half_t, Flash_kernel_traits<32, 128, 128, 4, cutlass::half_t>>, Is_dropout=true, Is_causal=false, Is_even_N=true, Is_even_K=false, Return_softmax=true, Params=Flash_fwd_params]" 
(588): here
            instantiation of "void flash::compute_attn<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &) [with Kernel_traits=Flash_fwd_kernel_traits<32, 128, 128, 4, false, false, cutlass::half_t, Flash_kernel_traits<32, 128, 128, 4, cutlass::half_t>>, Is_dropout=true, Is_causal=false, Is_even_N=true, Is_even_K=false, Return_softmax=true, Params=Flash_fwd_params]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(15): here
            instantiation of "void flash_fwd_kernel<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax>(Flash_fwd_params) [with Kernel_traits=Flash_fwd_kernel_traits<32, 128, 128, 4, false, false, cutlass::half_t, Flash_kernel_traits<32, 128, 128, 4, cutlass::half_t>>, Is_dropout=true, Is_causal=false, Is_even_N=true, Is_even_K=false, Return_softmax=true]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(34): here
            instantiation of "void run_flash_fwd<Kernel_traits,Is_dropout,Is_causal>(Flash_fwd_params &, cudaStream_t) [with Kernel_traits=Flash_fwd_kernel_traits<32, 128, 128, 4, false, false, cutlass::half_t, Flash_kernel_traits<32, 128, 128, 4, cutlass::half_t>>, Is_dropout=true, Is_causal=false]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(58): here
            instantiation of "void run_mha_fwd_hdim32<T>(Flash_fwd_params &, cudaStream_t) [with T=cutlass::half_t]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_hdim32_fp16_sm80.cu(22): here

/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_kernel.h(275): warning: variable "tQrQ" was declared but never referenced
          detected during:
            instantiation of "void flash::compute_attn_1rowblock<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &, int, int, int) [with Kernel_traits=Flash_fwd_kernel_traits<32, 128, 128, 4, false, false, cutlass::half_t, Flash_kernel_traits<32, 128, 128, 4, cutlass::half_t>>, Is_dropout=true, Is_causal=false, Is_even_N=true, Is_even_K=false, Return_softmax=true, Params=Flash_fwd_params]" 
(588): here
            instantiation of "void flash::compute_attn<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &) [with Kernel_traits=Flash_fwd_kernel_traits<32, 128, 128, 4, false, false, cutlass::half_t, Flash_kernel_traits<32, 128, 128, 4, cutlass::half_t>>, Is_dropout=true, Is_causal=false, Is_even_N=true, Is_even_K=false, Return_softmax=true, Params=Flash_fwd_params]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(15): here
            instantiation of "void flash_fwd_kernel<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax>(Flash_fwd_params) [with Kernel_traits=Flash_fwd_kernel_traits<32, 128, 128, 4, false, false, cutlass::half_t, Flash_kernel_traits<32, 128, 128, 4, cutlass::half_t>>, Is_dropout=true, Is_causal=false, Is_even_N=true, Is_even_K=false, Return_softmax=true]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(34): here
            instantiation of "void run_flash_fwd<Kernel_traits,Is_dropout,Is_causal>(Flash_fwd_params &, cudaStream_t) [with Kernel_traits=Flash_fwd_kernel_traits<32, 128, 128, 4, false, false, cutlass::half_t, Flash_kernel_traits<32, 128, 128, 4, cutlass::half_t>>, Is_dropout=true, Is_causal=false]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(58): here
            instantiation of "void run_mha_fwd_hdim32<T>(Flash_fwd_params &, cudaStream_t) [with T=cutlass::half_t]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_hdim32_fp16_sm80.cu(22): here

/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_kernel.h(140): warning: variable "MMA_M" was declared but never referenced
          detected during:
            instantiation of "void flash::compute_attn_1rowblock<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &, int, int, int) [with Kernel_traits=Flash_fwd_kernel_traits<32, 128, 128, 4, false, false, cutlass::half_t, Flash_kernel_traits<32, 128, 128, 4, cutlass::half_t>>, Is_dropout=true, Is_causal=false, Is_even_N=true, Is_even_K=false, Return_softmax=false, Params=Flash_fwd_params]" 
(588): here
            instantiation of "void flash::compute_attn<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &) [with Kernel_traits=Flash_fwd_kernel_traits<32, 128, 128, 4, false, false, cutlass::half_t, Flash_kernel_traits<32, 128, 128, 4, cutlass::half_t>>, Is_dropout=true, Is_causal=false, Is_even_N=true, Is_even_K=false, Return_softmax=false, Params=Flash_fwd_params]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(15): here
            instantiation of "void flash_fwd_kernel<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax>(Flash_fwd_params) [with Kernel_traits=Flash_fwd_kernel_traits<32, 128, 128, 4, false, false, cutlass::half_t, Flash_kernel_traits<32, 128, 128, 4, cutlass::half_t>>, Is_dropout=true, Is_causal=false, Is_even_N=true, Is_even_K=false, Return_softmax=false]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(34): here
            instantiation of "void run_flash_fwd<Kernel_traits,Is_dropout,Is_causal>(Flash_fwd_params &, cudaStream_t) [with Kernel_traits=Flash_fwd_kernel_traits<32, 128, 128, 4, false, false, cutlass::half_t, Flash_kernel_traits<32, 128, 128, 4, cutlass::half_t>>, Is_dropout=true, Is_causal=false]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(58): here
            instantiation of "void run_mha_fwd_hdim32<T>(Flash_fwd_params &, cudaStream_t) [with T=cutlass::half_t]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_hdim32_fp16_sm80.cu(22): here

/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_kernel.h(275): warning: variable "tQrQ" was declared but never referenced
          detected during:
            instantiation of "void flash::compute_attn_1rowblock<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &, int, int, int) [with Kernel_traits=Flash_fwd_kernel_traits<32, 128, 128, 4, false, false, cutlass::half_t, Flash_kernel_traits<32, 128, 128, 4, cutlass::half_t>>, Is_dropout=true, Is_causal=false, Is_even_N=true, Is_even_K=false, Return_softmax=false, Params=Flash_fwd_params]" 
(588): here
            instantiation of "void flash::compute_attn<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &) [with Kernel_traits=Flash_fwd_kernel_traits<32, 128, 128, 4, false, false, cutlass::half_t, Flash_kernel_traits<32, 128, 128, 4, cutlass::half_t>>, Is_dropout=true, Is_causal=false, Is_even_N=true, Is_even_K=false, Return_softmax=false, Params=Flash_fwd_params]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(15): here
            instantiation of "void flash_fwd_kernel<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax>(Flash_fwd_params) [with Kernel_traits=Flash_fwd_kernel_traits<32, 128, 128, 4, false, false, cutlass::half_t, Flash_kernel_traits<32, 128, 128, 4, cutlass::half_t>>, Is_dropout=true, Is_causal=false, Is_even_N=true, Is_even_K=false, Return_softmax=false]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(34): here
            instantiation of "void run_flash_fwd<Kernel_traits,Is_dropout,Is_causal>(Flash_fwd_params &, cudaStream_t) [with Kernel_traits=Flash_fwd_kernel_traits<32, 128, 128, 4, false, false, cutlass::half_t, Flash_kernel_traits<32, 128, 128, 4, cutlass::half_t>>, Is_dropout=true, Is_causal=false]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(58): here
            instantiation of "void run_mha_fwd_hdim32<T>(Flash_fwd_params &, cudaStream_t) [with T=cutlass::half_t]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_hdim32_fp16_sm80.cu(22): here

/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_kernel.h(140): warning: variable "MMA_M" was declared but never referenced
          detected during:
            instantiation of "void flash::compute_attn_1rowblock<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &, int, int, int) [with Kernel_traits=Flash_fwd_kernel_traits<32, 128, 128, 4, false, false, cutlass::half_t, Flash_kernel_traits<32, 128, 128, 4, cutlass::half_t>>, Is_dropout=true, Is_causal=false, Is_even_N=false, Is_even_K=true, Return_softmax=true, Params=Flash_fwd_params]" 
(588): here
            instantiation of "void flash::compute_attn<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &) [with Kernel_traits=Flash_fwd_kernel_traits<32, 128, 128, 4, false, false, cutlass::half_t, Flash_kernel_traits<32, 128, 128, 4, cutlass::half_t>>, Is_dropout=true, Is_causal=false, Is_even_N=false, Is_even_K=true, Return_softmax=true, Params=Flash_fwd_params]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(15): here
            instantiation of "void flash_fwd_kernel<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax>(Flash_fwd_params) [with Kernel_traits=Flash_fwd_kernel_traits<32, 128, 128, 4, false, false, cutlass::half_t, Flash_kernel_traits<32, 128, 128, 4, cutlass::half_t>>, Is_dropout=true, Is_causal=false, Is_even_N=false, Is_even_K=true, Return_softmax=true]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(34): here
            instantiation of "void run_flash_fwd<Kernel_traits,Is_dropout,Is_causal>(Flash_fwd_params &, cudaStream_t) [with Kernel_traits=Flash_fwd_kernel_traits<32, 128, 128, 4, false, false, cutlass::half_t, Flash_kernel_traits<32, 128, 128, 4, cutlass::half_t>>, Is_dropout=true, Is_causal=false]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(58): here
            instantiation of "void run_mha_fwd_hdim32<T>(Flash_fwd_params &, cudaStream_t) [with T=cutlass::half_t]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_hdim32_fp16_sm80.cu(22): here

/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_kernel.h(275): warning: variable "tQrQ" was declared but never referenced
          detected during:
            instantiation of "void flash::compute_attn_1rowblock<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &, int, int, int) [with Kernel_traits=Flash_fwd_kernel_traits<32, 128, 128, 4, false, false, cutlass::half_t, Flash_kernel_traits<32, 128, 128, 4, cutlass::half_t>>, Is_dropout=true, Is_causal=false, Is_even_N=false, Is_even_K=true, Return_softmax=true, Params=Flash_fwd_params]" 
(588): here
            instantiation of "void flash::compute_attn<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &) [with Kernel_traits=Flash_fwd_kernel_traits<32, 128, 128, 4, false, false, cutlass::half_t, Flash_kernel_traits<32, 128, 128, 4, cutlass::half_t>>, Is_dropout=true, Is_causal=false, Is_even_N=false, Is_even_K=true, Return_softmax=true, Params=Flash_fwd_params]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(15): here
            instantiation of "void flash_fwd_kernel<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax>(Flash_fwd_params) [with Kernel_traits=Flash_fwd_kernel_traits<32, 128, 128, 4, false, false, cutlass::half_t, Flash_kernel_traits<32, 128, 128, 4, cutlass::half_t>>, Is_dropout=true, Is_causal=false, Is_even_N=false, Is_even_K=true, Return_softmax=true]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(34): here
            instantiation of "void run_flash_fwd<Kernel_traits,Is_dropout,Is_causal>(Flash_fwd_params &, cudaStream_t) [with Kernel_traits=Flash_fwd_kernel_traits<32, 128, 128, 4, false, false, cutlass::half_t, Flash_kernel_traits<32, 128, 128, 4, cutlass::half_t>>, Is_dropout=true, Is_causal=false]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(58): here
            instantiation of "void run_mha_fwd_hdim32<T>(Flash_fwd_params &, cudaStream_t) [with T=cutlass::half_t]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_hdim32_fp16_sm80.cu(22): here

/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_kernel.h(140): warning: variable "MMA_M" was declared but never referenced
          detected during:
            instantiation of "void flash::compute_attn_1rowblock<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &, int, int, int) [with Kernel_traits=Flash_fwd_kernel_traits<32, 128, 128, 4, false, false, cutlass::half_t, Flash_kernel_traits<32, 128, 128, 4, cutlass::half_t>>, Is_dropout=true, Is_causal=false, Is_even_N=false, Is_even_K=true, Return_softmax=false, Params=Flash_fwd_params]" 
(588): here
            instantiation of "void flash::compute_attn<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &) [with Kernel_traits=Flash_fwd_kernel_traits<32, 128, 128, 4, false, false, cutlass::half_t, Flash_kernel_traits<32, 128, 128, 4, cutlass::half_t>>, Is_dropout=true, Is_causal=false, Is_even_N=false, Is_even_K=true, Return_softmax=false, Params=Flash_fwd_params]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(15): here
            instantiation of "void flash_fwd_kernel<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax>(Flash_fwd_params) [with Kernel_traits=Flash_fwd_kernel_traits<32, 128, 128, 4, false, false, cutlass::half_t, Flash_kernel_traits<32, 128, 128, 4, cutlass::half_t>>, Is_dropout=true, Is_causal=false, Is_even_N=false, Is_even_K=true, Return_softmax=false]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(34): here
            instantiation of "void run_flash_fwd<Kernel_traits,Is_dropout,Is_causal>(Flash_fwd_params &, cudaStream_t) [with Kernel_traits=Flash_fwd_kernel_traits<32, 128, 128, 4, false, false, cutlass::half_t, Flash_kernel_traits<32, 128, 128, 4, cutlass::half_t>>, Is_dropout=true, Is_causal=false]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(58): here
            instantiation of "void run_mha_fwd_hdim32<T>(Flash_fwd_params &, cudaStream_t) [with T=cutlass::half_t]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_hdim32_fp16_sm80.cu(22): here

/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_kernel.h(275): warning: variable "tQrQ" was declared but never referenced
          detected during:
            instantiation of "void flash::compute_attn_1rowblock<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &, int, int, int) [with Kernel_traits=Flash_fwd_kernel_traits<32, 128, 128, 4, false, false, cutlass::half_t, Flash_kernel_traits<32, 128, 128, 4, cutlass::half_t>>, Is_dropout=true, Is_causal=false, Is_even_N=false, Is_even_K=true, Return_softmax=false, Params=Flash_fwd_params]" 
(588): here
            instantiation of "void flash::compute_attn<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &) [with Kernel_traits=Flash_fwd_kernel_traits<32, 128, 128, 4, false, false, cutlass::half_t, Flash_kernel_traits<32, 128, 128, 4, cutlass::half_t>>, Is_dropout=true, Is_causal=false, Is_even_N=false, Is_even_K=true, Return_softmax=false, Params=Flash_fwd_params]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(15): here
            instantiation of "void flash_fwd_kernel<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax>(Flash_fwd_params) [with Kernel_traits=Flash_fwd_kernel_traits<32, 128, 128, 4, false, false, cutlass::half_t, Flash_kernel_traits<32, 128, 128, 4, cutlass::half_t>>, Is_dropout=true, Is_causal=false, Is_even_N=false, Is_even_K=true, Return_softmax=false]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(34): here
            instantiation of "void run_flash_fwd<Kernel_traits,Is_dropout,Is_causal>(Flash_fwd_params &, cudaStream_t) [with Kernel_traits=Flash_fwd_kernel_traits<32, 128, 128, 4, false, false, cutlass::half_t, Flash_kernel_traits<32, 128, 128, 4, cutlass::half_t>>, Is_dropout=true, Is_causal=false]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(58): here
            instantiation of "void run_mha_fwd_hdim32<T>(Flash_fwd_params &, cudaStream_t) [with T=cutlass::half_t]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_hdim32_fp16_sm80.cu(22): here

/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_kernel.h(140): warning: variable "MMA_M" was declared but never referenced
          detected during:
            instantiation of "void flash::compute_attn_1rowblock<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &, int, int, int) [with Kernel_traits=Flash_fwd_kernel_traits<32, 128, 128, 4, false, false, cutlass::half_t, Flash_kernel_traits<32, 128, 128, 4, cutlass::half_t>>, Is_dropout=true, Is_causal=false, Is_even_N=false, Is_even_K=false, Return_softmax=true, Params=Flash_fwd_params]" 
(588): here
            instantiation of "void flash::compute_attn<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &) [with Kernel_traits=Flash_fwd_kernel_traits<32, 128, 128, 4, false, false, cutlass::half_t, Flash_kernel_traits<32, 128, 128, 4, cutlass::half_t>>, Is_dropout=true, Is_causal=false, Is_even_N=false, Is_even_K=false, Return_softmax=true, Params=Flash_fwd_params]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(15): here
            instantiation of "void flash_fwd_kernel<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax>(Flash_fwd_params) [with Kernel_traits=Flash_fwd_kernel_traits<32, 128, 128, 4, false, false, cutlass::half_t, Flash_kernel_traits<32, 128, 128, 4, cutlass::half_t>>, Is_dropout=true, Is_causal=false, Is_even_N=false, Is_even_K=false, Return_softmax=true]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(34): here
            instantiation of "void run_flash_fwd<Kernel_traits,Is_dropout,Is_causal>(Flash_fwd_params &, cudaStream_t) [with Kernel_traits=Flash_fwd_kernel_traits<32, 128, 128, 4, false, false, cutlass::half_t, Flash_kernel_traits<32, 128, 128, 4, cutlass::half_t>>, Is_dropout=true, Is_causal=false]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(58): here
            instantiation of "void run_mha_fwd_hdim32<T>(Flash_fwd_params &, cudaStream_t) [with T=cutlass::half_t]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_hdim32_fp16_sm80.cu(22): here

/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_kernel.h(275): warning: variable "tQrQ" was declared but never referenced
          detected during:
            instantiation of "void flash::compute_attn_1rowblock<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &, int, int, int) [with Kernel_traits=Flash_fwd_kernel_traits<32, 128, 128, 4, false, false, cutlass::half_t, Flash_kernel_traits<32, 128, 128, 4, cutlass::half_t>>, Is_dropout=true, Is_causal=false, Is_even_N=false, Is_even_K=false, Return_softmax=true, Params=Flash_fwd_params]" 
(588): here
            instantiation of "void flash::compute_attn<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &) [with Kernel_traits=Flash_fwd_kernel_traits<32, 128, 128, 4, false, false, cutlass::half_t, Flash_kernel_traits<32, 128, 128, 4, cutlass::half_t>>, Is_dropout=true, Is_causal=false, Is_even_N=false, Is_even_K=false, Return_softmax=true, Params=Flash_fwd_params]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(15): here
            instantiation of "void flash_fwd_kernel<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax>(Flash_fwd_params) [with Kernel_traits=Flash_fwd_kernel_traits<32, 128, 128, 4, false, false, cutlass::half_t, Flash_kernel_traits<32, 128, 128, 4, cutlass::half_t>>, Is_dropout=true, Is_causal=false, Is_even_N=false, Is_even_K=false, Return_softmax=true]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(34): here
            instantiation of "void run_flash_fwd<Kernel_traits,Is_dropout,Is_causal>(Flash_fwd_params &, cudaStream_t) [with Kernel_traits=Flash_fwd_kernel_traits<32, 128, 128, 4, false, false, cutlass::half_t, Flash_kernel_traits<32, 128, 128, 4, cutlass::half_t>>, Is_dropout=true, Is_causal=false]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(58): here
            instantiation of "void run_mha_fwd_hdim32<T>(Flash_fwd_params &, cudaStream_t) [with T=cutlass::half_t]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_hdim32_fp16_sm80.cu(22): here

/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_kernel.h(140): warning: variable "MMA_M" was declared but never referenced
          detected during:
            instantiation of "void flash::compute_attn_1rowblock<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &, int, int, int) [with Kernel_traits=Flash_fwd_kernel_traits<32, 128, 128, 4, false, false, cutlass::half_t, Flash_kernel_traits<32, 128, 128, 4, cutlass::half_t>>, Is_dropout=true, Is_causal=false, Is_even_N=false, Is_even_K=false, Return_softmax=false, Params=Flash_fwd_params]" 
(588): here
            instantiation of "void flash::compute_attn<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &) [with Kernel_traits=Flash_fwd_kernel_traits<32, 128, 128, 4, false, false, cutlass::half_t, Flash_kernel_traits<32, 128, 128, 4, cutlass::half_t>>, Is_dropout=true, Is_causal=false, Is_even_N=false, Is_even_K=false, Return_softmax=false, Params=Flash_fwd_params]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(15): here
            instantiation of "void flash_fwd_kernel<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax>(Flash_fwd_params) [with Kernel_traits=Flash_fwd_kernel_traits<32, 128, 128, 4, false, false, cutlass::half_t, Flash_kernel_traits<32, 128, 128, 4, cutlass::half_t>>, Is_dropout=true, Is_causal=false, Is_even_N=false, Is_even_K=false, Return_softmax=false]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(34): here
            instantiation of "void run_flash_fwd<Kernel_traits,Is_dropout,Is_causal>(Flash_fwd_params &, cudaStream_t) [with Kernel_traits=Flash_fwd_kernel_traits<32, 128, 128, 4, false, false, cutlass::half_t, Flash_kernel_traits<32, 128, 128, 4, cutlass::half_t>>, Is_dropout=true, Is_causal=false]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(58): here
            instantiation of "void run_mha_fwd_hdim32<T>(Flash_fwd_params &, cudaStream_t) [with T=cutlass::half_t]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_hdim32_fp16_sm80.cu(22): here

/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_kernel.h(275): warning: variable "tQrQ" was declared but never referenced
          detected during:
            instantiation of "void flash::compute_attn_1rowblock<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &, int, int, int) [with Kernel_traits=Flash_fwd_kernel_traits<32, 128, 128, 4, false, false, cutlass::half_t, Flash_kernel_traits<32, 128, 128, 4, cutlass::half_t>>, Is_dropout=true, Is_causal=false, Is_even_N=false, Is_even_K=false, Return_softmax=false, Params=Flash_fwd_params]" 
(588): here
            instantiation of "void flash::compute_attn<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &) [with Kernel_traits=Flash_fwd_kernel_traits<32, 128, 128, 4, false, false, cutlass::half_t, Flash_kernel_traits<32, 128, 128, 4, cutlass::half_t>>, Is_dropout=true, Is_causal=false, Is_even_N=false, Is_even_K=false, Return_softmax=false, Params=Flash_fwd_params]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(15): here
            instantiation of "void flash_fwd_kernel<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax>(Flash_fwd_params) [with Kernel_traits=Flash_fwd_kernel_traits<32, 128, 128, 4, false, false, cutlass::half_t, Flash_kernel_traits<32, 128, 128, 4, cutlass::half_t>>, Is_dropout=true, Is_causal=false, Is_even_N=false, Is_even_K=false, Return_softmax=false]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(34): here
            instantiation of "void run_flash_fwd<Kernel_traits,Is_dropout,Is_causal>(Flash_fwd_params &, cudaStream_t) [with Kernel_traits=Flash_fwd_kernel_traits<32, 128, 128, 4, false, false, cutlass::half_t, Flash_kernel_traits<32, 128, 128, 4, cutlass::half_t>>, Is_dropout=true, Is_causal=false]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(58): here
            instantiation of "void run_mha_fwd_hdim32<T>(Flash_fwd_params &, cudaStream_t) [with T=cutlass::half_t]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_hdim32_fp16_sm80.cu(22): here

/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_kernel.h(140): warning: variable "MMA_M" was declared but never referenced
          detected during:
            instantiation of "void flash::compute_attn_1rowblock<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &, int, int, int) [with Kernel_traits=Flash_fwd_kernel_traits<32, 128, 128, 4, false, false, cutlass::half_t, Flash_kernel_traits<32, 128, 128, 4, cutlass::half_t>>, Is_dropout=false, Is_causal=true, Is_even_N=true, Is_even_K=true, Return_softmax=false, Params=Flash_fwd_params]" 
(588): here
            instantiation of "void flash::compute_attn<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &) [with Kernel_traits=Flash_fwd_kernel_traits<32, 128, 128, 4, false, false, cutlass::half_t, Flash_kernel_traits<32, 128, 128, 4, cutlass::half_t>>, Is_dropout=false, Is_causal=true, Is_even_N=true, Is_even_K=true, Return_softmax=false, Params=Flash_fwd_params]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(15): here
            instantiation of "void flash_fwd_kernel<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax>(Flash_fwd_params) [with Kernel_traits=Flash_fwd_kernel_traits<32, 128, 128, 4, false, false, cutlass::half_t, Flash_kernel_traits<32, 128, 128, 4, cutlass::half_t>>, Is_dropout=false, Is_causal=true, Is_even_N=true, Is_even_K=true, Return_softmax=false]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(34): here
            instantiation of "void run_flash_fwd<Kernel_traits,Is_dropout,Is_causal>(Flash_fwd_params &, cudaStream_t) [with Kernel_traits=Flash_fwd_kernel_traits<32, 128, 128, 4, false, false, cutlass::half_t, Flash_kernel_traits<32, 128, 128, 4, cutlass::half_t>>, Is_dropout=false, Is_causal=true]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(58): here
            instantiation of "void run_mha_fwd_hdim32<T>(Flash_fwd_params &, cudaStream_t) [with T=cutlass::half_t]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_hdim32_fp16_sm80.cu(22): here

/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_kernel.h(275): warning: variable "tQrQ" was declared but never referenced
          detected during:
            instantiation of "void flash::compute_attn_1rowblock<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &, int, int, int) [with Kernel_traits=Flash_fwd_kernel_traits<32, 128, 128, 4, false, false, cutlass::half_t, Flash_kernel_traits<32, 128, 128, 4, cutlass::half_t>>, Is_dropout=false, Is_causal=true, Is_even_N=true, Is_even_K=true, Return_softmax=false, Params=Flash_fwd_params]" 
(588): here
            instantiation of "void flash::compute_attn<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &) [with Kernel_traits=Flash_fwd_kernel_traits<32, 128, 128, 4, false, false, cutlass::half_t, Flash_kernel_traits<32, 128, 128, 4, cutlass::half_t>>, Is_dropout=false, Is_causal=true, Is_even_N=true, Is_even_K=true, Return_softmax=false, Params=Flash_fwd_params]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(15): here
            instantiation of "void flash_fwd_kernel<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax>(Flash_fwd_params) [with Kernel_traits=Flash_fwd_kernel_traits<32, 128, 128, 4, false, false, cutlass::half_t, Flash_kernel_traits<32, 128, 128, 4, cutlass::half_t>>, Is_dropout=false, Is_causal=true, Is_even_N=true, Is_even_K=true, Return_softmax=false]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(34): here
            instantiation of "void run_flash_fwd<Kernel_traits,Is_dropout,Is_causal>(Flash_fwd_params &, cudaStream_t) [with Kernel_traits=Flash_fwd_kernel_traits<32, 128, 128, 4, false, false, cutlass::half_t, Flash_kernel_traits<32, 128, 128, 4, cutlass::half_t>>, Is_dropout=false, Is_causal=true]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(58): here
            instantiation of "void run_mha_fwd_hdim32<T>(Flash_fwd_params &, cudaStream_t) [with T=cutlass::half_t]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_hdim32_fp16_sm80.cu(22): here

/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_kernel.h(140): warning: variable "MMA_M" was declared but never referenced
          detected during:
            instantiation of "void flash::compute_attn_1rowblock<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &, int, int, int) [with Kernel_traits=Flash_fwd_kernel_traits<32, 128, 128, 4, false, false, cutlass::half_t, Flash_kernel_traits<32, 128, 128, 4, cutlass::half_t>>, Is_dropout=false, Is_causal=true, Is_even_N=true, Is_even_K=false, Return_softmax=false, Params=Flash_fwd_params]" 
(588): here
            instantiation of "void flash::compute_attn<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &) [with Kernel_traits=Flash_fwd_kernel_traits<32, 128, 128, 4, false, false, cutlass::half_t, Flash_kernel_traits<32, 128, 128, 4, cutlass::half_t>>, Is_dropout=false, Is_causal=true, Is_even_N=true, Is_even_K=false, Return_softmax=false, Params=Flash_fwd_params]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(15): here
            instantiation of "void flash_fwd_kernel<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax>(Flash_fwd_params) [with Kernel_traits=Flash_fwd_kernel_traits<32, 128, 128, 4, false, false, cutlass::half_t, Flash_kernel_traits<32, 128, 128, 4, cutlass::half_t>>, Is_dropout=false, Is_causal=true, Is_even_N=true, Is_even_K=false, Return_softmax=false]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(34): here
            instantiation of "void run_flash_fwd<Kernel_traits,Is_dropout,Is_causal>(Flash_fwd_params &, cudaStream_t) [with Kernel_traits=Flash_fwd_kernel_traits<32, 128, 128, 4, false, false, cutlass::half_t, Flash_kernel_traits<32, 128, 128, 4, cutlass::half_t>>, Is_dropout=false, Is_causal=true]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(58): here
            instantiation of "void run_mha_fwd_hdim32<T>(Flash_fwd_params &, cudaStream_t) [with T=cutlass::half_t]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_hdim32_fp16_sm80.cu(22): here

/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_kernel.h(275): warning: variable "tQrQ" was declared but never referenced
          detected during:
            instantiation of "void flash::compute_attn_1rowblock<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &, int, int, int) [with Kernel_traits=Flash_fwd_kernel_traits<32, 128, 128, 4, false, false, cutlass::half_t, Flash_kernel_traits<32, 128, 128, 4, cutlass::half_t>>, Is_dropout=false, Is_causal=true, Is_even_N=true, Is_even_K=false, Return_softmax=false, Params=Flash_fwd_params]" 
(588): here
            instantiation of "void flash::compute_attn<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &) [with Kernel_traits=Flash_fwd_kernel_traits<32, 128, 128, 4, false, false, cutlass::half_t, Flash_kernel_traits<32, 128, 128, 4, cutlass::half_t>>, Is_dropout=false, Is_causal=true, Is_even_N=true, Is_even_K=false, Return_softmax=false, Params=Flash_fwd_params]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(15): here
            instantiation of "void flash_fwd_kernel<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax>(Flash_fwd_params) [with Kernel_traits=Flash_fwd_kernel_traits<32, 128, 128, 4, false, false, cutlass::half_t, Flash_kernel_traits<32, 128, 128, 4, cutlass::half_t>>, Is_dropout=false, Is_causal=true, Is_even_N=true, Is_even_K=false, Return_softmax=false]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(34): here
            instantiation of "void run_flash_fwd<Kernel_traits,Is_dropout,Is_causal>(Flash_fwd_params &, cudaStream_t) [with Kernel_traits=Flash_fwd_kernel_traits<32, 128, 128, 4, false, false, cutlass::half_t, Flash_kernel_traits<32, 128, 128, 4, cutlass::half_t>>, Is_dropout=false, Is_causal=true]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(58): here
            instantiation of "void run_mha_fwd_hdim32<T>(Flash_fwd_params &, cudaStream_t) [with T=cutlass::half_t]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_hdim32_fp16_sm80.cu(22): here

/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_kernel.h(140): warning: variable "MMA_M" was declared but never referenced
          detected during:
            instantiation of "void flash::compute_attn_1rowblock<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &, int, int, int) [with Kernel_traits=Flash_fwd_kernel_traits<32, 128, 128, 4, false, false, cutlass::half_t, Flash_kernel_traits<32, 128, 128, 4, cutlass::half_t>>, Is_dropout=false, Is_causal=true, Is_even_N=false, Is_even_K=true, Return_softmax=false, Params=Flash_fwd_params]" 
(588): here
            instantiation of "void flash::compute_attn<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &) [with Kernel_traits=Flash_fwd_kernel_traits<32, 128, 128, 4, false, false, cutlass::half_t, Flash_kernel_traits<32, 128, 128, 4, cutlass::half_t>>, Is_dropout=false, Is_causal=true, Is_even_N=false, Is_even_K=true, Return_softmax=false, Params=Flash_fwd_params]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(15): here
            instantiation of "void flash_fwd_kernel<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax>(Flash_fwd_params) [with Kernel_traits=Flash_fwd_kernel_traits<32, 128, 128, 4, false, false, cutlass::half_t, Flash_kernel_traits<32, 128, 128, 4, cutlass::half_t>>, Is_dropout=false, Is_causal=true, Is_even_N=false, Is_even_K=true, Return_softmax=false]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(34): here
            instantiation of "void run_flash_fwd<Kernel_traits,Is_dropout,Is_causal>(Flash_fwd_params &, cudaStream_t) [with Kernel_traits=Flash_fwd_kernel_traits<32, 128, 128, 4, false, false, cutlass::half_t, Flash_kernel_traits<32, 128, 128, 4, cutlass::half_t>>, Is_dropout=false, Is_causal=true]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(58): here
            instantiation of "void run_mha_fwd_hdim32<T>(Flash_fwd_params &, cudaStream_t) [with T=cutlass::half_t]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_hdim32_fp16_sm80.cu(22): here

/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_kernel.h(275): warning: variable "tQrQ" was declared but never referenced
          detected during:
            instantiation of "void flash::compute_attn_1rowblock<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &, int, int, int) [with Kernel_traits=Flash_fwd_kernel_traits<32, 128, 128, 4, false, false, cutlass::half_t, Flash_kernel_traits<32, 128, 128, 4, cutlass::half_t>>, Is_dropout=false, Is_causal=true, Is_even_N=false, Is_even_K=true, Return_softmax=false, Params=Flash_fwd_params]" 
(588): here
            instantiation of "void flash::compute_attn<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &) [with Kernel_traits=Flash_fwd_kernel_traits<32, 128, 128, 4, false, false, cutlass::half_t, Flash_kernel_traits<32, 128, 128, 4, cutlass::half_t>>, Is_dropout=false, Is_causal=true, Is_even_N=false, Is_even_K=true, Return_softmax=false, Params=Flash_fwd_params]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(15): here
            instantiation of "void flash_fwd_kernel<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax>(Flash_fwd_params) [with Kernel_traits=Flash_fwd_kernel_traits<32, 128, 128, 4, false, false, cutlass::half_t, Flash_kernel_traits<32, 128, 128, 4, cutlass::half_t>>, Is_dropout=false, Is_causal=true, Is_even_N=false, Is_even_K=true, Return_softmax=false]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(34): here
            instantiation of "void run_flash_fwd<Kernel_traits,Is_dropout,Is_causal>(Flash_fwd_params &, cudaStream_t) [with Kernel_traits=Flash_fwd_kernel_traits<32, 128, 128, 4, false, false, cutlass::half_t, Flash_kernel_traits<32, 128, 128, 4, cutlass::half_t>>, Is_dropout=false, Is_causal=true]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(58): here
            instantiation of "void run_mha_fwd_hdim32<T>(Flash_fwd_params &, cudaStream_t) [with T=cutlass::half_t]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_hdim32_fp16_sm80.cu(22): here

/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_kernel.h(140): warning: variable "MMA_M" was declared but never referenced
          detected during:
            instantiation of "void flash::compute_attn_1rowblock<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &, int, int, int) [with Kernel_traits=Flash_fwd_kernel_traits<32, 128, 128, 4, false, false, cutlass::half_t, Flash_kernel_traits<32, 128, 128, 4, cutlass::half_t>>, Is_dropout=false, Is_causal=true, Is_even_N=false, Is_even_K=false, Return_softmax=false, Params=Flash_fwd_params]" 
(588): here
            instantiation of "void flash::compute_attn<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &) [with Kernel_traits=Flash_fwd_kernel_traits<32, 128, 128, 4, false, false, cutlass::half_t, Flash_kernel_traits<32, 128, 128, 4, cutlass::half_t>>, Is_dropout=false, Is_causal=true, Is_even_N=false, Is_even_K=false, Return_softmax=false, Params=Flash_fwd_params]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(15): here
            instantiation of "void flash_fwd_kernel<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax>(Flash_fwd_params) [with Kernel_traits=Flash_fwd_kernel_traits<32, 128, 128, 4, false, false, cutlass::half_t, Flash_kernel_traits<32, 128, 128, 4, cutlass::half_t>>, Is_dropout=false, Is_causal=true, Is_even_N=false, Is_even_K=false, Return_softmax=false]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(34): here
            instantiation of "void run_flash_fwd<Kernel_traits,Is_dropout,Is_causal>(Flash_fwd_params &, cudaStream_t) [with Kernel_traits=Flash_fwd_kernel_traits<32, 128, 128, 4, false, false, cutlass::half_t, Flash_kernel_traits<32, 128, 128, 4, cutlass::half_t>>, Is_dropout=false, Is_causal=true]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(58): here
            instantiation of "void run_mha_fwd_hdim32<T>(Flash_fwd_params &, cudaStream_t) [with T=cutlass::half_t]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_hdim32_fp16_sm80.cu(22): here

/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_kernel.h(275): warning: variable "tQrQ" was declared but never referenced
          detected during:
            instantiation of "void flash::compute_attn_1rowblock<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &, int, int, int) [with Kernel_traits=Flash_fwd_kernel_traits<32, 128, 128, 4, false, false, cutlass::half_t, Flash_kernel_traits<32, 128, 128, 4, cutlass::half_t>>, Is_dropout=false, Is_causal=true, Is_even_N=false, Is_even_K=false, Return_softmax=false, Params=Flash_fwd_params]" 
(588): here
            instantiation of "void flash::compute_attn<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &) [with Kernel_traits=Flash_fwd_kernel_traits<32, 128, 128, 4, false, false, cutlass::half_t, Flash_kernel_traits<32, 128, 128, 4, cutlass::half_t>>, Is_dropout=false, Is_causal=true, Is_even_N=false, Is_even_K=false, Return_softmax=false, Params=Flash_fwd_params]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(15): here
            instantiation of "void flash_fwd_kernel<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax>(Flash_fwd_params) [with Kernel_traits=Flash_fwd_kernel_traits<32, 128, 128, 4, false, false, cutlass::half_t, Flash_kernel_traits<32, 128, 128, 4, cutlass::half_t>>, Is_dropout=false, Is_causal=true, Is_even_N=false, Is_even_K=false, Return_softmax=false]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(34): here
            instantiation of "void run_flash_fwd<Kernel_traits,Is_dropout,Is_causal>(Flash_fwd_params &, cudaStream_t) [with Kernel_traits=Flash_fwd_kernel_traits<32, 128, 128, 4, false, false, cutlass::half_t, Flash_kernel_traits<32, 128, 128, 4, cutlass::half_t>>, Is_dropout=false, Is_causal=true]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(58): here
            instantiation of "void run_mha_fwd_hdim32<T>(Flash_fwd_params &, cudaStream_t) [with T=cutlass::half_t]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_hdim32_fp16_sm80.cu(22): here

/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_kernel.h(140): warning: variable "MMA_M" was declared but never referenced
          detected during:
            instantiation of "void flash::compute_attn_1rowblock<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &, int, int, int) [with Kernel_traits=Flash_fwd_kernel_traits<32, 128, 128, 4, false, false, cutlass::half_t, Flash_kernel_traits<32, 128, 128, 4, cutlass::half_t>>, Is_dropout=false, Is_causal=false, Is_even_N=true, Is_even_K=true, Return_softmax=false, Params=Flash_fwd_params]" 
(588): here
            instantiation of "void flash::compute_attn<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &) [with Kernel_traits=Flash_fwd_kernel_traits<32, 128, 128, 4, false, false, cutlass::half_t, Flash_kernel_traits<32, 128, 128, 4, cutlass::half_t>>, Is_dropout=false, Is_causal=false, Is_even_N=true, Is_even_K=true, Return_softmax=false, Params=Flash_fwd_params]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(15): here
            instantiation of "void flash_fwd_kernel<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax>(Flash_fwd_params) [with Kernel_traits=Flash_fwd_kernel_traits<32, 128, 128, 4, false, false, cutlass::half_t, Flash_kernel_traits<32, 128, 128, 4, cutlass::half_t>>, Is_dropout=false, Is_causal=false, Is_even_N=true, Is_even_K=true, Return_softmax=false]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(34): here
            instantiation of "void run_flash_fwd<Kernel_traits,Is_dropout,Is_causal>(Flash_fwd_params &, cudaStream_t) [with Kernel_traits=Flash_fwd_kernel_traits<32, 128, 128, 4, false, false, cutlass::half_t, Flash_kernel_traits<32, 128, 128, 4, cutlass::half_t>>, Is_dropout=false, Is_causal=false]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(58): here
            instantiation of "void run_mha_fwd_hdim32<T>(Flash_fwd_params &, cudaStream_t) [with T=cutlass::half_t]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_hdim32_fp16_sm80.cu(22): here

/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_kernel.h(275): warning: variable "tQrQ" was declared but never referenced
          detected during:
            instantiation of "void flash::compute_attn_1rowblock<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &, int, int, int) [with Kernel_traits=Flash_fwd_kernel_traits<32, 128, 128, 4, false, false, cutlass::half_t, Flash_kernel_traits<32, 128, 128, 4, cutlass::half_t>>, Is_dropout=false, Is_causal=false, Is_even_N=true, Is_even_K=true, Return_softmax=false, Params=Flash_fwd_params]" 
(588): here
            instantiation of "void flash::compute_attn<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &) [with Kernel_traits=Flash_fwd_kernel_traits<32, 128, 128, 4, false, false, cutlass::half_t, Flash_kernel_traits<32, 128, 128, 4, cutlass::half_t>>, Is_dropout=false, Is_causal=false, Is_even_N=true, Is_even_K=true, Return_softmax=false, Params=Flash_fwd_params]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(15): here
            instantiation of "void flash_fwd_kernel<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax>(Flash_fwd_params) [with Kernel_traits=Flash_fwd_kernel_traits<32, 128, 128, 4, false, false, cutlass::half_t, Flash_kernel_traits<32, 128, 128, 4, cutlass::half_t>>, Is_dropout=false, Is_causal=false, Is_even_N=true, Is_even_K=true, Return_softmax=false]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(34): here
            instantiation of "void run_flash_fwd<Kernel_traits,Is_dropout,Is_causal>(Flash_fwd_params &, cudaStream_t) [with Kernel_traits=Flash_fwd_kernel_traits<32, 128, 128, 4, false, false, cutlass::half_t, Flash_kernel_traits<32, 128, 128, 4, cutlass::half_t>>, Is_dropout=false, Is_causal=false]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(58): here
            instantiation of "void run_mha_fwd_hdim32<T>(Flash_fwd_params &, cudaStream_t) [with T=cutlass::half_t]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_hdim32_fp16_sm80.cu(22): here

/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_kernel.h(140): warning: variable "MMA_M" was declared but never referenced
          detected during:
            instantiation of "void flash::compute_attn_1rowblock<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &, int, int, int) [with Kernel_traits=Flash_fwd_kernel_traits<32, 128, 128, 4, false, false, cutlass::half_t, Flash_kernel_traits<32, 128, 128, 4, cutlass::half_t>>, Is_dropout=false, Is_causal=false, Is_even_N=true, Is_even_K=false, Return_softmax=false, Params=Flash_fwd_params]" 
(588): here
            instantiation of "void flash::compute_attn<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &) [with Kernel_traits=Flash_fwd_kernel_traits<32, 128, 128, 4, false, false, cutlass::half_t, Flash_kernel_traits<32, 128, 128, 4, cutlass::half_t>>, Is_dropout=false, Is_causal=false, Is_even_N=true, Is_even_K=false, Return_softmax=false, Params=Flash_fwd_params]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(15): here
            instantiation of "void flash_fwd_kernel<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax>(Flash_fwd_params) [with Kernel_traits=Flash_fwd_kernel_traits<32, 128, 128, 4, false, false, cutlass::half_t, Flash_kernel_traits<32, 128, 128, 4, cutlass::half_t>>, Is_dropout=false, Is_causal=false, Is_even_N=true, Is_even_K=false, Return_softmax=false]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(34): here
            instantiation of "void run_flash_fwd<Kernel_traits,Is_dropout,Is_causal>(Flash_fwd_params &, cudaStream_t) [with Kernel_traits=Flash_fwd_kernel_traits<32, 128, 128, 4, false, false, cutlass::half_t, Flash_kernel_traits<32, 128, 128, 4, cutlass::half_t>>, Is_dropout=false, Is_causal=false]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(58): here
            instantiation of "void run_mha_fwd_hdim32<T>(Flash_fwd_params &, cudaStream_t) [with T=cutlass::half_t]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_hdim32_fp16_sm80.cu(22): here

/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_kernel.h(275): warning: variable "tQrQ" was declared but never referenced
          detected during:
            instantiation of "void flash::compute_attn_1rowblock<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &, int, int, int) [with Kernel_traits=Flash_fwd_kernel_traits<32, 128, 128, 4, false, false, cutlass::half_t, Flash_kernel_traits<32, 128, 128, 4, cutlass::half_t>>, Is_dropout=false, Is_causal=false, Is_even_N=true, Is_even_K=false, Return_softmax=false, Params=Flash_fwd_params]" 
(588): here
            instantiation of "void flash::compute_attn<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &) [with Kernel_traits=Flash_fwd_kernel_traits<32, 128, 128, 4, false, false, cutlass::half_t, Flash_kernel_traits<32, 128, 128, 4, cutlass::half_t>>, Is_dropout=false, Is_causal=false, Is_even_N=true, Is_even_K=false, Return_softmax=false, Params=Flash_fwd_params]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(15): here
            instantiation of "void flash_fwd_kernel<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax>(Flash_fwd_params) [with Kernel_traits=Flash_fwd_kernel_traits<32, 128, 128, 4, false, false, cutlass::half_t, Flash_kernel_traits<32, 128, 128, 4, cutlass::half_t>>, Is_dropout=false, Is_causal=false, Is_even_N=true, Is_even_K=false, Return_softmax=false]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(34): here
            instantiation of "void run_flash_fwd<Kernel_traits,Is_dropout,Is_causal>(Flash_fwd_params &, cudaStream_t) [with Kernel_traits=Flash_fwd_kernel_traits<32, 128, 128, 4, false, false, cutlass::half_t, Flash_kernel_traits<32, 128, 128, 4, cutlass::half_t>>, Is_dropout=false, Is_causal=false]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(58): here
            instantiation of "void run_mha_fwd_hdim32<T>(Flash_fwd_params &, cudaStream_t) [with T=cutlass::half_t]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_hdim32_fp16_sm80.cu(22): here

/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_kernel.h(140): warning: variable "MMA_M" was declared but never referenced
          detected during:
            instantiation of "void flash::compute_attn_1rowblock<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &, int, int, int) [with Kernel_traits=Flash_fwd_kernel_traits<32, 128, 128, 4, false, false, cutlass::half_t, Flash_kernel_traits<32, 128, 128, 4, cutlass::half_t>>, Is_dropout=false, Is_causal=false, Is_even_N=false, Is_even_K=true, Return_softmax=false, Params=Flash_fwd_params]" 
(588): here
            instantiation of "void flash::compute_attn<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &) [with Kernel_traits=Flash_fwd_kernel_traits<32, 128, 128, 4, false, false, cutlass::half_t, Flash_kernel_traits<32, 128, 128, 4, cutlass::half_t>>, Is_dropout=false, Is_causal=false, Is_even_N=false, Is_even_K=true, Return_softmax=false, Params=Flash_fwd_params]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(15): here
            instantiation of "void flash_fwd_kernel<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax>(Flash_fwd_params) [with Kernel_traits=Flash_fwd_kernel_traits<32, 128, 128, 4, false, false, cutlass::half_t, Flash_kernel_traits<32, 128, 128, 4, cutlass::half_t>>, Is_dropout=false, Is_causal=false, Is_even_N=false, Is_even_K=true, Return_softmax=false]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(34): here
            instantiation of "void run_flash_fwd<Kernel_traits,Is_dropout,Is_causal>(Flash_fwd_params &, cudaStream_t) [with Kernel_traits=Flash_fwd_kernel_traits<32, 128, 128, 4, false, false, cutlass::half_t, Flash_kernel_traits<32, 128, 128, 4, cutlass::half_t>>, Is_dropout=false, Is_causal=false]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(58): here
            instantiation of "void run_mha_fwd_hdim32<T>(Flash_fwd_params &, cudaStream_t) [with T=cutlass::half_t]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_hdim32_fp16_sm80.cu(22): here

/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_kernel.h(275): warning: variable "tQrQ" was declared but never referenced
          detected during:
            instantiation of "void flash::compute_attn_1rowblock<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &, int, int, int) [with Kernel_traits=Flash_fwd_kernel_traits<32, 128, 128, 4, false, false, cutlass::half_t, Flash_kernel_traits<32, 128, 128, 4, cutlass::half_t>>, Is_dropout=false, Is_causal=false, Is_even_N=false, Is_even_K=true, Return_softmax=false, Params=Flash_fwd_params]" 
(588): here
            instantiation of "void flash::compute_attn<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &) [with Kernel_traits=Flash_fwd_kernel_traits<32, 128, 128, 4, false, false, cutlass::half_t, Flash_kernel_traits<32, 128, 128, 4, cutlass::half_t>>, Is_dropout=false, Is_causal=false, Is_even_N=false, Is_even_K=true, Return_softmax=false, Params=Flash_fwd_params]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(15): here
            instantiation of "void flash_fwd_kernel<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax>(Flash_fwd_params) [with Kernel_traits=Flash_fwd_kernel_traits<32, 128, 128, 4, false, false, cutlass::half_t, Flash_kernel_traits<32, 128, 128, 4, cutlass::half_t>>, Is_dropout=false, Is_causal=false, Is_even_N=false, Is_even_K=true, Return_softmax=false]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(34): here
            instantiation of "void run_flash_fwd<Kernel_traits,Is_dropout,Is_causal>(Flash_fwd_params &, cudaStream_t) [with Kernel_traits=Flash_fwd_kernel_traits<32, 128, 128, 4, false, false, cutlass::half_t, Flash_kernel_traits<32, 128, 128, 4, cutlass::half_t>>, Is_dropout=false, Is_causal=false]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(58): here
            instantiation of "void run_mha_fwd_hdim32<T>(Flash_fwd_params &, cudaStream_t) [with T=cutlass::half_t]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_hdim32_fp16_sm80.cu(22): here

/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_kernel.h(140): warning: variable "MMA_M" was declared but never referenced
          detected during:
            instantiation of "void flash::compute_attn_1rowblock<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &, int, int, int) [with Kernel_traits=Flash_fwd_kernel_traits<32, 128, 128, 4, false, false, cutlass::half_t, Flash_kernel_traits<32, 128, 128, 4, cutlass::half_t>>, Is_dropout=false, Is_causal=false, Is_even_N=false, Is_even_K=false, Return_softmax=false, Params=Flash_fwd_params]" 
(588): here
            instantiation of "void flash::compute_attn<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &) [with Kernel_traits=Flash_fwd_kernel_traits<32, 128, 128, 4, false, false, cutlass::half_t, Flash_kernel_traits<32, 128, 128, 4, cutlass::half_t>>, Is_dropout=false, Is_causal=false, Is_even_N=false, Is_even_K=false, Return_softmax=false, Params=Flash_fwd_params]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(15): here
            instantiation of "void flash_fwd_kernel<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax>(Flash_fwd_params) [with Kernel_traits=Flash_fwd_kernel_traits<32, 128, 128, 4, false, false, cutlass::half_t, Flash_kernel_traits<32, 128, 128, 4, cutlass::half_t>>, Is_dropout=false, Is_causal=false, Is_even_N=false, Is_even_K=false, Return_softmax=false]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(34): here
            instantiation of "void run_flash_fwd<Kernel_traits,Is_dropout,Is_causal>(Flash_fwd_params &, cudaStream_t) [with Kernel_traits=Flash_fwd_kernel_traits<32, 128, 128, 4, false, false, cutlass::half_t, Flash_kernel_traits<32, 128, 128, 4, cutlass::half_t>>, Is_dropout=false, Is_causal=false]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(58): here
            instantiation of "void run_mha_fwd_hdim32<T>(Flash_fwd_params &, cudaStream_t) [with T=cutlass::half_t]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_hdim32_fp16_sm80.cu(22): here

/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_kernel.h(275): warning: variable "tQrQ" was declared but never referenced
          detected during:
            instantiation of "void flash::compute_attn_1rowblock<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &, int, int, int) [with Kernel_traits=Flash_fwd_kernel_traits<32, 128, 128, 4, false, false, cutlass::half_t, Flash_kernel_traits<32, 128, 128, 4, cutlass::half_t>>, Is_dropout=false, Is_causal=false, Is_even_N=false, Is_even_K=false, Return_softmax=false, Params=Flash_fwd_params]" 
(588): here
            instantiation of "void flash::compute_attn<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &) [with Kernel_traits=Flash_fwd_kernel_traits<32, 128, 128, 4, false, false, cutlass::half_t, Flash_kernel_traits<32, 128, 128, 4, cutlass::half_t>>, Is_dropout=false, Is_causal=false, Is_even_N=false, Is_even_K=false, Return_softmax=false, Params=Flash_fwd_params]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(15): here
            instantiation of "void flash_fwd_kernel<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax>(Flash_fwd_params) [with Kernel_traits=Flash_fwd_kernel_traits<32, 128, 128, 4, false, false, cutlass::half_t, Flash_kernel_traits<32, 128, 128, 4, cutlass::half_t>>, Is_dropout=false, Is_causal=false, Is_even_N=false, Is_even_K=false, Return_softmax=false]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(34): here
            instantiation of "void run_flash_fwd<Kernel_traits,Is_dropout,Is_causal>(Flash_fwd_params &, cudaStream_t) [with Kernel_traits=Flash_fwd_kernel_traits<32, 128, 128, 4, false, false, cutlass::half_t, Flash_kernel_traits<32, 128, 128, 4, cutlass::half_t>>, Is_dropout=false, Is_causal=false]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(58): here
            instantiation of "void run_mha_fwd_hdim32<T>(Flash_fwd_params &, cudaStream_t) [with T=cutlass::half_t]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_hdim32_fp16_sm80.cu(22): here

ptxas info    : 2 bytes gmem
ptxas info    : Compiling entry function '_Z16flash_fwd_kernelI23Flash_fwd_kernel_traitsILi32ELi128ELi128ELi4ELb0ELb0EN7cutlass6half_tE19Flash_kernel_traitsILi32ELi128ELi128ELi4ES2_EELb0ELb0ELb0ELb0ELb0EEv16Flash_fwd_params' for 'sm_80'
ptxas info    : Function properties for _Z16flash_fwd_kernelI23Flash_fwd_kernel_traitsILi32ELi128ELi128ELi4ELb0ELb0EN7cutlass6half_tE19Flash_kernel_traitsILi32ELi128ELi128ELi4ES2_EELb0ELb0ELb0ELb0ELb0EEv16Flash_fwd_params
    0 bytes stack frame, 0 bytes spill stores, 0 bytes spill loads
ptxas info    : Used 254 registers, 584 bytes cmem[0]
ptxas info    : Compiling entry function '_Z16flash_fwd_kernelI23Flash_fwd_kernel_traitsILi32ELi128ELi128ELi4ELb0ELb0EN7cutlass6half_tE19Flash_kernel_traitsILi32ELi128ELi128ELi4ES2_EELb0ELb0ELb0ELb1ELb0EEv16Flash_fwd_params' for 'sm_80'
ptxas info    : Function properties for _Z16flash_fwd_kernelI23Flash_fwd_kernel_traitsILi32ELi128ELi128ELi4ELb0ELb0EN7cutlass6half_tE19Flash_kernel_traitsILi32ELi128ELi128ELi4ES2_EELb0ELb0ELb0ELb1ELb0EEv16Flash_fwd_params
    0 bytes stack frame, 0 bytes spill stores, 0 bytes spill loads
ptxas info    : Used 254 registers, 584 bytes cmem[0]
ptxas info    : Compiling entry function '_Z16flash_fwd_kernelI23Flash_fwd_kernel_traitsILi32ELi128ELi128ELi4ELb0ELb0EN7cutlass6half_tE19Flash_kernel_traitsILi32ELi128ELi128ELi4ES2_EELb0ELb0ELb1ELb0ELb0EEv16Flash_fwd_params' for 'sm_80'
ptxas info    : Function properties for _Z16flash_fwd_kernelI23Flash_fwd_kernel_traitsILi32ELi128ELi128ELi4ELb0ELb0EN7cutlass6half_tE19Flash_kernel_traitsILi32ELi128ELi128ELi4ES2_EELb0ELb0ELb1ELb0ELb0EEv16Flash_fwd_params
    0 bytes stack frame, 0 bytes spill stores, 0 bytes spill loads
ptxas info    : Used 232 registers, 584 bytes cmem[0]
ptxas info    : Compiling entry function '_Z16flash_fwd_kernelI23Flash_fwd_kernel_traitsILi32ELi128ELi128ELi4ELb0ELb0EN7cutlass6half_tE19Flash_kernel_traitsILi32ELi128ELi128ELi4ES2_EELb0ELb0ELb1ELb1ELb0EEv16Flash_fwd_params' for 'sm_80'
ptxas info    : Function properties for _Z16flash_fwd_kernelI23Flash_fwd_kernel_traitsILi32ELi128ELi128ELi4ELb0ELb0EN7cutlass6half_tE19Flash_kernel_traitsILi32ELi128ELi128ELi4ES2_EELb0ELb0ELb1ELb1ELb0EEv16Flash_fwd_params
    0 bytes stack frame, 0 bytes spill stores, 0 bytes spill loads
ptxas info    : Used 236 registers, 584 bytes cmem[0]
ptxas info    : Compiling entry function '_Z16flash_fwd_kernelI23Flash_fwd_kernel_traitsILi32ELi128ELi128ELi4ELb0ELb0EN7cutlass6half_tE19Flash_kernel_traitsILi32ELi128ELi128ELi4ES2_EELb0ELb1ELb0ELb0ELb0EEv16Flash_fwd_params' for 'sm_80'
ptxas info    : Function properties for _Z16flash_fwd_kernelI23Flash_fwd_kernel_traitsILi32ELi128ELi128ELi4ELb0ELb0EN7cutlass6half_tE19Flash_kernel_traitsILi32ELi128ELi128ELi4ES2_EELb0ELb1ELb0ELb0ELb0EEv16Flash_fwd_params
    0 bytes stack frame, 0 bytes spill stores, 0 bytes spill loads
ptxas info    : Used 233 registers, 584 bytes cmem[0]
ptxas info    : Compiling entry function '_Z16flash_fwd_kernelI23Flash_fwd_kernel_traitsILi32ELi128ELi128ELi4ELb0ELb0EN7cutlass6half_tE19Flash_kernel_traitsILi32ELi128ELi128ELi4ES2_EELb0ELb1ELb0ELb1ELb0EEv16Flash_fwd_params' for 'sm_80'
ptxas info    : Function properties for _Z16flash_fwd_kernelI23Flash_fwd_kernel_traitsILi32ELi128ELi128ELi4ELb0ELb0EN7cutlass6half_tE19Flash_kernel_traitsILi32ELi128ELi128ELi4ES2_EELb0ELb1ELb0ELb1ELb0EEv16Flash_fwd_params
    0 bytes stack frame, 0 bytes spill stores, 0 bytes spill loads
ptxas info    : Used 254 registers, 584 bytes cmem[0]
ptxas info    : Compiling entry function '_Z16flash_fwd_kernelI23Flash_fwd_kernel_traitsILi32ELi128ELi128ELi4ELb0ELb0EN7cutlass6half_tE19Flash_kernel_traitsILi32ELi128ELi128ELi4ES2_EELb0ELb1ELb1ELb0ELb0EEv16Flash_fwd_params' for 'sm_80'
ptxas info    : Function properties for _Z16flash_fwd_kernelI23Flash_fwd_kernel_traitsILi32ELi128ELi128ELi4ELb0ELb0EN7cutlass6half_tE19Flash_kernel_traitsILi32ELi128ELi128ELi4ES2_EELb0ELb1ELb1ELb0ELb0EEv16Flash_fwd_params
    0 bytes stack frame, 0 bytes spill stores, 0 bytes spill loads
ptxas info    : Used 254 registers, 584 bytes cmem[0]
ptxas info    : Compiling entry function '_Z16flash_fwd_kernelI23Flash_fwd_kernel_traitsILi32ELi128ELi128ELi4ELb0ELb0EN7cutlass6half_tE19Flash_kernel_traitsILi32ELi128ELi128ELi4ES2_EELb0ELb1ELb1ELb1ELb0EEv16Flash_fwd_params' for 'sm_80'
ptxas info    : Function properties for _Z16flash_fwd_kernelI23Flash_fwd_kernel_traitsILi32ELi128ELi128ELi4ELb0ELb0EN7cutlass6half_tE19Flash_kernel_traitsILi32ELi128ELi128ELi4ES2_EELb0ELb1ELb1ELb1ELb0EEv16Flash_fwd_params
    0 bytes stack frame, 0 bytes spill stores, 0 bytes spill loads
ptxas info    : Used 255 registers, 584 bytes cmem[0]
ptxas info    : Compiling entry function '_Z16flash_fwd_kernelI23Flash_fwd_kernel_traitsILi32ELi128ELi128ELi4ELb0ELb0EN7cutlass6half_tE19Flash_kernel_traitsILi32ELi128ELi128ELi4ES2_EELb1ELb0ELb0ELb0ELb0EEv16Flash_fwd_params' for 'sm_80'
ptxas info    : Function properties for _Z16flash_fwd_kernelI23Flash_fwd_kernel_traitsILi32ELi128ELi128ELi4ELb0ELb0EN7cutlass6half_tE19Flash_kernel_traitsILi32ELi128ELi128ELi4ES2_EELb1ELb0ELb0ELb0ELb0EEv16Flash_fwd_params
    0 bytes stack frame, 0 bytes spill stores, 0 bytes spill loads
ptxas info    : Used 255 registers, 584 bytes cmem[0]
ptxas info    : Compiling entry function '_Z16flash_fwd_kernelI23Flash_fwd_kernel_traitsILi32ELi128ELi128ELi4ELb0ELb0EN7cutlass6half_tE19Flash_kernel_traitsILi32ELi128ELi128ELi4ES2_EELb1ELb0ELb0ELb0ELb1EEv16Flash_fwd_params' for 'sm_80'
ptxas info    : Function properties for _Z16flash_fwd_kernelI23Flash_fwd_kernel_traitsILi32ELi128ELi128ELi4ELb0ELb0EN7cutlass6half_tE19Flash_kernel_traitsILi32ELi128ELi128ELi4ES2_EELb1ELb0ELb0ELb0ELb1EEv16Flash_fwd_params
    648 bytes stack frame, 404 bytes spill stores, 480 bytes spill loads
ptxas info    : Used 255 registers, 584 bytes cmem[0]
ptxas info    : Compiling entry function '_Z16flash_fwd_kernelI23Flash_fwd_kernel_traitsILi32ELi128ELi128ELi4ELb0ELb0EN7cutlass6half_tE19Flash_kernel_traitsILi32ELi128ELi128ELi4ES2_EELb1ELb0ELb0ELb1ELb0EEv16Flash_fwd_params' for 'sm_80'
ptxas info    : Function properties for _Z16flash_fwd_kernelI23Flash_fwd_kernel_traitsILi32ELi128ELi128ELi4ELb0ELb0EN7cutlass6half_tE19Flash_kernel_traitsILi32ELi128ELi128ELi4ES2_EELb1ELb0ELb0ELb1ELb0EEv16Flash_fwd_params
    24 bytes stack frame, 20 bytes spill stores, 20 bytes spill loads
ptxas info    : Used 255 registers, 584 bytes cmem[0]
ptxas info    : Compiling entry function '_Z16flash_fwd_kernelI23Flash_fwd_kernel_traitsILi32ELi128ELi128ELi4ELb0ELb0EN7cutlass6half_tE19Flash_kernel_traitsILi32ELi128ELi128ELi4ES2_EELb1ELb0ELb0ELb1ELb1EEv16Flash_fwd_params' for 'sm_80'
ptxas info    : Function properties for _Z16flash_fwd_kernelI23Flash_fwd_kernel_traitsILi32ELi128ELi128ELi4ELb0ELb0EN7cutlass6half_tE19Flash_kernel_traitsILi32ELi128ELi128ELi4ES2_EELb1ELb0ELb0ELb1ELb1EEv16Flash_fwd_params
    712 bytes stack frame, 528 bytes spill stores, 586 bytes spill loads
ptxas info    : Used 255 registers, 584 bytes cmem[0]
ptxas info    : Compiling entry function '_Z16flash_fwd_kernelI23Flash_fwd_kernel_traitsILi32ELi128ELi128ELi4ELb0ELb0EN7cutlass6half_tE19Flash_kernel_traitsILi32ELi128ELi128ELi4ES2_EELb1ELb0ELb1ELb0ELb0EEv16Flash_fwd_params' for 'sm_80'
ptxas info    : Function properties for _Z16flash_fwd_kernelI23Flash_fwd_kernel_traitsILi32ELi128ELi128ELi4ELb0ELb0EN7cutlass6half_tE19Flash_kernel_traitsILi32ELi128ELi128ELi4ES2_EELb1ELb0ELb1ELb0ELb0EEv16Flash_fwd_params
    48 bytes stack frame, 44 bytes spill stores, 44 bytes spill loads
ptxas info    : Used 255 registers, 584 bytes cmem[0]
ptxas info    : Compiling entry function '_Z16flash_fwd_kernelI23Flash_fwd_kernel_traitsILi32ELi128ELi128ELi4ELb0ELb0EN7cutlass6half_tE19Flash_kernel_traitsILi32ELi128ELi128ELi4ES2_EELb1ELb0ELb1ELb0ELb1EEv16Flash_fwd_params' for 'sm_80'
ptxas info    : Function properties for _Z16flash_fwd_kernelI23Flash_fwd_kernel_traitsILi32ELi128ELi128ELi4ELb0ELb0EN7cutlass6half_tE19Flash_kernel_traitsILi32ELi128ELi128ELi4ES2_EELb1ELb0ELb1ELb0ELb1EEv16Flash_fwd_params
    648 bytes stack frame, 412 bytes spill stores, 476 bytes spill loads
ptxas info    : Used 255 registers, 584 bytes cmem[0]
ptxas info    : Compiling entry function '_Z16flash_fwd_kernelI23Flash_fwd_kernel_traitsILi32ELi128ELi128ELi4ELb0ELb0EN7cutlass6half_tE19Flash_kernel_traitsILi32ELi128ELi128ELi4ES2_EELb1ELb0ELb1ELb1ELb0EEv16Flash_fwd_params' for 'sm_80'
ptxas info    : Function properties for _Z16flash_fwd_kernelI23Flash_fwd_kernel_traitsILi32ELi128ELi128ELi4ELb0ELb0EN7cutlass6half_tE19Flash_kernel_traitsILi32ELi128ELi128ELi4ES2_EELb1ELb0ELb1ELb1ELb0EEv16Flash_fwd_params
    0 bytes stack frame, 0 bytes spill stores, 0 bytes spill loads
ptxas info    : Used 255 registers, 584 bytes cmem[0]
ptxas info    : Compiling entry function '_Z16flash_fwd_kernelI23Flash_fwd_kernel_traitsILi32ELi128ELi128ELi4ELb0ELb0EN7cutlass6half_tE19Flash_kernel_traitsILi32ELi128ELi128ELi4ES2_EELb1ELb0ELb1ELb1ELb1EEv16Flash_fwd_params' for 'sm_80'
ptxas info    : Function properties for _Z16flash_fwd_kernelI23Flash_fwd_kernel_traitsILi32ELi128ELi128ELi4ELb0ELb0EN7cutlass6half_tE19Flash_kernel_traitsILi32ELi128ELi128ELi4ES2_EELb1ELb0ELb1ELb1ELb1EEv16Flash_fwd_params
    712 bytes stack frame, 488 bytes spill stores, 568 bytes spill loads
ptxas info    : Used 255 registers, 584 bytes cmem[0]
ptxas info    : Compiling entry function '_Z16flash_fwd_kernelI23Flash_fwd_kernel_traitsILi32ELi128ELi128ELi4ELb0ELb0EN7cutlass6half_tE19Flash_kernel_traitsILi32ELi128ELi128ELi4ES2_EELb1ELb1ELb0ELb0ELb0EEv16Flash_fwd_params' for 'sm_80'
ptxas info    : Function properties for _Z16flash_fwd_kernelI23Flash_fwd_kernel_traitsILi32ELi128ELi128ELi4ELb0ELb0EN7cutlass6half_tE19Flash_kernel_traitsILi32ELi128ELi128ELi4ES2_EELb1ELb1ELb0ELb0ELb0EEv16Flash_fwd_params
    0 bytes stack frame, 0 bytes spill stores, 0 bytes spill loads
ptxas info    : Used 252 registers, 584 bytes cmem[0]
ptxas info    : Compiling entry function '_Z16flash_fwd_kernelI23Flash_fwd_kernel_traitsILi32ELi128ELi128ELi4ELb0ELb0EN7cutlass6half_tE19Flash_kernel_traitsILi32ELi128ELi128ELi4ES2_EELb1ELb1ELb0ELb0ELb1EEv16Flash_fwd_params' for 'sm_80'
ptxas info    : Function properties for _Z16flash_fwd_kernelI23Flash_fwd_kernel_traitsILi32ELi128ELi128ELi4ELb0ELb0EN7cutlass6half_tE19Flash_kernel_traitsILi32ELi128ELi128ELi4ES2_EELb1ELb1ELb0ELb0ELb1EEv16Flash_fwd_params
    632 bytes stack frame, 396 bytes spill stores, 474 bytes spill loads
ptxas info    : Used 255 registers, 584 bytes cmem[0]
ptxas info    : Compiling entry function '_Z16flash_fwd_kernelI23Flash_fwd_kernel_traitsILi32ELi128ELi128ELi4ELb0ELb0EN7cutlass6half_tE19Flash_kernel_traitsILi32ELi128ELi128ELi4ES2_EELb1ELb1ELb0ELb1ELb0EEv16Flash_fwd_params' for 'sm_80'
ptxas info    : Function properties for _Z16flash_fwd_kernelI23Flash_fwd_kernel_traitsILi32ELi128ELi128ELi4ELb0ELb0EN7cutlass6half_tE19Flash_kernel_traitsILi32ELi128ELi128ELi4ES2_EELb1ELb1ELb0ELb1ELb0EEv16Flash_fwd_params
    24 bytes stack frame, 20 bytes spill stores, 20 bytes spill loads
ptxas info    : Used 255 registers, 584 bytes cmem[0]
ptxas info    : Compiling entry function '_Z16flash_fwd_kernelI23Flash_fwd_kernel_traitsILi32ELi128ELi128ELi4ELb0ELb0EN7cutlass6half_tE19Flash_kernel_traitsILi32ELi128ELi128ELi4ES2_EELb1ELb1ELb0ELb1ELb1EEv16Flash_fwd_params' for 'sm_80'
ptxas info    : Function properties for _Z16flash_fwd_kernelI23Flash_fwd_kernel_traitsILi32ELi128ELi128ELi4ELb0ELb0EN7cutlass6half_tE19Flash_kernel_traitsILi32ELi128ELi128ELi4ES2_EELb1ELb1ELb0ELb1ELb1EEv16Flash_fwd_params
    712 bytes stack frame, 568 bytes spill stores, 610 bytes spill loads
ptxas info    : Used 255 registers, 584 bytes cmem[0]
ptxas info    : Compiling entry function '_Z16flash_fwd_kernelI23Flash_fwd_kernel_traitsILi32ELi128ELi128ELi4ELb0ELb0EN7cutlass6half_tE19Flash_kernel_traitsILi32ELi128ELi128ELi4ES2_EELb1ELb1ELb1ELb0ELb0EEv16Flash_fwd_params' for 'sm_80'
ptxas info    : Function properties for _Z16flash_fwd_kernelI23Flash_fwd_kernel_traitsILi32ELi128ELi128ELi4ELb0ELb0EN7cutlass6half_tE19Flash_kernel_traitsILi32ELi128ELi128ELi4ES2_EELb1ELb1ELb1ELb0ELb0EEv16Flash_fwd_params
    48 bytes stack frame, 44 bytes spill stores, 44 bytes spill loads
ptxas info    : Used 255 registers, 584 bytes cmem[0]
ptxas info    : Compiling entry function '_Z16flash_fwd_kernelI23Flash_fwd_kernel_traitsILi32ELi128ELi128ELi4ELb0ELb0EN7cutlass6half_tE19Flash_kernel_traitsILi32ELi128ELi128ELi4ES2_EELb1ELb1ELb1ELb0ELb1EEv16Flash_fwd_params' for 'sm_80'
ptxas info    : Function properties for _Z16flash_fwd_kernelI23Flash_fwd_kernel_traitsILi32ELi128ELi128ELi4ELb0ELb0EN7cutlass6half_tE19Flash_kernel_traitsILi32ELi128ELi128ELi4ES2_EELb1ELb1ELb1ELb0ELb1EEv16Flash_fwd_params
    632 bytes stack frame, 428 bytes spill stores, 492 bytes spill loads
ptxas info    : Used 255 registers, 584 bytes cmem[0]
ptxas info    : Compiling entry function '_Z16flash_fwd_kernelI23Flash_fwd_kernel_traitsILi32ELi128ELi128ELi4ELb0ELb0EN7cutlass6half_tE19Flash_kernel_traitsILi32ELi128ELi128ELi4ES2_EELb1ELb1ELb1ELb1ELb0EEv16Flash_fwd_params' for 'sm_80'
ptxas info    : Function properties for _Z16flash_fwd_kernelI23Flash_fwd_kernel_traitsILi32ELi128ELi128ELi4ELb0ELb0EN7cutlass6half_tE19Flash_kernel_traitsILi32ELi128ELi128ELi4ES2_EELb1ELb1ELb1ELb1ELb0EEv16Flash_fwd_params
    8 bytes stack frame, 4 bytes spill stores, 4 bytes spill loads
ptxas info    : Used 255 registers, 584 bytes cmem[0]
ptxas info    : Compiling entry function '_Z16flash_fwd_kernelI23Flash_fwd_kernel_traitsILi32ELi128ELi128ELi4ELb0ELb0EN7cutlass6half_tE19Flash_kernel_traitsILi32ELi128ELi128ELi4ES2_EELb1ELb1ELb1ELb1ELb1EEv16Flash_fwd_params' for 'sm_80'
ptxas info    : Function properties for _Z16flash_fwd_kernelI23Flash_fwd_kernel_traitsILi32ELi128ELi128ELi4ELb0ELb0EN7cutlass6half_tE19Flash_kernel_traitsILi32ELi128ELi128ELi4ES2_EELb1ELb1ELb1ELb1ELb1EEv16Flash_fwd_params
    720 bytes stack frame, 564 bytes spill stores, 630 bytes spill loads
ptxas info    : Used 255 registers, 584 bytes cmem[0]
/anaconda/envs/flashattention/lib/python3.9/site-packages/torch/include/c10/core/SymInt.h(84): warning: integer conversion resulted in a change of sign

[5/13] /usr/local/cuda-11.4/bin/nvcc  -I/home/v-feiychen/flash-attention/csrc/flash_attn -I/home/v-feiychen/flash-attention/csrc/flash_attn/src -I/home/v-feiychen/flash-attention/csrc/cutlass/include -I/anaconda/envs/flashattention/lib/python3.9/site-packages/torch/include -I/anaconda/envs/flashattention/lib/python3.9/site-packages/torch/include/torch/csrc/api/include -I/anaconda/envs/flashattention/lib/python3.9/site-packages/torch/include/TH -I/anaconda/envs/flashattention/lib/python3.9/site-packages/torch/include/THC -I/usr/local/cuda-11.4/include -I/anaconda/envs/flashattention/include/python3.9 -c -c /home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_hdim64_fp16_sm80.cu -o /home/v-feiychen/flash-attention/build/temp.linux-x86_64-cpython-39/csrc/flash_attn/src/flash_fwd_hdim64_fp16_sm80.o -D__CUDA_NO_HALF_OPERATORS__ -D__CUDA_NO_HALF_CONVERSIONS__ -D__CUDA_NO_BFLOAT16_CONVERSIONS__ -D__CUDA_NO_HALF2_OPERATORS__ --expt-relaxed-constexpr --compiler-options ''"'"'-fPIC'"'"'' -O3 -std=c++17 -U__CUDA_NO_HALF_OPERATORS__ -U__CUDA_NO_HALF_CONVERSIONS__ -U__CUDA_NO_HALF2_OPERATORS__ -U__CUDA_NO_BFLOAT16_CONVERSIONS__ --expt-relaxed-constexpr --expt-extended-lambda --use_fast_math --ptxas-options=-v -lineinfo -gencode arch=compute_80,code=sm_80 --threads 4 -DTORCH_API_INCLUDE_EXTENSION_H '-DPYBIND11_COMPILER_TYPE="_gcc"' '-DPYBIND11_STDLIB="_libstdcpp"' '-DPYBIND11_BUILD_ABI="_cxxabi1011"' -DTORCH_EXTENSION_NAME=flash_attn_2_cuda -D_GLIBCXX_USE_CXX11_ABI=0
/anaconda/envs/flashattention/lib/python3.9/site-packages/torch/include/c10/core/SymInt.h(84): warning: integer conversion resulted in a change of sign

/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_kernel.h(140): warning: variable "MMA_M" was declared but never referenced
          detected during:
            instantiation of "void flash::compute_attn_1rowblock<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &, int, int, int) [with Kernel_traits=Flash_fwd_kernel_traits<64, 128, 64, 4, false, false, cutlass::half_t, Flash_kernel_traits<64, 128, 64, 4, cutlass::half_t>>, Is_dropout=true, Is_causal=true, Is_even_N=true, Is_even_K=true, Return_softmax=true, Params=Flash_fwd_params]" 
(588): here
            instantiation of "void flash::compute_attn<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &) [with Kernel_traits=Flash_fwd_kernel_traits<64, 128, 64, 4, false, false, cutlass::half_t, Flash_kernel_traits<64, 128, 64, 4, cutlass::half_t>>, Is_dropout=true, Is_causal=true, Is_even_N=true, Is_even_K=true, Return_softmax=true, Params=Flash_fwd_params]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(15): here
            instantiation of "void flash_fwd_kernel<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax>(Flash_fwd_params) [with Kernel_traits=Flash_fwd_kernel_traits<64, 128, 64, 4, false, false, cutlass::half_t, Flash_kernel_traits<64, 128, 64, 4, cutlass::half_t>>, Is_dropout=true, Is_causal=true, Is_even_N=true, Is_even_K=true, Return_softmax=true]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(34): here
            instantiation of "void run_flash_fwd<Kernel_traits,Is_dropout,Is_causal>(Flash_fwd_params &, cudaStream_t) [with Kernel_traits=Flash_fwd_kernel_traits<64, 128, 64, 4, false, false, cutlass::half_t, Flash_kernel_traits<64, 128, 64, 4, cutlass::half_t>>, Is_dropout=true, Is_causal=true]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(68): here
            instantiation of "void run_mha_fwd_hdim64<T>(Flash_fwd_params &, cudaStream_t) [with T=cutlass::half_t]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_hdim64_fp16_sm80.cu(25): here

/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_kernel.h(275): warning: variable "tQrQ" was declared but never referenced
          detected during:
            instantiation of "void flash::compute_attn_1rowblock<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &, int, int, int) [with Kernel_traits=Flash_fwd_kernel_traits<64, 128, 64, 4, false, false, cutlass::half_t, Flash_kernel_traits<64, 128, 64, 4, cutlass::half_t>>, Is_dropout=true, Is_causal=true, Is_even_N=true, Is_even_K=true, Return_softmax=true, Params=Flash_fwd_params]" 
(588): here
            instantiation of "void flash::compute_attn<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &) [with Kernel_traits=Flash_fwd_kernel_traits<64, 128, 64, 4, false, false, cutlass::half_t, Flash_kernel_traits<64, 128, 64, 4, cutlass::half_t>>, Is_dropout=true, Is_causal=true, Is_even_N=true, Is_even_K=true, Return_softmax=true, Params=Flash_fwd_params]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(15): here
            instantiation of "void flash_fwd_kernel<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax>(Flash_fwd_params) [with Kernel_traits=Flash_fwd_kernel_traits<64, 128, 64, 4, false, false, cutlass::half_t, Flash_kernel_traits<64, 128, 64, 4, cutlass::half_t>>, Is_dropout=true, Is_causal=true, Is_even_N=true, Is_even_K=true, Return_softmax=true]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(34): here
            instantiation of "void run_flash_fwd<Kernel_traits,Is_dropout,Is_causal>(Flash_fwd_params &, cudaStream_t) [with Kernel_traits=Flash_fwd_kernel_traits<64, 128, 64, 4, false, false, cutlass::half_t, Flash_kernel_traits<64, 128, 64, 4, cutlass::half_t>>, Is_dropout=true, Is_causal=true]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(68): here
            instantiation of "void run_mha_fwd_hdim64<T>(Flash_fwd_params &, cudaStream_t) [with T=cutlass::half_t]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_hdim64_fp16_sm80.cu(25): here

/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_kernel.h(140): warning: variable "MMA_M" was declared but never referenced
          detected during:
            instantiation of "void flash::compute_attn_1rowblock<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &, int, int, int) [with Kernel_traits=Flash_fwd_kernel_traits<64, 128, 64, 4, false, false, cutlass::half_t, Flash_kernel_traits<64, 128, 64, 4, cutlass::half_t>>, Is_dropout=true, Is_causal=true, Is_even_N=true, Is_even_K=true, Return_softmax=false, Params=Flash_fwd_params]" 
(588): here
            instantiation of "void flash::compute_attn<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &) [with Kernel_traits=Flash_fwd_kernel_traits<64, 128, 64, 4, false, false, cutlass::half_t, Flash_kernel_traits<64, 128, 64, 4, cutlass::half_t>>, Is_dropout=true, Is_causal=true, Is_even_N=true, Is_even_K=true, Return_softmax=false, Params=Flash_fwd_params]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(15): here
            instantiation of "void flash_fwd_kernel<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax>(Flash_fwd_params) [with Kernel_traits=Flash_fwd_kernel_traits<64, 128, 64, 4, false, false, cutlass::half_t, Flash_kernel_traits<64, 128, 64, 4, cutlass::half_t>>, Is_dropout=true, Is_causal=true, Is_even_N=true, Is_even_K=true, Return_softmax=false]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(34): here
            instantiation of "void run_flash_fwd<Kernel_traits,Is_dropout,Is_causal>(Flash_fwd_params &, cudaStream_t) [with Kernel_traits=Flash_fwd_kernel_traits<64, 128, 64, 4, false, false, cutlass::half_t, Flash_kernel_traits<64, 128, 64, 4, cutlass::half_t>>, Is_dropout=true, Is_causal=true]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(68): here
            instantiation of "void run_mha_fwd_hdim64<T>(Flash_fwd_params &, cudaStream_t) [with T=cutlass::half_t]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_hdim64_fp16_sm80.cu(25): here

/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_kernel.h(275): warning: variable "tQrQ" was declared but never referenced
          detected during:
            instantiation of "void flash::compute_attn_1rowblock<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &, int, int, int) [with Kernel_traits=Flash_fwd_kernel_traits<64, 128, 64, 4, false, false, cutlass::half_t, Flash_kernel_traits<64, 128, 64, 4, cutlass::half_t>>, Is_dropout=true, Is_causal=true, Is_even_N=true, Is_even_K=true, Return_softmax=false, Params=Flash_fwd_params]" 
(588): here
            instantiation of "void flash::compute_attn<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &) [with Kernel_traits=Flash_fwd_kernel_traits<64, 128, 64, 4, false, false, cutlass::half_t, Flash_kernel_traits<64, 128, 64, 4, cutlass::half_t>>, Is_dropout=true, Is_causal=true, Is_even_N=true, Is_even_K=true, Return_softmax=false, Params=Flash_fwd_params]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(15): here
            instantiation of "void flash_fwd_kernel<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax>(Flash_fwd_params) [with Kernel_traits=Flash_fwd_kernel_traits<64, 128, 64, 4, false, false, cutlass::half_t, Flash_kernel_traits<64, 128, 64, 4, cutlass::half_t>>, Is_dropout=true, Is_causal=true, Is_even_N=true, Is_even_K=true, Return_softmax=false]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(34): here
            instantiation of "void run_flash_fwd<Kernel_traits,Is_dropout,Is_causal>(Flash_fwd_params &, cudaStream_t) [with Kernel_traits=Flash_fwd_kernel_traits<64, 128, 64, 4, false, false, cutlass::half_t, Flash_kernel_traits<64, 128, 64, 4, cutlass::half_t>>, Is_dropout=true, Is_causal=true]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(68): here
            instantiation of "void run_mha_fwd_hdim64<T>(Flash_fwd_params &, cudaStream_t) [with T=cutlass::half_t]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_hdim64_fp16_sm80.cu(25): here

/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_kernel.h(140): warning: variable "MMA_M" was declared but never referenced
          detected during:
            instantiation of "void flash::compute_attn_1rowblock<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &, int, int, int) [with Kernel_traits=Flash_fwd_kernel_traits<64, 128, 64, 4, false, false, cutlass::half_t, Flash_kernel_traits<64, 128, 64, 4, cutlass::half_t>>, Is_dropout=true, Is_causal=true, Is_even_N=true, Is_even_K=false, Return_softmax=true, Params=Flash_fwd_params]" 
(588): here
            instantiation of "void flash::compute_attn<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &) [with Kernel_traits=Flash_fwd_kernel_traits<64, 128, 64, 4, false, false, cutlass::half_t, Flash_kernel_traits<64, 128, 64, 4, cutlass::half_t>>, Is_dropout=true, Is_causal=true, Is_even_N=true, Is_even_K=false, Return_softmax=true, Params=Flash_fwd_params]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(15): here
            instantiation of "void flash_fwd_kernel<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax>(Flash_fwd_params) [with Kernel_traits=Flash_fwd_kernel_traits<64, 128, 64, 4, false, false, cutlass::half_t, Flash_kernel_traits<64, 128, 64, 4, cutlass::half_t>>, Is_dropout=true, Is_causal=true, Is_even_N=true, Is_even_K=false, Return_softmax=true]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(34): here
            instantiation of "void run_flash_fwd<Kernel_traits,Is_dropout,Is_causal>(Flash_fwd_params &, cudaStream_t) [with Kernel_traits=Flash_fwd_kernel_traits<64, 128, 64, 4, false, false, cutlass::half_t, Flash_kernel_traits<64, 128, 64, 4, cutlass::half_t>>, Is_dropout=true, Is_causal=true]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(68): here
            instantiation of "void run_mha_fwd_hdim64<T>(Flash_fwd_params &, cudaStream_t) [with T=cutlass::half_t]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_hdim64_fp16_sm80.cu(25): here

/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_kernel.h(275): warning: variable "tQrQ" was declared but never referenced
          detected during:
            instantiation of "void flash::compute_attn_1rowblock<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &, int, int, int) [with Kernel_traits=Flash_fwd_kernel_traits<64, 128, 64, 4, false, false, cutlass::half_t, Flash_kernel_traits<64, 128, 64, 4, cutlass::half_t>>, Is_dropout=true, Is_causal=true, Is_even_N=true, Is_even_K=false, Return_softmax=true, Params=Flash_fwd_params]" 
(588): here
            instantiation of "void flash::compute_attn<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &) [with Kernel_traits=Flash_fwd_kernel_traits<64, 128, 64, 4, false, false, cutlass::half_t, Flash_kernel_traits<64, 128, 64, 4, cutlass::half_t>>, Is_dropout=true, Is_causal=true, Is_even_N=true, Is_even_K=false, Return_softmax=true, Params=Flash_fwd_params]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(15): here
            instantiation of "void flash_fwd_kernel<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax>(Flash_fwd_params) [with Kernel_traits=Flash_fwd_kernel_traits<64, 128, 64, 4, false, false, cutlass::half_t, Flash_kernel_traits<64, 128, 64, 4, cutlass::half_t>>, Is_dropout=true, Is_causal=true, Is_even_N=true, Is_even_K=false, Return_softmax=true]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(34): here
            instantiation of "void run_flash_fwd<Kernel_traits,Is_dropout,Is_causal>(Flash_fwd_params &, cudaStream_t) [with Kernel_traits=Flash_fwd_kernel_traits<64, 128, 64, 4, false, false, cutlass::half_t, Flash_kernel_traits<64, 128, 64, 4, cutlass::half_t>>, Is_dropout=true, Is_causal=true]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(68): here
            instantiation of "void run_mha_fwd_hdim64<T>(Flash_fwd_params &, cudaStream_t) [with T=cutlass::half_t]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_hdim64_fp16_sm80.cu(25): here

/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_kernel.h(140): warning: variable "MMA_M" was declared but never referenced
          detected during:
            instantiation of "void flash::compute_attn_1rowblock<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &, int, int, int) [with Kernel_traits=Flash_fwd_kernel_traits<64, 128, 64, 4, false, false, cutlass::half_t, Flash_kernel_traits<64, 128, 64, 4, cutlass::half_t>>, Is_dropout=true, Is_causal=true, Is_even_N=true, Is_even_K=false, Return_softmax=false, Params=Flash_fwd_params]" 
(588): here
            instantiation of "void flash::compute_attn<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &) [with Kernel_traits=Flash_fwd_kernel_traits<64, 128, 64, 4, false, false, cutlass::half_t, Flash_kernel_traits<64, 128, 64, 4, cutlass::half_t>>, Is_dropout=true, Is_causal=true, Is_even_N=true, Is_even_K=false, Return_softmax=false, Params=Flash_fwd_params]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(15): here
            instantiation of "void flash_fwd_kernel<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax>(Flash_fwd_params) [with Kernel_traits=Flash_fwd_kernel_traits<64, 128, 64, 4, false, false, cutlass::half_t, Flash_kernel_traits<64, 128, 64, 4, cutlass::half_t>>, Is_dropout=true, Is_causal=true, Is_even_N=true, Is_even_K=false, Return_softmax=false]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(34): here
            instantiation of "void run_flash_fwd<Kernel_traits,Is_dropout,Is_causal>(Flash_fwd_params &, cudaStream_t) [with Kernel_traits=Flash_fwd_kernel_traits<64, 128, 64, 4, false, false, cutlass::half_t, Flash_kernel_traits<64, 128, 64, 4, cutlass::half_t>>, Is_dropout=true, Is_causal=true]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(68): here
            instantiation of "void run_mha_fwd_hdim64<T>(Flash_fwd_params &, cudaStream_t) [with T=cutlass::half_t]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_hdim64_fp16_sm80.cu(25): here

/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_kernel.h(275): warning: variable "tQrQ" was declared but never referenced
          detected during:
            instantiation of "void flash::compute_attn_1rowblock<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &, int, int, int) [with Kernel_traits=Flash_fwd_kernel_traits<64, 128, 64, 4, false, false, cutlass::half_t, Flash_kernel_traits<64, 128, 64, 4, cutlass::half_t>>, Is_dropout=true, Is_causal=true, Is_even_N=true, Is_even_K=false, Return_softmax=false, Params=Flash_fwd_params]" 
(588): here
            instantiation of "void flash::compute_attn<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &) [with Kernel_traits=Flash_fwd_kernel_traits<64, 128, 64, 4, false, false, cutlass::half_t, Flash_kernel_traits<64, 128, 64, 4, cutlass::half_t>>, Is_dropout=true, Is_causal=true, Is_even_N=true, Is_even_K=false, Return_softmax=false, Params=Flash_fwd_params]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(15): here
            instantiation of "void flash_fwd_kernel<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax>(Flash_fwd_params) [with Kernel_traits=Flash_fwd_kernel_traits<64, 128, 64, 4, false, false, cutlass::half_t, Flash_kernel_traits<64, 128, 64, 4, cutlass::half_t>>, Is_dropout=true, Is_causal=true, Is_even_N=true, Is_even_K=false, Return_softmax=false]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(34): here
            instantiation of "void run_flash_fwd<Kernel_traits,Is_dropout,Is_causal>(Flash_fwd_params &, cudaStream_t) [with Kernel_traits=Flash_fwd_kernel_traits<64, 128, 64, 4, false, false, cutlass::half_t, Flash_kernel_traits<64, 128, 64, 4, cutlass::half_t>>, Is_dropout=true, Is_causal=true]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(68): here
            instantiation of "void run_mha_fwd_hdim64<T>(Flash_fwd_params &, cudaStream_t) [with T=cutlass::half_t]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_hdim64_fp16_sm80.cu(25): here

/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_kernel.h(140): warning: variable "MMA_M" was declared but never referenced
          detected during:
            instantiation of "void flash::compute_attn_1rowblock<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &, int, int, int) [with Kernel_traits=Flash_fwd_kernel_traits<64, 128, 64, 4, false, false, cutlass::half_t, Flash_kernel_traits<64, 128, 64, 4, cutlass::half_t>>, Is_dropout=true, Is_causal=true, Is_even_N=false, Is_even_K=true, Return_softmax=true, Params=Flash_fwd_params]" 
(588): here
            instantiation of "void flash::compute_attn<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &) [with Kernel_traits=Flash_fwd_kernel_traits<64, 128, 64, 4, false, false, cutlass::half_t, Flash_kernel_traits<64, 128, 64, 4, cutlass::half_t>>, Is_dropout=true, Is_causal=true, Is_even_N=false, Is_even_K=true, Return_softmax=true, Params=Flash_fwd_params]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(15): here
            instantiation of "void flash_fwd_kernel<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax>(Flash_fwd_params) [with Kernel_traits=Flash_fwd_kernel_traits<64, 128, 64, 4, false, false, cutlass::half_t, Flash_kernel_traits<64, 128, 64, 4, cutlass::half_t>>, Is_dropout=true, Is_causal=true, Is_even_N=false, Is_even_K=true, Return_softmax=true]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(34): here
            instantiation of "void run_flash_fwd<Kernel_traits,Is_dropout,Is_causal>(Flash_fwd_params &, cudaStream_t) [with Kernel_traits=Flash_fwd_kernel_traits<64, 128, 64, 4, false, false, cutlass::half_t, Flash_kernel_traits<64, 128, 64, 4, cutlass::half_t>>, Is_dropout=true, Is_causal=true]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(68): here
            instantiation of "void run_mha_fwd_hdim64<T>(Flash_fwd_params &, cudaStream_t) [with T=cutlass::half_t]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_hdim64_fp16_sm80.cu(25): here

/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_kernel.h(275): warning: variable "tQrQ" was declared but never referenced
          detected during:
            instantiation of "void flash::compute_attn_1rowblock<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &, int, int, int) [with Kernel_traits=Flash_fwd_kernel_traits<64, 128, 64, 4, false, false, cutlass::half_t, Flash_kernel_traits<64, 128, 64, 4, cutlass::half_t>>, Is_dropout=true, Is_causal=true, Is_even_N=false, Is_even_K=true, Return_softmax=true, Params=Flash_fwd_params]" 
(588): here
            instantiation of "void flash::compute_attn<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &) [with Kernel_traits=Flash_fwd_kernel_traits<64, 128, 64, 4, false, false, cutlass::half_t, Flash_kernel_traits<64, 128, 64, 4, cutlass::half_t>>, Is_dropout=true, Is_causal=true, Is_even_N=false, Is_even_K=true, Return_softmax=true, Params=Flash_fwd_params]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(15): here
            instantiation of "void flash_fwd_kernel<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax>(Flash_fwd_params) [with Kernel_traits=Flash_fwd_kernel_traits<64, 128, 64, 4, false, false, cutlass::half_t, Flash_kernel_traits<64, 128, 64, 4, cutlass::half_t>>, Is_dropout=true, Is_causal=true, Is_even_N=false, Is_even_K=true, Return_softmax=true]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(34): here
            instantiation of "void run_flash_fwd<Kernel_traits,Is_dropout,Is_causal>(Flash_fwd_params &, cudaStream_t) [with Kernel_traits=Flash_fwd_kernel_traits<64, 128, 64, 4, false, false, cutlass::half_t, Flash_kernel_traits<64, 128, 64, 4, cutlass::half_t>>, Is_dropout=true, Is_causal=true]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(68): here
            instantiation of "void run_mha_fwd_hdim64<T>(Flash_fwd_params &, cudaStream_t) [with T=cutlass::half_t]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_hdim64_fp16_sm80.cu(25): here

/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_kernel.h(140): warning: variable "MMA_M" was declared but never referenced
          detected during:
            instantiation of "void flash::compute_attn_1rowblock<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &, int, int, int) [with Kernel_traits=Flash_fwd_kernel_traits<64, 128, 64, 4, false, false, cutlass::half_t, Flash_kernel_traits<64, 128, 64, 4, cutlass::half_t>>, Is_dropout=true, Is_causal=true, Is_even_N=false, Is_even_K=true, Return_softmax=false, Params=Flash_fwd_params]" 
(588): here
            instantiation of "void flash::compute_attn<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &) [with Kernel_traits=Flash_fwd_kernel_traits<64, 128, 64, 4, false, false, cutlass::half_t, Flash_kernel_traits<64, 128, 64, 4, cutlass::half_t>>, Is_dropout=true, Is_causal=true, Is_even_N=false, Is_even_K=true, Return_softmax=false, Params=Flash_fwd_params]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(15): here
            instantiation of "void flash_fwd_kernel<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax>(Flash_fwd_params) [with Kernel_traits=Flash_fwd_kernel_traits<64, 128, 64, 4, false, false, cutlass::half_t, Flash_kernel_traits<64, 128, 64, 4, cutlass::half_t>>, Is_dropout=true, Is_causal=true, Is_even_N=false, Is_even_K=true, Return_softmax=false]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(34): here
            instantiation of "void run_flash_fwd<Kernel_traits,Is_dropout,Is_causal>(Flash_fwd_params &, cudaStream_t) [with Kernel_traits=Flash_fwd_kernel_traits<64, 128, 64, 4, false, false, cutlass::half_t, Flash_kernel_traits<64, 128, 64, 4, cutlass::half_t>>, Is_dropout=true, Is_causal=true]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(68): here
            instantiation of "void run_mha_fwd_hdim64<T>(Flash_fwd_params &, cudaStream_t) [with T=cutlass::half_t]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_hdim64_fp16_sm80.cu(25): here

/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_kernel.h(275): warning: variable "tQrQ" was declared but never referenced
          detected during:
            instantiation of "void flash::compute_attn_1rowblock<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &, int, int, int) [with Kernel_traits=Flash_fwd_kernel_traits<64, 128, 64, 4, false, false, cutlass::half_t, Flash_kernel_traits<64, 128, 64, 4, cutlass::half_t>>, Is_dropout=true, Is_causal=true, Is_even_N=false, Is_even_K=true, Return_softmax=false, Params=Flash_fwd_params]" 
(588): here
            instantiation of "void flash::compute_attn<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &) [with Kernel_traits=Flash_fwd_kernel_traits<64, 128, 64, 4, false, false, cutlass::half_t, Flash_kernel_traits<64, 128, 64, 4, cutlass::half_t>>, Is_dropout=true, Is_causal=true, Is_even_N=false, Is_even_K=true, Return_softmax=false, Params=Flash_fwd_params]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(15): here
            instantiation of "void flash_fwd_kernel<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax>(Flash_fwd_params) [with Kernel_traits=Flash_fwd_kernel_traits<64, 128, 64, 4, false, false, cutlass::half_t, Flash_kernel_traits<64, 128, 64, 4, cutlass::half_t>>, Is_dropout=true, Is_causal=true, Is_even_N=false, Is_even_K=true, Return_softmax=false]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(34): here
            instantiation of "void run_flash_fwd<Kernel_traits,Is_dropout,Is_causal>(Flash_fwd_params &, cudaStream_t) [with Kernel_traits=Flash_fwd_kernel_traits<64, 128, 64, 4, false, false, cutlass::half_t, Flash_kernel_traits<64, 128, 64, 4, cutlass::half_t>>, Is_dropout=true, Is_causal=true]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(68): here
            instantiation of "void run_mha_fwd_hdim64<T>(Flash_fwd_params &, cudaStream_t) [with T=cutlass::half_t]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_hdim64_fp16_sm80.cu(25): here

/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_kernel.h(140): warning: variable "MMA_M" was declared but never referenced
          detected during:
            instantiation of "void flash::compute_attn_1rowblock<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &, int, int, int) [with Kernel_traits=Flash_fwd_kernel_traits<64, 128, 64, 4, false, false, cutlass::half_t, Flash_kernel_traits<64, 128, 64, 4, cutlass::half_t>>, Is_dropout=true, Is_causal=true, Is_even_N=false, Is_even_K=false, Return_softmax=true, Params=Flash_fwd_params]" 
(588): here
            instantiation of "void flash::compute_attn<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &) [with Kernel_traits=Flash_fwd_kernel_traits<64, 128, 64, 4, false, false, cutlass::half_t, Flash_kernel_traits<64, 128, 64, 4, cutlass::half_t>>, Is_dropout=true, Is_causal=true, Is_even_N=false, Is_even_K=false, Return_softmax=true, Params=Flash_fwd_params]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(15): here
            instantiation of "void flash_fwd_kernel<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax>(Flash_fwd_params) [with Kernel_traits=Flash_fwd_kernel_traits<64, 128, 64, 4, false, false, cutlass::half_t, Flash_kernel_traits<64, 128, 64, 4, cutlass::half_t>>, Is_dropout=true, Is_causal=true, Is_even_N=false, Is_even_K=false, Return_softmax=true]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(34): here
            instantiation of "void run_flash_fwd<Kernel_traits,Is_dropout,Is_causal>(Flash_fwd_params &, cudaStream_t) [with Kernel_traits=Flash_fwd_kernel_traits<64, 128, 64, 4, false, false, cutlass::half_t, Flash_kernel_traits<64, 128, 64, 4, cutlass::half_t>>, Is_dropout=true, Is_causal=true]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(68): here
            instantiation of "void run_mha_fwd_hdim64<T>(Flash_fwd_params &, cudaStream_t) [with T=cutlass::half_t]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_hdim64_fp16_sm80.cu(25): here

/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_kernel.h(275): warning: variable "tQrQ" was declared but never referenced
          detected during:
            instantiation of "void flash::compute_attn_1rowblock<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &, int, int, int) [with Kernel_traits=Flash_fwd_kernel_traits<64, 128, 64, 4, false, false, cutlass::half_t, Flash_kernel_traits<64, 128, 64, 4, cutlass::half_t>>, Is_dropout=true, Is_causal=true, Is_even_N=false, Is_even_K=false, Return_softmax=true, Params=Flash_fwd_params]" 
(588): here
            instantiation of "void flash::compute_attn<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &) [with Kernel_traits=Flash_fwd_kernel_traits<64, 128, 64, 4, false, false, cutlass::half_t, Flash_kernel_traits<64, 128, 64, 4, cutlass::half_t>>, Is_dropout=true, Is_causal=true, Is_even_N=false, Is_even_K=false, Return_softmax=true, Params=Flash_fwd_params]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(15): here
            instantiation of "void flash_fwd_kernel<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax>(Flash_fwd_params) [with Kernel_traits=Flash_fwd_kernel_traits<64, 128, 64, 4, false, false, cutlass::half_t, Flash_kernel_traits<64, 128, 64, 4, cutlass::half_t>>, Is_dropout=true, Is_causal=true, Is_even_N=false, Is_even_K=false, Return_softmax=true]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(34): here
            instantiation of "void run_flash_fwd<Kernel_traits,Is_dropout,Is_causal>(Flash_fwd_params &, cudaStream_t) [with Kernel_traits=Flash_fwd_kernel_traits<64, 128, 64, 4, false, false, cutlass::half_t, Flash_kernel_traits<64, 128, 64, 4, cutlass::half_t>>, Is_dropout=true, Is_causal=true]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(68): here
            instantiation of "void run_mha_fwd_hdim64<T>(Flash_fwd_params &, cudaStream_t) [with T=cutlass::half_t]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_hdim64_fp16_sm80.cu(25): here

/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_kernel.h(140): warning: variable "MMA_M" was declared but never referenced
          detected during:
            instantiation of "void flash::compute_attn_1rowblock<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &, int, int, int) [with Kernel_traits=Flash_fwd_kernel_traits<64, 128, 64, 4, false, false, cutlass::half_t, Flash_kernel_traits<64, 128, 64, 4, cutlass::half_t>>, Is_dropout=true, Is_causal=true, Is_even_N=false, Is_even_K=false, Return_softmax=false, Params=Flash_fwd_params]" 
(588): here
            instantiation of "void flash::compute_attn<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &) [with Kernel_traits=Flash_fwd_kernel_traits<64, 128, 64, 4, false, false, cutlass::half_t, Flash_kernel_traits<64, 128, 64, 4, cutlass::half_t>>, Is_dropout=true, Is_causal=true, Is_even_N=false, Is_even_K=false, Return_softmax=false, Params=Flash_fwd_params]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(15): here
            instantiation of "void flash_fwd_kernel<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax>(Flash_fwd_params) [with Kernel_traits=Flash_fwd_kernel_traits<64, 128, 64, 4, false, false, cutlass::half_t, Flash_kernel_traits<64, 128, 64, 4, cutlass::half_t>>, Is_dropout=true, Is_causal=true, Is_even_N=false, Is_even_K=false, Return_softmax=false]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(34): here
            instantiation of "void run_flash_fwd<Kernel_traits,Is_dropout,Is_causal>(Flash_fwd_params &, cudaStream_t) [with Kernel_traits=Flash_fwd_kernel_traits<64, 128, 64, 4, false, false, cutlass::half_t, Flash_kernel_traits<64, 128, 64, 4, cutlass::half_t>>, Is_dropout=true, Is_causal=true]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(68): here
            instantiation of "void run_mha_fwd_hdim64<T>(Flash_fwd_params &, cudaStream_t) [with T=cutlass::half_t]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_hdim64_fp16_sm80.cu(25): here

/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_kernel.h(275): warning: variable "tQrQ" was declared but never referenced
          detected during:
            instantiation of "void flash::compute_attn_1rowblock<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &, int, int, int) [with Kernel_traits=Flash_fwd_kernel_traits<64, 128, 64, 4, false, false, cutlass::half_t, Flash_kernel_traits<64, 128, 64, 4, cutlass::half_t>>, Is_dropout=true, Is_causal=true, Is_even_N=false, Is_even_K=false, Return_softmax=false, Params=Flash_fwd_params]" 
(588): here
            instantiation of "void flash::compute_attn<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &) [with Kernel_traits=Flash_fwd_kernel_traits<64, 128, 64, 4, false, false, cutlass::half_t, Flash_kernel_traits<64, 128, 64, 4, cutlass::half_t>>, Is_dropout=true, Is_causal=true, Is_even_N=false, Is_even_K=false, Return_softmax=false, Params=Flash_fwd_params]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(15): here
            instantiation of "void flash_fwd_kernel<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax>(Flash_fwd_params) [with Kernel_traits=Flash_fwd_kernel_traits<64, 128, 64, 4, false, false, cutlass::half_t, Flash_kernel_traits<64, 128, 64, 4, cutlass::half_t>>, Is_dropout=true, Is_causal=true, Is_even_N=false, Is_even_K=false, Return_softmax=false]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(34): here
            instantiation of "void run_flash_fwd<Kernel_traits,Is_dropout,Is_causal>(Flash_fwd_params &, cudaStream_t) [with Kernel_traits=Flash_fwd_kernel_traits<64, 128, 64, 4, false, false, cutlass::half_t, Flash_kernel_traits<64, 128, 64, 4, cutlass::half_t>>, Is_dropout=true, Is_causal=true]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(68): here
            instantiation of "void run_mha_fwd_hdim64<T>(Flash_fwd_params &, cudaStream_t) [with T=cutlass::half_t]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_hdim64_fp16_sm80.cu(25): here

/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_kernel.h(140): warning: variable "MMA_M" was declared but never referenced
          detected during:
            instantiation of "void flash::compute_attn_1rowblock<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &, int, int, int) [with Kernel_traits=Flash_fwd_kernel_traits<64, 128, 64, 4, false, false, cutlass::half_t, Flash_kernel_traits<64, 128, 64, 4, cutlass::half_t>>, Is_dropout=true, Is_causal=false, Is_even_N=true, Is_even_K=true, Return_softmax=true, Params=Flash_fwd_params]" 
(588): here
            instantiation of "void flash::compute_attn<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &) [with Kernel_traits=Flash_fwd_kernel_traits<64, 128, 64, 4, false, false, cutlass::half_t, Flash_kernel_traits<64, 128, 64, 4, cutlass::half_t>>, Is_dropout=true, Is_causal=false, Is_even_N=true, Is_even_K=true, Return_softmax=true, Params=Flash_fwd_params]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(15): here
            instantiation of "void flash_fwd_kernel<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax>(Flash_fwd_params) [with Kernel_traits=Flash_fwd_kernel_traits<64, 128, 64, 4, false, false, cutlass::half_t, Flash_kernel_traits<64, 128, 64, 4, cutlass::half_t>>, Is_dropout=true, Is_causal=false, Is_even_N=true, Is_even_K=true, Return_softmax=true]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(34): here
            instantiation of "void run_flash_fwd<Kernel_traits,Is_dropout,Is_causal>(Flash_fwd_params &, cudaStream_t) [with Kernel_traits=Flash_fwd_kernel_traits<64, 128, 64, 4, false, false, cutlass::half_t, Flash_kernel_traits<64, 128, 64, 4, cutlass::half_t>>, Is_dropout=true, Is_causal=false]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(68): here
            instantiation of "void run_mha_fwd_hdim64<T>(Flash_fwd_params &, cudaStream_t) [with T=cutlass::half_t]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_hdim64_fp16_sm80.cu(25): here

/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_kernel.h(275): warning: variable "tQrQ" was declared but never referenced
          detected during:
            instantiation of "void flash::compute_attn_1rowblock<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &, int, int, int) [with Kernel_traits=Flash_fwd_kernel_traits<64, 128, 64, 4, false, false, cutlass::half_t, Flash_kernel_traits<64, 128, 64, 4, cutlass::half_t>>, Is_dropout=true, Is_causal=false, Is_even_N=true, Is_even_K=true, Return_softmax=true, Params=Flash_fwd_params]" 
(588): here
            instantiation of "void flash::compute_attn<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &) [with Kernel_traits=Flash_fwd_kernel_traits<64, 128, 64, 4, false, false, cutlass::half_t, Flash_kernel_traits<64, 128, 64, 4, cutlass::half_t>>, Is_dropout=true, Is_causal=false, Is_even_N=true, Is_even_K=true, Return_softmax=true, Params=Flash_fwd_params]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(15): here
            instantiation of "void flash_fwd_kernel<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax>(Flash_fwd_params) [with Kernel_traits=Flash_fwd_kernel_traits<64, 128, 64, 4, false, false, cutlass::half_t, Flash_kernel_traits<64, 128, 64, 4, cutlass::half_t>>, Is_dropout=true, Is_causal=false, Is_even_N=true, Is_even_K=true, Return_softmax=true]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(34): here
            instantiation of "void run_flash_fwd<Kernel_traits,Is_dropout,Is_causal>(Flash_fwd_params &, cudaStream_t) [with Kernel_traits=Flash_fwd_kernel_traits<64, 128, 64, 4, false, false, cutlass::half_t, Flash_kernel_traits<64, 128, 64, 4, cutlass::half_t>>, Is_dropout=true, Is_causal=false]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(68): here
            instantiation of "void run_mha_fwd_hdim64<T>(Flash_fwd_params &, cudaStream_t) [with T=cutlass::half_t]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_hdim64_fp16_sm80.cu(25): here

/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_kernel.h(140): warning: variable "MMA_M" was declared but never referenced
          detected during:
            instantiation of "void flash::compute_attn_1rowblock<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &, int, int, int) [with Kernel_traits=Flash_fwd_kernel_traits<64, 128, 64, 4, false, false, cutlass::half_t, Flash_kernel_traits<64, 128, 64, 4, cutlass::half_t>>, Is_dropout=true, Is_causal=false, Is_even_N=true, Is_even_K=true, Return_softmax=false, Params=Flash_fwd_params]" 
(588): here
            instantiation of "void flash::compute_attn<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &) [with Kernel_traits=Flash_fwd_kernel_traits<64, 128, 64, 4, false, false, cutlass::half_t, Flash_kernel_traits<64, 128, 64, 4, cutlass::half_t>>, Is_dropout=true, Is_causal=false, Is_even_N=true, Is_even_K=true, Return_softmax=false, Params=Flash_fwd_params]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(15): here
            instantiation of "void flash_fwd_kernel<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax>(Flash_fwd_params) [with Kernel_traits=Flash_fwd_kernel_traits<64, 128, 64, 4, false, false, cutlass::half_t, Flash_kernel_traits<64, 128, 64, 4, cutlass::half_t>>, Is_dropout=true, Is_causal=false, Is_even_N=true, Is_even_K=true, Return_softmax=false]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(34): here
            instantiation of "void run_flash_fwd<Kernel_traits,Is_dropout,Is_causal>(Flash_fwd_params &, cudaStream_t) [with Kernel_traits=Flash_fwd_kernel_traits<64, 128, 64, 4, false, false, cutlass::half_t, Flash_kernel_traits<64, 128, 64, 4, cutlass::half_t>>, Is_dropout=true, Is_causal=false]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(68): here
            instantiation of "void run_mha_fwd_hdim64<T>(Flash_fwd_params &, cudaStream_t) [with T=cutlass::half_t]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_hdim64_fp16_sm80.cu(25): here

/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_kernel.h(275): warning: variable "tQrQ" was declared but never referenced
          detected during:
            instantiation of "void flash::compute_attn_1rowblock<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &, int, int, int) [with Kernel_traits=Flash_fwd_kernel_traits<64, 128, 64, 4, false, false, cutlass::half_t, Flash_kernel_traits<64, 128, 64, 4, cutlass::half_t>>, Is_dropout=true, Is_causal=false, Is_even_N=true, Is_even_K=true, Return_softmax=false, Params=Flash_fwd_params]" 
(588): here
            instantiation of "void flash::compute_attn<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &) [with Kernel_traits=Flash_fwd_kernel_traits<64, 128, 64, 4, false, false, cutlass::half_t, Flash_kernel_traits<64, 128, 64, 4, cutlass::half_t>>, Is_dropout=true, Is_causal=false, Is_even_N=true, Is_even_K=true, Return_softmax=false, Params=Flash_fwd_params]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(15): here
            instantiation of "void flash_fwd_kernel<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax>(Flash_fwd_params) [with Kernel_traits=Flash_fwd_kernel_traits<64, 128, 64, 4, false, false, cutlass::half_t, Flash_kernel_traits<64, 128, 64, 4, cutlass::half_t>>, Is_dropout=true, Is_causal=false, Is_even_N=true, Is_even_K=true, Return_softmax=false]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(34): here
            instantiation of "void run_flash_fwd<Kernel_traits,Is_dropout,Is_causal>(Flash_fwd_params &, cudaStream_t) [with Kernel_traits=Flash_fwd_kernel_traits<64, 128, 64, 4, false, false, cutlass::half_t, Flash_kernel_traits<64, 128, 64, 4, cutlass::half_t>>, Is_dropout=true, Is_causal=false]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(68): here
            instantiation of "void run_mha_fwd_hdim64<T>(Flash_fwd_params &, cudaStream_t) [with T=cutlass::half_t]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_hdim64_fp16_sm80.cu(25): here

/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_kernel.h(140): warning: variable "MMA_M" was declared but never referenced
          detected during:
            instantiation of "void flash::compute_attn_1rowblock<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &, int, int, int) [with Kernel_traits=Flash_fwd_kernel_traits<64, 128, 64, 4, false, false, cutlass::half_t, Flash_kernel_traits<64, 128, 64, 4, cutlass::half_t>>, Is_dropout=true, Is_causal=false, Is_even_N=true, Is_even_K=false, Return_softmax=true, Params=Flash_fwd_params]" 
(588): here
            instantiation of "void flash::compute_attn<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &) [with Kernel_traits=Flash_fwd_kernel_traits<64, 128, 64, 4, false, false, cutlass::half_t, Flash_kernel_traits<64, 128, 64, 4, cutlass::half_t>>, Is_dropout=true, Is_causal=false, Is_even_N=true, Is_even_K=false, Return_softmax=true, Params=Flash_fwd_params]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(15): here
            instantiation of "void flash_fwd_kernel<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax>(Flash_fwd_params) [with Kernel_traits=Flash_fwd_kernel_traits<64, 128, 64, 4, false, false, cutlass::half_t, Flash_kernel_traits<64, 128, 64, 4, cutlass::half_t>>, Is_dropout=true, Is_causal=false, Is_even_N=true, Is_even_K=false, Return_softmax=true]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(34): here
            instantiation of "void run_flash_fwd<Kernel_traits,Is_dropout,Is_causal>(Flash_fwd_params &, cudaStream_t) [with Kernel_traits=Flash_fwd_kernel_traits<64, 128, 64, 4, false, false, cutlass::half_t, Flash_kernel_traits<64, 128, 64, 4, cutlass::half_t>>, Is_dropout=true, Is_causal=false]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(68): here
            instantiation of "void run_mha_fwd_hdim64<T>(Flash_fwd_params &, cudaStream_t) [with T=cutlass::half_t]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_hdim64_fp16_sm80.cu(25): here

/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_kernel.h(275): warning: variable "tQrQ" was declared but never referenced
          detected during:
            instantiation of "void flash::compute_attn_1rowblock<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &, int, int, int) [with Kernel_traits=Flash_fwd_kernel_traits<64, 128, 64, 4, false, false, cutlass::half_t, Flash_kernel_traits<64, 128, 64, 4, cutlass::half_t>>, Is_dropout=true, Is_causal=false, Is_even_N=true, Is_even_K=false, Return_softmax=true, Params=Flash_fwd_params]" 
(588): here
            instantiation of "void flash::compute_attn<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &) [with Kernel_traits=Flash_fwd_kernel_traits<64, 128, 64, 4, false, false, cutlass::half_t, Flash_kernel_traits<64, 128, 64, 4, cutlass::half_t>>, Is_dropout=true, Is_causal=false, Is_even_N=true, Is_even_K=false, Return_softmax=true, Params=Flash_fwd_params]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(15): here
            instantiation of "void flash_fwd_kernel<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax>(Flash_fwd_params) [with Kernel_traits=Flash_fwd_kernel_traits<64, 128, 64, 4, false, false, cutlass::half_t, Flash_kernel_traits<64, 128, 64, 4, cutlass::half_t>>, Is_dropout=true, Is_causal=false, Is_even_N=true, Is_even_K=false, Return_softmax=true]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(34): here
            instantiation of "void run_flash_fwd<Kernel_traits,Is_dropout,Is_causal>(Flash_fwd_params &, cudaStream_t) [with Kernel_traits=Flash_fwd_kernel_traits<64, 128, 64, 4, false, false, cutlass::half_t, Flash_kernel_traits<64, 128, 64, 4, cutlass::half_t>>, Is_dropout=true, Is_causal=false]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(68): here
            instantiation of "void run_mha_fwd_hdim64<T>(Flash_fwd_params &, cudaStream_t) [with T=cutlass::half_t]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_hdim64_fp16_sm80.cu(25): here

/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_kernel.h(140): warning: variable "MMA_M" was declared but never referenced
          detected during:
            instantiation of "void flash::compute_attn_1rowblock<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &, int, int, int) [with Kernel_traits=Flash_fwd_kernel_traits<64, 128, 64, 4, false, false, cutlass::half_t, Flash_kernel_traits<64, 128, 64, 4, cutlass::half_t>>, Is_dropout=true, Is_causal=false, Is_even_N=true, Is_even_K=false, Return_softmax=false, Params=Flash_fwd_params]" 
(588): here
            instantiation of "void flash::compute_attn<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &) [with Kernel_traits=Flash_fwd_kernel_traits<64, 128, 64, 4, false, false, cutlass::half_t, Flash_kernel_traits<64, 128, 64, 4, cutlass::half_t>>, Is_dropout=true, Is_causal=false, Is_even_N=true, Is_even_K=false, Return_softmax=false, Params=Flash_fwd_params]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(15): here
            instantiation of "void flash_fwd_kernel<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax>(Flash_fwd_params) [with Kernel_traits=Flash_fwd_kernel_traits<64, 128, 64, 4, false, false, cutlass::half_t, Flash_kernel_traits<64, 128, 64, 4, cutlass::half_t>>, Is_dropout=true, Is_causal=false, Is_even_N=true, Is_even_K=false, Return_softmax=false]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(34): here
            instantiation of "void run_flash_fwd<Kernel_traits,Is_dropout,Is_causal>(Flash_fwd_params &, cudaStream_t) [with Kernel_traits=Flash_fwd_kernel_traits<64, 128, 64, 4, false, false, cutlass::half_t, Flash_kernel_traits<64, 128, 64, 4, cutlass::half_t>>, Is_dropout=true, Is_causal=false]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(68): here
            instantiation of "void run_mha_fwd_hdim64<T>(Flash_fwd_params &, cudaStream_t) [with T=cutlass::half_t]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_hdim64_fp16_sm80.cu(25): here

/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_kernel.h(275): warning: variable "tQrQ" was declared but never referenced
          detected during:
            instantiation of "void flash::compute_attn_1rowblock<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &, int, int, int) [with Kernel_traits=Flash_fwd_kernel_traits<64, 128, 64, 4, false, false, cutlass::half_t, Flash_kernel_traits<64, 128, 64, 4, cutlass::half_t>>, Is_dropout=true, Is_causal=false, Is_even_N=true, Is_even_K=false, Return_softmax=false, Params=Flash_fwd_params]" 
(588): here
            instantiation of "void flash::compute_attn<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &) [with Kernel_traits=Flash_fwd_kernel_traits<64, 128, 64, 4, false, false, cutlass::half_t, Flash_kernel_traits<64, 128, 64, 4, cutlass::half_t>>, Is_dropout=true, Is_causal=false, Is_even_N=true, Is_even_K=false, Return_softmax=false, Params=Flash_fwd_params]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(15): here
            instantiation of "void flash_fwd_kernel<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax>(Flash_fwd_params) [with Kernel_traits=Flash_fwd_kernel_traits<64, 128, 64, 4, false, false, cutlass::half_t, Flash_kernel_traits<64, 128, 64, 4, cutlass::half_t>>, Is_dropout=true, Is_causal=false, Is_even_N=true, Is_even_K=false, Return_softmax=false]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(34): here
            instantiation of "void run_flash_fwd<Kernel_traits,Is_dropout,Is_causal>(Flash_fwd_params &, cudaStream_t) [with Kernel_traits=Flash_fwd_kernel_traits<64, 128, 64, 4, false, false, cutlass::half_t, Flash_kernel_traits<64, 128, 64, 4, cutlass::half_t>>, Is_dropout=true, Is_causal=false]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(68): here
            instantiation of "void run_mha_fwd_hdim64<T>(Flash_fwd_params &, cudaStream_t) [with T=cutlass::half_t]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_hdim64_fp16_sm80.cu(25): here

/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_kernel.h(140): warning: variable "MMA_M" was declared but never referenced
          detected during:
            instantiation of "void flash::compute_attn_1rowblock<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &, int, int, int) [with Kernel_traits=Flash_fwd_kernel_traits<64, 128, 64, 4, false, false, cutlass::half_t, Flash_kernel_traits<64, 128, 64, 4, cutlass::half_t>>, Is_dropout=true, Is_causal=false, Is_even_N=false, Is_even_K=true, Return_softmax=true, Params=Flash_fwd_params]" 
(588): here
            instantiation of "void flash::compute_attn<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &) [with Kernel_traits=Flash_fwd_kernel_traits<64, 128, 64, 4, false, false, cutlass::half_t, Flash_kernel_traits<64, 128, 64, 4, cutlass::half_t>>, Is_dropout=true, Is_causal=false, Is_even_N=false, Is_even_K=true, Return_softmax=true, Params=Flash_fwd_params]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(15): here
            instantiation of "void flash_fwd_kernel<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax>(Flash_fwd_params) [with Kernel_traits=Flash_fwd_kernel_traits<64, 128, 64, 4, false, false, cutlass::half_t, Flash_kernel_traits<64, 128, 64, 4, cutlass::half_t>>, Is_dropout=true, Is_causal=false, Is_even_N=false, Is_even_K=true, Return_softmax=true]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(34): here
            instantiation of "void run_flash_fwd<Kernel_traits,Is_dropout,Is_causal>(Flash_fwd_params &, cudaStream_t) [with Kernel_traits=Flash_fwd_kernel_traits<64, 128, 64, 4, false, false, cutlass::half_t, Flash_kernel_traits<64, 128, 64, 4, cutlass::half_t>>, Is_dropout=true, Is_causal=false]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(68): here
            instantiation of "void run_mha_fwd_hdim64<T>(Flash_fwd_params &, cudaStream_t) [with T=cutlass::half_t]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_hdim64_fp16_sm80.cu(25): here

/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_kernel.h(275): warning: variable "tQrQ" was declared but never referenced
          detected during:
            instantiation of "void flash::compute_attn_1rowblock<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &, int, int, int) [with Kernel_traits=Flash_fwd_kernel_traits<64, 128, 64, 4, false, false, cutlass::half_t, Flash_kernel_traits<64, 128, 64, 4, cutlass::half_t>>, Is_dropout=true, Is_causal=false, Is_even_N=false, Is_even_K=true, Return_softmax=true, Params=Flash_fwd_params]" 
(588): here
            instantiation of "void flash::compute_attn<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &) [with Kernel_traits=Flash_fwd_kernel_traits<64, 128, 64, 4, false, false, cutlass::half_t, Flash_kernel_traits<64, 128, 64, 4, cutlass::half_t>>, Is_dropout=true, Is_causal=false, Is_even_N=false, Is_even_K=true, Return_softmax=true, Params=Flash_fwd_params]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(15): here
            instantiation of "void flash_fwd_kernel<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax>(Flash_fwd_params) [with Kernel_traits=Flash_fwd_kernel_traits<64, 128, 64, 4, false, false, cutlass::half_t, Flash_kernel_traits<64, 128, 64, 4, cutlass::half_t>>, Is_dropout=true, Is_causal=false, Is_even_N=false, Is_even_K=true, Return_softmax=true]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(34): here
            instantiation of "void run_flash_fwd<Kernel_traits,Is_dropout,Is_causal>(Flash_fwd_params &, cudaStream_t) [with Kernel_traits=Flash_fwd_kernel_traits<64, 128, 64, 4, false, false, cutlass::half_t, Flash_kernel_traits<64, 128, 64, 4, cutlass::half_t>>, Is_dropout=true, Is_causal=false]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(68): here
            instantiation of "void run_mha_fwd_hdim64<T>(Flash_fwd_params &, cudaStream_t) [with T=cutlass::half_t]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_hdim64_fp16_sm80.cu(25): here

/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_kernel.h(140): warning: variable "MMA_M" was declared but never referenced
          detected during:
            instantiation of "void flash::compute_attn_1rowblock<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &, int, int, int) [with Kernel_traits=Flash_fwd_kernel_traits<64, 128, 64, 4, false, false, cutlass::half_t, Flash_kernel_traits<64, 128, 64, 4, cutlass::half_t>>, Is_dropout=true, Is_causal=false, Is_even_N=false, Is_even_K=true, Return_softmax=false, Params=Flash_fwd_params]" 
(588): here
            instantiation of "void flash::compute_attn<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &) [with Kernel_traits=Flash_fwd_kernel_traits<64, 128, 64, 4, false, false, cutlass::half_t, Flash_kernel_traits<64, 128, 64, 4, cutlass::half_t>>, Is_dropout=true, Is_causal=false, Is_even_N=false, Is_even_K=true, Return_softmax=false, Params=Flash_fwd_params]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(15): here
            instantiation of "void flash_fwd_kernel<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax>(Flash_fwd_params) [with Kernel_traits=Flash_fwd_kernel_traits<64, 128, 64, 4, false, false, cutlass::half_t, Flash_kernel_traits<64, 128, 64, 4, cutlass::half_t>>, Is_dropout=true, Is_causal=false, Is_even_N=false, Is_even_K=true, Return_softmax=false]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(34): here
            instantiation of "void run_flash_fwd<Kernel_traits,Is_dropout,Is_causal>(Flash_fwd_params &, cudaStream_t) [with Kernel_traits=Flash_fwd_kernel_traits<64, 128, 64, 4, false, false, cutlass::half_t, Flash_kernel_traits<64, 128, 64, 4, cutlass::half_t>>, Is_dropout=true, Is_causal=false]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(68): here
            instantiation of "void run_mha_fwd_hdim64<T>(Flash_fwd_params &, cudaStream_t) [with T=cutlass::half_t]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_hdim64_fp16_sm80.cu(25): here

/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_kernel.h(275): warning: variable "tQrQ" was declared but never referenced
          detected during:
            instantiation of "void flash::compute_attn_1rowblock<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &, int, int, int) [with Kernel_traits=Flash_fwd_kernel_traits<64, 128, 64, 4, false, false, cutlass::half_t, Flash_kernel_traits<64, 128, 64, 4, cutlass::half_t>>, Is_dropout=true, Is_causal=false, Is_even_N=false, Is_even_K=true, Return_softmax=false, Params=Flash_fwd_params]" 
(588): here
            instantiation of "void flash::compute_attn<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &) [with Kernel_traits=Flash_fwd_kernel_traits<64, 128, 64, 4, false, false, cutlass::half_t, Flash_kernel_traits<64, 128, 64, 4, cutlass::half_t>>, Is_dropout=true, Is_causal=false, Is_even_N=false, Is_even_K=true, Return_softmax=false, Params=Flash_fwd_params]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(15): here
            instantiation of "void flash_fwd_kernel<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax>(Flash_fwd_params) [with Kernel_traits=Flash_fwd_kernel_traits<64, 128, 64, 4, false, false, cutlass::half_t, Flash_kernel_traits<64, 128, 64, 4, cutlass::half_t>>, Is_dropout=true, Is_causal=false, Is_even_N=false, Is_even_K=true, Return_softmax=false]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(34): here
            instantiation of "void run_flash_fwd<Kernel_traits,Is_dropout,Is_causal>(Flash_fwd_params &, cudaStream_t) [with Kernel_traits=Flash_fwd_kernel_traits<64, 128, 64, 4, false, false, cutlass::half_t, Flash_kernel_traits<64, 128, 64, 4, cutlass::half_t>>, Is_dropout=true, Is_causal=false]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(68): here
            instantiation of "void run_mha_fwd_hdim64<T>(Flash_fwd_params &, cudaStream_t) [with T=cutlass::half_t]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_hdim64_fp16_sm80.cu(25): here

/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_kernel.h(140): warning: variable "MMA_M" was declared but never referenced
          detected during:
            instantiation of "void flash::compute_attn_1rowblock<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &, int, int, int) [with Kernel_traits=Flash_fwd_kernel_traits<64, 128, 64, 4, false, false, cutlass::half_t, Flash_kernel_traits<64, 128, 64, 4, cutlass::half_t>>, Is_dropout=true, Is_causal=false, Is_even_N=false, Is_even_K=false, Return_softmax=true, Params=Flash_fwd_params]" 
(588): here
            instantiation of "void flash::compute_attn<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &) [with Kernel_traits=Flash_fwd_kernel_traits<64, 128, 64, 4, false, false, cutlass::half_t, Flash_kernel_traits<64, 128, 64, 4, cutlass::half_t>>, Is_dropout=true, Is_causal=false, Is_even_N=false, Is_even_K=false, Return_softmax=true, Params=Flash_fwd_params]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(15): here
            instantiation of "void flash_fwd_kernel<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax>(Flash_fwd_params) [with Kernel_traits=Flash_fwd_kernel_traits<64, 128, 64, 4, false, false, cutlass::half_t, Flash_kernel_traits<64, 128, 64, 4, cutlass::half_t>>, Is_dropout=true, Is_causal=false, Is_even_N=false, Is_even_K=false, Return_softmax=true]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(34): here
            instantiation of "void run_flash_fwd<Kernel_traits,Is_dropout,Is_causal>(Flash_fwd_params &, cudaStream_t) [with Kernel_traits=Flash_fwd_kernel_traits<64, 128, 64, 4, false, false, cutlass::half_t, Flash_kernel_traits<64, 128, 64, 4, cutlass::half_t>>, Is_dropout=true, Is_causal=false]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(68): here
            instantiation of "void run_mha_fwd_hdim64<T>(Flash_fwd_params &, cudaStream_t) [with T=cutlass::half_t]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_hdim64_fp16_sm80.cu(25): here

/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_kernel.h(275): warning: variable "tQrQ" was declared but never referenced
          detected during:
            instantiation of "void flash::compute_attn_1rowblock<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &, int, int, int) [with Kernel_traits=Flash_fwd_kernel_traits<64, 128, 64, 4, false, false, cutlass::half_t, Flash_kernel_traits<64, 128, 64, 4, cutlass::half_t>>, Is_dropout=true, Is_causal=false, Is_even_N=false, Is_even_K=false, Return_softmax=true, Params=Flash_fwd_params]" 
(588): here
            instantiation of "void flash::compute_attn<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &) [with Kernel_traits=Flash_fwd_kernel_traits<64, 128, 64, 4, false, false, cutlass::half_t, Flash_kernel_traits<64, 128, 64, 4, cutlass::half_t>>, Is_dropout=true, Is_causal=false, Is_even_N=false, Is_even_K=false, Return_softmax=true, Params=Flash_fwd_params]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(15): here
            instantiation of "void flash_fwd_kernel<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax>(Flash_fwd_params) [with Kernel_traits=Flash_fwd_kernel_traits<64, 128, 64, 4, false, false, cutlass::half_t, Flash_kernel_traits<64, 128, 64, 4, cutlass::half_t>>, Is_dropout=true, Is_causal=false, Is_even_N=false, Is_even_K=false, Return_softmax=true]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(34): here
            instantiation of "void run_flash_fwd<Kernel_traits,Is_dropout,Is_causal>(Flash_fwd_params &, cudaStream_t) [with Kernel_traits=Flash_fwd_kernel_traits<64, 128, 64, 4, false, false, cutlass::half_t, Flash_kernel_traits<64, 128, 64, 4, cutlass::half_t>>, Is_dropout=true, Is_causal=false]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(68): here
            instantiation of "void run_mha_fwd_hdim64<T>(Flash_fwd_params &, cudaStream_t) [with T=cutlass::half_t]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_hdim64_fp16_sm80.cu(25): here

/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_kernel.h(140): warning: variable "MMA_M" was declared but never referenced
          detected during:
            instantiation of "void flash::compute_attn_1rowblock<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &, int, int, int) [with Kernel_traits=Flash_fwd_kernel_traits<64, 128, 64, 4, false, false, cutlass::half_t, Flash_kernel_traits<64, 128, 64, 4, cutlass::half_t>>, Is_dropout=true, Is_causal=false, Is_even_N=false, Is_even_K=false, Return_softmax=false, Params=Flash_fwd_params]" 
(588): here
            instantiation of "void flash::compute_attn<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &) [with Kernel_traits=Flash_fwd_kernel_traits<64, 128, 64, 4, false, false, cutlass::half_t, Flash_kernel_traits<64, 128, 64, 4, cutlass::half_t>>, Is_dropout=true, Is_causal=false, Is_even_N=false, Is_even_K=false, Return_softmax=false, Params=Flash_fwd_params]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(15): here
            instantiation of "void flash_fwd_kernel<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax>(Flash_fwd_params) [with Kernel_traits=Flash_fwd_kernel_traits<64, 128, 64, 4, false, false, cutlass::half_t, Flash_kernel_traits<64, 128, 64, 4, cutlass::half_t>>, Is_dropout=true, Is_causal=false, Is_even_N=false, Is_even_K=false, Return_softmax=false]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(34): here
            instantiation of "void run_flash_fwd<Kernel_traits,Is_dropout,Is_causal>(Flash_fwd_params &, cudaStream_t) [with Kernel_traits=Flash_fwd_kernel_traits<64, 128, 64, 4, false, false, cutlass::half_t, Flash_kernel_traits<64, 128, 64, 4, cutlass::half_t>>, Is_dropout=true, Is_causal=false]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(68): here
            instantiation of "void run_mha_fwd_hdim64<T>(Flash_fwd_params &, cudaStream_t) [with T=cutlass::half_t]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_hdim64_fp16_sm80.cu(25): here

/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_kernel.h(275): warning: variable "tQrQ" was declared but never referenced
          detected during:
            instantiation of "void flash::compute_attn_1rowblock<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &, int, int, int) [with Kernel_traits=Flash_fwd_kernel_traits<64, 128, 64, 4, false, false, cutlass::half_t, Flash_kernel_traits<64, 128, 64, 4, cutlass::half_t>>, Is_dropout=true, Is_causal=false, Is_even_N=false, Is_even_K=false, Return_softmax=false, Params=Flash_fwd_params]" 
(588): here
            instantiation of "void flash::compute_attn<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &) [with Kernel_traits=Flash_fwd_kernel_traits<64, 128, 64, 4, false, false, cutlass::half_t, Flash_kernel_traits<64, 128, 64, 4, cutlass::half_t>>, Is_dropout=true, Is_causal=false, Is_even_N=false, Is_even_K=false, Return_softmax=false, Params=Flash_fwd_params]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(15): here
            instantiation of "void flash_fwd_kernel<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax>(Flash_fwd_params) [with Kernel_traits=Flash_fwd_kernel_traits<64, 128, 64, 4, false, false, cutlass::half_t, Flash_kernel_traits<64, 128, 64, 4, cutlass::half_t>>, Is_dropout=true, Is_causal=false, Is_even_N=false, Is_even_K=false, Return_softmax=false]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(34): here
            instantiation of "void run_flash_fwd<Kernel_traits,Is_dropout,Is_causal>(Flash_fwd_params &, cudaStream_t) [with Kernel_traits=Flash_fwd_kernel_traits<64, 128, 64, 4, false, false, cutlass::half_t, Flash_kernel_traits<64, 128, 64, 4, cutlass::half_t>>, Is_dropout=true, Is_causal=false]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(68): here
            instantiation of "void run_mha_fwd_hdim64<T>(Flash_fwd_params &, cudaStream_t) [with T=cutlass::half_t]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_hdim64_fp16_sm80.cu(25): here

/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_kernel.h(140): warning: variable "MMA_M" was declared but never referenced
          detected during:
            instantiation of "void flash::compute_attn_1rowblock<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &, int, int, int) [with Kernel_traits=Flash_fwd_kernel_traits<64, 128, 128, 4, false, false, cutlass::half_t, Flash_kernel_traits<64, 128, 128, 4, cutlass::half_t>>, Is_dropout=false, Is_causal=true, Is_even_N=true, Is_even_K=true, Return_softmax=false, Params=Flash_fwd_params]" 
(588): here
            instantiation of "void flash::compute_attn<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &) [with Kernel_traits=Flash_fwd_kernel_traits<64, 128, 128, 4, false, false, cutlass::half_t, Flash_kernel_traits<64, 128, 128, 4, cutlass::half_t>>, Is_dropout=false, Is_causal=true, Is_even_N=true, Is_even_K=true, Return_softmax=false, Params=Flash_fwd_params]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(15): here
            instantiation of "void flash_fwd_kernel<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax>(Flash_fwd_params) [with Kernel_traits=Flash_fwd_kernel_traits<64, 128, 128, 4, false, false, cutlass::half_t, Flash_kernel_traits<64, 128, 128, 4, cutlass::half_t>>, Is_dropout=false, Is_causal=true, Is_even_N=true, Is_even_K=true, Return_softmax=false]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(34): here
            instantiation of "void run_flash_fwd<Kernel_traits,Is_dropout,Is_causal>(Flash_fwd_params &, cudaStream_t) [with Kernel_traits=Flash_fwd_kernel_traits<64, 128, 128, 4, false, false, cutlass::half_t, Flash_kernel_traits<64, 128, 128, 4, cutlass::half_t>>, Is_dropout=false, Is_causal=true]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(68): here
            instantiation of "void run_mha_fwd_hdim64<T>(Flash_fwd_params &, cudaStream_t) [with T=cutlass::half_t]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_hdim64_fp16_sm80.cu(25): here

/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_kernel.h(275): warning: variable "tQrQ" was declared but never referenced
          detected during:
            instantiation of "void flash::compute_attn_1rowblock<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &, int, int, int) [with Kernel_traits=Flash_fwd_kernel_traits<64, 128, 128, 4, false, false, cutlass::half_t, Flash_kernel_traits<64, 128, 128, 4, cutlass::half_t>>, Is_dropout=false, Is_causal=true, Is_even_N=true, Is_even_K=true, Return_softmax=false, Params=Flash_fwd_params]" 
(588): here
            instantiation of "void flash::compute_attn<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &) [with Kernel_traits=Flash_fwd_kernel_traits<64, 128, 128, 4, false, false, cutlass::half_t, Flash_kernel_traits<64, 128, 128, 4, cutlass::half_t>>, Is_dropout=false, Is_causal=true, Is_even_N=true, Is_even_K=true, Return_softmax=false, Params=Flash_fwd_params]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(15): here
            instantiation of "void flash_fwd_kernel<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax>(Flash_fwd_params) [with Kernel_traits=Flash_fwd_kernel_traits<64, 128, 128, 4, false, false, cutlass::half_t, Flash_kernel_traits<64, 128, 128, 4, cutlass::half_t>>, Is_dropout=false, Is_causal=true, Is_even_N=true, Is_even_K=true, Return_softmax=false]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(34): here
            instantiation of "void run_flash_fwd<Kernel_traits,Is_dropout,Is_causal>(Flash_fwd_params &, cudaStream_t) [with Kernel_traits=Flash_fwd_kernel_traits<64, 128, 128, 4, false, false, cutlass::half_t, Flash_kernel_traits<64, 128, 128, 4, cutlass::half_t>>, Is_dropout=false, Is_causal=true]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(68): here
            instantiation of "void run_mha_fwd_hdim64<T>(Flash_fwd_params &, cudaStream_t) [with T=cutlass::half_t]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_hdim64_fp16_sm80.cu(25): here

/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_kernel.h(140): warning: variable "MMA_M" was declared but never referenced
          detected during:
            instantiation of "void flash::compute_attn_1rowblock<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &, int, int, int) [with Kernel_traits=Flash_fwd_kernel_traits<64, 128, 128, 4, false, false, cutlass::half_t, Flash_kernel_traits<64, 128, 128, 4, cutlass::half_t>>, Is_dropout=false, Is_causal=true, Is_even_N=true, Is_even_K=false, Return_softmax=false, Params=Flash_fwd_params]" 
(588): here
            instantiation of "void flash::compute_attn<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &) [with Kernel_traits=Flash_fwd_kernel_traits<64, 128, 128, 4, false, false, cutlass::half_t, Flash_kernel_traits<64, 128, 128, 4, cutlass::half_t>>, Is_dropout=false, Is_causal=true, Is_even_N=true, Is_even_K=false, Return_softmax=false, Params=Flash_fwd_params]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(15): here
            instantiation of "void flash_fwd_kernel<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax>(Flash_fwd_params) [with Kernel_traits=Flash_fwd_kernel_traits<64, 128, 128, 4, false, false, cutlass::half_t, Flash_kernel_traits<64, 128, 128, 4, cutlass::half_t>>, Is_dropout=false, Is_causal=true, Is_even_N=true, Is_even_K=false, Return_softmax=false]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(34): here
            instantiation of "void run_flash_fwd<Kernel_traits,Is_dropout,Is_causal>(Flash_fwd_params &, cudaStream_t) [with Kernel_traits=Flash_fwd_kernel_traits<64, 128, 128, 4, false, false, cutlass::half_t, Flash_kernel_traits<64, 128, 128, 4, cutlass::half_t>>, Is_dropout=false, Is_causal=true]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(68): here
            instantiation of "void run_mha_fwd_hdim64<T>(Flash_fwd_params &, cudaStream_t) [with T=cutlass::half_t]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_hdim64_fp16_sm80.cu(25): here

/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_kernel.h(275): warning: variable "tQrQ" was declared but never referenced
          detected during:
            instantiation of "void flash::compute_attn_1rowblock<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &, int, int, int) [with Kernel_traits=Flash_fwd_kernel_traits<64, 128, 128, 4, false, false, cutlass::half_t, Flash_kernel_traits<64, 128, 128, 4, cutlass::half_t>>, Is_dropout=false, Is_causal=true, Is_even_N=true, Is_even_K=false, Return_softmax=false, Params=Flash_fwd_params]" 
(588): here
            instantiation of "void flash::compute_attn<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &) [with Kernel_traits=Flash_fwd_kernel_traits<64, 128, 128, 4, false, false, cutlass::half_t, Flash_kernel_traits<64, 128, 128, 4, cutlass::half_t>>, Is_dropout=false, Is_causal=true, Is_even_N=true, Is_even_K=false, Return_softmax=false, Params=Flash_fwd_params]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(15): here
            instantiation of "void flash_fwd_kernel<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax>(Flash_fwd_params) [with Kernel_traits=Flash_fwd_kernel_traits<64, 128, 128, 4, false, false, cutlass::half_t, Flash_kernel_traits<64, 128, 128, 4, cutlass::half_t>>, Is_dropout=false, Is_causal=true, Is_even_N=true, Is_even_K=false, Return_softmax=false]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(34): here
            instantiation of "void run_flash_fwd<Kernel_traits,Is_dropout,Is_causal>(Flash_fwd_params &, cudaStream_t) [with Kernel_traits=Flash_fwd_kernel_traits<64, 128, 128, 4, false, false, cutlass::half_t, Flash_kernel_traits<64, 128, 128, 4, cutlass::half_t>>, Is_dropout=false, Is_causal=true]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(68): here
            instantiation of "void run_mha_fwd_hdim64<T>(Flash_fwd_params &, cudaStream_t) [with T=cutlass::half_t]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_hdim64_fp16_sm80.cu(25): here

/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_kernel.h(140): warning: variable "MMA_M" was declared but never referenced
          detected during:
            instantiation of "void flash::compute_attn_1rowblock<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &, int, int, int) [with Kernel_traits=Flash_fwd_kernel_traits<64, 128, 128, 4, false, false, cutlass::half_t, Flash_kernel_traits<64, 128, 128, 4, cutlass::half_t>>, Is_dropout=false, Is_causal=true, Is_even_N=false, Is_even_K=true, Return_softmax=false, Params=Flash_fwd_params]" 
(588): here
            instantiation of "void flash::compute_attn<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &) [with Kernel_traits=Flash_fwd_kernel_traits<64, 128, 128, 4, false, false, cutlass::half_t, Flash_kernel_traits<64, 128, 128, 4, cutlass::half_t>>, Is_dropout=false, Is_causal=true, Is_even_N=false, Is_even_K=true, Return_softmax=false, Params=Flash_fwd_params]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(15): here
            instantiation of "void flash_fwd_kernel<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax>(Flash_fwd_params) [with Kernel_traits=Flash_fwd_kernel_traits<64, 128, 128, 4, false, false, cutlass::half_t, Flash_kernel_traits<64, 128, 128, 4, cutlass::half_t>>, Is_dropout=false, Is_causal=true, Is_even_N=false, Is_even_K=true, Return_softmax=false]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(34): here
            instantiation of "void run_flash_fwd<Kernel_traits,Is_dropout,Is_causal>(Flash_fwd_params &, cudaStream_t) [with Kernel_traits=Flash_fwd_kernel_traits<64, 128, 128, 4, false, false, cutlass::half_t, Flash_kernel_traits<64, 128, 128, 4, cutlass::half_t>>, Is_dropout=false, Is_causal=true]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(68): here
            instantiation of "void run_mha_fwd_hdim64<T>(Flash_fwd_params &, cudaStream_t) [with T=cutlass::half_t]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_hdim64_fp16_sm80.cu(25): here

/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_kernel.h(275): warning: variable "tQrQ" was declared but never referenced
          detected during:
            instantiation of "void flash::compute_attn_1rowblock<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &, int, int, int) [with Kernel_traits=Flash_fwd_kernel_traits<64, 128, 128, 4, false, false, cutlass::half_t, Flash_kernel_traits<64, 128, 128, 4, cutlass::half_t>>, Is_dropout=false, Is_causal=true, Is_even_N=false, Is_even_K=true, Return_softmax=false, Params=Flash_fwd_params]" 
(588): here
            instantiation of "void flash::compute_attn<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &) [with Kernel_traits=Flash_fwd_kernel_traits<64, 128, 128, 4, false, false, cutlass::half_t, Flash_kernel_traits<64, 128, 128, 4, cutlass::half_t>>, Is_dropout=false, Is_causal=true, Is_even_N=false, Is_even_K=true, Return_softmax=false, Params=Flash_fwd_params]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(15): here
            instantiation of "void flash_fwd_kernel<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax>(Flash_fwd_params) [with Kernel_traits=Flash_fwd_kernel_traits<64, 128, 128, 4, false, false, cutlass::half_t, Flash_kernel_traits<64, 128, 128, 4, cutlass::half_t>>, Is_dropout=false, Is_causal=true, Is_even_N=false, Is_even_K=true, Return_softmax=false]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(34): here
            instantiation of "void run_flash_fwd<Kernel_traits,Is_dropout,Is_causal>(Flash_fwd_params &, cudaStream_t) [with Kernel_traits=Flash_fwd_kernel_traits<64, 128, 128, 4, false, false, cutlass::half_t, Flash_kernel_traits<64, 128, 128, 4, cutlass::half_t>>, Is_dropout=false, Is_causal=true]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(68): here
            instantiation of "void run_mha_fwd_hdim64<T>(Flash_fwd_params &, cudaStream_t) [with T=cutlass::half_t]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_hdim64_fp16_sm80.cu(25): here

/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_kernel.h(140): warning: variable "MMA_M" was declared but never referenced
          detected during:
            instantiation of "void flash::compute_attn_1rowblock<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &, int, int, int) [with Kernel_traits=Flash_fwd_kernel_traits<64, 128, 128, 4, false, false, cutlass::half_t, Flash_kernel_traits<64, 128, 128, 4, cutlass::half_t>>, Is_dropout=false, Is_causal=true, Is_even_N=false, Is_even_K=false, Return_softmax=false, Params=Flash_fwd_params]" 
(588): here
            instantiation of "void flash::compute_attn<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &) [with Kernel_traits=Flash_fwd_kernel_traits<64, 128, 128, 4, false, false, cutlass::half_t, Flash_kernel_traits<64, 128, 128, 4, cutlass::half_t>>, Is_dropout=false, Is_causal=true, Is_even_N=false, Is_even_K=false, Return_softmax=false, Params=Flash_fwd_params]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(15): here
            instantiation of "void flash_fwd_kernel<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax>(Flash_fwd_params) [with Kernel_traits=Flash_fwd_kernel_traits<64, 128, 128, 4, false, false, cutlass::half_t, Flash_kernel_traits<64, 128, 128, 4, cutlass::half_t>>, Is_dropout=false, Is_causal=true, Is_even_N=false, Is_even_K=false, Return_softmax=false]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(34): here
            instantiation of "void run_flash_fwd<Kernel_traits,Is_dropout,Is_causal>(Flash_fwd_params &, cudaStream_t) [with Kernel_traits=Flash_fwd_kernel_traits<64, 128, 128, 4, false, false, cutlass::half_t, Flash_kernel_traits<64, 128, 128, 4, cutlass::half_t>>, Is_dropout=false, Is_causal=true]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(68): here
            instantiation of "void run_mha_fwd_hdim64<T>(Flash_fwd_params &, cudaStream_t) [with T=cutlass::half_t]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_hdim64_fp16_sm80.cu(25): here

/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_kernel.h(275): warning: variable "tQrQ" was declared but never referenced
          detected during:
            instantiation of "void flash::compute_attn_1rowblock<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &, int, int, int) [with Kernel_traits=Flash_fwd_kernel_traits<64, 128, 128, 4, false, false, cutlass::half_t, Flash_kernel_traits<64, 128, 128, 4, cutlass::half_t>>, Is_dropout=false, Is_causal=true, Is_even_N=false, Is_even_K=false, Return_softmax=false, Params=Flash_fwd_params]" 
(588): here
            instantiation of "void flash::compute_attn<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &) [with Kernel_traits=Flash_fwd_kernel_traits<64, 128, 128, 4, false, false, cutlass::half_t, Flash_kernel_traits<64, 128, 128, 4, cutlass::half_t>>, Is_dropout=false, Is_causal=true, Is_even_N=false, Is_even_K=false, Return_softmax=false, Params=Flash_fwd_params]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(15): here
            instantiation of "void flash_fwd_kernel<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax>(Flash_fwd_params) [with Kernel_traits=Flash_fwd_kernel_traits<64, 128, 128, 4, false, false, cutlass::half_t, Flash_kernel_traits<64, 128, 128, 4, cutlass::half_t>>, Is_dropout=false, Is_causal=true, Is_even_N=false, Is_even_K=false, Return_softmax=false]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(34): here
            instantiation of "void run_flash_fwd<Kernel_traits,Is_dropout,Is_causal>(Flash_fwd_params &, cudaStream_t) [with Kernel_traits=Flash_fwd_kernel_traits<64, 128, 128, 4, false, false, cutlass::half_t, Flash_kernel_traits<64, 128, 128, 4, cutlass::half_t>>, Is_dropout=false, Is_causal=true]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(68): here
            instantiation of "void run_mha_fwd_hdim64<T>(Flash_fwd_params &, cudaStream_t) [with T=cutlass::half_t]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_hdim64_fp16_sm80.cu(25): here

/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_kernel.h(140): warning: variable "MMA_M" was declared but never referenced
          detected during:
            instantiation of "void flash::compute_attn_1rowblock<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &, int, int, int) [with Kernel_traits=Flash_fwd_kernel_traits<64, 128, 128, 4, false, false, cutlass::half_t, Flash_kernel_traits<64, 128, 128, 4, cutlass::half_t>>, Is_dropout=false, Is_causal=false, Is_even_N=true, Is_even_K=true, Return_softmax=false, Params=Flash_fwd_params]" 
(588): here
            instantiation of "void flash::compute_attn<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &) [with Kernel_traits=Flash_fwd_kernel_traits<64, 128, 128, 4, false, false, cutlass::half_t, Flash_kernel_traits<64, 128, 128, 4, cutlass::half_t>>, Is_dropout=false, Is_causal=false, Is_even_N=true, Is_even_K=true, Return_softmax=false, Params=Flash_fwd_params]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(15): here
            instantiation of "void flash_fwd_kernel<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax>(Flash_fwd_params) [with Kernel_traits=Flash_fwd_kernel_traits<64, 128, 128, 4, false, false, cutlass::half_t, Flash_kernel_traits<64, 128, 128, 4, cutlass::half_t>>, Is_dropout=false, Is_causal=false, Is_even_N=true, Is_even_K=true, Return_softmax=false]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(34): here
            instantiation of "void run_flash_fwd<Kernel_traits,Is_dropout,Is_causal>(Flash_fwd_params &, cudaStream_t) [with Kernel_traits=Flash_fwd_kernel_traits<64, 128, 128, 4, false, false, cutlass::half_t, Flash_kernel_traits<64, 128, 128, 4, cutlass::half_t>>, Is_dropout=false, Is_causal=false]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(68): here
            instantiation of "void run_mha_fwd_hdim64<T>(Flash_fwd_params &, cudaStream_t) [with T=cutlass::half_t]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_hdim64_fp16_sm80.cu(25): here

/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_kernel.h(275): warning: variable "tQrQ" was declared but never referenced
          detected during:
            instantiation of "void flash::compute_attn_1rowblock<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &, int, int, int) [with Kernel_traits=Flash_fwd_kernel_traits<64, 128, 128, 4, false, false, cutlass::half_t, Flash_kernel_traits<64, 128, 128, 4, cutlass::half_t>>, Is_dropout=false, Is_causal=false, Is_even_N=true, Is_even_K=true, Return_softmax=false, Params=Flash_fwd_params]" 
(588): here
            instantiation of "void flash::compute_attn<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &) [with Kernel_traits=Flash_fwd_kernel_traits<64, 128, 128, 4, false, false, cutlass::half_t, Flash_kernel_traits<64, 128, 128, 4, cutlass::half_t>>, Is_dropout=false, Is_causal=false, Is_even_N=true, Is_even_K=true, Return_softmax=false, Params=Flash_fwd_params]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(15): here
            instantiation of "void flash_fwd_kernel<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax>(Flash_fwd_params) [with Kernel_traits=Flash_fwd_kernel_traits<64, 128, 128, 4, false, false, cutlass::half_t, Flash_kernel_traits<64, 128, 128, 4, cutlass::half_t>>, Is_dropout=false, Is_causal=false, Is_even_N=true, Is_even_K=true, Return_softmax=false]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(34): here
            instantiation of "void run_flash_fwd<Kernel_traits,Is_dropout,Is_causal>(Flash_fwd_params &, cudaStream_t) [with Kernel_traits=Flash_fwd_kernel_traits<64, 128, 128, 4, false, false, cutlass::half_t, Flash_kernel_traits<64, 128, 128, 4, cutlass::half_t>>, Is_dropout=false, Is_causal=false]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(68): here
            instantiation of "void run_mha_fwd_hdim64<T>(Flash_fwd_params &, cudaStream_t) [with T=cutlass::half_t]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_hdim64_fp16_sm80.cu(25): here

/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_kernel.h(140): warning: variable "MMA_M" was declared but never referenced
          detected during:
            instantiation of "void flash::compute_attn_1rowblock<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &, int, int, int) [with Kernel_traits=Flash_fwd_kernel_traits<64, 128, 128, 4, false, false, cutlass::half_t, Flash_kernel_traits<64, 128, 128, 4, cutlass::half_t>>, Is_dropout=false, Is_causal=false, Is_even_N=true, Is_even_K=false, Return_softmax=false, Params=Flash_fwd_params]" 
(588): here
            instantiation of "void flash::compute_attn<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &) [with Kernel_traits=Flash_fwd_kernel_traits<64, 128, 128, 4, false, false, cutlass::half_t, Flash_kernel_traits<64, 128, 128, 4, cutlass::half_t>>, Is_dropout=false, Is_causal=false, Is_even_N=true, Is_even_K=false, Return_softmax=false, Params=Flash_fwd_params]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(15): here
            instantiation of "void flash_fwd_kernel<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax>(Flash_fwd_params) [with Kernel_traits=Flash_fwd_kernel_traits<64, 128, 128, 4, false, false, cutlass::half_t, Flash_kernel_traits<64, 128, 128, 4, cutlass::half_t>>, Is_dropout=false, Is_causal=false, Is_even_N=true, Is_even_K=false, Return_softmax=false]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(34): here
            instantiation of "void run_flash_fwd<Kernel_traits,Is_dropout,Is_causal>(Flash_fwd_params &, cudaStream_t) [with Kernel_traits=Flash_fwd_kernel_traits<64, 128, 128, 4, false, false, cutlass::half_t, Flash_kernel_traits<64, 128, 128, 4, cutlass::half_t>>, Is_dropout=false, Is_causal=false]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(68): here
            instantiation of "void run_mha_fwd_hdim64<T>(Flash_fwd_params &, cudaStream_t) [with T=cutlass::half_t]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_hdim64_fp16_sm80.cu(25): here

/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_kernel.h(275): warning: variable "tQrQ" was declared but never referenced
          detected during:
            instantiation of "void flash::compute_attn_1rowblock<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &, int, int, int) [with Kernel_traits=Flash_fwd_kernel_traits<64, 128, 128, 4, false, false, cutlass::half_t, Flash_kernel_traits<64, 128, 128, 4, cutlass::half_t>>, Is_dropout=false, Is_causal=false, Is_even_N=true, Is_even_K=false, Return_softmax=false, Params=Flash_fwd_params]" 
(588): here
            instantiation of "void flash::compute_attn<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &) [with Kernel_traits=Flash_fwd_kernel_traits<64, 128, 128, 4, false, false, cutlass::half_t, Flash_kernel_traits<64, 128, 128, 4, cutlass::half_t>>, Is_dropout=false, Is_causal=false, Is_even_N=true, Is_even_K=false, Return_softmax=false, Params=Flash_fwd_params]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(15): here
            instantiation of "void flash_fwd_kernel<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax>(Flash_fwd_params) [with Kernel_traits=Flash_fwd_kernel_traits<64, 128, 128, 4, false, false, cutlass::half_t, Flash_kernel_traits<64, 128, 128, 4, cutlass::half_t>>, Is_dropout=false, Is_causal=false, Is_even_N=true, Is_even_K=false, Return_softmax=false]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(34): here
            instantiation of "void run_flash_fwd<Kernel_traits,Is_dropout,Is_causal>(Flash_fwd_params &, cudaStream_t) [with Kernel_traits=Flash_fwd_kernel_traits<64, 128, 128, 4, false, false, cutlass::half_t, Flash_kernel_traits<64, 128, 128, 4, cutlass::half_t>>, Is_dropout=false, Is_causal=false]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(68): here
            instantiation of "void run_mha_fwd_hdim64<T>(Flash_fwd_params &, cudaStream_t) [with T=cutlass::half_t]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_hdim64_fp16_sm80.cu(25): here

/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_kernel.h(140): warning: variable "MMA_M" was declared but never referenced
          detected during:
            instantiation of "void flash::compute_attn_1rowblock<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &, int, int, int) [with Kernel_traits=Flash_fwd_kernel_traits<64, 128, 128, 4, false, false, cutlass::half_t, Flash_kernel_traits<64, 128, 128, 4, cutlass::half_t>>, Is_dropout=false, Is_causal=false, Is_even_N=false, Is_even_K=true, Return_softmax=false, Params=Flash_fwd_params]" 
(588): here
            instantiation of "void flash::compute_attn<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &) [with Kernel_traits=Flash_fwd_kernel_traits<64, 128, 128, 4, false, false, cutlass::half_t, Flash_kernel_traits<64, 128, 128, 4, cutlass::half_t>>, Is_dropout=false, Is_causal=false, Is_even_N=false, Is_even_K=true, Return_softmax=false, Params=Flash_fwd_params]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(15): here
            instantiation of "void flash_fwd_kernel<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax>(Flash_fwd_params) [with Kernel_traits=Flash_fwd_kernel_traits<64, 128, 128, 4, false, false, cutlass::half_t, Flash_kernel_traits<64, 128, 128, 4, cutlass::half_t>>, Is_dropout=false, Is_causal=false, Is_even_N=false, Is_even_K=true, Return_softmax=false]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(34): here
            instantiation of "void run_flash_fwd<Kernel_traits,Is_dropout,Is_causal>(Flash_fwd_params &, cudaStream_t) [with Kernel_traits=Flash_fwd_kernel_traits<64, 128, 128, 4, false, false, cutlass::half_t, Flash_kernel_traits<64, 128, 128, 4, cutlass::half_t>>, Is_dropout=false, Is_causal=false]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(68): here
            instantiation of "void run_mha_fwd_hdim64<T>(Flash_fwd_params &, cudaStream_t) [with T=cutlass::half_t]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_hdim64_fp16_sm80.cu(25): here

/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_kernel.h(275): warning: variable "tQrQ" was declared but never referenced
          detected during:
            instantiation of "void flash::compute_attn_1rowblock<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &, int, int, int) [with Kernel_traits=Flash_fwd_kernel_traits<64, 128, 128, 4, false, false, cutlass::half_t, Flash_kernel_traits<64, 128, 128, 4, cutlass::half_t>>, Is_dropout=false, Is_causal=false, Is_even_N=false, Is_even_K=true, Return_softmax=false, Params=Flash_fwd_params]" 
(588): here
            instantiation of "void flash::compute_attn<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &) [with Kernel_traits=Flash_fwd_kernel_traits<64, 128, 128, 4, false, false, cutlass::half_t, Flash_kernel_traits<64, 128, 128, 4, cutlass::half_t>>, Is_dropout=false, Is_causal=false, Is_even_N=false, Is_even_K=true, Return_softmax=false, Params=Flash_fwd_params]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(15): here
            instantiation of "void flash_fwd_kernel<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax>(Flash_fwd_params) [with Kernel_traits=Flash_fwd_kernel_traits<64, 128, 128, 4, false, false, cutlass::half_t, Flash_kernel_traits<64, 128, 128, 4, cutlass::half_t>>, Is_dropout=false, Is_causal=false, Is_even_N=false, Is_even_K=true, Return_softmax=false]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(34): here
            instantiation of "void run_flash_fwd<Kernel_traits,Is_dropout,Is_causal>(Flash_fwd_params &, cudaStream_t) [with Kernel_traits=Flash_fwd_kernel_traits<64, 128, 128, 4, false, false, cutlass::half_t, Flash_kernel_traits<64, 128, 128, 4, cutlass::half_t>>, Is_dropout=false, Is_causal=false]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(68): here
            instantiation of "void run_mha_fwd_hdim64<T>(Flash_fwd_params &, cudaStream_t) [with T=cutlass::half_t]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_hdim64_fp16_sm80.cu(25): here

/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_kernel.h(140): warning: variable "MMA_M" was declared but never referenced
          detected during:
            instantiation of "void flash::compute_attn_1rowblock<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &, int, int, int) [with Kernel_traits=Flash_fwd_kernel_traits<64, 128, 128, 4, false, false, cutlass::half_t, Flash_kernel_traits<64, 128, 128, 4, cutlass::half_t>>, Is_dropout=false, Is_causal=false, Is_even_N=false, Is_even_K=false, Return_softmax=false, Params=Flash_fwd_params]" 
(588): here
            instantiation of "void flash::compute_attn<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &) [with Kernel_traits=Flash_fwd_kernel_traits<64, 128, 128, 4, false, false, cutlass::half_t, Flash_kernel_traits<64, 128, 128, 4, cutlass::half_t>>, Is_dropout=false, Is_causal=false, Is_even_N=false, Is_even_K=false, Return_softmax=false, Params=Flash_fwd_params]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(15): here
            instantiation of "void flash_fwd_kernel<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax>(Flash_fwd_params) [with Kernel_traits=Flash_fwd_kernel_traits<64, 128, 128, 4, false, false, cutlass::half_t, Flash_kernel_traits<64, 128, 128, 4, cutlass::half_t>>, Is_dropout=false, Is_causal=false, Is_even_N=false, Is_even_K=false, Return_softmax=false]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(34): here
            instantiation of "void run_flash_fwd<Kernel_traits,Is_dropout,Is_causal>(Flash_fwd_params &, cudaStream_t) [with Kernel_traits=Flash_fwd_kernel_traits<64, 128, 128, 4, false, false, cutlass::half_t, Flash_kernel_traits<64, 128, 128, 4, cutlass::half_t>>, Is_dropout=false, Is_causal=false]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(68): here
            instantiation of "void run_mha_fwd_hdim64<T>(Flash_fwd_params &, cudaStream_t) [with T=cutlass::half_t]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_hdim64_fp16_sm80.cu(25): here

/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_kernel.h(275): warning: variable "tQrQ" was declared but never referenced
          detected during:
            instantiation of "void flash::compute_attn_1rowblock<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &, int, int, int) [with Kernel_traits=Flash_fwd_kernel_traits<64, 128, 128, 4, false, false, cutlass::half_t, Flash_kernel_traits<64, 128, 128, 4, cutlass::half_t>>, Is_dropout=false, Is_causal=false, Is_even_N=false, Is_even_K=false, Return_softmax=false, Params=Flash_fwd_params]" 
(588): here
            instantiation of "void flash::compute_attn<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &) [with Kernel_traits=Flash_fwd_kernel_traits<64, 128, 128, 4, false, false, cutlass::half_t, Flash_kernel_traits<64, 128, 128, 4, cutlass::half_t>>, Is_dropout=false, Is_causal=false, Is_even_N=false, Is_even_K=false, Return_softmax=false, Params=Flash_fwd_params]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(15): here
            instantiation of "void flash_fwd_kernel<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax>(Flash_fwd_params) [with Kernel_traits=Flash_fwd_kernel_traits<64, 128, 128, 4, false, false, cutlass::half_t, Flash_kernel_traits<64, 128, 128, 4, cutlass::half_t>>, Is_dropout=false, Is_causal=false, Is_even_N=false, Is_even_K=false, Return_softmax=false]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(34): here
            instantiation of "void run_flash_fwd<Kernel_traits,Is_dropout,Is_causal>(Flash_fwd_params &, cudaStream_t) [with Kernel_traits=Flash_fwd_kernel_traits<64, 128, 128, 4, false, false, cutlass::half_t, Flash_kernel_traits<64, 128, 128, 4, cutlass::half_t>>, Is_dropout=false, Is_causal=false]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(68): here
            instantiation of "void run_mha_fwd_hdim64<T>(Flash_fwd_params &, cudaStream_t) [with T=cutlass::half_t]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_hdim64_fp16_sm80.cu(25): here

ptxas info    : 2 bytes gmem
ptxas info    : Compiling entry function '_Z16flash_fwd_kernelI23Flash_fwd_kernel_traitsILi64ELi128ELi128ELi4ELb0ELb0EN7cutlass6half_tE19Flash_kernel_traitsILi64ELi128ELi128ELi4ES2_EELb0ELb0ELb0ELb0ELb0EEv16Flash_fwd_params' for 'sm_80'
ptxas info    : Function properties for _Z16flash_fwd_kernelI23Flash_fwd_kernel_traitsILi64ELi128ELi128ELi4ELb0ELb0EN7cutlass6half_tE19Flash_kernel_traitsILi64ELi128ELi128ELi4ES2_EELb0ELb0ELb0ELb0ELb0EEv16Flash_fwd_params
    40 bytes stack frame, 56 bytes spill stores, 56 bytes spill loads
ptxas info    : Used 255 registers, 584 bytes cmem[0]
ptxas info    : Compiling entry function '_Z16flash_fwd_kernelI23Flash_fwd_kernel_traitsILi64ELi128ELi128ELi4ELb0ELb0EN7cutlass6half_tE19Flash_kernel_traitsILi64ELi128ELi128ELi4ES2_EELb0ELb0ELb0ELb1ELb0EEv16Flash_fwd_params' for 'sm_80'
ptxas info    : Function properties for _Z16flash_fwd_kernelI23Flash_fwd_kernel_traitsILi64ELi128ELi128ELi4ELb0ELb0EN7cutlass6half_tE19Flash_kernel_traitsILi64ELi128ELi128ELi4ES2_EELb0ELb0ELb0ELb1ELb0EEv16Flash_fwd_params
    48 bytes stack frame, 76 bytes spill stores, 60 bytes spill loads
ptxas info    : Used 255 registers, 584 bytes cmem[0]
ptxas info    : Compiling entry function '_Z16flash_fwd_kernelI23Flash_fwd_kernel_traitsILi64ELi128ELi128ELi4ELb0ELb0EN7cutlass6half_tE19Flash_kernel_traitsILi64ELi128ELi128ELi4ES2_EELb0ELb0ELb1ELb0ELb0EEv16Flash_fwd_params' for 'sm_80'
ptxas info    : Function properties for _Z16flash_fwd_kernelI23Flash_fwd_kernel_traitsILi64ELi128ELi128ELi4ELb0ELb0EN7cutlass6half_tE19Flash_kernel_traitsILi64ELi128ELi128ELi4ES2_EELb0ELb0ELb1ELb0ELb0EEv16Flash_fwd_params
    40 bytes stack frame, 48 bytes spill stores, 48 bytes spill loads
ptxas info    : Used 255 registers, 584 bytes cmem[0]
ptxas info    : Compiling entry function '_Z16flash_fwd_kernelI23Flash_fwd_kernel_traitsILi64ELi128ELi128ELi4ELb0ELb0EN7cutlass6half_tE19Flash_kernel_traitsILi64ELi128ELi128ELi4ES2_EELb0ELb0ELb1ELb1ELb0EEv16Flash_fwd_params' for 'sm_80'
ptxas info    : Function properties for _Z16flash_fwd_kernelI23Flash_fwd_kernel_traitsILi64ELi128ELi128ELi4ELb0ELb0EN7cutlass6half_tE19Flash_kernel_traitsILi64ELi128ELi128ELi4ES2_EELb0ELb0ELb1ELb1ELb0EEv16Flash_fwd_params
    40 bytes stack frame, 52 bytes spill stores, 52 bytes spill loads
ptxas info    : Used 255 registers, 584 bytes cmem[0]
ptxas info    : Compiling entry function '_Z16flash_fwd_kernelI23Flash_fwd_kernel_traitsILi64ELi128ELi128ELi4ELb0ELb0EN7cutlass6half_tE19Flash_kernel_traitsILi64ELi128ELi128ELi4ES2_EELb0ELb1ELb0ELb0ELb0EEv16Flash_fwd_params' for 'sm_80'
ptxas info    : Function properties for _Z16flash_fwd_kernelI23Flash_fwd_kernel_traitsILi64ELi128ELi128ELi4ELb0ELb0EN7cutlass6half_tE19Flash_kernel_traitsILi64ELi128ELi128ELi4ES2_EELb0ELb1ELb0ELb0ELb0EEv16Flash_fwd_params
    40 bytes stack frame, 56 bytes spill stores, 56 bytes spill loads
ptxas info    : Used 255 registers, 584 bytes cmem[0]
ptxas info    : Compiling entry function '_Z16flash_fwd_kernelI23Flash_fwd_kernel_traitsILi64ELi128ELi128ELi4ELb0ELb0EN7cutlass6half_tE19Flash_kernel_traitsILi64ELi128ELi128ELi4ES2_EELb0ELb1ELb0ELb1ELb0EEv16Flash_fwd_params' for 'sm_80'
ptxas info    : Function properties for _Z16flash_fwd_kernelI23Flash_fwd_kernel_traitsILi64ELi128ELi128ELi4ELb0ELb0EN7cutlass6half_tE19Flash_kernel_traitsILi64ELi128ELi128ELi4ES2_EELb0ELb1ELb0ELb1ELb0EEv16Flash_fwd_params
    48 bytes stack frame, 80 bytes spill stores, 64 bytes spill loads
ptxas info    : Used 255 registers, 584 bytes cmem[0]
ptxas info    : Compiling entry function '_Z16flash_fwd_kernelI23Flash_fwd_kernel_traitsILi64ELi128ELi128ELi4ELb0ELb0EN7cutlass6half_tE19Flash_kernel_traitsILi64ELi128ELi128ELi4ES2_EELb0ELb1ELb1ELb0ELb0EEv16Flash_fwd_params' for 'sm_80'
ptxas info    : Function properties for _Z16flash_fwd_kernelI23Flash_fwd_kernel_traitsILi64ELi128ELi128ELi4ELb0ELb0EN7cutlass6half_tE19Flash_kernel_traitsILi64ELi128ELi128ELi4ES2_EELb0ELb1ELb1ELb0ELb0EEv16Flash_fwd_params
    40 bytes stack frame, 56 bytes spill stores, 56 bytes spill loads
ptxas info    : Used 255 registers, 584 bytes cmem[0]
ptxas info    : Compiling entry function '_Z16flash_fwd_kernelI23Flash_fwd_kernel_traitsILi64ELi128ELi128ELi4ELb0ELb0EN7cutlass6half_tE19Flash_kernel_traitsILi64ELi128ELi128ELi4ES2_EELb0ELb1ELb1ELb1ELb0EEv16Flash_fwd_params' for 'sm_80'
ptxas info    : Function properties for _Z16flash_fwd_kernelI23Flash_fwd_kernel_traitsILi64ELi128ELi128ELi4ELb0ELb0EN7cutlass6half_tE19Flash_kernel_traitsILi64ELi128ELi128ELi4ES2_EELb0ELb1ELb1ELb1ELb0EEv16Flash_fwd_params
    40 bytes stack frame, 52 bytes spill stores, 52 bytes spill loads
ptxas info    : Used 255 registers, 584 bytes cmem[0]
ptxas info    : Compiling entry function '_Z16flash_fwd_kernelI23Flash_fwd_kernel_traitsILi64ELi128ELi64ELi4ELb0ELb0EN7cutlass6half_tE19Flash_kernel_traitsILi64ELi128ELi64ELi4ES2_EELb1ELb0ELb0ELb0ELb0EEv16Flash_fwd_params' for 'sm_80'
ptxas info    : Function properties for _Z16flash_fwd_kernelI23Flash_fwd_kernel_traitsILi64ELi128ELi64ELi4ELb0ELb0EN7cutlass6half_tE19Flash_kernel_traitsILi64ELi128ELi64ELi4ES2_EELb1ELb0ELb0ELb0ELb0EEv16Flash_fwd_params
    0 bytes stack frame, 0 bytes spill stores, 0 bytes spill loads
ptxas info    : Used 242 registers, 584 bytes cmem[0]
ptxas info    : Compiling entry function '_Z16flash_fwd_kernelI23Flash_fwd_kernel_traitsILi64ELi128ELi64ELi4ELb0ELb0EN7cutlass6half_tE19Flash_kernel_traitsILi64ELi128ELi64ELi4ES2_EELb1ELb0ELb0ELb0ELb1EEv16Flash_fwd_params' for 'sm_80'
ptxas info    : Function properties for _Z16flash_fwd_kernelI23Flash_fwd_kernel_traitsILi64ELi128ELi64ELi4ELb0ELb0EN7cutlass6half_tE19Flash_kernel_traitsILi64ELi128ELi64ELi4ES2_EELb1ELb0ELb0ELb0ELb1EEv16Flash_fwd_params
    160 bytes stack frame, 126 bytes spill stores, 126 bytes spill loads
ptxas info    : Used 255 registers, 584 bytes cmem[0]
ptxas info    : Compiling entry function '_Z16flash_fwd_kernelI23Flash_fwd_kernel_traitsILi64ELi128ELi64ELi4ELb0ELb0EN7cutlass6half_tE19Flash_kernel_traitsILi64ELi128ELi64ELi4ES2_EELb1ELb0ELb0ELb1ELb0EEv16Flash_fwd_params' for 'sm_80'
ptxas info    : Function properties for _Z16flash_fwd_kernelI23Flash_fwd_kernel_traitsILi64ELi128ELi64ELi4ELb0ELb0EN7cutlass6half_tE19Flash_kernel_traitsILi64ELi128ELi64ELi4ES2_EELb1ELb0ELb0ELb1ELb0EEv16Flash_fwd_params
    0 bytes stack frame, 0 bytes spill stores, 0 bytes spill loads
ptxas info    : Used 236 registers, 584 bytes cmem[0]
ptxas info    : Compiling entry function '_Z16flash_fwd_kernelI23Flash_fwd_kernel_traitsILi64ELi128ELi64ELi4ELb0ELb0EN7cutlass6half_tE19Flash_kernel_traitsILi64ELi128ELi64ELi4ES2_EELb1ELb0ELb0ELb1ELb1EEv16Flash_fwd_params' for 'sm_80'
ptxas info    : Function properties for _Z16flash_fwd_kernelI23Flash_fwd_kernel_traitsILi64ELi128ELi64ELi4ELb0ELb0EN7cutlass6half_tE19Flash_kernel_traitsILi64ELi128ELi64ELi4ES2_EELb1ELb0ELb0ELb1ELb1EEv16Flash_fwd_params
    216 bytes stack frame, 212 bytes spill stores, 218 bytes spill loads
ptxas info    : Used 255 registers, 584 bytes cmem[0]
ptxas info    : Compiling entry function '_Z16flash_fwd_kernelI23Flash_fwd_kernel_traitsILi64ELi128ELi64ELi4ELb0ELb0EN7cutlass6half_tE19Flash_kernel_traitsILi64ELi128ELi64ELi4ES2_EELb1ELb0ELb1ELb0ELb0EEv16Flash_fwd_params' for 'sm_80'
ptxas info    : Function properties for _Z16flash_fwd_kernelI23Flash_fwd_kernel_traitsILi64ELi128ELi64ELi4ELb0ELb0EN7cutlass6half_tE19Flash_kernel_traitsILi64ELi128ELi64ELi4ES2_EELb1ELb0ELb1ELb0ELb0EEv16Flash_fwd_params
    0 bytes stack frame, 0 bytes spill stores, 0 bytes spill loads
ptxas info    : Used 236 registers, 584 bytes cmem[0]
ptxas info    : Compiling entry function '_Z16flash_fwd_kernelI23Flash_fwd_kernel_traitsILi64ELi128ELi64ELi4ELb0ELb0EN7cutlass6half_tE19Flash_kernel_traitsILi64ELi128ELi64ELi4ES2_EELb1ELb0ELb1ELb0ELb1EEv16Flash_fwd_params' for 'sm_80'
ptxas info    : Function properties for _Z16flash_fwd_kernelI23Flash_fwd_kernel_traitsILi64ELi128ELi64ELi4ELb0ELb0EN7cutlass6half_tE19Flash_kernel_traitsILi64ELi128ELi64ELi4ES2_EELb1ELb0ELb1ELb0ELb1EEv16Flash_fwd_params
    136 bytes stack frame, 116 bytes spill stores, 116 bytes spill loads
ptxas info    : Used 255 registers, 584 bytes cmem[0]
ptxas info    : Compiling entry function '_Z16flash_fwd_kernelI23Flash_fwd_kernel_traitsILi64ELi128ELi64ELi4ELb0ELb0EN7cutlass6half_tE19Flash_kernel_traitsILi64ELi128ELi64ELi4ES2_EELb1ELb0ELb1ELb1ELb0EEv16Flash_fwd_params' for 'sm_80'
ptxas info    : Function properties for _Z16flash_fwd_kernelI23Flash_fwd_kernel_traitsILi64ELi128ELi64ELi4ELb0ELb0EN7cutlass6half_tE19Flash_kernel_traitsILi64ELi128ELi64ELi4ES2_EELb1ELb0ELb1ELb1ELb0EEv16Flash_fwd_params
    0 bytes stack frame, 0 bytes spill stores, 0 bytes spill loads
ptxas info    : Used 238 registers, 584 bytes cmem[0]
ptxas info    : Compiling entry function '_Z16flash_fwd_kernelI23Flash_fwd_kernel_traitsILi64ELi128ELi64ELi4ELb0ELb0EN7cutlass6half_tE19Flash_kernel_traitsILi64ELi128ELi64ELi4ES2_EELb1ELb0ELb1ELb1ELb1EEv16Flash_fwd_params' for 'sm_80'
ptxas info    : Function properties for _Z16flash_fwd_kernelI23Flash_fwd_kernel_traitsILi64ELi128ELi64ELi4ELb0ELb0EN7cutlass6half_tE19Flash_kernel_traitsILi64ELi128ELi64ELi4ES2_EELb1ELb0ELb1ELb1ELb1EEv16Flash_fwd_params
    192 bytes stack frame, 164 bytes spill stores, 184 bytes spill loads
ptxas info    : Used 255 registers, 584 bytes cmem[0]
ptxas info    : Compiling entry function '_Z16flash_fwd_kernelI23Flash_fwd_kernel_traitsILi64ELi128ELi64ELi4ELb0ELb0EN7cutlass6half_tE19Flash_kernel_traitsILi64ELi128ELi64ELi4ES2_EELb1ELb1ELb0ELb0ELb0EEv16Flash_fwd_params' for 'sm_80'
ptxas info    : Function properties for _Z16flash_fwd_kernelI23Flash_fwd_kernel_traitsILi64ELi128ELi64ELi4ELb0ELb0EN7cutlass6half_tE19Flash_kernel_traitsILi64ELi128ELi64ELi4ES2_EELb1ELb1ELb0ELb0ELb0EEv16Flash_fwd_params
    0 bytes stack frame, 0 bytes spill stores, 0 bytes spill loads
ptxas info    : Used 250 registers, 584 bytes cmem[0]
ptxas info    : Compiling entry function '_Z16flash_fwd_kernelI23Flash_fwd_kernel_traitsILi64ELi128ELi64ELi4ELb0ELb0EN7cutlass6half_tE19Flash_kernel_traitsILi64ELi128ELi64ELi4ES2_EELb1ELb1ELb0ELb0ELb1EEv16Flash_fwd_params' for 'sm_80'
ptxas info    : Function properties for _Z16flash_fwd_kernelI23Flash_fwd_kernel_traitsILi64ELi128ELi64ELi4ELb0ELb0EN7cutlass6half_tE19Flash_kernel_traitsILi64ELi128ELi64ELi4ES2_EELb1ELb1ELb0ELb0ELb1EEv16Flash_fwd_params
    160 bytes stack frame, 178 bytes spill stores, 202 bytes spill loads
ptxas info    : Used 255 registers, 584 bytes cmem[0]
ptxas info    : Compiling entry function '_Z16flash_fwd_kernelI23Flash_fwd_kernel_traitsILi64ELi128ELi64ELi4ELb0ELb0EN7cutlass6half_tE19Flash_kernel_traitsILi64ELi128ELi64ELi4ES2_EELb1ELb1ELb0ELb1ELb0EEv16Flash_fwd_params' for 'sm_80'
ptxas info    : Function properties for _Z16flash_fwd_kernelI23Flash_fwd_kernel_traitsILi64ELi128ELi64ELi4ELb0ELb0EN7cutlass6half_tE19Flash_kernel_traitsILi64ELi128ELi64ELi4ES2_EELb1ELb1ELb0ELb1ELb0EEv16Flash_fwd_params
    0 bytes stack frame, 0 bytes spill stores, 0 bytes spill loads
ptxas info    : Used 244 registers, 584 bytes cmem[0]
ptxas info    : Compiling entry function '_Z16flash_fwd_kernelI23Flash_fwd_kernel_traitsILi64ELi128ELi64ELi4ELb0ELb0EN7cutlass6half_tE19Flash_kernel_traitsILi64ELi128ELi64ELi4ES2_EELb1ELb1ELb0ELb1ELb1EEv16Flash_fwd_params' for 'sm_80'
ptxas info    : Function properties for _Z16flash_fwd_kernelI23Flash_fwd_kernel_traitsILi64ELi128ELi64ELi4ELb0ELb0EN7cutlass6half_tE19Flash_kernel_traitsILi64ELi128ELi64ELi4ES2_EELb1ELb1ELb0ELb1ELb1EEv16Flash_fwd_params
    216 bytes stack frame, 212 bytes spill stores, 236 bytes spill loads
ptxas info    : Used 255 registers, 584 bytes cmem[0]
ptxas info    : Compiling entry function '_Z16flash_fwd_kernelI23Flash_fwd_kernel_traitsILi64ELi128ELi64ELi4ELb0ELb0EN7cutlass6half_tE19Flash_kernel_traitsILi64ELi128ELi64ELi4ES2_EELb1ELb1ELb1ELb0ELb0EEv16Flash_fwd_params' for 'sm_80'
ptxas info    : Function properties for _Z16flash_fwd_kernelI23Flash_fwd_kernel_traitsILi64ELi128ELi64ELi4ELb0ELb0EN7cutlass6half_tE19Flash_kernel_traitsILi64ELi128ELi64ELi4ES2_EELb1ELb1ELb1ELb0ELb0EEv16Flash_fwd_params
    0 bytes stack frame, 0 bytes spill stores, 0 bytes spill loads
ptxas info    : Used 248 registers, 584 bytes cmem[0]
ptxas info    : Compiling entry function '_Z16flash_fwd_kernelI23Flash_fwd_kernel_traitsILi64ELi128ELi64ELi4ELb0ELb0EN7cutlass6half_tE19Flash_kernel_traitsILi64ELi128ELi64ELi4ES2_EELb1ELb1ELb1ELb0ELb1EEv16Flash_fwd_params' for 'sm_80'
ptxas info    : Function properties for _Z16flash_fwd_kernelI23Flash_fwd_kernel_traitsILi64ELi128ELi64ELi4ELb0ELb0EN7cutlass6half_tE19Flash_kernel_traitsILi64ELi128ELi64ELi4ES2_EELb1ELb1ELb1ELb0ELb1EEv16Flash_fwd_params
    152 bytes stack frame, 142 bytes spill stores, 182 bytes spill loads
ptxas info    : Used 255 registers, 584 bytes cmem[0]
ptxas info    : Compiling entry function '_Z16flash_fwd_kernelI23Flash_fwd_kernel_traitsILi64ELi128ELi64ELi4ELb0ELb0EN7cutlass6half_tE19Flash_kernel_traitsILi64ELi128ELi64ELi4ES2_EELb1ELb1ELb1ELb1ELb0EEv16Flash_fwd_params' for 'sm_80'
ptxas info    : Function properties for _Z16flash_fwd_kernelI23Flash_fwd_kernel_traitsILi64ELi128ELi64ELi4ELb0ELb0EN7cutlass6half_tE19Flash_kernel_traitsILi64ELi128ELi64ELi4ES2_EELb1ELb1ELb1ELb1ELb0EEv16Flash_fwd_params
    0 bytes stack frame, 0 bytes spill stores, 0 bytes spill loads
ptxas info    : Used 246 registers, 584 bytes cmem[0]
ptxas info    : Compiling entry function '_Z16flash_fwd_kernelI23Flash_fwd_kernel_traitsILi64ELi128ELi64ELi4ELb0ELb0EN7cutlass6half_tE19Flash_kernel_traitsILi64ELi128ELi64ELi4ES2_EELb1ELb1ELb1ELb1ELb1EEv16Flash_fwd_params' for 'sm_80'
ptxas info    : Function properties for _Z16flash_fwd_kernelI23Flash_fwd_kernel_traitsILi64ELi128ELi64ELi4ELb0ELb0EN7cutlass6half_tE19Flash_kernel_traitsILi64ELi128ELi64ELi4ES2_EELb1ELb1ELb1ELb1ELb1EEv16Flash_fwd_params
    160 bytes stack frame, 152 bytes spill stores, 190 bytes spill loads
ptxas info    : Used 255 registers, 584 bytes cmem[0]
/anaconda/envs/flashattention/lib/python3.9/site-packages/torch/include/c10/core/SymInt.h(84): warning: integer conversion resulted in a change of sign

[6/13] /usr/local/cuda-11.4/bin/nvcc  -I/home/v-feiychen/flash-attention/csrc/flash_attn -I/home/v-feiychen/flash-attention/csrc/flash_attn/src -I/home/v-feiychen/flash-attention/csrc/cutlass/include -I/anaconda/envs/flashattention/lib/python3.9/site-packages/torch/include -I/anaconda/envs/flashattention/lib/python3.9/site-packages/torch/include/torch/csrc/api/include -I/anaconda/envs/flashattention/lib/python3.9/site-packages/torch/include/TH -I/anaconda/envs/flashattention/lib/python3.9/site-packages/torch/include/THC -I/usr/local/cuda-11.4/include -I/anaconda/envs/flashattention/include/python3.9 -c -c /home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_hdim64_bf16_sm80.cu -o /home/v-feiychen/flash-attention/build/temp.linux-x86_64-cpython-39/csrc/flash_attn/src/flash_fwd_hdim64_bf16_sm80.o -D__CUDA_NO_HALF_OPERATORS__ -D__CUDA_NO_HALF_CONVERSIONS__ -D__CUDA_NO_BFLOAT16_CONVERSIONS__ -D__CUDA_NO_HALF2_OPERATORS__ --expt-relaxed-constexpr --compiler-options ''"'"'-fPIC'"'"'' -O3 -std=c++17 -U__CUDA_NO_HALF_OPERATORS__ -U__CUDA_NO_HALF_CONVERSIONS__ -U__CUDA_NO_HALF2_OPERATORS__ -U__CUDA_NO_BFLOAT16_CONVERSIONS__ --expt-relaxed-constexpr --expt-extended-lambda --use_fast_math --ptxas-options=-v -lineinfo -gencode arch=compute_80,code=sm_80 --threads 4 -DTORCH_API_INCLUDE_EXTENSION_H '-DPYBIND11_COMPILER_TYPE="_gcc"' '-DPYBIND11_STDLIB="_libstdcpp"' '-DPYBIND11_BUILD_ABI="_cxxabi1011"' -DTORCH_EXTENSION_NAME=flash_attn_2_cuda -D_GLIBCXX_USE_CXX11_ABI=0
/anaconda/envs/flashattention/lib/python3.9/site-packages/torch/include/c10/core/SymInt.h(84): warning: integer conversion resulted in a change of sign

/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_kernel.h(140): warning: variable "MMA_M" was declared but never referenced
          detected during:
            instantiation of "void flash::compute_attn_1rowblock<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &, int, int, int) [with Kernel_traits=Flash_fwd_kernel_traits<64, 128, 64, 4, false, false, cutlass::bfloat16_t, Flash_kernel_traits<64, 128, 64, 4, cutlass::bfloat16_t>>, Is_dropout=true, Is_causal=true, Is_even_N=true, Is_even_K=true, Return_softmax=true, Params=Flash_fwd_params]" 
(588): here
            instantiation of "void flash::compute_attn<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &) [with Kernel_traits=Flash_fwd_kernel_traits<64, 128, 64, 4, false, false, cutlass::bfloat16_t, Flash_kernel_traits<64, 128, 64, 4, cutlass::bfloat16_t>>, Is_dropout=true, Is_causal=true, Is_even_N=true, Is_even_K=true, Return_softmax=true, Params=Flash_fwd_params]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(15): here
            instantiation of "void flash_fwd_kernel<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax>(Flash_fwd_params) [with Kernel_traits=Flash_fwd_kernel_traits<64, 128, 64, 4, false, false, cutlass::bfloat16_t, Flash_kernel_traits<64, 128, 64, 4, cutlass::bfloat16_t>>, Is_dropout=true, Is_causal=true, Is_even_N=true, Is_even_K=true, Return_softmax=true]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(34): here
            instantiation of "void run_flash_fwd<Kernel_traits,Is_dropout,Is_causal>(Flash_fwd_params &, cudaStream_t) [with Kernel_traits=Flash_fwd_kernel_traits<64, 128, 64, 4, false, false, cutlass::bfloat16_t, Flash_kernel_traits<64, 128, 64, 4, cutlass::bfloat16_t>>, Is_dropout=true, Is_causal=true]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(68): here
            instantiation of "void run_mha_fwd_hdim64<T>(Flash_fwd_params &, cudaStream_t) [with T=cutlass::bfloat16_t]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_hdim64_bf16_sm80.cu(18): here

/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_kernel.h(275): warning: variable "tQrQ" was declared but never referenced
          detected during:
            instantiation of "void flash::compute_attn_1rowblock<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &, int, int, int) [with Kernel_traits=Flash_fwd_kernel_traits<64, 128, 64, 4, false, false, cutlass::bfloat16_t, Flash_kernel_traits<64, 128, 64, 4, cutlass::bfloat16_t>>, Is_dropout=true, Is_causal=true, Is_even_N=true, Is_even_K=true, Return_softmax=true, Params=Flash_fwd_params]" 
(588): here
            instantiation of "void flash::compute_attn<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &) [with Kernel_traits=Flash_fwd_kernel_traits<64, 128, 64, 4, false, false, cutlass::bfloat16_t, Flash_kernel_traits<64, 128, 64, 4, cutlass::bfloat16_t>>, Is_dropout=true, Is_causal=true, Is_even_N=true, Is_even_K=true, Return_softmax=true, Params=Flash_fwd_params]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(15): here
            instantiation of "void flash_fwd_kernel<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax>(Flash_fwd_params) [with Kernel_traits=Flash_fwd_kernel_traits<64, 128, 64, 4, false, false, cutlass::bfloat16_t, Flash_kernel_traits<64, 128, 64, 4, cutlass::bfloat16_t>>, Is_dropout=true, Is_causal=true, Is_even_N=true, Is_even_K=true, Return_softmax=true]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(34): here
            instantiation of "void run_flash_fwd<Kernel_traits,Is_dropout,Is_causal>(Flash_fwd_params &, cudaStream_t) [with Kernel_traits=Flash_fwd_kernel_traits<64, 128, 64, 4, false, false, cutlass::bfloat16_t, Flash_kernel_traits<64, 128, 64, 4, cutlass::bfloat16_t>>, Is_dropout=true, Is_causal=true]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(68): here
            instantiation of "void run_mha_fwd_hdim64<T>(Flash_fwd_params &, cudaStream_t) [with T=cutlass::bfloat16_t]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_hdim64_bf16_sm80.cu(18): here

/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_kernel.h(140): warning: variable "MMA_M" was declared but never referenced
          detected during:
            instantiation of "void flash::compute_attn_1rowblock<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &, int, int, int) [with Kernel_traits=Flash_fwd_kernel_traits<64, 128, 64, 4, false, false, cutlass::bfloat16_t, Flash_kernel_traits<64, 128, 64, 4, cutlass::bfloat16_t>>, Is_dropout=true, Is_causal=true, Is_even_N=true, Is_even_K=true, Return_softmax=false, Params=Flash_fwd_params]" 
(588): here
            instantiation of "void flash::compute_attn<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &) [with Kernel_traits=Flash_fwd_kernel_traits<64, 128, 64, 4, false, false, cutlass::bfloat16_t, Flash_kernel_traits<64, 128, 64, 4, cutlass::bfloat16_t>>, Is_dropout=true, Is_causal=true, Is_even_N=true, Is_even_K=true, Return_softmax=false, Params=Flash_fwd_params]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(15): here
            instantiation of "void flash_fwd_kernel<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax>(Flash_fwd_params) [with Kernel_traits=Flash_fwd_kernel_traits<64, 128, 64, 4, false, false, cutlass::bfloat16_t, Flash_kernel_traits<64, 128, 64, 4, cutlass::bfloat16_t>>, Is_dropout=true, Is_causal=true, Is_even_N=true, Is_even_K=true, Return_softmax=false]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(34): here
            instantiation of "void run_flash_fwd<Kernel_traits,Is_dropout,Is_causal>(Flash_fwd_params &, cudaStream_t) [with Kernel_traits=Flash_fwd_kernel_traits<64, 128, 64, 4, false, false, cutlass::bfloat16_t, Flash_kernel_traits<64, 128, 64, 4, cutlass::bfloat16_t>>, Is_dropout=true, Is_causal=true]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(68): here
            instantiation of "void run_mha_fwd_hdim64<T>(Flash_fwd_params &, cudaStream_t) [with T=cutlass::bfloat16_t]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_hdim64_bf16_sm80.cu(18): here

/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_kernel.h(275): warning: variable "tQrQ" was declared but never referenced
          detected during:
            instantiation of "void flash::compute_attn_1rowblock<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &, int, int, int) [with Kernel_traits=Flash_fwd_kernel_traits<64, 128, 64, 4, false, false, cutlass::bfloat16_t, Flash_kernel_traits<64, 128, 64, 4, cutlass::bfloat16_t>>, Is_dropout=true, Is_causal=true, Is_even_N=true, Is_even_K=true, Return_softmax=false, Params=Flash_fwd_params]" 
(588): here
            instantiation of "void flash::compute_attn<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &) [with Kernel_traits=Flash_fwd_kernel_traits<64, 128, 64, 4, false, false, cutlass::bfloat16_t, Flash_kernel_traits<64, 128, 64, 4, cutlass::bfloat16_t>>, Is_dropout=true, Is_causal=true, Is_even_N=true, Is_even_K=true, Return_softmax=false, Params=Flash_fwd_params]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(15): here
            instantiation of "void flash_fwd_kernel<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax>(Flash_fwd_params) [with Kernel_traits=Flash_fwd_kernel_traits<64, 128, 64, 4, false, false, cutlass::bfloat16_t, Flash_kernel_traits<64, 128, 64, 4, cutlass::bfloat16_t>>, Is_dropout=true, Is_causal=true, Is_even_N=true, Is_even_K=true, Return_softmax=false]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(34): here
            instantiation of "void run_flash_fwd<Kernel_traits,Is_dropout,Is_causal>(Flash_fwd_params &, cudaStream_t) [with Kernel_traits=Flash_fwd_kernel_traits<64, 128, 64, 4, false, false, cutlass::bfloat16_t, Flash_kernel_traits<64, 128, 64, 4, cutlass::bfloat16_t>>, Is_dropout=true, Is_causal=true]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(68): here
            instantiation of "void run_mha_fwd_hdim64<T>(Flash_fwd_params &, cudaStream_t) [with T=cutlass::bfloat16_t]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_hdim64_bf16_sm80.cu(18): here

/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_kernel.h(140): warning: variable "MMA_M" was declared but never referenced
          detected during:
            instantiation of "void flash::compute_attn_1rowblock<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &, int, int, int) [with Kernel_traits=Flash_fwd_kernel_traits<64, 128, 64, 4, false, false, cutlass::bfloat16_t, Flash_kernel_traits<64, 128, 64, 4, cutlass::bfloat16_t>>, Is_dropout=true, Is_causal=true, Is_even_N=true, Is_even_K=false, Return_softmax=true, Params=Flash_fwd_params]" 
(588): here
            instantiation of "void flash::compute_attn<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &) [with Kernel_traits=Flash_fwd_kernel_traits<64, 128, 64, 4, false, false, cutlass::bfloat16_t, Flash_kernel_traits<64, 128, 64, 4, cutlass::bfloat16_t>>, Is_dropout=true, Is_causal=true, Is_even_N=true, Is_even_K=false, Return_softmax=true, Params=Flash_fwd_params]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(15): here
            instantiation of "void flash_fwd_kernel<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax>(Flash_fwd_params) [with Kernel_traits=Flash_fwd_kernel_traits<64, 128, 64, 4, false, false, cutlass::bfloat16_t, Flash_kernel_traits<64, 128, 64, 4, cutlass::bfloat16_t>>, Is_dropout=true, Is_causal=true, Is_even_N=true, Is_even_K=false, Return_softmax=true]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(34): here
            instantiation of "void run_flash_fwd<Kernel_traits,Is_dropout,Is_causal>(Flash_fwd_params &, cudaStream_t) [with Kernel_traits=Flash_fwd_kernel_traits<64, 128, 64, 4, false, false, cutlass::bfloat16_t, Flash_kernel_traits<64, 128, 64, 4, cutlass::bfloat16_t>>, Is_dropout=true, Is_causal=true]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(68): here
            instantiation of "void run_mha_fwd_hdim64<T>(Flash_fwd_params &, cudaStream_t) [with T=cutlass::bfloat16_t]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_hdim64_bf16_sm80.cu(18): here

/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_kernel.h(275): warning: variable "tQrQ" was declared but never referenced
          detected during:
            instantiation of "void flash::compute_attn_1rowblock<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &, int, int, int) [with Kernel_traits=Flash_fwd_kernel_traits<64, 128, 64, 4, false, false, cutlass::bfloat16_t, Flash_kernel_traits<64, 128, 64, 4, cutlass::bfloat16_t>>, Is_dropout=true, Is_causal=true, Is_even_N=true, Is_even_K=false, Return_softmax=true, Params=Flash_fwd_params]" 
(588): here
            instantiation of "void flash::compute_attn<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &) [with Kernel_traits=Flash_fwd_kernel_traits<64, 128, 64, 4, false, false, cutlass::bfloat16_t, Flash_kernel_traits<64, 128, 64, 4, cutlass::bfloat16_t>>, Is_dropout=true, Is_causal=true, Is_even_N=true, Is_even_K=false, Return_softmax=true, Params=Flash_fwd_params]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(15): here
            instantiation of "void flash_fwd_kernel<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax>(Flash_fwd_params) [with Kernel_traits=Flash_fwd_kernel_traits<64, 128, 64, 4, false, false, cutlass::bfloat16_t, Flash_kernel_traits<64, 128, 64, 4, cutlass::bfloat16_t>>, Is_dropout=true, Is_causal=true, Is_even_N=true, Is_even_K=false, Return_softmax=true]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(34): here
            instantiation of "void run_flash_fwd<Kernel_traits,Is_dropout,Is_causal>(Flash_fwd_params &, cudaStream_t) [with Kernel_traits=Flash_fwd_kernel_traits<64, 128, 64, 4, false, false, cutlass::bfloat16_t, Flash_kernel_traits<64, 128, 64, 4, cutlass::bfloat16_t>>, Is_dropout=true, Is_causal=true]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(68): here
            instantiation of "void run_mha_fwd_hdim64<T>(Flash_fwd_params &, cudaStream_t) [with T=cutlass::bfloat16_t]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_hdim64_bf16_sm80.cu(18): here

/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_kernel.h(140): warning: variable "MMA_M" was declared but never referenced
          detected during:
            instantiation of "void flash::compute_attn_1rowblock<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &, int, int, int) [with Kernel_traits=Flash_fwd_kernel_traits<64, 128, 64, 4, false, false, cutlass::bfloat16_t, Flash_kernel_traits<64, 128, 64, 4, cutlass::bfloat16_t>>, Is_dropout=true, Is_causal=true, Is_even_N=true, Is_even_K=false, Return_softmax=false, Params=Flash_fwd_params]" 
(588): here
            instantiation of "void flash::compute_attn<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &) [with Kernel_traits=Flash_fwd_kernel_traits<64, 128, 64, 4, false, false, cutlass::bfloat16_t, Flash_kernel_traits<64, 128, 64, 4, cutlass::bfloat16_t>>, Is_dropout=true, Is_causal=true, Is_even_N=true, Is_even_K=false, Return_softmax=false, Params=Flash_fwd_params]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(15): here
            instantiation of "void flash_fwd_kernel<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax>(Flash_fwd_params) [with Kernel_traits=Flash_fwd_kernel_traits<64, 128, 64, 4, false, false, cutlass::bfloat16_t, Flash_kernel_traits<64, 128, 64, 4, cutlass::bfloat16_t>>, Is_dropout=true, Is_causal=true, Is_even_N=true, Is_even_K=false, Return_softmax=false]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(34): here
            instantiation of "void run_flash_fwd<Kernel_traits,Is_dropout,Is_causal>(Flash_fwd_params &, cudaStream_t) [with Kernel_traits=Flash_fwd_kernel_traits<64, 128, 64, 4, false, false, cutlass::bfloat16_t, Flash_kernel_traits<64, 128, 64, 4, cutlass::bfloat16_t>>, Is_dropout=true, Is_causal=true]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(68): here
            instantiation of "void run_mha_fwd_hdim64<T>(Flash_fwd_params &, cudaStream_t) [with T=cutlass::bfloat16_t]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_hdim64_bf16_sm80.cu(18): here

/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_kernel.h(275): warning: variable "tQrQ" was declared but never referenced
          detected during:
            instantiation of "void flash::compute_attn_1rowblock<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &, int, int, int) [with Kernel_traits=Flash_fwd_kernel_traits<64, 128, 64, 4, false, false, cutlass::bfloat16_t, Flash_kernel_traits<64, 128, 64, 4, cutlass::bfloat16_t>>, Is_dropout=true, Is_causal=true, Is_even_N=true, Is_even_K=false, Return_softmax=false, Params=Flash_fwd_params]" 
(588): here
            instantiation of "void flash::compute_attn<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &) [with Kernel_traits=Flash_fwd_kernel_traits<64, 128, 64, 4, false, false, cutlass::bfloat16_t, Flash_kernel_traits<64, 128, 64, 4, cutlass::bfloat16_t>>, Is_dropout=true, Is_causal=true, Is_even_N=true, Is_even_K=false, Return_softmax=false, Params=Flash_fwd_params]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(15): here
            instantiation of "void flash_fwd_kernel<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax>(Flash_fwd_params) [with Kernel_traits=Flash_fwd_kernel_traits<64, 128, 64, 4, false, false, cutlass::bfloat16_t, Flash_kernel_traits<64, 128, 64, 4, cutlass::bfloat16_t>>, Is_dropout=true, Is_causal=true, Is_even_N=true, Is_even_K=false, Return_softmax=false]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(34): here
            instantiation of "void run_flash_fwd<Kernel_traits,Is_dropout,Is_causal>(Flash_fwd_params &, cudaStream_t) [with Kernel_traits=Flash_fwd_kernel_traits<64, 128, 64, 4, false, false, cutlass::bfloat16_t, Flash_kernel_traits<64, 128, 64, 4, cutlass::bfloat16_t>>, Is_dropout=true, Is_causal=true]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(68): here
            instantiation of "void run_mha_fwd_hdim64<T>(Flash_fwd_params &, cudaStream_t) [with T=cutlass::bfloat16_t]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_hdim64_bf16_sm80.cu(18): here

/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_kernel.h(140): warning: variable "MMA_M" was declared but never referenced
          detected during:
            instantiation of "void flash::compute_attn_1rowblock<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &, int, int, int) [with Kernel_traits=Flash_fwd_kernel_traits<64, 128, 64, 4, false, false, cutlass::bfloat16_t, Flash_kernel_traits<64, 128, 64, 4, cutlass::bfloat16_t>>, Is_dropout=true, Is_causal=true, Is_even_N=false, Is_even_K=true, Return_softmax=true, Params=Flash_fwd_params]" 
(588): here
            instantiation of "void flash::compute_attn<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &) [with Kernel_traits=Flash_fwd_kernel_traits<64, 128, 64, 4, false, false, cutlass::bfloat16_t, Flash_kernel_traits<64, 128, 64, 4, cutlass::bfloat16_t>>, Is_dropout=true, Is_causal=true, Is_even_N=false, Is_even_K=true, Return_softmax=true, Params=Flash_fwd_params]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(15): here
            instantiation of "void flash_fwd_kernel<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax>(Flash_fwd_params) [with Kernel_traits=Flash_fwd_kernel_traits<64, 128, 64, 4, false, false, cutlass::bfloat16_t, Flash_kernel_traits<64, 128, 64, 4, cutlass::bfloat16_t>>, Is_dropout=true, Is_causal=true, Is_even_N=false, Is_even_K=true, Return_softmax=true]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(34): here
            instantiation of "void run_flash_fwd<Kernel_traits,Is_dropout,Is_causal>(Flash_fwd_params &, cudaStream_t) [with Kernel_traits=Flash_fwd_kernel_traits<64, 128, 64, 4, false, false, cutlass::bfloat16_t, Flash_kernel_traits<64, 128, 64, 4, cutlass::bfloat16_t>>, Is_dropout=true, Is_causal=true]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(68): here
            instantiation of "void run_mha_fwd_hdim64<T>(Flash_fwd_params &, cudaStream_t) [with T=cutlass::bfloat16_t]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_hdim64_bf16_sm80.cu(18): here

/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_kernel.h(275): warning: variable "tQrQ" was declared but never referenced
          detected during:
            instantiation of "void flash::compute_attn_1rowblock<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &, int, int, int) [with Kernel_traits=Flash_fwd_kernel_traits<64, 128, 64, 4, false, false, cutlass::bfloat16_t, Flash_kernel_traits<64, 128, 64, 4, cutlass::bfloat16_t>>, Is_dropout=true, Is_causal=true, Is_even_N=false, Is_even_K=true, Return_softmax=true, Params=Flash_fwd_params]" 
(588): here
            instantiation of "void flash::compute_attn<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &) [with Kernel_traits=Flash_fwd_kernel_traits<64, 128, 64, 4, false, false, cutlass::bfloat16_t, Flash_kernel_traits<64, 128, 64, 4, cutlass::bfloat16_t>>, Is_dropout=true, Is_causal=true, Is_even_N=false, Is_even_K=true, Return_softmax=true, Params=Flash_fwd_params]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(15): here
            instantiation of "void flash_fwd_kernel<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax>(Flash_fwd_params) [with Kernel_traits=Flash_fwd_kernel_traits<64, 128, 64, 4, false, false, cutlass::bfloat16_t, Flash_kernel_traits<64, 128, 64, 4, cutlass::bfloat16_t>>, Is_dropout=true, Is_causal=true, Is_even_N=false, Is_even_K=true, Return_softmax=true]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(34): here
            instantiation of "void run_flash_fwd<Kernel_traits,Is_dropout,Is_causal>(Flash_fwd_params &, cudaStream_t) [with Kernel_traits=Flash_fwd_kernel_traits<64, 128, 64, 4, false, false, cutlass::bfloat16_t, Flash_kernel_traits<64, 128, 64, 4, cutlass::bfloat16_t>>, Is_dropout=true, Is_causal=true]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(68): here
            instantiation of "void run_mha_fwd_hdim64<T>(Flash_fwd_params &, cudaStream_t) [with T=cutlass::bfloat16_t]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_hdim64_bf16_sm80.cu(18): here

/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_kernel.h(140): warning: variable "MMA_M" was declared but never referenced
          detected during:
            instantiation of "void flash::compute_attn_1rowblock<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &, int, int, int) [with Kernel_traits=Flash_fwd_kernel_traits<64, 128, 64, 4, false, false, cutlass::bfloat16_t, Flash_kernel_traits<64, 128, 64, 4, cutlass::bfloat16_t>>, Is_dropout=true, Is_causal=true, Is_even_N=false, Is_even_K=true, Return_softmax=false, Params=Flash_fwd_params]" 
(588): here
            instantiation of "void flash::compute_attn<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &) [with Kernel_traits=Flash_fwd_kernel_traits<64, 128, 64, 4, false, false, cutlass::bfloat16_t, Flash_kernel_traits<64, 128, 64, 4, cutlass::bfloat16_t>>, Is_dropout=true, Is_causal=true, Is_even_N=false, Is_even_K=true, Return_softmax=false, Params=Flash_fwd_params]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(15): here
            instantiation of "void flash_fwd_kernel<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax>(Flash_fwd_params) [with Kernel_traits=Flash_fwd_kernel_traits<64, 128, 64, 4, false, false, cutlass::bfloat16_t, Flash_kernel_traits<64, 128, 64, 4, cutlass::bfloat16_t>>, Is_dropout=true, Is_causal=true, Is_even_N=false, Is_even_K=true, Return_softmax=false]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(34): here
            instantiation of "void run_flash_fwd<Kernel_traits,Is_dropout,Is_causal>(Flash_fwd_params &, cudaStream_t) [with Kernel_traits=Flash_fwd_kernel_traits<64, 128, 64, 4, false, false, cutlass::bfloat16_t, Flash_kernel_traits<64, 128, 64, 4, cutlass::bfloat16_t>>, Is_dropout=true, Is_causal=true]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(68): here
            instantiation of "void run_mha_fwd_hdim64<T>(Flash_fwd_params &, cudaStream_t) [with T=cutlass::bfloat16_t]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_hdim64_bf16_sm80.cu(18): here

/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_kernel.h(275): warning: variable "tQrQ" was declared but never referenced
          detected during:
            instantiation of "void flash::compute_attn_1rowblock<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &, int, int, int) [with Kernel_traits=Flash_fwd_kernel_traits<64, 128, 64, 4, false, false, cutlass::bfloat16_t, Flash_kernel_traits<64, 128, 64, 4, cutlass::bfloat16_t>>, Is_dropout=true, Is_causal=true, Is_even_N=false, Is_even_K=true, Return_softmax=false, Params=Flash_fwd_params]" 
(588): here
            instantiation of "void flash::compute_attn<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &) [with Kernel_traits=Flash_fwd_kernel_traits<64, 128, 64, 4, false, false, cutlass::bfloat16_t, Flash_kernel_traits<64, 128, 64, 4, cutlass::bfloat16_t>>, Is_dropout=true, Is_causal=true, Is_even_N=false, Is_even_K=true, Return_softmax=false, Params=Flash_fwd_params]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(15): here
            instantiation of "void flash_fwd_kernel<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax>(Flash_fwd_params) [with Kernel_traits=Flash_fwd_kernel_traits<64, 128, 64, 4, false, false, cutlass::bfloat16_t, Flash_kernel_traits<64, 128, 64, 4, cutlass::bfloat16_t>>, Is_dropout=true, Is_causal=true, Is_even_N=false, Is_even_K=true, Return_softmax=false]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(34): here
            instantiation of "void run_flash_fwd<Kernel_traits,Is_dropout,Is_causal>(Flash_fwd_params &, cudaStream_t) [with Kernel_traits=Flash_fwd_kernel_traits<64, 128, 64, 4, false, false, cutlass::bfloat16_t, Flash_kernel_traits<64, 128, 64, 4, cutlass::bfloat16_t>>, Is_dropout=true, Is_causal=true]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(68): here
            instantiation of "void run_mha_fwd_hdim64<T>(Flash_fwd_params &, cudaStream_t) [with T=cutlass::bfloat16_t]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_hdim64_bf16_sm80.cu(18): here

/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_kernel.h(140): warning: variable "MMA_M" was declared but never referenced
          detected during:
            instantiation of "void flash::compute_attn_1rowblock<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &, int, int, int) [with Kernel_traits=Flash_fwd_kernel_traits<64, 128, 64, 4, false, false, cutlass::bfloat16_t, Flash_kernel_traits<64, 128, 64, 4, cutlass::bfloat16_t>>, Is_dropout=true, Is_causal=true, Is_even_N=false, Is_even_K=false, Return_softmax=true, Params=Flash_fwd_params]" 
(588): here
            instantiation of "void flash::compute_attn<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &) [with Kernel_traits=Flash_fwd_kernel_traits<64, 128, 64, 4, false, false, cutlass::bfloat16_t, Flash_kernel_traits<64, 128, 64, 4, cutlass::bfloat16_t>>, Is_dropout=true, Is_causal=true, Is_even_N=false, Is_even_K=false, Return_softmax=true, Params=Flash_fwd_params]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(15): here
            instantiation of "void flash_fwd_kernel<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax>(Flash_fwd_params) [with Kernel_traits=Flash_fwd_kernel_traits<64, 128, 64, 4, false, false, cutlass::bfloat16_t, Flash_kernel_traits<64, 128, 64, 4, cutlass::bfloat16_t>>, Is_dropout=true, Is_causal=true, Is_even_N=false, Is_even_K=false, Return_softmax=true]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(34): here
            instantiation of "void run_flash_fwd<Kernel_traits,Is_dropout,Is_causal>(Flash_fwd_params &, cudaStream_t) [with Kernel_traits=Flash_fwd_kernel_traits<64, 128, 64, 4, false, false, cutlass::bfloat16_t, Flash_kernel_traits<64, 128, 64, 4, cutlass::bfloat16_t>>, Is_dropout=true, Is_causal=true]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(68): here
            instantiation of "void run_mha_fwd_hdim64<T>(Flash_fwd_params &, cudaStream_t) [with T=cutlass::bfloat16_t]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_hdim64_bf16_sm80.cu(18): here

/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_kernel.h(275): warning: variable "tQrQ" was declared but never referenced
          detected during:
            instantiation of "void flash::compute_attn_1rowblock<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &, int, int, int) [with Kernel_traits=Flash_fwd_kernel_traits<64, 128, 64, 4, false, false, cutlass::bfloat16_t, Flash_kernel_traits<64, 128, 64, 4, cutlass::bfloat16_t>>, Is_dropout=true, Is_causal=true, Is_even_N=false, Is_even_K=false, Return_softmax=true, Params=Flash_fwd_params]" 
(588): here
            instantiation of "void flash::compute_attn<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &) [with Kernel_traits=Flash_fwd_kernel_traits<64, 128, 64, 4, false, false, cutlass::bfloat16_t, Flash_kernel_traits<64, 128, 64, 4, cutlass::bfloat16_t>>, Is_dropout=true, Is_causal=true, Is_even_N=false, Is_even_K=false, Return_softmax=true, Params=Flash_fwd_params]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(15): here
            instantiation of "void flash_fwd_kernel<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax>(Flash_fwd_params) [with Kernel_traits=Flash_fwd_kernel_traits<64, 128, 64, 4, false, false, cutlass::bfloat16_t, Flash_kernel_traits<64, 128, 64, 4, cutlass::bfloat16_t>>, Is_dropout=true, Is_causal=true, Is_even_N=false, Is_even_K=false, Return_softmax=true]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(34): here
            instantiation of "void run_flash_fwd<Kernel_traits,Is_dropout,Is_causal>(Flash_fwd_params &, cudaStream_t) [with Kernel_traits=Flash_fwd_kernel_traits<64, 128, 64, 4, false, false, cutlass::bfloat16_t, Flash_kernel_traits<64, 128, 64, 4, cutlass::bfloat16_t>>, Is_dropout=true, Is_causal=true]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(68): here
            instantiation of "void run_mha_fwd_hdim64<T>(Flash_fwd_params &, cudaStream_t) [with T=cutlass::bfloat16_t]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_hdim64_bf16_sm80.cu(18): here

/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_kernel.h(140): warning: variable "MMA_M" was declared but never referenced
          detected during:
            instantiation of "void flash::compute_attn_1rowblock<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &, int, int, int) [with Kernel_traits=Flash_fwd_kernel_traits<64, 128, 64, 4, false, false, cutlass::bfloat16_t, Flash_kernel_traits<64, 128, 64, 4, cutlass::bfloat16_t>>, Is_dropout=true, Is_causal=true, Is_even_N=false, Is_even_K=false, Return_softmax=false, Params=Flash_fwd_params]" 
(588): here
            instantiation of "void flash::compute_attn<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &) [with Kernel_traits=Flash_fwd_kernel_traits<64, 128, 64, 4, false, false, cutlass::bfloat16_t, Flash_kernel_traits<64, 128, 64, 4, cutlass::bfloat16_t>>, Is_dropout=true, Is_causal=true, Is_even_N=false, Is_even_K=false, Return_softmax=false, Params=Flash_fwd_params]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(15): here
            instantiation of "void flash_fwd_kernel<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax>(Flash_fwd_params) [with Kernel_traits=Flash_fwd_kernel_traits<64, 128, 64, 4, false, false, cutlass::bfloat16_t, Flash_kernel_traits<64, 128, 64, 4, cutlass::bfloat16_t>>, Is_dropout=true, Is_causal=true, Is_even_N=false, Is_even_K=false, Return_softmax=false]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(34): here
            instantiation of "void run_flash_fwd<Kernel_traits,Is_dropout,Is_causal>(Flash_fwd_params &, cudaStream_t) [with Kernel_traits=Flash_fwd_kernel_traits<64, 128, 64, 4, false, false, cutlass::bfloat16_t, Flash_kernel_traits<64, 128, 64, 4, cutlass::bfloat16_t>>, Is_dropout=true, Is_causal=true]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(68): here
            instantiation of "void run_mha_fwd_hdim64<T>(Flash_fwd_params &, cudaStream_t) [with T=cutlass::bfloat16_t]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_hdim64_bf16_sm80.cu(18): here

/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_kernel.h(275): warning: variable "tQrQ" was declared but never referenced
          detected during:
            instantiation of "void flash::compute_attn_1rowblock<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &, int, int, int) [with Kernel_traits=Flash_fwd_kernel_traits<64, 128, 64, 4, false, false, cutlass::bfloat16_t, Flash_kernel_traits<64, 128, 64, 4, cutlass::bfloat16_t>>, Is_dropout=true, Is_causal=true, Is_even_N=false, Is_even_K=false, Return_softmax=false, Params=Flash_fwd_params]" 
(588): here
            instantiation of "void flash::compute_attn<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &) [with Kernel_traits=Flash_fwd_kernel_traits<64, 128, 64, 4, false, false, cutlass::bfloat16_t, Flash_kernel_traits<64, 128, 64, 4, cutlass::bfloat16_t>>, Is_dropout=true, Is_causal=true, Is_even_N=false, Is_even_K=false, Return_softmax=false, Params=Flash_fwd_params]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(15): here
            instantiation of "void flash_fwd_kernel<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax>(Flash_fwd_params) [with Kernel_traits=Flash_fwd_kernel_traits<64, 128, 64, 4, false, false, cutlass::bfloat16_t, Flash_kernel_traits<64, 128, 64, 4, cutlass::bfloat16_t>>, Is_dropout=true, Is_causal=true, Is_even_N=false, Is_even_K=false, Return_softmax=false]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(34): here
            instantiation of "void run_flash_fwd<Kernel_traits,Is_dropout,Is_causal>(Flash_fwd_params &, cudaStream_t) [with Kernel_traits=Flash_fwd_kernel_traits<64, 128, 64, 4, false, false, cutlass::bfloat16_t, Flash_kernel_traits<64, 128, 64, 4, cutlass::bfloat16_t>>, Is_dropout=true, Is_causal=true]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(68): here
            instantiation of "void run_mha_fwd_hdim64<T>(Flash_fwd_params &, cudaStream_t) [with T=cutlass::bfloat16_t]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_hdim64_bf16_sm80.cu(18): here

/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_kernel.h(140): warning: variable "MMA_M" was declared but never referenced
          detected during:
            instantiation of "void flash::compute_attn_1rowblock<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &, int, int, int) [with Kernel_traits=Flash_fwd_kernel_traits<64, 128, 64, 4, false, false, cutlass::bfloat16_t, Flash_kernel_traits<64, 128, 64, 4, cutlass::bfloat16_t>>, Is_dropout=true, Is_causal=false, Is_even_N=true, Is_even_K=true, Return_softmax=true, Params=Flash_fwd_params]" 
(588): here
            instantiation of "void flash::compute_attn<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &) [with Kernel_traits=Flash_fwd_kernel_traits<64, 128, 64, 4, false, false, cutlass::bfloat16_t, Flash_kernel_traits<64, 128, 64, 4, cutlass::bfloat16_t>>, Is_dropout=true, Is_causal=false, Is_even_N=true, Is_even_K=true, Return_softmax=true, Params=Flash_fwd_params]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(15): here
            instantiation of "void flash_fwd_kernel<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax>(Flash_fwd_params) [with Kernel_traits=Flash_fwd_kernel_traits<64, 128, 64, 4, false, false, cutlass::bfloat16_t, Flash_kernel_traits<64, 128, 64, 4, cutlass::bfloat16_t>>, Is_dropout=true, Is_causal=false, Is_even_N=true, Is_even_K=true, Return_softmax=true]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(34): here
            instantiation of "void run_flash_fwd<Kernel_traits,Is_dropout,Is_causal>(Flash_fwd_params &, cudaStream_t) [with Kernel_traits=Flash_fwd_kernel_traits<64, 128, 64, 4, false, false, cutlass::bfloat16_t, Flash_kernel_traits<64, 128, 64, 4, cutlass::bfloat16_t>>, Is_dropout=true, Is_causal=false]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(68): here
            instantiation of "void run_mha_fwd_hdim64<T>(Flash_fwd_params &, cudaStream_t) [with T=cutlass::bfloat16_t]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_hdim64_bf16_sm80.cu(18): here

/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_kernel.h(275): warning: variable "tQrQ" was declared but never referenced
          detected during:
            instantiation of "void flash::compute_attn_1rowblock<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &, int, int, int) [with Kernel_traits=Flash_fwd_kernel_traits<64, 128, 64, 4, false, false, cutlass::bfloat16_t, Flash_kernel_traits<64, 128, 64, 4, cutlass::bfloat16_t>>, Is_dropout=true, Is_causal=false, Is_even_N=true, Is_even_K=true, Return_softmax=true, Params=Flash_fwd_params]" 
(588): here
            instantiation of "void flash::compute_attn<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &) [with Kernel_traits=Flash_fwd_kernel_traits<64, 128, 64, 4, false, false, cutlass::bfloat16_t, Flash_kernel_traits<64, 128, 64, 4, cutlass::bfloat16_t>>, Is_dropout=true, Is_causal=false, Is_even_N=true, Is_even_K=true, Return_softmax=true, Params=Flash_fwd_params]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(15): here
            instantiation of "void flash_fwd_kernel<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax>(Flash_fwd_params) [with Kernel_traits=Flash_fwd_kernel_traits<64, 128, 64, 4, false, false, cutlass::bfloat16_t, Flash_kernel_traits<64, 128, 64, 4, cutlass::bfloat16_t>>, Is_dropout=true, Is_causal=false, Is_even_N=true, Is_even_K=true, Return_softmax=true]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(34): here
            instantiation of "void run_flash_fwd<Kernel_traits,Is_dropout,Is_causal>(Flash_fwd_params &, cudaStream_t) [with Kernel_traits=Flash_fwd_kernel_traits<64, 128, 64, 4, false, false, cutlass::bfloat16_t, Flash_kernel_traits<64, 128, 64, 4, cutlass::bfloat16_t>>, Is_dropout=true, Is_causal=false]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(68): here
            instantiation of "void run_mha_fwd_hdim64<T>(Flash_fwd_params &, cudaStream_t) [with T=cutlass::bfloat16_t]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_hdim64_bf16_sm80.cu(18): here

/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_kernel.h(140): warning: variable "MMA_M" was declared but never referenced
          detected during:
            instantiation of "void flash::compute_attn_1rowblock<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &, int, int, int) [with Kernel_traits=Flash_fwd_kernel_traits<64, 128, 64, 4, false, false, cutlass::bfloat16_t, Flash_kernel_traits<64, 128, 64, 4, cutlass::bfloat16_t>>, Is_dropout=true, Is_causal=false, Is_even_N=true, Is_even_K=true, Return_softmax=false, Params=Flash_fwd_params]" 
(588): here
            instantiation of "void flash::compute_attn<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &) [with Kernel_traits=Flash_fwd_kernel_traits<64, 128, 64, 4, false, false, cutlass::bfloat16_t, Flash_kernel_traits<64, 128, 64, 4, cutlass::bfloat16_t>>, Is_dropout=true, Is_causal=false, Is_even_N=true, Is_even_K=true, Return_softmax=false, Params=Flash_fwd_params]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(15): here
            instantiation of "void flash_fwd_kernel<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax>(Flash_fwd_params) [with Kernel_traits=Flash_fwd_kernel_traits<64, 128, 64, 4, false, false, cutlass::bfloat16_t, Flash_kernel_traits<64, 128, 64, 4, cutlass::bfloat16_t>>, Is_dropout=true, Is_causal=false, Is_even_N=true, Is_even_K=true, Return_softmax=false]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(34): here
            instantiation of "void run_flash_fwd<Kernel_traits,Is_dropout,Is_causal>(Flash_fwd_params &, cudaStream_t) [with Kernel_traits=Flash_fwd_kernel_traits<64, 128, 64, 4, false, false, cutlass::bfloat16_t, Flash_kernel_traits<64, 128, 64, 4, cutlass::bfloat16_t>>, Is_dropout=true, Is_causal=false]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(68): here
            instantiation of "void run_mha_fwd_hdim64<T>(Flash_fwd_params &, cudaStream_t) [with T=cutlass::bfloat16_t]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_hdim64_bf16_sm80.cu(18): here

/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_kernel.h(275): warning: variable "tQrQ" was declared but never referenced
          detected during:
            instantiation of "void flash::compute_attn_1rowblock<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &, int, int, int) [with Kernel_traits=Flash_fwd_kernel_traits<64, 128, 64, 4, false, false, cutlass::bfloat16_t, Flash_kernel_traits<64, 128, 64, 4, cutlass::bfloat16_t>>, Is_dropout=true, Is_causal=false, Is_even_N=true, Is_even_K=true, Return_softmax=false, Params=Flash_fwd_params]" 
(588): here
            instantiation of "void flash::compute_attn<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &) [with Kernel_traits=Flash_fwd_kernel_traits<64, 128, 64, 4, false, false, cutlass::bfloat16_t, Flash_kernel_traits<64, 128, 64, 4, cutlass::bfloat16_t>>, Is_dropout=true, Is_causal=false, Is_even_N=true, Is_even_K=true, Return_softmax=false, Params=Flash_fwd_params]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(15): here
            instantiation of "void flash_fwd_kernel<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax>(Flash_fwd_params) [with Kernel_traits=Flash_fwd_kernel_traits<64, 128, 64, 4, false, false, cutlass::bfloat16_t, Flash_kernel_traits<64, 128, 64, 4, cutlass::bfloat16_t>>, Is_dropout=true, Is_causal=false, Is_even_N=true, Is_even_K=true, Return_softmax=false]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(34): here
            instantiation of "void run_flash_fwd<Kernel_traits,Is_dropout,Is_causal>(Flash_fwd_params &, cudaStream_t) [with Kernel_traits=Flash_fwd_kernel_traits<64, 128, 64, 4, false, false, cutlass::bfloat16_t, Flash_kernel_traits<64, 128, 64, 4, cutlass::bfloat16_t>>, Is_dropout=true, Is_causal=false]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(68): here
            instantiation of "void run_mha_fwd_hdim64<T>(Flash_fwd_params &, cudaStream_t) [with T=cutlass::bfloat16_t]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_hdim64_bf16_sm80.cu(18): here

/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_kernel.h(140): warning: variable "MMA_M" was declared but never referenced
          detected during:
            instantiation of "void flash::compute_attn_1rowblock<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &, int, int, int) [with Kernel_traits=Flash_fwd_kernel_traits<64, 128, 64, 4, false, false, cutlass::bfloat16_t, Flash_kernel_traits<64, 128, 64, 4, cutlass::bfloat16_t>>, Is_dropout=true, Is_causal=false, Is_even_N=true, Is_even_K=false, Return_softmax=true, Params=Flash_fwd_params]" 
(588): here
            instantiation of "void flash::compute_attn<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &) [with Kernel_traits=Flash_fwd_kernel_traits<64, 128, 64, 4, false, false, cutlass::bfloat16_t, Flash_kernel_traits<64, 128, 64, 4, cutlass::bfloat16_t>>, Is_dropout=true, Is_causal=false, Is_even_N=true, Is_even_K=false, Return_softmax=true, Params=Flash_fwd_params]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(15): here
            instantiation of "void flash_fwd_kernel<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax>(Flash_fwd_params) [with Kernel_traits=Flash_fwd_kernel_traits<64, 128, 64, 4, false, false, cutlass::bfloat16_t, Flash_kernel_traits<64, 128, 64, 4, cutlass::bfloat16_t>>, Is_dropout=true, Is_causal=false, Is_even_N=true, Is_even_K=false, Return_softmax=true]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(34): here
            instantiation of "void run_flash_fwd<Kernel_traits,Is_dropout,Is_causal>(Flash_fwd_params &, cudaStream_t) [with Kernel_traits=Flash_fwd_kernel_traits<64, 128, 64, 4, false, false, cutlass::bfloat16_t, Flash_kernel_traits<64, 128, 64, 4, cutlass::bfloat16_t>>, Is_dropout=true, Is_causal=false]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(68): here
            instantiation of "void run_mha_fwd_hdim64<T>(Flash_fwd_params &, cudaStream_t) [with T=cutlass::bfloat16_t]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_hdim64_bf16_sm80.cu(18): here

/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_kernel.h(275): warning: variable "tQrQ" was declared but never referenced
          detected during:
            instantiation of "void flash::compute_attn_1rowblock<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &, int, int, int) [with Kernel_traits=Flash_fwd_kernel_traits<64, 128, 64, 4, false, false, cutlass::bfloat16_t, Flash_kernel_traits<64, 128, 64, 4, cutlass::bfloat16_t>>, Is_dropout=true, Is_causal=false, Is_even_N=true, Is_even_K=false, Return_softmax=true, Params=Flash_fwd_params]" 
(588): here
            instantiation of "void flash::compute_attn<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &) [with Kernel_traits=Flash_fwd_kernel_traits<64, 128, 64, 4, false, false, cutlass::bfloat16_t, Flash_kernel_traits<64, 128, 64, 4, cutlass::bfloat16_t>>, Is_dropout=true, Is_causal=false, Is_even_N=true, Is_even_K=false, Return_softmax=true, Params=Flash_fwd_params]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(15): here
            instantiation of "void flash_fwd_kernel<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax>(Flash_fwd_params) [with Kernel_traits=Flash_fwd_kernel_traits<64, 128, 64, 4, false, false, cutlass::bfloat16_t, Flash_kernel_traits<64, 128, 64, 4, cutlass::bfloat16_t>>, Is_dropout=true, Is_causal=false, Is_even_N=true, Is_even_K=false, Return_softmax=true]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(34): here
            instantiation of "void run_flash_fwd<Kernel_traits,Is_dropout,Is_causal>(Flash_fwd_params &, cudaStream_t) [with Kernel_traits=Flash_fwd_kernel_traits<64, 128, 64, 4, false, false, cutlass::bfloat16_t, Flash_kernel_traits<64, 128, 64, 4, cutlass::bfloat16_t>>, Is_dropout=true, Is_causal=false]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(68): here
            instantiation of "void run_mha_fwd_hdim64<T>(Flash_fwd_params &, cudaStream_t) [with T=cutlass::bfloat16_t]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_hdim64_bf16_sm80.cu(18): here

/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_kernel.h(140): warning: variable "MMA_M" was declared but never referenced
          detected during:
            instantiation of "void flash::compute_attn_1rowblock<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &, int, int, int) [with Kernel_traits=Flash_fwd_kernel_traits<64, 128, 64, 4, false, false, cutlass::bfloat16_t, Flash_kernel_traits<64, 128, 64, 4, cutlass::bfloat16_t>>, Is_dropout=true, Is_causal=false, Is_even_N=true, Is_even_K=false, Return_softmax=false, Params=Flash_fwd_params]" 
(588): here
            instantiation of "void flash::compute_attn<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &) [with Kernel_traits=Flash_fwd_kernel_traits<64, 128, 64, 4, false, false, cutlass::bfloat16_t, Flash_kernel_traits<64, 128, 64, 4, cutlass::bfloat16_t>>, Is_dropout=true, Is_causal=false, Is_even_N=true, Is_even_K=false, Return_softmax=false, Params=Flash_fwd_params]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(15): here
            instantiation of "void flash_fwd_kernel<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax>(Flash_fwd_params) [with Kernel_traits=Flash_fwd_kernel_traits<64, 128, 64, 4, false, false, cutlass::bfloat16_t, Flash_kernel_traits<64, 128, 64, 4, cutlass::bfloat16_t>>, Is_dropout=true, Is_causal=false, Is_even_N=true, Is_even_K=false, Return_softmax=false]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(34): here
            instantiation of "void run_flash_fwd<Kernel_traits,Is_dropout,Is_causal>(Flash_fwd_params &, cudaStream_t) [with Kernel_traits=Flash_fwd_kernel_traits<64, 128, 64, 4, false, false, cutlass::bfloat16_t, Flash_kernel_traits<64, 128, 64, 4, cutlass::bfloat16_t>>, Is_dropout=true, Is_causal=false]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(68): here
            instantiation of "void run_mha_fwd_hdim64<T>(Flash_fwd_params &, cudaStream_t) [with T=cutlass::bfloat16_t]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_hdim64_bf16_sm80.cu(18): here

/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_kernel.h(275): warning: variable "tQrQ" was declared but never referenced
          detected during:
            instantiation of "void flash::compute_attn_1rowblock<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &, int, int, int) [with Kernel_traits=Flash_fwd_kernel_traits<64, 128, 64, 4, false, false, cutlass::bfloat16_t, Flash_kernel_traits<64, 128, 64, 4, cutlass::bfloat16_t>>, Is_dropout=true, Is_causal=false, Is_even_N=true, Is_even_K=false, Return_softmax=false, Params=Flash_fwd_params]" 
(588): here
            instantiation of "void flash::compute_attn<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &) [with Kernel_traits=Flash_fwd_kernel_traits<64, 128, 64, 4, false, false, cutlass::bfloat16_t, Flash_kernel_traits<64, 128, 64, 4, cutlass::bfloat16_t>>, Is_dropout=true, Is_causal=false, Is_even_N=true, Is_even_K=false, Return_softmax=false, Params=Flash_fwd_params]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(15): here
            instantiation of "void flash_fwd_kernel<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax>(Flash_fwd_params) [with Kernel_traits=Flash_fwd_kernel_traits<64, 128, 64, 4, false, false, cutlass::bfloat16_t, Flash_kernel_traits<64, 128, 64, 4, cutlass::bfloat16_t>>, Is_dropout=true, Is_causal=false, Is_even_N=true, Is_even_K=false, Return_softmax=false]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(34): here
            instantiation of "void run_flash_fwd<Kernel_traits,Is_dropout,Is_causal>(Flash_fwd_params &, cudaStream_t) [with Kernel_traits=Flash_fwd_kernel_traits<64, 128, 64, 4, false, false, cutlass::bfloat16_t, Flash_kernel_traits<64, 128, 64, 4, cutlass::bfloat16_t>>, Is_dropout=true, Is_causal=false]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(68): here
            instantiation of "void run_mha_fwd_hdim64<T>(Flash_fwd_params &, cudaStream_t) [with T=cutlass::bfloat16_t]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_hdim64_bf16_sm80.cu(18): here

/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_kernel.h(140): warning: variable "MMA_M" was declared but never referenced
          detected during:
            instantiation of "void flash::compute_attn_1rowblock<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &, int, int, int) [with Kernel_traits=Flash_fwd_kernel_traits<64, 128, 64, 4, false, false, cutlass::bfloat16_t, Flash_kernel_traits<64, 128, 64, 4, cutlass::bfloat16_t>>, Is_dropout=true, Is_causal=false, Is_even_N=false, Is_even_K=true, Return_softmax=true, Params=Flash_fwd_params]" 
(588): here
            instantiation of "void flash::compute_attn<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &) [with Kernel_traits=Flash_fwd_kernel_traits<64, 128, 64, 4, false, false, cutlass::bfloat16_t, Flash_kernel_traits<64, 128, 64, 4, cutlass::bfloat16_t>>, Is_dropout=true, Is_causal=false, Is_even_N=false, Is_even_K=true, Return_softmax=true, Params=Flash_fwd_params]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(15): here
            instantiation of "void flash_fwd_kernel<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax>(Flash_fwd_params) [with Kernel_traits=Flash_fwd_kernel_traits<64, 128, 64, 4, false, false, cutlass::bfloat16_t, Flash_kernel_traits<64, 128, 64, 4, cutlass::bfloat16_t>>, Is_dropout=true, Is_causal=false, Is_even_N=false, Is_even_K=true, Return_softmax=true]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(34): here
            instantiation of "void run_flash_fwd<Kernel_traits,Is_dropout,Is_causal>(Flash_fwd_params &, cudaStream_t) [with Kernel_traits=Flash_fwd_kernel_traits<64, 128, 64, 4, false, false, cutlass::bfloat16_t, Flash_kernel_traits<64, 128, 64, 4, cutlass::bfloat16_t>>, Is_dropout=true, Is_causal=false]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(68): here
            instantiation of "void run_mha_fwd_hdim64<T>(Flash_fwd_params &, cudaStream_t) [with T=cutlass::bfloat16_t]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_hdim64_bf16_sm80.cu(18): here

/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_kernel.h(275): warning: variable "tQrQ" was declared but never referenced
          detected during:
            instantiation of "void flash::compute_attn_1rowblock<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &, int, int, int) [with Kernel_traits=Flash_fwd_kernel_traits<64, 128, 64, 4, false, false, cutlass::bfloat16_t, Flash_kernel_traits<64, 128, 64, 4, cutlass::bfloat16_t>>, Is_dropout=true, Is_causal=false, Is_even_N=false, Is_even_K=true, Return_softmax=true, Params=Flash_fwd_params]" 
(588): here
            instantiation of "void flash::compute_attn<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &) [with Kernel_traits=Flash_fwd_kernel_traits<64, 128, 64, 4, false, false, cutlass::bfloat16_t, Flash_kernel_traits<64, 128, 64, 4, cutlass::bfloat16_t>>, Is_dropout=true, Is_causal=false, Is_even_N=false, Is_even_K=true, Return_softmax=true, Params=Flash_fwd_params]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(15): here
            instantiation of "void flash_fwd_kernel<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax>(Flash_fwd_params) [with Kernel_traits=Flash_fwd_kernel_traits<64, 128, 64, 4, false, false, cutlass::bfloat16_t, Flash_kernel_traits<64, 128, 64, 4, cutlass::bfloat16_t>>, Is_dropout=true, Is_causal=false, Is_even_N=false, Is_even_K=true, Return_softmax=true]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(34): here
            instantiation of "void run_flash_fwd<Kernel_traits,Is_dropout,Is_causal>(Flash_fwd_params &, cudaStream_t) [with Kernel_traits=Flash_fwd_kernel_traits<64, 128, 64, 4, false, false, cutlass::bfloat16_t, Flash_kernel_traits<64, 128, 64, 4, cutlass::bfloat16_t>>, Is_dropout=true, Is_causal=false]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(68): here
            instantiation of "void run_mha_fwd_hdim64<T>(Flash_fwd_params &, cudaStream_t) [with T=cutlass::bfloat16_t]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_hdim64_bf16_sm80.cu(18): here

/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_kernel.h(140): warning: variable "MMA_M" was declared but never referenced
          detected during:
            instantiation of "void flash::compute_attn_1rowblock<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &, int, int, int) [with Kernel_traits=Flash_fwd_kernel_traits<64, 128, 64, 4, false, false, cutlass::bfloat16_t, Flash_kernel_traits<64, 128, 64, 4, cutlass::bfloat16_t>>, Is_dropout=true, Is_causal=false, Is_even_N=false, Is_even_K=true, Return_softmax=false, Params=Flash_fwd_params]" 
(588): here
            instantiation of "void flash::compute_attn<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &) [with Kernel_traits=Flash_fwd_kernel_traits<64, 128, 64, 4, false, false, cutlass::bfloat16_t, Flash_kernel_traits<64, 128, 64, 4, cutlass::bfloat16_t>>, Is_dropout=true, Is_causal=false, Is_even_N=false, Is_even_K=true, Return_softmax=false, Params=Flash_fwd_params]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(15): here
            instantiation of "void flash_fwd_kernel<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax>(Flash_fwd_params) [with Kernel_traits=Flash_fwd_kernel_traits<64, 128, 64, 4, false, false, cutlass::bfloat16_t, Flash_kernel_traits<64, 128, 64, 4, cutlass::bfloat16_t>>, Is_dropout=true, Is_causal=false, Is_even_N=false, Is_even_K=true, Return_softmax=false]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(34): here
            instantiation of "void run_flash_fwd<Kernel_traits,Is_dropout,Is_causal>(Flash_fwd_params &, cudaStream_t) [with Kernel_traits=Flash_fwd_kernel_traits<64, 128, 64, 4, false, false, cutlass::bfloat16_t, Flash_kernel_traits<64, 128, 64, 4, cutlass::bfloat16_t>>, Is_dropout=true, Is_causal=false]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(68): here
            instantiation of "void run_mha_fwd_hdim64<T>(Flash_fwd_params &, cudaStream_t) [with T=cutlass::bfloat16_t]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_hdim64_bf16_sm80.cu(18): here

/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_kernel.h(275): warning: variable "tQrQ" was declared but never referenced
          detected during:
            instantiation of "void flash::compute_attn_1rowblock<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &, int, int, int) [with Kernel_traits=Flash_fwd_kernel_traits<64, 128, 64, 4, false, false, cutlass::bfloat16_t, Flash_kernel_traits<64, 128, 64, 4, cutlass::bfloat16_t>>, Is_dropout=true, Is_causal=false, Is_even_N=false, Is_even_K=true, Return_softmax=false, Params=Flash_fwd_params]" 
(588): here
            instantiation of "void flash::compute_attn<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &) [with Kernel_traits=Flash_fwd_kernel_traits<64, 128, 64, 4, false, false, cutlass::bfloat16_t, Flash_kernel_traits<64, 128, 64, 4, cutlass::bfloat16_t>>, Is_dropout=true, Is_causal=false, Is_even_N=false, Is_even_K=true, Return_softmax=false, Params=Flash_fwd_params]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(15): here
            instantiation of "void flash_fwd_kernel<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax>(Flash_fwd_params) [with Kernel_traits=Flash_fwd_kernel_traits<64, 128, 64, 4, false, false, cutlass::bfloat16_t, Flash_kernel_traits<64, 128, 64, 4, cutlass::bfloat16_t>>, Is_dropout=true, Is_causal=false, Is_even_N=false, Is_even_K=true, Return_softmax=false]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(34): here
            instantiation of "void run_flash_fwd<Kernel_traits,Is_dropout,Is_causal>(Flash_fwd_params &, cudaStream_t) [with Kernel_traits=Flash_fwd_kernel_traits<64, 128, 64, 4, false, false, cutlass::bfloat16_t, Flash_kernel_traits<64, 128, 64, 4, cutlass::bfloat16_t>>, Is_dropout=true, Is_causal=false]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(68): here
            instantiation of "void run_mha_fwd_hdim64<T>(Flash_fwd_params &, cudaStream_t) [with T=cutlass::bfloat16_t]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_hdim64_bf16_sm80.cu(18): here

/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_kernel.h(140): warning: variable "MMA_M" was declared but never referenced
          detected during:
            instantiation of "void flash::compute_attn_1rowblock<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &, int, int, int) [with Kernel_traits=Flash_fwd_kernel_traits<64, 128, 64, 4, false, false, cutlass::bfloat16_t, Flash_kernel_traits<64, 128, 64, 4, cutlass::bfloat16_t>>, Is_dropout=true, Is_causal=false, Is_even_N=false, Is_even_K=false, Return_softmax=true, Params=Flash_fwd_params]" 
(588): here
            instantiation of "void flash::compute_attn<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &) [with Kernel_traits=Flash_fwd_kernel_traits<64, 128, 64, 4, false, false, cutlass::bfloat16_t, Flash_kernel_traits<64, 128, 64, 4, cutlass::bfloat16_t>>, Is_dropout=true, Is_causal=false, Is_even_N=false, Is_even_K=false, Return_softmax=true, Params=Flash_fwd_params]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(15): here
            instantiation of "void flash_fwd_kernel<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax>(Flash_fwd_params) [with Kernel_traits=Flash_fwd_kernel_traits<64, 128, 64, 4, false, false, cutlass::bfloat16_t, Flash_kernel_traits<64, 128, 64, 4, cutlass::bfloat16_t>>, Is_dropout=true, Is_causal=false, Is_even_N=false, Is_even_K=false, Return_softmax=true]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(34): here
            instantiation of "void run_flash_fwd<Kernel_traits,Is_dropout,Is_causal>(Flash_fwd_params &, cudaStream_t) [with Kernel_traits=Flash_fwd_kernel_traits<64, 128, 64, 4, false, false, cutlass::bfloat16_t, Flash_kernel_traits<64, 128, 64, 4, cutlass::bfloat16_t>>, Is_dropout=true, Is_causal=false]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(68): here
            instantiation of "void run_mha_fwd_hdim64<T>(Flash_fwd_params &, cudaStream_t) [with T=cutlass::bfloat16_t]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_hdim64_bf16_sm80.cu(18): here

/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_kernel.h(275): warning: variable "tQrQ" was declared but never referenced
          detected during:
            instantiation of "void flash::compute_attn_1rowblock<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &, int, int, int) [with Kernel_traits=Flash_fwd_kernel_traits<64, 128, 64, 4, false, false, cutlass::bfloat16_t, Flash_kernel_traits<64, 128, 64, 4, cutlass::bfloat16_t>>, Is_dropout=true, Is_causal=false, Is_even_N=false, Is_even_K=false, Return_softmax=true, Params=Flash_fwd_params]" 
(588): here
            instantiation of "void flash::compute_attn<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &) [with Kernel_traits=Flash_fwd_kernel_traits<64, 128, 64, 4, false, false, cutlass::bfloat16_t, Flash_kernel_traits<64, 128, 64, 4, cutlass::bfloat16_t>>, Is_dropout=true, Is_causal=false, Is_even_N=false, Is_even_K=false, Return_softmax=true, Params=Flash_fwd_params]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(15): here
            instantiation of "void flash_fwd_kernel<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax>(Flash_fwd_params) [with Kernel_traits=Flash_fwd_kernel_traits<64, 128, 64, 4, false, false, cutlass::bfloat16_t, Flash_kernel_traits<64, 128, 64, 4, cutlass::bfloat16_t>>, Is_dropout=true, Is_causal=false, Is_even_N=false, Is_even_K=false, Return_softmax=true]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(34): here
            instantiation of "void run_flash_fwd<Kernel_traits,Is_dropout,Is_causal>(Flash_fwd_params &, cudaStream_t) [with Kernel_traits=Flash_fwd_kernel_traits<64, 128, 64, 4, false, false, cutlass::bfloat16_t, Flash_kernel_traits<64, 128, 64, 4, cutlass::bfloat16_t>>, Is_dropout=true, Is_causal=false]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(68): here
            instantiation of "void run_mha_fwd_hdim64<T>(Flash_fwd_params &, cudaStream_t) [with T=cutlass::bfloat16_t]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_hdim64_bf16_sm80.cu(18): here

/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_kernel.h(140): warning: variable "MMA_M" was declared but never referenced
          detected during:
            instantiation of "void flash::compute_attn_1rowblock<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &, int, int, int) [with Kernel_traits=Flash_fwd_kernel_traits<64, 128, 64, 4, false, false, cutlass::bfloat16_t, Flash_kernel_traits<64, 128, 64, 4, cutlass::bfloat16_t>>, Is_dropout=true, Is_causal=false, Is_even_N=false, Is_even_K=false, Return_softmax=false, Params=Flash_fwd_params]" 
(588): here
            instantiation of "void flash::compute_attn<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &) [with Kernel_traits=Flash_fwd_kernel_traits<64, 128, 64, 4, false, false, cutlass::bfloat16_t, Flash_kernel_traits<64, 128, 64, 4, cutlass::bfloat16_t>>, Is_dropout=true, Is_causal=false, Is_even_N=false, Is_even_K=false, Return_softmax=false, Params=Flash_fwd_params]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(15): here
            instantiation of "void flash_fwd_kernel<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax>(Flash_fwd_params) [with Kernel_traits=Flash_fwd_kernel_traits<64, 128, 64, 4, false, false, cutlass::bfloat16_t, Flash_kernel_traits<64, 128, 64, 4, cutlass::bfloat16_t>>, Is_dropout=true, Is_causal=false, Is_even_N=false, Is_even_K=false, Return_softmax=false]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(34): here
            instantiation of "void run_flash_fwd<Kernel_traits,Is_dropout,Is_causal>(Flash_fwd_params &, cudaStream_t) [with Kernel_traits=Flash_fwd_kernel_traits<64, 128, 64, 4, false, false, cutlass::bfloat16_t, Flash_kernel_traits<64, 128, 64, 4, cutlass::bfloat16_t>>, Is_dropout=true, Is_causal=false]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(68): here
            instantiation of "void run_mha_fwd_hdim64<T>(Flash_fwd_params &, cudaStream_t) [with T=cutlass::bfloat16_t]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_hdim64_bf16_sm80.cu(18): here

/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_kernel.h(275): warning: variable "tQrQ" was declared but never referenced
          detected during:
            instantiation of "void flash::compute_attn_1rowblock<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &, int, int, int) [with Kernel_traits=Flash_fwd_kernel_traits<64, 128, 64, 4, false, false, cutlass::bfloat16_t, Flash_kernel_traits<64, 128, 64, 4, cutlass::bfloat16_t>>, Is_dropout=true, Is_causal=false, Is_even_N=false, Is_even_K=false, Return_softmax=false, Params=Flash_fwd_params]" 
(588): here
            instantiation of "void flash::compute_attn<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &) [with Kernel_traits=Flash_fwd_kernel_traits<64, 128, 64, 4, false, false, cutlass::bfloat16_t, Flash_kernel_traits<64, 128, 64, 4, cutlass::bfloat16_t>>, Is_dropout=true, Is_causal=false, Is_even_N=false, Is_even_K=false, Return_softmax=false, Params=Flash_fwd_params]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(15): here
            instantiation of "void flash_fwd_kernel<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax>(Flash_fwd_params) [with Kernel_traits=Flash_fwd_kernel_traits<64, 128, 64, 4, false, false, cutlass::bfloat16_t, Flash_kernel_traits<64, 128, 64, 4, cutlass::bfloat16_t>>, Is_dropout=true, Is_causal=false, Is_even_N=false, Is_even_K=false, Return_softmax=false]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(34): here
            instantiation of "void run_flash_fwd<Kernel_traits,Is_dropout,Is_causal>(Flash_fwd_params &, cudaStream_t) [with Kernel_traits=Flash_fwd_kernel_traits<64, 128, 64, 4, false, false, cutlass::bfloat16_t, Flash_kernel_traits<64, 128, 64, 4, cutlass::bfloat16_t>>, Is_dropout=true, Is_causal=false]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(68): here
            instantiation of "void run_mha_fwd_hdim64<T>(Flash_fwd_params &, cudaStream_t) [with T=cutlass::bfloat16_t]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_hdim64_bf16_sm80.cu(18): here

/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_kernel.h(140): warning: variable "MMA_M" was declared but never referenced
          detected during:
            instantiation of "void flash::compute_attn_1rowblock<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &, int, int, int) [with Kernel_traits=Flash_fwd_kernel_traits<64, 128, 128, 4, false, false, cutlass::bfloat16_t, Flash_kernel_traits<64, 128, 128, 4, cutlass::bfloat16_t>>, Is_dropout=false, Is_causal=true, Is_even_N=true, Is_even_K=true, Return_softmax=false, Params=Flash_fwd_params]" 
(588): here
            instantiation of "void flash::compute_attn<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &) [with Kernel_traits=Flash_fwd_kernel_traits<64, 128, 128, 4, false, false, cutlass::bfloat16_t, Flash_kernel_traits<64, 128, 128, 4, cutlass::bfloat16_t>>, Is_dropout=false, Is_causal=true, Is_even_N=true, Is_even_K=true, Return_softmax=false, Params=Flash_fwd_params]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(15): here
            instantiation of "void flash_fwd_kernel<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax>(Flash_fwd_params) [with Kernel_traits=Flash_fwd_kernel_traits<64, 128, 128, 4, false, false, cutlass::bfloat16_t, Flash_kernel_traits<64, 128, 128, 4, cutlass::bfloat16_t>>, Is_dropout=false, Is_causal=true, Is_even_N=true, Is_even_K=true, Return_softmax=false]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(34): here
            instantiation of "void run_flash_fwd<Kernel_traits,Is_dropout,Is_causal>(Flash_fwd_params &, cudaStream_t) [with Kernel_traits=Flash_fwd_kernel_traits<64, 128, 128, 4, false, false, cutlass::bfloat16_t, Flash_kernel_traits<64, 128, 128, 4, cutlass::bfloat16_t>>, Is_dropout=false, Is_causal=true]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(68): here
            instantiation of "void run_mha_fwd_hdim64<T>(Flash_fwd_params &, cudaStream_t) [with T=cutlass::bfloat16_t]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_hdim64_bf16_sm80.cu(18): here

/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_kernel.h(275): warning: variable "tQrQ" was declared but never referenced
          detected during:
            instantiation of "void flash::compute_attn_1rowblock<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &, int, int, int) [with Kernel_traits=Flash_fwd_kernel_traits<64, 128, 128, 4, false, false, cutlass::bfloat16_t, Flash_kernel_traits<64, 128, 128, 4, cutlass::bfloat16_t>>, Is_dropout=false, Is_causal=true, Is_even_N=true, Is_even_K=true, Return_softmax=false, Params=Flash_fwd_params]" 
(588): here
            instantiation of "void flash::compute_attn<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &) [with Kernel_traits=Flash_fwd_kernel_traits<64, 128, 128, 4, false, false, cutlass::bfloat16_t, Flash_kernel_traits<64, 128, 128, 4, cutlass::bfloat16_t>>, Is_dropout=false, Is_causal=true, Is_even_N=true, Is_even_K=true, Return_softmax=false, Params=Flash_fwd_params]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(15): here
            instantiation of "void flash_fwd_kernel<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax>(Flash_fwd_params) [with Kernel_traits=Flash_fwd_kernel_traits<64, 128, 128, 4, false, false, cutlass::bfloat16_t, Flash_kernel_traits<64, 128, 128, 4, cutlass::bfloat16_t>>, Is_dropout=false, Is_causal=true, Is_even_N=true, Is_even_K=true, Return_softmax=false]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(34): here
            instantiation of "void run_flash_fwd<Kernel_traits,Is_dropout,Is_causal>(Flash_fwd_params &, cudaStream_t) [with Kernel_traits=Flash_fwd_kernel_traits<64, 128, 128, 4, false, false, cutlass::bfloat16_t, Flash_kernel_traits<64, 128, 128, 4, cutlass::bfloat16_t>>, Is_dropout=false, Is_causal=true]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(68): here
            instantiation of "void run_mha_fwd_hdim64<T>(Flash_fwd_params &, cudaStream_t) [with T=cutlass::bfloat16_t]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_hdim64_bf16_sm80.cu(18): here

/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_kernel.h(140): warning: variable "MMA_M" was declared but never referenced
          detected during:
            instantiation of "void flash::compute_attn_1rowblock<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &, int, int, int) [with Kernel_traits=Flash_fwd_kernel_traits<64, 128, 128, 4, false, false, cutlass::bfloat16_t, Flash_kernel_traits<64, 128, 128, 4, cutlass::bfloat16_t>>, Is_dropout=false, Is_causal=true, Is_even_N=true, Is_even_K=false, Return_softmax=false, Params=Flash_fwd_params]" 
(588): here
            instantiation of "void flash::compute_attn<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &) [with Kernel_traits=Flash_fwd_kernel_traits<64, 128, 128, 4, false, false, cutlass::bfloat16_t, Flash_kernel_traits<64, 128, 128, 4, cutlass::bfloat16_t>>, Is_dropout=false, Is_causal=true, Is_even_N=true, Is_even_K=false, Return_softmax=false, Params=Flash_fwd_params]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(15): here
            instantiation of "void flash_fwd_kernel<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax>(Flash_fwd_params) [with Kernel_traits=Flash_fwd_kernel_traits<64, 128, 128, 4, false, false, cutlass::bfloat16_t, Flash_kernel_traits<64, 128, 128, 4, cutlass::bfloat16_t>>, Is_dropout=false, Is_causal=true, Is_even_N=true, Is_even_K=false, Return_softmax=false]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(34): here
            instantiation of "void run_flash_fwd<Kernel_traits,Is_dropout,Is_causal>(Flash_fwd_params &, cudaStream_t) [with Kernel_traits=Flash_fwd_kernel_traits<64, 128, 128, 4, false, false, cutlass::bfloat16_t, Flash_kernel_traits<64, 128, 128, 4, cutlass::bfloat16_t>>, Is_dropout=false, Is_causal=true]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(68): here
            instantiation of "void run_mha_fwd_hdim64<T>(Flash_fwd_params &, cudaStream_t) [with T=cutlass::bfloat16_t]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_hdim64_bf16_sm80.cu(18): here

/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_kernel.h(275): warning: variable "tQrQ" was declared but never referenced
          detected during:
            instantiation of "void flash::compute_attn_1rowblock<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &, int, int, int) [with Kernel_traits=Flash_fwd_kernel_traits<64, 128, 128, 4, false, false, cutlass::bfloat16_t, Flash_kernel_traits<64, 128, 128, 4, cutlass::bfloat16_t>>, Is_dropout=false, Is_causal=true, Is_even_N=true, Is_even_K=false, Return_softmax=false, Params=Flash_fwd_params]" 
(588): here
            instantiation of "void flash::compute_attn<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &) [with Kernel_traits=Flash_fwd_kernel_traits<64, 128, 128, 4, false, false, cutlass::bfloat16_t, Flash_kernel_traits<64, 128, 128, 4, cutlass::bfloat16_t>>, Is_dropout=false, Is_causal=true, Is_even_N=true, Is_even_K=false, Return_softmax=false, Params=Flash_fwd_params]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(15): here
            instantiation of "void flash_fwd_kernel<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax>(Flash_fwd_params) [with Kernel_traits=Flash_fwd_kernel_traits<64, 128, 128, 4, false, false, cutlass::bfloat16_t, Flash_kernel_traits<64, 128, 128, 4, cutlass::bfloat16_t>>, Is_dropout=false, Is_causal=true, Is_even_N=true, Is_even_K=false, Return_softmax=false]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(34): here
            instantiation of "void run_flash_fwd<Kernel_traits,Is_dropout,Is_causal>(Flash_fwd_params &, cudaStream_t) [with Kernel_traits=Flash_fwd_kernel_traits<64, 128, 128, 4, false, false, cutlass::bfloat16_t, Flash_kernel_traits<64, 128, 128, 4, cutlass::bfloat16_t>>, Is_dropout=false, Is_causal=true]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(68): here
            instantiation of "void run_mha_fwd_hdim64<T>(Flash_fwd_params &, cudaStream_t) [with T=cutlass::bfloat16_t]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_hdim64_bf16_sm80.cu(18): here

/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_kernel.h(140): warning: variable "MMA_M" was declared but never referenced
          detected during:
            instantiation of "void flash::compute_attn_1rowblock<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &, int, int, int) [with Kernel_traits=Flash_fwd_kernel_traits<64, 128, 128, 4, false, false, cutlass::bfloat16_t, Flash_kernel_traits<64, 128, 128, 4, cutlass::bfloat16_t>>, Is_dropout=false, Is_causal=true, Is_even_N=false, Is_even_K=true, Return_softmax=false, Params=Flash_fwd_params]" 
(588): here
            instantiation of "void flash::compute_attn<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &) [with Kernel_traits=Flash_fwd_kernel_traits<64, 128, 128, 4, false, false, cutlass::bfloat16_t, Flash_kernel_traits<64, 128, 128, 4, cutlass::bfloat16_t>>, Is_dropout=false, Is_causal=true, Is_even_N=false, Is_even_K=true, Return_softmax=false, Params=Flash_fwd_params]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(15): here
            instantiation of "void flash_fwd_kernel<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax>(Flash_fwd_params) [with Kernel_traits=Flash_fwd_kernel_traits<64, 128, 128, 4, false, false, cutlass::bfloat16_t, Flash_kernel_traits<64, 128, 128, 4, cutlass::bfloat16_t>>, Is_dropout=false, Is_causal=true, Is_even_N=false, Is_even_K=true, Return_softmax=false]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(34): here
            instantiation of "void run_flash_fwd<Kernel_traits,Is_dropout,Is_causal>(Flash_fwd_params &, cudaStream_t) [with Kernel_traits=Flash_fwd_kernel_traits<64, 128, 128, 4, false, false, cutlass::bfloat16_t, Flash_kernel_traits<64, 128, 128, 4, cutlass::bfloat16_t>>, Is_dropout=false, Is_causal=true]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(68): here
            instantiation of "void run_mha_fwd_hdim64<T>(Flash_fwd_params &, cudaStream_t) [with T=cutlass::bfloat16_t]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_hdim64_bf16_sm80.cu(18): here

/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_kernel.h(275): warning: variable "tQrQ" was declared but never referenced
          detected during:
            instantiation of "void flash::compute_attn_1rowblock<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &, int, int, int) [with Kernel_traits=Flash_fwd_kernel_traits<64, 128, 128, 4, false, false, cutlass::bfloat16_t, Flash_kernel_traits<64, 128, 128, 4, cutlass::bfloat16_t>>, Is_dropout=false, Is_causal=true, Is_even_N=false, Is_even_K=true, Return_softmax=false, Params=Flash_fwd_params]" 
(588): here
            instantiation of "void flash::compute_attn<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &) [with Kernel_traits=Flash_fwd_kernel_traits<64, 128, 128, 4, false, false, cutlass::bfloat16_t, Flash_kernel_traits<64, 128, 128, 4, cutlass::bfloat16_t>>, Is_dropout=false, Is_causal=true, Is_even_N=false, Is_even_K=true, Return_softmax=false, Params=Flash_fwd_params]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(15): here
            instantiation of "void flash_fwd_kernel<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax>(Flash_fwd_params) [with Kernel_traits=Flash_fwd_kernel_traits<64, 128, 128, 4, false, false, cutlass::bfloat16_t, Flash_kernel_traits<64, 128, 128, 4, cutlass::bfloat16_t>>, Is_dropout=false, Is_causal=true, Is_even_N=false, Is_even_K=true, Return_softmax=false]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(34): here
            instantiation of "void run_flash_fwd<Kernel_traits,Is_dropout,Is_causal>(Flash_fwd_params &, cudaStream_t) [with Kernel_traits=Flash_fwd_kernel_traits<64, 128, 128, 4, false, false, cutlass::bfloat16_t, Flash_kernel_traits<64, 128, 128, 4, cutlass::bfloat16_t>>, Is_dropout=false, Is_causal=true]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(68): here
            instantiation of "void run_mha_fwd_hdim64<T>(Flash_fwd_params &, cudaStream_t) [with T=cutlass::bfloat16_t]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_hdim64_bf16_sm80.cu(18): here

/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_kernel.h(140): warning: variable "MMA_M" was declared but never referenced
          detected during:
            instantiation of "void flash::compute_attn_1rowblock<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &, int, int, int) [with Kernel_traits=Flash_fwd_kernel_traits<64, 128, 128, 4, false, false, cutlass::bfloat16_t, Flash_kernel_traits<64, 128, 128, 4, cutlass::bfloat16_t>>, Is_dropout=false, Is_causal=true, Is_even_N=false, Is_even_K=false, Return_softmax=false, Params=Flash_fwd_params]" 
(588): here
            instantiation of "void flash::compute_attn<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &) [with Kernel_traits=Flash_fwd_kernel_traits<64, 128, 128, 4, false, false, cutlass::bfloat16_t, Flash_kernel_traits<64, 128, 128, 4, cutlass::bfloat16_t>>, Is_dropout=false, Is_causal=true, Is_even_N=false, Is_even_K=false, Return_softmax=false, Params=Flash_fwd_params]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(15): here
            instantiation of "void flash_fwd_kernel<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax>(Flash_fwd_params) [with Kernel_traits=Flash_fwd_kernel_traits<64, 128, 128, 4, false, false, cutlass::bfloat16_t, Flash_kernel_traits<64, 128, 128, 4, cutlass::bfloat16_t>>, Is_dropout=false, Is_causal=true, Is_even_N=false, Is_even_K=false, Return_softmax=false]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(34): here
            instantiation of "void run_flash_fwd<Kernel_traits,Is_dropout,Is_causal>(Flash_fwd_params &, cudaStream_t) [with Kernel_traits=Flash_fwd_kernel_traits<64, 128, 128, 4, false, false, cutlass::bfloat16_t, Flash_kernel_traits<64, 128, 128, 4, cutlass::bfloat16_t>>, Is_dropout=false, Is_causal=true]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(68): here
            instantiation of "void run_mha_fwd_hdim64<T>(Flash_fwd_params &, cudaStream_t) [with T=cutlass::bfloat16_t]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_hdim64_bf16_sm80.cu(18): here

/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_kernel.h(275): warning: variable "tQrQ" was declared but never referenced
          detected during:
            instantiation of "void flash::compute_attn_1rowblock<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &, int, int, int) [with Kernel_traits=Flash_fwd_kernel_traits<64, 128, 128, 4, false, false, cutlass::bfloat16_t, Flash_kernel_traits<64, 128, 128, 4, cutlass::bfloat16_t>>, Is_dropout=false, Is_causal=true, Is_even_N=false, Is_even_K=false, Return_softmax=false, Params=Flash_fwd_params]" 
(588): here
            instantiation of "void flash::compute_attn<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &) [with Kernel_traits=Flash_fwd_kernel_traits<64, 128, 128, 4, false, false, cutlass::bfloat16_t, Flash_kernel_traits<64, 128, 128, 4, cutlass::bfloat16_t>>, Is_dropout=false, Is_causal=true, Is_even_N=false, Is_even_K=false, Return_softmax=false, Params=Flash_fwd_params]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(15): here
            instantiation of "void flash_fwd_kernel<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax>(Flash_fwd_params) [with Kernel_traits=Flash_fwd_kernel_traits<64, 128, 128, 4, false, false, cutlass::bfloat16_t, Flash_kernel_traits<64, 128, 128, 4, cutlass::bfloat16_t>>, Is_dropout=false, Is_causal=true, Is_even_N=false, Is_even_K=false, Return_softmax=false]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(34): here
            instantiation of "void run_flash_fwd<Kernel_traits,Is_dropout,Is_causal>(Flash_fwd_params &, cudaStream_t) [with Kernel_traits=Flash_fwd_kernel_traits<64, 128, 128, 4, false, false, cutlass::bfloat16_t, Flash_kernel_traits<64, 128, 128, 4, cutlass::bfloat16_t>>, Is_dropout=false, Is_causal=true]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(68): here
            instantiation of "void run_mha_fwd_hdim64<T>(Flash_fwd_params &, cudaStream_t) [with T=cutlass::bfloat16_t]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_hdim64_bf16_sm80.cu(18): here

/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_kernel.h(140): warning: variable "MMA_M" was declared but never referenced
          detected during:
            instantiation of "void flash::compute_attn_1rowblock<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &, int, int, int) [with Kernel_traits=Flash_fwd_kernel_traits<64, 128, 128, 4, false, false, cutlass::bfloat16_t, Flash_kernel_traits<64, 128, 128, 4, cutlass::bfloat16_t>>, Is_dropout=false, Is_causal=false, Is_even_N=true, Is_even_K=true, Return_softmax=false, Params=Flash_fwd_params]" 
(588): here
            instantiation of "void flash::compute_attn<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &) [with Kernel_traits=Flash_fwd_kernel_traits<64, 128, 128, 4, false, false, cutlass::bfloat16_t, Flash_kernel_traits<64, 128, 128, 4, cutlass::bfloat16_t>>, Is_dropout=false, Is_causal=false, Is_even_N=true, Is_even_K=true, Return_softmax=false, Params=Flash_fwd_params]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(15): here
            instantiation of "void flash_fwd_kernel<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax>(Flash_fwd_params) [with Kernel_traits=Flash_fwd_kernel_traits<64, 128, 128, 4, false, false, cutlass::bfloat16_t, Flash_kernel_traits<64, 128, 128, 4, cutlass::bfloat16_t>>, Is_dropout=false, Is_causal=false, Is_even_N=true, Is_even_K=true, Return_softmax=false]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(34): here
            instantiation of "void run_flash_fwd<Kernel_traits,Is_dropout,Is_causal>(Flash_fwd_params &, cudaStream_t) [with Kernel_traits=Flash_fwd_kernel_traits<64, 128, 128, 4, false, false, cutlass::bfloat16_t, Flash_kernel_traits<64, 128, 128, 4, cutlass::bfloat16_t>>, Is_dropout=false, Is_causal=false]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(68): here
            instantiation of "void run_mha_fwd_hdim64<T>(Flash_fwd_params &, cudaStream_t) [with T=cutlass::bfloat16_t]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_hdim64_bf16_sm80.cu(18): here

/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_kernel.h(275): warning: variable "tQrQ" was declared but never referenced
          detected during:
            instantiation of "void flash::compute_attn_1rowblock<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &, int, int, int) [with Kernel_traits=Flash_fwd_kernel_traits<64, 128, 128, 4, false, false, cutlass::bfloat16_t, Flash_kernel_traits<64, 128, 128, 4, cutlass::bfloat16_t>>, Is_dropout=false, Is_causal=false, Is_even_N=true, Is_even_K=true, Return_softmax=false, Params=Flash_fwd_params]" 
(588): here
            instantiation of "void flash::compute_attn<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &) [with Kernel_traits=Flash_fwd_kernel_traits<64, 128, 128, 4, false, false, cutlass::bfloat16_t, Flash_kernel_traits<64, 128, 128, 4, cutlass::bfloat16_t>>, Is_dropout=false, Is_causal=false, Is_even_N=true, Is_even_K=true, Return_softmax=false, Params=Flash_fwd_params]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(15): here
            instantiation of "void flash_fwd_kernel<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax>(Flash_fwd_params) [with Kernel_traits=Flash_fwd_kernel_traits<64, 128, 128, 4, false, false, cutlass::bfloat16_t, Flash_kernel_traits<64, 128, 128, 4, cutlass::bfloat16_t>>, Is_dropout=false, Is_causal=false, Is_even_N=true, Is_even_K=true, Return_softmax=false]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(34): here
            instantiation of "void run_flash_fwd<Kernel_traits,Is_dropout,Is_causal>(Flash_fwd_params &, cudaStream_t) [with Kernel_traits=Flash_fwd_kernel_traits<64, 128, 128, 4, false, false, cutlass::bfloat16_t, Flash_kernel_traits<64, 128, 128, 4, cutlass::bfloat16_t>>, Is_dropout=false, Is_causal=false]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(68): here
            instantiation of "void run_mha_fwd_hdim64<T>(Flash_fwd_params &, cudaStream_t) [with T=cutlass::bfloat16_t]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_hdim64_bf16_sm80.cu(18): here

/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_kernel.h(140): warning: variable "MMA_M" was declared but never referenced
          detected during:
            instantiation of "void flash::compute_attn_1rowblock<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &, int, int, int) [with Kernel_traits=Flash_fwd_kernel_traits<64, 128, 128, 4, false, false, cutlass::bfloat16_t, Flash_kernel_traits<64, 128, 128, 4, cutlass::bfloat16_t>>, Is_dropout=false, Is_causal=false, Is_even_N=true, Is_even_K=false, Return_softmax=false, Params=Flash_fwd_params]" 
(588): here
            instantiation of "void flash::compute_attn<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &) [with Kernel_traits=Flash_fwd_kernel_traits<64, 128, 128, 4, false, false, cutlass::bfloat16_t, Flash_kernel_traits<64, 128, 128, 4, cutlass::bfloat16_t>>, Is_dropout=false, Is_causal=false, Is_even_N=true, Is_even_K=false, Return_softmax=false, Params=Flash_fwd_params]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(15): here
            instantiation of "void flash_fwd_kernel<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax>(Flash_fwd_params) [with Kernel_traits=Flash_fwd_kernel_traits<64, 128, 128, 4, false, false, cutlass::bfloat16_t, Flash_kernel_traits<64, 128, 128, 4, cutlass::bfloat16_t>>, Is_dropout=false, Is_causal=false, Is_even_N=true, Is_even_K=false, Return_softmax=false]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(34): here
            instantiation of "void run_flash_fwd<Kernel_traits,Is_dropout,Is_causal>(Flash_fwd_params &, cudaStream_t) [with Kernel_traits=Flash_fwd_kernel_traits<64, 128, 128, 4, false, false, cutlass::bfloat16_t, Flash_kernel_traits<64, 128, 128, 4, cutlass::bfloat16_t>>, Is_dropout=false, Is_causal=false]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(68): here
            instantiation of "void run_mha_fwd_hdim64<T>(Flash_fwd_params &, cudaStream_t) [with T=cutlass::bfloat16_t]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_hdim64_bf16_sm80.cu(18): here

/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_kernel.h(275): warning: variable "tQrQ" was declared but never referenced
          detected during:
            instantiation of "void flash::compute_attn_1rowblock<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &, int, int, int) [with Kernel_traits=Flash_fwd_kernel_traits<64, 128, 128, 4, false, false, cutlass::bfloat16_t, Flash_kernel_traits<64, 128, 128, 4, cutlass::bfloat16_t>>, Is_dropout=false, Is_causal=false, Is_even_N=true, Is_even_K=false, Return_softmax=false, Params=Flash_fwd_params]" 
(588): here
            instantiation of "void flash::compute_attn<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &) [with Kernel_traits=Flash_fwd_kernel_traits<64, 128, 128, 4, false, false, cutlass::bfloat16_t, Flash_kernel_traits<64, 128, 128, 4, cutlass::bfloat16_t>>, Is_dropout=false, Is_causal=false, Is_even_N=true, Is_even_K=false, Return_softmax=false, Params=Flash_fwd_params]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(15): here
            instantiation of "void flash_fwd_kernel<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax>(Flash_fwd_params) [with Kernel_traits=Flash_fwd_kernel_traits<64, 128, 128, 4, false, false, cutlass::bfloat16_t, Flash_kernel_traits<64, 128, 128, 4, cutlass::bfloat16_t>>, Is_dropout=false, Is_causal=false, Is_even_N=true, Is_even_K=false, Return_softmax=false]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(34): here
            instantiation of "void run_flash_fwd<Kernel_traits,Is_dropout,Is_causal>(Flash_fwd_params &, cudaStream_t) [with Kernel_traits=Flash_fwd_kernel_traits<64, 128, 128, 4, false, false, cutlass::bfloat16_t, Flash_kernel_traits<64, 128, 128, 4, cutlass::bfloat16_t>>, Is_dropout=false, Is_causal=false]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(68): here
            instantiation of "void run_mha_fwd_hdim64<T>(Flash_fwd_params &, cudaStream_t) [with T=cutlass::bfloat16_t]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_hdim64_bf16_sm80.cu(18): here

/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_kernel.h(140): warning: variable "MMA_M" was declared but never referenced
          detected during:
            instantiation of "void flash::compute_attn_1rowblock<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &, int, int, int) [with Kernel_traits=Flash_fwd_kernel_traits<64, 128, 128, 4, false, false, cutlass::bfloat16_t, Flash_kernel_traits<64, 128, 128, 4, cutlass::bfloat16_t>>, Is_dropout=false, Is_causal=false, Is_even_N=false, Is_even_K=true, Return_softmax=false, Params=Flash_fwd_params]" 
(588): here
            instantiation of "void flash::compute_attn<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &) [with Kernel_traits=Flash_fwd_kernel_traits<64, 128, 128, 4, false, false, cutlass::bfloat16_t, Flash_kernel_traits<64, 128, 128, 4, cutlass::bfloat16_t>>, Is_dropout=false, Is_causal=false, Is_even_N=false, Is_even_K=true, Return_softmax=false, Params=Flash_fwd_params]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(15): here
            instantiation of "void flash_fwd_kernel<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax>(Flash_fwd_params) [with Kernel_traits=Flash_fwd_kernel_traits<64, 128, 128, 4, false, false, cutlass::bfloat16_t, Flash_kernel_traits<64, 128, 128, 4, cutlass::bfloat16_t>>, Is_dropout=false, Is_causal=false, Is_even_N=false, Is_even_K=true, Return_softmax=false]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(34): here
            instantiation of "void run_flash_fwd<Kernel_traits,Is_dropout,Is_causal>(Flash_fwd_params &, cudaStream_t) [with Kernel_traits=Flash_fwd_kernel_traits<64, 128, 128, 4, false, false, cutlass::bfloat16_t, Flash_kernel_traits<64, 128, 128, 4, cutlass::bfloat16_t>>, Is_dropout=false, Is_causal=false]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(68): here
            instantiation of "void run_mha_fwd_hdim64<T>(Flash_fwd_params &, cudaStream_t) [with T=cutlass::bfloat16_t]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_hdim64_bf16_sm80.cu(18): here

/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_kernel.h(275): warning: variable "tQrQ" was declared but never referenced
          detected during:
            instantiation of "void flash::compute_attn_1rowblock<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &, int, int, int) [with Kernel_traits=Flash_fwd_kernel_traits<64, 128, 128, 4, false, false, cutlass::bfloat16_t, Flash_kernel_traits<64, 128, 128, 4, cutlass::bfloat16_t>>, Is_dropout=false, Is_causal=false, Is_even_N=false, Is_even_K=true, Return_softmax=false, Params=Flash_fwd_params]" 
(588): here
            instantiation of "void flash::compute_attn<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &) [with Kernel_traits=Flash_fwd_kernel_traits<64, 128, 128, 4, false, false, cutlass::bfloat16_t, Flash_kernel_traits<64, 128, 128, 4, cutlass::bfloat16_t>>, Is_dropout=false, Is_causal=false, Is_even_N=false, Is_even_K=true, Return_softmax=false, Params=Flash_fwd_params]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(15): here
            instantiation of "void flash_fwd_kernel<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax>(Flash_fwd_params) [with Kernel_traits=Flash_fwd_kernel_traits<64, 128, 128, 4, false, false, cutlass::bfloat16_t, Flash_kernel_traits<64, 128, 128, 4, cutlass::bfloat16_t>>, Is_dropout=false, Is_causal=false, Is_even_N=false, Is_even_K=true, Return_softmax=false]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(34): here
            instantiation of "void run_flash_fwd<Kernel_traits,Is_dropout,Is_causal>(Flash_fwd_params &, cudaStream_t) [with Kernel_traits=Flash_fwd_kernel_traits<64, 128, 128, 4, false, false, cutlass::bfloat16_t, Flash_kernel_traits<64, 128, 128, 4, cutlass::bfloat16_t>>, Is_dropout=false, Is_causal=false]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(68): here
            instantiation of "void run_mha_fwd_hdim64<T>(Flash_fwd_params &, cudaStream_t) [with T=cutlass::bfloat16_t]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_hdim64_bf16_sm80.cu(18): here

/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_kernel.h(140): warning: variable "MMA_M" was declared but never referenced
          detected during:
            instantiation of "void flash::compute_attn_1rowblock<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &, int, int, int) [with Kernel_traits=Flash_fwd_kernel_traits<64, 128, 128, 4, false, false, cutlass::bfloat16_t, Flash_kernel_traits<64, 128, 128, 4, cutlass::bfloat16_t>>, Is_dropout=false, Is_causal=false, Is_even_N=false, Is_even_K=false, Return_softmax=false, Params=Flash_fwd_params]" 
(588): here
            instantiation of "void flash::compute_attn<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &) [with Kernel_traits=Flash_fwd_kernel_traits<64, 128, 128, 4, false, false, cutlass::bfloat16_t, Flash_kernel_traits<64, 128, 128, 4, cutlass::bfloat16_t>>, Is_dropout=false, Is_causal=false, Is_even_N=false, Is_even_K=false, Return_softmax=false, Params=Flash_fwd_params]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(15): here
            instantiation of "void flash_fwd_kernel<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax>(Flash_fwd_params) [with Kernel_traits=Flash_fwd_kernel_traits<64, 128, 128, 4, false, false, cutlass::bfloat16_t, Flash_kernel_traits<64, 128, 128, 4, cutlass::bfloat16_t>>, Is_dropout=false, Is_causal=false, Is_even_N=false, Is_even_K=false, Return_softmax=false]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(34): here
            instantiation of "void run_flash_fwd<Kernel_traits,Is_dropout,Is_causal>(Flash_fwd_params &, cudaStream_t) [with Kernel_traits=Flash_fwd_kernel_traits<64, 128, 128, 4, false, false, cutlass::bfloat16_t, Flash_kernel_traits<64, 128, 128, 4, cutlass::bfloat16_t>>, Is_dropout=false, Is_causal=false]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(68): here
            instantiation of "void run_mha_fwd_hdim64<T>(Flash_fwd_params &, cudaStream_t) [with T=cutlass::bfloat16_t]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_hdim64_bf16_sm80.cu(18): here

/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_kernel.h(275): warning: variable "tQrQ" was declared but never referenced
          detected during:
            instantiation of "void flash::compute_attn_1rowblock<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &, int, int, int) [with Kernel_traits=Flash_fwd_kernel_traits<64, 128, 128, 4, false, false, cutlass::bfloat16_t, Flash_kernel_traits<64, 128, 128, 4, cutlass::bfloat16_t>>, Is_dropout=false, Is_causal=false, Is_even_N=false, Is_even_K=false, Return_softmax=false, Params=Flash_fwd_params]" 
(588): here
            instantiation of "void flash::compute_attn<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &) [with Kernel_traits=Flash_fwd_kernel_traits<64, 128, 128, 4, false, false, cutlass::bfloat16_t, Flash_kernel_traits<64, 128, 128, 4, cutlass::bfloat16_t>>, Is_dropout=false, Is_causal=false, Is_even_N=false, Is_even_K=false, Return_softmax=false, Params=Flash_fwd_params]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(15): here
            instantiation of "void flash_fwd_kernel<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax>(Flash_fwd_params) [with Kernel_traits=Flash_fwd_kernel_traits<64, 128, 128, 4, false, false, cutlass::bfloat16_t, Flash_kernel_traits<64, 128, 128, 4, cutlass::bfloat16_t>>, Is_dropout=false, Is_causal=false, Is_even_N=false, Is_even_K=false, Return_softmax=false]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(34): here
            instantiation of "void run_flash_fwd<Kernel_traits,Is_dropout,Is_causal>(Flash_fwd_params &, cudaStream_t) [with Kernel_traits=Flash_fwd_kernel_traits<64, 128, 128, 4, false, false, cutlass::bfloat16_t, Flash_kernel_traits<64, 128, 128, 4, cutlass::bfloat16_t>>, Is_dropout=false, Is_causal=false]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(68): here
            instantiation of "void run_mha_fwd_hdim64<T>(Flash_fwd_params &, cudaStream_t) [with T=cutlass::bfloat16_t]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_hdim64_bf16_sm80.cu(18): here

ptxas info    : 2 bytes gmem
ptxas info    : Compiling entry function '_Z16flash_fwd_kernelI23Flash_fwd_kernel_traitsILi64ELi128ELi128ELi4ELb0ELb0EN7cutlass10bfloat16_tE19Flash_kernel_traitsILi64ELi128ELi128ELi4ES2_EELb0ELb0ELb0ELb0ELb0EEv16Flash_fwd_params' for 'sm_80'
ptxas info    : Function properties for _Z16flash_fwd_kernelI23Flash_fwd_kernel_traitsILi64ELi128ELi128ELi4ELb0ELb0EN7cutlass10bfloat16_tE19Flash_kernel_traitsILi64ELi128ELi128ELi4ES2_EELb0ELb0ELb0ELb0ELb0EEv16Flash_fwd_params
    40 bytes stack frame, 56 bytes spill stores, 56 bytes spill loads
ptxas info    : Used 255 registers, 584 bytes cmem[0]
ptxas info    : Compiling entry function '_Z16flash_fwd_kernelI23Flash_fwd_kernel_traitsILi64ELi128ELi128ELi4ELb0ELb0EN7cutlass10bfloat16_tE19Flash_kernel_traitsILi64ELi128ELi128ELi4ES2_EELb0ELb0ELb0ELb1ELb0EEv16Flash_fwd_params' for 'sm_80'
ptxas info    : Function properties for _Z16flash_fwd_kernelI23Flash_fwd_kernel_traitsILi64ELi128ELi128ELi4ELb0ELb0EN7cutlass10bfloat16_tE19Flash_kernel_traitsILi64ELi128ELi128ELi4ES2_EELb0ELb0ELb0ELb1ELb0EEv16Flash_fwd_params
    48 bytes stack frame, 76 bytes spill stores, 60 bytes spill loads
ptxas info    : Used 255 registers, 584 bytes cmem[0]
ptxas info    : Compiling entry function '_Z16flash_fwd_kernelI23Flash_fwd_kernel_traitsILi64ELi128ELi128ELi4ELb0ELb0EN7cutlass10bfloat16_tE19Flash_kernel_traitsILi64ELi128ELi128ELi4ES2_EELb0ELb0ELb1ELb0ELb0EEv16Flash_fwd_params' for 'sm_80'
ptxas info    : Function properties for _Z16flash_fwd_kernelI23Flash_fwd_kernel_traitsILi64ELi128ELi128ELi4ELb0ELb0EN7cutlass10bfloat16_tE19Flash_kernel_traitsILi64ELi128ELi128ELi4ES2_EELb0ELb0ELb1ELb0ELb0EEv16Flash_fwd_params
    40 bytes stack frame, 48 bytes spill stores, 48 bytes spill loads
ptxas info    : Used 255 registers, 584 bytes cmem[0]
ptxas info    : Compiling entry function '_Z16flash_fwd_kernelI23Flash_fwd_kernel_traitsILi64ELi128ELi128ELi4ELb0ELb0EN7cutlass10bfloat16_tE19Flash_kernel_traitsILi64ELi128ELi128ELi4ES2_EELb0ELb0ELb1ELb1ELb0EEv16Flash_fwd_params' for 'sm_80'
ptxas info    : Function properties for _Z16flash_fwd_kernelI23Flash_fwd_kernel_traitsILi64ELi128ELi128ELi4ELb0ELb0EN7cutlass10bfloat16_tE19Flash_kernel_traitsILi64ELi128ELi128ELi4ES2_EELb0ELb0ELb1ELb1ELb0EEv16Flash_fwd_params
    40 bytes stack frame, 52 bytes spill stores, 52 bytes spill loads
ptxas info    : Used 255 registers, 584 bytes cmem[0]
ptxas info    : Compiling entry function '_Z16flash_fwd_kernelI23Flash_fwd_kernel_traitsILi64ELi128ELi128ELi4ELb0ELb0EN7cutlass10bfloat16_tE19Flash_kernel_traitsILi64ELi128ELi128ELi4ES2_EELb0ELb1ELb0ELb0ELb0EEv16Flash_fwd_params' for 'sm_80'
ptxas info    : Function properties for _Z16flash_fwd_kernelI23Flash_fwd_kernel_traitsILi64ELi128ELi128ELi4ELb0ELb0EN7cutlass10bfloat16_tE19Flash_kernel_traitsILi64ELi128ELi128ELi4ES2_EELb0ELb1ELb0ELb0ELb0EEv16Flash_fwd_params
    40 bytes stack frame, 56 bytes spill stores, 56 bytes spill loads
ptxas info    : Used 255 registers, 584 bytes cmem[0]
ptxas info    : Compiling entry function '_Z16flash_fwd_kernelI23Flash_fwd_kernel_traitsILi64ELi128ELi128ELi4ELb0ELb0EN7cutlass10bfloat16_tE19Flash_kernel_traitsILi64ELi128ELi128ELi4ES2_EELb0ELb1ELb0ELb1ELb0EEv16Flash_fwd_params' for 'sm_80'
ptxas info    : Function properties for _Z16flash_fwd_kernelI23Flash_fwd_kernel_traitsILi64ELi128ELi128ELi4ELb0ELb0EN7cutlass10bfloat16_tE19Flash_kernel_traitsILi64ELi128ELi128ELi4ES2_EELb0ELb1ELb0ELb1ELb0EEv16Flash_fwd_params
    48 bytes stack frame, 80 bytes spill stores, 64 bytes spill loads
ptxas info    : Used 255 registers, 584 bytes cmem[0]
ptxas info    : Compiling entry function '_Z16flash_fwd_kernelI23Flash_fwd_kernel_traitsILi64ELi128ELi128ELi4ELb0ELb0EN7cutlass10bfloat16_tE19Flash_kernel_traitsILi64ELi128ELi128ELi4ES2_EELb0ELb1ELb1ELb0ELb0EEv16Flash_fwd_params' for 'sm_80'
ptxas info    : Function properties for _Z16flash_fwd_kernelI23Flash_fwd_kernel_traitsILi64ELi128ELi128ELi4ELb0ELb0EN7cutlass10bfloat16_tE19Flash_kernel_traitsILi64ELi128ELi128ELi4ES2_EELb0ELb1ELb1ELb0ELb0EEv16Flash_fwd_params
    40 bytes stack frame, 56 bytes spill stores, 56 bytes spill loads
ptxas info    : Used 255 registers, 584 bytes cmem[0]
ptxas info    : Compiling entry function '_Z16flash_fwd_kernelI23Flash_fwd_kernel_traitsILi64ELi128ELi128ELi4ELb0ELb0EN7cutlass10bfloat16_tE19Flash_kernel_traitsILi64ELi128ELi128ELi4ES2_EELb0ELb1ELb1ELb1ELb0EEv16Flash_fwd_params' for 'sm_80'
ptxas info    : Function properties for _Z16flash_fwd_kernelI23Flash_fwd_kernel_traitsILi64ELi128ELi128ELi4ELb0ELb0EN7cutlass10bfloat16_tE19Flash_kernel_traitsILi64ELi128ELi128ELi4ES2_EELb0ELb1ELb1ELb1ELb0EEv16Flash_fwd_params
    40 bytes stack frame, 52 bytes spill stores, 52 bytes spill loads
ptxas info    : Used 255 registers, 584 bytes cmem[0]
ptxas info    : Compiling entry function '_Z16flash_fwd_kernelI23Flash_fwd_kernel_traitsILi64ELi128ELi64ELi4ELb0ELb0EN7cutlass10bfloat16_tE19Flash_kernel_traitsILi64ELi128ELi64ELi4ES2_EELb1ELb0ELb0ELb0ELb0EEv16Flash_fwd_params' for 'sm_80'
ptxas info    : Function properties for _Z16flash_fwd_kernelI23Flash_fwd_kernel_traitsILi64ELi128ELi64ELi4ELb0ELb0EN7cutlass10bfloat16_tE19Flash_kernel_traitsILi64ELi128ELi64ELi4ES2_EELb1ELb0ELb0ELb0ELb0EEv16Flash_fwd_params
    0 bytes stack frame, 0 bytes spill stores, 0 bytes spill loads
ptxas info    : Used 242 registers, 584 bytes cmem[0]
ptxas info    : Compiling entry function '_Z16flash_fwd_kernelI23Flash_fwd_kernel_traitsILi64ELi128ELi64ELi4ELb0ELb0EN7cutlass10bfloat16_tE19Flash_kernel_traitsILi64ELi128ELi64ELi4ES2_EELb1ELb0ELb0ELb0ELb1EEv16Flash_fwd_params' for 'sm_80'
ptxas info    : Function properties for _Z16flash_fwd_kernelI23Flash_fwd_kernel_traitsILi64ELi128ELi64ELi4ELb0ELb0EN7cutlass10bfloat16_tE19Flash_kernel_traitsILi64ELi128ELi64ELi4ES2_EELb1ELb0ELb0ELb0ELb1EEv16Flash_fwd_params
    48 bytes stack frame, 44 bytes spill stores, 44 bytes spill loads
ptxas info    : Used 255 registers, 584 bytes cmem[0]
ptxas info    : Compiling entry function '_Z16flash_fwd_kernelI23Flash_fwd_kernel_traitsILi64ELi128ELi64ELi4ELb0ELb0EN7cutlass10bfloat16_tE19Flash_kernel_traitsILi64ELi128ELi64ELi4ES2_EELb1ELb0ELb0ELb1ELb0EEv16Flash_fwd_params' for 'sm_80'
ptxas info    : Function properties for _Z16flash_fwd_kernelI23Flash_fwd_kernel_traitsILi64ELi128ELi64ELi4ELb0ELb0EN7cutlass10bfloat16_tE19Flash_kernel_traitsILi64ELi128ELi64ELi4ES2_EELb1ELb0ELb0ELb1ELb0EEv16Flash_fwd_params
    0 bytes stack frame, 0 bytes spill stores, 0 bytes spill loads
ptxas info    : Used 236 registers, 584 bytes cmem[0]
ptxas info    : Compiling entry function '_Z16flash_fwd_kernelI23Flash_fwd_kernel_traitsILi64ELi128ELi64ELi4ELb0ELb0EN7cutlass10bfloat16_tE19Flash_kernel_traitsILi64ELi128ELi64ELi4ES2_EELb1ELb0ELb0ELb1ELb1EEv16Flash_fwd_params' for 'sm_80'
ptxas info    : Function properties for _Z16flash_fwd_kernelI23Flash_fwd_kernel_traitsILi64ELi128ELi64ELi4ELb0ELb0EN7cutlass10bfloat16_tE19Flash_kernel_traitsILi64ELi128ELi64ELi4ES2_EELb1ELb0ELb0ELb1ELb1EEv16Flash_fwd_params
    32 bytes stack frame, 32 bytes spill stores, 32 bytes spill loads
ptxas info    : Used 255 registers, 584 bytes cmem[0]
ptxas info    : Compiling entry function '_Z16flash_fwd_kernelI23Flash_fwd_kernel_traitsILi64ELi128ELi64ELi4ELb0ELb0EN7cutlass10bfloat16_tE19Flash_kernel_traitsILi64ELi128ELi64ELi4ES2_EELb1ELb0ELb1ELb0ELb0EEv16Flash_fwd_params' for 'sm_80'
ptxas info    : Function properties for _Z16flash_fwd_kernelI23Flash_fwd_kernel_traitsILi64ELi128ELi64ELi4ELb0ELb0EN7cutlass10bfloat16_tE19Flash_kernel_traitsILi64ELi128ELi64ELi4ES2_EELb1ELb0ELb1ELb0ELb0EEv16Flash_fwd_params
    0 bytes stack frame, 0 bytes spill stores, 0 bytes spill loads
ptxas info    : Used 236 registers, 584 bytes cmem[0]
ptxas info    : Compiling entry function '_Z16flash_fwd_kernelI23Flash_fwd_kernel_traitsILi64ELi128ELi64ELi4ELb0ELb0EN7cutlass10bfloat16_tE19Flash_kernel_traitsILi64ELi128ELi64ELi4ES2_EELb1ELb0ELb1ELb0ELb1EEv16Flash_fwd_params' for 'sm_80'
ptxas info    : Function properties for _Z16flash_fwd_kernelI23Flash_fwd_kernel_traitsILi64ELi128ELi64ELi4ELb0ELb0EN7cutlass10bfloat16_tE19Flash_kernel_traitsILi64ELi128ELi64ELi4ES2_EELb1ELb0ELb1ELb0ELb1EEv16Flash_fwd_params
    40 bytes stack frame, 40 bytes spill stores, 40 bytes spill loads
ptxas info    : Used 255 registers, 584 bytes cmem[0]
ptxas info    : Compiling entry function '_Z16flash_fwd_kernelI23Flash_fwd_kernel_traitsILi64ELi128ELi64ELi4ELb0ELb0EN7cutlass10bfloat16_tE19Flash_kernel_traitsILi64ELi128ELi64ELi4ES2_EELb1ELb0ELb1ELb1ELb0EEv16Flash_fwd_params' for 'sm_80'
ptxas info    : Function properties for _Z16flash_fwd_kernelI23Flash_fwd_kernel_traitsILi64ELi128ELi64ELi4ELb0ELb0EN7cutlass10bfloat16_tE19Flash_kernel_traitsILi64ELi128ELi64ELi4ES2_EELb1ELb0ELb1ELb1ELb0EEv16Flash_fwd_params
    0 bytes stack frame, 0 bytes spill stores, 0 bytes spill loads
ptxas info    : Used 238 registers, 584 bytes cmem[0]
ptxas info    : Compiling entry function '_Z16flash_fwd_kernelI23Flash_fwd_kernel_traitsILi64ELi128ELi64ELi4ELb0ELb0EN7cutlass10bfloat16_tE19Flash_kernel_traitsILi64ELi128ELi64ELi4ES2_EELb1ELb0ELb1ELb1ELb1EEv16Flash_fwd_params' for 'sm_80'
ptxas info    : Function properties for _Z16flash_fwd_kernelI23Flash_fwd_kernel_traitsILi64ELi128ELi64ELi4ELb0ELb0EN7cutlass10bfloat16_tE19Flash_kernel_traitsILi64ELi128ELi64ELi4ES2_EELb1ELb0ELb1ELb1ELb1EEv16Flash_fwd_params
    0 bytes stack frame, 0 bytes spill stores, 0 bytes spill loads
ptxas info    : Used 255 registers, 584 bytes cmem[0]
ptxas info    : Compiling entry function '_Z16flash_fwd_kernelI23Flash_fwd_kernel_traitsILi64ELi128ELi64ELi4ELb0ELb0EN7cutlass10bfloat16_tE19Flash_kernel_traitsILi64ELi128ELi64ELi4ES2_EELb1ELb1ELb0ELb0ELb0EEv16Flash_fwd_params' for 'sm_80'
ptxas info    : Function properties for _Z16flash_fwd_kernelI23Flash_fwd_kernel_traitsILi64ELi128ELi64ELi4ELb0ELb0EN7cutlass10bfloat16_tE19Flash_kernel_traitsILi64ELi128ELi64ELi4ES2_EELb1ELb1ELb0ELb0ELb0EEv16Flash_fwd_params
    0 bytes stack frame, 0 bytes spill stores, 0 bytes spill loads
ptxas info    : Used 250 registers, 584 bytes cmem[0]
ptxas info    : Compiling entry function '_Z16flash_fwd_kernelI23Flash_fwd_kernel_traitsILi64ELi128ELi64ELi4ELb0ELb0EN7cutlass10bfloat16_tE19Flash_kernel_traitsILi64ELi128ELi64ELi4ES2_EELb1ELb1ELb0ELb0ELb1EEv16Flash_fwd_params' for 'sm_80'
ptxas info    : Function properties for _Z16flash_fwd_kernelI23Flash_fwd_kernel_traitsILi64ELi128ELi64ELi4ELb0ELb0EN7cutlass10bfloat16_tE19Flash_kernel_traitsILi64ELi128ELi64ELi4ES2_EELb1ELb1ELb0ELb0ELb1EEv16Flash_fwd_params
    40 bytes stack frame, 36 bytes spill stores, 36 bytes spill loads
ptxas info    : Used 255 registers, 584 bytes cmem[0]
ptxas info    : Compiling entry function '_Z16flash_fwd_kernelI23Flash_fwd_kernel_traitsILi64ELi128ELi64ELi4ELb0ELb0EN7cutlass10bfloat16_tE19Flash_kernel_traitsILi64ELi128ELi64ELi4ES2_EELb1ELb1ELb0ELb1ELb0EEv16Flash_fwd_params' for 'sm_80'
ptxas info    : Function properties for _Z16flash_fwd_kernelI23Flash_fwd_kernel_traitsILi64ELi128ELi64ELi4ELb0ELb0EN7cutlass10bfloat16_tE19Flash_kernel_traitsILi64ELi128ELi64ELi4ES2_EELb1ELb1ELb0ELb1ELb0EEv16Flash_fwd_params
    0 bytes stack frame, 0 bytes spill stores, 0 bytes spill loads
ptxas info    : Used 244 registers, 584 bytes cmem[0]
ptxas info    : Compiling entry function '_Z16flash_fwd_kernelI23Flash_fwd_kernel_traitsILi64ELi128ELi64ELi4ELb0ELb0EN7cutlass10bfloat16_tE19Flash_kernel_traitsILi64ELi128ELi64ELi4ES2_EELb1ELb1ELb0ELb1ELb1EEv16Flash_fwd_params' for 'sm_80'
ptxas info    : Function properties for _Z16flash_fwd_kernelI23Flash_fwd_kernel_traitsILi64ELi128ELi64ELi4ELb0ELb0EN7cutlass10bfloat16_tE19Flash_kernel_traitsILi64ELi128ELi64ELi4ES2_EELb1ELb1ELb0ELb1ELb1EEv16Flash_fwd_params
    56 bytes stack frame, 56 bytes spill stores, 56 bytes spill loads
ptxas info    : Used 255 registers, 584 bytes cmem[0]
ptxas info    : Compiling entry function '_Z16flash_fwd_kernelI23Flash_fwd_kernel_traitsILi64ELi128ELi64ELi4ELb0ELb0EN7cutlass10bfloat16_tE19Flash_kernel_traitsILi64ELi128ELi64ELi4ES2_EELb1ELb1ELb1ELb0ELb0EEv16Flash_fwd_params' for 'sm_80'
ptxas info    : Function properties for _Z16flash_fwd_kernelI23Flash_fwd_kernel_traitsILi64ELi128ELi64ELi4ELb0ELb0EN7cutlass10bfloat16_tE19Flash_kernel_traitsILi64ELi128ELi64ELi4ES2_EELb1ELb1ELb1ELb0ELb0EEv16Flash_fwd_params
    0 bytes stack frame, 0 bytes spill stores, 0 bytes spill loads
ptxas info    : Used 248 registers, 584 bytes cmem[0]
ptxas info    : Compiling entry function '_Z16flash_fwd_kernelI23Flash_fwd_kernel_traitsILi64ELi128ELi64ELi4ELb0ELb0EN7cutlass10bfloat16_tE19Flash_kernel_traitsILi64ELi128ELi64ELi4ES2_EELb1ELb1ELb1ELb0ELb1EEv16Flash_fwd_params' for 'sm_80'
ptxas info    : Function properties for _Z16flash_fwd_kernelI23Flash_fwd_kernel_traitsILi64ELi128ELi64ELi4ELb0ELb0EN7cutlass10bfloat16_tE19Flash_kernel_traitsILi64ELi128ELi64ELi4ES2_EELb1ELb1ELb1ELb0ELb1EEv16Flash_fwd_params
    32 bytes stack frame, 32 bytes spill stores, 32 bytes spill loads
ptxas info    : Used 255 registers, 584 bytes cmem[0]
ptxas info    : Compiling entry function '_Z16flash_fwd_kernelI23Flash_fwd_kernel_traitsILi64ELi128ELi64ELi4ELb0ELb0EN7cutlass10bfloat16_tE19Flash_kernel_traitsILi64ELi128ELi64ELi4ES2_EELb1ELb1ELb1ELb1ELb0EEv16Flash_fwd_params' for 'sm_80'
ptxas info    : Function properties for _Z16flash_fwd_kernelI23Flash_fwd_kernel_traitsILi64ELi128ELi64ELi4ELb0ELb0EN7cutlass10bfloat16_tE19Flash_kernel_traitsILi64ELi128ELi64ELi4ES2_EELb1ELb1ELb1ELb1ELb0EEv16Flash_fwd_params
    0 bytes stack frame, 0 bytes spill stores, 0 bytes spill loads
ptxas info    : Used 246 registers, 584 bytes cmem[0]
ptxas info    : Compiling entry function '_Z16flash_fwd_kernelI23Flash_fwd_kernel_traitsILi64ELi128ELi64ELi4ELb0ELb0EN7cutlass10bfloat16_tE19Flash_kernel_traitsILi64ELi128ELi64ELi4ES2_EELb1ELb1ELb1ELb1ELb1EEv16Flash_fwd_params' for 'sm_80'
ptxas info    : Function properties for _Z16flash_fwd_kernelI23Flash_fwd_kernel_traitsILi64ELi128ELi64ELi4ELb0ELb0EN7cutlass10bfloat16_tE19Flash_kernel_traitsILi64ELi128ELi64ELi4ES2_EELb1ELb1ELb1ELb1ELb1EEv16Flash_fwd_params
    24 bytes stack frame, 24 bytes spill stores, 24 bytes spill loads
ptxas info    : Used 255 registers, 584 bytes cmem[0]
/anaconda/envs/flashattention/lib/python3.9/site-packages/torch/include/c10/core/SymInt.h(84): warning: integer conversion resulted in a change of sign

[7/13] /usr/local/cuda-11.4/bin/nvcc  -I/home/v-feiychen/flash-attention/csrc/flash_attn -I/home/v-feiychen/flash-attention/csrc/flash_attn/src -I/home/v-feiychen/flash-attention/csrc/cutlass/include -I/anaconda/envs/flashattention/lib/python3.9/site-packages/torch/include -I/anaconda/envs/flashattention/lib/python3.9/site-packages/torch/include/torch/csrc/api/include -I/anaconda/envs/flashattention/lib/python3.9/site-packages/torch/include/TH -I/anaconda/envs/flashattention/lib/python3.9/site-packages/torch/include/THC -I/usr/local/cuda-11.4/include -I/anaconda/envs/flashattention/include/python3.9 -c -c /home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_hdim96_fp16_sm80.cu -o /home/v-feiychen/flash-attention/build/temp.linux-x86_64-cpython-39/csrc/flash_attn/src/flash_fwd_hdim96_fp16_sm80.o -D__CUDA_NO_HALF_OPERATORS__ -D__CUDA_NO_HALF_CONVERSIONS__ -D__CUDA_NO_BFLOAT16_CONVERSIONS__ -D__CUDA_NO_HALF2_OPERATORS__ --expt-relaxed-constexpr --compiler-options ''"'"'-fPIC'"'"'' -O3 -std=c++17 -U__CUDA_NO_HALF_OPERATORS__ -U__CUDA_NO_HALF_CONVERSIONS__ -U__CUDA_NO_HALF2_OPERATORS__ -U__CUDA_NO_BFLOAT16_CONVERSIONS__ --expt-relaxed-constexpr --expt-extended-lambda --use_fast_math --ptxas-options=-v -lineinfo -gencode arch=compute_80,code=sm_80 --threads 4 -DTORCH_API_INCLUDE_EXTENSION_H '-DPYBIND11_COMPILER_TYPE="_gcc"' '-DPYBIND11_STDLIB="_libstdcpp"' '-DPYBIND11_BUILD_ABI="_cxxabi1011"' -DTORCH_EXTENSION_NAME=flash_attn_2_cuda -D_GLIBCXX_USE_CXX11_ABI=0
/anaconda/envs/flashattention/lib/python3.9/site-packages/torch/include/c10/core/SymInt.h(84): warning: integer conversion resulted in a change of sign

/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_kernel.h(140): warning: variable "MMA_M" was declared but never referenced
          detected during:
            instantiation of "void flash::compute_attn_1rowblock<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &, int, int, int) [with Kernel_traits=Flash_fwd_kernel_traits<96, 64, 64, 4, false, false, cutlass::half_t, Flash_kernel_traits<96, 64, 64, 4, cutlass::half_t>>, Is_dropout=true, Is_causal=true, Is_even_N=true, Is_even_K=true, Return_softmax=true, Params=Flash_fwd_params]" 
(588): here
            instantiation of "void flash::compute_attn<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &) [with Kernel_traits=Flash_fwd_kernel_traits<96, 64, 64, 4, false, false, cutlass::half_t, Flash_kernel_traits<96, 64, 64, 4, cutlass::half_t>>, Is_dropout=true, Is_causal=true, Is_even_N=true, Is_even_K=true, Return_softmax=true, Params=Flash_fwd_params]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(15): here
            instantiation of "void flash_fwd_kernel<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax>(Flash_fwd_params) [with Kernel_traits=Flash_fwd_kernel_traits<96, 64, 64, 4, false, false, cutlass::half_t, Flash_kernel_traits<96, 64, 64, 4, cutlass::half_t>>, Is_dropout=true, Is_causal=true, Is_even_N=true, Is_even_K=true, Return_softmax=true]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(34): here
            instantiation of "void run_flash_fwd<Kernel_traits,Is_dropout,Is_causal>(Flash_fwd_params &, cudaStream_t) [with Kernel_traits=Flash_fwd_kernel_traits<96, 64, 64, 4, false, false, cutlass::half_t, Flash_kernel_traits<96, 64, 64, 4, cutlass::half_t>>, Is_dropout=true, Is_causal=true]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(92): here
            instantiation of "void run_mha_fwd_hdim96<T>(Flash_fwd_params &, cudaStream_t) [with T=cutlass::half_t]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_hdim96_fp16_sm80.cu(22): here

/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_kernel.h(275): warning: variable "tQrQ" was declared but never referenced
          detected during:
            instantiation of "void flash::compute_attn_1rowblock<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &, int, int, int) [with Kernel_traits=Flash_fwd_kernel_traits<96, 64, 64, 4, false, false, cutlass::half_t, Flash_kernel_traits<96, 64, 64, 4, cutlass::half_t>>, Is_dropout=true, Is_causal=true, Is_even_N=true, Is_even_K=true, Return_softmax=true, Params=Flash_fwd_params]" 
(588): here
            instantiation of "void flash::compute_attn<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &) [with Kernel_traits=Flash_fwd_kernel_traits<96, 64, 64, 4, false, false, cutlass::half_t, Flash_kernel_traits<96, 64, 64, 4, cutlass::half_t>>, Is_dropout=true, Is_causal=true, Is_even_N=true, Is_even_K=true, Return_softmax=true, Params=Flash_fwd_params]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(15): here
            instantiation of "void flash_fwd_kernel<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax>(Flash_fwd_params) [with Kernel_traits=Flash_fwd_kernel_traits<96, 64, 64, 4, false, false, cutlass::half_t, Flash_kernel_traits<96, 64, 64, 4, cutlass::half_t>>, Is_dropout=true, Is_causal=true, Is_even_N=true, Is_even_K=true, Return_softmax=true]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(34): here
            instantiation of "void run_flash_fwd<Kernel_traits,Is_dropout,Is_causal>(Flash_fwd_params &, cudaStream_t) [with Kernel_traits=Flash_fwd_kernel_traits<96, 64, 64, 4, false, false, cutlass::half_t, Flash_kernel_traits<96, 64, 64, 4, cutlass::half_t>>, Is_dropout=true, Is_causal=true]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(92): here
            instantiation of "void run_mha_fwd_hdim96<T>(Flash_fwd_params &, cudaStream_t) [with T=cutlass::half_t]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_hdim96_fp16_sm80.cu(22): here

/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_kernel.h(140): warning: variable "MMA_M" was declared but never referenced
          detected during:
            instantiation of "void flash::compute_attn_1rowblock<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &, int, int, int) [with Kernel_traits=Flash_fwd_kernel_traits<96, 64, 64, 4, false, false, cutlass::half_t, Flash_kernel_traits<96, 64, 64, 4, cutlass::half_t>>, Is_dropout=true, Is_causal=true, Is_even_N=true, Is_even_K=true, Return_softmax=false, Params=Flash_fwd_params]" 
(588): here
            instantiation of "void flash::compute_attn<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &) [with Kernel_traits=Flash_fwd_kernel_traits<96, 64, 64, 4, false, false, cutlass::half_t, Flash_kernel_traits<96, 64, 64, 4, cutlass::half_t>>, Is_dropout=true, Is_causal=true, Is_even_N=true, Is_even_K=true, Return_softmax=false, Params=Flash_fwd_params]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(15): here
            instantiation of "void flash_fwd_kernel<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax>(Flash_fwd_params) [with Kernel_traits=Flash_fwd_kernel_traits<96, 64, 64, 4, false, false, cutlass::half_t, Flash_kernel_traits<96, 64, 64, 4, cutlass::half_t>>, Is_dropout=true, Is_causal=true, Is_even_N=true, Is_even_K=true, Return_softmax=false]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(34): here
            instantiation of "void run_flash_fwd<Kernel_traits,Is_dropout,Is_causal>(Flash_fwd_params &, cudaStream_t) [with Kernel_traits=Flash_fwd_kernel_traits<96, 64, 64, 4, false, false, cutlass::half_t, Flash_kernel_traits<96, 64, 64, 4, cutlass::half_t>>, Is_dropout=true, Is_causal=true]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(92): here
            instantiation of "void run_mha_fwd_hdim96<T>(Flash_fwd_params &, cudaStream_t) [with T=cutlass::half_t]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_hdim96_fp16_sm80.cu(22): here

/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_kernel.h(275): warning: variable "tQrQ" was declared but never referenced
          detected during:
            instantiation of "void flash::compute_attn_1rowblock<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &, int, int, int) [with Kernel_traits=Flash_fwd_kernel_traits<96, 64, 64, 4, false, false, cutlass::half_t, Flash_kernel_traits<96, 64, 64, 4, cutlass::half_t>>, Is_dropout=true, Is_causal=true, Is_even_N=true, Is_even_K=true, Return_softmax=false, Params=Flash_fwd_params]" 
(588): here
            instantiation of "void flash::compute_attn<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &) [with Kernel_traits=Flash_fwd_kernel_traits<96, 64, 64, 4, false, false, cutlass::half_t, Flash_kernel_traits<96, 64, 64, 4, cutlass::half_t>>, Is_dropout=true, Is_causal=true, Is_even_N=true, Is_even_K=true, Return_softmax=false, Params=Flash_fwd_params]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(15): here
            instantiation of "void flash_fwd_kernel<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax>(Flash_fwd_params) [with Kernel_traits=Flash_fwd_kernel_traits<96, 64, 64, 4, false, false, cutlass::half_t, Flash_kernel_traits<96, 64, 64, 4, cutlass::half_t>>, Is_dropout=true, Is_causal=true, Is_even_N=true, Is_even_K=true, Return_softmax=false]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(34): here
            instantiation of "void run_flash_fwd<Kernel_traits,Is_dropout,Is_causal>(Flash_fwd_params &, cudaStream_t) [with Kernel_traits=Flash_fwd_kernel_traits<96, 64, 64, 4, false, false, cutlass::half_t, Flash_kernel_traits<96, 64, 64, 4, cutlass::half_t>>, Is_dropout=true, Is_causal=true]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(92): here
            instantiation of "void run_mha_fwd_hdim96<T>(Flash_fwd_params &, cudaStream_t) [with T=cutlass::half_t]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_hdim96_fp16_sm80.cu(22): here

/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_kernel.h(140): warning: variable "MMA_M" was declared but never referenced
          detected during:
            instantiation of "void flash::compute_attn_1rowblock<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &, int, int, int) [with Kernel_traits=Flash_fwd_kernel_traits<96, 64, 64, 4, false, false, cutlass::half_t, Flash_kernel_traits<96, 64, 64, 4, cutlass::half_t>>, Is_dropout=true, Is_causal=true, Is_even_N=true, Is_even_K=false, Return_softmax=true, Params=Flash_fwd_params]" 
(588): here
            instantiation of "void flash::compute_attn<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &) [with Kernel_traits=Flash_fwd_kernel_traits<96, 64, 64, 4, false, false, cutlass::half_t, Flash_kernel_traits<96, 64, 64, 4, cutlass::half_t>>, Is_dropout=true, Is_causal=true, Is_even_N=true, Is_even_K=false, Return_softmax=true, Params=Flash_fwd_params]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(15): here
            instantiation of "void flash_fwd_kernel<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax>(Flash_fwd_params) [with Kernel_traits=Flash_fwd_kernel_traits<96, 64, 64, 4, false, false, cutlass::half_t, Flash_kernel_traits<96, 64, 64, 4, cutlass::half_t>>, Is_dropout=true, Is_causal=true, Is_even_N=true, Is_even_K=false, Return_softmax=true]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(34): here
            instantiation of "void run_flash_fwd<Kernel_traits,Is_dropout,Is_causal>(Flash_fwd_params &, cudaStream_t) [with Kernel_traits=Flash_fwd_kernel_traits<96, 64, 64, 4, false, false, cutlass::half_t, Flash_kernel_traits<96, 64, 64, 4, cutlass::half_t>>, Is_dropout=true, Is_causal=true]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(92): here
            instantiation of "void run_mha_fwd_hdim96<T>(Flash_fwd_params &, cudaStream_t) [with T=cutlass::half_t]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_hdim96_fp16_sm80.cu(22): here

/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_kernel.h(275): warning: variable "tQrQ" was declared but never referenced
          detected during:
            instantiation of "void flash::compute_attn_1rowblock<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &, int, int, int) [with Kernel_traits=Flash_fwd_kernel_traits<96, 64, 64, 4, false, false, cutlass::half_t, Flash_kernel_traits<96, 64, 64, 4, cutlass::half_t>>, Is_dropout=true, Is_causal=true, Is_even_N=true, Is_even_K=false, Return_softmax=true, Params=Flash_fwd_params]" 
(588): here
            instantiation of "void flash::compute_attn<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &) [with Kernel_traits=Flash_fwd_kernel_traits<96, 64, 64, 4, false, false, cutlass::half_t, Flash_kernel_traits<96, 64, 64, 4, cutlass::half_t>>, Is_dropout=true, Is_causal=true, Is_even_N=true, Is_even_K=false, Return_softmax=true, Params=Flash_fwd_params]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(15): here
            instantiation of "void flash_fwd_kernel<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax>(Flash_fwd_params) [with Kernel_traits=Flash_fwd_kernel_traits<96, 64, 64, 4, false, false, cutlass::half_t, Flash_kernel_traits<96, 64, 64, 4, cutlass::half_t>>, Is_dropout=true, Is_causal=true, Is_even_N=true, Is_even_K=false, Return_softmax=true]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(34): here
            instantiation of "void run_flash_fwd<Kernel_traits,Is_dropout,Is_causal>(Flash_fwd_params &, cudaStream_t) [with Kernel_traits=Flash_fwd_kernel_traits<96, 64, 64, 4, false, false, cutlass::half_t, Flash_kernel_traits<96, 64, 64, 4, cutlass::half_t>>, Is_dropout=true, Is_causal=true]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(92): here
            instantiation of "void run_mha_fwd_hdim96<T>(Flash_fwd_params &, cudaStream_t) [with T=cutlass::half_t]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_hdim96_fp16_sm80.cu(22): here

/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_kernel.h(140): warning: variable "MMA_M" was declared but never referenced
          detected during:
            instantiation of "void flash::compute_attn_1rowblock<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &, int, int, int) [with Kernel_traits=Flash_fwd_kernel_traits<96, 64, 64, 4, false, false, cutlass::half_t, Flash_kernel_traits<96, 64, 64, 4, cutlass::half_t>>, Is_dropout=true, Is_causal=true, Is_even_N=true, Is_even_K=false, Return_softmax=false, Params=Flash_fwd_params]" 
(588): here
            instantiation of "void flash::compute_attn<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &) [with Kernel_traits=Flash_fwd_kernel_traits<96, 64, 64, 4, false, false, cutlass::half_t, Flash_kernel_traits<96, 64, 64, 4, cutlass::half_t>>, Is_dropout=true, Is_causal=true, Is_even_N=true, Is_even_K=false, Return_softmax=false, Params=Flash_fwd_params]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(15): here
            instantiation of "void flash_fwd_kernel<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax>(Flash_fwd_params) [with Kernel_traits=Flash_fwd_kernel_traits<96, 64, 64, 4, false, false, cutlass::half_t, Flash_kernel_traits<96, 64, 64, 4, cutlass::half_t>>, Is_dropout=true, Is_causal=true, Is_even_N=true, Is_even_K=false, Return_softmax=false]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(34): here
            instantiation of "void run_flash_fwd<Kernel_traits,Is_dropout,Is_causal>(Flash_fwd_params &, cudaStream_t) [with Kernel_traits=Flash_fwd_kernel_traits<96, 64, 64, 4, false, false, cutlass::half_t, Flash_kernel_traits<96, 64, 64, 4, cutlass::half_t>>, Is_dropout=true, Is_causal=true]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(92): here
            instantiation of "void run_mha_fwd_hdim96<T>(Flash_fwd_params &, cudaStream_t) [with T=cutlass::half_t]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_hdim96_fp16_sm80.cu(22): here

/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_kernel.h(275): warning: variable "tQrQ" was declared but never referenced
          detected during:
            instantiation of "void flash::compute_attn_1rowblock<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &, int, int, int) [with Kernel_traits=Flash_fwd_kernel_traits<96, 64, 64, 4, false, false, cutlass::half_t, Flash_kernel_traits<96, 64, 64, 4, cutlass::half_t>>, Is_dropout=true, Is_causal=true, Is_even_N=true, Is_even_K=false, Return_softmax=false, Params=Flash_fwd_params]" 
(588): here
            instantiation of "void flash::compute_attn<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &) [with Kernel_traits=Flash_fwd_kernel_traits<96, 64, 64, 4, false, false, cutlass::half_t, Flash_kernel_traits<96, 64, 64, 4, cutlass::half_t>>, Is_dropout=true, Is_causal=true, Is_even_N=true, Is_even_K=false, Return_softmax=false, Params=Flash_fwd_params]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(15): here
            instantiation of "void flash_fwd_kernel<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax>(Flash_fwd_params) [with Kernel_traits=Flash_fwd_kernel_traits<96, 64, 64, 4, false, false, cutlass::half_t, Flash_kernel_traits<96, 64, 64, 4, cutlass::half_t>>, Is_dropout=true, Is_causal=true, Is_even_N=true, Is_even_K=false, Return_softmax=false]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(34): here
            instantiation of "void run_flash_fwd<Kernel_traits,Is_dropout,Is_causal>(Flash_fwd_params &, cudaStream_t) [with Kernel_traits=Flash_fwd_kernel_traits<96, 64, 64, 4, false, false, cutlass::half_t, Flash_kernel_traits<96, 64, 64, 4, cutlass::half_t>>, Is_dropout=true, Is_causal=true]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(92): here
            instantiation of "void run_mha_fwd_hdim96<T>(Flash_fwd_params &, cudaStream_t) [with T=cutlass::half_t]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_hdim96_fp16_sm80.cu(22): here

/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_kernel.h(140): warning: variable "MMA_M" was declared but never referenced
          detected during:
            instantiation of "void flash::compute_attn_1rowblock<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &, int, int, int) [with Kernel_traits=Flash_fwd_kernel_traits<96, 64, 64, 4, false, false, cutlass::half_t, Flash_kernel_traits<96, 64, 64, 4, cutlass::half_t>>, Is_dropout=true, Is_causal=true, Is_even_N=false, Is_even_K=true, Return_softmax=true, Params=Flash_fwd_params]" 
(588): here
            instantiation of "void flash::compute_attn<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &) [with Kernel_traits=Flash_fwd_kernel_traits<96, 64, 64, 4, false, false, cutlass::half_t, Flash_kernel_traits<96, 64, 64, 4, cutlass::half_t>>, Is_dropout=true, Is_causal=true, Is_even_N=false, Is_even_K=true, Return_softmax=true, Params=Flash_fwd_params]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(15): here
            instantiation of "void flash_fwd_kernel<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax>(Flash_fwd_params) [with Kernel_traits=Flash_fwd_kernel_traits<96, 64, 64, 4, false, false, cutlass::half_t, Flash_kernel_traits<96, 64, 64, 4, cutlass::half_t>>, Is_dropout=true, Is_causal=true, Is_even_N=false, Is_even_K=true, Return_softmax=true]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(34): here
            instantiation of "void run_flash_fwd<Kernel_traits,Is_dropout,Is_causal>(Flash_fwd_params &, cudaStream_t) [with Kernel_traits=Flash_fwd_kernel_traits<96, 64, 64, 4, false, false, cutlass::half_t, Flash_kernel_traits<96, 64, 64, 4, cutlass::half_t>>, Is_dropout=true, Is_causal=true]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(92): here
            instantiation of "void run_mha_fwd_hdim96<T>(Flash_fwd_params &, cudaStream_t) [with T=cutlass::half_t]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_hdim96_fp16_sm80.cu(22): here

/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_kernel.h(275): warning: variable "tQrQ" was declared but never referenced
          detected during:
            instantiation of "void flash::compute_attn_1rowblock<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &, int, int, int) [with Kernel_traits=Flash_fwd_kernel_traits<96, 64, 64, 4, false, false, cutlass::half_t, Flash_kernel_traits<96, 64, 64, 4, cutlass::half_t>>, Is_dropout=true, Is_causal=true, Is_even_N=false, Is_even_K=true, Return_softmax=true, Params=Flash_fwd_params]" 
(588): here
            instantiation of "void flash::compute_attn<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &) [with Kernel_traits=Flash_fwd_kernel_traits<96, 64, 64, 4, false, false, cutlass::half_t, Flash_kernel_traits<96, 64, 64, 4, cutlass::half_t>>, Is_dropout=true, Is_causal=true, Is_even_N=false, Is_even_K=true, Return_softmax=true, Params=Flash_fwd_params]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(15): here
            instantiation of "void flash_fwd_kernel<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax>(Flash_fwd_params) [with Kernel_traits=Flash_fwd_kernel_traits<96, 64, 64, 4, false, false, cutlass::half_t, Flash_kernel_traits<96, 64, 64, 4, cutlass::half_t>>, Is_dropout=true, Is_causal=true, Is_even_N=false, Is_even_K=true, Return_softmax=true]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(34): here
            instantiation of "void run_flash_fwd<Kernel_traits,Is_dropout,Is_causal>(Flash_fwd_params &, cudaStream_t) [with Kernel_traits=Flash_fwd_kernel_traits<96, 64, 64, 4, false, false, cutlass::half_t, Flash_kernel_traits<96, 64, 64, 4, cutlass::half_t>>, Is_dropout=true, Is_causal=true]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(92): here
            instantiation of "void run_mha_fwd_hdim96<T>(Flash_fwd_params &, cudaStream_t) [with T=cutlass::half_t]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_hdim96_fp16_sm80.cu(22): here

/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_kernel.h(140): warning: variable "MMA_M" was declared but never referenced
          detected during:
            instantiation of "void flash::compute_attn_1rowblock<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &, int, int, int) [with Kernel_traits=Flash_fwd_kernel_traits<96, 64, 64, 4, false, false, cutlass::half_t, Flash_kernel_traits<96, 64, 64, 4, cutlass::half_t>>, Is_dropout=true, Is_causal=true, Is_even_N=false, Is_even_K=true, Return_softmax=false, Params=Flash_fwd_params]" 
(588): here
            instantiation of "void flash::compute_attn<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &) [with Kernel_traits=Flash_fwd_kernel_traits<96, 64, 64, 4, false, false, cutlass::half_t, Flash_kernel_traits<96, 64, 64, 4, cutlass::half_t>>, Is_dropout=true, Is_causal=true, Is_even_N=false, Is_even_K=true, Return_softmax=false, Params=Flash_fwd_params]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(15): here
            instantiation of "void flash_fwd_kernel<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax>(Flash_fwd_params) [with Kernel_traits=Flash_fwd_kernel_traits<96, 64, 64, 4, false, false, cutlass::half_t, Flash_kernel_traits<96, 64, 64, 4, cutlass::half_t>>, Is_dropout=true, Is_causal=true, Is_even_N=false, Is_even_K=true, Return_softmax=false]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(34): here
            instantiation of "void run_flash_fwd<Kernel_traits,Is_dropout,Is_causal>(Flash_fwd_params &, cudaStream_t) [with Kernel_traits=Flash_fwd_kernel_traits<96, 64, 64, 4, false, false, cutlass::half_t, Flash_kernel_traits<96, 64, 64, 4, cutlass::half_t>>, Is_dropout=true, Is_causal=true]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(92): here
            instantiation of "void run_mha_fwd_hdim96<T>(Flash_fwd_params &, cudaStream_t) [with T=cutlass::half_t]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_hdim96_fp16_sm80.cu(22): here

/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_kernel.h(275): warning: variable "tQrQ" was declared but never referenced
          detected during:
            instantiation of "void flash::compute_attn_1rowblock<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &, int, int, int) [with Kernel_traits=Flash_fwd_kernel_traits<96, 64, 64, 4, false, false, cutlass::half_t, Flash_kernel_traits<96, 64, 64, 4, cutlass::half_t>>, Is_dropout=true, Is_causal=true, Is_even_N=false, Is_even_K=true, Return_softmax=false, Params=Flash_fwd_params]" 
(588): here
            instantiation of "void flash::compute_attn<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &) [with Kernel_traits=Flash_fwd_kernel_traits<96, 64, 64, 4, false, false, cutlass::half_t, Flash_kernel_traits<96, 64, 64, 4, cutlass::half_t>>, Is_dropout=true, Is_causal=true, Is_even_N=false, Is_even_K=true, Return_softmax=false, Params=Flash_fwd_params]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(15): here
            instantiation of "void flash_fwd_kernel<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax>(Flash_fwd_params) [with Kernel_traits=Flash_fwd_kernel_traits<96, 64, 64, 4, false, false, cutlass::half_t, Flash_kernel_traits<96, 64, 64, 4, cutlass::half_t>>, Is_dropout=true, Is_causal=true, Is_even_N=false, Is_even_K=true, Return_softmax=false]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(34): here
            instantiation of "void run_flash_fwd<Kernel_traits,Is_dropout,Is_causal>(Flash_fwd_params &, cudaStream_t) [with Kernel_traits=Flash_fwd_kernel_traits<96, 64, 64, 4, false, false, cutlass::half_t, Flash_kernel_traits<96, 64, 64, 4, cutlass::half_t>>, Is_dropout=true, Is_causal=true]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(92): here
            instantiation of "void run_mha_fwd_hdim96<T>(Flash_fwd_params &, cudaStream_t) [with T=cutlass::half_t]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_hdim96_fp16_sm80.cu(22): here

/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_kernel.h(140): warning: variable "MMA_M" was declared but never referenced
          detected during:
            instantiation of "void flash::compute_attn_1rowblock<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &, int, int, int) [with Kernel_traits=Flash_fwd_kernel_traits<96, 64, 64, 4, false, false, cutlass::half_t, Flash_kernel_traits<96, 64, 64, 4, cutlass::half_t>>, Is_dropout=true, Is_causal=true, Is_even_N=false, Is_even_K=false, Return_softmax=true, Params=Flash_fwd_params]" 
(588): here
            instantiation of "void flash::compute_attn<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &) [with Kernel_traits=Flash_fwd_kernel_traits<96, 64, 64, 4, false, false, cutlass::half_t, Flash_kernel_traits<96, 64, 64, 4, cutlass::half_t>>, Is_dropout=true, Is_causal=true, Is_even_N=false, Is_even_K=false, Return_softmax=true, Params=Flash_fwd_params]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(15): here
            instantiation of "void flash_fwd_kernel<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax>(Flash_fwd_params) [with Kernel_traits=Flash_fwd_kernel_traits<96, 64, 64, 4, false, false, cutlass::half_t, Flash_kernel_traits<96, 64, 64, 4, cutlass::half_t>>, Is_dropout=true, Is_causal=true, Is_even_N=false, Is_even_K=false, Return_softmax=true]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(34): here
            instantiation of "void run_flash_fwd<Kernel_traits,Is_dropout,Is_causal>(Flash_fwd_params &, cudaStream_t) [with Kernel_traits=Flash_fwd_kernel_traits<96, 64, 64, 4, false, false, cutlass::half_t, Flash_kernel_traits<96, 64, 64, 4, cutlass::half_t>>, Is_dropout=true, Is_causal=true]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(92): here
            instantiation of "void run_mha_fwd_hdim96<T>(Flash_fwd_params &, cudaStream_t) [with T=cutlass::half_t]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_hdim96_fp16_sm80.cu(22): here

/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_kernel.h(275): warning: variable "tQrQ" was declared but never referenced
          detected during:
            instantiation of "void flash::compute_attn_1rowblock<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &, int, int, int) [with Kernel_traits=Flash_fwd_kernel_traits<96, 64, 64, 4, false, false, cutlass::half_t, Flash_kernel_traits<96, 64, 64, 4, cutlass::half_t>>, Is_dropout=true, Is_causal=true, Is_even_N=false, Is_even_K=false, Return_softmax=true, Params=Flash_fwd_params]" 
(588): here
            instantiation of "void flash::compute_attn<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &) [with Kernel_traits=Flash_fwd_kernel_traits<96, 64, 64, 4, false, false, cutlass::half_t, Flash_kernel_traits<96, 64, 64, 4, cutlass::half_t>>, Is_dropout=true, Is_causal=true, Is_even_N=false, Is_even_K=false, Return_softmax=true, Params=Flash_fwd_params]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(15): here
            instantiation of "void flash_fwd_kernel<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax>(Flash_fwd_params) [with Kernel_traits=Flash_fwd_kernel_traits<96, 64, 64, 4, false, false, cutlass::half_t, Flash_kernel_traits<96, 64, 64, 4, cutlass::half_t>>, Is_dropout=true, Is_causal=true, Is_even_N=false, Is_even_K=false, Return_softmax=true]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(34): here
            instantiation of "void run_flash_fwd<Kernel_traits,Is_dropout,Is_causal>(Flash_fwd_params &, cudaStream_t) [with Kernel_traits=Flash_fwd_kernel_traits<96, 64, 64, 4, false, false, cutlass::half_t, Flash_kernel_traits<96, 64, 64, 4, cutlass::half_t>>, Is_dropout=true, Is_causal=true]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(92): here
            instantiation of "void run_mha_fwd_hdim96<T>(Flash_fwd_params &, cudaStream_t) [with T=cutlass::half_t]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_hdim96_fp16_sm80.cu(22): here

/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_kernel.h(140): warning: variable "MMA_M" was declared but never referenced
          detected during:
            instantiation of "void flash::compute_attn_1rowblock<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &, int, int, int) [with Kernel_traits=Flash_fwd_kernel_traits<96, 64, 64, 4, false, false, cutlass::half_t, Flash_kernel_traits<96, 64, 64, 4, cutlass::half_t>>, Is_dropout=true, Is_causal=true, Is_even_N=false, Is_even_K=false, Return_softmax=false, Params=Flash_fwd_params]" 
(588): here
            instantiation of "void flash::compute_attn<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &) [with Kernel_traits=Flash_fwd_kernel_traits<96, 64, 64, 4, false, false, cutlass::half_t, Flash_kernel_traits<96, 64, 64, 4, cutlass::half_t>>, Is_dropout=true, Is_causal=true, Is_even_N=false, Is_even_K=false, Return_softmax=false, Params=Flash_fwd_params]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(15): here
            instantiation of "void flash_fwd_kernel<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax>(Flash_fwd_params) [with Kernel_traits=Flash_fwd_kernel_traits<96, 64, 64, 4, false, false, cutlass::half_t, Flash_kernel_traits<96, 64, 64, 4, cutlass::half_t>>, Is_dropout=true, Is_causal=true, Is_even_N=false, Is_even_K=false, Return_softmax=false]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(34): here
            instantiation of "void run_flash_fwd<Kernel_traits,Is_dropout,Is_causal>(Flash_fwd_params &, cudaStream_t) [with Kernel_traits=Flash_fwd_kernel_traits<96, 64, 64, 4, false, false, cutlass::half_t, Flash_kernel_traits<96, 64, 64, 4, cutlass::half_t>>, Is_dropout=true, Is_causal=true]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(92): here
            instantiation of "void run_mha_fwd_hdim96<T>(Flash_fwd_params &, cudaStream_t) [with T=cutlass::half_t]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_hdim96_fp16_sm80.cu(22): here

/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_kernel.h(275): warning: variable "tQrQ" was declared but never referenced
          detected during:
            instantiation of "void flash::compute_attn_1rowblock<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &, int, int, int) [with Kernel_traits=Flash_fwd_kernel_traits<96, 64, 64, 4, false, false, cutlass::half_t, Flash_kernel_traits<96, 64, 64, 4, cutlass::half_t>>, Is_dropout=true, Is_causal=true, Is_even_N=false, Is_even_K=false, Return_softmax=false, Params=Flash_fwd_params]" 
(588): here
            instantiation of "void flash::compute_attn<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &) [with Kernel_traits=Flash_fwd_kernel_traits<96, 64, 64, 4, false, false, cutlass::half_t, Flash_kernel_traits<96, 64, 64, 4, cutlass::half_t>>, Is_dropout=true, Is_causal=true, Is_even_N=false, Is_even_K=false, Return_softmax=false, Params=Flash_fwd_params]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(15): here
            instantiation of "void flash_fwd_kernel<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax>(Flash_fwd_params) [with Kernel_traits=Flash_fwd_kernel_traits<96, 64, 64, 4, false, false, cutlass::half_t, Flash_kernel_traits<96, 64, 64, 4, cutlass::half_t>>, Is_dropout=true, Is_causal=true, Is_even_N=false, Is_even_K=false, Return_softmax=false]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(34): here
            instantiation of "void run_flash_fwd<Kernel_traits,Is_dropout,Is_causal>(Flash_fwd_params &, cudaStream_t) [with Kernel_traits=Flash_fwd_kernel_traits<96, 64, 64, 4, false, false, cutlass::half_t, Flash_kernel_traits<96, 64, 64, 4, cutlass::half_t>>, Is_dropout=true, Is_causal=true]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(92): here
            instantiation of "void run_mha_fwd_hdim96<T>(Flash_fwd_params &, cudaStream_t) [with T=cutlass::half_t]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_hdim96_fp16_sm80.cu(22): here

/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_kernel.h(140): warning: variable "MMA_M" was declared but never referenced
          detected during:
            instantiation of "void flash::compute_attn_1rowblock<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &, int, int, int) [with Kernel_traits=Flash_fwd_kernel_traits<96, 128, 64, 4, false, false, cutlass::half_t, Flash_kernel_traits<96, 128, 64, 4, cutlass::half_t>>, Is_dropout=true, Is_causal=true, Is_even_N=true, Is_even_K=true, Return_softmax=true, Params=Flash_fwd_params]" 
(588): here
            instantiation of "void flash::compute_attn<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &) [with Kernel_traits=Flash_fwd_kernel_traits<96, 128, 64, 4, false, false, cutlass::half_t, Flash_kernel_traits<96, 128, 64, 4, cutlass::half_t>>, Is_dropout=true, Is_causal=true, Is_even_N=true, Is_even_K=true, Return_softmax=true, Params=Flash_fwd_params]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(15): here
            instantiation of "void flash_fwd_kernel<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax>(Flash_fwd_params) [with Kernel_traits=Flash_fwd_kernel_traits<96, 128, 64, 4, false, false, cutlass::half_t, Flash_kernel_traits<96, 128, 64, 4, cutlass::half_t>>, Is_dropout=true, Is_causal=true, Is_even_N=true, Is_even_K=true, Return_softmax=true]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(34): here
            instantiation of "void run_flash_fwd<Kernel_traits,Is_dropout,Is_causal>(Flash_fwd_params &, cudaStream_t) [with Kernel_traits=Flash_fwd_kernel_traits<96, 128, 64, 4, false, false, cutlass::half_t, Flash_kernel_traits<96, 128, 64, 4, cutlass::half_t>>, Is_dropout=true, Is_causal=true]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(92): here
            instantiation of "void run_mha_fwd_hdim96<T>(Flash_fwd_params &, cudaStream_t) [with T=cutlass::half_t]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_hdim96_fp16_sm80.cu(22): here

/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_kernel.h(275): warning: variable "tQrQ" was declared but never referenced
          detected during:
            instantiation of "void flash::compute_attn_1rowblock<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &, int, int, int) [with Kernel_traits=Flash_fwd_kernel_traits<96, 128, 64, 4, false, false, cutlass::half_t, Flash_kernel_traits<96, 128, 64, 4, cutlass::half_t>>, Is_dropout=true, Is_causal=true, Is_even_N=true, Is_even_K=true, Return_softmax=true, Params=Flash_fwd_params]" 
(588): here
            instantiation of "void flash::compute_attn<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &) [with Kernel_traits=Flash_fwd_kernel_traits<96, 128, 64, 4, false, false, cutlass::half_t, Flash_kernel_traits<96, 128, 64, 4, cutlass::half_t>>, Is_dropout=true, Is_causal=true, Is_even_N=true, Is_even_K=true, Return_softmax=true, Params=Flash_fwd_params]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(15): here
            instantiation of "void flash_fwd_kernel<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax>(Flash_fwd_params) [with Kernel_traits=Flash_fwd_kernel_traits<96, 128, 64, 4, false, false, cutlass::half_t, Flash_kernel_traits<96, 128, 64, 4, cutlass::half_t>>, Is_dropout=true, Is_causal=true, Is_even_N=true, Is_even_K=true, Return_softmax=true]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(34): here
            instantiation of "void run_flash_fwd<Kernel_traits,Is_dropout,Is_causal>(Flash_fwd_params &, cudaStream_t) [with Kernel_traits=Flash_fwd_kernel_traits<96, 128, 64, 4, false, false, cutlass::half_t, Flash_kernel_traits<96, 128, 64, 4, cutlass::half_t>>, Is_dropout=true, Is_causal=true]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(92): here
            instantiation of "void run_mha_fwd_hdim96<T>(Flash_fwd_params &, cudaStream_t) [with T=cutlass::half_t]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_hdim96_fp16_sm80.cu(22): here

/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_kernel.h(140): warning: variable "MMA_M" was declared but never referenced
          detected during:
            instantiation of "void flash::compute_attn_1rowblock<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &, int, int, int) [with Kernel_traits=Flash_fwd_kernel_traits<96, 128, 64, 4, false, false, cutlass::half_t, Flash_kernel_traits<96, 128, 64, 4, cutlass::half_t>>, Is_dropout=true, Is_causal=true, Is_even_N=true, Is_even_K=true, Return_softmax=false, Params=Flash_fwd_params]" 
(588): here
            instantiation of "void flash::compute_attn<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &) [with Kernel_traits=Flash_fwd_kernel_traits<96, 128, 64, 4, false, false, cutlass::half_t, Flash_kernel_traits<96, 128, 64, 4, cutlass::half_t>>, Is_dropout=true, Is_causal=true, Is_even_N=true, Is_even_K=true, Return_softmax=false, Params=Flash_fwd_params]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(15): here
            instantiation of "void flash_fwd_kernel<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax>(Flash_fwd_params) [with Kernel_traits=Flash_fwd_kernel_traits<96, 128, 64, 4, false, false, cutlass::half_t, Flash_kernel_traits<96, 128, 64, 4, cutlass::half_t>>, Is_dropout=true, Is_causal=true, Is_even_N=true, Is_even_K=true, Return_softmax=false]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(34): here
            instantiation of "void run_flash_fwd<Kernel_traits,Is_dropout,Is_causal>(Flash_fwd_params &, cudaStream_t) [with Kernel_traits=Flash_fwd_kernel_traits<96, 128, 64, 4, false, false, cutlass::half_t, Flash_kernel_traits<96, 128, 64, 4, cutlass::half_t>>, Is_dropout=true, Is_causal=true]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(92): here
            instantiation of "void run_mha_fwd_hdim96<T>(Flash_fwd_params &, cudaStream_t) [with T=cutlass::half_t]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_hdim96_fp16_sm80.cu(22): here

/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_kernel.h(275): warning: variable "tQrQ" was declared but never referenced
          detected during:
            instantiation of "void flash::compute_attn_1rowblock<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &, int, int, int) [with Kernel_traits=Flash_fwd_kernel_traits<96, 128, 64, 4, false, false, cutlass::half_t, Flash_kernel_traits<96, 128, 64, 4, cutlass::half_t>>, Is_dropout=true, Is_causal=true, Is_even_N=true, Is_even_K=true, Return_softmax=false, Params=Flash_fwd_params]" 
(588): here
            instantiation of "void flash::compute_attn<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &) [with Kernel_traits=Flash_fwd_kernel_traits<96, 128, 64, 4, false, false, cutlass::half_t, Flash_kernel_traits<96, 128, 64, 4, cutlass::half_t>>, Is_dropout=true, Is_causal=true, Is_even_N=true, Is_even_K=true, Return_softmax=false, Params=Flash_fwd_params]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(15): here
            instantiation of "void flash_fwd_kernel<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax>(Flash_fwd_params) [with Kernel_traits=Flash_fwd_kernel_traits<96, 128, 64, 4, false, false, cutlass::half_t, Flash_kernel_traits<96, 128, 64, 4, cutlass::half_t>>, Is_dropout=true, Is_causal=true, Is_even_N=true, Is_even_K=true, Return_softmax=false]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(34): here
            instantiation of "void run_flash_fwd<Kernel_traits,Is_dropout,Is_causal>(Flash_fwd_params &, cudaStream_t) [with Kernel_traits=Flash_fwd_kernel_traits<96, 128, 64, 4, false, false, cutlass::half_t, Flash_kernel_traits<96, 128, 64, 4, cutlass::half_t>>, Is_dropout=true, Is_causal=true]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(92): here
            instantiation of "void run_mha_fwd_hdim96<T>(Flash_fwd_params &, cudaStream_t) [with T=cutlass::half_t]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_hdim96_fp16_sm80.cu(22): here

/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_kernel.h(140): warning: variable "MMA_M" was declared but never referenced
          detected during:
            instantiation of "void flash::compute_attn_1rowblock<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &, int, int, int) [with Kernel_traits=Flash_fwd_kernel_traits<96, 128, 64, 4, false, false, cutlass::half_t, Flash_kernel_traits<96, 128, 64, 4, cutlass::half_t>>, Is_dropout=true, Is_causal=true, Is_even_N=true, Is_even_K=false, Return_softmax=true, Params=Flash_fwd_params]" 
(588): here
            instantiation of "void flash::compute_attn<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &) [with Kernel_traits=Flash_fwd_kernel_traits<96, 128, 64, 4, false, false, cutlass::half_t, Flash_kernel_traits<96, 128, 64, 4, cutlass::half_t>>, Is_dropout=true, Is_causal=true, Is_even_N=true, Is_even_K=false, Return_softmax=true, Params=Flash_fwd_params]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(15): here
            instantiation of "void flash_fwd_kernel<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax>(Flash_fwd_params) [with Kernel_traits=Flash_fwd_kernel_traits<96, 128, 64, 4, false, false, cutlass::half_t, Flash_kernel_traits<96, 128, 64, 4, cutlass::half_t>>, Is_dropout=true, Is_causal=true, Is_even_N=true, Is_even_K=false, Return_softmax=true]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(34): here
            instantiation of "void run_flash_fwd<Kernel_traits,Is_dropout,Is_causal>(Flash_fwd_params &, cudaStream_t) [with Kernel_traits=Flash_fwd_kernel_traits<96, 128, 64, 4, false, false, cutlass::half_t, Flash_kernel_traits<96, 128, 64, 4, cutlass::half_t>>, Is_dropout=true, Is_causal=true]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(92): here
            instantiation of "void run_mha_fwd_hdim96<T>(Flash_fwd_params &, cudaStream_t) [with T=cutlass::half_t]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_hdim96_fp16_sm80.cu(22): here

/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_kernel.h(275): warning: variable "tQrQ" was declared but never referenced
          detected during:
            instantiation of "void flash::compute_attn_1rowblock<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &, int, int, int) [with Kernel_traits=Flash_fwd_kernel_traits<96, 128, 64, 4, false, false, cutlass::half_t, Flash_kernel_traits<96, 128, 64, 4, cutlass::half_t>>, Is_dropout=true, Is_causal=true, Is_even_N=true, Is_even_K=false, Return_softmax=true, Params=Flash_fwd_params]" 
(588): here
            instantiation of "void flash::compute_attn<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &) [with Kernel_traits=Flash_fwd_kernel_traits<96, 128, 64, 4, false, false, cutlass::half_t, Flash_kernel_traits<96, 128, 64, 4, cutlass::half_t>>, Is_dropout=true, Is_causal=true, Is_even_N=true, Is_even_K=false, Return_softmax=true, Params=Flash_fwd_params]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(15): here
            instantiation of "void flash_fwd_kernel<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax>(Flash_fwd_params) [with Kernel_traits=Flash_fwd_kernel_traits<96, 128, 64, 4, false, false, cutlass::half_t, Flash_kernel_traits<96, 128, 64, 4, cutlass::half_t>>, Is_dropout=true, Is_causal=true, Is_even_N=true, Is_even_K=false, Return_softmax=true]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(34): here
            instantiation of "void run_flash_fwd<Kernel_traits,Is_dropout,Is_causal>(Flash_fwd_params &, cudaStream_t) [with Kernel_traits=Flash_fwd_kernel_traits<96, 128, 64, 4, false, false, cutlass::half_t, Flash_kernel_traits<96, 128, 64, 4, cutlass::half_t>>, Is_dropout=true, Is_causal=true]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(92): here
            instantiation of "void run_mha_fwd_hdim96<T>(Flash_fwd_params &, cudaStream_t) [with T=cutlass::half_t]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_hdim96_fp16_sm80.cu(22): here

/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_kernel.h(140): warning: variable "MMA_M" was declared but never referenced
          detected during:
            instantiation of "void flash::compute_attn_1rowblock<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &, int, int, int) [with Kernel_traits=Flash_fwd_kernel_traits<96, 128, 64, 4, false, false, cutlass::half_t, Flash_kernel_traits<96, 128, 64, 4, cutlass::half_t>>, Is_dropout=true, Is_causal=true, Is_even_N=true, Is_even_K=false, Return_softmax=false, Params=Flash_fwd_params]" 
(588): here
            instantiation of "void flash::compute_attn<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &) [with Kernel_traits=Flash_fwd_kernel_traits<96, 128, 64, 4, false, false, cutlass::half_t, Flash_kernel_traits<96, 128, 64, 4, cutlass::half_t>>, Is_dropout=true, Is_causal=true, Is_even_N=true, Is_even_K=false, Return_softmax=false, Params=Flash_fwd_params]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(15): here
            instantiation of "void flash_fwd_kernel<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax>(Flash_fwd_params) [with Kernel_traits=Flash_fwd_kernel_traits<96, 128, 64, 4, false, false, cutlass::half_t, Flash_kernel_traits<96, 128, 64, 4, cutlass::half_t>>, Is_dropout=true, Is_causal=true, Is_even_N=true, Is_even_K=false, Return_softmax=false]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(34): here
            instantiation of "void run_flash_fwd<Kernel_traits,Is_dropout,Is_causal>(Flash_fwd_params &, cudaStream_t) [with Kernel_traits=Flash_fwd_kernel_traits<96, 128, 64, 4, false, false, cutlass::half_t, Flash_kernel_traits<96, 128, 64, 4, cutlass::half_t>>, Is_dropout=true, Is_causal=true]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(92): here
            instantiation of "void run_mha_fwd_hdim96<T>(Flash_fwd_params &, cudaStream_t) [with T=cutlass::half_t]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_hdim96_fp16_sm80.cu(22): here

/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_kernel.h(275): warning: variable "tQrQ" was declared but never referenced
          detected during:
            instantiation of "void flash::compute_attn_1rowblock<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &, int, int, int) [with Kernel_traits=Flash_fwd_kernel_traits<96, 128, 64, 4, false, false, cutlass::half_t, Flash_kernel_traits<96, 128, 64, 4, cutlass::half_t>>, Is_dropout=true, Is_causal=true, Is_even_N=true, Is_even_K=false, Return_softmax=false, Params=Flash_fwd_params]" 
(588): here
            instantiation of "void flash::compute_attn<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &) [with Kernel_traits=Flash_fwd_kernel_traits<96, 128, 64, 4, false, false, cutlass::half_t, Flash_kernel_traits<96, 128, 64, 4, cutlass::half_t>>, Is_dropout=true, Is_causal=true, Is_even_N=true, Is_even_K=false, Return_softmax=false, Params=Flash_fwd_params]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(15): here
            instantiation of "void flash_fwd_kernel<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax>(Flash_fwd_params) [with Kernel_traits=Flash_fwd_kernel_traits<96, 128, 64, 4, false, false, cutlass::half_t, Flash_kernel_traits<96, 128, 64, 4, cutlass::half_t>>, Is_dropout=true, Is_causal=true, Is_even_N=true, Is_even_K=false, Return_softmax=false]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(34): here
            instantiation of "void run_flash_fwd<Kernel_traits,Is_dropout,Is_causal>(Flash_fwd_params &, cudaStream_t) [with Kernel_traits=Flash_fwd_kernel_traits<96, 128, 64, 4, false, false, cutlass::half_t, Flash_kernel_traits<96, 128, 64, 4, cutlass::half_t>>, Is_dropout=true, Is_causal=true]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(92): here
            instantiation of "void run_mha_fwd_hdim96<T>(Flash_fwd_params &, cudaStream_t) [with T=cutlass::half_t]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_hdim96_fp16_sm80.cu(22): here

/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_kernel.h(140): warning: variable "MMA_M" was declared but never referenced
          detected during:
            instantiation of "void flash::compute_attn_1rowblock<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &, int, int, int) [with Kernel_traits=Flash_fwd_kernel_traits<96, 128, 64, 4, false, false, cutlass::half_t, Flash_kernel_traits<96, 128, 64, 4, cutlass::half_t>>, Is_dropout=true, Is_causal=true, Is_even_N=false, Is_even_K=true, Return_softmax=true, Params=Flash_fwd_params]" 
(588): here
            instantiation of "void flash::compute_attn<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &) [with Kernel_traits=Flash_fwd_kernel_traits<96, 128, 64, 4, false, false, cutlass::half_t, Flash_kernel_traits<96, 128, 64, 4, cutlass::half_t>>, Is_dropout=true, Is_causal=true, Is_even_N=false, Is_even_K=true, Return_softmax=true, Params=Flash_fwd_params]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(15): here
            instantiation of "void flash_fwd_kernel<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax>(Flash_fwd_params) [with Kernel_traits=Flash_fwd_kernel_traits<96, 128, 64, 4, false, false, cutlass::half_t, Flash_kernel_traits<96, 128, 64, 4, cutlass::half_t>>, Is_dropout=true, Is_causal=true, Is_even_N=false, Is_even_K=true, Return_softmax=true]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(34): here
            instantiation of "void run_flash_fwd<Kernel_traits,Is_dropout,Is_causal>(Flash_fwd_params &, cudaStream_t) [with Kernel_traits=Flash_fwd_kernel_traits<96, 128, 64, 4, false, false, cutlass::half_t, Flash_kernel_traits<96, 128, 64, 4, cutlass::half_t>>, Is_dropout=true, Is_causal=true]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(92): here
            instantiation of "void run_mha_fwd_hdim96<T>(Flash_fwd_params &, cudaStream_t) [with T=cutlass::half_t]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_hdim96_fp16_sm80.cu(22): here

/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_kernel.h(275): warning: variable "tQrQ" was declared but never referenced
          detected during:
            instantiation of "void flash::compute_attn_1rowblock<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &, int, int, int) [with Kernel_traits=Flash_fwd_kernel_traits<96, 128, 64, 4, false, false, cutlass::half_t, Flash_kernel_traits<96, 128, 64, 4, cutlass::half_t>>, Is_dropout=true, Is_causal=true, Is_even_N=false, Is_even_K=true, Return_softmax=true, Params=Flash_fwd_params]" 
(588): here
            instantiation of "void flash::compute_attn<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &) [with Kernel_traits=Flash_fwd_kernel_traits<96, 128, 64, 4, false, false, cutlass::half_t, Flash_kernel_traits<96, 128, 64, 4, cutlass::half_t>>, Is_dropout=true, Is_causal=true, Is_even_N=false, Is_even_K=true, Return_softmax=true, Params=Flash_fwd_params]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(15): here
            instantiation of "void flash_fwd_kernel<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax>(Flash_fwd_params) [with Kernel_traits=Flash_fwd_kernel_traits<96, 128, 64, 4, false, false, cutlass::half_t, Flash_kernel_traits<96, 128, 64, 4, cutlass::half_t>>, Is_dropout=true, Is_causal=true, Is_even_N=false, Is_even_K=true, Return_softmax=true]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(34): here
            instantiation of "void run_flash_fwd<Kernel_traits,Is_dropout,Is_causal>(Flash_fwd_params &, cudaStream_t) [with Kernel_traits=Flash_fwd_kernel_traits<96, 128, 64, 4, false, false, cutlass::half_t, Flash_kernel_traits<96, 128, 64, 4, cutlass::half_t>>, Is_dropout=true, Is_causal=true]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(92): here
            instantiation of "void run_mha_fwd_hdim96<T>(Flash_fwd_params &, cudaStream_t) [with T=cutlass::half_t]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_hdim96_fp16_sm80.cu(22): here

/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_kernel.h(140): warning: variable "MMA_M" was declared but never referenced
          detected during:
            instantiation of "void flash::compute_attn_1rowblock<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &, int, int, int) [with Kernel_traits=Flash_fwd_kernel_traits<96, 128, 64, 4, false, false, cutlass::half_t, Flash_kernel_traits<96, 128, 64, 4, cutlass::half_t>>, Is_dropout=true, Is_causal=true, Is_even_N=false, Is_even_K=true, Return_softmax=false, Params=Flash_fwd_params]" 
(588): here
            instantiation of "void flash::compute_attn<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &) [with Kernel_traits=Flash_fwd_kernel_traits<96, 128, 64, 4, false, false, cutlass::half_t, Flash_kernel_traits<96, 128, 64, 4, cutlass::half_t>>, Is_dropout=true, Is_causal=true, Is_even_N=false, Is_even_K=true, Return_softmax=false, Params=Flash_fwd_params]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(15): here
            instantiation of "void flash_fwd_kernel<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax>(Flash_fwd_params) [with Kernel_traits=Flash_fwd_kernel_traits<96, 128, 64, 4, false, false, cutlass::half_t, Flash_kernel_traits<96, 128, 64, 4, cutlass::half_t>>, Is_dropout=true, Is_causal=true, Is_even_N=false, Is_even_K=true, Return_softmax=false]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(34): here
            instantiation of "void run_flash_fwd<Kernel_traits,Is_dropout,Is_causal>(Flash_fwd_params &, cudaStream_t) [with Kernel_traits=Flash_fwd_kernel_traits<96, 128, 64, 4, false, false, cutlass::half_t, Flash_kernel_traits<96, 128, 64, 4, cutlass::half_t>>, Is_dropout=true, Is_causal=true]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(92): here
            instantiation of "void run_mha_fwd_hdim96<T>(Flash_fwd_params &, cudaStream_t) [with T=cutlass::half_t]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_hdim96_fp16_sm80.cu(22): here

/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_kernel.h(275): warning: variable "tQrQ" was declared but never referenced
          detected during:
            instantiation of "void flash::compute_attn_1rowblock<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &, int, int, int) [with Kernel_traits=Flash_fwd_kernel_traits<96, 128, 64, 4, false, false, cutlass::half_t, Flash_kernel_traits<96, 128, 64, 4, cutlass::half_t>>, Is_dropout=true, Is_causal=true, Is_even_N=false, Is_even_K=true, Return_softmax=false, Params=Flash_fwd_params]" 
(588): here
            instantiation of "void flash::compute_attn<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &) [with Kernel_traits=Flash_fwd_kernel_traits<96, 128, 64, 4, false, false, cutlass::half_t, Flash_kernel_traits<96, 128, 64, 4, cutlass::half_t>>, Is_dropout=true, Is_causal=true, Is_even_N=false, Is_even_K=true, Return_softmax=false, Params=Flash_fwd_params]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(15): here
            instantiation of "void flash_fwd_kernel<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax>(Flash_fwd_params) [with Kernel_traits=Flash_fwd_kernel_traits<96, 128, 64, 4, false, false, cutlass::half_t, Flash_kernel_traits<96, 128, 64, 4, cutlass::half_t>>, Is_dropout=true, Is_causal=true, Is_even_N=false, Is_even_K=true, Return_softmax=false]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(34): here
            instantiation of "void run_flash_fwd<Kernel_traits,Is_dropout,Is_causal>(Flash_fwd_params &, cudaStream_t) [with Kernel_traits=Flash_fwd_kernel_traits<96, 128, 64, 4, false, false, cutlass::half_t, Flash_kernel_traits<96, 128, 64, 4, cutlass::half_t>>, Is_dropout=true, Is_causal=true]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(92): here
            instantiation of "void run_mha_fwd_hdim96<T>(Flash_fwd_params &, cudaStream_t) [with T=cutlass::half_t]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_hdim96_fp16_sm80.cu(22): here

/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_kernel.h(140): warning: variable "MMA_M" was declared but never referenced
          detected during:
            instantiation of "void flash::compute_attn_1rowblock<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &, int, int, int) [with Kernel_traits=Flash_fwd_kernel_traits<96, 128, 64, 4, false, false, cutlass::half_t, Flash_kernel_traits<96, 128, 64, 4, cutlass::half_t>>, Is_dropout=true, Is_causal=true, Is_even_N=false, Is_even_K=false, Return_softmax=true, Params=Flash_fwd_params]" 
(588): here
            instantiation of "void flash::compute_attn<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &) [with Kernel_traits=Flash_fwd_kernel_traits<96, 128, 64, 4, false, false, cutlass::half_t, Flash_kernel_traits<96, 128, 64, 4, cutlass::half_t>>, Is_dropout=true, Is_causal=true, Is_even_N=false, Is_even_K=false, Return_softmax=true, Params=Flash_fwd_params]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(15): here
            instantiation of "void flash_fwd_kernel<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax>(Flash_fwd_params) [with Kernel_traits=Flash_fwd_kernel_traits<96, 128, 64, 4, false, false, cutlass::half_t, Flash_kernel_traits<96, 128, 64, 4, cutlass::half_t>>, Is_dropout=true, Is_causal=true, Is_even_N=false, Is_even_K=false, Return_softmax=true]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(34): here
            instantiation of "void run_flash_fwd<Kernel_traits,Is_dropout,Is_causal>(Flash_fwd_params &, cudaStream_t) [with Kernel_traits=Flash_fwd_kernel_traits<96, 128, 64, 4, false, false, cutlass::half_t, Flash_kernel_traits<96, 128, 64, 4, cutlass::half_t>>, Is_dropout=true, Is_causal=true]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(92): here
            instantiation of "void run_mha_fwd_hdim96<T>(Flash_fwd_params &, cudaStream_t) [with T=cutlass::half_t]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_hdim96_fp16_sm80.cu(22): here

/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_kernel.h(275): warning: variable "tQrQ" was declared but never referenced
          detected during:
            instantiation of "void flash::compute_attn_1rowblock<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &, int, int, int) [with Kernel_traits=Flash_fwd_kernel_traits<96, 128, 64, 4, false, false, cutlass::half_t, Flash_kernel_traits<96, 128, 64, 4, cutlass::half_t>>, Is_dropout=true, Is_causal=true, Is_even_N=false, Is_even_K=false, Return_softmax=true, Params=Flash_fwd_params]" 
(588): here
            instantiation of "void flash::compute_attn<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &) [with Kernel_traits=Flash_fwd_kernel_traits<96, 128, 64, 4, false, false, cutlass::half_t, Flash_kernel_traits<96, 128, 64, 4, cutlass::half_t>>, Is_dropout=true, Is_causal=true, Is_even_N=false, Is_even_K=false, Return_softmax=true, Params=Flash_fwd_params]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(15): here
            instantiation of "void flash_fwd_kernel<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax>(Flash_fwd_params) [with Kernel_traits=Flash_fwd_kernel_traits<96, 128, 64, 4, false, false, cutlass::half_t, Flash_kernel_traits<96, 128, 64, 4, cutlass::half_t>>, Is_dropout=true, Is_causal=true, Is_even_N=false, Is_even_K=false, Return_softmax=true]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(34): here
            instantiation of "void run_flash_fwd<Kernel_traits,Is_dropout,Is_causal>(Flash_fwd_params &, cudaStream_t) [with Kernel_traits=Flash_fwd_kernel_traits<96, 128, 64, 4, false, false, cutlass::half_t, Flash_kernel_traits<96, 128, 64, 4, cutlass::half_t>>, Is_dropout=true, Is_causal=true]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(92): here
            instantiation of "void run_mha_fwd_hdim96<T>(Flash_fwd_params &, cudaStream_t) [with T=cutlass::half_t]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_hdim96_fp16_sm80.cu(22): here

/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_kernel.h(140): warning: variable "MMA_M" was declared but never referenced
          detected during:
            instantiation of "void flash::compute_attn_1rowblock<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &, int, int, int) [with Kernel_traits=Flash_fwd_kernel_traits<96, 128, 64, 4, false, false, cutlass::half_t, Flash_kernel_traits<96, 128, 64, 4, cutlass::half_t>>, Is_dropout=true, Is_causal=true, Is_even_N=false, Is_even_K=false, Return_softmax=false, Params=Flash_fwd_params]" 
(588): here
            instantiation of "void flash::compute_attn<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &) [with Kernel_traits=Flash_fwd_kernel_traits<96, 128, 64, 4, false, false, cutlass::half_t, Flash_kernel_traits<96, 128, 64, 4, cutlass::half_t>>, Is_dropout=true, Is_causal=true, Is_even_N=false, Is_even_K=false, Return_softmax=false, Params=Flash_fwd_params]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(15): here
            instantiation of "void flash_fwd_kernel<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax>(Flash_fwd_params) [with Kernel_traits=Flash_fwd_kernel_traits<96, 128, 64, 4, false, false, cutlass::half_t, Flash_kernel_traits<96, 128, 64, 4, cutlass::half_t>>, Is_dropout=true, Is_causal=true, Is_even_N=false, Is_even_K=false, Return_softmax=false]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(34): here
            instantiation of "void run_flash_fwd<Kernel_traits,Is_dropout,Is_causal>(Flash_fwd_params &, cudaStream_t) [with Kernel_traits=Flash_fwd_kernel_traits<96, 128, 64, 4, false, false, cutlass::half_t, Flash_kernel_traits<96, 128, 64, 4, cutlass::half_t>>, Is_dropout=true, Is_causal=true]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(92): here
            instantiation of "void run_mha_fwd_hdim96<T>(Flash_fwd_params &, cudaStream_t) [with T=cutlass::half_t]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_hdim96_fp16_sm80.cu(22): here

/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_kernel.h(275): warning: variable "tQrQ" was declared but never referenced
          detected during:
            instantiation of "void flash::compute_attn_1rowblock<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &, int, int, int) [with Kernel_traits=Flash_fwd_kernel_traits<96, 128, 64, 4, false, false, cutlass::half_t, Flash_kernel_traits<96, 128, 64, 4, cutlass::half_t>>, Is_dropout=true, Is_causal=true, Is_even_N=false, Is_even_K=false, Return_softmax=false, Params=Flash_fwd_params]" 
(588): here
            instantiation of "void flash::compute_attn<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &) [with Kernel_traits=Flash_fwd_kernel_traits<96, 128, 64, 4, false, false, cutlass::half_t, Flash_kernel_traits<96, 128, 64, 4, cutlass::half_t>>, Is_dropout=true, Is_causal=true, Is_even_N=false, Is_even_K=false, Return_softmax=false, Params=Flash_fwd_params]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(15): here
            instantiation of "void flash_fwd_kernel<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax>(Flash_fwd_params) [with Kernel_traits=Flash_fwd_kernel_traits<96, 128, 64, 4, false, false, cutlass::half_t, Flash_kernel_traits<96, 128, 64, 4, cutlass::half_t>>, Is_dropout=true, Is_causal=true, Is_even_N=false, Is_even_K=false, Return_softmax=false]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(34): here
            instantiation of "void run_flash_fwd<Kernel_traits,Is_dropout,Is_causal>(Flash_fwd_params &, cudaStream_t) [with Kernel_traits=Flash_fwd_kernel_traits<96, 128, 64, 4, false, false, cutlass::half_t, Flash_kernel_traits<96, 128, 64, 4, cutlass::half_t>>, Is_dropout=true, Is_causal=true]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(92): here
            instantiation of "void run_mha_fwd_hdim96<T>(Flash_fwd_params &, cudaStream_t) [with T=cutlass::half_t]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_hdim96_fp16_sm80.cu(22): here

/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_kernel.h(140): warning: variable "MMA_M" was declared but never referenced
          detected during:
            instantiation of "void flash::compute_attn_1rowblock<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &, int, int, int) [with Kernel_traits=Flash_fwd_kernel_traits<96, 128, 64, 4, false, false, cutlass::half_t, Flash_kernel_traits<96, 128, 64, 4, cutlass::half_t>>, Is_dropout=true, Is_causal=false, Is_even_N=true, Is_even_K=true, Return_softmax=true, Params=Flash_fwd_params]" 
(588): here
            instantiation of "void flash::compute_attn<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &) [with Kernel_traits=Flash_fwd_kernel_traits<96, 128, 64, 4, false, false, cutlass::half_t, Flash_kernel_traits<96, 128, 64, 4, cutlass::half_t>>, Is_dropout=true, Is_causal=false, Is_even_N=true, Is_even_K=true, Return_softmax=true, Params=Flash_fwd_params]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(15): here
            instantiation of "void flash_fwd_kernel<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax>(Flash_fwd_params) [with Kernel_traits=Flash_fwd_kernel_traits<96, 128, 64, 4, false, false, cutlass::half_t, Flash_kernel_traits<96, 128, 64, 4, cutlass::half_t>>, Is_dropout=true, Is_causal=false, Is_even_N=true, Is_even_K=true, Return_softmax=true]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(34): here
            instantiation of "void run_flash_fwd<Kernel_traits,Is_dropout,Is_causal>(Flash_fwd_params &, cudaStream_t) [with Kernel_traits=Flash_fwd_kernel_traits<96, 128, 64, 4, false, false, cutlass::half_t, Flash_kernel_traits<96, 128, 64, 4, cutlass::half_t>>, Is_dropout=true, Is_causal=false]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(92): here
            instantiation of "void run_mha_fwd_hdim96<T>(Flash_fwd_params &, cudaStream_t) [with T=cutlass::half_t]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_hdim96_fp16_sm80.cu(22): here

/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_kernel.h(275): warning: variable "tQrQ" was declared but never referenced
          detected during:
            instantiation of "void flash::compute_attn_1rowblock<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &, int, int, int) [with Kernel_traits=Flash_fwd_kernel_traits<96, 128, 64, 4, false, false, cutlass::half_t, Flash_kernel_traits<96, 128, 64, 4, cutlass::half_t>>, Is_dropout=true, Is_causal=false, Is_even_N=true, Is_even_K=true, Return_softmax=true, Params=Flash_fwd_params]" 
(588): here
            instantiation of "void flash::compute_attn<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &) [with Kernel_traits=Flash_fwd_kernel_traits<96, 128, 64, 4, false, false, cutlass::half_t, Flash_kernel_traits<96, 128, 64, 4, cutlass::half_t>>, Is_dropout=true, Is_causal=false, Is_even_N=true, Is_even_K=true, Return_softmax=true, Params=Flash_fwd_params]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(15): here
            instantiation of "void flash_fwd_kernel<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax>(Flash_fwd_params) [with Kernel_traits=Flash_fwd_kernel_traits<96, 128, 64, 4, false, false, cutlass::half_t, Flash_kernel_traits<96, 128, 64, 4, cutlass::half_t>>, Is_dropout=true, Is_causal=false, Is_even_N=true, Is_even_K=true, Return_softmax=true]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(34): here
            instantiation of "void run_flash_fwd<Kernel_traits,Is_dropout,Is_causal>(Flash_fwd_params &, cudaStream_t) [with Kernel_traits=Flash_fwd_kernel_traits<96, 128, 64, 4, false, false, cutlass::half_t, Flash_kernel_traits<96, 128, 64, 4, cutlass::half_t>>, Is_dropout=true, Is_causal=false]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(92): here
            instantiation of "void run_mha_fwd_hdim96<T>(Flash_fwd_params &, cudaStream_t) [with T=cutlass::half_t]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_hdim96_fp16_sm80.cu(22): here

/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_kernel.h(140): warning: variable "MMA_M" was declared but never referenced
          detected during:
            instantiation of "void flash::compute_attn_1rowblock<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &, int, int, int) [with Kernel_traits=Flash_fwd_kernel_traits<96, 128, 64, 4, false, false, cutlass::half_t, Flash_kernel_traits<96, 128, 64, 4, cutlass::half_t>>, Is_dropout=true, Is_causal=false, Is_even_N=true, Is_even_K=true, Return_softmax=false, Params=Flash_fwd_params]" 
(588): here
            instantiation of "void flash::compute_attn<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &) [with Kernel_traits=Flash_fwd_kernel_traits<96, 128, 64, 4, false, false, cutlass::half_t, Flash_kernel_traits<96, 128, 64, 4, cutlass::half_t>>, Is_dropout=true, Is_causal=false, Is_even_N=true, Is_even_K=true, Return_softmax=false, Params=Flash_fwd_params]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(15): here
            instantiation of "void flash_fwd_kernel<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax>(Flash_fwd_params) [with Kernel_traits=Flash_fwd_kernel_traits<96, 128, 64, 4, false, false, cutlass::half_t, Flash_kernel_traits<96, 128, 64, 4, cutlass::half_t>>, Is_dropout=true, Is_causal=false, Is_even_N=true, Is_even_K=true, Return_softmax=false]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(34): here
            instantiation of "void run_flash_fwd<Kernel_traits,Is_dropout,Is_causal>(Flash_fwd_params &, cudaStream_t) [with Kernel_traits=Flash_fwd_kernel_traits<96, 128, 64, 4, false, false, cutlass::half_t, Flash_kernel_traits<96, 128, 64, 4, cutlass::half_t>>, Is_dropout=true, Is_causal=false]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(92): here
            instantiation of "void run_mha_fwd_hdim96<T>(Flash_fwd_params &, cudaStream_t) [with T=cutlass::half_t]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_hdim96_fp16_sm80.cu(22): here

/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_kernel.h(275): warning: variable "tQrQ" was declared but never referenced
          detected during:
            instantiation of "void flash::compute_attn_1rowblock<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &, int, int, int) [with Kernel_traits=Flash_fwd_kernel_traits<96, 128, 64, 4, false, false, cutlass::half_t, Flash_kernel_traits<96, 128, 64, 4, cutlass::half_t>>, Is_dropout=true, Is_causal=false, Is_even_N=true, Is_even_K=true, Return_softmax=false, Params=Flash_fwd_params]" 
(588): here
            instantiation of "void flash::compute_attn<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &) [with Kernel_traits=Flash_fwd_kernel_traits<96, 128, 64, 4, false, false, cutlass::half_t, Flash_kernel_traits<96, 128, 64, 4, cutlass::half_t>>, Is_dropout=true, Is_causal=false, Is_even_N=true, Is_even_K=true, Return_softmax=false, Params=Flash_fwd_params]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(15): here
            instantiation of "void flash_fwd_kernel<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax>(Flash_fwd_params) [with Kernel_traits=Flash_fwd_kernel_traits<96, 128, 64, 4, false, false, cutlass::half_t, Flash_kernel_traits<96, 128, 64, 4, cutlass::half_t>>, Is_dropout=true, Is_causal=false, Is_even_N=true, Is_even_K=true, Return_softmax=false]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(34): here
            instantiation of "void run_flash_fwd<Kernel_traits,Is_dropout,Is_causal>(Flash_fwd_params &, cudaStream_t) [with Kernel_traits=Flash_fwd_kernel_traits<96, 128, 64, 4, false, false, cutlass::half_t, Flash_kernel_traits<96, 128, 64, 4, cutlass::half_t>>, Is_dropout=true, Is_causal=false]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(92): here
            instantiation of "void run_mha_fwd_hdim96<T>(Flash_fwd_params &, cudaStream_t) [with T=cutlass::half_t]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_hdim96_fp16_sm80.cu(22): here

/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_kernel.h(140): warning: variable "MMA_M" was declared but never referenced
          detected during:
            instantiation of "void flash::compute_attn_1rowblock<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &, int, int, int) [with Kernel_traits=Flash_fwd_kernel_traits<96, 128, 64, 4, false, false, cutlass::half_t, Flash_kernel_traits<96, 128, 64, 4, cutlass::half_t>>, Is_dropout=true, Is_causal=false, Is_even_N=true, Is_even_K=false, Return_softmax=true, Params=Flash_fwd_params]" 
(588): here
            instantiation of "void flash::compute_attn<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &) [with Kernel_traits=Flash_fwd_kernel_traits<96, 128, 64, 4, false, false, cutlass::half_t, Flash_kernel_traits<96, 128, 64, 4, cutlass::half_t>>, Is_dropout=true, Is_causal=false, Is_even_N=true, Is_even_K=false, Return_softmax=true, Params=Flash_fwd_params]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(15): here
            instantiation of "void flash_fwd_kernel<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax>(Flash_fwd_params) [with Kernel_traits=Flash_fwd_kernel_traits<96, 128, 64, 4, false, false, cutlass::half_t, Flash_kernel_traits<96, 128, 64, 4, cutlass::half_t>>, Is_dropout=true, Is_causal=false, Is_even_N=true, Is_even_K=false, Return_softmax=true]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(34): here
            instantiation of "void run_flash_fwd<Kernel_traits,Is_dropout,Is_causal>(Flash_fwd_params &, cudaStream_t) [with Kernel_traits=Flash_fwd_kernel_traits<96, 128, 64, 4, false, false, cutlass::half_t, Flash_kernel_traits<96, 128, 64, 4, cutlass::half_t>>, Is_dropout=true, Is_causal=false]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(92): here
            instantiation of "void run_mha_fwd_hdim96<T>(Flash_fwd_params &, cudaStream_t) [with T=cutlass::half_t]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_hdim96_fp16_sm80.cu(22): here

/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_kernel.h(275): warning: variable "tQrQ" was declared but never referenced
          detected during:
            instantiation of "void flash::compute_attn_1rowblock<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &, int, int, int) [with Kernel_traits=Flash_fwd_kernel_traits<96, 128, 64, 4, false, false, cutlass::half_t, Flash_kernel_traits<96, 128, 64, 4, cutlass::half_t>>, Is_dropout=true, Is_causal=false, Is_even_N=true, Is_even_K=false, Return_softmax=true, Params=Flash_fwd_params]" 
(588): here
            instantiation of "void flash::compute_attn<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &) [with Kernel_traits=Flash_fwd_kernel_traits<96, 128, 64, 4, false, false, cutlass::half_t, Flash_kernel_traits<96, 128, 64, 4, cutlass::half_t>>, Is_dropout=true, Is_causal=false, Is_even_N=true, Is_even_K=false, Return_softmax=true, Params=Flash_fwd_params]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(15): here
            instantiation of "void flash_fwd_kernel<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax>(Flash_fwd_params) [with Kernel_traits=Flash_fwd_kernel_traits<96, 128, 64, 4, false, false, cutlass::half_t, Flash_kernel_traits<96, 128, 64, 4, cutlass::half_t>>, Is_dropout=true, Is_causal=false, Is_even_N=true, Is_even_K=false, Return_softmax=true]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(34): here
            instantiation of "void run_flash_fwd<Kernel_traits,Is_dropout,Is_causal>(Flash_fwd_params &, cudaStream_t) [with Kernel_traits=Flash_fwd_kernel_traits<96, 128, 64, 4, false, false, cutlass::half_t, Flash_kernel_traits<96, 128, 64, 4, cutlass::half_t>>, Is_dropout=true, Is_causal=false]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(92): here
            instantiation of "void run_mha_fwd_hdim96<T>(Flash_fwd_params &, cudaStream_t) [with T=cutlass::half_t]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_hdim96_fp16_sm80.cu(22): here

/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_kernel.h(140): warning: variable "MMA_M" was declared but never referenced
          detected during:
            instantiation of "void flash::compute_attn_1rowblock<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &, int, int, int) [with Kernel_traits=Flash_fwd_kernel_traits<96, 128, 64, 4, false, false, cutlass::half_t, Flash_kernel_traits<96, 128, 64, 4, cutlass::half_t>>, Is_dropout=true, Is_causal=false, Is_even_N=true, Is_even_K=false, Return_softmax=false, Params=Flash_fwd_params]" 
(588): here
            instantiation of "void flash::compute_attn<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &) [with Kernel_traits=Flash_fwd_kernel_traits<96, 128, 64, 4, false, false, cutlass::half_t, Flash_kernel_traits<96, 128, 64, 4, cutlass::half_t>>, Is_dropout=true, Is_causal=false, Is_even_N=true, Is_even_K=false, Return_softmax=false, Params=Flash_fwd_params]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(15): here
            instantiation of "void flash_fwd_kernel<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax>(Flash_fwd_params) [with Kernel_traits=Flash_fwd_kernel_traits<96, 128, 64, 4, false, false, cutlass::half_t, Flash_kernel_traits<96, 128, 64, 4, cutlass::half_t>>, Is_dropout=true, Is_causal=false, Is_even_N=true, Is_even_K=false, Return_softmax=false]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(34): here
            instantiation of "void run_flash_fwd<Kernel_traits,Is_dropout,Is_causal>(Flash_fwd_params &, cudaStream_t) [with Kernel_traits=Flash_fwd_kernel_traits<96, 128, 64, 4, false, false, cutlass::half_t, Flash_kernel_traits<96, 128, 64, 4, cutlass::half_t>>, Is_dropout=true, Is_causal=false]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(92): here
            instantiation of "void run_mha_fwd_hdim96<T>(Flash_fwd_params &, cudaStream_t) [with T=cutlass::half_t]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_hdim96_fp16_sm80.cu(22): here

/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_kernel.h(275): warning: variable "tQrQ" was declared but never referenced
          detected during:
            instantiation of "void flash::compute_attn_1rowblock<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &, int, int, int) [with Kernel_traits=Flash_fwd_kernel_traits<96, 128, 64, 4, false, false, cutlass::half_t, Flash_kernel_traits<96, 128, 64, 4, cutlass::half_t>>, Is_dropout=true, Is_causal=false, Is_even_N=true, Is_even_K=false, Return_softmax=false, Params=Flash_fwd_params]" 
(588): here
            instantiation of "void flash::compute_attn<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &) [with Kernel_traits=Flash_fwd_kernel_traits<96, 128, 64, 4, false, false, cutlass::half_t, Flash_kernel_traits<96, 128, 64, 4, cutlass::half_t>>, Is_dropout=true, Is_causal=false, Is_even_N=true, Is_even_K=false, Return_softmax=false, Params=Flash_fwd_params]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(15): here
            instantiation of "void flash_fwd_kernel<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax>(Flash_fwd_params) [with Kernel_traits=Flash_fwd_kernel_traits<96, 128, 64, 4, false, false, cutlass::half_t, Flash_kernel_traits<96, 128, 64, 4, cutlass::half_t>>, Is_dropout=true, Is_causal=false, Is_even_N=true, Is_even_K=false, Return_softmax=false]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(34): here
            instantiation of "void run_flash_fwd<Kernel_traits,Is_dropout,Is_causal>(Flash_fwd_params &, cudaStream_t) [with Kernel_traits=Flash_fwd_kernel_traits<96, 128, 64, 4, false, false, cutlass::half_t, Flash_kernel_traits<96, 128, 64, 4, cutlass::half_t>>, Is_dropout=true, Is_causal=false]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(92): here
            instantiation of "void run_mha_fwd_hdim96<T>(Flash_fwd_params &, cudaStream_t) [with T=cutlass::half_t]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_hdim96_fp16_sm80.cu(22): here

/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_kernel.h(140): warning: variable "MMA_M" was declared but never referenced
          detected during:
            instantiation of "void flash::compute_attn_1rowblock<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &, int, int, int) [with Kernel_traits=Flash_fwd_kernel_traits<96, 128, 64, 4, false, false, cutlass::half_t, Flash_kernel_traits<96, 128, 64, 4, cutlass::half_t>>, Is_dropout=true, Is_causal=false, Is_even_N=false, Is_even_K=true, Return_softmax=true, Params=Flash_fwd_params]" 
(588): here
            instantiation of "void flash::compute_attn<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &) [with Kernel_traits=Flash_fwd_kernel_traits<96, 128, 64, 4, false, false, cutlass::half_t, Flash_kernel_traits<96, 128, 64, 4, cutlass::half_t>>, Is_dropout=true, Is_causal=false, Is_even_N=false, Is_even_K=true, Return_softmax=true, Params=Flash_fwd_params]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(15): here
            instantiation of "void flash_fwd_kernel<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax>(Flash_fwd_params) [with Kernel_traits=Flash_fwd_kernel_traits<96, 128, 64, 4, false, false, cutlass::half_t, Flash_kernel_traits<96, 128, 64, 4, cutlass::half_t>>, Is_dropout=true, Is_causal=false, Is_even_N=false, Is_even_K=true, Return_softmax=true]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(34): here
            instantiation of "void run_flash_fwd<Kernel_traits,Is_dropout,Is_causal>(Flash_fwd_params &, cudaStream_t) [with Kernel_traits=Flash_fwd_kernel_traits<96, 128, 64, 4, false, false, cutlass::half_t, Flash_kernel_traits<96, 128, 64, 4, cutlass::half_t>>, Is_dropout=true, Is_causal=false]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(92): here
            instantiation of "void run_mha_fwd_hdim96<T>(Flash_fwd_params &, cudaStream_t) [with T=cutlass::half_t]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_hdim96_fp16_sm80.cu(22): here

/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_kernel.h(275): warning: variable "tQrQ" was declared but never referenced
          detected during:
            instantiation of "void flash::compute_attn_1rowblock<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &, int, int, int) [with Kernel_traits=Flash_fwd_kernel_traits<96, 128, 64, 4, false, false, cutlass::half_t, Flash_kernel_traits<96, 128, 64, 4, cutlass::half_t>>, Is_dropout=true, Is_causal=false, Is_even_N=false, Is_even_K=true, Return_softmax=true, Params=Flash_fwd_params]" 
(588): here
            instantiation of "void flash::compute_attn<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &) [with Kernel_traits=Flash_fwd_kernel_traits<96, 128, 64, 4, false, false, cutlass::half_t, Flash_kernel_traits<96, 128, 64, 4, cutlass::half_t>>, Is_dropout=true, Is_causal=false, Is_even_N=false, Is_even_K=true, Return_softmax=true, Params=Flash_fwd_params]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(15): here
            instantiation of "void flash_fwd_kernel<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax>(Flash_fwd_params) [with Kernel_traits=Flash_fwd_kernel_traits<96, 128, 64, 4, false, false, cutlass::half_t, Flash_kernel_traits<96, 128, 64, 4, cutlass::half_t>>, Is_dropout=true, Is_causal=false, Is_even_N=false, Is_even_K=true, Return_softmax=true]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(34): here
            instantiation of "void run_flash_fwd<Kernel_traits,Is_dropout,Is_causal>(Flash_fwd_params &, cudaStream_t) [with Kernel_traits=Flash_fwd_kernel_traits<96, 128, 64, 4, false, false, cutlass::half_t, Flash_kernel_traits<96, 128, 64, 4, cutlass::half_t>>, Is_dropout=true, Is_causal=false]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(92): here
            instantiation of "void run_mha_fwd_hdim96<T>(Flash_fwd_params &, cudaStream_t) [with T=cutlass::half_t]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_hdim96_fp16_sm80.cu(22): here

/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_kernel.h(140): warning: variable "MMA_M" was declared but never referenced
          detected during:
            instantiation of "void flash::compute_attn_1rowblock<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &, int, int, int) [with Kernel_traits=Flash_fwd_kernel_traits<96, 128, 64, 4, false, false, cutlass::half_t, Flash_kernel_traits<96, 128, 64, 4, cutlass::half_t>>, Is_dropout=true, Is_causal=false, Is_even_N=false, Is_even_K=true, Return_softmax=false, Params=Flash_fwd_params]" 
(588): here
            instantiation of "void flash::compute_attn<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &) [with Kernel_traits=Flash_fwd_kernel_traits<96, 128, 64, 4, false, false, cutlass::half_t, Flash_kernel_traits<96, 128, 64, 4, cutlass::half_t>>, Is_dropout=true, Is_causal=false, Is_even_N=false, Is_even_K=true, Return_softmax=false, Params=Flash_fwd_params]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(15): here
            instantiation of "void flash_fwd_kernel<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax>(Flash_fwd_params) [with Kernel_traits=Flash_fwd_kernel_traits<96, 128, 64, 4, false, false, cutlass::half_t, Flash_kernel_traits<96, 128, 64, 4, cutlass::half_t>>, Is_dropout=true, Is_causal=false, Is_even_N=false, Is_even_K=true, Return_softmax=false]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(34): here
            instantiation of "void run_flash_fwd<Kernel_traits,Is_dropout,Is_causal>(Flash_fwd_params &, cudaStream_t) [with Kernel_traits=Flash_fwd_kernel_traits<96, 128, 64, 4, false, false, cutlass::half_t, Flash_kernel_traits<96, 128, 64, 4, cutlass::half_t>>, Is_dropout=true, Is_causal=false]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(92): here
            instantiation of "void run_mha_fwd_hdim96<T>(Flash_fwd_params &, cudaStream_t) [with T=cutlass::half_t]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_hdim96_fp16_sm80.cu(22): here

/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_kernel.h(275): warning: variable "tQrQ" was declared but never referenced
          detected during:
            instantiation of "void flash::compute_attn_1rowblock<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &, int, int, int) [with Kernel_traits=Flash_fwd_kernel_traits<96, 128, 64, 4, false, false, cutlass::half_t, Flash_kernel_traits<96, 128, 64, 4, cutlass::half_t>>, Is_dropout=true, Is_causal=false, Is_even_N=false, Is_even_K=true, Return_softmax=false, Params=Flash_fwd_params]" 
(588): here
            instantiation of "void flash::compute_attn<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &) [with Kernel_traits=Flash_fwd_kernel_traits<96, 128, 64, 4, false, false, cutlass::half_t, Flash_kernel_traits<96, 128, 64, 4, cutlass::half_t>>, Is_dropout=true, Is_causal=false, Is_even_N=false, Is_even_K=true, Return_softmax=false, Params=Flash_fwd_params]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(15): here
            instantiation of "void flash_fwd_kernel<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax>(Flash_fwd_params) [with Kernel_traits=Flash_fwd_kernel_traits<96, 128, 64, 4, false, false, cutlass::half_t, Flash_kernel_traits<96, 128, 64, 4, cutlass::half_t>>, Is_dropout=true, Is_causal=false, Is_even_N=false, Is_even_K=true, Return_softmax=false]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(34): here
            instantiation of "void run_flash_fwd<Kernel_traits,Is_dropout,Is_causal>(Flash_fwd_params &, cudaStream_t) [with Kernel_traits=Flash_fwd_kernel_traits<96, 128, 64, 4, false, false, cutlass::half_t, Flash_kernel_traits<96, 128, 64, 4, cutlass::half_t>>, Is_dropout=true, Is_causal=false]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(92): here
            instantiation of "void run_mha_fwd_hdim96<T>(Flash_fwd_params &, cudaStream_t) [with T=cutlass::half_t]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_hdim96_fp16_sm80.cu(22): here

/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_kernel.h(140): warning: variable "MMA_M" was declared but never referenced
          detected during:
            instantiation of "void flash::compute_attn_1rowblock<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &, int, int, int) [with Kernel_traits=Flash_fwd_kernel_traits<96, 128, 64, 4, false, false, cutlass::half_t, Flash_kernel_traits<96, 128, 64, 4, cutlass::half_t>>, Is_dropout=true, Is_causal=false, Is_even_N=false, Is_even_K=false, Return_softmax=true, Params=Flash_fwd_params]" 
(588): here
            instantiation of "void flash::compute_attn<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &) [with Kernel_traits=Flash_fwd_kernel_traits<96, 128, 64, 4, false, false, cutlass::half_t, Flash_kernel_traits<96, 128, 64, 4, cutlass::half_t>>, Is_dropout=true, Is_causal=false, Is_even_N=false, Is_even_K=false, Return_softmax=true, Params=Flash_fwd_params]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(15): here
            instantiation of "void flash_fwd_kernel<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax>(Flash_fwd_params) [with Kernel_traits=Flash_fwd_kernel_traits<96, 128, 64, 4, false, false, cutlass::half_t, Flash_kernel_traits<96, 128, 64, 4, cutlass::half_t>>, Is_dropout=true, Is_causal=false, Is_even_N=false, Is_even_K=false, Return_softmax=true]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(34): here
            instantiation of "void run_flash_fwd<Kernel_traits,Is_dropout,Is_causal>(Flash_fwd_params &, cudaStream_t) [with Kernel_traits=Flash_fwd_kernel_traits<96, 128, 64, 4, false, false, cutlass::half_t, Flash_kernel_traits<96, 128, 64, 4, cutlass::half_t>>, Is_dropout=true, Is_causal=false]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(92): here
            instantiation of "void run_mha_fwd_hdim96<T>(Flash_fwd_params &, cudaStream_t) [with T=cutlass::half_t]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_hdim96_fp16_sm80.cu(22): here

/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_kernel.h(275): warning: variable "tQrQ" was declared but never referenced
          detected during:
            instantiation of "void flash::compute_attn_1rowblock<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &, int, int, int) [with Kernel_traits=Flash_fwd_kernel_traits<96, 128, 64, 4, false, false, cutlass::half_t, Flash_kernel_traits<96, 128, 64, 4, cutlass::half_t>>, Is_dropout=true, Is_causal=false, Is_even_N=false, Is_even_K=false, Return_softmax=true, Params=Flash_fwd_params]" 
(588): here
            instantiation of "void flash::compute_attn<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &) [with Kernel_traits=Flash_fwd_kernel_traits<96, 128, 64, 4, false, false, cutlass::half_t, Flash_kernel_traits<96, 128, 64, 4, cutlass::half_t>>, Is_dropout=true, Is_causal=false, Is_even_N=false, Is_even_K=false, Return_softmax=true, Params=Flash_fwd_params]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(15): here
            instantiation of "void flash_fwd_kernel<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax>(Flash_fwd_params) [with Kernel_traits=Flash_fwd_kernel_traits<96, 128, 64, 4, false, false, cutlass::half_t, Flash_kernel_traits<96, 128, 64, 4, cutlass::half_t>>, Is_dropout=true, Is_causal=false, Is_even_N=false, Is_even_K=false, Return_softmax=true]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(34): here
            instantiation of "void run_flash_fwd<Kernel_traits,Is_dropout,Is_causal>(Flash_fwd_params &, cudaStream_t) [with Kernel_traits=Flash_fwd_kernel_traits<96, 128, 64, 4, false, false, cutlass::half_t, Flash_kernel_traits<96, 128, 64, 4, cutlass::half_t>>, Is_dropout=true, Is_causal=false]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(92): here
            instantiation of "void run_mha_fwd_hdim96<T>(Flash_fwd_params &, cudaStream_t) [with T=cutlass::half_t]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_hdim96_fp16_sm80.cu(22): here

/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_kernel.h(140): warning: variable "MMA_M" was declared but never referenced
          detected during:
            instantiation of "void flash::compute_attn_1rowblock<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &, int, int, int) [with Kernel_traits=Flash_fwd_kernel_traits<96, 128, 64, 4, false, false, cutlass::half_t, Flash_kernel_traits<96, 128, 64, 4, cutlass::half_t>>, Is_dropout=true, Is_causal=false, Is_even_N=false, Is_even_K=false, Return_softmax=false, Params=Flash_fwd_params]" 
(588): here
            instantiation of "void flash::compute_attn<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &) [with Kernel_traits=Flash_fwd_kernel_traits<96, 128, 64, 4, false, false, cutlass::half_t, Flash_kernel_traits<96, 128, 64, 4, cutlass::half_t>>, Is_dropout=true, Is_causal=false, Is_even_N=false, Is_even_K=false, Return_softmax=false, Params=Flash_fwd_params]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(15): here
            instantiation of "void flash_fwd_kernel<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax>(Flash_fwd_params) [with Kernel_traits=Flash_fwd_kernel_traits<96, 128, 64, 4, false, false, cutlass::half_t, Flash_kernel_traits<96, 128, 64, 4, cutlass::half_t>>, Is_dropout=true, Is_causal=false, Is_even_N=false, Is_even_K=false, Return_softmax=false]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(34): here
            instantiation of "void run_flash_fwd<Kernel_traits,Is_dropout,Is_causal>(Flash_fwd_params &, cudaStream_t) [with Kernel_traits=Flash_fwd_kernel_traits<96, 128, 64, 4, false, false, cutlass::half_t, Flash_kernel_traits<96, 128, 64, 4, cutlass::half_t>>, Is_dropout=true, Is_causal=false]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(92): here
            instantiation of "void run_mha_fwd_hdim96<T>(Flash_fwd_params &, cudaStream_t) [with T=cutlass::half_t]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_hdim96_fp16_sm80.cu(22): here

/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_kernel.h(275): warning: variable "tQrQ" was declared but never referenced
          detected during:
            instantiation of "void flash::compute_attn_1rowblock<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &, int, int, int) [with Kernel_traits=Flash_fwd_kernel_traits<96, 128, 64, 4, false, false, cutlass::half_t, Flash_kernel_traits<96, 128, 64, 4, cutlass::half_t>>, Is_dropout=true, Is_causal=false, Is_even_N=false, Is_even_K=false, Return_softmax=false, Params=Flash_fwd_params]" 
(588): here
            instantiation of "void flash::compute_attn<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &) [with Kernel_traits=Flash_fwd_kernel_traits<96, 128, 64, 4, false, false, cutlass::half_t, Flash_kernel_traits<96, 128, 64, 4, cutlass::half_t>>, Is_dropout=true, Is_causal=false, Is_even_N=false, Is_even_K=false, Return_softmax=false, Params=Flash_fwd_params]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(15): here
            instantiation of "void flash_fwd_kernel<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax>(Flash_fwd_params) [with Kernel_traits=Flash_fwd_kernel_traits<96, 128, 64, 4, false, false, cutlass::half_t, Flash_kernel_traits<96, 128, 64, 4, cutlass::half_t>>, Is_dropout=true, Is_causal=false, Is_even_N=false, Is_even_K=false, Return_softmax=false]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(34): here
            instantiation of "void run_flash_fwd<Kernel_traits,Is_dropout,Is_causal>(Flash_fwd_params &, cudaStream_t) [with Kernel_traits=Flash_fwd_kernel_traits<96, 128, 64, 4, false, false, cutlass::half_t, Flash_kernel_traits<96, 128, 64, 4, cutlass::half_t>>, Is_dropout=true, Is_causal=false]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(92): here
            instantiation of "void run_mha_fwd_hdim96<T>(Flash_fwd_params &, cudaStream_t) [with T=cutlass::half_t]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_hdim96_fp16_sm80.cu(22): here

/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_kernel.h(140): warning: variable "MMA_M" was declared but never referenced
          detected during:
            instantiation of "void flash::compute_attn_1rowblock<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &, int, int, int) [with Kernel_traits=Flash_fwd_kernel_traits<96, 64, 64, 4, false, false, cutlass::half_t, Flash_kernel_traits<96, 64, 64, 4, cutlass::half_t>>, Is_dropout=false, Is_causal=true, Is_even_N=true, Is_even_K=true, Return_softmax=false, Params=Flash_fwd_params]" 
(588): here
            instantiation of "void flash::compute_attn<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &) [with Kernel_traits=Flash_fwd_kernel_traits<96, 64, 64, 4, false, false, cutlass::half_t, Flash_kernel_traits<96, 64, 64, 4, cutlass::half_t>>, Is_dropout=false, Is_causal=true, Is_even_N=true, Is_even_K=true, Return_softmax=false, Params=Flash_fwd_params]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(15): here
            instantiation of "void flash_fwd_kernel<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax>(Flash_fwd_params) [with Kernel_traits=Flash_fwd_kernel_traits<96, 64, 64, 4, false, false, cutlass::half_t, Flash_kernel_traits<96, 64, 64, 4, cutlass::half_t>>, Is_dropout=false, Is_causal=true, Is_even_N=true, Is_even_K=true, Return_softmax=false]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(34): here
            instantiation of "void run_flash_fwd<Kernel_traits,Is_dropout,Is_causal>(Flash_fwd_params &, cudaStream_t) [with Kernel_traits=Flash_fwd_kernel_traits<96, 64, 64, 4, false, false, cutlass::half_t, Flash_kernel_traits<96, 64, 64, 4, cutlass::half_t>>, Is_dropout=false, Is_causal=true]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(92): here
            instantiation of "void run_mha_fwd_hdim96<T>(Flash_fwd_params &, cudaStream_t) [with T=cutlass::half_t]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_hdim96_fp16_sm80.cu(22): here

/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_kernel.h(275): warning: variable "tQrQ" was declared but never referenced
          detected during:
            instantiation of "void flash::compute_attn_1rowblock<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &, int, int, int) [with Kernel_traits=Flash_fwd_kernel_traits<96, 64, 64, 4, false, false, cutlass::half_t, Flash_kernel_traits<96, 64, 64, 4, cutlass::half_t>>, Is_dropout=false, Is_causal=true, Is_even_N=true, Is_even_K=true, Return_softmax=false, Params=Flash_fwd_params]" 
(588): here
            instantiation of "void flash::compute_attn<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &) [with Kernel_traits=Flash_fwd_kernel_traits<96, 64, 64, 4, false, false, cutlass::half_t, Flash_kernel_traits<96, 64, 64, 4, cutlass::half_t>>, Is_dropout=false, Is_causal=true, Is_even_N=true, Is_even_K=true, Return_softmax=false, Params=Flash_fwd_params]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(15): here
            instantiation of "void flash_fwd_kernel<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax>(Flash_fwd_params) [with Kernel_traits=Flash_fwd_kernel_traits<96, 64, 64, 4, false, false, cutlass::half_t, Flash_kernel_traits<96, 64, 64, 4, cutlass::half_t>>, Is_dropout=false, Is_causal=true, Is_even_N=true, Is_even_K=true, Return_softmax=false]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(34): here
            instantiation of "void run_flash_fwd<Kernel_traits,Is_dropout,Is_causal>(Flash_fwd_params &, cudaStream_t) [with Kernel_traits=Flash_fwd_kernel_traits<96, 64, 64, 4, false, false, cutlass::half_t, Flash_kernel_traits<96, 64, 64, 4, cutlass::half_t>>, Is_dropout=false, Is_causal=true]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(92): here
            instantiation of "void run_mha_fwd_hdim96<T>(Flash_fwd_params &, cudaStream_t) [with T=cutlass::half_t]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_hdim96_fp16_sm80.cu(22): here

/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_kernel.h(140): warning: variable "MMA_M" was declared but never referenced
          detected during:
            instantiation of "void flash::compute_attn_1rowblock<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &, int, int, int) [with Kernel_traits=Flash_fwd_kernel_traits<96, 64, 64, 4, false, false, cutlass::half_t, Flash_kernel_traits<96, 64, 64, 4, cutlass::half_t>>, Is_dropout=false, Is_causal=true, Is_even_N=true, Is_even_K=false, Return_softmax=false, Params=Flash_fwd_params]" 
(588): here
            instantiation of "void flash::compute_attn<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &) [with Kernel_traits=Flash_fwd_kernel_traits<96, 64, 64, 4, false, false, cutlass::half_t, Flash_kernel_traits<96, 64, 64, 4, cutlass::half_t>>, Is_dropout=false, Is_causal=true, Is_even_N=true, Is_even_K=false, Return_softmax=false, Params=Flash_fwd_params]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(15): here
            instantiation of "void flash_fwd_kernel<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax>(Flash_fwd_params) [with Kernel_traits=Flash_fwd_kernel_traits<96, 64, 64, 4, false, false, cutlass::half_t, Flash_kernel_traits<96, 64, 64, 4, cutlass::half_t>>, Is_dropout=false, Is_causal=true, Is_even_N=true, Is_even_K=false, Return_softmax=false]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(34): here
            instantiation of "void run_flash_fwd<Kernel_traits,Is_dropout,Is_causal>(Flash_fwd_params &, cudaStream_t) [with Kernel_traits=Flash_fwd_kernel_traits<96, 64, 64, 4, false, false, cutlass::half_t, Flash_kernel_traits<96, 64, 64, 4, cutlass::half_t>>, Is_dropout=false, Is_causal=true]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(92): here
            instantiation of "void run_mha_fwd_hdim96<T>(Flash_fwd_params &, cudaStream_t) [with T=cutlass::half_t]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_hdim96_fp16_sm80.cu(22): here

/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_kernel.h(275): warning: variable "tQrQ" was declared but never referenced
          detected during:
            instantiation of "void flash::compute_attn_1rowblock<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &, int, int, int) [with Kernel_traits=Flash_fwd_kernel_traits<96, 64, 64, 4, false, false, cutlass::half_t, Flash_kernel_traits<96, 64, 64, 4, cutlass::half_t>>, Is_dropout=false, Is_causal=true, Is_even_N=true, Is_even_K=false, Return_softmax=false, Params=Flash_fwd_params]" 
(588): here
            instantiation of "void flash::compute_attn<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &) [with Kernel_traits=Flash_fwd_kernel_traits<96, 64, 64, 4, false, false, cutlass::half_t, Flash_kernel_traits<96, 64, 64, 4, cutlass::half_t>>, Is_dropout=false, Is_causal=true, Is_even_N=true, Is_even_K=false, Return_softmax=false, Params=Flash_fwd_params]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(15): here
            instantiation of "void flash_fwd_kernel<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax>(Flash_fwd_params) [with Kernel_traits=Flash_fwd_kernel_traits<96, 64, 64, 4, false, false, cutlass::half_t, Flash_kernel_traits<96, 64, 64, 4, cutlass::half_t>>, Is_dropout=false, Is_causal=true, Is_even_N=true, Is_even_K=false, Return_softmax=false]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(34): here
            instantiation of "void run_flash_fwd<Kernel_traits,Is_dropout,Is_causal>(Flash_fwd_params &, cudaStream_t) [with Kernel_traits=Flash_fwd_kernel_traits<96, 64, 64, 4, false, false, cutlass::half_t, Flash_kernel_traits<96, 64, 64, 4, cutlass::half_t>>, Is_dropout=false, Is_causal=true]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(92): here
            instantiation of "void run_mha_fwd_hdim96<T>(Flash_fwd_params &, cudaStream_t) [with T=cutlass::half_t]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_hdim96_fp16_sm80.cu(22): here

/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_kernel.h(140): warning: variable "MMA_M" was declared but never referenced
          detected during:
            instantiation of "void flash::compute_attn_1rowblock<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &, int, int, int) [with Kernel_traits=Flash_fwd_kernel_traits<96, 64, 64, 4, false, false, cutlass::half_t, Flash_kernel_traits<96, 64, 64, 4, cutlass::half_t>>, Is_dropout=false, Is_causal=true, Is_even_N=false, Is_even_K=true, Return_softmax=false, Params=Flash_fwd_params]" 
(588): here
            instantiation of "void flash::compute_attn<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &) [with Kernel_traits=Flash_fwd_kernel_traits<96, 64, 64, 4, false, false, cutlass::half_t, Flash_kernel_traits<96, 64, 64, 4, cutlass::half_t>>, Is_dropout=false, Is_causal=true, Is_even_N=false, Is_even_K=true, Return_softmax=false, Params=Flash_fwd_params]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(15): here
            instantiation of "void flash_fwd_kernel<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax>(Flash_fwd_params) [with Kernel_traits=Flash_fwd_kernel_traits<96, 64, 64, 4, false, false, cutlass::half_t, Flash_kernel_traits<96, 64, 64, 4, cutlass::half_t>>, Is_dropout=false, Is_causal=true, Is_even_N=false, Is_even_K=true, Return_softmax=false]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(34): here
            instantiation of "void run_flash_fwd<Kernel_traits,Is_dropout,Is_causal>(Flash_fwd_params &, cudaStream_t) [with Kernel_traits=Flash_fwd_kernel_traits<96, 64, 64, 4, false, false, cutlass::half_t, Flash_kernel_traits<96, 64, 64, 4, cutlass::half_t>>, Is_dropout=false, Is_causal=true]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(92): here
            instantiation of "void run_mha_fwd_hdim96<T>(Flash_fwd_params &, cudaStream_t) [with T=cutlass::half_t]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_hdim96_fp16_sm80.cu(22): here

/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_kernel.h(275): warning: variable "tQrQ" was declared but never referenced
          detected during:
            instantiation of "void flash::compute_attn_1rowblock<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &, int, int, int) [with Kernel_traits=Flash_fwd_kernel_traits<96, 64, 64, 4, false, false, cutlass::half_t, Flash_kernel_traits<96, 64, 64, 4, cutlass::half_t>>, Is_dropout=false, Is_causal=true, Is_even_N=false, Is_even_K=true, Return_softmax=false, Params=Flash_fwd_params]" 
(588): here
            instantiation of "void flash::compute_attn<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &) [with Kernel_traits=Flash_fwd_kernel_traits<96, 64, 64, 4, false, false, cutlass::half_t, Flash_kernel_traits<96, 64, 64, 4, cutlass::half_t>>, Is_dropout=false, Is_causal=true, Is_even_N=false, Is_even_K=true, Return_softmax=false, Params=Flash_fwd_params]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(15): here
            instantiation of "void flash_fwd_kernel<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax>(Flash_fwd_params) [with Kernel_traits=Flash_fwd_kernel_traits<96, 64, 64, 4, false, false, cutlass::half_t, Flash_kernel_traits<96, 64, 64, 4, cutlass::half_t>>, Is_dropout=false, Is_causal=true, Is_even_N=false, Is_even_K=true, Return_softmax=false]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(34): here
            instantiation of "void run_flash_fwd<Kernel_traits,Is_dropout,Is_causal>(Flash_fwd_params &, cudaStream_t) [with Kernel_traits=Flash_fwd_kernel_traits<96, 64, 64, 4, false, false, cutlass::half_t, Flash_kernel_traits<96, 64, 64, 4, cutlass::half_t>>, Is_dropout=false, Is_causal=true]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(92): here
            instantiation of "void run_mha_fwd_hdim96<T>(Flash_fwd_params &, cudaStream_t) [with T=cutlass::half_t]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_hdim96_fp16_sm80.cu(22): here

/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_kernel.h(140): warning: variable "MMA_M" was declared but never referenced
          detected during:
            instantiation of "void flash::compute_attn_1rowblock<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &, int, int, int) [with Kernel_traits=Flash_fwd_kernel_traits<96, 64, 64, 4, false, false, cutlass::half_t, Flash_kernel_traits<96, 64, 64, 4, cutlass::half_t>>, Is_dropout=false, Is_causal=true, Is_even_N=false, Is_even_K=false, Return_softmax=false, Params=Flash_fwd_params]" 
(588): here
            instantiation of "void flash::compute_attn<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &) [with Kernel_traits=Flash_fwd_kernel_traits<96, 64, 64, 4, false, false, cutlass::half_t, Flash_kernel_traits<96, 64, 64, 4, cutlass::half_t>>, Is_dropout=false, Is_causal=true, Is_even_N=false, Is_even_K=false, Return_softmax=false, Params=Flash_fwd_params]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(15): here
            instantiation of "void flash_fwd_kernel<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax>(Flash_fwd_params) [with Kernel_traits=Flash_fwd_kernel_traits<96, 64, 64, 4, false, false, cutlass::half_t, Flash_kernel_traits<96, 64, 64, 4, cutlass::half_t>>, Is_dropout=false, Is_causal=true, Is_even_N=false, Is_even_K=false, Return_softmax=false]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(34): here
            instantiation of "void run_flash_fwd<Kernel_traits,Is_dropout,Is_causal>(Flash_fwd_params &, cudaStream_t) [with Kernel_traits=Flash_fwd_kernel_traits<96, 64, 64, 4, false, false, cutlass::half_t, Flash_kernel_traits<96, 64, 64, 4, cutlass::half_t>>, Is_dropout=false, Is_causal=true]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(92): here
            instantiation of "void run_mha_fwd_hdim96<T>(Flash_fwd_params &, cudaStream_t) [with T=cutlass::half_t]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_hdim96_fp16_sm80.cu(22): here

/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_kernel.h(275): warning: variable "tQrQ" was declared but never referenced
          detected during:
            instantiation of "void flash::compute_attn_1rowblock<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &, int, int, int) [with Kernel_traits=Flash_fwd_kernel_traits<96, 64, 64, 4, false, false, cutlass::half_t, Flash_kernel_traits<96, 64, 64, 4, cutlass::half_t>>, Is_dropout=false, Is_causal=true, Is_even_N=false, Is_even_K=false, Return_softmax=false, Params=Flash_fwd_params]" 
(588): here
            instantiation of "void flash::compute_attn<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &) [with Kernel_traits=Flash_fwd_kernel_traits<96, 64, 64, 4, false, false, cutlass::half_t, Flash_kernel_traits<96, 64, 64, 4, cutlass::half_t>>, Is_dropout=false, Is_causal=true, Is_even_N=false, Is_even_K=false, Return_softmax=false, Params=Flash_fwd_params]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(15): here
            instantiation of "void flash_fwd_kernel<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax>(Flash_fwd_params) [with Kernel_traits=Flash_fwd_kernel_traits<96, 64, 64, 4, false, false, cutlass::half_t, Flash_kernel_traits<96, 64, 64, 4, cutlass::half_t>>, Is_dropout=false, Is_causal=true, Is_even_N=false, Is_even_K=false, Return_softmax=false]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(34): here
            instantiation of "void run_flash_fwd<Kernel_traits,Is_dropout,Is_causal>(Flash_fwd_params &, cudaStream_t) [with Kernel_traits=Flash_fwd_kernel_traits<96, 64, 64, 4, false, false, cutlass::half_t, Flash_kernel_traits<96, 64, 64, 4, cutlass::half_t>>, Is_dropout=false, Is_causal=true]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(92): here
            instantiation of "void run_mha_fwd_hdim96<T>(Flash_fwd_params &, cudaStream_t) [with T=cutlass::half_t]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_hdim96_fp16_sm80.cu(22): here

/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_kernel.h(140): warning: variable "MMA_M" was declared but never referenced
          detected during:
            instantiation of "void flash::compute_attn_1rowblock<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &, int, int, int) [with Kernel_traits=Flash_fwd_kernel_traits<96, 128, 64, 4, false, false, cutlass::half_t, Flash_kernel_traits<96, 128, 64, 4, cutlass::half_t>>, Is_dropout=false, Is_causal=true, Is_even_N=true, Is_even_K=true, Return_softmax=false, Params=Flash_fwd_params]" 
(588): here
            instantiation of "void flash::compute_attn<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &) [with Kernel_traits=Flash_fwd_kernel_traits<96, 128, 64, 4, false, false, cutlass::half_t, Flash_kernel_traits<96, 128, 64, 4, cutlass::half_t>>, Is_dropout=false, Is_causal=true, Is_even_N=true, Is_even_K=true, Return_softmax=false, Params=Flash_fwd_params]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(15): here
            instantiation of "void flash_fwd_kernel<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax>(Flash_fwd_params) [with Kernel_traits=Flash_fwd_kernel_traits<96, 128, 64, 4, false, false, cutlass::half_t, Flash_kernel_traits<96, 128, 64, 4, cutlass::half_t>>, Is_dropout=false, Is_causal=true, Is_even_N=true, Is_even_K=true, Return_softmax=false]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(34): here
            instantiation of "void run_flash_fwd<Kernel_traits,Is_dropout,Is_causal>(Flash_fwd_params &, cudaStream_t) [with Kernel_traits=Flash_fwd_kernel_traits<96, 128, 64, 4, false, false, cutlass::half_t, Flash_kernel_traits<96, 128, 64, 4, cutlass::half_t>>, Is_dropout=false, Is_causal=true]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(92): here
            instantiation of "void run_mha_fwd_hdim96<T>(Flash_fwd_params &, cudaStream_t) [with T=cutlass::half_t]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_hdim96_fp16_sm80.cu(22): here

/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_kernel.h(275): warning: variable "tQrQ" was declared but never referenced
          detected during:
            instantiation of "void flash::compute_attn_1rowblock<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &, int, int, int) [with Kernel_traits=Flash_fwd_kernel_traits<96, 128, 64, 4, false, false, cutlass::half_t, Flash_kernel_traits<96, 128, 64, 4, cutlass::half_t>>, Is_dropout=false, Is_causal=true, Is_even_N=true, Is_even_K=true, Return_softmax=false, Params=Flash_fwd_params]" 
(588): here
            instantiation of "void flash::compute_attn<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &) [with Kernel_traits=Flash_fwd_kernel_traits<96, 128, 64, 4, false, false, cutlass::half_t, Flash_kernel_traits<96, 128, 64, 4, cutlass::half_t>>, Is_dropout=false, Is_causal=true, Is_even_N=true, Is_even_K=true, Return_softmax=false, Params=Flash_fwd_params]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(15): here
            instantiation of "void flash_fwd_kernel<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax>(Flash_fwd_params) [with Kernel_traits=Flash_fwd_kernel_traits<96, 128, 64, 4, false, false, cutlass::half_t, Flash_kernel_traits<96, 128, 64, 4, cutlass::half_t>>, Is_dropout=false, Is_causal=true, Is_even_N=true, Is_even_K=true, Return_softmax=false]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(34): here
            instantiation of "void run_flash_fwd<Kernel_traits,Is_dropout,Is_causal>(Flash_fwd_params &, cudaStream_t) [with Kernel_traits=Flash_fwd_kernel_traits<96, 128, 64, 4, false, false, cutlass::half_t, Flash_kernel_traits<96, 128, 64, 4, cutlass::half_t>>, Is_dropout=false, Is_causal=true]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(92): here
            instantiation of "void run_mha_fwd_hdim96<T>(Flash_fwd_params &, cudaStream_t) [with T=cutlass::half_t]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_hdim96_fp16_sm80.cu(22): here

/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_kernel.h(140): warning: variable "MMA_M" was declared but never referenced
          detected during:
            instantiation of "void flash::compute_attn_1rowblock<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &, int, int, int) [with Kernel_traits=Flash_fwd_kernel_traits<96, 128, 64, 4, false, false, cutlass::half_t, Flash_kernel_traits<96, 128, 64, 4, cutlass::half_t>>, Is_dropout=false, Is_causal=true, Is_even_N=true, Is_even_K=false, Return_softmax=false, Params=Flash_fwd_params]" 
(588): here
            instantiation of "void flash::compute_attn<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &) [with Kernel_traits=Flash_fwd_kernel_traits<96, 128, 64, 4, false, false, cutlass::half_t, Flash_kernel_traits<96, 128, 64, 4, cutlass::half_t>>, Is_dropout=false, Is_causal=true, Is_even_N=true, Is_even_K=false, Return_softmax=false, Params=Flash_fwd_params]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(15): here
            instantiation of "void flash_fwd_kernel<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax>(Flash_fwd_params) [with Kernel_traits=Flash_fwd_kernel_traits<96, 128, 64, 4, false, false, cutlass::half_t, Flash_kernel_traits<96, 128, 64, 4, cutlass::half_t>>, Is_dropout=false, Is_causal=true, Is_even_N=true, Is_even_K=false, Return_softmax=false]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(34): here
            instantiation of "void run_flash_fwd<Kernel_traits,Is_dropout,Is_causal>(Flash_fwd_params &, cudaStream_t) [with Kernel_traits=Flash_fwd_kernel_traits<96, 128, 64, 4, false, false, cutlass::half_t, Flash_kernel_traits<96, 128, 64, 4, cutlass::half_t>>, Is_dropout=false, Is_causal=true]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(92): here
            instantiation of "void run_mha_fwd_hdim96<T>(Flash_fwd_params &, cudaStream_t) [with T=cutlass::half_t]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_hdim96_fp16_sm80.cu(22): here

/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_kernel.h(275): warning: variable "tQrQ" was declared but never referenced
          detected during:
            instantiation of "void flash::compute_attn_1rowblock<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &, int, int, int) [with Kernel_traits=Flash_fwd_kernel_traits<96, 128, 64, 4, false, false, cutlass::half_t, Flash_kernel_traits<96, 128, 64, 4, cutlass::half_t>>, Is_dropout=false, Is_causal=true, Is_even_N=true, Is_even_K=false, Return_softmax=false, Params=Flash_fwd_params]" 
(588): here
            instantiation of "void flash::compute_attn<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &) [with Kernel_traits=Flash_fwd_kernel_traits<96, 128, 64, 4, false, false, cutlass::half_t, Flash_kernel_traits<96, 128, 64, 4, cutlass::half_t>>, Is_dropout=false, Is_causal=true, Is_even_N=true, Is_even_K=false, Return_softmax=false, Params=Flash_fwd_params]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(15): here
            instantiation of "void flash_fwd_kernel<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax>(Flash_fwd_params) [with Kernel_traits=Flash_fwd_kernel_traits<96, 128, 64, 4, false, false, cutlass::half_t, Flash_kernel_traits<96, 128, 64, 4, cutlass::half_t>>, Is_dropout=false, Is_causal=true, Is_even_N=true, Is_even_K=false, Return_softmax=false]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(34): here
            instantiation of "void run_flash_fwd<Kernel_traits,Is_dropout,Is_causal>(Flash_fwd_params &, cudaStream_t) [with Kernel_traits=Flash_fwd_kernel_traits<96, 128, 64, 4, false, false, cutlass::half_t, Flash_kernel_traits<96, 128, 64, 4, cutlass::half_t>>, Is_dropout=false, Is_causal=true]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(92): here
            instantiation of "void run_mha_fwd_hdim96<T>(Flash_fwd_params &, cudaStream_t) [with T=cutlass::half_t]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_hdim96_fp16_sm80.cu(22): here

/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_kernel.h(140): warning: variable "MMA_M" was declared but never referenced
          detected during:
            instantiation of "void flash::compute_attn_1rowblock<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &, int, int, int) [with Kernel_traits=Flash_fwd_kernel_traits<96, 128, 64, 4, false, false, cutlass::half_t, Flash_kernel_traits<96, 128, 64, 4, cutlass::half_t>>, Is_dropout=false, Is_causal=true, Is_even_N=false, Is_even_K=true, Return_softmax=false, Params=Flash_fwd_params]" 
(588): here
            instantiation of "void flash::compute_attn<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &) [with Kernel_traits=Flash_fwd_kernel_traits<96, 128, 64, 4, false, false, cutlass::half_t, Flash_kernel_traits<96, 128, 64, 4, cutlass::half_t>>, Is_dropout=false, Is_causal=true, Is_even_N=false, Is_even_K=true, Return_softmax=false, Params=Flash_fwd_params]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(15): here
            instantiation of "void flash_fwd_kernel<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax>(Flash_fwd_params) [with Kernel_traits=Flash_fwd_kernel_traits<96, 128, 64, 4, false, false, cutlass::half_t, Flash_kernel_traits<96, 128, 64, 4, cutlass::half_t>>, Is_dropout=false, Is_causal=true, Is_even_N=false, Is_even_K=true, Return_softmax=false]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(34): here
            instantiation of "void run_flash_fwd<Kernel_traits,Is_dropout,Is_causal>(Flash_fwd_params &, cudaStream_t) [with Kernel_traits=Flash_fwd_kernel_traits<96, 128, 64, 4, false, false, cutlass::half_t, Flash_kernel_traits<96, 128, 64, 4, cutlass::half_t>>, Is_dropout=false, Is_causal=true]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(92): here
            instantiation of "void run_mha_fwd_hdim96<T>(Flash_fwd_params &, cudaStream_t) [with T=cutlass::half_t]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_hdim96_fp16_sm80.cu(22): here

/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_kernel.h(275): warning: variable "tQrQ" was declared but never referenced
          detected during:
            instantiation of "void flash::compute_attn_1rowblock<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &, int, int, int) [with Kernel_traits=Flash_fwd_kernel_traits<96, 128, 64, 4, false, false, cutlass::half_t, Flash_kernel_traits<96, 128, 64, 4, cutlass::half_t>>, Is_dropout=false, Is_causal=true, Is_even_N=false, Is_even_K=true, Return_softmax=false, Params=Flash_fwd_params]" 
(588): here
            instantiation of "void flash::compute_attn<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &) [with Kernel_traits=Flash_fwd_kernel_traits<96, 128, 64, 4, false, false, cutlass::half_t, Flash_kernel_traits<96, 128, 64, 4, cutlass::half_t>>, Is_dropout=false, Is_causal=true, Is_even_N=false, Is_even_K=true, Return_softmax=false, Params=Flash_fwd_params]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(15): here
            instantiation of "void flash_fwd_kernel<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax>(Flash_fwd_params) [with Kernel_traits=Flash_fwd_kernel_traits<96, 128, 64, 4, false, false, cutlass::half_t, Flash_kernel_traits<96, 128, 64, 4, cutlass::half_t>>, Is_dropout=false, Is_causal=true, Is_even_N=false, Is_even_K=true, Return_softmax=false]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(34): here
            instantiation of "void run_flash_fwd<Kernel_traits,Is_dropout,Is_causal>(Flash_fwd_params &, cudaStream_t) [with Kernel_traits=Flash_fwd_kernel_traits<96, 128, 64, 4, false, false, cutlass::half_t, Flash_kernel_traits<96, 128, 64, 4, cutlass::half_t>>, Is_dropout=false, Is_causal=true]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(92): here
            instantiation of "void run_mha_fwd_hdim96<T>(Flash_fwd_params &, cudaStream_t) [with T=cutlass::half_t]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_hdim96_fp16_sm80.cu(22): here

/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_kernel.h(140): warning: variable "MMA_M" was declared but never referenced
          detected during:
            instantiation of "void flash::compute_attn_1rowblock<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &, int, int, int) [with Kernel_traits=Flash_fwd_kernel_traits<96, 128, 64, 4, false, false, cutlass::half_t, Flash_kernel_traits<96, 128, 64, 4, cutlass::half_t>>, Is_dropout=false, Is_causal=true, Is_even_N=false, Is_even_K=false, Return_softmax=false, Params=Flash_fwd_params]" 
(588): here
            instantiation of "void flash::compute_attn<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &) [with Kernel_traits=Flash_fwd_kernel_traits<96, 128, 64, 4, false, false, cutlass::half_t, Flash_kernel_traits<96, 128, 64, 4, cutlass::half_t>>, Is_dropout=false, Is_causal=true, Is_even_N=false, Is_even_K=false, Return_softmax=false, Params=Flash_fwd_params]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(15): here
            instantiation of "void flash_fwd_kernel<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax>(Flash_fwd_params) [with Kernel_traits=Flash_fwd_kernel_traits<96, 128, 64, 4, false, false, cutlass::half_t, Flash_kernel_traits<96, 128, 64, 4, cutlass::half_t>>, Is_dropout=false, Is_causal=true, Is_even_N=false, Is_even_K=false, Return_softmax=false]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(34): here
            instantiation of "void run_flash_fwd<Kernel_traits,Is_dropout,Is_causal>(Flash_fwd_params &, cudaStream_t) [with Kernel_traits=Flash_fwd_kernel_traits<96, 128, 64, 4, false, false, cutlass::half_t, Flash_kernel_traits<96, 128, 64, 4, cutlass::half_t>>, Is_dropout=false, Is_causal=true]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(92): here
            instantiation of "void run_mha_fwd_hdim96<T>(Flash_fwd_params &, cudaStream_t) [with T=cutlass::half_t]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_hdim96_fp16_sm80.cu(22): here

/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_kernel.h(275): warning: variable "tQrQ" was declared but never referenced
          detected during:
            instantiation of "void flash::compute_attn_1rowblock<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &, int, int, int) [with Kernel_traits=Flash_fwd_kernel_traits<96, 128, 64, 4, false, false, cutlass::half_t, Flash_kernel_traits<96, 128, 64, 4, cutlass::half_t>>, Is_dropout=false, Is_causal=true, Is_even_N=false, Is_even_K=false, Return_softmax=false, Params=Flash_fwd_params]" 
(588): here
            instantiation of "void flash::compute_attn<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &) [with Kernel_traits=Flash_fwd_kernel_traits<96, 128, 64, 4, false, false, cutlass::half_t, Flash_kernel_traits<96, 128, 64, 4, cutlass::half_t>>, Is_dropout=false, Is_causal=true, Is_even_N=false, Is_even_K=false, Return_softmax=false, Params=Flash_fwd_params]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(15): here
            instantiation of "void flash_fwd_kernel<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax>(Flash_fwd_params) [with Kernel_traits=Flash_fwd_kernel_traits<96, 128, 64, 4, false, false, cutlass::half_t, Flash_kernel_traits<96, 128, 64, 4, cutlass::half_t>>, Is_dropout=false, Is_causal=true, Is_even_N=false, Is_even_K=false, Return_softmax=false]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(34): here
            instantiation of "void run_flash_fwd<Kernel_traits,Is_dropout,Is_causal>(Flash_fwd_params &, cudaStream_t) [with Kernel_traits=Flash_fwd_kernel_traits<96, 128, 64, 4, false, false, cutlass::half_t, Flash_kernel_traits<96, 128, 64, 4, cutlass::half_t>>, Is_dropout=false, Is_causal=true]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(92): here
            instantiation of "void run_mha_fwd_hdim96<T>(Flash_fwd_params &, cudaStream_t) [with T=cutlass::half_t]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_hdim96_fp16_sm80.cu(22): here

/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_kernel.h(140): warning: variable "MMA_M" was declared but never referenced
          detected during:
            instantiation of "void flash::compute_attn_1rowblock<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &, int, int, int) [with Kernel_traits=Flash_fwd_kernel_traits<96, 128, 64, 4, false, false, cutlass::half_t, Flash_kernel_traits<96, 128, 64, 4, cutlass::half_t>>, Is_dropout=false, Is_causal=false, Is_even_N=true, Is_even_K=true, Return_softmax=false, Params=Flash_fwd_params]" 
(588): here
            instantiation of "void flash::compute_attn<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &) [with Kernel_traits=Flash_fwd_kernel_traits<96, 128, 64, 4, false, false, cutlass::half_t, Flash_kernel_traits<96, 128, 64, 4, cutlass::half_t>>, Is_dropout=false, Is_causal=false, Is_even_N=true, Is_even_K=true, Return_softmax=false, Params=Flash_fwd_params]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(15): here
            instantiation of "void flash_fwd_kernel<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax>(Flash_fwd_params) [with Kernel_traits=Flash_fwd_kernel_traits<96, 128, 64, 4, false, false, cutlass::half_t, Flash_kernel_traits<96, 128, 64, 4, cutlass::half_t>>, Is_dropout=false, Is_causal=false, Is_even_N=true, Is_even_K=true, Return_softmax=false]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(34): here
            instantiation of "void run_flash_fwd<Kernel_traits,Is_dropout,Is_causal>(Flash_fwd_params &, cudaStream_t) [with Kernel_traits=Flash_fwd_kernel_traits<96, 128, 64, 4, false, false, cutlass::half_t, Flash_kernel_traits<96, 128, 64, 4, cutlass::half_t>>, Is_dropout=false, Is_causal=false]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(92): here
            instantiation of "void run_mha_fwd_hdim96<T>(Flash_fwd_params &, cudaStream_t) [with T=cutlass::half_t]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_hdim96_fp16_sm80.cu(22): here

/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_kernel.h(275): warning: variable "tQrQ" was declared but never referenced
          detected during:
            instantiation of "void flash::compute_attn_1rowblock<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &, int, int, int) [with Kernel_traits=Flash_fwd_kernel_traits<96, 128, 64, 4, false, false, cutlass::half_t, Flash_kernel_traits<96, 128, 64, 4, cutlass::half_t>>, Is_dropout=false, Is_causal=false, Is_even_N=true, Is_even_K=true, Return_softmax=false, Params=Flash_fwd_params]" 
(588): here
            instantiation of "void flash::compute_attn<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &) [with Kernel_traits=Flash_fwd_kernel_traits<96, 128, 64, 4, false, false, cutlass::half_t, Flash_kernel_traits<96, 128, 64, 4, cutlass::half_t>>, Is_dropout=false, Is_causal=false, Is_even_N=true, Is_even_K=true, Return_softmax=false, Params=Flash_fwd_params]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(15): here
            instantiation of "void flash_fwd_kernel<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax>(Flash_fwd_params) [with Kernel_traits=Flash_fwd_kernel_traits<96, 128, 64, 4, false, false, cutlass::half_t, Flash_kernel_traits<96, 128, 64, 4, cutlass::half_t>>, Is_dropout=false, Is_causal=false, Is_even_N=true, Is_even_K=true, Return_softmax=false]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(34): here
            instantiation of "void run_flash_fwd<Kernel_traits,Is_dropout,Is_causal>(Flash_fwd_params &, cudaStream_t) [with Kernel_traits=Flash_fwd_kernel_traits<96, 128, 64, 4, false, false, cutlass::half_t, Flash_kernel_traits<96, 128, 64, 4, cutlass::half_t>>, Is_dropout=false, Is_causal=false]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(92): here
            instantiation of "void run_mha_fwd_hdim96<T>(Flash_fwd_params &, cudaStream_t) [with T=cutlass::half_t]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_hdim96_fp16_sm80.cu(22): here

/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_kernel.h(140): warning: variable "MMA_M" was declared but never referenced
          detected during:
            instantiation of "void flash::compute_attn_1rowblock<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &, int, int, int) [with Kernel_traits=Flash_fwd_kernel_traits<96, 128, 64, 4, false, false, cutlass::half_t, Flash_kernel_traits<96, 128, 64, 4, cutlass::half_t>>, Is_dropout=false, Is_causal=false, Is_even_N=true, Is_even_K=false, Return_softmax=false, Params=Flash_fwd_params]" 
(588): here
            instantiation of "void flash::compute_attn<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &) [with Kernel_traits=Flash_fwd_kernel_traits<96, 128, 64, 4, false, false, cutlass::half_t, Flash_kernel_traits<96, 128, 64, 4, cutlass::half_t>>, Is_dropout=false, Is_causal=false, Is_even_N=true, Is_even_K=false, Return_softmax=false, Params=Flash_fwd_params]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(15): here
            instantiation of "void flash_fwd_kernel<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax>(Flash_fwd_params) [with Kernel_traits=Flash_fwd_kernel_traits<96, 128, 64, 4, false, false, cutlass::half_t, Flash_kernel_traits<96, 128, 64, 4, cutlass::half_t>>, Is_dropout=false, Is_causal=false, Is_even_N=true, Is_even_K=false, Return_softmax=false]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(34): here
            instantiation of "void run_flash_fwd<Kernel_traits,Is_dropout,Is_causal>(Flash_fwd_params &, cudaStream_t) [with Kernel_traits=Flash_fwd_kernel_traits<96, 128, 64, 4, false, false, cutlass::half_t, Flash_kernel_traits<96, 128, 64, 4, cutlass::half_t>>, Is_dropout=false, Is_causal=false]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(92): here
            instantiation of "void run_mha_fwd_hdim96<T>(Flash_fwd_params &, cudaStream_t) [with T=cutlass::half_t]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_hdim96_fp16_sm80.cu(22): here

/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_kernel.h(275): warning: variable "tQrQ" was declared but never referenced
          detected during:
            instantiation of "void flash::compute_attn_1rowblock<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &, int, int, int) [with Kernel_traits=Flash_fwd_kernel_traits<96, 128, 64, 4, false, false, cutlass::half_t, Flash_kernel_traits<96, 128, 64, 4, cutlass::half_t>>, Is_dropout=false, Is_causal=false, Is_even_N=true, Is_even_K=false, Return_softmax=false, Params=Flash_fwd_params]" 
(588): here
            instantiation of "void flash::compute_attn<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &) [with Kernel_traits=Flash_fwd_kernel_traits<96, 128, 64, 4, false, false, cutlass::half_t, Flash_kernel_traits<96, 128, 64, 4, cutlass::half_t>>, Is_dropout=false, Is_causal=false, Is_even_N=true, Is_even_K=false, Return_softmax=false, Params=Flash_fwd_params]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(15): here
            instantiation of "void flash_fwd_kernel<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax>(Flash_fwd_params) [with Kernel_traits=Flash_fwd_kernel_traits<96, 128, 64, 4, false, false, cutlass::half_t, Flash_kernel_traits<96, 128, 64, 4, cutlass::half_t>>, Is_dropout=false, Is_causal=false, Is_even_N=true, Is_even_K=false, Return_softmax=false]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(34): here
            instantiation of "void run_flash_fwd<Kernel_traits,Is_dropout,Is_causal>(Flash_fwd_params &, cudaStream_t) [with Kernel_traits=Flash_fwd_kernel_traits<96, 128, 64, 4, false, false, cutlass::half_t, Flash_kernel_traits<96, 128, 64, 4, cutlass::half_t>>, Is_dropout=false, Is_causal=false]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(92): here
            instantiation of "void run_mha_fwd_hdim96<T>(Flash_fwd_params &, cudaStream_t) [with T=cutlass::half_t]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_hdim96_fp16_sm80.cu(22): here

/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_kernel.h(140): warning: variable "MMA_M" was declared but never referenced
          detected during:
            instantiation of "void flash::compute_attn_1rowblock<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &, int, int, int) [with Kernel_traits=Flash_fwd_kernel_traits<96, 128, 64, 4, false, false, cutlass::half_t, Flash_kernel_traits<96, 128, 64, 4, cutlass::half_t>>, Is_dropout=false, Is_causal=false, Is_even_N=false, Is_even_K=true, Return_softmax=false, Params=Flash_fwd_params]" 
(588): here
            instantiation of "void flash::compute_attn<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &) [with Kernel_traits=Flash_fwd_kernel_traits<96, 128, 64, 4, false, false, cutlass::half_t, Flash_kernel_traits<96, 128, 64, 4, cutlass::half_t>>, Is_dropout=false, Is_causal=false, Is_even_N=false, Is_even_K=true, Return_softmax=false, Params=Flash_fwd_params]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(15): here
            instantiation of "void flash_fwd_kernel<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax>(Flash_fwd_params) [with Kernel_traits=Flash_fwd_kernel_traits<96, 128, 64, 4, false, false, cutlass::half_t, Flash_kernel_traits<96, 128, 64, 4, cutlass::half_t>>, Is_dropout=false, Is_causal=false, Is_even_N=false, Is_even_K=true, Return_softmax=false]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(34): here
            instantiation of "void run_flash_fwd<Kernel_traits,Is_dropout,Is_causal>(Flash_fwd_params &, cudaStream_t) [with Kernel_traits=Flash_fwd_kernel_traits<96, 128, 64, 4, false, false, cutlass::half_t, Flash_kernel_traits<96, 128, 64, 4, cutlass::half_t>>, Is_dropout=false, Is_causal=false]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(92): here
            instantiation of "void run_mha_fwd_hdim96<T>(Flash_fwd_params &, cudaStream_t) [with T=cutlass::half_t]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_hdim96_fp16_sm80.cu(22): here

/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_kernel.h(275): warning: variable "tQrQ" was declared but never referenced
          detected during:
            instantiation of "void flash::compute_attn_1rowblock<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &, int, int, int) [with Kernel_traits=Flash_fwd_kernel_traits<96, 128, 64, 4, false, false, cutlass::half_t, Flash_kernel_traits<96, 128, 64, 4, cutlass::half_t>>, Is_dropout=false, Is_causal=false, Is_even_N=false, Is_even_K=true, Return_softmax=false, Params=Flash_fwd_params]" 
(588): here
            instantiation of "void flash::compute_attn<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &) [with Kernel_traits=Flash_fwd_kernel_traits<96, 128, 64, 4, false, false, cutlass::half_t, Flash_kernel_traits<96, 128, 64, 4, cutlass::half_t>>, Is_dropout=false, Is_causal=false, Is_even_N=false, Is_even_K=true, Return_softmax=false, Params=Flash_fwd_params]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(15): here
            instantiation of "void flash_fwd_kernel<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax>(Flash_fwd_params) [with Kernel_traits=Flash_fwd_kernel_traits<96, 128, 64, 4, false, false, cutlass::half_t, Flash_kernel_traits<96, 128, 64, 4, cutlass::half_t>>, Is_dropout=false, Is_causal=false, Is_even_N=false, Is_even_K=true, Return_softmax=false]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(34): here
            instantiation of "void run_flash_fwd<Kernel_traits,Is_dropout,Is_causal>(Flash_fwd_params &, cudaStream_t) [with Kernel_traits=Flash_fwd_kernel_traits<96, 128, 64, 4, false, false, cutlass::half_t, Flash_kernel_traits<96, 128, 64, 4, cutlass::half_t>>, Is_dropout=false, Is_causal=false]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(92): here
            instantiation of "void run_mha_fwd_hdim96<T>(Flash_fwd_params &, cudaStream_t) [with T=cutlass::half_t]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_hdim96_fp16_sm80.cu(22): here

/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_kernel.h(140): warning: variable "MMA_M" was declared but never referenced
          detected during:
            instantiation of "void flash::compute_attn_1rowblock<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &, int, int, int) [with Kernel_traits=Flash_fwd_kernel_traits<96, 128, 64, 4, false, false, cutlass::half_t, Flash_kernel_traits<96, 128, 64, 4, cutlass::half_t>>, Is_dropout=false, Is_causal=false, Is_even_N=false, Is_even_K=false, Return_softmax=false, Params=Flash_fwd_params]" 
(588): here
            instantiation of "void flash::compute_attn<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &) [with Kernel_traits=Flash_fwd_kernel_traits<96, 128, 64, 4, false, false, cutlass::half_t, Flash_kernel_traits<96, 128, 64, 4, cutlass::half_t>>, Is_dropout=false, Is_causal=false, Is_even_N=false, Is_even_K=false, Return_softmax=false, Params=Flash_fwd_params]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(15): here
            instantiation of "void flash_fwd_kernel<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax>(Flash_fwd_params) [with Kernel_traits=Flash_fwd_kernel_traits<96, 128, 64, 4, false, false, cutlass::half_t, Flash_kernel_traits<96, 128, 64, 4, cutlass::half_t>>, Is_dropout=false, Is_causal=false, Is_even_N=false, Is_even_K=false, Return_softmax=false]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(34): here
            instantiation of "void run_flash_fwd<Kernel_traits,Is_dropout,Is_causal>(Flash_fwd_params &, cudaStream_t) [with Kernel_traits=Flash_fwd_kernel_traits<96, 128, 64, 4, false, false, cutlass::half_t, Flash_kernel_traits<96, 128, 64, 4, cutlass::half_t>>, Is_dropout=false, Is_causal=false]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(92): here
            instantiation of "void run_mha_fwd_hdim96<T>(Flash_fwd_params &, cudaStream_t) [with T=cutlass::half_t]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_hdim96_fp16_sm80.cu(22): here

/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_kernel.h(275): warning: variable "tQrQ" was declared but never referenced
          detected during:
            instantiation of "void flash::compute_attn_1rowblock<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &, int, int, int) [with Kernel_traits=Flash_fwd_kernel_traits<96, 128, 64, 4, false, false, cutlass::half_t, Flash_kernel_traits<96, 128, 64, 4, cutlass::half_t>>, Is_dropout=false, Is_causal=false, Is_even_N=false, Is_even_K=false, Return_softmax=false, Params=Flash_fwd_params]" 
(588): here
            instantiation of "void flash::compute_attn<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &) [with Kernel_traits=Flash_fwd_kernel_traits<96, 128, 64, 4, false, false, cutlass::half_t, Flash_kernel_traits<96, 128, 64, 4, cutlass::half_t>>, Is_dropout=false, Is_causal=false, Is_even_N=false, Is_even_K=false, Return_softmax=false, Params=Flash_fwd_params]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(15): here
            instantiation of "void flash_fwd_kernel<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax>(Flash_fwd_params) [with Kernel_traits=Flash_fwd_kernel_traits<96, 128, 64, 4, false, false, cutlass::half_t, Flash_kernel_traits<96, 128, 64, 4, cutlass::half_t>>, Is_dropout=false, Is_causal=false, Is_even_N=false, Is_even_K=false, Return_softmax=false]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(34): here
            instantiation of "void run_flash_fwd<Kernel_traits,Is_dropout,Is_causal>(Flash_fwd_params &, cudaStream_t) [with Kernel_traits=Flash_fwd_kernel_traits<96, 128, 64, 4, false, false, cutlass::half_t, Flash_kernel_traits<96, 128, 64, 4, cutlass::half_t>>, Is_dropout=false, Is_causal=false]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(92): here
            instantiation of "void run_mha_fwd_hdim96<T>(Flash_fwd_params &, cudaStream_t) [with T=cutlass::half_t]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_hdim96_fp16_sm80.cu(22): here

ptxas info    : 2 bytes gmem
ptxas info    : Compiling entry function '_Z16flash_fwd_kernelI23Flash_fwd_kernel_traitsILi96ELi128ELi64ELi4ELb0ELb0EN7cutlass6half_tE19Flash_kernel_traitsILi96ELi128ELi64ELi4ES2_EELb0ELb0ELb0ELb0ELb0EEv16Flash_fwd_params' for 'sm_80'
ptxas info    : Function properties for _Z16flash_fwd_kernelI23Flash_fwd_kernel_traitsILi96ELi128ELi64ELi4ELb0ELb0EN7cutlass6half_tE19Flash_kernel_traitsILi96ELi128ELi64ELi4ES2_EELb0ELb0ELb0ELb0ELb0EEv16Flash_fwd_params
    0 bytes stack frame, 0 bytes spill stores, 0 bytes spill loads
ptxas info    : Used 240 registers, 584 bytes cmem[0]
ptxas info    : Compiling entry function '_Z16flash_fwd_kernelI23Flash_fwd_kernel_traitsILi96ELi128ELi64ELi4ELb0ELb0EN7cutlass6half_tE19Flash_kernel_traitsILi96ELi128ELi64ELi4ES2_EELb0ELb0ELb0ELb1ELb0EEv16Flash_fwd_params' for 'sm_80'
ptxas info    : Function properties for _Z16flash_fwd_kernelI23Flash_fwd_kernel_traitsILi96ELi128ELi64ELi4ELb0ELb0EN7cutlass6half_tE19Flash_kernel_traitsILi96ELi128ELi64ELi4ES2_EELb0ELb0ELb0ELb1ELb0EEv16Flash_fwd_params
    0 bytes stack frame, 0 bytes spill stores, 0 bytes spill loads
ptxas info    : Used 237 registers, 584 bytes cmem[0]
ptxas info    : Compiling entry function '_Z16flash_fwd_kernelI23Flash_fwd_kernel_traitsILi96ELi128ELi64ELi4ELb0ELb0EN7cutlass6half_tE19Flash_kernel_traitsILi96ELi128ELi64ELi4ES2_EELb0ELb0ELb1ELb0ELb0EEv16Flash_fwd_params' for 'sm_80'
ptxas info    : Function properties for _Z16flash_fwd_kernelI23Flash_fwd_kernel_traitsILi96ELi128ELi64ELi4ELb0ELb0EN7cutlass6half_tE19Flash_kernel_traitsILi96ELi128ELi64ELi4ES2_EELb0ELb0ELb1ELb0ELb0EEv16Flash_fwd_params
    0 bytes stack frame, 0 bytes spill stores, 0 bytes spill loads
ptxas info    : Used 239 registers, 584 bytes cmem[0]
ptxas info    : Compiling entry function '_Z16flash_fwd_kernelI23Flash_fwd_kernel_traitsILi96ELi128ELi64ELi4ELb0ELb0EN7cutlass6half_tE19Flash_kernel_traitsILi96ELi128ELi64ELi4ES2_EELb0ELb0ELb1ELb1ELb0EEv16Flash_fwd_params' for 'sm_80'
ptxas info    : Function properties for _Z16flash_fwd_kernelI23Flash_fwd_kernel_traitsILi96ELi128ELi64ELi4ELb0ELb0EN7cutlass6half_tE19Flash_kernel_traitsILi96ELi128ELi64ELi4ES2_EELb0ELb0ELb1ELb1ELb0EEv16Flash_fwd_params
    0 bytes stack frame, 0 bytes spill stores, 0 bytes spill loads
ptxas info    : Used 237 registers, 584 bytes cmem[0]
ptxas info    : Compiling entry function '_Z16flash_fwd_kernelI23Flash_fwd_kernel_traitsILi96ELi128ELi64ELi4ELb0ELb0EN7cutlass6half_tE19Flash_kernel_traitsILi96ELi128ELi64ELi4ES2_EELb0ELb1ELb0ELb0ELb0EEv16Flash_fwd_params' for 'sm_80'
ptxas info    : Function properties for _Z16flash_fwd_kernelI23Flash_fwd_kernel_traitsILi96ELi128ELi64ELi4ELb0ELb0EN7cutlass6half_tE19Flash_kernel_traitsILi96ELi128ELi64ELi4ES2_EELb0ELb1ELb0ELb0ELb0EEv16Flash_fwd_params
    0 bytes stack frame, 0 bytes spill stores, 0 bytes spill loads
ptxas info    : Used 255 registers, 584 bytes cmem[0]
ptxas info    : Compiling entry function '_Z16flash_fwd_kernelI23Flash_fwd_kernel_traitsILi96ELi128ELi64ELi4ELb0ELb0EN7cutlass6half_tE19Flash_kernel_traitsILi96ELi128ELi64ELi4ES2_EELb0ELb1ELb0ELb1ELb0EEv16Flash_fwd_params' for 'sm_80'
ptxas info    : Function properties for _Z16flash_fwd_kernelI23Flash_fwd_kernel_traitsILi96ELi128ELi64ELi4ELb0ELb0EN7cutlass6half_tE19Flash_kernel_traitsILi96ELi128ELi64ELi4ES2_EELb0ELb1ELb0ELb1ELb0EEv16Flash_fwd_params
    0 bytes stack frame, 0 bytes spill stores, 0 bytes spill loads
ptxas info    : Used 254 registers, 584 bytes cmem[0]
ptxas info    : Compiling entry function '_Z16flash_fwd_kernelI23Flash_fwd_kernel_traitsILi96ELi128ELi64ELi4ELb0ELb0EN7cutlass6half_tE19Flash_kernel_traitsILi96ELi128ELi64ELi4ES2_EELb0ELb1ELb1ELb0ELb0EEv16Flash_fwd_params' for 'sm_80'
ptxas info    : Function properties for _Z16flash_fwd_kernelI23Flash_fwd_kernel_traitsILi96ELi128ELi64ELi4ELb0ELb0EN7cutlass6half_tE19Flash_kernel_traitsILi96ELi128ELi64ELi4ES2_EELb0ELb1ELb1ELb0ELb0EEv16Flash_fwd_params
    0 bytes stack frame, 0 bytes spill stores, 0 bytes spill loads
ptxas info    : Used 254 registers, 584 bytes cmem[0]
ptxas info    : Compiling entry function '_Z16flash_fwd_kernelI23Flash_fwd_kernel_traitsILi96ELi128ELi64ELi4ELb0ELb0EN7cutlass6half_tE19Flash_kernel_traitsILi96ELi128ELi64ELi4ES2_EELb0ELb1ELb1ELb1ELb0EEv16Flash_fwd_params' for 'sm_80'
ptxas info    : Function properties for _Z16flash_fwd_kernelI23Flash_fwd_kernel_traitsILi96ELi128ELi64ELi4ELb0ELb0EN7cutlass6half_tE19Flash_kernel_traitsILi96ELi128ELi64ELi4ES2_EELb0ELb1ELb1ELb1ELb0EEv16Flash_fwd_params
    0 bytes stack frame, 0 bytes spill stores, 0 bytes spill loads
ptxas info    : Used 255 registers, 584 bytes cmem[0]
ptxas info    : Compiling entry function '_Z16flash_fwd_kernelI23Flash_fwd_kernel_traitsILi96ELi64ELi64ELi4ELb0ELb0EN7cutlass6half_tE19Flash_kernel_traitsILi96ELi64ELi64ELi4ES2_EELb0ELb1ELb0ELb0ELb0EEv16Flash_fwd_params' for 'sm_80'
ptxas info    : Function properties for _Z16flash_fwd_kernelI23Flash_fwd_kernel_traitsILi96ELi64ELi64ELi4ELb0ELb0EN7cutlass6half_tE19Flash_kernel_traitsILi96ELi64ELi64ELi4ES2_EELb0ELb1ELb0ELb0ELb0EEv16Flash_fwd_params
    0 bytes stack frame, 0 bytes spill stores, 0 bytes spill loads
ptxas info    : Used 133 registers, 584 bytes cmem[0]
ptxas info    : Compiling entry function '_Z16flash_fwd_kernelI23Flash_fwd_kernel_traitsILi96ELi64ELi64ELi4ELb0ELb0EN7cutlass6half_tE19Flash_kernel_traitsILi96ELi64ELi64ELi4ES2_EELb0ELb1ELb0ELb1ELb0EEv16Flash_fwd_params' for 'sm_80'
ptxas info    : Function properties for _Z16flash_fwd_kernelI23Flash_fwd_kernel_traitsILi96ELi64ELi64ELi4ELb0ELb0EN7cutlass6half_tE19Flash_kernel_traitsILi96ELi64ELi64ELi4ES2_EELb0ELb1ELb0ELb1ELb0EEv16Flash_fwd_params
    0 bytes stack frame, 0 bytes spill stores, 0 bytes spill loads
ptxas info    : Used 130 registers, 584 bytes cmem[0]
ptxas info    : Compiling entry function '_Z16flash_fwd_kernelI23Flash_fwd_kernel_traitsILi96ELi64ELi64ELi4ELb0ELb0EN7cutlass6half_tE19Flash_kernel_traitsILi96ELi64ELi64ELi4ES2_EELb0ELb1ELb1ELb0ELb0EEv16Flash_fwd_params' for 'sm_80'
ptxas info    : Function properties for _Z16flash_fwd_kernelI23Flash_fwd_kernel_traitsILi96ELi64ELi64ELi4ELb0ELb0EN7cutlass6half_tE19Flash_kernel_traitsILi96ELi64ELi64ELi4ES2_EELb0ELb1ELb1ELb0ELb0EEv16Flash_fwd_params
    0 bytes stack frame, 0 bytes spill stores, 0 bytes spill loads
ptxas info    : Used 130 registers, 584 bytes cmem[0]
ptxas info    : Compiling entry function '_Z16flash_fwd_kernelI23Flash_fwd_kernel_traitsILi96ELi64ELi64ELi4ELb0ELb0EN7cutlass6half_tE19Flash_kernel_traitsILi96ELi64ELi64ELi4ES2_EELb0ELb1ELb1ELb1ELb0EEv16Flash_fwd_params' for 'sm_80'
ptxas info    : Function properties for _Z16flash_fwd_kernelI23Flash_fwd_kernel_traitsILi96ELi64ELi64ELi4ELb0ELb0EN7cutlass6half_tE19Flash_kernel_traitsILi96ELi64ELi64ELi4ES2_EELb0ELb1ELb1ELb1ELb0EEv16Flash_fwd_params
    0 bytes stack frame, 0 bytes spill stores, 0 bytes spill loads
ptxas info    : Used 133 registers, 584 bytes cmem[0]
ptxas info    : Compiling entry function '_Z16flash_fwd_kernelI23Flash_fwd_kernel_traitsILi96ELi128ELi64ELi4ELb0ELb0EN7cutlass6half_tE19Flash_kernel_traitsILi96ELi128ELi64ELi4ES2_EELb1ELb0ELb0ELb0ELb0EEv16Flash_fwd_params' for 'sm_80'
ptxas info    : Function properties for _Z16flash_fwd_kernelI23Flash_fwd_kernel_traitsILi96ELi128ELi64ELi4ELb0ELb0EN7cutlass6half_tE19Flash_kernel_traitsILi96ELi128ELi64ELi4ES2_EELb1ELb0ELb0ELb0ELb0EEv16Flash_fwd_params
    16 bytes stack frame, 16 bytes spill stores, 16 bytes spill loads
ptxas info    : Used 255 registers, 584 bytes cmem[0]
ptxas info    : Compiling entry function '_Z16flash_fwd_kernelI23Flash_fwd_kernel_traitsILi96ELi128ELi64ELi4ELb0ELb0EN7cutlass6half_tE19Flash_kernel_traitsILi96ELi128ELi64ELi4ES2_EELb1ELb0ELb0ELb0ELb1EEv16Flash_fwd_params' for 'sm_80'
ptxas info    : Function properties for _Z16flash_fwd_kernelI23Flash_fwd_kernel_traitsILi96ELi128ELi64ELi4ELb0ELb0EN7cutlass6half_tE19Flash_kernel_traitsILi96ELi128ELi64ELi4ES2_EELb1ELb0ELb0ELb0ELb1EEv16Flash_fwd_params
    256 bytes stack frame, 184 bytes spill stores, 184 bytes spill loads
ptxas info    : Used 255 registers, 584 bytes cmem[0]
ptxas info    : Compiling entry function '_Z16flash_fwd_kernelI23Flash_fwd_kernel_traitsILi96ELi128ELi64ELi4ELb0ELb0EN7cutlass6half_tE19Flash_kernel_traitsILi96ELi128ELi64ELi4ES2_EELb1ELb0ELb0ELb1ELb0EEv16Flash_fwd_params' for 'sm_80'
ptxas info    : Function properties for _Z16flash_fwd_kernelI23Flash_fwd_kernel_traitsILi96ELi128ELi64ELi4ELb0ELb0EN7cutlass6half_tE19Flash_kernel_traitsILi96ELi128ELi64ELi4ES2_EELb1ELb0ELb0ELb1ELb0EEv16Flash_fwd_params
    0 bytes stack frame, 0 bytes spill stores, 0 bytes spill loads
ptxas info    : Used 255 registers, 584 bytes cmem[0]
ptxas info    : Compiling entry function '_Z16flash_fwd_kernelI23Flash_fwd_kernel_traitsILi96ELi128ELi64ELi4ELb0ELb0EN7cutlass6half_tE19Flash_kernel_traitsILi96ELi128ELi64ELi4ES2_EELb1ELb0ELb0ELb1ELb1EEv16Flash_fwd_params' for 'sm_80'
ptxas info    : Function properties for _Z16flash_fwd_kernelI23Flash_fwd_kernel_traitsILi96ELi128ELi64ELi4ELb0ELb0EN7cutlass6half_tE19Flash_kernel_traitsILi96ELi128ELi64ELi4ES2_EELb1ELb0ELb0ELb1ELb1EEv16Flash_fwd_params
    184 bytes stack frame, 146 bytes spill stores, 130 bytes spill loads
ptxas info    : Used 255 registers, 584 bytes cmem[0]
ptxas info    : Compiling entry function '_Z16flash_fwd_kernelI23Flash_fwd_kernel_traitsILi96ELi128ELi64ELi4ELb0ELb0EN7cutlass6half_tE19Flash_kernel_traitsILi96ELi128ELi64ELi4ES2_EELb1ELb0ELb1ELb0ELb0EEv16Flash_fwd_params' for 'sm_80'
ptxas info    : Function properties for _Z16flash_fwd_kernelI23Flash_fwd_kernel_traitsILi96ELi128ELi64ELi4ELb0ELb0EN7cutlass6half_tE19Flash_kernel_traitsILi96ELi128ELi64ELi4ES2_EELb1ELb0ELb1ELb0ELb0EEv16Flash_fwd_params
    0 bytes stack frame, 0 bytes spill stores, 0 bytes spill loads
ptxas info    : Used 255 registers, 584 bytes cmem[0]
ptxas info    : Compiling entry function '_Z16flash_fwd_kernelI23Flash_fwd_kernel_traitsILi96ELi128ELi64ELi4ELb0ELb0EN7cutlass6half_tE19Flash_kernel_traitsILi96ELi128ELi64ELi4ES2_EELb1ELb0ELb1ELb0ELb1EEv16Flash_fwd_params' for 'sm_80'
ptxas info    : Function properties for _Z16flash_fwd_kernelI23Flash_fwd_kernel_traitsILi96ELi128ELi64ELi4ELb0ELb0EN7cutlass6half_tE19Flash_kernel_traitsILi96ELi128ELi64ELi4ES2_EELb1ELb0ELb1ELb0ELb1EEv16Flash_fwd_params
    200 bytes stack frame, 132 bytes spill stores, 132 bytes spill loads
ptxas info    : Used 255 registers, 584 bytes cmem[0]
ptxas info    : Compiling entry function '_Z16flash_fwd_kernelI23Flash_fwd_kernel_traitsILi96ELi128ELi64ELi4ELb0ELb0EN7cutlass6half_tE19Flash_kernel_traitsILi96ELi128ELi64ELi4ES2_EELb1ELb0ELb1ELb1ELb0EEv16Flash_fwd_params' for 'sm_80'
ptxas info    : Function properties for _Z16flash_fwd_kernelI23Flash_fwd_kernel_traitsILi96ELi128ELi64ELi4ELb0ELb0EN7cutlass6half_tE19Flash_kernel_traitsILi96ELi128ELi64ELi4ES2_EELb1ELb0ELb1ELb1ELb0EEv16Flash_fwd_params
    0 bytes stack frame, 0 bytes spill stores, 0 bytes spill loads
ptxas info    : Used 255 registers, 584 bytes cmem[0]
ptxas info    : Compiling entry function '_Z16flash_fwd_kernelI23Flash_fwd_kernel_traitsILi96ELi128ELi64ELi4ELb0ELb0EN7cutlass6half_tE19Flash_kernel_traitsILi96ELi128ELi64ELi4ES2_EELb1ELb0ELb1ELb1ELb1EEv16Flash_fwd_params' for 'sm_80'
ptxas info    : Function properties for _Z16flash_fwd_kernelI23Flash_fwd_kernel_traitsILi96ELi128ELi64ELi4ELb0ELb0EN7cutlass6half_tE19Flash_kernel_traitsILi96ELi128ELi64ELi4ES2_EELb1ELb0ELb1ELb1ELb1EEv16Flash_fwd_params
    240 bytes stack frame, 200 bytes spill stores, 200 bytes spill loads
ptxas info    : Used 255 registers, 584 bytes cmem[0]
ptxas info    : Compiling entry function '_Z16flash_fwd_kernelI23Flash_fwd_kernel_traitsILi96ELi128ELi64ELi4ELb0ELb0EN7cutlass6half_tE19Flash_kernel_traitsILi96ELi128ELi64ELi4ES2_EELb1ELb1ELb0ELb0ELb0EEv16Flash_fwd_params' for 'sm_80'
ptxas info    : Function properties for _Z16flash_fwd_kernelI23Flash_fwd_kernel_traitsILi96ELi128ELi64ELi4ELb0ELb0EN7cutlass6half_tE19Flash_kernel_traitsILi96ELi128ELi64ELi4ES2_EELb1ELb1ELb0ELb0ELb0EEv16Flash_fwd_params
    48 bytes stack frame, 44 bytes spill stores, 44 bytes spill loads
ptxas info    : Used 255 registers, 584 bytes cmem[0]
ptxas info    : Compiling entry function '_Z16flash_fwd_kernelI23Flash_fwd_kernel_traitsILi96ELi128ELi64ELi4ELb0ELb0EN7cutlass6half_tE19Flash_kernel_traitsILi96ELi128ELi64ELi4ES2_EELb1ELb1ELb0ELb0ELb1EEv16Flash_fwd_params' for 'sm_80'
ptxas info    : Function properties for _Z16flash_fwd_kernelI23Flash_fwd_kernel_traitsILi96ELi128ELi64ELi4ELb0ELb0EN7cutlass6half_tE19Flash_kernel_traitsILi96ELi128ELi64ELi4ES2_EELb1ELb1ELb0ELb0ELb1EEv16Flash_fwd_params
    224 bytes stack frame, 252 bytes spill stores, 288 bytes spill loads
ptxas info    : Used 255 registers, 584 bytes cmem[0]
ptxas info    : Compiling entry function '_Z16flash_fwd_kernelI23Flash_fwd_kernel_traitsILi96ELi128ELi64ELi4ELb0ELb0EN7cutlass6half_tE19Flash_kernel_traitsILi96ELi128ELi64ELi4ES2_EELb1ELb1ELb0ELb1ELb0EEv16Flash_fwd_params' for 'sm_80'
ptxas info    : Function properties for _Z16flash_fwd_kernelI23Flash_fwd_kernel_traitsILi96ELi128ELi64ELi4ELb0ELb0EN7cutlass6half_tE19Flash_kernel_traitsILi96ELi128ELi64ELi4ES2_EELb1ELb1ELb0ELb1ELb0EEv16Flash_fwd_params
    8 bytes stack frame, 8 bytes spill stores, 8 bytes spill loads
ptxas info    : Used 255 registers, 584 bytes cmem[0]
ptxas info    : Compiling entry function '_Z16flash_fwd_kernelI23Flash_fwd_kernel_traitsILi96ELi128ELi64ELi4ELb0ELb0EN7cutlass6half_tE19Flash_kernel_traitsILi96ELi128ELi64ELi4ES2_EELb1ELb1ELb0ELb1ELb1EEv16Flash_fwd_params' for 'sm_80'
ptxas info    : Function properties for _Z16flash_fwd_kernelI23Flash_fwd_kernel_traitsILi96ELi128ELi64ELi4ELb0ELb0EN7cutlass6half_tE19Flash_kernel_traitsILi96ELi128ELi64ELi4ES2_EELb1ELb1ELb0ELb1ELb1EEv16Flash_fwd_params
    208 bytes stack frame, 272 bytes spill stores, 294 bytes spill loads
ptxas info    : Used 255 registers, 584 bytes cmem[0]
ptxas info    : Compiling entry function '_Z16flash_fwd_kernelI23Flash_fwd_kernel_traitsILi96ELi128ELi64ELi4ELb0ELb0EN7cutlass6half_tE19Flash_kernel_traitsILi96ELi128ELi64ELi4ES2_EELb1ELb1ELb1ELb0ELb0EEv16Flash_fwd_params' for 'sm_80'
ptxas info    : Function properties for _Z16flash_fwd_kernelI23Flash_fwd_kernel_traitsILi96ELi128ELi64ELi4ELb0ELb0EN7cutlass6half_tE19Flash_kernel_traitsILi96ELi128ELi64ELi4ES2_EELb1ELb1ELb1ELb0ELb0EEv16Flash_fwd_params
    40 bytes stack frame, 32 bytes spill stores, 32 bytes spill loads
ptxas info    : Used 255 registers, 584 bytes cmem[0]
ptxas info    : Compiling entry function '_Z16flash_fwd_kernelI23Flash_fwd_kernel_traitsILi96ELi128ELi64ELi4ELb0ELb0EN7cutlass6half_tE19Flash_kernel_traitsILi96ELi128ELi64ELi4ES2_EELb1ELb1ELb1ELb0ELb1EEv16Flash_fwd_params' for 'sm_80'
ptxas info    : Function properties for _Z16flash_fwd_kernelI23Flash_fwd_kernel_traitsILi96ELi128ELi64ELi4ELb0ELb0EN7cutlass6half_tE19Flash_kernel_traitsILi96ELi128ELi64ELi4ES2_EELb1ELb1ELb1ELb0ELb1EEv16Flash_fwd_params
    272 bytes stack frame, 250 bytes spill stores, 254 bytes spill loads
ptxas info    : Used 255 registers, 584 bytes cmem[0]
ptxas info    : Compiling entry function '_Z16flash_fwd_kernelI23Flash_fwd_kernel_traitsILi96ELi128ELi64ELi4ELb0ELb0EN7cutlass6half_tE19Flash_kernel_traitsILi96ELi128ELi64ELi4ES2_EELb1ELb1ELb1ELb1ELb0EEv16Flash_fwd_params' for 'sm_80'
ptxas info    : Function properties for _Z16flash_fwd_kernelI23Flash_fwd_kernel_traitsILi96ELi128ELi64ELi4ELb0ELb0EN7cutlass6half_tE19Flash_kernel_traitsILi96ELi128ELi64ELi4ES2_EELb1ELb1ELb1ELb1ELb0EEv16Flash_fwd_params
    40 bytes stack frame, 44 bytes spill stores, 40 bytes spill loads
ptxas info    : Used 255 registers, 584 bytes cmem[0]
ptxas info    : Compiling entry function '_Z16flash_fwd_kernelI23Flash_fwd_kernel_traitsILi96ELi128ELi64ELi4ELb0ELb0EN7cutlass6half_tE19Flash_kernel_traitsILi96ELi128ELi64ELi4ES2_EELb1ELb1ELb1ELb1ELb1EEv16Flash_fwd_params' for 'sm_80'
ptxas info    : Function properties for _Z16flash_fwd_kernelI23Flash_fwd_kernel_traitsILi96ELi128ELi64ELi4ELb0ELb0EN7cutlass6half_tE19Flash_kernel_traitsILi96ELi128ELi64ELi4ES2_EELb1ELb1ELb1ELb1ELb1EEv16Flash_fwd_params
    264 bytes stack frame, 250 bytes spill stores, 268 bytes spill loads
ptxas info    : Used 255 registers, 584 bytes cmem[0]
ptxas info    : Compiling entry function '_Z16flash_fwd_kernelI23Flash_fwd_kernel_traitsILi96ELi64ELi64ELi4ELb0ELb0EN7cutlass6half_tE19Flash_kernel_traitsILi96ELi64ELi64ELi4ES2_EELb1ELb1ELb0ELb0ELb0EEv16Flash_fwd_params' for 'sm_80'
ptxas info    : Function properties for _Z16flash_fwd_kernelI23Flash_fwd_kernel_traitsILi96ELi64ELi64ELi4ELb0ELb0EN7cutlass6half_tE19Flash_kernel_traitsILi96ELi64ELi64ELi4ES2_EELb1ELb1ELb0ELb0ELb0EEv16Flash_fwd_params
    0 bytes stack frame, 0 bytes spill stores, 0 bytes spill loads
ptxas info    : Used 146 registers, 584 bytes cmem[0]
ptxas info    : Compiling entry function '_Z16flash_fwd_kernelI23Flash_fwd_kernel_traitsILi96ELi64ELi64ELi4ELb0ELb0EN7cutlass6half_tE19Flash_kernel_traitsILi96ELi64ELi64ELi4ES2_EELb1ELb1ELb0ELb0ELb1EEv16Flash_fwd_params' for 'sm_80'
ptxas info    : Function properties for _Z16flash_fwd_kernelI23Flash_fwd_kernel_traitsILi96ELi64ELi64ELi4ELb0ELb0EN7cutlass6half_tE19Flash_kernel_traitsILi96ELi64ELi64ELi4ES2_EELb1ELb1ELb0ELb0ELb1EEv16Flash_fwd_params
    0 bytes stack frame, 0 bytes spill stores, 0 bytes spill loads
ptxas info    : Used 192 registers, 584 bytes cmem[0]
ptxas info    : Compiling entry function '_Z16flash_fwd_kernelI23Flash_fwd_kernel_traitsILi96ELi64ELi64ELi4ELb0ELb0EN7cutlass6half_tE19Flash_kernel_traitsILi96ELi64ELi64ELi4ES2_EELb1ELb1ELb0ELb1ELb0EEv16Flash_fwd_params' for 'sm_80'
ptxas info    : Function properties for _Z16flash_fwd_kernelI23Flash_fwd_kernel_traitsILi96ELi64ELi64ELi4ELb0ELb0EN7cutlass6half_tE19Flash_kernel_traitsILi96ELi64ELi64ELi4ES2_EELb1ELb1ELb0ELb1ELb0EEv16Flash_fwd_params
    0 bytes stack frame, 0 bytes spill stores, 0 bytes spill loads
ptxas info    : Used 140 registers, 584 bytes cmem[0]
ptxas info    : Compiling entry function '_Z16flash_fwd_kernelI23Flash_fwd_kernel_traitsILi96ELi64ELi64ELi4ELb0ELb0EN7cutlass6half_tE19Flash_kernel_traitsILi96ELi64ELi64ELi4ES2_EELb1ELb1ELb0ELb1ELb1EEv16Flash_fwd_params' for 'sm_80'
ptxas info    : Function properties for _Z16flash_fwd_kernelI23Flash_fwd_kernel_traitsILi96ELi64ELi64ELi4ELb0ELb0EN7cutlass6half_tE19Flash_kernel_traitsILi96ELi64ELi64ELi4ES2_EELb1ELb1ELb0ELb1ELb1EEv16Flash_fwd_params
    0 bytes stack frame, 0 bytes spill stores, 0 bytes spill loads
ptxas info    : Used 190 registers, 584 bytes cmem[0]
ptxas info    : Compiling entry function '_Z16flash_fwd_kernelI23Flash_fwd_kernel_traitsILi96ELi64ELi64ELi4ELb0ELb0EN7cutlass6half_tE19Flash_kernel_traitsILi96ELi64ELi64ELi4ES2_EELb1ELb1ELb1ELb0ELb0EEv16Flash_fwd_params' for 'sm_80'
ptxas info    : Function properties for _Z16flash_fwd_kernelI23Flash_fwd_kernel_traitsILi96ELi64ELi64ELi4ELb0ELb0EN7cutlass6half_tE19Flash_kernel_traitsILi96ELi64ELi64ELi4ES2_EELb1ELb1ELb1ELb0ELb0EEv16Flash_fwd_params
    0 bytes stack frame, 0 bytes spill stores, 0 bytes spill loads
ptxas info    : Used 148 registers, 584 bytes cmem[0]
ptxas info    : Compiling entry function '_Z16flash_fwd_kernelI23Flash_fwd_kernel_traitsILi96ELi64ELi64ELi4ELb0ELb0EN7cutlass6half_tE19Flash_kernel_traitsILi96ELi64ELi64ELi4ES2_EELb1ELb1ELb1ELb0ELb1EEv16Flash_fwd_params' for 'sm_80'
ptxas info    : Function properties for _Z16flash_fwd_kernelI23Flash_fwd_kernel_traitsILi96ELi64ELi64ELi4ELb0ELb0EN7cutlass6half_tE19Flash_kernel_traitsILi96ELi64ELi64ELi4ES2_EELb1ELb1ELb1ELb0ELb1EEv16Flash_fwd_params
    0 bytes stack frame, 0 bytes spill stores, 0 bytes spill loads
ptxas info    : Used 188 registers, 584 bytes cmem[0]
ptxas info    : Compiling entry function '_Z16flash_fwd_kernelI23Flash_fwd_kernel_traitsILi96ELi64ELi64ELi4ELb0ELb0EN7cutlass6half_tE19Flash_kernel_traitsILi96ELi64ELi64ELi4ES2_EELb1ELb1ELb1ELb1ELb0EEv16Flash_fwd_params' for 'sm_80'
ptxas info    : Function properties for _Z16flash_fwd_kernelI23Flash_fwd_kernel_traitsILi96ELi64ELi64ELi4ELb0ELb0EN7cutlass6half_tE19Flash_kernel_traitsILi96ELi64ELi64ELi4ES2_EELb1ELb1ELb1ELb1ELb0EEv16Flash_fwd_params
    0 bytes stack frame, 0 bytes spill stores, 0 bytes spill loads
ptxas info    : Used 142 registers, 584 bytes cmem[0]
ptxas info    : Compiling entry function '_Z16flash_fwd_kernelI23Flash_fwd_kernel_traitsILi96ELi64ELi64ELi4ELb0ELb0EN7cutlass6half_tE19Flash_kernel_traitsILi96ELi64ELi64ELi4ES2_EELb1ELb1ELb1ELb1ELb1EEv16Flash_fwd_params' for 'sm_80'
ptxas info    : Function properties for _Z16flash_fwd_kernelI23Flash_fwd_kernel_traitsILi96ELi64ELi64ELi4ELb0ELb0EN7cutlass6half_tE19Flash_kernel_traitsILi96ELi64ELi64ELi4ES2_EELb1ELb1ELb1ELb1ELb1EEv16Flash_fwd_params
    0 bytes stack frame, 0 bytes spill stores, 0 bytes spill loads
ptxas info    : Used 186 registers, 584 bytes cmem[0]
/anaconda/envs/flashattention/lib/python3.9/site-packages/torch/include/c10/core/SymInt.h(84): warning: integer conversion resulted in a change of sign

[8/13] /usr/local/cuda-11.4/bin/nvcc  -I/home/v-feiychen/flash-attention/csrc/flash_attn -I/home/v-feiychen/flash-attention/csrc/flash_attn/src -I/home/v-feiychen/flash-attention/csrc/cutlass/include -I/anaconda/envs/flashattention/lib/python3.9/site-packages/torch/include -I/anaconda/envs/flashattention/lib/python3.9/site-packages/torch/include/torch/csrc/api/include -I/anaconda/envs/flashattention/lib/python3.9/site-packages/torch/include/TH -I/anaconda/envs/flashattention/lib/python3.9/site-packages/torch/include/THC -I/usr/local/cuda-11.4/include -I/anaconda/envs/flashattention/include/python3.9 -c -c /home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_hdim96_bf16_sm80.cu -o /home/v-feiychen/flash-attention/build/temp.linux-x86_64-cpython-39/csrc/flash_attn/src/flash_fwd_hdim96_bf16_sm80.o -D__CUDA_NO_HALF_OPERATORS__ -D__CUDA_NO_HALF_CONVERSIONS__ -D__CUDA_NO_BFLOAT16_CONVERSIONS__ -D__CUDA_NO_HALF2_OPERATORS__ --expt-relaxed-constexpr --compiler-options ''"'"'-fPIC'"'"'' -O3 -std=c++17 -U__CUDA_NO_HALF_OPERATORS__ -U__CUDA_NO_HALF_CONVERSIONS__ -U__CUDA_NO_HALF2_OPERATORS__ -U__CUDA_NO_BFLOAT16_CONVERSIONS__ --expt-relaxed-constexpr --expt-extended-lambda --use_fast_math --ptxas-options=-v -lineinfo -gencode arch=compute_80,code=sm_80 --threads 4 -DTORCH_API_INCLUDE_EXTENSION_H '-DPYBIND11_COMPILER_TYPE="_gcc"' '-DPYBIND11_STDLIB="_libstdcpp"' '-DPYBIND11_BUILD_ABI="_cxxabi1011"' -DTORCH_EXTENSION_NAME=flash_attn_2_cuda -D_GLIBCXX_USE_CXX11_ABI=0
/anaconda/envs/flashattention/lib/python3.9/site-packages/torch/include/c10/core/SymInt.h(84): warning: integer conversion resulted in a change of sign

/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_kernel.h(140): warning: variable "MMA_M" was declared but never referenced
          detected during:
            instantiation of "void flash::compute_attn_1rowblock<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &, int, int, int) [with Kernel_traits=Flash_fwd_kernel_traits<96, 64, 64, 4, false, false, cutlass::bfloat16_t, Flash_kernel_traits<96, 64, 64, 4, cutlass::bfloat16_t>>, Is_dropout=true, Is_causal=true, Is_even_N=true, Is_even_K=true, Return_softmax=true, Params=Flash_fwd_params]" 
(588): here
            instantiation of "void flash::compute_attn<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &) [with Kernel_traits=Flash_fwd_kernel_traits<96, 64, 64, 4, false, false, cutlass::bfloat16_t, Flash_kernel_traits<96, 64, 64, 4, cutlass::bfloat16_t>>, Is_dropout=true, Is_causal=true, Is_even_N=true, Is_even_K=true, Return_softmax=true, Params=Flash_fwd_params]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(15): here
            instantiation of "void flash_fwd_kernel<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax>(Flash_fwd_params) [with Kernel_traits=Flash_fwd_kernel_traits<96, 64, 64, 4, false, false, cutlass::bfloat16_t, Flash_kernel_traits<96, 64, 64, 4, cutlass::bfloat16_t>>, Is_dropout=true, Is_causal=true, Is_even_N=true, Is_even_K=true, Return_softmax=true]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(34): here
            instantiation of "void run_flash_fwd<Kernel_traits,Is_dropout,Is_causal>(Flash_fwd_params &, cudaStream_t) [with Kernel_traits=Flash_fwd_kernel_traits<96, 64, 64, 4, false, false, cutlass::bfloat16_t, Flash_kernel_traits<96, 64, 64, 4, cutlass::bfloat16_t>>, Is_dropout=true, Is_causal=true]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(92): here
            instantiation of "void run_mha_fwd_hdim96<T>(Flash_fwd_params &, cudaStream_t) [with T=cutlass::bfloat16_t]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_hdim96_bf16_sm80.cu(16): here

/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_kernel.h(275): warning: variable "tQrQ" was declared but never referenced
          detected during:
            instantiation of "void flash::compute_attn_1rowblock<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &, int, int, int) [with Kernel_traits=Flash_fwd_kernel_traits<96, 64, 64, 4, false, false, cutlass::bfloat16_t, Flash_kernel_traits<96, 64, 64, 4, cutlass::bfloat16_t>>, Is_dropout=true, Is_causal=true, Is_even_N=true, Is_even_K=true, Return_softmax=true, Params=Flash_fwd_params]" 
(588): here
            instantiation of "void flash::compute_attn<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &) [with Kernel_traits=Flash_fwd_kernel_traits<96, 64, 64, 4, false, false, cutlass::bfloat16_t, Flash_kernel_traits<96, 64, 64, 4, cutlass::bfloat16_t>>, Is_dropout=true, Is_causal=true, Is_even_N=true, Is_even_K=true, Return_softmax=true, Params=Flash_fwd_params]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(15): here
            instantiation of "void flash_fwd_kernel<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax>(Flash_fwd_params) [with Kernel_traits=Flash_fwd_kernel_traits<96, 64, 64, 4, false, false, cutlass::bfloat16_t, Flash_kernel_traits<96, 64, 64, 4, cutlass::bfloat16_t>>, Is_dropout=true, Is_causal=true, Is_even_N=true, Is_even_K=true, Return_softmax=true]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(34): here
            instantiation of "void run_flash_fwd<Kernel_traits,Is_dropout,Is_causal>(Flash_fwd_params &, cudaStream_t) [with Kernel_traits=Flash_fwd_kernel_traits<96, 64, 64, 4, false, false, cutlass::bfloat16_t, Flash_kernel_traits<96, 64, 64, 4, cutlass::bfloat16_t>>, Is_dropout=true, Is_causal=true]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(92): here
            instantiation of "void run_mha_fwd_hdim96<T>(Flash_fwd_params &, cudaStream_t) [with T=cutlass::bfloat16_t]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_hdim96_bf16_sm80.cu(16): here

/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_kernel.h(140): warning: variable "MMA_M" was declared but never referenced
          detected during:
            instantiation of "void flash::compute_attn_1rowblock<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &, int, int, int) [with Kernel_traits=Flash_fwd_kernel_traits<96, 64, 64, 4, false, false, cutlass::bfloat16_t, Flash_kernel_traits<96, 64, 64, 4, cutlass::bfloat16_t>>, Is_dropout=true, Is_causal=true, Is_even_N=true, Is_even_K=true, Return_softmax=false, Params=Flash_fwd_params]" 
(588): here
            instantiation of "void flash::compute_attn<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &) [with Kernel_traits=Flash_fwd_kernel_traits<96, 64, 64, 4, false, false, cutlass::bfloat16_t, Flash_kernel_traits<96, 64, 64, 4, cutlass::bfloat16_t>>, Is_dropout=true, Is_causal=true, Is_even_N=true, Is_even_K=true, Return_softmax=false, Params=Flash_fwd_params]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(15): here
            instantiation of "void flash_fwd_kernel<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax>(Flash_fwd_params) [with Kernel_traits=Flash_fwd_kernel_traits<96, 64, 64, 4, false, false, cutlass::bfloat16_t, Flash_kernel_traits<96, 64, 64, 4, cutlass::bfloat16_t>>, Is_dropout=true, Is_causal=true, Is_even_N=true, Is_even_K=true, Return_softmax=false]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(34): here
            instantiation of "void run_flash_fwd<Kernel_traits,Is_dropout,Is_causal>(Flash_fwd_params &, cudaStream_t) [with Kernel_traits=Flash_fwd_kernel_traits<96, 64, 64, 4, false, false, cutlass::bfloat16_t, Flash_kernel_traits<96, 64, 64, 4, cutlass::bfloat16_t>>, Is_dropout=true, Is_causal=true]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(92): here
            instantiation of "void run_mha_fwd_hdim96<T>(Flash_fwd_params &, cudaStream_t) [with T=cutlass::bfloat16_t]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_hdim96_bf16_sm80.cu(16): here

/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_kernel.h(275): warning: variable "tQrQ" was declared but never referenced
          detected during:
            instantiation of "void flash::compute_attn_1rowblock<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &, int, int, int) [with Kernel_traits=Flash_fwd_kernel_traits<96, 64, 64, 4, false, false, cutlass::bfloat16_t, Flash_kernel_traits<96, 64, 64, 4, cutlass::bfloat16_t>>, Is_dropout=true, Is_causal=true, Is_even_N=true, Is_even_K=true, Return_softmax=false, Params=Flash_fwd_params]" 
(588): here
            instantiation of "void flash::compute_attn<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &) [with Kernel_traits=Flash_fwd_kernel_traits<96, 64, 64, 4, false, false, cutlass::bfloat16_t, Flash_kernel_traits<96, 64, 64, 4, cutlass::bfloat16_t>>, Is_dropout=true, Is_causal=true, Is_even_N=true, Is_even_K=true, Return_softmax=false, Params=Flash_fwd_params]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(15): here
            instantiation of "void flash_fwd_kernel<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax>(Flash_fwd_params) [with Kernel_traits=Flash_fwd_kernel_traits<96, 64, 64, 4, false, false, cutlass::bfloat16_t, Flash_kernel_traits<96, 64, 64, 4, cutlass::bfloat16_t>>, Is_dropout=true, Is_causal=true, Is_even_N=true, Is_even_K=true, Return_softmax=false]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(34): here
            instantiation of "void run_flash_fwd<Kernel_traits,Is_dropout,Is_causal>(Flash_fwd_params &, cudaStream_t) [with Kernel_traits=Flash_fwd_kernel_traits<96, 64, 64, 4, false, false, cutlass::bfloat16_t, Flash_kernel_traits<96, 64, 64, 4, cutlass::bfloat16_t>>, Is_dropout=true, Is_causal=true]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(92): here
            instantiation of "void run_mha_fwd_hdim96<T>(Flash_fwd_params &, cudaStream_t) [with T=cutlass::bfloat16_t]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_hdim96_bf16_sm80.cu(16): here

/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_kernel.h(140): warning: variable "MMA_M" was declared but never referenced
          detected during:
            instantiation of "void flash::compute_attn_1rowblock<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &, int, int, int) [with Kernel_traits=Flash_fwd_kernel_traits<96, 64, 64, 4, false, false, cutlass::bfloat16_t, Flash_kernel_traits<96, 64, 64, 4, cutlass::bfloat16_t>>, Is_dropout=true, Is_causal=true, Is_even_N=true, Is_even_K=false, Return_softmax=true, Params=Flash_fwd_params]" 
(588): here
            instantiation of "void flash::compute_attn<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &) [with Kernel_traits=Flash_fwd_kernel_traits<96, 64, 64, 4, false, false, cutlass::bfloat16_t, Flash_kernel_traits<96, 64, 64, 4, cutlass::bfloat16_t>>, Is_dropout=true, Is_causal=true, Is_even_N=true, Is_even_K=false, Return_softmax=true, Params=Flash_fwd_params]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(15): here
            instantiation of "void flash_fwd_kernel<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax>(Flash_fwd_params) [with Kernel_traits=Flash_fwd_kernel_traits<96, 64, 64, 4, false, false, cutlass::bfloat16_t, Flash_kernel_traits<96, 64, 64, 4, cutlass::bfloat16_t>>, Is_dropout=true, Is_causal=true, Is_even_N=true, Is_even_K=false, Return_softmax=true]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(34): here
            instantiation of "void run_flash_fwd<Kernel_traits,Is_dropout,Is_causal>(Flash_fwd_params &, cudaStream_t) [with Kernel_traits=Flash_fwd_kernel_traits<96, 64, 64, 4, false, false, cutlass::bfloat16_t, Flash_kernel_traits<96, 64, 64, 4, cutlass::bfloat16_t>>, Is_dropout=true, Is_causal=true]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(92): here
            instantiation of "void run_mha_fwd_hdim96<T>(Flash_fwd_params &, cudaStream_t) [with T=cutlass::bfloat16_t]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_hdim96_bf16_sm80.cu(16): here

/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_kernel.h(275): warning: variable "tQrQ" was declared but never referenced
          detected during:
            instantiation of "void flash::compute_attn_1rowblock<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &, int, int, int) [with Kernel_traits=Flash_fwd_kernel_traits<96, 64, 64, 4, false, false, cutlass::bfloat16_t, Flash_kernel_traits<96, 64, 64, 4, cutlass::bfloat16_t>>, Is_dropout=true, Is_causal=true, Is_even_N=true, Is_even_K=false, Return_softmax=true, Params=Flash_fwd_params]" 
(588): here
            instantiation of "void flash::compute_attn<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &) [with Kernel_traits=Flash_fwd_kernel_traits<96, 64, 64, 4, false, false, cutlass::bfloat16_t, Flash_kernel_traits<96, 64, 64, 4, cutlass::bfloat16_t>>, Is_dropout=true, Is_causal=true, Is_even_N=true, Is_even_K=false, Return_softmax=true, Params=Flash_fwd_params]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(15): here
            instantiation of "void flash_fwd_kernel<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax>(Flash_fwd_params) [with Kernel_traits=Flash_fwd_kernel_traits<96, 64, 64, 4, false, false, cutlass::bfloat16_t, Flash_kernel_traits<96, 64, 64, 4, cutlass::bfloat16_t>>, Is_dropout=true, Is_causal=true, Is_even_N=true, Is_even_K=false, Return_softmax=true]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(34): here
            instantiation of "void run_flash_fwd<Kernel_traits,Is_dropout,Is_causal>(Flash_fwd_params &, cudaStream_t) [with Kernel_traits=Flash_fwd_kernel_traits<96, 64, 64, 4, false, false, cutlass::bfloat16_t, Flash_kernel_traits<96, 64, 64, 4, cutlass::bfloat16_t>>, Is_dropout=true, Is_causal=true]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(92): here
            instantiation of "void run_mha_fwd_hdim96<T>(Flash_fwd_params &, cudaStream_t) [with T=cutlass::bfloat16_t]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_hdim96_bf16_sm80.cu(16): here

/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_kernel.h(140): warning: variable "MMA_M" was declared but never referenced
          detected during:
            instantiation of "void flash::compute_attn_1rowblock<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &, int, int, int) [with Kernel_traits=Flash_fwd_kernel_traits<96, 64, 64, 4, false, false, cutlass::bfloat16_t, Flash_kernel_traits<96, 64, 64, 4, cutlass::bfloat16_t>>, Is_dropout=true, Is_causal=true, Is_even_N=true, Is_even_K=false, Return_softmax=false, Params=Flash_fwd_params]" 
(588): here
            instantiation of "void flash::compute_attn<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &) [with Kernel_traits=Flash_fwd_kernel_traits<96, 64, 64, 4, false, false, cutlass::bfloat16_t, Flash_kernel_traits<96, 64, 64, 4, cutlass::bfloat16_t>>, Is_dropout=true, Is_causal=true, Is_even_N=true, Is_even_K=false, Return_softmax=false, Params=Flash_fwd_params]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(15): here
            instantiation of "void flash_fwd_kernel<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax>(Flash_fwd_params) [with Kernel_traits=Flash_fwd_kernel_traits<96, 64, 64, 4, false, false, cutlass::bfloat16_t, Flash_kernel_traits<96, 64, 64, 4, cutlass::bfloat16_t>>, Is_dropout=true, Is_causal=true, Is_even_N=true, Is_even_K=false, Return_softmax=false]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(34): here
            instantiation of "void run_flash_fwd<Kernel_traits,Is_dropout,Is_causal>(Flash_fwd_params &, cudaStream_t) [with Kernel_traits=Flash_fwd_kernel_traits<96, 64, 64, 4, false, false, cutlass::bfloat16_t, Flash_kernel_traits<96, 64, 64, 4, cutlass::bfloat16_t>>, Is_dropout=true, Is_causal=true]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(92): here
            instantiation of "void run_mha_fwd_hdim96<T>(Flash_fwd_params &, cudaStream_t) [with T=cutlass::bfloat16_t]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_hdim96_bf16_sm80.cu(16): here

/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_kernel.h(275): warning: variable "tQrQ" was declared but never referenced
          detected during:
            instantiation of "void flash::compute_attn_1rowblock<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &, int, int, int) [with Kernel_traits=Flash_fwd_kernel_traits<96, 64, 64, 4, false, false, cutlass::bfloat16_t, Flash_kernel_traits<96, 64, 64, 4, cutlass::bfloat16_t>>, Is_dropout=true, Is_causal=true, Is_even_N=true, Is_even_K=false, Return_softmax=false, Params=Flash_fwd_params]" 
(588): here
            instantiation of "void flash::compute_attn<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &) [with Kernel_traits=Flash_fwd_kernel_traits<96, 64, 64, 4, false, false, cutlass::bfloat16_t, Flash_kernel_traits<96, 64, 64, 4, cutlass::bfloat16_t>>, Is_dropout=true, Is_causal=true, Is_even_N=true, Is_even_K=false, Return_softmax=false, Params=Flash_fwd_params]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(15): here
            instantiation of "void flash_fwd_kernel<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax>(Flash_fwd_params) [with Kernel_traits=Flash_fwd_kernel_traits<96, 64, 64, 4, false, false, cutlass::bfloat16_t, Flash_kernel_traits<96, 64, 64, 4, cutlass::bfloat16_t>>, Is_dropout=true, Is_causal=true, Is_even_N=true, Is_even_K=false, Return_softmax=false]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(34): here
            instantiation of "void run_flash_fwd<Kernel_traits,Is_dropout,Is_causal>(Flash_fwd_params &, cudaStream_t) [with Kernel_traits=Flash_fwd_kernel_traits<96, 64, 64, 4, false, false, cutlass::bfloat16_t, Flash_kernel_traits<96, 64, 64, 4, cutlass::bfloat16_t>>, Is_dropout=true, Is_causal=true]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(92): here
            instantiation of "void run_mha_fwd_hdim96<T>(Flash_fwd_params &, cudaStream_t) [with T=cutlass::bfloat16_t]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_hdim96_bf16_sm80.cu(16): here

/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_kernel.h(140): warning: variable "MMA_M" was declared but never referenced
          detected during:
            instantiation of "void flash::compute_attn_1rowblock<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &, int, int, int) [with Kernel_traits=Flash_fwd_kernel_traits<96, 64, 64, 4, false, false, cutlass::bfloat16_t, Flash_kernel_traits<96, 64, 64, 4, cutlass::bfloat16_t>>, Is_dropout=true, Is_causal=true, Is_even_N=false, Is_even_K=true, Return_softmax=true, Params=Flash_fwd_params]" 
(588): here
            instantiation of "void flash::compute_attn<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &) [with Kernel_traits=Flash_fwd_kernel_traits<96, 64, 64, 4, false, false, cutlass::bfloat16_t, Flash_kernel_traits<96, 64, 64, 4, cutlass::bfloat16_t>>, Is_dropout=true, Is_causal=true, Is_even_N=false, Is_even_K=true, Return_softmax=true, Params=Flash_fwd_params]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(15): here
            instantiation of "void flash_fwd_kernel<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax>(Flash_fwd_params) [with Kernel_traits=Flash_fwd_kernel_traits<96, 64, 64, 4, false, false, cutlass::bfloat16_t, Flash_kernel_traits<96, 64, 64, 4, cutlass::bfloat16_t>>, Is_dropout=true, Is_causal=true, Is_even_N=false, Is_even_K=true, Return_softmax=true]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(34): here
            instantiation of "void run_flash_fwd<Kernel_traits,Is_dropout,Is_causal>(Flash_fwd_params &, cudaStream_t) [with Kernel_traits=Flash_fwd_kernel_traits<96, 64, 64, 4, false, false, cutlass::bfloat16_t, Flash_kernel_traits<96, 64, 64, 4, cutlass::bfloat16_t>>, Is_dropout=true, Is_causal=true]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(92): here
            instantiation of "void run_mha_fwd_hdim96<T>(Flash_fwd_params &, cudaStream_t) [with T=cutlass::bfloat16_t]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_hdim96_bf16_sm80.cu(16): here

/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_kernel.h(275): warning: variable "tQrQ" was declared but never referenced
          detected during:
            instantiation of "void flash::compute_attn_1rowblock<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &, int, int, int) [with Kernel_traits=Flash_fwd_kernel_traits<96, 64, 64, 4, false, false, cutlass::bfloat16_t, Flash_kernel_traits<96, 64, 64, 4, cutlass::bfloat16_t>>, Is_dropout=true, Is_causal=true, Is_even_N=false, Is_even_K=true, Return_softmax=true, Params=Flash_fwd_params]" 
(588): here
            instantiation of "void flash::compute_attn<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &) [with Kernel_traits=Flash_fwd_kernel_traits<96, 64, 64, 4, false, false, cutlass::bfloat16_t, Flash_kernel_traits<96, 64, 64, 4, cutlass::bfloat16_t>>, Is_dropout=true, Is_causal=true, Is_even_N=false, Is_even_K=true, Return_softmax=true, Params=Flash_fwd_params]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(15): here
            instantiation of "void flash_fwd_kernel<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax>(Flash_fwd_params) [with Kernel_traits=Flash_fwd_kernel_traits<96, 64, 64, 4, false, false, cutlass::bfloat16_t, Flash_kernel_traits<96, 64, 64, 4, cutlass::bfloat16_t>>, Is_dropout=true, Is_causal=true, Is_even_N=false, Is_even_K=true, Return_softmax=true]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(34): here
            instantiation of "void run_flash_fwd<Kernel_traits,Is_dropout,Is_causal>(Flash_fwd_params &, cudaStream_t) [with Kernel_traits=Flash_fwd_kernel_traits<96, 64, 64, 4, false, false, cutlass::bfloat16_t, Flash_kernel_traits<96, 64, 64, 4, cutlass::bfloat16_t>>, Is_dropout=true, Is_causal=true]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(92): here
            instantiation of "void run_mha_fwd_hdim96<T>(Flash_fwd_params &, cudaStream_t) [with T=cutlass::bfloat16_t]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_hdim96_bf16_sm80.cu(16): here

/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_kernel.h(140): warning: variable "MMA_M" was declared but never referenced
          detected during:
            instantiation of "void flash::compute_attn_1rowblock<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &, int, int, int) [with Kernel_traits=Flash_fwd_kernel_traits<96, 64, 64, 4, false, false, cutlass::bfloat16_t, Flash_kernel_traits<96, 64, 64, 4, cutlass::bfloat16_t>>, Is_dropout=true, Is_causal=true, Is_even_N=false, Is_even_K=true, Return_softmax=false, Params=Flash_fwd_params]" 
(588): here
            instantiation of "void flash::compute_attn<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &) [with Kernel_traits=Flash_fwd_kernel_traits<96, 64, 64, 4, false, false, cutlass::bfloat16_t, Flash_kernel_traits<96, 64, 64, 4, cutlass::bfloat16_t>>, Is_dropout=true, Is_causal=true, Is_even_N=false, Is_even_K=true, Return_softmax=false, Params=Flash_fwd_params]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(15): here
            instantiation of "void flash_fwd_kernel<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax>(Flash_fwd_params) [with Kernel_traits=Flash_fwd_kernel_traits<96, 64, 64, 4, false, false, cutlass::bfloat16_t, Flash_kernel_traits<96, 64, 64, 4, cutlass::bfloat16_t>>, Is_dropout=true, Is_causal=true, Is_even_N=false, Is_even_K=true, Return_softmax=false]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(34): here
            instantiation of "void run_flash_fwd<Kernel_traits,Is_dropout,Is_causal>(Flash_fwd_params &, cudaStream_t) [with Kernel_traits=Flash_fwd_kernel_traits<96, 64, 64, 4, false, false, cutlass::bfloat16_t, Flash_kernel_traits<96, 64, 64, 4, cutlass::bfloat16_t>>, Is_dropout=true, Is_causal=true]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(92): here
            instantiation of "void run_mha_fwd_hdim96<T>(Flash_fwd_params &, cudaStream_t) [with T=cutlass::bfloat16_t]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_hdim96_bf16_sm80.cu(16): here

/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_kernel.h(275): warning: variable "tQrQ" was declared but never referenced
          detected during:
            instantiation of "void flash::compute_attn_1rowblock<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &, int, int, int) [with Kernel_traits=Flash_fwd_kernel_traits<96, 64, 64, 4, false, false, cutlass::bfloat16_t, Flash_kernel_traits<96, 64, 64, 4, cutlass::bfloat16_t>>, Is_dropout=true, Is_causal=true, Is_even_N=false, Is_even_K=true, Return_softmax=false, Params=Flash_fwd_params]" 
(588): here
            instantiation of "void flash::compute_attn<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &) [with Kernel_traits=Flash_fwd_kernel_traits<96, 64, 64, 4, false, false, cutlass::bfloat16_t, Flash_kernel_traits<96, 64, 64, 4, cutlass::bfloat16_t>>, Is_dropout=true, Is_causal=true, Is_even_N=false, Is_even_K=true, Return_softmax=false, Params=Flash_fwd_params]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(15): here
            instantiation of "void flash_fwd_kernel<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax>(Flash_fwd_params) [with Kernel_traits=Flash_fwd_kernel_traits<96, 64, 64, 4, false, false, cutlass::bfloat16_t, Flash_kernel_traits<96, 64, 64, 4, cutlass::bfloat16_t>>, Is_dropout=true, Is_causal=true, Is_even_N=false, Is_even_K=true, Return_softmax=false]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(34): here
            instantiation of "void run_flash_fwd<Kernel_traits,Is_dropout,Is_causal>(Flash_fwd_params &, cudaStream_t) [with Kernel_traits=Flash_fwd_kernel_traits<96, 64, 64, 4, false, false, cutlass::bfloat16_t, Flash_kernel_traits<96, 64, 64, 4, cutlass::bfloat16_t>>, Is_dropout=true, Is_causal=true]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(92): here
            instantiation of "void run_mha_fwd_hdim96<T>(Flash_fwd_params &, cudaStream_t) [with T=cutlass::bfloat16_t]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_hdim96_bf16_sm80.cu(16): here

/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_kernel.h(140): warning: variable "MMA_M" was declared but never referenced
          detected during:
            instantiation of "void flash::compute_attn_1rowblock<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &, int, int, int) [with Kernel_traits=Flash_fwd_kernel_traits<96, 64, 64, 4, false, false, cutlass::bfloat16_t, Flash_kernel_traits<96, 64, 64, 4, cutlass::bfloat16_t>>, Is_dropout=true, Is_causal=true, Is_even_N=false, Is_even_K=false, Return_softmax=true, Params=Flash_fwd_params]" 
(588): here
            instantiation of "void flash::compute_attn<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &) [with Kernel_traits=Flash_fwd_kernel_traits<96, 64, 64, 4, false, false, cutlass::bfloat16_t, Flash_kernel_traits<96, 64, 64, 4, cutlass::bfloat16_t>>, Is_dropout=true, Is_causal=true, Is_even_N=false, Is_even_K=false, Return_softmax=true, Params=Flash_fwd_params]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(15): here
            instantiation of "void flash_fwd_kernel<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax>(Flash_fwd_params) [with Kernel_traits=Flash_fwd_kernel_traits<96, 64, 64, 4, false, false, cutlass::bfloat16_t, Flash_kernel_traits<96, 64, 64, 4, cutlass::bfloat16_t>>, Is_dropout=true, Is_causal=true, Is_even_N=false, Is_even_K=false, Return_softmax=true]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(34): here
            instantiation of "void run_flash_fwd<Kernel_traits,Is_dropout,Is_causal>(Flash_fwd_params &, cudaStream_t) [with Kernel_traits=Flash_fwd_kernel_traits<96, 64, 64, 4, false, false, cutlass::bfloat16_t, Flash_kernel_traits<96, 64, 64, 4, cutlass::bfloat16_t>>, Is_dropout=true, Is_causal=true]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(92): here
            instantiation of "void run_mha_fwd_hdim96<T>(Flash_fwd_params &, cudaStream_t) [with T=cutlass::bfloat16_t]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_hdim96_bf16_sm80.cu(16): here

/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_kernel.h(275): warning: variable "tQrQ" was declared but never referenced
          detected during:
            instantiation of "void flash::compute_attn_1rowblock<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &, int, int, int) [with Kernel_traits=Flash_fwd_kernel_traits<96, 64, 64, 4, false, false, cutlass::bfloat16_t, Flash_kernel_traits<96, 64, 64, 4, cutlass::bfloat16_t>>, Is_dropout=true, Is_causal=true, Is_even_N=false, Is_even_K=false, Return_softmax=true, Params=Flash_fwd_params]" 
(588): here
            instantiation of "void flash::compute_attn<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &) [with Kernel_traits=Flash_fwd_kernel_traits<96, 64, 64, 4, false, false, cutlass::bfloat16_t, Flash_kernel_traits<96, 64, 64, 4, cutlass::bfloat16_t>>, Is_dropout=true, Is_causal=true, Is_even_N=false, Is_even_K=false, Return_softmax=true, Params=Flash_fwd_params]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(15): here
            instantiation of "void flash_fwd_kernel<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax>(Flash_fwd_params) [with Kernel_traits=Flash_fwd_kernel_traits<96, 64, 64, 4, false, false, cutlass::bfloat16_t, Flash_kernel_traits<96, 64, 64, 4, cutlass::bfloat16_t>>, Is_dropout=true, Is_causal=true, Is_even_N=false, Is_even_K=false, Return_softmax=true]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(34): here
            instantiation of "void run_flash_fwd<Kernel_traits,Is_dropout,Is_causal>(Flash_fwd_params &, cudaStream_t) [with Kernel_traits=Flash_fwd_kernel_traits<96, 64, 64, 4, false, false, cutlass::bfloat16_t, Flash_kernel_traits<96, 64, 64, 4, cutlass::bfloat16_t>>, Is_dropout=true, Is_causal=true]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(92): here
            instantiation of "void run_mha_fwd_hdim96<T>(Flash_fwd_params &, cudaStream_t) [with T=cutlass::bfloat16_t]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_hdim96_bf16_sm80.cu(16): here

/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_kernel.h(140): warning: variable "MMA_M" was declared but never referenced
          detected during:
            instantiation of "void flash::compute_attn_1rowblock<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &, int, int, int) [with Kernel_traits=Flash_fwd_kernel_traits<96, 64, 64, 4, false, false, cutlass::bfloat16_t, Flash_kernel_traits<96, 64, 64, 4, cutlass::bfloat16_t>>, Is_dropout=true, Is_causal=true, Is_even_N=false, Is_even_K=false, Return_softmax=false, Params=Flash_fwd_params]" 
(588): here
            instantiation of "void flash::compute_attn<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &) [with Kernel_traits=Flash_fwd_kernel_traits<96, 64, 64, 4, false, false, cutlass::bfloat16_t, Flash_kernel_traits<96, 64, 64, 4, cutlass::bfloat16_t>>, Is_dropout=true, Is_causal=true, Is_even_N=false, Is_even_K=false, Return_softmax=false, Params=Flash_fwd_params]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(15): here
            instantiation of "void flash_fwd_kernel<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax>(Flash_fwd_params) [with Kernel_traits=Flash_fwd_kernel_traits<96, 64, 64, 4, false, false, cutlass::bfloat16_t, Flash_kernel_traits<96, 64, 64, 4, cutlass::bfloat16_t>>, Is_dropout=true, Is_causal=true, Is_even_N=false, Is_even_K=false, Return_softmax=false]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(34): here
            instantiation of "void run_flash_fwd<Kernel_traits,Is_dropout,Is_causal>(Flash_fwd_params &, cudaStream_t) [with Kernel_traits=Flash_fwd_kernel_traits<96, 64, 64, 4, false, false, cutlass::bfloat16_t, Flash_kernel_traits<96, 64, 64, 4, cutlass::bfloat16_t>>, Is_dropout=true, Is_causal=true]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(92): here
            instantiation of "void run_mha_fwd_hdim96<T>(Flash_fwd_params &, cudaStream_t) [with T=cutlass::bfloat16_t]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_hdim96_bf16_sm80.cu(16): here

/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_kernel.h(275): warning: variable "tQrQ" was declared but never referenced
          detected during:
            instantiation of "void flash::compute_attn_1rowblock<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &, int, int, int) [with Kernel_traits=Flash_fwd_kernel_traits<96, 64, 64, 4, false, false, cutlass::bfloat16_t, Flash_kernel_traits<96, 64, 64, 4, cutlass::bfloat16_t>>, Is_dropout=true, Is_causal=true, Is_even_N=false, Is_even_K=false, Return_softmax=false, Params=Flash_fwd_params]" 
(588): here
            instantiation of "void flash::compute_attn<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &) [with Kernel_traits=Flash_fwd_kernel_traits<96, 64, 64, 4, false, false, cutlass::bfloat16_t, Flash_kernel_traits<96, 64, 64, 4, cutlass::bfloat16_t>>, Is_dropout=true, Is_causal=true, Is_even_N=false, Is_even_K=false, Return_softmax=false, Params=Flash_fwd_params]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(15): here
            instantiation of "void flash_fwd_kernel<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax>(Flash_fwd_params) [with Kernel_traits=Flash_fwd_kernel_traits<96, 64, 64, 4, false, false, cutlass::bfloat16_t, Flash_kernel_traits<96, 64, 64, 4, cutlass::bfloat16_t>>, Is_dropout=true, Is_causal=true, Is_even_N=false, Is_even_K=false, Return_softmax=false]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(34): here
            instantiation of "void run_flash_fwd<Kernel_traits,Is_dropout,Is_causal>(Flash_fwd_params &, cudaStream_t) [with Kernel_traits=Flash_fwd_kernel_traits<96, 64, 64, 4, false, false, cutlass::bfloat16_t, Flash_kernel_traits<96, 64, 64, 4, cutlass::bfloat16_t>>, Is_dropout=true, Is_causal=true]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(92): here
            instantiation of "void run_mha_fwd_hdim96<T>(Flash_fwd_params &, cudaStream_t) [with T=cutlass::bfloat16_t]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_hdim96_bf16_sm80.cu(16): here

/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_kernel.h(140): warning: variable "MMA_M" was declared but never referenced
          detected during:
            instantiation of "void flash::compute_attn_1rowblock<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &, int, int, int) [with Kernel_traits=Flash_fwd_kernel_traits<96, 128, 64, 4, false, false, cutlass::bfloat16_t, Flash_kernel_traits<96, 128, 64, 4, cutlass::bfloat16_t>>, Is_dropout=true, Is_causal=true, Is_even_N=true, Is_even_K=true, Return_softmax=true, Params=Flash_fwd_params]" 
(588): here
            instantiation of "void flash::compute_attn<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &) [with Kernel_traits=Flash_fwd_kernel_traits<96, 128, 64, 4, false, false, cutlass::bfloat16_t, Flash_kernel_traits<96, 128, 64, 4, cutlass::bfloat16_t>>, Is_dropout=true, Is_causal=true, Is_even_N=true, Is_even_K=true, Return_softmax=true, Params=Flash_fwd_params]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(15): here
            instantiation of "void flash_fwd_kernel<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax>(Flash_fwd_params) [with Kernel_traits=Flash_fwd_kernel_traits<96, 128, 64, 4, false, false, cutlass::bfloat16_t, Flash_kernel_traits<96, 128, 64, 4, cutlass::bfloat16_t>>, Is_dropout=true, Is_causal=true, Is_even_N=true, Is_even_K=true, Return_softmax=true]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(34): here
            instantiation of "void run_flash_fwd<Kernel_traits,Is_dropout,Is_causal>(Flash_fwd_params &, cudaStream_t) [with Kernel_traits=Flash_fwd_kernel_traits<96, 128, 64, 4, false, false, cutlass::bfloat16_t, Flash_kernel_traits<96, 128, 64, 4, cutlass::bfloat16_t>>, Is_dropout=true, Is_causal=true]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(92): here
            instantiation of "void run_mha_fwd_hdim96<T>(Flash_fwd_params &, cudaStream_t) [with T=cutlass::bfloat16_t]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_hdim96_bf16_sm80.cu(16): here

/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_kernel.h(275): warning: variable "tQrQ" was declared but never referenced
          detected during:
            instantiation of "void flash::compute_attn_1rowblock<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &, int, int, int) [with Kernel_traits=Flash_fwd_kernel_traits<96, 128, 64, 4, false, false, cutlass::bfloat16_t, Flash_kernel_traits<96, 128, 64, 4, cutlass::bfloat16_t>>, Is_dropout=true, Is_causal=true, Is_even_N=true, Is_even_K=true, Return_softmax=true, Params=Flash_fwd_params]" 
(588): here
            instantiation of "void flash::compute_attn<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &) [with Kernel_traits=Flash_fwd_kernel_traits<96, 128, 64, 4, false, false, cutlass::bfloat16_t, Flash_kernel_traits<96, 128, 64, 4, cutlass::bfloat16_t>>, Is_dropout=true, Is_causal=true, Is_even_N=true, Is_even_K=true, Return_softmax=true, Params=Flash_fwd_params]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(15): here
            instantiation of "void flash_fwd_kernel<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax>(Flash_fwd_params) [with Kernel_traits=Flash_fwd_kernel_traits<96, 128, 64, 4, false, false, cutlass::bfloat16_t, Flash_kernel_traits<96, 128, 64, 4, cutlass::bfloat16_t>>, Is_dropout=true, Is_causal=true, Is_even_N=true, Is_even_K=true, Return_softmax=true]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(34): here
            instantiation of "void run_flash_fwd<Kernel_traits,Is_dropout,Is_causal>(Flash_fwd_params &, cudaStream_t) [with Kernel_traits=Flash_fwd_kernel_traits<96, 128, 64, 4, false, false, cutlass::bfloat16_t, Flash_kernel_traits<96, 128, 64, 4, cutlass::bfloat16_t>>, Is_dropout=true, Is_causal=true]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(92): here
            instantiation of "void run_mha_fwd_hdim96<T>(Flash_fwd_params &, cudaStream_t) [with T=cutlass::bfloat16_t]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_hdim96_bf16_sm80.cu(16): here

/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_kernel.h(140): warning: variable "MMA_M" was declared but never referenced
          detected during:
            instantiation of "void flash::compute_attn_1rowblock<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &, int, int, int) [with Kernel_traits=Flash_fwd_kernel_traits<96, 128, 64, 4, false, false, cutlass::bfloat16_t, Flash_kernel_traits<96, 128, 64, 4, cutlass::bfloat16_t>>, Is_dropout=true, Is_causal=true, Is_even_N=true, Is_even_K=true, Return_softmax=false, Params=Flash_fwd_params]" 
(588): here
            instantiation of "void flash::compute_attn<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &) [with Kernel_traits=Flash_fwd_kernel_traits<96, 128, 64, 4, false, false, cutlass::bfloat16_t, Flash_kernel_traits<96, 128, 64, 4, cutlass::bfloat16_t>>, Is_dropout=true, Is_causal=true, Is_even_N=true, Is_even_K=true, Return_softmax=false, Params=Flash_fwd_params]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(15): here
            instantiation of "void flash_fwd_kernel<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax>(Flash_fwd_params) [with Kernel_traits=Flash_fwd_kernel_traits<96, 128, 64, 4, false, false, cutlass::bfloat16_t, Flash_kernel_traits<96, 128, 64, 4, cutlass::bfloat16_t>>, Is_dropout=true, Is_causal=true, Is_even_N=true, Is_even_K=true, Return_softmax=false]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(34): here
            instantiation of "void run_flash_fwd<Kernel_traits,Is_dropout,Is_causal>(Flash_fwd_params &, cudaStream_t) [with Kernel_traits=Flash_fwd_kernel_traits<96, 128, 64, 4, false, false, cutlass::bfloat16_t, Flash_kernel_traits<96, 128, 64, 4, cutlass::bfloat16_t>>, Is_dropout=true, Is_causal=true]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(92): here
            instantiation of "void run_mha_fwd_hdim96<T>(Flash_fwd_params &, cudaStream_t) [with T=cutlass::bfloat16_t]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_hdim96_bf16_sm80.cu(16): here

/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_kernel.h(275): warning: variable "tQrQ" was declared but never referenced
          detected during:
            instantiation of "void flash::compute_attn_1rowblock<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &, int, int, int) [with Kernel_traits=Flash_fwd_kernel_traits<96, 128, 64, 4, false, false, cutlass::bfloat16_t, Flash_kernel_traits<96, 128, 64, 4, cutlass::bfloat16_t>>, Is_dropout=true, Is_causal=true, Is_even_N=true, Is_even_K=true, Return_softmax=false, Params=Flash_fwd_params]" 
(588): here
            instantiation of "void flash::compute_attn<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &) [with Kernel_traits=Flash_fwd_kernel_traits<96, 128, 64, 4, false, false, cutlass::bfloat16_t, Flash_kernel_traits<96, 128, 64, 4, cutlass::bfloat16_t>>, Is_dropout=true, Is_causal=true, Is_even_N=true, Is_even_K=true, Return_softmax=false, Params=Flash_fwd_params]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(15): here
            instantiation of "void flash_fwd_kernel<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax>(Flash_fwd_params) [with Kernel_traits=Flash_fwd_kernel_traits<96, 128, 64, 4, false, false, cutlass::bfloat16_t, Flash_kernel_traits<96, 128, 64, 4, cutlass::bfloat16_t>>, Is_dropout=true, Is_causal=true, Is_even_N=true, Is_even_K=true, Return_softmax=false]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(34): here
            instantiation of "void run_flash_fwd<Kernel_traits,Is_dropout,Is_causal>(Flash_fwd_params &, cudaStream_t) [with Kernel_traits=Flash_fwd_kernel_traits<96, 128, 64, 4, false, false, cutlass::bfloat16_t, Flash_kernel_traits<96, 128, 64, 4, cutlass::bfloat16_t>>, Is_dropout=true, Is_causal=true]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(92): here
            instantiation of "void run_mha_fwd_hdim96<T>(Flash_fwd_params &, cudaStream_t) [with T=cutlass::bfloat16_t]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_hdim96_bf16_sm80.cu(16): here

/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_kernel.h(140): warning: variable "MMA_M" was declared but never referenced
          detected during:
            instantiation of "void flash::compute_attn_1rowblock<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &, int, int, int) [with Kernel_traits=Flash_fwd_kernel_traits<96, 128, 64, 4, false, false, cutlass::bfloat16_t, Flash_kernel_traits<96, 128, 64, 4, cutlass::bfloat16_t>>, Is_dropout=true, Is_causal=true, Is_even_N=true, Is_even_K=false, Return_softmax=true, Params=Flash_fwd_params]" 
(588): here
            instantiation of "void flash::compute_attn<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &) [with Kernel_traits=Flash_fwd_kernel_traits<96, 128, 64, 4, false, false, cutlass::bfloat16_t, Flash_kernel_traits<96, 128, 64, 4, cutlass::bfloat16_t>>, Is_dropout=true, Is_causal=true, Is_even_N=true, Is_even_K=false, Return_softmax=true, Params=Flash_fwd_params]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(15): here
            instantiation of "void flash_fwd_kernel<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax>(Flash_fwd_params) [with Kernel_traits=Flash_fwd_kernel_traits<96, 128, 64, 4, false, false, cutlass::bfloat16_t, Flash_kernel_traits<96, 128, 64, 4, cutlass::bfloat16_t>>, Is_dropout=true, Is_causal=true, Is_even_N=true, Is_even_K=false, Return_softmax=true]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(34): here
            instantiation of "void run_flash_fwd<Kernel_traits,Is_dropout,Is_causal>(Flash_fwd_params &, cudaStream_t) [with Kernel_traits=Flash_fwd_kernel_traits<96, 128, 64, 4, false, false, cutlass::bfloat16_t, Flash_kernel_traits<96, 128, 64, 4, cutlass::bfloat16_t>>, Is_dropout=true, Is_causal=true]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(92): here
            instantiation of "void run_mha_fwd_hdim96<T>(Flash_fwd_params &, cudaStream_t) [with T=cutlass::bfloat16_t]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_hdim96_bf16_sm80.cu(16): here

/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_kernel.h(275): warning: variable "tQrQ" was declared but never referenced
          detected during:
            instantiation of "void flash::compute_attn_1rowblock<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &, int, int, int) [with Kernel_traits=Flash_fwd_kernel_traits<96, 128, 64, 4, false, false, cutlass::bfloat16_t, Flash_kernel_traits<96, 128, 64, 4, cutlass::bfloat16_t>>, Is_dropout=true, Is_causal=true, Is_even_N=true, Is_even_K=false, Return_softmax=true, Params=Flash_fwd_params]" 
(588): here
            instantiation of "void flash::compute_attn<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &) [with Kernel_traits=Flash_fwd_kernel_traits<96, 128, 64, 4, false, false, cutlass::bfloat16_t, Flash_kernel_traits<96, 128, 64, 4, cutlass::bfloat16_t>>, Is_dropout=true, Is_causal=true, Is_even_N=true, Is_even_K=false, Return_softmax=true, Params=Flash_fwd_params]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(15): here
            instantiation of "void flash_fwd_kernel<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax>(Flash_fwd_params) [with Kernel_traits=Flash_fwd_kernel_traits<96, 128, 64, 4, false, false, cutlass::bfloat16_t, Flash_kernel_traits<96, 128, 64, 4, cutlass::bfloat16_t>>, Is_dropout=true, Is_causal=true, Is_even_N=true, Is_even_K=false, Return_softmax=true]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(34): here
            instantiation of "void run_flash_fwd<Kernel_traits,Is_dropout,Is_causal>(Flash_fwd_params &, cudaStream_t) [with Kernel_traits=Flash_fwd_kernel_traits<96, 128, 64, 4, false, false, cutlass::bfloat16_t, Flash_kernel_traits<96, 128, 64, 4, cutlass::bfloat16_t>>, Is_dropout=true, Is_causal=true]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(92): here
            instantiation of "void run_mha_fwd_hdim96<T>(Flash_fwd_params &, cudaStream_t) [with T=cutlass::bfloat16_t]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_hdim96_bf16_sm80.cu(16): here

/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_kernel.h(140): warning: variable "MMA_M" was declared but never referenced
          detected during:
            instantiation of "void flash::compute_attn_1rowblock<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &, int, int, int) [with Kernel_traits=Flash_fwd_kernel_traits<96, 128, 64, 4, false, false, cutlass::bfloat16_t, Flash_kernel_traits<96, 128, 64, 4, cutlass::bfloat16_t>>, Is_dropout=true, Is_causal=true, Is_even_N=true, Is_even_K=false, Return_softmax=false, Params=Flash_fwd_params]" 
(588): here
            instantiation of "void flash::compute_attn<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &) [with Kernel_traits=Flash_fwd_kernel_traits<96, 128, 64, 4, false, false, cutlass::bfloat16_t, Flash_kernel_traits<96, 128, 64, 4, cutlass::bfloat16_t>>, Is_dropout=true, Is_causal=true, Is_even_N=true, Is_even_K=false, Return_softmax=false, Params=Flash_fwd_params]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(15): here
            instantiation of "void flash_fwd_kernel<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax>(Flash_fwd_params) [with Kernel_traits=Flash_fwd_kernel_traits<96, 128, 64, 4, false, false, cutlass::bfloat16_t, Flash_kernel_traits<96, 128, 64, 4, cutlass::bfloat16_t>>, Is_dropout=true, Is_causal=true, Is_even_N=true, Is_even_K=false, Return_softmax=false]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(34): here
            instantiation of "void run_flash_fwd<Kernel_traits,Is_dropout,Is_causal>(Flash_fwd_params &, cudaStream_t) [with Kernel_traits=Flash_fwd_kernel_traits<96, 128, 64, 4, false, false, cutlass::bfloat16_t, Flash_kernel_traits<96, 128, 64, 4, cutlass::bfloat16_t>>, Is_dropout=true, Is_causal=true]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(92): here
            instantiation of "void run_mha_fwd_hdim96<T>(Flash_fwd_params &, cudaStream_t) [with T=cutlass::bfloat16_t]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_hdim96_bf16_sm80.cu(16): here

/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_kernel.h(275): warning: variable "tQrQ" was declared but never referenced
          detected during:
            instantiation of "void flash::compute_attn_1rowblock<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &, int, int, int) [with Kernel_traits=Flash_fwd_kernel_traits<96, 128, 64, 4, false, false, cutlass::bfloat16_t, Flash_kernel_traits<96, 128, 64, 4, cutlass::bfloat16_t>>, Is_dropout=true, Is_causal=true, Is_even_N=true, Is_even_K=false, Return_softmax=false, Params=Flash_fwd_params]" 
(588): here
            instantiation of "void flash::compute_attn<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &) [with Kernel_traits=Flash_fwd_kernel_traits<96, 128, 64, 4, false, false, cutlass::bfloat16_t, Flash_kernel_traits<96, 128, 64, 4, cutlass::bfloat16_t>>, Is_dropout=true, Is_causal=true, Is_even_N=true, Is_even_K=false, Return_softmax=false, Params=Flash_fwd_params]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(15): here
            instantiation of "void flash_fwd_kernel<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax>(Flash_fwd_params) [with Kernel_traits=Flash_fwd_kernel_traits<96, 128, 64, 4, false, false, cutlass::bfloat16_t, Flash_kernel_traits<96, 128, 64, 4, cutlass::bfloat16_t>>, Is_dropout=true, Is_causal=true, Is_even_N=true, Is_even_K=false, Return_softmax=false]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(34): here
            instantiation of "void run_flash_fwd<Kernel_traits,Is_dropout,Is_causal>(Flash_fwd_params &, cudaStream_t) [with Kernel_traits=Flash_fwd_kernel_traits<96, 128, 64, 4, false, false, cutlass::bfloat16_t, Flash_kernel_traits<96, 128, 64, 4, cutlass::bfloat16_t>>, Is_dropout=true, Is_causal=true]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(92): here
            instantiation of "void run_mha_fwd_hdim96<T>(Flash_fwd_params &, cudaStream_t) [with T=cutlass::bfloat16_t]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_hdim96_bf16_sm80.cu(16): here

/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_kernel.h(140): warning: variable "MMA_M" was declared but never referenced
          detected during:
            instantiation of "void flash::compute_attn_1rowblock<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &, int, int, int) [with Kernel_traits=Flash_fwd_kernel_traits<96, 128, 64, 4, false, false, cutlass::bfloat16_t, Flash_kernel_traits<96, 128, 64, 4, cutlass::bfloat16_t>>, Is_dropout=true, Is_causal=true, Is_even_N=false, Is_even_K=true, Return_softmax=true, Params=Flash_fwd_params]" 
(588): here
            instantiation of "void flash::compute_attn<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &) [with Kernel_traits=Flash_fwd_kernel_traits<96, 128, 64, 4, false, false, cutlass::bfloat16_t, Flash_kernel_traits<96, 128, 64, 4, cutlass::bfloat16_t>>, Is_dropout=true, Is_causal=true, Is_even_N=false, Is_even_K=true, Return_softmax=true, Params=Flash_fwd_params]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(15): here
            instantiation of "void flash_fwd_kernel<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax>(Flash_fwd_params) [with Kernel_traits=Flash_fwd_kernel_traits<96, 128, 64, 4, false, false, cutlass::bfloat16_t, Flash_kernel_traits<96, 128, 64, 4, cutlass::bfloat16_t>>, Is_dropout=true, Is_causal=true, Is_even_N=false, Is_even_K=true, Return_softmax=true]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(34): here
            instantiation of "void run_flash_fwd<Kernel_traits,Is_dropout,Is_causal>(Flash_fwd_params &, cudaStream_t) [with Kernel_traits=Flash_fwd_kernel_traits<96, 128, 64, 4, false, false, cutlass::bfloat16_t, Flash_kernel_traits<96, 128, 64, 4, cutlass::bfloat16_t>>, Is_dropout=true, Is_causal=true]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(92): here
            instantiation of "void run_mha_fwd_hdim96<T>(Flash_fwd_params &, cudaStream_t) [with T=cutlass::bfloat16_t]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_hdim96_bf16_sm80.cu(16): here

/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_kernel.h(275): warning: variable "tQrQ" was declared but never referenced
          detected during:
            instantiation of "void flash::compute_attn_1rowblock<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &, int, int, int) [with Kernel_traits=Flash_fwd_kernel_traits<96, 128, 64, 4, false, false, cutlass::bfloat16_t, Flash_kernel_traits<96, 128, 64, 4, cutlass::bfloat16_t>>, Is_dropout=true, Is_causal=true, Is_even_N=false, Is_even_K=true, Return_softmax=true, Params=Flash_fwd_params]" 
(588): here
            instantiation of "void flash::compute_attn<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &) [with Kernel_traits=Flash_fwd_kernel_traits<96, 128, 64, 4, false, false, cutlass::bfloat16_t, Flash_kernel_traits<96, 128, 64, 4, cutlass::bfloat16_t>>, Is_dropout=true, Is_causal=true, Is_even_N=false, Is_even_K=true, Return_softmax=true, Params=Flash_fwd_params]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(15): here
            instantiation of "void flash_fwd_kernel<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax>(Flash_fwd_params) [with Kernel_traits=Flash_fwd_kernel_traits<96, 128, 64, 4, false, false, cutlass::bfloat16_t, Flash_kernel_traits<96, 128, 64, 4, cutlass::bfloat16_t>>, Is_dropout=true, Is_causal=true, Is_even_N=false, Is_even_K=true, Return_softmax=true]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(34): here
            instantiation of "void run_flash_fwd<Kernel_traits,Is_dropout,Is_causal>(Flash_fwd_params &, cudaStream_t) [with Kernel_traits=Flash_fwd_kernel_traits<96, 128, 64, 4, false, false, cutlass::bfloat16_t, Flash_kernel_traits<96, 128, 64, 4, cutlass::bfloat16_t>>, Is_dropout=true, Is_causal=true]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(92): here
            instantiation of "void run_mha_fwd_hdim96<T>(Flash_fwd_params &, cudaStream_t) [with T=cutlass::bfloat16_t]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_hdim96_bf16_sm80.cu(16): here

/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_kernel.h(140): warning: variable "MMA_M" was declared but never referenced
          detected during:
            instantiation of "void flash::compute_attn_1rowblock<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &, int, int, int) [with Kernel_traits=Flash_fwd_kernel_traits<96, 128, 64, 4, false, false, cutlass::bfloat16_t, Flash_kernel_traits<96, 128, 64, 4, cutlass::bfloat16_t>>, Is_dropout=true, Is_causal=true, Is_even_N=false, Is_even_K=true, Return_softmax=false, Params=Flash_fwd_params]" 
(588): here
            instantiation of "void flash::compute_attn<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &) [with Kernel_traits=Flash_fwd_kernel_traits<96, 128, 64, 4, false, false, cutlass::bfloat16_t, Flash_kernel_traits<96, 128, 64, 4, cutlass::bfloat16_t>>, Is_dropout=true, Is_causal=true, Is_even_N=false, Is_even_K=true, Return_softmax=false, Params=Flash_fwd_params]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(15): here
            instantiation of "void flash_fwd_kernel<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax>(Flash_fwd_params) [with Kernel_traits=Flash_fwd_kernel_traits<96, 128, 64, 4, false, false, cutlass::bfloat16_t, Flash_kernel_traits<96, 128, 64, 4, cutlass::bfloat16_t>>, Is_dropout=true, Is_causal=true, Is_even_N=false, Is_even_K=true, Return_softmax=false]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(34): here
            instantiation of "void run_flash_fwd<Kernel_traits,Is_dropout,Is_causal>(Flash_fwd_params &, cudaStream_t) [with Kernel_traits=Flash_fwd_kernel_traits<96, 128, 64, 4, false, false, cutlass::bfloat16_t, Flash_kernel_traits<96, 128, 64, 4, cutlass::bfloat16_t>>, Is_dropout=true, Is_causal=true]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(92): here
            instantiation of "void run_mha_fwd_hdim96<T>(Flash_fwd_params &, cudaStream_t) [with T=cutlass::bfloat16_t]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_hdim96_bf16_sm80.cu(16): here

/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_kernel.h(275): warning: variable "tQrQ" was declared but never referenced
          detected during:
            instantiation of "void flash::compute_attn_1rowblock<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &, int, int, int) [with Kernel_traits=Flash_fwd_kernel_traits<96, 128, 64, 4, false, false, cutlass::bfloat16_t, Flash_kernel_traits<96, 128, 64, 4, cutlass::bfloat16_t>>, Is_dropout=true, Is_causal=true, Is_even_N=false, Is_even_K=true, Return_softmax=false, Params=Flash_fwd_params]" 
(588): here
            instantiation of "void flash::compute_attn<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &) [with Kernel_traits=Flash_fwd_kernel_traits<96, 128, 64, 4, false, false, cutlass::bfloat16_t, Flash_kernel_traits<96, 128, 64, 4, cutlass::bfloat16_t>>, Is_dropout=true, Is_causal=true, Is_even_N=false, Is_even_K=true, Return_softmax=false, Params=Flash_fwd_params]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(15): here
            instantiation of "void flash_fwd_kernel<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax>(Flash_fwd_params) [with Kernel_traits=Flash_fwd_kernel_traits<96, 128, 64, 4, false, false, cutlass::bfloat16_t, Flash_kernel_traits<96, 128, 64, 4, cutlass::bfloat16_t>>, Is_dropout=true, Is_causal=true, Is_even_N=false, Is_even_K=true, Return_softmax=false]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(34): here
            instantiation of "void run_flash_fwd<Kernel_traits,Is_dropout,Is_causal>(Flash_fwd_params &, cudaStream_t) [with Kernel_traits=Flash_fwd_kernel_traits<96, 128, 64, 4, false, false, cutlass::bfloat16_t, Flash_kernel_traits<96, 128, 64, 4, cutlass::bfloat16_t>>, Is_dropout=true, Is_causal=true]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(92): here
            instantiation of "void run_mha_fwd_hdim96<T>(Flash_fwd_params &, cudaStream_t) [with T=cutlass::bfloat16_t]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_hdim96_bf16_sm80.cu(16): here

/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_kernel.h(140): warning: variable "MMA_M" was declared but never referenced
          detected during:
            instantiation of "void flash::compute_attn_1rowblock<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &, int, int, int) [with Kernel_traits=Flash_fwd_kernel_traits<96, 128, 64, 4, false, false, cutlass::bfloat16_t, Flash_kernel_traits<96, 128, 64, 4, cutlass::bfloat16_t>>, Is_dropout=true, Is_causal=true, Is_even_N=false, Is_even_K=false, Return_softmax=true, Params=Flash_fwd_params]" 
(588): here
            instantiation of "void flash::compute_attn<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &) [with Kernel_traits=Flash_fwd_kernel_traits<96, 128, 64, 4, false, false, cutlass::bfloat16_t, Flash_kernel_traits<96, 128, 64, 4, cutlass::bfloat16_t>>, Is_dropout=true, Is_causal=true, Is_even_N=false, Is_even_K=false, Return_softmax=true, Params=Flash_fwd_params]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(15): here
            instantiation of "void flash_fwd_kernel<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax>(Flash_fwd_params) [with Kernel_traits=Flash_fwd_kernel_traits<96, 128, 64, 4, false, false, cutlass::bfloat16_t, Flash_kernel_traits<96, 128, 64, 4, cutlass::bfloat16_t>>, Is_dropout=true, Is_causal=true, Is_even_N=false, Is_even_K=false, Return_softmax=true]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(34): here
            instantiation of "void run_flash_fwd<Kernel_traits,Is_dropout,Is_causal>(Flash_fwd_params &, cudaStream_t) [with Kernel_traits=Flash_fwd_kernel_traits<96, 128, 64, 4, false, false, cutlass::bfloat16_t, Flash_kernel_traits<96, 128, 64, 4, cutlass::bfloat16_t>>, Is_dropout=true, Is_causal=true]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(92): here
            instantiation of "void run_mha_fwd_hdim96<T>(Flash_fwd_params &, cudaStream_t) [with T=cutlass::bfloat16_t]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_hdim96_bf16_sm80.cu(16): here

/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_kernel.h(275): warning: variable "tQrQ" was declared but never referenced
          detected during:
            instantiation of "void flash::compute_attn_1rowblock<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &, int, int, int) [with Kernel_traits=Flash_fwd_kernel_traits<96, 128, 64, 4, false, false, cutlass::bfloat16_t, Flash_kernel_traits<96, 128, 64, 4, cutlass::bfloat16_t>>, Is_dropout=true, Is_causal=true, Is_even_N=false, Is_even_K=false, Return_softmax=true, Params=Flash_fwd_params]" 
(588): here
            instantiation of "void flash::compute_attn<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &) [with Kernel_traits=Flash_fwd_kernel_traits<96, 128, 64, 4, false, false, cutlass::bfloat16_t, Flash_kernel_traits<96, 128, 64, 4, cutlass::bfloat16_t>>, Is_dropout=true, Is_causal=true, Is_even_N=false, Is_even_K=false, Return_softmax=true, Params=Flash_fwd_params]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(15): here
            instantiation of "void flash_fwd_kernel<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax>(Flash_fwd_params) [with Kernel_traits=Flash_fwd_kernel_traits<96, 128, 64, 4, false, false, cutlass::bfloat16_t, Flash_kernel_traits<96, 128, 64, 4, cutlass::bfloat16_t>>, Is_dropout=true, Is_causal=true, Is_even_N=false, Is_even_K=false, Return_softmax=true]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(34): here
            instantiation of "void run_flash_fwd<Kernel_traits,Is_dropout,Is_causal>(Flash_fwd_params &, cudaStream_t) [with Kernel_traits=Flash_fwd_kernel_traits<96, 128, 64, 4, false, false, cutlass::bfloat16_t, Flash_kernel_traits<96, 128, 64, 4, cutlass::bfloat16_t>>, Is_dropout=true, Is_causal=true]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(92): here
            instantiation of "void run_mha_fwd_hdim96<T>(Flash_fwd_params &, cudaStream_t) [with T=cutlass::bfloat16_t]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_hdim96_bf16_sm80.cu(16): here

/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_kernel.h(140): warning: variable "MMA_M" was declared but never referenced
          detected during:
            instantiation of "void flash::compute_attn_1rowblock<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &, int, int, int) [with Kernel_traits=Flash_fwd_kernel_traits<96, 128, 64, 4, false, false, cutlass::bfloat16_t, Flash_kernel_traits<96, 128, 64, 4, cutlass::bfloat16_t>>, Is_dropout=true, Is_causal=true, Is_even_N=false, Is_even_K=false, Return_softmax=false, Params=Flash_fwd_params]" 
(588): here
            instantiation of "void flash::compute_attn<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &) [with Kernel_traits=Flash_fwd_kernel_traits<96, 128, 64, 4, false, false, cutlass::bfloat16_t, Flash_kernel_traits<96, 128, 64, 4, cutlass::bfloat16_t>>, Is_dropout=true, Is_causal=true, Is_even_N=false, Is_even_K=false, Return_softmax=false, Params=Flash_fwd_params]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(15): here
            instantiation of "void flash_fwd_kernel<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax>(Flash_fwd_params) [with Kernel_traits=Flash_fwd_kernel_traits<96, 128, 64, 4, false, false, cutlass::bfloat16_t, Flash_kernel_traits<96, 128, 64, 4, cutlass::bfloat16_t>>, Is_dropout=true, Is_causal=true, Is_even_N=false, Is_even_K=false, Return_softmax=false]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(34): here
            instantiation of "void run_flash_fwd<Kernel_traits,Is_dropout,Is_causal>(Flash_fwd_params &, cudaStream_t) [with Kernel_traits=Flash_fwd_kernel_traits<96, 128, 64, 4, false, false, cutlass::bfloat16_t, Flash_kernel_traits<96, 128, 64, 4, cutlass::bfloat16_t>>, Is_dropout=true, Is_causal=true]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(92): here
            instantiation of "void run_mha_fwd_hdim96<T>(Flash_fwd_params &, cudaStream_t) [with T=cutlass::bfloat16_t]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_hdim96_bf16_sm80.cu(16): here

/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_kernel.h(275): warning: variable "tQrQ" was declared but never referenced
          detected during:
            instantiation of "void flash::compute_attn_1rowblock<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &, int, int, int) [with Kernel_traits=Flash_fwd_kernel_traits<96, 128, 64, 4, false, false, cutlass::bfloat16_t, Flash_kernel_traits<96, 128, 64, 4, cutlass::bfloat16_t>>, Is_dropout=true, Is_causal=true, Is_even_N=false, Is_even_K=false, Return_softmax=false, Params=Flash_fwd_params]" 
(588): here
            instantiation of "void flash::compute_attn<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &) [with Kernel_traits=Flash_fwd_kernel_traits<96, 128, 64, 4, false, false, cutlass::bfloat16_t, Flash_kernel_traits<96, 128, 64, 4, cutlass::bfloat16_t>>, Is_dropout=true, Is_causal=true, Is_even_N=false, Is_even_K=false, Return_softmax=false, Params=Flash_fwd_params]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(15): here
            instantiation of "void flash_fwd_kernel<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax>(Flash_fwd_params) [with Kernel_traits=Flash_fwd_kernel_traits<96, 128, 64, 4, false, false, cutlass::bfloat16_t, Flash_kernel_traits<96, 128, 64, 4, cutlass::bfloat16_t>>, Is_dropout=true, Is_causal=true, Is_even_N=false, Is_even_K=false, Return_softmax=false]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(34): here
            instantiation of "void run_flash_fwd<Kernel_traits,Is_dropout,Is_causal>(Flash_fwd_params &, cudaStream_t) [with Kernel_traits=Flash_fwd_kernel_traits<96, 128, 64, 4, false, false, cutlass::bfloat16_t, Flash_kernel_traits<96, 128, 64, 4, cutlass::bfloat16_t>>, Is_dropout=true, Is_causal=true]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(92): here
            instantiation of "void run_mha_fwd_hdim96<T>(Flash_fwd_params &, cudaStream_t) [with T=cutlass::bfloat16_t]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_hdim96_bf16_sm80.cu(16): here

/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_kernel.h(140): warning: variable "MMA_M" was declared but never referenced
          detected during:
            instantiation of "void flash::compute_attn_1rowblock<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &, int, int, int) [with Kernel_traits=Flash_fwd_kernel_traits<96, 128, 64, 4, false, false, cutlass::bfloat16_t, Flash_kernel_traits<96, 128, 64, 4, cutlass::bfloat16_t>>, Is_dropout=true, Is_causal=false, Is_even_N=true, Is_even_K=true, Return_softmax=true, Params=Flash_fwd_params]" 
(588): here
            instantiation of "void flash::compute_attn<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &) [with Kernel_traits=Flash_fwd_kernel_traits<96, 128, 64, 4, false, false, cutlass::bfloat16_t, Flash_kernel_traits<96, 128, 64, 4, cutlass::bfloat16_t>>, Is_dropout=true, Is_causal=false, Is_even_N=true, Is_even_K=true, Return_softmax=true, Params=Flash_fwd_params]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(15): here
            instantiation of "void flash_fwd_kernel<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax>(Flash_fwd_params) [with Kernel_traits=Flash_fwd_kernel_traits<96, 128, 64, 4, false, false, cutlass::bfloat16_t, Flash_kernel_traits<96, 128, 64, 4, cutlass::bfloat16_t>>, Is_dropout=true, Is_causal=false, Is_even_N=true, Is_even_K=true, Return_softmax=true]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(34): here
            instantiation of "void run_flash_fwd<Kernel_traits,Is_dropout,Is_causal>(Flash_fwd_params &, cudaStream_t) [with Kernel_traits=Flash_fwd_kernel_traits<96, 128, 64, 4, false, false, cutlass::bfloat16_t, Flash_kernel_traits<96, 128, 64, 4, cutlass::bfloat16_t>>, Is_dropout=true, Is_causal=false]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(92): here
            instantiation of "void run_mha_fwd_hdim96<T>(Flash_fwd_params &, cudaStream_t) [with T=cutlass::bfloat16_t]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_hdim96_bf16_sm80.cu(16): here

/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_kernel.h(275): warning: variable "tQrQ" was declared but never referenced
          detected during:
            instantiation of "void flash::compute_attn_1rowblock<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &, int, int, int) [with Kernel_traits=Flash_fwd_kernel_traits<96, 128, 64, 4, false, false, cutlass::bfloat16_t, Flash_kernel_traits<96, 128, 64, 4, cutlass::bfloat16_t>>, Is_dropout=true, Is_causal=false, Is_even_N=true, Is_even_K=true, Return_softmax=true, Params=Flash_fwd_params]" 
(588): here
            instantiation of "void flash::compute_attn<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &) [with Kernel_traits=Flash_fwd_kernel_traits<96, 128, 64, 4, false, false, cutlass::bfloat16_t, Flash_kernel_traits<96, 128, 64, 4, cutlass::bfloat16_t>>, Is_dropout=true, Is_causal=false, Is_even_N=true, Is_even_K=true, Return_softmax=true, Params=Flash_fwd_params]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(15): here
            instantiation of "void flash_fwd_kernel<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax>(Flash_fwd_params) [with Kernel_traits=Flash_fwd_kernel_traits<96, 128, 64, 4, false, false, cutlass::bfloat16_t, Flash_kernel_traits<96, 128, 64, 4, cutlass::bfloat16_t>>, Is_dropout=true, Is_causal=false, Is_even_N=true, Is_even_K=true, Return_softmax=true]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(34): here
            instantiation of "void run_flash_fwd<Kernel_traits,Is_dropout,Is_causal>(Flash_fwd_params &, cudaStream_t) [with Kernel_traits=Flash_fwd_kernel_traits<96, 128, 64, 4, false, false, cutlass::bfloat16_t, Flash_kernel_traits<96, 128, 64, 4, cutlass::bfloat16_t>>, Is_dropout=true, Is_causal=false]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(92): here
            instantiation of "void run_mha_fwd_hdim96<T>(Flash_fwd_params &, cudaStream_t) [with T=cutlass::bfloat16_t]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_hdim96_bf16_sm80.cu(16): here

/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_kernel.h(140): warning: variable "MMA_M" was declared but never referenced
          detected during:
            instantiation of "void flash::compute_attn_1rowblock<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &, int, int, int) [with Kernel_traits=Flash_fwd_kernel_traits<96, 128, 64, 4, false, false, cutlass::bfloat16_t, Flash_kernel_traits<96, 128, 64, 4, cutlass::bfloat16_t>>, Is_dropout=true, Is_causal=false, Is_even_N=true, Is_even_K=true, Return_softmax=false, Params=Flash_fwd_params]" 
(588): here
            instantiation of "void flash::compute_attn<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &) [with Kernel_traits=Flash_fwd_kernel_traits<96, 128, 64, 4, false, false, cutlass::bfloat16_t, Flash_kernel_traits<96, 128, 64, 4, cutlass::bfloat16_t>>, Is_dropout=true, Is_causal=false, Is_even_N=true, Is_even_K=true, Return_softmax=false, Params=Flash_fwd_params]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(15): here
            instantiation of "void flash_fwd_kernel<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax>(Flash_fwd_params) [with Kernel_traits=Flash_fwd_kernel_traits<96, 128, 64, 4, false, false, cutlass::bfloat16_t, Flash_kernel_traits<96, 128, 64, 4, cutlass::bfloat16_t>>, Is_dropout=true, Is_causal=false, Is_even_N=true, Is_even_K=true, Return_softmax=false]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(34): here
            instantiation of "void run_flash_fwd<Kernel_traits,Is_dropout,Is_causal>(Flash_fwd_params &, cudaStream_t) [with Kernel_traits=Flash_fwd_kernel_traits<96, 128, 64, 4, false, false, cutlass::bfloat16_t, Flash_kernel_traits<96, 128, 64, 4, cutlass::bfloat16_t>>, Is_dropout=true, Is_causal=false]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(92): here
            instantiation of "void run_mha_fwd_hdim96<T>(Flash_fwd_params &, cudaStream_t) [with T=cutlass::bfloat16_t]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_hdim96_bf16_sm80.cu(16): here

/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_kernel.h(275): warning: variable "tQrQ" was declared but never referenced
          detected during:
            instantiation of "void flash::compute_attn_1rowblock<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &, int, int, int) [with Kernel_traits=Flash_fwd_kernel_traits<96, 128, 64, 4, false, false, cutlass::bfloat16_t, Flash_kernel_traits<96, 128, 64, 4, cutlass::bfloat16_t>>, Is_dropout=true, Is_causal=false, Is_even_N=true, Is_even_K=true, Return_softmax=false, Params=Flash_fwd_params]" 
(588): here
            instantiation of "void flash::compute_attn<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &) [with Kernel_traits=Flash_fwd_kernel_traits<96, 128, 64, 4, false, false, cutlass::bfloat16_t, Flash_kernel_traits<96, 128, 64, 4, cutlass::bfloat16_t>>, Is_dropout=true, Is_causal=false, Is_even_N=true, Is_even_K=true, Return_softmax=false, Params=Flash_fwd_params]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(15): here
            instantiation of "void flash_fwd_kernel<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax>(Flash_fwd_params) [with Kernel_traits=Flash_fwd_kernel_traits<96, 128, 64, 4, false, false, cutlass::bfloat16_t, Flash_kernel_traits<96, 128, 64, 4, cutlass::bfloat16_t>>, Is_dropout=true, Is_causal=false, Is_even_N=true, Is_even_K=true, Return_softmax=false]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(34): here
            instantiation of "void run_flash_fwd<Kernel_traits,Is_dropout,Is_causal>(Flash_fwd_params &, cudaStream_t) [with Kernel_traits=Flash_fwd_kernel_traits<96, 128, 64, 4, false, false, cutlass::bfloat16_t, Flash_kernel_traits<96, 128, 64, 4, cutlass::bfloat16_t>>, Is_dropout=true, Is_causal=false]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(92): here
            instantiation of "void run_mha_fwd_hdim96<T>(Flash_fwd_params &, cudaStream_t) [with T=cutlass::bfloat16_t]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_hdim96_bf16_sm80.cu(16): here

/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_kernel.h(140): warning: variable "MMA_M" was declared but never referenced
          detected during:
            instantiation of "void flash::compute_attn_1rowblock<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &, int, int, int) [with Kernel_traits=Flash_fwd_kernel_traits<96, 128, 64, 4, false, false, cutlass::bfloat16_t, Flash_kernel_traits<96, 128, 64, 4, cutlass::bfloat16_t>>, Is_dropout=true, Is_causal=false, Is_even_N=true, Is_even_K=false, Return_softmax=true, Params=Flash_fwd_params]" 
(588): here
            instantiation of "void flash::compute_attn<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &) [with Kernel_traits=Flash_fwd_kernel_traits<96, 128, 64, 4, false, false, cutlass::bfloat16_t, Flash_kernel_traits<96, 128, 64, 4, cutlass::bfloat16_t>>, Is_dropout=true, Is_causal=false, Is_even_N=true, Is_even_K=false, Return_softmax=true, Params=Flash_fwd_params]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(15): here
            instantiation of "void flash_fwd_kernel<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax>(Flash_fwd_params) [with Kernel_traits=Flash_fwd_kernel_traits<96, 128, 64, 4, false, false, cutlass::bfloat16_t, Flash_kernel_traits<96, 128, 64, 4, cutlass::bfloat16_t>>, Is_dropout=true, Is_causal=false, Is_even_N=true, Is_even_K=false, Return_softmax=true]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(34): here
            instantiation of "void run_flash_fwd<Kernel_traits,Is_dropout,Is_causal>(Flash_fwd_params &, cudaStream_t) [with Kernel_traits=Flash_fwd_kernel_traits<96, 128, 64, 4, false, false, cutlass::bfloat16_t, Flash_kernel_traits<96, 128, 64, 4, cutlass::bfloat16_t>>, Is_dropout=true, Is_causal=false]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(92): here
            instantiation of "void run_mha_fwd_hdim96<T>(Flash_fwd_params &, cudaStream_t) [with T=cutlass::bfloat16_t]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_hdim96_bf16_sm80.cu(16): here

/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_kernel.h(275): warning: variable "tQrQ" was declared but never referenced
          detected during:
            instantiation of "void flash::compute_attn_1rowblock<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &, int, int, int) [with Kernel_traits=Flash_fwd_kernel_traits<96, 128, 64, 4, false, false, cutlass::bfloat16_t, Flash_kernel_traits<96, 128, 64, 4, cutlass::bfloat16_t>>, Is_dropout=true, Is_causal=false, Is_even_N=true, Is_even_K=false, Return_softmax=true, Params=Flash_fwd_params]" 
(588): here
            instantiation of "void flash::compute_attn<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &) [with Kernel_traits=Flash_fwd_kernel_traits<96, 128, 64, 4, false, false, cutlass::bfloat16_t, Flash_kernel_traits<96, 128, 64, 4, cutlass::bfloat16_t>>, Is_dropout=true, Is_causal=false, Is_even_N=true, Is_even_K=false, Return_softmax=true, Params=Flash_fwd_params]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(15): here
            instantiation of "void flash_fwd_kernel<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax>(Flash_fwd_params) [with Kernel_traits=Flash_fwd_kernel_traits<96, 128, 64, 4, false, false, cutlass::bfloat16_t, Flash_kernel_traits<96, 128, 64, 4, cutlass::bfloat16_t>>, Is_dropout=true, Is_causal=false, Is_even_N=true, Is_even_K=false, Return_softmax=true]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(34): here
            instantiation of "void run_flash_fwd<Kernel_traits,Is_dropout,Is_causal>(Flash_fwd_params &, cudaStream_t) [with Kernel_traits=Flash_fwd_kernel_traits<96, 128, 64, 4, false, false, cutlass::bfloat16_t, Flash_kernel_traits<96, 128, 64, 4, cutlass::bfloat16_t>>, Is_dropout=true, Is_causal=false]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(92): here
            instantiation of "void run_mha_fwd_hdim96<T>(Flash_fwd_params &, cudaStream_t) [with T=cutlass::bfloat16_t]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_hdim96_bf16_sm80.cu(16): here

/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_kernel.h(140): warning: variable "MMA_M" was declared but never referenced
          detected during:
            instantiation of "void flash::compute_attn_1rowblock<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &, int, int, int) [with Kernel_traits=Flash_fwd_kernel_traits<96, 128, 64, 4, false, false, cutlass::bfloat16_t, Flash_kernel_traits<96, 128, 64, 4, cutlass::bfloat16_t>>, Is_dropout=true, Is_causal=false, Is_even_N=true, Is_even_K=false, Return_softmax=false, Params=Flash_fwd_params]" 
(588): here
            instantiation of "void flash::compute_attn<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &) [with Kernel_traits=Flash_fwd_kernel_traits<96, 128, 64, 4, false, false, cutlass::bfloat16_t, Flash_kernel_traits<96, 128, 64, 4, cutlass::bfloat16_t>>, Is_dropout=true, Is_causal=false, Is_even_N=true, Is_even_K=false, Return_softmax=false, Params=Flash_fwd_params]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(15): here
            instantiation of "void flash_fwd_kernel<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax>(Flash_fwd_params) [with Kernel_traits=Flash_fwd_kernel_traits<96, 128, 64, 4, false, false, cutlass::bfloat16_t, Flash_kernel_traits<96, 128, 64, 4, cutlass::bfloat16_t>>, Is_dropout=true, Is_causal=false, Is_even_N=true, Is_even_K=false, Return_softmax=false]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(34): here
            instantiation of "void run_flash_fwd<Kernel_traits,Is_dropout,Is_causal>(Flash_fwd_params &, cudaStream_t) [with Kernel_traits=Flash_fwd_kernel_traits<96, 128, 64, 4, false, false, cutlass::bfloat16_t, Flash_kernel_traits<96, 128, 64, 4, cutlass::bfloat16_t>>, Is_dropout=true, Is_causal=false]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(92): here
            instantiation of "void run_mha_fwd_hdim96<T>(Flash_fwd_params &, cudaStream_t) [with T=cutlass::bfloat16_t]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_hdim96_bf16_sm80.cu(16): here

/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_kernel.h(275): warning: variable "tQrQ" was declared but never referenced
          detected during:
            instantiation of "void flash::compute_attn_1rowblock<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &, int, int, int) [with Kernel_traits=Flash_fwd_kernel_traits<96, 128, 64, 4, false, false, cutlass::bfloat16_t, Flash_kernel_traits<96, 128, 64, 4, cutlass::bfloat16_t>>, Is_dropout=true, Is_causal=false, Is_even_N=true, Is_even_K=false, Return_softmax=false, Params=Flash_fwd_params]" 
(588): here
            instantiation of "void flash::compute_attn<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &) [with Kernel_traits=Flash_fwd_kernel_traits<96, 128, 64, 4, false, false, cutlass::bfloat16_t, Flash_kernel_traits<96, 128, 64, 4, cutlass::bfloat16_t>>, Is_dropout=true, Is_causal=false, Is_even_N=true, Is_even_K=false, Return_softmax=false, Params=Flash_fwd_params]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(15): here
            instantiation of "void flash_fwd_kernel<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax>(Flash_fwd_params) [with Kernel_traits=Flash_fwd_kernel_traits<96, 128, 64, 4, false, false, cutlass::bfloat16_t, Flash_kernel_traits<96, 128, 64, 4, cutlass::bfloat16_t>>, Is_dropout=true, Is_causal=false, Is_even_N=true, Is_even_K=false, Return_softmax=false]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(34): here
            instantiation of "void run_flash_fwd<Kernel_traits,Is_dropout,Is_causal>(Flash_fwd_params &, cudaStream_t) [with Kernel_traits=Flash_fwd_kernel_traits<96, 128, 64, 4, false, false, cutlass::bfloat16_t, Flash_kernel_traits<96, 128, 64, 4, cutlass::bfloat16_t>>, Is_dropout=true, Is_causal=false]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(92): here
            instantiation of "void run_mha_fwd_hdim96<T>(Flash_fwd_params &, cudaStream_t) [with T=cutlass::bfloat16_t]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_hdim96_bf16_sm80.cu(16): here

/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_kernel.h(140): warning: variable "MMA_M" was declared but never referenced
          detected during:
            instantiation of "void flash::compute_attn_1rowblock<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &, int, int, int) [with Kernel_traits=Flash_fwd_kernel_traits<96, 128, 64, 4, false, false, cutlass::bfloat16_t, Flash_kernel_traits<96, 128, 64, 4, cutlass::bfloat16_t>>, Is_dropout=true, Is_causal=false, Is_even_N=false, Is_even_K=true, Return_softmax=true, Params=Flash_fwd_params]" 
(588): here
            instantiation of "void flash::compute_attn<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &) [with Kernel_traits=Flash_fwd_kernel_traits<96, 128, 64, 4, false, false, cutlass::bfloat16_t, Flash_kernel_traits<96, 128, 64, 4, cutlass::bfloat16_t>>, Is_dropout=true, Is_causal=false, Is_even_N=false, Is_even_K=true, Return_softmax=true, Params=Flash_fwd_params]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(15): here
            instantiation of "void flash_fwd_kernel<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax>(Flash_fwd_params) [with Kernel_traits=Flash_fwd_kernel_traits<96, 128, 64, 4, false, false, cutlass::bfloat16_t, Flash_kernel_traits<96, 128, 64, 4, cutlass::bfloat16_t>>, Is_dropout=true, Is_causal=false, Is_even_N=false, Is_even_K=true, Return_softmax=true]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(34): here
            instantiation of "void run_flash_fwd<Kernel_traits,Is_dropout,Is_causal>(Flash_fwd_params &, cudaStream_t) [with Kernel_traits=Flash_fwd_kernel_traits<96, 128, 64, 4, false, false, cutlass::bfloat16_t, Flash_kernel_traits<96, 128, 64, 4, cutlass::bfloat16_t>>, Is_dropout=true, Is_causal=false]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(92): here
            instantiation of "void run_mha_fwd_hdim96<T>(Flash_fwd_params &, cudaStream_t) [with T=cutlass::bfloat16_t]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_hdim96_bf16_sm80.cu(16): here

/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_kernel.h(275): warning: variable "tQrQ" was declared but never referenced
          detected during:
            instantiation of "void flash::compute_attn_1rowblock<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &, int, int, int) [with Kernel_traits=Flash_fwd_kernel_traits<96, 128, 64, 4, false, false, cutlass::bfloat16_t, Flash_kernel_traits<96, 128, 64, 4, cutlass::bfloat16_t>>, Is_dropout=true, Is_causal=false, Is_even_N=false, Is_even_K=true, Return_softmax=true, Params=Flash_fwd_params]" 
(588): here
            instantiation of "void flash::compute_attn<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &) [with Kernel_traits=Flash_fwd_kernel_traits<96, 128, 64, 4, false, false, cutlass::bfloat16_t, Flash_kernel_traits<96, 128, 64, 4, cutlass::bfloat16_t>>, Is_dropout=true, Is_causal=false, Is_even_N=false, Is_even_K=true, Return_softmax=true, Params=Flash_fwd_params]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(15): here
            instantiation of "void flash_fwd_kernel<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax>(Flash_fwd_params) [with Kernel_traits=Flash_fwd_kernel_traits<96, 128, 64, 4, false, false, cutlass::bfloat16_t, Flash_kernel_traits<96, 128, 64, 4, cutlass::bfloat16_t>>, Is_dropout=true, Is_causal=false, Is_even_N=false, Is_even_K=true, Return_softmax=true]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(34): here
            instantiation of "void run_flash_fwd<Kernel_traits,Is_dropout,Is_causal>(Flash_fwd_params &, cudaStream_t) [with Kernel_traits=Flash_fwd_kernel_traits<96, 128, 64, 4, false, false, cutlass::bfloat16_t, Flash_kernel_traits<96, 128, 64, 4, cutlass::bfloat16_t>>, Is_dropout=true, Is_causal=false]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(92): here
            instantiation of "void run_mha_fwd_hdim96<T>(Flash_fwd_params &, cudaStream_t) [with T=cutlass::bfloat16_t]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_hdim96_bf16_sm80.cu(16): here

/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_kernel.h(140): warning: variable "MMA_M" was declared but never referenced
          detected during:
            instantiation of "void flash::compute_attn_1rowblock<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &, int, int, int) [with Kernel_traits=Flash_fwd_kernel_traits<96, 128, 64, 4, false, false, cutlass::bfloat16_t, Flash_kernel_traits<96, 128, 64, 4, cutlass::bfloat16_t>>, Is_dropout=true, Is_causal=false, Is_even_N=false, Is_even_K=true, Return_softmax=false, Params=Flash_fwd_params]" 
(588): here
            instantiation of "void flash::compute_attn<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &) [with Kernel_traits=Flash_fwd_kernel_traits<96, 128, 64, 4, false, false, cutlass::bfloat16_t, Flash_kernel_traits<96, 128, 64, 4, cutlass::bfloat16_t>>, Is_dropout=true, Is_causal=false, Is_even_N=false, Is_even_K=true, Return_softmax=false, Params=Flash_fwd_params]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(15): here
            instantiation of "void flash_fwd_kernel<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax>(Flash_fwd_params) [with Kernel_traits=Flash_fwd_kernel_traits<96, 128, 64, 4, false, false, cutlass::bfloat16_t, Flash_kernel_traits<96, 128, 64, 4, cutlass::bfloat16_t>>, Is_dropout=true, Is_causal=false, Is_even_N=false, Is_even_K=true, Return_softmax=false]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(34): here
            instantiation of "void run_flash_fwd<Kernel_traits,Is_dropout,Is_causal>(Flash_fwd_params &, cudaStream_t) [with Kernel_traits=Flash_fwd_kernel_traits<96, 128, 64, 4, false, false, cutlass::bfloat16_t, Flash_kernel_traits<96, 128, 64, 4, cutlass::bfloat16_t>>, Is_dropout=true, Is_causal=false]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(92): here
            instantiation of "void run_mha_fwd_hdim96<T>(Flash_fwd_params &, cudaStream_t) [with T=cutlass::bfloat16_t]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_hdim96_bf16_sm80.cu(16): here

/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_kernel.h(275): warning: variable "tQrQ" was declared but never referenced
          detected during:
            instantiation of "void flash::compute_attn_1rowblock<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &, int, int, int) [with Kernel_traits=Flash_fwd_kernel_traits<96, 128, 64, 4, false, false, cutlass::bfloat16_t, Flash_kernel_traits<96, 128, 64, 4, cutlass::bfloat16_t>>, Is_dropout=true, Is_causal=false, Is_even_N=false, Is_even_K=true, Return_softmax=false, Params=Flash_fwd_params]" 
(588): here
            instantiation of "void flash::compute_attn<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &) [with Kernel_traits=Flash_fwd_kernel_traits<96, 128, 64, 4, false, false, cutlass::bfloat16_t, Flash_kernel_traits<96, 128, 64, 4, cutlass::bfloat16_t>>, Is_dropout=true, Is_causal=false, Is_even_N=false, Is_even_K=true, Return_softmax=false, Params=Flash_fwd_params]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(15): here
            instantiation of "void flash_fwd_kernel<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax>(Flash_fwd_params) [with Kernel_traits=Flash_fwd_kernel_traits<96, 128, 64, 4, false, false, cutlass::bfloat16_t, Flash_kernel_traits<96, 128, 64, 4, cutlass::bfloat16_t>>, Is_dropout=true, Is_causal=false, Is_even_N=false, Is_even_K=true, Return_softmax=false]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(34): here
            instantiation of "void run_flash_fwd<Kernel_traits,Is_dropout,Is_causal>(Flash_fwd_params &, cudaStream_t) [with Kernel_traits=Flash_fwd_kernel_traits<96, 128, 64, 4, false, false, cutlass::bfloat16_t, Flash_kernel_traits<96, 128, 64, 4, cutlass::bfloat16_t>>, Is_dropout=true, Is_causal=false]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(92): here
            instantiation of "void run_mha_fwd_hdim96<T>(Flash_fwd_params &, cudaStream_t) [with T=cutlass::bfloat16_t]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_hdim96_bf16_sm80.cu(16): here

/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_kernel.h(140): warning: variable "MMA_M" was declared but never referenced
          detected during:
            instantiation of "void flash::compute_attn_1rowblock<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &, int, int, int) [with Kernel_traits=Flash_fwd_kernel_traits<96, 128, 64, 4, false, false, cutlass::bfloat16_t, Flash_kernel_traits<96, 128, 64, 4, cutlass::bfloat16_t>>, Is_dropout=true, Is_causal=false, Is_even_N=false, Is_even_K=false, Return_softmax=true, Params=Flash_fwd_params]" 
(588): here
            instantiation of "void flash::compute_attn<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &) [with Kernel_traits=Flash_fwd_kernel_traits<96, 128, 64, 4, false, false, cutlass::bfloat16_t, Flash_kernel_traits<96, 128, 64, 4, cutlass::bfloat16_t>>, Is_dropout=true, Is_causal=false, Is_even_N=false, Is_even_K=false, Return_softmax=true, Params=Flash_fwd_params]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(15): here
            instantiation of "void flash_fwd_kernel<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax>(Flash_fwd_params) [with Kernel_traits=Flash_fwd_kernel_traits<96, 128, 64, 4, false, false, cutlass::bfloat16_t, Flash_kernel_traits<96, 128, 64, 4, cutlass::bfloat16_t>>, Is_dropout=true, Is_causal=false, Is_even_N=false, Is_even_K=false, Return_softmax=true]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(34): here
            instantiation of "void run_flash_fwd<Kernel_traits,Is_dropout,Is_causal>(Flash_fwd_params &, cudaStream_t) [with Kernel_traits=Flash_fwd_kernel_traits<96, 128, 64, 4, false, false, cutlass::bfloat16_t, Flash_kernel_traits<96, 128, 64, 4, cutlass::bfloat16_t>>, Is_dropout=true, Is_causal=false]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(92): here
            instantiation of "void run_mha_fwd_hdim96<T>(Flash_fwd_params &, cudaStream_t) [with T=cutlass::bfloat16_t]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_hdim96_bf16_sm80.cu(16): here

/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_kernel.h(275): warning: variable "tQrQ" was declared but never referenced
          detected during:
            instantiation of "void flash::compute_attn_1rowblock<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &, int, int, int) [with Kernel_traits=Flash_fwd_kernel_traits<96, 128, 64, 4, false, false, cutlass::bfloat16_t, Flash_kernel_traits<96, 128, 64, 4, cutlass::bfloat16_t>>, Is_dropout=true, Is_causal=false, Is_even_N=false, Is_even_K=false, Return_softmax=true, Params=Flash_fwd_params]" 
(588): here
            instantiation of "void flash::compute_attn<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &) [with Kernel_traits=Flash_fwd_kernel_traits<96, 128, 64, 4, false, false, cutlass::bfloat16_t, Flash_kernel_traits<96, 128, 64, 4, cutlass::bfloat16_t>>, Is_dropout=true, Is_causal=false, Is_even_N=false, Is_even_K=false, Return_softmax=true, Params=Flash_fwd_params]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(15): here
            instantiation of "void flash_fwd_kernel<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax>(Flash_fwd_params) [with Kernel_traits=Flash_fwd_kernel_traits<96, 128, 64, 4, false, false, cutlass::bfloat16_t, Flash_kernel_traits<96, 128, 64, 4, cutlass::bfloat16_t>>, Is_dropout=true, Is_causal=false, Is_even_N=false, Is_even_K=false, Return_softmax=true]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(34): here
            instantiation of "void run_flash_fwd<Kernel_traits,Is_dropout,Is_causal>(Flash_fwd_params &, cudaStream_t) [with Kernel_traits=Flash_fwd_kernel_traits<96, 128, 64, 4, false, false, cutlass::bfloat16_t, Flash_kernel_traits<96, 128, 64, 4, cutlass::bfloat16_t>>, Is_dropout=true, Is_causal=false]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(92): here
            instantiation of "void run_mha_fwd_hdim96<T>(Flash_fwd_params &, cudaStream_t) [with T=cutlass::bfloat16_t]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_hdim96_bf16_sm80.cu(16): here

/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_kernel.h(140): warning: variable "MMA_M" was declared but never referenced
          detected during:
            instantiation of "void flash::compute_attn_1rowblock<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &, int, int, int) [with Kernel_traits=Flash_fwd_kernel_traits<96, 128, 64, 4, false, false, cutlass::bfloat16_t, Flash_kernel_traits<96, 128, 64, 4, cutlass::bfloat16_t>>, Is_dropout=true, Is_causal=false, Is_even_N=false, Is_even_K=false, Return_softmax=false, Params=Flash_fwd_params]" 
(588): here
            instantiation of "void flash::compute_attn<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &) [with Kernel_traits=Flash_fwd_kernel_traits<96, 128, 64, 4, false, false, cutlass::bfloat16_t, Flash_kernel_traits<96, 128, 64, 4, cutlass::bfloat16_t>>, Is_dropout=true, Is_causal=false, Is_even_N=false, Is_even_K=false, Return_softmax=false, Params=Flash_fwd_params]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(15): here
            instantiation of "void flash_fwd_kernel<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax>(Flash_fwd_params) [with Kernel_traits=Flash_fwd_kernel_traits<96, 128, 64, 4, false, false, cutlass::bfloat16_t, Flash_kernel_traits<96, 128, 64, 4, cutlass::bfloat16_t>>, Is_dropout=true, Is_causal=false, Is_even_N=false, Is_even_K=false, Return_softmax=false]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(34): here
            instantiation of "void run_flash_fwd<Kernel_traits,Is_dropout,Is_causal>(Flash_fwd_params &, cudaStream_t) [with Kernel_traits=Flash_fwd_kernel_traits<96, 128, 64, 4, false, false, cutlass::bfloat16_t, Flash_kernel_traits<96, 128, 64, 4, cutlass::bfloat16_t>>, Is_dropout=true, Is_causal=false]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(92): here
            instantiation of "void run_mha_fwd_hdim96<T>(Flash_fwd_params &, cudaStream_t) [with T=cutlass::bfloat16_t]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_hdim96_bf16_sm80.cu(16): here

/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_kernel.h(275): warning: variable "tQrQ" was declared but never referenced
          detected during:
            instantiation of "void flash::compute_attn_1rowblock<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &, int, int, int) [with Kernel_traits=Flash_fwd_kernel_traits<96, 128, 64, 4, false, false, cutlass::bfloat16_t, Flash_kernel_traits<96, 128, 64, 4, cutlass::bfloat16_t>>, Is_dropout=true, Is_causal=false, Is_even_N=false, Is_even_K=false, Return_softmax=false, Params=Flash_fwd_params]" 
(588): here
            instantiation of "void flash::compute_attn<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &) [with Kernel_traits=Flash_fwd_kernel_traits<96, 128, 64, 4, false, false, cutlass::bfloat16_t, Flash_kernel_traits<96, 128, 64, 4, cutlass::bfloat16_t>>, Is_dropout=true, Is_causal=false, Is_even_N=false, Is_even_K=false, Return_softmax=false, Params=Flash_fwd_params]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(15): here
            instantiation of "void flash_fwd_kernel<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax>(Flash_fwd_params) [with Kernel_traits=Flash_fwd_kernel_traits<96, 128, 64, 4, false, false, cutlass::bfloat16_t, Flash_kernel_traits<96, 128, 64, 4, cutlass::bfloat16_t>>, Is_dropout=true, Is_causal=false, Is_even_N=false, Is_even_K=false, Return_softmax=false]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(34): here
            instantiation of "void run_flash_fwd<Kernel_traits,Is_dropout,Is_causal>(Flash_fwd_params &, cudaStream_t) [with Kernel_traits=Flash_fwd_kernel_traits<96, 128, 64, 4, false, false, cutlass::bfloat16_t, Flash_kernel_traits<96, 128, 64, 4, cutlass::bfloat16_t>>, Is_dropout=true, Is_causal=false]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(92): here
            instantiation of "void run_mha_fwd_hdim96<T>(Flash_fwd_params &, cudaStream_t) [with T=cutlass::bfloat16_t]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_hdim96_bf16_sm80.cu(16): here

/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_kernel.h(140): warning: variable "MMA_M" was declared but never referenced
          detected during:
            instantiation of "void flash::compute_attn_1rowblock<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &, int, int, int) [with Kernel_traits=Flash_fwd_kernel_traits<96, 64, 64, 4, false, false, cutlass::bfloat16_t, Flash_kernel_traits<96, 64, 64, 4, cutlass::bfloat16_t>>, Is_dropout=false, Is_causal=true, Is_even_N=true, Is_even_K=true, Return_softmax=false, Params=Flash_fwd_params]" 
(588): here
            instantiation of "void flash::compute_attn<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &) [with Kernel_traits=Flash_fwd_kernel_traits<96, 64, 64, 4, false, false, cutlass::bfloat16_t, Flash_kernel_traits<96, 64, 64, 4, cutlass::bfloat16_t>>, Is_dropout=false, Is_causal=true, Is_even_N=true, Is_even_K=true, Return_softmax=false, Params=Flash_fwd_params]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(15): here
            instantiation of "void flash_fwd_kernel<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax>(Flash_fwd_params) [with Kernel_traits=Flash_fwd_kernel_traits<96, 64, 64, 4, false, false, cutlass::bfloat16_t, Flash_kernel_traits<96, 64, 64, 4, cutlass::bfloat16_t>>, Is_dropout=false, Is_causal=true, Is_even_N=true, Is_even_K=true, Return_softmax=false]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(34): here
            instantiation of "void run_flash_fwd<Kernel_traits,Is_dropout,Is_causal>(Flash_fwd_params &, cudaStream_t) [with Kernel_traits=Flash_fwd_kernel_traits<96, 64, 64, 4, false, false, cutlass::bfloat16_t, Flash_kernel_traits<96, 64, 64, 4, cutlass::bfloat16_t>>, Is_dropout=false, Is_causal=true]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(92): here
            instantiation of "void run_mha_fwd_hdim96<T>(Flash_fwd_params &, cudaStream_t) [with T=cutlass::bfloat16_t]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_hdim96_bf16_sm80.cu(16): here

/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_kernel.h(275): warning: variable "tQrQ" was declared but never referenced
          detected during:
            instantiation of "void flash::compute_attn_1rowblock<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &, int, int, int) [with Kernel_traits=Flash_fwd_kernel_traits<96, 64, 64, 4, false, false, cutlass::bfloat16_t, Flash_kernel_traits<96, 64, 64, 4, cutlass::bfloat16_t>>, Is_dropout=false, Is_causal=true, Is_even_N=true, Is_even_K=true, Return_softmax=false, Params=Flash_fwd_params]" 
(588): here
            instantiation of "void flash::compute_attn<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &) [with Kernel_traits=Flash_fwd_kernel_traits<96, 64, 64, 4, false, false, cutlass::bfloat16_t, Flash_kernel_traits<96, 64, 64, 4, cutlass::bfloat16_t>>, Is_dropout=false, Is_causal=true, Is_even_N=true, Is_even_K=true, Return_softmax=false, Params=Flash_fwd_params]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(15): here
            instantiation of "void flash_fwd_kernel<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax>(Flash_fwd_params) [with Kernel_traits=Flash_fwd_kernel_traits<96, 64, 64, 4, false, false, cutlass::bfloat16_t, Flash_kernel_traits<96, 64, 64, 4, cutlass::bfloat16_t>>, Is_dropout=false, Is_causal=true, Is_even_N=true, Is_even_K=true, Return_softmax=false]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(34): here
            instantiation of "void run_flash_fwd<Kernel_traits,Is_dropout,Is_causal>(Flash_fwd_params &, cudaStream_t) [with Kernel_traits=Flash_fwd_kernel_traits<96, 64, 64, 4, false, false, cutlass::bfloat16_t, Flash_kernel_traits<96, 64, 64, 4, cutlass::bfloat16_t>>, Is_dropout=false, Is_causal=true]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(92): here
            instantiation of "void run_mha_fwd_hdim96<T>(Flash_fwd_params &, cudaStream_t) [with T=cutlass::bfloat16_t]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_hdim96_bf16_sm80.cu(16): here

/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_kernel.h(140): warning: variable "MMA_M" was declared but never referenced
          detected during:
            instantiation of "void flash::compute_attn_1rowblock<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &, int, int, int) [with Kernel_traits=Flash_fwd_kernel_traits<96, 64, 64, 4, false, false, cutlass::bfloat16_t, Flash_kernel_traits<96, 64, 64, 4, cutlass::bfloat16_t>>, Is_dropout=false, Is_causal=true, Is_even_N=true, Is_even_K=false, Return_softmax=false, Params=Flash_fwd_params]" 
(588): here
            instantiation of "void flash::compute_attn<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &) [with Kernel_traits=Flash_fwd_kernel_traits<96, 64, 64, 4, false, false, cutlass::bfloat16_t, Flash_kernel_traits<96, 64, 64, 4, cutlass::bfloat16_t>>, Is_dropout=false, Is_causal=true, Is_even_N=true, Is_even_K=false, Return_softmax=false, Params=Flash_fwd_params]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(15): here
            instantiation of "void flash_fwd_kernel<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax>(Flash_fwd_params) [with Kernel_traits=Flash_fwd_kernel_traits<96, 64, 64, 4, false, false, cutlass::bfloat16_t, Flash_kernel_traits<96, 64, 64, 4, cutlass::bfloat16_t>>, Is_dropout=false, Is_causal=true, Is_even_N=true, Is_even_K=false, Return_softmax=false]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(34): here
            instantiation of "void run_flash_fwd<Kernel_traits,Is_dropout,Is_causal>(Flash_fwd_params &, cudaStream_t) [with Kernel_traits=Flash_fwd_kernel_traits<96, 64, 64, 4, false, false, cutlass::bfloat16_t, Flash_kernel_traits<96, 64, 64, 4, cutlass::bfloat16_t>>, Is_dropout=false, Is_causal=true]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(92): here
            instantiation of "void run_mha_fwd_hdim96<T>(Flash_fwd_params &, cudaStream_t) [with T=cutlass::bfloat16_t]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_hdim96_bf16_sm80.cu(16): here

/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_kernel.h(275): warning: variable "tQrQ" was declared but never referenced
          detected during:
            instantiation of "void flash::compute_attn_1rowblock<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &, int, int, int) [with Kernel_traits=Flash_fwd_kernel_traits<96, 64, 64, 4, false, false, cutlass::bfloat16_t, Flash_kernel_traits<96, 64, 64, 4, cutlass::bfloat16_t>>, Is_dropout=false, Is_causal=true, Is_even_N=true, Is_even_K=false, Return_softmax=false, Params=Flash_fwd_params]" 
(588): here
            instantiation of "void flash::compute_attn<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &) [with Kernel_traits=Flash_fwd_kernel_traits<96, 64, 64, 4, false, false, cutlass::bfloat16_t, Flash_kernel_traits<96, 64, 64, 4, cutlass::bfloat16_t>>, Is_dropout=false, Is_causal=true, Is_even_N=true, Is_even_K=false, Return_softmax=false, Params=Flash_fwd_params]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(15): here
            instantiation of "void flash_fwd_kernel<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax>(Flash_fwd_params) [with Kernel_traits=Flash_fwd_kernel_traits<96, 64, 64, 4, false, false, cutlass::bfloat16_t, Flash_kernel_traits<96, 64, 64, 4, cutlass::bfloat16_t>>, Is_dropout=false, Is_causal=true, Is_even_N=true, Is_even_K=false, Return_softmax=false]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(34): here
            instantiation of "void run_flash_fwd<Kernel_traits,Is_dropout,Is_causal>(Flash_fwd_params &, cudaStream_t) [with Kernel_traits=Flash_fwd_kernel_traits<96, 64, 64, 4, false, false, cutlass::bfloat16_t, Flash_kernel_traits<96, 64, 64, 4, cutlass::bfloat16_t>>, Is_dropout=false, Is_causal=true]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(92): here
            instantiation of "void run_mha_fwd_hdim96<T>(Flash_fwd_params &, cudaStream_t) [with T=cutlass::bfloat16_t]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_hdim96_bf16_sm80.cu(16): here

/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_kernel.h(140): warning: variable "MMA_M" was declared but never referenced
          detected during:
            instantiation of "void flash::compute_attn_1rowblock<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &, int, int, int) [with Kernel_traits=Flash_fwd_kernel_traits<96, 64, 64, 4, false, false, cutlass::bfloat16_t, Flash_kernel_traits<96, 64, 64, 4, cutlass::bfloat16_t>>, Is_dropout=false, Is_causal=true, Is_even_N=false, Is_even_K=true, Return_softmax=false, Params=Flash_fwd_params]" 
(588): here
            instantiation of "void flash::compute_attn<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &) [with Kernel_traits=Flash_fwd_kernel_traits<96, 64, 64, 4, false, false, cutlass::bfloat16_t, Flash_kernel_traits<96, 64, 64, 4, cutlass::bfloat16_t>>, Is_dropout=false, Is_causal=true, Is_even_N=false, Is_even_K=true, Return_softmax=false, Params=Flash_fwd_params]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(15): here
            instantiation of "void flash_fwd_kernel<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax>(Flash_fwd_params) [with Kernel_traits=Flash_fwd_kernel_traits<96, 64, 64, 4, false, false, cutlass::bfloat16_t, Flash_kernel_traits<96, 64, 64, 4, cutlass::bfloat16_t>>, Is_dropout=false, Is_causal=true, Is_even_N=false, Is_even_K=true, Return_softmax=false]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(34): here
            instantiation of "void run_flash_fwd<Kernel_traits,Is_dropout,Is_causal>(Flash_fwd_params &, cudaStream_t) [with Kernel_traits=Flash_fwd_kernel_traits<96, 64, 64, 4, false, false, cutlass::bfloat16_t, Flash_kernel_traits<96, 64, 64, 4, cutlass::bfloat16_t>>, Is_dropout=false, Is_causal=true]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(92): here
            instantiation of "void run_mha_fwd_hdim96<T>(Flash_fwd_params &, cudaStream_t) [with T=cutlass::bfloat16_t]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_hdim96_bf16_sm80.cu(16): here

/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_kernel.h(275): warning: variable "tQrQ" was declared but never referenced
          detected during:
            instantiation of "void flash::compute_attn_1rowblock<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &, int, int, int) [with Kernel_traits=Flash_fwd_kernel_traits<96, 64, 64, 4, false, false, cutlass::bfloat16_t, Flash_kernel_traits<96, 64, 64, 4, cutlass::bfloat16_t>>, Is_dropout=false, Is_causal=true, Is_even_N=false, Is_even_K=true, Return_softmax=false, Params=Flash_fwd_params]" 
(588): here
            instantiation of "void flash::compute_attn<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &) [with Kernel_traits=Flash_fwd_kernel_traits<96, 64, 64, 4, false, false, cutlass::bfloat16_t, Flash_kernel_traits<96, 64, 64, 4, cutlass::bfloat16_t>>, Is_dropout=false, Is_causal=true, Is_even_N=false, Is_even_K=true, Return_softmax=false, Params=Flash_fwd_params]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(15): here
            instantiation of "void flash_fwd_kernel<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax>(Flash_fwd_params) [with Kernel_traits=Flash_fwd_kernel_traits<96, 64, 64, 4, false, false, cutlass::bfloat16_t, Flash_kernel_traits<96, 64, 64, 4, cutlass::bfloat16_t>>, Is_dropout=false, Is_causal=true, Is_even_N=false, Is_even_K=true, Return_softmax=false]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(34): here
            instantiation of "void run_flash_fwd<Kernel_traits,Is_dropout,Is_causal>(Flash_fwd_params &, cudaStream_t) [with Kernel_traits=Flash_fwd_kernel_traits<96, 64, 64, 4, false, false, cutlass::bfloat16_t, Flash_kernel_traits<96, 64, 64, 4, cutlass::bfloat16_t>>, Is_dropout=false, Is_causal=true]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(92): here
            instantiation of "void run_mha_fwd_hdim96<T>(Flash_fwd_params &, cudaStream_t) [with T=cutlass::bfloat16_t]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_hdim96_bf16_sm80.cu(16): here

/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_kernel.h(140): warning: variable "MMA_M" was declared but never referenced
          detected during:
            instantiation of "void flash::compute_attn_1rowblock<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &, int, int, int) [with Kernel_traits=Flash_fwd_kernel_traits<96, 64, 64, 4, false, false, cutlass::bfloat16_t, Flash_kernel_traits<96, 64, 64, 4, cutlass::bfloat16_t>>, Is_dropout=false, Is_causal=true, Is_even_N=false, Is_even_K=false, Return_softmax=false, Params=Flash_fwd_params]" 
(588): here
            instantiation of "void flash::compute_attn<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &) [with Kernel_traits=Flash_fwd_kernel_traits<96, 64, 64, 4, false, false, cutlass::bfloat16_t, Flash_kernel_traits<96, 64, 64, 4, cutlass::bfloat16_t>>, Is_dropout=false, Is_causal=true, Is_even_N=false, Is_even_K=false, Return_softmax=false, Params=Flash_fwd_params]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(15): here
            instantiation of "void flash_fwd_kernel<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax>(Flash_fwd_params) [with Kernel_traits=Flash_fwd_kernel_traits<96, 64, 64, 4, false, false, cutlass::bfloat16_t, Flash_kernel_traits<96, 64, 64, 4, cutlass::bfloat16_t>>, Is_dropout=false, Is_causal=true, Is_even_N=false, Is_even_K=false, Return_softmax=false]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(34): here
            instantiation of "void run_flash_fwd<Kernel_traits,Is_dropout,Is_causal>(Flash_fwd_params &, cudaStream_t) [with Kernel_traits=Flash_fwd_kernel_traits<96, 64, 64, 4, false, false, cutlass::bfloat16_t, Flash_kernel_traits<96, 64, 64, 4, cutlass::bfloat16_t>>, Is_dropout=false, Is_causal=true]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(92): here
            instantiation of "void run_mha_fwd_hdim96<T>(Flash_fwd_params &, cudaStream_t) [with T=cutlass::bfloat16_t]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_hdim96_bf16_sm80.cu(16): here

/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_kernel.h(275): warning: variable "tQrQ" was declared but never referenced
          detected during:
            instantiation of "void flash::compute_attn_1rowblock<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &, int, int, int) [with Kernel_traits=Flash_fwd_kernel_traits<96, 64, 64, 4, false, false, cutlass::bfloat16_t, Flash_kernel_traits<96, 64, 64, 4, cutlass::bfloat16_t>>, Is_dropout=false, Is_causal=true, Is_even_N=false, Is_even_K=false, Return_softmax=false, Params=Flash_fwd_params]" 
(588): here
            instantiation of "void flash::compute_attn<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &) [with Kernel_traits=Flash_fwd_kernel_traits<96, 64, 64, 4, false, false, cutlass::bfloat16_t, Flash_kernel_traits<96, 64, 64, 4, cutlass::bfloat16_t>>, Is_dropout=false, Is_causal=true, Is_even_N=false, Is_even_K=false, Return_softmax=false, Params=Flash_fwd_params]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(15): here
            instantiation of "void flash_fwd_kernel<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax>(Flash_fwd_params) [with Kernel_traits=Flash_fwd_kernel_traits<96, 64, 64, 4, false, false, cutlass::bfloat16_t, Flash_kernel_traits<96, 64, 64, 4, cutlass::bfloat16_t>>, Is_dropout=false, Is_causal=true, Is_even_N=false, Is_even_K=false, Return_softmax=false]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(34): here
            instantiation of "void run_flash_fwd<Kernel_traits,Is_dropout,Is_causal>(Flash_fwd_params &, cudaStream_t) [with Kernel_traits=Flash_fwd_kernel_traits<96, 64, 64, 4, false, false, cutlass::bfloat16_t, Flash_kernel_traits<96, 64, 64, 4, cutlass::bfloat16_t>>, Is_dropout=false, Is_causal=true]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(92): here
            instantiation of "void run_mha_fwd_hdim96<T>(Flash_fwd_params &, cudaStream_t) [with T=cutlass::bfloat16_t]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_hdim96_bf16_sm80.cu(16): here

/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_kernel.h(140): warning: variable "MMA_M" was declared but never referenced
          detected during:
            instantiation of "void flash::compute_attn_1rowblock<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &, int, int, int) [with Kernel_traits=Flash_fwd_kernel_traits<96, 128, 64, 4, false, false, cutlass::bfloat16_t, Flash_kernel_traits<96, 128, 64, 4, cutlass::bfloat16_t>>, Is_dropout=false, Is_causal=true, Is_even_N=true, Is_even_K=true, Return_softmax=false, Params=Flash_fwd_params]" 
(588): here
            instantiation of "void flash::compute_attn<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &) [with Kernel_traits=Flash_fwd_kernel_traits<96, 128, 64, 4, false, false, cutlass::bfloat16_t, Flash_kernel_traits<96, 128, 64, 4, cutlass::bfloat16_t>>, Is_dropout=false, Is_causal=true, Is_even_N=true, Is_even_K=true, Return_softmax=false, Params=Flash_fwd_params]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(15): here
            instantiation of "void flash_fwd_kernel<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax>(Flash_fwd_params) [with Kernel_traits=Flash_fwd_kernel_traits<96, 128, 64, 4, false, false, cutlass::bfloat16_t, Flash_kernel_traits<96, 128, 64, 4, cutlass::bfloat16_t>>, Is_dropout=false, Is_causal=true, Is_even_N=true, Is_even_K=true, Return_softmax=false]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(34): here
            instantiation of "void run_flash_fwd<Kernel_traits,Is_dropout,Is_causal>(Flash_fwd_params &, cudaStream_t) [with Kernel_traits=Flash_fwd_kernel_traits<96, 128, 64, 4, false, false, cutlass::bfloat16_t, Flash_kernel_traits<96, 128, 64, 4, cutlass::bfloat16_t>>, Is_dropout=false, Is_causal=true]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(92): here
            instantiation of "void run_mha_fwd_hdim96<T>(Flash_fwd_params &, cudaStream_t) [with T=cutlass::bfloat16_t]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_hdim96_bf16_sm80.cu(16): here

/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_kernel.h(275): warning: variable "tQrQ" was declared but never referenced
          detected during:
            instantiation of "void flash::compute_attn_1rowblock<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &, int, int, int) [with Kernel_traits=Flash_fwd_kernel_traits<96, 128, 64, 4, false, false, cutlass::bfloat16_t, Flash_kernel_traits<96, 128, 64, 4, cutlass::bfloat16_t>>, Is_dropout=false, Is_causal=true, Is_even_N=true, Is_even_K=true, Return_softmax=false, Params=Flash_fwd_params]" 
(588): here
            instantiation of "void flash::compute_attn<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &) [with Kernel_traits=Flash_fwd_kernel_traits<96, 128, 64, 4, false, false, cutlass::bfloat16_t, Flash_kernel_traits<96, 128, 64, 4, cutlass::bfloat16_t>>, Is_dropout=false, Is_causal=true, Is_even_N=true, Is_even_K=true, Return_softmax=false, Params=Flash_fwd_params]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(15): here
            instantiation of "void flash_fwd_kernel<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax>(Flash_fwd_params) [with Kernel_traits=Flash_fwd_kernel_traits<96, 128, 64, 4, false, false, cutlass::bfloat16_t, Flash_kernel_traits<96, 128, 64, 4, cutlass::bfloat16_t>>, Is_dropout=false, Is_causal=true, Is_even_N=true, Is_even_K=true, Return_softmax=false]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(34): here
            instantiation of "void run_flash_fwd<Kernel_traits,Is_dropout,Is_causal>(Flash_fwd_params &, cudaStream_t) [with Kernel_traits=Flash_fwd_kernel_traits<96, 128, 64, 4, false, false, cutlass::bfloat16_t, Flash_kernel_traits<96, 128, 64, 4, cutlass::bfloat16_t>>, Is_dropout=false, Is_causal=true]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(92): here
            instantiation of "void run_mha_fwd_hdim96<T>(Flash_fwd_params &, cudaStream_t) [with T=cutlass::bfloat16_t]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_hdim96_bf16_sm80.cu(16): here

/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_kernel.h(140): warning: variable "MMA_M" was declared but never referenced
          detected during:
            instantiation of "void flash::compute_attn_1rowblock<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &, int, int, int) [with Kernel_traits=Flash_fwd_kernel_traits<96, 128, 64, 4, false, false, cutlass::bfloat16_t, Flash_kernel_traits<96, 128, 64, 4, cutlass::bfloat16_t>>, Is_dropout=false, Is_causal=true, Is_even_N=true, Is_even_K=false, Return_softmax=false, Params=Flash_fwd_params]" 
(588): here
            instantiation of "void flash::compute_attn<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &) [with Kernel_traits=Flash_fwd_kernel_traits<96, 128, 64, 4, false, false, cutlass::bfloat16_t, Flash_kernel_traits<96, 128, 64, 4, cutlass::bfloat16_t>>, Is_dropout=false, Is_causal=true, Is_even_N=true, Is_even_K=false, Return_softmax=false, Params=Flash_fwd_params]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(15): here
            instantiation of "void flash_fwd_kernel<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax>(Flash_fwd_params) [with Kernel_traits=Flash_fwd_kernel_traits<96, 128, 64, 4, false, false, cutlass::bfloat16_t, Flash_kernel_traits<96, 128, 64, 4, cutlass::bfloat16_t>>, Is_dropout=false, Is_causal=true, Is_even_N=true, Is_even_K=false, Return_softmax=false]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(34): here
            instantiation of "void run_flash_fwd<Kernel_traits,Is_dropout,Is_causal>(Flash_fwd_params &, cudaStream_t) [with Kernel_traits=Flash_fwd_kernel_traits<96, 128, 64, 4, false, false, cutlass::bfloat16_t, Flash_kernel_traits<96, 128, 64, 4, cutlass::bfloat16_t>>, Is_dropout=false, Is_causal=true]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(92): here
            instantiation of "void run_mha_fwd_hdim96<T>(Flash_fwd_params &, cudaStream_t) [with T=cutlass::bfloat16_t]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_hdim96_bf16_sm80.cu(16): here

/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_kernel.h(275): warning: variable "tQrQ" was declared but never referenced
          detected during:
            instantiation of "void flash::compute_attn_1rowblock<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &, int, int, int) [with Kernel_traits=Flash_fwd_kernel_traits<96, 128, 64, 4, false, false, cutlass::bfloat16_t, Flash_kernel_traits<96, 128, 64, 4, cutlass::bfloat16_t>>, Is_dropout=false, Is_causal=true, Is_even_N=true, Is_even_K=false, Return_softmax=false, Params=Flash_fwd_params]" 
(588): here
            instantiation of "void flash::compute_attn<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &) [with Kernel_traits=Flash_fwd_kernel_traits<96, 128, 64, 4, false, false, cutlass::bfloat16_t, Flash_kernel_traits<96, 128, 64, 4, cutlass::bfloat16_t>>, Is_dropout=false, Is_causal=true, Is_even_N=true, Is_even_K=false, Return_softmax=false, Params=Flash_fwd_params]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(15): here
            instantiation of "void flash_fwd_kernel<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax>(Flash_fwd_params) [with Kernel_traits=Flash_fwd_kernel_traits<96, 128, 64, 4, false, false, cutlass::bfloat16_t, Flash_kernel_traits<96, 128, 64, 4, cutlass::bfloat16_t>>, Is_dropout=false, Is_causal=true, Is_even_N=true, Is_even_K=false, Return_softmax=false]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(34): here
            instantiation of "void run_flash_fwd<Kernel_traits,Is_dropout,Is_causal>(Flash_fwd_params &, cudaStream_t) [with Kernel_traits=Flash_fwd_kernel_traits<96, 128, 64, 4, false, false, cutlass::bfloat16_t, Flash_kernel_traits<96, 128, 64, 4, cutlass::bfloat16_t>>, Is_dropout=false, Is_causal=true]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(92): here
            instantiation of "void run_mha_fwd_hdim96<T>(Flash_fwd_params &, cudaStream_t) [with T=cutlass::bfloat16_t]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_hdim96_bf16_sm80.cu(16): here

/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_kernel.h(140): warning: variable "MMA_M" was declared but never referenced
          detected during:
            instantiation of "void flash::compute_attn_1rowblock<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &, int, int, int) [with Kernel_traits=Flash_fwd_kernel_traits<96, 128, 64, 4, false, false, cutlass::bfloat16_t, Flash_kernel_traits<96, 128, 64, 4, cutlass::bfloat16_t>>, Is_dropout=false, Is_causal=true, Is_even_N=false, Is_even_K=true, Return_softmax=false, Params=Flash_fwd_params]" 
(588): here
            instantiation of "void flash::compute_attn<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &) [with Kernel_traits=Flash_fwd_kernel_traits<96, 128, 64, 4, false, false, cutlass::bfloat16_t, Flash_kernel_traits<96, 128, 64, 4, cutlass::bfloat16_t>>, Is_dropout=false, Is_causal=true, Is_even_N=false, Is_even_K=true, Return_softmax=false, Params=Flash_fwd_params]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(15): here
            instantiation of "void flash_fwd_kernel<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax>(Flash_fwd_params) [with Kernel_traits=Flash_fwd_kernel_traits<96, 128, 64, 4, false, false, cutlass::bfloat16_t, Flash_kernel_traits<96, 128, 64, 4, cutlass::bfloat16_t>>, Is_dropout=false, Is_causal=true, Is_even_N=false, Is_even_K=true, Return_softmax=false]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(34): here
            instantiation of "void run_flash_fwd<Kernel_traits,Is_dropout,Is_causal>(Flash_fwd_params &, cudaStream_t) [with Kernel_traits=Flash_fwd_kernel_traits<96, 128, 64, 4, false, false, cutlass::bfloat16_t, Flash_kernel_traits<96, 128, 64, 4, cutlass::bfloat16_t>>, Is_dropout=false, Is_causal=true]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(92): here
            instantiation of "void run_mha_fwd_hdim96<T>(Flash_fwd_params &, cudaStream_t) [with T=cutlass::bfloat16_t]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_hdim96_bf16_sm80.cu(16): here

/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_kernel.h(275): warning: variable "tQrQ" was declared but never referenced
          detected during:
            instantiation of "void flash::compute_attn_1rowblock<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &, int, int, int) [with Kernel_traits=Flash_fwd_kernel_traits<96, 128, 64, 4, false, false, cutlass::bfloat16_t, Flash_kernel_traits<96, 128, 64, 4, cutlass::bfloat16_t>>, Is_dropout=false, Is_causal=true, Is_even_N=false, Is_even_K=true, Return_softmax=false, Params=Flash_fwd_params]" 
(588): here
            instantiation of "void flash::compute_attn<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &) [with Kernel_traits=Flash_fwd_kernel_traits<96, 128, 64, 4, false, false, cutlass::bfloat16_t, Flash_kernel_traits<96, 128, 64, 4, cutlass::bfloat16_t>>, Is_dropout=false, Is_causal=true, Is_even_N=false, Is_even_K=true, Return_softmax=false, Params=Flash_fwd_params]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(15): here
            instantiation of "void flash_fwd_kernel<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax>(Flash_fwd_params) [with Kernel_traits=Flash_fwd_kernel_traits<96, 128, 64, 4, false, false, cutlass::bfloat16_t, Flash_kernel_traits<96, 128, 64, 4, cutlass::bfloat16_t>>, Is_dropout=false, Is_causal=true, Is_even_N=false, Is_even_K=true, Return_softmax=false]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(34): here
            instantiation of "void run_flash_fwd<Kernel_traits,Is_dropout,Is_causal>(Flash_fwd_params &, cudaStream_t) [with Kernel_traits=Flash_fwd_kernel_traits<96, 128, 64, 4, false, false, cutlass::bfloat16_t, Flash_kernel_traits<96, 128, 64, 4, cutlass::bfloat16_t>>, Is_dropout=false, Is_causal=true]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(92): here
            instantiation of "void run_mha_fwd_hdim96<T>(Flash_fwd_params &, cudaStream_t) [with T=cutlass::bfloat16_t]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_hdim96_bf16_sm80.cu(16): here

/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_kernel.h(140): warning: variable "MMA_M" was declared but never referenced
          detected during:
            instantiation of "void flash::compute_attn_1rowblock<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &, int, int, int) [with Kernel_traits=Flash_fwd_kernel_traits<96, 128, 64, 4, false, false, cutlass::bfloat16_t, Flash_kernel_traits<96, 128, 64, 4, cutlass::bfloat16_t>>, Is_dropout=false, Is_causal=true, Is_even_N=false, Is_even_K=false, Return_softmax=false, Params=Flash_fwd_params]" 
(588): here
            instantiation of "void flash::compute_attn<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &) [with Kernel_traits=Flash_fwd_kernel_traits<96, 128, 64, 4, false, false, cutlass::bfloat16_t, Flash_kernel_traits<96, 128, 64, 4, cutlass::bfloat16_t>>, Is_dropout=false, Is_causal=true, Is_even_N=false, Is_even_K=false, Return_softmax=false, Params=Flash_fwd_params]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(15): here
            instantiation of "void flash_fwd_kernel<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax>(Flash_fwd_params) [with Kernel_traits=Flash_fwd_kernel_traits<96, 128, 64, 4, false, false, cutlass::bfloat16_t, Flash_kernel_traits<96, 128, 64, 4, cutlass::bfloat16_t>>, Is_dropout=false, Is_causal=true, Is_even_N=false, Is_even_K=false, Return_softmax=false]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(34): here
            instantiation of "void run_flash_fwd<Kernel_traits,Is_dropout,Is_causal>(Flash_fwd_params &, cudaStream_t) [with Kernel_traits=Flash_fwd_kernel_traits<96, 128, 64, 4, false, false, cutlass::bfloat16_t, Flash_kernel_traits<96, 128, 64, 4, cutlass::bfloat16_t>>, Is_dropout=false, Is_causal=true]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(92): here
            instantiation of "void run_mha_fwd_hdim96<T>(Flash_fwd_params &, cudaStream_t) [with T=cutlass::bfloat16_t]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_hdim96_bf16_sm80.cu(16): here

/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_kernel.h(275): warning: variable "tQrQ" was declared but never referenced
          detected during:
            instantiation of "void flash::compute_attn_1rowblock<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &, int, int, int) [with Kernel_traits=Flash_fwd_kernel_traits<96, 128, 64, 4, false, false, cutlass::bfloat16_t, Flash_kernel_traits<96, 128, 64, 4, cutlass::bfloat16_t>>, Is_dropout=false, Is_causal=true, Is_even_N=false, Is_even_K=false, Return_softmax=false, Params=Flash_fwd_params]" 
(588): here
            instantiation of "void flash::compute_attn<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &) [with Kernel_traits=Flash_fwd_kernel_traits<96, 128, 64, 4, false, false, cutlass::bfloat16_t, Flash_kernel_traits<96, 128, 64, 4, cutlass::bfloat16_t>>, Is_dropout=false, Is_causal=true, Is_even_N=false, Is_even_K=false, Return_softmax=false, Params=Flash_fwd_params]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(15): here
            instantiation of "void flash_fwd_kernel<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax>(Flash_fwd_params) [with Kernel_traits=Flash_fwd_kernel_traits<96, 128, 64, 4, false, false, cutlass::bfloat16_t, Flash_kernel_traits<96, 128, 64, 4, cutlass::bfloat16_t>>, Is_dropout=false, Is_causal=true, Is_even_N=false, Is_even_K=false, Return_softmax=false]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(34): here
            instantiation of "void run_flash_fwd<Kernel_traits,Is_dropout,Is_causal>(Flash_fwd_params &, cudaStream_t) [with Kernel_traits=Flash_fwd_kernel_traits<96, 128, 64, 4, false, false, cutlass::bfloat16_t, Flash_kernel_traits<96, 128, 64, 4, cutlass::bfloat16_t>>, Is_dropout=false, Is_causal=true]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(92): here
            instantiation of "void run_mha_fwd_hdim96<T>(Flash_fwd_params &, cudaStream_t) [with T=cutlass::bfloat16_t]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_hdim96_bf16_sm80.cu(16): here

/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_kernel.h(140): warning: variable "MMA_M" was declared but never referenced
          detected during:
            instantiation of "void flash::compute_attn_1rowblock<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &, int, int, int) [with Kernel_traits=Flash_fwd_kernel_traits<96, 128, 64, 4, false, false, cutlass::bfloat16_t, Flash_kernel_traits<96, 128, 64, 4, cutlass::bfloat16_t>>, Is_dropout=false, Is_causal=false, Is_even_N=true, Is_even_K=true, Return_softmax=false, Params=Flash_fwd_params]" 
(588): here
            instantiation of "void flash::compute_attn<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &) [with Kernel_traits=Flash_fwd_kernel_traits<96, 128, 64, 4, false, false, cutlass::bfloat16_t, Flash_kernel_traits<96, 128, 64, 4, cutlass::bfloat16_t>>, Is_dropout=false, Is_causal=false, Is_even_N=true, Is_even_K=true, Return_softmax=false, Params=Flash_fwd_params]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(15): here
            instantiation of "void flash_fwd_kernel<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax>(Flash_fwd_params) [with Kernel_traits=Flash_fwd_kernel_traits<96, 128, 64, 4, false, false, cutlass::bfloat16_t, Flash_kernel_traits<96, 128, 64, 4, cutlass::bfloat16_t>>, Is_dropout=false, Is_causal=false, Is_even_N=true, Is_even_K=true, Return_softmax=false]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(34): here
            instantiation of "void run_flash_fwd<Kernel_traits,Is_dropout,Is_causal>(Flash_fwd_params &, cudaStream_t) [with Kernel_traits=Flash_fwd_kernel_traits<96, 128, 64, 4, false, false, cutlass::bfloat16_t, Flash_kernel_traits<96, 128, 64, 4, cutlass::bfloat16_t>>, Is_dropout=false, Is_causal=false]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(92): here
            instantiation of "void run_mha_fwd_hdim96<T>(Flash_fwd_params &, cudaStream_t) [with T=cutlass::bfloat16_t]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_hdim96_bf16_sm80.cu(16): here

/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_kernel.h(275): warning: variable "tQrQ" was declared but never referenced
          detected during:
            instantiation of "void flash::compute_attn_1rowblock<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &, int, int, int) [with Kernel_traits=Flash_fwd_kernel_traits<96, 128, 64, 4, false, false, cutlass::bfloat16_t, Flash_kernel_traits<96, 128, 64, 4, cutlass::bfloat16_t>>, Is_dropout=false, Is_causal=false, Is_even_N=true, Is_even_K=true, Return_softmax=false, Params=Flash_fwd_params]" 
(588): here
            instantiation of "void flash::compute_attn<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &) [with Kernel_traits=Flash_fwd_kernel_traits<96, 128, 64, 4, false, false, cutlass::bfloat16_t, Flash_kernel_traits<96, 128, 64, 4, cutlass::bfloat16_t>>, Is_dropout=false, Is_causal=false, Is_even_N=true, Is_even_K=true, Return_softmax=false, Params=Flash_fwd_params]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(15): here
            instantiation of "void flash_fwd_kernel<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax>(Flash_fwd_params) [with Kernel_traits=Flash_fwd_kernel_traits<96, 128, 64, 4, false, false, cutlass::bfloat16_t, Flash_kernel_traits<96, 128, 64, 4, cutlass::bfloat16_t>>, Is_dropout=false, Is_causal=false, Is_even_N=true, Is_even_K=true, Return_softmax=false]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(34): here
            instantiation of "void run_flash_fwd<Kernel_traits,Is_dropout,Is_causal>(Flash_fwd_params &, cudaStream_t) [with Kernel_traits=Flash_fwd_kernel_traits<96, 128, 64, 4, false, false, cutlass::bfloat16_t, Flash_kernel_traits<96, 128, 64, 4, cutlass::bfloat16_t>>, Is_dropout=false, Is_causal=false]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(92): here
            instantiation of "void run_mha_fwd_hdim96<T>(Flash_fwd_params &, cudaStream_t) [with T=cutlass::bfloat16_t]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_hdim96_bf16_sm80.cu(16): here

/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_kernel.h(140): warning: variable "MMA_M" was declared but never referenced
          detected during:
            instantiation of "void flash::compute_attn_1rowblock<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &, int, int, int) [with Kernel_traits=Flash_fwd_kernel_traits<96, 128, 64, 4, false, false, cutlass::bfloat16_t, Flash_kernel_traits<96, 128, 64, 4, cutlass::bfloat16_t>>, Is_dropout=false, Is_causal=false, Is_even_N=true, Is_even_K=false, Return_softmax=false, Params=Flash_fwd_params]" 
(588): here
            instantiation of "void flash::compute_attn<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &) [with Kernel_traits=Flash_fwd_kernel_traits<96, 128, 64, 4, false, false, cutlass::bfloat16_t, Flash_kernel_traits<96, 128, 64, 4, cutlass::bfloat16_t>>, Is_dropout=false, Is_causal=false, Is_even_N=true, Is_even_K=false, Return_softmax=false, Params=Flash_fwd_params]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(15): here
            instantiation of "void flash_fwd_kernel<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax>(Flash_fwd_params) [with Kernel_traits=Flash_fwd_kernel_traits<96, 128, 64, 4, false, false, cutlass::bfloat16_t, Flash_kernel_traits<96, 128, 64, 4, cutlass::bfloat16_t>>, Is_dropout=false, Is_causal=false, Is_even_N=true, Is_even_K=false, Return_softmax=false]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(34): here
            instantiation of "void run_flash_fwd<Kernel_traits,Is_dropout,Is_causal>(Flash_fwd_params &, cudaStream_t) [with Kernel_traits=Flash_fwd_kernel_traits<96, 128, 64, 4, false, false, cutlass::bfloat16_t, Flash_kernel_traits<96, 128, 64, 4, cutlass::bfloat16_t>>, Is_dropout=false, Is_causal=false]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(92): here
            instantiation of "void run_mha_fwd_hdim96<T>(Flash_fwd_params &, cudaStream_t) [with T=cutlass::bfloat16_t]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_hdim96_bf16_sm80.cu(16): here

/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_kernel.h(275): warning: variable "tQrQ" was declared but never referenced
          detected during:
            instantiation of "void flash::compute_attn_1rowblock<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &, int, int, int) [with Kernel_traits=Flash_fwd_kernel_traits<96, 128, 64, 4, false, false, cutlass::bfloat16_t, Flash_kernel_traits<96, 128, 64, 4, cutlass::bfloat16_t>>, Is_dropout=false, Is_causal=false, Is_even_N=true, Is_even_K=false, Return_softmax=false, Params=Flash_fwd_params]" 
(588): here
            instantiation of "void flash::compute_attn<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &) [with Kernel_traits=Flash_fwd_kernel_traits<96, 128, 64, 4, false, false, cutlass::bfloat16_t, Flash_kernel_traits<96, 128, 64, 4, cutlass::bfloat16_t>>, Is_dropout=false, Is_causal=false, Is_even_N=true, Is_even_K=false, Return_softmax=false, Params=Flash_fwd_params]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(15): here
            instantiation of "void flash_fwd_kernel<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax>(Flash_fwd_params) [with Kernel_traits=Flash_fwd_kernel_traits<96, 128, 64, 4, false, false, cutlass::bfloat16_t, Flash_kernel_traits<96, 128, 64, 4, cutlass::bfloat16_t>>, Is_dropout=false, Is_causal=false, Is_even_N=true, Is_even_K=false, Return_softmax=false]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(34): here
            instantiation of "void run_flash_fwd<Kernel_traits,Is_dropout,Is_causal>(Flash_fwd_params &, cudaStream_t) [with Kernel_traits=Flash_fwd_kernel_traits<96, 128, 64, 4, false, false, cutlass::bfloat16_t, Flash_kernel_traits<96, 128, 64, 4, cutlass::bfloat16_t>>, Is_dropout=false, Is_causal=false]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(92): here
            instantiation of "void run_mha_fwd_hdim96<T>(Flash_fwd_params &, cudaStream_t) [with T=cutlass::bfloat16_t]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_hdim96_bf16_sm80.cu(16): here

/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_kernel.h(140): warning: variable "MMA_M" was declared but never referenced
          detected during:
            instantiation of "void flash::compute_attn_1rowblock<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &, int, int, int) [with Kernel_traits=Flash_fwd_kernel_traits<96, 128, 64, 4, false, false, cutlass::bfloat16_t, Flash_kernel_traits<96, 128, 64, 4, cutlass::bfloat16_t>>, Is_dropout=false, Is_causal=false, Is_even_N=false, Is_even_K=true, Return_softmax=false, Params=Flash_fwd_params]" 
(588): here
            instantiation of "void flash::compute_attn<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &) [with Kernel_traits=Flash_fwd_kernel_traits<96, 128, 64, 4, false, false, cutlass::bfloat16_t, Flash_kernel_traits<96, 128, 64, 4, cutlass::bfloat16_t>>, Is_dropout=false, Is_causal=false, Is_even_N=false, Is_even_K=true, Return_softmax=false, Params=Flash_fwd_params]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(15): here
            instantiation of "void flash_fwd_kernel<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax>(Flash_fwd_params) [with Kernel_traits=Flash_fwd_kernel_traits<96, 128, 64, 4, false, false, cutlass::bfloat16_t, Flash_kernel_traits<96, 128, 64, 4, cutlass::bfloat16_t>>, Is_dropout=false, Is_causal=false, Is_even_N=false, Is_even_K=true, Return_softmax=false]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(34): here
            instantiation of "void run_flash_fwd<Kernel_traits,Is_dropout,Is_causal>(Flash_fwd_params &, cudaStream_t) [with Kernel_traits=Flash_fwd_kernel_traits<96, 128, 64, 4, false, false, cutlass::bfloat16_t, Flash_kernel_traits<96, 128, 64, 4, cutlass::bfloat16_t>>, Is_dropout=false, Is_causal=false]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(92): here
            instantiation of "void run_mha_fwd_hdim96<T>(Flash_fwd_params &, cudaStream_t) [with T=cutlass::bfloat16_t]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_hdim96_bf16_sm80.cu(16): here

/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_kernel.h(275): warning: variable "tQrQ" was declared but never referenced
          detected during:
            instantiation of "void flash::compute_attn_1rowblock<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &, int, int, int) [with Kernel_traits=Flash_fwd_kernel_traits<96, 128, 64, 4, false, false, cutlass::bfloat16_t, Flash_kernel_traits<96, 128, 64, 4, cutlass::bfloat16_t>>, Is_dropout=false, Is_causal=false, Is_even_N=false, Is_even_K=true, Return_softmax=false, Params=Flash_fwd_params]" 
(588): here
            instantiation of "void flash::compute_attn<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &) [with Kernel_traits=Flash_fwd_kernel_traits<96, 128, 64, 4, false, false, cutlass::bfloat16_t, Flash_kernel_traits<96, 128, 64, 4, cutlass::bfloat16_t>>, Is_dropout=false, Is_causal=false, Is_even_N=false, Is_even_K=true, Return_softmax=false, Params=Flash_fwd_params]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(15): here
            instantiation of "void flash_fwd_kernel<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax>(Flash_fwd_params) [with Kernel_traits=Flash_fwd_kernel_traits<96, 128, 64, 4, false, false, cutlass::bfloat16_t, Flash_kernel_traits<96, 128, 64, 4, cutlass::bfloat16_t>>, Is_dropout=false, Is_causal=false, Is_even_N=false, Is_even_K=true, Return_softmax=false]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(34): here
            instantiation of "void run_flash_fwd<Kernel_traits,Is_dropout,Is_causal>(Flash_fwd_params &, cudaStream_t) [with Kernel_traits=Flash_fwd_kernel_traits<96, 128, 64, 4, false, false, cutlass::bfloat16_t, Flash_kernel_traits<96, 128, 64, 4, cutlass::bfloat16_t>>, Is_dropout=false, Is_causal=false]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(92): here
            instantiation of "void run_mha_fwd_hdim96<T>(Flash_fwd_params &, cudaStream_t) [with T=cutlass::bfloat16_t]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_hdim96_bf16_sm80.cu(16): here

/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_kernel.h(140): warning: variable "MMA_M" was declared but never referenced
          detected during:
            instantiation of "void flash::compute_attn_1rowblock<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &, int, int, int) [with Kernel_traits=Flash_fwd_kernel_traits<96, 128, 64, 4, false, false, cutlass::bfloat16_t, Flash_kernel_traits<96, 128, 64, 4, cutlass::bfloat16_t>>, Is_dropout=false, Is_causal=false, Is_even_N=false, Is_even_K=false, Return_softmax=false, Params=Flash_fwd_params]" 
(588): here
            instantiation of "void flash::compute_attn<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &) [with Kernel_traits=Flash_fwd_kernel_traits<96, 128, 64, 4, false, false, cutlass::bfloat16_t, Flash_kernel_traits<96, 128, 64, 4, cutlass::bfloat16_t>>, Is_dropout=false, Is_causal=false, Is_even_N=false, Is_even_K=false, Return_softmax=false, Params=Flash_fwd_params]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(15): here
            instantiation of "void flash_fwd_kernel<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax>(Flash_fwd_params) [with Kernel_traits=Flash_fwd_kernel_traits<96, 128, 64, 4, false, false, cutlass::bfloat16_t, Flash_kernel_traits<96, 128, 64, 4, cutlass::bfloat16_t>>, Is_dropout=false, Is_causal=false, Is_even_N=false, Is_even_K=false, Return_softmax=false]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(34): here
            instantiation of "void run_flash_fwd<Kernel_traits,Is_dropout,Is_causal>(Flash_fwd_params &, cudaStream_t) [with Kernel_traits=Flash_fwd_kernel_traits<96, 128, 64, 4, false, false, cutlass::bfloat16_t, Flash_kernel_traits<96, 128, 64, 4, cutlass::bfloat16_t>>, Is_dropout=false, Is_causal=false]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(92): here
            instantiation of "void run_mha_fwd_hdim96<T>(Flash_fwd_params &, cudaStream_t) [with T=cutlass::bfloat16_t]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_hdim96_bf16_sm80.cu(16): here

/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_kernel.h(275): warning: variable "tQrQ" was declared but never referenced
          detected during:
            instantiation of "void flash::compute_attn_1rowblock<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &, int, int, int) [with Kernel_traits=Flash_fwd_kernel_traits<96, 128, 64, 4, false, false, cutlass::bfloat16_t, Flash_kernel_traits<96, 128, 64, 4, cutlass::bfloat16_t>>, Is_dropout=false, Is_causal=false, Is_even_N=false, Is_even_K=false, Return_softmax=false, Params=Flash_fwd_params]" 
(588): here
            instantiation of "void flash::compute_attn<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &) [with Kernel_traits=Flash_fwd_kernel_traits<96, 128, 64, 4, false, false, cutlass::bfloat16_t, Flash_kernel_traits<96, 128, 64, 4, cutlass::bfloat16_t>>, Is_dropout=false, Is_causal=false, Is_even_N=false, Is_even_K=false, Return_softmax=false, Params=Flash_fwd_params]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(15): here
            instantiation of "void flash_fwd_kernel<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax>(Flash_fwd_params) [with Kernel_traits=Flash_fwd_kernel_traits<96, 128, 64, 4, false, false, cutlass::bfloat16_t, Flash_kernel_traits<96, 128, 64, 4, cutlass::bfloat16_t>>, Is_dropout=false, Is_causal=false, Is_even_N=false, Is_even_K=false, Return_softmax=false]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(34): here
            instantiation of "void run_flash_fwd<Kernel_traits,Is_dropout,Is_causal>(Flash_fwd_params &, cudaStream_t) [with Kernel_traits=Flash_fwd_kernel_traits<96, 128, 64, 4, false, false, cutlass::bfloat16_t, Flash_kernel_traits<96, 128, 64, 4, cutlass::bfloat16_t>>, Is_dropout=false, Is_causal=false]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(92): here
            instantiation of "void run_mha_fwd_hdim96<T>(Flash_fwd_params &, cudaStream_t) [with T=cutlass::bfloat16_t]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_hdim96_bf16_sm80.cu(16): here

ptxas info    : 2 bytes gmem
ptxas info    : Compiling entry function '_Z16flash_fwd_kernelI23Flash_fwd_kernel_traitsILi96ELi128ELi64ELi4ELb0ELb0EN7cutlass10bfloat16_tE19Flash_kernel_traitsILi96ELi128ELi64ELi4ES2_EELb0ELb0ELb0ELb0ELb0EEv16Flash_fwd_params' for 'sm_80'
ptxas info    : Function properties for _Z16flash_fwd_kernelI23Flash_fwd_kernel_traitsILi96ELi128ELi64ELi4ELb0ELb0EN7cutlass10bfloat16_tE19Flash_kernel_traitsILi96ELi128ELi64ELi4ES2_EELb0ELb0ELb0ELb0ELb0EEv16Flash_fwd_params
    0 bytes stack frame, 0 bytes spill stores, 0 bytes spill loads
ptxas info    : Used 240 registers, 584 bytes cmem[0]
ptxas info    : Compiling entry function '_Z16flash_fwd_kernelI23Flash_fwd_kernel_traitsILi96ELi128ELi64ELi4ELb0ELb0EN7cutlass10bfloat16_tE19Flash_kernel_traitsILi96ELi128ELi64ELi4ES2_EELb0ELb0ELb0ELb1ELb0EEv16Flash_fwd_params' for 'sm_80'
ptxas info    : Function properties for _Z16flash_fwd_kernelI23Flash_fwd_kernel_traitsILi96ELi128ELi64ELi4ELb0ELb0EN7cutlass10bfloat16_tE19Flash_kernel_traitsILi96ELi128ELi64ELi4ES2_EELb0ELb0ELb0ELb1ELb0EEv16Flash_fwd_params
    0 bytes stack frame, 0 bytes spill stores, 0 bytes spill loads
ptxas info    : Used 237 registers, 584 bytes cmem[0]
ptxas info    : Compiling entry function '_Z16flash_fwd_kernelI23Flash_fwd_kernel_traitsILi96ELi128ELi64ELi4ELb0ELb0EN7cutlass10bfloat16_tE19Flash_kernel_traitsILi96ELi128ELi64ELi4ES2_EELb0ELb0ELb1ELb0ELb0EEv16Flash_fwd_params' for 'sm_80'
ptxas info    : Function properties for _Z16flash_fwd_kernelI23Flash_fwd_kernel_traitsILi96ELi128ELi64ELi4ELb0ELb0EN7cutlass10bfloat16_tE19Flash_kernel_traitsILi96ELi128ELi64ELi4ES2_EELb0ELb0ELb1ELb0ELb0EEv16Flash_fwd_params
    0 bytes stack frame, 0 bytes spill stores, 0 bytes spill loads
ptxas info    : Used 239 registers, 584 bytes cmem[0]
ptxas info    : Compiling entry function '_Z16flash_fwd_kernelI23Flash_fwd_kernel_traitsILi96ELi128ELi64ELi4ELb0ELb0EN7cutlass10bfloat16_tE19Flash_kernel_traitsILi96ELi128ELi64ELi4ES2_EELb0ELb0ELb1ELb1ELb0EEv16Flash_fwd_params' for 'sm_80'
ptxas info    : Function properties for _Z16flash_fwd_kernelI23Flash_fwd_kernel_traitsILi96ELi128ELi64ELi4ELb0ELb0EN7cutlass10bfloat16_tE19Flash_kernel_traitsILi96ELi128ELi64ELi4ES2_EELb0ELb0ELb1ELb1ELb0EEv16Flash_fwd_params
    0 bytes stack frame, 0 bytes spill stores, 0 bytes spill loads
ptxas info    : Used 237 registers, 584 bytes cmem[0]
ptxas info    : Compiling entry function '_Z16flash_fwd_kernelI23Flash_fwd_kernel_traitsILi96ELi128ELi64ELi4ELb0ELb0EN7cutlass10bfloat16_tE19Flash_kernel_traitsILi96ELi128ELi64ELi4ES2_EELb0ELb1ELb0ELb0ELb0EEv16Flash_fwd_params' for 'sm_80'
ptxas info    : Function properties for _Z16flash_fwd_kernelI23Flash_fwd_kernel_traitsILi96ELi128ELi64ELi4ELb0ELb0EN7cutlass10bfloat16_tE19Flash_kernel_traitsILi96ELi128ELi64ELi4ES2_EELb0ELb1ELb0ELb0ELb0EEv16Flash_fwd_params
    0 bytes stack frame, 0 bytes spill stores, 0 bytes spill loads
ptxas info    : Used 255 registers, 584 bytes cmem[0]
ptxas info    : Compiling entry function '_Z16flash_fwd_kernelI23Flash_fwd_kernel_traitsILi96ELi128ELi64ELi4ELb0ELb0EN7cutlass10bfloat16_tE19Flash_kernel_traitsILi96ELi128ELi64ELi4ES2_EELb0ELb1ELb0ELb1ELb0EEv16Flash_fwd_params' for 'sm_80'
ptxas info    : Function properties for _Z16flash_fwd_kernelI23Flash_fwd_kernel_traitsILi96ELi128ELi64ELi4ELb0ELb0EN7cutlass10bfloat16_tE19Flash_kernel_traitsILi96ELi128ELi64ELi4ES2_EELb0ELb1ELb0ELb1ELb0EEv16Flash_fwd_params
    0 bytes stack frame, 0 bytes spill stores, 0 bytes spill loads
ptxas info    : Used 254 registers, 584 bytes cmem[0]
ptxas info    : Compiling entry function '_Z16flash_fwd_kernelI23Flash_fwd_kernel_traitsILi96ELi128ELi64ELi4ELb0ELb0EN7cutlass10bfloat16_tE19Flash_kernel_traitsILi96ELi128ELi64ELi4ES2_EELb0ELb1ELb1ELb0ELb0EEv16Flash_fwd_params' for 'sm_80'
ptxas info    : Function properties for _Z16flash_fwd_kernelI23Flash_fwd_kernel_traitsILi96ELi128ELi64ELi4ELb0ELb0EN7cutlass10bfloat16_tE19Flash_kernel_traitsILi96ELi128ELi64ELi4ES2_EELb0ELb1ELb1ELb0ELb0EEv16Flash_fwd_params
    0 bytes stack frame, 0 bytes spill stores, 0 bytes spill loads
ptxas info    : Used 254 registers, 584 bytes cmem[0]
ptxas info    : Compiling entry function '_Z16flash_fwd_kernelI23Flash_fwd_kernel_traitsILi96ELi128ELi64ELi4ELb0ELb0EN7cutlass10bfloat16_tE19Flash_kernel_traitsILi96ELi128ELi64ELi4ES2_EELb0ELb1ELb1ELb1ELb0EEv16Flash_fwd_params' for 'sm_80'
ptxas info    : Function properties for _Z16flash_fwd_kernelI23Flash_fwd_kernel_traitsILi96ELi128ELi64ELi4ELb0ELb0EN7cutlass10bfloat16_tE19Flash_kernel_traitsILi96ELi128ELi64ELi4ES2_EELb0ELb1ELb1ELb1ELb0EEv16Flash_fwd_params
    0 bytes stack frame, 0 bytes spill stores, 0 bytes spill loads
ptxas info    : Used 255 registers, 584 bytes cmem[0]
ptxas info    : Compiling entry function '_Z16flash_fwd_kernelI23Flash_fwd_kernel_traitsILi96ELi64ELi64ELi4ELb0ELb0EN7cutlass10bfloat16_tE19Flash_kernel_traitsILi96ELi64ELi64ELi4ES2_EELb0ELb1ELb0ELb0ELb0EEv16Flash_fwd_params' for 'sm_80'
ptxas info    : Function properties for _Z16flash_fwd_kernelI23Flash_fwd_kernel_traitsILi96ELi64ELi64ELi4ELb0ELb0EN7cutlass10bfloat16_tE19Flash_kernel_traitsILi96ELi64ELi64ELi4ES2_EELb0ELb1ELb0ELb0ELb0EEv16Flash_fwd_params
    0 bytes stack frame, 0 bytes spill stores, 0 bytes spill loads
ptxas info    : Used 133 registers, 584 bytes cmem[0]
ptxas info    : Compiling entry function '_Z16flash_fwd_kernelI23Flash_fwd_kernel_traitsILi96ELi64ELi64ELi4ELb0ELb0EN7cutlass10bfloat16_tE19Flash_kernel_traitsILi96ELi64ELi64ELi4ES2_EELb0ELb1ELb0ELb1ELb0EEv16Flash_fwd_params' for 'sm_80'
ptxas info    : Function properties for _Z16flash_fwd_kernelI23Flash_fwd_kernel_traitsILi96ELi64ELi64ELi4ELb0ELb0EN7cutlass10bfloat16_tE19Flash_kernel_traitsILi96ELi64ELi64ELi4ES2_EELb0ELb1ELb0ELb1ELb0EEv16Flash_fwd_params
    0 bytes stack frame, 0 bytes spill stores, 0 bytes spill loads
ptxas info    : Used 130 registers, 584 bytes cmem[0]
ptxas info    : Compiling entry function '_Z16flash_fwd_kernelI23Flash_fwd_kernel_traitsILi96ELi64ELi64ELi4ELb0ELb0EN7cutlass10bfloat16_tE19Flash_kernel_traitsILi96ELi64ELi64ELi4ES2_EELb0ELb1ELb1ELb0ELb0EEv16Flash_fwd_params' for 'sm_80'
ptxas info    : Function properties for _Z16flash_fwd_kernelI23Flash_fwd_kernel_traitsILi96ELi64ELi64ELi4ELb0ELb0EN7cutlass10bfloat16_tE19Flash_kernel_traitsILi96ELi64ELi64ELi4ES2_EELb0ELb1ELb1ELb0ELb0EEv16Flash_fwd_params
    0 bytes stack frame, 0 bytes spill stores, 0 bytes spill loads
ptxas info    : Used 130 registers, 584 bytes cmem[0]
ptxas info    : Compiling entry function '_Z16flash_fwd_kernelI23Flash_fwd_kernel_traitsILi96ELi64ELi64ELi4ELb0ELb0EN7cutlass10bfloat16_tE19Flash_kernel_traitsILi96ELi64ELi64ELi4ES2_EELb0ELb1ELb1ELb1ELb0EEv16Flash_fwd_params' for 'sm_80'
ptxas info    : Function properties for _Z16flash_fwd_kernelI23Flash_fwd_kernel_traitsILi96ELi64ELi64ELi4ELb0ELb0EN7cutlass10bfloat16_tE19Flash_kernel_traitsILi96ELi64ELi64ELi4ES2_EELb0ELb1ELb1ELb1ELb0EEv16Flash_fwd_params
    0 bytes stack frame, 0 bytes spill stores, 0 bytes spill loads
ptxas info    : Used 133 registers, 584 bytes cmem[0]
ptxas info    : Compiling entry function '_Z16flash_fwd_kernelI23Flash_fwd_kernel_traitsILi96ELi128ELi64ELi4ELb0ELb0EN7cutlass10bfloat16_tE19Flash_kernel_traitsILi96ELi128ELi64ELi4ES2_EELb1ELb0ELb0ELb0ELb0EEv16Flash_fwd_params' for 'sm_80'
ptxas info    : Function properties for _Z16flash_fwd_kernelI23Flash_fwd_kernel_traitsILi96ELi128ELi64ELi4ELb0ELb0EN7cutlass10bfloat16_tE19Flash_kernel_traitsILi96ELi128ELi64ELi4ES2_EELb1ELb0ELb0ELb0ELb0EEv16Flash_fwd_params
    16 bytes stack frame, 16 bytes spill stores, 16 bytes spill loads
ptxas info    : Used 255 registers, 584 bytes cmem[0]
ptxas info    : Compiling entry function '_Z16flash_fwd_kernelI23Flash_fwd_kernel_traitsILi96ELi128ELi64ELi4ELb0ELb0EN7cutlass10bfloat16_tE19Flash_kernel_traitsILi96ELi128ELi64ELi4ES2_EELb1ELb0ELb0ELb0ELb1EEv16Flash_fwd_params' for 'sm_80'
ptxas info    : Function properties for _Z16flash_fwd_kernelI23Flash_fwd_kernel_traitsILi96ELi128ELi64ELi4ELb0ELb0EN7cutlass10bfloat16_tE19Flash_kernel_traitsILi96ELi128ELi64ELi4ES2_EELb1ELb0ELb0ELb0ELb1EEv16Flash_fwd_params
    40 bytes stack frame, 36 bytes spill stores, 36 bytes spill loads
ptxas info    : Used 255 registers, 584 bytes cmem[0]
ptxas info    : Compiling entry function '_Z16flash_fwd_kernelI23Flash_fwd_kernel_traitsILi96ELi128ELi64ELi4ELb0ELb0EN7cutlass10bfloat16_tE19Flash_kernel_traitsILi96ELi128ELi64ELi4ES2_EELb1ELb0ELb0ELb1ELb0EEv16Flash_fwd_params' for 'sm_80'
ptxas info    : Function properties for _Z16flash_fwd_kernelI23Flash_fwd_kernel_traitsILi96ELi128ELi64ELi4ELb0ELb0EN7cutlass10bfloat16_tE19Flash_kernel_traitsILi96ELi128ELi64ELi4ES2_EELb1ELb0ELb0ELb1ELb0EEv16Flash_fwd_params
    0 bytes stack frame, 0 bytes spill stores, 0 bytes spill loads
ptxas info    : Used 255 registers, 584 bytes cmem[0]
ptxas info    : Compiling entry function '_Z16flash_fwd_kernelI23Flash_fwd_kernel_traitsILi96ELi128ELi64ELi4ELb0ELb0EN7cutlass10bfloat16_tE19Flash_kernel_traitsILi96ELi128ELi64ELi4ES2_EELb1ELb0ELb0ELb1ELb1EEv16Flash_fwd_params' for 'sm_80'
ptxas info    : Function properties for _Z16flash_fwd_kernelI23Flash_fwd_kernel_traitsILi96ELi128ELi64ELi4ELb0ELb0EN7cutlass10bfloat16_tE19Flash_kernel_traitsILi96ELi128ELi64ELi4ES2_EELb1ELb0ELb0ELb1ELb1EEv16Flash_fwd_params
    40 bytes stack frame, 36 bytes spill stores, 36 bytes spill loads
ptxas info    : Used 255 registers, 584 bytes cmem[0]
ptxas info    : Compiling entry function '_Z16flash_fwd_kernelI23Flash_fwd_kernel_traitsILi96ELi128ELi64ELi4ELb0ELb0EN7cutlass10bfloat16_tE19Flash_kernel_traitsILi96ELi128ELi64ELi4ES2_EELb1ELb0ELb1ELb0ELb0EEv16Flash_fwd_params' for 'sm_80'
ptxas info    : Function properties for _Z16flash_fwd_kernelI23Flash_fwd_kernel_traitsILi96ELi128ELi64ELi4ELb0ELb0EN7cutlass10bfloat16_tE19Flash_kernel_traitsILi96ELi128ELi64ELi4ES2_EELb1ELb0ELb1ELb0ELb0EEv16Flash_fwd_params
    0 bytes stack frame, 0 bytes spill stores, 0 bytes spill loads
ptxas info    : Used 255 registers, 584 bytes cmem[0]
ptxas info    : Compiling entry function '_Z16flash_fwd_kernelI23Flash_fwd_kernel_traitsILi96ELi128ELi64ELi4ELb0ELb0EN7cutlass10bfloat16_tE19Flash_kernel_traitsILi96ELi128ELi64ELi4ES2_EELb1ELb0ELb1ELb0ELb1EEv16Flash_fwd_params' for 'sm_80'
ptxas info    : Function properties for _Z16flash_fwd_kernelI23Flash_fwd_kernel_traitsILi96ELi128ELi64ELi4ELb0ELb0EN7cutlass10bfloat16_tE19Flash_kernel_traitsILi96ELi128ELi64ELi4ES2_EELb1ELb0ELb1ELb0ELb1EEv16Flash_fwd_params
    8 bytes stack frame, 8 bytes spill stores, 8 bytes spill loads
ptxas info    : Used 255 registers, 584 bytes cmem[0]
ptxas info    : Compiling entry function '_Z16flash_fwd_kernelI23Flash_fwd_kernel_traitsILi96ELi128ELi64ELi4ELb0ELb0EN7cutlass10bfloat16_tE19Flash_kernel_traitsILi96ELi128ELi64ELi4ES2_EELb1ELb0ELb1ELb1ELb0EEv16Flash_fwd_params' for 'sm_80'
ptxas info    : Function properties for _Z16flash_fwd_kernelI23Flash_fwd_kernel_traitsILi96ELi128ELi64ELi4ELb0ELb0EN7cutlass10bfloat16_tE19Flash_kernel_traitsILi96ELi128ELi64ELi4ES2_EELb1ELb0ELb1ELb1ELb0EEv16Flash_fwd_params
    0 bytes stack frame, 0 bytes spill stores, 0 bytes spill loads
ptxas info    : Used 255 registers, 584 bytes cmem[0]
ptxas info    : Compiling entry function '_Z16flash_fwd_kernelI23Flash_fwd_kernel_traitsILi96ELi128ELi64ELi4ELb0ELb0EN7cutlass10bfloat16_tE19Flash_kernel_traitsILi96ELi128ELi64ELi4ES2_EELb1ELb0ELb1ELb1ELb1EEv16Flash_fwd_params' for 'sm_80'
ptxas info    : Function properties for _Z16flash_fwd_kernelI23Flash_fwd_kernel_traitsILi96ELi128ELi64ELi4ELb0ELb0EN7cutlass10bfloat16_tE19Flash_kernel_traitsILi96ELi128ELi64ELi4ES2_EELb1ELb0ELb1ELb1ELb1EEv16Flash_fwd_params
    16 bytes stack frame, 24 bytes spill stores, 16 bytes spill loads
ptxas info    : Used 255 registers, 584 bytes cmem[0]
ptxas info    : Compiling entry function '_Z16flash_fwd_kernelI23Flash_fwd_kernel_traitsILi96ELi128ELi64ELi4ELb0ELb0EN7cutlass10bfloat16_tE19Flash_kernel_traitsILi96ELi128ELi64ELi4ES2_EELb1ELb1ELb0ELb0ELb0EEv16Flash_fwd_params' for 'sm_80'
ptxas info    : Function properties for _Z16flash_fwd_kernelI23Flash_fwd_kernel_traitsILi96ELi128ELi64ELi4ELb0ELb0EN7cutlass10bfloat16_tE19Flash_kernel_traitsILi96ELi128ELi64ELi4ES2_EELb1ELb1ELb0ELb0ELb0EEv16Flash_fwd_params
    48 bytes stack frame, 44 bytes spill stores, 44 bytes spill loads
ptxas info    : Used 255 registers, 584 bytes cmem[0]
ptxas info    : Compiling entry function '_Z16flash_fwd_kernelI23Flash_fwd_kernel_traitsILi96ELi128ELi64ELi4ELb0ELb0EN7cutlass10bfloat16_tE19Flash_kernel_traitsILi96ELi128ELi64ELi4ES2_EELb1ELb1ELb0ELb0ELb1EEv16Flash_fwd_params' for 'sm_80'
ptxas info    : Function properties for _Z16flash_fwd_kernelI23Flash_fwd_kernel_traitsILi96ELi128ELi64ELi4ELb0ELb0EN7cutlass10bfloat16_tE19Flash_kernel_traitsILi96ELi128ELi64ELi4ES2_EELb1ELb1ELb0ELb0ELb1EEv16Flash_fwd_params
    120 bytes stack frame, 140 bytes spill stores, 140 bytes spill loads
ptxas info    : Used 255 registers, 584 bytes cmem[0]
ptxas info    : Compiling entry function '_Z16flash_fwd_kernelI23Flash_fwd_kernel_traitsILi96ELi128ELi64ELi4ELb0ELb0EN7cutlass10bfloat16_tE19Flash_kernel_traitsILi96ELi128ELi64ELi4ES2_EELb1ELb1ELb0ELb1ELb0EEv16Flash_fwd_params' for 'sm_80'
ptxas info    : Function properties for _Z16flash_fwd_kernelI23Flash_fwd_kernel_traitsILi96ELi128ELi64ELi4ELb0ELb0EN7cutlass10bfloat16_tE19Flash_kernel_traitsILi96ELi128ELi64ELi4ES2_EELb1ELb1ELb0ELb1ELb0EEv16Flash_fwd_params
    8 bytes stack frame, 8 bytes spill stores, 8 bytes spill loads
ptxas info    : Used 255 registers, 584 bytes cmem[0]
ptxas info    : Compiling entry function '_Z16flash_fwd_kernelI23Flash_fwd_kernel_traitsILi96ELi128ELi64ELi4ELb0ELb0EN7cutlass10bfloat16_tE19Flash_kernel_traitsILi96ELi128ELi64ELi4ES2_EELb1ELb1ELb0ELb1ELb1EEv16Flash_fwd_params' for 'sm_80'
ptxas info    : Function properties for _Z16flash_fwd_kernelI23Flash_fwd_kernel_traitsILi96ELi128ELi64ELi4ELb0ELb0EN7cutlass10bfloat16_tE19Flash_kernel_traitsILi96ELi128ELi64ELi4ES2_EELb1ELb1ELb0ELb1ELb1EEv16Flash_fwd_params
    144 bytes stack frame, 164 bytes spill stores, 168 bytes spill loads
ptxas info    : Used 255 registers, 584 bytes cmem[0]
ptxas info    : Compiling entry function '_Z16flash_fwd_kernelI23Flash_fwd_kernel_traitsILi96ELi128ELi64ELi4ELb0ELb0EN7cutlass10bfloat16_tE19Flash_kernel_traitsILi96ELi128ELi64ELi4ES2_EELb1ELb1ELb1ELb0ELb0EEv16Flash_fwd_params' for 'sm_80'
ptxas info    : Function properties for _Z16flash_fwd_kernelI23Flash_fwd_kernel_traitsILi96ELi128ELi64ELi4ELb0ELb0EN7cutlass10bfloat16_tE19Flash_kernel_traitsILi96ELi128ELi64ELi4ES2_EELb1ELb1ELb1ELb0ELb0EEv16Flash_fwd_params
    40 bytes stack frame, 32 bytes spill stores, 32 bytes spill loads
ptxas info    : Used 255 registers, 584 bytes cmem[0]
ptxas info    : Compiling entry function '_Z16flash_fwd_kernelI23Flash_fwd_kernel_traitsILi96ELi128ELi64ELi4ELb0ELb0EN7cutlass10bfloat16_tE19Flash_kernel_traitsILi96ELi128ELi64ELi4ES2_EELb1ELb1ELb1ELb0ELb1EEv16Flash_fwd_params' for 'sm_80'
ptxas info    : Function properties for _Z16flash_fwd_kernelI23Flash_fwd_kernel_traitsILi96ELi128ELi64ELi4ELb0ELb0EN7cutlass10bfloat16_tE19Flash_kernel_traitsILi96ELi128ELi64ELi4ES2_EELb1ELb1ELb1ELb0ELb1EEv16Flash_fwd_params
    64 bytes stack frame, 96 bytes spill stores, 88 bytes spill loads
ptxas info    : Used 255 registers, 584 bytes cmem[0]
ptxas info    : Compiling entry function '_Z16flash_fwd_kernelI23Flash_fwd_kernel_traitsILi96ELi128ELi64ELi4ELb0ELb0EN7cutlass10bfloat16_tE19Flash_kernel_traitsILi96ELi128ELi64ELi4ES2_EELb1ELb1ELb1ELb1ELb0EEv16Flash_fwd_params' for 'sm_80'
ptxas info    : Function properties for _Z16flash_fwd_kernelI23Flash_fwd_kernel_traitsILi96ELi128ELi64ELi4ELb0ELb0EN7cutlass10bfloat16_tE19Flash_kernel_traitsILi96ELi128ELi64ELi4ES2_EELb1ELb1ELb1ELb1ELb0EEv16Flash_fwd_params
    40 bytes stack frame, 44 bytes spill stores, 40 bytes spill loads
ptxas info    : Used 255 registers, 584 bytes cmem[0]
ptxas info    : Compiling entry function '_Z16flash_fwd_kernelI23Flash_fwd_kernel_traitsILi96ELi128ELi64ELi4ELb0ELb0EN7cutlass10bfloat16_tE19Flash_kernel_traitsILi96ELi128ELi64ELi4ES2_EELb1ELb1ELb1ELb1ELb1EEv16Flash_fwd_params' for 'sm_80'
ptxas info    : Function properties for _Z16flash_fwd_kernelI23Flash_fwd_kernel_traitsILi96ELi128ELi64ELi4ELb0ELb0EN7cutlass10bfloat16_tE19Flash_kernel_traitsILi96ELi128ELi64ELi4ES2_EELb1ELb1ELb1ELb1ELb1EEv16Flash_fwd_params
    24 bytes stack frame, 52 bytes spill stores, 36 bytes spill loads
ptxas info    : Used 255 registers, 584 bytes cmem[0]
ptxas info    : Compiling entry function '_Z16flash_fwd_kernelI23Flash_fwd_kernel_traitsILi96ELi64ELi64ELi4ELb0ELb0EN7cutlass10bfloat16_tE19Flash_kernel_traitsILi96ELi64ELi64ELi4ES2_EELb1ELb1ELb0ELb0ELb0EEv16Flash_fwd_params' for 'sm_80'
ptxas info    : Function properties for _Z16flash_fwd_kernelI23Flash_fwd_kernel_traitsILi96ELi64ELi64ELi4ELb0ELb0EN7cutlass10bfloat16_tE19Flash_kernel_traitsILi96ELi64ELi64ELi4ES2_EELb1ELb1ELb0ELb0ELb0EEv16Flash_fwd_params
    0 bytes stack frame, 0 bytes spill stores, 0 bytes spill loads
ptxas info    : Used 146 registers, 584 bytes cmem[0]
ptxas info    : Compiling entry function '_Z16flash_fwd_kernelI23Flash_fwd_kernel_traitsILi96ELi64ELi64ELi4ELb0ELb0EN7cutlass10bfloat16_tE19Flash_kernel_traitsILi96ELi64ELi64ELi4ES2_EELb1ELb1ELb0ELb0ELb1EEv16Flash_fwd_params' for 'sm_80'
ptxas info    : Function properties for _Z16flash_fwd_kernelI23Flash_fwd_kernel_traitsILi96ELi64ELi64ELi4ELb0ELb0EN7cutlass10bfloat16_tE19Flash_kernel_traitsILi96ELi64ELi64ELi4ES2_EELb1ELb1ELb0ELb0ELb1EEv16Flash_fwd_params
    0 bytes stack frame, 0 bytes spill stores, 0 bytes spill loads
ptxas info    : Used 174 registers, 584 bytes cmem[0]
ptxas info    : Compiling entry function '_Z16flash_fwd_kernelI23Flash_fwd_kernel_traitsILi96ELi64ELi64ELi4ELb0ELb0EN7cutlass10bfloat16_tE19Flash_kernel_traitsILi96ELi64ELi64ELi4ES2_EELb1ELb1ELb0ELb1ELb0EEv16Flash_fwd_params' for 'sm_80'
ptxas info    : Function properties for _Z16flash_fwd_kernelI23Flash_fwd_kernel_traitsILi96ELi64ELi64ELi4ELb0ELb0EN7cutlass10bfloat16_tE19Flash_kernel_traitsILi96ELi64ELi64ELi4ES2_EELb1ELb1ELb0ELb1ELb0EEv16Flash_fwd_params
    0 bytes stack frame, 0 bytes spill stores, 0 bytes spill loads
ptxas info    : Used 140 registers, 584 bytes cmem[0]
ptxas info    : Compiling entry function '_Z16flash_fwd_kernelI23Flash_fwd_kernel_traitsILi96ELi64ELi64ELi4ELb0ELb0EN7cutlass10bfloat16_tE19Flash_kernel_traitsILi96ELi64ELi64ELi4ES2_EELb1ELb1ELb0ELb1ELb1EEv16Flash_fwd_params' for 'sm_80'
ptxas info    : Function properties for _Z16flash_fwd_kernelI23Flash_fwd_kernel_traitsILi96ELi64ELi64ELi4ELb0ELb0EN7cutlass10bfloat16_tE19Flash_kernel_traitsILi96ELi64ELi64ELi4ES2_EELb1ELb1ELb0ELb1ELb1EEv16Flash_fwd_params
    0 bytes stack frame, 0 bytes spill stores, 0 bytes spill loads
ptxas info    : Used 138 registers, 584 bytes cmem[0]
ptxas info    : Compiling entry function '_Z16flash_fwd_kernelI23Flash_fwd_kernel_traitsILi96ELi64ELi64ELi4ELb0ELb0EN7cutlass10bfloat16_tE19Flash_kernel_traitsILi96ELi64ELi64ELi4ES2_EELb1ELb1ELb1ELb0ELb0EEv16Flash_fwd_params' for 'sm_80'
ptxas info    : Function properties for _Z16flash_fwd_kernelI23Flash_fwd_kernel_traitsILi96ELi64ELi64ELi4ELb0ELb0EN7cutlass10bfloat16_tE19Flash_kernel_traitsILi96ELi64ELi64ELi4ES2_EELb1ELb1ELb1ELb0ELb0EEv16Flash_fwd_params
    0 bytes stack frame, 0 bytes spill stores, 0 bytes spill loads
ptxas info    : Used 148 registers, 584 bytes cmem[0]
ptxas info    : Compiling entry function '_Z16flash_fwd_kernelI23Flash_fwd_kernel_traitsILi96ELi64ELi64ELi4ELb0ELb0EN7cutlass10bfloat16_tE19Flash_kernel_traitsILi96ELi64ELi64ELi4ES2_EELb1ELb1ELb1ELb0ELb1EEv16Flash_fwd_params' for 'sm_80'
ptxas info    : Function properties for _Z16flash_fwd_kernelI23Flash_fwd_kernel_traitsILi96ELi64ELi64ELi4ELb0ELb0EN7cutlass10bfloat16_tE19Flash_kernel_traitsILi96ELi64ELi64ELi4ES2_EELb1ELb1ELb1ELb0ELb1EEv16Flash_fwd_params
    0 bytes stack frame, 0 bytes spill stores, 0 bytes spill loads
ptxas info    : Used 134 registers, 584 bytes cmem[0]
ptxas info    : Compiling entry function '_Z16flash_fwd_kernelI23Flash_fwd_kernel_traitsILi96ELi64ELi64ELi4ELb0ELb0EN7cutlass10bfloat16_tE19Flash_kernel_traitsILi96ELi64ELi64ELi4ES2_EELb1ELb1ELb1ELb1ELb0EEv16Flash_fwd_params' for 'sm_80'
ptxas info    : Function properties for _Z16flash_fwd_kernelI23Flash_fwd_kernel_traitsILi96ELi64ELi64ELi4ELb0ELb0EN7cutlass10bfloat16_tE19Flash_kernel_traitsILi96ELi64ELi64ELi4ES2_EELb1ELb1ELb1ELb1ELb0EEv16Flash_fwd_params
    0 bytes stack frame, 0 bytes spill stores, 0 bytes spill loads
ptxas info    : Used 142 registers, 584 bytes cmem[0]
ptxas info    : Compiling entry function '_Z16flash_fwd_kernelI23Flash_fwd_kernel_traitsILi96ELi64ELi64ELi4ELb0ELb0EN7cutlass10bfloat16_tE19Flash_kernel_traitsILi96ELi64ELi64ELi4ES2_EELb1ELb1ELb1ELb1ELb1EEv16Flash_fwd_params' for 'sm_80'
ptxas info    : Function properties for _Z16flash_fwd_kernelI23Flash_fwd_kernel_traitsILi96ELi64ELi64ELi4ELb0ELb0EN7cutlass10bfloat16_tE19Flash_kernel_traitsILi96ELi64ELi64ELi4ES2_EELb1ELb1ELb1ELb1ELb1EEv16Flash_fwd_params
    0 bytes stack frame, 0 bytes spill stores, 0 bytes spill loads
ptxas info    : Used 136 registers, 584 bytes cmem[0]
/anaconda/envs/flashattention/lib/python3.9/site-packages/torch/include/c10/core/SymInt.h(84): warning: integer conversion resulted in a change of sign

[9/13] /usr/local/cuda-11.4/bin/nvcc  -I/home/v-feiychen/flash-attention/csrc/flash_attn -I/home/v-feiychen/flash-attention/csrc/flash_attn/src -I/home/v-feiychen/flash-attention/csrc/cutlass/include -I/anaconda/envs/flashattention/lib/python3.9/site-packages/torch/include -I/anaconda/envs/flashattention/lib/python3.9/site-packages/torch/include/torch/csrc/api/include -I/anaconda/envs/flashattention/lib/python3.9/site-packages/torch/include/TH -I/anaconda/envs/flashattention/lib/python3.9/site-packages/torch/include/THC -I/usr/local/cuda-11.4/include -I/anaconda/envs/flashattention/include/python3.9 -c -c /home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_hdim224_fp16_sm80.cu -o /home/v-feiychen/flash-attention/build/temp.linux-x86_64-cpython-39/csrc/flash_attn/src/flash_fwd_hdim224_fp16_sm80.o -D__CUDA_NO_HALF_OPERATORS__ -D__CUDA_NO_HALF_CONVERSIONS__ -D__CUDA_NO_BFLOAT16_CONVERSIONS__ -D__CUDA_NO_HALF2_OPERATORS__ --expt-relaxed-constexpr --compiler-options ''"'"'-fPIC'"'"'' -O3 -std=c++17 -U__CUDA_NO_HALF_OPERATORS__ -U__CUDA_NO_HALF_CONVERSIONS__ -U__CUDA_NO_HALF2_OPERATORS__ -U__CUDA_NO_BFLOAT16_CONVERSIONS__ --expt-relaxed-constexpr --expt-extended-lambda --use_fast_math --ptxas-options=-v -lineinfo -gencode arch=compute_80,code=sm_80 --threads 4 -DTORCH_API_INCLUDE_EXTENSION_H '-DPYBIND11_COMPILER_TYPE="_gcc"' '-DPYBIND11_STDLIB="_libstdcpp"' '-DPYBIND11_BUILD_ABI="_cxxabi1011"' -DTORCH_EXTENSION_NAME=flash_attn_2_cuda -D_GLIBCXX_USE_CXX11_ABI=0
/anaconda/envs/flashattention/lib/python3.9/site-packages/torch/include/c10/core/SymInt.h(84): warning: integer conversion resulted in a change of sign

/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_kernel.h(140): warning: variable "MMA_M" was declared but never referenced
          detected during:
            instantiation of "void flash::compute_attn_1rowblock<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &, int, int, int) [with Kernel_traits=Flash_fwd_kernel_traits<224, 128, 64, 8, false, false, cutlass::half_t, Flash_kernel_traits<224, 128, 64, 8, cutlass::half_t>>, Is_dropout=true, Is_causal=true, Is_even_N=true, Is_even_K=true, Return_softmax=true, Params=Flash_fwd_params]" 
(588): here
            instantiation of "void flash::compute_attn<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &) [with Kernel_traits=Flash_fwd_kernel_traits<224, 128, 64, 8, false, false, cutlass::half_t, Flash_kernel_traits<224, 128, 64, 8, cutlass::half_t>>, Is_dropout=true, Is_causal=true, Is_even_N=true, Is_even_K=true, Return_softmax=true, Params=Flash_fwd_params]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(15): here
            instantiation of "void flash_fwd_kernel<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax>(Flash_fwd_params) [with Kernel_traits=Flash_fwd_kernel_traits<224, 128, 64, 8, false, false, cutlass::half_t, Flash_kernel_traits<224, 128, 64, 8, cutlass::half_t>>, Is_dropout=true, Is_causal=true, Is_even_N=true, Is_even_K=true, Return_softmax=true]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(34): here
            instantiation of "void run_flash_fwd<Kernel_traits,Is_dropout,Is_causal>(Flash_fwd_params &, cudaStream_t) [with Kernel_traits=Flash_fwd_kernel_traits<224, 128, 64, 8, false, false, cutlass::half_t, Flash_kernel_traits<224, 128, 64, 8, cutlass::half_t>>, Is_dropout=true, Is_causal=true]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(208): here
            instantiation of "void run_mha_fwd_hdim224<T>(Flash_fwd_params &, cudaStream_t) [with T=cutlass::half_t]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_hdim224_fp16_sm80.cu(8): here

/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_kernel.h(275): warning: variable "tQrQ" was declared but never referenced
          detected during:
            instantiation of "void flash::compute_attn_1rowblock<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &, int, int, int) [with Kernel_traits=Flash_fwd_kernel_traits<224, 128, 64, 8, false, false, cutlass::half_t, Flash_kernel_traits<224, 128, 64, 8, cutlass::half_t>>, Is_dropout=true, Is_causal=true, Is_even_N=true, Is_even_K=true, Return_softmax=true, Params=Flash_fwd_params]" 
(588): here
            instantiation of "void flash::compute_attn<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &) [with Kernel_traits=Flash_fwd_kernel_traits<224, 128, 64, 8, false, false, cutlass::half_t, Flash_kernel_traits<224, 128, 64, 8, cutlass::half_t>>, Is_dropout=true, Is_causal=true, Is_even_N=true, Is_even_K=true, Return_softmax=true, Params=Flash_fwd_params]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(15): here
            instantiation of "void flash_fwd_kernel<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax>(Flash_fwd_params) [with Kernel_traits=Flash_fwd_kernel_traits<224, 128, 64, 8, false, false, cutlass::half_t, Flash_kernel_traits<224, 128, 64, 8, cutlass::half_t>>, Is_dropout=true, Is_causal=true, Is_even_N=true, Is_even_K=true, Return_softmax=true]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(34): here
            instantiation of "void run_flash_fwd<Kernel_traits,Is_dropout,Is_causal>(Flash_fwd_params &, cudaStream_t) [with Kernel_traits=Flash_fwd_kernel_traits<224, 128, 64, 8, false, false, cutlass::half_t, Flash_kernel_traits<224, 128, 64, 8, cutlass::half_t>>, Is_dropout=true, Is_causal=true]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(208): here
            instantiation of "void run_mha_fwd_hdim224<T>(Flash_fwd_params &, cudaStream_t) [with T=cutlass::half_t]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_hdim224_fp16_sm80.cu(8): here

/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_kernel.h(140): warning: variable "MMA_M" was declared but never referenced
          detected during:
            instantiation of "void flash::compute_attn_1rowblock<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &, int, int, int) [with Kernel_traits=Flash_fwd_kernel_traits<224, 128, 64, 8, false, false, cutlass::half_t, Flash_kernel_traits<224, 128, 64, 8, cutlass::half_t>>, Is_dropout=true, Is_causal=true, Is_even_N=true, Is_even_K=true, Return_softmax=false, Params=Flash_fwd_params]" 
(588): here
            instantiation of "void flash::compute_attn<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &) [with Kernel_traits=Flash_fwd_kernel_traits<224, 128, 64, 8, false, false, cutlass::half_t, Flash_kernel_traits<224, 128, 64, 8, cutlass::half_t>>, Is_dropout=true, Is_causal=true, Is_even_N=true, Is_even_K=true, Return_softmax=false, Params=Flash_fwd_params]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(15): here
            instantiation of "void flash_fwd_kernel<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax>(Flash_fwd_params) [with Kernel_traits=Flash_fwd_kernel_traits<224, 128, 64, 8, false, false, cutlass::half_t, Flash_kernel_traits<224, 128, 64, 8, cutlass::half_t>>, Is_dropout=true, Is_causal=true, Is_even_N=true, Is_even_K=true, Return_softmax=false]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(34): here
            instantiation of "void run_flash_fwd<Kernel_traits,Is_dropout,Is_causal>(Flash_fwd_params &, cudaStream_t) [with Kernel_traits=Flash_fwd_kernel_traits<224, 128, 64, 8, false, false, cutlass::half_t, Flash_kernel_traits<224, 128, 64, 8, cutlass::half_t>>, Is_dropout=true, Is_causal=true]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(208): here
            instantiation of "void run_mha_fwd_hdim224<T>(Flash_fwd_params &, cudaStream_t) [with T=cutlass::half_t]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_hdim224_fp16_sm80.cu(8): here

/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_kernel.h(275): warning: variable "tQrQ" was declared but never referenced
          detected during:
            instantiation of "void flash::compute_attn_1rowblock<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &, int, int, int) [with Kernel_traits=Flash_fwd_kernel_traits<224, 128, 64, 8, false, false, cutlass::half_t, Flash_kernel_traits<224, 128, 64, 8, cutlass::half_t>>, Is_dropout=true, Is_causal=true, Is_even_N=true, Is_even_K=true, Return_softmax=false, Params=Flash_fwd_params]" 
(588): here
            instantiation of "void flash::compute_attn<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &) [with Kernel_traits=Flash_fwd_kernel_traits<224, 128, 64, 8, false, false, cutlass::half_t, Flash_kernel_traits<224, 128, 64, 8, cutlass::half_t>>, Is_dropout=true, Is_causal=true, Is_even_N=true, Is_even_K=true, Return_softmax=false, Params=Flash_fwd_params]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(15): here
            instantiation of "void flash_fwd_kernel<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax>(Flash_fwd_params) [with Kernel_traits=Flash_fwd_kernel_traits<224, 128, 64, 8, false, false, cutlass::half_t, Flash_kernel_traits<224, 128, 64, 8, cutlass::half_t>>, Is_dropout=true, Is_causal=true, Is_even_N=true, Is_even_K=true, Return_softmax=false]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(34): here
            instantiation of "void run_flash_fwd<Kernel_traits,Is_dropout,Is_causal>(Flash_fwd_params &, cudaStream_t) [with Kernel_traits=Flash_fwd_kernel_traits<224, 128, 64, 8, false, false, cutlass::half_t, Flash_kernel_traits<224, 128, 64, 8, cutlass::half_t>>, Is_dropout=true, Is_causal=true]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(208): here
            instantiation of "void run_mha_fwd_hdim224<T>(Flash_fwd_params &, cudaStream_t) [with T=cutlass::half_t]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_hdim224_fp16_sm80.cu(8): here

/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_kernel.h(140): warning: variable "MMA_M" was declared but never referenced
          detected during:
            instantiation of "void flash::compute_attn_1rowblock<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &, int, int, int) [with Kernel_traits=Flash_fwd_kernel_traits<224, 128, 64, 8, false, false, cutlass::half_t, Flash_kernel_traits<224, 128, 64, 8, cutlass::half_t>>, Is_dropout=true, Is_causal=true, Is_even_N=true, Is_even_K=false, Return_softmax=true, Params=Flash_fwd_params]" 
(588): here
            instantiation of "void flash::compute_attn<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &) [with Kernel_traits=Flash_fwd_kernel_traits<224, 128, 64, 8, false, false, cutlass::half_t, Flash_kernel_traits<224, 128, 64, 8, cutlass::half_t>>, Is_dropout=true, Is_causal=true, Is_even_N=true, Is_even_K=false, Return_softmax=true, Params=Flash_fwd_params]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(15): here
            instantiation of "void flash_fwd_kernel<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax>(Flash_fwd_params) [with Kernel_traits=Flash_fwd_kernel_traits<224, 128, 64, 8, false, false, cutlass::half_t, Flash_kernel_traits<224, 128, 64, 8, cutlass::half_t>>, Is_dropout=true, Is_causal=true, Is_even_N=true, Is_even_K=false, Return_softmax=true]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(34): here
            instantiation of "void run_flash_fwd<Kernel_traits,Is_dropout,Is_causal>(Flash_fwd_params &, cudaStream_t) [with Kernel_traits=Flash_fwd_kernel_traits<224, 128, 64, 8, false, false, cutlass::half_t, Flash_kernel_traits<224, 128, 64, 8, cutlass::half_t>>, Is_dropout=true, Is_causal=true]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(208): here
            instantiation of "void run_mha_fwd_hdim224<T>(Flash_fwd_params &, cudaStream_t) [with T=cutlass::half_t]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_hdim224_fp16_sm80.cu(8): here

/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_kernel.h(275): warning: variable "tQrQ" was declared but never referenced
          detected during:
            instantiation of "void flash::compute_attn_1rowblock<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &, int, int, int) [with Kernel_traits=Flash_fwd_kernel_traits<224, 128, 64, 8, false, false, cutlass::half_t, Flash_kernel_traits<224, 128, 64, 8, cutlass::half_t>>, Is_dropout=true, Is_causal=true, Is_even_N=true, Is_even_K=false, Return_softmax=true, Params=Flash_fwd_params]" 
(588): here
            instantiation of "void flash::compute_attn<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &) [with Kernel_traits=Flash_fwd_kernel_traits<224, 128, 64, 8, false, false, cutlass::half_t, Flash_kernel_traits<224, 128, 64, 8, cutlass::half_t>>, Is_dropout=true, Is_causal=true, Is_even_N=true, Is_even_K=false, Return_softmax=true, Params=Flash_fwd_params]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(15): here
            instantiation of "void flash_fwd_kernel<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax>(Flash_fwd_params) [with Kernel_traits=Flash_fwd_kernel_traits<224, 128, 64, 8, false, false, cutlass::half_t, Flash_kernel_traits<224, 128, 64, 8, cutlass::half_t>>, Is_dropout=true, Is_causal=true, Is_even_N=true, Is_even_K=false, Return_softmax=true]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(34): here
            instantiation of "void run_flash_fwd<Kernel_traits,Is_dropout,Is_causal>(Flash_fwd_params &, cudaStream_t) [with Kernel_traits=Flash_fwd_kernel_traits<224, 128, 64, 8, false, false, cutlass::half_t, Flash_kernel_traits<224, 128, 64, 8, cutlass::half_t>>, Is_dropout=true, Is_causal=true]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(208): here
            instantiation of "void run_mha_fwd_hdim224<T>(Flash_fwd_params &, cudaStream_t) [with T=cutlass::half_t]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_hdim224_fp16_sm80.cu(8): here

/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_kernel.h(140): warning: variable "MMA_M" was declared but never referenced
          detected during:
            instantiation of "void flash::compute_attn_1rowblock<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &, int, int, int) [with Kernel_traits=Flash_fwd_kernel_traits<224, 128, 64, 8, false, false, cutlass::half_t, Flash_kernel_traits<224, 128, 64, 8, cutlass::half_t>>, Is_dropout=true, Is_causal=true, Is_even_N=true, Is_even_K=false, Return_softmax=false, Params=Flash_fwd_params]" 
(588): here
            instantiation of "void flash::compute_attn<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &) [with Kernel_traits=Flash_fwd_kernel_traits<224, 128, 64, 8, false, false, cutlass::half_t, Flash_kernel_traits<224, 128, 64, 8, cutlass::half_t>>, Is_dropout=true, Is_causal=true, Is_even_N=true, Is_even_K=false, Return_softmax=false, Params=Flash_fwd_params]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(15): here
            instantiation of "void flash_fwd_kernel<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax>(Flash_fwd_params) [with Kernel_traits=Flash_fwd_kernel_traits<224, 128, 64, 8, false, false, cutlass::half_t, Flash_kernel_traits<224, 128, 64, 8, cutlass::half_t>>, Is_dropout=true, Is_causal=true, Is_even_N=true, Is_even_K=false, Return_softmax=false]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(34): here
            instantiation of "void run_flash_fwd<Kernel_traits,Is_dropout,Is_causal>(Flash_fwd_params &, cudaStream_t) [with Kernel_traits=Flash_fwd_kernel_traits<224, 128, 64, 8, false, false, cutlass::half_t, Flash_kernel_traits<224, 128, 64, 8, cutlass::half_t>>, Is_dropout=true, Is_causal=true]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(208): here
            instantiation of "void run_mha_fwd_hdim224<T>(Flash_fwd_params &, cudaStream_t) [with T=cutlass::half_t]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_hdim224_fp16_sm80.cu(8): here

/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_kernel.h(275): warning: variable "tQrQ" was declared but never referenced
          detected during:
            instantiation of "void flash::compute_attn_1rowblock<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &, int, int, int) [with Kernel_traits=Flash_fwd_kernel_traits<224, 128, 64, 8, false, false, cutlass::half_t, Flash_kernel_traits<224, 128, 64, 8, cutlass::half_t>>, Is_dropout=true, Is_causal=true, Is_even_N=true, Is_even_K=false, Return_softmax=false, Params=Flash_fwd_params]" 
(588): here
            instantiation of "void flash::compute_attn<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &) [with Kernel_traits=Flash_fwd_kernel_traits<224, 128, 64, 8, false, false, cutlass::half_t, Flash_kernel_traits<224, 128, 64, 8, cutlass::half_t>>, Is_dropout=true, Is_causal=true, Is_even_N=true, Is_even_K=false, Return_softmax=false, Params=Flash_fwd_params]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(15): here
            instantiation of "void flash_fwd_kernel<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax>(Flash_fwd_params) [with Kernel_traits=Flash_fwd_kernel_traits<224, 128, 64, 8, false, false, cutlass::half_t, Flash_kernel_traits<224, 128, 64, 8, cutlass::half_t>>, Is_dropout=true, Is_causal=true, Is_even_N=true, Is_even_K=false, Return_softmax=false]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(34): here
            instantiation of "void run_flash_fwd<Kernel_traits,Is_dropout,Is_causal>(Flash_fwd_params &, cudaStream_t) [with Kernel_traits=Flash_fwd_kernel_traits<224, 128, 64, 8, false, false, cutlass::half_t, Flash_kernel_traits<224, 128, 64, 8, cutlass::half_t>>, Is_dropout=true, Is_causal=true]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(208): here
            instantiation of "void run_mha_fwd_hdim224<T>(Flash_fwd_params &, cudaStream_t) [with T=cutlass::half_t]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_hdim224_fp16_sm80.cu(8): here

/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_kernel.h(140): warning: variable "MMA_M" was declared but never referenced
          detected during:
            instantiation of "void flash::compute_attn_1rowblock<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &, int, int, int) [with Kernel_traits=Flash_fwd_kernel_traits<224, 128, 64, 8, false, false, cutlass::half_t, Flash_kernel_traits<224, 128, 64, 8, cutlass::half_t>>, Is_dropout=true, Is_causal=true, Is_even_N=false, Is_even_K=true, Return_softmax=true, Params=Flash_fwd_params]" 
(588): here
            instantiation of "void flash::compute_attn<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &) [with Kernel_traits=Flash_fwd_kernel_traits<224, 128, 64, 8, false, false, cutlass::half_t, Flash_kernel_traits<224, 128, 64, 8, cutlass::half_t>>, Is_dropout=true, Is_causal=true, Is_even_N=false, Is_even_K=true, Return_softmax=true, Params=Flash_fwd_params]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(15): here
            instantiation of "void flash_fwd_kernel<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax>(Flash_fwd_params) [with Kernel_traits=Flash_fwd_kernel_traits<224, 128, 64, 8, false, false, cutlass::half_t, Flash_kernel_traits<224, 128, 64, 8, cutlass::half_t>>, Is_dropout=true, Is_causal=true, Is_even_N=false, Is_even_K=true, Return_softmax=true]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(34): here
            instantiation of "void run_flash_fwd<Kernel_traits,Is_dropout,Is_causal>(Flash_fwd_params &, cudaStream_t) [with Kernel_traits=Flash_fwd_kernel_traits<224, 128, 64, 8, false, false, cutlass::half_t, Flash_kernel_traits<224, 128, 64, 8, cutlass::half_t>>, Is_dropout=true, Is_causal=true]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(208): here
            instantiation of "void run_mha_fwd_hdim224<T>(Flash_fwd_params &, cudaStream_t) [with T=cutlass::half_t]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_hdim224_fp16_sm80.cu(8): here

/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_kernel.h(275): warning: variable "tQrQ" was declared but never referenced
          detected during:
            instantiation of "void flash::compute_attn_1rowblock<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &, int, int, int) [with Kernel_traits=Flash_fwd_kernel_traits<224, 128, 64, 8, false, false, cutlass::half_t, Flash_kernel_traits<224, 128, 64, 8, cutlass::half_t>>, Is_dropout=true, Is_causal=true, Is_even_N=false, Is_even_K=true, Return_softmax=true, Params=Flash_fwd_params]" 
(588): here
            instantiation of "void flash::compute_attn<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &) [with Kernel_traits=Flash_fwd_kernel_traits<224, 128, 64, 8, false, false, cutlass::half_t, Flash_kernel_traits<224, 128, 64, 8, cutlass::half_t>>, Is_dropout=true, Is_causal=true, Is_even_N=false, Is_even_K=true, Return_softmax=true, Params=Flash_fwd_params]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(15): here
            instantiation of "void flash_fwd_kernel<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax>(Flash_fwd_params) [with Kernel_traits=Flash_fwd_kernel_traits<224, 128, 64, 8, false, false, cutlass::half_t, Flash_kernel_traits<224, 128, 64, 8, cutlass::half_t>>, Is_dropout=true, Is_causal=true, Is_even_N=false, Is_even_K=true, Return_softmax=true]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(34): here
            instantiation of "void run_flash_fwd<Kernel_traits,Is_dropout,Is_causal>(Flash_fwd_params &, cudaStream_t) [with Kernel_traits=Flash_fwd_kernel_traits<224, 128, 64, 8, false, false, cutlass::half_t, Flash_kernel_traits<224, 128, 64, 8, cutlass::half_t>>, Is_dropout=true, Is_causal=true]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(208): here
            instantiation of "void run_mha_fwd_hdim224<T>(Flash_fwd_params &, cudaStream_t) [with T=cutlass::half_t]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_hdim224_fp16_sm80.cu(8): here

/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_kernel.h(140): warning: variable "MMA_M" was declared but never referenced
          detected during:
            instantiation of "void flash::compute_attn_1rowblock<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &, int, int, int) [with Kernel_traits=Flash_fwd_kernel_traits<224, 128, 64, 8, false, false, cutlass::half_t, Flash_kernel_traits<224, 128, 64, 8, cutlass::half_t>>, Is_dropout=true, Is_causal=true, Is_even_N=false, Is_even_K=true, Return_softmax=false, Params=Flash_fwd_params]" 
(588): here
            instantiation of "void flash::compute_attn<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &) [with Kernel_traits=Flash_fwd_kernel_traits<224, 128, 64, 8, false, false, cutlass::half_t, Flash_kernel_traits<224, 128, 64, 8, cutlass::half_t>>, Is_dropout=true, Is_causal=true, Is_even_N=false, Is_even_K=true, Return_softmax=false, Params=Flash_fwd_params]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(15): here
            instantiation of "void flash_fwd_kernel<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax>(Flash_fwd_params) [with Kernel_traits=Flash_fwd_kernel_traits<224, 128, 64, 8, false, false, cutlass::half_t, Flash_kernel_traits<224, 128, 64, 8, cutlass::half_t>>, Is_dropout=true, Is_causal=true, Is_even_N=false, Is_even_K=true, Return_softmax=false]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(34): here
            instantiation of "void run_flash_fwd<Kernel_traits,Is_dropout,Is_causal>(Flash_fwd_params &, cudaStream_t) [with Kernel_traits=Flash_fwd_kernel_traits<224, 128, 64, 8, false, false, cutlass::half_t, Flash_kernel_traits<224, 128, 64, 8, cutlass::half_t>>, Is_dropout=true, Is_causal=true]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(208): here
            instantiation of "void run_mha_fwd_hdim224<T>(Flash_fwd_params &, cudaStream_t) [with T=cutlass::half_t]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_hdim224_fp16_sm80.cu(8): here

/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_kernel.h(275): warning: variable "tQrQ" was declared but never referenced
          detected during:
            instantiation of "void flash::compute_attn_1rowblock<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &, int, int, int) [with Kernel_traits=Flash_fwd_kernel_traits<224, 128, 64, 8, false, false, cutlass::half_t, Flash_kernel_traits<224, 128, 64, 8, cutlass::half_t>>, Is_dropout=true, Is_causal=true, Is_even_N=false, Is_even_K=true, Return_softmax=false, Params=Flash_fwd_params]" 
(588): here
            instantiation of "void flash::compute_attn<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &) [with Kernel_traits=Flash_fwd_kernel_traits<224, 128, 64, 8, false, false, cutlass::half_t, Flash_kernel_traits<224, 128, 64, 8, cutlass::half_t>>, Is_dropout=true, Is_causal=true, Is_even_N=false, Is_even_K=true, Return_softmax=false, Params=Flash_fwd_params]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(15): here
            instantiation of "void flash_fwd_kernel<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax>(Flash_fwd_params) [with Kernel_traits=Flash_fwd_kernel_traits<224, 128, 64, 8, false, false, cutlass::half_t, Flash_kernel_traits<224, 128, 64, 8, cutlass::half_t>>, Is_dropout=true, Is_causal=true, Is_even_N=false, Is_even_K=true, Return_softmax=false]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(34): here
            instantiation of "void run_flash_fwd<Kernel_traits,Is_dropout,Is_causal>(Flash_fwd_params &, cudaStream_t) [with Kernel_traits=Flash_fwd_kernel_traits<224, 128, 64, 8, false, false, cutlass::half_t, Flash_kernel_traits<224, 128, 64, 8, cutlass::half_t>>, Is_dropout=true, Is_causal=true]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(208): here
            instantiation of "void run_mha_fwd_hdim224<T>(Flash_fwd_params &, cudaStream_t) [with T=cutlass::half_t]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_hdim224_fp16_sm80.cu(8): here

/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_kernel.h(140): warning: variable "MMA_M" was declared but never referenced
          detected during:
            instantiation of "void flash::compute_attn_1rowblock<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &, int, int, int) [with Kernel_traits=Flash_fwd_kernel_traits<224, 128, 64, 8, false, false, cutlass::half_t, Flash_kernel_traits<224, 128, 64, 8, cutlass::half_t>>, Is_dropout=true, Is_causal=true, Is_even_N=false, Is_even_K=false, Return_softmax=true, Params=Flash_fwd_params]" 
(588): here
            instantiation of "void flash::compute_attn<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &) [with Kernel_traits=Flash_fwd_kernel_traits<224, 128, 64, 8, false, false, cutlass::half_t, Flash_kernel_traits<224, 128, 64, 8, cutlass::half_t>>, Is_dropout=true, Is_causal=true, Is_even_N=false, Is_even_K=false, Return_softmax=true, Params=Flash_fwd_params]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(15): here
            instantiation of "void flash_fwd_kernel<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax>(Flash_fwd_params) [with Kernel_traits=Flash_fwd_kernel_traits<224, 128, 64, 8, false, false, cutlass::half_t, Flash_kernel_traits<224, 128, 64, 8, cutlass::half_t>>, Is_dropout=true, Is_causal=true, Is_even_N=false, Is_even_K=false, Return_softmax=true]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(34): here
            instantiation of "void run_flash_fwd<Kernel_traits,Is_dropout,Is_causal>(Flash_fwd_params &, cudaStream_t) [with Kernel_traits=Flash_fwd_kernel_traits<224, 128, 64, 8, false, false, cutlass::half_t, Flash_kernel_traits<224, 128, 64, 8, cutlass::half_t>>, Is_dropout=true, Is_causal=true]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(208): here
            instantiation of "void run_mha_fwd_hdim224<T>(Flash_fwd_params &, cudaStream_t) [with T=cutlass::half_t]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_hdim224_fp16_sm80.cu(8): here

/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_kernel.h(275): warning: variable "tQrQ" was declared but never referenced
          detected during:
            instantiation of "void flash::compute_attn_1rowblock<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &, int, int, int) [with Kernel_traits=Flash_fwd_kernel_traits<224, 128, 64, 8, false, false, cutlass::half_t, Flash_kernel_traits<224, 128, 64, 8, cutlass::half_t>>, Is_dropout=true, Is_causal=true, Is_even_N=false, Is_even_K=false, Return_softmax=true, Params=Flash_fwd_params]" 
(588): here
            instantiation of "void flash::compute_attn<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &) [with Kernel_traits=Flash_fwd_kernel_traits<224, 128, 64, 8, false, false, cutlass::half_t, Flash_kernel_traits<224, 128, 64, 8, cutlass::half_t>>, Is_dropout=true, Is_causal=true, Is_even_N=false, Is_even_K=false, Return_softmax=true, Params=Flash_fwd_params]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(15): here
            instantiation of "void flash_fwd_kernel<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax>(Flash_fwd_params) [with Kernel_traits=Flash_fwd_kernel_traits<224, 128, 64, 8, false, false, cutlass::half_t, Flash_kernel_traits<224, 128, 64, 8, cutlass::half_t>>, Is_dropout=true, Is_causal=true, Is_even_N=false, Is_even_K=false, Return_softmax=true]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(34): here
            instantiation of "void run_flash_fwd<Kernel_traits,Is_dropout,Is_causal>(Flash_fwd_params &, cudaStream_t) [with Kernel_traits=Flash_fwd_kernel_traits<224, 128, 64, 8, false, false, cutlass::half_t, Flash_kernel_traits<224, 128, 64, 8, cutlass::half_t>>, Is_dropout=true, Is_causal=true]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(208): here
            instantiation of "void run_mha_fwd_hdim224<T>(Flash_fwd_params &, cudaStream_t) [with T=cutlass::half_t]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_hdim224_fp16_sm80.cu(8): here

/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_kernel.h(140): warning: variable "MMA_M" was declared but never referenced
          detected during:
            instantiation of "void flash::compute_attn_1rowblock<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &, int, int, int) [with Kernel_traits=Flash_fwd_kernel_traits<224, 128, 64, 8, false, false, cutlass::half_t, Flash_kernel_traits<224, 128, 64, 8, cutlass::half_t>>, Is_dropout=true, Is_causal=true, Is_even_N=false, Is_even_K=false, Return_softmax=false, Params=Flash_fwd_params]" 
(588): here
            instantiation of "void flash::compute_attn<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &) [with Kernel_traits=Flash_fwd_kernel_traits<224, 128, 64, 8, false, false, cutlass::half_t, Flash_kernel_traits<224, 128, 64, 8, cutlass::half_t>>, Is_dropout=true, Is_causal=true, Is_even_N=false, Is_even_K=false, Return_softmax=false, Params=Flash_fwd_params]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(15): here
            instantiation of "void flash_fwd_kernel<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax>(Flash_fwd_params) [with Kernel_traits=Flash_fwd_kernel_traits<224, 128, 64, 8, false, false, cutlass::half_t, Flash_kernel_traits<224, 128, 64, 8, cutlass::half_t>>, Is_dropout=true, Is_causal=true, Is_even_N=false, Is_even_K=false, Return_softmax=false]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(34): here
            instantiation of "void run_flash_fwd<Kernel_traits,Is_dropout,Is_causal>(Flash_fwd_params &, cudaStream_t) [with Kernel_traits=Flash_fwd_kernel_traits<224, 128, 64, 8, false, false, cutlass::half_t, Flash_kernel_traits<224, 128, 64, 8, cutlass::half_t>>, Is_dropout=true, Is_causal=true]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(208): here
            instantiation of "void run_mha_fwd_hdim224<T>(Flash_fwd_params &, cudaStream_t) [with T=cutlass::half_t]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_hdim224_fp16_sm80.cu(8): here

/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_kernel.h(275): warning: variable "tQrQ" was declared but never referenced
          detected during:
            instantiation of "void flash::compute_attn_1rowblock<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &, int, int, int) [with Kernel_traits=Flash_fwd_kernel_traits<224, 128, 64, 8, false, false, cutlass::half_t, Flash_kernel_traits<224, 128, 64, 8, cutlass::half_t>>, Is_dropout=true, Is_causal=true, Is_even_N=false, Is_even_K=false, Return_softmax=false, Params=Flash_fwd_params]" 
(588): here
            instantiation of "void flash::compute_attn<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &) [with Kernel_traits=Flash_fwd_kernel_traits<224, 128, 64, 8, false, false, cutlass::half_t, Flash_kernel_traits<224, 128, 64, 8, cutlass::half_t>>, Is_dropout=true, Is_causal=true, Is_even_N=false, Is_even_K=false, Return_softmax=false, Params=Flash_fwd_params]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(15): here
            instantiation of "void flash_fwd_kernel<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax>(Flash_fwd_params) [with Kernel_traits=Flash_fwd_kernel_traits<224, 128, 64, 8, false, false, cutlass::half_t, Flash_kernel_traits<224, 128, 64, 8, cutlass::half_t>>, Is_dropout=true, Is_causal=true, Is_even_N=false, Is_even_K=false, Return_softmax=false]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(34): here
            instantiation of "void run_flash_fwd<Kernel_traits,Is_dropout,Is_causal>(Flash_fwd_params &, cudaStream_t) [with Kernel_traits=Flash_fwd_kernel_traits<224, 128, 64, 8, false, false, cutlass::half_t, Flash_kernel_traits<224, 128, 64, 8, cutlass::half_t>>, Is_dropout=true, Is_causal=true]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(208): here
            instantiation of "void run_mha_fwd_hdim224<T>(Flash_fwd_params &, cudaStream_t) [with T=cutlass::half_t]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_hdim224_fp16_sm80.cu(8): here

/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_kernel.h(140): warning: variable "MMA_M" was declared but never referenced
          detected during:
            instantiation of "void flash::compute_attn_1rowblock<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &, int, int, int) [with Kernel_traits=Flash_fwd_kernel_traits<224, 64, 64, 4, false, false, cutlass::half_t, Flash_kernel_traits<224, 64, 64, 4, cutlass::half_t>>, Is_dropout=true, Is_causal=true, Is_even_N=true, Is_even_K=true, Return_softmax=true, Params=Flash_fwd_params]" 
(588): here
            instantiation of "void flash::compute_attn<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &) [with Kernel_traits=Flash_fwd_kernel_traits<224, 64, 64, 4, false, false, cutlass::half_t, Flash_kernel_traits<224, 64, 64, 4, cutlass::half_t>>, Is_dropout=true, Is_causal=true, Is_even_N=true, Is_even_K=true, Return_softmax=true, Params=Flash_fwd_params]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(15): here
            instantiation of "void flash_fwd_kernel<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax>(Flash_fwd_params) [with Kernel_traits=Flash_fwd_kernel_traits<224, 64, 64, 4, false, false, cutlass::half_t, Flash_kernel_traits<224, 64, 64, 4, cutlass::half_t>>, Is_dropout=true, Is_causal=true, Is_even_N=true, Is_even_K=true, Return_softmax=true]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(34): here
            instantiation of "void run_flash_fwd<Kernel_traits,Is_dropout,Is_causal>(Flash_fwd_params &, cudaStream_t) [with Kernel_traits=Flash_fwd_kernel_traits<224, 64, 64, 4, false, false, cutlass::half_t, Flash_kernel_traits<224, 64, 64, 4, cutlass::half_t>>, Is_dropout=true, Is_causal=true]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(208): here
            instantiation of "void run_mha_fwd_hdim224<T>(Flash_fwd_params &, cudaStream_t) [with T=cutlass::half_t]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_hdim224_fp16_sm80.cu(8): here

/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_kernel.h(275): warning: variable "tQrQ" was declared but never referenced
          detected during:
            instantiation of "void flash::compute_attn_1rowblock<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &, int, int, int) [with Kernel_traits=Flash_fwd_kernel_traits<224, 64, 64, 4, false, false, cutlass::half_t, Flash_kernel_traits<224, 64, 64, 4, cutlass::half_t>>, Is_dropout=true, Is_causal=true, Is_even_N=true, Is_even_K=true, Return_softmax=true, Params=Flash_fwd_params]" 
(588): here
            instantiation of "void flash::compute_attn<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &) [with Kernel_traits=Flash_fwd_kernel_traits<224, 64, 64, 4, false, false, cutlass::half_t, Flash_kernel_traits<224, 64, 64, 4, cutlass::half_t>>, Is_dropout=true, Is_causal=true, Is_even_N=true, Is_even_K=true, Return_softmax=true, Params=Flash_fwd_params]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(15): here
            instantiation of "void flash_fwd_kernel<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax>(Flash_fwd_params) [with Kernel_traits=Flash_fwd_kernel_traits<224, 64, 64, 4, false, false, cutlass::half_t, Flash_kernel_traits<224, 64, 64, 4, cutlass::half_t>>, Is_dropout=true, Is_causal=true, Is_even_N=true, Is_even_K=true, Return_softmax=true]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(34): here
            instantiation of "void run_flash_fwd<Kernel_traits,Is_dropout,Is_causal>(Flash_fwd_params &, cudaStream_t) [with Kernel_traits=Flash_fwd_kernel_traits<224, 64, 64, 4, false, false, cutlass::half_t, Flash_kernel_traits<224, 64, 64, 4, cutlass::half_t>>, Is_dropout=true, Is_causal=true]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(208): here
            instantiation of "void run_mha_fwd_hdim224<T>(Flash_fwd_params &, cudaStream_t) [with T=cutlass::half_t]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_hdim224_fp16_sm80.cu(8): here

/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_kernel.h(140): warning: variable "MMA_M" was declared but never referenced
          detected during:
            instantiation of "void flash::compute_attn_1rowblock<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &, int, int, int) [with Kernel_traits=Flash_fwd_kernel_traits<224, 64, 64, 4, false, false, cutlass::half_t, Flash_kernel_traits<224, 64, 64, 4, cutlass::half_t>>, Is_dropout=true, Is_causal=true, Is_even_N=true, Is_even_K=true, Return_softmax=false, Params=Flash_fwd_params]" 
(588): here
            instantiation of "void flash::compute_attn<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &) [with Kernel_traits=Flash_fwd_kernel_traits<224, 64, 64, 4, false, false, cutlass::half_t, Flash_kernel_traits<224, 64, 64, 4, cutlass::half_t>>, Is_dropout=true, Is_causal=true, Is_even_N=true, Is_even_K=true, Return_softmax=false, Params=Flash_fwd_params]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(15): here
            instantiation of "void flash_fwd_kernel<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax>(Flash_fwd_params) [with Kernel_traits=Flash_fwd_kernel_traits<224, 64, 64, 4, false, false, cutlass::half_t, Flash_kernel_traits<224, 64, 64, 4, cutlass::half_t>>, Is_dropout=true, Is_causal=true, Is_even_N=true, Is_even_K=true, Return_softmax=false]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(34): here
            instantiation of "void run_flash_fwd<Kernel_traits,Is_dropout,Is_causal>(Flash_fwd_params &, cudaStream_t) [with Kernel_traits=Flash_fwd_kernel_traits<224, 64, 64, 4, false, false, cutlass::half_t, Flash_kernel_traits<224, 64, 64, 4, cutlass::half_t>>, Is_dropout=true, Is_causal=true]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(208): here
            instantiation of "void run_mha_fwd_hdim224<T>(Flash_fwd_params &, cudaStream_t) [with T=cutlass::half_t]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_hdim224_fp16_sm80.cu(8): here

/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_kernel.h(275): warning: variable "tQrQ" was declared but never referenced
          detected during:
            instantiation of "void flash::compute_attn_1rowblock<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &, int, int, int) [with Kernel_traits=Flash_fwd_kernel_traits<224, 64, 64, 4, false, false, cutlass::half_t, Flash_kernel_traits<224, 64, 64, 4, cutlass::half_t>>, Is_dropout=true, Is_causal=true, Is_even_N=true, Is_even_K=true, Return_softmax=false, Params=Flash_fwd_params]" 
(588): here
            instantiation of "void flash::compute_attn<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &) [with Kernel_traits=Flash_fwd_kernel_traits<224, 64, 64, 4, false, false, cutlass::half_t, Flash_kernel_traits<224, 64, 64, 4, cutlass::half_t>>, Is_dropout=true, Is_causal=true, Is_even_N=true, Is_even_K=true, Return_softmax=false, Params=Flash_fwd_params]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(15): here
            instantiation of "void flash_fwd_kernel<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax>(Flash_fwd_params) [with Kernel_traits=Flash_fwd_kernel_traits<224, 64, 64, 4, false, false, cutlass::half_t, Flash_kernel_traits<224, 64, 64, 4, cutlass::half_t>>, Is_dropout=true, Is_causal=true, Is_even_N=true, Is_even_K=true, Return_softmax=false]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(34): here
            instantiation of "void run_flash_fwd<Kernel_traits,Is_dropout,Is_causal>(Flash_fwd_params &, cudaStream_t) [with Kernel_traits=Flash_fwd_kernel_traits<224, 64, 64, 4, false, false, cutlass::half_t, Flash_kernel_traits<224, 64, 64, 4, cutlass::half_t>>, Is_dropout=true, Is_causal=true]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(208): here
            instantiation of "void run_mha_fwd_hdim224<T>(Flash_fwd_params &, cudaStream_t) [with T=cutlass::half_t]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_hdim224_fp16_sm80.cu(8): here

/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_kernel.h(140): warning: variable "MMA_M" was declared but never referenced
          detected during:
            instantiation of "void flash::compute_attn_1rowblock<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &, int, int, int) [with Kernel_traits=Flash_fwd_kernel_traits<224, 64, 64, 4, false, false, cutlass::half_t, Flash_kernel_traits<224, 64, 64, 4, cutlass::half_t>>, Is_dropout=true, Is_causal=true, Is_even_N=true, Is_even_K=false, Return_softmax=true, Params=Flash_fwd_params]" 
(588): here
            instantiation of "void flash::compute_attn<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &) [with Kernel_traits=Flash_fwd_kernel_traits<224, 64, 64, 4, false, false, cutlass::half_t, Flash_kernel_traits<224, 64, 64, 4, cutlass::half_t>>, Is_dropout=true, Is_causal=true, Is_even_N=true, Is_even_K=false, Return_softmax=true, Params=Flash_fwd_params]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(15): here
            instantiation of "void flash_fwd_kernel<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax>(Flash_fwd_params) [with Kernel_traits=Flash_fwd_kernel_traits<224, 64, 64, 4, false, false, cutlass::half_t, Flash_kernel_traits<224, 64, 64, 4, cutlass::half_t>>, Is_dropout=true, Is_causal=true, Is_even_N=true, Is_even_K=false, Return_softmax=true]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(34): here
            instantiation of "void run_flash_fwd<Kernel_traits,Is_dropout,Is_causal>(Flash_fwd_params &, cudaStream_t) [with Kernel_traits=Flash_fwd_kernel_traits<224, 64, 64, 4, false, false, cutlass::half_t, Flash_kernel_traits<224, 64, 64, 4, cutlass::half_t>>, Is_dropout=true, Is_causal=true]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(208): here
            instantiation of "void run_mha_fwd_hdim224<T>(Flash_fwd_params &, cudaStream_t) [with T=cutlass::half_t]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_hdim224_fp16_sm80.cu(8): here

/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_kernel.h(275): warning: variable "tQrQ" was declared but never referenced
          detected during:
            instantiation of "void flash::compute_attn_1rowblock<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &, int, int, int) [with Kernel_traits=Flash_fwd_kernel_traits<224, 64, 64, 4, false, false, cutlass::half_t, Flash_kernel_traits<224, 64, 64, 4, cutlass::half_t>>, Is_dropout=true, Is_causal=true, Is_even_N=true, Is_even_K=false, Return_softmax=true, Params=Flash_fwd_params]" 
(588): here
            instantiation of "void flash::compute_attn<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &) [with Kernel_traits=Flash_fwd_kernel_traits<224, 64, 64, 4, false, false, cutlass::half_t, Flash_kernel_traits<224, 64, 64, 4, cutlass::half_t>>, Is_dropout=true, Is_causal=true, Is_even_N=true, Is_even_K=false, Return_softmax=true, Params=Flash_fwd_params]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(15): here
            instantiation of "void flash_fwd_kernel<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax>(Flash_fwd_params) [with Kernel_traits=Flash_fwd_kernel_traits<224, 64, 64, 4, false, false, cutlass::half_t, Flash_kernel_traits<224, 64, 64, 4, cutlass::half_t>>, Is_dropout=true, Is_causal=true, Is_even_N=true, Is_even_K=false, Return_softmax=true]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(34): here
            instantiation of "void run_flash_fwd<Kernel_traits,Is_dropout,Is_causal>(Flash_fwd_params &, cudaStream_t) [with Kernel_traits=Flash_fwd_kernel_traits<224, 64, 64, 4, false, false, cutlass::half_t, Flash_kernel_traits<224, 64, 64, 4, cutlass::half_t>>, Is_dropout=true, Is_causal=true]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(208): here
            instantiation of "void run_mha_fwd_hdim224<T>(Flash_fwd_params &, cudaStream_t) [with T=cutlass::half_t]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_hdim224_fp16_sm80.cu(8): here

/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_kernel.h(140): warning: variable "MMA_M" was declared but never referenced
          detected during:
            instantiation of "void flash::compute_attn_1rowblock<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &, int, int, int) [with Kernel_traits=Flash_fwd_kernel_traits<224, 64, 64, 4, false, false, cutlass::half_t, Flash_kernel_traits<224, 64, 64, 4, cutlass::half_t>>, Is_dropout=true, Is_causal=true, Is_even_N=true, Is_even_K=false, Return_softmax=false, Params=Flash_fwd_params]" 
(588): here
            instantiation of "void flash::compute_attn<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &) [with Kernel_traits=Flash_fwd_kernel_traits<224, 64, 64, 4, false, false, cutlass::half_t, Flash_kernel_traits<224, 64, 64, 4, cutlass::half_t>>, Is_dropout=true, Is_causal=true, Is_even_N=true, Is_even_K=false, Return_softmax=false, Params=Flash_fwd_params]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(15): here
            instantiation of "void flash_fwd_kernel<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax>(Flash_fwd_params) [with Kernel_traits=Flash_fwd_kernel_traits<224, 64, 64, 4, false, false, cutlass::half_t, Flash_kernel_traits<224, 64, 64, 4, cutlass::half_t>>, Is_dropout=true, Is_causal=true, Is_even_N=true, Is_even_K=false, Return_softmax=false]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(34): here
            instantiation of "void run_flash_fwd<Kernel_traits,Is_dropout,Is_causal>(Flash_fwd_params &, cudaStream_t) [with Kernel_traits=Flash_fwd_kernel_traits<224, 64, 64, 4, false, false, cutlass::half_t, Flash_kernel_traits<224, 64, 64, 4, cutlass::half_t>>, Is_dropout=true, Is_causal=true]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(208): here
            instantiation of "void run_mha_fwd_hdim224<T>(Flash_fwd_params &, cudaStream_t) [with T=cutlass::half_t]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_hdim224_fp16_sm80.cu(8): here

/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_kernel.h(275): warning: variable "tQrQ" was declared but never referenced
          detected during:
            instantiation of "void flash::compute_attn_1rowblock<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &, int, int, int) [with Kernel_traits=Flash_fwd_kernel_traits<224, 64, 64, 4, false, false, cutlass::half_t, Flash_kernel_traits<224, 64, 64, 4, cutlass::half_t>>, Is_dropout=true, Is_causal=true, Is_even_N=true, Is_even_K=false, Return_softmax=false, Params=Flash_fwd_params]" 
(588): here
            instantiation of "void flash::compute_attn<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &) [with Kernel_traits=Flash_fwd_kernel_traits<224, 64, 64, 4, false, false, cutlass::half_t, Flash_kernel_traits<224, 64, 64, 4, cutlass::half_t>>, Is_dropout=true, Is_causal=true, Is_even_N=true, Is_even_K=false, Return_softmax=false, Params=Flash_fwd_params]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(15): here
            instantiation of "void flash_fwd_kernel<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax>(Flash_fwd_params) [with Kernel_traits=Flash_fwd_kernel_traits<224, 64, 64, 4, false, false, cutlass::half_t, Flash_kernel_traits<224, 64, 64, 4, cutlass::half_t>>, Is_dropout=true, Is_causal=true, Is_even_N=true, Is_even_K=false, Return_softmax=false]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(34): here
            instantiation of "void run_flash_fwd<Kernel_traits,Is_dropout,Is_causal>(Flash_fwd_params &, cudaStream_t) [with Kernel_traits=Flash_fwd_kernel_traits<224, 64, 64, 4, false, false, cutlass::half_t, Flash_kernel_traits<224, 64, 64, 4, cutlass::half_t>>, Is_dropout=true, Is_causal=true]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(208): here
            instantiation of "void run_mha_fwd_hdim224<T>(Flash_fwd_params &, cudaStream_t) [with T=cutlass::half_t]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_hdim224_fp16_sm80.cu(8): here

/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_kernel.h(140): warning: variable "MMA_M" was declared but never referenced
          detected during:
            instantiation of "void flash::compute_attn_1rowblock<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &, int, int, int) [with Kernel_traits=Flash_fwd_kernel_traits<224, 64, 64, 4, false, false, cutlass::half_t, Flash_kernel_traits<224, 64, 64, 4, cutlass::half_t>>, Is_dropout=true, Is_causal=true, Is_even_N=false, Is_even_K=true, Return_softmax=true, Params=Flash_fwd_params]" 
(588): here
            instantiation of "void flash::compute_attn<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &) [with Kernel_traits=Flash_fwd_kernel_traits<224, 64, 64, 4, false, false, cutlass::half_t, Flash_kernel_traits<224, 64, 64, 4, cutlass::half_t>>, Is_dropout=true, Is_causal=true, Is_even_N=false, Is_even_K=true, Return_softmax=true, Params=Flash_fwd_params]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(15): here
            instantiation of "void flash_fwd_kernel<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax>(Flash_fwd_params) [with Kernel_traits=Flash_fwd_kernel_traits<224, 64, 64, 4, false, false, cutlass::half_t, Flash_kernel_traits<224, 64, 64, 4, cutlass::half_t>>, Is_dropout=true, Is_causal=true, Is_even_N=false, Is_even_K=true, Return_softmax=true]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(34): here
            instantiation of "void run_flash_fwd<Kernel_traits,Is_dropout,Is_causal>(Flash_fwd_params &, cudaStream_t) [with Kernel_traits=Flash_fwd_kernel_traits<224, 64, 64, 4, false, false, cutlass::half_t, Flash_kernel_traits<224, 64, 64, 4, cutlass::half_t>>, Is_dropout=true, Is_causal=true]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(208): here
            instantiation of "void run_mha_fwd_hdim224<T>(Flash_fwd_params &, cudaStream_t) [with T=cutlass::half_t]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_hdim224_fp16_sm80.cu(8): here

/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_kernel.h(275): warning: variable "tQrQ" was declared but never referenced
          detected during:
            instantiation of "void flash::compute_attn_1rowblock<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &, int, int, int) [with Kernel_traits=Flash_fwd_kernel_traits<224, 64, 64, 4, false, false, cutlass::half_t, Flash_kernel_traits<224, 64, 64, 4, cutlass::half_t>>, Is_dropout=true, Is_causal=true, Is_even_N=false, Is_even_K=true, Return_softmax=true, Params=Flash_fwd_params]" 
(588): here
            instantiation of "void flash::compute_attn<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &) [with Kernel_traits=Flash_fwd_kernel_traits<224, 64, 64, 4, false, false, cutlass::half_t, Flash_kernel_traits<224, 64, 64, 4, cutlass::half_t>>, Is_dropout=true, Is_causal=true, Is_even_N=false, Is_even_K=true, Return_softmax=true, Params=Flash_fwd_params]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(15): here
            instantiation of "void flash_fwd_kernel<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax>(Flash_fwd_params) [with Kernel_traits=Flash_fwd_kernel_traits<224, 64, 64, 4, false, false, cutlass::half_t, Flash_kernel_traits<224, 64, 64, 4, cutlass::half_t>>, Is_dropout=true, Is_causal=true, Is_even_N=false, Is_even_K=true, Return_softmax=true]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(34): here
            instantiation of "void run_flash_fwd<Kernel_traits,Is_dropout,Is_causal>(Flash_fwd_params &, cudaStream_t) [with Kernel_traits=Flash_fwd_kernel_traits<224, 64, 64, 4, false, false, cutlass::half_t, Flash_kernel_traits<224, 64, 64, 4, cutlass::half_t>>, Is_dropout=true, Is_causal=true]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(208): here
            instantiation of "void run_mha_fwd_hdim224<T>(Flash_fwd_params &, cudaStream_t) [with T=cutlass::half_t]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_hdim224_fp16_sm80.cu(8): here

/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_kernel.h(140): warning: variable "MMA_M" was declared but never referenced
          detected during:
            instantiation of "void flash::compute_attn_1rowblock<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &, int, int, int) [with Kernel_traits=Flash_fwd_kernel_traits<224, 64, 64, 4, false, false, cutlass::half_t, Flash_kernel_traits<224, 64, 64, 4, cutlass::half_t>>, Is_dropout=true, Is_causal=true, Is_even_N=false, Is_even_K=true, Return_softmax=false, Params=Flash_fwd_params]" 
(588): here
            instantiation of "void flash::compute_attn<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &) [with Kernel_traits=Flash_fwd_kernel_traits<224, 64, 64, 4, false, false, cutlass::half_t, Flash_kernel_traits<224, 64, 64, 4, cutlass::half_t>>, Is_dropout=true, Is_causal=true, Is_even_N=false, Is_even_K=true, Return_softmax=false, Params=Flash_fwd_params]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(15): here
            instantiation of "void flash_fwd_kernel<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax>(Flash_fwd_params) [with Kernel_traits=Flash_fwd_kernel_traits<224, 64, 64, 4, false, false, cutlass::half_t, Flash_kernel_traits<224, 64, 64, 4, cutlass::half_t>>, Is_dropout=true, Is_causal=true, Is_even_N=false, Is_even_K=true, Return_softmax=false]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(34): here
            instantiation of "void run_flash_fwd<Kernel_traits,Is_dropout,Is_causal>(Flash_fwd_params &, cudaStream_t) [with Kernel_traits=Flash_fwd_kernel_traits<224, 64, 64, 4, false, false, cutlass::half_t, Flash_kernel_traits<224, 64, 64, 4, cutlass::half_t>>, Is_dropout=true, Is_causal=true]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(208): here
            instantiation of "void run_mha_fwd_hdim224<T>(Flash_fwd_params &, cudaStream_t) [with T=cutlass::half_t]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_hdim224_fp16_sm80.cu(8): here

/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_kernel.h(275): warning: variable "tQrQ" was declared but never referenced
          detected during:
            instantiation of "void flash::compute_attn_1rowblock<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &, int, int, int) [with Kernel_traits=Flash_fwd_kernel_traits<224, 64, 64, 4, false, false, cutlass::half_t, Flash_kernel_traits<224, 64, 64, 4, cutlass::half_t>>, Is_dropout=true, Is_causal=true, Is_even_N=false, Is_even_K=true, Return_softmax=false, Params=Flash_fwd_params]" 
(588): here
            instantiation of "void flash::compute_attn<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &) [with Kernel_traits=Flash_fwd_kernel_traits<224, 64, 64, 4, false, false, cutlass::half_t, Flash_kernel_traits<224, 64, 64, 4, cutlass::half_t>>, Is_dropout=true, Is_causal=true, Is_even_N=false, Is_even_K=true, Return_softmax=false, Params=Flash_fwd_params]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(15): here
            instantiation of "void flash_fwd_kernel<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax>(Flash_fwd_params) [with Kernel_traits=Flash_fwd_kernel_traits<224, 64, 64, 4, false, false, cutlass::half_t, Flash_kernel_traits<224, 64, 64, 4, cutlass::half_t>>, Is_dropout=true, Is_causal=true, Is_even_N=false, Is_even_K=true, Return_softmax=false]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(34): here
            instantiation of "void run_flash_fwd<Kernel_traits,Is_dropout,Is_causal>(Flash_fwd_params &, cudaStream_t) [with Kernel_traits=Flash_fwd_kernel_traits<224, 64, 64, 4, false, false, cutlass::half_t, Flash_kernel_traits<224, 64, 64, 4, cutlass::half_t>>, Is_dropout=true, Is_causal=true]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(208): here
            instantiation of "void run_mha_fwd_hdim224<T>(Flash_fwd_params &, cudaStream_t) [with T=cutlass::half_t]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_hdim224_fp16_sm80.cu(8): here

/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_kernel.h(140): warning: variable "MMA_M" was declared but never referenced
          detected during:
            instantiation of "void flash::compute_attn_1rowblock<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &, int, int, int) [with Kernel_traits=Flash_fwd_kernel_traits<224, 64, 64, 4, false, false, cutlass::half_t, Flash_kernel_traits<224, 64, 64, 4, cutlass::half_t>>, Is_dropout=true, Is_causal=true, Is_even_N=false, Is_even_K=false, Return_softmax=true, Params=Flash_fwd_params]" 
(588): here
            instantiation of "void flash::compute_attn<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &) [with Kernel_traits=Flash_fwd_kernel_traits<224, 64, 64, 4, false, false, cutlass::half_t, Flash_kernel_traits<224, 64, 64, 4, cutlass::half_t>>, Is_dropout=true, Is_causal=true, Is_even_N=false, Is_even_K=false, Return_softmax=true, Params=Flash_fwd_params]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(15): here
            instantiation of "void flash_fwd_kernel<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax>(Flash_fwd_params) [with Kernel_traits=Flash_fwd_kernel_traits<224, 64, 64, 4, false, false, cutlass::half_t, Flash_kernel_traits<224, 64, 64, 4, cutlass::half_t>>, Is_dropout=true, Is_causal=true, Is_even_N=false, Is_even_K=false, Return_softmax=true]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(34): here
            instantiation of "void run_flash_fwd<Kernel_traits,Is_dropout,Is_causal>(Flash_fwd_params &, cudaStream_t) [with Kernel_traits=Flash_fwd_kernel_traits<224, 64, 64, 4, false, false, cutlass::half_t, Flash_kernel_traits<224, 64, 64, 4, cutlass::half_t>>, Is_dropout=true, Is_causal=true]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(208): here
            instantiation of "void run_mha_fwd_hdim224<T>(Flash_fwd_params &, cudaStream_t) [with T=cutlass::half_t]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_hdim224_fp16_sm80.cu(8): here

/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_kernel.h(275): warning: variable "tQrQ" was declared but never referenced
          detected during:
            instantiation of "void flash::compute_attn_1rowblock<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &, int, int, int) [with Kernel_traits=Flash_fwd_kernel_traits<224, 64, 64, 4, false, false, cutlass::half_t, Flash_kernel_traits<224, 64, 64, 4, cutlass::half_t>>, Is_dropout=true, Is_causal=true, Is_even_N=false, Is_even_K=false, Return_softmax=true, Params=Flash_fwd_params]" 
(588): here
            instantiation of "void flash::compute_attn<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &) [with Kernel_traits=Flash_fwd_kernel_traits<224, 64, 64, 4, false, false, cutlass::half_t, Flash_kernel_traits<224, 64, 64, 4, cutlass::half_t>>, Is_dropout=true, Is_causal=true, Is_even_N=false, Is_even_K=false, Return_softmax=true, Params=Flash_fwd_params]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(15): here
            instantiation of "void flash_fwd_kernel<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax>(Flash_fwd_params) [with Kernel_traits=Flash_fwd_kernel_traits<224, 64, 64, 4, false, false, cutlass::half_t, Flash_kernel_traits<224, 64, 64, 4, cutlass::half_t>>, Is_dropout=true, Is_causal=true, Is_even_N=false, Is_even_K=false, Return_softmax=true]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(34): here
            instantiation of "void run_flash_fwd<Kernel_traits,Is_dropout,Is_causal>(Flash_fwd_params &, cudaStream_t) [with Kernel_traits=Flash_fwd_kernel_traits<224, 64, 64, 4, false, false, cutlass::half_t, Flash_kernel_traits<224, 64, 64, 4, cutlass::half_t>>, Is_dropout=true, Is_causal=true]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(208): here
            instantiation of "void run_mha_fwd_hdim224<T>(Flash_fwd_params &, cudaStream_t) [with T=cutlass::half_t]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_hdim224_fp16_sm80.cu(8): here

/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_kernel.h(140): warning: variable "MMA_M" was declared but never referenced
          detected during:
            instantiation of "void flash::compute_attn_1rowblock<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &, int, int, int) [with Kernel_traits=Flash_fwd_kernel_traits<224, 64, 64, 4, false, false, cutlass::half_t, Flash_kernel_traits<224, 64, 64, 4, cutlass::half_t>>, Is_dropout=true, Is_causal=true, Is_even_N=false, Is_even_K=false, Return_softmax=false, Params=Flash_fwd_params]" 
(588): here
            instantiation of "void flash::compute_attn<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &) [with Kernel_traits=Flash_fwd_kernel_traits<224, 64, 64, 4, false, false, cutlass::half_t, Flash_kernel_traits<224, 64, 64, 4, cutlass::half_t>>, Is_dropout=true, Is_causal=true, Is_even_N=false, Is_even_K=false, Return_softmax=false, Params=Flash_fwd_params]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(15): here
            instantiation of "void flash_fwd_kernel<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax>(Flash_fwd_params) [with Kernel_traits=Flash_fwd_kernel_traits<224, 64, 64, 4, false, false, cutlass::half_t, Flash_kernel_traits<224, 64, 64, 4, cutlass::half_t>>, Is_dropout=true, Is_causal=true, Is_even_N=false, Is_even_K=false, Return_softmax=false]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(34): here
            instantiation of "void run_flash_fwd<Kernel_traits,Is_dropout,Is_causal>(Flash_fwd_params &, cudaStream_t) [with Kernel_traits=Flash_fwd_kernel_traits<224, 64, 64, 4, false, false, cutlass::half_t, Flash_kernel_traits<224, 64, 64, 4, cutlass::half_t>>, Is_dropout=true, Is_causal=true]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(208): here
            instantiation of "void run_mha_fwd_hdim224<T>(Flash_fwd_params &, cudaStream_t) [with T=cutlass::half_t]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_hdim224_fp16_sm80.cu(8): here

/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_kernel.h(275): warning: variable "tQrQ" was declared but never referenced
          detected during:
            instantiation of "void flash::compute_attn_1rowblock<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &, int, int, int) [with Kernel_traits=Flash_fwd_kernel_traits<224, 64, 64, 4, false, false, cutlass::half_t, Flash_kernel_traits<224, 64, 64, 4, cutlass::half_t>>, Is_dropout=true, Is_causal=true, Is_even_N=false, Is_even_K=false, Return_softmax=false, Params=Flash_fwd_params]" 
(588): here
            instantiation of "void flash::compute_attn<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &) [with Kernel_traits=Flash_fwd_kernel_traits<224, 64, 64, 4, false, false, cutlass::half_t, Flash_kernel_traits<224, 64, 64, 4, cutlass::half_t>>, Is_dropout=true, Is_causal=true, Is_even_N=false, Is_even_K=false, Return_softmax=false, Params=Flash_fwd_params]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(15): here
            instantiation of "void flash_fwd_kernel<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax>(Flash_fwd_params) [with Kernel_traits=Flash_fwd_kernel_traits<224, 64, 64, 4, false, false, cutlass::half_t, Flash_kernel_traits<224, 64, 64, 4, cutlass::half_t>>, Is_dropout=true, Is_causal=true, Is_even_N=false, Is_even_K=false, Return_softmax=false]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(34): here
            instantiation of "void run_flash_fwd<Kernel_traits,Is_dropout,Is_causal>(Flash_fwd_params &, cudaStream_t) [with Kernel_traits=Flash_fwd_kernel_traits<224, 64, 64, 4, false, false, cutlass::half_t, Flash_kernel_traits<224, 64, 64, 4, cutlass::half_t>>, Is_dropout=true, Is_causal=true]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(208): here
            instantiation of "void run_mha_fwd_hdim224<T>(Flash_fwd_params &, cudaStream_t) [with T=cutlass::half_t]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_hdim224_fp16_sm80.cu(8): here

/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_kernel.h(140): warning: variable "MMA_M" was declared but never referenced
          detected during:
            instantiation of "void flash::compute_attn_1rowblock<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &, int, int, int) [with Kernel_traits=Flash_fwd_kernel_traits<224, 128, 64, 8, false, false, cutlass::half_t, Flash_kernel_traits<224, 128, 64, 8, cutlass::half_t>>, Is_dropout=true, Is_causal=false, Is_even_N=true, Is_even_K=true, Return_softmax=true, Params=Flash_fwd_params]" 
(588): here
            instantiation of "void flash::compute_attn<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &) [with Kernel_traits=Flash_fwd_kernel_traits<224, 128, 64, 8, false, false, cutlass::half_t, Flash_kernel_traits<224, 128, 64, 8, cutlass::half_t>>, Is_dropout=true, Is_causal=false, Is_even_N=true, Is_even_K=true, Return_softmax=true, Params=Flash_fwd_params]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(15): here
            instantiation of "void flash_fwd_kernel<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax>(Flash_fwd_params) [with Kernel_traits=Flash_fwd_kernel_traits<224, 128, 64, 8, false, false, cutlass::half_t, Flash_kernel_traits<224, 128, 64, 8, cutlass::half_t>>, Is_dropout=true, Is_causal=false, Is_even_N=true, Is_even_K=true, Return_softmax=true]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(34): here
            instantiation of "void run_flash_fwd<Kernel_traits,Is_dropout,Is_causal>(Flash_fwd_params &, cudaStream_t) [with Kernel_traits=Flash_fwd_kernel_traits<224, 128, 64, 8, false, false, cutlass::half_t, Flash_kernel_traits<224, 128, 64, 8, cutlass::half_t>>, Is_dropout=true, Is_causal=false]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(208): here
            instantiation of "void run_mha_fwd_hdim224<T>(Flash_fwd_params &, cudaStream_t) [with T=cutlass::half_t]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_hdim224_fp16_sm80.cu(8): here

/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_kernel.h(275): warning: variable "tQrQ" was declared but never referenced
          detected during:
            instantiation of "void flash::compute_attn_1rowblock<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &, int, int, int) [with Kernel_traits=Flash_fwd_kernel_traits<224, 128, 64, 8, false, false, cutlass::half_t, Flash_kernel_traits<224, 128, 64, 8, cutlass::half_t>>, Is_dropout=true, Is_causal=false, Is_even_N=true, Is_even_K=true, Return_softmax=true, Params=Flash_fwd_params]" 
(588): here
            instantiation of "void flash::compute_attn<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &) [with Kernel_traits=Flash_fwd_kernel_traits<224, 128, 64, 8, false, false, cutlass::half_t, Flash_kernel_traits<224, 128, 64, 8, cutlass::half_t>>, Is_dropout=true, Is_causal=false, Is_even_N=true, Is_even_K=true, Return_softmax=true, Params=Flash_fwd_params]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(15): here
            instantiation of "void flash_fwd_kernel<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax>(Flash_fwd_params) [with Kernel_traits=Flash_fwd_kernel_traits<224, 128, 64, 8, false, false, cutlass::half_t, Flash_kernel_traits<224, 128, 64, 8, cutlass::half_t>>, Is_dropout=true, Is_causal=false, Is_even_N=true, Is_even_K=true, Return_softmax=true]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(34): here
            instantiation of "void run_flash_fwd<Kernel_traits,Is_dropout,Is_causal>(Flash_fwd_params &, cudaStream_t) [with Kernel_traits=Flash_fwd_kernel_traits<224, 128, 64, 8, false, false, cutlass::half_t, Flash_kernel_traits<224, 128, 64, 8, cutlass::half_t>>, Is_dropout=true, Is_causal=false]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(208): here
            instantiation of "void run_mha_fwd_hdim224<T>(Flash_fwd_params &, cudaStream_t) [with T=cutlass::half_t]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_hdim224_fp16_sm80.cu(8): here

/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_kernel.h(140): warning: variable "MMA_M" was declared but never referenced
          detected during:
            instantiation of "void flash::compute_attn_1rowblock<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &, int, int, int) [with Kernel_traits=Flash_fwd_kernel_traits<224, 128, 64, 8, false, false, cutlass::half_t, Flash_kernel_traits<224, 128, 64, 8, cutlass::half_t>>, Is_dropout=true, Is_causal=false, Is_even_N=true, Is_even_K=true, Return_softmax=false, Params=Flash_fwd_params]" 
(588): here
            instantiation of "void flash::compute_attn<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &) [with Kernel_traits=Flash_fwd_kernel_traits<224, 128, 64, 8, false, false, cutlass::half_t, Flash_kernel_traits<224, 128, 64, 8, cutlass::half_t>>, Is_dropout=true, Is_causal=false, Is_even_N=true, Is_even_K=true, Return_softmax=false, Params=Flash_fwd_params]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(15): here
            instantiation of "void flash_fwd_kernel<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax>(Flash_fwd_params) [with Kernel_traits=Flash_fwd_kernel_traits<224, 128, 64, 8, false, false, cutlass::half_t, Flash_kernel_traits<224, 128, 64, 8, cutlass::half_t>>, Is_dropout=true, Is_causal=false, Is_even_N=true, Is_even_K=true, Return_softmax=false]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(34): here
            instantiation of "void run_flash_fwd<Kernel_traits,Is_dropout,Is_causal>(Flash_fwd_params &, cudaStream_t) [with Kernel_traits=Flash_fwd_kernel_traits<224, 128, 64, 8, false, false, cutlass::half_t, Flash_kernel_traits<224, 128, 64, 8, cutlass::half_t>>, Is_dropout=true, Is_causal=false]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(208): here
            instantiation of "void run_mha_fwd_hdim224<T>(Flash_fwd_params &, cudaStream_t) [with T=cutlass::half_t]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_hdim224_fp16_sm80.cu(8): here

/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_kernel.h(275): warning: variable "tQrQ" was declared but never referenced
          detected during:
            instantiation of "void flash::compute_attn_1rowblock<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &, int, int, int) [with Kernel_traits=Flash_fwd_kernel_traits<224, 128, 64, 8, false, false, cutlass::half_t, Flash_kernel_traits<224, 128, 64, 8, cutlass::half_t>>, Is_dropout=true, Is_causal=false, Is_even_N=true, Is_even_K=true, Return_softmax=false, Params=Flash_fwd_params]" 
(588): here
            instantiation of "void flash::compute_attn<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &) [with Kernel_traits=Flash_fwd_kernel_traits<224, 128, 64, 8, false, false, cutlass::half_t, Flash_kernel_traits<224, 128, 64, 8, cutlass::half_t>>, Is_dropout=true, Is_causal=false, Is_even_N=true, Is_even_K=true, Return_softmax=false, Params=Flash_fwd_params]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(15): here
            instantiation of "void flash_fwd_kernel<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax>(Flash_fwd_params) [with Kernel_traits=Flash_fwd_kernel_traits<224, 128, 64, 8, false, false, cutlass::half_t, Flash_kernel_traits<224, 128, 64, 8, cutlass::half_t>>, Is_dropout=true, Is_causal=false, Is_even_N=true, Is_even_K=true, Return_softmax=false]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(34): here
            instantiation of "void run_flash_fwd<Kernel_traits,Is_dropout,Is_causal>(Flash_fwd_params &, cudaStream_t) [with Kernel_traits=Flash_fwd_kernel_traits<224, 128, 64, 8, false, false, cutlass::half_t, Flash_kernel_traits<224, 128, 64, 8, cutlass::half_t>>, Is_dropout=true, Is_causal=false]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(208): here
            instantiation of "void run_mha_fwd_hdim224<T>(Flash_fwd_params &, cudaStream_t) [with T=cutlass::half_t]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_hdim224_fp16_sm80.cu(8): here

/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_kernel.h(140): warning: variable "MMA_M" was declared but never referenced
          detected during:
            instantiation of "void flash::compute_attn_1rowblock<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &, int, int, int) [with Kernel_traits=Flash_fwd_kernel_traits<224, 128, 64, 8, false, false, cutlass::half_t, Flash_kernel_traits<224, 128, 64, 8, cutlass::half_t>>, Is_dropout=true, Is_causal=false, Is_even_N=true, Is_even_K=false, Return_softmax=true, Params=Flash_fwd_params]" 
(588): here
            instantiation of "void flash::compute_attn<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &) [with Kernel_traits=Flash_fwd_kernel_traits<224, 128, 64, 8, false, false, cutlass::half_t, Flash_kernel_traits<224, 128, 64, 8, cutlass::half_t>>, Is_dropout=true, Is_causal=false, Is_even_N=true, Is_even_K=false, Return_softmax=true, Params=Flash_fwd_params]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(15): here
            instantiation of "void flash_fwd_kernel<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax>(Flash_fwd_params) [with Kernel_traits=Flash_fwd_kernel_traits<224, 128, 64, 8, false, false, cutlass::half_t, Flash_kernel_traits<224, 128, 64, 8, cutlass::half_t>>, Is_dropout=true, Is_causal=false, Is_even_N=true, Is_even_K=false, Return_softmax=true]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(34): here
            instantiation of "void run_flash_fwd<Kernel_traits,Is_dropout,Is_causal>(Flash_fwd_params &, cudaStream_t) [with Kernel_traits=Flash_fwd_kernel_traits<224, 128, 64, 8, false, false, cutlass::half_t, Flash_kernel_traits<224, 128, 64, 8, cutlass::half_t>>, Is_dropout=true, Is_causal=false]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(208): here
            instantiation of "void run_mha_fwd_hdim224<T>(Flash_fwd_params &, cudaStream_t) [with T=cutlass::half_t]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_hdim224_fp16_sm80.cu(8): here

/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_kernel.h(275): warning: variable "tQrQ" was declared but never referenced
          detected during:
            instantiation of "void flash::compute_attn_1rowblock<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &, int, int, int) [with Kernel_traits=Flash_fwd_kernel_traits<224, 128, 64, 8, false, false, cutlass::half_t, Flash_kernel_traits<224, 128, 64, 8, cutlass::half_t>>, Is_dropout=true, Is_causal=false, Is_even_N=true, Is_even_K=false, Return_softmax=true, Params=Flash_fwd_params]" 
(588): here
            instantiation of "void flash::compute_attn<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &) [with Kernel_traits=Flash_fwd_kernel_traits<224, 128, 64, 8, false, false, cutlass::half_t, Flash_kernel_traits<224, 128, 64, 8, cutlass::half_t>>, Is_dropout=true, Is_causal=false, Is_even_N=true, Is_even_K=false, Return_softmax=true, Params=Flash_fwd_params]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(15): here
            instantiation of "void flash_fwd_kernel<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax>(Flash_fwd_params) [with Kernel_traits=Flash_fwd_kernel_traits<224, 128, 64, 8, false, false, cutlass::half_t, Flash_kernel_traits<224, 128, 64, 8, cutlass::half_t>>, Is_dropout=true, Is_causal=false, Is_even_N=true, Is_even_K=false, Return_softmax=true]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(34): here
            instantiation of "void run_flash_fwd<Kernel_traits,Is_dropout,Is_causal>(Flash_fwd_params &, cudaStream_t) [with Kernel_traits=Flash_fwd_kernel_traits<224, 128, 64, 8, false, false, cutlass::half_t, Flash_kernel_traits<224, 128, 64, 8, cutlass::half_t>>, Is_dropout=true, Is_causal=false]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(208): here
            instantiation of "void run_mha_fwd_hdim224<T>(Flash_fwd_params &, cudaStream_t) [with T=cutlass::half_t]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_hdim224_fp16_sm80.cu(8): here

/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_kernel.h(140): warning: variable "MMA_M" was declared but never referenced
          detected during:
            instantiation of "void flash::compute_attn_1rowblock<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &, int, int, int) [with Kernel_traits=Flash_fwd_kernel_traits<224, 128, 64, 8, false, false, cutlass::half_t, Flash_kernel_traits<224, 128, 64, 8, cutlass::half_t>>, Is_dropout=true, Is_causal=false, Is_even_N=true, Is_even_K=false, Return_softmax=false, Params=Flash_fwd_params]" 
(588): here
            instantiation of "void flash::compute_attn<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &) [with Kernel_traits=Flash_fwd_kernel_traits<224, 128, 64, 8, false, false, cutlass::half_t, Flash_kernel_traits<224, 128, 64, 8, cutlass::half_t>>, Is_dropout=true, Is_causal=false, Is_even_N=true, Is_even_K=false, Return_softmax=false, Params=Flash_fwd_params]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(15): here
            instantiation of "void flash_fwd_kernel<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax>(Flash_fwd_params) [with Kernel_traits=Flash_fwd_kernel_traits<224, 128, 64, 8, false, false, cutlass::half_t, Flash_kernel_traits<224, 128, 64, 8, cutlass::half_t>>, Is_dropout=true, Is_causal=false, Is_even_N=true, Is_even_K=false, Return_softmax=false]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(34): here
            instantiation of "void run_flash_fwd<Kernel_traits,Is_dropout,Is_causal>(Flash_fwd_params &, cudaStream_t) [with Kernel_traits=Flash_fwd_kernel_traits<224, 128, 64, 8, false, false, cutlass::half_t, Flash_kernel_traits<224, 128, 64, 8, cutlass::half_t>>, Is_dropout=true, Is_causal=false]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(208): here
            instantiation of "void run_mha_fwd_hdim224<T>(Flash_fwd_params &, cudaStream_t) [with T=cutlass::half_t]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_hdim224_fp16_sm80.cu(8): here

/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_kernel.h(275): warning: variable "tQrQ" was declared but never referenced
          detected during:
            instantiation of "void flash::compute_attn_1rowblock<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &, int, int, int) [with Kernel_traits=Flash_fwd_kernel_traits<224, 128, 64, 8, false, false, cutlass::half_t, Flash_kernel_traits<224, 128, 64, 8, cutlass::half_t>>, Is_dropout=true, Is_causal=false, Is_even_N=true, Is_even_K=false, Return_softmax=false, Params=Flash_fwd_params]" 
(588): here
            instantiation of "void flash::compute_attn<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &) [with Kernel_traits=Flash_fwd_kernel_traits<224, 128, 64, 8, false, false, cutlass::half_t, Flash_kernel_traits<224, 128, 64, 8, cutlass::half_t>>, Is_dropout=true, Is_causal=false, Is_even_N=true, Is_even_K=false, Return_softmax=false, Params=Flash_fwd_params]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(15): here
            instantiation of "void flash_fwd_kernel<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax>(Flash_fwd_params) [with Kernel_traits=Flash_fwd_kernel_traits<224, 128, 64, 8, false, false, cutlass::half_t, Flash_kernel_traits<224, 128, 64, 8, cutlass::half_t>>, Is_dropout=true, Is_causal=false, Is_even_N=true, Is_even_K=false, Return_softmax=false]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(34): here
            instantiation of "void run_flash_fwd<Kernel_traits,Is_dropout,Is_causal>(Flash_fwd_params &, cudaStream_t) [with Kernel_traits=Flash_fwd_kernel_traits<224, 128, 64, 8, false, false, cutlass::half_t, Flash_kernel_traits<224, 128, 64, 8, cutlass::half_t>>, Is_dropout=true, Is_causal=false]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(208): here
            instantiation of "void run_mha_fwd_hdim224<T>(Flash_fwd_params &, cudaStream_t) [with T=cutlass::half_t]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_hdim224_fp16_sm80.cu(8): here

/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_kernel.h(140): warning: variable "MMA_M" was declared but never referenced
          detected during:
            instantiation of "void flash::compute_attn_1rowblock<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &, int, int, int) [with Kernel_traits=Flash_fwd_kernel_traits<224, 128, 64, 8, false, false, cutlass::half_t, Flash_kernel_traits<224, 128, 64, 8, cutlass::half_t>>, Is_dropout=true, Is_causal=false, Is_even_N=false, Is_even_K=true, Return_softmax=true, Params=Flash_fwd_params]" 
(588): here
            instantiation of "void flash::compute_attn<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &) [with Kernel_traits=Flash_fwd_kernel_traits<224, 128, 64, 8, false, false, cutlass::half_t, Flash_kernel_traits<224, 128, 64, 8, cutlass::half_t>>, Is_dropout=true, Is_causal=false, Is_even_N=false, Is_even_K=true, Return_softmax=true, Params=Flash_fwd_params]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(15): here
            instantiation of "void flash_fwd_kernel<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax>(Flash_fwd_params) [with Kernel_traits=Flash_fwd_kernel_traits<224, 128, 64, 8, false, false, cutlass::half_t, Flash_kernel_traits<224, 128, 64, 8, cutlass::half_t>>, Is_dropout=true, Is_causal=false, Is_even_N=false, Is_even_K=true, Return_softmax=true]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(34): here
            instantiation of "void run_flash_fwd<Kernel_traits,Is_dropout,Is_causal>(Flash_fwd_params &, cudaStream_t) [with Kernel_traits=Flash_fwd_kernel_traits<224, 128, 64, 8, false, false, cutlass::half_t, Flash_kernel_traits<224, 128, 64, 8, cutlass::half_t>>, Is_dropout=true, Is_causal=false]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(208): here
            instantiation of "void run_mha_fwd_hdim224<T>(Flash_fwd_params &, cudaStream_t) [with T=cutlass::half_t]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_hdim224_fp16_sm80.cu(8): here

/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_kernel.h(275): warning: variable "tQrQ" was declared but never referenced
          detected during:
            instantiation of "void flash::compute_attn_1rowblock<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &, int, int, int) [with Kernel_traits=Flash_fwd_kernel_traits<224, 128, 64, 8, false, false, cutlass::half_t, Flash_kernel_traits<224, 128, 64, 8, cutlass::half_t>>, Is_dropout=true, Is_causal=false, Is_even_N=false, Is_even_K=true, Return_softmax=true, Params=Flash_fwd_params]" 
(588): here
            instantiation of "void flash::compute_attn<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &) [with Kernel_traits=Flash_fwd_kernel_traits<224, 128, 64, 8, false, false, cutlass::half_t, Flash_kernel_traits<224, 128, 64, 8, cutlass::half_t>>, Is_dropout=true, Is_causal=false, Is_even_N=false, Is_even_K=true, Return_softmax=true, Params=Flash_fwd_params]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(15): here
            instantiation of "void flash_fwd_kernel<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax>(Flash_fwd_params) [with Kernel_traits=Flash_fwd_kernel_traits<224, 128, 64, 8, false, false, cutlass::half_t, Flash_kernel_traits<224, 128, 64, 8, cutlass::half_t>>, Is_dropout=true, Is_causal=false, Is_even_N=false, Is_even_K=true, Return_softmax=true]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(34): here
            instantiation of "void run_flash_fwd<Kernel_traits,Is_dropout,Is_causal>(Flash_fwd_params &, cudaStream_t) [with Kernel_traits=Flash_fwd_kernel_traits<224, 128, 64, 8, false, false, cutlass::half_t, Flash_kernel_traits<224, 128, 64, 8, cutlass::half_t>>, Is_dropout=true, Is_causal=false]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(208): here
            instantiation of "void run_mha_fwd_hdim224<T>(Flash_fwd_params &, cudaStream_t) [with T=cutlass::half_t]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_hdim224_fp16_sm80.cu(8): here

/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_kernel.h(140): warning: variable "MMA_M" was declared but never referenced
          detected during:
            instantiation of "void flash::compute_attn_1rowblock<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &, int, int, int) [with Kernel_traits=Flash_fwd_kernel_traits<224, 128, 64, 8, false, false, cutlass::half_t, Flash_kernel_traits<224, 128, 64, 8, cutlass::half_t>>, Is_dropout=true, Is_causal=false, Is_even_N=false, Is_even_K=true, Return_softmax=false, Params=Flash_fwd_params]" 
(588): here
            instantiation of "void flash::compute_attn<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &) [with Kernel_traits=Flash_fwd_kernel_traits<224, 128, 64, 8, false, false, cutlass::half_t, Flash_kernel_traits<224, 128, 64, 8, cutlass::half_t>>, Is_dropout=true, Is_causal=false, Is_even_N=false, Is_even_K=true, Return_softmax=false, Params=Flash_fwd_params]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(15): here
            instantiation of "void flash_fwd_kernel<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax>(Flash_fwd_params) [with Kernel_traits=Flash_fwd_kernel_traits<224, 128, 64, 8, false, false, cutlass::half_t, Flash_kernel_traits<224, 128, 64, 8, cutlass::half_t>>, Is_dropout=true, Is_causal=false, Is_even_N=false, Is_even_K=true, Return_softmax=false]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(34): here
            instantiation of "void run_flash_fwd<Kernel_traits,Is_dropout,Is_causal>(Flash_fwd_params &, cudaStream_t) [with Kernel_traits=Flash_fwd_kernel_traits<224, 128, 64, 8, false, false, cutlass::half_t, Flash_kernel_traits<224, 128, 64, 8, cutlass::half_t>>, Is_dropout=true, Is_causal=false]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(208): here
            instantiation of "void run_mha_fwd_hdim224<T>(Flash_fwd_params &, cudaStream_t) [with T=cutlass::half_t]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_hdim224_fp16_sm80.cu(8): here

/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_kernel.h(275): warning: variable "tQrQ" was declared but never referenced
          detected during:
            instantiation of "void flash::compute_attn_1rowblock<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &, int, int, int) [with Kernel_traits=Flash_fwd_kernel_traits<224, 128, 64, 8, false, false, cutlass::half_t, Flash_kernel_traits<224, 128, 64, 8, cutlass::half_t>>, Is_dropout=true, Is_causal=false, Is_even_N=false, Is_even_K=true, Return_softmax=false, Params=Flash_fwd_params]" 
(588): here
            instantiation of "void flash::compute_attn<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &) [with Kernel_traits=Flash_fwd_kernel_traits<224, 128, 64, 8, false, false, cutlass::half_t, Flash_kernel_traits<224, 128, 64, 8, cutlass::half_t>>, Is_dropout=true, Is_causal=false, Is_even_N=false, Is_even_K=true, Return_softmax=false, Params=Flash_fwd_params]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(15): here
            instantiation of "void flash_fwd_kernel<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax>(Flash_fwd_params) [with Kernel_traits=Flash_fwd_kernel_traits<224, 128, 64, 8, false, false, cutlass::half_t, Flash_kernel_traits<224, 128, 64, 8, cutlass::half_t>>, Is_dropout=true, Is_causal=false, Is_even_N=false, Is_even_K=true, Return_softmax=false]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(34): here
            instantiation of "void run_flash_fwd<Kernel_traits,Is_dropout,Is_causal>(Flash_fwd_params &, cudaStream_t) [with Kernel_traits=Flash_fwd_kernel_traits<224, 128, 64, 8, false, false, cutlass::half_t, Flash_kernel_traits<224, 128, 64, 8, cutlass::half_t>>, Is_dropout=true, Is_causal=false]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(208): here
            instantiation of "void run_mha_fwd_hdim224<T>(Flash_fwd_params &, cudaStream_t) [with T=cutlass::half_t]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_hdim224_fp16_sm80.cu(8): here

/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_kernel.h(140): warning: variable "MMA_M" was declared but never referenced
          detected during:
            instantiation of "void flash::compute_attn_1rowblock<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &, int, int, int) [with Kernel_traits=Flash_fwd_kernel_traits<224, 128, 64, 8, false, false, cutlass::half_t, Flash_kernel_traits<224, 128, 64, 8, cutlass::half_t>>, Is_dropout=true, Is_causal=false, Is_even_N=false, Is_even_K=false, Return_softmax=true, Params=Flash_fwd_params]" 
(588): here
            instantiation of "void flash::compute_attn<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &) [with Kernel_traits=Flash_fwd_kernel_traits<224, 128, 64, 8, false, false, cutlass::half_t, Flash_kernel_traits<224, 128, 64, 8, cutlass::half_t>>, Is_dropout=true, Is_causal=false, Is_even_N=false, Is_even_K=false, Return_softmax=true, Params=Flash_fwd_params]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(15): here
            instantiation of "void flash_fwd_kernel<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax>(Flash_fwd_params) [with Kernel_traits=Flash_fwd_kernel_traits<224, 128, 64, 8, false, false, cutlass::half_t, Flash_kernel_traits<224, 128, 64, 8, cutlass::half_t>>, Is_dropout=true, Is_causal=false, Is_even_N=false, Is_even_K=false, Return_softmax=true]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(34): here
            instantiation of "void run_flash_fwd<Kernel_traits,Is_dropout,Is_causal>(Flash_fwd_params &, cudaStream_t) [with Kernel_traits=Flash_fwd_kernel_traits<224, 128, 64, 8, false, false, cutlass::half_t, Flash_kernel_traits<224, 128, 64, 8, cutlass::half_t>>, Is_dropout=true, Is_causal=false]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(208): here
            instantiation of "void run_mha_fwd_hdim224<T>(Flash_fwd_params &, cudaStream_t) [with T=cutlass::half_t]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_hdim224_fp16_sm80.cu(8): here

/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_kernel.h(275): warning: variable "tQrQ" was declared but never referenced
          detected during:
            instantiation of "void flash::compute_attn_1rowblock<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &, int, int, int) [with Kernel_traits=Flash_fwd_kernel_traits<224, 128, 64, 8, false, false, cutlass::half_t, Flash_kernel_traits<224, 128, 64, 8, cutlass::half_t>>, Is_dropout=true, Is_causal=false, Is_even_N=false, Is_even_K=false, Return_softmax=true, Params=Flash_fwd_params]" 
(588): here
            instantiation of "void flash::compute_attn<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &) [with Kernel_traits=Flash_fwd_kernel_traits<224, 128, 64, 8, false, false, cutlass::half_t, Flash_kernel_traits<224, 128, 64, 8, cutlass::half_t>>, Is_dropout=true, Is_causal=false, Is_even_N=false, Is_even_K=false, Return_softmax=true, Params=Flash_fwd_params]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(15): here
            instantiation of "void flash_fwd_kernel<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax>(Flash_fwd_params) [with Kernel_traits=Flash_fwd_kernel_traits<224, 128, 64, 8, false, false, cutlass::half_t, Flash_kernel_traits<224, 128, 64, 8, cutlass::half_t>>, Is_dropout=true, Is_causal=false, Is_even_N=false, Is_even_K=false, Return_softmax=true]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(34): here
            instantiation of "void run_flash_fwd<Kernel_traits,Is_dropout,Is_causal>(Flash_fwd_params &, cudaStream_t) [with Kernel_traits=Flash_fwd_kernel_traits<224, 128, 64, 8, false, false, cutlass::half_t, Flash_kernel_traits<224, 128, 64, 8, cutlass::half_t>>, Is_dropout=true, Is_causal=false]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(208): here
            instantiation of "void run_mha_fwd_hdim224<T>(Flash_fwd_params &, cudaStream_t) [with T=cutlass::half_t]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_hdim224_fp16_sm80.cu(8): here

/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_kernel.h(140): warning: variable "MMA_M" was declared but never referenced
          detected during:
            instantiation of "void flash::compute_attn_1rowblock<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &, int, int, int) [with Kernel_traits=Flash_fwd_kernel_traits<224, 128, 64, 8, false, false, cutlass::half_t, Flash_kernel_traits<224, 128, 64, 8, cutlass::half_t>>, Is_dropout=true, Is_causal=false, Is_even_N=false, Is_even_K=false, Return_softmax=false, Params=Flash_fwd_params]" 
(588): here
            instantiation of "void flash::compute_attn<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &) [with Kernel_traits=Flash_fwd_kernel_traits<224, 128, 64, 8, false, false, cutlass::half_t, Flash_kernel_traits<224, 128, 64, 8, cutlass::half_t>>, Is_dropout=true, Is_causal=false, Is_even_N=false, Is_even_K=false, Return_softmax=false, Params=Flash_fwd_params]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(15): here
            instantiation of "void flash_fwd_kernel<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax>(Flash_fwd_params) [with Kernel_traits=Flash_fwd_kernel_traits<224, 128, 64, 8, false, false, cutlass::half_t, Flash_kernel_traits<224, 128, 64, 8, cutlass::half_t>>, Is_dropout=true, Is_causal=false, Is_even_N=false, Is_even_K=false, Return_softmax=false]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(34): here
            instantiation of "void run_flash_fwd<Kernel_traits,Is_dropout,Is_causal>(Flash_fwd_params &, cudaStream_t) [with Kernel_traits=Flash_fwd_kernel_traits<224, 128, 64, 8, false, false, cutlass::half_t, Flash_kernel_traits<224, 128, 64, 8, cutlass::half_t>>, Is_dropout=true, Is_causal=false]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(208): here
            instantiation of "void run_mha_fwd_hdim224<T>(Flash_fwd_params &, cudaStream_t) [with T=cutlass::half_t]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_hdim224_fp16_sm80.cu(8): here

/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_kernel.h(275): warning: variable "tQrQ" was declared but never referenced
          detected during:
            instantiation of "void flash::compute_attn_1rowblock<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &, int, int, int) [with Kernel_traits=Flash_fwd_kernel_traits<224, 128, 64, 8, false, false, cutlass::half_t, Flash_kernel_traits<224, 128, 64, 8, cutlass::half_t>>, Is_dropout=true, Is_causal=false, Is_even_N=false, Is_even_K=false, Return_softmax=false, Params=Flash_fwd_params]" 
(588): here
            instantiation of "void flash::compute_attn<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &) [with Kernel_traits=Flash_fwd_kernel_traits<224, 128, 64, 8, false, false, cutlass::half_t, Flash_kernel_traits<224, 128, 64, 8, cutlass::half_t>>, Is_dropout=true, Is_causal=false, Is_even_N=false, Is_even_K=false, Return_softmax=false, Params=Flash_fwd_params]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(15): here
            instantiation of "void flash_fwd_kernel<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax>(Flash_fwd_params) [with Kernel_traits=Flash_fwd_kernel_traits<224, 128, 64, 8, false, false, cutlass::half_t, Flash_kernel_traits<224, 128, 64, 8, cutlass::half_t>>, Is_dropout=true, Is_causal=false, Is_even_N=false, Is_even_K=false, Return_softmax=false]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(34): here
            instantiation of "void run_flash_fwd<Kernel_traits,Is_dropout,Is_causal>(Flash_fwd_params &, cudaStream_t) [with Kernel_traits=Flash_fwd_kernel_traits<224, 128, 64, 8, false, false, cutlass::half_t, Flash_kernel_traits<224, 128, 64, 8, cutlass::half_t>>, Is_dropout=true, Is_causal=false]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(208): here
            instantiation of "void run_mha_fwd_hdim224<T>(Flash_fwd_params &, cudaStream_t) [with T=cutlass::half_t]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_hdim224_fp16_sm80.cu(8): here

/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_kernel.h(140): warning: variable "MMA_M" was declared but never referenced
          detected during:
            instantiation of "void flash::compute_attn_1rowblock<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &, int, int, int) [with Kernel_traits=Flash_fwd_kernel_traits<224, 64, 64, 4, false, false, cutlass::half_t, Flash_kernel_traits<224, 64, 64, 4, cutlass::half_t>>, Is_dropout=true, Is_causal=false, Is_even_N=true, Is_even_K=true, Return_softmax=true, Params=Flash_fwd_params]" 
(588): here
            instantiation of "void flash::compute_attn<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &) [with Kernel_traits=Flash_fwd_kernel_traits<224, 64, 64, 4, false, false, cutlass::half_t, Flash_kernel_traits<224, 64, 64, 4, cutlass::half_t>>, Is_dropout=true, Is_causal=false, Is_even_N=true, Is_even_K=true, Return_softmax=true, Params=Flash_fwd_params]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(15): here
            instantiation of "void flash_fwd_kernel<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax>(Flash_fwd_params) [with Kernel_traits=Flash_fwd_kernel_traits<224, 64, 64, 4, false, false, cutlass::half_t, Flash_kernel_traits<224, 64, 64, 4, cutlass::half_t>>, Is_dropout=true, Is_causal=false, Is_even_N=true, Is_even_K=true, Return_softmax=true]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(34): here
            instantiation of "void run_flash_fwd<Kernel_traits,Is_dropout,Is_causal>(Flash_fwd_params &, cudaStream_t) [with Kernel_traits=Flash_fwd_kernel_traits<224, 64, 64, 4, false, false, cutlass::half_t, Flash_kernel_traits<224, 64, 64, 4, cutlass::half_t>>, Is_dropout=true, Is_causal=false]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(208): here
            instantiation of "void run_mha_fwd_hdim224<T>(Flash_fwd_params &, cudaStream_t) [with T=cutlass::half_t]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_hdim224_fp16_sm80.cu(8): here

/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_kernel.h(275): warning: variable "tQrQ" was declared but never referenced
          detected during:
            instantiation of "void flash::compute_attn_1rowblock<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &, int, int, int) [with Kernel_traits=Flash_fwd_kernel_traits<224, 64, 64, 4, false, false, cutlass::half_t, Flash_kernel_traits<224, 64, 64, 4, cutlass::half_t>>, Is_dropout=true, Is_causal=false, Is_even_N=true, Is_even_K=true, Return_softmax=true, Params=Flash_fwd_params]" 
(588): here
            instantiation of "void flash::compute_attn<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &) [with Kernel_traits=Flash_fwd_kernel_traits<224, 64, 64, 4, false, false, cutlass::half_t, Flash_kernel_traits<224, 64, 64, 4, cutlass::half_t>>, Is_dropout=true, Is_causal=false, Is_even_N=true, Is_even_K=true, Return_softmax=true, Params=Flash_fwd_params]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(15): here
            instantiation of "void flash_fwd_kernel<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax>(Flash_fwd_params) [with Kernel_traits=Flash_fwd_kernel_traits<224, 64, 64, 4, false, false, cutlass::half_t, Flash_kernel_traits<224, 64, 64, 4, cutlass::half_t>>, Is_dropout=true, Is_causal=false, Is_even_N=true, Is_even_K=true, Return_softmax=true]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(34): here
            instantiation of "void run_flash_fwd<Kernel_traits,Is_dropout,Is_causal>(Flash_fwd_params &, cudaStream_t) [with Kernel_traits=Flash_fwd_kernel_traits<224, 64, 64, 4, false, false, cutlass::half_t, Flash_kernel_traits<224, 64, 64, 4, cutlass::half_t>>, Is_dropout=true, Is_causal=false]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(208): here
            instantiation of "void run_mha_fwd_hdim224<T>(Flash_fwd_params &, cudaStream_t) [with T=cutlass::half_t]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_hdim224_fp16_sm80.cu(8): here

/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_kernel.h(140): warning: variable "MMA_M" was declared but never referenced
          detected during:
            instantiation of "void flash::compute_attn_1rowblock<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &, int, int, int) [with Kernel_traits=Flash_fwd_kernel_traits<224, 64, 64, 4, false, false, cutlass::half_t, Flash_kernel_traits<224, 64, 64, 4, cutlass::half_t>>, Is_dropout=true, Is_causal=false, Is_even_N=true, Is_even_K=true, Return_softmax=false, Params=Flash_fwd_params]" 
(588): here
            instantiation of "void flash::compute_attn<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &) [with Kernel_traits=Flash_fwd_kernel_traits<224, 64, 64, 4, false, false, cutlass::half_t, Flash_kernel_traits<224, 64, 64, 4, cutlass::half_t>>, Is_dropout=true, Is_causal=false, Is_even_N=true, Is_even_K=true, Return_softmax=false, Params=Flash_fwd_params]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(15): here
            instantiation of "void flash_fwd_kernel<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax>(Flash_fwd_params) [with Kernel_traits=Flash_fwd_kernel_traits<224, 64, 64, 4, false, false, cutlass::half_t, Flash_kernel_traits<224, 64, 64, 4, cutlass::half_t>>, Is_dropout=true, Is_causal=false, Is_even_N=true, Is_even_K=true, Return_softmax=false]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(34): here
            instantiation of "void run_flash_fwd<Kernel_traits,Is_dropout,Is_causal>(Flash_fwd_params &, cudaStream_t) [with Kernel_traits=Flash_fwd_kernel_traits<224, 64, 64, 4, false, false, cutlass::half_t, Flash_kernel_traits<224, 64, 64, 4, cutlass::half_t>>, Is_dropout=true, Is_causal=false]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(208): here
            instantiation of "void run_mha_fwd_hdim224<T>(Flash_fwd_params &, cudaStream_t) [with T=cutlass::half_t]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_hdim224_fp16_sm80.cu(8): here

/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_kernel.h(275): warning: variable "tQrQ" was declared but never referenced
          detected during:
            instantiation of "void flash::compute_attn_1rowblock<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &, int, int, int) [with Kernel_traits=Flash_fwd_kernel_traits<224, 64, 64, 4, false, false, cutlass::half_t, Flash_kernel_traits<224, 64, 64, 4, cutlass::half_t>>, Is_dropout=true, Is_causal=false, Is_even_N=true, Is_even_K=true, Return_softmax=false, Params=Flash_fwd_params]" 
(588): here
            instantiation of "void flash::compute_attn<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &) [with Kernel_traits=Flash_fwd_kernel_traits<224, 64, 64, 4, false, false, cutlass::half_t, Flash_kernel_traits<224, 64, 64, 4, cutlass::half_t>>, Is_dropout=true, Is_causal=false, Is_even_N=true, Is_even_K=true, Return_softmax=false, Params=Flash_fwd_params]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(15): here
            instantiation of "void flash_fwd_kernel<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax>(Flash_fwd_params) [with Kernel_traits=Flash_fwd_kernel_traits<224, 64, 64, 4, false, false, cutlass::half_t, Flash_kernel_traits<224, 64, 64, 4, cutlass::half_t>>, Is_dropout=true, Is_causal=false, Is_even_N=true, Is_even_K=true, Return_softmax=false]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(34): here
            instantiation of "void run_flash_fwd<Kernel_traits,Is_dropout,Is_causal>(Flash_fwd_params &, cudaStream_t) [with Kernel_traits=Flash_fwd_kernel_traits<224, 64, 64, 4, false, false, cutlass::half_t, Flash_kernel_traits<224, 64, 64, 4, cutlass::half_t>>, Is_dropout=true, Is_causal=false]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(208): here
            instantiation of "void run_mha_fwd_hdim224<T>(Flash_fwd_params &, cudaStream_t) [with T=cutlass::half_t]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_hdim224_fp16_sm80.cu(8): here

/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_kernel.h(140): warning: variable "MMA_M" was declared but never referenced
          detected during:
            instantiation of "void flash::compute_attn_1rowblock<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &, int, int, int) [with Kernel_traits=Flash_fwd_kernel_traits<224, 64, 64, 4, false, false, cutlass::half_t, Flash_kernel_traits<224, 64, 64, 4, cutlass::half_t>>, Is_dropout=true, Is_causal=false, Is_even_N=true, Is_even_K=false, Return_softmax=true, Params=Flash_fwd_params]" 
(588): here
            instantiation of "void flash::compute_attn<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &) [with Kernel_traits=Flash_fwd_kernel_traits<224, 64, 64, 4, false, false, cutlass::half_t, Flash_kernel_traits<224, 64, 64, 4, cutlass::half_t>>, Is_dropout=true, Is_causal=false, Is_even_N=true, Is_even_K=false, Return_softmax=true, Params=Flash_fwd_params]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(15): here
            instantiation of "void flash_fwd_kernel<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax>(Flash_fwd_params) [with Kernel_traits=Flash_fwd_kernel_traits<224, 64, 64, 4, false, false, cutlass::half_t, Flash_kernel_traits<224, 64, 64, 4, cutlass::half_t>>, Is_dropout=true, Is_causal=false, Is_even_N=true, Is_even_K=false, Return_softmax=true]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(34): here
            instantiation of "void run_flash_fwd<Kernel_traits,Is_dropout,Is_causal>(Flash_fwd_params &, cudaStream_t) [with Kernel_traits=Flash_fwd_kernel_traits<224, 64, 64, 4, false, false, cutlass::half_t, Flash_kernel_traits<224, 64, 64, 4, cutlass::half_t>>, Is_dropout=true, Is_causal=false]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(208): here
            instantiation of "void run_mha_fwd_hdim224<T>(Flash_fwd_params &, cudaStream_t) [with T=cutlass::half_t]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_hdim224_fp16_sm80.cu(8): here

/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_kernel.h(275): warning: variable "tQrQ" was declared but never referenced
          detected during:
            instantiation of "void flash::compute_attn_1rowblock<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &, int, int, int) [with Kernel_traits=Flash_fwd_kernel_traits<224, 64, 64, 4, false, false, cutlass::half_t, Flash_kernel_traits<224, 64, 64, 4, cutlass::half_t>>, Is_dropout=true, Is_causal=false, Is_even_N=true, Is_even_K=false, Return_softmax=true, Params=Flash_fwd_params]" 
(588): here
            instantiation of "void flash::compute_attn<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &) [with Kernel_traits=Flash_fwd_kernel_traits<224, 64, 64, 4, false, false, cutlass::half_t, Flash_kernel_traits<224, 64, 64, 4, cutlass::half_t>>, Is_dropout=true, Is_causal=false, Is_even_N=true, Is_even_K=false, Return_softmax=true, Params=Flash_fwd_params]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(15): here
            instantiation of "void flash_fwd_kernel<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax>(Flash_fwd_params) [with Kernel_traits=Flash_fwd_kernel_traits<224, 64, 64, 4, false, false, cutlass::half_t, Flash_kernel_traits<224, 64, 64, 4, cutlass::half_t>>, Is_dropout=true, Is_causal=false, Is_even_N=true, Is_even_K=false, Return_softmax=true]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(34): here
            instantiation of "void run_flash_fwd<Kernel_traits,Is_dropout,Is_causal>(Flash_fwd_params &, cudaStream_t) [with Kernel_traits=Flash_fwd_kernel_traits<224, 64, 64, 4, false, false, cutlass::half_t, Flash_kernel_traits<224, 64, 64, 4, cutlass::half_t>>, Is_dropout=true, Is_causal=false]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(208): here
            instantiation of "void run_mha_fwd_hdim224<T>(Flash_fwd_params &, cudaStream_t) [with T=cutlass::half_t]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_hdim224_fp16_sm80.cu(8): here

/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_kernel.h(140): warning: variable "MMA_M" was declared but never referenced
          detected during:
            instantiation of "void flash::compute_attn_1rowblock<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &, int, int, int) [with Kernel_traits=Flash_fwd_kernel_traits<224, 64, 64, 4, false, false, cutlass::half_t, Flash_kernel_traits<224, 64, 64, 4, cutlass::half_t>>, Is_dropout=true, Is_causal=false, Is_even_N=true, Is_even_K=false, Return_softmax=false, Params=Flash_fwd_params]" 
(588): here
            instantiation of "void flash::compute_attn<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &) [with Kernel_traits=Flash_fwd_kernel_traits<224, 64, 64, 4, false, false, cutlass::half_t, Flash_kernel_traits<224, 64, 64, 4, cutlass::half_t>>, Is_dropout=true, Is_causal=false, Is_even_N=true, Is_even_K=false, Return_softmax=false, Params=Flash_fwd_params]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(15): here
            instantiation of "void flash_fwd_kernel<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax>(Flash_fwd_params) [with Kernel_traits=Flash_fwd_kernel_traits<224, 64, 64, 4, false, false, cutlass::half_t, Flash_kernel_traits<224, 64, 64, 4, cutlass::half_t>>, Is_dropout=true, Is_causal=false, Is_even_N=true, Is_even_K=false, Return_softmax=false]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(34): here
            instantiation of "void run_flash_fwd<Kernel_traits,Is_dropout,Is_causal>(Flash_fwd_params &, cudaStream_t) [with Kernel_traits=Flash_fwd_kernel_traits<224, 64, 64, 4, false, false, cutlass::half_t, Flash_kernel_traits<224, 64, 64, 4, cutlass::half_t>>, Is_dropout=true, Is_causal=false]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(208): here
            instantiation of "void run_mha_fwd_hdim224<T>(Flash_fwd_params &, cudaStream_t) [with T=cutlass::half_t]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_hdim224_fp16_sm80.cu(8): here

/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_kernel.h(275): warning: variable "tQrQ" was declared but never referenced
          detected during:
            instantiation of "void flash::compute_attn_1rowblock<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &, int, int, int) [with Kernel_traits=Flash_fwd_kernel_traits<224, 64, 64, 4, false, false, cutlass::half_t, Flash_kernel_traits<224, 64, 64, 4, cutlass::half_t>>, Is_dropout=true, Is_causal=false, Is_even_N=true, Is_even_K=false, Return_softmax=false, Params=Flash_fwd_params]" 
(588): here
            instantiation of "void flash::compute_attn<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &) [with Kernel_traits=Flash_fwd_kernel_traits<224, 64, 64, 4, false, false, cutlass::half_t, Flash_kernel_traits<224, 64, 64, 4, cutlass::half_t>>, Is_dropout=true, Is_causal=false, Is_even_N=true, Is_even_K=false, Return_softmax=false, Params=Flash_fwd_params]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(15): here
            instantiation of "void flash_fwd_kernel<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax>(Flash_fwd_params) [with Kernel_traits=Flash_fwd_kernel_traits<224, 64, 64, 4, false, false, cutlass::half_t, Flash_kernel_traits<224, 64, 64, 4, cutlass::half_t>>, Is_dropout=true, Is_causal=false, Is_even_N=true, Is_even_K=false, Return_softmax=false]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(34): here
            instantiation of "void run_flash_fwd<Kernel_traits,Is_dropout,Is_causal>(Flash_fwd_params &, cudaStream_t) [with Kernel_traits=Flash_fwd_kernel_traits<224, 64, 64, 4, false, false, cutlass::half_t, Flash_kernel_traits<224, 64, 64, 4, cutlass::half_t>>, Is_dropout=true, Is_causal=false]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(208): here
            instantiation of "void run_mha_fwd_hdim224<T>(Flash_fwd_params &, cudaStream_t) [with T=cutlass::half_t]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_hdim224_fp16_sm80.cu(8): here

/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_kernel.h(140): warning: variable "MMA_M" was declared but never referenced
          detected during:
            instantiation of "void flash::compute_attn_1rowblock<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &, int, int, int) [with Kernel_traits=Flash_fwd_kernel_traits<224, 64, 64, 4, false, false, cutlass::half_t, Flash_kernel_traits<224, 64, 64, 4, cutlass::half_t>>, Is_dropout=true, Is_causal=false, Is_even_N=false, Is_even_K=true, Return_softmax=true, Params=Flash_fwd_params]" 
(588): here
            instantiation of "void flash::compute_attn<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &) [with Kernel_traits=Flash_fwd_kernel_traits<224, 64, 64, 4, false, false, cutlass::half_t, Flash_kernel_traits<224, 64, 64, 4, cutlass::half_t>>, Is_dropout=true, Is_causal=false, Is_even_N=false, Is_even_K=true, Return_softmax=true, Params=Flash_fwd_params]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(15): here
            instantiation of "void flash_fwd_kernel<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax>(Flash_fwd_params) [with Kernel_traits=Flash_fwd_kernel_traits<224, 64, 64, 4, false, false, cutlass::half_t, Flash_kernel_traits<224, 64, 64, 4, cutlass::half_t>>, Is_dropout=true, Is_causal=false, Is_even_N=false, Is_even_K=true, Return_softmax=true]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(34): here
            instantiation of "void run_flash_fwd<Kernel_traits,Is_dropout,Is_causal>(Flash_fwd_params &, cudaStream_t) [with Kernel_traits=Flash_fwd_kernel_traits<224, 64, 64, 4, false, false, cutlass::half_t, Flash_kernel_traits<224, 64, 64, 4, cutlass::half_t>>, Is_dropout=true, Is_causal=false]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(208): here
            instantiation of "void run_mha_fwd_hdim224<T>(Flash_fwd_params &, cudaStream_t) [with T=cutlass::half_t]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_hdim224_fp16_sm80.cu(8): here

/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_kernel.h(275): warning: variable "tQrQ" was declared but never referenced
          detected during:
            instantiation of "void flash::compute_attn_1rowblock<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &, int, int, int) [with Kernel_traits=Flash_fwd_kernel_traits<224, 64, 64, 4, false, false, cutlass::half_t, Flash_kernel_traits<224, 64, 64, 4, cutlass::half_t>>, Is_dropout=true, Is_causal=false, Is_even_N=false, Is_even_K=true, Return_softmax=true, Params=Flash_fwd_params]" 
(588): here
            instantiation of "void flash::compute_attn<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &) [with Kernel_traits=Flash_fwd_kernel_traits<224, 64, 64, 4, false, false, cutlass::half_t, Flash_kernel_traits<224, 64, 64, 4, cutlass::half_t>>, Is_dropout=true, Is_causal=false, Is_even_N=false, Is_even_K=true, Return_softmax=true, Params=Flash_fwd_params]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(15): here
            instantiation of "void flash_fwd_kernel<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax>(Flash_fwd_params) [with Kernel_traits=Flash_fwd_kernel_traits<224, 64, 64, 4, false, false, cutlass::half_t, Flash_kernel_traits<224, 64, 64, 4, cutlass::half_t>>, Is_dropout=true, Is_causal=false, Is_even_N=false, Is_even_K=true, Return_softmax=true]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(34): here
            instantiation of "void run_flash_fwd<Kernel_traits,Is_dropout,Is_causal>(Flash_fwd_params &, cudaStream_t) [with Kernel_traits=Flash_fwd_kernel_traits<224, 64, 64, 4, false, false, cutlass::half_t, Flash_kernel_traits<224, 64, 64, 4, cutlass::half_t>>, Is_dropout=true, Is_causal=false]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(208): here
            instantiation of "void run_mha_fwd_hdim224<T>(Flash_fwd_params &, cudaStream_t) [with T=cutlass::half_t]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_hdim224_fp16_sm80.cu(8): here

/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_kernel.h(140): warning: variable "MMA_M" was declared but never referenced
          detected during:
            instantiation of "void flash::compute_attn_1rowblock<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &, int, int, int) [with Kernel_traits=Flash_fwd_kernel_traits<224, 64, 64, 4, false, false, cutlass::half_t, Flash_kernel_traits<224, 64, 64, 4, cutlass::half_t>>, Is_dropout=true, Is_causal=false, Is_even_N=false, Is_even_K=true, Return_softmax=false, Params=Flash_fwd_params]" 
(588): here
            instantiation of "void flash::compute_attn<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &) [with Kernel_traits=Flash_fwd_kernel_traits<224, 64, 64, 4, false, false, cutlass::half_t, Flash_kernel_traits<224, 64, 64, 4, cutlass::half_t>>, Is_dropout=true, Is_causal=false, Is_even_N=false, Is_even_K=true, Return_softmax=false, Params=Flash_fwd_params]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(15): here
            instantiation of "void flash_fwd_kernel<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax>(Flash_fwd_params) [with Kernel_traits=Flash_fwd_kernel_traits<224, 64, 64, 4, false, false, cutlass::half_t, Flash_kernel_traits<224, 64, 64, 4, cutlass::half_t>>, Is_dropout=true, Is_causal=false, Is_even_N=false, Is_even_K=true, Return_softmax=false]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(34): here
            instantiation of "void run_flash_fwd<Kernel_traits,Is_dropout,Is_causal>(Flash_fwd_params &, cudaStream_t) [with Kernel_traits=Flash_fwd_kernel_traits<224, 64, 64, 4, false, false, cutlass::half_t, Flash_kernel_traits<224, 64, 64, 4, cutlass::half_t>>, Is_dropout=true, Is_causal=false]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(208): here
            instantiation of "void run_mha_fwd_hdim224<T>(Flash_fwd_params &, cudaStream_t) [with T=cutlass::half_t]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_hdim224_fp16_sm80.cu(8): here

/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_kernel.h(275): warning: variable "tQrQ" was declared but never referenced
          detected during:
            instantiation of "void flash::compute_attn_1rowblock<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &, int, int, int) [with Kernel_traits=Flash_fwd_kernel_traits<224, 64, 64, 4, false, false, cutlass::half_t, Flash_kernel_traits<224, 64, 64, 4, cutlass::half_t>>, Is_dropout=true, Is_causal=false, Is_even_N=false, Is_even_K=true, Return_softmax=false, Params=Flash_fwd_params]" 
(588): here
            instantiation of "void flash::compute_attn<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &) [with Kernel_traits=Flash_fwd_kernel_traits<224, 64, 64, 4, false, false, cutlass::half_t, Flash_kernel_traits<224, 64, 64, 4, cutlass::half_t>>, Is_dropout=true, Is_causal=false, Is_even_N=false, Is_even_K=true, Return_softmax=false, Params=Flash_fwd_params]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(15): here
            instantiation of "void flash_fwd_kernel<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax>(Flash_fwd_params) [with Kernel_traits=Flash_fwd_kernel_traits<224, 64, 64, 4, false, false, cutlass::half_t, Flash_kernel_traits<224, 64, 64, 4, cutlass::half_t>>, Is_dropout=true, Is_causal=false, Is_even_N=false, Is_even_K=true, Return_softmax=false]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(34): here
            instantiation of "void run_flash_fwd<Kernel_traits,Is_dropout,Is_causal>(Flash_fwd_params &, cudaStream_t) [with Kernel_traits=Flash_fwd_kernel_traits<224, 64, 64, 4, false, false, cutlass::half_t, Flash_kernel_traits<224, 64, 64, 4, cutlass::half_t>>, Is_dropout=true, Is_causal=false]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(208): here
            instantiation of "void run_mha_fwd_hdim224<T>(Flash_fwd_params &, cudaStream_t) [with T=cutlass::half_t]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_hdim224_fp16_sm80.cu(8): here

/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_kernel.h(140): warning: variable "MMA_M" was declared but never referenced
          detected during:
            instantiation of "void flash::compute_attn_1rowblock<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &, int, int, int) [with Kernel_traits=Flash_fwd_kernel_traits<224, 64, 64, 4, false, false, cutlass::half_t, Flash_kernel_traits<224, 64, 64, 4, cutlass::half_t>>, Is_dropout=true, Is_causal=false, Is_even_N=false, Is_even_K=false, Return_softmax=true, Params=Flash_fwd_params]" 
(588): here
            instantiation of "void flash::compute_attn<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &) [with Kernel_traits=Flash_fwd_kernel_traits<224, 64, 64, 4, false, false, cutlass::half_t, Flash_kernel_traits<224, 64, 64, 4, cutlass::half_t>>, Is_dropout=true, Is_causal=false, Is_even_N=false, Is_even_K=false, Return_softmax=true, Params=Flash_fwd_params]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(15): here
            instantiation of "void flash_fwd_kernel<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax>(Flash_fwd_params) [with Kernel_traits=Flash_fwd_kernel_traits<224, 64, 64, 4, false, false, cutlass::half_t, Flash_kernel_traits<224, 64, 64, 4, cutlass::half_t>>, Is_dropout=true, Is_causal=false, Is_even_N=false, Is_even_K=false, Return_softmax=true]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(34): here
            instantiation of "void run_flash_fwd<Kernel_traits,Is_dropout,Is_causal>(Flash_fwd_params &, cudaStream_t) [with Kernel_traits=Flash_fwd_kernel_traits<224, 64, 64, 4, false, false, cutlass::half_t, Flash_kernel_traits<224, 64, 64, 4, cutlass::half_t>>, Is_dropout=true, Is_causal=false]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(208): here
            instantiation of "void run_mha_fwd_hdim224<T>(Flash_fwd_params &, cudaStream_t) [with T=cutlass::half_t]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_hdim224_fp16_sm80.cu(8): here

/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_kernel.h(275): warning: variable "tQrQ" was declared but never referenced
          detected during:
            instantiation of "void flash::compute_attn_1rowblock<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &, int, int, int) [with Kernel_traits=Flash_fwd_kernel_traits<224, 64, 64, 4, false, false, cutlass::half_t, Flash_kernel_traits<224, 64, 64, 4, cutlass::half_t>>, Is_dropout=true, Is_causal=false, Is_even_N=false, Is_even_K=false, Return_softmax=true, Params=Flash_fwd_params]" 
(588): here
            instantiation of "void flash::compute_attn<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &) [with Kernel_traits=Flash_fwd_kernel_traits<224, 64, 64, 4, false, false, cutlass::half_t, Flash_kernel_traits<224, 64, 64, 4, cutlass::half_t>>, Is_dropout=true, Is_causal=false, Is_even_N=false, Is_even_K=false, Return_softmax=true, Params=Flash_fwd_params]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(15): here
            instantiation of "void flash_fwd_kernel<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax>(Flash_fwd_params) [with Kernel_traits=Flash_fwd_kernel_traits<224, 64, 64, 4, false, false, cutlass::half_t, Flash_kernel_traits<224, 64, 64, 4, cutlass::half_t>>, Is_dropout=true, Is_causal=false, Is_even_N=false, Is_even_K=false, Return_softmax=true]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(34): here
            instantiation of "void run_flash_fwd<Kernel_traits,Is_dropout,Is_causal>(Flash_fwd_params &, cudaStream_t) [with Kernel_traits=Flash_fwd_kernel_traits<224, 64, 64, 4, false, false, cutlass::half_t, Flash_kernel_traits<224, 64, 64, 4, cutlass::half_t>>, Is_dropout=true, Is_causal=false]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(208): here
            instantiation of "void run_mha_fwd_hdim224<T>(Flash_fwd_params &, cudaStream_t) [with T=cutlass::half_t]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_hdim224_fp16_sm80.cu(8): here

/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_kernel.h(140): warning: variable "MMA_M" was declared but never referenced
          detected during:
            instantiation of "void flash::compute_attn_1rowblock<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &, int, int, int) [with Kernel_traits=Flash_fwd_kernel_traits<224, 64, 64, 4, false, false, cutlass::half_t, Flash_kernel_traits<224, 64, 64, 4, cutlass::half_t>>, Is_dropout=true, Is_causal=false, Is_even_N=false, Is_even_K=false, Return_softmax=false, Params=Flash_fwd_params]" 
(588): here
            instantiation of "void flash::compute_attn<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &) [with Kernel_traits=Flash_fwd_kernel_traits<224, 64, 64, 4, false, false, cutlass::half_t, Flash_kernel_traits<224, 64, 64, 4, cutlass::half_t>>, Is_dropout=true, Is_causal=false, Is_even_N=false, Is_even_K=false, Return_softmax=false, Params=Flash_fwd_params]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(15): here
            instantiation of "void flash_fwd_kernel<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax>(Flash_fwd_params) [with Kernel_traits=Flash_fwd_kernel_traits<224, 64, 64, 4, false, false, cutlass::half_t, Flash_kernel_traits<224, 64, 64, 4, cutlass::half_t>>, Is_dropout=true, Is_causal=false, Is_even_N=false, Is_even_K=false, Return_softmax=false]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(34): here
            instantiation of "void run_flash_fwd<Kernel_traits,Is_dropout,Is_causal>(Flash_fwd_params &, cudaStream_t) [with Kernel_traits=Flash_fwd_kernel_traits<224, 64, 64, 4, false, false, cutlass::half_t, Flash_kernel_traits<224, 64, 64, 4, cutlass::half_t>>, Is_dropout=true, Is_causal=false]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(208): here
            instantiation of "void run_mha_fwd_hdim224<T>(Flash_fwd_params &, cudaStream_t) [with T=cutlass::half_t]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_hdim224_fp16_sm80.cu(8): here

/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_kernel.h(275): warning: variable "tQrQ" was declared but never referenced
          detected during:
            instantiation of "void flash::compute_attn_1rowblock<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &, int, int, int) [with Kernel_traits=Flash_fwd_kernel_traits<224, 64, 64, 4, false, false, cutlass::half_t, Flash_kernel_traits<224, 64, 64, 4, cutlass::half_t>>, Is_dropout=true, Is_causal=false, Is_even_N=false, Is_even_K=false, Return_softmax=false, Params=Flash_fwd_params]" 
(588): here
            instantiation of "void flash::compute_attn<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &) [with Kernel_traits=Flash_fwd_kernel_traits<224, 64, 64, 4, false, false, cutlass::half_t, Flash_kernel_traits<224, 64, 64, 4, cutlass::half_t>>, Is_dropout=true, Is_causal=false, Is_even_N=false, Is_even_K=false, Return_softmax=false, Params=Flash_fwd_params]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(15): here
            instantiation of "void flash_fwd_kernel<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax>(Flash_fwd_params) [with Kernel_traits=Flash_fwd_kernel_traits<224, 64, 64, 4, false, false, cutlass::half_t, Flash_kernel_traits<224, 64, 64, 4, cutlass::half_t>>, Is_dropout=true, Is_causal=false, Is_even_N=false, Is_even_K=false, Return_softmax=false]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(34): here
            instantiation of "void run_flash_fwd<Kernel_traits,Is_dropout,Is_causal>(Flash_fwd_params &, cudaStream_t) [with Kernel_traits=Flash_fwd_kernel_traits<224, 64, 64, 4, false, false, cutlass::half_t, Flash_kernel_traits<224, 64, 64, 4, cutlass::half_t>>, Is_dropout=true, Is_causal=false]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(208): here
            instantiation of "void run_mha_fwd_hdim224<T>(Flash_fwd_params &, cudaStream_t) [with T=cutlass::half_t]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_hdim224_fp16_sm80.cu(8): here

/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_kernel.h(140): warning: variable "MMA_M" was declared but never referenced
          detected during:
            instantiation of "void flash::compute_attn_1rowblock<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &, int, int, int) [with Kernel_traits=Flash_fwd_kernel_traits<224, 128, 64, 8, false, false, cutlass::half_t, Flash_kernel_traits<224, 128, 64, 8, cutlass::half_t>>, Is_dropout=false, Is_causal=true, Is_even_N=true, Is_even_K=true, Return_softmax=false, Params=Flash_fwd_params]" 
(588): here
            instantiation of "void flash::compute_attn<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &) [with Kernel_traits=Flash_fwd_kernel_traits<224, 128, 64, 8, false, false, cutlass::half_t, Flash_kernel_traits<224, 128, 64, 8, cutlass::half_t>>, Is_dropout=false, Is_causal=true, Is_even_N=true, Is_even_K=true, Return_softmax=false, Params=Flash_fwd_params]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(15): here
            instantiation of "void flash_fwd_kernel<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax>(Flash_fwd_params) [with Kernel_traits=Flash_fwd_kernel_traits<224, 128, 64, 8, false, false, cutlass::half_t, Flash_kernel_traits<224, 128, 64, 8, cutlass::half_t>>, Is_dropout=false, Is_causal=true, Is_even_N=true, Is_even_K=true, Return_softmax=false]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(34): here
            instantiation of "void run_flash_fwd<Kernel_traits,Is_dropout,Is_causal>(Flash_fwd_params &, cudaStream_t) [with Kernel_traits=Flash_fwd_kernel_traits<224, 128, 64, 8, false, false, cutlass::half_t, Flash_kernel_traits<224, 128, 64, 8, cutlass::half_t>>, Is_dropout=false, Is_causal=true]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(208): here
            instantiation of "void run_mha_fwd_hdim224<T>(Flash_fwd_params &, cudaStream_t) [with T=cutlass::half_t]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_hdim224_fp16_sm80.cu(8): here

/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_kernel.h(275): warning: variable "tQrQ" was declared but never referenced
          detected during:
            instantiation of "void flash::compute_attn_1rowblock<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &, int, int, int) [with Kernel_traits=Flash_fwd_kernel_traits<224, 128, 64, 8, false, false, cutlass::half_t, Flash_kernel_traits<224, 128, 64, 8, cutlass::half_t>>, Is_dropout=false, Is_causal=true, Is_even_N=true, Is_even_K=true, Return_softmax=false, Params=Flash_fwd_params]" 
(588): here
            instantiation of "void flash::compute_attn<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &) [with Kernel_traits=Flash_fwd_kernel_traits<224, 128, 64, 8, false, false, cutlass::half_t, Flash_kernel_traits<224, 128, 64, 8, cutlass::half_t>>, Is_dropout=false, Is_causal=true, Is_even_N=true, Is_even_K=true, Return_softmax=false, Params=Flash_fwd_params]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(15): here
            instantiation of "void flash_fwd_kernel<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax>(Flash_fwd_params) [with Kernel_traits=Flash_fwd_kernel_traits<224, 128, 64, 8, false, false, cutlass::half_t, Flash_kernel_traits<224, 128, 64, 8, cutlass::half_t>>, Is_dropout=false, Is_causal=true, Is_even_N=true, Is_even_K=true, Return_softmax=false]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(34): here
            instantiation of "void run_flash_fwd<Kernel_traits,Is_dropout,Is_causal>(Flash_fwd_params &, cudaStream_t) [with Kernel_traits=Flash_fwd_kernel_traits<224, 128, 64, 8, false, false, cutlass::half_t, Flash_kernel_traits<224, 128, 64, 8, cutlass::half_t>>, Is_dropout=false, Is_causal=true]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(208): here
            instantiation of "void run_mha_fwd_hdim224<T>(Flash_fwd_params &, cudaStream_t) [with T=cutlass::half_t]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_hdim224_fp16_sm80.cu(8): here

/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_kernel.h(140): warning: variable "MMA_M" was declared but never referenced
          detected during:
            instantiation of "void flash::compute_attn_1rowblock<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &, int, int, int) [with Kernel_traits=Flash_fwd_kernel_traits<224, 128, 64, 8, false, false, cutlass::half_t, Flash_kernel_traits<224, 128, 64, 8, cutlass::half_t>>, Is_dropout=false, Is_causal=true, Is_even_N=true, Is_even_K=false, Return_softmax=false, Params=Flash_fwd_params]" 
(588): here
            instantiation of "void flash::compute_attn<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &) [with Kernel_traits=Flash_fwd_kernel_traits<224, 128, 64, 8, false, false, cutlass::half_t, Flash_kernel_traits<224, 128, 64, 8, cutlass::half_t>>, Is_dropout=false, Is_causal=true, Is_even_N=true, Is_even_K=false, Return_softmax=false, Params=Flash_fwd_params]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(15): here
            instantiation of "void flash_fwd_kernel<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax>(Flash_fwd_params) [with Kernel_traits=Flash_fwd_kernel_traits<224, 128, 64, 8, false, false, cutlass::half_t, Flash_kernel_traits<224, 128, 64, 8, cutlass::half_t>>, Is_dropout=false, Is_causal=true, Is_even_N=true, Is_even_K=false, Return_softmax=false]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(34): here
            instantiation of "void run_flash_fwd<Kernel_traits,Is_dropout,Is_causal>(Flash_fwd_params &, cudaStream_t) [with Kernel_traits=Flash_fwd_kernel_traits<224, 128, 64, 8, false, false, cutlass::half_t, Flash_kernel_traits<224, 128, 64, 8, cutlass::half_t>>, Is_dropout=false, Is_causal=true]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(208): here
            instantiation of "void run_mha_fwd_hdim224<T>(Flash_fwd_params &, cudaStream_t) [with T=cutlass::half_t]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_hdim224_fp16_sm80.cu(8): here

/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_kernel.h(275): warning: variable "tQrQ" was declared but never referenced
          detected during:
            instantiation of "void flash::compute_attn_1rowblock<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &, int, int, int) [with Kernel_traits=Flash_fwd_kernel_traits<224, 128, 64, 8, false, false, cutlass::half_t, Flash_kernel_traits<224, 128, 64, 8, cutlass::half_t>>, Is_dropout=false, Is_causal=true, Is_even_N=true, Is_even_K=false, Return_softmax=false, Params=Flash_fwd_params]" 
(588): here
            instantiation of "void flash::compute_attn<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &) [with Kernel_traits=Flash_fwd_kernel_traits<224, 128, 64, 8, false, false, cutlass::half_t, Flash_kernel_traits<224, 128, 64, 8, cutlass::half_t>>, Is_dropout=false, Is_causal=true, Is_even_N=true, Is_even_K=false, Return_softmax=false, Params=Flash_fwd_params]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(15): here
            instantiation of "void flash_fwd_kernel<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax>(Flash_fwd_params) [with Kernel_traits=Flash_fwd_kernel_traits<224, 128, 64, 8, false, false, cutlass::half_t, Flash_kernel_traits<224, 128, 64, 8, cutlass::half_t>>, Is_dropout=false, Is_causal=true, Is_even_N=true, Is_even_K=false, Return_softmax=false]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(34): here
            instantiation of "void run_flash_fwd<Kernel_traits,Is_dropout,Is_causal>(Flash_fwd_params &, cudaStream_t) [with Kernel_traits=Flash_fwd_kernel_traits<224, 128, 64, 8, false, false, cutlass::half_t, Flash_kernel_traits<224, 128, 64, 8, cutlass::half_t>>, Is_dropout=false, Is_causal=true]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(208): here
            instantiation of "void run_mha_fwd_hdim224<T>(Flash_fwd_params &, cudaStream_t) [with T=cutlass::half_t]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_hdim224_fp16_sm80.cu(8): here

/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_kernel.h(140): warning: variable "MMA_M" was declared but never referenced
          detected during:
            instantiation of "void flash::compute_attn_1rowblock<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &, int, int, int) [with Kernel_traits=Flash_fwd_kernel_traits<224, 128, 64, 8, false, false, cutlass::half_t, Flash_kernel_traits<224, 128, 64, 8, cutlass::half_t>>, Is_dropout=false, Is_causal=true, Is_even_N=false, Is_even_K=true, Return_softmax=false, Params=Flash_fwd_params]" 
(588): here
            instantiation of "void flash::compute_attn<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &) [with Kernel_traits=Flash_fwd_kernel_traits<224, 128, 64, 8, false, false, cutlass::half_t, Flash_kernel_traits<224, 128, 64, 8, cutlass::half_t>>, Is_dropout=false, Is_causal=true, Is_even_N=false, Is_even_K=true, Return_softmax=false, Params=Flash_fwd_params]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(15): here
            instantiation of "void flash_fwd_kernel<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax>(Flash_fwd_params) [with Kernel_traits=Flash_fwd_kernel_traits<224, 128, 64, 8, false, false, cutlass::half_t, Flash_kernel_traits<224, 128, 64, 8, cutlass::half_t>>, Is_dropout=false, Is_causal=true, Is_even_N=false, Is_even_K=true, Return_softmax=false]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(34): here
            instantiation of "void run_flash_fwd<Kernel_traits,Is_dropout,Is_causal>(Flash_fwd_params &, cudaStream_t) [with Kernel_traits=Flash_fwd_kernel_traits<224, 128, 64, 8, false, false, cutlass::half_t, Flash_kernel_traits<224, 128, 64, 8, cutlass::half_t>>, Is_dropout=false, Is_causal=true]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(208): here
            instantiation of "void run_mha_fwd_hdim224<T>(Flash_fwd_params &, cudaStream_t) [with T=cutlass::half_t]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_hdim224_fp16_sm80.cu(8): here

/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_kernel.h(275): warning: variable "tQrQ" was declared but never referenced
          detected during:
            instantiation of "void flash::compute_attn_1rowblock<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &, int, int, int) [with Kernel_traits=Flash_fwd_kernel_traits<224, 128, 64, 8, false, false, cutlass::half_t, Flash_kernel_traits<224, 128, 64, 8, cutlass::half_t>>, Is_dropout=false, Is_causal=true, Is_even_N=false, Is_even_K=true, Return_softmax=false, Params=Flash_fwd_params]" 
(588): here
            instantiation of "void flash::compute_attn<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &) [with Kernel_traits=Flash_fwd_kernel_traits<224, 128, 64, 8, false, false, cutlass::half_t, Flash_kernel_traits<224, 128, 64, 8, cutlass::half_t>>, Is_dropout=false, Is_causal=true, Is_even_N=false, Is_even_K=true, Return_softmax=false, Params=Flash_fwd_params]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(15): here
            instantiation of "void flash_fwd_kernel<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax>(Flash_fwd_params) [with Kernel_traits=Flash_fwd_kernel_traits<224, 128, 64, 8, false, false, cutlass::half_t, Flash_kernel_traits<224, 128, 64, 8, cutlass::half_t>>, Is_dropout=false, Is_causal=true, Is_even_N=false, Is_even_K=true, Return_softmax=false]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(34): here
            instantiation of "void run_flash_fwd<Kernel_traits,Is_dropout,Is_causal>(Flash_fwd_params &, cudaStream_t) [with Kernel_traits=Flash_fwd_kernel_traits<224, 128, 64, 8, false, false, cutlass::half_t, Flash_kernel_traits<224, 128, 64, 8, cutlass::half_t>>, Is_dropout=false, Is_causal=true]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(208): here
            instantiation of "void run_mha_fwd_hdim224<T>(Flash_fwd_params &, cudaStream_t) [with T=cutlass::half_t]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_hdim224_fp16_sm80.cu(8): here

/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_kernel.h(140): warning: variable "MMA_M" was declared but never referenced
          detected during:
            instantiation of "void flash::compute_attn_1rowblock<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &, int, int, int) [with Kernel_traits=Flash_fwd_kernel_traits<224, 128, 64, 8, false, false, cutlass::half_t, Flash_kernel_traits<224, 128, 64, 8, cutlass::half_t>>, Is_dropout=false, Is_causal=true, Is_even_N=false, Is_even_K=false, Return_softmax=false, Params=Flash_fwd_params]" 
(588): here
            instantiation of "void flash::compute_attn<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &) [with Kernel_traits=Flash_fwd_kernel_traits<224, 128, 64, 8, false, false, cutlass::half_t, Flash_kernel_traits<224, 128, 64, 8, cutlass::half_t>>, Is_dropout=false, Is_causal=true, Is_even_N=false, Is_even_K=false, Return_softmax=false, Params=Flash_fwd_params]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(15): here
            instantiation of "void flash_fwd_kernel<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax>(Flash_fwd_params) [with Kernel_traits=Flash_fwd_kernel_traits<224, 128, 64, 8, false, false, cutlass::half_t, Flash_kernel_traits<224, 128, 64, 8, cutlass::half_t>>, Is_dropout=false, Is_causal=true, Is_even_N=false, Is_even_K=false, Return_softmax=false]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(34): here
            instantiation of "void run_flash_fwd<Kernel_traits,Is_dropout,Is_causal>(Flash_fwd_params &, cudaStream_t) [with Kernel_traits=Flash_fwd_kernel_traits<224, 128, 64, 8, false, false, cutlass::half_t, Flash_kernel_traits<224, 128, 64, 8, cutlass::half_t>>, Is_dropout=false, Is_causal=true]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(208): here
            instantiation of "void run_mha_fwd_hdim224<T>(Flash_fwd_params &, cudaStream_t) [with T=cutlass::half_t]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_hdim224_fp16_sm80.cu(8): here

/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_kernel.h(275): warning: variable "tQrQ" was declared but never referenced
          detected during:
            instantiation of "void flash::compute_attn_1rowblock<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &, int, int, int) [with Kernel_traits=Flash_fwd_kernel_traits<224, 128, 64, 8, false, false, cutlass::half_t, Flash_kernel_traits<224, 128, 64, 8, cutlass::half_t>>, Is_dropout=false, Is_causal=true, Is_even_N=false, Is_even_K=false, Return_softmax=false, Params=Flash_fwd_params]" 
(588): here
            instantiation of "void flash::compute_attn<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &) [with Kernel_traits=Flash_fwd_kernel_traits<224, 128, 64, 8, false, false, cutlass::half_t, Flash_kernel_traits<224, 128, 64, 8, cutlass::half_t>>, Is_dropout=false, Is_causal=true, Is_even_N=false, Is_even_K=false, Return_softmax=false, Params=Flash_fwd_params]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(15): here
            instantiation of "void flash_fwd_kernel<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax>(Flash_fwd_params) [with Kernel_traits=Flash_fwd_kernel_traits<224, 128, 64, 8, false, false, cutlass::half_t, Flash_kernel_traits<224, 128, 64, 8, cutlass::half_t>>, Is_dropout=false, Is_causal=true, Is_even_N=false, Is_even_K=false, Return_softmax=false]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(34): here
            instantiation of "void run_flash_fwd<Kernel_traits,Is_dropout,Is_causal>(Flash_fwd_params &, cudaStream_t) [with Kernel_traits=Flash_fwd_kernel_traits<224, 128, 64, 8, false, false, cutlass::half_t, Flash_kernel_traits<224, 128, 64, 8, cutlass::half_t>>, Is_dropout=false, Is_causal=true]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(208): here
            instantiation of "void run_mha_fwd_hdim224<T>(Flash_fwd_params &, cudaStream_t) [with T=cutlass::half_t]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_hdim224_fp16_sm80.cu(8): here

/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_kernel.h(140): warning: variable "MMA_M" was declared but never referenced
          detected during:
            instantiation of "void flash::compute_attn_1rowblock<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &, int, int, int) [with Kernel_traits=Flash_fwd_kernel_traits<224, 64, 64, 4, false, false, cutlass::half_t, Flash_kernel_traits<224, 64, 64, 4, cutlass::half_t>>, Is_dropout=false, Is_causal=true, Is_even_N=true, Is_even_K=true, Return_softmax=false, Params=Flash_fwd_params]" 
(588): here
            instantiation of "void flash::compute_attn<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &) [with Kernel_traits=Flash_fwd_kernel_traits<224, 64, 64, 4, false, false, cutlass::half_t, Flash_kernel_traits<224, 64, 64, 4, cutlass::half_t>>, Is_dropout=false, Is_causal=true, Is_even_N=true, Is_even_K=true, Return_softmax=false, Params=Flash_fwd_params]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(15): here
            instantiation of "void flash_fwd_kernel<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax>(Flash_fwd_params) [with Kernel_traits=Flash_fwd_kernel_traits<224, 64, 64, 4, false, false, cutlass::half_t, Flash_kernel_traits<224, 64, 64, 4, cutlass::half_t>>, Is_dropout=false, Is_causal=true, Is_even_N=true, Is_even_K=true, Return_softmax=false]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(34): here
            instantiation of "void run_flash_fwd<Kernel_traits,Is_dropout,Is_causal>(Flash_fwd_params &, cudaStream_t) [with Kernel_traits=Flash_fwd_kernel_traits<224, 64, 64, 4, false, false, cutlass::half_t, Flash_kernel_traits<224, 64, 64, 4, cutlass::half_t>>, Is_dropout=false, Is_causal=true]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(208): here
            instantiation of "void run_mha_fwd_hdim224<T>(Flash_fwd_params &, cudaStream_t) [with T=cutlass::half_t]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_hdim224_fp16_sm80.cu(8): here

/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_kernel.h(275): warning: variable "tQrQ" was declared but never referenced
          detected during:
            instantiation of "void flash::compute_attn_1rowblock<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &, int, int, int) [with Kernel_traits=Flash_fwd_kernel_traits<224, 64, 64, 4, false, false, cutlass::half_t, Flash_kernel_traits<224, 64, 64, 4, cutlass::half_t>>, Is_dropout=false, Is_causal=true, Is_even_N=true, Is_even_K=true, Return_softmax=false, Params=Flash_fwd_params]" 
(588): here
            instantiation of "void flash::compute_attn<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &) [with Kernel_traits=Flash_fwd_kernel_traits<224, 64, 64, 4, false, false, cutlass::half_t, Flash_kernel_traits<224, 64, 64, 4, cutlass::half_t>>, Is_dropout=false, Is_causal=true, Is_even_N=true, Is_even_K=true, Return_softmax=false, Params=Flash_fwd_params]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(15): here
            instantiation of "void flash_fwd_kernel<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax>(Flash_fwd_params) [with Kernel_traits=Flash_fwd_kernel_traits<224, 64, 64, 4, false, false, cutlass::half_t, Flash_kernel_traits<224, 64, 64, 4, cutlass::half_t>>, Is_dropout=false, Is_causal=true, Is_even_N=true, Is_even_K=true, Return_softmax=false]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(34): here
            instantiation of "void run_flash_fwd<Kernel_traits,Is_dropout,Is_causal>(Flash_fwd_params &, cudaStream_t) [with Kernel_traits=Flash_fwd_kernel_traits<224, 64, 64, 4, false, false, cutlass::half_t, Flash_kernel_traits<224, 64, 64, 4, cutlass::half_t>>, Is_dropout=false, Is_causal=true]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(208): here
            instantiation of "void run_mha_fwd_hdim224<T>(Flash_fwd_params &, cudaStream_t) [with T=cutlass::half_t]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_hdim224_fp16_sm80.cu(8): here

/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_kernel.h(140): warning: variable "MMA_M" was declared but never referenced
          detected during:
            instantiation of "void flash::compute_attn_1rowblock<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &, int, int, int) [with Kernel_traits=Flash_fwd_kernel_traits<224, 64, 64, 4, false, false, cutlass::half_t, Flash_kernel_traits<224, 64, 64, 4, cutlass::half_t>>, Is_dropout=false, Is_causal=true, Is_even_N=true, Is_even_K=false, Return_softmax=false, Params=Flash_fwd_params]" 
(588): here
            instantiation of "void flash::compute_attn<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &) [with Kernel_traits=Flash_fwd_kernel_traits<224, 64, 64, 4, false, false, cutlass::half_t, Flash_kernel_traits<224, 64, 64, 4, cutlass::half_t>>, Is_dropout=false, Is_causal=true, Is_even_N=true, Is_even_K=false, Return_softmax=false, Params=Flash_fwd_params]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(15): here
            instantiation of "void flash_fwd_kernel<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax>(Flash_fwd_params) [with Kernel_traits=Flash_fwd_kernel_traits<224, 64, 64, 4, false, false, cutlass::half_t, Flash_kernel_traits<224, 64, 64, 4, cutlass::half_t>>, Is_dropout=false, Is_causal=true, Is_even_N=true, Is_even_K=false, Return_softmax=false]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(34): here
            instantiation of "void run_flash_fwd<Kernel_traits,Is_dropout,Is_causal>(Flash_fwd_params &, cudaStream_t) [with Kernel_traits=Flash_fwd_kernel_traits<224, 64, 64, 4, false, false, cutlass::half_t, Flash_kernel_traits<224, 64, 64, 4, cutlass::half_t>>, Is_dropout=false, Is_causal=true]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(208): here
            instantiation of "void run_mha_fwd_hdim224<T>(Flash_fwd_params &, cudaStream_t) [with T=cutlass::half_t]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_hdim224_fp16_sm80.cu(8): here

/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_kernel.h(275): warning: variable "tQrQ" was declared but never referenced
          detected during:
            instantiation of "void flash::compute_attn_1rowblock<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &, int, int, int) [with Kernel_traits=Flash_fwd_kernel_traits<224, 64, 64, 4, false, false, cutlass::half_t, Flash_kernel_traits<224, 64, 64, 4, cutlass::half_t>>, Is_dropout=false, Is_causal=true, Is_even_N=true, Is_even_K=false, Return_softmax=false, Params=Flash_fwd_params]" 
(588): here
            instantiation of "void flash::compute_attn<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &) [with Kernel_traits=Flash_fwd_kernel_traits<224, 64, 64, 4, false, false, cutlass::half_t, Flash_kernel_traits<224, 64, 64, 4, cutlass::half_t>>, Is_dropout=false, Is_causal=true, Is_even_N=true, Is_even_K=false, Return_softmax=false, Params=Flash_fwd_params]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(15): here
            instantiation of "void flash_fwd_kernel<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax>(Flash_fwd_params) [with Kernel_traits=Flash_fwd_kernel_traits<224, 64, 64, 4, false, false, cutlass::half_t, Flash_kernel_traits<224, 64, 64, 4, cutlass::half_t>>, Is_dropout=false, Is_causal=true, Is_even_N=true, Is_even_K=false, Return_softmax=false]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(34): here
            instantiation of "void run_flash_fwd<Kernel_traits,Is_dropout,Is_causal>(Flash_fwd_params &, cudaStream_t) [with Kernel_traits=Flash_fwd_kernel_traits<224, 64, 64, 4, false, false, cutlass::half_t, Flash_kernel_traits<224, 64, 64, 4, cutlass::half_t>>, Is_dropout=false, Is_causal=true]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(208): here
            instantiation of "void run_mha_fwd_hdim224<T>(Flash_fwd_params &, cudaStream_t) [with T=cutlass::half_t]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_hdim224_fp16_sm80.cu(8): here

/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_kernel.h(140): warning: variable "MMA_M" was declared but never referenced
          detected during:
            instantiation of "void flash::compute_attn_1rowblock<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &, int, int, int) [with Kernel_traits=Flash_fwd_kernel_traits<224, 64, 64, 4, false, false, cutlass::half_t, Flash_kernel_traits<224, 64, 64, 4, cutlass::half_t>>, Is_dropout=false, Is_causal=true, Is_even_N=false, Is_even_K=true, Return_softmax=false, Params=Flash_fwd_params]" 
(588): here
            instantiation of "void flash::compute_attn<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &) [with Kernel_traits=Flash_fwd_kernel_traits<224, 64, 64, 4, false, false, cutlass::half_t, Flash_kernel_traits<224, 64, 64, 4, cutlass::half_t>>, Is_dropout=false, Is_causal=true, Is_even_N=false, Is_even_K=true, Return_softmax=false, Params=Flash_fwd_params]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(15): here
            instantiation of "void flash_fwd_kernel<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax>(Flash_fwd_params) [with Kernel_traits=Flash_fwd_kernel_traits<224, 64, 64, 4, false, false, cutlass::half_t, Flash_kernel_traits<224, 64, 64, 4, cutlass::half_t>>, Is_dropout=false, Is_causal=true, Is_even_N=false, Is_even_K=true, Return_softmax=false]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(34): here
            instantiation of "void run_flash_fwd<Kernel_traits,Is_dropout,Is_causal>(Flash_fwd_params &, cudaStream_t) [with Kernel_traits=Flash_fwd_kernel_traits<224, 64, 64, 4, false, false, cutlass::half_t, Flash_kernel_traits<224, 64, 64, 4, cutlass::half_t>>, Is_dropout=false, Is_causal=true]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(208): here
            instantiation of "void run_mha_fwd_hdim224<T>(Flash_fwd_params &, cudaStream_t) [with T=cutlass::half_t]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_hdim224_fp16_sm80.cu(8): here

/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_kernel.h(275): warning: variable "tQrQ" was declared but never referenced
          detected during:
            instantiation of "void flash::compute_attn_1rowblock<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &, int, int, int) [with Kernel_traits=Flash_fwd_kernel_traits<224, 64, 64, 4, false, false, cutlass::half_t, Flash_kernel_traits<224, 64, 64, 4, cutlass::half_t>>, Is_dropout=false, Is_causal=true, Is_even_N=false, Is_even_K=true, Return_softmax=false, Params=Flash_fwd_params]" 
(588): here
            instantiation of "void flash::compute_attn<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &) [with Kernel_traits=Flash_fwd_kernel_traits<224, 64, 64, 4, false, false, cutlass::half_t, Flash_kernel_traits<224, 64, 64, 4, cutlass::half_t>>, Is_dropout=false, Is_causal=true, Is_even_N=false, Is_even_K=true, Return_softmax=false, Params=Flash_fwd_params]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(15): here
            instantiation of "void flash_fwd_kernel<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax>(Flash_fwd_params) [with Kernel_traits=Flash_fwd_kernel_traits<224, 64, 64, 4, false, false, cutlass::half_t, Flash_kernel_traits<224, 64, 64, 4, cutlass::half_t>>, Is_dropout=false, Is_causal=true, Is_even_N=false, Is_even_K=true, Return_softmax=false]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(34): here
            instantiation of "void run_flash_fwd<Kernel_traits,Is_dropout,Is_causal>(Flash_fwd_params &, cudaStream_t) [with Kernel_traits=Flash_fwd_kernel_traits<224, 64, 64, 4, false, false, cutlass::half_t, Flash_kernel_traits<224, 64, 64, 4, cutlass::half_t>>, Is_dropout=false, Is_causal=true]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(208): here
            instantiation of "void run_mha_fwd_hdim224<T>(Flash_fwd_params &, cudaStream_t) [with T=cutlass::half_t]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_hdim224_fp16_sm80.cu(8): here

/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_kernel.h(140): warning: variable "MMA_M" was declared but never referenced
          detected during:
            instantiation of "void flash::compute_attn_1rowblock<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &, int, int, int) [with Kernel_traits=Flash_fwd_kernel_traits<224, 64, 64, 4, false, false, cutlass::half_t, Flash_kernel_traits<224, 64, 64, 4, cutlass::half_t>>, Is_dropout=false, Is_causal=true, Is_even_N=false, Is_even_K=false, Return_softmax=false, Params=Flash_fwd_params]" 
(588): here
            instantiation of "void flash::compute_attn<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &) [with Kernel_traits=Flash_fwd_kernel_traits<224, 64, 64, 4, false, false, cutlass::half_t, Flash_kernel_traits<224, 64, 64, 4, cutlass::half_t>>, Is_dropout=false, Is_causal=true, Is_even_N=false, Is_even_K=false, Return_softmax=false, Params=Flash_fwd_params]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(15): here
            instantiation of "void flash_fwd_kernel<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax>(Flash_fwd_params) [with Kernel_traits=Flash_fwd_kernel_traits<224, 64, 64, 4, false, false, cutlass::half_t, Flash_kernel_traits<224, 64, 64, 4, cutlass::half_t>>, Is_dropout=false, Is_causal=true, Is_even_N=false, Is_even_K=false, Return_softmax=false]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(34): here
            instantiation of "void run_flash_fwd<Kernel_traits,Is_dropout,Is_causal>(Flash_fwd_params &, cudaStream_t) [with Kernel_traits=Flash_fwd_kernel_traits<224, 64, 64, 4, false, false, cutlass::half_t, Flash_kernel_traits<224, 64, 64, 4, cutlass::half_t>>, Is_dropout=false, Is_causal=true]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(208): here
            instantiation of "void run_mha_fwd_hdim224<T>(Flash_fwd_params &, cudaStream_t) [with T=cutlass::half_t]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_hdim224_fp16_sm80.cu(8): here

/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_kernel.h(275): warning: variable "tQrQ" was declared but never referenced
          detected during:
            instantiation of "void flash::compute_attn_1rowblock<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &, int, int, int) [with Kernel_traits=Flash_fwd_kernel_traits<224, 64, 64, 4, false, false, cutlass::half_t, Flash_kernel_traits<224, 64, 64, 4, cutlass::half_t>>, Is_dropout=false, Is_causal=true, Is_even_N=false, Is_even_K=false, Return_softmax=false, Params=Flash_fwd_params]" 
(588): here
            instantiation of "void flash::compute_attn<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &) [with Kernel_traits=Flash_fwd_kernel_traits<224, 64, 64, 4, false, false, cutlass::half_t, Flash_kernel_traits<224, 64, 64, 4, cutlass::half_t>>, Is_dropout=false, Is_causal=true, Is_even_N=false, Is_even_K=false, Return_softmax=false, Params=Flash_fwd_params]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(15): here
            instantiation of "void flash_fwd_kernel<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax>(Flash_fwd_params) [with Kernel_traits=Flash_fwd_kernel_traits<224, 64, 64, 4, false, false, cutlass::half_t, Flash_kernel_traits<224, 64, 64, 4, cutlass::half_t>>, Is_dropout=false, Is_causal=true, Is_even_N=false, Is_even_K=false, Return_softmax=false]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(34): here
            instantiation of "void run_flash_fwd<Kernel_traits,Is_dropout,Is_causal>(Flash_fwd_params &, cudaStream_t) [with Kernel_traits=Flash_fwd_kernel_traits<224, 64, 64, 4, false, false, cutlass::half_t, Flash_kernel_traits<224, 64, 64, 4, cutlass::half_t>>, Is_dropout=false, Is_causal=true]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(208): here
            instantiation of "void run_mha_fwd_hdim224<T>(Flash_fwd_params &, cudaStream_t) [with T=cutlass::half_t]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_hdim224_fp16_sm80.cu(8): here

/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_kernel.h(140): warning: variable "MMA_M" was declared but never referenced
          detected during:
            instantiation of "void flash::compute_attn_1rowblock<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &, int, int, int) [with Kernel_traits=Flash_fwd_kernel_traits<224, 128, 64, 8, false, false, cutlass::half_t, Flash_kernel_traits<224, 128, 64, 8, cutlass::half_t>>, Is_dropout=false, Is_causal=false, Is_even_N=true, Is_even_K=true, Return_softmax=false, Params=Flash_fwd_params]" 
(588): here
            instantiation of "void flash::compute_attn<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &) [with Kernel_traits=Flash_fwd_kernel_traits<224, 128, 64, 8, false, false, cutlass::half_t, Flash_kernel_traits<224, 128, 64, 8, cutlass::half_t>>, Is_dropout=false, Is_causal=false, Is_even_N=true, Is_even_K=true, Return_softmax=false, Params=Flash_fwd_params]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(15): here
            instantiation of "void flash_fwd_kernel<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax>(Flash_fwd_params) [with Kernel_traits=Flash_fwd_kernel_traits<224, 128, 64, 8, false, false, cutlass::half_t, Flash_kernel_traits<224, 128, 64, 8, cutlass::half_t>>, Is_dropout=false, Is_causal=false, Is_even_N=true, Is_even_K=true, Return_softmax=false]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(34): here
            instantiation of "void run_flash_fwd<Kernel_traits,Is_dropout,Is_causal>(Flash_fwd_params &, cudaStream_t) [with Kernel_traits=Flash_fwd_kernel_traits<224, 128, 64, 8, false, false, cutlass::half_t, Flash_kernel_traits<224, 128, 64, 8, cutlass::half_t>>, Is_dropout=false, Is_causal=false]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(208): here
            instantiation of "void run_mha_fwd_hdim224<T>(Flash_fwd_params &, cudaStream_t) [with T=cutlass::half_t]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_hdim224_fp16_sm80.cu(8): here

/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_kernel.h(275): warning: variable "tQrQ" was declared but never referenced
          detected during:
            instantiation of "void flash::compute_attn_1rowblock<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &, int, int, int) [with Kernel_traits=Flash_fwd_kernel_traits<224, 128, 64, 8, false, false, cutlass::half_t, Flash_kernel_traits<224, 128, 64, 8, cutlass::half_t>>, Is_dropout=false, Is_causal=false, Is_even_N=true, Is_even_K=true, Return_softmax=false, Params=Flash_fwd_params]" 
(588): here
            instantiation of "void flash::compute_attn<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &) [with Kernel_traits=Flash_fwd_kernel_traits<224, 128, 64, 8, false, false, cutlass::half_t, Flash_kernel_traits<224, 128, 64, 8, cutlass::half_t>>, Is_dropout=false, Is_causal=false, Is_even_N=true, Is_even_K=true, Return_softmax=false, Params=Flash_fwd_params]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(15): here
            instantiation of "void flash_fwd_kernel<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax>(Flash_fwd_params) [with Kernel_traits=Flash_fwd_kernel_traits<224, 128, 64, 8, false, false, cutlass::half_t, Flash_kernel_traits<224, 128, 64, 8, cutlass::half_t>>, Is_dropout=false, Is_causal=false, Is_even_N=true, Is_even_K=true, Return_softmax=false]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(34): here
            instantiation of "void run_flash_fwd<Kernel_traits,Is_dropout,Is_causal>(Flash_fwd_params &, cudaStream_t) [with Kernel_traits=Flash_fwd_kernel_traits<224, 128, 64, 8, false, false, cutlass::half_t, Flash_kernel_traits<224, 128, 64, 8, cutlass::half_t>>, Is_dropout=false, Is_causal=false]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(208): here
            instantiation of "void run_mha_fwd_hdim224<T>(Flash_fwd_params &, cudaStream_t) [with T=cutlass::half_t]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_hdim224_fp16_sm80.cu(8): here

/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_kernel.h(140): warning: variable "MMA_M" was declared but never referenced
          detected during:
            instantiation of "void flash::compute_attn_1rowblock<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &, int, int, int) [with Kernel_traits=Flash_fwd_kernel_traits<224, 128, 64, 8, false, false, cutlass::half_t, Flash_kernel_traits<224, 128, 64, 8, cutlass::half_t>>, Is_dropout=false, Is_causal=false, Is_even_N=true, Is_even_K=false, Return_softmax=false, Params=Flash_fwd_params]" 
(588): here
            instantiation of "void flash::compute_attn<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &) [with Kernel_traits=Flash_fwd_kernel_traits<224, 128, 64, 8, false, false, cutlass::half_t, Flash_kernel_traits<224, 128, 64, 8, cutlass::half_t>>, Is_dropout=false, Is_causal=false, Is_even_N=true, Is_even_K=false, Return_softmax=false, Params=Flash_fwd_params]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(15): here
            instantiation of "void flash_fwd_kernel<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax>(Flash_fwd_params) [with Kernel_traits=Flash_fwd_kernel_traits<224, 128, 64, 8, false, false, cutlass::half_t, Flash_kernel_traits<224, 128, 64, 8, cutlass::half_t>>, Is_dropout=false, Is_causal=false, Is_even_N=true, Is_even_K=false, Return_softmax=false]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(34): here
            instantiation of "void run_flash_fwd<Kernel_traits,Is_dropout,Is_causal>(Flash_fwd_params &, cudaStream_t) [with Kernel_traits=Flash_fwd_kernel_traits<224, 128, 64, 8, false, false, cutlass::half_t, Flash_kernel_traits<224, 128, 64, 8, cutlass::half_t>>, Is_dropout=false, Is_causal=false]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(208): here
            instantiation of "void run_mha_fwd_hdim224<T>(Flash_fwd_params &, cudaStream_t) [with T=cutlass::half_t]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_hdim224_fp16_sm80.cu(8): here

/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_kernel.h(275): warning: variable "tQrQ" was declared but never referenced
          detected during:
            instantiation of "void flash::compute_attn_1rowblock<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &, int, int, int) [with Kernel_traits=Flash_fwd_kernel_traits<224, 128, 64, 8, false, false, cutlass::half_t, Flash_kernel_traits<224, 128, 64, 8, cutlass::half_t>>, Is_dropout=false, Is_causal=false, Is_even_N=true, Is_even_K=false, Return_softmax=false, Params=Flash_fwd_params]" 
(588): here
            instantiation of "void flash::compute_attn<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &) [with Kernel_traits=Flash_fwd_kernel_traits<224, 128, 64, 8, false, false, cutlass::half_t, Flash_kernel_traits<224, 128, 64, 8, cutlass::half_t>>, Is_dropout=false, Is_causal=false, Is_even_N=true, Is_even_K=false, Return_softmax=false, Params=Flash_fwd_params]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(15): here
            instantiation of "void flash_fwd_kernel<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax>(Flash_fwd_params) [with Kernel_traits=Flash_fwd_kernel_traits<224, 128, 64, 8, false, false, cutlass::half_t, Flash_kernel_traits<224, 128, 64, 8, cutlass::half_t>>, Is_dropout=false, Is_causal=false, Is_even_N=true, Is_even_K=false, Return_softmax=false]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(34): here
            instantiation of "void run_flash_fwd<Kernel_traits,Is_dropout,Is_causal>(Flash_fwd_params &, cudaStream_t) [with Kernel_traits=Flash_fwd_kernel_traits<224, 128, 64, 8, false, false, cutlass::half_t, Flash_kernel_traits<224, 128, 64, 8, cutlass::half_t>>, Is_dropout=false, Is_causal=false]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(208): here
            instantiation of "void run_mha_fwd_hdim224<T>(Flash_fwd_params &, cudaStream_t) [with T=cutlass::half_t]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_hdim224_fp16_sm80.cu(8): here

/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_kernel.h(140): warning: variable "MMA_M" was declared but never referenced
          detected during:
            instantiation of "void flash::compute_attn_1rowblock<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &, int, int, int) [with Kernel_traits=Flash_fwd_kernel_traits<224, 128, 64, 8, false, false, cutlass::half_t, Flash_kernel_traits<224, 128, 64, 8, cutlass::half_t>>, Is_dropout=false, Is_causal=false, Is_even_N=false, Is_even_K=true, Return_softmax=false, Params=Flash_fwd_params]" 
(588): here
            instantiation of "void flash::compute_attn<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &) [with Kernel_traits=Flash_fwd_kernel_traits<224, 128, 64, 8, false, false, cutlass::half_t, Flash_kernel_traits<224, 128, 64, 8, cutlass::half_t>>, Is_dropout=false, Is_causal=false, Is_even_N=false, Is_even_K=true, Return_softmax=false, Params=Flash_fwd_params]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(15): here
            instantiation of "void flash_fwd_kernel<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax>(Flash_fwd_params) [with Kernel_traits=Flash_fwd_kernel_traits<224, 128, 64, 8, false, false, cutlass::half_t, Flash_kernel_traits<224, 128, 64, 8, cutlass::half_t>>, Is_dropout=false, Is_causal=false, Is_even_N=false, Is_even_K=true, Return_softmax=false]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(34): here
            instantiation of "void run_flash_fwd<Kernel_traits,Is_dropout,Is_causal>(Flash_fwd_params &, cudaStream_t) [with Kernel_traits=Flash_fwd_kernel_traits<224, 128, 64, 8, false, false, cutlass::half_t, Flash_kernel_traits<224, 128, 64, 8, cutlass::half_t>>, Is_dropout=false, Is_causal=false]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(208): here
            instantiation of "void run_mha_fwd_hdim224<T>(Flash_fwd_params &, cudaStream_t) [with T=cutlass::half_t]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_hdim224_fp16_sm80.cu(8): here

/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_kernel.h(275): warning: variable "tQrQ" was declared but never referenced
          detected during:
            instantiation of "void flash::compute_attn_1rowblock<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &, int, int, int) [with Kernel_traits=Flash_fwd_kernel_traits<224, 128, 64, 8, false, false, cutlass::half_t, Flash_kernel_traits<224, 128, 64, 8, cutlass::half_t>>, Is_dropout=false, Is_causal=false, Is_even_N=false, Is_even_K=true, Return_softmax=false, Params=Flash_fwd_params]" 
(588): here
            instantiation of "void flash::compute_attn<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &) [with Kernel_traits=Flash_fwd_kernel_traits<224, 128, 64, 8, false, false, cutlass::half_t, Flash_kernel_traits<224, 128, 64, 8, cutlass::half_t>>, Is_dropout=false, Is_causal=false, Is_even_N=false, Is_even_K=true, Return_softmax=false, Params=Flash_fwd_params]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(15): here
            instantiation of "void flash_fwd_kernel<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax>(Flash_fwd_params) [with Kernel_traits=Flash_fwd_kernel_traits<224, 128, 64, 8, false, false, cutlass::half_t, Flash_kernel_traits<224, 128, 64, 8, cutlass::half_t>>, Is_dropout=false, Is_causal=false, Is_even_N=false, Is_even_K=true, Return_softmax=false]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(34): here
            instantiation of "void run_flash_fwd<Kernel_traits,Is_dropout,Is_causal>(Flash_fwd_params &, cudaStream_t) [with Kernel_traits=Flash_fwd_kernel_traits<224, 128, 64, 8, false, false, cutlass::half_t, Flash_kernel_traits<224, 128, 64, 8, cutlass::half_t>>, Is_dropout=false, Is_causal=false]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(208): here
            instantiation of "void run_mha_fwd_hdim224<T>(Flash_fwd_params &, cudaStream_t) [with T=cutlass::half_t]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_hdim224_fp16_sm80.cu(8): here

/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_kernel.h(140): warning: variable "MMA_M" was declared but never referenced
          detected during:
            instantiation of "void flash::compute_attn_1rowblock<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &, int, int, int) [with Kernel_traits=Flash_fwd_kernel_traits<224, 128, 64, 8, false, false, cutlass::half_t, Flash_kernel_traits<224, 128, 64, 8, cutlass::half_t>>, Is_dropout=false, Is_causal=false, Is_even_N=false, Is_even_K=false, Return_softmax=false, Params=Flash_fwd_params]" 
(588): here
            instantiation of "void flash::compute_attn<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &) [with Kernel_traits=Flash_fwd_kernel_traits<224, 128, 64, 8, false, false, cutlass::half_t, Flash_kernel_traits<224, 128, 64, 8, cutlass::half_t>>, Is_dropout=false, Is_causal=false, Is_even_N=false, Is_even_K=false, Return_softmax=false, Params=Flash_fwd_params]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(15): here
            instantiation of "void flash_fwd_kernel<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax>(Flash_fwd_params) [with Kernel_traits=Flash_fwd_kernel_traits<224, 128, 64, 8, false, false, cutlass::half_t, Flash_kernel_traits<224, 128, 64, 8, cutlass::half_t>>, Is_dropout=false, Is_causal=false, Is_even_N=false, Is_even_K=false, Return_softmax=false]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(34): here
            instantiation of "void run_flash_fwd<Kernel_traits,Is_dropout,Is_causal>(Flash_fwd_params &, cudaStream_t) [with Kernel_traits=Flash_fwd_kernel_traits<224, 128, 64, 8, false, false, cutlass::half_t, Flash_kernel_traits<224, 128, 64, 8, cutlass::half_t>>, Is_dropout=false, Is_causal=false]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(208): here
            instantiation of "void run_mha_fwd_hdim224<T>(Flash_fwd_params &, cudaStream_t) [with T=cutlass::half_t]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_hdim224_fp16_sm80.cu(8): here

/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_kernel.h(275): warning: variable "tQrQ" was declared but never referenced
          detected during:
            instantiation of "void flash::compute_attn_1rowblock<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &, int, int, int) [with Kernel_traits=Flash_fwd_kernel_traits<224, 128, 64, 8, false, false, cutlass::half_t, Flash_kernel_traits<224, 128, 64, 8, cutlass::half_t>>, Is_dropout=false, Is_causal=false, Is_even_N=false, Is_even_K=false, Return_softmax=false, Params=Flash_fwd_params]" 
(588): here
            instantiation of "void flash::compute_attn<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &) [with Kernel_traits=Flash_fwd_kernel_traits<224, 128, 64, 8, false, false, cutlass::half_t, Flash_kernel_traits<224, 128, 64, 8, cutlass::half_t>>, Is_dropout=false, Is_causal=false, Is_even_N=false, Is_even_K=false, Return_softmax=false, Params=Flash_fwd_params]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(15): here
            instantiation of "void flash_fwd_kernel<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax>(Flash_fwd_params) [with Kernel_traits=Flash_fwd_kernel_traits<224, 128, 64, 8, false, false, cutlass::half_t, Flash_kernel_traits<224, 128, 64, 8, cutlass::half_t>>, Is_dropout=false, Is_causal=false, Is_even_N=false, Is_even_K=false, Return_softmax=false]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(34): here
            instantiation of "void run_flash_fwd<Kernel_traits,Is_dropout,Is_causal>(Flash_fwd_params &, cudaStream_t) [with Kernel_traits=Flash_fwd_kernel_traits<224, 128, 64, 8, false, false, cutlass::half_t, Flash_kernel_traits<224, 128, 64, 8, cutlass::half_t>>, Is_dropout=false, Is_causal=false]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(208): here
            instantiation of "void run_mha_fwd_hdim224<T>(Flash_fwd_params &, cudaStream_t) [with T=cutlass::half_t]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_hdim224_fp16_sm80.cu(8): here

/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_kernel.h(140): warning: variable "MMA_M" was declared but never referenced
          detected during:
            instantiation of "void flash::compute_attn_1rowblock<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &, int, int, int) [with Kernel_traits=Flash_fwd_kernel_traits<224, 64, 64, 4, false, false, cutlass::half_t, Flash_kernel_traits<224, 64, 64, 4, cutlass::half_t>>, Is_dropout=false, Is_causal=false, Is_even_N=true, Is_even_K=true, Return_softmax=false, Params=Flash_fwd_params]" 
(588): here
            instantiation of "void flash::compute_attn<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &) [with Kernel_traits=Flash_fwd_kernel_traits<224, 64, 64, 4, false, false, cutlass::half_t, Flash_kernel_traits<224, 64, 64, 4, cutlass::half_t>>, Is_dropout=false, Is_causal=false, Is_even_N=true, Is_even_K=true, Return_softmax=false, Params=Flash_fwd_params]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(15): here
            instantiation of "void flash_fwd_kernel<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax>(Flash_fwd_params) [with Kernel_traits=Flash_fwd_kernel_traits<224, 64, 64, 4, false, false, cutlass::half_t, Flash_kernel_traits<224, 64, 64, 4, cutlass::half_t>>, Is_dropout=false, Is_causal=false, Is_even_N=true, Is_even_K=true, Return_softmax=false]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(34): here
            instantiation of "void run_flash_fwd<Kernel_traits,Is_dropout,Is_causal>(Flash_fwd_params &, cudaStream_t) [with Kernel_traits=Flash_fwd_kernel_traits<224, 64, 64, 4, false, false, cutlass::half_t, Flash_kernel_traits<224, 64, 64, 4, cutlass::half_t>>, Is_dropout=false, Is_causal=false]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(208): here
            instantiation of "void run_mha_fwd_hdim224<T>(Flash_fwd_params &, cudaStream_t) [with T=cutlass::half_t]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_hdim224_fp16_sm80.cu(8): here

/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_kernel.h(275): warning: variable "tQrQ" was declared but never referenced
          detected during:
            instantiation of "void flash::compute_attn_1rowblock<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &, int, int, int) [with Kernel_traits=Flash_fwd_kernel_traits<224, 64, 64, 4, false, false, cutlass::half_t, Flash_kernel_traits<224, 64, 64, 4, cutlass::half_t>>, Is_dropout=false, Is_causal=false, Is_even_N=true, Is_even_K=true, Return_softmax=false, Params=Flash_fwd_params]" 
(588): here
            instantiation of "void flash::compute_attn<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &) [with Kernel_traits=Flash_fwd_kernel_traits<224, 64, 64, 4, false, false, cutlass::half_t, Flash_kernel_traits<224, 64, 64, 4, cutlass::half_t>>, Is_dropout=false, Is_causal=false, Is_even_N=true, Is_even_K=true, Return_softmax=false, Params=Flash_fwd_params]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(15): here
            instantiation of "void flash_fwd_kernel<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax>(Flash_fwd_params) [with Kernel_traits=Flash_fwd_kernel_traits<224, 64, 64, 4, false, false, cutlass::half_t, Flash_kernel_traits<224, 64, 64, 4, cutlass::half_t>>, Is_dropout=false, Is_causal=false, Is_even_N=true, Is_even_K=true, Return_softmax=false]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(34): here
            instantiation of "void run_flash_fwd<Kernel_traits,Is_dropout,Is_causal>(Flash_fwd_params &, cudaStream_t) [with Kernel_traits=Flash_fwd_kernel_traits<224, 64, 64, 4, false, false, cutlass::half_t, Flash_kernel_traits<224, 64, 64, 4, cutlass::half_t>>, Is_dropout=false, Is_causal=false]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(208): here
            instantiation of "void run_mha_fwd_hdim224<T>(Flash_fwd_params &, cudaStream_t) [with T=cutlass::half_t]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_hdim224_fp16_sm80.cu(8): here

/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_kernel.h(140): warning: variable "MMA_M" was declared but never referenced
          detected during:
            instantiation of "void flash::compute_attn_1rowblock<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &, int, int, int) [with Kernel_traits=Flash_fwd_kernel_traits<224, 64, 64, 4, false, false, cutlass::half_t, Flash_kernel_traits<224, 64, 64, 4, cutlass::half_t>>, Is_dropout=false, Is_causal=false, Is_even_N=true, Is_even_K=false, Return_softmax=false, Params=Flash_fwd_params]" 
(588): here
            instantiation of "void flash::compute_attn<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &) [with Kernel_traits=Flash_fwd_kernel_traits<224, 64, 64, 4, false, false, cutlass::half_t, Flash_kernel_traits<224, 64, 64, 4, cutlass::half_t>>, Is_dropout=false, Is_causal=false, Is_even_N=true, Is_even_K=false, Return_softmax=false, Params=Flash_fwd_params]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(15): here
            instantiation of "void flash_fwd_kernel<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax>(Flash_fwd_params) [with Kernel_traits=Flash_fwd_kernel_traits<224, 64, 64, 4, false, false, cutlass::half_t, Flash_kernel_traits<224, 64, 64, 4, cutlass::half_t>>, Is_dropout=false, Is_causal=false, Is_even_N=true, Is_even_K=false, Return_softmax=false]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(34): here
            instantiation of "void run_flash_fwd<Kernel_traits,Is_dropout,Is_causal>(Flash_fwd_params &, cudaStream_t) [with Kernel_traits=Flash_fwd_kernel_traits<224, 64, 64, 4, false, false, cutlass::half_t, Flash_kernel_traits<224, 64, 64, 4, cutlass::half_t>>, Is_dropout=false, Is_causal=false]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(208): here
            instantiation of "void run_mha_fwd_hdim224<T>(Flash_fwd_params &, cudaStream_t) [with T=cutlass::half_t]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_hdim224_fp16_sm80.cu(8): here

/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_kernel.h(275): warning: variable "tQrQ" was declared but never referenced
          detected during:
            instantiation of "void flash::compute_attn_1rowblock<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &, int, int, int) [with Kernel_traits=Flash_fwd_kernel_traits<224, 64, 64, 4, false, false, cutlass::half_t, Flash_kernel_traits<224, 64, 64, 4, cutlass::half_t>>, Is_dropout=false, Is_causal=false, Is_even_N=true, Is_even_K=false, Return_softmax=false, Params=Flash_fwd_params]" 
(588): here
            instantiation of "void flash::compute_attn<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &) [with Kernel_traits=Flash_fwd_kernel_traits<224, 64, 64, 4, false, false, cutlass::half_t, Flash_kernel_traits<224, 64, 64, 4, cutlass::half_t>>, Is_dropout=false, Is_causal=false, Is_even_N=true, Is_even_K=false, Return_softmax=false, Params=Flash_fwd_params]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(15): here
            instantiation of "void flash_fwd_kernel<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax>(Flash_fwd_params) [with Kernel_traits=Flash_fwd_kernel_traits<224, 64, 64, 4, false, false, cutlass::half_t, Flash_kernel_traits<224, 64, 64, 4, cutlass::half_t>>, Is_dropout=false, Is_causal=false, Is_even_N=true, Is_even_K=false, Return_softmax=false]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(34): here
            instantiation of "void run_flash_fwd<Kernel_traits,Is_dropout,Is_causal>(Flash_fwd_params &, cudaStream_t) [with Kernel_traits=Flash_fwd_kernel_traits<224, 64, 64, 4, false, false, cutlass::half_t, Flash_kernel_traits<224, 64, 64, 4, cutlass::half_t>>, Is_dropout=false, Is_causal=false]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(208): here
            instantiation of "void run_mha_fwd_hdim224<T>(Flash_fwd_params &, cudaStream_t) [with T=cutlass::half_t]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_hdim224_fp16_sm80.cu(8): here

/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_kernel.h(140): warning: variable "MMA_M" was declared but never referenced
          detected during:
            instantiation of "void flash::compute_attn_1rowblock<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &, int, int, int) [with Kernel_traits=Flash_fwd_kernel_traits<224, 64, 64, 4, false, false, cutlass::half_t, Flash_kernel_traits<224, 64, 64, 4, cutlass::half_t>>, Is_dropout=false, Is_causal=false, Is_even_N=false, Is_even_K=true, Return_softmax=false, Params=Flash_fwd_params]" 
(588): here
            instantiation of "void flash::compute_attn<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &) [with Kernel_traits=Flash_fwd_kernel_traits<224, 64, 64, 4, false, false, cutlass::half_t, Flash_kernel_traits<224, 64, 64, 4, cutlass::half_t>>, Is_dropout=false, Is_causal=false, Is_even_N=false, Is_even_K=true, Return_softmax=false, Params=Flash_fwd_params]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(15): here
            instantiation of "void flash_fwd_kernel<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax>(Flash_fwd_params) [with Kernel_traits=Flash_fwd_kernel_traits<224, 64, 64, 4, false, false, cutlass::half_t, Flash_kernel_traits<224, 64, 64, 4, cutlass::half_t>>, Is_dropout=false, Is_causal=false, Is_even_N=false, Is_even_K=true, Return_softmax=false]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(34): here
            instantiation of "void run_flash_fwd<Kernel_traits,Is_dropout,Is_causal>(Flash_fwd_params &, cudaStream_t) [with Kernel_traits=Flash_fwd_kernel_traits<224, 64, 64, 4, false, false, cutlass::half_t, Flash_kernel_traits<224, 64, 64, 4, cutlass::half_t>>, Is_dropout=false, Is_causal=false]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(208): here
            instantiation of "void run_mha_fwd_hdim224<T>(Flash_fwd_params &, cudaStream_t) [with T=cutlass::half_t]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_hdim224_fp16_sm80.cu(8): here

/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_kernel.h(275): warning: variable "tQrQ" was declared but never referenced
          detected during:
            instantiation of "void flash::compute_attn_1rowblock<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &, int, int, int) [with Kernel_traits=Flash_fwd_kernel_traits<224, 64, 64, 4, false, false, cutlass::half_t, Flash_kernel_traits<224, 64, 64, 4, cutlass::half_t>>, Is_dropout=false, Is_causal=false, Is_even_N=false, Is_even_K=true, Return_softmax=false, Params=Flash_fwd_params]" 
(588): here
            instantiation of "void flash::compute_attn<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &) [with Kernel_traits=Flash_fwd_kernel_traits<224, 64, 64, 4, false, false, cutlass::half_t, Flash_kernel_traits<224, 64, 64, 4, cutlass::half_t>>, Is_dropout=false, Is_causal=false, Is_even_N=false, Is_even_K=true, Return_softmax=false, Params=Flash_fwd_params]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(15): here
            instantiation of "void flash_fwd_kernel<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax>(Flash_fwd_params) [with Kernel_traits=Flash_fwd_kernel_traits<224, 64, 64, 4, false, false, cutlass::half_t, Flash_kernel_traits<224, 64, 64, 4, cutlass::half_t>>, Is_dropout=false, Is_causal=false, Is_even_N=false, Is_even_K=true, Return_softmax=false]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(34): here
            instantiation of "void run_flash_fwd<Kernel_traits,Is_dropout,Is_causal>(Flash_fwd_params &, cudaStream_t) [with Kernel_traits=Flash_fwd_kernel_traits<224, 64, 64, 4, false, false, cutlass::half_t, Flash_kernel_traits<224, 64, 64, 4, cutlass::half_t>>, Is_dropout=false, Is_causal=false]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(208): here
            instantiation of "void run_mha_fwd_hdim224<T>(Flash_fwd_params &, cudaStream_t) [with T=cutlass::half_t]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_hdim224_fp16_sm80.cu(8): here

/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_kernel.h(140): warning: variable "MMA_M" was declared but never referenced
          detected during:
            instantiation of "void flash::compute_attn_1rowblock<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &, int, int, int) [with Kernel_traits=Flash_fwd_kernel_traits<224, 64, 64, 4, false, false, cutlass::half_t, Flash_kernel_traits<224, 64, 64, 4, cutlass::half_t>>, Is_dropout=false, Is_causal=false, Is_even_N=false, Is_even_K=false, Return_softmax=false, Params=Flash_fwd_params]" 
(588): here
            instantiation of "void flash::compute_attn<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &) [with Kernel_traits=Flash_fwd_kernel_traits<224, 64, 64, 4, false, false, cutlass::half_t, Flash_kernel_traits<224, 64, 64, 4, cutlass::half_t>>, Is_dropout=false, Is_causal=false, Is_even_N=false, Is_even_K=false, Return_softmax=false, Params=Flash_fwd_params]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(15): here
            instantiation of "void flash_fwd_kernel<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax>(Flash_fwd_params) [with Kernel_traits=Flash_fwd_kernel_traits<224, 64, 64, 4, false, false, cutlass::half_t, Flash_kernel_traits<224, 64, 64, 4, cutlass::half_t>>, Is_dropout=false, Is_causal=false, Is_even_N=false, Is_even_K=false, Return_softmax=false]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(34): here
            instantiation of "void run_flash_fwd<Kernel_traits,Is_dropout,Is_causal>(Flash_fwd_params &, cudaStream_t) [with Kernel_traits=Flash_fwd_kernel_traits<224, 64, 64, 4, false, false, cutlass::half_t, Flash_kernel_traits<224, 64, 64, 4, cutlass::half_t>>, Is_dropout=false, Is_causal=false]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(208): here
            instantiation of "void run_mha_fwd_hdim224<T>(Flash_fwd_params &, cudaStream_t) [with T=cutlass::half_t]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_hdim224_fp16_sm80.cu(8): here

/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_kernel.h(275): warning: variable "tQrQ" was declared but never referenced
          detected during:
            instantiation of "void flash::compute_attn_1rowblock<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &, int, int, int) [with Kernel_traits=Flash_fwd_kernel_traits<224, 64, 64, 4, false, false, cutlass::half_t, Flash_kernel_traits<224, 64, 64, 4, cutlass::half_t>>, Is_dropout=false, Is_causal=false, Is_even_N=false, Is_even_K=false, Return_softmax=false, Params=Flash_fwd_params]" 
(588): here
            instantiation of "void flash::compute_attn<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &) [with Kernel_traits=Flash_fwd_kernel_traits<224, 64, 64, 4, false, false, cutlass::half_t, Flash_kernel_traits<224, 64, 64, 4, cutlass::half_t>>, Is_dropout=false, Is_causal=false, Is_even_N=false, Is_even_K=false, Return_softmax=false, Params=Flash_fwd_params]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(15): here
            instantiation of "void flash_fwd_kernel<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax>(Flash_fwd_params) [with Kernel_traits=Flash_fwd_kernel_traits<224, 64, 64, 4, false, false, cutlass::half_t, Flash_kernel_traits<224, 64, 64, 4, cutlass::half_t>>, Is_dropout=false, Is_causal=false, Is_even_N=false, Is_even_K=false, Return_softmax=false]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(34): here
            instantiation of "void run_flash_fwd<Kernel_traits,Is_dropout,Is_causal>(Flash_fwd_params &, cudaStream_t) [with Kernel_traits=Flash_fwd_kernel_traits<224, 64, 64, 4, false, false, cutlass::half_t, Flash_kernel_traits<224, 64, 64, 4, cutlass::half_t>>, Is_dropout=false, Is_causal=false]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(208): here
            instantiation of "void run_mha_fwd_hdim224<T>(Flash_fwd_params &, cudaStream_t) [with T=cutlass::half_t]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_hdim224_fp16_sm80.cu(8): here

ptxas info    : 2 bytes gmem
ptxas info    : Compiling entry function '_Z16flash_fwd_kernelI23Flash_fwd_kernel_traitsILi224ELi64ELi64ELi4ELb0ELb0EN7cutlass6half_tE19Flash_kernel_traitsILi224ELi64ELi64ELi4ES2_EELb0ELb0ELb0ELb0ELb0EEv16Flash_fwd_params' for 'sm_80'
ptxas info    : Function properties for _Z16flash_fwd_kernelI23Flash_fwd_kernel_traitsILi224ELi64ELi64ELi4ELb0ELb0EN7cutlass6half_tE19Flash_kernel_traitsILi224ELi64ELi64ELi4ES2_EELb0ELb0ELb0ELb0ELb0EEv16Flash_fwd_params
    0 bytes stack frame, 0 bytes spill stores, 0 bytes spill loads
ptxas info    : Used 218 registers, 584 bytes cmem[0]
ptxas info    : Compiling entry function '_Z16flash_fwd_kernelI23Flash_fwd_kernel_traitsILi224ELi64ELi64ELi4ELb0ELb0EN7cutlass6half_tE19Flash_kernel_traitsILi224ELi64ELi64ELi4ES2_EELb0ELb0ELb0ELb1ELb0EEv16Flash_fwd_params' for 'sm_80'
ptxas info    : Function properties for _Z16flash_fwd_kernelI23Flash_fwd_kernel_traitsILi224ELi64ELi64ELi4ELb0ELb0EN7cutlass6half_tE19Flash_kernel_traitsILi224ELi64ELi64ELi4ES2_EELb0ELb0ELb0ELb1ELb0EEv16Flash_fwd_params
    0 bytes stack frame, 0 bytes spill stores, 0 bytes spill loads
ptxas info    : Used 211 registers, 584 bytes cmem[0]
ptxas info    : Compiling entry function '_Z16flash_fwd_kernelI23Flash_fwd_kernel_traitsILi224ELi64ELi64ELi4ELb0ELb0EN7cutlass6half_tE19Flash_kernel_traitsILi224ELi64ELi64ELi4ES2_EELb0ELb0ELb1ELb0ELb0EEv16Flash_fwd_params' for 'sm_80'
ptxas info    : Function properties for _Z16flash_fwd_kernelI23Flash_fwd_kernel_traitsILi224ELi64ELi64ELi4ELb0ELb0EN7cutlass6half_tE19Flash_kernel_traitsILi224ELi64ELi64ELi4ES2_EELb0ELb0ELb1ELb0ELb0EEv16Flash_fwd_params
    0 bytes stack frame, 0 bytes spill stores, 0 bytes spill loads
ptxas info    : Used 215 registers, 584 bytes cmem[0]
ptxas info    : Compiling entry function '_Z16flash_fwd_kernelI23Flash_fwd_kernel_traitsILi224ELi64ELi64ELi4ELb0ELb0EN7cutlass6half_tE19Flash_kernel_traitsILi224ELi64ELi64ELi4ES2_EELb0ELb0ELb1ELb1ELb0EEv16Flash_fwd_params' for 'sm_80'
ptxas info    : Function properties for _Z16flash_fwd_kernelI23Flash_fwd_kernel_traitsILi224ELi64ELi64ELi4ELb0ELb0EN7cutlass6half_tE19Flash_kernel_traitsILi224ELi64ELi64ELi4ES2_EELb0ELb0ELb1ELb1ELb0EEv16Flash_fwd_params
    0 bytes stack frame, 0 bytes spill stores, 0 bytes spill loads
ptxas info    : Used 211 registers, 584 bytes cmem[0]
ptxas info    : Compiling entry function '_Z16flash_fwd_kernelI23Flash_fwd_kernel_traitsILi224ELi128ELi64ELi8ELb0ELb0EN7cutlass6half_tE19Flash_kernel_traitsILi224ELi128ELi64ELi8ES2_EELb0ELb0ELb0ELb0ELb0EEv16Flash_fwd_params' for 'sm_80'
ptxas info    : Function properties for _Z16flash_fwd_kernelI23Flash_fwd_kernel_traitsILi224ELi128ELi64ELi8ELb0ELb0EN7cutlass6half_tE19Flash_kernel_traitsILi224ELi128ELi64ELi8ES2_EELb0ELb0ELb0ELb0ELb0EEv16Flash_fwd_params
    0 bytes stack frame, 0 bytes spill stores, 0 bytes spill loads
ptxas info    : Used 221 registers, 584 bytes cmem[0]
ptxas info    : Compiling entry function '_Z16flash_fwd_kernelI23Flash_fwd_kernel_traitsILi224ELi128ELi64ELi8ELb0ELb0EN7cutlass6half_tE19Flash_kernel_traitsILi224ELi128ELi64ELi8ES2_EELb0ELb0ELb0ELb1ELb0EEv16Flash_fwd_params' for 'sm_80'
ptxas info    : Function properties for _Z16flash_fwd_kernelI23Flash_fwd_kernel_traitsILi224ELi128ELi64ELi8ELb0ELb0EN7cutlass6half_tE19Flash_kernel_traitsILi224ELi128ELi64ELi8ES2_EELb0ELb0ELb0ELb1ELb0EEv16Flash_fwd_params
    0 bytes stack frame, 0 bytes spill stores, 0 bytes spill loads
ptxas info    : Used 214 registers, 584 bytes cmem[0]
ptxas info    : Compiling entry function '_Z16flash_fwd_kernelI23Flash_fwd_kernel_traitsILi224ELi128ELi64ELi8ELb0ELb0EN7cutlass6half_tE19Flash_kernel_traitsILi224ELi128ELi64ELi8ES2_EELb0ELb0ELb1ELb0ELb0EEv16Flash_fwd_params' for 'sm_80'
ptxas info    : Function properties for _Z16flash_fwd_kernelI23Flash_fwd_kernel_traitsILi224ELi128ELi64ELi8ELb0ELb0EN7cutlass6half_tE19Flash_kernel_traitsILi224ELi128ELi64ELi8ES2_EELb0ELb0ELb1ELb0ELb0EEv16Flash_fwd_params
    0 bytes stack frame, 0 bytes spill stores, 0 bytes spill loads
ptxas info    : Used 218 registers, 584 bytes cmem[0]
ptxas info    : Compiling entry function '_Z16flash_fwd_kernelI23Flash_fwd_kernel_traitsILi224ELi128ELi64ELi8ELb0ELb0EN7cutlass6half_tE19Flash_kernel_traitsILi224ELi128ELi64ELi8ES2_EELb0ELb0ELb1ELb1ELb0EEv16Flash_fwd_params' for 'sm_80'
ptxas info    : Function properties for _Z16flash_fwd_kernelI23Flash_fwd_kernel_traitsILi224ELi128ELi64ELi8ELb0ELb0EN7cutlass6half_tE19Flash_kernel_traitsILi224ELi128ELi64ELi8ES2_EELb0ELb0ELb1ELb1ELb0EEv16Flash_fwd_params
    0 bytes stack frame, 0 bytes spill stores, 0 bytes spill loads
ptxas info    : Used 212 registers, 584 bytes cmem[0]
ptxas info    : Compiling entry function '_Z16flash_fwd_kernelI23Flash_fwd_kernel_traitsILi224ELi64ELi64ELi4ELb0ELb0EN7cutlass6half_tE19Flash_kernel_traitsILi224ELi64ELi64ELi4ES2_EELb0ELb1ELb0ELb0ELb0EEv16Flash_fwd_params' for 'sm_80'
ptxas info    : Function properties for _Z16flash_fwd_kernelI23Flash_fwd_kernel_traitsILi224ELi64ELi64ELi4ELb0ELb0EN7cutlass6half_tE19Flash_kernel_traitsILi224ELi64ELi64ELi4ES2_EELb0ELb1ELb0ELb0ELb0EEv16Flash_fwd_params
    0 bytes stack frame, 0 bytes spill stores, 0 bytes spill loads
ptxas info    : Used 219 registers, 584 bytes cmem[0]
ptxas info    : Compiling entry function '_Z16flash_fwd_kernelI23Flash_fwd_kernel_traitsILi224ELi64ELi64ELi4ELb0ELb0EN7cutlass6half_tE19Flash_kernel_traitsILi224ELi64ELi64ELi4ES2_EELb0ELb1ELb0ELb1ELb0EEv16Flash_fwd_params' for 'sm_80'
ptxas info    : Function properties for _Z16flash_fwd_kernelI23Flash_fwd_kernel_traitsILi224ELi64ELi64ELi4ELb0ELb0EN7cutlass6half_tE19Flash_kernel_traitsILi224ELi64ELi64ELi4ES2_EELb0ELb1ELb0ELb1ELb0EEv16Flash_fwd_params
    0 bytes stack frame, 0 bytes spill stores, 0 bytes spill loads
ptxas info    : Used 212 registers, 584 bytes cmem[0]
ptxas info    : Compiling entry function '_Z16flash_fwd_kernelI23Flash_fwd_kernel_traitsILi224ELi64ELi64ELi4ELb0ELb0EN7cutlass6half_tE19Flash_kernel_traitsILi224ELi64ELi64ELi4ES2_EELb0ELb1ELb1ELb0ELb0EEv16Flash_fwd_params' for 'sm_80'
ptxas info    : Function properties for _Z16flash_fwd_kernelI23Flash_fwd_kernel_traitsILi224ELi64ELi64ELi4ELb0ELb0EN7cutlass6half_tE19Flash_kernel_traitsILi224ELi64ELi64ELi4ES2_EELb0ELb1ELb1ELb0ELb0EEv16Flash_fwd_params
    0 bytes stack frame, 0 bytes spill stores, 0 bytes spill loads
ptxas info    : Used 230 registers, 584 bytes cmem[0]
ptxas info    : Compiling entry function '_Z16flash_fwd_kernelI23Flash_fwd_kernel_traitsILi224ELi64ELi64ELi4ELb0ELb0EN7cutlass6half_tE19Flash_kernel_traitsILi224ELi64ELi64ELi4ES2_EELb0ELb1ELb1ELb1ELb0EEv16Flash_fwd_params' for 'sm_80'
ptxas info    : Function properties for _Z16flash_fwd_kernelI23Flash_fwd_kernel_traitsILi224ELi64ELi64ELi4ELb0ELb0EN7cutlass6half_tE19Flash_kernel_traitsILi224ELi64ELi64ELi4ES2_EELb0ELb1ELb1ELb1ELb0EEv16Flash_fwd_params
    0 bytes stack frame, 0 bytes spill stores, 0 bytes spill loads
ptxas info    : Used 222 registers, 584 bytes cmem[0]
ptxas info    : Compiling entry function '_Z16flash_fwd_kernelI23Flash_fwd_kernel_traitsILi224ELi128ELi64ELi8ELb0ELb0EN7cutlass6half_tE19Flash_kernel_traitsILi224ELi128ELi64ELi8ES2_EELb0ELb1ELb0ELb0ELb0EEv16Flash_fwd_params' for 'sm_80'
ptxas info    : Function properties for _Z16flash_fwd_kernelI23Flash_fwd_kernel_traitsILi224ELi128ELi64ELi8ELb0ELb0EN7cutlass6half_tE19Flash_kernel_traitsILi224ELi128ELi64ELi8ES2_EELb0ELb1ELb0ELb0ELb0EEv16Flash_fwd_params
    0 bytes stack frame, 0 bytes spill stores, 0 bytes spill loads
ptxas info    : Used 234 registers, 584 bytes cmem[0]
ptxas info    : Compiling entry function '_Z16flash_fwd_kernelI23Flash_fwd_kernel_traitsILi224ELi128ELi64ELi8ELb0ELb0EN7cutlass6half_tE19Flash_kernel_traitsILi224ELi128ELi64ELi8ES2_EELb0ELb1ELb0ELb1ELb0EEv16Flash_fwd_params' for 'sm_80'
ptxas info    : Function properties for _Z16flash_fwd_kernelI23Flash_fwd_kernel_traitsILi224ELi128ELi64ELi8ELb0ELb0EN7cutlass6half_tE19Flash_kernel_traitsILi224ELi128ELi64ELi8ES2_EELb0ELb1ELb0ELb1ELb0EEv16Flash_fwd_params
    0 bytes stack frame, 0 bytes spill stores, 0 bytes spill loads
ptxas info    : Used 226 registers, 584 bytes cmem[0]
ptxas info    : Compiling entry function '_Z16flash_fwd_kernelI23Flash_fwd_kernel_traitsILi224ELi128ELi64ELi8ELb0ELb0EN7cutlass6half_tE19Flash_kernel_traitsILi224ELi128ELi64ELi8ES2_EELb0ELb1ELb1ELb0ELb0EEv16Flash_fwd_params' for 'sm_80'
ptxas info    : Function properties for _Z16flash_fwd_kernelI23Flash_fwd_kernel_traitsILi224ELi128ELi64ELi8ELb0ELb0EN7cutlass6half_tE19Flash_kernel_traitsILi224ELi128ELi64ELi8ES2_EELb0ELb1ELb1ELb0ELb0EEv16Flash_fwd_params
    48 bytes stack frame, 80 bytes spill stores, 72 bytes spill loads
ptxas info    : Used 255 registers, 584 bytes cmem[0]
ptxas info    : Compiling entry function '_Z16flash_fwd_kernelI23Flash_fwd_kernel_traitsILi224ELi128ELi64ELi8ELb0ELb0EN7cutlass6half_tE19Flash_kernel_traitsILi224ELi128ELi64ELi8ES2_EELb0ELb1ELb1ELb1ELb0EEv16Flash_fwd_params' for 'sm_80'
ptxas info    : Function properties for _Z16flash_fwd_kernelI23Flash_fwd_kernel_traitsILi224ELi128ELi64ELi8ELb0ELb0EN7cutlass6half_tE19Flash_kernel_traitsILi224ELi128ELi64ELi8ES2_EELb0ELb1ELb1ELb1ELb0EEv16Flash_fwd_params
    0 bytes stack frame, 0 bytes spill stores, 0 bytes spill loads
ptxas info    : Used 228 registers, 584 bytes cmem[0]
ptxas info    : Compiling entry function '_Z16flash_fwd_kernelI23Flash_fwd_kernel_traitsILi224ELi64ELi64ELi4ELb0ELb0EN7cutlass6half_tE19Flash_kernel_traitsILi224ELi64ELi64ELi4ES2_EELb1ELb0ELb0ELb0ELb0EEv16Flash_fwd_params' for 'sm_80'
ptxas info    : Function properties for _Z16flash_fwd_kernelI23Flash_fwd_kernel_traitsILi224ELi64ELi64ELi4ELb0ELb0EN7cutlass6half_tE19Flash_kernel_traitsILi224ELi64ELi64ELi4ES2_EELb1ELb0ELb0ELb0ELb0EEv16Flash_fwd_params
    0 bytes stack frame, 0 bytes spill stores, 0 bytes spill loads
ptxas info    : Used 230 registers, 584 bytes cmem[0]
ptxas info    : Compiling entry function '_Z16flash_fwd_kernelI23Flash_fwd_kernel_traitsILi224ELi64ELi64ELi4ELb0ELb0EN7cutlass6half_tE19Flash_kernel_traitsILi224ELi64ELi64ELi4ES2_EELb1ELb0ELb0ELb0ELb1EEv16Flash_fwd_params' for 'sm_80'
ptxas info    : Function properties for _Z16flash_fwd_kernelI23Flash_fwd_kernel_traitsILi224ELi64ELi64ELi4ELb0ELb0EN7cutlass6half_tE19Flash_kernel_traitsILi224ELi64ELi64ELi4ES2_EELb1ELb0ELb0ELb0ELb1EEv16Flash_fwd_params
    48 bytes stack frame, 60 bytes spill stores, 52 bytes spill loads
ptxas info    : Used 255 registers, 584 bytes cmem[0]
ptxas info    : Compiling entry function '_Z16flash_fwd_kernelI23Flash_fwd_kernel_traitsILi224ELi64ELi64ELi4ELb0ELb0EN7cutlass6half_tE19Flash_kernel_traitsILi224ELi64ELi64ELi4ES2_EELb1ELb0ELb0ELb1ELb0EEv16Flash_fwd_params' for 'sm_80'
ptxas info    : Function properties for _Z16flash_fwd_kernelI23Flash_fwd_kernel_traitsILi224ELi64ELi64ELi4ELb0ELb0EN7cutlass6half_tE19Flash_kernel_traitsILi224ELi64ELi64ELi4ES2_EELb1ELb0ELb0ELb1ELb0EEv16Flash_fwd_params
    0 bytes stack frame, 0 bytes spill stores, 0 bytes spill loads
ptxas info    : Used 222 registers, 584 bytes cmem[0]
ptxas info    : Compiling entry function '_Z16flash_fwd_kernelI23Flash_fwd_kernel_traitsILi224ELi64ELi64ELi4ELb0ELb0EN7cutlass6half_tE19Flash_kernel_traitsILi224ELi64ELi64ELi4ES2_EELb1ELb0ELb0ELb1ELb1EEv16Flash_fwd_params' for 'sm_80'
ptxas info    : Function properties for _Z16flash_fwd_kernelI23Flash_fwd_kernel_traitsILi224ELi64ELi64ELi4ELb0ELb0EN7cutlass6half_tE19Flash_kernel_traitsILi224ELi64ELi64ELi4ES2_EELb1ELb0ELb0ELb1ELb1EEv16Flash_fwd_params
    40 bytes stack frame, 68 bytes spill stores, 44 bytes spill loads
ptxas info    : Used 255 registers, 584 bytes cmem[0]
ptxas info    : Compiling entry function '_Z16flash_fwd_kernelI23Flash_fwd_kernel_traitsILi224ELi64ELi64ELi4ELb0ELb0EN7cutlass6half_tE19Flash_kernel_traitsILi224ELi64ELi64ELi4ES2_EELb1ELb0ELb1ELb0ELb0EEv16Flash_fwd_params' for 'sm_80'
ptxas info    : Function properties for _Z16flash_fwd_kernelI23Flash_fwd_kernel_traitsILi224ELi64ELi64ELi4ELb0ELb0EN7cutlass6half_tE19Flash_kernel_traitsILi224ELi64ELi64ELi4ES2_EELb1ELb0ELb1ELb0ELb0EEv16Flash_fwd_params
    0 bytes stack frame, 0 bytes spill stores, 0 bytes spill loads
ptxas info    : Used 228 registers, 584 bytes cmem[0]
ptxas info    : Compiling entry function '_Z16flash_fwd_kernelI23Flash_fwd_kernel_traitsILi224ELi64ELi64ELi4ELb0ELb0EN7cutlass6half_tE19Flash_kernel_traitsILi224ELi64ELi64ELi4ES2_EELb1ELb0ELb1ELb0ELb1EEv16Flash_fwd_params' for 'sm_80'
ptxas info    : Function properties for _Z16flash_fwd_kernelI23Flash_fwd_kernel_traitsILi224ELi64ELi64ELi4ELb0ELb0EN7cutlass6half_tE19Flash_kernel_traitsILi224ELi64ELi64ELi4ES2_EELb1ELb0ELb1ELb0ELb1EEv16Flash_fwd_params
    64 bytes stack frame, 68 bytes spill stores, 60 bytes spill loads
ptxas info    : Used 255 registers, 584 bytes cmem[0]
ptxas info    : Compiling entry function '_Z16flash_fwd_kernelI23Flash_fwd_kernel_traitsILi224ELi64ELi64ELi4ELb0ELb0EN7cutlass6half_tE19Flash_kernel_traitsILi224ELi64ELi64ELi4ES2_EELb1ELb0ELb1ELb1ELb0EEv16Flash_fwd_params' for 'sm_80'
ptxas info    : Function properties for _Z16flash_fwd_kernelI23Flash_fwd_kernel_traitsILi224ELi64ELi64ELi4ELb0ELb0EN7cutlass6half_tE19Flash_kernel_traitsILi224ELi64ELi64ELi4ES2_EELb1ELb0ELb1ELb1ELb0EEv16Flash_fwd_params
    0 bytes stack frame, 0 bytes spill stores, 0 bytes spill loads
ptxas info    : Used 222 registers, 584 bytes cmem[0]
ptxas info    : Compiling entry function '_Z16flash_fwd_kernelI23Flash_fwd_kernel_traitsILi224ELi64ELi64ELi4ELb0ELb0EN7cutlass6half_tE19Flash_kernel_traitsILi224ELi64ELi64ELi4ES2_EELb1ELb0ELb1ELb1ELb1EEv16Flash_fwd_params' for 'sm_80'
ptxas info    : Function properties for _Z16flash_fwd_kernelI23Flash_fwd_kernel_traitsILi224ELi64ELi64ELi4ELb0ELb0EN7cutlass6half_tE19Flash_kernel_traitsILi224ELi64ELi64ELi4ES2_EELb1ELb0ELb1ELb1ELb1EEv16Flash_fwd_params
    48 bytes stack frame, 62 bytes spill stores, 54 bytes spill loads
ptxas info    : Used 255 registers, 584 bytes cmem[0]
ptxas info    : Compiling entry function '_Z16flash_fwd_kernelI23Flash_fwd_kernel_traitsILi224ELi128ELi64ELi8ELb0ELb0EN7cutlass6half_tE19Flash_kernel_traitsILi224ELi128ELi64ELi8ES2_EELb1ELb0ELb0ELb0ELb0EEv16Flash_fwd_params' for 'sm_80'
ptxas info    : Function properties for _Z16flash_fwd_kernelI23Flash_fwd_kernel_traitsILi224ELi128ELi64ELi8ELb0ELb0EN7cutlass6half_tE19Flash_kernel_traitsILi224ELi128ELi64ELi8ES2_EELb1ELb0ELb0ELb0ELb0EEv16Flash_fwd_params
    0 bytes stack frame, 0 bytes spill stores, 0 bytes spill loads
ptxas info    : Used 234 registers, 584 bytes cmem[0]
ptxas info    : Compiling entry function '_Z16flash_fwd_kernelI23Flash_fwd_kernel_traitsILi224ELi128ELi64ELi8ELb0ELb0EN7cutlass6half_tE19Flash_kernel_traitsILi224ELi128ELi64ELi8ES2_EELb1ELb0ELb0ELb0ELb1EEv16Flash_fwd_params' for 'sm_80'
ptxas info    : Function properties for _Z16flash_fwd_kernelI23Flash_fwd_kernel_traitsILi224ELi128ELi64ELi8ELb0ELb0EN7cutlass6half_tE19Flash_kernel_traitsILi224ELi128ELi64ELi8ES2_EELb1ELb0ELb0ELb0ELb1EEv16Flash_fwd_params
    48 bytes stack frame, 44 bytes spill stores, 44 bytes spill loads
ptxas info    : Used 255 registers, 584 bytes cmem[0]
ptxas info    : Compiling entry function '_Z16flash_fwd_kernelI23Flash_fwd_kernel_traitsILi224ELi128ELi64ELi8ELb0ELb0EN7cutlass6half_tE19Flash_kernel_traitsILi224ELi128ELi64ELi8ES2_EELb1ELb0ELb0ELb1ELb0EEv16Flash_fwd_params' for 'sm_80'
ptxas info    : Function properties for _Z16flash_fwd_kernelI23Flash_fwd_kernel_traitsILi224ELi128ELi64ELi8ELb0ELb0EN7cutlass6half_tE19Flash_kernel_traitsILi224ELi128ELi64ELi8ES2_EELb1ELb0ELb0ELb1ELb0EEv16Flash_fwd_params
    0 bytes stack frame, 0 bytes spill stores, 0 bytes spill loads
ptxas info    : Used 226 registers, 584 bytes cmem[0]
ptxas info    : Compiling entry function '_Z16flash_fwd_kernelI23Flash_fwd_kernel_traitsILi224ELi128ELi64ELi8ELb0ELb0EN7cutlass6half_tE19Flash_kernel_traitsILi224ELi128ELi64ELi8ES2_EELb1ELb0ELb0ELb1ELb1EEv16Flash_fwd_params' for 'sm_80'
ptxas info    : Function properties for _Z16flash_fwd_kernelI23Flash_fwd_kernel_traitsILi224ELi128ELi64ELi8ELb0ELb0EN7cutlass6half_tE19Flash_kernel_traitsILi224ELi128ELi64ELi8ES2_EELb1ELb0ELb0ELb1ELb1EEv16Flash_fwd_params
    40 bytes stack frame, 72 bytes spill stores, 48 bytes spill loads
ptxas info    : Used 255 registers, 584 bytes cmem[0]
ptxas info    : Compiling entry function '_Z16flash_fwd_kernelI23Flash_fwd_kernel_traitsILi224ELi128ELi64ELi8ELb0ELb0EN7cutlass6half_tE19Flash_kernel_traitsILi224ELi128ELi64ELi8ES2_EELb1ELb0ELb1ELb0ELb0EEv16Flash_fwd_params' for 'sm_80'
ptxas info    : Function properties for _Z16flash_fwd_kernelI23Flash_fwd_kernel_traitsILi224ELi128ELi64ELi8ELb0ELb0EN7cutlass6half_tE19Flash_kernel_traitsILi224ELi128ELi64ELi8ES2_EELb1ELb0ELb1ELb0ELb0EEv16Flash_fwd_params
    0 bytes stack frame, 0 bytes spill stores, 0 bytes spill loads
ptxas info    : Used 234 registers, 584 bytes cmem[0]
ptxas info    : Compiling entry function '_Z16flash_fwd_kernelI23Flash_fwd_kernel_traitsILi224ELi128ELi64ELi8ELb0ELb0EN7cutlass6half_tE19Flash_kernel_traitsILi224ELi128ELi64ELi8ES2_EELb1ELb0ELb1ELb0ELb1EEv16Flash_fwd_params' for 'sm_80'
ptxas info    : Function properties for _Z16flash_fwd_kernelI23Flash_fwd_kernel_traitsILi224ELi128ELi64ELi8ELb0ELb0EN7cutlass6half_tE19Flash_kernel_traitsILi224ELi128ELi64ELi8ES2_EELb1ELb0ELb1ELb0ELb1EEv16Flash_fwd_params
    64 bytes stack frame, 56 bytes spill stores, 56 bytes spill loads
ptxas info    : Used 255 registers, 584 bytes cmem[0]
ptxas info    : Compiling entry function '_Z16flash_fwd_kernelI23Flash_fwd_kernel_traitsILi224ELi128ELi64ELi8ELb0ELb0EN7cutlass6half_tE19Flash_kernel_traitsILi224ELi128ELi64ELi8ES2_EELb1ELb0ELb1ELb1ELb0EEv16Flash_fwd_params' for 'sm_80'
ptxas info    : Function properties for _Z16flash_fwd_kernelI23Flash_fwd_kernel_traitsILi224ELi128ELi64ELi8ELb0ELb0EN7cutlass6half_tE19Flash_kernel_traitsILi224ELi128ELi64ELi8ES2_EELb1ELb0ELb1ELb1ELb0EEv16Flash_fwd_params
    0 bytes stack frame, 0 bytes spill stores, 0 bytes spill loads
ptxas info    : Used 222 registers, 584 bytes cmem[0]
ptxas info    : Compiling entry function '_Z16flash_fwd_kernelI23Flash_fwd_kernel_traitsILi224ELi128ELi64ELi8ELb0ELb0EN7cutlass6half_tE19Flash_kernel_traitsILi224ELi128ELi64ELi8ES2_EELb1ELb0ELb1ELb1ELb1EEv16Flash_fwd_params' for 'sm_80'
ptxas info    : Function properties for _Z16flash_fwd_kernelI23Flash_fwd_kernel_traitsILi224ELi128ELi64ELi8ELb0ELb0EN7cutlass6half_tE19Flash_kernel_traitsILi224ELi128ELi64ELi8ES2_EELb1ELb0ELb1ELb1ELb1EEv16Flash_fwd_params
    104 bytes stack frame, 100 bytes spill stores, 92 bytes spill loads
ptxas info    : Used 255 registers, 584 bytes cmem[0]
ptxas info    : Compiling entry function '_Z16flash_fwd_kernelI23Flash_fwd_kernel_traitsILi224ELi64ELi64ELi4ELb0ELb0EN7cutlass6half_tE19Flash_kernel_traitsILi224ELi64ELi64ELi4ES2_EELb1ELb1ELb0ELb0ELb0EEv16Flash_fwd_params' for 'sm_80'
ptxas info    : Function properties for _Z16flash_fwd_kernelI23Flash_fwd_kernel_traitsILi224ELi64ELi64ELi4ELb0ELb0EN7cutlass6half_tE19Flash_kernel_traitsILi224ELi64ELi64ELi4ES2_EELb1ELb1ELb0ELb0ELb0EEv16Flash_fwd_params
    0 bytes stack frame, 0 bytes spill stores, 0 bytes spill loads
ptxas info    : Used 232 registers, 584 bytes cmem[0]
ptxas info    : Compiling entry function '_Z16flash_fwd_kernelI23Flash_fwd_kernel_traitsILi224ELi64ELi64ELi4ELb0ELb0EN7cutlass6half_tE19Flash_kernel_traitsILi224ELi64ELi64ELi4ES2_EELb1ELb1ELb0ELb0ELb1EEv16Flash_fwd_params' for 'sm_80'
ptxas info    : Function properties for _Z16flash_fwd_kernelI23Flash_fwd_kernel_traitsILi224ELi64ELi64ELi4ELb0ELb0EN7cutlass6half_tE19Flash_kernel_traitsILi224ELi64ELi64ELi4ES2_EELb1ELb1ELb0ELb0ELb1EEv16Flash_fwd_params
    48 bytes stack frame, 62 bytes spill stores, 54 bytes spill loads
ptxas info    : Used 255 registers, 584 bytes cmem[0]
ptxas info    : Compiling entry function '_Z16flash_fwd_kernelI23Flash_fwd_kernel_traitsILi224ELi64ELi64ELi4ELb0ELb0EN7cutlass6half_tE19Flash_kernel_traitsILi224ELi64ELi64ELi4ES2_EELb1ELb1ELb0ELb1ELb0EEv16Flash_fwd_params' for 'sm_80'
ptxas info    : Function properties for _Z16flash_fwd_kernelI23Flash_fwd_kernel_traitsILi224ELi64ELi64ELi4ELb0ELb0EN7cutlass6half_tE19Flash_kernel_traitsILi224ELi64ELi64ELi4ES2_EELb1ELb1ELb0ELb1ELb0EEv16Flash_fwd_params
    0 bytes stack frame, 0 bytes spill stores, 0 bytes spill loads
ptxas info    : Used 222 registers, 584 bytes cmem[0]
ptxas info    : Compiling entry function '_Z16flash_fwd_kernelI23Flash_fwd_kernel_traitsILi224ELi64ELi64ELi4ELb0ELb0EN7cutlass6half_tE19Flash_kernel_traitsILi224ELi64ELi64ELi4ES2_EELb1ELb1ELb0ELb1ELb1EEv16Flash_fwd_params' for 'sm_80'
ptxas info    : Function properties for _Z16flash_fwd_kernelI23Flash_fwd_kernel_traitsILi224ELi64ELi64ELi4ELb0ELb0EN7cutlass6half_tE19Flash_kernel_traitsILi224ELi64ELi64ELi4ES2_EELb1ELb1ELb0ELb1ELb1EEv16Flash_fwd_params
    32 bytes stack frame, 56 bytes spill stores, 32 bytes spill loads
ptxas info    : Used 255 registers, 584 bytes cmem[0]
ptxas info    : Compiling entry function '_Z16flash_fwd_kernelI23Flash_fwd_kernel_traitsILi224ELi64ELi64ELi4ELb0ELb0EN7cutlass6half_tE19Flash_kernel_traitsILi224ELi64ELi64ELi4ES2_EELb1ELb1ELb1ELb0ELb0EEv16Flash_fwd_params' for 'sm_80'
ptxas info    : Function properties for _Z16flash_fwd_kernelI23Flash_fwd_kernel_traitsILi224ELi64ELi64ELi4ELb0ELb0EN7cutlass6half_tE19Flash_kernel_traitsILi224ELi64ELi64ELi4ES2_EELb1ELb1ELb1ELb0ELb0EEv16Flash_fwd_params
    0 bytes stack frame, 0 bytes spill stores, 0 bytes spill loads
ptxas info    : Used 238 registers, 584 bytes cmem[0]
ptxas info    : Compiling entry function '_Z16flash_fwd_kernelI23Flash_fwd_kernel_traitsILi224ELi64ELi64ELi4ELb0ELb0EN7cutlass6half_tE19Flash_kernel_traitsILi224ELi64ELi64ELi4ES2_EELb1ELb1ELb1ELb0ELb1EEv16Flash_fwd_params' for 'sm_80'
ptxas info    : Function properties for _Z16flash_fwd_kernelI23Flash_fwd_kernel_traitsILi224ELi64ELi64ELi4ELb0ELb0EN7cutlass6half_tE19Flash_kernel_traitsILi224ELi64ELi64ELi4ES2_EELb1ELb1ELb1ELb0ELb1EEv16Flash_fwd_params
    64 bytes stack frame, 70 bytes spill stores, 62 bytes spill loads
ptxas info    : Used 255 registers, 584 bytes cmem[0]
ptxas info    : Compiling entry function '_Z16flash_fwd_kernelI23Flash_fwd_kernel_traitsILi224ELi64ELi64ELi4ELb0ELb0EN7cutlass6half_tE19Flash_kernel_traitsILi224ELi64ELi64ELi4ES2_EELb1ELb1ELb1ELb1ELb0EEv16Flash_fwd_params' for 'sm_80'
ptxas info    : Function properties for _Z16flash_fwd_kernelI23Flash_fwd_kernel_traitsILi224ELi64ELi64ELi4ELb0ELb0EN7cutlass6half_tE19Flash_kernel_traitsILi224ELi64ELi64ELi4ES2_EELb1ELb1ELb1ELb1ELb0EEv16Flash_fwd_params
    0 bytes stack frame, 0 bytes spill stores, 0 bytes spill loads
ptxas info    : Used 232 registers, 584 bytes cmem[0]
ptxas info    : Compiling entry function '_Z16flash_fwd_kernelI23Flash_fwd_kernel_traitsILi224ELi64ELi64ELi4ELb0ELb0EN7cutlass6half_tE19Flash_kernel_traitsILi224ELi64ELi64ELi4ES2_EELb1ELb1ELb1ELb1ELb1EEv16Flash_fwd_params' for 'sm_80'
ptxas info    : Function properties for _Z16flash_fwd_kernelI23Flash_fwd_kernel_traitsILi224ELi64ELi64ELi4ELb0ELb0EN7cutlass6half_tE19Flash_kernel_traitsILi224ELi64ELi64ELi4ES2_EELb1ELb1ELb1ELb1ELb1EEv16Flash_fwd_params
    32 bytes stack frame, 40 bytes spill stores, 32 bytes spill loads
ptxas info    : Used 255 registers, 584 bytes cmem[0]
ptxas info    : Compiling entry function '_Z16flash_fwd_kernelI23Flash_fwd_kernel_traitsILi224ELi128ELi64ELi8ELb0ELb0EN7cutlass6half_tE19Flash_kernel_traitsILi224ELi128ELi64ELi8ES2_EELb1ELb1ELb0ELb0ELb0EEv16Flash_fwd_params' for 'sm_80'
ptxas info    : Function properties for _Z16flash_fwd_kernelI23Flash_fwd_kernel_traitsILi224ELi128ELi64ELi8ELb0ELb0EN7cutlass6half_tE19Flash_kernel_traitsILi224ELi128ELi64ELi8ES2_EELb1ELb1ELb0ELb0ELb0EEv16Flash_fwd_params
    72 bytes stack frame, 72 bytes spill stores, 72 bytes spill loads
ptxas info    : Used 255 registers, 584 bytes cmem[0]
ptxas info    : Compiling entry function '_Z16flash_fwd_kernelI23Flash_fwd_kernel_traitsILi224ELi128ELi64ELi8ELb0ELb0EN7cutlass6half_tE19Flash_kernel_traitsILi224ELi128ELi64ELi8ES2_EELb1ELb1ELb0ELb0ELb1EEv16Flash_fwd_params' for 'sm_80'
ptxas info    : Function properties for _Z16flash_fwd_kernelI23Flash_fwd_kernel_traitsILi224ELi128ELi64ELi8ELb0ELb0EN7cutlass6half_tE19Flash_kernel_traitsILi224ELi128ELi64ELi8ES2_EELb1ELb1ELb0ELb0ELb1EEv16Flash_fwd_params
    80 bytes stack frame, 106 bytes spill stores, 110 bytes spill loads
ptxas info    : Used 255 registers, 584 bytes cmem[0]
ptxas info    : Compiling entry function '_Z16flash_fwd_kernelI23Flash_fwd_kernel_traitsILi224ELi128ELi64ELi8ELb0ELb0EN7cutlass6half_tE19Flash_kernel_traitsILi224ELi128ELi64ELi8ES2_EELb1ELb1ELb0ELb1ELb0EEv16Flash_fwd_params' for 'sm_80'
ptxas info    : Function properties for _Z16flash_fwd_kernelI23Flash_fwd_kernel_traitsILi224ELi128ELi64ELi8ELb0ELb0EN7cutlass6half_tE19Flash_kernel_traitsILi224ELi128ELi64ELi8ES2_EELb1ELb1ELb0ELb1ELb0EEv16Flash_fwd_params
    0 bytes stack frame, 0 bytes spill stores, 0 bytes spill loads
ptxas info    : Used 251 registers, 584 bytes cmem[0]
ptxas info    : Compiling entry function '_Z16flash_fwd_kernelI23Flash_fwd_kernel_traitsILi224ELi128ELi64ELi8ELb0ELb0EN7cutlass6half_tE19Flash_kernel_traitsILi224ELi128ELi64ELi8ES2_EELb1ELb1ELb0ELb1ELb1EEv16Flash_fwd_params' for 'sm_80'
ptxas info    : Function properties for _Z16flash_fwd_kernelI23Flash_fwd_kernel_traitsILi224ELi128ELi64ELi8ELb0ELb0EN7cutlass6half_tE19Flash_kernel_traitsILi224ELi128ELi64ELi8ES2_EELb1ELb1ELb0ELb1ELb1EEv16Flash_fwd_params
    88 bytes stack frame, 60 bytes spill stores, 78 bytes spill loads
ptxas info    : Used 255 registers, 584 bytes cmem[0]
ptxas info    : Compiling entry function '_Z16flash_fwd_kernelI23Flash_fwd_kernel_traitsILi224ELi128ELi64ELi8ELb0ELb0EN7cutlass6half_tE19Flash_kernel_traitsILi224ELi128ELi64ELi8ES2_EELb1ELb1ELb1ELb0ELb0EEv16Flash_fwd_params' for 'sm_80'
ptxas info    : Function properties for _Z16flash_fwd_kernelI23Flash_fwd_kernel_traitsILi224ELi128ELi64ELi8ELb0ELb0EN7cutlass6half_tE19Flash_kernel_traitsILi224ELi128ELi64ELi8ES2_EELb1ELb1ELb1ELb0ELb0EEv16Flash_fwd_params
    72 bytes stack frame, 84 bytes spill stores, 84 bytes spill loads
ptxas info    : Used 255 registers, 584 bytes cmem[0]
ptxas info    : Compiling entry function '_Z16flash_fwd_kernelI23Flash_fwd_kernel_traitsILi224ELi128ELi64ELi8ELb0ELb0EN7cutlass6half_tE19Flash_kernel_traitsILi224ELi128ELi64ELi8ES2_EELb1ELb1ELb1ELb0ELb1EEv16Flash_fwd_params' for 'sm_80'
ptxas info    : Function properties for _Z16flash_fwd_kernelI23Flash_fwd_kernel_traitsILi224ELi128ELi64ELi8ELb0ELb0EN7cutlass6half_tE19Flash_kernel_traitsILi224ELi128ELi64ELi8ES2_EELb1ELb1ELb1ELb0ELb1EEv16Flash_fwd_params
    56 bytes stack frame, 68 bytes spill stores, 60 bytes spill loads
ptxas info    : Used 255 registers, 584 bytes cmem[0]
ptxas info    : Compiling entry function '_Z16flash_fwd_kernelI23Flash_fwd_kernel_traitsILi224ELi128ELi64ELi8ELb0ELb0EN7cutlass6half_tE19Flash_kernel_traitsILi224ELi128ELi64ELi8ES2_EELb1ELb1ELb1ELb1ELb0EEv16Flash_fwd_params' for 'sm_80'
ptxas info    : Function properties for _Z16flash_fwd_kernelI23Flash_fwd_kernel_traitsILi224ELi128ELi64ELi8ELb0ELb0EN7cutlass6half_tE19Flash_kernel_traitsILi224ELi128ELi64ELi8ES2_EELb1ELb1ELb1ELb1ELb0EEv16Flash_fwd_params
    0 bytes stack frame, 0 bytes spill stores, 0 bytes spill loads
ptxas info    : Used 235 registers, 584 bytes cmem[0]
ptxas info    : Compiling entry function '_Z16flash_fwd_kernelI23Flash_fwd_kernel_traitsILi224ELi128ELi64ELi8ELb0ELb0EN7cutlass6half_tE19Flash_kernel_traitsILi224ELi128ELi64ELi8ES2_EELb1ELb1ELb1ELb1ELb1EEv16Flash_fwd_params' for 'sm_80'
ptxas info    : Function properties for _Z16flash_fwd_kernelI23Flash_fwd_kernel_traitsILi224ELi128ELi64ELi8ELb0ELb0EN7cutlass6half_tE19Flash_kernel_traitsILi224ELi128ELi64ELi8ES2_EELb1ELb1ELb1ELb1ELb1EEv16Flash_fwd_params
    88 bytes stack frame, 64 bytes spill stores, 94 bytes spill loads
ptxas info    : Used 255 registers, 584 bytes cmem[0]
/anaconda/envs/flashattention/lib/python3.9/site-packages/torch/include/c10/core/SymInt.h(84): warning: integer conversion resulted in a change of sign

[10/13] /usr/local/cuda-11.4/bin/nvcc  -I/home/v-feiychen/flash-attention/csrc/flash_attn -I/home/v-feiychen/flash-attention/csrc/flash_attn/src -I/home/v-feiychen/flash-attention/csrc/cutlass/include -I/anaconda/envs/flashattention/lib/python3.9/site-packages/torch/include -I/anaconda/envs/flashattention/lib/python3.9/site-packages/torch/include/torch/csrc/api/include -I/anaconda/envs/flashattention/lib/python3.9/site-packages/torch/include/TH -I/anaconda/envs/flashattention/lib/python3.9/site-packages/torch/include/THC -I/usr/local/cuda-11.4/include -I/anaconda/envs/flashattention/include/python3.9 -c -c /home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_hdim256_bf16_sm80.cu -o /home/v-feiychen/flash-attention/build/temp.linux-x86_64-cpython-39/csrc/flash_attn/src/flash_fwd_hdim256_bf16_sm80.o -D__CUDA_NO_HALF_OPERATORS__ -D__CUDA_NO_HALF_CONVERSIONS__ -D__CUDA_NO_BFLOAT16_CONVERSIONS__ -D__CUDA_NO_HALF2_OPERATORS__ --expt-relaxed-constexpr --compiler-options ''"'"'-fPIC'"'"'' -O3 -std=c++17 -U__CUDA_NO_HALF_OPERATORS__ -U__CUDA_NO_HALF_CONVERSIONS__ -U__CUDA_NO_HALF2_OPERATORS__ -U__CUDA_NO_BFLOAT16_CONVERSIONS__ --expt-relaxed-constexpr --expt-extended-lambda --use_fast_math --ptxas-options=-v -lineinfo -gencode arch=compute_80,code=sm_80 --threads 4 -DTORCH_API_INCLUDE_EXTENSION_H '-DPYBIND11_COMPILER_TYPE="_gcc"' '-DPYBIND11_STDLIB="_libstdcpp"' '-DPYBIND11_BUILD_ABI="_cxxabi1011"' -DTORCH_EXTENSION_NAME=flash_attn_2_cuda -D_GLIBCXX_USE_CXX11_ABI=0
/anaconda/envs/flashattention/lib/python3.9/site-packages/torch/include/c10/core/SymInt.h(84): warning: integer conversion resulted in a change of sign

/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_kernel.h(140): warning: variable "MMA_M" was declared but never referenced
          detected during:
            instantiation of "void flash::compute_attn_1rowblock<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &, int, int, int) [with Kernel_traits=Flash_fwd_kernel_traits<256, 128, 64, 8, false, false, cutlass::bfloat16_t, Flash_kernel_traits<256, 128, 64, 8, cutlass::bfloat16_t>>, Is_dropout=true, Is_causal=true, Is_even_N=true, Is_even_K=true, Return_softmax=true, Params=Flash_fwd_params]" 
(588): here
            instantiation of "void flash::compute_attn<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &) [with Kernel_traits=Flash_fwd_kernel_traits<256, 128, 64, 8, false, false, cutlass::bfloat16_t, Flash_kernel_traits<256, 128, 64, 8, cutlass::bfloat16_t>>, Is_dropout=true, Is_causal=true, Is_even_N=true, Is_even_K=true, Return_softmax=true, Params=Flash_fwd_params]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(15): here
            instantiation of "void flash_fwd_kernel<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax>(Flash_fwd_params) [with Kernel_traits=Flash_fwd_kernel_traits<256, 128, 64, 8, false, false, cutlass::bfloat16_t, Flash_kernel_traits<256, 128, 64, 8, cutlass::bfloat16_t>>, Is_dropout=true, Is_causal=true, Is_even_N=true, Is_even_K=true, Return_softmax=true]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(34): here
            instantiation of "void run_flash_fwd<Kernel_traits,Is_dropout,Is_causal>(Flash_fwd_params &, cudaStream_t) [with Kernel_traits=Flash_fwd_kernel_traits<256, 128, 64, 8, false, false, cutlass::bfloat16_t, Flash_kernel_traits<256, 128, 64, 8, cutlass::bfloat16_t>>, Is_dropout=true, Is_causal=true]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(236): here
            instantiation of "void run_mha_fwd_hdim256<T>(Flash_fwd_params &, cudaStream_t) [with T=cutlass::bfloat16_t]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_hdim256_bf16_sm80.cu(8): here

/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_kernel.h(275): warning: variable "tQrQ" was declared but never referenced
          detected during:
            instantiation of "void flash::compute_attn_1rowblock<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &, int, int, int) [with Kernel_traits=Flash_fwd_kernel_traits<256, 128, 64, 8, false, false, cutlass::bfloat16_t, Flash_kernel_traits<256, 128, 64, 8, cutlass::bfloat16_t>>, Is_dropout=true, Is_causal=true, Is_even_N=true, Is_even_K=true, Return_softmax=true, Params=Flash_fwd_params]" 
(588): here
            instantiation of "void flash::compute_attn<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &) [with Kernel_traits=Flash_fwd_kernel_traits<256, 128, 64, 8, false, false, cutlass::bfloat16_t, Flash_kernel_traits<256, 128, 64, 8, cutlass::bfloat16_t>>, Is_dropout=true, Is_causal=true, Is_even_N=true, Is_even_K=true, Return_softmax=true, Params=Flash_fwd_params]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(15): here
            instantiation of "void flash_fwd_kernel<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax>(Flash_fwd_params) [with Kernel_traits=Flash_fwd_kernel_traits<256, 128, 64, 8, false, false, cutlass::bfloat16_t, Flash_kernel_traits<256, 128, 64, 8, cutlass::bfloat16_t>>, Is_dropout=true, Is_causal=true, Is_even_N=true, Is_even_K=true, Return_softmax=true]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(34): here
            instantiation of "void run_flash_fwd<Kernel_traits,Is_dropout,Is_causal>(Flash_fwd_params &, cudaStream_t) [with Kernel_traits=Flash_fwd_kernel_traits<256, 128, 64, 8, false, false, cutlass::bfloat16_t, Flash_kernel_traits<256, 128, 64, 8, cutlass::bfloat16_t>>, Is_dropout=true, Is_causal=true]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(236): here
            instantiation of "void run_mha_fwd_hdim256<T>(Flash_fwd_params &, cudaStream_t) [with T=cutlass::bfloat16_t]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_hdim256_bf16_sm80.cu(8): here

/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_kernel.h(140): warning: variable "MMA_M" was declared but never referenced
          detected during:
            instantiation of "void flash::compute_attn_1rowblock<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &, int, int, int) [with Kernel_traits=Flash_fwd_kernel_traits<256, 128, 64, 8, false, false, cutlass::bfloat16_t, Flash_kernel_traits<256, 128, 64, 8, cutlass::bfloat16_t>>, Is_dropout=true, Is_causal=true, Is_even_N=true, Is_even_K=true, Return_softmax=false, Params=Flash_fwd_params]" 
(588): here
            instantiation of "void flash::compute_attn<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &) [with Kernel_traits=Flash_fwd_kernel_traits<256, 128, 64, 8, false, false, cutlass::bfloat16_t, Flash_kernel_traits<256, 128, 64, 8, cutlass::bfloat16_t>>, Is_dropout=true, Is_causal=true, Is_even_N=true, Is_even_K=true, Return_softmax=false, Params=Flash_fwd_params]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(15): here
            instantiation of "void flash_fwd_kernel<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax>(Flash_fwd_params) [with Kernel_traits=Flash_fwd_kernel_traits<256, 128, 64, 8, false, false, cutlass::bfloat16_t, Flash_kernel_traits<256, 128, 64, 8, cutlass::bfloat16_t>>, Is_dropout=true, Is_causal=true, Is_even_N=true, Is_even_K=true, Return_softmax=false]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(34): here
            instantiation of "void run_flash_fwd<Kernel_traits,Is_dropout,Is_causal>(Flash_fwd_params &, cudaStream_t) [with Kernel_traits=Flash_fwd_kernel_traits<256, 128, 64, 8, false, false, cutlass::bfloat16_t, Flash_kernel_traits<256, 128, 64, 8, cutlass::bfloat16_t>>, Is_dropout=true, Is_causal=true]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(236): here
            instantiation of "void run_mha_fwd_hdim256<T>(Flash_fwd_params &, cudaStream_t) [with T=cutlass::bfloat16_t]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_hdim256_bf16_sm80.cu(8): here

/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_kernel.h(275): warning: variable "tQrQ" was declared but never referenced
          detected during:
            instantiation of "void flash::compute_attn_1rowblock<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &, int, int, int) [with Kernel_traits=Flash_fwd_kernel_traits<256, 128, 64, 8, false, false, cutlass::bfloat16_t, Flash_kernel_traits<256, 128, 64, 8, cutlass::bfloat16_t>>, Is_dropout=true, Is_causal=true, Is_even_N=true, Is_even_K=true, Return_softmax=false, Params=Flash_fwd_params]" 
(588): here
            instantiation of "void flash::compute_attn<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &) [with Kernel_traits=Flash_fwd_kernel_traits<256, 128, 64, 8, false, false, cutlass::bfloat16_t, Flash_kernel_traits<256, 128, 64, 8, cutlass::bfloat16_t>>, Is_dropout=true, Is_causal=true, Is_even_N=true, Is_even_K=true, Return_softmax=false, Params=Flash_fwd_params]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(15): here
            instantiation of "void flash_fwd_kernel<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax>(Flash_fwd_params) [with Kernel_traits=Flash_fwd_kernel_traits<256, 128, 64, 8, false, false, cutlass::bfloat16_t, Flash_kernel_traits<256, 128, 64, 8, cutlass::bfloat16_t>>, Is_dropout=true, Is_causal=true, Is_even_N=true, Is_even_K=true, Return_softmax=false]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(34): here
            instantiation of "void run_flash_fwd<Kernel_traits,Is_dropout,Is_causal>(Flash_fwd_params &, cudaStream_t) [with Kernel_traits=Flash_fwd_kernel_traits<256, 128, 64, 8, false, false, cutlass::bfloat16_t, Flash_kernel_traits<256, 128, 64, 8, cutlass::bfloat16_t>>, Is_dropout=true, Is_causal=true]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(236): here
            instantiation of "void run_mha_fwd_hdim256<T>(Flash_fwd_params &, cudaStream_t) [with T=cutlass::bfloat16_t]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_hdim256_bf16_sm80.cu(8): here

/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_kernel.h(140): warning: variable "MMA_M" was declared but never referenced
          detected during:
            instantiation of "void flash::compute_attn_1rowblock<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &, int, int, int) [with Kernel_traits=Flash_fwd_kernel_traits<256, 128, 64, 8, false, false, cutlass::bfloat16_t, Flash_kernel_traits<256, 128, 64, 8, cutlass::bfloat16_t>>, Is_dropout=true, Is_causal=true, Is_even_N=true, Is_even_K=false, Return_softmax=true, Params=Flash_fwd_params]" 
(588): here
            instantiation of "void flash::compute_attn<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &) [with Kernel_traits=Flash_fwd_kernel_traits<256, 128, 64, 8, false, false, cutlass::bfloat16_t, Flash_kernel_traits<256, 128, 64, 8, cutlass::bfloat16_t>>, Is_dropout=true, Is_causal=true, Is_even_N=true, Is_even_K=false, Return_softmax=true, Params=Flash_fwd_params]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(15): here
            instantiation of "void flash_fwd_kernel<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax>(Flash_fwd_params) [with Kernel_traits=Flash_fwd_kernel_traits<256, 128, 64, 8, false, false, cutlass::bfloat16_t, Flash_kernel_traits<256, 128, 64, 8, cutlass::bfloat16_t>>, Is_dropout=true, Is_causal=true, Is_even_N=true, Is_even_K=false, Return_softmax=true]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(34): here
            instantiation of "void run_flash_fwd<Kernel_traits,Is_dropout,Is_causal>(Flash_fwd_params &, cudaStream_t) [with Kernel_traits=Flash_fwd_kernel_traits<256, 128, 64, 8, false, false, cutlass::bfloat16_t, Flash_kernel_traits<256, 128, 64, 8, cutlass::bfloat16_t>>, Is_dropout=true, Is_causal=true]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(236): here
            instantiation of "void run_mha_fwd_hdim256<T>(Flash_fwd_params &, cudaStream_t) [with T=cutlass::bfloat16_t]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_hdim256_bf16_sm80.cu(8): here

/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_kernel.h(275): warning: variable "tQrQ" was declared but never referenced
          detected during:
            instantiation of "void flash::compute_attn_1rowblock<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &, int, int, int) [with Kernel_traits=Flash_fwd_kernel_traits<256, 128, 64, 8, false, false, cutlass::bfloat16_t, Flash_kernel_traits<256, 128, 64, 8, cutlass::bfloat16_t>>, Is_dropout=true, Is_causal=true, Is_even_N=true, Is_even_K=false, Return_softmax=true, Params=Flash_fwd_params]" 
(588): here
            instantiation of "void flash::compute_attn<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &) [with Kernel_traits=Flash_fwd_kernel_traits<256, 128, 64, 8, false, false, cutlass::bfloat16_t, Flash_kernel_traits<256, 128, 64, 8, cutlass::bfloat16_t>>, Is_dropout=true, Is_causal=true, Is_even_N=true, Is_even_K=false, Return_softmax=true, Params=Flash_fwd_params]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(15): here
            instantiation of "void flash_fwd_kernel<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax>(Flash_fwd_params) [with Kernel_traits=Flash_fwd_kernel_traits<256, 128, 64, 8, false, false, cutlass::bfloat16_t, Flash_kernel_traits<256, 128, 64, 8, cutlass::bfloat16_t>>, Is_dropout=true, Is_causal=true, Is_even_N=true, Is_even_K=false, Return_softmax=true]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(34): here
            instantiation of "void run_flash_fwd<Kernel_traits,Is_dropout,Is_causal>(Flash_fwd_params &, cudaStream_t) [with Kernel_traits=Flash_fwd_kernel_traits<256, 128, 64, 8, false, false, cutlass::bfloat16_t, Flash_kernel_traits<256, 128, 64, 8, cutlass::bfloat16_t>>, Is_dropout=true, Is_causal=true]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(236): here
            instantiation of "void run_mha_fwd_hdim256<T>(Flash_fwd_params &, cudaStream_t) [with T=cutlass::bfloat16_t]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_hdim256_bf16_sm80.cu(8): here

/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_kernel.h(140): warning: variable "MMA_M" was declared but never referenced
          detected during:
            instantiation of "void flash::compute_attn_1rowblock<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &, int, int, int) [with Kernel_traits=Flash_fwd_kernel_traits<256, 128, 64, 8, false, false, cutlass::bfloat16_t, Flash_kernel_traits<256, 128, 64, 8, cutlass::bfloat16_t>>, Is_dropout=true, Is_causal=true, Is_even_N=true, Is_even_K=false, Return_softmax=false, Params=Flash_fwd_params]" 
(588): here
            instantiation of "void flash::compute_attn<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &) [with Kernel_traits=Flash_fwd_kernel_traits<256, 128, 64, 8, false, false, cutlass::bfloat16_t, Flash_kernel_traits<256, 128, 64, 8, cutlass::bfloat16_t>>, Is_dropout=true, Is_causal=true, Is_even_N=true, Is_even_K=false, Return_softmax=false, Params=Flash_fwd_params]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(15): here
            instantiation of "void flash_fwd_kernel<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax>(Flash_fwd_params) [with Kernel_traits=Flash_fwd_kernel_traits<256, 128, 64, 8, false, false, cutlass::bfloat16_t, Flash_kernel_traits<256, 128, 64, 8, cutlass::bfloat16_t>>, Is_dropout=true, Is_causal=true, Is_even_N=true, Is_even_K=false, Return_softmax=false]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(34): here
            instantiation of "void run_flash_fwd<Kernel_traits,Is_dropout,Is_causal>(Flash_fwd_params &, cudaStream_t) [with Kernel_traits=Flash_fwd_kernel_traits<256, 128, 64, 8, false, false, cutlass::bfloat16_t, Flash_kernel_traits<256, 128, 64, 8, cutlass::bfloat16_t>>, Is_dropout=true, Is_causal=true]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(236): here
            instantiation of "void run_mha_fwd_hdim256<T>(Flash_fwd_params &, cudaStream_t) [with T=cutlass::bfloat16_t]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_hdim256_bf16_sm80.cu(8): here

/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_kernel.h(275): warning: variable "tQrQ" was declared but never referenced
          detected during:
            instantiation of "void flash::compute_attn_1rowblock<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &, int, int, int) [with Kernel_traits=Flash_fwd_kernel_traits<256, 128, 64, 8, false, false, cutlass::bfloat16_t, Flash_kernel_traits<256, 128, 64, 8, cutlass::bfloat16_t>>, Is_dropout=true, Is_causal=true, Is_even_N=true, Is_even_K=false, Return_softmax=false, Params=Flash_fwd_params]" 
(588): here
            instantiation of "void flash::compute_attn<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &) [with Kernel_traits=Flash_fwd_kernel_traits<256, 128, 64, 8, false, false, cutlass::bfloat16_t, Flash_kernel_traits<256, 128, 64, 8, cutlass::bfloat16_t>>, Is_dropout=true, Is_causal=true, Is_even_N=true, Is_even_K=false, Return_softmax=false, Params=Flash_fwd_params]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(15): here
            instantiation of "void flash_fwd_kernel<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax>(Flash_fwd_params) [with Kernel_traits=Flash_fwd_kernel_traits<256, 128, 64, 8, false, false, cutlass::bfloat16_t, Flash_kernel_traits<256, 128, 64, 8, cutlass::bfloat16_t>>, Is_dropout=true, Is_causal=true, Is_even_N=true, Is_even_K=false, Return_softmax=false]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(34): here
            instantiation of "void run_flash_fwd<Kernel_traits,Is_dropout,Is_causal>(Flash_fwd_params &, cudaStream_t) [with Kernel_traits=Flash_fwd_kernel_traits<256, 128, 64, 8, false, false, cutlass::bfloat16_t, Flash_kernel_traits<256, 128, 64, 8, cutlass::bfloat16_t>>, Is_dropout=true, Is_causal=true]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(236): here
            instantiation of "void run_mha_fwd_hdim256<T>(Flash_fwd_params &, cudaStream_t) [with T=cutlass::bfloat16_t]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_hdim256_bf16_sm80.cu(8): here

/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_kernel.h(140): warning: variable "MMA_M" was declared but never referenced
          detected during:
            instantiation of "void flash::compute_attn_1rowblock<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &, int, int, int) [with Kernel_traits=Flash_fwd_kernel_traits<256, 128, 64, 8, false, false, cutlass::bfloat16_t, Flash_kernel_traits<256, 128, 64, 8, cutlass::bfloat16_t>>, Is_dropout=true, Is_causal=true, Is_even_N=false, Is_even_K=true, Return_softmax=true, Params=Flash_fwd_params]" 
(588): here
            instantiation of "void flash::compute_attn<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &) [with Kernel_traits=Flash_fwd_kernel_traits<256, 128, 64, 8, false, false, cutlass::bfloat16_t, Flash_kernel_traits<256, 128, 64, 8, cutlass::bfloat16_t>>, Is_dropout=true, Is_causal=true, Is_even_N=false, Is_even_K=true, Return_softmax=true, Params=Flash_fwd_params]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(15): here
            instantiation of "void flash_fwd_kernel<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax>(Flash_fwd_params) [with Kernel_traits=Flash_fwd_kernel_traits<256, 128, 64, 8, false, false, cutlass::bfloat16_t, Flash_kernel_traits<256, 128, 64, 8, cutlass::bfloat16_t>>, Is_dropout=true, Is_causal=true, Is_even_N=false, Is_even_K=true, Return_softmax=true]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(34): here
            instantiation of "void run_flash_fwd<Kernel_traits,Is_dropout,Is_causal>(Flash_fwd_params &, cudaStream_t) [with Kernel_traits=Flash_fwd_kernel_traits<256, 128, 64, 8, false, false, cutlass::bfloat16_t, Flash_kernel_traits<256, 128, 64, 8, cutlass::bfloat16_t>>, Is_dropout=true, Is_causal=true]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(236): here
            instantiation of "void run_mha_fwd_hdim256<T>(Flash_fwd_params &, cudaStream_t) [with T=cutlass::bfloat16_t]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_hdim256_bf16_sm80.cu(8): here

/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_kernel.h(275): warning: variable "tQrQ" was declared but never referenced
          detected during:
            instantiation of "void flash::compute_attn_1rowblock<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &, int, int, int) [with Kernel_traits=Flash_fwd_kernel_traits<256, 128, 64, 8, false, false, cutlass::bfloat16_t, Flash_kernel_traits<256, 128, 64, 8, cutlass::bfloat16_t>>, Is_dropout=true, Is_causal=true, Is_even_N=false, Is_even_K=true, Return_softmax=true, Params=Flash_fwd_params]" 
(588): here
            instantiation of "void flash::compute_attn<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &) [with Kernel_traits=Flash_fwd_kernel_traits<256, 128, 64, 8, false, false, cutlass::bfloat16_t, Flash_kernel_traits<256, 128, 64, 8, cutlass::bfloat16_t>>, Is_dropout=true, Is_causal=true, Is_even_N=false, Is_even_K=true, Return_softmax=true, Params=Flash_fwd_params]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(15): here
            instantiation of "void flash_fwd_kernel<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax>(Flash_fwd_params) [with Kernel_traits=Flash_fwd_kernel_traits<256, 128, 64, 8, false, false, cutlass::bfloat16_t, Flash_kernel_traits<256, 128, 64, 8, cutlass::bfloat16_t>>, Is_dropout=true, Is_causal=true, Is_even_N=false, Is_even_K=true, Return_softmax=true]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(34): here
            instantiation of "void run_flash_fwd<Kernel_traits,Is_dropout,Is_causal>(Flash_fwd_params &, cudaStream_t) [with Kernel_traits=Flash_fwd_kernel_traits<256, 128, 64, 8, false, false, cutlass::bfloat16_t, Flash_kernel_traits<256, 128, 64, 8, cutlass::bfloat16_t>>, Is_dropout=true, Is_causal=true]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(236): here
            instantiation of "void run_mha_fwd_hdim256<T>(Flash_fwd_params &, cudaStream_t) [with T=cutlass::bfloat16_t]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_hdim256_bf16_sm80.cu(8): here

/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_kernel.h(140): warning: variable "MMA_M" was declared but never referenced
          detected during:
            instantiation of "void flash::compute_attn_1rowblock<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &, int, int, int) [with Kernel_traits=Flash_fwd_kernel_traits<256, 128, 64, 8, false, false, cutlass::bfloat16_t, Flash_kernel_traits<256, 128, 64, 8, cutlass::bfloat16_t>>, Is_dropout=true, Is_causal=true, Is_even_N=false, Is_even_K=true, Return_softmax=false, Params=Flash_fwd_params]" 
(588): here
            instantiation of "void flash::compute_attn<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &) [with Kernel_traits=Flash_fwd_kernel_traits<256, 128, 64, 8, false, false, cutlass::bfloat16_t, Flash_kernel_traits<256, 128, 64, 8, cutlass::bfloat16_t>>, Is_dropout=true, Is_causal=true, Is_even_N=false, Is_even_K=true, Return_softmax=false, Params=Flash_fwd_params]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(15): here
            instantiation of "void flash_fwd_kernel<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax>(Flash_fwd_params) [with Kernel_traits=Flash_fwd_kernel_traits<256, 128, 64, 8, false, false, cutlass::bfloat16_t, Flash_kernel_traits<256, 128, 64, 8, cutlass::bfloat16_t>>, Is_dropout=true, Is_causal=true, Is_even_N=false, Is_even_K=true, Return_softmax=false]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(34): here
            instantiation of "void run_flash_fwd<Kernel_traits,Is_dropout,Is_causal>(Flash_fwd_params &, cudaStream_t) [with Kernel_traits=Flash_fwd_kernel_traits<256, 128, 64, 8, false, false, cutlass::bfloat16_t, Flash_kernel_traits<256, 128, 64, 8, cutlass::bfloat16_t>>, Is_dropout=true, Is_causal=true]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(236): here
            instantiation of "void run_mha_fwd_hdim256<T>(Flash_fwd_params &, cudaStream_t) [with T=cutlass::bfloat16_t]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_hdim256_bf16_sm80.cu(8): here

/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_kernel.h(275): warning: variable "tQrQ" was declared but never referenced
          detected during:
            instantiation of "void flash::compute_attn_1rowblock<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &, int, int, int) [with Kernel_traits=Flash_fwd_kernel_traits<256, 128, 64, 8, false, false, cutlass::bfloat16_t, Flash_kernel_traits<256, 128, 64, 8, cutlass::bfloat16_t>>, Is_dropout=true, Is_causal=true, Is_even_N=false, Is_even_K=true, Return_softmax=false, Params=Flash_fwd_params]" 
(588): here
            instantiation of "void flash::compute_attn<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &) [with Kernel_traits=Flash_fwd_kernel_traits<256, 128, 64, 8, false, false, cutlass::bfloat16_t, Flash_kernel_traits<256, 128, 64, 8, cutlass::bfloat16_t>>, Is_dropout=true, Is_causal=true, Is_even_N=false, Is_even_K=true, Return_softmax=false, Params=Flash_fwd_params]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(15): here
            instantiation of "void flash_fwd_kernel<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax>(Flash_fwd_params) [with Kernel_traits=Flash_fwd_kernel_traits<256, 128, 64, 8, false, false, cutlass::bfloat16_t, Flash_kernel_traits<256, 128, 64, 8, cutlass::bfloat16_t>>, Is_dropout=true, Is_causal=true, Is_even_N=false, Is_even_K=true, Return_softmax=false]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(34): here
            instantiation of "void run_flash_fwd<Kernel_traits,Is_dropout,Is_causal>(Flash_fwd_params &, cudaStream_t) [with Kernel_traits=Flash_fwd_kernel_traits<256, 128, 64, 8, false, false, cutlass::bfloat16_t, Flash_kernel_traits<256, 128, 64, 8, cutlass::bfloat16_t>>, Is_dropout=true, Is_causal=true]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(236): here
            instantiation of "void run_mha_fwd_hdim256<T>(Flash_fwd_params &, cudaStream_t) [with T=cutlass::bfloat16_t]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_hdim256_bf16_sm80.cu(8): here

/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_kernel.h(140): warning: variable "MMA_M" was declared but never referenced
          detected during:
            instantiation of "void flash::compute_attn_1rowblock<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &, int, int, int) [with Kernel_traits=Flash_fwd_kernel_traits<256, 128, 64, 8, false, false, cutlass::bfloat16_t, Flash_kernel_traits<256, 128, 64, 8, cutlass::bfloat16_t>>, Is_dropout=true, Is_causal=true, Is_even_N=false, Is_even_K=false, Return_softmax=true, Params=Flash_fwd_params]" 
(588): here
            instantiation of "void flash::compute_attn<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &) [with Kernel_traits=Flash_fwd_kernel_traits<256, 128, 64, 8, false, false, cutlass::bfloat16_t, Flash_kernel_traits<256, 128, 64, 8, cutlass::bfloat16_t>>, Is_dropout=true, Is_causal=true, Is_even_N=false, Is_even_K=false, Return_softmax=true, Params=Flash_fwd_params]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(15): here
            instantiation of "void flash_fwd_kernel<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax>(Flash_fwd_params) [with Kernel_traits=Flash_fwd_kernel_traits<256, 128, 64, 8, false, false, cutlass::bfloat16_t, Flash_kernel_traits<256, 128, 64, 8, cutlass::bfloat16_t>>, Is_dropout=true, Is_causal=true, Is_even_N=false, Is_even_K=false, Return_softmax=true]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(34): here
            instantiation of "void run_flash_fwd<Kernel_traits,Is_dropout,Is_causal>(Flash_fwd_params &, cudaStream_t) [with Kernel_traits=Flash_fwd_kernel_traits<256, 128, 64, 8, false, false, cutlass::bfloat16_t, Flash_kernel_traits<256, 128, 64, 8, cutlass::bfloat16_t>>, Is_dropout=true, Is_causal=true]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(236): here
            instantiation of "void run_mha_fwd_hdim256<T>(Flash_fwd_params &, cudaStream_t) [with T=cutlass::bfloat16_t]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_hdim256_bf16_sm80.cu(8): here

/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_kernel.h(275): warning: variable "tQrQ" was declared but never referenced
          detected during:
            instantiation of "void flash::compute_attn_1rowblock<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &, int, int, int) [with Kernel_traits=Flash_fwd_kernel_traits<256, 128, 64, 8, false, false, cutlass::bfloat16_t, Flash_kernel_traits<256, 128, 64, 8, cutlass::bfloat16_t>>, Is_dropout=true, Is_causal=true, Is_even_N=false, Is_even_K=false, Return_softmax=true, Params=Flash_fwd_params]" 
(588): here
            instantiation of "void flash::compute_attn<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &) [with Kernel_traits=Flash_fwd_kernel_traits<256, 128, 64, 8, false, false, cutlass::bfloat16_t, Flash_kernel_traits<256, 128, 64, 8, cutlass::bfloat16_t>>, Is_dropout=true, Is_causal=true, Is_even_N=false, Is_even_K=false, Return_softmax=true, Params=Flash_fwd_params]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(15): here
            instantiation of "void flash_fwd_kernel<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax>(Flash_fwd_params) [with Kernel_traits=Flash_fwd_kernel_traits<256, 128, 64, 8, false, false, cutlass::bfloat16_t, Flash_kernel_traits<256, 128, 64, 8, cutlass::bfloat16_t>>, Is_dropout=true, Is_causal=true, Is_even_N=false, Is_even_K=false, Return_softmax=true]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(34): here
            instantiation of "void run_flash_fwd<Kernel_traits,Is_dropout,Is_causal>(Flash_fwd_params &, cudaStream_t) [with Kernel_traits=Flash_fwd_kernel_traits<256, 128, 64, 8, false, false, cutlass::bfloat16_t, Flash_kernel_traits<256, 128, 64, 8, cutlass::bfloat16_t>>, Is_dropout=true, Is_causal=true]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(236): here
            instantiation of "void run_mha_fwd_hdim256<T>(Flash_fwd_params &, cudaStream_t) [with T=cutlass::bfloat16_t]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_hdim256_bf16_sm80.cu(8): here

/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_kernel.h(140): warning: variable "MMA_M" was declared but never referenced
          detected during:
            instantiation of "void flash::compute_attn_1rowblock<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &, int, int, int) [with Kernel_traits=Flash_fwd_kernel_traits<256, 128, 64, 8, false, false, cutlass::bfloat16_t, Flash_kernel_traits<256, 128, 64, 8, cutlass::bfloat16_t>>, Is_dropout=true, Is_causal=true, Is_even_N=false, Is_even_K=false, Return_softmax=false, Params=Flash_fwd_params]" 
(588): here
            instantiation of "void flash::compute_attn<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &) [with Kernel_traits=Flash_fwd_kernel_traits<256, 128, 64, 8, false, false, cutlass::bfloat16_t, Flash_kernel_traits<256, 128, 64, 8, cutlass::bfloat16_t>>, Is_dropout=true, Is_causal=true, Is_even_N=false, Is_even_K=false, Return_softmax=false, Params=Flash_fwd_params]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(15): here
            instantiation of "void flash_fwd_kernel<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax>(Flash_fwd_params) [with Kernel_traits=Flash_fwd_kernel_traits<256, 128, 64, 8, false, false, cutlass::bfloat16_t, Flash_kernel_traits<256, 128, 64, 8, cutlass::bfloat16_t>>, Is_dropout=true, Is_causal=true, Is_even_N=false, Is_even_K=false, Return_softmax=false]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(34): here
            instantiation of "void run_flash_fwd<Kernel_traits,Is_dropout,Is_causal>(Flash_fwd_params &, cudaStream_t) [with Kernel_traits=Flash_fwd_kernel_traits<256, 128, 64, 8, false, false, cutlass::bfloat16_t, Flash_kernel_traits<256, 128, 64, 8, cutlass::bfloat16_t>>, Is_dropout=true, Is_causal=true]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(236): here
            instantiation of "void run_mha_fwd_hdim256<T>(Flash_fwd_params &, cudaStream_t) [with T=cutlass::bfloat16_t]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_hdim256_bf16_sm80.cu(8): here

/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_kernel.h(275): warning: variable "tQrQ" was declared but never referenced
          detected during:
            instantiation of "void flash::compute_attn_1rowblock<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &, int, int, int) [with Kernel_traits=Flash_fwd_kernel_traits<256, 128, 64, 8, false, false, cutlass::bfloat16_t, Flash_kernel_traits<256, 128, 64, 8, cutlass::bfloat16_t>>, Is_dropout=true, Is_causal=true, Is_even_N=false, Is_even_K=false, Return_softmax=false, Params=Flash_fwd_params]" 
(588): here
            instantiation of "void flash::compute_attn<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &) [with Kernel_traits=Flash_fwd_kernel_traits<256, 128, 64, 8, false, false, cutlass::bfloat16_t, Flash_kernel_traits<256, 128, 64, 8, cutlass::bfloat16_t>>, Is_dropout=true, Is_causal=true, Is_even_N=false, Is_even_K=false, Return_softmax=false, Params=Flash_fwd_params]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(15): here
            instantiation of "void flash_fwd_kernel<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax>(Flash_fwd_params) [with Kernel_traits=Flash_fwd_kernel_traits<256, 128, 64, 8, false, false, cutlass::bfloat16_t, Flash_kernel_traits<256, 128, 64, 8, cutlass::bfloat16_t>>, Is_dropout=true, Is_causal=true, Is_even_N=false, Is_even_K=false, Return_softmax=false]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(34): here
            instantiation of "void run_flash_fwd<Kernel_traits,Is_dropout,Is_causal>(Flash_fwd_params &, cudaStream_t) [with Kernel_traits=Flash_fwd_kernel_traits<256, 128, 64, 8, false, false, cutlass::bfloat16_t, Flash_kernel_traits<256, 128, 64, 8, cutlass::bfloat16_t>>, Is_dropout=true, Is_causal=true]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(236): here
            instantiation of "void run_mha_fwd_hdim256<T>(Flash_fwd_params &, cudaStream_t) [with T=cutlass::bfloat16_t]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_hdim256_bf16_sm80.cu(8): here

/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_kernel.h(140): warning: variable "MMA_M" was declared but never referenced
          detected during:
            instantiation of "void flash::compute_attn_1rowblock<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &, int, int, int) [with Kernel_traits=Flash_fwd_kernel_traits<256, 64, 64, 4, false, false, cutlass::bfloat16_t, Flash_kernel_traits<256, 64, 64, 4, cutlass::bfloat16_t>>, Is_dropout=true, Is_causal=true, Is_even_N=true, Is_even_K=true, Return_softmax=true, Params=Flash_fwd_params]" 
(588): here
            instantiation of "void flash::compute_attn<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &) [with Kernel_traits=Flash_fwd_kernel_traits<256, 64, 64, 4, false, false, cutlass::bfloat16_t, Flash_kernel_traits<256, 64, 64, 4, cutlass::bfloat16_t>>, Is_dropout=true, Is_causal=true, Is_even_N=true, Is_even_K=true, Return_softmax=true, Params=Flash_fwd_params]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(15): here
            instantiation of "void flash_fwd_kernel<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax>(Flash_fwd_params) [with Kernel_traits=Flash_fwd_kernel_traits<256, 64, 64, 4, false, false, cutlass::bfloat16_t, Flash_kernel_traits<256, 64, 64, 4, cutlass::bfloat16_t>>, Is_dropout=true, Is_causal=true, Is_even_N=true, Is_even_K=true, Return_softmax=true]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(34): here
            instantiation of "void run_flash_fwd<Kernel_traits,Is_dropout,Is_causal>(Flash_fwd_params &, cudaStream_t) [with Kernel_traits=Flash_fwd_kernel_traits<256, 64, 64, 4, false, false, cutlass::bfloat16_t, Flash_kernel_traits<256, 64, 64, 4, cutlass::bfloat16_t>>, Is_dropout=true, Is_causal=true]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(236): here
            instantiation of "void run_mha_fwd_hdim256<T>(Flash_fwd_params &, cudaStream_t) [with T=cutlass::bfloat16_t]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_hdim256_bf16_sm80.cu(8): here

/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_kernel.h(275): warning: variable "tQrQ" was declared but never referenced
          detected during:
            instantiation of "void flash::compute_attn_1rowblock<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &, int, int, int) [with Kernel_traits=Flash_fwd_kernel_traits<256, 64, 64, 4, false, false, cutlass::bfloat16_t, Flash_kernel_traits<256, 64, 64, 4, cutlass::bfloat16_t>>, Is_dropout=true, Is_causal=true, Is_even_N=true, Is_even_K=true, Return_softmax=true, Params=Flash_fwd_params]" 
(588): here
            instantiation of "void flash::compute_attn<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &) [with Kernel_traits=Flash_fwd_kernel_traits<256, 64, 64, 4, false, false, cutlass::bfloat16_t, Flash_kernel_traits<256, 64, 64, 4, cutlass::bfloat16_t>>, Is_dropout=true, Is_causal=true, Is_even_N=true, Is_even_K=true, Return_softmax=true, Params=Flash_fwd_params]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(15): here
            instantiation of "void flash_fwd_kernel<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax>(Flash_fwd_params) [with Kernel_traits=Flash_fwd_kernel_traits<256, 64, 64, 4, false, false, cutlass::bfloat16_t, Flash_kernel_traits<256, 64, 64, 4, cutlass::bfloat16_t>>, Is_dropout=true, Is_causal=true, Is_even_N=true, Is_even_K=true, Return_softmax=true]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(34): here
            instantiation of "void run_flash_fwd<Kernel_traits,Is_dropout,Is_causal>(Flash_fwd_params &, cudaStream_t) [with Kernel_traits=Flash_fwd_kernel_traits<256, 64, 64, 4, false, false, cutlass::bfloat16_t, Flash_kernel_traits<256, 64, 64, 4, cutlass::bfloat16_t>>, Is_dropout=true, Is_causal=true]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(236): here
            instantiation of "void run_mha_fwd_hdim256<T>(Flash_fwd_params &, cudaStream_t) [with T=cutlass::bfloat16_t]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_hdim256_bf16_sm80.cu(8): here

/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_kernel.h(140): warning: variable "MMA_M" was declared but never referenced
          detected during:
            instantiation of "void flash::compute_attn_1rowblock<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &, int, int, int) [with Kernel_traits=Flash_fwd_kernel_traits<256, 64, 64, 4, false, false, cutlass::bfloat16_t, Flash_kernel_traits<256, 64, 64, 4, cutlass::bfloat16_t>>, Is_dropout=true, Is_causal=true, Is_even_N=true, Is_even_K=true, Return_softmax=false, Params=Flash_fwd_params]" 
(588): here
            instantiation of "void flash::compute_attn<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &) [with Kernel_traits=Flash_fwd_kernel_traits<256, 64, 64, 4, false, false, cutlass::bfloat16_t, Flash_kernel_traits<256, 64, 64, 4, cutlass::bfloat16_t>>, Is_dropout=true, Is_causal=true, Is_even_N=true, Is_even_K=true, Return_softmax=false, Params=Flash_fwd_params]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(15): here
            instantiation of "void flash_fwd_kernel<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax>(Flash_fwd_params) [with Kernel_traits=Flash_fwd_kernel_traits<256, 64, 64, 4, false, false, cutlass::bfloat16_t, Flash_kernel_traits<256, 64, 64, 4, cutlass::bfloat16_t>>, Is_dropout=true, Is_causal=true, Is_even_N=true, Is_even_K=true, Return_softmax=false]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(34): here
            instantiation of "void run_flash_fwd<Kernel_traits,Is_dropout,Is_causal>(Flash_fwd_params &, cudaStream_t) [with Kernel_traits=Flash_fwd_kernel_traits<256, 64, 64, 4, false, false, cutlass::bfloat16_t, Flash_kernel_traits<256, 64, 64, 4, cutlass::bfloat16_t>>, Is_dropout=true, Is_causal=true]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(236): here
            instantiation of "void run_mha_fwd_hdim256<T>(Flash_fwd_params &, cudaStream_t) [with T=cutlass::bfloat16_t]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_hdim256_bf16_sm80.cu(8): here

/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_kernel.h(275): warning: variable "tQrQ" was declared but never referenced
          detected during:
            instantiation of "void flash::compute_attn_1rowblock<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &, int, int, int) [with Kernel_traits=Flash_fwd_kernel_traits<256, 64, 64, 4, false, false, cutlass::bfloat16_t, Flash_kernel_traits<256, 64, 64, 4, cutlass::bfloat16_t>>, Is_dropout=true, Is_causal=true, Is_even_N=true, Is_even_K=true, Return_softmax=false, Params=Flash_fwd_params]" 
(588): here
            instantiation of "void flash::compute_attn<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &) [with Kernel_traits=Flash_fwd_kernel_traits<256, 64, 64, 4, false, false, cutlass::bfloat16_t, Flash_kernel_traits<256, 64, 64, 4, cutlass::bfloat16_t>>, Is_dropout=true, Is_causal=true, Is_even_N=true, Is_even_K=true, Return_softmax=false, Params=Flash_fwd_params]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(15): here
            instantiation of "void flash_fwd_kernel<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax>(Flash_fwd_params) [with Kernel_traits=Flash_fwd_kernel_traits<256, 64, 64, 4, false, false, cutlass::bfloat16_t, Flash_kernel_traits<256, 64, 64, 4, cutlass::bfloat16_t>>, Is_dropout=true, Is_causal=true, Is_even_N=true, Is_even_K=true, Return_softmax=false]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(34): here
            instantiation of "void run_flash_fwd<Kernel_traits,Is_dropout,Is_causal>(Flash_fwd_params &, cudaStream_t) [with Kernel_traits=Flash_fwd_kernel_traits<256, 64, 64, 4, false, false, cutlass::bfloat16_t, Flash_kernel_traits<256, 64, 64, 4, cutlass::bfloat16_t>>, Is_dropout=true, Is_causal=true]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(236): here
            instantiation of "void run_mha_fwd_hdim256<T>(Flash_fwd_params &, cudaStream_t) [with T=cutlass::bfloat16_t]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_hdim256_bf16_sm80.cu(8): here

/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_kernel.h(140): warning: variable "MMA_M" was declared but never referenced
          detected during:
            instantiation of "void flash::compute_attn_1rowblock<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &, int, int, int) [with Kernel_traits=Flash_fwd_kernel_traits<256, 64, 64, 4, false, false, cutlass::bfloat16_t, Flash_kernel_traits<256, 64, 64, 4, cutlass::bfloat16_t>>, Is_dropout=true, Is_causal=true, Is_even_N=true, Is_even_K=false, Return_softmax=true, Params=Flash_fwd_params]" 
(588): here
            instantiation of "void flash::compute_attn<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &) [with Kernel_traits=Flash_fwd_kernel_traits<256, 64, 64, 4, false, false, cutlass::bfloat16_t, Flash_kernel_traits<256, 64, 64, 4, cutlass::bfloat16_t>>, Is_dropout=true, Is_causal=true, Is_even_N=true, Is_even_K=false, Return_softmax=true, Params=Flash_fwd_params]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(15): here
            instantiation of "void flash_fwd_kernel<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax>(Flash_fwd_params) [with Kernel_traits=Flash_fwd_kernel_traits<256, 64, 64, 4, false, false, cutlass::bfloat16_t, Flash_kernel_traits<256, 64, 64, 4, cutlass::bfloat16_t>>, Is_dropout=true, Is_causal=true, Is_even_N=true, Is_even_K=false, Return_softmax=true]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(34): here
            instantiation of "void run_flash_fwd<Kernel_traits,Is_dropout,Is_causal>(Flash_fwd_params &, cudaStream_t) [with Kernel_traits=Flash_fwd_kernel_traits<256, 64, 64, 4, false, false, cutlass::bfloat16_t, Flash_kernel_traits<256, 64, 64, 4, cutlass::bfloat16_t>>, Is_dropout=true, Is_causal=true]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(236): here
            instantiation of "void run_mha_fwd_hdim256<T>(Flash_fwd_params &, cudaStream_t) [with T=cutlass::bfloat16_t]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_hdim256_bf16_sm80.cu(8): here

/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_kernel.h(275): warning: variable "tQrQ" was declared but never referenced
          detected during:
            instantiation of "void flash::compute_attn_1rowblock<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &, int, int, int) [with Kernel_traits=Flash_fwd_kernel_traits<256, 64, 64, 4, false, false, cutlass::bfloat16_t, Flash_kernel_traits<256, 64, 64, 4, cutlass::bfloat16_t>>, Is_dropout=true, Is_causal=true, Is_even_N=true, Is_even_K=false, Return_softmax=true, Params=Flash_fwd_params]" 
(588): here
            instantiation of "void flash::compute_attn<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &) [with Kernel_traits=Flash_fwd_kernel_traits<256, 64, 64, 4, false, false, cutlass::bfloat16_t, Flash_kernel_traits<256, 64, 64, 4, cutlass::bfloat16_t>>, Is_dropout=true, Is_causal=true, Is_even_N=true, Is_even_K=false, Return_softmax=true, Params=Flash_fwd_params]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(15): here
            instantiation of "void flash_fwd_kernel<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax>(Flash_fwd_params) [with Kernel_traits=Flash_fwd_kernel_traits<256, 64, 64, 4, false, false, cutlass::bfloat16_t, Flash_kernel_traits<256, 64, 64, 4, cutlass::bfloat16_t>>, Is_dropout=true, Is_causal=true, Is_even_N=true, Is_even_K=false, Return_softmax=true]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(34): here
            instantiation of "void run_flash_fwd<Kernel_traits,Is_dropout,Is_causal>(Flash_fwd_params &, cudaStream_t) [with Kernel_traits=Flash_fwd_kernel_traits<256, 64, 64, 4, false, false, cutlass::bfloat16_t, Flash_kernel_traits<256, 64, 64, 4, cutlass::bfloat16_t>>, Is_dropout=true, Is_causal=true]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(236): here
            instantiation of "void run_mha_fwd_hdim256<T>(Flash_fwd_params &, cudaStream_t) [with T=cutlass::bfloat16_t]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_hdim256_bf16_sm80.cu(8): here

/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_kernel.h(140): warning: variable "MMA_M" was declared but never referenced
          detected during:
            instantiation of "void flash::compute_attn_1rowblock<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &, int, int, int) [with Kernel_traits=Flash_fwd_kernel_traits<256, 64, 64, 4, false, false, cutlass::bfloat16_t, Flash_kernel_traits<256, 64, 64, 4, cutlass::bfloat16_t>>, Is_dropout=true, Is_causal=true, Is_even_N=true, Is_even_K=false, Return_softmax=false, Params=Flash_fwd_params]" 
(588): here
            instantiation of "void flash::compute_attn<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &) [with Kernel_traits=Flash_fwd_kernel_traits<256, 64, 64, 4, false, false, cutlass::bfloat16_t, Flash_kernel_traits<256, 64, 64, 4, cutlass::bfloat16_t>>, Is_dropout=true, Is_causal=true, Is_even_N=true, Is_even_K=false, Return_softmax=false, Params=Flash_fwd_params]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(15): here
            instantiation of "void flash_fwd_kernel<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax>(Flash_fwd_params) [with Kernel_traits=Flash_fwd_kernel_traits<256, 64, 64, 4, false, false, cutlass::bfloat16_t, Flash_kernel_traits<256, 64, 64, 4, cutlass::bfloat16_t>>, Is_dropout=true, Is_causal=true, Is_even_N=true, Is_even_K=false, Return_softmax=false]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(34): here
            instantiation of "void run_flash_fwd<Kernel_traits,Is_dropout,Is_causal>(Flash_fwd_params &, cudaStream_t) [with Kernel_traits=Flash_fwd_kernel_traits<256, 64, 64, 4, false, false, cutlass::bfloat16_t, Flash_kernel_traits<256, 64, 64, 4, cutlass::bfloat16_t>>, Is_dropout=true, Is_causal=true]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(236): here
            instantiation of "void run_mha_fwd_hdim256<T>(Flash_fwd_params &, cudaStream_t) [with T=cutlass::bfloat16_t]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_hdim256_bf16_sm80.cu(8): here

/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_kernel.h(275): warning: variable "tQrQ" was declared but never referenced
          detected during:
            instantiation of "void flash::compute_attn_1rowblock<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &, int, int, int) [with Kernel_traits=Flash_fwd_kernel_traits<256, 64, 64, 4, false, false, cutlass::bfloat16_t, Flash_kernel_traits<256, 64, 64, 4, cutlass::bfloat16_t>>, Is_dropout=true, Is_causal=true, Is_even_N=true, Is_even_K=false, Return_softmax=false, Params=Flash_fwd_params]" 
(588): here
            instantiation of "void flash::compute_attn<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &) [with Kernel_traits=Flash_fwd_kernel_traits<256, 64, 64, 4, false, false, cutlass::bfloat16_t, Flash_kernel_traits<256, 64, 64, 4, cutlass::bfloat16_t>>, Is_dropout=true, Is_causal=true, Is_even_N=true, Is_even_K=false, Return_softmax=false, Params=Flash_fwd_params]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(15): here
            instantiation of "void flash_fwd_kernel<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax>(Flash_fwd_params) [with Kernel_traits=Flash_fwd_kernel_traits<256, 64, 64, 4, false, false, cutlass::bfloat16_t, Flash_kernel_traits<256, 64, 64, 4, cutlass::bfloat16_t>>, Is_dropout=true, Is_causal=true, Is_even_N=true, Is_even_K=false, Return_softmax=false]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(34): here
            instantiation of "void run_flash_fwd<Kernel_traits,Is_dropout,Is_causal>(Flash_fwd_params &, cudaStream_t) [with Kernel_traits=Flash_fwd_kernel_traits<256, 64, 64, 4, false, false, cutlass::bfloat16_t, Flash_kernel_traits<256, 64, 64, 4, cutlass::bfloat16_t>>, Is_dropout=true, Is_causal=true]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(236): here
            instantiation of "void run_mha_fwd_hdim256<T>(Flash_fwd_params &, cudaStream_t) [with T=cutlass::bfloat16_t]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_hdim256_bf16_sm80.cu(8): here

/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_kernel.h(140): warning: variable "MMA_M" was declared but never referenced
          detected during:
            instantiation of "void flash::compute_attn_1rowblock<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &, int, int, int) [with Kernel_traits=Flash_fwd_kernel_traits<256, 64, 64, 4, false, false, cutlass::bfloat16_t, Flash_kernel_traits<256, 64, 64, 4, cutlass::bfloat16_t>>, Is_dropout=true, Is_causal=true, Is_even_N=false, Is_even_K=true, Return_softmax=true, Params=Flash_fwd_params]" 
(588): here
            instantiation of "void flash::compute_attn<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &) [with Kernel_traits=Flash_fwd_kernel_traits<256, 64, 64, 4, false, false, cutlass::bfloat16_t, Flash_kernel_traits<256, 64, 64, 4, cutlass::bfloat16_t>>, Is_dropout=true, Is_causal=true, Is_even_N=false, Is_even_K=true, Return_softmax=true, Params=Flash_fwd_params]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(15): here
            instantiation of "void flash_fwd_kernel<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax>(Flash_fwd_params) [with Kernel_traits=Flash_fwd_kernel_traits<256, 64, 64, 4, false, false, cutlass::bfloat16_t, Flash_kernel_traits<256, 64, 64, 4, cutlass::bfloat16_t>>, Is_dropout=true, Is_causal=true, Is_even_N=false, Is_even_K=true, Return_softmax=true]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(34): here
            instantiation of "void run_flash_fwd<Kernel_traits,Is_dropout,Is_causal>(Flash_fwd_params &, cudaStream_t) [with Kernel_traits=Flash_fwd_kernel_traits<256, 64, 64, 4, false, false, cutlass::bfloat16_t, Flash_kernel_traits<256, 64, 64, 4, cutlass::bfloat16_t>>, Is_dropout=true, Is_causal=true]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(236): here
            instantiation of "void run_mha_fwd_hdim256<T>(Flash_fwd_params &, cudaStream_t) [with T=cutlass::bfloat16_t]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_hdim256_bf16_sm80.cu(8): here

/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_kernel.h(275): warning: variable "tQrQ" was declared but never referenced
          detected during:
            instantiation of "void flash::compute_attn_1rowblock<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &, int, int, int) [with Kernel_traits=Flash_fwd_kernel_traits<256, 64, 64, 4, false, false, cutlass::bfloat16_t, Flash_kernel_traits<256, 64, 64, 4, cutlass::bfloat16_t>>, Is_dropout=true, Is_causal=true, Is_even_N=false, Is_even_K=true, Return_softmax=true, Params=Flash_fwd_params]" 
(588): here
            instantiation of "void flash::compute_attn<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &) [with Kernel_traits=Flash_fwd_kernel_traits<256, 64, 64, 4, false, false, cutlass::bfloat16_t, Flash_kernel_traits<256, 64, 64, 4, cutlass::bfloat16_t>>, Is_dropout=true, Is_causal=true, Is_even_N=false, Is_even_K=true, Return_softmax=true, Params=Flash_fwd_params]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(15): here
            instantiation of "void flash_fwd_kernel<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax>(Flash_fwd_params) [with Kernel_traits=Flash_fwd_kernel_traits<256, 64, 64, 4, false, false, cutlass::bfloat16_t, Flash_kernel_traits<256, 64, 64, 4, cutlass::bfloat16_t>>, Is_dropout=true, Is_causal=true, Is_even_N=false, Is_even_K=true, Return_softmax=true]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(34): here
            instantiation of "void run_flash_fwd<Kernel_traits,Is_dropout,Is_causal>(Flash_fwd_params &, cudaStream_t) [with Kernel_traits=Flash_fwd_kernel_traits<256, 64, 64, 4, false, false, cutlass::bfloat16_t, Flash_kernel_traits<256, 64, 64, 4, cutlass::bfloat16_t>>, Is_dropout=true, Is_causal=true]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(236): here
            instantiation of "void run_mha_fwd_hdim256<T>(Flash_fwd_params &, cudaStream_t) [with T=cutlass::bfloat16_t]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_hdim256_bf16_sm80.cu(8): here

/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_kernel.h(140): warning: variable "MMA_M" was declared but never referenced
          detected during:
            instantiation of "void flash::compute_attn_1rowblock<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &, int, int, int) [with Kernel_traits=Flash_fwd_kernel_traits<256, 64, 64, 4, false, false, cutlass::bfloat16_t, Flash_kernel_traits<256, 64, 64, 4, cutlass::bfloat16_t>>, Is_dropout=true, Is_causal=true, Is_even_N=false, Is_even_K=true, Return_softmax=false, Params=Flash_fwd_params]" 
(588): here
            instantiation of "void flash::compute_attn<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &) [with Kernel_traits=Flash_fwd_kernel_traits<256, 64, 64, 4, false, false, cutlass::bfloat16_t, Flash_kernel_traits<256, 64, 64, 4, cutlass::bfloat16_t>>, Is_dropout=true, Is_causal=true, Is_even_N=false, Is_even_K=true, Return_softmax=false, Params=Flash_fwd_params]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(15): here
            instantiation of "void flash_fwd_kernel<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax>(Flash_fwd_params) [with Kernel_traits=Flash_fwd_kernel_traits<256, 64, 64, 4, false, false, cutlass::bfloat16_t, Flash_kernel_traits<256, 64, 64, 4, cutlass::bfloat16_t>>, Is_dropout=true, Is_causal=true, Is_even_N=false, Is_even_K=true, Return_softmax=false]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(34): here
            instantiation of "void run_flash_fwd<Kernel_traits,Is_dropout,Is_causal>(Flash_fwd_params &, cudaStream_t) [with Kernel_traits=Flash_fwd_kernel_traits<256, 64, 64, 4, false, false, cutlass::bfloat16_t, Flash_kernel_traits<256, 64, 64, 4, cutlass::bfloat16_t>>, Is_dropout=true, Is_causal=true]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(236): here
            instantiation of "void run_mha_fwd_hdim256<T>(Flash_fwd_params &, cudaStream_t) [with T=cutlass::bfloat16_t]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_hdim256_bf16_sm80.cu(8): here

/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_kernel.h(275): warning: variable "tQrQ" was declared but never referenced
          detected during:
            instantiation of "void flash::compute_attn_1rowblock<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &, int, int, int) [with Kernel_traits=Flash_fwd_kernel_traits<256, 64, 64, 4, false, false, cutlass::bfloat16_t, Flash_kernel_traits<256, 64, 64, 4, cutlass::bfloat16_t>>, Is_dropout=true, Is_causal=true, Is_even_N=false, Is_even_K=true, Return_softmax=false, Params=Flash_fwd_params]" 
(588): here
            instantiation of "void flash::compute_attn<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &) [with Kernel_traits=Flash_fwd_kernel_traits<256, 64, 64, 4, false, false, cutlass::bfloat16_t, Flash_kernel_traits<256, 64, 64, 4, cutlass::bfloat16_t>>, Is_dropout=true, Is_causal=true, Is_even_N=false, Is_even_K=true, Return_softmax=false, Params=Flash_fwd_params]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(15): here
            instantiation of "void flash_fwd_kernel<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax>(Flash_fwd_params) [with Kernel_traits=Flash_fwd_kernel_traits<256, 64, 64, 4, false, false, cutlass::bfloat16_t, Flash_kernel_traits<256, 64, 64, 4, cutlass::bfloat16_t>>, Is_dropout=true, Is_causal=true, Is_even_N=false, Is_even_K=true, Return_softmax=false]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(34): here
            instantiation of "void run_flash_fwd<Kernel_traits,Is_dropout,Is_causal>(Flash_fwd_params &, cudaStream_t) [with Kernel_traits=Flash_fwd_kernel_traits<256, 64, 64, 4, false, false, cutlass::bfloat16_t, Flash_kernel_traits<256, 64, 64, 4, cutlass::bfloat16_t>>, Is_dropout=true, Is_causal=true]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(236): here
            instantiation of "void run_mha_fwd_hdim256<T>(Flash_fwd_params &, cudaStream_t) [with T=cutlass::bfloat16_t]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_hdim256_bf16_sm80.cu(8): here

/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_kernel.h(140): warning: variable "MMA_M" was declared but never referenced
          detected during:
            instantiation of "void flash::compute_attn_1rowblock<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &, int, int, int) [with Kernel_traits=Flash_fwd_kernel_traits<256, 64, 64, 4, false, false, cutlass::bfloat16_t, Flash_kernel_traits<256, 64, 64, 4, cutlass::bfloat16_t>>, Is_dropout=true, Is_causal=true, Is_even_N=false, Is_even_K=false, Return_softmax=true, Params=Flash_fwd_params]" 
(588): here
            instantiation of "void flash::compute_attn<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &) [with Kernel_traits=Flash_fwd_kernel_traits<256, 64, 64, 4, false, false, cutlass::bfloat16_t, Flash_kernel_traits<256, 64, 64, 4, cutlass::bfloat16_t>>, Is_dropout=true, Is_causal=true, Is_even_N=false, Is_even_K=false, Return_softmax=true, Params=Flash_fwd_params]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(15): here
            instantiation of "void flash_fwd_kernel<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax>(Flash_fwd_params) [with Kernel_traits=Flash_fwd_kernel_traits<256, 64, 64, 4, false, false, cutlass::bfloat16_t, Flash_kernel_traits<256, 64, 64, 4, cutlass::bfloat16_t>>, Is_dropout=true, Is_causal=true, Is_even_N=false, Is_even_K=false, Return_softmax=true]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(34): here
            instantiation of "void run_flash_fwd<Kernel_traits,Is_dropout,Is_causal>(Flash_fwd_params &, cudaStream_t) [with Kernel_traits=Flash_fwd_kernel_traits<256, 64, 64, 4, false, false, cutlass::bfloat16_t, Flash_kernel_traits<256, 64, 64, 4, cutlass::bfloat16_t>>, Is_dropout=true, Is_causal=true]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(236): here
            instantiation of "void run_mha_fwd_hdim256<T>(Flash_fwd_params &, cudaStream_t) [with T=cutlass::bfloat16_t]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_hdim256_bf16_sm80.cu(8): here

/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_kernel.h(275): warning: variable "tQrQ" was declared but never referenced
          detected during:
            instantiation of "void flash::compute_attn_1rowblock<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &, int, int, int) [with Kernel_traits=Flash_fwd_kernel_traits<256, 64, 64, 4, false, false, cutlass::bfloat16_t, Flash_kernel_traits<256, 64, 64, 4, cutlass::bfloat16_t>>, Is_dropout=true, Is_causal=true, Is_even_N=false, Is_even_K=false, Return_softmax=true, Params=Flash_fwd_params]" 
(588): here
            instantiation of "void flash::compute_attn<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &) [with Kernel_traits=Flash_fwd_kernel_traits<256, 64, 64, 4, false, false, cutlass::bfloat16_t, Flash_kernel_traits<256, 64, 64, 4, cutlass::bfloat16_t>>, Is_dropout=true, Is_causal=true, Is_even_N=false, Is_even_K=false, Return_softmax=true, Params=Flash_fwd_params]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(15): here
            instantiation of "void flash_fwd_kernel<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax>(Flash_fwd_params) [with Kernel_traits=Flash_fwd_kernel_traits<256, 64, 64, 4, false, false, cutlass::bfloat16_t, Flash_kernel_traits<256, 64, 64, 4, cutlass::bfloat16_t>>, Is_dropout=true, Is_causal=true, Is_even_N=false, Is_even_K=false, Return_softmax=true]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(34): here
            instantiation of "void run_flash_fwd<Kernel_traits,Is_dropout,Is_causal>(Flash_fwd_params &, cudaStream_t) [with Kernel_traits=Flash_fwd_kernel_traits<256, 64, 64, 4, false, false, cutlass::bfloat16_t, Flash_kernel_traits<256, 64, 64, 4, cutlass::bfloat16_t>>, Is_dropout=true, Is_causal=true]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(236): here
            instantiation of "void run_mha_fwd_hdim256<T>(Flash_fwd_params &, cudaStream_t) [with T=cutlass::bfloat16_t]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_hdim256_bf16_sm80.cu(8): here

/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_kernel.h(140): warning: variable "MMA_M" was declared but never referenced
          detected during:
            instantiation of "void flash::compute_attn_1rowblock<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &, int, int, int) [with Kernel_traits=Flash_fwd_kernel_traits<256, 64, 64, 4, false, false, cutlass::bfloat16_t, Flash_kernel_traits<256, 64, 64, 4, cutlass::bfloat16_t>>, Is_dropout=true, Is_causal=true, Is_even_N=false, Is_even_K=false, Return_softmax=false, Params=Flash_fwd_params]" 
(588): here
            instantiation of "void flash::compute_attn<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &) [with Kernel_traits=Flash_fwd_kernel_traits<256, 64, 64, 4, false, false, cutlass::bfloat16_t, Flash_kernel_traits<256, 64, 64, 4, cutlass::bfloat16_t>>, Is_dropout=true, Is_causal=true, Is_even_N=false, Is_even_K=false, Return_softmax=false, Params=Flash_fwd_params]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(15): here
            instantiation of "void flash_fwd_kernel<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax>(Flash_fwd_params) [with Kernel_traits=Flash_fwd_kernel_traits<256, 64, 64, 4, false, false, cutlass::bfloat16_t, Flash_kernel_traits<256, 64, 64, 4, cutlass::bfloat16_t>>, Is_dropout=true, Is_causal=true, Is_even_N=false, Is_even_K=false, Return_softmax=false]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(34): here
            instantiation of "void run_flash_fwd<Kernel_traits,Is_dropout,Is_causal>(Flash_fwd_params &, cudaStream_t) [with Kernel_traits=Flash_fwd_kernel_traits<256, 64, 64, 4, false, false, cutlass::bfloat16_t, Flash_kernel_traits<256, 64, 64, 4, cutlass::bfloat16_t>>, Is_dropout=true, Is_causal=true]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(236): here
            instantiation of "void run_mha_fwd_hdim256<T>(Flash_fwd_params &, cudaStream_t) [with T=cutlass::bfloat16_t]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_hdim256_bf16_sm80.cu(8): here

/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_kernel.h(275): warning: variable "tQrQ" was declared but never referenced
          detected during:
            instantiation of "void flash::compute_attn_1rowblock<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &, int, int, int) [with Kernel_traits=Flash_fwd_kernel_traits<256, 64, 64, 4, false, false, cutlass::bfloat16_t, Flash_kernel_traits<256, 64, 64, 4, cutlass::bfloat16_t>>, Is_dropout=true, Is_causal=true, Is_even_N=false, Is_even_K=false, Return_softmax=false, Params=Flash_fwd_params]" 
(588): here
            instantiation of "void flash::compute_attn<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &) [with Kernel_traits=Flash_fwd_kernel_traits<256, 64, 64, 4, false, false, cutlass::bfloat16_t, Flash_kernel_traits<256, 64, 64, 4, cutlass::bfloat16_t>>, Is_dropout=true, Is_causal=true, Is_even_N=false, Is_even_K=false, Return_softmax=false, Params=Flash_fwd_params]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(15): here
            instantiation of "void flash_fwd_kernel<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax>(Flash_fwd_params) [with Kernel_traits=Flash_fwd_kernel_traits<256, 64, 64, 4, false, false, cutlass::bfloat16_t, Flash_kernel_traits<256, 64, 64, 4, cutlass::bfloat16_t>>, Is_dropout=true, Is_causal=true, Is_even_N=false, Is_even_K=false, Return_softmax=false]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(34): here
            instantiation of "void run_flash_fwd<Kernel_traits,Is_dropout,Is_causal>(Flash_fwd_params &, cudaStream_t) [with Kernel_traits=Flash_fwd_kernel_traits<256, 64, 64, 4, false, false, cutlass::bfloat16_t, Flash_kernel_traits<256, 64, 64, 4, cutlass::bfloat16_t>>, Is_dropout=true, Is_causal=true]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(236): here
            instantiation of "void run_mha_fwd_hdim256<T>(Flash_fwd_params &, cudaStream_t) [with T=cutlass::bfloat16_t]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_hdim256_bf16_sm80.cu(8): here

/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_kernel.h(140): warning: variable "MMA_M" was declared but never referenced
          detected during:
            instantiation of "void flash::compute_attn_1rowblock<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &, int, int, int) [with Kernel_traits=Flash_fwd_kernel_traits<256, 128, 64, 8, false, false, cutlass::bfloat16_t, Flash_kernel_traits<256, 128, 64, 8, cutlass::bfloat16_t>>, Is_dropout=true, Is_causal=false, Is_even_N=true, Is_even_K=true, Return_softmax=true, Params=Flash_fwd_params]" 
(588): here
            instantiation of "void flash::compute_attn<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &) [with Kernel_traits=Flash_fwd_kernel_traits<256, 128, 64, 8, false, false, cutlass::bfloat16_t, Flash_kernel_traits<256, 128, 64, 8, cutlass::bfloat16_t>>, Is_dropout=true, Is_causal=false, Is_even_N=true, Is_even_K=true, Return_softmax=true, Params=Flash_fwd_params]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(15): here
            instantiation of "void flash_fwd_kernel<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax>(Flash_fwd_params) [with Kernel_traits=Flash_fwd_kernel_traits<256, 128, 64, 8, false, false, cutlass::bfloat16_t, Flash_kernel_traits<256, 128, 64, 8, cutlass::bfloat16_t>>, Is_dropout=true, Is_causal=false, Is_even_N=true, Is_even_K=true, Return_softmax=true]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(34): here
            instantiation of "void run_flash_fwd<Kernel_traits,Is_dropout,Is_causal>(Flash_fwd_params &, cudaStream_t) [with Kernel_traits=Flash_fwd_kernel_traits<256, 128, 64, 8, false, false, cutlass::bfloat16_t, Flash_kernel_traits<256, 128, 64, 8, cutlass::bfloat16_t>>, Is_dropout=true, Is_causal=false]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(236): here
            instantiation of "void run_mha_fwd_hdim256<T>(Flash_fwd_params &, cudaStream_t) [with T=cutlass::bfloat16_t]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_hdim256_bf16_sm80.cu(8): here

/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_kernel.h(275): warning: variable "tQrQ" was declared but never referenced
          detected during:
            instantiation of "void flash::compute_attn_1rowblock<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &, int, int, int) [with Kernel_traits=Flash_fwd_kernel_traits<256, 128, 64, 8, false, false, cutlass::bfloat16_t, Flash_kernel_traits<256, 128, 64, 8, cutlass::bfloat16_t>>, Is_dropout=true, Is_causal=false, Is_even_N=true, Is_even_K=true, Return_softmax=true, Params=Flash_fwd_params]" 
(588): here
            instantiation of "void flash::compute_attn<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &) [with Kernel_traits=Flash_fwd_kernel_traits<256, 128, 64, 8, false, false, cutlass::bfloat16_t, Flash_kernel_traits<256, 128, 64, 8, cutlass::bfloat16_t>>, Is_dropout=true, Is_causal=false, Is_even_N=true, Is_even_K=true, Return_softmax=true, Params=Flash_fwd_params]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(15): here
            instantiation of "void flash_fwd_kernel<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax>(Flash_fwd_params) [with Kernel_traits=Flash_fwd_kernel_traits<256, 128, 64, 8, false, false, cutlass::bfloat16_t, Flash_kernel_traits<256, 128, 64, 8, cutlass::bfloat16_t>>, Is_dropout=true, Is_causal=false, Is_even_N=true, Is_even_K=true, Return_softmax=true]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(34): here
            instantiation of "void run_flash_fwd<Kernel_traits,Is_dropout,Is_causal>(Flash_fwd_params &, cudaStream_t) [with Kernel_traits=Flash_fwd_kernel_traits<256, 128, 64, 8, false, false, cutlass::bfloat16_t, Flash_kernel_traits<256, 128, 64, 8, cutlass::bfloat16_t>>, Is_dropout=true, Is_causal=false]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(236): here
            instantiation of "void run_mha_fwd_hdim256<T>(Flash_fwd_params &, cudaStream_t) [with T=cutlass::bfloat16_t]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_hdim256_bf16_sm80.cu(8): here

/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_kernel.h(140): warning: variable "MMA_M" was declared but never referenced
          detected during:
            instantiation of "void flash::compute_attn_1rowblock<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &, int, int, int) [with Kernel_traits=Flash_fwd_kernel_traits<256, 128, 64, 8, false, false, cutlass::bfloat16_t, Flash_kernel_traits<256, 128, 64, 8, cutlass::bfloat16_t>>, Is_dropout=true, Is_causal=false, Is_even_N=true, Is_even_K=true, Return_softmax=false, Params=Flash_fwd_params]" 
(588): here
            instantiation of "void flash::compute_attn<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &) [with Kernel_traits=Flash_fwd_kernel_traits<256, 128, 64, 8, false, false, cutlass::bfloat16_t, Flash_kernel_traits<256, 128, 64, 8, cutlass::bfloat16_t>>, Is_dropout=true, Is_causal=false, Is_even_N=true, Is_even_K=true, Return_softmax=false, Params=Flash_fwd_params]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(15): here
            instantiation of "void flash_fwd_kernel<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax>(Flash_fwd_params) [with Kernel_traits=Flash_fwd_kernel_traits<256, 128, 64, 8, false, false, cutlass::bfloat16_t, Flash_kernel_traits<256, 128, 64, 8, cutlass::bfloat16_t>>, Is_dropout=true, Is_causal=false, Is_even_N=true, Is_even_K=true, Return_softmax=false]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(34): here
            instantiation of "void run_flash_fwd<Kernel_traits,Is_dropout,Is_causal>(Flash_fwd_params &, cudaStream_t) [with Kernel_traits=Flash_fwd_kernel_traits<256, 128, 64, 8, false, false, cutlass::bfloat16_t, Flash_kernel_traits<256, 128, 64, 8, cutlass::bfloat16_t>>, Is_dropout=true, Is_causal=false]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(236): here
            instantiation of "void run_mha_fwd_hdim256<T>(Flash_fwd_params &, cudaStream_t) [with T=cutlass::bfloat16_t]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_hdim256_bf16_sm80.cu(8): here

/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_kernel.h(275): warning: variable "tQrQ" was declared but never referenced
          detected during:
            instantiation of "void flash::compute_attn_1rowblock<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &, int, int, int) [with Kernel_traits=Flash_fwd_kernel_traits<256, 128, 64, 8, false, false, cutlass::bfloat16_t, Flash_kernel_traits<256, 128, 64, 8, cutlass::bfloat16_t>>, Is_dropout=true, Is_causal=false, Is_even_N=true, Is_even_K=true, Return_softmax=false, Params=Flash_fwd_params]" 
(588): here
            instantiation of "void flash::compute_attn<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &) [with Kernel_traits=Flash_fwd_kernel_traits<256, 128, 64, 8, false, false, cutlass::bfloat16_t, Flash_kernel_traits<256, 128, 64, 8, cutlass::bfloat16_t>>, Is_dropout=true, Is_causal=false, Is_even_N=true, Is_even_K=true, Return_softmax=false, Params=Flash_fwd_params]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(15): here
            instantiation of "void flash_fwd_kernel<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax>(Flash_fwd_params) [with Kernel_traits=Flash_fwd_kernel_traits<256, 128, 64, 8, false, false, cutlass::bfloat16_t, Flash_kernel_traits<256, 128, 64, 8, cutlass::bfloat16_t>>, Is_dropout=true, Is_causal=false, Is_even_N=true, Is_even_K=true, Return_softmax=false]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(34): here
            instantiation of "void run_flash_fwd<Kernel_traits,Is_dropout,Is_causal>(Flash_fwd_params &, cudaStream_t) [with Kernel_traits=Flash_fwd_kernel_traits<256, 128, 64, 8, false, false, cutlass::bfloat16_t, Flash_kernel_traits<256, 128, 64, 8, cutlass::bfloat16_t>>, Is_dropout=true, Is_causal=false]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(236): here
            instantiation of "void run_mha_fwd_hdim256<T>(Flash_fwd_params &, cudaStream_t) [with T=cutlass::bfloat16_t]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_hdim256_bf16_sm80.cu(8): here

/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_kernel.h(140): warning: variable "MMA_M" was declared but never referenced
          detected during:
            instantiation of "void flash::compute_attn_1rowblock<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &, int, int, int) [with Kernel_traits=Flash_fwd_kernel_traits<256, 128, 64, 8, false, false, cutlass::bfloat16_t, Flash_kernel_traits<256, 128, 64, 8, cutlass::bfloat16_t>>, Is_dropout=true, Is_causal=false, Is_even_N=true, Is_even_K=false, Return_softmax=true, Params=Flash_fwd_params]" 
(588): here
            instantiation of "void flash::compute_attn<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &) [with Kernel_traits=Flash_fwd_kernel_traits<256, 128, 64, 8, false, false, cutlass::bfloat16_t, Flash_kernel_traits<256, 128, 64, 8, cutlass::bfloat16_t>>, Is_dropout=true, Is_causal=false, Is_even_N=true, Is_even_K=false, Return_softmax=true, Params=Flash_fwd_params]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(15): here
            instantiation of "void flash_fwd_kernel<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax>(Flash_fwd_params) [with Kernel_traits=Flash_fwd_kernel_traits<256, 128, 64, 8, false, false, cutlass::bfloat16_t, Flash_kernel_traits<256, 128, 64, 8, cutlass::bfloat16_t>>, Is_dropout=true, Is_causal=false, Is_even_N=true, Is_even_K=false, Return_softmax=true]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(34): here
            instantiation of "void run_flash_fwd<Kernel_traits,Is_dropout,Is_causal>(Flash_fwd_params &, cudaStream_t) [with Kernel_traits=Flash_fwd_kernel_traits<256, 128, 64, 8, false, false, cutlass::bfloat16_t, Flash_kernel_traits<256, 128, 64, 8, cutlass::bfloat16_t>>, Is_dropout=true, Is_causal=false]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(236): here
            instantiation of "void run_mha_fwd_hdim256<T>(Flash_fwd_params &, cudaStream_t) [with T=cutlass::bfloat16_t]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_hdim256_bf16_sm80.cu(8): here

/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_kernel.h(275): warning: variable "tQrQ" was declared but never referenced
          detected during:
            instantiation of "void flash::compute_attn_1rowblock<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &, int, int, int) [with Kernel_traits=Flash_fwd_kernel_traits<256, 128, 64, 8, false, false, cutlass::bfloat16_t, Flash_kernel_traits<256, 128, 64, 8, cutlass::bfloat16_t>>, Is_dropout=true, Is_causal=false, Is_even_N=true, Is_even_K=false, Return_softmax=true, Params=Flash_fwd_params]" 
(588): here
            instantiation of "void flash::compute_attn<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &) [with Kernel_traits=Flash_fwd_kernel_traits<256, 128, 64, 8, false, false, cutlass::bfloat16_t, Flash_kernel_traits<256, 128, 64, 8, cutlass::bfloat16_t>>, Is_dropout=true, Is_causal=false, Is_even_N=true, Is_even_K=false, Return_softmax=true, Params=Flash_fwd_params]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(15): here
            instantiation of "void flash_fwd_kernel<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax>(Flash_fwd_params) [with Kernel_traits=Flash_fwd_kernel_traits<256, 128, 64, 8, false, false, cutlass::bfloat16_t, Flash_kernel_traits<256, 128, 64, 8, cutlass::bfloat16_t>>, Is_dropout=true, Is_causal=false, Is_even_N=true, Is_even_K=false, Return_softmax=true]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(34): here
            instantiation of "void run_flash_fwd<Kernel_traits,Is_dropout,Is_causal>(Flash_fwd_params &, cudaStream_t) [with Kernel_traits=Flash_fwd_kernel_traits<256, 128, 64, 8, false, false, cutlass::bfloat16_t, Flash_kernel_traits<256, 128, 64, 8, cutlass::bfloat16_t>>, Is_dropout=true, Is_causal=false]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(236): here
            instantiation of "void run_mha_fwd_hdim256<T>(Flash_fwd_params &, cudaStream_t) [with T=cutlass::bfloat16_t]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_hdim256_bf16_sm80.cu(8): here

/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_kernel.h(140): warning: variable "MMA_M" was declared but never referenced
          detected during:
            instantiation of "void flash::compute_attn_1rowblock<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &, int, int, int) [with Kernel_traits=Flash_fwd_kernel_traits<256, 128, 64, 8, false, false, cutlass::bfloat16_t, Flash_kernel_traits<256, 128, 64, 8, cutlass::bfloat16_t>>, Is_dropout=true, Is_causal=false, Is_even_N=true, Is_even_K=false, Return_softmax=false, Params=Flash_fwd_params]" 
(588): here
            instantiation of "void flash::compute_attn<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &) [with Kernel_traits=Flash_fwd_kernel_traits<256, 128, 64, 8, false, false, cutlass::bfloat16_t, Flash_kernel_traits<256, 128, 64, 8, cutlass::bfloat16_t>>, Is_dropout=true, Is_causal=false, Is_even_N=true, Is_even_K=false, Return_softmax=false, Params=Flash_fwd_params]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(15): here
            instantiation of "void flash_fwd_kernel<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax>(Flash_fwd_params) [with Kernel_traits=Flash_fwd_kernel_traits<256, 128, 64, 8, false, false, cutlass::bfloat16_t, Flash_kernel_traits<256, 128, 64, 8, cutlass::bfloat16_t>>, Is_dropout=true, Is_causal=false, Is_even_N=true, Is_even_K=false, Return_softmax=false]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(34): here
            instantiation of "void run_flash_fwd<Kernel_traits,Is_dropout,Is_causal>(Flash_fwd_params &, cudaStream_t) [with Kernel_traits=Flash_fwd_kernel_traits<256, 128, 64, 8, false, false, cutlass::bfloat16_t, Flash_kernel_traits<256, 128, 64, 8, cutlass::bfloat16_t>>, Is_dropout=true, Is_causal=false]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(236): here
            instantiation of "void run_mha_fwd_hdim256<T>(Flash_fwd_params &, cudaStream_t) [with T=cutlass::bfloat16_t]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_hdim256_bf16_sm80.cu(8): here

/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_kernel.h(275): warning: variable "tQrQ" was declared but never referenced
          detected during:
            instantiation of "void flash::compute_attn_1rowblock<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &, int, int, int) [with Kernel_traits=Flash_fwd_kernel_traits<256, 128, 64, 8, false, false, cutlass::bfloat16_t, Flash_kernel_traits<256, 128, 64, 8, cutlass::bfloat16_t>>, Is_dropout=true, Is_causal=false, Is_even_N=true, Is_even_K=false, Return_softmax=false, Params=Flash_fwd_params]" 
(588): here
            instantiation of "void flash::compute_attn<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &) [with Kernel_traits=Flash_fwd_kernel_traits<256, 128, 64, 8, false, false, cutlass::bfloat16_t, Flash_kernel_traits<256, 128, 64, 8, cutlass::bfloat16_t>>, Is_dropout=true, Is_causal=false, Is_even_N=true, Is_even_K=false, Return_softmax=false, Params=Flash_fwd_params]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(15): here
            instantiation of "void flash_fwd_kernel<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax>(Flash_fwd_params) [with Kernel_traits=Flash_fwd_kernel_traits<256, 128, 64, 8, false, false, cutlass::bfloat16_t, Flash_kernel_traits<256, 128, 64, 8, cutlass::bfloat16_t>>, Is_dropout=true, Is_causal=false, Is_even_N=true, Is_even_K=false, Return_softmax=false]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(34): here
            instantiation of "void run_flash_fwd<Kernel_traits,Is_dropout,Is_causal>(Flash_fwd_params &, cudaStream_t) [with Kernel_traits=Flash_fwd_kernel_traits<256, 128, 64, 8, false, false, cutlass::bfloat16_t, Flash_kernel_traits<256, 128, 64, 8, cutlass::bfloat16_t>>, Is_dropout=true, Is_causal=false]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(236): here
            instantiation of "void run_mha_fwd_hdim256<T>(Flash_fwd_params &, cudaStream_t) [with T=cutlass::bfloat16_t]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_hdim256_bf16_sm80.cu(8): here

/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_kernel.h(140): warning: variable "MMA_M" was declared but never referenced
          detected during:
            instantiation of "void flash::compute_attn_1rowblock<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &, int, int, int) [with Kernel_traits=Flash_fwd_kernel_traits<256, 128, 64, 8, false, false, cutlass::bfloat16_t, Flash_kernel_traits<256, 128, 64, 8, cutlass::bfloat16_t>>, Is_dropout=true, Is_causal=false, Is_even_N=false, Is_even_K=true, Return_softmax=true, Params=Flash_fwd_params]" 
(588): here
            instantiation of "void flash::compute_attn<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &) [with Kernel_traits=Flash_fwd_kernel_traits<256, 128, 64, 8, false, false, cutlass::bfloat16_t, Flash_kernel_traits<256, 128, 64, 8, cutlass::bfloat16_t>>, Is_dropout=true, Is_causal=false, Is_even_N=false, Is_even_K=true, Return_softmax=true, Params=Flash_fwd_params]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(15): here
            instantiation of "void flash_fwd_kernel<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax>(Flash_fwd_params) [with Kernel_traits=Flash_fwd_kernel_traits<256, 128, 64, 8, false, false, cutlass::bfloat16_t, Flash_kernel_traits<256, 128, 64, 8, cutlass::bfloat16_t>>, Is_dropout=true, Is_causal=false, Is_even_N=false, Is_even_K=true, Return_softmax=true]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(34): here
            instantiation of "void run_flash_fwd<Kernel_traits,Is_dropout,Is_causal>(Flash_fwd_params &, cudaStream_t) [with Kernel_traits=Flash_fwd_kernel_traits<256, 128, 64, 8, false, false, cutlass::bfloat16_t, Flash_kernel_traits<256, 128, 64, 8, cutlass::bfloat16_t>>, Is_dropout=true, Is_causal=false]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(236): here
            instantiation of "void run_mha_fwd_hdim256<T>(Flash_fwd_params &, cudaStream_t) [with T=cutlass::bfloat16_t]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_hdim256_bf16_sm80.cu(8): here

/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_kernel.h(275): warning: variable "tQrQ" was declared but never referenced
          detected during:
            instantiation of "void flash::compute_attn_1rowblock<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &, int, int, int) [with Kernel_traits=Flash_fwd_kernel_traits<256, 128, 64, 8, false, false, cutlass::bfloat16_t, Flash_kernel_traits<256, 128, 64, 8, cutlass::bfloat16_t>>, Is_dropout=true, Is_causal=false, Is_even_N=false, Is_even_K=true, Return_softmax=true, Params=Flash_fwd_params]" 
(588): here
            instantiation of "void flash::compute_attn<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &) [with Kernel_traits=Flash_fwd_kernel_traits<256, 128, 64, 8, false, false, cutlass::bfloat16_t, Flash_kernel_traits<256, 128, 64, 8, cutlass::bfloat16_t>>, Is_dropout=true, Is_causal=false, Is_even_N=false, Is_even_K=true, Return_softmax=true, Params=Flash_fwd_params]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(15): here
            instantiation of "void flash_fwd_kernel<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax>(Flash_fwd_params) [with Kernel_traits=Flash_fwd_kernel_traits<256, 128, 64, 8, false, false, cutlass::bfloat16_t, Flash_kernel_traits<256, 128, 64, 8, cutlass::bfloat16_t>>, Is_dropout=true, Is_causal=false, Is_even_N=false, Is_even_K=true, Return_softmax=true]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(34): here
            instantiation of "void run_flash_fwd<Kernel_traits,Is_dropout,Is_causal>(Flash_fwd_params &, cudaStream_t) [with Kernel_traits=Flash_fwd_kernel_traits<256, 128, 64, 8, false, false, cutlass::bfloat16_t, Flash_kernel_traits<256, 128, 64, 8, cutlass::bfloat16_t>>, Is_dropout=true, Is_causal=false]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(236): here
            instantiation of "void run_mha_fwd_hdim256<T>(Flash_fwd_params &, cudaStream_t) [with T=cutlass::bfloat16_t]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_hdim256_bf16_sm80.cu(8): here

/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_kernel.h(140): warning: variable "MMA_M" was declared but never referenced
          detected during:
            instantiation of "void flash::compute_attn_1rowblock<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &, int, int, int) [with Kernel_traits=Flash_fwd_kernel_traits<256, 128, 64, 8, false, false, cutlass::bfloat16_t, Flash_kernel_traits<256, 128, 64, 8, cutlass::bfloat16_t>>, Is_dropout=true, Is_causal=false, Is_even_N=false, Is_even_K=true, Return_softmax=false, Params=Flash_fwd_params]" 
(588): here
            instantiation of "void flash::compute_attn<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &) [with Kernel_traits=Flash_fwd_kernel_traits<256, 128, 64, 8, false, false, cutlass::bfloat16_t, Flash_kernel_traits<256, 128, 64, 8, cutlass::bfloat16_t>>, Is_dropout=true, Is_causal=false, Is_even_N=false, Is_even_K=true, Return_softmax=false, Params=Flash_fwd_params]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(15): here
            instantiation of "void flash_fwd_kernel<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax>(Flash_fwd_params) [with Kernel_traits=Flash_fwd_kernel_traits<256, 128, 64, 8, false, false, cutlass::bfloat16_t, Flash_kernel_traits<256, 128, 64, 8, cutlass::bfloat16_t>>, Is_dropout=true, Is_causal=false, Is_even_N=false, Is_even_K=true, Return_softmax=false]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(34): here
            instantiation of "void run_flash_fwd<Kernel_traits,Is_dropout,Is_causal>(Flash_fwd_params &, cudaStream_t) [with Kernel_traits=Flash_fwd_kernel_traits<256, 128, 64, 8, false, false, cutlass::bfloat16_t, Flash_kernel_traits<256, 128, 64, 8, cutlass::bfloat16_t>>, Is_dropout=true, Is_causal=false]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(236): here
            instantiation of "void run_mha_fwd_hdim256<T>(Flash_fwd_params &, cudaStream_t) [with T=cutlass::bfloat16_t]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_hdim256_bf16_sm80.cu(8): here

/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_kernel.h(275): warning: variable "tQrQ" was declared but never referenced
          detected during:
            instantiation of "void flash::compute_attn_1rowblock<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &, int, int, int) [with Kernel_traits=Flash_fwd_kernel_traits<256, 128, 64, 8, false, false, cutlass::bfloat16_t, Flash_kernel_traits<256, 128, 64, 8, cutlass::bfloat16_t>>, Is_dropout=true, Is_causal=false, Is_even_N=false, Is_even_K=true, Return_softmax=false, Params=Flash_fwd_params]" 
(588): here
            instantiation of "void flash::compute_attn<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &) [with Kernel_traits=Flash_fwd_kernel_traits<256, 128, 64, 8, false, false, cutlass::bfloat16_t, Flash_kernel_traits<256, 128, 64, 8, cutlass::bfloat16_t>>, Is_dropout=true, Is_causal=false, Is_even_N=false, Is_even_K=true, Return_softmax=false, Params=Flash_fwd_params]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(15): here
            instantiation of "void flash_fwd_kernel<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax>(Flash_fwd_params) [with Kernel_traits=Flash_fwd_kernel_traits<256, 128, 64, 8, false, false, cutlass::bfloat16_t, Flash_kernel_traits<256, 128, 64, 8, cutlass::bfloat16_t>>, Is_dropout=true, Is_causal=false, Is_even_N=false, Is_even_K=true, Return_softmax=false]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(34): here
            instantiation of "void run_flash_fwd<Kernel_traits,Is_dropout,Is_causal>(Flash_fwd_params &, cudaStream_t) [with Kernel_traits=Flash_fwd_kernel_traits<256, 128, 64, 8, false, false, cutlass::bfloat16_t, Flash_kernel_traits<256, 128, 64, 8, cutlass::bfloat16_t>>, Is_dropout=true, Is_causal=false]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(236): here
            instantiation of "void run_mha_fwd_hdim256<T>(Flash_fwd_params &, cudaStream_t) [with T=cutlass::bfloat16_t]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_hdim256_bf16_sm80.cu(8): here

/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_kernel.h(140): warning: variable "MMA_M" was declared but never referenced
          detected during:
            instantiation of "void flash::compute_attn_1rowblock<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &, int, int, int) [with Kernel_traits=Flash_fwd_kernel_traits<256, 128, 64, 8, false, false, cutlass::bfloat16_t, Flash_kernel_traits<256, 128, 64, 8, cutlass::bfloat16_t>>, Is_dropout=true, Is_causal=false, Is_even_N=false, Is_even_K=false, Return_softmax=true, Params=Flash_fwd_params]" 
(588): here
            instantiation of "void flash::compute_attn<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &) [with Kernel_traits=Flash_fwd_kernel_traits<256, 128, 64, 8, false, false, cutlass::bfloat16_t, Flash_kernel_traits<256, 128, 64, 8, cutlass::bfloat16_t>>, Is_dropout=true, Is_causal=false, Is_even_N=false, Is_even_K=false, Return_softmax=true, Params=Flash_fwd_params]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(15): here
            instantiation of "void flash_fwd_kernel<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax>(Flash_fwd_params) [with Kernel_traits=Flash_fwd_kernel_traits<256, 128, 64, 8, false, false, cutlass::bfloat16_t, Flash_kernel_traits<256, 128, 64, 8, cutlass::bfloat16_t>>, Is_dropout=true, Is_causal=false, Is_even_N=false, Is_even_K=false, Return_softmax=true]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(34): here
            instantiation of "void run_flash_fwd<Kernel_traits,Is_dropout,Is_causal>(Flash_fwd_params &, cudaStream_t) [with Kernel_traits=Flash_fwd_kernel_traits<256, 128, 64, 8, false, false, cutlass::bfloat16_t, Flash_kernel_traits<256, 128, 64, 8, cutlass::bfloat16_t>>, Is_dropout=true, Is_causal=false]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(236): here
            instantiation of "void run_mha_fwd_hdim256<T>(Flash_fwd_params &, cudaStream_t) [with T=cutlass::bfloat16_t]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_hdim256_bf16_sm80.cu(8): here

/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_kernel.h(275): warning: variable "tQrQ" was declared but never referenced
          detected during:
            instantiation of "void flash::compute_attn_1rowblock<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &, int, int, int) [with Kernel_traits=Flash_fwd_kernel_traits<256, 128, 64, 8, false, false, cutlass::bfloat16_t, Flash_kernel_traits<256, 128, 64, 8, cutlass::bfloat16_t>>, Is_dropout=true, Is_causal=false, Is_even_N=false, Is_even_K=false, Return_softmax=true, Params=Flash_fwd_params]" 
(588): here
            instantiation of "void flash::compute_attn<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &) [with Kernel_traits=Flash_fwd_kernel_traits<256, 128, 64, 8, false, false, cutlass::bfloat16_t, Flash_kernel_traits<256, 128, 64, 8, cutlass::bfloat16_t>>, Is_dropout=true, Is_causal=false, Is_even_N=false, Is_even_K=false, Return_softmax=true, Params=Flash_fwd_params]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(15): here
            instantiation of "void flash_fwd_kernel<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax>(Flash_fwd_params) [with Kernel_traits=Flash_fwd_kernel_traits<256, 128, 64, 8, false, false, cutlass::bfloat16_t, Flash_kernel_traits<256, 128, 64, 8, cutlass::bfloat16_t>>, Is_dropout=true, Is_causal=false, Is_even_N=false, Is_even_K=false, Return_softmax=true]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(34): here
            instantiation of "void run_flash_fwd<Kernel_traits,Is_dropout,Is_causal>(Flash_fwd_params &, cudaStream_t) [with Kernel_traits=Flash_fwd_kernel_traits<256, 128, 64, 8, false, false, cutlass::bfloat16_t, Flash_kernel_traits<256, 128, 64, 8, cutlass::bfloat16_t>>, Is_dropout=true, Is_causal=false]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(236): here
            instantiation of "void run_mha_fwd_hdim256<T>(Flash_fwd_params &, cudaStream_t) [with T=cutlass::bfloat16_t]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_hdim256_bf16_sm80.cu(8): here

/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_kernel.h(140): warning: variable "MMA_M" was declared but never referenced
          detected during:
            instantiation of "void flash::compute_attn_1rowblock<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &, int, int, int) [with Kernel_traits=Flash_fwd_kernel_traits<256, 128, 64, 8, false, false, cutlass::bfloat16_t, Flash_kernel_traits<256, 128, 64, 8, cutlass::bfloat16_t>>, Is_dropout=true, Is_causal=false, Is_even_N=false, Is_even_K=false, Return_softmax=false, Params=Flash_fwd_params]" 
(588): here
            instantiation of "void flash::compute_attn<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &) [with Kernel_traits=Flash_fwd_kernel_traits<256, 128, 64, 8, false, false, cutlass::bfloat16_t, Flash_kernel_traits<256, 128, 64, 8, cutlass::bfloat16_t>>, Is_dropout=true, Is_causal=false, Is_even_N=false, Is_even_K=false, Return_softmax=false, Params=Flash_fwd_params]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(15): here
            instantiation of "void flash_fwd_kernel<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax>(Flash_fwd_params) [with Kernel_traits=Flash_fwd_kernel_traits<256, 128, 64, 8, false, false, cutlass::bfloat16_t, Flash_kernel_traits<256, 128, 64, 8, cutlass::bfloat16_t>>, Is_dropout=true, Is_causal=false, Is_even_N=false, Is_even_K=false, Return_softmax=false]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(34): here
            instantiation of "void run_flash_fwd<Kernel_traits,Is_dropout,Is_causal>(Flash_fwd_params &, cudaStream_t) [with Kernel_traits=Flash_fwd_kernel_traits<256, 128, 64, 8, false, false, cutlass::bfloat16_t, Flash_kernel_traits<256, 128, 64, 8, cutlass::bfloat16_t>>, Is_dropout=true, Is_causal=false]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(236): here
            instantiation of "void run_mha_fwd_hdim256<T>(Flash_fwd_params &, cudaStream_t) [with T=cutlass::bfloat16_t]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_hdim256_bf16_sm80.cu(8): here

/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_kernel.h(275): warning: variable "tQrQ" was declared but never referenced
          detected during:
            instantiation of "void flash::compute_attn_1rowblock<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &, int, int, int) [with Kernel_traits=Flash_fwd_kernel_traits<256, 128, 64, 8, false, false, cutlass::bfloat16_t, Flash_kernel_traits<256, 128, 64, 8, cutlass::bfloat16_t>>, Is_dropout=true, Is_causal=false, Is_even_N=false, Is_even_K=false, Return_softmax=false, Params=Flash_fwd_params]" 
(588): here
            instantiation of "void flash::compute_attn<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &) [with Kernel_traits=Flash_fwd_kernel_traits<256, 128, 64, 8, false, false, cutlass::bfloat16_t, Flash_kernel_traits<256, 128, 64, 8, cutlass::bfloat16_t>>, Is_dropout=true, Is_causal=false, Is_even_N=false, Is_even_K=false, Return_softmax=false, Params=Flash_fwd_params]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(15): here
            instantiation of "void flash_fwd_kernel<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax>(Flash_fwd_params) [with Kernel_traits=Flash_fwd_kernel_traits<256, 128, 64, 8, false, false, cutlass::bfloat16_t, Flash_kernel_traits<256, 128, 64, 8, cutlass::bfloat16_t>>, Is_dropout=true, Is_causal=false, Is_even_N=false, Is_even_K=false, Return_softmax=false]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(34): here
            instantiation of "void run_flash_fwd<Kernel_traits,Is_dropout,Is_causal>(Flash_fwd_params &, cudaStream_t) [with Kernel_traits=Flash_fwd_kernel_traits<256, 128, 64, 8, false, false, cutlass::bfloat16_t, Flash_kernel_traits<256, 128, 64, 8, cutlass::bfloat16_t>>, Is_dropout=true, Is_causal=false]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(236): here
            instantiation of "void run_mha_fwd_hdim256<T>(Flash_fwd_params &, cudaStream_t) [with T=cutlass::bfloat16_t]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_hdim256_bf16_sm80.cu(8): here

/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_kernel.h(140): warning: variable "MMA_M" was declared but never referenced
          detected during:
            instantiation of "void flash::compute_attn_1rowblock<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &, int, int, int) [with Kernel_traits=Flash_fwd_kernel_traits<256, 64, 64, 4, false, false, cutlass::bfloat16_t, Flash_kernel_traits<256, 64, 64, 4, cutlass::bfloat16_t>>, Is_dropout=true, Is_causal=false, Is_even_N=true, Is_even_K=true, Return_softmax=true, Params=Flash_fwd_params]" 
(588): here
            instantiation of "void flash::compute_attn<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &) [with Kernel_traits=Flash_fwd_kernel_traits<256, 64, 64, 4, false, false, cutlass::bfloat16_t, Flash_kernel_traits<256, 64, 64, 4, cutlass::bfloat16_t>>, Is_dropout=true, Is_causal=false, Is_even_N=true, Is_even_K=true, Return_softmax=true, Params=Flash_fwd_params]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(15): here
            instantiation of "void flash_fwd_kernel<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax>(Flash_fwd_params) [with Kernel_traits=Flash_fwd_kernel_traits<256, 64, 64, 4, false, false, cutlass::bfloat16_t, Flash_kernel_traits<256, 64, 64, 4, cutlass::bfloat16_t>>, Is_dropout=true, Is_causal=false, Is_even_N=true, Is_even_K=true, Return_softmax=true]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(34): here
            instantiation of "void run_flash_fwd<Kernel_traits,Is_dropout,Is_causal>(Flash_fwd_params &, cudaStream_t) [with Kernel_traits=Flash_fwd_kernel_traits<256, 64, 64, 4, false, false, cutlass::bfloat16_t, Flash_kernel_traits<256, 64, 64, 4, cutlass::bfloat16_t>>, Is_dropout=true, Is_causal=false]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(236): here
            instantiation of "void run_mha_fwd_hdim256<T>(Flash_fwd_params &, cudaStream_t) [with T=cutlass::bfloat16_t]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_hdim256_bf16_sm80.cu(8): here

/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_kernel.h(275): warning: variable "tQrQ" was declared but never referenced
          detected during:
            instantiation of "void flash::compute_attn_1rowblock<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &, int, int, int) [with Kernel_traits=Flash_fwd_kernel_traits<256, 64, 64, 4, false, false, cutlass::bfloat16_t, Flash_kernel_traits<256, 64, 64, 4, cutlass::bfloat16_t>>, Is_dropout=true, Is_causal=false, Is_even_N=true, Is_even_K=true, Return_softmax=true, Params=Flash_fwd_params]" 
(588): here
            instantiation of "void flash::compute_attn<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &) [with Kernel_traits=Flash_fwd_kernel_traits<256, 64, 64, 4, false, false, cutlass::bfloat16_t, Flash_kernel_traits<256, 64, 64, 4, cutlass::bfloat16_t>>, Is_dropout=true, Is_causal=false, Is_even_N=true, Is_even_K=true, Return_softmax=true, Params=Flash_fwd_params]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(15): here
            instantiation of "void flash_fwd_kernel<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax>(Flash_fwd_params) [with Kernel_traits=Flash_fwd_kernel_traits<256, 64, 64, 4, false, false, cutlass::bfloat16_t, Flash_kernel_traits<256, 64, 64, 4, cutlass::bfloat16_t>>, Is_dropout=true, Is_causal=false, Is_even_N=true, Is_even_K=true, Return_softmax=true]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(34): here
            instantiation of "void run_flash_fwd<Kernel_traits,Is_dropout,Is_causal>(Flash_fwd_params &, cudaStream_t) [with Kernel_traits=Flash_fwd_kernel_traits<256, 64, 64, 4, false, false, cutlass::bfloat16_t, Flash_kernel_traits<256, 64, 64, 4, cutlass::bfloat16_t>>, Is_dropout=true, Is_causal=false]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(236): here
            instantiation of "void run_mha_fwd_hdim256<T>(Flash_fwd_params &, cudaStream_t) [with T=cutlass::bfloat16_t]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_hdim256_bf16_sm80.cu(8): here

/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_kernel.h(140): warning: variable "MMA_M" was declared but never referenced
          detected during:
            instantiation of "void flash::compute_attn_1rowblock<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &, int, int, int) [with Kernel_traits=Flash_fwd_kernel_traits<256, 64, 64, 4, false, false, cutlass::bfloat16_t, Flash_kernel_traits<256, 64, 64, 4, cutlass::bfloat16_t>>, Is_dropout=true, Is_causal=false, Is_even_N=true, Is_even_K=true, Return_softmax=false, Params=Flash_fwd_params]" 
(588): here
            instantiation of "void flash::compute_attn<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &) [with Kernel_traits=Flash_fwd_kernel_traits<256, 64, 64, 4, false, false, cutlass::bfloat16_t, Flash_kernel_traits<256, 64, 64, 4, cutlass::bfloat16_t>>, Is_dropout=true, Is_causal=false, Is_even_N=true, Is_even_K=true, Return_softmax=false, Params=Flash_fwd_params]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(15): here
            instantiation of "void flash_fwd_kernel<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax>(Flash_fwd_params) [with Kernel_traits=Flash_fwd_kernel_traits<256, 64, 64, 4, false, false, cutlass::bfloat16_t, Flash_kernel_traits<256, 64, 64, 4, cutlass::bfloat16_t>>, Is_dropout=true, Is_causal=false, Is_even_N=true, Is_even_K=true, Return_softmax=false]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(34): here
            instantiation of "void run_flash_fwd<Kernel_traits,Is_dropout,Is_causal>(Flash_fwd_params &, cudaStream_t) [with Kernel_traits=Flash_fwd_kernel_traits<256, 64, 64, 4, false, false, cutlass::bfloat16_t, Flash_kernel_traits<256, 64, 64, 4, cutlass::bfloat16_t>>, Is_dropout=true, Is_causal=false]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(236): here
            instantiation of "void run_mha_fwd_hdim256<T>(Flash_fwd_params &, cudaStream_t) [with T=cutlass::bfloat16_t]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_hdim256_bf16_sm80.cu(8): here

/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_kernel.h(275): warning: variable "tQrQ" was declared but never referenced
          detected during:
            instantiation of "void flash::compute_attn_1rowblock<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &, int, int, int) [with Kernel_traits=Flash_fwd_kernel_traits<256, 64, 64, 4, false, false, cutlass::bfloat16_t, Flash_kernel_traits<256, 64, 64, 4, cutlass::bfloat16_t>>, Is_dropout=true, Is_causal=false, Is_even_N=true, Is_even_K=true, Return_softmax=false, Params=Flash_fwd_params]" 
(588): here
            instantiation of "void flash::compute_attn<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &) [with Kernel_traits=Flash_fwd_kernel_traits<256, 64, 64, 4, false, false, cutlass::bfloat16_t, Flash_kernel_traits<256, 64, 64, 4, cutlass::bfloat16_t>>, Is_dropout=true, Is_causal=false, Is_even_N=true, Is_even_K=true, Return_softmax=false, Params=Flash_fwd_params]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(15): here
            instantiation of "void flash_fwd_kernel<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax>(Flash_fwd_params) [with Kernel_traits=Flash_fwd_kernel_traits<256, 64, 64, 4, false, false, cutlass::bfloat16_t, Flash_kernel_traits<256, 64, 64, 4, cutlass::bfloat16_t>>, Is_dropout=true, Is_causal=false, Is_even_N=true, Is_even_K=true, Return_softmax=false]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(34): here
            instantiation of "void run_flash_fwd<Kernel_traits,Is_dropout,Is_causal>(Flash_fwd_params &, cudaStream_t) [with Kernel_traits=Flash_fwd_kernel_traits<256, 64, 64, 4, false, false, cutlass::bfloat16_t, Flash_kernel_traits<256, 64, 64, 4, cutlass::bfloat16_t>>, Is_dropout=true, Is_causal=false]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(236): here
            instantiation of "void run_mha_fwd_hdim256<T>(Flash_fwd_params &, cudaStream_t) [with T=cutlass::bfloat16_t]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_hdim256_bf16_sm80.cu(8): here

/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_kernel.h(140): warning: variable "MMA_M" was declared but never referenced
          detected during:
            instantiation of "void flash::compute_attn_1rowblock<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &, int, int, int) [with Kernel_traits=Flash_fwd_kernel_traits<256, 64, 64, 4, false, false, cutlass::bfloat16_t, Flash_kernel_traits<256, 64, 64, 4, cutlass::bfloat16_t>>, Is_dropout=true, Is_causal=false, Is_even_N=true, Is_even_K=false, Return_softmax=true, Params=Flash_fwd_params]" 
(588): here
            instantiation of "void flash::compute_attn<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &) [with Kernel_traits=Flash_fwd_kernel_traits<256, 64, 64, 4, false, false, cutlass::bfloat16_t, Flash_kernel_traits<256, 64, 64, 4, cutlass::bfloat16_t>>, Is_dropout=true, Is_causal=false, Is_even_N=true, Is_even_K=false, Return_softmax=true, Params=Flash_fwd_params]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(15): here
            instantiation of "void flash_fwd_kernel<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax>(Flash_fwd_params) [with Kernel_traits=Flash_fwd_kernel_traits<256, 64, 64, 4, false, false, cutlass::bfloat16_t, Flash_kernel_traits<256, 64, 64, 4, cutlass::bfloat16_t>>, Is_dropout=true, Is_causal=false, Is_even_N=true, Is_even_K=false, Return_softmax=true]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(34): here
            instantiation of "void run_flash_fwd<Kernel_traits,Is_dropout,Is_causal>(Flash_fwd_params &, cudaStream_t) [with Kernel_traits=Flash_fwd_kernel_traits<256, 64, 64, 4, false, false, cutlass::bfloat16_t, Flash_kernel_traits<256, 64, 64, 4, cutlass::bfloat16_t>>, Is_dropout=true, Is_causal=false]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(236): here
            instantiation of "void run_mha_fwd_hdim256<T>(Flash_fwd_params &, cudaStream_t) [with T=cutlass::bfloat16_t]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_hdim256_bf16_sm80.cu(8): here

/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_kernel.h(275): warning: variable "tQrQ" was declared but never referenced
          detected during:
            instantiation of "void flash::compute_attn_1rowblock<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &, int, int, int) [with Kernel_traits=Flash_fwd_kernel_traits<256, 64, 64, 4, false, false, cutlass::bfloat16_t, Flash_kernel_traits<256, 64, 64, 4, cutlass::bfloat16_t>>, Is_dropout=true, Is_causal=false, Is_even_N=true, Is_even_K=false, Return_softmax=true, Params=Flash_fwd_params]" 
(588): here
            instantiation of "void flash::compute_attn<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &) [with Kernel_traits=Flash_fwd_kernel_traits<256, 64, 64, 4, false, false, cutlass::bfloat16_t, Flash_kernel_traits<256, 64, 64, 4, cutlass::bfloat16_t>>, Is_dropout=true, Is_causal=false, Is_even_N=true, Is_even_K=false, Return_softmax=true, Params=Flash_fwd_params]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(15): here
            instantiation of "void flash_fwd_kernel<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax>(Flash_fwd_params) [with Kernel_traits=Flash_fwd_kernel_traits<256, 64, 64, 4, false, false, cutlass::bfloat16_t, Flash_kernel_traits<256, 64, 64, 4, cutlass::bfloat16_t>>, Is_dropout=true, Is_causal=false, Is_even_N=true, Is_even_K=false, Return_softmax=true]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(34): here
            instantiation of "void run_flash_fwd<Kernel_traits,Is_dropout,Is_causal>(Flash_fwd_params &, cudaStream_t) [with Kernel_traits=Flash_fwd_kernel_traits<256, 64, 64, 4, false, false, cutlass::bfloat16_t, Flash_kernel_traits<256, 64, 64, 4, cutlass::bfloat16_t>>, Is_dropout=true, Is_causal=false]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(236): here
            instantiation of "void run_mha_fwd_hdim256<T>(Flash_fwd_params &, cudaStream_t) [with T=cutlass::bfloat16_t]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_hdim256_bf16_sm80.cu(8): here

/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_kernel.h(140): warning: variable "MMA_M" was declared but never referenced
          detected during:
            instantiation of "void flash::compute_attn_1rowblock<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &, int, int, int) [with Kernel_traits=Flash_fwd_kernel_traits<256, 64, 64, 4, false, false, cutlass::bfloat16_t, Flash_kernel_traits<256, 64, 64, 4, cutlass::bfloat16_t>>, Is_dropout=true, Is_causal=false, Is_even_N=true, Is_even_K=false, Return_softmax=false, Params=Flash_fwd_params]" 
(588): here
            instantiation of "void flash::compute_attn<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &) [with Kernel_traits=Flash_fwd_kernel_traits<256, 64, 64, 4, false, false, cutlass::bfloat16_t, Flash_kernel_traits<256, 64, 64, 4, cutlass::bfloat16_t>>, Is_dropout=true, Is_causal=false, Is_even_N=true, Is_even_K=false, Return_softmax=false, Params=Flash_fwd_params]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(15): here
            instantiation of "void flash_fwd_kernel<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax>(Flash_fwd_params) [with Kernel_traits=Flash_fwd_kernel_traits<256, 64, 64, 4, false, false, cutlass::bfloat16_t, Flash_kernel_traits<256, 64, 64, 4, cutlass::bfloat16_t>>, Is_dropout=true, Is_causal=false, Is_even_N=true, Is_even_K=false, Return_softmax=false]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(34): here
            instantiation of "void run_flash_fwd<Kernel_traits,Is_dropout,Is_causal>(Flash_fwd_params &, cudaStream_t) [with Kernel_traits=Flash_fwd_kernel_traits<256, 64, 64, 4, false, false, cutlass::bfloat16_t, Flash_kernel_traits<256, 64, 64, 4, cutlass::bfloat16_t>>, Is_dropout=true, Is_causal=false]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(236): here
            instantiation of "void run_mha_fwd_hdim256<T>(Flash_fwd_params &, cudaStream_t) [with T=cutlass::bfloat16_t]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_hdim256_bf16_sm80.cu(8): here

/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_kernel.h(275): warning: variable "tQrQ" was declared but never referenced
          detected during:
            instantiation of "void flash::compute_attn_1rowblock<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &, int, int, int) [with Kernel_traits=Flash_fwd_kernel_traits<256, 64, 64, 4, false, false, cutlass::bfloat16_t, Flash_kernel_traits<256, 64, 64, 4, cutlass::bfloat16_t>>, Is_dropout=true, Is_causal=false, Is_even_N=true, Is_even_K=false, Return_softmax=false, Params=Flash_fwd_params]" 
(588): here
            instantiation of "void flash::compute_attn<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &) [with Kernel_traits=Flash_fwd_kernel_traits<256, 64, 64, 4, false, false, cutlass::bfloat16_t, Flash_kernel_traits<256, 64, 64, 4, cutlass::bfloat16_t>>, Is_dropout=true, Is_causal=false, Is_even_N=true, Is_even_K=false, Return_softmax=false, Params=Flash_fwd_params]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(15): here
            instantiation of "void flash_fwd_kernel<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax>(Flash_fwd_params) [with Kernel_traits=Flash_fwd_kernel_traits<256, 64, 64, 4, false, false, cutlass::bfloat16_t, Flash_kernel_traits<256, 64, 64, 4, cutlass::bfloat16_t>>, Is_dropout=true, Is_causal=false, Is_even_N=true, Is_even_K=false, Return_softmax=false]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(34): here
            instantiation of "void run_flash_fwd<Kernel_traits,Is_dropout,Is_causal>(Flash_fwd_params &, cudaStream_t) [with Kernel_traits=Flash_fwd_kernel_traits<256, 64, 64, 4, false, false, cutlass::bfloat16_t, Flash_kernel_traits<256, 64, 64, 4, cutlass::bfloat16_t>>, Is_dropout=true, Is_causal=false]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(236): here
            instantiation of "void run_mha_fwd_hdim256<T>(Flash_fwd_params &, cudaStream_t) [with T=cutlass::bfloat16_t]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_hdim256_bf16_sm80.cu(8): here

/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_kernel.h(140): warning: variable "MMA_M" was declared but never referenced
          detected during:
            instantiation of "void flash::compute_attn_1rowblock<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &, int, int, int) [with Kernel_traits=Flash_fwd_kernel_traits<256, 64, 64, 4, false, false, cutlass::bfloat16_t, Flash_kernel_traits<256, 64, 64, 4, cutlass::bfloat16_t>>, Is_dropout=true, Is_causal=false, Is_even_N=false, Is_even_K=true, Return_softmax=true, Params=Flash_fwd_params]" 
(588): here
            instantiation of "void flash::compute_attn<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &) [with Kernel_traits=Flash_fwd_kernel_traits<256, 64, 64, 4, false, false, cutlass::bfloat16_t, Flash_kernel_traits<256, 64, 64, 4, cutlass::bfloat16_t>>, Is_dropout=true, Is_causal=false, Is_even_N=false, Is_even_K=true, Return_softmax=true, Params=Flash_fwd_params]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(15): here
            instantiation of "void flash_fwd_kernel<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax>(Flash_fwd_params) [with Kernel_traits=Flash_fwd_kernel_traits<256, 64, 64, 4, false, false, cutlass::bfloat16_t, Flash_kernel_traits<256, 64, 64, 4, cutlass::bfloat16_t>>, Is_dropout=true, Is_causal=false, Is_even_N=false, Is_even_K=true, Return_softmax=true]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(34): here
            instantiation of "void run_flash_fwd<Kernel_traits,Is_dropout,Is_causal>(Flash_fwd_params &, cudaStream_t) [with Kernel_traits=Flash_fwd_kernel_traits<256, 64, 64, 4, false, false, cutlass::bfloat16_t, Flash_kernel_traits<256, 64, 64, 4, cutlass::bfloat16_t>>, Is_dropout=true, Is_causal=false]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(236): here
            instantiation of "void run_mha_fwd_hdim256<T>(Flash_fwd_params &, cudaStream_t) [with T=cutlass::bfloat16_t]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_hdim256_bf16_sm80.cu(8): here

/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_kernel.h(275): warning: variable "tQrQ" was declared but never referenced
          detected during:
            instantiation of "void flash::compute_attn_1rowblock<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &, int, int, int) [with Kernel_traits=Flash_fwd_kernel_traits<256, 64, 64, 4, false, false, cutlass::bfloat16_t, Flash_kernel_traits<256, 64, 64, 4, cutlass::bfloat16_t>>, Is_dropout=true, Is_causal=false, Is_even_N=false, Is_even_K=true, Return_softmax=true, Params=Flash_fwd_params]" 
(588): here
            instantiation of "void flash::compute_attn<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &) [with Kernel_traits=Flash_fwd_kernel_traits<256, 64, 64, 4, false, false, cutlass::bfloat16_t, Flash_kernel_traits<256, 64, 64, 4, cutlass::bfloat16_t>>, Is_dropout=true, Is_causal=false, Is_even_N=false, Is_even_K=true, Return_softmax=true, Params=Flash_fwd_params]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(15): here
            instantiation of "void flash_fwd_kernel<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax>(Flash_fwd_params) [with Kernel_traits=Flash_fwd_kernel_traits<256, 64, 64, 4, false, false, cutlass::bfloat16_t, Flash_kernel_traits<256, 64, 64, 4, cutlass::bfloat16_t>>, Is_dropout=true, Is_causal=false, Is_even_N=false, Is_even_K=true, Return_softmax=true]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(34): here
            instantiation of "void run_flash_fwd<Kernel_traits,Is_dropout,Is_causal>(Flash_fwd_params &, cudaStream_t) [with Kernel_traits=Flash_fwd_kernel_traits<256, 64, 64, 4, false, false, cutlass::bfloat16_t, Flash_kernel_traits<256, 64, 64, 4, cutlass::bfloat16_t>>, Is_dropout=true, Is_causal=false]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(236): here
            instantiation of "void run_mha_fwd_hdim256<T>(Flash_fwd_params &, cudaStream_t) [with T=cutlass::bfloat16_t]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_hdim256_bf16_sm80.cu(8): here

/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_kernel.h(140): warning: variable "MMA_M" was declared but never referenced
          detected during:
            instantiation of "void flash::compute_attn_1rowblock<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &, int, int, int) [with Kernel_traits=Flash_fwd_kernel_traits<256, 64, 64, 4, false, false, cutlass::bfloat16_t, Flash_kernel_traits<256, 64, 64, 4, cutlass::bfloat16_t>>, Is_dropout=true, Is_causal=false, Is_even_N=false, Is_even_K=true, Return_softmax=false, Params=Flash_fwd_params]" 
(588): here
            instantiation of "void flash::compute_attn<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &) [with Kernel_traits=Flash_fwd_kernel_traits<256, 64, 64, 4, false, false, cutlass::bfloat16_t, Flash_kernel_traits<256, 64, 64, 4, cutlass::bfloat16_t>>, Is_dropout=true, Is_causal=false, Is_even_N=false, Is_even_K=true, Return_softmax=false, Params=Flash_fwd_params]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(15): here
            instantiation of "void flash_fwd_kernel<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax>(Flash_fwd_params) [with Kernel_traits=Flash_fwd_kernel_traits<256, 64, 64, 4, false, false, cutlass::bfloat16_t, Flash_kernel_traits<256, 64, 64, 4, cutlass::bfloat16_t>>, Is_dropout=true, Is_causal=false, Is_even_N=false, Is_even_K=true, Return_softmax=false]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(34): here
            instantiation of "void run_flash_fwd<Kernel_traits,Is_dropout,Is_causal>(Flash_fwd_params &, cudaStream_t) [with Kernel_traits=Flash_fwd_kernel_traits<256, 64, 64, 4, false, false, cutlass::bfloat16_t, Flash_kernel_traits<256, 64, 64, 4, cutlass::bfloat16_t>>, Is_dropout=true, Is_causal=false]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(236): here
            instantiation of "void run_mha_fwd_hdim256<T>(Flash_fwd_params &, cudaStream_t) [with T=cutlass::bfloat16_t]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_hdim256_bf16_sm80.cu(8): here

/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_kernel.h(275): warning: variable "tQrQ" was declared but never referenced
          detected during:
            instantiation of "void flash::compute_attn_1rowblock<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &, int, int, int) [with Kernel_traits=Flash_fwd_kernel_traits<256, 64, 64, 4, false, false, cutlass::bfloat16_t, Flash_kernel_traits<256, 64, 64, 4, cutlass::bfloat16_t>>, Is_dropout=true, Is_causal=false, Is_even_N=false, Is_even_K=true, Return_softmax=false, Params=Flash_fwd_params]" 
(588): here
            instantiation of "void flash::compute_attn<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &) [with Kernel_traits=Flash_fwd_kernel_traits<256, 64, 64, 4, false, false, cutlass::bfloat16_t, Flash_kernel_traits<256, 64, 64, 4, cutlass::bfloat16_t>>, Is_dropout=true, Is_causal=false, Is_even_N=false, Is_even_K=true, Return_softmax=false, Params=Flash_fwd_params]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(15): here
            instantiation of "void flash_fwd_kernel<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax>(Flash_fwd_params) [with Kernel_traits=Flash_fwd_kernel_traits<256, 64, 64, 4, false, false, cutlass::bfloat16_t, Flash_kernel_traits<256, 64, 64, 4, cutlass::bfloat16_t>>, Is_dropout=true, Is_causal=false, Is_even_N=false, Is_even_K=true, Return_softmax=false]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(34): here
            instantiation of "void run_flash_fwd<Kernel_traits,Is_dropout,Is_causal>(Flash_fwd_params &, cudaStream_t) [with Kernel_traits=Flash_fwd_kernel_traits<256, 64, 64, 4, false, false, cutlass::bfloat16_t, Flash_kernel_traits<256, 64, 64, 4, cutlass::bfloat16_t>>, Is_dropout=true, Is_causal=false]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(236): here
            instantiation of "void run_mha_fwd_hdim256<T>(Flash_fwd_params &, cudaStream_t) [with T=cutlass::bfloat16_t]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_hdim256_bf16_sm80.cu(8): here

/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_kernel.h(140): warning: variable "MMA_M" was declared but never referenced
          detected during:
            instantiation of "void flash::compute_attn_1rowblock<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &, int, int, int) [with Kernel_traits=Flash_fwd_kernel_traits<256, 64, 64, 4, false, false, cutlass::bfloat16_t, Flash_kernel_traits<256, 64, 64, 4, cutlass::bfloat16_t>>, Is_dropout=true, Is_causal=false, Is_even_N=false, Is_even_K=false, Return_softmax=true, Params=Flash_fwd_params]" 
(588): here
            instantiation of "void flash::compute_attn<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &) [with Kernel_traits=Flash_fwd_kernel_traits<256, 64, 64, 4, false, false, cutlass::bfloat16_t, Flash_kernel_traits<256, 64, 64, 4, cutlass::bfloat16_t>>, Is_dropout=true, Is_causal=false, Is_even_N=false, Is_even_K=false, Return_softmax=true, Params=Flash_fwd_params]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(15): here
            instantiation of "void flash_fwd_kernel<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax>(Flash_fwd_params) [with Kernel_traits=Flash_fwd_kernel_traits<256, 64, 64, 4, false, false, cutlass::bfloat16_t, Flash_kernel_traits<256, 64, 64, 4, cutlass::bfloat16_t>>, Is_dropout=true, Is_causal=false, Is_even_N=false, Is_even_K=false, Return_softmax=true]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(34): here
            instantiation of "void run_flash_fwd<Kernel_traits,Is_dropout,Is_causal>(Flash_fwd_params &, cudaStream_t) [with Kernel_traits=Flash_fwd_kernel_traits<256, 64, 64, 4, false, false, cutlass::bfloat16_t, Flash_kernel_traits<256, 64, 64, 4, cutlass::bfloat16_t>>, Is_dropout=true, Is_causal=false]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(236): here
            instantiation of "void run_mha_fwd_hdim256<T>(Flash_fwd_params &, cudaStream_t) [with T=cutlass::bfloat16_t]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_hdim256_bf16_sm80.cu(8): here

/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_kernel.h(275): warning: variable "tQrQ" was declared but never referenced
          detected during:
            instantiation of "void flash::compute_attn_1rowblock<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &, int, int, int) [with Kernel_traits=Flash_fwd_kernel_traits<256, 64, 64, 4, false, false, cutlass::bfloat16_t, Flash_kernel_traits<256, 64, 64, 4, cutlass::bfloat16_t>>, Is_dropout=true, Is_causal=false, Is_even_N=false, Is_even_K=false, Return_softmax=true, Params=Flash_fwd_params]" 
(588): here
            instantiation of "void flash::compute_attn<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &) [with Kernel_traits=Flash_fwd_kernel_traits<256, 64, 64, 4, false, false, cutlass::bfloat16_t, Flash_kernel_traits<256, 64, 64, 4, cutlass::bfloat16_t>>, Is_dropout=true, Is_causal=false, Is_even_N=false, Is_even_K=false, Return_softmax=true, Params=Flash_fwd_params]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(15): here
            instantiation of "void flash_fwd_kernel<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax>(Flash_fwd_params) [with Kernel_traits=Flash_fwd_kernel_traits<256, 64, 64, 4, false, false, cutlass::bfloat16_t, Flash_kernel_traits<256, 64, 64, 4, cutlass::bfloat16_t>>, Is_dropout=true, Is_causal=false, Is_even_N=false, Is_even_K=false, Return_softmax=true]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(34): here
            instantiation of "void run_flash_fwd<Kernel_traits,Is_dropout,Is_causal>(Flash_fwd_params &, cudaStream_t) [with Kernel_traits=Flash_fwd_kernel_traits<256, 64, 64, 4, false, false, cutlass::bfloat16_t, Flash_kernel_traits<256, 64, 64, 4, cutlass::bfloat16_t>>, Is_dropout=true, Is_causal=false]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(236): here
            instantiation of "void run_mha_fwd_hdim256<T>(Flash_fwd_params &, cudaStream_t) [with T=cutlass::bfloat16_t]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_hdim256_bf16_sm80.cu(8): here

/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_kernel.h(140): warning: variable "MMA_M" was declared but never referenced
          detected during:
            instantiation of "void flash::compute_attn_1rowblock<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &, int, int, int) [with Kernel_traits=Flash_fwd_kernel_traits<256, 64, 64, 4, false, false, cutlass::bfloat16_t, Flash_kernel_traits<256, 64, 64, 4, cutlass::bfloat16_t>>, Is_dropout=true, Is_causal=false, Is_even_N=false, Is_even_K=false, Return_softmax=false, Params=Flash_fwd_params]" 
(588): here
            instantiation of "void flash::compute_attn<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &) [with Kernel_traits=Flash_fwd_kernel_traits<256, 64, 64, 4, false, false, cutlass::bfloat16_t, Flash_kernel_traits<256, 64, 64, 4, cutlass::bfloat16_t>>, Is_dropout=true, Is_causal=false, Is_even_N=false, Is_even_K=false, Return_softmax=false, Params=Flash_fwd_params]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(15): here
            instantiation of "void flash_fwd_kernel<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax>(Flash_fwd_params) [with Kernel_traits=Flash_fwd_kernel_traits<256, 64, 64, 4, false, false, cutlass::bfloat16_t, Flash_kernel_traits<256, 64, 64, 4, cutlass::bfloat16_t>>, Is_dropout=true, Is_causal=false, Is_even_N=false, Is_even_K=false, Return_softmax=false]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(34): here
            instantiation of "void run_flash_fwd<Kernel_traits,Is_dropout,Is_causal>(Flash_fwd_params &, cudaStream_t) [with Kernel_traits=Flash_fwd_kernel_traits<256, 64, 64, 4, false, false, cutlass::bfloat16_t, Flash_kernel_traits<256, 64, 64, 4, cutlass::bfloat16_t>>, Is_dropout=true, Is_causal=false]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(236): here
            instantiation of "void run_mha_fwd_hdim256<T>(Flash_fwd_params &, cudaStream_t) [with T=cutlass::bfloat16_t]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_hdim256_bf16_sm80.cu(8): here

/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_kernel.h(275): warning: variable "tQrQ" was declared but never referenced
          detected during:
            instantiation of "void flash::compute_attn_1rowblock<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &, int, int, int) [with Kernel_traits=Flash_fwd_kernel_traits<256, 64, 64, 4, false, false, cutlass::bfloat16_t, Flash_kernel_traits<256, 64, 64, 4, cutlass::bfloat16_t>>, Is_dropout=true, Is_causal=false, Is_even_N=false, Is_even_K=false, Return_softmax=false, Params=Flash_fwd_params]" 
(588): here
            instantiation of "void flash::compute_attn<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &) [with Kernel_traits=Flash_fwd_kernel_traits<256, 64, 64, 4, false, false, cutlass::bfloat16_t, Flash_kernel_traits<256, 64, 64, 4, cutlass::bfloat16_t>>, Is_dropout=true, Is_causal=false, Is_even_N=false, Is_even_K=false, Return_softmax=false, Params=Flash_fwd_params]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(15): here
            instantiation of "void flash_fwd_kernel<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax>(Flash_fwd_params) [with Kernel_traits=Flash_fwd_kernel_traits<256, 64, 64, 4, false, false, cutlass::bfloat16_t, Flash_kernel_traits<256, 64, 64, 4, cutlass::bfloat16_t>>, Is_dropout=true, Is_causal=false, Is_even_N=false, Is_even_K=false, Return_softmax=false]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(34): here
            instantiation of "void run_flash_fwd<Kernel_traits,Is_dropout,Is_causal>(Flash_fwd_params &, cudaStream_t) [with Kernel_traits=Flash_fwd_kernel_traits<256, 64, 64, 4, false, false, cutlass::bfloat16_t, Flash_kernel_traits<256, 64, 64, 4, cutlass::bfloat16_t>>, Is_dropout=true, Is_causal=false]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(236): here
            instantiation of "void run_mha_fwd_hdim256<T>(Flash_fwd_params &, cudaStream_t) [with T=cutlass::bfloat16_t]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_hdim256_bf16_sm80.cu(8): here

/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_kernel.h(140): warning: variable "MMA_M" was declared but never referenced
          detected during:
            instantiation of "void flash::compute_attn_1rowblock<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &, int, int, int) [with Kernel_traits=Flash_fwd_kernel_traits<256, 128, 64, 8, false, false, cutlass::bfloat16_t, Flash_kernel_traits<256, 128, 64, 8, cutlass::bfloat16_t>>, Is_dropout=false, Is_causal=true, Is_even_N=true, Is_even_K=true, Return_softmax=false, Params=Flash_fwd_params]" 
(588): here
            instantiation of "void flash::compute_attn<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &) [with Kernel_traits=Flash_fwd_kernel_traits<256, 128, 64, 8, false, false, cutlass::bfloat16_t, Flash_kernel_traits<256, 128, 64, 8, cutlass::bfloat16_t>>, Is_dropout=false, Is_causal=true, Is_even_N=true, Is_even_K=true, Return_softmax=false, Params=Flash_fwd_params]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(15): here
            instantiation of "void flash_fwd_kernel<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax>(Flash_fwd_params) [with Kernel_traits=Flash_fwd_kernel_traits<256, 128, 64, 8, false, false, cutlass::bfloat16_t, Flash_kernel_traits<256, 128, 64, 8, cutlass::bfloat16_t>>, Is_dropout=false, Is_causal=true, Is_even_N=true, Is_even_K=true, Return_softmax=false]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(34): here
            instantiation of "void run_flash_fwd<Kernel_traits,Is_dropout,Is_causal>(Flash_fwd_params &, cudaStream_t) [with Kernel_traits=Flash_fwd_kernel_traits<256, 128, 64, 8, false, false, cutlass::bfloat16_t, Flash_kernel_traits<256, 128, 64, 8, cutlass::bfloat16_t>>, Is_dropout=false, Is_causal=true]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(236): here
            instantiation of "void run_mha_fwd_hdim256<T>(Flash_fwd_params &, cudaStream_t) [with T=cutlass::bfloat16_t]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_hdim256_bf16_sm80.cu(8): here

/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_kernel.h(275): warning: variable "tQrQ" was declared but never referenced
          detected during:
            instantiation of "void flash::compute_attn_1rowblock<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &, int, int, int) [with Kernel_traits=Flash_fwd_kernel_traits<256, 128, 64, 8, false, false, cutlass::bfloat16_t, Flash_kernel_traits<256, 128, 64, 8, cutlass::bfloat16_t>>, Is_dropout=false, Is_causal=true, Is_even_N=true, Is_even_K=true, Return_softmax=false, Params=Flash_fwd_params]" 
(588): here
            instantiation of "void flash::compute_attn<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &) [with Kernel_traits=Flash_fwd_kernel_traits<256, 128, 64, 8, false, false, cutlass::bfloat16_t, Flash_kernel_traits<256, 128, 64, 8, cutlass::bfloat16_t>>, Is_dropout=false, Is_causal=true, Is_even_N=true, Is_even_K=true, Return_softmax=false, Params=Flash_fwd_params]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(15): here
            instantiation of "void flash_fwd_kernel<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax>(Flash_fwd_params) [with Kernel_traits=Flash_fwd_kernel_traits<256, 128, 64, 8, false, false, cutlass::bfloat16_t, Flash_kernel_traits<256, 128, 64, 8, cutlass::bfloat16_t>>, Is_dropout=false, Is_causal=true, Is_even_N=true, Is_even_K=true, Return_softmax=false]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(34): here
            instantiation of "void run_flash_fwd<Kernel_traits,Is_dropout,Is_causal>(Flash_fwd_params &, cudaStream_t) [with Kernel_traits=Flash_fwd_kernel_traits<256, 128, 64, 8, false, false, cutlass::bfloat16_t, Flash_kernel_traits<256, 128, 64, 8, cutlass::bfloat16_t>>, Is_dropout=false, Is_causal=true]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(236): here
            instantiation of "void run_mha_fwd_hdim256<T>(Flash_fwd_params &, cudaStream_t) [with T=cutlass::bfloat16_t]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_hdim256_bf16_sm80.cu(8): here

/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_kernel.h(140): warning: variable "MMA_M" was declared but never referenced
          detected during:
            instantiation of "void flash::compute_attn_1rowblock<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &, int, int, int) [with Kernel_traits=Flash_fwd_kernel_traits<256, 128, 64, 8, false, false, cutlass::bfloat16_t, Flash_kernel_traits<256, 128, 64, 8, cutlass::bfloat16_t>>, Is_dropout=false, Is_causal=true, Is_even_N=true, Is_even_K=false, Return_softmax=false, Params=Flash_fwd_params]" 
(588): here
            instantiation of "void flash::compute_attn<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &) [with Kernel_traits=Flash_fwd_kernel_traits<256, 128, 64, 8, false, false, cutlass::bfloat16_t, Flash_kernel_traits<256, 128, 64, 8, cutlass::bfloat16_t>>, Is_dropout=false, Is_causal=true, Is_even_N=true, Is_even_K=false, Return_softmax=false, Params=Flash_fwd_params]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(15): here
            instantiation of "void flash_fwd_kernel<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax>(Flash_fwd_params) [with Kernel_traits=Flash_fwd_kernel_traits<256, 128, 64, 8, false, false, cutlass::bfloat16_t, Flash_kernel_traits<256, 128, 64, 8, cutlass::bfloat16_t>>, Is_dropout=false, Is_causal=true, Is_even_N=true, Is_even_K=false, Return_softmax=false]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(34): here
            instantiation of "void run_flash_fwd<Kernel_traits,Is_dropout,Is_causal>(Flash_fwd_params &, cudaStream_t) [with Kernel_traits=Flash_fwd_kernel_traits<256, 128, 64, 8, false, false, cutlass::bfloat16_t, Flash_kernel_traits<256, 128, 64, 8, cutlass::bfloat16_t>>, Is_dropout=false, Is_causal=true]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(236): here
            instantiation of "void run_mha_fwd_hdim256<T>(Flash_fwd_params &, cudaStream_t) [with T=cutlass::bfloat16_t]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_hdim256_bf16_sm80.cu(8): here

/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_kernel.h(275): warning: variable "tQrQ" was declared but never referenced
          detected during:
            instantiation of "void flash::compute_attn_1rowblock<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &, int, int, int) [with Kernel_traits=Flash_fwd_kernel_traits<256, 128, 64, 8, false, false, cutlass::bfloat16_t, Flash_kernel_traits<256, 128, 64, 8, cutlass::bfloat16_t>>, Is_dropout=false, Is_causal=true, Is_even_N=true, Is_even_K=false, Return_softmax=false, Params=Flash_fwd_params]" 
(588): here
            instantiation of "void flash::compute_attn<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &) [with Kernel_traits=Flash_fwd_kernel_traits<256, 128, 64, 8, false, false, cutlass::bfloat16_t, Flash_kernel_traits<256, 128, 64, 8, cutlass::bfloat16_t>>, Is_dropout=false, Is_causal=true, Is_even_N=true, Is_even_K=false, Return_softmax=false, Params=Flash_fwd_params]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(15): here
            instantiation of "void flash_fwd_kernel<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax>(Flash_fwd_params) [with Kernel_traits=Flash_fwd_kernel_traits<256, 128, 64, 8, false, false, cutlass::bfloat16_t, Flash_kernel_traits<256, 128, 64, 8, cutlass::bfloat16_t>>, Is_dropout=false, Is_causal=true, Is_even_N=true, Is_even_K=false, Return_softmax=false]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(34): here
            instantiation of "void run_flash_fwd<Kernel_traits,Is_dropout,Is_causal>(Flash_fwd_params &, cudaStream_t) [with Kernel_traits=Flash_fwd_kernel_traits<256, 128, 64, 8, false, false, cutlass::bfloat16_t, Flash_kernel_traits<256, 128, 64, 8, cutlass::bfloat16_t>>, Is_dropout=false, Is_causal=true]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(236): here
            instantiation of "void run_mha_fwd_hdim256<T>(Flash_fwd_params &, cudaStream_t) [with T=cutlass::bfloat16_t]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_hdim256_bf16_sm80.cu(8): here

/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_kernel.h(140): warning: variable "MMA_M" was declared but never referenced
          detected during:
            instantiation of "void flash::compute_attn_1rowblock<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &, int, int, int) [with Kernel_traits=Flash_fwd_kernel_traits<256, 128, 64, 8, false, false, cutlass::bfloat16_t, Flash_kernel_traits<256, 128, 64, 8, cutlass::bfloat16_t>>, Is_dropout=false, Is_causal=true, Is_even_N=false, Is_even_K=true, Return_softmax=false, Params=Flash_fwd_params]" 
(588): here
            instantiation of "void flash::compute_attn<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &) [with Kernel_traits=Flash_fwd_kernel_traits<256, 128, 64, 8, false, false, cutlass::bfloat16_t, Flash_kernel_traits<256, 128, 64, 8, cutlass::bfloat16_t>>, Is_dropout=false, Is_causal=true, Is_even_N=false, Is_even_K=true, Return_softmax=false, Params=Flash_fwd_params]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(15): here
            instantiation of "void flash_fwd_kernel<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax>(Flash_fwd_params) [with Kernel_traits=Flash_fwd_kernel_traits<256, 128, 64, 8, false, false, cutlass::bfloat16_t, Flash_kernel_traits<256, 128, 64, 8, cutlass::bfloat16_t>>, Is_dropout=false, Is_causal=true, Is_even_N=false, Is_even_K=true, Return_softmax=false]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(34): here
            instantiation of "void run_flash_fwd<Kernel_traits,Is_dropout,Is_causal>(Flash_fwd_params &, cudaStream_t) [with Kernel_traits=Flash_fwd_kernel_traits<256, 128, 64, 8, false, false, cutlass::bfloat16_t, Flash_kernel_traits<256, 128, 64, 8, cutlass::bfloat16_t>>, Is_dropout=false, Is_causal=true]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(236): here
            instantiation of "void run_mha_fwd_hdim256<T>(Flash_fwd_params &, cudaStream_t) [with T=cutlass::bfloat16_t]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_hdim256_bf16_sm80.cu(8): here

/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_kernel.h(275): warning: variable "tQrQ" was declared but never referenced
          detected during:
            instantiation of "void flash::compute_attn_1rowblock<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &, int, int, int) [with Kernel_traits=Flash_fwd_kernel_traits<256, 128, 64, 8, false, false, cutlass::bfloat16_t, Flash_kernel_traits<256, 128, 64, 8, cutlass::bfloat16_t>>, Is_dropout=false, Is_causal=true, Is_even_N=false, Is_even_K=true, Return_softmax=false, Params=Flash_fwd_params]" 
(588): here
            instantiation of "void flash::compute_attn<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &) [with Kernel_traits=Flash_fwd_kernel_traits<256, 128, 64, 8, false, false, cutlass::bfloat16_t, Flash_kernel_traits<256, 128, 64, 8, cutlass::bfloat16_t>>, Is_dropout=false, Is_causal=true, Is_even_N=false, Is_even_K=true, Return_softmax=false, Params=Flash_fwd_params]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(15): here
            instantiation of "void flash_fwd_kernel<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax>(Flash_fwd_params) [with Kernel_traits=Flash_fwd_kernel_traits<256, 128, 64, 8, false, false, cutlass::bfloat16_t, Flash_kernel_traits<256, 128, 64, 8, cutlass::bfloat16_t>>, Is_dropout=false, Is_causal=true, Is_even_N=false, Is_even_K=true, Return_softmax=false]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(34): here
            instantiation of "void run_flash_fwd<Kernel_traits,Is_dropout,Is_causal>(Flash_fwd_params &, cudaStream_t) [with Kernel_traits=Flash_fwd_kernel_traits<256, 128, 64, 8, false, false, cutlass::bfloat16_t, Flash_kernel_traits<256, 128, 64, 8, cutlass::bfloat16_t>>, Is_dropout=false, Is_causal=true]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(236): here
            instantiation of "void run_mha_fwd_hdim256<T>(Flash_fwd_params &, cudaStream_t) [with T=cutlass::bfloat16_t]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_hdim256_bf16_sm80.cu(8): here

/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_kernel.h(140): warning: variable "MMA_M" was declared but never referenced
          detected during:
            instantiation of "void flash::compute_attn_1rowblock<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &, int, int, int) [with Kernel_traits=Flash_fwd_kernel_traits<256, 128, 64, 8, false, false, cutlass::bfloat16_t, Flash_kernel_traits<256, 128, 64, 8, cutlass::bfloat16_t>>, Is_dropout=false, Is_causal=true, Is_even_N=false, Is_even_K=false, Return_softmax=false, Params=Flash_fwd_params]" 
(588): here
            instantiation of "void flash::compute_attn<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &) [with Kernel_traits=Flash_fwd_kernel_traits<256, 128, 64, 8, false, false, cutlass::bfloat16_t, Flash_kernel_traits<256, 128, 64, 8, cutlass::bfloat16_t>>, Is_dropout=false, Is_causal=true, Is_even_N=false, Is_even_K=false, Return_softmax=false, Params=Flash_fwd_params]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(15): here
            instantiation of "void flash_fwd_kernel<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax>(Flash_fwd_params) [with Kernel_traits=Flash_fwd_kernel_traits<256, 128, 64, 8, false, false, cutlass::bfloat16_t, Flash_kernel_traits<256, 128, 64, 8, cutlass::bfloat16_t>>, Is_dropout=false, Is_causal=true, Is_even_N=false, Is_even_K=false, Return_softmax=false]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(34): here
            instantiation of "void run_flash_fwd<Kernel_traits,Is_dropout,Is_causal>(Flash_fwd_params &, cudaStream_t) [with Kernel_traits=Flash_fwd_kernel_traits<256, 128, 64, 8, false, false, cutlass::bfloat16_t, Flash_kernel_traits<256, 128, 64, 8, cutlass::bfloat16_t>>, Is_dropout=false, Is_causal=true]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(236): here
            instantiation of "void run_mha_fwd_hdim256<T>(Flash_fwd_params &, cudaStream_t) [with T=cutlass::bfloat16_t]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_hdim256_bf16_sm80.cu(8): here

/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_kernel.h(275): warning: variable "tQrQ" was declared but never referenced
          detected during:
            instantiation of "void flash::compute_attn_1rowblock<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &, int, int, int) [with Kernel_traits=Flash_fwd_kernel_traits<256, 128, 64, 8, false, false, cutlass::bfloat16_t, Flash_kernel_traits<256, 128, 64, 8, cutlass::bfloat16_t>>, Is_dropout=false, Is_causal=true, Is_even_N=false, Is_even_K=false, Return_softmax=false, Params=Flash_fwd_params]" 
(588): here
            instantiation of "void flash::compute_attn<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &) [with Kernel_traits=Flash_fwd_kernel_traits<256, 128, 64, 8, false, false, cutlass::bfloat16_t, Flash_kernel_traits<256, 128, 64, 8, cutlass::bfloat16_t>>, Is_dropout=false, Is_causal=true, Is_even_N=false, Is_even_K=false, Return_softmax=false, Params=Flash_fwd_params]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(15): here
            instantiation of "void flash_fwd_kernel<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax>(Flash_fwd_params) [with Kernel_traits=Flash_fwd_kernel_traits<256, 128, 64, 8, false, false, cutlass::bfloat16_t, Flash_kernel_traits<256, 128, 64, 8, cutlass::bfloat16_t>>, Is_dropout=false, Is_causal=true, Is_even_N=false, Is_even_K=false, Return_softmax=false]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(34): here
            instantiation of "void run_flash_fwd<Kernel_traits,Is_dropout,Is_causal>(Flash_fwd_params &, cudaStream_t) [with Kernel_traits=Flash_fwd_kernel_traits<256, 128, 64, 8, false, false, cutlass::bfloat16_t, Flash_kernel_traits<256, 128, 64, 8, cutlass::bfloat16_t>>, Is_dropout=false, Is_causal=true]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(236): here
            instantiation of "void run_mha_fwd_hdim256<T>(Flash_fwd_params &, cudaStream_t) [with T=cutlass::bfloat16_t]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_hdim256_bf16_sm80.cu(8): here

/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_kernel.h(140): warning: variable "MMA_M" was declared but never referenced
          detected during:
            instantiation of "void flash::compute_attn_1rowblock<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &, int, int, int) [with Kernel_traits=Flash_fwd_kernel_traits<256, 64, 64, 4, false, false, cutlass::bfloat16_t, Flash_kernel_traits<256, 64, 64, 4, cutlass::bfloat16_t>>, Is_dropout=false, Is_causal=true, Is_even_N=true, Is_even_K=true, Return_softmax=false, Params=Flash_fwd_params]" 
(588): here
            instantiation of "void flash::compute_attn<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &) [with Kernel_traits=Flash_fwd_kernel_traits<256, 64, 64, 4, false, false, cutlass::bfloat16_t, Flash_kernel_traits<256, 64, 64, 4, cutlass::bfloat16_t>>, Is_dropout=false, Is_causal=true, Is_even_N=true, Is_even_K=true, Return_softmax=false, Params=Flash_fwd_params]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(15): here
            instantiation of "void flash_fwd_kernel<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax>(Flash_fwd_params) [with Kernel_traits=Flash_fwd_kernel_traits<256, 64, 64, 4, false, false, cutlass::bfloat16_t, Flash_kernel_traits<256, 64, 64, 4, cutlass::bfloat16_t>>, Is_dropout=false, Is_causal=true, Is_even_N=true, Is_even_K=true, Return_softmax=false]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(34): here
            instantiation of "void run_flash_fwd<Kernel_traits,Is_dropout,Is_causal>(Flash_fwd_params &, cudaStream_t) [with Kernel_traits=Flash_fwd_kernel_traits<256, 64, 64, 4, false, false, cutlass::bfloat16_t, Flash_kernel_traits<256, 64, 64, 4, cutlass::bfloat16_t>>, Is_dropout=false, Is_causal=true]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(236): here
            instantiation of "void run_mha_fwd_hdim256<T>(Flash_fwd_params &, cudaStream_t) [with T=cutlass::bfloat16_t]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_hdim256_bf16_sm80.cu(8): here

/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_kernel.h(275): warning: variable "tQrQ" was declared but never referenced
          detected during:
            instantiation of "void flash::compute_attn_1rowblock<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &, int, int, int) [with Kernel_traits=Flash_fwd_kernel_traits<256, 64, 64, 4, false, false, cutlass::bfloat16_t, Flash_kernel_traits<256, 64, 64, 4, cutlass::bfloat16_t>>, Is_dropout=false, Is_causal=true, Is_even_N=true, Is_even_K=true, Return_softmax=false, Params=Flash_fwd_params]" 
(588): here
            instantiation of "void flash::compute_attn<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &) [with Kernel_traits=Flash_fwd_kernel_traits<256, 64, 64, 4, false, false, cutlass::bfloat16_t, Flash_kernel_traits<256, 64, 64, 4, cutlass::bfloat16_t>>, Is_dropout=false, Is_causal=true, Is_even_N=true, Is_even_K=true, Return_softmax=false, Params=Flash_fwd_params]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(15): here
            instantiation of "void flash_fwd_kernel<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax>(Flash_fwd_params) [with Kernel_traits=Flash_fwd_kernel_traits<256, 64, 64, 4, false, false, cutlass::bfloat16_t, Flash_kernel_traits<256, 64, 64, 4, cutlass::bfloat16_t>>, Is_dropout=false, Is_causal=true, Is_even_N=true, Is_even_K=true, Return_softmax=false]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(34): here
            instantiation of "void run_flash_fwd<Kernel_traits,Is_dropout,Is_causal>(Flash_fwd_params &, cudaStream_t) [with Kernel_traits=Flash_fwd_kernel_traits<256, 64, 64, 4, false, false, cutlass::bfloat16_t, Flash_kernel_traits<256, 64, 64, 4, cutlass::bfloat16_t>>, Is_dropout=false, Is_causal=true]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(236): here
            instantiation of "void run_mha_fwd_hdim256<T>(Flash_fwd_params &, cudaStream_t) [with T=cutlass::bfloat16_t]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_hdim256_bf16_sm80.cu(8): here

/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_kernel.h(140): warning: variable "MMA_M" was declared but never referenced
          detected during:
            instantiation of "void flash::compute_attn_1rowblock<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &, int, int, int) [with Kernel_traits=Flash_fwd_kernel_traits<256, 64, 64, 4, false, false, cutlass::bfloat16_t, Flash_kernel_traits<256, 64, 64, 4, cutlass::bfloat16_t>>, Is_dropout=false, Is_causal=true, Is_even_N=true, Is_even_K=false, Return_softmax=false, Params=Flash_fwd_params]" 
(588): here
            instantiation of "void flash::compute_attn<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &) [with Kernel_traits=Flash_fwd_kernel_traits<256, 64, 64, 4, false, false, cutlass::bfloat16_t, Flash_kernel_traits<256, 64, 64, 4, cutlass::bfloat16_t>>, Is_dropout=false, Is_causal=true, Is_even_N=true, Is_even_K=false, Return_softmax=false, Params=Flash_fwd_params]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(15): here
            instantiation of "void flash_fwd_kernel<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax>(Flash_fwd_params) [with Kernel_traits=Flash_fwd_kernel_traits<256, 64, 64, 4, false, false, cutlass::bfloat16_t, Flash_kernel_traits<256, 64, 64, 4, cutlass::bfloat16_t>>, Is_dropout=false, Is_causal=true, Is_even_N=true, Is_even_K=false, Return_softmax=false]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(34): here
            instantiation of "void run_flash_fwd<Kernel_traits,Is_dropout,Is_causal>(Flash_fwd_params &, cudaStream_t) [with Kernel_traits=Flash_fwd_kernel_traits<256, 64, 64, 4, false, false, cutlass::bfloat16_t, Flash_kernel_traits<256, 64, 64, 4, cutlass::bfloat16_t>>, Is_dropout=false, Is_causal=true]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(236): here
            instantiation of "void run_mha_fwd_hdim256<T>(Flash_fwd_params &, cudaStream_t) [with T=cutlass::bfloat16_t]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_hdim256_bf16_sm80.cu(8): here

/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_kernel.h(275): warning: variable "tQrQ" was declared but never referenced
          detected during:
            instantiation of "void flash::compute_attn_1rowblock<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &, int, int, int) [with Kernel_traits=Flash_fwd_kernel_traits<256, 64, 64, 4, false, false, cutlass::bfloat16_t, Flash_kernel_traits<256, 64, 64, 4, cutlass::bfloat16_t>>, Is_dropout=false, Is_causal=true, Is_even_N=true, Is_even_K=false, Return_softmax=false, Params=Flash_fwd_params]" 
(588): here
            instantiation of "void flash::compute_attn<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &) [with Kernel_traits=Flash_fwd_kernel_traits<256, 64, 64, 4, false, false, cutlass::bfloat16_t, Flash_kernel_traits<256, 64, 64, 4, cutlass::bfloat16_t>>, Is_dropout=false, Is_causal=true, Is_even_N=true, Is_even_K=false, Return_softmax=false, Params=Flash_fwd_params]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(15): here
            instantiation of "void flash_fwd_kernel<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax>(Flash_fwd_params) [with Kernel_traits=Flash_fwd_kernel_traits<256, 64, 64, 4, false, false, cutlass::bfloat16_t, Flash_kernel_traits<256, 64, 64, 4, cutlass::bfloat16_t>>, Is_dropout=false, Is_causal=true, Is_even_N=true, Is_even_K=false, Return_softmax=false]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(34): here
            instantiation of "void run_flash_fwd<Kernel_traits,Is_dropout,Is_causal>(Flash_fwd_params &, cudaStream_t) [with Kernel_traits=Flash_fwd_kernel_traits<256, 64, 64, 4, false, false, cutlass::bfloat16_t, Flash_kernel_traits<256, 64, 64, 4, cutlass::bfloat16_t>>, Is_dropout=false, Is_causal=true]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(236): here
            instantiation of "void run_mha_fwd_hdim256<T>(Flash_fwd_params &, cudaStream_t) [with T=cutlass::bfloat16_t]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_hdim256_bf16_sm80.cu(8): here

/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_kernel.h(140): warning: variable "MMA_M" was declared but never referenced
          detected during:
            instantiation of "void flash::compute_attn_1rowblock<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &, int, int, int) [with Kernel_traits=Flash_fwd_kernel_traits<256, 64, 64, 4, false, false, cutlass::bfloat16_t, Flash_kernel_traits<256, 64, 64, 4, cutlass::bfloat16_t>>, Is_dropout=false, Is_causal=true, Is_even_N=false, Is_even_K=true, Return_softmax=false, Params=Flash_fwd_params]" 
(588): here
            instantiation of "void flash::compute_attn<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &) [with Kernel_traits=Flash_fwd_kernel_traits<256, 64, 64, 4, false, false, cutlass::bfloat16_t, Flash_kernel_traits<256, 64, 64, 4, cutlass::bfloat16_t>>, Is_dropout=false, Is_causal=true, Is_even_N=false, Is_even_K=true, Return_softmax=false, Params=Flash_fwd_params]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(15): here
            instantiation of "void flash_fwd_kernel<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax>(Flash_fwd_params) [with Kernel_traits=Flash_fwd_kernel_traits<256, 64, 64, 4, false, false, cutlass::bfloat16_t, Flash_kernel_traits<256, 64, 64, 4, cutlass::bfloat16_t>>, Is_dropout=false, Is_causal=true, Is_even_N=false, Is_even_K=true, Return_softmax=false]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(34): here
            instantiation of "void run_flash_fwd<Kernel_traits,Is_dropout,Is_causal>(Flash_fwd_params &, cudaStream_t) [with Kernel_traits=Flash_fwd_kernel_traits<256, 64, 64, 4, false, false, cutlass::bfloat16_t, Flash_kernel_traits<256, 64, 64, 4, cutlass::bfloat16_t>>, Is_dropout=false, Is_causal=true]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(236): here
            instantiation of "void run_mha_fwd_hdim256<T>(Flash_fwd_params &, cudaStream_t) [with T=cutlass::bfloat16_t]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_hdim256_bf16_sm80.cu(8): here

/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_kernel.h(275): warning: variable "tQrQ" was declared but never referenced
          detected during:
            instantiation of "void flash::compute_attn_1rowblock<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &, int, int, int) [with Kernel_traits=Flash_fwd_kernel_traits<256, 64, 64, 4, false, false, cutlass::bfloat16_t, Flash_kernel_traits<256, 64, 64, 4, cutlass::bfloat16_t>>, Is_dropout=false, Is_causal=true, Is_even_N=false, Is_even_K=true, Return_softmax=false, Params=Flash_fwd_params]" 
(588): here
            instantiation of "void flash::compute_attn<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &) [with Kernel_traits=Flash_fwd_kernel_traits<256, 64, 64, 4, false, false, cutlass::bfloat16_t, Flash_kernel_traits<256, 64, 64, 4, cutlass::bfloat16_t>>, Is_dropout=false, Is_causal=true, Is_even_N=false, Is_even_K=true, Return_softmax=false, Params=Flash_fwd_params]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(15): here
            instantiation of "void flash_fwd_kernel<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax>(Flash_fwd_params) [with Kernel_traits=Flash_fwd_kernel_traits<256, 64, 64, 4, false, false, cutlass::bfloat16_t, Flash_kernel_traits<256, 64, 64, 4, cutlass::bfloat16_t>>, Is_dropout=false, Is_causal=true, Is_even_N=false, Is_even_K=true, Return_softmax=false]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(34): here
            instantiation of "void run_flash_fwd<Kernel_traits,Is_dropout,Is_causal>(Flash_fwd_params &, cudaStream_t) [with Kernel_traits=Flash_fwd_kernel_traits<256, 64, 64, 4, false, false, cutlass::bfloat16_t, Flash_kernel_traits<256, 64, 64, 4, cutlass::bfloat16_t>>, Is_dropout=false, Is_causal=true]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(236): here
            instantiation of "void run_mha_fwd_hdim256<T>(Flash_fwd_params &, cudaStream_t) [with T=cutlass::bfloat16_t]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_hdim256_bf16_sm80.cu(8): here

/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_kernel.h(140): warning: variable "MMA_M" was declared but never referenced
          detected during:
            instantiation of "void flash::compute_attn_1rowblock<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &, int, int, int) [with Kernel_traits=Flash_fwd_kernel_traits<256, 64, 64, 4, false, false, cutlass::bfloat16_t, Flash_kernel_traits<256, 64, 64, 4, cutlass::bfloat16_t>>, Is_dropout=false, Is_causal=true, Is_even_N=false, Is_even_K=false, Return_softmax=false, Params=Flash_fwd_params]" 
(588): here
            instantiation of "void flash::compute_attn<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &) [with Kernel_traits=Flash_fwd_kernel_traits<256, 64, 64, 4, false, false, cutlass::bfloat16_t, Flash_kernel_traits<256, 64, 64, 4, cutlass::bfloat16_t>>, Is_dropout=false, Is_causal=true, Is_even_N=false, Is_even_K=false, Return_softmax=false, Params=Flash_fwd_params]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(15): here
            instantiation of "void flash_fwd_kernel<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax>(Flash_fwd_params) [with Kernel_traits=Flash_fwd_kernel_traits<256, 64, 64, 4, false, false, cutlass::bfloat16_t, Flash_kernel_traits<256, 64, 64, 4, cutlass::bfloat16_t>>, Is_dropout=false, Is_causal=true, Is_even_N=false, Is_even_K=false, Return_softmax=false]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(34): here
            instantiation of "void run_flash_fwd<Kernel_traits,Is_dropout,Is_causal>(Flash_fwd_params &, cudaStream_t) [with Kernel_traits=Flash_fwd_kernel_traits<256, 64, 64, 4, false, false, cutlass::bfloat16_t, Flash_kernel_traits<256, 64, 64, 4, cutlass::bfloat16_t>>, Is_dropout=false, Is_causal=true]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(236): here
            instantiation of "void run_mha_fwd_hdim256<T>(Flash_fwd_params &, cudaStream_t) [with T=cutlass::bfloat16_t]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_hdim256_bf16_sm80.cu(8): here

/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_kernel.h(275): warning: variable "tQrQ" was declared but never referenced
          detected during:
            instantiation of "void flash::compute_attn_1rowblock<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &, int, int, int) [with Kernel_traits=Flash_fwd_kernel_traits<256, 64, 64, 4, false, false, cutlass::bfloat16_t, Flash_kernel_traits<256, 64, 64, 4, cutlass::bfloat16_t>>, Is_dropout=false, Is_causal=true, Is_even_N=false, Is_even_K=false, Return_softmax=false, Params=Flash_fwd_params]" 
(588): here
            instantiation of "void flash::compute_attn<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &) [with Kernel_traits=Flash_fwd_kernel_traits<256, 64, 64, 4, false, false, cutlass::bfloat16_t, Flash_kernel_traits<256, 64, 64, 4, cutlass::bfloat16_t>>, Is_dropout=false, Is_causal=true, Is_even_N=false, Is_even_K=false, Return_softmax=false, Params=Flash_fwd_params]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(15): here
            instantiation of "void flash_fwd_kernel<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax>(Flash_fwd_params) [with Kernel_traits=Flash_fwd_kernel_traits<256, 64, 64, 4, false, false, cutlass::bfloat16_t, Flash_kernel_traits<256, 64, 64, 4, cutlass::bfloat16_t>>, Is_dropout=false, Is_causal=true, Is_even_N=false, Is_even_K=false, Return_softmax=false]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(34): here
            instantiation of "void run_flash_fwd<Kernel_traits,Is_dropout,Is_causal>(Flash_fwd_params &, cudaStream_t) [with Kernel_traits=Flash_fwd_kernel_traits<256, 64, 64, 4, false, false, cutlass::bfloat16_t, Flash_kernel_traits<256, 64, 64, 4, cutlass::bfloat16_t>>, Is_dropout=false, Is_causal=true]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(236): here
            instantiation of "void run_mha_fwd_hdim256<T>(Flash_fwd_params &, cudaStream_t) [with T=cutlass::bfloat16_t]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_hdim256_bf16_sm80.cu(8): here

/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_kernel.h(140): warning: variable "MMA_M" was declared but never referenced
          detected during:
            instantiation of "void flash::compute_attn_1rowblock<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &, int, int, int) [with Kernel_traits=Flash_fwd_kernel_traits<256, 128, 64, 8, false, false, cutlass::bfloat16_t, Flash_kernel_traits<256, 128, 64, 8, cutlass::bfloat16_t>>, Is_dropout=false, Is_causal=false, Is_even_N=true, Is_even_K=true, Return_softmax=false, Params=Flash_fwd_params]" 
(588): here
            instantiation of "void flash::compute_attn<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &) [with Kernel_traits=Flash_fwd_kernel_traits<256, 128, 64, 8, false, false, cutlass::bfloat16_t, Flash_kernel_traits<256, 128, 64, 8, cutlass::bfloat16_t>>, Is_dropout=false, Is_causal=false, Is_even_N=true, Is_even_K=true, Return_softmax=false, Params=Flash_fwd_params]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(15): here
            instantiation of "void flash_fwd_kernel<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax>(Flash_fwd_params) [with Kernel_traits=Flash_fwd_kernel_traits<256, 128, 64, 8, false, false, cutlass::bfloat16_t, Flash_kernel_traits<256, 128, 64, 8, cutlass::bfloat16_t>>, Is_dropout=false, Is_causal=false, Is_even_N=true, Is_even_K=true, Return_softmax=false]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(34): here
            instantiation of "void run_flash_fwd<Kernel_traits,Is_dropout,Is_causal>(Flash_fwd_params &, cudaStream_t) [with Kernel_traits=Flash_fwd_kernel_traits<256, 128, 64, 8, false, false, cutlass::bfloat16_t, Flash_kernel_traits<256, 128, 64, 8, cutlass::bfloat16_t>>, Is_dropout=false, Is_causal=false]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(236): here
            instantiation of "void run_mha_fwd_hdim256<T>(Flash_fwd_params &, cudaStream_t) [with T=cutlass::bfloat16_t]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_hdim256_bf16_sm80.cu(8): here

/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_kernel.h(275): warning: variable "tQrQ" was declared but never referenced
          detected during:
            instantiation of "void flash::compute_attn_1rowblock<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &, int, int, int) [with Kernel_traits=Flash_fwd_kernel_traits<256, 128, 64, 8, false, false, cutlass::bfloat16_t, Flash_kernel_traits<256, 128, 64, 8, cutlass::bfloat16_t>>, Is_dropout=false, Is_causal=false, Is_even_N=true, Is_even_K=true, Return_softmax=false, Params=Flash_fwd_params]" 
(588): here
            instantiation of "void flash::compute_attn<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &) [with Kernel_traits=Flash_fwd_kernel_traits<256, 128, 64, 8, false, false, cutlass::bfloat16_t, Flash_kernel_traits<256, 128, 64, 8, cutlass::bfloat16_t>>, Is_dropout=false, Is_causal=false, Is_even_N=true, Is_even_K=true, Return_softmax=false, Params=Flash_fwd_params]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(15): here
            instantiation of "void flash_fwd_kernel<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax>(Flash_fwd_params) [with Kernel_traits=Flash_fwd_kernel_traits<256, 128, 64, 8, false, false, cutlass::bfloat16_t, Flash_kernel_traits<256, 128, 64, 8, cutlass::bfloat16_t>>, Is_dropout=false, Is_causal=false, Is_even_N=true, Is_even_K=true, Return_softmax=false]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(34): here
            instantiation of "void run_flash_fwd<Kernel_traits,Is_dropout,Is_causal>(Flash_fwd_params &, cudaStream_t) [with Kernel_traits=Flash_fwd_kernel_traits<256, 128, 64, 8, false, false, cutlass::bfloat16_t, Flash_kernel_traits<256, 128, 64, 8, cutlass::bfloat16_t>>, Is_dropout=false, Is_causal=false]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(236): here
            instantiation of "void run_mha_fwd_hdim256<T>(Flash_fwd_params &, cudaStream_t) [with T=cutlass::bfloat16_t]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_hdim256_bf16_sm80.cu(8): here

/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_kernel.h(140): warning: variable "MMA_M" was declared but never referenced
          detected during:
            instantiation of "void flash::compute_attn_1rowblock<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &, int, int, int) [with Kernel_traits=Flash_fwd_kernel_traits<256, 128, 64, 8, false, false, cutlass::bfloat16_t, Flash_kernel_traits<256, 128, 64, 8, cutlass::bfloat16_t>>, Is_dropout=false, Is_causal=false, Is_even_N=true, Is_even_K=false, Return_softmax=false, Params=Flash_fwd_params]" 
(588): here
            instantiation of "void flash::compute_attn<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &) [with Kernel_traits=Flash_fwd_kernel_traits<256, 128, 64, 8, false, false, cutlass::bfloat16_t, Flash_kernel_traits<256, 128, 64, 8, cutlass::bfloat16_t>>, Is_dropout=false, Is_causal=false, Is_even_N=true, Is_even_K=false, Return_softmax=false, Params=Flash_fwd_params]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(15): here
            instantiation of "void flash_fwd_kernel<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax>(Flash_fwd_params) [with Kernel_traits=Flash_fwd_kernel_traits<256, 128, 64, 8, false, false, cutlass::bfloat16_t, Flash_kernel_traits<256, 128, 64, 8, cutlass::bfloat16_t>>, Is_dropout=false, Is_causal=false, Is_even_N=true, Is_even_K=false, Return_softmax=false]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(34): here
            instantiation of "void run_flash_fwd<Kernel_traits,Is_dropout,Is_causal>(Flash_fwd_params &, cudaStream_t) [with Kernel_traits=Flash_fwd_kernel_traits<256, 128, 64, 8, false, false, cutlass::bfloat16_t, Flash_kernel_traits<256, 128, 64, 8, cutlass::bfloat16_t>>, Is_dropout=false, Is_causal=false]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(236): here
            instantiation of "void run_mha_fwd_hdim256<T>(Flash_fwd_params &, cudaStream_t) [with T=cutlass::bfloat16_t]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_hdim256_bf16_sm80.cu(8): here

/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_kernel.h(275): warning: variable "tQrQ" was declared but never referenced
          detected during:
            instantiation of "void flash::compute_attn_1rowblock<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &, int, int, int) [with Kernel_traits=Flash_fwd_kernel_traits<256, 128, 64, 8, false, false, cutlass::bfloat16_t, Flash_kernel_traits<256, 128, 64, 8, cutlass::bfloat16_t>>, Is_dropout=false, Is_causal=false, Is_even_N=true, Is_even_K=false, Return_softmax=false, Params=Flash_fwd_params]" 
(588): here
            instantiation of "void flash::compute_attn<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &) [with Kernel_traits=Flash_fwd_kernel_traits<256, 128, 64, 8, false, false, cutlass::bfloat16_t, Flash_kernel_traits<256, 128, 64, 8, cutlass::bfloat16_t>>, Is_dropout=false, Is_causal=false, Is_even_N=true, Is_even_K=false, Return_softmax=false, Params=Flash_fwd_params]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(15): here
            instantiation of "void flash_fwd_kernel<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax>(Flash_fwd_params) [with Kernel_traits=Flash_fwd_kernel_traits<256, 128, 64, 8, false, false, cutlass::bfloat16_t, Flash_kernel_traits<256, 128, 64, 8, cutlass::bfloat16_t>>, Is_dropout=false, Is_causal=false, Is_even_N=true, Is_even_K=false, Return_softmax=false]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(34): here
            instantiation of "void run_flash_fwd<Kernel_traits,Is_dropout,Is_causal>(Flash_fwd_params &, cudaStream_t) [with Kernel_traits=Flash_fwd_kernel_traits<256, 128, 64, 8, false, false, cutlass::bfloat16_t, Flash_kernel_traits<256, 128, 64, 8, cutlass::bfloat16_t>>, Is_dropout=false, Is_causal=false]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(236): here
            instantiation of "void run_mha_fwd_hdim256<T>(Flash_fwd_params &, cudaStream_t) [with T=cutlass::bfloat16_t]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_hdim256_bf16_sm80.cu(8): here

/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_kernel.h(140): warning: variable "MMA_M" was declared but never referenced
          detected during:
            instantiation of "void flash::compute_attn_1rowblock<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &, int, int, int) [with Kernel_traits=Flash_fwd_kernel_traits<256, 128, 64, 8, false, false, cutlass::bfloat16_t, Flash_kernel_traits<256, 128, 64, 8, cutlass::bfloat16_t>>, Is_dropout=false, Is_causal=false, Is_even_N=false, Is_even_K=true, Return_softmax=false, Params=Flash_fwd_params]" 
(588): here
            instantiation of "void flash::compute_attn<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &) [with Kernel_traits=Flash_fwd_kernel_traits<256, 128, 64, 8, false, false, cutlass::bfloat16_t, Flash_kernel_traits<256, 128, 64, 8, cutlass::bfloat16_t>>, Is_dropout=false, Is_causal=false, Is_even_N=false, Is_even_K=true, Return_softmax=false, Params=Flash_fwd_params]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(15): here
            instantiation of "void flash_fwd_kernel<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax>(Flash_fwd_params) [with Kernel_traits=Flash_fwd_kernel_traits<256, 128, 64, 8, false, false, cutlass::bfloat16_t, Flash_kernel_traits<256, 128, 64, 8, cutlass::bfloat16_t>>, Is_dropout=false, Is_causal=false, Is_even_N=false, Is_even_K=true, Return_softmax=false]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(34): here
            instantiation of "void run_flash_fwd<Kernel_traits,Is_dropout,Is_causal>(Flash_fwd_params &, cudaStream_t) [with Kernel_traits=Flash_fwd_kernel_traits<256, 128, 64, 8, false, false, cutlass::bfloat16_t, Flash_kernel_traits<256, 128, 64, 8, cutlass::bfloat16_t>>, Is_dropout=false, Is_causal=false]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(236): here
            instantiation of "void run_mha_fwd_hdim256<T>(Flash_fwd_params &, cudaStream_t) [with T=cutlass::bfloat16_t]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_hdim256_bf16_sm80.cu(8): here

/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_kernel.h(275): warning: variable "tQrQ" was declared but never referenced
          detected during:
            instantiation of "void flash::compute_attn_1rowblock<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &, int, int, int) [with Kernel_traits=Flash_fwd_kernel_traits<256, 128, 64, 8, false, false, cutlass::bfloat16_t, Flash_kernel_traits<256, 128, 64, 8, cutlass::bfloat16_t>>, Is_dropout=false, Is_causal=false, Is_even_N=false, Is_even_K=true, Return_softmax=false, Params=Flash_fwd_params]" 
(588): here
            instantiation of "void flash::compute_attn<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &) [with Kernel_traits=Flash_fwd_kernel_traits<256, 128, 64, 8, false, false, cutlass::bfloat16_t, Flash_kernel_traits<256, 128, 64, 8, cutlass::bfloat16_t>>, Is_dropout=false, Is_causal=false, Is_even_N=false, Is_even_K=true, Return_softmax=false, Params=Flash_fwd_params]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(15): here
            instantiation of "void flash_fwd_kernel<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax>(Flash_fwd_params) [with Kernel_traits=Flash_fwd_kernel_traits<256, 128, 64, 8, false, false, cutlass::bfloat16_t, Flash_kernel_traits<256, 128, 64, 8, cutlass::bfloat16_t>>, Is_dropout=false, Is_causal=false, Is_even_N=false, Is_even_K=true, Return_softmax=false]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(34): here
            instantiation of "void run_flash_fwd<Kernel_traits,Is_dropout,Is_causal>(Flash_fwd_params &, cudaStream_t) [with Kernel_traits=Flash_fwd_kernel_traits<256, 128, 64, 8, false, false, cutlass::bfloat16_t, Flash_kernel_traits<256, 128, 64, 8, cutlass::bfloat16_t>>, Is_dropout=false, Is_causal=false]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(236): here
            instantiation of "void run_mha_fwd_hdim256<T>(Flash_fwd_params &, cudaStream_t) [with T=cutlass::bfloat16_t]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_hdim256_bf16_sm80.cu(8): here

/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_kernel.h(140): warning: variable "MMA_M" was declared but never referenced
          detected during:
            instantiation of "void flash::compute_attn_1rowblock<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &, int, int, int) [with Kernel_traits=Flash_fwd_kernel_traits<256, 128, 64, 8, false, false, cutlass::bfloat16_t, Flash_kernel_traits<256, 128, 64, 8, cutlass::bfloat16_t>>, Is_dropout=false, Is_causal=false, Is_even_N=false, Is_even_K=false, Return_softmax=false, Params=Flash_fwd_params]" 
(588): here
            instantiation of "void flash::compute_attn<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &) [with Kernel_traits=Flash_fwd_kernel_traits<256, 128, 64, 8, false, false, cutlass::bfloat16_t, Flash_kernel_traits<256, 128, 64, 8, cutlass::bfloat16_t>>, Is_dropout=false, Is_causal=false, Is_even_N=false, Is_even_K=false, Return_softmax=false, Params=Flash_fwd_params]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(15): here
            instantiation of "void flash_fwd_kernel<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax>(Flash_fwd_params) [with Kernel_traits=Flash_fwd_kernel_traits<256, 128, 64, 8, false, false, cutlass::bfloat16_t, Flash_kernel_traits<256, 128, 64, 8, cutlass::bfloat16_t>>, Is_dropout=false, Is_causal=false, Is_even_N=false, Is_even_K=false, Return_softmax=false]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(34): here
            instantiation of "void run_flash_fwd<Kernel_traits,Is_dropout,Is_causal>(Flash_fwd_params &, cudaStream_t) [with Kernel_traits=Flash_fwd_kernel_traits<256, 128, 64, 8, false, false, cutlass::bfloat16_t, Flash_kernel_traits<256, 128, 64, 8, cutlass::bfloat16_t>>, Is_dropout=false, Is_causal=false]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(236): here
            instantiation of "void run_mha_fwd_hdim256<T>(Flash_fwd_params &, cudaStream_t) [with T=cutlass::bfloat16_t]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_hdim256_bf16_sm80.cu(8): here

/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_kernel.h(275): warning: variable "tQrQ" was declared but never referenced
          detected during:
            instantiation of "void flash::compute_attn_1rowblock<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &, int, int, int) [with Kernel_traits=Flash_fwd_kernel_traits<256, 128, 64, 8, false, false, cutlass::bfloat16_t, Flash_kernel_traits<256, 128, 64, 8, cutlass::bfloat16_t>>, Is_dropout=false, Is_causal=false, Is_even_N=false, Is_even_K=false, Return_softmax=false, Params=Flash_fwd_params]" 
(588): here
            instantiation of "void flash::compute_attn<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &) [with Kernel_traits=Flash_fwd_kernel_traits<256, 128, 64, 8, false, false, cutlass::bfloat16_t, Flash_kernel_traits<256, 128, 64, 8, cutlass::bfloat16_t>>, Is_dropout=false, Is_causal=false, Is_even_N=false, Is_even_K=false, Return_softmax=false, Params=Flash_fwd_params]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(15): here
            instantiation of "void flash_fwd_kernel<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax>(Flash_fwd_params) [with Kernel_traits=Flash_fwd_kernel_traits<256, 128, 64, 8, false, false, cutlass::bfloat16_t, Flash_kernel_traits<256, 128, 64, 8, cutlass::bfloat16_t>>, Is_dropout=false, Is_causal=false, Is_even_N=false, Is_even_K=false, Return_softmax=false]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(34): here
            instantiation of "void run_flash_fwd<Kernel_traits,Is_dropout,Is_causal>(Flash_fwd_params &, cudaStream_t) [with Kernel_traits=Flash_fwd_kernel_traits<256, 128, 64, 8, false, false, cutlass::bfloat16_t, Flash_kernel_traits<256, 128, 64, 8, cutlass::bfloat16_t>>, Is_dropout=false, Is_causal=false]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(236): here
            instantiation of "void run_mha_fwd_hdim256<T>(Flash_fwd_params &, cudaStream_t) [with T=cutlass::bfloat16_t]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_hdim256_bf16_sm80.cu(8): here

/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_kernel.h(140): warning: variable "MMA_M" was declared but never referenced
          detected during:
            instantiation of "void flash::compute_attn_1rowblock<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &, int, int, int) [with Kernel_traits=Flash_fwd_kernel_traits<256, 64, 64, 4, false, false, cutlass::bfloat16_t, Flash_kernel_traits<256, 64, 64, 4, cutlass::bfloat16_t>>, Is_dropout=false, Is_causal=false, Is_even_N=true, Is_even_K=true, Return_softmax=false, Params=Flash_fwd_params]" 
(588): here
            instantiation of "void flash::compute_attn<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &) [with Kernel_traits=Flash_fwd_kernel_traits<256, 64, 64, 4, false, false, cutlass::bfloat16_t, Flash_kernel_traits<256, 64, 64, 4, cutlass::bfloat16_t>>, Is_dropout=false, Is_causal=false, Is_even_N=true, Is_even_K=true, Return_softmax=false, Params=Flash_fwd_params]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(15): here
            instantiation of "void flash_fwd_kernel<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax>(Flash_fwd_params) [with Kernel_traits=Flash_fwd_kernel_traits<256, 64, 64, 4, false, false, cutlass::bfloat16_t, Flash_kernel_traits<256, 64, 64, 4, cutlass::bfloat16_t>>, Is_dropout=false, Is_causal=false, Is_even_N=true, Is_even_K=true, Return_softmax=false]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(34): here
            instantiation of "void run_flash_fwd<Kernel_traits,Is_dropout,Is_causal>(Flash_fwd_params &, cudaStream_t) [with Kernel_traits=Flash_fwd_kernel_traits<256, 64, 64, 4, false, false, cutlass::bfloat16_t, Flash_kernel_traits<256, 64, 64, 4, cutlass::bfloat16_t>>, Is_dropout=false, Is_causal=false]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(236): here
            instantiation of "void run_mha_fwd_hdim256<T>(Flash_fwd_params &, cudaStream_t) [with T=cutlass::bfloat16_t]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_hdim256_bf16_sm80.cu(8): here

/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_kernel.h(275): warning: variable "tQrQ" was declared but never referenced
          detected during:
            instantiation of "void flash::compute_attn_1rowblock<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &, int, int, int) [with Kernel_traits=Flash_fwd_kernel_traits<256, 64, 64, 4, false, false, cutlass::bfloat16_t, Flash_kernel_traits<256, 64, 64, 4, cutlass::bfloat16_t>>, Is_dropout=false, Is_causal=false, Is_even_N=true, Is_even_K=true, Return_softmax=false, Params=Flash_fwd_params]" 
(588): here
            instantiation of "void flash::compute_attn<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &) [with Kernel_traits=Flash_fwd_kernel_traits<256, 64, 64, 4, false, false, cutlass::bfloat16_t, Flash_kernel_traits<256, 64, 64, 4, cutlass::bfloat16_t>>, Is_dropout=false, Is_causal=false, Is_even_N=true, Is_even_K=true, Return_softmax=false, Params=Flash_fwd_params]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(15): here
            instantiation of "void flash_fwd_kernel<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax>(Flash_fwd_params) [with Kernel_traits=Flash_fwd_kernel_traits<256, 64, 64, 4, false, false, cutlass::bfloat16_t, Flash_kernel_traits<256, 64, 64, 4, cutlass::bfloat16_t>>, Is_dropout=false, Is_causal=false, Is_even_N=true, Is_even_K=true, Return_softmax=false]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(34): here
            instantiation of "void run_flash_fwd<Kernel_traits,Is_dropout,Is_causal>(Flash_fwd_params &, cudaStream_t) [with Kernel_traits=Flash_fwd_kernel_traits<256, 64, 64, 4, false, false, cutlass::bfloat16_t, Flash_kernel_traits<256, 64, 64, 4, cutlass::bfloat16_t>>, Is_dropout=false, Is_causal=false]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(236): here
            instantiation of "void run_mha_fwd_hdim256<T>(Flash_fwd_params &, cudaStream_t) [with T=cutlass::bfloat16_t]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_hdim256_bf16_sm80.cu(8): here

/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_kernel.h(140): warning: variable "MMA_M" was declared but never referenced
          detected during:
            instantiation of "void flash::compute_attn_1rowblock<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &, int, int, int) [with Kernel_traits=Flash_fwd_kernel_traits<256, 64, 64, 4, false, false, cutlass::bfloat16_t, Flash_kernel_traits<256, 64, 64, 4, cutlass::bfloat16_t>>, Is_dropout=false, Is_causal=false, Is_even_N=true, Is_even_K=false, Return_softmax=false, Params=Flash_fwd_params]" 
(588): here
            instantiation of "void flash::compute_attn<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &) [with Kernel_traits=Flash_fwd_kernel_traits<256, 64, 64, 4, false, false, cutlass::bfloat16_t, Flash_kernel_traits<256, 64, 64, 4, cutlass::bfloat16_t>>, Is_dropout=false, Is_causal=false, Is_even_N=true, Is_even_K=false, Return_softmax=false, Params=Flash_fwd_params]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(15): here
            instantiation of "void flash_fwd_kernel<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax>(Flash_fwd_params) [with Kernel_traits=Flash_fwd_kernel_traits<256, 64, 64, 4, false, false, cutlass::bfloat16_t, Flash_kernel_traits<256, 64, 64, 4, cutlass::bfloat16_t>>, Is_dropout=false, Is_causal=false, Is_even_N=true, Is_even_K=false, Return_softmax=false]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(34): here
            instantiation of "void run_flash_fwd<Kernel_traits,Is_dropout,Is_causal>(Flash_fwd_params &, cudaStream_t) [with Kernel_traits=Flash_fwd_kernel_traits<256, 64, 64, 4, false, false, cutlass::bfloat16_t, Flash_kernel_traits<256, 64, 64, 4, cutlass::bfloat16_t>>, Is_dropout=false, Is_causal=false]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(236): here
            instantiation of "void run_mha_fwd_hdim256<T>(Flash_fwd_params &, cudaStream_t) [with T=cutlass::bfloat16_t]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_hdim256_bf16_sm80.cu(8): here

/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_kernel.h(275): warning: variable "tQrQ" was declared but never referenced
          detected during:
            instantiation of "void flash::compute_attn_1rowblock<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &, int, int, int) [with Kernel_traits=Flash_fwd_kernel_traits<256, 64, 64, 4, false, false, cutlass::bfloat16_t, Flash_kernel_traits<256, 64, 64, 4, cutlass::bfloat16_t>>, Is_dropout=false, Is_causal=false, Is_even_N=true, Is_even_K=false, Return_softmax=false, Params=Flash_fwd_params]" 
(588): here
            instantiation of "void flash::compute_attn<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &) [with Kernel_traits=Flash_fwd_kernel_traits<256, 64, 64, 4, false, false, cutlass::bfloat16_t, Flash_kernel_traits<256, 64, 64, 4, cutlass::bfloat16_t>>, Is_dropout=false, Is_causal=false, Is_even_N=true, Is_even_K=false, Return_softmax=false, Params=Flash_fwd_params]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(15): here
            instantiation of "void flash_fwd_kernel<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax>(Flash_fwd_params) [with Kernel_traits=Flash_fwd_kernel_traits<256, 64, 64, 4, false, false, cutlass::bfloat16_t, Flash_kernel_traits<256, 64, 64, 4, cutlass::bfloat16_t>>, Is_dropout=false, Is_causal=false, Is_even_N=true, Is_even_K=false, Return_softmax=false]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(34): here
            instantiation of "void run_flash_fwd<Kernel_traits,Is_dropout,Is_causal>(Flash_fwd_params &, cudaStream_t) [with Kernel_traits=Flash_fwd_kernel_traits<256, 64, 64, 4, false, false, cutlass::bfloat16_t, Flash_kernel_traits<256, 64, 64, 4, cutlass::bfloat16_t>>, Is_dropout=false, Is_causal=false]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(236): here
            instantiation of "void run_mha_fwd_hdim256<T>(Flash_fwd_params &, cudaStream_t) [with T=cutlass::bfloat16_t]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_hdim256_bf16_sm80.cu(8): here

/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_kernel.h(140): warning: variable "MMA_M" was declared but never referenced
          detected during:
            instantiation of "void flash::compute_attn_1rowblock<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &, int, int, int) [with Kernel_traits=Flash_fwd_kernel_traits<256, 64, 64, 4, false, false, cutlass::bfloat16_t, Flash_kernel_traits<256, 64, 64, 4, cutlass::bfloat16_t>>, Is_dropout=false, Is_causal=false, Is_even_N=false, Is_even_K=true, Return_softmax=false, Params=Flash_fwd_params]" 
(588): here
            instantiation of "void flash::compute_attn<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &) [with Kernel_traits=Flash_fwd_kernel_traits<256, 64, 64, 4, false, false, cutlass::bfloat16_t, Flash_kernel_traits<256, 64, 64, 4, cutlass::bfloat16_t>>, Is_dropout=false, Is_causal=false, Is_even_N=false, Is_even_K=true, Return_softmax=false, Params=Flash_fwd_params]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(15): here
            instantiation of "void flash_fwd_kernel<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax>(Flash_fwd_params) [with Kernel_traits=Flash_fwd_kernel_traits<256, 64, 64, 4, false, false, cutlass::bfloat16_t, Flash_kernel_traits<256, 64, 64, 4, cutlass::bfloat16_t>>, Is_dropout=false, Is_causal=false, Is_even_N=false, Is_even_K=true, Return_softmax=false]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(34): here
            instantiation of "void run_flash_fwd<Kernel_traits,Is_dropout,Is_causal>(Flash_fwd_params &, cudaStream_t) [with Kernel_traits=Flash_fwd_kernel_traits<256, 64, 64, 4, false, false, cutlass::bfloat16_t, Flash_kernel_traits<256, 64, 64, 4, cutlass::bfloat16_t>>, Is_dropout=false, Is_causal=false]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(236): here
            instantiation of "void run_mha_fwd_hdim256<T>(Flash_fwd_params &, cudaStream_t) [with T=cutlass::bfloat16_t]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_hdim256_bf16_sm80.cu(8): here

/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_kernel.h(275): warning: variable "tQrQ" was declared but never referenced
          detected during:
            instantiation of "void flash::compute_attn_1rowblock<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &, int, int, int) [with Kernel_traits=Flash_fwd_kernel_traits<256, 64, 64, 4, false, false, cutlass::bfloat16_t, Flash_kernel_traits<256, 64, 64, 4, cutlass::bfloat16_t>>, Is_dropout=false, Is_causal=false, Is_even_N=false, Is_even_K=true, Return_softmax=false, Params=Flash_fwd_params]" 
(588): here
            instantiation of "void flash::compute_attn<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &) [with Kernel_traits=Flash_fwd_kernel_traits<256, 64, 64, 4, false, false, cutlass::bfloat16_t, Flash_kernel_traits<256, 64, 64, 4, cutlass::bfloat16_t>>, Is_dropout=false, Is_causal=false, Is_even_N=false, Is_even_K=true, Return_softmax=false, Params=Flash_fwd_params]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(15): here
            instantiation of "void flash_fwd_kernel<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax>(Flash_fwd_params) [with Kernel_traits=Flash_fwd_kernel_traits<256, 64, 64, 4, false, false, cutlass::bfloat16_t, Flash_kernel_traits<256, 64, 64, 4, cutlass::bfloat16_t>>, Is_dropout=false, Is_causal=false, Is_even_N=false, Is_even_K=true, Return_softmax=false]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(34): here
            instantiation of "void run_flash_fwd<Kernel_traits,Is_dropout,Is_causal>(Flash_fwd_params &, cudaStream_t) [with Kernel_traits=Flash_fwd_kernel_traits<256, 64, 64, 4, false, false, cutlass::bfloat16_t, Flash_kernel_traits<256, 64, 64, 4, cutlass::bfloat16_t>>, Is_dropout=false, Is_causal=false]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(236): here
            instantiation of "void run_mha_fwd_hdim256<T>(Flash_fwd_params &, cudaStream_t) [with T=cutlass::bfloat16_t]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_hdim256_bf16_sm80.cu(8): here

/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_kernel.h(140): warning: variable "MMA_M" was declared but never referenced
          detected during:
            instantiation of "void flash::compute_attn_1rowblock<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &, int, int, int) [with Kernel_traits=Flash_fwd_kernel_traits<256, 64, 64, 4, false, false, cutlass::bfloat16_t, Flash_kernel_traits<256, 64, 64, 4, cutlass::bfloat16_t>>, Is_dropout=false, Is_causal=false, Is_even_N=false, Is_even_K=false, Return_softmax=false, Params=Flash_fwd_params]" 
(588): here
            instantiation of "void flash::compute_attn<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &) [with Kernel_traits=Flash_fwd_kernel_traits<256, 64, 64, 4, false, false, cutlass::bfloat16_t, Flash_kernel_traits<256, 64, 64, 4, cutlass::bfloat16_t>>, Is_dropout=false, Is_causal=false, Is_even_N=false, Is_even_K=false, Return_softmax=false, Params=Flash_fwd_params]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(15): here
            instantiation of "void flash_fwd_kernel<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax>(Flash_fwd_params) [with Kernel_traits=Flash_fwd_kernel_traits<256, 64, 64, 4, false, false, cutlass::bfloat16_t, Flash_kernel_traits<256, 64, 64, 4, cutlass::bfloat16_t>>, Is_dropout=false, Is_causal=false, Is_even_N=false, Is_even_K=false, Return_softmax=false]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(34): here
            instantiation of "void run_flash_fwd<Kernel_traits,Is_dropout,Is_causal>(Flash_fwd_params &, cudaStream_t) [with Kernel_traits=Flash_fwd_kernel_traits<256, 64, 64, 4, false, false, cutlass::bfloat16_t, Flash_kernel_traits<256, 64, 64, 4, cutlass::bfloat16_t>>, Is_dropout=false, Is_causal=false]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(236): here
            instantiation of "void run_mha_fwd_hdim256<T>(Flash_fwd_params &, cudaStream_t) [with T=cutlass::bfloat16_t]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_hdim256_bf16_sm80.cu(8): here

/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_kernel.h(275): warning: variable "tQrQ" was declared but never referenced
          detected during:
            instantiation of "void flash::compute_attn_1rowblock<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &, int, int, int) [with Kernel_traits=Flash_fwd_kernel_traits<256, 64, 64, 4, false, false, cutlass::bfloat16_t, Flash_kernel_traits<256, 64, 64, 4, cutlass::bfloat16_t>>, Is_dropout=false, Is_causal=false, Is_even_N=false, Is_even_K=false, Return_softmax=false, Params=Flash_fwd_params]" 
(588): here
            instantiation of "void flash::compute_attn<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &) [with Kernel_traits=Flash_fwd_kernel_traits<256, 64, 64, 4, false, false, cutlass::bfloat16_t, Flash_kernel_traits<256, 64, 64, 4, cutlass::bfloat16_t>>, Is_dropout=false, Is_causal=false, Is_even_N=false, Is_even_K=false, Return_softmax=false, Params=Flash_fwd_params]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(15): here
            instantiation of "void flash_fwd_kernel<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax>(Flash_fwd_params) [with Kernel_traits=Flash_fwd_kernel_traits<256, 64, 64, 4, false, false, cutlass::bfloat16_t, Flash_kernel_traits<256, 64, 64, 4, cutlass::bfloat16_t>>, Is_dropout=false, Is_causal=false, Is_even_N=false, Is_even_K=false, Return_softmax=false]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(34): here
            instantiation of "void run_flash_fwd<Kernel_traits,Is_dropout,Is_causal>(Flash_fwd_params &, cudaStream_t) [with Kernel_traits=Flash_fwd_kernel_traits<256, 64, 64, 4, false, false, cutlass::bfloat16_t, Flash_kernel_traits<256, 64, 64, 4, cutlass::bfloat16_t>>, Is_dropout=false, Is_causal=false]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(236): here
            instantiation of "void run_mha_fwd_hdim256<T>(Flash_fwd_params &, cudaStream_t) [with T=cutlass::bfloat16_t]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_hdim256_bf16_sm80.cu(8): here

ptxas info    : 2 bytes gmem
ptxas info    : Compiling entry function '_Z16flash_fwd_kernelI23Flash_fwd_kernel_traitsILi256ELi64ELi64ELi4ELb0ELb0EN7cutlass10bfloat16_tE19Flash_kernel_traitsILi256ELi64ELi64ELi4ES2_EELb0ELb0ELb0ELb0ELb0EEv16Flash_fwd_params' for 'sm_80'
ptxas info    : Function properties for _Z16flash_fwd_kernelI23Flash_fwd_kernel_traitsILi256ELi64ELi64ELi4ELb0ELb0EN7cutlass10bfloat16_tE19Flash_kernel_traitsILi256ELi64ELi64ELi4ES2_EELb0ELb0ELb0ELb0ELb0EEv16Flash_fwd_params
    0 bytes stack frame, 0 bytes spill stores, 0 bytes spill loads
ptxas info    : Used 241 registers, 584 bytes cmem[0]
ptxas info    : Compiling entry function '_Z16flash_fwd_kernelI23Flash_fwd_kernel_traitsILi256ELi64ELi64ELi4ELb0ELb0EN7cutlass10bfloat16_tE19Flash_kernel_traitsILi256ELi64ELi64ELi4ES2_EELb0ELb0ELb0ELb1ELb0EEv16Flash_fwd_params' for 'sm_80'
ptxas info    : Function properties for _Z16flash_fwd_kernelI23Flash_fwd_kernel_traitsILi256ELi64ELi64ELi4ELb0ELb0EN7cutlass10bfloat16_tE19Flash_kernel_traitsILi256ELi64ELi64ELi4ES2_EELb0ELb0ELb0ELb1ELb0EEv16Flash_fwd_params
    0 bytes stack frame, 0 bytes spill stores, 0 bytes spill loads
ptxas info    : Used 237 registers, 584 bytes cmem[0]
ptxas info    : Compiling entry function '_Z16flash_fwd_kernelI23Flash_fwd_kernel_traitsILi256ELi64ELi64ELi4ELb0ELb0EN7cutlass10bfloat16_tE19Flash_kernel_traitsILi256ELi64ELi64ELi4ES2_EELb0ELb0ELb1ELb0ELb0EEv16Flash_fwd_params' for 'sm_80'
ptxas info    : Function properties for _Z16flash_fwd_kernelI23Flash_fwd_kernel_traitsILi256ELi64ELi64ELi4ELb0ELb0EN7cutlass10bfloat16_tE19Flash_kernel_traitsILi256ELi64ELi64ELi4ES2_EELb0ELb0ELb1ELb0ELb0EEv16Flash_fwd_params
    0 bytes stack frame, 0 bytes spill stores, 0 bytes spill loads
ptxas info    : Used 233 registers, 584 bytes cmem[0]
ptxas info    : Compiling entry function '_Z16flash_fwd_kernelI23Flash_fwd_kernel_traitsILi256ELi64ELi64ELi4ELb0ELb0EN7cutlass10bfloat16_tE19Flash_kernel_traitsILi256ELi64ELi64ELi4ES2_EELb0ELb0ELb1ELb1ELb0EEv16Flash_fwd_params' for 'sm_80'
ptxas info    : Function properties for _Z16flash_fwd_kernelI23Flash_fwd_kernel_traitsILi256ELi64ELi64ELi4ELb0ELb0EN7cutlass10bfloat16_tE19Flash_kernel_traitsILi256ELi64ELi64ELi4ES2_EELb0ELb0ELb1ELb1ELb0EEv16Flash_fwd_params
    0 bytes stack frame, 0 bytes spill stores, 0 bytes spill loads
ptxas info    : Used 237 registers, 584 bytes cmem[0]
ptxas info    : Compiling entry function '_Z16flash_fwd_kernelI23Flash_fwd_kernel_traitsILi256ELi128ELi64ELi8ELb0ELb0EN7cutlass10bfloat16_tE19Flash_kernel_traitsILi256ELi128ELi64ELi8ES2_EELb0ELb0ELb0ELb0ELb0EEv16Flash_fwd_params' for 'sm_80'
ptxas info    : Function properties for _Z16flash_fwd_kernelI23Flash_fwd_kernel_traitsILi256ELi128ELi64ELi8ELb0ELb0EN7cutlass10bfloat16_tE19Flash_kernel_traitsILi256ELi128ELi64ELi8ES2_EELb0ELb0ELb0ELb0ELb0EEv16Flash_fwd_params
    0 bytes stack frame, 0 bytes spill stores, 0 bytes spill loads
ptxas info    : Used 241 registers, 584 bytes cmem[0]
ptxas info    : Compiling entry function '_Z16flash_fwd_kernelI23Flash_fwd_kernel_traitsILi256ELi128ELi64ELi8ELb0ELb0EN7cutlass10bfloat16_tE19Flash_kernel_traitsILi256ELi128ELi64ELi8ES2_EELb0ELb0ELb0ELb1ELb0EEv16Flash_fwd_params' for 'sm_80'
ptxas info    : Function properties for _Z16flash_fwd_kernelI23Flash_fwd_kernel_traitsILi256ELi128ELi64ELi8ELb0ELb0EN7cutlass10bfloat16_tE19Flash_kernel_traitsILi256ELi128ELi64ELi8ES2_EELb0ELb0ELb0ELb1ELb0EEv16Flash_fwd_params
    0 bytes stack frame, 0 bytes spill stores, 0 bytes spill loads
ptxas info    : Used 237 registers, 584 bytes cmem[0]
ptxas info    : Compiling entry function '_Z16flash_fwd_kernelI23Flash_fwd_kernel_traitsILi256ELi128ELi64ELi8ELb0ELb0EN7cutlass10bfloat16_tE19Flash_kernel_traitsILi256ELi128ELi64ELi8ES2_EELb0ELb0ELb1ELb0ELb0EEv16Flash_fwd_params' for 'sm_80'
ptxas info    : Function properties for _Z16flash_fwd_kernelI23Flash_fwd_kernel_traitsILi256ELi128ELi64ELi8ELb0ELb0EN7cutlass10bfloat16_tE19Flash_kernel_traitsILi256ELi128ELi64ELi8ES2_EELb0ELb0ELb1ELb0ELb0EEv16Flash_fwd_params
    0 bytes stack frame, 0 bytes spill stores, 0 bytes spill loads
ptxas info    : Used 240 registers, 584 bytes cmem[0]
ptxas info    : Compiling entry function '_Z16flash_fwd_kernelI23Flash_fwd_kernel_traitsILi256ELi128ELi64ELi8ELb0ELb0EN7cutlass10bfloat16_tE19Flash_kernel_traitsILi256ELi128ELi64ELi8ES2_EELb0ELb0ELb1ELb1ELb0EEv16Flash_fwd_params' for 'sm_80'
ptxas info    : Function properties for _Z16flash_fwd_kernelI23Flash_fwd_kernel_traitsILi256ELi128ELi64ELi8ELb0ELb0EN7cutlass10bfloat16_tE19Flash_kernel_traitsILi256ELi128ELi64ELi8ES2_EELb0ELb0ELb1ELb1ELb0EEv16Flash_fwd_params
    0 bytes stack frame, 0 bytes spill stores, 0 bytes spill loads
ptxas info    : Used 237 registers, 584 bytes cmem[0]
ptxas info    : Compiling entry function '_Z16flash_fwd_kernelI23Flash_fwd_kernel_traitsILi256ELi64ELi64ELi4ELb0ELb0EN7cutlass10bfloat16_tE19Flash_kernel_traitsILi256ELi64ELi64ELi4ES2_EELb0ELb1ELb0ELb0ELb0EEv16Flash_fwd_params' for 'sm_80'
ptxas info    : Function properties for _Z16flash_fwd_kernelI23Flash_fwd_kernel_traitsILi256ELi64ELi64ELi4ELb0ELb0EN7cutlass10bfloat16_tE19Flash_kernel_traitsILi256ELi64ELi64ELi4ES2_EELb0ELb1ELb0ELb0ELb0EEv16Flash_fwd_params
    0 bytes stack frame, 0 bytes spill stores, 0 bytes spill loads
ptxas info    : Used 241 registers, 584 bytes cmem[0]
ptxas info    : Compiling entry function '_Z16flash_fwd_kernelI23Flash_fwd_kernel_traitsILi256ELi64ELi64ELi4ELb0ELb0EN7cutlass10bfloat16_tE19Flash_kernel_traitsILi256ELi64ELi64ELi4ES2_EELb0ELb1ELb0ELb1ELb0EEv16Flash_fwd_params' for 'sm_80'
ptxas info    : Function properties for _Z16flash_fwd_kernelI23Flash_fwd_kernel_traitsILi256ELi64ELi64ELi4ELb0ELb0EN7cutlass10bfloat16_tE19Flash_kernel_traitsILi256ELi64ELi64ELi4ES2_EELb0ELb1ELb0ELb1ELb0EEv16Flash_fwd_params
    0 bytes stack frame, 0 bytes spill stores, 0 bytes spill loads
ptxas info    : Used 238 registers, 584 bytes cmem[0]
ptxas info    : Compiling entry function '_Z16flash_fwd_kernelI23Flash_fwd_kernel_traitsILi256ELi64ELi64ELi4ELb0ELb0EN7cutlass10bfloat16_tE19Flash_kernel_traitsILi256ELi64ELi64ELi4ES2_EELb0ELb1ELb1ELb0ELb0EEv16Flash_fwd_params' for 'sm_80'
ptxas info    : Function properties for _Z16flash_fwd_kernelI23Flash_fwd_kernel_traitsILi256ELi64ELi64ELi4ELb0ELb0EN7cutlass10bfloat16_tE19Flash_kernel_traitsILi256ELi64ELi64ELi4ES2_EELb0ELb1ELb1ELb0ELb0EEv16Flash_fwd_params
    0 bytes stack frame, 0 bytes spill stores, 0 bytes spill loads
ptxas info    : Used 245 registers, 584 bytes cmem[0]
ptxas info    : Compiling entry function '_Z16flash_fwd_kernelI23Flash_fwd_kernel_traitsILi256ELi64ELi64ELi4ELb0ELb0EN7cutlass10bfloat16_tE19Flash_kernel_traitsILi256ELi64ELi64ELi4ES2_EELb0ELb1ELb1ELb1ELb0EEv16Flash_fwd_params' for 'sm_80'
ptxas info    : Function properties for _Z16flash_fwd_kernelI23Flash_fwd_kernel_traitsILi256ELi64ELi64ELi4ELb0ELb0EN7cutlass10bfloat16_tE19Flash_kernel_traitsILi256ELi64ELi64ELi4ES2_EELb0ELb1ELb1ELb1ELb0EEv16Flash_fwd_params
    0 bytes stack frame, 0 bytes spill stores, 0 bytes spill loads
ptxas info    : Used 241 registers, 584 bytes cmem[0]
ptxas info    : Compiling entry function '_Z16flash_fwd_kernelI23Flash_fwd_kernel_traitsILi256ELi128ELi64ELi8ELb0ELb0EN7cutlass10bfloat16_tE19Flash_kernel_traitsILi256ELi128ELi64ELi8ES2_EELb0ELb1ELb0ELb0ELb0EEv16Flash_fwd_params' for 'sm_80'
ptxas info    : Function properties for _Z16flash_fwd_kernelI23Flash_fwd_kernel_traitsILi256ELi128ELi64ELi8ELb0ELb0EN7cutlass10bfloat16_tE19Flash_kernel_traitsILi256ELi128ELi64ELi8ES2_EELb0ELb1ELb0ELb0ELb0EEv16Flash_fwd_params
    0 bytes stack frame, 0 bytes spill stores, 0 bytes spill loads
ptxas info    : Used 248 registers, 584 bytes cmem[0]
ptxas info    : Compiling entry function '_Z16flash_fwd_kernelI23Flash_fwd_kernel_traitsILi256ELi128ELi64ELi8ELb0ELb0EN7cutlass10bfloat16_tE19Flash_kernel_traitsILi256ELi128ELi64ELi8ES2_EELb0ELb1ELb0ELb1ELb0EEv16Flash_fwd_params' for 'sm_80'
ptxas info    : Function properties for _Z16flash_fwd_kernelI23Flash_fwd_kernel_traitsILi256ELi128ELi64ELi8ELb0ELb0EN7cutlass10bfloat16_tE19Flash_kernel_traitsILi256ELi128ELi64ELi8ES2_EELb0ELb1ELb0ELb1ELb0EEv16Flash_fwd_params
    0 bytes stack frame, 0 bytes spill stores, 0 bytes spill loads
ptxas info    : Used 245 registers, 584 bytes cmem[0]
ptxas info    : Compiling entry function '_Z16flash_fwd_kernelI23Flash_fwd_kernel_traitsILi256ELi128ELi64ELi8ELb0ELb0EN7cutlass10bfloat16_tE19Flash_kernel_traitsILi256ELi128ELi64ELi8ES2_EELb0ELb1ELb1ELb0ELb0EEv16Flash_fwd_params' for 'sm_80'
ptxas info    : Function properties for _Z16flash_fwd_kernelI23Flash_fwd_kernel_traitsILi256ELi128ELi64ELi8ELb0ELb0EN7cutlass10bfloat16_tE19Flash_kernel_traitsILi256ELi128ELi64ELi8ES2_EELb0ELb1ELb1ELb0ELb0EEv16Flash_fwd_params
    0 bytes stack frame, 0 bytes spill stores, 0 bytes spill loads
ptxas info    : Used 248 registers, 584 bytes cmem[0]
ptxas info    : Compiling entry function '_Z16flash_fwd_kernelI23Flash_fwd_kernel_traitsILi256ELi128ELi64ELi8ELb0ELb0EN7cutlass10bfloat16_tE19Flash_kernel_traitsILi256ELi128ELi64ELi8ES2_EELb0ELb1ELb1ELb1ELb0EEv16Flash_fwd_params' for 'sm_80'
ptxas info    : Function properties for _Z16flash_fwd_kernelI23Flash_fwd_kernel_traitsILi256ELi128ELi64ELi8ELb0ELb0EN7cutlass10bfloat16_tE19Flash_kernel_traitsILi256ELi128ELi64ELi8ES2_EELb0ELb1ELb1ELb1ELb0EEv16Flash_fwd_params
    0 bytes stack frame, 0 bytes spill stores, 0 bytes spill loads
ptxas info    : Used 250 registers, 584 bytes cmem[0]
ptxas info    : Compiling entry function '_Z16flash_fwd_kernelI23Flash_fwd_kernel_traitsILi256ELi64ELi64ELi4ELb0ELb0EN7cutlass10bfloat16_tE19Flash_kernel_traitsILi256ELi64ELi64ELi4ES2_EELb1ELb0ELb0ELb0ELb0EEv16Flash_fwd_params' for 'sm_80'
ptxas info    : Function properties for _Z16flash_fwd_kernelI23Flash_fwd_kernel_traitsILi256ELi64ELi64ELi4ELb0ELb0EN7cutlass10bfloat16_tE19Flash_kernel_traitsILi256ELi64ELi64ELi4ES2_EELb1ELb0ELb0ELb0ELb0EEv16Flash_fwd_params
    0 bytes stack frame, 0 bytes spill stores, 0 bytes spill loads
ptxas info    : Used 246 registers, 584 bytes cmem[0]
ptxas info    : Compiling entry function '_Z16flash_fwd_kernelI23Flash_fwd_kernel_traitsILi256ELi64ELi64ELi4ELb0ELb0EN7cutlass10bfloat16_tE19Flash_kernel_traitsILi256ELi64ELi64ELi4ES2_EELb1ELb0ELb0ELb0ELb1EEv16Flash_fwd_params' for 'sm_80'
ptxas info    : Function properties for _Z16flash_fwd_kernelI23Flash_fwd_kernel_traitsILi256ELi64ELi64ELi4ELb0ELb0EN7cutlass10bfloat16_tE19Flash_kernel_traitsILi256ELi64ELi64ELi4ES2_EELb1ELb0ELb0ELb0ELb1EEv16Flash_fwd_params
    24 bytes stack frame, 28 bytes spill stores, 28 bytes spill loads
ptxas info    : Used 255 registers, 584 bytes cmem[0]
ptxas info    : Compiling entry function '_Z16flash_fwd_kernelI23Flash_fwd_kernel_traitsILi256ELi64ELi64ELi4ELb0ELb0EN7cutlass10bfloat16_tE19Flash_kernel_traitsILi256ELi64ELi64ELi4ES2_EELb1ELb0ELb0ELb1ELb0EEv16Flash_fwd_params' for 'sm_80'
ptxas info    : Function properties for _Z16flash_fwd_kernelI23Flash_fwd_kernel_traitsILi256ELi64ELi64ELi4ELb0ELb0EN7cutlass10bfloat16_tE19Flash_kernel_traitsILi256ELi64ELi64ELi4ES2_EELb1ELb0ELb0ELb1ELb0EEv16Flash_fwd_params
    0 bytes stack frame, 0 bytes spill stores, 0 bytes spill loads
ptxas info    : Used 242 registers, 584 bytes cmem[0]
ptxas info    : Compiling entry function '_Z16flash_fwd_kernelI23Flash_fwd_kernel_traitsILi256ELi64ELi64ELi4ELb0ELb0EN7cutlass10bfloat16_tE19Flash_kernel_traitsILi256ELi64ELi64ELi4ES2_EELb1ELb0ELb0ELb1ELb1EEv16Flash_fwd_params' for 'sm_80'
ptxas info    : Function properties for _Z16flash_fwd_kernelI23Flash_fwd_kernel_traitsILi256ELi64ELi64ELi4ELb0ELb0EN7cutlass10bfloat16_tE19Flash_kernel_traitsILi256ELi64ELi64ELi4ES2_EELb1ELb0ELb0ELb1ELb1EEv16Flash_fwd_params
    16 bytes stack frame, 12 bytes spill stores, 12 bytes spill loads
ptxas info    : Used 255 registers, 584 bytes cmem[0]
ptxas info    : Compiling entry function '_Z16flash_fwd_kernelI23Flash_fwd_kernel_traitsILi256ELi64ELi64ELi4ELb0ELb0EN7cutlass10bfloat16_tE19Flash_kernel_traitsILi256ELi64ELi64ELi4ES2_EELb1ELb0ELb1ELb0ELb0EEv16Flash_fwd_params' for 'sm_80'
ptxas info    : Function properties for _Z16flash_fwd_kernelI23Flash_fwd_kernel_traitsILi256ELi64ELi64ELi4ELb0ELb0EN7cutlass10bfloat16_tE19Flash_kernel_traitsILi256ELi64ELi64ELi4ES2_EELb1ELb0ELb1ELb0ELb0EEv16Flash_fwd_params
    0 bytes stack frame, 0 bytes spill stores, 0 bytes spill loads
ptxas info    : Used 246 registers, 584 bytes cmem[0]
ptxas info    : Compiling entry function '_Z16flash_fwd_kernelI23Flash_fwd_kernel_traitsILi256ELi64ELi64ELi4ELb0ELb0EN7cutlass10bfloat16_tE19Flash_kernel_traitsILi256ELi64ELi64ELi4ES2_EELb1ELb0ELb1ELb0ELb1EEv16Flash_fwd_params' for 'sm_80'
ptxas info    : Function properties for _Z16flash_fwd_kernelI23Flash_fwd_kernel_traitsILi256ELi64ELi64ELi4ELb0ELb0EN7cutlass10bfloat16_tE19Flash_kernel_traitsILi256ELi64ELi64ELi4ES2_EELb1ELb0ELb1ELb0ELb1EEv16Flash_fwd_params
    16 bytes stack frame, 20 bytes spill stores, 20 bytes spill loads
ptxas info    : Used 255 registers, 584 bytes cmem[0]
ptxas info    : Compiling entry function '_Z16flash_fwd_kernelI23Flash_fwd_kernel_traitsILi256ELi64ELi64ELi4ELb0ELb0EN7cutlass10bfloat16_tE19Flash_kernel_traitsILi256ELi64ELi64ELi4ES2_EELb1ELb0ELb1ELb1ELb0EEv16Flash_fwd_params' for 'sm_80'
ptxas info    : Function properties for _Z16flash_fwd_kernelI23Flash_fwd_kernel_traitsILi256ELi64ELi64ELi4ELb0ELb0EN7cutlass10bfloat16_tE19Flash_kernel_traitsILi256ELi64ELi64ELi4ES2_EELb1ELb0ELb1ELb1ELb0EEv16Flash_fwd_params
    0 bytes stack frame, 0 bytes spill stores, 0 bytes spill loads
ptxas info    : Used 242 registers, 584 bytes cmem[0]
ptxas info    : Compiling entry function '_Z16flash_fwd_kernelI23Flash_fwd_kernel_traitsILi256ELi64ELi64ELi4ELb0ELb0EN7cutlass10bfloat16_tE19Flash_kernel_traitsILi256ELi64ELi64ELi4ES2_EELb1ELb0ELb1ELb1ELb1EEv16Flash_fwd_params' for 'sm_80'
ptxas info    : Function properties for _Z16flash_fwd_kernelI23Flash_fwd_kernel_traitsILi256ELi64ELi64ELi4ELb0ELb0EN7cutlass10bfloat16_tE19Flash_kernel_traitsILi256ELi64ELi64ELi4ES2_EELb1ELb0ELb1ELb1ELb1EEv16Flash_fwd_params
    0 bytes stack frame, 0 bytes spill stores, 0 bytes spill loads
ptxas info    : Used 254 registers, 584 bytes cmem[0]
ptxas info    : Compiling entry function '_Z16flash_fwd_kernelI23Flash_fwd_kernel_traitsILi256ELi128ELi64ELi8ELb0ELb0EN7cutlass10bfloat16_tE19Flash_kernel_traitsILi256ELi128ELi64ELi8ES2_EELb1ELb0ELb0ELb0ELb0EEv16Flash_fwd_params' for 'sm_80'
ptxas info    : Function properties for _Z16flash_fwd_kernelI23Flash_fwd_kernel_traitsILi256ELi128ELi64ELi8ELb0ELb0EN7cutlass10bfloat16_tE19Flash_kernel_traitsILi256ELi128ELi64ELi8ES2_EELb1ELb0ELb0ELb0ELb0EEv16Flash_fwd_params
    0 bytes stack frame, 0 bytes spill stores, 0 bytes spill loads
ptxas info    : Used 246 registers, 584 bytes cmem[0]
ptxas info    : Compiling entry function '_Z16flash_fwd_kernelI23Flash_fwd_kernel_traitsILi256ELi128ELi64ELi8ELb0ELb0EN7cutlass10bfloat16_tE19Flash_kernel_traitsILi256ELi128ELi64ELi8ES2_EELb1ELb0ELb0ELb0ELb1EEv16Flash_fwd_params' for 'sm_80'
ptxas info    : Function properties for _Z16flash_fwd_kernelI23Flash_fwd_kernel_traitsILi256ELi128ELi64ELi8ELb0ELb0EN7cutlass10bfloat16_tE19Flash_kernel_traitsILi256ELi128ELi64ELi8ES2_EELb1ELb0ELb0ELb0ELb1EEv16Flash_fwd_params
    8 bytes stack frame, 8 bytes spill stores, 8 bytes spill loads
ptxas info    : Used 255 registers, 584 bytes cmem[0]
ptxas info    : Compiling entry function '_Z16flash_fwd_kernelI23Flash_fwd_kernel_traitsILi256ELi128ELi64ELi8ELb0ELb0EN7cutlass10bfloat16_tE19Flash_kernel_traitsILi256ELi128ELi64ELi8ES2_EELb1ELb0ELb0ELb1ELb0EEv16Flash_fwd_params' for 'sm_80'
ptxas info    : Function properties for _Z16flash_fwd_kernelI23Flash_fwd_kernel_traitsILi256ELi128ELi64ELi8ELb0ELb0EN7cutlass10bfloat16_tE19Flash_kernel_traitsILi256ELi128ELi64ELi8ES2_EELb1ELb0ELb0ELb1ELb0EEv16Flash_fwd_params
    0 bytes stack frame, 0 bytes spill stores, 0 bytes spill loads
ptxas info    : Used 242 registers, 584 bytes cmem[0]
ptxas info    : Compiling entry function '_Z16flash_fwd_kernelI23Flash_fwd_kernel_traitsILi256ELi128ELi64ELi8ELb0ELb0EN7cutlass10bfloat16_tE19Flash_kernel_traitsILi256ELi128ELi64ELi8ES2_EELb1ELb0ELb0ELb1ELb1EEv16Flash_fwd_params' for 'sm_80'
ptxas info    : Function properties for _Z16flash_fwd_kernelI23Flash_fwd_kernel_traitsILi256ELi128ELi64ELi8ELb0ELb0EN7cutlass10bfloat16_tE19Flash_kernel_traitsILi256ELi128ELi64ELi8ES2_EELb1ELb0ELb0ELb1ELb1EEv16Flash_fwd_params
    8 bytes stack frame, 4 bytes spill stores, 4 bytes spill loads
ptxas info    : Used 255 registers, 584 bytes cmem[0]
ptxas info    : Compiling entry function '_Z16flash_fwd_kernelI23Flash_fwd_kernel_traitsILi256ELi128ELi64ELi8ELb0ELb0EN7cutlass10bfloat16_tE19Flash_kernel_traitsILi256ELi128ELi64ELi8ES2_EELb1ELb0ELb1ELb0ELb0EEv16Flash_fwd_params' for 'sm_80'
ptxas info    : Function properties for _Z16flash_fwd_kernelI23Flash_fwd_kernel_traitsILi256ELi128ELi64ELi8ELb0ELb0EN7cutlass10bfloat16_tE19Flash_kernel_traitsILi256ELi128ELi64ELi8ES2_EELb1ELb0ELb1ELb0ELb0EEv16Flash_fwd_params
    0 bytes stack frame, 0 bytes spill stores, 0 bytes spill loads
ptxas info    : Used 244 registers, 584 bytes cmem[0]
ptxas info    : Compiling entry function '_Z16flash_fwd_kernelI23Flash_fwd_kernel_traitsILi256ELi128ELi64ELi8ELb0ELb0EN7cutlass10bfloat16_tE19Flash_kernel_traitsILi256ELi128ELi64ELi8ES2_EELb1ELb0ELb1ELb0ELb1EEv16Flash_fwd_params' for 'sm_80'
ptxas info    : Function properties for _Z16flash_fwd_kernelI23Flash_fwd_kernel_traitsILi256ELi128ELi64ELi8ELb0ELb0EN7cutlass10bfloat16_tE19Flash_kernel_traitsILi256ELi128ELi64ELi8ES2_EELb1ELb0ELb1ELb0ELb1EEv16Flash_fwd_params
    0 bytes stack frame, 0 bytes spill stores, 0 bytes spill loads
ptxas info    : Used 255 registers, 584 bytes cmem[0]
ptxas info    : Compiling entry function '_Z16flash_fwd_kernelI23Flash_fwd_kernel_traitsILi256ELi128ELi64ELi8ELb0ELb0EN7cutlass10bfloat16_tE19Flash_kernel_traitsILi256ELi128ELi64ELi8ES2_EELb1ELb0ELb1ELb1ELb0EEv16Flash_fwd_params' for 'sm_80'
ptxas info    : Function properties for _Z16flash_fwd_kernelI23Flash_fwd_kernel_traitsILi256ELi128ELi64ELi8ELb0ELb0EN7cutlass10bfloat16_tE19Flash_kernel_traitsILi256ELi128ELi64ELi8ES2_EELb1ELb0ELb1ELb1ELb0EEv16Flash_fwd_params
    0 bytes stack frame, 0 bytes spill stores, 0 bytes spill loads
ptxas info    : Used 242 registers, 584 bytes cmem[0]
ptxas info    : Compiling entry function '_Z16flash_fwd_kernelI23Flash_fwd_kernel_traitsILi256ELi128ELi64ELi8ELb0ELb0EN7cutlass10bfloat16_tE19Flash_kernel_traitsILi256ELi128ELi64ELi8ES2_EELb1ELb0ELb1ELb1ELb1EEv16Flash_fwd_params' for 'sm_80'
ptxas info    : Function properties for _Z16flash_fwd_kernelI23Flash_fwd_kernel_traitsILi256ELi128ELi64ELi8ELb0ELb0EN7cutlass10bfloat16_tE19Flash_kernel_traitsILi256ELi128ELi64ELi8ES2_EELb1ELb0ELb1ELb1ELb1EEv16Flash_fwd_params
    0 bytes stack frame, 0 bytes spill stores, 0 bytes spill loads
ptxas info    : Used 254 registers, 584 bytes cmem[0]
ptxas info    : Compiling entry function '_Z16flash_fwd_kernelI23Flash_fwd_kernel_traitsILi256ELi64ELi64ELi4ELb0ELb0EN7cutlass10bfloat16_tE19Flash_kernel_traitsILi256ELi64ELi64ELi4ES2_EELb1ELb1ELb0ELb0ELb0EEv16Flash_fwd_params' for 'sm_80'
ptxas info    : Function properties for _Z16flash_fwd_kernelI23Flash_fwd_kernel_traitsILi256ELi64ELi64ELi4ELb0ELb0EN7cutlass10bfloat16_tE19Flash_kernel_traitsILi256ELi64ELi64ELi4ES2_EELb1ELb1ELb0ELb0ELb0EEv16Flash_fwd_params
    0 bytes stack frame, 0 bytes spill stores, 0 bytes spill loads
ptxas info    : Used 246 registers, 584 bytes cmem[0]
ptxas info    : Compiling entry function '_Z16flash_fwd_kernelI23Flash_fwd_kernel_traitsILi256ELi64ELi64ELi4ELb0ELb0EN7cutlass10bfloat16_tE19Flash_kernel_traitsILi256ELi64ELi64ELi4ES2_EELb1ELb1ELb0ELb0ELb1EEv16Flash_fwd_params' for 'sm_80'
ptxas info    : Function properties for _Z16flash_fwd_kernelI23Flash_fwd_kernel_traitsILi256ELi64ELi64ELi4ELb0ELb0EN7cutlass10bfloat16_tE19Flash_kernel_traitsILi256ELi64ELi64ELi4ES2_EELb1ELb1ELb0ELb0ELb1EEv16Flash_fwd_params
    16 bytes stack frame, 16 bytes spill stores, 16 bytes spill loads
ptxas info    : Used 254 registers, 584 bytes cmem[0]
ptxas info    : Compiling entry function '_Z16flash_fwd_kernelI23Flash_fwd_kernel_traitsILi256ELi64ELi64ELi4ELb0ELb0EN7cutlass10bfloat16_tE19Flash_kernel_traitsILi256ELi64ELi64ELi4ES2_EELb1ELb1ELb0ELb1ELb0EEv16Flash_fwd_params' for 'sm_80'
ptxas info    : Function properties for _Z16flash_fwd_kernelI23Flash_fwd_kernel_traitsILi256ELi64ELi64ELi4ELb0ELb0EN7cutlass10bfloat16_tE19Flash_kernel_traitsILi256ELi64ELi64ELi4ES2_EELb1ELb1ELb0ELb1ELb0EEv16Flash_fwd_params
    0 bytes stack frame, 0 bytes spill stores, 0 bytes spill loads
ptxas info    : Used 248 registers, 584 bytes cmem[0]
ptxas info    : Compiling entry function '_Z16flash_fwd_kernelI23Flash_fwd_kernel_traitsILi256ELi64ELi64ELi4ELb0ELb0EN7cutlass10bfloat16_tE19Flash_kernel_traitsILi256ELi64ELi64ELi4ES2_EELb1ELb1ELb0ELb1ELb1EEv16Flash_fwd_params' for 'sm_80'
ptxas info    : Function properties for _Z16flash_fwd_kernelI23Flash_fwd_kernel_traitsILi256ELi64ELi64ELi4ELb0ELb0EN7cutlass10bfloat16_tE19Flash_kernel_traitsILi256ELi64ELi64ELi4ES2_EELb1ELb1ELb0ELb1ELb1EEv16Flash_fwd_params
    16 bytes stack frame, 12 bytes spill stores, 12 bytes spill loads
ptxas info    : Used 255 registers, 584 bytes cmem[0]
ptxas info    : Compiling entry function '_Z16flash_fwd_kernelI23Flash_fwd_kernel_traitsILi256ELi64ELi64ELi4ELb0ELb0EN7cutlass10bfloat16_tE19Flash_kernel_traitsILi256ELi64ELi64ELi4ES2_EELb1ELb1ELb1ELb0ELb0EEv16Flash_fwd_params' for 'sm_80'
ptxas info    : Function properties for _Z16flash_fwd_kernelI23Flash_fwd_kernel_traitsILi256ELi64ELi64ELi4ELb0ELb0EN7cutlass10bfloat16_tE19Flash_kernel_traitsILi256ELi64ELi64ELi4ES2_EELb1ELb1ELb1ELb0ELb0EEv16Flash_fwd_params
    0 bytes stack frame, 0 bytes spill stores, 0 bytes spill loads
ptxas info    : Used 254 registers, 584 bytes cmem[0]
ptxas info    : Compiling entry function '_Z16flash_fwd_kernelI23Flash_fwd_kernel_traitsILi256ELi64ELi64ELi4ELb0ELb0EN7cutlass10bfloat16_tE19Flash_kernel_traitsILi256ELi64ELi64ELi4ES2_EELb1ELb1ELb1ELb0ELb1EEv16Flash_fwd_params' for 'sm_80'
ptxas info    : Function properties for _Z16flash_fwd_kernelI23Flash_fwd_kernel_traitsILi256ELi64ELi64ELi4ELb0ELb0EN7cutlass10bfloat16_tE19Flash_kernel_traitsILi256ELi64ELi64ELi4ES2_EELb1ELb1ELb1ELb0ELb1EEv16Flash_fwd_params
    16 bytes stack frame, 16 bytes spill stores, 16 bytes spill loads
ptxas info    : Used 255 registers, 584 bytes cmem[0]
ptxas info    : Compiling entry function '_Z16flash_fwd_kernelI23Flash_fwd_kernel_traitsILi256ELi64ELi64ELi4ELb0ELb0EN7cutlass10bfloat16_tE19Flash_kernel_traitsILi256ELi64ELi64ELi4ES2_EELb1ELb1ELb1ELb1ELb0EEv16Flash_fwd_params' for 'sm_80'
ptxas info    : Function properties for _Z16flash_fwd_kernelI23Flash_fwd_kernel_traitsILi256ELi64ELi64ELi4ELb0ELb0EN7cutlass10bfloat16_tE19Flash_kernel_traitsILi256ELi64ELi64ELi4ES2_EELb1ELb1ELb1ELb1ELb0EEv16Flash_fwd_params
    0 bytes stack frame, 0 bytes spill stores, 0 bytes spill loads
ptxas info    : Used 254 registers, 584 bytes cmem[0]
ptxas info    : Compiling entry function '_Z16flash_fwd_kernelI23Flash_fwd_kernel_traitsILi256ELi64ELi64ELi4ELb0ELb0EN7cutlass10bfloat16_tE19Flash_kernel_traitsILi256ELi64ELi64ELi4ES2_EELb1ELb1ELb1ELb1ELb1EEv16Flash_fwd_params' for 'sm_80'
ptxas info    : Function properties for _Z16flash_fwd_kernelI23Flash_fwd_kernel_traitsILi256ELi64ELi64ELi4ELb0ELb0EN7cutlass10bfloat16_tE19Flash_kernel_traitsILi256ELi64ELi64ELi4ES2_EELb1ELb1ELb1ELb1ELb1EEv16Flash_fwd_params
    8 bytes stack frame, 4 bytes spill stores, 4 bytes spill loads
ptxas info    : Used 255 registers, 584 bytes cmem[0]
ptxas info    : Compiling entry function '_Z16flash_fwd_kernelI23Flash_fwd_kernel_traitsILi256ELi128ELi64ELi8ELb0ELb0EN7cutlass10bfloat16_tE19Flash_kernel_traitsILi256ELi128ELi64ELi8ES2_EELb1ELb1ELb0ELb0ELb0EEv16Flash_fwd_params' for 'sm_80'
ptxas info    : Function properties for _Z16flash_fwd_kernelI23Flash_fwd_kernel_traitsILi256ELi128ELi64ELi8ELb0ELb0EN7cutlass10bfloat16_tE19Flash_kernel_traitsILi256ELi128ELi64ELi8ES2_EELb1ELb1ELb0ELb0ELb0EEv16Flash_fwd_params
    0 bytes stack frame, 0 bytes spill stores, 0 bytes spill loads
ptxas info    : Used 255 registers, 584 bytes cmem[0]
ptxas info    : Compiling entry function '_Z16flash_fwd_kernelI23Flash_fwd_kernel_traitsILi256ELi128ELi64ELi8ELb0ELb0EN7cutlass10bfloat16_tE19Flash_kernel_traitsILi256ELi128ELi64ELi8ES2_EELb1ELb1ELb0ELb0ELb1EEv16Flash_fwd_params' for 'sm_80'
ptxas info    : Function properties for _Z16flash_fwd_kernelI23Flash_fwd_kernel_traitsILi256ELi128ELi64ELi8ELb0ELb0EN7cutlass10bfloat16_tE19Flash_kernel_traitsILi256ELi128ELi64ELi8ES2_EELb1ELb1ELb0ELb0ELb1EEv16Flash_fwd_params
    144 bytes stack frame, 184 bytes spill stores, 188 bytes spill loads
ptxas info    : Used 255 registers, 584 bytes cmem[0]
ptxas info    : Compiling entry function '_Z16flash_fwd_kernelI23Flash_fwd_kernel_traitsILi256ELi128ELi64ELi8ELb0ELb0EN7cutlass10bfloat16_tE19Flash_kernel_traitsILi256ELi128ELi64ELi8ES2_EELb1ELb1ELb0ELb1ELb0EEv16Flash_fwd_params' for 'sm_80'
ptxas info    : Function properties for _Z16flash_fwd_kernelI23Flash_fwd_kernel_traitsILi256ELi128ELi64ELi8ELb0ELb0EN7cutlass10bfloat16_tE19Flash_kernel_traitsILi256ELi128ELi64ELi8ES2_EELb1ELb1ELb0ELb1ELb0EEv16Flash_fwd_params
    8 bytes stack frame, 4 bytes spill stores, 4 bytes spill loads
ptxas info    : Used 255 registers, 584 bytes cmem[0]
ptxas info    : Compiling entry function '_Z16flash_fwd_kernelI23Flash_fwd_kernel_traitsILi256ELi128ELi64ELi8ELb0ELb0EN7cutlass10bfloat16_tE19Flash_kernel_traitsILi256ELi128ELi64ELi8ES2_EELb1ELb1ELb0ELb1ELb1EEv16Flash_fwd_params' for 'sm_80'
ptxas info    : Function properties for _Z16flash_fwd_kernelI23Flash_fwd_kernel_traitsILi256ELi128ELi64ELi8ELb0ELb0EN7cutlass10bfloat16_tE19Flash_kernel_traitsILi256ELi128ELi64ELi8ES2_EELb1ELb1ELb0ELb1ELb1EEv16Flash_fwd_params
    8 bytes stack frame, 8 bytes spill stores, 8 bytes spill loads
ptxas info    : Used 255 registers, 584 bytes cmem[0]
ptxas info    : Compiling entry function '_Z16flash_fwd_kernelI23Flash_fwd_kernel_traitsILi256ELi128ELi64ELi8ELb0ELb0EN7cutlass10bfloat16_tE19Flash_kernel_traitsILi256ELi128ELi64ELi8ES2_EELb1ELb1ELb1ELb0ELb0EEv16Flash_fwd_params' for 'sm_80'
ptxas info    : Function properties for _Z16flash_fwd_kernelI23Flash_fwd_kernel_traitsILi256ELi128ELi64ELi8ELb0ELb0EN7cutlass10bfloat16_tE19Flash_kernel_traitsILi256ELi128ELi64ELi8ES2_EELb1ELb1ELb1ELb0ELb0EEv16Flash_fwd_params
    0 bytes stack frame, 0 bytes spill stores, 0 bytes spill loads
ptxas info    : Used 253 registers, 584 bytes cmem[0]
ptxas info    : Compiling entry function '_Z16flash_fwd_kernelI23Flash_fwd_kernel_traitsILi256ELi128ELi64ELi8ELb0ELb0EN7cutlass10bfloat16_tE19Flash_kernel_traitsILi256ELi128ELi64ELi8ES2_EELb1ELb1ELb1ELb0ELb1EEv16Flash_fwd_params' for 'sm_80'
ptxas info    : Function properties for _Z16flash_fwd_kernelI23Flash_fwd_kernel_traitsILi256ELi128ELi64ELi8ELb0ELb0EN7cutlass10bfloat16_tE19Flash_kernel_traitsILi256ELi128ELi64ELi8ES2_EELb1ELb1ELb1ELb0ELb1EEv16Flash_fwd_params
    48 bytes stack frame, 72 bytes spill stores, 72 bytes spill loads
ptxas info    : Used 255 registers, 584 bytes cmem[0]
ptxas info    : Compiling entry function '_Z16flash_fwd_kernelI23Flash_fwd_kernel_traitsILi256ELi128ELi64ELi8ELb0ELb0EN7cutlass10bfloat16_tE19Flash_kernel_traitsILi256ELi128ELi64ELi8ES2_EELb1ELb1ELb1ELb1ELb0EEv16Flash_fwd_params' for 'sm_80'
ptxas info    : Function properties for _Z16flash_fwd_kernelI23Flash_fwd_kernel_traitsILi256ELi128ELi64ELi8ELb0ELb0EN7cutlass10bfloat16_tE19Flash_kernel_traitsILi256ELi128ELi64ELi8ES2_EELb1ELb1ELb1ELb1ELb0EEv16Flash_fwd_params
    8 bytes stack frame, 4 bytes spill stores, 4 bytes spill loads
ptxas info    : Used 255 registers, 584 bytes cmem[0]
ptxas info    : Compiling entry function '_Z16flash_fwd_kernelI23Flash_fwd_kernel_traitsILi256ELi128ELi64ELi8ELb0ELb0EN7cutlass10bfloat16_tE19Flash_kernel_traitsILi256ELi128ELi64ELi8ES2_EELb1ELb1ELb1ELb1ELb1EEv16Flash_fwd_params' for 'sm_80'
ptxas info    : Function properties for _Z16flash_fwd_kernelI23Flash_fwd_kernel_traitsILi256ELi128ELi64ELi8ELb0ELb0EN7cutlass10bfloat16_tE19Flash_kernel_traitsILi256ELi128ELi64ELi8ES2_EELb1ELb1ELb1ELb1ELb1EEv16Flash_fwd_params
    24 bytes stack frame, 24 bytes spill stores, 20 bytes spill loads
ptxas info    : Used 255 registers, 584 bytes cmem[0]
/anaconda/envs/flashattention/lib/python3.9/site-packages/torch/include/c10/core/SymInt.h(84): warning: integer conversion resulted in a change of sign

[11/13] /usr/local/cuda-11.4/bin/nvcc  -I/home/v-feiychen/flash-attention/csrc/flash_attn -I/home/v-feiychen/flash-attention/csrc/flash_attn/src -I/home/v-feiychen/flash-attention/csrc/cutlass/include -I/anaconda/envs/flashattention/lib/python3.9/site-packages/torch/include -I/anaconda/envs/flashattention/lib/python3.9/site-packages/torch/include/torch/csrc/api/include -I/anaconda/envs/flashattention/lib/python3.9/site-packages/torch/include/TH -I/anaconda/envs/flashattention/lib/python3.9/site-packages/torch/include/THC -I/usr/local/cuda-11.4/include -I/anaconda/envs/flashattention/include/python3.9 -c -c /home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_hdim256_fp16_sm80.cu -o /home/v-feiychen/flash-attention/build/temp.linux-x86_64-cpython-39/csrc/flash_attn/src/flash_fwd_hdim256_fp16_sm80.o -D__CUDA_NO_HALF_OPERATORS__ -D__CUDA_NO_HALF_CONVERSIONS__ -D__CUDA_NO_BFLOAT16_CONVERSIONS__ -D__CUDA_NO_HALF2_OPERATORS__ --expt-relaxed-constexpr --compiler-options ''"'"'-fPIC'"'"'' -O3 -std=c++17 -U__CUDA_NO_HALF_OPERATORS__ -U__CUDA_NO_HALF_CONVERSIONS__ -U__CUDA_NO_HALF2_OPERATORS__ -U__CUDA_NO_BFLOAT16_CONVERSIONS__ --expt-relaxed-constexpr --expt-extended-lambda --use_fast_math --ptxas-options=-v -lineinfo -gencode arch=compute_80,code=sm_80 --threads 4 -DTORCH_API_INCLUDE_EXTENSION_H '-DPYBIND11_COMPILER_TYPE="_gcc"' '-DPYBIND11_STDLIB="_libstdcpp"' '-DPYBIND11_BUILD_ABI="_cxxabi1011"' -DTORCH_EXTENSION_NAME=flash_attn_2_cuda -D_GLIBCXX_USE_CXX11_ABI=0
/anaconda/envs/flashattention/lib/python3.9/site-packages/torch/include/c10/core/SymInt.h(84): warning: integer conversion resulted in a change of sign

/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_kernel.h(140): warning: variable "MMA_M" was declared but never referenced
          detected during:
            instantiation of "void flash::compute_attn_1rowblock<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &, int, int, int) [with Kernel_traits=Flash_fwd_kernel_traits<256, 128, 64, 8, false, false, cutlass::half_t, Flash_kernel_traits<256, 128, 64, 8, cutlass::half_t>>, Is_dropout=true, Is_causal=true, Is_even_N=true, Is_even_K=true, Return_softmax=true, Params=Flash_fwd_params]" 
(588): here
            instantiation of "void flash::compute_attn<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &) [with Kernel_traits=Flash_fwd_kernel_traits<256, 128, 64, 8, false, false, cutlass::half_t, Flash_kernel_traits<256, 128, 64, 8, cutlass::half_t>>, Is_dropout=true, Is_causal=true, Is_even_N=true, Is_even_K=true, Return_softmax=true, Params=Flash_fwd_params]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(15): here
            instantiation of "void flash_fwd_kernel<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax>(Flash_fwd_params) [with Kernel_traits=Flash_fwd_kernel_traits<256, 128, 64, 8, false, false, cutlass::half_t, Flash_kernel_traits<256, 128, 64, 8, cutlass::half_t>>, Is_dropout=true, Is_causal=true, Is_even_N=true, Is_even_K=true, Return_softmax=true]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(34): here
            instantiation of "void run_flash_fwd<Kernel_traits,Is_dropout,Is_causal>(Flash_fwd_params &, cudaStream_t) [with Kernel_traits=Flash_fwd_kernel_traits<256, 128, 64, 8, false, false, cutlass::half_t, Flash_kernel_traits<256, 128, 64, 8, cutlass::half_t>>, Is_dropout=true, Is_causal=true]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(236): here
            instantiation of "void run_mha_fwd_hdim256<T>(Flash_fwd_params &, cudaStream_t) [with T=cutlass::half_t]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_hdim256_fp16_sm80.cu(8): here

/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_kernel.h(275): warning: variable "tQrQ" was declared but never referenced
          detected during:
            instantiation of "void flash::compute_attn_1rowblock<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &, int, int, int) [with Kernel_traits=Flash_fwd_kernel_traits<256, 128, 64, 8, false, false, cutlass::half_t, Flash_kernel_traits<256, 128, 64, 8, cutlass::half_t>>, Is_dropout=true, Is_causal=true, Is_even_N=true, Is_even_K=true, Return_softmax=true, Params=Flash_fwd_params]" 
(588): here
            instantiation of "void flash::compute_attn<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &) [with Kernel_traits=Flash_fwd_kernel_traits<256, 128, 64, 8, false, false, cutlass::half_t, Flash_kernel_traits<256, 128, 64, 8, cutlass::half_t>>, Is_dropout=true, Is_causal=true, Is_even_N=true, Is_even_K=true, Return_softmax=true, Params=Flash_fwd_params]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(15): here
            instantiation of "void flash_fwd_kernel<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax>(Flash_fwd_params) [with Kernel_traits=Flash_fwd_kernel_traits<256, 128, 64, 8, false, false, cutlass::half_t, Flash_kernel_traits<256, 128, 64, 8, cutlass::half_t>>, Is_dropout=true, Is_causal=true, Is_even_N=true, Is_even_K=true, Return_softmax=true]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(34): here
            instantiation of "void run_flash_fwd<Kernel_traits,Is_dropout,Is_causal>(Flash_fwd_params &, cudaStream_t) [with Kernel_traits=Flash_fwd_kernel_traits<256, 128, 64, 8, false, false, cutlass::half_t, Flash_kernel_traits<256, 128, 64, 8, cutlass::half_t>>, Is_dropout=true, Is_causal=true]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(236): here
            instantiation of "void run_mha_fwd_hdim256<T>(Flash_fwd_params &, cudaStream_t) [with T=cutlass::half_t]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_hdim256_fp16_sm80.cu(8): here

/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_kernel.h(140): warning: variable "MMA_M" was declared but never referenced
          detected during:
            instantiation of "void flash::compute_attn_1rowblock<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &, int, int, int) [with Kernel_traits=Flash_fwd_kernel_traits<256, 128, 64, 8, false, false, cutlass::half_t, Flash_kernel_traits<256, 128, 64, 8, cutlass::half_t>>, Is_dropout=true, Is_causal=true, Is_even_N=true, Is_even_K=true, Return_softmax=false, Params=Flash_fwd_params]" 
(588): here
            instantiation of "void flash::compute_attn<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &) [with Kernel_traits=Flash_fwd_kernel_traits<256, 128, 64, 8, false, false, cutlass::half_t, Flash_kernel_traits<256, 128, 64, 8, cutlass::half_t>>, Is_dropout=true, Is_causal=true, Is_even_N=true, Is_even_K=true, Return_softmax=false, Params=Flash_fwd_params]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(15): here
            instantiation of "void flash_fwd_kernel<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax>(Flash_fwd_params) [with Kernel_traits=Flash_fwd_kernel_traits<256, 128, 64, 8, false, false, cutlass::half_t, Flash_kernel_traits<256, 128, 64, 8, cutlass::half_t>>, Is_dropout=true, Is_causal=true, Is_even_N=true, Is_even_K=true, Return_softmax=false]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(34): here
            instantiation of "void run_flash_fwd<Kernel_traits,Is_dropout,Is_causal>(Flash_fwd_params &, cudaStream_t) [with Kernel_traits=Flash_fwd_kernel_traits<256, 128, 64, 8, false, false, cutlass::half_t, Flash_kernel_traits<256, 128, 64, 8, cutlass::half_t>>, Is_dropout=true, Is_causal=true]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(236): here
            instantiation of "void run_mha_fwd_hdim256<T>(Flash_fwd_params &, cudaStream_t) [with T=cutlass::half_t]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_hdim256_fp16_sm80.cu(8): here

/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_kernel.h(275): warning: variable "tQrQ" was declared but never referenced
          detected during:
            instantiation of "void flash::compute_attn_1rowblock<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &, int, int, int) [with Kernel_traits=Flash_fwd_kernel_traits<256, 128, 64, 8, false, false, cutlass::half_t, Flash_kernel_traits<256, 128, 64, 8, cutlass::half_t>>, Is_dropout=true, Is_causal=true, Is_even_N=true, Is_even_K=true, Return_softmax=false, Params=Flash_fwd_params]" 
(588): here
            instantiation of "void flash::compute_attn<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &) [with Kernel_traits=Flash_fwd_kernel_traits<256, 128, 64, 8, false, false, cutlass::half_t, Flash_kernel_traits<256, 128, 64, 8, cutlass::half_t>>, Is_dropout=true, Is_causal=true, Is_even_N=true, Is_even_K=true, Return_softmax=false, Params=Flash_fwd_params]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(15): here
            instantiation of "void flash_fwd_kernel<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax>(Flash_fwd_params) [with Kernel_traits=Flash_fwd_kernel_traits<256, 128, 64, 8, false, false, cutlass::half_t, Flash_kernel_traits<256, 128, 64, 8, cutlass::half_t>>, Is_dropout=true, Is_causal=true, Is_even_N=true, Is_even_K=true, Return_softmax=false]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(34): here
            instantiation of "void run_flash_fwd<Kernel_traits,Is_dropout,Is_causal>(Flash_fwd_params &, cudaStream_t) [with Kernel_traits=Flash_fwd_kernel_traits<256, 128, 64, 8, false, false, cutlass::half_t, Flash_kernel_traits<256, 128, 64, 8, cutlass::half_t>>, Is_dropout=true, Is_causal=true]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(236): here
            instantiation of "void run_mha_fwd_hdim256<T>(Flash_fwd_params &, cudaStream_t) [with T=cutlass::half_t]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_hdim256_fp16_sm80.cu(8): here

/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_kernel.h(140): warning: variable "MMA_M" was declared but never referenced
          detected during:
            instantiation of "void flash::compute_attn_1rowblock<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &, int, int, int) [with Kernel_traits=Flash_fwd_kernel_traits<256, 128, 64, 8, false, false, cutlass::half_t, Flash_kernel_traits<256, 128, 64, 8, cutlass::half_t>>, Is_dropout=true, Is_causal=true, Is_even_N=true, Is_even_K=false, Return_softmax=true, Params=Flash_fwd_params]" 
(588): here
            instantiation of "void flash::compute_attn<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &) [with Kernel_traits=Flash_fwd_kernel_traits<256, 128, 64, 8, false, false, cutlass::half_t, Flash_kernel_traits<256, 128, 64, 8, cutlass::half_t>>, Is_dropout=true, Is_causal=true, Is_even_N=true, Is_even_K=false, Return_softmax=true, Params=Flash_fwd_params]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(15): here
            instantiation of "void flash_fwd_kernel<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax>(Flash_fwd_params) [with Kernel_traits=Flash_fwd_kernel_traits<256, 128, 64, 8, false, false, cutlass::half_t, Flash_kernel_traits<256, 128, 64, 8, cutlass::half_t>>, Is_dropout=true, Is_causal=true, Is_even_N=true, Is_even_K=false, Return_softmax=true]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(34): here
            instantiation of "void run_flash_fwd<Kernel_traits,Is_dropout,Is_causal>(Flash_fwd_params &, cudaStream_t) [with Kernel_traits=Flash_fwd_kernel_traits<256, 128, 64, 8, false, false, cutlass::half_t, Flash_kernel_traits<256, 128, 64, 8, cutlass::half_t>>, Is_dropout=true, Is_causal=true]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(236): here
            instantiation of "void run_mha_fwd_hdim256<T>(Flash_fwd_params &, cudaStream_t) [with T=cutlass::half_t]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_hdim256_fp16_sm80.cu(8): here

/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_kernel.h(275): warning: variable "tQrQ" was declared but never referenced
          detected during:
            instantiation of "void flash::compute_attn_1rowblock<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &, int, int, int) [with Kernel_traits=Flash_fwd_kernel_traits<256, 128, 64, 8, false, false, cutlass::half_t, Flash_kernel_traits<256, 128, 64, 8, cutlass::half_t>>, Is_dropout=true, Is_causal=true, Is_even_N=true, Is_even_K=false, Return_softmax=true, Params=Flash_fwd_params]" 
(588): here
            instantiation of "void flash::compute_attn<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &) [with Kernel_traits=Flash_fwd_kernel_traits<256, 128, 64, 8, false, false, cutlass::half_t, Flash_kernel_traits<256, 128, 64, 8, cutlass::half_t>>, Is_dropout=true, Is_causal=true, Is_even_N=true, Is_even_K=false, Return_softmax=true, Params=Flash_fwd_params]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(15): here
            instantiation of "void flash_fwd_kernel<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax>(Flash_fwd_params) [with Kernel_traits=Flash_fwd_kernel_traits<256, 128, 64, 8, false, false, cutlass::half_t, Flash_kernel_traits<256, 128, 64, 8, cutlass::half_t>>, Is_dropout=true, Is_causal=true, Is_even_N=true, Is_even_K=false, Return_softmax=true]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(34): here
            instantiation of "void run_flash_fwd<Kernel_traits,Is_dropout,Is_causal>(Flash_fwd_params &, cudaStream_t) [with Kernel_traits=Flash_fwd_kernel_traits<256, 128, 64, 8, false, false, cutlass::half_t, Flash_kernel_traits<256, 128, 64, 8, cutlass::half_t>>, Is_dropout=true, Is_causal=true]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(236): here
            instantiation of "void run_mha_fwd_hdim256<T>(Flash_fwd_params &, cudaStream_t) [with T=cutlass::half_t]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_hdim256_fp16_sm80.cu(8): here

/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_kernel.h(140): warning: variable "MMA_M" was declared but never referenced
          detected during:
            instantiation of "void flash::compute_attn_1rowblock<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &, int, int, int) [with Kernel_traits=Flash_fwd_kernel_traits<256, 128, 64, 8, false, false, cutlass::half_t, Flash_kernel_traits<256, 128, 64, 8, cutlass::half_t>>, Is_dropout=true, Is_causal=true, Is_even_N=true, Is_even_K=false, Return_softmax=false, Params=Flash_fwd_params]" 
(588): here
            instantiation of "void flash::compute_attn<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &) [with Kernel_traits=Flash_fwd_kernel_traits<256, 128, 64, 8, false, false, cutlass::half_t, Flash_kernel_traits<256, 128, 64, 8, cutlass::half_t>>, Is_dropout=true, Is_causal=true, Is_even_N=true, Is_even_K=false, Return_softmax=false, Params=Flash_fwd_params]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(15): here
            instantiation of "void flash_fwd_kernel<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax>(Flash_fwd_params) [with Kernel_traits=Flash_fwd_kernel_traits<256, 128, 64, 8, false, false, cutlass::half_t, Flash_kernel_traits<256, 128, 64, 8, cutlass::half_t>>, Is_dropout=true, Is_causal=true, Is_even_N=true, Is_even_K=false, Return_softmax=false]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(34): here
            instantiation of "void run_flash_fwd<Kernel_traits,Is_dropout,Is_causal>(Flash_fwd_params &, cudaStream_t) [with Kernel_traits=Flash_fwd_kernel_traits<256, 128, 64, 8, false, false, cutlass::half_t, Flash_kernel_traits<256, 128, 64, 8, cutlass::half_t>>, Is_dropout=true, Is_causal=true]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(236): here
            instantiation of "void run_mha_fwd_hdim256<T>(Flash_fwd_params &, cudaStream_t) [with T=cutlass::half_t]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_hdim256_fp16_sm80.cu(8): here

/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_kernel.h(275): warning: variable "tQrQ" was declared but never referenced
          detected during:
            instantiation of "void flash::compute_attn_1rowblock<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &, int, int, int) [with Kernel_traits=Flash_fwd_kernel_traits<256, 128, 64, 8, false, false, cutlass::half_t, Flash_kernel_traits<256, 128, 64, 8, cutlass::half_t>>, Is_dropout=true, Is_causal=true, Is_even_N=true, Is_even_K=false, Return_softmax=false, Params=Flash_fwd_params]" 
(588): here
            instantiation of "void flash::compute_attn<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &) [with Kernel_traits=Flash_fwd_kernel_traits<256, 128, 64, 8, false, false, cutlass::half_t, Flash_kernel_traits<256, 128, 64, 8, cutlass::half_t>>, Is_dropout=true, Is_causal=true, Is_even_N=true, Is_even_K=false, Return_softmax=false, Params=Flash_fwd_params]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(15): here
            instantiation of "void flash_fwd_kernel<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax>(Flash_fwd_params) [with Kernel_traits=Flash_fwd_kernel_traits<256, 128, 64, 8, false, false, cutlass::half_t, Flash_kernel_traits<256, 128, 64, 8, cutlass::half_t>>, Is_dropout=true, Is_causal=true, Is_even_N=true, Is_even_K=false, Return_softmax=false]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(34): here
            instantiation of "void run_flash_fwd<Kernel_traits,Is_dropout,Is_causal>(Flash_fwd_params &, cudaStream_t) [with Kernel_traits=Flash_fwd_kernel_traits<256, 128, 64, 8, false, false, cutlass::half_t, Flash_kernel_traits<256, 128, 64, 8, cutlass::half_t>>, Is_dropout=true, Is_causal=true]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(236): here
            instantiation of "void run_mha_fwd_hdim256<T>(Flash_fwd_params &, cudaStream_t) [with T=cutlass::half_t]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_hdim256_fp16_sm80.cu(8): here

/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_kernel.h(140): warning: variable "MMA_M" was declared but never referenced
          detected during:
            instantiation of "void flash::compute_attn_1rowblock<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &, int, int, int) [with Kernel_traits=Flash_fwd_kernel_traits<256, 128, 64, 8, false, false, cutlass::half_t, Flash_kernel_traits<256, 128, 64, 8, cutlass::half_t>>, Is_dropout=true, Is_causal=true, Is_even_N=false, Is_even_K=true, Return_softmax=true, Params=Flash_fwd_params]" 
(588): here
            instantiation of "void flash::compute_attn<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &) [with Kernel_traits=Flash_fwd_kernel_traits<256, 128, 64, 8, false, false, cutlass::half_t, Flash_kernel_traits<256, 128, 64, 8, cutlass::half_t>>, Is_dropout=true, Is_causal=true, Is_even_N=false, Is_even_K=true, Return_softmax=true, Params=Flash_fwd_params]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(15): here
            instantiation of "void flash_fwd_kernel<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax>(Flash_fwd_params) [with Kernel_traits=Flash_fwd_kernel_traits<256, 128, 64, 8, false, false, cutlass::half_t, Flash_kernel_traits<256, 128, 64, 8, cutlass::half_t>>, Is_dropout=true, Is_causal=true, Is_even_N=false, Is_even_K=true, Return_softmax=true]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(34): here
            instantiation of "void run_flash_fwd<Kernel_traits,Is_dropout,Is_causal>(Flash_fwd_params &, cudaStream_t) [with Kernel_traits=Flash_fwd_kernel_traits<256, 128, 64, 8, false, false, cutlass::half_t, Flash_kernel_traits<256, 128, 64, 8, cutlass::half_t>>, Is_dropout=true, Is_causal=true]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(236): here
            instantiation of "void run_mha_fwd_hdim256<T>(Flash_fwd_params &, cudaStream_t) [with T=cutlass::half_t]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_hdim256_fp16_sm80.cu(8): here

/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_kernel.h(275): warning: variable "tQrQ" was declared but never referenced
          detected during:
            instantiation of "void flash::compute_attn_1rowblock<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &, int, int, int) [with Kernel_traits=Flash_fwd_kernel_traits<256, 128, 64, 8, false, false, cutlass::half_t, Flash_kernel_traits<256, 128, 64, 8, cutlass::half_t>>, Is_dropout=true, Is_causal=true, Is_even_N=false, Is_even_K=true, Return_softmax=true, Params=Flash_fwd_params]" 
(588): here
            instantiation of "void flash::compute_attn<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &) [with Kernel_traits=Flash_fwd_kernel_traits<256, 128, 64, 8, false, false, cutlass::half_t, Flash_kernel_traits<256, 128, 64, 8, cutlass::half_t>>, Is_dropout=true, Is_causal=true, Is_even_N=false, Is_even_K=true, Return_softmax=true, Params=Flash_fwd_params]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(15): here
            instantiation of "void flash_fwd_kernel<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax>(Flash_fwd_params) [with Kernel_traits=Flash_fwd_kernel_traits<256, 128, 64, 8, false, false, cutlass::half_t, Flash_kernel_traits<256, 128, 64, 8, cutlass::half_t>>, Is_dropout=true, Is_causal=true, Is_even_N=false, Is_even_K=true, Return_softmax=true]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(34): here
            instantiation of "void run_flash_fwd<Kernel_traits,Is_dropout,Is_causal>(Flash_fwd_params &, cudaStream_t) [with Kernel_traits=Flash_fwd_kernel_traits<256, 128, 64, 8, false, false, cutlass::half_t, Flash_kernel_traits<256, 128, 64, 8, cutlass::half_t>>, Is_dropout=true, Is_causal=true]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(236): here
            instantiation of "void run_mha_fwd_hdim256<T>(Flash_fwd_params &, cudaStream_t) [with T=cutlass::half_t]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_hdim256_fp16_sm80.cu(8): here

/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_kernel.h(140): warning: variable "MMA_M" was declared but never referenced
          detected during:
            instantiation of "void flash::compute_attn_1rowblock<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &, int, int, int) [with Kernel_traits=Flash_fwd_kernel_traits<256, 128, 64, 8, false, false, cutlass::half_t, Flash_kernel_traits<256, 128, 64, 8, cutlass::half_t>>, Is_dropout=true, Is_causal=true, Is_even_N=false, Is_even_K=true, Return_softmax=false, Params=Flash_fwd_params]" 
(588): here
            instantiation of "void flash::compute_attn<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &) [with Kernel_traits=Flash_fwd_kernel_traits<256, 128, 64, 8, false, false, cutlass::half_t, Flash_kernel_traits<256, 128, 64, 8, cutlass::half_t>>, Is_dropout=true, Is_causal=true, Is_even_N=false, Is_even_K=true, Return_softmax=false, Params=Flash_fwd_params]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(15): here
            instantiation of "void flash_fwd_kernel<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax>(Flash_fwd_params) [with Kernel_traits=Flash_fwd_kernel_traits<256, 128, 64, 8, false, false, cutlass::half_t, Flash_kernel_traits<256, 128, 64, 8, cutlass::half_t>>, Is_dropout=true, Is_causal=true, Is_even_N=false, Is_even_K=true, Return_softmax=false]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(34): here
            instantiation of "void run_flash_fwd<Kernel_traits,Is_dropout,Is_causal>(Flash_fwd_params &, cudaStream_t) [with Kernel_traits=Flash_fwd_kernel_traits<256, 128, 64, 8, false, false, cutlass::half_t, Flash_kernel_traits<256, 128, 64, 8, cutlass::half_t>>, Is_dropout=true, Is_causal=true]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(236): here
            instantiation of "void run_mha_fwd_hdim256<T>(Flash_fwd_params &, cudaStream_t) [with T=cutlass::half_t]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_hdim256_fp16_sm80.cu(8): here

/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_kernel.h(275): warning: variable "tQrQ" was declared but never referenced
          detected during:
            instantiation of "void flash::compute_attn_1rowblock<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &, int, int, int) [with Kernel_traits=Flash_fwd_kernel_traits<256, 128, 64, 8, false, false, cutlass::half_t, Flash_kernel_traits<256, 128, 64, 8, cutlass::half_t>>, Is_dropout=true, Is_causal=true, Is_even_N=false, Is_even_K=true, Return_softmax=false, Params=Flash_fwd_params]" 
(588): here
            instantiation of "void flash::compute_attn<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &) [with Kernel_traits=Flash_fwd_kernel_traits<256, 128, 64, 8, false, false, cutlass::half_t, Flash_kernel_traits<256, 128, 64, 8, cutlass::half_t>>, Is_dropout=true, Is_causal=true, Is_even_N=false, Is_even_K=true, Return_softmax=false, Params=Flash_fwd_params]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(15): here
            instantiation of "void flash_fwd_kernel<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax>(Flash_fwd_params) [with Kernel_traits=Flash_fwd_kernel_traits<256, 128, 64, 8, false, false, cutlass::half_t, Flash_kernel_traits<256, 128, 64, 8, cutlass::half_t>>, Is_dropout=true, Is_causal=true, Is_even_N=false, Is_even_K=true, Return_softmax=false]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(34): here
            instantiation of "void run_flash_fwd<Kernel_traits,Is_dropout,Is_causal>(Flash_fwd_params &, cudaStream_t) [with Kernel_traits=Flash_fwd_kernel_traits<256, 128, 64, 8, false, false, cutlass::half_t, Flash_kernel_traits<256, 128, 64, 8, cutlass::half_t>>, Is_dropout=true, Is_causal=true]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(236): here
            instantiation of "void run_mha_fwd_hdim256<T>(Flash_fwd_params &, cudaStream_t) [with T=cutlass::half_t]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_hdim256_fp16_sm80.cu(8): here

/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_kernel.h(140): warning: variable "MMA_M" was declared but never referenced
          detected during:
            instantiation of "void flash::compute_attn_1rowblock<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &, int, int, int) [with Kernel_traits=Flash_fwd_kernel_traits<256, 128, 64, 8, false, false, cutlass::half_t, Flash_kernel_traits<256, 128, 64, 8, cutlass::half_t>>, Is_dropout=true, Is_causal=true, Is_even_N=false, Is_even_K=false, Return_softmax=true, Params=Flash_fwd_params]" 
(588): here
            instantiation of "void flash::compute_attn<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &) [with Kernel_traits=Flash_fwd_kernel_traits<256, 128, 64, 8, false, false, cutlass::half_t, Flash_kernel_traits<256, 128, 64, 8, cutlass::half_t>>, Is_dropout=true, Is_causal=true, Is_even_N=false, Is_even_K=false, Return_softmax=true, Params=Flash_fwd_params]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(15): here
            instantiation of "void flash_fwd_kernel<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax>(Flash_fwd_params) [with Kernel_traits=Flash_fwd_kernel_traits<256, 128, 64, 8, false, false, cutlass::half_t, Flash_kernel_traits<256, 128, 64, 8, cutlass::half_t>>, Is_dropout=true, Is_causal=true, Is_even_N=false, Is_even_K=false, Return_softmax=true]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(34): here
            instantiation of "void run_flash_fwd<Kernel_traits,Is_dropout,Is_causal>(Flash_fwd_params &, cudaStream_t) [with Kernel_traits=Flash_fwd_kernel_traits<256, 128, 64, 8, false, false, cutlass::half_t, Flash_kernel_traits<256, 128, 64, 8, cutlass::half_t>>, Is_dropout=true, Is_causal=true]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(236): here
            instantiation of "void run_mha_fwd_hdim256<T>(Flash_fwd_params &, cudaStream_t) [with T=cutlass::half_t]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_hdim256_fp16_sm80.cu(8): here

/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_kernel.h(275): warning: variable "tQrQ" was declared but never referenced
          detected during:
            instantiation of "void flash::compute_attn_1rowblock<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &, int, int, int) [with Kernel_traits=Flash_fwd_kernel_traits<256, 128, 64, 8, false, false, cutlass::half_t, Flash_kernel_traits<256, 128, 64, 8, cutlass::half_t>>, Is_dropout=true, Is_causal=true, Is_even_N=false, Is_even_K=false, Return_softmax=true, Params=Flash_fwd_params]" 
(588): here
            instantiation of "void flash::compute_attn<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &) [with Kernel_traits=Flash_fwd_kernel_traits<256, 128, 64, 8, false, false, cutlass::half_t, Flash_kernel_traits<256, 128, 64, 8, cutlass::half_t>>, Is_dropout=true, Is_causal=true, Is_even_N=false, Is_even_K=false, Return_softmax=true, Params=Flash_fwd_params]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(15): here
            instantiation of "void flash_fwd_kernel<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax>(Flash_fwd_params) [with Kernel_traits=Flash_fwd_kernel_traits<256, 128, 64, 8, false, false, cutlass::half_t, Flash_kernel_traits<256, 128, 64, 8, cutlass::half_t>>, Is_dropout=true, Is_causal=true, Is_even_N=false, Is_even_K=false, Return_softmax=true]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(34): here
            instantiation of "void run_flash_fwd<Kernel_traits,Is_dropout,Is_causal>(Flash_fwd_params &, cudaStream_t) [with Kernel_traits=Flash_fwd_kernel_traits<256, 128, 64, 8, false, false, cutlass::half_t, Flash_kernel_traits<256, 128, 64, 8, cutlass::half_t>>, Is_dropout=true, Is_causal=true]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(236): here
            instantiation of "void run_mha_fwd_hdim256<T>(Flash_fwd_params &, cudaStream_t) [with T=cutlass::half_t]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_hdim256_fp16_sm80.cu(8): here

/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_kernel.h(140): warning: variable "MMA_M" was declared but never referenced
          detected during:
            instantiation of "void flash::compute_attn_1rowblock<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &, int, int, int) [with Kernel_traits=Flash_fwd_kernel_traits<256, 128, 64, 8, false, false, cutlass::half_t, Flash_kernel_traits<256, 128, 64, 8, cutlass::half_t>>, Is_dropout=true, Is_causal=true, Is_even_N=false, Is_even_K=false, Return_softmax=false, Params=Flash_fwd_params]" 
(588): here
            instantiation of "void flash::compute_attn<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &) [with Kernel_traits=Flash_fwd_kernel_traits<256, 128, 64, 8, false, false, cutlass::half_t, Flash_kernel_traits<256, 128, 64, 8, cutlass::half_t>>, Is_dropout=true, Is_causal=true, Is_even_N=false, Is_even_K=false, Return_softmax=false, Params=Flash_fwd_params]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(15): here
            instantiation of "void flash_fwd_kernel<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax>(Flash_fwd_params) [with Kernel_traits=Flash_fwd_kernel_traits<256, 128, 64, 8, false, false, cutlass::half_t, Flash_kernel_traits<256, 128, 64, 8, cutlass::half_t>>, Is_dropout=true, Is_causal=true, Is_even_N=false, Is_even_K=false, Return_softmax=false]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(34): here
            instantiation of "void run_flash_fwd<Kernel_traits,Is_dropout,Is_causal>(Flash_fwd_params &, cudaStream_t) [with Kernel_traits=Flash_fwd_kernel_traits<256, 128, 64, 8, false, false, cutlass::half_t, Flash_kernel_traits<256, 128, 64, 8, cutlass::half_t>>, Is_dropout=true, Is_causal=true]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(236): here
            instantiation of "void run_mha_fwd_hdim256<T>(Flash_fwd_params &, cudaStream_t) [with T=cutlass::half_t]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_hdim256_fp16_sm80.cu(8): here

/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_kernel.h(275): warning: variable "tQrQ" was declared but never referenced
          detected during:
            instantiation of "void flash::compute_attn_1rowblock<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &, int, int, int) [with Kernel_traits=Flash_fwd_kernel_traits<256, 128, 64, 8, false, false, cutlass::half_t, Flash_kernel_traits<256, 128, 64, 8, cutlass::half_t>>, Is_dropout=true, Is_causal=true, Is_even_N=false, Is_even_K=false, Return_softmax=false, Params=Flash_fwd_params]" 
(588): here
            instantiation of "void flash::compute_attn<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &) [with Kernel_traits=Flash_fwd_kernel_traits<256, 128, 64, 8, false, false, cutlass::half_t, Flash_kernel_traits<256, 128, 64, 8, cutlass::half_t>>, Is_dropout=true, Is_causal=true, Is_even_N=false, Is_even_K=false, Return_softmax=false, Params=Flash_fwd_params]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(15): here
            instantiation of "void flash_fwd_kernel<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax>(Flash_fwd_params) [with Kernel_traits=Flash_fwd_kernel_traits<256, 128, 64, 8, false, false, cutlass::half_t, Flash_kernel_traits<256, 128, 64, 8, cutlass::half_t>>, Is_dropout=true, Is_causal=true, Is_even_N=false, Is_even_K=false, Return_softmax=false]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(34): here
            instantiation of "void run_flash_fwd<Kernel_traits,Is_dropout,Is_causal>(Flash_fwd_params &, cudaStream_t) [with Kernel_traits=Flash_fwd_kernel_traits<256, 128, 64, 8, false, false, cutlass::half_t, Flash_kernel_traits<256, 128, 64, 8, cutlass::half_t>>, Is_dropout=true, Is_causal=true]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(236): here
            instantiation of "void run_mha_fwd_hdim256<T>(Flash_fwd_params &, cudaStream_t) [with T=cutlass::half_t]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_hdim256_fp16_sm80.cu(8): here

/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_kernel.h(140): warning: variable "MMA_M" was declared but never referenced
          detected during:
            instantiation of "void flash::compute_attn_1rowblock<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &, int, int, int) [with Kernel_traits=Flash_fwd_kernel_traits<256, 64, 64, 4, false, false, cutlass::half_t, Flash_kernel_traits<256, 64, 64, 4, cutlass::half_t>>, Is_dropout=true, Is_causal=true, Is_even_N=true, Is_even_K=true, Return_softmax=true, Params=Flash_fwd_params]" 
(588): here
            instantiation of "void flash::compute_attn<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &) [with Kernel_traits=Flash_fwd_kernel_traits<256, 64, 64, 4, false, false, cutlass::half_t, Flash_kernel_traits<256, 64, 64, 4, cutlass::half_t>>, Is_dropout=true, Is_causal=true, Is_even_N=true, Is_even_K=true, Return_softmax=true, Params=Flash_fwd_params]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(15): here
            instantiation of "void flash_fwd_kernel<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax>(Flash_fwd_params) [with Kernel_traits=Flash_fwd_kernel_traits<256, 64, 64, 4, false, false, cutlass::half_t, Flash_kernel_traits<256, 64, 64, 4, cutlass::half_t>>, Is_dropout=true, Is_causal=true, Is_even_N=true, Is_even_K=true, Return_softmax=true]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(34): here
            instantiation of "void run_flash_fwd<Kernel_traits,Is_dropout,Is_causal>(Flash_fwd_params &, cudaStream_t) [with Kernel_traits=Flash_fwd_kernel_traits<256, 64, 64, 4, false, false, cutlass::half_t, Flash_kernel_traits<256, 64, 64, 4, cutlass::half_t>>, Is_dropout=true, Is_causal=true]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(236): here
            instantiation of "void run_mha_fwd_hdim256<T>(Flash_fwd_params &, cudaStream_t) [with T=cutlass::half_t]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_hdim256_fp16_sm80.cu(8): here

/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_kernel.h(275): warning: variable "tQrQ" was declared but never referenced
          detected during:
            instantiation of "void flash::compute_attn_1rowblock<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &, int, int, int) [with Kernel_traits=Flash_fwd_kernel_traits<256, 64, 64, 4, false, false, cutlass::half_t, Flash_kernel_traits<256, 64, 64, 4, cutlass::half_t>>, Is_dropout=true, Is_causal=true, Is_even_N=true, Is_even_K=true, Return_softmax=true, Params=Flash_fwd_params]" 
(588): here
            instantiation of "void flash::compute_attn<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &) [with Kernel_traits=Flash_fwd_kernel_traits<256, 64, 64, 4, false, false, cutlass::half_t, Flash_kernel_traits<256, 64, 64, 4, cutlass::half_t>>, Is_dropout=true, Is_causal=true, Is_even_N=true, Is_even_K=true, Return_softmax=true, Params=Flash_fwd_params]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(15): here
            instantiation of "void flash_fwd_kernel<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax>(Flash_fwd_params) [with Kernel_traits=Flash_fwd_kernel_traits<256, 64, 64, 4, false, false, cutlass::half_t, Flash_kernel_traits<256, 64, 64, 4, cutlass::half_t>>, Is_dropout=true, Is_causal=true, Is_even_N=true, Is_even_K=true, Return_softmax=true]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(34): here
            instantiation of "void run_flash_fwd<Kernel_traits,Is_dropout,Is_causal>(Flash_fwd_params &, cudaStream_t) [with Kernel_traits=Flash_fwd_kernel_traits<256, 64, 64, 4, false, false, cutlass::half_t, Flash_kernel_traits<256, 64, 64, 4, cutlass::half_t>>, Is_dropout=true, Is_causal=true]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(236): here
            instantiation of "void run_mha_fwd_hdim256<T>(Flash_fwd_params &, cudaStream_t) [with T=cutlass::half_t]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_hdim256_fp16_sm80.cu(8): here

/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_kernel.h(140): warning: variable "MMA_M" was declared but never referenced
          detected during:
            instantiation of "void flash::compute_attn_1rowblock<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &, int, int, int) [with Kernel_traits=Flash_fwd_kernel_traits<256, 64, 64, 4, false, false, cutlass::half_t, Flash_kernel_traits<256, 64, 64, 4, cutlass::half_t>>, Is_dropout=true, Is_causal=true, Is_even_N=true, Is_even_K=true, Return_softmax=false, Params=Flash_fwd_params]" 
(588): here
            instantiation of "void flash::compute_attn<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &) [with Kernel_traits=Flash_fwd_kernel_traits<256, 64, 64, 4, false, false, cutlass::half_t, Flash_kernel_traits<256, 64, 64, 4, cutlass::half_t>>, Is_dropout=true, Is_causal=true, Is_even_N=true, Is_even_K=true, Return_softmax=false, Params=Flash_fwd_params]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(15): here
            instantiation of "void flash_fwd_kernel<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax>(Flash_fwd_params) [with Kernel_traits=Flash_fwd_kernel_traits<256, 64, 64, 4, false, false, cutlass::half_t, Flash_kernel_traits<256, 64, 64, 4, cutlass::half_t>>, Is_dropout=true, Is_causal=true, Is_even_N=true, Is_even_K=true, Return_softmax=false]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(34): here
            instantiation of "void run_flash_fwd<Kernel_traits,Is_dropout,Is_causal>(Flash_fwd_params &, cudaStream_t) [with Kernel_traits=Flash_fwd_kernel_traits<256, 64, 64, 4, false, false, cutlass::half_t, Flash_kernel_traits<256, 64, 64, 4, cutlass::half_t>>, Is_dropout=true, Is_causal=true]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(236): here
            instantiation of "void run_mha_fwd_hdim256<T>(Flash_fwd_params &, cudaStream_t) [with T=cutlass::half_t]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_hdim256_fp16_sm80.cu(8): here

/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_kernel.h(275): warning: variable "tQrQ" was declared but never referenced
          detected during:
            instantiation of "void flash::compute_attn_1rowblock<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &, int, int, int) [with Kernel_traits=Flash_fwd_kernel_traits<256, 64, 64, 4, false, false, cutlass::half_t, Flash_kernel_traits<256, 64, 64, 4, cutlass::half_t>>, Is_dropout=true, Is_causal=true, Is_even_N=true, Is_even_K=true, Return_softmax=false, Params=Flash_fwd_params]" 
(588): here
            instantiation of "void flash::compute_attn<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &) [with Kernel_traits=Flash_fwd_kernel_traits<256, 64, 64, 4, false, false, cutlass::half_t, Flash_kernel_traits<256, 64, 64, 4, cutlass::half_t>>, Is_dropout=true, Is_causal=true, Is_even_N=true, Is_even_K=true, Return_softmax=false, Params=Flash_fwd_params]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(15): here
            instantiation of "void flash_fwd_kernel<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax>(Flash_fwd_params) [with Kernel_traits=Flash_fwd_kernel_traits<256, 64, 64, 4, false, false, cutlass::half_t, Flash_kernel_traits<256, 64, 64, 4, cutlass::half_t>>, Is_dropout=true, Is_causal=true, Is_even_N=true, Is_even_K=true, Return_softmax=false]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(34): here
            instantiation of "void run_flash_fwd<Kernel_traits,Is_dropout,Is_causal>(Flash_fwd_params &, cudaStream_t) [with Kernel_traits=Flash_fwd_kernel_traits<256, 64, 64, 4, false, false, cutlass::half_t, Flash_kernel_traits<256, 64, 64, 4, cutlass::half_t>>, Is_dropout=true, Is_causal=true]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(236): here
            instantiation of "void run_mha_fwd_hdim256<T>(Flash_fwd_params &, cudaStream_t) [with T=cutlass::half_t]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_hdim256_fp16_sm80.cu(8): here

/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_kernel.h(140): warning: variable "MMA_M" was declared but never referenced
          detected during:
            instantiation of "void flash::compute_attn_1rowblock<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &, int, int, int) [with Kernel_traits=Flash_fwd_kernel_traits<256, 64, 64, 4, false, false, cutlass::half_t, Flash_kernel_traits<256, 64, 64, 4, cutlass::half_t>>, Is_dropout=true, Is_causal=true, Is_even_N=true, Is_even_K=false, Return_softmax=true, Params=Flash_fwd_params]" 
(588): here
            instantiation of "void flash::compute_attn<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &) [with Kernel_traits=Flash_fwd_kernel_traits<256, 64, 64, 4, false, false, cutlass::half_t, Flash_kernel_traits<256, 64, 64, 4, cutlass::half_t>>, Is_dropout=true, Is_causal=true, Is_even_N=true, Is_even_K=false, Return_softmax=true, Params=Flash_fwd_params]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(15): here
            instantiation of "void flash_fwd_kernel<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax>(Flash_fwd_params) [with Kernel_traits=Flash_fwd_kernel_traits<256, 64, 64, 4, false, false, cutlass::half_t, Flash_kernel_traits<256, 64, 64, 4, cutlass::half_t>>, Is_dropout=true, Is_causal=true, Is_even_N=true, Is_even_K=false, Return_softmax=true]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(34): here
            instantiation of "void run_flash_fwd<Kernel_traits,Is_dropout,Is_causal>(Flash_fwd_params &, cudaStream_t) [with Kernel_traits=Flash_fwd_kernel_traits<256, 64, 64, 4, false, false, cutlass::half_t, Flash_kernel_traits<256, 64, 64, 4, cutlass::half_t>>, Is_dropout=true, Is_causal=true]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(236): here
            instantiation of "void run_mha_fwd_hdim256<T>(Flash_fwd_params &, cudaStream_t) [with T=cutlass::half_t]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_hdim256_fp16_sm80.cu(8): here

/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_kernel.h(275): warning: variable "tQrQ" was declared but never referenced
          detected during:
            instantiation of "void flash::compute_attn_1rowblock<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &, int, int, int) [with Kernel_traits=Flash_fwd_kernel_traits<256, 64, 64, 4, false, false, cutlass::half_t, Flash_kernel_traits<256, 64, 64, 4, cutlass::half_t>>, Is_dropout=true, Is_causal=true, Is_even_N=true, Is_even_K=false, Return_softmax=true, Params=Flash_fwd_params]" 
(588): here
            instantiation of "void flash::compute_attn<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &) [with Kernel_traits=Flash_fwd_kernel_traits<256, 64, 64, 4, false, false, cutlass::half_t, Flash_kernel_traits<256, 64, 64, 4, cutlass::half_t>>, Is_dropout=true, Is_causal=true, Is_even_N=true, Is_even_K=false, Return_softmax=true, Params=Flash_fwd_params]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(15): here
            instantiation of "void flash_fwd_kernel<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax>(Flash_fwd_params) [with Kernel_traits=Flash_fwd_kernel_traits<256, 64, 64, 4, false, false, cutlass::half_t, Flash_kernel_traits<256, 64, 64, 4, cutlass::half_t>>, Is_dropout=true, Is_causal=true, Is_even_N=true, Is_even_K=false, Return_softmax=true]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(34): here
            instantiation of "void run_flash_fwd<Kernel_traits,Is_dropout,Is_causal>(Flash_fwd_params &, cudaStream_t) [with Kernel_traits=Flash_fwd_kernel_traits<256, 64, 64, 4, false, false, cutlass::half_t, Flash_kernel_traits<256, 64, 64, 4, cutlass::half_t>>, Is_dropout=true, Is_causal=true]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(236): here
            instantiation of "void run_mha_fwd_hdim256<T>(Flash_fwd_params &, cudaStream_t) [with T=cutlass::half_t]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_hdim256_fp16_sm80.cu(8): here

/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_kernel.h(140): warning: variable "MMA_M" was declared but never referenced
          detected during:
            instantiation of "void flash::compute_attn_1rowblock<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &, int, int, int) [with Kernel_traits=Flash_fwd_kernel_traits<256, 64, 64, 4, false, false, cutlass::half_t, Flash_kernel_traits<256, 64, 64, 4, cutlass::half_t>>, Is_dropout=true, Is_causal=true, Is_even_N=true, Is_even_K=false, Return_softmax=false, Params=Flash_fwd_params]" 
(588): here
            instantiation of "void flash::compute_attn<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &) [with Kernel_traits=Flash_fwd_kernel_traits<256, 64, 64, 4, false, false, cutlass::half_t, Flash_kernel_traits<256, 64, 64, 4, cutlass::half_t>>, Is_dropout=true, Is_causal=true, Is_even_N=true, Is_even_K=false, Return_softmax=false, Params=Flash_fwd_params]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(15): here
            instantiation of "void flash_fwd_kernel<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax>(Flash_fwd_params) [with Kernel_traits=Flash_fwd_kernel_traits<256, 64, 64, 4, false, false, cutlass::half_t, Flash_kernel_traits<256, 64, 64, 4, cutlass::half_t>>, Is_dropout=true, Is_causal=true, Is_even_N=true, Is_even_K=false, Return_softmax=false]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(34): here
            instantiation of "void run_flash_fwd<Kernel_traits,Is_dropout,Is_causal>(Flash_fwd_params &, cudaStream_t) [with Kernel_traits=Flash_fwd_kernel_traits<256, 64, 64, 4, false, false, cutlass::half_t, Flash_kernel_traits<256, 64, 64, 4, cutlass::half_t>>, Is_dropout=true, Is_causal=true]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(236): here
            instantiation of "void run_mha_fwd_hdim256<T>(Flash_fwd_params &, cudaStream_t) [with T=cutlass::half_t]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_hdim256_fp16_sm80.cu(8): here

/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_kernel.h(275): warning: variable "tQrQ" was declared but never referenced
          detected during:
            instantiation of "void flash::compute_attn_1rowblock<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &, int, int, int) [with Kernel_traits=Flash_fwd_kernel_traits<256, 64, 64, 4, false, false, cutlass::half_t, Flash_kernel_traits<256, 64, 64, 4, cutlass::half_t>>, Is_dropout=true, Is_causal=true, Is_even_N=true, Is_even_K=false, Return_softmax=false, Params=Flash_fwd_params]" 
(588): here
            instantiation of "void flash::compute_attn<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &) [with Kernel_traits=Flash_fwd_kernel_traits<256, 64, 64, 4, false, false, cutlass::half_t, Flash_kernel_traits<256, 64, 64, 4, cutlass::half_t>>, Is_dropout=true, Is_causal=true, Is_even_N=true, Is_even_K=false, Return_softmax=false, Params=Flash_fwd_params]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(15): here
            instantiation of "void flash_fwd_kernel<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax>(Flash_fwd_params) [with Kernel_traits=Flash_fwd_kernel_traits<256, 64, 64, 4, false, false, cutlass::half_t, Flash_kernel_traits<256, 64, 64, 4, cutlass::half_t>>, Is_dropout=true, Is_causal=true, Is_even_N=true, Is_even_K=false, Return_softmax=false]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(34): here
            instantiation of "void run_flash_fwd<Kernel_traits,Is_dropout,Is_causal>(Flash_fwd_params &, cudaStream_t) [with Kernel_traits=Flash_fwd_kernel_traits<256, 64, 64, 4, false, false, cutlass::half_t, Flash_kernel_traits<256, 64, 64, 4, cutlass::half_t>>, Is_dropout=true, Is_causal=true]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(236): here
            instantiation of "void run_mha_fwd_hdim256<T>(Flash_fwd_params &, cudaStream_t) [with T=cutlass::half_t]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_hdim256_fp16_sm80.cu(8): here

/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_kernel.h(140): warning: variable "MMA_M" was declared but never referenced
          detected during:
            instantiation of "void flash::compute_attn_1rowblock<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &, int, int, int) [with Kernel_traits=Flash_fwd_kernel_traits<256, 64, 64, 4, false, false, cutlass::half_t, Flash_kernel_traits<256, 64, 64, 4, cutlass::half_t>>, Is_dropout=true, Is_causal=true, Is_even_N=false, Is_even_K=true, Return_softmax=true, Params=Flash_fwd_params]" 
(588): here
            instantiation of "void flash::compute_attn<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &) [with Kernel_traits=Flash_fwd_kernel_traits<256, 64, 64, 4, false, false, cutlass::half_t, Flash_kernel_traits<256, 64, 64, 4, cutlass::half_t>>, Is_dropout=true, Is_causal=true, Is_even_N=false, Is_even_K=true, Return_softmax=true, Params=Flash_fwd_params]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(15): here
            instantiation of "void flash_fwd_kernel<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax>(Flash_fwd_params) [with Kernel_traits=Flash_fwd_kernel_traits<256, 64, 64, 4, false, false, cutlass::half_t, Flash_kernel_traits<256, 64, 64, 4, cutlass::half_t>>, Is_dropout=true, Is_causal=true, Is_even_N=false, Is_even_K=true, Return_softmax=true]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(34): here
            instantiation of "void run_flash_fwd<Kernel_traits,Is_dropout,Is_causal>(Flash_fwd_params &, cudaStream_t) [with Kernel_traits=Flash_fwd_kernel_traits<256, 64, 64, 4, false, false, cutlass::half_t, Flash_kernel_traits<256, 64, 64, 4, cutlass::half_t>>, Is_dropout=true, Is_causal=true]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(236): here
            instantiation of "void run_mha_fwd_hdim256<T>(Flash_fwd_params &, cudaStream_t) [with T=cutlass::half_t]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_hdim256_fp16_sm80.cu(8): here

/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_kernel.h(275): warning: variable "tQrQ" was declared but never referenced
          detected during:
            instantiation of "void flash::compute_attn_1rowblock<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &, int, int, int) [with Kernel_traits=Flash_fwd_kernel_traits<256, 64, 64, 4, false, false, cutlass::half_t, Flash_kernel_traits<256, 64, 64, 4, cutlass::half_t>>, Is_dropout=true, Is_causal=true, Is_even_N=false, Is_even_K=true, Return_softmax=true, Params=Flash_fwd_params]" 
(588): here
            instantiation of "void flash::compute_attn<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &) [with Kernel_traits=Flash_fwd_kernel_traits<256, 64, 64, 4, false, false, cutlass::half_t, Flash_kernel_traits<256, 64, 64, 4, cutlass::half_t>>, Is_dropout=true, Is_causal=true, Is_even_N=false, Is_even_K=true, Return_softmax=true, Params=Flash_fwd_params]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(15): here
            instantiation of "void flash_fwd_kernel<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax>(Flash_fwd_params) [with Kernel_traits=Flash_fwd_kernel_traits<256, 64, 64, 4, false, false, cutlass::half_t, Flash_kernel_traits<256, 64, 64, 4, cutlass::half_t>>, Is_dropout=true, Is_causal=true, Is_even_N=false, Is_even_K=true, Return_softmax=true]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(34): here
            instantiation of "void run_flash_fwd<Kernel_traits,Is_dropout,Is_causal>(Flash_fwd_params &, cudaStream_t) [with Kernel_traits=Flash_fwd_kernel_traits<256, 64, 64, 4, false, false, cutlass::half_t, Flash_kernel_traits<256, 64, 64, 4, cutlass::half_t>>, Is_dropout=true, Is_causal=true]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(236): here
            instantiation of "void run_mha_fwd_hdim256<T>(Flash_fwd_params &, cudaStream_t) [with T=cutlass::half_t]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_hdim256_fp16_sm80.cu(8): here

/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_kernel.h(140): warning: variable "MMA_M" was declared but never referenced
          detected during:
            instantiation of "void flash::compute_attn_1rowblock<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &, int, int, int) [with Kernel_traits=Flash_fwd_kernel_traits<256, 64, 64, 4, false, false, cutlass::half_t, Flash_kernel_traits<256, 64, 64, 4, cutlass::half_t>>, Is_dropout=true, Is_causal=true, Is_even_N=false, Is_even_K=true, Return_softmax=false, Params=Flash_fwd_params]" 
(588): here
            instantiation of "void flash::compute_attn<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &) [with Kernel_traits=Flash_fwd_kernel_traits<256, 64, 64, 4, false, false, cutlass::half_t, Flash_kernel_traits<256, 64, 64, 4, cutlass::half_t>>, Is_dropout=true, Is_causal=true, Is_even_N=false, Is_even_K=true, Return_softmax=false, Params=Flash_fwd_params]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(15): here
            instantiation of "void flash_fwd_kernel<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax>(Flash_fwd_params) [with Kernel_traits=Flash_fwd_kernel_traits<256, 64, 64, 4, false, false, cutlass::half_t, Flash_kernel_traits<256, 64, 64, 4, cutlass::half_t>>, Is_dropout=true, Is_causal=true, Is_even_N=false, Is_even_K=true, Return_softmax=false]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(34): here
            instantiation of "void run_flash_fwd<Kernel_traits,Is_dropout,Is_causal>(Flash_fwd_params &, cudaStream_t) [with Kernel_traits=Flash_fwd_kernel_traits<256, 64, 64, 4, false, false, cutlass::half_t, Flash_kernel_traits<256, 64, 64, 4, cutlass::half_t>>, Is_dropout=true, Is_causal=true]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(236): here
            instantiation of "void run_mha_fwd_hdim256<T>(Flash_fwd_params &, cudaStream_t) [with T=cutlass::half_t]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_hdim256_fp16_sm80.cu(8): here

/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_kernel.h(275): warning: variable "tQrQ" was declared but never referenced
          detected during:
            instantiation of "void flash::compute_attn_1rowblock<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &, int, int, int) [with Kernel_traits=Flash_fwd_kernel_traits<256, 64, 64, 4, false, false, cutlass::half_t, Flash_kernel_traits<256, 64, 64, 4, cutlass::half_t>>, Is_dropout=true, Is_causal=true, Is_even_N=false, Is_even_K=true, Return_softmax=false, Params=Flash_fwd_params]" 
(588): here
            instantiation of "void flash::compute_attn<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &) [with Kernel_traits=Flash_fwd_kernel_traits<256, 64, 64, 4, false, false, cutlass::half_t, Flash_kernel_traits<256, 64, 64, 4, cutlass::half_t>>, Is_dropout=true, Is_causal=true, Is_even_N=false, Is_even_K=true, Return_softmax=false, Params=Flash_fwd_params]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(15): here
            instantiation of "void flash_fwd_kernel<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax>(Flash_fwd_params) [with Kernel_traits=Flash_fwd_kernel_traits<256, 64, 64, 4, false, false, cutlass::half_t, Flash_kernel_traits<256, 64, 64, 4, cutlass::half_t>>, Is_dropout=true, Is_causal=true, Is_even_N=false, Is_even_K=true, Return_softmax=false]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(34): here
            instantiation of "void run_flash_fwd<Kernel_traits,Is_dropout,Is_causal>(Flash_fwd_params &, cudaStream_t) [with Kernel_traits=Flash_fwd_kernel_traits<256, 64, 64, 4, false, false, cutlass::half_t, Flash_kernel_traits<256, 64, 64, 4, cutlass::half_t>>, Is_dropout=true, Is_causal=true]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(236): here
            instantiation of "void run_mha_fwd_hdim256<T>(Flash_fwd_params &, cudaStream_t) [with T=cutlass::half_t]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_hdim256_fp16_sm80.cu(8): here

/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_kernel.h(140): warning: variable "MMA_M" was declared but never referenced
          detected during:
            instantiation of "void flash::compute_attn_1rowblock<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &, int, int, int) [with Kernel_traits=Flash_fwd_kernel_traits<256, 64, 64, 4, false, false, cutlass::half_t, Flash_kernel_traits<256, 64, 64, 4, cutlass::half_t>>, Is_dropout=true, Is_causal=true, Is_even_N=false, Is_even_K=false, Return_softmax=true, Params=Flash_fwd_params]" 
(588): here
            instantiation of "void flash::compute_attn<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &) [with Kernel_traits=Flash_fwd_kernel_traits<256, 64, 64, 4, false, false, cutlass::half_t, Flash_kernel_traits<256, 64, 64, 4, cutlass::half_t>>, Is_dropout=true, Is_causal=true, Is_even_N=false, Is_even_K=false, Return_softmax=true, Params=Flash_fwd_params]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(15): here
            instantiation of "void flash_fwd_kernel<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax>(Flash_fwd_params) [with Kernel_traits=Flash_fwd_kernel_traits<256, 64, 64, 4, false, false, cutlass::half_t, Flash_kernel_traits<256, 64, 64, 4, cutlass::half_t>>, Is_dropout=true, Is_causal=true, Is_even_N=false, Is_even_K=false, Return_softmax=true]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(34): here
            instantiation of "void run_flash_fwd<Kernel_traits,Is_dropout,Is_causal>(Flash_fwd_params &, cudaStream_t) [with Kernel_traits=Flash_fwd_kernel_traits<256, 64, 64, 4, false, false, cutlass::half_t, Flash_kernel_traits<256, 64, 64, 4, cutlass::half_t>>, Is_dropout=true, Is_causal=true]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(236): here
            instantiation of "void run_mha_fwd_hdim256<T>(Flash_fwd_params &, cudaStream_t) [with T=cutlass::half_t]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_hdim256_fp16_sm80.cu(8): here

/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_kernel.h(275): warning: variable "tQrQ" was declared but never referenced
          detected during:
            instantiation of "void flash::compute_attn_1rowblock<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &, int, int, int) [with Kernel_traits=Flash_fwd_kernel_traits<256, 64, 64, 4, false, false, cutlass::half_t, Flash_kernel_traits<256, 64, 64, 4, cutlass::half_t>>, Is_dropout=true, Is_causal=true, Is_even_N=false, Is_even_K=false, Return_softmax=true, Params=Flash_fwd_params]" 
(588): here
            instantiation of "void flash::compute_attn<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &) [with Kernel_traits=Flash_fwd_kernel_traits<256, 64, 64, 4, false, false, cutlass::half_t, Flash_kernel_traits<256, 64, 64, 4, cutlass::half_t>>, Is_dropout=true, Is_causal=true, Is_even_N=false, Is_even_K=false, Return_softmax=true, Params=Flash_fwd_params]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(15): here
            instantiation of "void flash_fwd_kernel<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax>(Flash_fwd_params) [with Kernel_traits=Flash_fwd_kernel_traits<256, 64, 64, 4, false, false, cutlass::half_t, Flash_kernel_traits<256, 64, 64, 4, cutlass::half_t>>, Is_dropout=true, Is_causal=true, Is_even_N=false, Is_even_K=false, Return_softmax=true]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(34): here
            instantiation of "void run_flash_fwd<Kernel_traits,Is_dropout,Is_causal>(Flash_fwd_params &, cudaStream_t) [with Kernel_traits=Flash_fwd_kernel_traits<256, 64, 64, 4, false, false, cutlass::half_t, Flash_kernel_traits<256, 64, 64, 4, cutlass::half_t>>, Is_dropout=true, Is_causal=true]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(236): here
            instantiation of "void run_mha_fwd_hdim256<T>(Flash_fwd_params &, cudaStream_t) [with T=cutlass::half_t]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_hdim256_fp16_sm80.cu(8): here

/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_kernel.h(140): warning: variable "MMA_M" was declared but never referenced
          detected during:
            instantiation of "void flash::compute_attn_1rowblock<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &, int, int, int) [with Kernel_traits=Flash_fwd_kernel_traits<256, 64, 64, 4, false, false, cutlass::half_t, Flash_kernel_traits<256, 64, 64, 4, cutlass::half_t>>, Is_dropout=true, Is_causal=true, Is_even_N=false, Is_even_K=false, Return_softmax=false, Params=Flash_fwd_params]" 
(588): here
            instantiation of "void flash::compute_attn<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &) [with Kernel_traits=Flash_fwd_kernel_traits<256, 64, 64, 4, false, false, cutlass::half_t, Flash_kernel_traits<256, 64, 64, 4, cutlass::half_t>>, Is_dropout=true, Is_causal=true, Is_even_N=false, Is_even_K=false, Return_softmax=false, Params=Flash_fwd_params]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(15): here
            instantiation of "void flash_fwd_kernel<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax>(Flash_fwd_params) [with Kernel_traits=Flash_fwd_kernel_traits<256, 64, 64, 4, false, false, cutlass::half_t, Flash_kernel_traits<256, 64, 64, 4, cutlass::half_t>>, Is_dropout=true, Is_causal=true, Is_even_N=false, Is_even_K=false, Return_softmax=false]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(34): here
            instantiation of "void run_flash_fwd<Kernel_traits,Is_dropout,Is_causal>(Flash_fwd_params &, cudaStream_t) [with Kernel_traits=Flash_fwd_kernel_traits<256, 64, 64, 4, false, false, cutlass::half_t, Flash_kernel_traits<256, 64, 64, 4, cutlass::half_t>>, Is_dropout=true, Is_causal=true]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(236): here
            instantiation of "void run_mha_fwd_hdim256<T>(Flash_fwd_params &, cudaStream_t) [with T=cutlass::half_t]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_hdim256_fp16_sm80.cu(8): here

/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_kernel.h(275): warning: variable "tQrQ" was declared but never referenced
          detected during:
            instantiation of "void flash::compute_attn_1rowblock<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &, int, int, int) [with Kernel_traits=Flash_fwd_kernel_traits<256, 64, 64, 4, false, false, cutlass::half_t, Flash_kernel_traits<256, 64, 64, 4, cutlass::half_t>>, Is_dropout=true, Is_causal=true, Is_even_N=false, Is_even_K=false, Return_softmax=false, Params=Flash_fwd_params]" 
(588): here
            instantiation of "void flash::compute_attn<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &) [with Kernel_traits=Flash_fwd_kernel_traits<256, 64, 64, 4, false, false, cutlass::half_t, Flash_kernel_traits<256, 64, 64, 4, cutlass::half_t>>, Is_dropout=true, Is_causal=true, Is_even_N=false, Is_even_K=false, Return_softmax=false, Params=Flash_fwd_params]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(15): here
            instantiation of "void flash_fwd_kernel<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax>(Flash_fwd_params) [with Kernel_traits=Flash_fwd_kernel_traits<256, 64, 64, 4, false, false, cutlass::half_t, Flash_kernel_traits<256, 64, 64, 4, cutlass::half_t>>, Is_dropout=true, Is_causal=true, Is_even_N=false, Is_even_K=false, Return_softmax=false]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(34): here
            instantiation of "void run_flash_fwd<Kernel_traits,Is_dropout,Is_causal>(Flash_fwd_params &, cudaStream_t) [with Kernel_traits=Flash_fwd_kernel_traits<256, 64, 64, 4, false, false, cutlass::half_t, Flash_kernel_traits<256, 64, 64, 4, cutlass::half_t>>, Is_dropout=true, Is_causal=true]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(236): here
            instantiation of "void run_mha_fwd_hdim256<T>(Flash_fwd_params &, cudaStream_t) [with T=cutlass::half_t]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_hdim256_fp16_sm80.cu(8): here

/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_kernel.h(140): warning: variable "MMA_M" was declared but never referenced
          detected during:
            instantiation of "void flash::compute_attn_1rowblock<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &, int, int, int) [with Kernel_traits=Flash_fwd_kernel_traits<256, 128, 64, 8, false, false, cutlass::half_t, Flash_kernel_traits<256, 128, 64, 8, cutlass::half_t>>, Is_dropout=true, Is_causal=false, Is_even_N=true, Is_even_K=true, Return_softmax=true, Params=Flash_fwd_params]" 
(588): here
            instantiation of "void flash::compute_attn<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &) [with Kernel_traits=Flash_fwd_kernel_traits<256, 128, 64, 8, false, false, cutlass::half_t, Flash_kernel_traits<256, 128, 64, 8, cutlass::half_t>>, Is_dropout=true, Is_causal=false, Is_even_N=true, Is_even_K=true, Return_softmax=true, Params=Flash_fwd_params]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(15): here
            instantiation of "void flash_fwd_kernel<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax>(Flash_fwd_params) [with Kernel_traits=Flash_fwd_kernel_traits<256, 128, 64, 8, false, false, cutlass::half_t, Flash_kernel_traits<256, 128, 64, 8, cutlass::half_t>>, Is_dropout=true, Is_causal=false, Is_even_N=true, Is_even_K=true, Return_softmax=true]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(34): here
            instantiation of "void run_flash_fwd<Kernel_traits,Is_dropout,Is_causal>(Flash_fwd_params &, cudaStream_t) [with Kernel_traits=Flash_fwd_kernel_traits<256, 128, 64, 8, false, false, cutlass::half_t, Flash_kernel_traits<256, 128, 64, 8, cutlass::half_t>>, Is_dropout=true, Is_causal=false]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(236): here
            instantiation of "void run_mha_fwd_hdim256<T>(Flash_fwd_params &, cudaStream_t) [with T=cutlass::half_t]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_hdim256_fp16_sm80.cu(8): here

/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_kernel.h(275): warning: variable "tQrQ" was declared but never referenced
          detected during:
            instantiation of "void flash::compute_attn_1rowblock<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &, int, int, int) [with Kernel_traits=Flash_fwd_kernel_traits<256, 128, 64, 8, false, false, cutlass::half_t, Flash_kernel_traits<256, 128, 64, 8, cutlass::half_t>>, Is_dropout=true, Is_causal=false, Is_even_N=true, Is_even_K=true, Return_softmax=true, Params=Flash_fwd_params]" 
(588): here
            instantiation of "void flash::compute_attn<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &) [with Kernel_traits=Flash_fwd_kernel_traits<256, 128, 64, 8, false, false, cutlass::half_t, Flash_kernel_traits<256, 128, 64, 8, cutlass::half_t>>, Is_dropout=true, Is_causal=false, Is_even_N=true, Is_even_K=true, Return_softmax=true, Params=Flash_fwd_params]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(15): here
            instantiation of "void flash_fwd_kernel<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax>(Flash_fwd_params) [with Kernel_traits=Flash_fwd_kernel_traits<256, 128, 64, 8, false, false, cutlass::half_t, Flash_kernel_traits<256, 128, 64, 8, cutlass::half_t>>, Is_dropout=true, Is_causal=false, Is_even_N=true, Is_even_K=true, Return_softmax=true]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(34): here
            instantiation of "void run_flash_fwd<Kernel_traits,Is_dropout,Is_causal>(Flash_fwd_params &, cudaStream_t) [with Kernel_traits=Flash_fwd_kernel_traits<256, 128, 64, 8, false, false, cutlass::half_t, Flash_kernel_traits<256, 128, 64, 8, cutlass::half_t>>, Is_dropout=true, Is_causal=false]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(236): here
            instantiation of "void run_mha_fwd_hdim256<T>(Flash_fwd_params &, cudaStream_t) [with T=cutlass::half_t]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_hdim256_fp16_sm80.cu(8): here

/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_kernel.h(140): warning: variable "MMA_M" was declared but never referenced
          detected during:
            instantiation of "void flash::compute_attn_1rowblock<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &, int, int, int) [with Kernel_traits=Flash_fwd_kernel_traits<256, 128, 64, 8, false, false, cutlass::half_t, Flash_kernel_traits<256, 128, 64, 8, cutlass::half_t>>, Is_dropout=true, Is_causal=false, Is_even_N=true, Is_even_K=true, Return_softmax=false, Params=Flash_fwd_params]" 
(588): here
            instantiation of "void flash::compute_attn<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &) [with Kernel_traits=Flash_fwd_kernel_traits<256, 128, 64, 8, false, false, cutlass::half_t, Flash_kernel_traits<256, 128, 64, 8, cutlass::half_t>>, Is_dropout=true, Is_causal=false, Is_even_N=true, Is_even_K=true, Return_softmax=false, Params=Flash_fwd_params]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(15): here
            instantiation of "void flash_fwd_kernel<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax>(Flash_fwd_params) [with Kernel_traits=Flash_fwd_kernel_traits<256, 128, 64, 8, false, false, cutlass::half_t, Flash_kernel_traits<256, 128, 64, 8, cutlass::half_t>>, Is_dropout=true, Is_causal=false, Is_even_N=true, Is_even_K=true, Return_softmax=false]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(34): here
            instantiation of "void run_flash_fwd<Kernel_traits,Is_dropout,Is_causal>(Flash_fwd_params &, cudaStream_t) [with Kernel_traits=Flash_fwd_kernel_traits<256, 128, 64, 8, false, false, cutlass::half_t, Flash_kernel_traits<256, 128, 64, 8, cutlass::half_t>>, Is_dropout=true, Is_causal=false]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(236): here
            instantiation of "void run_mha_fwd_hdim256<T>(Flash_fwd_params &, cudaStream_t) [with T=cutlass::half_t]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_hdim256_fp16_sm80.cu(8): here

/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_kernel.h(275): warning: variable "tQrQ" was declared but never referenced
          detected during:
            instantiation of "void flash::compute_attn_1rowblock<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &, int, int, int) [with Kernel_traits=Flash_fwd_kernel_traits<256, 128, 64, 8, false, false, cutlass::half_t, Flash_kernel_traits<256, 128, 64, 8, cutlass::half_t>>, Is_dropout=true, Is_causal=false, Is_even_N=true, Is_even_K=true, Return_softmax=false, Params=Flash_fwd_params]" 
(588): here
            instantiation of "void flash::compute_attn<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &) [with Kernel_traits=Flash_fwd_kernel_traits<256, 128, 64, 8, false, false, cutlass::half_t, Flash_kernel_traits<256, 128, 64, 8, cutlass::half_t>>, Is_dropout=true, Is_causal=false, Is_even_N=true, Is_even_K=true, Return_softmax=false, Params=Flash_fwd_params]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(15): here
            instantiation of "void flash_fwd_kernel<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax>(Flash_fwd_params) [with Kernel_traits=Flash_fwd_kernel_traits<256, 128, 64, 8, false, false, cutlass::half_t, Flash_kernel_traits<256, 128, 64, 8, cutlass::half_t>>, Is_dropout=true, Is_causal=false, Is_even_N=true, Is_even_K=true, Return_softmax=false]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(34): here
            instantiation of "void run_flash_fwd<Kernel_traits,Is_dropout,Is_causal>(Flash_fwd_params &, cudaStream_t) [with Kernel_traits=Flash_fwd_kernel_traits<256, 128, 64, 8, false, false, cutlass::half_t, Flash_kernel_traits<256, 128, 64, 8, cutlass::half_t>>, Is_dropout=true, Is_causal=false]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(236): here
            instantiation of "void run_mha_fwd_hdim256<T>(Flash_fwd_params &, cudaStream_t) [with T=cutlass::half_t]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_hdim256_fp16_sm80.cu(8): here

/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_kernel.h(140): warning: variable "MMA_M" was declared but never referenced
          detected during:
            instantiation of "void flash::compute_attn_1rowblock<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &, int, int, int) [with Kernel_traits=Flash_fwd_kernel_traits<256, 128, 64, 8, false, false, cutlass::half_t, Flash_kernel_traits<256, 128, 64, 8, cutlass::half_t>>, Is_dropout=true, Is_causal=false, Is_even_N=true, Is_even_K=false, Return_softmax=true, Params=Flash_fwd_params]" 
(588): here
            instantiation of "void flash::compute_attn<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &) [with Kernel_traits=Flash_fwd_kernel_traits<256, 128, 64, 8, false, false, cutlass::half_t, Flash_kernel_traits<256, 128, 64, 8, cutlass::half_t>>, Is_dropout=true, Is_causal=false, Is_even_N=true, Is_even_K=false, Return_softmax=true, Params=Flash_fwd_params]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(15): here
            instantiation of "void flash_fwd_kernel<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax>(Flash_fwd_params) [with Kernel_traits=Flash_fwd_kernel_traits<256, 128, 64, 8, false, false, cutlass::half_t, Flash_kernel_traits<256, 128, 64, 8, cutlass::half_t>>, Is_dropout=true, Is_causal=false, Is_even_N=true, Is_even_K=false, Return_softmax=true]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(34): here
            instantiation of "void run_flash_fwd<Kernel_traits,Is_dropout,Is_causal>(Flash_fwd_params &, cudaStream_t) [with Kernel_traits=Flash_fwd_kernel_traits<256, 128, 64, 8, false, false, cutlass::half_t, Flash_kernel_traits<256, 128, 64, 8, cutlass::half_t>>, Is_dropout=true, Is_causal=false]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(236): here
            instantiation of "void run_mha_fwd_hdim256<T>(Flash_fwd_params &, cudaStream_t) [with T=cutlass::half_t]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_hdim256_fp16_sm80.cu(8): here

/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_kernel.h(275): warning: variable "tQrQ" was declared but never referenced
          detected during:
            instantiation of "void flash::compute_attn_1rowblock<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &, int, int, int) [with Kernel_traits=Flash_fwd_kernel_traits<256, 128, 64, 8, false, false, cutlass::half_t, Flash_kernel_traits<256, 128, 64, 8, cutlass::half_t>>, Is_dropout=true, Is_causal=false, Is_even_N=true, Is_even_K=false, Return_softmax=true, Params=Flash_fwd_params]" 
(588): here
            instantiation of "void flash::compute_attn<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &) [with Kernel_traits=Flash_fwd_kernel_traits<256, 128, 64, 8, false, false, cutlass::half_t, Flash_kernel_traits<256, 128, 64, 8, cutlass::half_t>>, Is_dropout=true, Is_causal=false, Is_even_N=true, Is_even_K=false, Return_softmax=true, Params=Flash_fwd_params]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(15): here
            instantiation of "void flash_fwd_kernel<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax>(Flash_fwd_params) [with Kernel_traits=Flash_fwd_kernel_traits<256, 128, 64, 8, false, false, cutlass::half_t, Flash_kernel_traits<256, 128, 64, 8, cutlass::half_t>>, Is_dropout=true, Is_causal=false, Is_even_N=true, Is_even_K=false, Return_softmax=true]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(34): here
            instantiation of "void run_flash_fwd<Kernel_traits,Is_dropout,Is_causal>(Flash_fwd_params &, cudaStream_t) [with Kernel_traits=Flash_fwd_kernel_traits<256, 128, 64, 8, false, false, cutlass::half_t, Flash_kernel_traits<256, 128, 64, 8, cutlass::half_t>>, Is_dropout=true, Is_causal=false]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(236): here
            instantiation of "void run_mha_fwd_hdim256<T>(Flash_fwd_params &, cudaStream_t) [with T=cutlass::half_t]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_hdim256_fp16_sm80.cu(8): here

/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_kernel.h(140): warning: variable "MMA_M" was declared but never referenced
          detected during:
            instantiation of "void flash::compute_attn_1rowblock<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &, int, int, int) [with Kernel_traits=Flash_fwd_kernel_traits<256, 128, 64, 8, false, false, cutlass::half_t, Flash_kernel_traits<256, 128, 64, 8, cutlass::half_t>>, Is_dropout=true, Is_causal=false, Is_even_N=true, Is_even_K=false, Return_softmax=false, Params=Flash_fwd_params]" 
(588): here
            instantiation of "void flash::compute_attn<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &) [with Kernel_traits=Flash_fwd_kernel_traits<256, 128, 64, 8, false, false, cutlass::half_t, Flash_kernel_traits<256, 128, 64, 8, cutlass::half_t>>, Is_dropout=true, Is_causal=false, Is_even_N=true, Is_even_K=false, Return_softmax=false, Params=Flash_fwd_params]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(15): here
            instantiation of "void flash_fwd_kernel<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax>(Flash_fwd_params) [with Kernel_traits=Flash_fwd_kernel_traits<256, 128, 64, 8, false, false, cutlass::half_t, Flash_kernel_traits<256, 128, 64, 8, cutlass::half_t>>, Is_dropout=true, Is_causal=false, Is_even_N=true, Is_even_K=false, Return_softmax=false]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(34): here
            instantiation of "void run_flash_fwd<Kernel_traits,Is_dropout,Is_causal>(Flash_fwd_params &, cudaStream_t) [with Kernel_traits=Flash_fwd_kernel_traits<256, 128, 64, 8, false, false, cutlass::half_t, Flash_kernel_traits<256, 128, 64, 8, cutlass::half_t>>, Is_dropout=true, Is_causal=false]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(236): here
            instantiation of "void run_mha_fwd_hdim256<T>(Flash_fwd_params &, cudaStream_t) [with T=cutlass::half_t]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_hdim256_fp16_sm80.cu(8): here

/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_kernel.h(275): warning: variable "tQrQ" was declared but never referenced
          detected during:
            instantiation of "void flash::compute_attn_1rowblock<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &, int, int, int) [with Kernel_traits=Flash_fwd_kernel_traits<256, 128, 64, 8, false, false, cutlass::half_t, Flash_kernel_traits<256, 128, 64, 8, cutlass::half_t>>, Is_dropout=true, Is_causal=false, Is_even_N=true, Is_even_K=false, Return_softmax=false, Params=Flash_fwd_params]" 
(588): here
            instantiation of "void flash::compute_attn<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &) [with Kernel_traits=Flash_fwd_kernel_traits<256, 128, 64, 8, false, false, cutlass::half_t, Flash_kernel_traits<256, 128, 64, 8, cutlass::half_t>>, Is_dropout=true, Is_causal=false, Is_even_N=true, Is_even_K=false, Return_softmax=false, Params=Flash_fwd_params]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(15): here
            instantiation of "void flash_fwd_kernel<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax>(Flash_fwd_params) [with Kernel_traits=Flash_fwd_kernel_traits<256, 128, 64, 8, false, false, cutlass::half_t, Flash_kernel_traits<256, 128, 64, 8, cutlass::half_t>>, Is_dropout=true, Is_causal=false, Is_even_N=true, Is_even_K=false, Return_softmax=false]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(34): here
            instantiation of "void run_flash_fwd<Kernel_traits,Is_dropout,Is_causal>(Flash_fwd_params &, cudaStream_t) [with Kernel_traits=Flash_fwd_kernel_traits<256, 128, 64, 8, false, false, cutlass::half_t, Flash_kernel_traits<256, 128, 64, 8, cutlass::half_t>>, Is_dropout=true, Is_causal=false]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(236): here
            instantiation of "void run_mha_fwd_hdim256<T>(Flash_fwd_params &, cudaStream_t) [with T=cutlass::half_t]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_hdim256_fp16_sm80.cu(8): here

/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_kernel.h(140): warning: variable "MMA_M" was declared but never referenced
          detected during:
            instantiation of "void flash::compute_attn_1rowblock<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &, int, int, int) [with Kernel_traits=Flash_fwd_kernel_traits<256, 128, 64, 8, false, false, cutlass::half_t, Flash_kernel_traits<256, 128, 64, 8, cutlass::half_t>>, Is_dropout=true, Is_causal=false, Is_even_N=false, Is_even_K=true, Return_softmax=true, Params=Flash_fwd_params]" 
(588): here
            instantiation of "void flash::compute_attn<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &) [with Kernel_traits=Flash_fwd_kernel_traits<256, 128, 64, 8, false, false, cutlass::half_t, Flash_kernel_traits<256, 128, 64, 8, cutlass::half_t>>, Is_dropout=true, Is_causal=false, Is_even_N=false, Is_even_K=true, Return_softmax=true, Params=Flash_fwd_params]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(15): here
            instantiation of "void flash_fwd_kernel<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax>(Flash_fwd_params) [with Kernel_traits=Flash_fwd_kernel_traits<256, 128, 64, 8, false, false, cutlass::half_t, Flash_kernel_traits<256, 128, 64, 8, cutlass::half_t>>, Is_dropout=true, Is_causal=false, Is_even_N=false, Is_even_K=true, Return_softmax=true]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(34): here
            instantiation of "void run_flash_fwd<Kernel_traits,Is_dropout,Is_causal>(Flash_fwd_params &, cudaStream_t) [with Kernel_traits=Flash_fwd_kernel_traits<256, 128, 64, 8, false, false, cutlass::half_t, Flash_kernel_traits<256, 128, 64, 8, cutlass::half_t>>, Is_dropout=true, Is_causal=false]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(236): here
            instantiation of "void run_mha_fwd_hdim256<T>(Flash_fwd_params &, cudaStream_t) [with T=cutlass::half_t]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_hdim256_fp16_sm80.cu(8): here

/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_kernel.h(275): warning: variable "tQrQ" was declared but never referenced
          detected during:
            instantiation of "void flash::compute_attn_1rowblock<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &, int, int, int) [with Kernel_traits=Flash_fwd_kernel_traits<256, 128, 64, 8, false, false, cutlass::half_t, Flash_kernel_traits<256, 128, 64, 8, cutlass::half_t>>, Is_dropout=true, Is_causal=false, Is_even_N=false, Is_even_K=true, Return_softmax=true, Params=Flash_fwd_params]" 
(588): here
            instantiation of "void flash::compute_attn<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &) [with Kernel_traits=Flash_fwd_kernel_traits<256, 128, 64, 8, false, false, cutlass::half_t, Flash_kernel_traits<256, 128, 64, 8, cutlass::half_t>>, Is_dropout=true, Is_causal=false, Is_even_N=false, Is_even_K=true, Return_softmax=true, Params=Flash_fwd_params]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(15): here
            instantiation of "void flash_fwd_kernel<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax>(Flash_fwd_params) [with Kernel_traits=Flash_fwd_kernel_traits<256, 128, 64, 8, false, false, cutlass::half_t, Flash_kernel_traits<256, 128, 64, 8, cutlass::half_t>>, Is_dropout=true, Is_causal=false, Is_even_N=false, Is_even_K=true, Return_softmax=true]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(34): here
            instantiation of "void run_flash_fwd<Kernel_traits,Is_dropout,Is_causal>(Flash_fwd_params &, cudaStream_t) [with Kernel_traits=Flash_fwd_kernel_traits<256, 128, 64, 8, false, false, cutlass::half_t, Flash_kernel_traits<256, 128, 64, 8, cutlass::half_t>>, Is_dropout=true, Is_causal=false]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(236): here
            instantiation of "void run_mha_fwd_hdim256<T>(Flash_fwd_params &, cudaStream_t) [with T=cutlass::half_t]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_hdim256_fp16_sm80.cu(8): here

/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_kernel.h(140): warning: variable "MMA_M" was declared but never referenced
          detected during:
            instantiation of "void flash::compute_attn_1rowblock<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &, int, int, int) [with Kernel_traits=Flash_fwd_kernel_traits<256, 128, 64, 8, false, false, cutlass::half_t, Flash_kernel_traits<256, 128, 64, 8, cutlass::half_t>>, Is_dropout=true, Is_causal=false, Is_even_N=false, Is_even_K=true, Return_softmax=false, Params=Flash_fwd_params]" 
(588): here
            instantiation of "void flash::compute_attn<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &) [with Kernel_traits=Flash_fwd_kernel_traits<256, 128, 64, 8, false, false, cutlass::half_t, Flash_kernel_traits<256, 128, 64, 8, cutlass::half_t>>, Is_dropout=true, Is_causal=false, Is_even_N=false, Is_even_K=true, Return_softmax=false, Params=Flash_fwd_params]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(15): here
            instantiation of "void flash_fwd_kernel<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax>(Flash_fwd_params) [with Kernel_traits=Flash_fwd_kernel_traits<256, 128, 64, 8, false, false, cutlass::half_t, Flash_kernel_traits<256, 128, 64, 8, cutlass::half_t>>, Is_dropout=true, Is_causal=false, Is_even_N=false, Is_even_K=true, Return_softmax=false]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(34): here
            instantiation of "void run_flash_fwd<Kernel_traits,Is_dropout,Is_causal>(Flash_fwd_params &, cudaStream_t) [with Kernel_traits=Flash_fwd_kernel_traits<256, 128, 64, 8, false, false, cutlass::half_t, Flash_kernel_traits<256, 128, 64, 8, cutlass::half_t>>, Is_dropout=true, Is_causal=false]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(236): here
            instantiation of "void run_mha_fwd_hdim256<T>(Flash_fwd_params &, cudaStream_t) [with T=cutlass::half_t]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_hdim256_fp16_sm80.cu(8): here

/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_kernel.h(275): warning: variable "tQrQ" was declared but never referenced
          detected during:
            instantiation of "void flash::compute_attn_1rowblock<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &, int, int, int) [with Kernel_traits=Flash_fwd_kernel_traits<256, 128, 64, 8, false, false, cutlass::half_t, Flash_kernel_traits<256, 128, 64, 8, cutlass::half_t>>, Is_dropout=true, Is_causal=false, Is_even_N=false, Is_even_K=true, Return_softmax=false, Params=Flash_fwd_params]" 
(588): here
            instantiation of "void flash::compute_attn<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &) [with Kernel_traits=Flash_fwd_kernel_traits<256, 128, 64, 8, false, false, cutlass::half_t, Flash_kernel_traits<256, 128, 64, 8, cutlass::half_t>>, Is_dropout=true, Is_causal=false, Is_even_N=false, Is_even_K=true, Return_softmax=false, Params=Flash_fwd_params]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(15): here
            instantiation of "void flash_fwd_kernel<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax>(Flash_fwd_params) [with Kernel_traits=Flash_fwd_kernel_traits<256, 128, 64, 8, false, false, cutlass::half_t, Flash_kernel_traits<256, 128, 64, 8, cutlass::half_t>>, Is_dropout=true, Is_causal=false, Is_even_N=false, Is_even_K=true, Return_softmax=false]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(34): here
            instantiation of "void run_flash_fwd<Kernel_traits,Is_dropout,Is_causal>(Flash_fwd_params &, cudaStream_t) [with Kernel_traits=Flash_fwd_kernel_traits<256, 128, 64, 8, false, false, cutlass::half_t, Flash_kernel_traits<256, 128, 64, 8, cutlass::half_t>>, Is_dropout=true, Is_causal=false]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(236): here
            instantiation of "void run_mha_fwd_hdim256<T>(Flash_fwd_params &, cudaStream_t) [with T=cutlass::half_t]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_hdim256_fp16_sm80.cu(8): here

/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_kernel.h(140): warning: variable "MMA_M" was declared but never referenced
          detected during:
            instantiation of "void flash::compute_attn_1rowblock<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &, int, int, int) [with Kernel_traits=Flash_fwd_kernel_traits<256, 128, 64, 8, false, false, cutlass::half_t, Flash_kernel_traits<256, 128, 64, 8, cutlass::half_t>>, Is_dropout=true, Is_causal=false, Is_even_N=false, Is_even_K=false, Return_softmax=true, Params=Flash_fwd_params]" 
(588): here
            instantiation of "void flash::compute_attn<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &) [with Kernel_traits=Flash_fwd_kernel_traits<256, 128, 64, 8, false, false, cutlass::half_t, Flash_kernel_traits<256, 128, 64, 8, cutlass::half_t>>, Is_dropout=true, Is_causal=false, Is_even_N=false, Is_even_K=false, Return_softmax=true, Params=Flash_fwd_params]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(15): here
            instantiation of "void flash_fwd_kernel<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax>(Flash_fwd_params) [with Kernel_traits=Flash_fwd_kernel_traits<256, 128, 64, 8, false, false, cutlass::half_t, Flash_kernel_traits<256, 128, 64, 8, cutlass::half_t>>, Is_dropout=true, Is_causal=false, Is_even_N=false, Is_even_K=false, Return_softmax=true]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(34): here
            instantiation of "void run_flash_fwd<Kernel_traits,Is_dropout,Is_causal>(Flash_fwd_params &, cudaStream_t) [with Kernel_traits=Flash_fwd_kernel_traits<256, 128, 64, 8, false, false, cutlass::half_t, Flash_kernel_traits<256, 128, 64, 8, cutlass::half_t>>, Is_dropout=true, Is_causal=false]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(236): here
            instantiation of "void run_mha_fwd_hdim256<T>(Flash_fwd_params &, cudaStream_t) [with T=cutlass::half_t]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_hdim256_fp16_sm80.cu(8): here

/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_kernel.h(275): warning: variable "tQrQ" was declared but never referenced
          detected during:
            instantiation of "void flash::compute_attn_1rowblock<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &, int, int, int) [with Kernel_traits=Flash_fwd_kernel_traits<256, 128, 64, 8, false, false, cutlass::half_t, Flash_kernel_traits<256, 128, 64, 8, cutlass::half_t>>, Is_dropout=true, Is_causal=false, Is_even_N=false, Is_even_K=false, Return_softmax=true, Params=Flash_fwd_params]" 
(588): here
            instantiation of "void flash::compute_attn<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &) [with Kernel_traits=Flash_fwd_kernel_traits<256, 128, 64, 8, false, false, cutlass::half_t, Flash_kernel_traits<256, 128, 64, 8, cutlass::half_t>>, Is_dropout=true, Is_causal=false, Is_even_N=false, Is_even_K=false, Return_softmax=true, Params=Flash_fwd_params]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(15): here
            instantiation of "void flash_fwd_kernel<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax>(Flash_fwd_params) [with Kernel_traits=Flash_fwd_kernel_traits<256, 128, 64, 8, false, false, cutlass::half_t, Flash_kernel_traits<256, 128, 64, 8, cutlass::half_t>>, Is_dropout=true, Is_causal=false, Is_even_N=false, Is_even_K=false, Return_softmax=true]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(34): here
            instantiation of "void run_flash_fwd<Kernel_traits,Is_dropout,Is_causal>(Flash_fwd_params &, cudaStream_t) [with Kernel_traits=Flash_fwd_kernel_traits<256, 128, 64, 8, false, false, cutlass::half_t, Flash_kernel_traits<256, 128, 64, 8, cutlass::half_t>>, Is_dropout=true, Is_causal=false]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(236): here
            instantiation of "void run_mha_fwd_hdim256<T>(Flash_fwd_params &, cudaStream_t) [with T=cutlass::half_t]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_hdim256_fp16_sm80.cu(8): here

/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_kernel.h(140): warning: variable "MMA_M" was declared but never referenced
          detected during:
            instantiation of "void flash::compute_attn_1rowblock<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &, int, int, int) [with Kernel_traits=Flash_fwd_kernel_traits<256, 128, 64, 8, false, false, cutlass::half_t, Flash_kernel_traits<256, 128, 64, 8, cutlass::half_t>>, Is_dropout=true, Is_causal=false, Is_even_N=false, Is_even_K=false, Return_softmax=false, Params=Flash_fwd_params]" 
(588): here
            instantiation of "void flash::compute_attn<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &) [with Kernel_traits=Flash_fwd_kernel_traits<256, 128, 64, 8, false, false, cutlass::half_t, Flash_kernel_traits<256, 128, 64, 8, cutlass::half_t>>, Is_dropout=true, Is_causal=false, Is_even_N=false, Is_even_K=false, Return_softmax=false, Params=Flash_fwd_params]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(15): here
            instantiation of "void flash_fwd_kernel<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax>(Flash_fwd_params) [with Kernel_traits=Flash_fwd_kernel_traits<256, 128, 64, 8, false, false, cutlass::half_t, Flash_kernel_traits<256, 128, 64, 8, cutlass::half_t>>, Is_dropout=true, Is_causal=false, Is_even_N=false, Is_even_K=false, Return_softmax=false]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(34): here
            instantiation of "void run_flash_fwd<Kernel_traits,Is_dropout,Is_causal>(Flash_fwd_params &, cudaStream_t) [with Kernel_traits=Flash_fwd_kernel_traits<256, 128, 64, 8, false, false, cutlass::half_t, Flash_kernel_traits<256, 128, 64, 8, cutlass::half_t>>, Is_dropout=true, Is_causal=false]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(236): here
            instantiation of "void run_mha_fwd_hdim256<T>(Flash_fwd_params &, cudaStream_t) [with T=cutlass::half_t]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_hdim256_fp16_sm80.cu(8): here

/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_kernel.h(275): warning: variable "tQrQ" was declared but never referenced
          detected during:
            instantiation of "void flash::compute_attn_1rowblock<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &, int, int, int) [with Kernel_traits=Flash_fwd_kernel_traits<256, 128, 64, 8, false, false, cutlass::half_t, Flash_kernel_traits<256, 128, 64, 8, cutlass::half_t>>, Is_dropout=true, Is_causal=false, Is_even_N=false, Is_even_K=false, Return_softmax=false, Params=Flash_fwd_params]" 
(588): here
            instantiation of "void flash::compute_attn<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &) [with Kernel_traits=Flash_fwd_kernel_traits<256, 128, 64, 8, false, false, cutlass::half_t, Flash_kernel_traits<256, 128, 64, 8, cutlass::half_t>>, Is_dropout=true, Is_causal=false, Is_even_N=false, Is_even_K=false, Return_softmax=false, Params=Flash_fwd_params]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(15): here
            instantiation of "void flash_fwd_kernel<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax>(Flash_fwd_params) [with Kernel_traits=Flash_fwd_kernel_traits<256, 128, 64, 8, false, false, cutlass::half_t, Flash_kernel_traits<256, 128, 64, 8, cutlass::half_t>>, Is_dropout=true, Is_causal=false, Is_even_N=false, Is_even_K=false, Return_softmax=false]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(34): here
            instantiation of "void run_flash_fwd<Kernel_traits,Is_dropout,Is_causal>(Flash_fwd_params &, cudaStream_t) [with Kernel_traits=Flash_fwd_kernel_traits<256, 128, 64, 8, false, false, cutlass::half_t, Flash_kernel_traits<256, 128, 64, 8, cutlass::half_t>>, Is_dropout=true, Is_causal=false]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(236): here
            instantiation of "void run_mha_fwd_hdim256<T>(Flash_fwd_params &, cudaStream_t) [with T=cutlass::half_t]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_hdim256_fp16_sm80.cu(8): here

/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_kernel.h(140): warning: variable "MMA_M" was declared but never referenced
          detected during:
            instantiation of "void flash::compute_attn_1rowblock<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &, int, int, int) [with Kernel_traits=Flash_fwd_kernel_traits<256, 64, 64, 4, false, false, cutlass::half_t, Flash_kernel_traits<256, 64, 64, 4, cutlass::half_t>>, Is_dropout=true, Is_causal=false, Is_even_N=true, Is_even_K=true, Return_softmax=true, Params=Flash_fwd_params]" 
(588): here
            instantiation of "void flash::compute_attn<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &) [with Kernel_traits=Flash_fwd_kernel_traits<256, 64, 64, 4, false, false, cutlass::half_t, Flash_kernel_traits<256, 64, 64, 4, cutlass::half_t>>, Is_dropout=true, Is_causal=false, Is_even_N=true, Is_even_K=true, Return_softmax=true, Params=Flash_fwd_params]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(15): here
            instantiation of "void flash_fwd_kernel<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax>(Flash_fwd_params) [with Kernel_traits=Flash_fwd_kernel_traits<256, 64, 64, 4, false, false, cutlass::half_t, Flash_kernel_traits<256, 64, 64, 4, cutlass::half_t>>, Is_dropout=true, Is_causal=false, Is_even_N=true, Is_even_K=true, Return_softmax=true]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(34): here
            instantiation of "void run_flash_fwd<Kernel_traits,Is_dropout,Is_causal>(Flash_fwd_params &, cudaStream_t) [with Kernel_traits=Flash_fwd_kernel_traits<256, 64, 64, 4, false, false, cutlass::half_t, Flash_kernel_traits<256, 64, 64, 4, cutlass::half_t>>, Is_dropout=true, Is_causal=false]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(236): here
            instantiation of "void run_mha_fwd_hdim256<T>(Flash_fwd_params &, cudaStream_t) [with T=cutlass::half_t]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_hdim256_fp16_sm80.cu(8): here

/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_kernel.h(275): warning: variable "tQrQ" was declared but never referenced
          detected during:
            instantiation of "void flash::compute_attn_1rowblock<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &, int, int, int) [with Kernel_traits=Flash_fwd_kernel_traits<256, 64, 64, 4, false, false, cutlass::half_t, Flash_kernel_traits<256, 64, 64, 4, cutlass::half_t>>, Is_dropout=true, Is_causal=false, Is_even_N=true, Is_even_K=true, Return_softmax=true, Params=Flash_fwd_params]" 
(588): here
            instantiation of "void flash::compute_attn<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &) [with Kernel_traits=Flash_fwd_kernel_traits<256, 64, 64, 4, false, false, cutlass::half_t, Flash_kernel_traits<256, 64, 64, 4, cutlass::half_t>>, Is_dropout=true, Is_causal=false, Is_even_N=true, Is_even_K=true, Return_softmax=true, Params=Flash_fwd_params]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(15): here
            instantiation of "void flash_fwd_kernel<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax>(Flash_fwd_params) [with Kernel_traits=Flash_fwd_kernel_traits<256, 64, 64, 4, false, false, cutlass::half_t, Flash_kernel_traits<256, 64, 64, 4, cutlass::half_t>>, Is_dropout=true, Is_causal=false, Is_even_N=true, Is_even_K=true, Return_softmax=true]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(34): here
            instantiation of "void run_flash_fwd<Kernel_traits,Is_dropout,Is_causal>(Flash_fwd_params &, cudaStream_t) [with Kernel_traits=Flash_fwd_kernel_traits<256, 64, 64, 4, false, false, cutlass::half_t, Flash_kernel_traits<256, 64, 64, 4, cutlass::half_t>>, Is_dropout=true, Is_causal=false]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(236): here
            instantiation of "void run_mha_fwd_hdim256<T>(Flash_fwd_params &, cudaStream_t) [with T=cutlass::half_t]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_hdim256_fp16_sm80.cu(8): here

/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_kernel.h(140): warning: variable "MMA_M" was declared but never referenced
          detected during:
            instantiation of "void flash::compute_attn_1rowblock<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &, int, int, int) [with Kernel_traits=Flash_fwd_kernel_traits<256, 64, 64, 4, false, false, cutlass::half_t, Flash_kernel_traits<256, 64, 64, 4, cutlass::half_t>>, Is_dropout=true, Is_causal=false, Is_even_N=true, Is_even_K=true, Return_softmax=false, Params=Flash_fwd_params]" 
(588): here
            instantiation of "void flash::compute_attn<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &) [with Kernel_traits=Flash_fwd_kernel_traits<256, 64, 64, 4, false, false, cutlass::half_t, Flash_kernel_traits<256, 64, 64, 4, cutlass::half_t>>, Is_dropout=true, Is_causal=false, Is_even_N=true, Is_even_K=true, Return_softmax=false, Params=Flash_fwd_params]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(15): here
            instantiation of "void flash_fwd_kernel<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax>(Flash_fwd_params) [with Kernel_traits=Flash_fwd_kernel_traits<256, 64, 64, 4, false, false, cutlass::half_t, Flash_kernel_traits<256, 64, 64, 4, cutlass::half_t>>, Is_dropout=true, Is_causal=false, Is_even_N=true, Is_even_K=true, Return_softmax=false]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(34): here
            instantiation of "void run_flash_fwd<Kernel_traits,Is_dropout,Is_causal>(Flash_fwd_params &, cudaStream_t) [with Kernel_traits=Flash_fwd_kernel_traits<256, 64, 64, 4, false, false, cutlass::half_t, Flash_kernel_traits<256, 64, 64, 4, cutlass::half_t>>, Is_dropout=true, Is_causal=false]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(236): here
            instantiation of "void run_mha_fwd_hdim256<T>(Flash_fwd_params &, cudaStream_t) [with T=cutlass::half_t]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_hdim256_fp16_sm80.cu(8): here

/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_kernel.h(275): warning: variable "tQrQ" was declared but never referenced
          detected during:
            instantiation of "void flash::compute_attn_1rowblock<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &, int, int, int) [with Kernel_traits=Flash_fwd_kernel_traits<256, 64, 64, 4, false, false, cutlass::half_t, Flash_kernel_traits<256, 64, 64, 4, cutlass::half_t>>, Is_dropout=true, Is_causal=false, Is_even_N=true, Is_even_K=true, Return_softmax=false, Params=Flash_fwd_params]" 
(588): here
            instantiation of "void flash::compute_attn<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &) [with Kernel_traits=Flash_fwd_kernel_traits<256, 64, 64, 4, false, false, cutlass::half_t, Flash_kernel_traits<256, 64, 64, 4, cutlass::half_t>>, Is_dropout=true, Is_causal=false, Is_even_N=true, Is_even_K=true, Return_softmax=false, Params=Flash_fwd_params]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(15): here
            instantiation of "void flash_fwd_kernel<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax>(Flash_fwd_params) [with Kernel_traits=Flash_fwd_kernel_traits<256, 64, 64, 4, false, false, cutlass::half_t, Flash_kernel_traits<256, 64, 64, 4, cutlass::half_t>>, Is_dropout=true, Is_causal=false, Is_even_N=true, Is_even_K=true, Return_softmax=false]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(34): here
            instantiation of "void run_flash_fwd<Kernel_traits,Is_dropout,Is_causal>(Flash_fwd_params &, cudaStream_t) [with Kernel_traits=Flash_fwd_kernel_traits<256, 64, 64, 4, false, false, cutlass::half_t, Flash_kernel_traits<256, 64, 64, 4, cutlass::half_t>>, Is_dropout=true, Is_causal=false]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(236): here
            instantiation of "void run_mha_fwd_hdim256<T>(Flash_fwd_params &, cudaStream_t) [with T=cutlass::half_t]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_hdim256_fp16_sm80.cu(8): here

/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_kernel.h(140): warning: variable "MMA_M" was declared but never referenced
          detected during:
            instantiation of "void flash::compute_attn_1rowblock<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &, int, int, int) [with Kernel_traits=Flash_fwd_kernel_traits<256, 64, 64, 4, false, false, cutlass::half_t, Flash_kernel_traits<256, 64, 64, 4, cutlass::half_t>>, Is_dropout=true, Is_causal=false, Is_even_N=true, Is_even_K=false, Return_softmax=true, Params=Flash_fwd_params]" 
(588): here
            instantiation of "void flash::compute_attn<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &) [with Kernel_traits=Flash_fwd_kernel_traits<256, 64, 64, 4, false, false, cutlass::half_t, Flash_kernel_traits<256, 64, 64, 4, cutlass::half_t>>, Is_dropout=true, Is_causal=false, Is_even_N=true, Is_even_K=false, Return_softmax=true, Params=Flash_fwd_params]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(15): here
            instantiation of "void flash_fwd_kernel<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax>(Flash_fwd_params) [with Kernel_traits=Flash_fwd_kernel_traits<256, 64, 64, 4, false, false, cutlass::half_t, Flash_kernel_traits<256, 64, 64, 4, cutlass::half_t>>, Is_dropout=true, Is_causal=false, Is_even_N=true, Is_even_K=false, Return_softmax=true]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(34): here
            instantiation of "void run_flash_fwd<Kernel_traits,Is_dropout,Is_causal>(Flash_fwd_params &, cudaStream_t) [with Kernel_traits=Flash_fwd_kernel_traits<256, 64, 64, 4, false, false, cutlass::half_t, Flash_kernel_traits<256, 64, 64, 4, cutlass::half_t>>, Is_dropout=true, Is_causal=false]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(236): here
            instantiation of "void run_mha_fwd_hdim256<T>(Flash_fwd_params &, cudaStream_t) [with T=cutlass::half_t]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_hdim256_fp16_sm80.cu(8): here

/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_kernel.h(275): warning: variable "tQrQ" was declared but never referenced
          detected during:
            instantiation of "void flash::compute_attn_1rowblock<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &, int, int, int) [with Kernel_traits=Flash_fwd_kernel_traits<256, 64, 64, 4, false, false, cutlass::half_t, Flash_kernel_traits<256, 64, 64, 4, cutlass::half_t>>, Is_dropout=true, Is_causal=false, Is_even_N=true, Is_even_K=false, Return_softmax=true, Params=Flash_fwd_params]" 
(588): here
            instantiation of "void flash::compute_attn<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &) [with Kernel_traits=Flash_fwd_kernel_traits<256, 64, 64, 4, false, false, cutlass::half_t, Flash_kernel_traits<256, 64, 64, 4, cutlass::half_t>>, Is_dropout=true, Is_causal=false, Is_even_N=true, Is_even_K=false, Return_softmax=true, Params=Flash_fwd_params]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(15): here
            instantiation of "void flash_fwd_kernel<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax>(Flash_fwd_params) [with Kernel_traits=Flash_fwd_kernel_traits<256, 64, 64, 4, false, false, cutlass::half_t, Flash_kernel_traits<256, 64, 64, 4, cutlass::half_t>>, Is_dropout=true, Is_causal=false, Is_even_N=true, Is_even_K=false, Return_softmax=true]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(34): here
            instantiation of "void run_flash_fwd<Kernel_traits,Is_dropout,Is_causal>(Flash_fwd_params &, cudaStream_t) [with Kernel_traits=Flash_fwd_kernel_traits<256, 64, 64, 4, false, false, cutlass::half_t, Flash_kernel_traits<256, 64, 64, 4, cutlass::half_t>>, Is_dropout=true, Is_causal=false]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(236): here
            instantiation of "void run_mha_fwd_hdim256<T>(Flash_fwd_params &, cudaStream_t) [with T=cutlass::half_t]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_hdim256_fp16_sm80.cu(8): here

/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_kernel.h(140): warning: variable "MMA_M" was declared but never referenced
          detected during:
            instantiation of "void flash::compute_attn_1rowblock<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &, int, int, int) [with Kernel_traits=Flash_fwd_kernel_traits<256, 64, 64, 4, false, false, cutlass::half_t, Flash_kernel_traits<256, 64, 64, 4, cutlass::half_t>>, Is_dropout=true, Is_causal=false, Is_even_N=true, Is_even_K=false, Return_softmax=false, Params=Flash_fwd_params]" 
(588): here
            instantiation of "void flash::compute_attn<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &) [with Kernel_traits=Flash_fwd_kernel_traits<256, 64, 64, 4, false, false, cutlass::half_t, Flash_kernel_traits<256, 64, 64, 4, cutlass::half_t>>, Is_dropout=true, Is_causal=false, Is_even_N=true, Is_even_K=false, Return_softmax=false, Params=Flash_fwd_params]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(15): here
            instantiation of "void flash_fwd_kernel<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax>(Flash_fwd_params) [with Kernel_traits=Flash_fwd_kernel_traits<256, 64, 64, 4, false, false, cutlass::half_t, Flash_kernel_traits<256, 64, 64, 4, cutlass::half_t>>, Is_dropout=true, Is_causal=false, Is_even_N=true, Is_even_K=false, Return_softmax=false]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(34): here
            instantiation of "void run_flash_fwd<Kernel_traits,Is_dropout,Is_causal>(Flash_fwd_params &, cudaStream_t) [with Kernel_traits=Flash_fwd_kernel_traits<256, 64, 64, 4, false, false, cutlass::half_t, Flash_kernel_traits<256, 64, 64, 4, cutlass::half_t>>, Is_dropout=true, Is_causal=false]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(236): here
            instantiation of "void run_mha_fwd_hdim256<T>(Flash_fwd_params &, cudaStream_t) [with T=cutlass::half_t]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_hdim256_fp16_sm80.cu(8): here

/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_kernel.h(275): warning: variable "tQrQ" was declared but never referenced
          detected during:
            instantiation of "void flash::compute_attn_1rowblock<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &, int, int, int) [with Kernel_traits=Flash_fwd_kernel_traits<256, 64, 64, 4, false, false, cutlass::half_t, Flash_kernel_traits<256, 64, 64, 4, cutlass::half_t>>, Is_dropout=true, Is_causal=false, Is_even_N=true, Is_even_K=false, Return_softmax=false, Params=Flash_fwd_params]" 
(588): here
            instantiation of "void flash::compute_attn<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &) [with Kernel_traits=Flash_fwd_kernel_traits<256, 64, 64, 4, false, false, cutlass::half_t, Flash_kernel_traits<256, 64, 64, 4, cutlass::half_t>>, Is_dropout=true, Is_causal=false, Is_even_N=true, Is_even_K=false, Return_softmax=false, Params=Flash_fwd_params]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(15): here
            instantiation of "void flash_fwd_kernel<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax>(Flash_fwd_params) [with Kernel_traits=Flash_fwd_kernel_traits<256, 64, 64, 4, false, false, cutlass::half_t, Flash_kernel_traits<256, 64, 64, 4, cutlass::half_t>>, Is_dropout=true, Is_causal=false, Is_even_N=true, Is_even_K=false, Return_softmax=false]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(34): here
            instantiation of "void run_flash_fwd<Kernel_traits,Is_dropout,Is_causal>(Flash_fwd_params &, cudaStream_t) [with Kernel_traits=Flash_fwd_kernel_traits<256, 64, 64, 4, false, false, cutlass::half_t, Flash_kernel_traits<256, 64, 64, 4, cutlass::half_t>>, Is_dropout=true, Is_causal=false]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(236): here
            instantiation of "void run_mha_fwd_hdim256<T>(Flash_fwd_params &, cudaStream_t) [with T=cutlass::half_t]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_hdim256_fp16_sm80.cu(8): here

/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_kernel.h(140): warning: variable "MMA_M" was declared but never referenced
          detected during:
            instantiation of "void flash::compute_attn_1rowblock<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &, int, int, int) [with Kernel_traits=Flash_fwd_kernel_traits<256, 64, 64, 4, false, false, cutlass::half_t, Flash_kernel_traits<256, 64, 64, 4, cutlass::half_t>>, Is_dropout=true, Is_causal=false, Is_even_N=false, Is_even_K=true, Return_softmax=true, Params=Flash_fwd_params]" 
(588): here
            instantiation of "void flash::compute_attn<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &) [with Kernel_traits=Flash_fwd_kernel_traits<256, 64, 64, 4, false, false, cutlass::half_t, Flash_kernel_traits<256, 64, 64, 4, cutlass::half_t>>, Is_dropout=true, Is_causal=false, Is_even_N=false, Is_even_K=true, Return_softmax=true, Params=Flash_fwd_params]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(15): here
            instantiation of "void flash_fwd_kernel<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax>(Flash_fwd_params) [with Kernel_traits=Flash_fwd_kernel_traits<256, 64, 64, 4, false, false, cutlass::half_t, Flash_kernel_traits<256, 64, 64, 4, cutlass::half_t>>, Is_dropout=true, Is_causal=false, Is_even_N=false, Is_even_K=true, Return_softmax=true]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(34): here
            instantiation of "void run_flash_fwd<Kernel_traits,Is_dropout,Is_causal>(Flash_fwd_params &, cudaStream_t) [with Kernel_traits=Flash_fwd_kernel_traits<256, 64, 64, 4, false, false, cutlass::half_t, Flash_kernel_traits<256, 64, 64, 4, cutlass::half_t>>, Is_dropout=true, Is_causal=false]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(236): here
            instantiation of "void run_mha_fwd_hdim256<T>(Flash_fwd_params &, cudaStream_t) [with T=cutlass::half_t]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_hdim256_fp16_sm80.cu(8): here

/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_kernel.h(275): warning: variable "tQrQ" was declared but never referenced
          detected during:
            instantiation of "void flash::compute_attn_1rowblock<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &, int, int, int) [with Kernel_traits=Flash_fwd_kernel_traits<256, 64, 64, 4, false, false, cutlass::half_t, Flash_kernel_traits<256, 64, 64, 4, cutlass::half_t>>, Is_dropout=true, Is_causal=false, Is_even_N=false, Is_even_K=true, Return_softmax=true, Params=Flash_fwd_params]" 
(588): here
            instantiation of "void flash::compute_attn<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &) [with Kernel_traits=Flash_fwd_kernel_traits<256, 64, 64, 4, false, false, cutlass::half_t, Flash_kernel_traits<256, 64, 64, 4, cutlass::half_t>>, Is_dropout=true, Is_causal=false, Is_even_N=false, Is_even_K=true, Return_softmax=true, Params=Flash_fwd_params]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(15): here
            instantiation of "void flash_fwd_kernel<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax>(Flash_fwd_params) [with Kernel_traits=Flash_fwd_kernel_traits<256, 64, 64, 4, false, false, cutlass::half_t, Flash_kernel_traits<256, 64, 64, 4, cutlass::half_t>>, Is_dropout=true, Is_causal=false, Is_even_N=false, Is_even_K=true, Return_softmax=true]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(34): here
            instantiation of "void run_flash_fwd<Kernel_traits,Is_dropout,Is_causal>(Flash_fwd_params &, cudaStream_t) [with Kernel_traits=Flash_fwd_kernel_traits<256, 64, 64, 4, false, false, cutlass::half_t, Flash_kernel_traits<256, 64, 64, 4, cutlass::half_t>>, Is_dropout=true, Is_causal=false]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(236): here
            instantiation of "void run_mha_fwd_hdim256<T>(Flash_fwd_params &, cudaStream_t) [with T=cutlass::half_t]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_hdim256_fp16_sm80.cu(8): here

/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_kernel.h(140): warning: variable "MMA_M" was declared but never referenced
          detected during:
            instantiation of "void flash::compute_attn_1rowblock<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &, int, int, int) [with Kernel_traits=Flash_fwd_kernel_traits<256, 64, 64, 4, false, false, cutlass::half_t, Flash_kernel_traits<256, 64, 64, 4, cutlass::half_t>>, Is_dropout=true, Is_causal=false, Is_even_N=false, Is_even_K=true, Return_softmax=false, Params=Flash_fwd_params]" 
(588): here
            instantiation of "void flash::compute_attn<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &) [with Kernel_traits=Flash_fwd_kernel_traits<256, 64, 64, 4, false, false, cutlass::half_t, Flash_kernel_traits<256, 64, 64, 4, cutlass::half_t>>, Is_dropout=true, Is_causal=false, Is_even_N=false, Is_even_K=true, Return_softmax=false, Params=Flash_fwd_params]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(15): here
            instantiation of "void flash_fwd_kernel<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax>(Flash_fwd_params) [with Kernel_traits=Flash_fwd_kernel_traits<256, 64, 64, 4, false, false, cutlass::half_t, Flash_kernel_traits<256, 64, 64, 4, cutlass::half_t>>, Is_dropout=true, Is_causal=false, Is_even_N=false, Is_even_K=true, Return_softmax=false]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(34): here
            instantiation of "void run_flash_fwd<Kernel_traits,Is_dropout,Is_causal>(Flash_fwd_params &, cudaStream_t) [with Kernel_traits=Flash_fwd_kernel_traits<256, 64, 64, 4, false, false, cutlass::half_t, Flash_kernel_traits<256, 64, 64, 4, cutlass::half_t>>, Is_dropout=true, Is_causal=false]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(236): here
            instantiation of "void run_mha_fwd_hdim256<T>(Flash_fwd_params &, cudaStream_t) [with T=cutlass::half_t]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_hdim256_fp16_sm80.cu(8): here

/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_kernel.h(275): warning: variable "tQrQ" was declared but never referenced
          detected during:
            instantiation of "void flash::compute_attn_1rowblock<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &, int, int, int) [with Kernel_traits=Flash_fwd_kernel_traits<256, 64, 64, 4, false, false, cutlass::half_t, Flash_kernel_traits<256, 64, 64, 4, cutlass::half_t>>, Is_dropout=true, Is_causal=false, Is_even_N=false, Is_even_K=true, Return_softmax=false, Params=Flash_fwd_params]" 
(588): here
            instantiation of "void flash::compute_attn<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &) [with Kernel_traits=Flash_fwd_kernel_traits<256, 64, 64, 4, false, false, cutlass::half_t, Flash_kernel_traits<256, 64, 64, 4, cutlass::half_t>>, Is_dropout=true, Is_causal=false, Is_even_N=false, Is_even_K=true, Return_softmax=false, Params=Flash_fwd_params]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(15): here
            instantiation of "void flash_fwd_kernel<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax>(Flash_fwd_params) [with Kernel_traits=Flash_fwd_kernel_traits<256, 64, 64, 4, false, false, cutlass::half_t, Flash_kernel_traits<256, 64, 64, 4, cutlass::half_t>>, Is_dropout=true, Is_causal=false, Is_even_N=false, Is_even_K=true, Return_softmax=false]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(34): here
            instantiation of "void run_flash_fwd<Kernel_traits,Is_dropout,Is_causal>(Flash_fwd_params &, cudaStream_t) [with Kernel_traits=Flash_fwd_kernel_traits<256, 64, 64, 4, false, false, cutlass::half_t, Flash_kernel_traits<256, 64, 64, 4, cutlass::half_t>>, Is_dropout=true, Is_causal=false]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(236): here
            instantiation of "void run_mha_fwd_hdim256<T>(Flash_fwd_params &, cudaStream_t) [with T=cutlass::half_t]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_hdim256_fp16_sm80.cu(8): here

/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_kernel.h(140): warning: variable "MMA_M" was declared but never referenced
          detected during:
            instantiation of "void flash::compute_attn_1rowblock<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &, int, int, int) [with Kernel_traits=Flash_fwd_kernel_traits<256, 64, 64, 4, false, false, cutlass::half_t, Flash_kernel_traits<256, 64, 64, 4, cutlass::half_t>>, Is_dropout=true, Is_causal=false, Is_even_N=false, Is_even_K=false, Return_softmax=true, Params=Flash_fwd_params]" 
(588): here
            instantiation of "void flash::compute_attn<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &) [with Kernel_traits=Flash_fwd_kernel_traits<256, 64, 64, 4, false, false, cutlass::half_t, Flash_kernel_traits<256, 64, 64, 4, cutlass::half_t>>, Is_dropout=true, Is_causal=false, Is_even_N=false, Is_even_K=false, Return_softmax=true, Params=Flash_fwd_params]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(15): here
            instantiation of "void flash_fwd_kernel<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax>(Flash_fwd_params) [with Kernel_traits=Flash_fwd_kernel_traits<256, 64, 64, 4, false, false, cutlass::half_t, Flash_kernel_traits<256, 64, 64, 4, cutlass::half_t>>, Is_dropout=true, Is_causal=false, Is_even_N=false, Is_even_K=false, Return_softmax=true]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(34): here
            instantiation of "void run_flash_fwd<Kernel_traits,Is_dropout,Is_causal>(Flash_fwd_params &, cudaStream_t) [with Kernel_traits=Flash_fwd_kernel_traits<256, 64, 64, 4, false, false, cutlass::half_t, Flash_kernel_traits<256, 64, 64, 4, cutlass::half_t>>, Is_dropout=true, Is_causal=false]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(236): here
            instantiation of "void run_mha_fwd_hdim256<T>(Flash_fwd_params &, cudaStream_t) [with T=cutlass::half_t]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_hdim256_fp16_sm80.cu(8): here

/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_kernel.h(275): warning: variable "tQrQ" was declared but never referenced
          detected during:
            instantiation of "void flash::compute_attn_1rowblock<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &, int, int, int) [with Kernel_traits=Flash_fwd_kernel_traits<256, 64, 64, 4, false, false, cutlass::half_t, Flash_kernel_traits<256, 64, 64, 4, cutlass::half_t>>, Is_dropout=true, Is_causal=false, Is_even_N=false, Is_even_K=false, Return_softmax=true, Params=Flash_fwd_params]" 
(588): here
            instantiation of "void flash::compute_attn<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &) [with Kernel_traits=Flash_fwd_kernel_traits<256, 64, 64, 4, false, false, cutlass::half_t, Flash_kernel_traits<256, 64, 64, 4, cutlass::half_t>>, Is_dropout=true, Is_causal=false, Is_even_N=false, Is_even_K=false, Return_softmax=true, Params=Flash_fwd_params]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(15): here
            instantiation of "void flash_fwd_kernel<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax>(Flash_fwd_params) [with Kernel_traits=Flash_fwd_kernel_traits<256, 64, 64, 4, false, false, cutlass::half_t, Flash_kernel_traits<256, 64, 64, 4, cutlass::half_t>>, Is_dropout=true, Is_causal=false, Is_even_N=false, Is_even_K=false, Return_softmax=true]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(34): here
            instantiation of "void run_flash_fwd<Kernel_traits,Is_dropout,Is_causal>(Flash_fwd_params &, cudaStream_t) [with Kernel_traits=Flash_fwd_kernel_traits<256, 64, 64, 4, false, false, cutlass::half_t, Flash_kernel_traits<256, 64, 64, 4, cutlass::half_t>>, Is_dropout=true, Is_causal=false]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(236): here
            instantiation of "void run_mha_fwd_hdim256<T>(Flash_fwd_params &, cudaStream_t) [with T=cutlass::half_t]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_hdim256_fp16_sm80.cu(8): here

/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_kernel.h(140): warning: variable "MMA_M" was declared but never referenced
          detected during:
            instantiation of "void flash::compute_attn_1rowblock<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &, int, int, int) [with Kernel_traits=Flash_fwd_kernel_traits<256, 64, 64, 4, false, false, cutlass::half_t, Flash_kernel_traits<256, 64, 64, 4, cutlass::half_t>>, Is_dropout=true, Is_causal=false, Is_even_N=false, Is_even_K=false, Return_softmax=false, Params=Flash_fwd_params]" 
(588): here
            instantiation of "void flash::compute_attn<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &) [with Kernel_traits=Flash_fwd_kernel_traits<256, 64, 64, 4, false, false, cutlass::half_t, Flash_kernel_traits<256, 64, 64, 4, cutlass::half_t>>, Is_dropout=true, Is_causal=false, Is_even_N=false, Is_even_K=false, Return_softmax=false, Params=Flash_fwd_params]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(15): here
            instantiation of "void flash_fwd_kernel<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax>(Flash_fwd_params) [with Kernel_traits=Flash_fwd_kernel_traits<256, 64, 64, 4, false, false, cutlass::half_t, Flash_kernel_traits<256, 64, 64, 4, cutlass::half_t>>, Is_dropout=true, Is_causal=false, Is_even_N=false, Is_even_K=false, Return_softmax=false]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(34): here
            instantiation of "void run_flash_fwd<Kernel_traits,Is_dropout,Is_causal>(Flash_fwd_params &, cudaStream_t) [with Kernel_traits=Flash_fwd_kernel_traits<256, 64, 64, 4, false, false, cutlass::half_t, Flash_kernel_traits<256, 64, 64, 4, cutlass::half_t>>, Is_dropout=true, Is_causal=false]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(236): here
            instantiation of "void run_mha_fwd_hdim256<T>(Flash_fwd_params &, cudaStream_t) [with T=cutlass::half_t]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_hdim256_fp16_sm80.cu(8): here

/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_kernel.h(275): warning: variable "tQrQ" was declared but never referenced
          detected during:
            instantiation of "void flash::compute_attn_1rowblock<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &, int, int, int) [with Kernel_traits=Flash_fwd_kernel_traits<256, 64, 64, 4, false, false, cutlass::half_t, Flash_kernel_traits<256, 64, 64, 4, cutlass::half_t>>, Is_dropout=true, Is_causal=false, Is_even_N=false, Is_even_K=false, Return_softmax=false, Params=Flash_fwd_params]" 
(588): here
            instantiation of "void flash::compute_attn<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &) [with Kernel_traits=Flash_fwd_kernel_traits<256, 64, 64, 4, false, false, cutlass::half_t, Flash_kernel_traits<256, 64, 64, 4, cutlass::half_t>>, Is_dropout=true, Is_causal=false, Is_even_N=false, Is_even_K=false, Return_softmax=false, Params=Flash_fwd_params]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(15): here
            instantiation of "void flash_fwd_kernel<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax>(Flash_fwd_params) [with Kernel_traits=Flash_fwd_kernel_traits<256, 64, 64, 4, false, false, cutlass::half_t, Flash_kernel_traits<256, 64, 64, 4, cutlass::half_t>>, Is_dropout=true, Is_causal=false, Is_even_N=false, Is_even_K=false, Return_softmax=false]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(34): here
            instantiation of "void run_flash_fwd<Kernel_traits,Is_dropout,Is_causal>(Flash_fwd_params &, cudaStream_t) [with Kernel_traits=Flash_fwd_kernel_traits<256, 64, 64, 4, false, false, cutlass::half_t, Flash_kernel_traits<256, 64, 64, 4, cutlass::half_t>>, Is_dropout=true, Is_causal=false]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(236): here
            instantiation of "void run_mha_fwd_hdim256<T>(Flash_fwd_params &, cudaStream_t) [with T=cutlass::half_t]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_hdim256_fp16_sm80.cu(8): here

/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_kernel.h(140): warning: variable "MMA_M" was declared but never referenced
          detected during:
            instantiation of "void flash::compute_attn_1rowblock<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &, int, int, int) [with Kernel_traits=Flash_fwd_kernel_traits<256, 128, 64, 8, false, false, cutlass::half_t, Flash_kernel_traits<256, 128, 64, 8, cutlass::half_t>>, Is_dropout=false, Is_causal=true, Is_even_N=true, Is_even_K=true, Return_softmax=false, Params=Flash_fwd_params]" 
(588): here
            instantiation of "void flash::compute_attn<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &) [with Kernel_traits=Flash_fwd_kernel_traits<256, 128, 64, 8, false, false, cutlass::half_t, Flash_kernel_traits<256, 128, 64, 8, cutlass::half_t>>, Is_dropout=false, Is_causal=true, Is_even_N=true, Is_even_K=true, Return_softmax=false, Params=Flash_fwd_params]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(15): here
            instantiation of "void flash_fwd_kernel<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax>(Flash_fwd_params) [with Kernel_traits=Flash_fwd_kernel_traits<256, 128, 64, 8, false, false, cutlass::half_t, Flash_kernel_traits<256, 128, 64, 8, cutlass::half_t>>, Is_dropout=false, Is_causal=true, Is_even_N=true, Is_even_K=true, Return_softmax=false]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(34): here
            instantiation of "void run_flash_fwd<Kernel_traits,Is_dropout,Is_causal>(Flash_fwd_params &, cudaStream_t) [with Kernel_traits=Flash_fwd_kernel_traits<256, 128, 64, 8, false, false, cutlass::half_t, Flash_kernel_traits<256, 128, 64, 8, cutlass::half_t>>, Is_dropout=false, Is_causal=true]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(236): here
            instantiation of "void run_mha_fwd_hdim256<T>(Flash_fwd_params &, cudaStream_t) [with T=cutlass::half_t]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_hdim256_fp16_sm80.cu(8): here

/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_kernel.h(275): warning: variable "tQrQ" was declared but never referenced
          detected during:
            instantiation of "void flash::compute_attn_1rowblock<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &, int, int, int) [with Kernel_traits=Flash_fwd_kernel_traits<256, 128, 64, 8, false, false, cutlass::half_t, Flash_kernel_traits<256, 128, 64, 8, cutlass::half_t>>, Is_dropout=false, Is_causal=true, Is_even_N=true, Is_even_K=true, Return_softmax=false, Params=Flash_fwd_params]" 
(588): here
            instantiation of "void flash::compute_attn<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &) [with Kernel_traits=Flash_fwd_kernel_traits<256, 128, 64, 8, false, false, cutlass::half_t, Flash_kernel_traits<256, 128, 64, 8, cutlass::half_t>>, Is_dropout=false, Is_causal=true, Is_even_N=true, Is_even_K=true, Return_softmax=false, Params=Flash_fwd_params]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(15): here
            instantiation of "void flash_fwd_kernel<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax>(Flash_fwd_params) [with Kernel_traits=Flash_fwd_kernel_traits<256, 128, 64, 8, false, false, cutlass::half_t, Flash_kernel_traits<256, 128, 64, 8, cutlass::half_t>>, Is_dropout=false, Is_causal=true, Is_even_N=true, Is_even_K=true, Return_softmax=false]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(34): here
            instantiation of "void run_flash_fwd<Kernel_traits,Is_dropout,Is_causal>(Flash_fwd_params &, cudaStream_t) [with Kernel_traits=Flash_fwd_kernel_traits<256, 128, 64, 8, false, false, cutlass::half_t, Flash_kernel_traits<256, 128, 64, 8, cutlass::half_t>>, Is_dropout=false, Is_causal=true]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(236): here
            instantiation of "void run_mha_fwd_hdim256<T>(Flash_fwd_params &, cudaStream_t) [with T=cutlass::half_t]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_hdim256_fp16_sm80.cu(8): here

/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_kernel.h(140): warning: variable "MMA_M" was declared but never referenced
          detected during:
            instantiation of "void flash::compute_attn_1rowblock<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &, int, int, int) [with Kernel_traits=Flash_fwd_kernel_traits<256, 128, 64, 8, false, false, cutlass::half_t, Flash_kernel_traits<256, 128, 64, 8, cutlass::half_t>>, Is_dropout=false, Is_causal=true, Is_even_N=true, Is_even_K=false, Return_softmax=false, Params=Flash_fwd_params]" 
(588): here
            instantiation of "void flash::compute_attn<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &) [with Kernel_traits=Flash_fwd_kernel_traits<256, 128, 64, 8, false, false, cutlass::half_t, Flash_kernel_traits<256, 128, 64, 8, cutlass::half_t>>, Is_dropout=false, Is_causal=true, Is_even_N=true, Is_even_K=false, Return_softmax=false, Params=Flash_fwd_params]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(15): here
            instantiation of "void flash_fwd_kernel<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax>(Flash_fwd_params) [with Kernel_traits=Flash_fwd_kernel_traits<256, 128, 64, 8, false, false, cutlass::half_t, Flash_kernel_traits<256, 128, 64, 8, cutlass::half_t>>, Is_dropout=false, Is_causal=true, Is_even_N=true, Is_even_K=false, Return_softmax=false]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(34): here
            instantiation of "void run_flash_fwd<Kernel_traits,Is_dropout,Is_causal>(Flash_fwd_params &, cudaStream_t) [with Kernel_traits=Flash_fwd_kernel_traits<256, 128, 64, 8, false, false, cutlass::half_t, Flash_kernel_traits<256, 128, 64, 8, cutlass::half_t>>, Is_dropout=false, Is_causal=true]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(236): here
            instantiation of "void run_mha_fwd_hdim256<T>(Flash_fwd_params &, cudaStream_t) [with T=cutlass::half_t]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_hdim256_fp16_sm80.cu(8): here

/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_kernel.h(275): warning: variable "tQrQ" was declared but never referenced
          detected during:
            instantiation of "void flash::compute_attn_1rowblock<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &, int, int, int) [with Kernel_traits=Flash_fwd_kernel_traits<256, 128, 64, 8, false, false, cutlass::half_t, Flash_kernel_traits<256, 128, 64, 8, cutlass::half_t>>, Is_dropout=false, Is_causal=true, Is_even_N=true, Is_even_K=false, Return_softmax=false, Params=Flash_fwd_params]" 
(588): here
            instantiation of "void flash::compute_attn<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &) [with Kernel_traits=Flash_fwd_kernel_traits<256, 128, 64, 8, false, false, cutlass::half_t, Flash_kernel_traits<256, 128, 64, 8, cutlass::half_t>>, Is_dropout=false, Is_causal=true, Is_even_N=true, Is_even_K=false, Return_softmax=false, Params=Flash_fwd_params]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(15): here
            instantiation of "void flash_fwd_kernel<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax>(Flash_fwd_params) [with Kernel_traits=Flash_fwd_kernel_traits<256, 128, 64, 8, false, false, cutlass::half_t, Flash_kernel_traits<256, 128, 64, 8, cutlass::half_t>>, Is_dropout=false, Is_causal=true, Is_even_N=true, Is_even_K=false, Return_softmax=false]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(34): here
            instantiation of "void run_flash_fwd<Kernel_traits,Is_dropout,Is_causal>(Flash_fwd_params &, cudaStream_t) [with Kernel_traits=Flash_fwd_kernel_traits<256, 128, 64, 8, false, false, cutlass::half_t, Flash_kernel_traits<256, 128, 64, 8, cutlass::half_t>>, Is_dropout=false, Is_causal=true]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(236): here
            instantiation of "void run_mha_fwd_hdim256<T>(Flash_fwd_params &, cudaStream_t) [with T=cutlass::half_t]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_hdim256_fp16_sm80.cu(8): here

/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_kernel.h(140): warning: variable "MMA_M" was declared but never referenced
          detected during:
            instantiation of "void flash::compute_attn_1rowblock<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &, int, int, int) [with Kernel_traits=Flash_fwd_kernel_traits<256, 128, 64, 8, false, false, cutlass::half_t, Flash_kernel_traits<256, 128, 64, 8, cutlass::half_t>>, Is_dropout=false, Is_causal=true, Is_even_N=false, Is_even_K=true, Return_softmax=false, Params=Flash_fwd_params]" 
(588): here
            instantiation of "void flash::compute_attn<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &) [with Kernel_traits=Flash_fwd_kernel_traits<256, 128, 64, 8, false, false, cutlass::half_t, Flash_kernel_traits<256, 128, 64, 8, cutlass::half_t>>, Is_dropout=false, Is_causal=true, Is_even_N=false, Is_even_K=true, Return_softmax=false, Params=Flash_fwd_params]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(15): here
            instantiation of "void flash_fwd_kernel<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax>(Flash_fwd_params) [with Kernel_traits=Flash_fwd_kernel_traits<256, 128, 64, 8, false, false, cutlass::half_t, Flash_kernel_traits<256, 128, 64, 8, cutlass::half_t>>, Is_dropout=false, Is_causal=true, Is_even_N=false, Is_even_K=true, Return_softmax=false]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(34): here
            instantiation of "void run_flash_fwd<Kernel_traits,Is_dropout,Is_causal>(Flash_fwd_params &, cudaStream_t) [with Kernel_traits=Flash_fwd_kernel_traits<256, 128, 64, 8, false, false, cutlass::half_t, Flash_kernel_traits<256, 128, 64, 8, cutlass::half_t>>, Is_dropout=false, Is_causal=true]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(236): here
            instantiation of "void run_mha_fwd_hdim256<T>(Flash_fwd_params &, cudaStream_t) [with T=cutlass::half_t]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_hdim256_fp16_sm80.cu(8): here

/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_kernel.h(275): warning: variable "tQrQ" was declared but never referenced
          detected during:
            instantiation of "void flash::compute_attn_1rowblock<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &, int, int, int) [with Kernel_traits=Flash_fwd_kernel_traits<256, 128, 64, 8, false, false, cutlass::half_t, Flash_kernel_traits<256, 128, 64, 8, cutlass::half_t>>, Is_dropout=false, Is_causal=true, Is_even_N=false, Is_even_K=true, Return_softmax=false, Params=Flash_fwd_params]" 
(588): here
            instantiation of "void flash::compute_attn<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &) [with Kernel_traits=Flash_fwd_kernel_traits<256, 128, 64, 8, false, false, cutlass::half_t, Flash_kernel_traits<256, 128, 64, 8, cutlass::half_t>>, Is_dropout=false, Is_causal=true, Is_even_N=false, Is_even_K=true, Return_softmax=false, Params=Flash_fwd_params]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(15): here
            instantiation of "void flash_fwd_kernel<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax>(Flash_fwd_params) [with Kernel_traits=Flash_fwd_kernel_traits<256, 128, 64, 8, false, false, cutlass::half_t, Flash_kernel_traits<256, 128, 64, 8, cutlass::half_t>>, Is_dropout=false, Is_causal=true, Is_even_N=false, Is_even_K=true, Return_softmax=false]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(34): here
            instantiation of "void run_flash_fwd<Kernel_traits,Is_dropout,Is_causal>(Flash_fwd_params &, cudaStream_t) [with Kernel_traits=Flash_fwd_kernel_traits<256, 128, 64, 8, false, false, cutlass::half_t, Flash_kernel_traits<256, 128, 64, 8, cutlass::half_t>>, Is_dropout=false, Is_causal=true]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(236): here
            instantiation of "void run_mha_fwd_hdim256<T>(Flash_fwd_params &, cudaStream_t) [with T=cutlass::half_t]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_hdim256_fp16_sm80.cu(8): here

/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_kernel.h(140): warning: variable "MMA_M" was declared but never referenced
          detected during:
            instantiation of "void flash::compute_attn_1rowblock<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &, int, int, int) [with Kernel_traits=Flash_fwd_kernel_traits<256, 128, 64, 8, false, false, cutlass::half_t, Flash_kernel_traits<256, 128, 64, 8, cutlass::half_t>>, Is_dropout=false, Is_causal=true, Is_even_N=false, Is_even_K=false, Return_softmax=false, Params=Flash_fwd_params]" 
(588): here
            instantiation of "void flash::compute_attn<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &) [with Kernel_traits=Flash_fwd_kernel_traits<256, 128, 64, 8, false, false, cutlass::half_t, Flash_kernel_traits<256, 128, 64, 8, cutlass::half_t>>, Is_dropout=false, Is_causal=true, Is_even_N=false, Is_even_K=false, Return_softmax=false, Params=Flash_fwd_params]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(15): here
            instantiation of "void flash_fwd_kernel<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax>(Flash_fwd_params) [with Kernel_traits=Flash_fwd_kernel_traits<256, 128, 64, 8, false, false, cutlass::half_t, Flash_kernel_traits<256, 128, 64, 8, cutlass::half_t>>, Is_dropout=false, Is_causal=true, Is_even_N=false, Is_even_K=false, Return_softmax=false]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(34): here
            instantiation of "void run_flash_fwd<Kernel_traits,Is_dropout,Is_causal>(Flash_fwd_params &, cudaStream_t) [with Kernel_traits=Flash_fwd_kernel_traits<256, 128, 64, 8, false, false, cutlass::half_t, Flash_kernel_traits<256, 128, 64, 8, cutlass::half_t>>, Is_dropout=false, Is_causal=true]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(236): here
            instantiation of "void run_mha_fwd_hdim256<T>(Flash_fwd_params &, cudaStream_t) [with T=cutlass::half_t]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_hdim256_fp16_sm80.cu(8): here

/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_kernel.h(275): warning: variable "tQrQ" was declared but never referenced
          detected during:
            instantiation of "void flash::compute_attn_1rowblock<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &, int, int, int) [with Kernel_traits=Flash_fwd_kernel_traits<256, 128, 64, 8, false, false, cutlass::half_t, Flash_kernel_traits<256, 128, 64, 8, cutlass::half_t>>, Is_dropout=false, Is_causal=true, Is_even_N=false, Is_even_K=false, Return_softmax=false, Params=Flash_fwd_params]" 
(588): here
            instantiation of "void flash::compute_attn<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &) [with Kernel_traits=Flash_fwd_kernel_traits<256, 128, 64, 8, false, false, cutlass::half_t, Flash_kernel_traits<256, 128, 64, 8, cutlass::half_t>>, Is_dropout=false, Is_causal=true, Is_even_N=false, Is_even_K=false, Return_softmax=false, Params=Flash_fwd_params]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(15): here
            instantiation of "void flash_fwd_kernel<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax>(Flash_fwd_params) [with Kernel_traits=Flash_fwd_kernel_traits<256, 128, 64, 8, false, false, cutlass::half_t, Flash_kernel_traits<256, 128, 64, 8, cutlass::half_t>>, Is_dropout=false, Is_causal=true, Is_even_N=false, Is_even_K=false, Return_softmax=false]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(34): here
            instantiation of "void run_flash_fwd<Kernel_traits,Is_dropout,Is_causal>(Flash_fwd_params &, cudaStream_t) [with Kernel_traits=Flash_fwd_kernel_traits<256, 128, 64, 8, false, false, cutlass::half_t, Flash_kernel_traits<256, 128, 64, 8, cutlass::half_t>>, Is_dropout=false, Is_causal=true]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(236): here
            instantiation of "void run_mha_fwd_hdim256<T>(Flash_fwd_params &, cudaStream_t) [with T=cutlass::half_t]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_hdim256_fp16_sm80.cu(8): here

/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_kernel.h(140): warning: variable "MMA_M" was declared but never referenced
          detected during:
            instantiation of "void flash::compute_attn_1rowblock<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &, int, int, int) [with Kernel_traits=Flash_fwd_kernel_traits<256, 64, 64, 4, false, false, cutlass::half_t, Flash_kernel_traits<256, 64, 64, 4, cutlass::half_t>>, Is_dropout=false, Is_causal=true, Is_even_N=true, Is_even_K=true, Return_softmax=false, Params=Flash_fwd_params]" 
(588): here
            instantiation of "void flash::compute_attn<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &) [with Kernel_traits=Flash_fwd_kernel_traits<256, 64, 64, 4, false, false, cutlass::half_t, Flash_kernel_traits<256, 64, 64, 4, cutlass::half_t>>, Is_dropout=false, Is_causal=true, Is_even_N=true, Is_even_K=true, Return_softmax=false, Params=Flash_fwd_params]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(15): here
            instantiation of "void flash_fwd_kernel<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax>(Flash_fwd_params) [with Kernel_traits=Flash_fwd_kernel_traits<256, 64, 64, 4, false, false, cutlass::half_t, Flash_kernel_traits<256, 64, 64, 4, cutlass::half_t>>, Is_dropout=false, Is_causal=true, Is_even_N=true, Is_even_K=true, Return_softmax=false]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(34): here
            instantiation of "void run_flash_fwd<Kernel_traits,Is_dropout,Is_causal>(Flash_fwd_params &, cudaStream_t) [with Kernel_traits=Flash_fwd_kernel_traits<256, 64, 64, 4, false, false, cutlass::half_t, Flash_kernel_traits<256, 64, 64, 4, cutlass::half_t>>, Is_dropout=false, Is_causal=true]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(236): here
            instantiation of "void run_mha_fwd_hdim256<T>(Flash_fwd_params &, cudaStream_t) [with T=cutlass::half_t]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_hdim256_fp16_sm80.cu(8): here

/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_kernel.h(275): warning: variable "tQrQ" was declared but never referenced
          detected during:
            instantiation of "void flash::compute_attn_1rowblock<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &, int, int, int) [with Kernel_traits=Flash_fwd_kernel_traits<256, 64, 64, 4, false, false, cutlass::half_t, Flash_kernel_traits<256, 64, 64, 4, cutlass::half_t>>, Is_dropout=false, Is_causal=true, Is_even_N=true, Is_even_K=true, Return_softmax=false, Params=Flash_fwd_params]" 
(588): here
            instantiation of "void flash::compute_attn<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &) [with Kernel_traits=Flash_fwd_kernel_traits<256, 64, 64, 4, false, false, cutlass::half_t, Flash_kernel_traits<256, 64, 64, 4, cutlass::half_t>>, Is_dropout=false, Is_causal=true, Is_even_N=true, Is_even_K=true, Return_softmax=false, Params=Flash_fwd_params]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(15): here
            instantiation of "void flash_fwd_kernel<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax>(Flash_fwd_params) [with Kernel_traits=Flash_fwd_kernel_traits<256, 64, 64, 4, false, false, cutlass::half_t, Flash_kernel_traits<256, 64, 64, 4, cutlass::half_t>>, Is_dropout=false, Is_causal=true, Is_even_N=true, Is_even_K=true, Return_softmax=false]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(34): here
            instantiation of "void run_flash_fwd<Kernel_traits,Is_dropout,Is_causal>(Flash_fwd_params &, cudaStream_t) [with Kernel_traits=Flash_fwd_kernel_traits<256, 64, 64, 4, false, false, cutlass::half_t, Flash_kernel_traits<256, 64, 64, 4, cutlass::half_t>>, Is_dropout=false, Is_causal=true]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(236): here
            instantiation of "void run_mha_fwd_hdim256<T>(Flash_fwd_params &, cudaStream_t) [with T=cutlass::half_t]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_hdim256_fp16_sm80.cu(8): here

/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_kernel.h(140): warning: variable "MMA_M" was declared but never referenced
          detected during:
            instantiation of "void flash::compute_attn_1rowblock<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &, int, int, int) [with Kernel_traits=Flash_fwd_kernel_traits<256, 64, 64, 4, false, false, cutlass::half_t, Flash_kernel_traits<256, 64, 64, 4, cutlass::half_t>>, Is_dropout=false, Is_causal=true, Is_even_N=true, Is_even_K=false, Return_softmax=false, Params=Flash_fwd_params]" 
(588): here
            instantiation of "void flash::compute_attn<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &) [with Kernel_traits=Flash_fwd_kernel_traits<256, 64, 64, 4, false, false, cutlass::half_t, Flash_kernel_traits<256, 64, 64, 4, cutlass::half_t>>, Is_dropout=false, Is_causal=true, Is_even_N=true, Is_even_K=false, Return_softmax=false, Params=Flash_fwd_params]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(15): here
            instantiation of "void flash_fwd_kernel<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax>(Flash_fwd_params) [with Kernel_traits=Flash_fwd_kernel_traits<256, 64, 64, 4, false, false, cutlass::half_t, Flash_kernel_traits<256, 64, 64, 4, cutlass::half_t>>, Is_dropout=false, Is_causal=true, Is_even_N=true, Is_even_K=false, Return_softmax=false]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(34): here
            instantiation of "void run_flash_fwd<Kernel_traits,Is_dropout,Is_causal>(Flash_fwd_params &, cudaStream_t) [with Kernel_traits=Flash_fwd_kernel_traits<256, 64, 64, 4, false, false, cutlass::half_t, Flash_kernel_traits<256, 64, 64, 4, cutlass::half_t>>, Is_dropout=false, Is_causal=true]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(236): here
            instantiation of "void run_mha_fwd_hdim256<T>(Flash_fwd_params &, cudaStream_t) [with T=cutlass::half_t]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_hdim256_fp16_sm80.cu(8): here

/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_kernel.h(275): warning: variable "tQrQ" was declared but never referenced
          detected during:
            instantiation of "void flash::compute_attn_1rowblock<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &, int, int, int) [with Kernel_traits=Flash_fwd_kernel_traits<256, 64, 64, 4, false, false, cutlass::half_t, Flash_kernel_traits<256, 64, 64, 4, cutlass::half_t>>, Is_dropout=false, Is_causal=true, Is_even_N=true, Is_even_K=false, Return_softmax=false, Params=Flash_fwd_params]" 
(588): here
            instantiation of "void flash::compute_attn<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &) [with Kernel_traits=Flash_fwd_kernel_traits<256, 64, 64, 4, false, false, cutlass::half_t, Flash_kernel_traits<256, 64, 64, 4, cutlass::half_t>>, Is_dropout=false, Is_causal=true, Is_even_N=true, Is_even_K=false, Return_softmax=false, Params=Flash_fwd_params]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(15): here
            instantiation of "void flash_fwd_kernel<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax>(Flash_fwd_params) [with Kernel_traits=Flash_fwd_kernel_traits<256, 64, 64, 4, false, false, cutlass::half_t, Flash_kernel_traits<256, 64, 64, 4, cutlass::half_t>>, Is_dropout=false, Is_causal=true, Is_even_N=true, Is_even_K=false, Return_softmax=false]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(34): here
            instantiation of "void run_flash_fwd<Kernel_traits,Is_dropout,Is_causal>(Flash_fwd_params &, cudaStream_t) [with Kernel_traits=Flash_fwd_kernel_traits<256, 64, 64, 4, false, false, cutlass::half_t, Flash_kernel_traits<256, 64, 64, 4, cutlass::half_t>>, Is_dropout=false, Is_causal=true]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(236): here
            instantiation of "void run_mha_fwd_hdim256<T>(Flash_fwd_params &, cudaStream_t) [with T=cutlass::half_t]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_hdim256_fp16_sm80.cu(8): here

/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_kernel.h(140): warning: variable "MMA_M" was declared but never referenced
          detected during:
            instantiation of "void flash::compute_attn_1rowblock<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &, int, int, int) [with Kernel_traits=Flash_fwd_kernel_traits<256, 64, 64, 4, false, false, cutlass::half_t, Flash_kernel_traits<256, 64, 64, 4, cutlass::half_t>>, Is_dropout=false, Is_causal=true, Is_even_N=false, Is_even_K=true, Return_softmax=false, Params=Flash_fwd_params]" 
(588): here
            instantiation of "void flash::compute_attn<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &) [with Kernel_traits=Flash_fwd_kernel_traits<256, 64, 64, 4, false, false, cutlass::half_t, Flash_kernel_traits<256, 64, 64, 4, cutlass::half_t>>, Is_dropout=false, Is_causal=true, Is_even_N=false, Is_even_K=true, Return_softmax=false, Params=Flash_fwd_params]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(15): here
            instantiation of "void flash_fwd_kernel<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax>(Flash_fwd_params) [with Kernel_traits=Flash_fwd_kernel_traits<256, 64, 64, 4, false, false, cutlass::half_t, Flash_kernel_traits<256, 64, 64, 4, cutlass::half_t>>, Is_dropout=false, Is_causal=true, Is_even_N=false, Is_even_K=true, Return_softmax=false]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(34): here
            instantiation of "void run_flash_fwd<Kernel_traits,Is_dropout,Is_causal>(Flash_fwd_params &, cudaStream_t) [with Kernel_traits=Flash_fwd_kernel_traits<256, 64, 64, 4, false, false, cutlass::half_t, Flash_kernel_traits<256, 64, 64, 4, cutlass::half_t>>, Is_dropout=false, Is_causal=true]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(236): here
            instantiation of "void run_mha_fwd_hdim256<T>(Flash_fwd_params &, cudaStream_t) [with T=cutlass::half_t]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_hdim256_fp16_sm80.cu(8): here

/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_kernel.h(275): warning: variable "tQrQ" was declared but never referenced
          detected during:
            instantiation of "void flash::compute_attn_1rowblock<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &, int, int, int) [with Kernel_traits=Flash_fwd_kernel_traits<256, 64, 64, 4, false, false, cutlass::half_t, Flash_kernel_traits<256, 64, 64, 4, cutlass::half_t>>, Is_dropout=false, Is_causal=true, Is_even_N=false, Is_even_K=true, Return_softmax=false, Params=Flash_fwd_params]" 
(588): here
            instantiation of "void flash::compute_attn<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &) [with Kernel_traits=Flash_fwd_kernel_traits<256, 64, 64, 4, false, false, cutlass::half_t, Flash_kernel_traits<256, 64, 64, 4, cutlass::half_t>>, Is_dropout=false, Is_causal=true, Is_even_N=false, Is_even_K=true, Return_softmax=false, Params=Flash_fwd_params]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(15): here
            instantiation of "void flash_fwd_kernel<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax>(Flash_fwd_params) [with Kernel_traits=Flash_fwd_kernel_traits<256, 64, 64, 4, false, false, cutlass::half_t, Flash_kernel_traits<256, 64, 64, 4, cutlass::half_t>>, Is_dropout=false, Is_causal=true, Is_even_N=false, Is_even_K=true, Return_softmax=false]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(34): here
            instantiation of "void run_flash_fwd<Kernel_traits,Is_dropout,Is_causal>(Flash_fwd_params &, cudaStream_t) [with Kernel_traits=Flash_fwd_kernel_traits<256, 64, 64, 4, false, false, cutlass::half_t, Flash_kernel_traits<256, 64, 64, 4, cutlass::half_t>>, Is_dropout=false, Is_causal=true]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(236): here
            instantiation of "void run_mha_fwd_hdim256<T>(Flash_fwd_params &, cudaStream_t) [with T=cutlass::half_t]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_hdim256_fp16_sm80.cu(8): here

/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_kernel.h(140): warning: variable "MMA_M" was declared but never referenced
          detected during:
            instantiation of "void flash::compute_attn_1rowblock<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &, int, int, int) [with Kernel_traits=Flash_fwd_kernel_traits<256, 64, 64, 4, false, false, cutlass::half_t, Flash_kernel_traits<256, 64, 64, 4, cutlass::half_t>>, Is_dropout=false, Is_causal=true, Is_even_N=false, Is_even_K=false, Return_softmax=false, Params=Flash_fwd_params]" 
(588): here
            instantiation of "void flash::compute_attn<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &) [with Kernel_traits=Flash_fwd_kernel_traits<256, 64, 64, 4, false, false, cutlass::half_t, Flash_kernel_traits<256, 64, 64, 4, cutlass::half_t>>, Is_dropout=false, Is_causal=true, Is_even_N=false, Is_even_K=false, Return_softmax=false, Params=Flash_fwd_params]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(15): here
            instantiation of "void flash_fwd_kernel<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax>(Flash_fwd_params) [with Kernel_traits=Flash_fwd_kernel_traits<256, 64, 64, 4, false, false, cutlass::half_t, Flash_kernel_traits<256, 64, 64, 4, cutlass::half_t>>, Is_dropout=false, Is_causal=true, Is_even_N=false, Is_even_K=false, Return_softmax=false]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(34): here
            instantiation of "void run_flash_fwd<Kernel_traits,Is_dropout,Is_causal>(Flash_fwd_params &, cudaStream_t) [with Kernel_traits=Flash_fwd_kernel_traits<256, 64, 64, 4, false, false, cutlass::half_t, Flash_kernel_traits<256, 64, 64, 4, cutlass::half_t>>, Is_dropout=false, Is_causal=true]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(236): here
            instantiation of "void run_mha_fwd_hdim256<T>(Flash_fwd_params &, cudaStream_t) [with T=cutlass::half_t]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_hdim256_fp16_sm80.cu(8): here

/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_kernel.h(275): warning: variable "tQrQ" was declared but never referenced
          detected during:
            instantiation of "void flash::compute_attn_1rowblock<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &, int, int, int) [with Kernel_traits=Flash_fwd_kernel_traits<256, 64, 64, 4, false, false, cutlass::half_t, Flash_kernel_traits<256, 64, 64, 4, cutlass::half_t>>, Is_dropout=false, Is_causal=true, Is_even_N=false, Is_even_K=false, Return_softmax=false, Params=Flash_fwd_params]" 
(588): here
            instantiation of "void flash::compute_attn<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &) [with Kernel_traits=Flash_fwd_kernel_traits<256, 64, 64, 4, false, false, cutlass::half_t, Flash_kernel_traits<256, 64, 64, 4, cutlass::half_t>>, Is_dropout=false, Is_causal=true, Is_even_N=false, Is_even_K=false, Return_softmax=false, Params=Flash_fwd_params]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(15): here
            instantiation of "void flash_fwd_kernel<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax>(Flash_fwd_params) [with Kernel_traits=Flash_fwd_kernel_traits<256, 64, 64, 4, false, false, cutlass::half_t, Flash_kernel_traits<256, 64, 64, 4, cutlass::half_t>>, Is_dropout=false, Is_causal=true, Is_even_N=false, Is_even_K=false, Return_softmax=false]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(34): here
            instantiation of "void run_flash_fwd<Kernel_traits,Is_dropout,Is_causal>(Flash_fwd_params &, cudaStream_t) [with Kernel_traits=Flash_fwd_kernel_traits<256, 64, 64, 4, false, false, cutlass::half_t, Flash_kernel_traits<256, 64, 64, 4, cutlass::half_t>>, Is_dropout=false, Is_causal=true]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(236): here
            instantiation of "void run_mha_fwd_hdim256<T>(Flash_fwd_params &, cudaStream_t) [with T=cutlass::half_t]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_hdim256_fp16_sm80.cu(8): here

/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_kernel.h(140): warning: variable "MMA_M" was declared but never referenced
          detected during:
            instantiation of "void flash::compute_attn_1rowblock<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &, int, int, int) [with Kernel_traits=Flash_fwd_kernel_traits<256, 128, 64, 8, false, false, cutlass::half_t, Flash_kernel_traits<256, 128, 64, 8, cutlass::half_t>>, Is_dropout=false, Is_causal=false, Is_even_N=true, Is_even_K=true, Return_softmax=false, Params=Flash_fwd_params]" 
(588): here
            instantiation of "void flash::compute_attn<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &) [with Kernel_traits=Flash_fwd_kernel_traits<256, 128, 64, 8, false, false, cutlass::half_t, Flash_kernel_traits<256, 128, 64, 8, cutlass::half_t>>, Is_dropout=false, Is_causal=false, Is_even_N=true, Is_even_K=true, Return_softmax=false, Params=Flash_fwd_params]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(15): here
            instantiation of "void flash_fwd_kernel<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax>(Flash_fwd_params) [with Kernel_traits=Flash_fwd_kernel_traits<256, 128, 64, 8, false, false, cutlass::half_t, Flash_kernel_traits<256, 128, 64, 8, cutlass::half_t>>, Is_dropout=false, Is_causal=false, Is_even_N=true, Is_even_K=true, Return_softmax=false]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(34): here
            instantiation of "void run_flash_fwd<Kernel_traits,Is_dropout,Is_causal>(Flash_fwd_params &, cudaStream_t) [with Kernel_traits=Flash_fwd_kernel_traits<256, 128, 64, 8, false, false, cutlass::half_t, Flash_kernel_traits<256, 128, 64, 8, cutlass::half_t>>, Is_dropout=false, Is_causal=false]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(236): here
            instantiation of "void run_mha_fwd_hdim256<T>(Flash_fwd_params &, cudaStream_t) [with T=cutlass::half_t]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_hdim256_fp16_sm80.cu(8): here

/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_kernel.h(275): warning: variable "tQrQ" was declared but never referenced
          detected during:
            instantiation of "void flash::compute_attn_1rowblock<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &, int, int, int) [with Kernel_traits=Flash_fwd_kernel_traits<256, 128, 64, 8, false, false, cutlass::half_t, Flash_kernel_traits<256, 128, 64, 8, cutlass::half_t>>, Is_dropout=false, Is_causal=false, Is_even_N=true, Is_even_K=true, Return_softmax=false, Params=Flash_fwd_params]" 
(588): here
            instantiation of "void flash::compute_attn<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &) [with Kernel_traits=Flash_fwd_kernel_traits<256, 128, 64, 8, false, false, cutlass::half_t, Flash_kernel_traits<256, 128, 64, 8, cutlass::half_t>>, Is_dropout=false, Is_causal=false, Is_even_N=true, Is_even_K=true, Return_softmax=false, Params=Flash_fwd_params]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(15): here
            instantiation of "void flash_fwd_kernel<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax>(Flash_fwd_params) [with Kernel_traits=Flash_fwd_kernel_traits<256, 128, 64, 8, false, false, cutlass::half_t, Flash_kernel_traits<256, 128, 64, 8, cutlass::half_t>>, Is_dropout=false, Is_causal=false, Is_even_N=true, Is_even_K=true, Return_softmax=false]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(34): here
            instantiation of "void run_flash_fwd<Kernel_traits,Is_dropout,Is_causal>(Flash_fwd_params &, cudaStream_t) [with Kernel_traits=Flash_fwd_kernel_traits<256, 128, 64, 8, false, false, cutlass::half_t, Flash_kernel_traits<256, 128, 64, 8, cutlass::half_t>>, Is_dropout=false, Is_causal=false]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(236): here
            instantiation of "void run_mha_fwd_hdim256<T>(Flash_fwd_params &, cudaStream_t) [with T=cutlass::half_t]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_hdim256_fp16_sm80.cu(8): here

/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_kernel.h(140): warning: variable "MMA_M" was declared but never referenced
          detected during:
            instantiation of "void flash::compute_attn_1rowblock<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &, int, int, int) [with Kernel_traits=Flash_fwd_kernel_traits<256, 128, 64, 8, false, false, cutlass::half_t, Flash_kernel_traits<256, 128, 64, 8, cutlass::half_t>>, Is_dropout=false, Is_causal=false, Is_even_N=true, Is_even_K=false, Return_softmax=false, Params=Flash_fwd_params]" 
(588): here
            instantiation of "void flash::compute_attn<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &) [with Kernel_traits=Flash_fwd_kernel_traits<256, 128, 64, 8, false, false, cutlass::half_t, Flash_kernel_traits<256, 128, 64, 8, cutlass::half_t>>, Is_dropout=false, Is_causal=false, Is_even_N=true, Is_even_K=false, Return_softmax=false, Params=Flash_fwd_params]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(15): here
            instantiation of "void flash_fwd_kernel<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax>(Flash_fwd_params) [with Kernel_traits=Flash_fwd_kernel_traits<256, 128, 64, 8, false, false, cutlass::half_t, Flash_kernel_traits<256, 128, 64, 8, cutlass::half_t>>, Is_dropout=false, Is_causal=false, Is_even_N=true, Is_even_K=false, Return_softmax=false]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(34): here
            instantiation of "void run_flash_fwd<Kernel_traits,Is_dropout,Is_causal>(Flash_fwd_params &, cudaStream_t) [with Kernel_traits=Flash_fwd_kernel_traits<256, 128, 64, 8, false, false, cutlass::half_t, Flash_kernel_traits<256, 128, 64, 8, cutlass::half_t>>, Is_dropout=false, Is_causal=false]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(236): here
            instantiation of "void run_mha_fwd_hdim256<T>(Flash_fwd_params &, cudaStream_t) [with T=cutlass::half_t]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_hdim256_fp16_sm80.cu(8): here

/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_kernel.h(275): warning: variable "tQrQ" was declared but never referenced
          detected during:
            instantiation of "void flash::compute_attn_1rowblock<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &, int, int, int) [with Kernel_traits=Flash_fwd_kernel_traits<256, 128, 64, 8, false, false, cutlass::half_t, Flash_kernel_traits<256, 128, 64, 8, cutlass::half_t>>, Is_dropout=false, Is_causal=false, Is_even_N=true, Is_even_K=false, Return_softmax=false, Params=Flash_fwd_params]" 
(588): here
            instantiation of "void flash::compute_attn<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &) [with Kernel_traits=Flash_fwd_kernel_traits<256, 128, 64, 8, false, false, cutlass::half_t, Flash_kernel_traits<256, 128, 64, 8, cutlass::half_t>>, Is_dropout=false, Is_causal=false, Is_even_N=true, Is_even_K=false, Return_softmax=false, Params=Flash_fwd_params]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(15): here
            instantiation of "void flash_fwd_kernel<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax>(Flash_fwd_params) [with Kernel_traits=Flash_fwd_kernel_traits<256, 128, 64, 8, false, false, cutlass::half_t, Flash_kernel_traits<256, 128, 64, 8, cutlass::half_t>>, Is_dropout=false, Is_causal=false, Is_even_N=true, Is_even_K=false, Return_softmax=false]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(34): here
            instantiation of "void run_flash_fwd<Kernel_traits,Is_dropout,Is_causal>(Flash_fwd_params &, cudaStream_t) [with Kernel_traits=Flash_fwd_kernel_traits<256, 128, 64, 8, false, false, cutlass::half_t, Flash_kernel_traits<256, 128, 64, 8, cutlass::half_t>>, Is_dropout=false, Is_causal=false]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(236): here
            instantiation of "void run_mha_fwd_hdim256<T>(Flash_fwd_params &, cudaStream_t) [with T=cutlass::half_t]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_hdim256_fp16_sm80.cu(8): here

/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_kernel.h(140): warning: variable "MMA_M" was declared but never referenced
          detected during:
            instantiation of "void flash::compute_attn_1rowblock<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &, int, int, int) [with Kernel_traits=Flash_fwd_kernel_traits<256, 128, 64, 8, false, false, cutlass::half_t, Flash_kernel_traits<256, 128, 64, 8, cutlass::half_t>>, Is_dropout=false, Is_causal=false, Is_even_N=false, Is_even_K=true, Return_softmax=false, Params=Flash_fwd_params]" 
(588): here
            instantiation of "void flash::compute_attn<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &) [with Kernel_traits=Flash_fwd_kernel_traits<256, 128, 64, 8, false, false, cutlass::half_t, Flash_kernel_traits<256, 128, 64, 8, cutlass::half_t>>, Is_dropout=false, Is_causal=false, Is_even_N=false, Is_even_K=true, Return_softmax=false, Params=Flash_fwd_params]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(15): here
            instantiation of "void flash_fwd_kernel<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax>(Flash_fwd_params) [with Kernel_traits=Flash_fwd_kernel_traits<256, 128, 64, 8, false, false, cutlass::half_t, Flash_kernel_traits<256, 128, 64, 8, cutlass::half_t>>, Is_dropout=false, Is_causal=false, Is_even_N=false, Is_even_K=true, Return_softmax=false]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(34): here
            instantiation of "void run_flash_fwd<Kernel_traits,Is_dropout,Is_causal>(Flash_fwd_params &, cudaStream_t) [with Kernel_traits=Flash_fwd_kernel_traits<256, 128, 64, 8, false, false, cutlass::half_t, Flash_kernel_traits<256, 128, 64, 8, cutlass::half_t>>, Is_dropout=false, Is_causal=false]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(236): here
            instantiation of "void run_mha_fwd_hdim256<T>(Flash_fwd_params &, cudaStream_t) [with T=cutlass::half_t]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_hdim256_fp16_sm80.cu(8): here

/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_kernel.h(275): warning: variable "tQrQ" was declared but never referenced
          detected during:
            instantiation of "void flash::compute_attn_1rowblock<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &, int, int, int) [with Kernel_traits=Flash_fwd_kernel_traits<256, 128, 64, 8, false, false, cutlass::half_t, Flash_kernel_traits<256, 128, 64, 8, cutlass::half_t>>, Is_dropout=false, Is_causal=false, Is_even_N=false, Is_even_K=true, Return_softmax=false, Params=Flash_fwd_params]" 
(588): here
            instantiation of "void flash::compute_attn<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &) [with Kernel_traits=Flash_fwd_kernel_traits<256, 128, 64, 8, false, false, cutlass::half_t, Flash_kernel_traits<256, 128, 64, 8, cutlass::half_t>>, Is_dropout=false, Is_causal=false, Is_even_N=false, Is_even_K=true, Return_softmax=false, Params=Flash_fwd_params]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(15): here
            instantiation of "void flash_fwd_kernel<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax>(Flash_fwd_params) [with Kernel_traits=Flash_fwd_kernel_traits<256, 128, 64, 8, false, false, cutlass::half_t, Flash_kernel_traits<256, 128, 64, 8, cutlass::half_t>>, Is_dropout=false, Is_causal=false, Is_even_N=false, Is_even_K=true, Return_softmax=false]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(34): here
            instantiation of "void run_flash_fwd<Kernel_traits,Is_dropout,Is_causal>(Flash_fwd_params &, cudaStream_t) [with Kernel_traits=Flash_fwd_kernel_traits<256, 128, 64, 8, false, false, cutlass::half_t, Flash_kernel_traits<256, 128, 64, 8, cutlass::half_t>>, Is_dropout=false, Is_causal=false]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(236): here
            instantiation of "void run_mha_fwd_hdim256<T>(Flash_fwd_params &, cudaStream_t) [with T=cutlass::half_t]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_hdim256_fp16_sm80.cu(8): here

/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_kernel.h(140): warning: variable "MMA_M" was declared but never referenced
          detected during:
            instantiation of "void flash::compute_attn_1rowblock<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &, int, int, int) [with Kernel_traits=Flash_fwd_kernel_traits<256, 128, 64, 8, false, false, cutlass::half_t, Flash_kernel_traits<256, 128, 64, 8, cutlass::half_t>>, Is_dropout=false, Is_causal=false, Is_even_N=false, Is_even_K=false, Return_softmax=false, Params=Flash_fwd_params]" 
(588): here
            instantiation of "void flash::compute_attn<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &) [with Kernel_traits=Flash_fwd_kernel_traits<256, 128, 64, 8, false, false, cutlass::half_t, Flash_kernel_traits<256, 128, 64, 8, cutlass::half_t>>, Is_dropout=false, Is_causal=false, Is_even_N=false, Is_even_K=false, Return_softmax=false, Params=Flash_fwd_params]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(15): here
            instantiation of "void flash_fwd_kernel<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax>(Flash_fwd_params) [with Kernel_traits=Flash_fwd_kernel_traits<256, 128, 64, 8, false, false, cutlass::half_t, Flash_kernel_traits<256, 128, 64, 8, cutlass::half_t>>, Is_dropout=false, Is_causal=false, Is_even_N=false, Is_even_K=false, Return_softmax=false]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(34): here
            instantiation of "void run_flash_fwd<Kernel_traits,Is_dropout,Is_causal>(Flash_fwd_params &, cudaStream_t) [with Kernel_traits=Flash_fwd_kernel_traits<256, 128, 64, 8, false, false, cutlass::half_t, Flash_kernel_traits<256, 128, 64, 8, cutlass::half_t>>, Is_dropout=false, Is_causal=false]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(236): here
            instantiation of "void run_mha_fwd_hdim256<T>(Flash_fwd_params &, cudaStream_t) [with T=cutlass::half_t]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_hdim256_fp16_sm80.cu(8): here

/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_kernel.h(275): warning: variable "tQrQ" was declared but never referenced
          detected during:
            instantiation of "void flash::compute_attn_1rowblock<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &, int, int, int) [with Kernel_traits=Flash_fwd_kernel_traits<256, 128, 64, 8, false, false, cutlass::half_t, Flash_kernel_traits<256, 128, 64, 8, cutlass::half_t>>, Is_dropout=false, Is_causal=false, Is_even_N=false, Is_even_K=false, Return_softmax=false, Params=Flash_fwd_params]" 
(588): here
            instantiation of "void flash::compute_attn<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &) [with Kernel_traits=Flash_fwd_kernel_traits<256, 128, 64, 8, false, false, cutlass::half_t, Flash_kernel_traits<256, 128, 64, 8, cutlass::half_t>>, Is_dropout=false, Is_causal=false, Is_even_N=false, Is_even_K=false, Return_softmax=false, Params=Flash_fwd_params]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(15): here
            instantiation of "void flash_fwd_kernel<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax>(Flash_fwd_params) [with Kernel_traits=Flash_fwd_kernel_traits<256, 128, 64, 8, false, false, cutlass::half_t, Flash_kernel_traits<256, 128, 64, 8, cutlass::half_t>>, Is_dropout=false, Is_causal=false, Is_even_N=false, Is_even_K=false, Return_softmax=false]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(34): here
            instantiation of "void run_flash_fwd<Kernel_traits,Is_dropout,Is_causal>(Flash_fwd_params &, cudaStream_t) [with Kernel_traits=Flash_fwd_kernel_traits<256, 128, 64, 8, false, false, cutlass::half_t, Flash_kernel_traits<256, 128, 64, 8, cutlass::half_t>>, Is_dropout=false, Is_causal=false]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(236): here
            instantiation of "void run_mha_fwd_hdim256<T>(Flash_fwd_params &, cudaStream_t) [with T=cutlass::half_t]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_hdim256_fp16_sm80.cu(8): here

/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_kernel.h(140): warning: variable "MMA_M" was declared but never referenced
          detected during:
            instantiation of "void flash::compute_attn_1rowblock<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &, int, int, int) [with Kernel_traits=Flash_fwd_kernel_traits<256, 64, 64, 4, false, false, cutlass::half_t, Flash_kernel_traits<256, 64, 64, 4, cutlass::half_t>>, Is_dropout=false, Is_causal=false, Is_even_N=true, Is_even_K=true, Return_softmax=false, Params=Flash_fwd_params]" 
(588): here
            instantiation of "void flash::compute_attn<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &) [with Kernel_traits=Flash_fwd_kernel_traits<256, 64, 64, 4, false, false, cutlass::half_t, Flash_kernel_traits<256, 64, 64, 4, cutlass::half_t>>, Is_dropout=false, Is_causal=false, Is_even_N=true, Is_even_K=true, Return_softmax=false, Params=Flash_fwd_params]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(15): here
            instantiation of "void flash_fwd_kernel<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax>(Flash_fwd_params) [with Kernel_traits=Flash_fwd_kernel_traits<256, 64, 64, 4, false, false, cutlass::half_t, Flash_kernel_traits<256, 64, 64, 4, cutlass::half_t>>, Is_dropout=false, Is_causal=false, Is_even_N=true, Is_even_K=true, Return_softmax=false]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(34): here
            instantiation of "void run_flash_fwd<Kernel_traits,Is_dropout,Is_causal>(Flash_fwd_params &, cudaStream_t) [with Kernel_traits=Flash_fwd_kernel_traits<256, 64, 64, 4, false, false, cutlass::half_t, Flash_kernel_traits<256, 64, 64, 4, cutlass::half_t>>, Is_dropout=false, Is_causal=false]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(236): here
            instantiation of "void run_mha_fwd_hdim256<T>(Flash_fwd_params &, cudaStream_t) [with T=cutlass::half_t]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_hdim256_fp16_sm80.cu(8): here

/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_kernel.h(275): warning: variable "tQrQ" was declared but never referenced
          detected during:
            instantiation of "void flash::compute_attn_1rowblock<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &, int, int, int) [with Kernel_traits=Flash_fwd_kernel_traits<256, 64, 64, 4, false, false, cutlass::half_t, Flash_kernel_traits<256, 64, 64, 4, cutlass::half_t>>, Is_dropout=false, Is_causal=false, Is_even_N=true, Is_even_K=true, Return_softmax=false, Params=Flash_fwd_params]" 
(588): here
            instantiation of "void flash::compute_attn<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &) [with Kernel_traits=Flash_fwd_kernel_traits<256, 64, 64, 4, false, false, cutlass::half_t, Flash_kernel_traits<256, 64, 64, 4, cutlass::half_t>>, Is_dropout=false, Is_causal=false, Is_even_N=true, Is_even_K=true, Return_softmax=false, Params=Flash_fwd_params]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(15): here
            instantiation of "void flash_fwd_kernel<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax>(Flash_fwd_params) [with Kernel_traits=Flash_fwd_kernel_traits<256, 64, 64, 4, false, false, cutlass::half_t, Flash_kernel_traits<256, 64, 64, 4, cutlass::half_t>>, Is_dropout=false, Is_causal=false, Is_even_N=true, Is_even_K=true, Return_softmax=false]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(34): here
            instantiation of "void run_flash_fwd<Kernel_traits,Is_dropout,Is_causal>(Flash_fwd_params &, cudaStream_t) [with Kernel_traits=Flash_fwd_kernel_traits<256, 64, 64, 4, false, false, cutlass::half_t, Flash_kernel_traits<256, 64, 64, 4, cutlass::half_t>>, Is_dropout=false, Is_causal=false]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(236): here
            instantiation of "void run_mha_fwd_hdim256<T>(Flash_fwd_params &, cudaStream_t) [with T=cutlass::half_t]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_hdim256_fp16_sm80.cu(8): here

/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_kernel.h(140): warning: variable "MMA_M" was declared but never referenced
          detected during:
            instantiation of "void flash::compute_attn_1rowblock<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &, int, int, int) [with Kernel_traits=Flash_fwd_kernel_traits<256, 64, 64, 4, false, false, cutlass::half_t, Flash_kernel_traits<256, 64, 64, 4, cutlass::half_t>>, Is_dropout=false, Is_causal=false, Is_even_N=true, Is_even_K=false, Return_softmax=false, Params=Flash_fwd_params]" 
(588): here
            instantiation of "void flash::compute_attn<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &) [with Kernel_traits=Flash_fwd_kernel_traits<256, 64, 64, 4, false, false, cutlass::half_t, Flash_kernel_traits<256, 64, 64, 4, cutlass::half_t>>, Is_dropout=false, Is_causal=false, Is_even_N=true, Is_even_K=false, Return_softmax=false, Params=Flash_fwd_params]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(15): here
            instantiation of "void flash_fwd_kernel<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax>(Flash_fwd_params) [with Kernel_traits=Flash_fwd_kernel_traits<256, 64, 64, 4, false, false, cutlass::half_t, Flash_kernel_traits<256, 64, 64, 4, cutlass::half_t>>, Is_dropout=false, Is_causal=false, Is_even_N=true, Is_even_K=false, Return_softmax=false]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(34): here
            instantiation of "void run_flash_fwd<Kernel_traits,Is_dropout,Is_causal>(Flash_fwd_params &, cudaStream_t) [with Kernel_traits=Flash_fwd_kernel_traits<256, 64, 64, 4, false, false, cutlass::half_t, Flash_kernel_traits<256, 64, 64, 4, cutlass::half_t>>, Is_dropout=false, Is_causal=false]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(236): here
            instantiation of "void run_mha_fwd_hdim256<T>(Flash_fwd_params &, cudaStream_t) [with T=cutlass::half_t]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_hdim256_fp16_sm80.cu(8): here

/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_kernel.h(275): warning: variable "tQrQ" was declared but never referenced
          detected during:
            instantiation of "void flash::compute_attn_1rowblock<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &, int, int, int) [with Kernel_traits=Flash_fwd_kernel_traits<256, 64, 64, 4, false, false, cutlass::half_t, Flash_kernel_traits<256, 64, 64, 4, cutlass::half_t>>, Is_dropout=false, Is_causal=false, Is_even_N=true, Is_even_K=false, Return_softmax=false, Params=Flash_fwd_params]" 
(588): here
            instantiation of "void flash::compute_attn<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &) [with Kernel_traits=Flash_fwd_kernel_traits<256, 64, 64, 4, false, false, cutlass::half_t, Flash_kernel_traits<256, 64, 64, 4, cutlass::half_t>>, Is_dropout=false, Is_causal=false, Is_even_N=true, Is_even_K=false, Return_softmax=false, Params=Flash_fwd_params]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(15): here
            instantiation of "void flash_fwd_kernel<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax>(Flash_fwd_params) [with Kernel_traits=Flash_fwd_kernel_traits<256, 64, 64, 4, false, false, cutlass::half_t, Flash_kernel_traits<256, 64, 64, 4, cutlass::half_t>>, Is_dropout=false, Is_causal=false, Is_even_N=true, Is_even_K=false, Return_softmax=false]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(34): here
            instantiation of "void run_flash_fwd<Kernel_traits,Is_dropout,Is_causal>(Flash_fwd_params &, cudaStream_t) [with Kernel_traits=Flash_fwd_kernel_traits<256, 64, 64, 4, false, false, cutlass::half_t, Flash_kernel_traits<256, 64, 64, 4, cutlass::half_t>>, Is_dropout=false, Is_causal=false]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(236): here
            instantiation of "void run_mha_fwd_hdim256<T>(Flash_fwd_params &, cudaStream_t) [with T=cutlass::half_t]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_hdim256_fp16_sm80.cu(8): here

/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_kernel.h(140): warning: variable "MMA_M" was declared but never referenced
          detected during:
            instantiation of "void flash::compute_attn_1rowblock<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &, int, int, int) [with Kernel_traits=Flash_fwd_kernel_traits<256, 64, 64, 4, false, false, cutlass::half_t, Flash_kernel_traits<256, 64, 64, 4, cutlass::half_t>>, Is_dropout=false, Is_causal=false, Is_even_N=false, Is_even_K=true, Return_softmax=false, Params=Flash_fwd_params]" 
(588): here
            instantiation of "void flash::compute_attn<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &) [with Kernel_traits=Flash_fwd_kernel_traits<256, 64, 64, 4, false, false, cutlass::half_t, Flash_kernel_traits<256, 64, 64, 4, cutlass::half_t>>, Is_dropout=false, Is_causal=false, Is_even_N=false, Is_even_K=true, Return_softmax=false, Params=Flash_fwd_params]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(15): here
            instantiation of "void flash_fwd_kernel<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax>(Flash_fwd_params) [with Kernel_traits=Flash_fwd_kernel_traits<256, 64, 64, 4, false, false, cutlass::half_t, Flash_kernel_traits<256, 64, 64, 4, cutlass::half_t>>, Is_dropout=false, Is_causal=false, Is_even_N=false, Is_even_K=true, Return_softmax=false]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(34): here
            instantiation of "void run_flash_fwd<Kernel_traits,Is_dropout,Is_causal>(Flash_fwd_params &, cudaStream_t) [with Kernel_traits=Flash_fwd_kernel_traits<256, 64, 64, 4, false, false, cutlass::half_t, Flash_kernel_traits<256, 64, 64, 4, cutlass::half_t>>, Is_dropout=false, Is_causal=false]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(236): here
            instantiation of "void run_mha_fwd_hdim256<T>(Flash_fwd_params &, cudaStream_t) [with T=cutlass::half_t]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_hdim256_fp16_sm80.cu(8): here

/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_kernel.h(275): warning: variable "tQrQ" was declared but never referenced
          detected during:
            instantiation of "void flash::compute_attn_1rowblock<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &, int, int, int) [with Kernel_traits=Flash_fwd_kernel_traits<256, 64, 64, 4, false, false, cutlass::half_t, Flash_kernel_traits<256, 64, 64, 4, cutlass::half_t>>, Is_dropout=false, Is_causal=false, Is_even_N=false, Is_even_K=true, Return_softmax=false, Params=Flash_fwd_params]" 
(588): here
            instantiation of "void flash::compute_attn<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &) [with Kernel_traits=Flash_fwd_kernel_traits<256, 64, 64, 4, false, false, cutlass::half_t, Flash_kernel_traits<256, 64, 64, 4, cutlass::half_t>>, Is_dropout=false, Is_causal=false, Is_even_N=false, Is_even_K=true, Return_softmax=false, Params=Flash_fwd_params]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(15): here
            instantiation of "void flash_fwd_kernel<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax>(Flash_fwd_params) [with Kernel_traits=Flash_fwd_kernel_traits<256, 64, 64, 4, false, false, cutlass::half_t, Flash_kernel_traits<256, 64, 64, 4, cutlass::half_t>>, Is_dropout=false, Is_causal=false, Is_even_N=false, Is_even_K=true, Return_softmax=false]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(34): here
            instantiation of "void run_flash_fwd<Kernel_traits,Is_dropout,Is_causal>(Flash_fwd_params &, cudaStream_t) [with Kernel_traits=Flash_fwd_kernel_traits<256, 64, 64, 4, false, false, cutlass::half_t, Flash_kernel_traits<256, 64, 64, 4, cutlass::half_t>>, Is_dropout=false, Is_causal=false]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(236): here
            instantiation of "void run_mha_fwd_hdim256<T>(Flash_fwd_params &, cudaStream_t) [with T=cutlass::half_t]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_hdim256_fp16_sm80.cu(8): here

/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_kernel.h(140): warning: variable "MMA_M" was declared but never referenced
          detected during:
            instantiation of "void flash::compute_attn_1rowblock<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &, int, int, int) [with Kernel_traits=Flash_fwd_kernel_traits<256, 64, 64, 4, false, false, cutlass::half_t, Flash_kernel_traits<256, 64, 64, 4, cutlass::half_t>>, Is_dropout=false, Is_causal=false, Is_even_N=false, Is_even_K=false, Return_softmax=false, Params=Flash_fwd_params]" 
(588): here
            instantiation of "void flash::compute_attn<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &) [with Kernel_traits=Flash_fwd_kernel_traits<256, 64, 64, 4, false, false, cutlass::half_t, Flash_kernel_traits<256, 64, 64, 4, cutlass::half_t>>, Is_dropout=false, Is_causal=false, Is_even_N=false, Is_even_K=false, Return_softmax=false, Params=Flash_fwd_params]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(15): here
            instantiation of "void flash_fwd_kernel<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax>(Flash_fwd_params) [with Kernel_traits=Flash_fwd_kernel_traits<256, 64, 64, 4, false, false, cutlass::half_t, Flash_kernel_traits<256, 64, 64, 4, cutlass::half_t>>, Is_dropout=false, Is_causal=false, Is_even_N=false, Is_even_K=false, Return_softmax=false]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(34): here
            instantiation of "void run_flash_fwd<Kernel_traits,Is_dropout,Is_causal>(Flash_fwd_params &, cudaStream_t) [with Kernel_traits=Flash_fwd_kernel_traits<256, 64, 64, 4, false, false, cutlass::half_t, Flash_kernel_traits<256, 64, 64, 4, cutlass::half_t>>, Is_dropout=false, Is_causal=false]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(236): here
            instantiation of "void run_mha_fwd_hdim256<T>(Flash_fwd_params &, cudaStream_t) [with T=cutlass::half_t]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_hdim256_fp16_sm80.cu(8): here

/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_kernel.h(275): warning: variable "tQrQ" was declared but never referenced
          detected during:
            instantiation of "void flash::compute_attn_1rowblock<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &, int, int, int) [with Kernel_traits=Flash_fwd_kernel_traits<256, 64, 64, 4, false, false, cutlass::half_t, Flash_kernel_traits<256, 64, 64, 4, cutlass::half_t>>, Is_dropout=false, Is_causal=false, Is_even_N=false, Is_even_K=false, Return_softmax=false, Params=Flash_fwd_params]" 
(588): here
            instantiation of "void flash::compute_attn<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &) [with Kernel_traits=Flash_fwd_kernel_traits<256, 64, 64, 4, false, false, cutlass::half_t, Flash_kernel_traits<256, 64, 64, 4, cutlass::half_t>>, Is_dropout=false, Is_causal=false, Is_even_N=false, Is_even_K=false, Return_softmax=false, Params=Flash_fwd_params]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(15): here
            instantiation of "void flash_fwd_kernel<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax>(Flash_fwd_params) [with Kernel_traits=Flash_fwd_kernel_traits<256, 64, 64, 4, false, false, cutlass::half_t, Flash_kernel_traits<256, 64, 64, 4, cutlass::half_t>>, Is_dropout=false, Is_causal=false, Is_even_N=false, Is_even_K=false, Return_softmax=false]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(34): here
            instantiation of "void run_flash_fwd<Kernel_traits,Is_dropout,Is_causal>(Flash_fwd_params &, cudaStream_t) [with Kernel_traits=Flash_fwd_kernel_traits<256, 64, 64, 4, false, false, cutlass::half_t, Flash_kernel_traits<256, 64, 64, 4, cutlass::half_t>>, Is_dropout=false, Is_causal=false]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(236): here
            instantiation of "void run_mha_fwd_hdim256<T>(Flash_fwd_params &, cudaStream_t) [with T=cutlass::half_t]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_hdim256_fp16_sm80.cu(8): here

ptxas info    : 2 bytes gmem
ptxas info    : Compiling entry function '_Z16flash_fwd_kernelI23Flash_fwd_kernel_traitsILi256ELi64ELi64ELi4ELb0ELb0EN7cutlass6half_tE19Flash_kernel_traitsILi256ELi64ELi64ELi4ES2_EELb0ELb0ELb0ELb0ELb0EEv16Flash_fwd_params' for 'sm_80'
ptxas info    : Function properties for _Z16flash_fwd_kernelI23Flash_fwd_kernel_traitsILi256ELi64ELi64ELi4ELb0ELb0EN7cutlass6half_tE19Flash_kernel_traitsILi256ELi64ELi64ELi4ES2_EELb0ELb0ELb0ELb0ELb0EEv16Flash_fwd_params
    0 bytes stack frame, 0 bytes spill stores, 0 bytes spill loads
ptxas info    : Used 241 registers, 584 bytes cmem[0]
ptxas info    : Compiling entry function '_Z16flash_fwd_kernelI23Flash_fwd_kernel_traitsILi256ELi64ELi64ELi4ELb0ELb0EN7cutlass6half_tE19Flash_kernel_traitsILi256ELi64ELi64ELi4ES2_EELb0ELb0ELb0ELb1ELb0EEv16Flash_fwd_params' for 'sm_80'
ptxas info    : Function properties for _Z16flash_fwd_kernelI23Flash_fwd_kernel_traitsILi256ELi64ELi64ELi4ELb0ELb0EN7cutlass6half_tE19Flash_kernel_traitsILi256ELi64ELi64ELi4ES2_EELb0ELb0ELb0ELb1ELb0EEv16Flash_fwd_params
    0 bytes stack frame, 0 bytes spill stores, 0 bytes spill loads
ptxas info    : Used 237 registers, 584 bytes cmem[0]
ptxas info    : Compiling entry function '_Z16flash_fwd_kernelI23Flash_fwd_kernel_traitsILi256ELi64ELi64ELi4ELb0ELb0EN7cutlass6half_tE19Flash_kernel_traitsILi256ELi64ELi64ELi4ES2_EELb0ELb0ELb1ELb0ELb0EEv16Flash_fwd_params' for 'sm_80'
ptxas info    : Function properties for _Z16flash_fwd_kernelI23Flash_fwd_kernel_traitsILi256ELi64ELi64ELi4ELb0ELb0EN7cutlass6half_tE19Flash_kernel_traitsILi256ELi64ELi64ELi4ES2_EELb0ELb0ELb1ELb0ELb0EEv16Flash_fwd_params
    0 bytes stack frame, 0 bytes spill stores, 0 bytes spill loads
ptxas info    : Used 233 registers, 584 bytes cmem[0]
ptxas info    : Compiling entry function '_Z16flash_fwd_kernelI23Flash_fwd_kernel_traitsILi256ELi64ELi64ELi4ELb0ELb0EN7cutlass6half_tE19Flash_kernel_traitsILi256ELi64ELi64ELi4ES2_EELb0ELb0ELb1ELb1ELb0EEv16Flash_fwd_params' for 'sm_80'
ptxas info    : Function properties for _Z16flash_fwd_kernelI23Flash_fwd_kernel_traitsILi256ELi64ELi64ELi4ELb0ELb0EN7cutlass6half_tE19Flash_kernel_traitsILi256ELi64ELi64ELi4ES2_EELb0ELb0ELb1ELb1ELb0EEv16Flash_fwd_params
    0 bytes stack frame, 0 bytes spill stores, 0 bytes spill loads
ptxas info    : Used 237 registers, 584 bytes cmem[0]
ptxas info    : Compiling entry function '_Z16flash_fwd_kernelI23Flash_fwd_kernel_traitsILi256ELi128ELi64ELi8ELb0ELb0EN7cutlass6half_tE19Flash_kernel_traitsILi256ELi128ELi64ELi8ES2_EELb0ELb0ELb0ELb0ELb0EEv16Flash_fwd_params' for 'sm_80'
ptxas info    : Function properties for _Z16flash_fwd_kernelI23Flash_fwd_kernel_traitsILi256ELi128ELi64ELi8ELb0ELb0EN7cutlass6half_tE19Flash_kernel_traitsILi256ELi128ELi64ELi8ES2_EELb0ELb0ELb0ELb0ELb0EEv16Flash_fwd_params
    0 bytes stack frame, 0 bytes spill stores, 0 bytes spill loads
ptxas info    : Used 241 registers, 584 bytes cmem[0]
ptxas info    : Compiling entry function '_Z16flash_fwd_kernelI23Flash_fwd_kernel_traitsILi256ELi128ELi64ELi8ELb0ELb0EN7cutlass6half_tE19Flash_kernel_traitsILi256ELi128ELi64ELi8ES2_EELb0ELb0ELb0ELb1ELb0EEv16Flash_fwd_params' for 'sm_80'
ptxas info    : Function properties for _Z16flash_fwd_kernelI23Flash_fwd_kernel_traitsILi256ELi128ELi64ELi8ELb0ELb0EN7cutlass6half_tE19Flash_kernel_traitsILi256ELi128ELi64ELi8ES2_EELb0ELb0ELb0ELb1ELb0EEv16Flash_fwd_params
    0 bytes stack frame, 0 bytes spill stores, 0 bytes spill loads
ptxas info    : Used 237 registers, 584 bytes cmem[0]
ptxas info    : Compiling entry function '_Z16flash_fwd_kernelI23Flash_fwd_kernel_traitsILi256ELi128ELi64ELi8ELb0ELb0EN7cutlass6half_tE19Flash_kernel_traitsILi256ELi128ELi64ELi8ES2_EELb0ELb0ELb1ELb0ELb0EEv16Flash_fwd_params' for 'sm_80'
ptxas info    : Function properties for _Z16flash_fwd_kernelI23Flash_fwd_kernel_traitsILi256ELi128ELi64ELi8ELb0ELb0EN7cutlass6half_tE19Flash_kernel_traitsILi256ELi128ELi64ELi8ES2_EELb0ELb0ELb1ELb0ELb0EEv16Flash_fwd_params
    0 bytes stack frame, 0 bytes spill stores, 0 bytes spill loads
ptxas info    : Used 240 registers, 584 bytes cmem[0]
ptxas info    : Compiling entry function '_Z16flash_fwd_kernelI23Flash_fwd_kernel_traitsILi256ELi128ELi64ELi8ELb0ELb0EN7cutlass6half_tE19Flash_kernel_traitsILi256ELi128ELi64ELi8ES2_EELb0ELb0ELb1ELb1ELb0EEv16Flash_fwd_params' for 'sm_80'
ptxas info    : Function properties for _Z16flash_fwd_kernelI23Flash_fwd_kernel_traitsILi256ELi128ELi64ELi8ELb0ELb0EN7cutlass6half_tE19Flash_kernel_traitsILi256ELi128ELi64ELi8ES2_EELb0ELb0ELb1ELb1ELb0EEv16Flash_fwd_params
    0 bytes stack frame, 0 bytes spill stores, 0 bytes spill loads
ptxas info    : Used 237 registers, 584 bytes cmem[0]
ptxas info    : Compiling entry function '_Z16flash_fwd_kernelI23Flash_fwd_kernel_traitsILi256ELi64ELi64ELi4ELb0ELb0EN7cutlass6half_tE19Flash_kernel_traitsILi256ELi64ELi64ELi4ES2_EELb0ELb1ELb0ELb0ELb0EEv16Flash_fwd_params' for 'sm_80'
ptxas info    : Function properties for _Z16flash_fwd_kernelI23Flash_fwd_kernel_traitsILi256ELi64ELi64ELi4ELb0ELb0EN7cutlass6half_tE19Flash_kernel_traitsILi256ELi64ELi64ELi4ES2_EELb0ELb1ELb0ELb0ELb0EEv16Flash_fwd_params
    0 bytes stack frame, 0 bytes spill stores, 0 bytes spill loads
ptxas info    : Used 241 registers, 584 bytes cmem[0]
ptxas info    : Compiling entry function '_Z16flash_fwd_kernelI23Flash_fwd_kernel_traitsILi256ELi64ELi64ELi4ELb0ELb0EN7cutlass6half_tE19Flash_kernel_traitsILi256ELi64ELi64ELi4ES2_EELb0ELb1ELb0ELb1ELb0EEv16Flash_fwd_params' for 'sm_80'
ptxas info    : Function properties for _Z16flash_fwd_kernelI23Flash_fwd_kernel_traitsILi256ELi64ELi64ELi4ELb0ELb0EN7cutlass6half_tE19Flash_kernel_traitsILi256ELi64ELi64ELi4ES2_EELb0ELb1ELb0ELb1ELb0EEv16Flash_fwd_params
    0 bytes stack frame, 0 bytes spill stores, 0 bytes spill loads
ptxas info    : Used 238 registers, 584 bytes cmem[0]
ptxas info    : Compiling entry function '_Z16flash_fwd_kernelI23Flash_fwd_kernel_traitsILi256ELi64ELi64ELi4ELb0ELb0EN7cutlass6half_tE19Flash_kernel_traitsILi256ELi64ELi64ELi4ES2_EELb0ELb1ELb1ELb0ELb0EEv16Flash_fwd_params' for 'sm_80'
ptxas info    : Function properties for _Z16flash_fwd_kernelI23Flash_fwd_kernel_traitsILi256ELi64ELi64ELi4ELb0ELb0EN7cutlass6half_tE19Flash_kernel_traitsILi256ELi64ELi64ELi4ES2_EELb0ELb1ELb1ELb0ELb0EEv16Flash_fwd_params
    0 bytes stack frame, 0 bytes spill stores, 0 bytes spill loads
ptxas info    : Used 245 registers, 584 bytes cmem[0]
ptxas info    : Compiling entry function '_Z16flash_fwd_kernelI23Flash_fwd_kernel_traitsILi256ELi64ELi64ELi4ELb0ELb0EN7cutlass6half_tE19Flash_kernel_traitsILi256ELi64ELi64ELi4ES2_EELb0ELb1ELb1ELb1ELb0EEv16Flash_fwd_params' for 'sm_80'
ptxas info    : Function properties for _Z16flash_fwd_kernelI23Flash_fwd_kernel_traitsILi256ELi64ELi64ELi4ELb0ELb0EN7cutlass6half_tE19Flash_kernel_traitsILi256ELi64ELi64ELi4ES2_EELb0ELb1ELb1ELb1ELb0EEv16Flash_fwd_params
    0 bytes stack frame, 0 bytes spill stores, 0 bytes spill loads
ptxas info    : Used 241 registers, 584 bytes cmem[0]
ptxas info    : Compiling entry function '_Z16flash_fwd_kernelI23Flash_fwd_kernel_traitsILi256ELi128ELi64ELi8ELb0ELb0EN7cutlass6half_tE19Flash_kernel_traitsILi256ELi128ELi64ELi8ES2_EELb0ELb1ELb0ELb0ELb0EEv16Flash_fwd_params' for 'sm_80'
ptxas info    : Function properties for _Z16flash_fwd_kernelI23Flash_fwd_kernel_traitsILi256ELi128ELi64ELi8ELb0ELb0EN7cutlass6half_tE19Flash_kernel_traitsILi256ELi128ELi64ELi8ES2_EELb0ELb1ELb0ELb0ELb0EEv16Flash_fwd_params
    0 bytes stack frame, 0 bytes spill stores, 0 bytes spill loads
ptxas info    : Used 248 registers, 584 bytes cmem[0]
ptxas info    : Compiling entry function '_Z16flash_fwd_kernelI23Flash_fwd_kernel_traitsILi256ELi128ELi64ELi8ELb0ELb0EN7cutlass6half_tE19Flash_kernel_traitsILi256ELi128ELi64ELi8ES2_EELb0ELb1ELb0ELb1ELb0EEv16Flash_fwd_params' for 'sm_80'
ptxas info    : Function properties for _Z16flash_fwd_kernelI23Flash_fwd_kernel_traitsILi256ELi128ELi64ELi8ELb0ELb0EN7cutlass6half_tE19Flash_kernel_traitsILi256ELi128ELi64ELi8ES2_EELb0ELb1ELb0ELb1ELb0EEv16Flash_fwd_params
    0 bytes stack frame, 0 bytes spill stores, 0 bytes spill loads
ptxas info    : Used 245 registers, 584 bytes cmem[0]
ptxas info    : Compiling entry function '_Z16flash_fwd_kernelI23Flash_fwd_kernel_traitsILi256ELi128ELi64ELi8ELb0ELb0EN7cutlass6half_tE19Flash_kernel_traitsILi256ELi128ELi64ELi8ES2_EELb0ELb1ELb1ELb0ELb0EEv16Flash_fwd_params' for 'sm_80'
ptxas info    : Function properties for _Z16flash_fwd_kernelI23Flash_fwd_kernel_traitsILi256ELi128ELi64ELi8ELb0ELb0EN7cutlass6half_tE19Flash_kernel_traitsILi256ELi128ELi64ELi8ES2_EELb0ELb1ELb1ELb0ELb0EEv16Flash_fwd_params
    0 bytes stack frame, 0 bytes spill stores, 0 bytes spill loads
ptxas info    : Used 248 registers, 584 bytes cmem[0]
ptxas info    : Compiling entry function '_Z16flash_fwd_kernelI23Flash_fwd_kernel_traitsILi256ELi128ELi64ELi8ELb0ELb0EN7cutlass6half_tE19Flash_kernel_traitsILi256ELi128ELi64ELi8ES2_EELb0ELb1ELb1ELb1ELb0EEv16Flash_fwd_params' for 'sm_80'
ptxas info    : Function properties for _Z16flash_fwd_kernelI23Flash_fwd_kernel_traitsILi256ELi128ELi64ELi8ELb0ELb0EN7cutlass6half_tE19Flash_kernel_traitsILi256ELi128ELi64ELi8ES2_EELb0ELb1ELb1ELb1ELb0EEv16Flash_fwd_params
    0 bytes stack frame, 0 bytes spill stores, 0 bytes spill loads
ptxas info    : Used 250 registers, 584 bytes cmem[0]
ptxas info    : Compiling entry function '_Z16flash_fwd_kernelI23Flash_fwd_kernel_traitsILi256ELi64ELi64ELi4ELb0ELb0EN7cutlass6half_tE19Flash_kernel_traitsILi256ELi64ELi64ELi4ES2_EELb1ELb0ELb0ELb0ELb0EEv16Flash_fwd_params' for 'sm_80'
ptxas info    : Function properties for _Z16flash_fwd_kernelI23Flash_fwd_kernel_traitsILi256ELi64ELi64ELi4ELb0ELb0EN7cutlass6half_tE19Flash_kernel_traitsILi256ELi64ELi64ELi4ES2_EELb1ELb0ELb0ELb0ELb0EEv16Flash_fwd_params
    0 bytes stack frame, 0 bytes spill stores, 0 bytes spill loads
ptxas info    : Used 246 registers, 584 bytes cmem[0]
ptxas info    : Compiling entry function '_Z16flash_fwd_kernelI23Flash_fwd_kernel_traitsILi256ELi64ELi64ELi4ELb0ELb0EN7cutlass6half_tE19Flash_kernel_traitsILi256ELi64ELi64ELi4ES2_EELb1ELb0ELb0ELb0ELb1EEv16Flash_fwd_params' for 'sm_80'
ptxas info    : Function properties for _Z16flash_fwd_kernelI23Flash_fwd_kernel_traitsILi256ELi64ELi64ELi4ELb0ELb0EN7cutlass6half_tE19Flash_kernel_traitsILi256ELi64ELi64ELi4ES2_EELb1ELb0ELb0ELb0ELb1EEv16Flash_fwd_params
    112 bytes stack frame, 136 bytes spill stores, 128 bytes spill loads
ptxas info    : Used 255 registers, 584 bytes cmem[0]
ptxas info    : Compiling entry function '_Z16flash_fwd_kernelI23Flash_fwd_kernel_traitsILi256ELi64ELi64ELi4ELb0ELb0EN7cutlass6half_tE19Flash_kernel_traitsILi256ELi64ELi64ELi4ES2_EELb1ELb0ELb0ELb1ELb0EEv16Flash_fwd_params' for 'sm_80'
ptxas info    : Function properties for _Z16flash_fwd_kernelI23Flash_fwd_kernel_traitsILi256ELi64ELi64ELi4ELb0ELb0EN7cutlass6half_tE19Flash_kernel_traitsILi256ELi64ELi64ELi4ES2_EELb1ELb0ELb0ELb1ELb0EEv16Flash_fwd_params
    0 bytes stack frame, 0 bytes spill stores, 0 bytes spill loads
ptxas info    : Used 242 registers, 584 bytes cmem[0]
ptxas info    : Compiling entry function '_Z16flash_fwd_kernelI23Flash_fwd_kernel_traitsILi256ELi64ELi64ELi4ELb0ELb0EN7cutlass6half_tE19Flash_kernel_traitsILi256ELi64ELi64ELi4ES2_EELb1ELb0ELb0ELb1ELb1EEv16Flash_fwd_params' for 'sm_80'
ptxas info    : Function properties for _Z16flash_fwd_kernelI23Flash_fwd_kernel_traitsILi256ELi64ELi64ELi4ELb0ELb0EN7cutlass6half_tE19Flash_kernel_traitsILi256ELi64ELi64ELi4ES2_EELb1ELb0ELb0ELb1ELb1EEv16Flash_fwd_params
    104 bytes stack frame, 132 bytes spill stores, 132 bytes spill loads
ptxas info    : Used 255 registers, 584 bytes cmem[0]
ptxas info    : Compiling entry function '_Z16flash_fwd_kernelI23Flash_fwd_kernel_traitsILi256ELi64ELi64ELi4ELb0ELb0EN7cutlass6half_tE19Flash_kernel_traitsILi256ELi64ELi64ELi4ES2_EELb1ELb0ELb1ELb0ELb0EEv16Flash_fwd_params' for 'sm_80'
ptxas info    : Function properties for _Z16flash_fwd_kernelI23Flash_fwd_kernel_traitsILi256ELi64ELi64ELi4ELb0ELb0EN7cutlass6half_tE19Flash_kernel_traitsILi256ELi64ELi64ELi4ES2_EELb1ELb0ELb1ELb0ELb0EEv16Flash_fwd_params
    0 bytes stack frame, 0 bytes spill stores, 0 bytes spill loads
ptxas info    : Used 246 registers, 584 bytes cmem[0]
ptxas info    : Compiling entry function '_Z16flash_fwd_kernelI23Flash_fwd_kernel_traitsILi256ELi64ELi64ELi4ELb0ELb0EN7cutlass6half_tE19Flash_kernel_traitsILi256ELi64ELi64ELi4ES2_EELb1ELb0ELb1ELb0ELb1EEv16Flash_fwd_params' for 'sm_80'
ptxas info    : Function properties for _Z16flash_fwd_kernelI23Flash_fwd_kernel_traitsILi256ELi64ELi64ELi4ELb0ELb0EN7cutlass6half_tE19Flash_kernel_traitsILi256ELi64ELi64ELi4ES2_EELb1ELb0ELb1ELb0ELb1EEv16Flash_fwd_params
    104 bytes stack frame, 140 bytes spill stores, 132 bytes spill loads
ptxas info    : Used 255 registers, 584 bytes cmem[0]
ptxas info    : Compiling entry function '_Z16flash_fwd_kernelI23Flash_fwd_kernel_traitsILi256ELi64ELi64ELi4ELb0ELb0EN7cutlass6half_tE19Flash_kernel_traitsILi256ELi64ELi64ELi4ES2_EELb1ELb0ELb1ELb1ELb0EEv16Flash_fwd_params' for 'sm_80'
ptxas info    : Function properties for _Z16flash_fwd_kernelI23Flash_fwd_kernel_traitsILi256ELi64ELi64ELi4ELb0ELb0EN7cutlass6half_tE19Flash_kernel_traitsILi256ELi64ELi64ELi4ES2_EELb1ELb0ELb1ELb1ELb0EEv16Flash_fwd_params
    0 bytes stack frame, 0 bytes spill stores, 0 bytes spill loads
ptxas info    : Used 242 registers, 584 bytes cmem[0]
ptxas info    : Compiling entry function '_Z16flash_fwd_kernelI23Flash_fwd_kernel_traitsILi256ELi64ELi64ELi4ELb0ELb0EN7cutlass6half_tE19Flash_kernel_traitsILi256ELi64ELi64ELi4ES2_EELb1ELb0ELb1ELb1ELb1EEv16Flash_fwd_params' for 'sm_80'
ptxas info    : Function properties for _Z16flash_fwd_kernelI23Flash_fwd_kernel_traitsILi256ELi64ELi64ELi4ELb0ELb0EN7cutlass6half_tE19Flash_kernel_traitsILi256ELi64ELi64ELi4ES2_EELb1ELb0ELb1ELb1ELb1EEv16Flash_fwd_params
    104 bytes stack frame, 116 bytes spill stores, 116 bytes spill loads
ptxas info    : Used 255 registers, 584 bytes cmem[0]
ptxas info    : Compiling entry function '_Z16flash_fwd_kernelI23Flash_fwd_kernel_traitsILi256ELi128ELi64ELi8ELb0ELb0EN7cutlass6half_tE19Flash_kernel_traitsILi256ELi128ELi64ELi8ES2_EELb1ELb0ELb0ELb0ELb0EEv16Flash_fwd_params' for 'sm_80'
ptxas info    : Function properties for _Z16flash_fwd_kernelI23Flash_fwd_kernel_traitsILi256ELi128ELi64ELi8ELb0ELb0EN7cutlass6half_tE19Flash_kernel_traitsILi256ELi128ELi64ELi8ES2_EELb1ELb0ELb0ELb0ELb0EEv16Flash_fwd_params
    0 bytes stack frame, 0 bytes spill stores, 0 bytes spill loads
ptxas info    : Used 246 registers, 584 bytes cmem[0]
ptxas info    : Compiling entry function '_Z16flash_fwd_kernelI23Flash_fwd_kernel_traitsILi256ELi128ELi64ELi8ELb0ELb0EN7cutlass6half_tE19Flash_kernel_traitsILi256ELi128ELi64ELi8ES2_EELb1ELb0ELb0ELb0ELb1EEv16Flash_fwd_params' for 'sm_80'
ptxas info    : Function properties for _Z16flash_fwd_kernelI23Flash_fwd_kernel_traitsILi256ELi128ELi64ELi8ELb0ELb0EN7cutlass6half_tE19Flash_kernel_traitsILi256ELi128ELi64ELi8ES2_EELb1ELb0ELb0ELb0ELb1EEv16Flash_fwd_params
    112 bytes stack frame, 94 bytes spill stores, 94 bytes spill loads
ptxas info    : Used 255 registers, 584 bytes cmem[0]
ptxas info    : Compiling entry function '_Z16flash_fwd_kernelI23Flash_fwd_kernel_traitsILi256ELi128ELi64ELi8ELb0ELb0EN7cutlass6half_tE19Flash_kernel_traitsILi256ELi128ELi64ELi8ES2_EELb1ELb0ELb0ELb1ELb0EEv16Flash_fwd_params' for 'sm_80'
ptxas info    : Function properties for _Z16flash_fwd_kernelI23Flash_fwd_kernel_traitsILi256ELi128ELi64ELi8ELb0ELb0EN7cutlass6half_tE19Flash_kernel_traitsILi256ELi128ELi64ELi8ES2_EELb1ELb0ELb0ELb1ELb0EEv16Flash_fwd_params
    0 bytes stack frame, 0 bytes spill stores, 0 bytes spill loads
ptxas info    : Used 242 registers, 584 bytes cmem[0]
ptxas info    : Compiling entry function '_Z16flash_fwd_kernelI23Flash_fwd_kernel_traitsILi256ELi128ELi64ELi8ELb0ELb0EN7cutlass6half_tE19Flash_kernel_traitsILi256ELi128ELi64ELi8ES2_EELb1ELb0ELb0ELb1ELb1EEv16Flash_fwd_params' for 'sm_80'
ptxas info    : Function properties for _Z16flash_fwd_kernelI23Flash_fwd_kernel_traitsILi256ELi128ELi64ELi8ELb0ELb0EN7cutlass6half_tE19Flash_kernel_traitsILi256ELi128ELi64ELi8ES2_EELb1ELb0ELb0ELb1ELb1EEv16Flash_fwd_params
    80 bytes stack frame, 106 bytes spill stores, 82 bytes spill loads
ptxas info    : Used 255 registers, 584 bytes cmem[0]
ptxas info    : Compiling entry function '_Z16flash_fwd_kernelI23Flash_fwd_kernel_traitsILi256ELi128ELi64ELi8ELb0ELb0EN7cutlass6half_tE19Flash_kernel_traitsILi256ELi128ELi64ELi8ES2_EELb1ELb0ELb1ELb0ELb0EEv16Flash_fwd_params' for 'sm_80'
ptxas info    : Function properties for _Z16flash_fwd_kernelI23Flash_fwd_kernel_traitsILi256ELi128ELi64ELi8ELb0ELb0EN7cutlass6half_tE19Flash_kernel_traitsILi256ELi128ELi64ELi8ES2_EELb1ELb0ELb1ELb0ELb0EEv16Flash_fwd_params
    0 bytes stack frame, 0 bytes spill stores, 0 bytes spill loads
ptxas info    : Used 244 registers, 584 bytes cmem[0]
ptxas info    : Compiling entry function '_Z16flash_fwd_kernelI23Flash_fwd_kernel_traitsILi256ELi128ELi64ELi8ELb0ELb0EN7cutlass6half_tE19Flash_kernel_traitsILi256ELi128ELi64ELi8ES2_EELb1ELb0ELb1ELb0ELb1EEv16Flash_fwd_params' for 'sm_80'
ptxas info    : Function properties for _Z16flash_fwd_kernelI23Flash_fwd_kernel_traitsILi256ELi128ELi64ELi8ELb0ELb0EN7cutlass6half_tE19Flash_kernel_traitsILi256ELi128ELi64ELi8ES2_EELb1ELb0ELb1ELb0ELb1EEv16Flash_fwd_params
    56 bytes stack frame, 44 bytes spill stores, 44 bytes spill loads
ptxas info    : Used 255 registers, 584 bytes cmem[0]
ptxas info    : Compiling entry function '_Z16flash_fwd_kernelI23Flash_fwd_kernel_traitsILi256ELi128ELi64ELi8ELb0ELb0EN7cutlass6half_tE19Flash_kernel_traitsILi256ELi128ELi64ELi8ES2_EELb1ELb0ELb1ELb1ELb0EEv16Flash_fwd_params' for 'sm_80'
ptxas info    : Function properties for _Z16flash_fwd_kernelI23Flash_fwd_kernel_traitsILi256ELi128ELi64ELi8ELb0ELb0EN7cutlass6half_tE19Flash_kernel_traitsILi256ELi128ELi64ELi8ES2_EELb1ELb0ELb1ELb1ELb0EEv16Flash_fwd_params
    0 bytes stack frame, 0 bytes spill stores, 0 bytes spill loads
ptxas info    : Used 242 registers, 584 bytes cmem[0]
ptxas info    : Compiling entry function '_Z16flash_fwd_kernelI23Flash_fwd_kernel_traitsILi256ELi128ELi64ELi8ELb0ELb0EN7cutlass6half_tE19Flash_kernel_traitsILi256ELi128ELi64ELi8ES2_EELb1ELb0ELb1ELb1ELb1EEv16Flash_fwd_params' for 'sm_80'
ptxas info    : Function properties for _Z16flash_fwd_kernelI23Flash_fwd_kernel_traitsILi256ELi128ELi64ELi8ELb0ELb0EN7cutlass6half_tE19Flash_kernel_traitsILi256ELi128ELi64ELi8ES2_EELb1ELb0ELb1ELb1ELb1EEv16Flash_fwd_params
    80 bytes stack frame, 78 bytes spill stores, 70 bytes spill loads
ptxas info    : Used 255 registers, 584 bytes cmem[0]
ptxas info    : Compiling entry function '_Z16flash_fwd_kernelI23Flash_fwd_kernel_traitsILi256ELi64ELi64ELi4ELb0ELb0EN7cutlass6half_tE19Flash_kernel_traitsILi256ELi64ELi64ELi4ES2_EELb1ELb1ELb0ELb0ELb0EEv16Flash_fwd_params' for 'sm_80'
ptxas info    : Function properties for _Z16flash_fwd_kernelI23Flash_fwd_kernel_traitsILi256ELi64ELi64ELi4ELb0ELb0EN7cutlass6half_tE19Flash_kernel_traitsILi256ELi64ELi64ELi4ES2_EELb1ELb1ELb0ELb0ELb0EEv16Flash_fwd_params
    0 bytes stack frame, 0 bytes spill stores, 0 bytes spill loads
ptxas info    : Used 246 registers, 584 bytes cmem[0]
ptxas info    : Compiling entry function '_Z16flash_fwd_kernelI23Flash_fwd_kernel_traitsILi256ELi64ELi64ELi4ELb0ELb0EN7cutlass6half_tE19Flash_kernel_traitsILi256ELi64ELi64ELi4ES2_EELb1ELb1ELb0ELb0ELb1EEv16Flash_fwd_params' for 'sm_80'
ptxas info    : Function properties for _Z16flash_fwd_kernelI23Flash_fwd_kernel_traitsILi256ELi64ELi64ELi4ELb0ELb0EN7cutlass6half_tE19Flash_kernel_traitsILi256ELi64ELi64ELi4ES2_EELb1ELb1ELb0ELb0ELb1EEv16Flash_fwd_params
    104 bytes stack frame, 132 bytes spill stores, 124 bytes spill loads
ptxas info    : Used 255 registers, 584 bytes cmem[0]
ptxas info    : Compiling entry function '_Z16flash_fwd_kernelI23Flash_fwd_kernel_traitsILi256ELi64ELi64ELi4ELb0ELb0EN7cutlass6half_tE19Flash_kernel_traitsILi256ELi64ELi64ELi4ES2_EELb1ELb1ELb0ELb1ELb0EEv16Flash_fwd_params' for 'sm_80'
ptxas info    : Function properties for _Z16flash_fwd_kernelI23Flash_fwd_kernel_traitsILi256ELi64ELi64ELi4ELb0ELb0EN7cutlass6half_tE19Flash_kernel_traitsILi256ELi64ELi64ELi4ES2_EELb1ELb1ELb0ELb1ELb0EEv16Flash_fwd_params
    0 bytes stack frame, 0 bytes spill stores, 0 bytes spill loads
ptxas info    : Used 248 registers, 584 bytes cmem[0]
ptxas info    : Compiling entry function '_Z16flash_fwd_kernelI23Flash_fwd_kernel_traitsILi256ELi64ELi64ELi4ELb0ELb0EN7cutlass6half_tE19Flash_kernel_traitsILi256ELi64ELi64ELi4ES2_EELb1ELb1ELb0ELb1ELb1EEv16Flash_fwd_params' for 'sm_80'
ptxas info    : Function properties for _Z16flash_fwd_kernelI23Flash_fwd_kernel_traitsILi256ELi64ELi64ELi4ELb0ELb0EN7cutlass6half_tE19Flash_kernel_traitsILi256ELi64ELi64ELi4ES2_EELb1ELb1ELb0ELb1ELb1EEv16Flash_fwd_params
    104 bytes stack frame, 132 bytes spill stores, 132 bytes spill loads
ptxas info    : Used 255 registers, 584 bytes cmem[0]
ptxas info    : Compiling entry function '_Z16flash_fwd_kernelI23Flash_fwd_kernel_traitsILi256ELi64ELi64ELi4ELb0ELb0EN7cutlass6half_tE19Flash_kernel_traitsILi256ELi64ELi64ELi4ES2_EELb1ELb1ELb1ELb0ELb0EEv16Flash_fwd_params' for 'sm_80'
ptxas info    : Function properties for _Z16flash_fwd_kernelI23Flash_fwd_kernel_traitsILi256ELi64ELi64ELi4ELb0ELb0EN7cutlass6half_tE19Flash_kernel_traitsILi256ELi64ELi64ELi4ES2_EELb1ELb1ELb1ELb0ELb0EEv16Flash_fwd_params
    0 bytes stack frame, 0 bytes spill stores, 0 bytes spill loads
ptxas info    : Used 254 registers, 584 bytes cmem[0]
ptxas info    : Compiling entry function '_Z16flash_fwd_kernelI23Flash_fwd_kernel_traitsILi256ELi64ELi64ELi4ELb0ELb0EN7cutlass6half_tE19Flash_kernel_traitsILi256ELi64ELi64ELi4ES2_EELb1ELb1ELb1ELb0ELb1EEv16Flash_fwd_params' for 'sm_80'
ptxas info    : Function properties for _Z16flash_fwd_kernelI23Flash_fwd_kernel_traitsILi256ELi64ELi64ELi4ELb0ELb0EN7cutlass6half_tE19Flash_kernel_traitsILi256ELi64ELi64ELi4ES2_EELb1ELb1ELb1ELb0ELb1EEv16Flash_fwd_params
    112 bytes stack frame, 132 bytes spill stores, 124 bytes spill loads
ptxas info    : Used 255 registers, 584 bytes cmem[0]
ptxas info    : Compiling entry function '_Z16flash_fwd_kernelI23Flash_fwd_kernel_traitsILi256ELi64ELi64ELi4ELb0ELb0EN7cutlass6half_tE19Flash_kernel_traitsILi256ELi64ELi64ELi4ES2_EELb1ELb1ELb1ELb1ELb0EEv16Flash_fwd_params' for 'sm_80'
ptxas info    : Function properties for _Z16flash_fwd_kernelI23Flash_fwd_kernel_traitsILi256ELi64ELi64ELi4ELb0ELb0EN7cutlass6half_tE19Flash_kernel_traitsILi256ELi64ELi64ELi4ES2_EELb1ELb1ELb1ELb1ELb0EEv16Flash_fwd_params
    0 bytes stack frame, 0 bytes spill stores, 0 bytes spill loads
ptxas info    : Used 254 registers, 584 bytes cmem[0]
ptxas info    : Compiling entry function '_Z16flash_fwd_kernelI23Flash_fwd_kernel_traitsILi256ELi64ELi64ELi4ELb0ELb0EN7cutlass6half_tE19Flash_kernel_traitsILi256ELi64ELi64ELi4ES2_EELb1ELb1ELb1ELb1ELb1EEv16Flash_fwd_params' for 'sm_80'
ptxas info    : Function properties for _Z16flash_fwd_kernelI23Flash_fwd_kernel_traitsILi256ELi64ELi64ELi4ELb0ELb0EN7cutlass6half_tE19Flash_kernel_traitsILi256ELi64ELi64ELi4ES2_EELb1ELb1ELb1ELb1ELb1EEv16Flash_fwd_params
    96 bytes stack frame, 116 bytes spill stores, 116 bytes spill loads
ptxas info    : Used 255 registers, 584 bytes cmem[0]
ptxas info    : Compiling entry function '_Z16flash_fwd_kernelI23Flash_fwd_kernel_traitsILi256ELi128ELi64ELi8ELb0ELb0EN7cutlass6half_tE19Flash_kernel_traitsILi256ELi128ELi64ELi8ES2_EELb1ELb1ELb0ELb0ELb0EEv16Flash_fwd_params' for 'sm_80'
ptxas info    : Function properties for _Z16flash_fwd_kernelI23Flash_fwd_kernel_traitsILi256ELi128ELi64ELi8ELb0ELb0EN7cutlass6half_tE19Flash_kernel_traitsILi256ELi128ELi64ELi8ES2_EELb1ELb1ELb0ELb0ELb0EEv16Flash_fwd_params
    0 bytes stack frame, 0 bytes spill stores, 0 bytes spill loads
ptxas info    : Used 255 registers, 584 bytes cmem[0]
ptxas info    : Compiling entry function '_Z16flash_fwd_kernelI23Flash_fwd_kernel_traitsILi256ELi128ELi64ELi8ELb0ELb0EN7cutlass6half_tE19Flash_kernel_traitsILi256ELi128ELi64ELi8ES2_EELb1ELb1ELb0ELb0ELb1EEv16Flash_fwd_params' for 'sm_80'
ptxas info    : Function properties for _Z16flash_fwd_kernelI23Flash_fwd_kernel_traitsILi256ELi128ELi64ELi8ELb0ELb0EN7cutlass6half_tE19Flash_kernel_traitsILi256ELi128ELi64ELi8ES2_EELb1ELb1ELb0ELb0ELb1EEv16Flash_fwd_params
    104 bytes stack frame, 200 bytes spill stores, 176 bytes spill loads
ptxas info    : Used 255 registers, 584 bytes cmem[0]
ptxas info    : Compiling entry function '_Z16flash_fwd_kernelI23Flash_fwd_kernel_traitsILi256ELi128ELi64ELi8ELb0ELb0EN7cutlass6half_tE19Flash_kernel_traitsILi256ELi128ELi64ELi8ES2_EELb1ELb1ELb0ELb1ELb0EEv16Flash_fwd_params' for 'sm_80'
ptxas info    : Function properties for _Z16flash_fwd_kernelI23Flash_fwd_kernel_traitsILi256ELi128ELi64ELi8ELb0ELb0EN7cutlass6half_tE19Flash_kernel_traitsILi256ELi128ELi64ELi8ES2_EELb1ELb1ELb0ELb1ELb0EEv16Flash_fwd_params
    8 bytes stack frame, 4 bytes spill stores, 4 bytes spill loads
ptxas info    : Used 255 registers, 584 bytes cmem[0]
ptxas info    : Compiling entry function '_Z16flash_fwd_kernelI23Flash_fwd_kernel_traitsILi256ELi128ELi64ELi8ELb0ELb0EN7cutlass6half_tE19Flash_kernel_traitsILi256ELi128ELi64ELi8ES2_EELb1ELb1ELb0ELb1ELb1EEv16Flash_fwd_params' for 'sm_80'
ptxas info    : Function properties for _Z16flash_fwd_kernelI23Flash_fwd_kernel_traitsILi256ELi128ELi64ELi8ELb0ELb0EN7cutlass6half_tE19Flash_kernel_traitsILi256ELi128ELi64ELi8ES2_EELb1ELb1ELb0ELb1ELb1EEv16Flash_fwd_params
    152 bytes stack frame, 204 bytes spill stores, 204 bytes spill loads
ptxas info    : Used 255 registers, 584 bytes cmem[0]
ptxas info    : Compiling entry function '_Z16flash_fwd_kernelI23Flash_fwd_kernel_traitsILi256ELi128ELi64ELi8ELb0ELb0EN7cutlass6half_tE19Flash_kernel_traitsILi256ELi128ELi64ELi8ES2_EELb1ELb1ELb1ELb0ELb0EEv16Flash_fwd_params' for 'sm_80'
ptxas info    : Function properties for _Z16flash_fwd_kernelI23Flash_fwd_kernel_traitsILi256ELi128ELi64ELi8ELb0ELb0EN7cutlass6half_tE19Flash_kernel_traitsILi256ELi128ELi64ELi8ES2_EELb1ELb1ELb1ELb0ELb0EEv16Flash_fwd_params
    0 bytes stack frame, 0 bytes spill stores, 0 bytes spill loads
ptxas info    : Used 254 registers, 584 bytes cmem[0]
ptxas info    : Compiling entry function '_Z16flash_fwd_kernelI23Flash_fwd_kernel_traitsILi256ELi128ELi64ELi8ELb0ELb0EN7cutlass6half_tE19Flash_kernel_traitsILi256ELi128ELi64ELi8ES2_EELb1ELb1ELb1ELb0ELb1EEv16Flash_fwd_params' for 'sm_80'
ptxas info    : Function properties for _Z16flash_fwd_kernelI23Flash_fwd_kernel_traitsILi256ELi128ELi64ELi8ELb0ELb0EN7cutlass6half_tE19Flash_kernel_traitsILi256ELi128ELi64ELi8ES2_EELb1ELb1ELb1ELb0ELb1EEv16Flash_fwd_params
    144 bytes stack frame, 192 bytes spill stores, 168 bytes spill loads
ptxas info    : Used 255 registers, 584 bytes cmem[0]
ptxas info    : Compiling entry function '_Z16flash_fwd_kernelI23Flash_fwd_kernel_traitsILi256ELi128ELi64ELi8ELb0ELb0EN7cutlass6half_tE19Flash_kernel_traitsILi256ELi128ELi64ELi8ES2_EELb1ELb1ELb1ELb1ELb0EEv16Flash_fwd_params' for 'sm_80'
ptxas info    : Function properties for _Z16flash_fwd_kernelI23Flash_fwd_kernel_traitsILi256ELi128ELi64ELi8ELb0ELb0EN7cutlass6half_tE19Flash_kernel_traitsILi256ELi128ELi64ELi8ES2_EELb1ELb1ELb1ELb1ELb0EEv16Flash_fwd_params
    8 bytes stack frame, 4 bytes spill stores, 4 bytes spill loads
ptxas info    : Used 255 registers, 584 bytes cmem[0]
ptxas info    : Compiling entry function '_Z16flash_fwd_kernelI23Flash_fwd_kernel_traitsILi256ELi128ELi64ELi8ELb0ELb0EN7cutlass6half_tE19Flash_kernel_traitsILi256ELi128ELi64ELi8ES2_EELb1ELb1ELb1ELb1ELb1EEv16Flash_fwd_params' for 'sm_80'
ptxas info    : Function properties for _Z16flash_fwd_kernelI23Flash_fwd_kernel_traitsILi256ELi128ELi64ELi8ELb0ELb0EN7cutlass6half_tE19Flash_kernel_traitsILi256ELi128ELi64ELi8ES2_EELb1ELb1ELb1ELb1ELb1EEv16Flash_fwd_params
    208 bytes stack frame, 220 bytes spill stores, 224 bytes spill loads
ptxas info    : Used 255 registers, 584 bytes cmem[0]
/anaconda/envs/flashattention/lib/python3.9/site-packages/torch/include/c10/core/SymInt.h(84): warning: integer conversion resulted in a change of sign

[12/13] /usr/local/cuda-11.4/bin/nvcc  -I/home/v-feiychen/flash-attention/csrc/flash_attn -I/home/v-feiychen/flash-attention/csrc/flash_attn/src -I/home/v-feiychen/flash-attention/csrc/cutlass/include -I/anaconda/envs/flashattention/lib/python3.9/site-packages/torch/include -I/anaconda/envs/flashattention/lib/python3.9/site-packages/torch/include/torch/csrc/api/include -I/anaconda/envs/flashattention/lib/python3.9/site-packages/torch/include/TH -I/anaconda/envs/flashattention/lib/python3.9/site-packages/torch/include/THC -I/usr/local/cuda-11.4/include -I/anaconda/envs/flashattention/include/python3.9 -c -c /home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_hdim512_bf16_sm80.cu -o /home/v-feiychen/flash-attention/build/temp.linux-x86_64-cpython-39/csrc/flash_attn/src/flash_fwd_hdim512_bf16_sm80.o -D__CUDA_NO_HALF_OPERATORS__ -D__CUDA_NO_HALF_CONVERSIONS__ -D__CUDA_NO_BFLOAT16_CONVERSIONS__ -D__CUDA_NO_HALF2_OPERATORS__ --expt-relaxed-constexpr --compiler-options ''"'"'-fPIC'"'"'' -O3 -std=c++17 -U__CUDA_NO_HALF_OPERATORS__ -U__CUDA_NO_HALF_CONVERSIONS__ -U__CUDA_NO_HALF2_OPERATORS__ -U__CUDA_NO_BFLOAT16_CONVERSIONS__ --expt-relaxed-constexpr --expt-extended-lambda --use_fast_math --ptxas-options=-v -lineinfo -gencode arch=compute_80,code=sm_80 --threads 4 -DTORCH_API_INCLUDE_EXTENSION_H '-DPYBIND11_COMPILER_TYPE="_gcc"' '-DPYBIND11_STDLIB="_libstdcpp"' '-DPYBIND11_BUILD_ABI="_cxxabi1011"' -DTORCH_EXTENSION_NAME=flash_attn_2_cuda -D_GLIBCXX_USE_CXX11_ABI=0
/anaconda/envs/flashattention/lib/python3.9/site-packages/torch/include/c10/core/SymInt.h(84): warning: integer conversion resulted in a change of sign

/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_kernel.h(140): warning: variable "MMA_M" was declared but never referenced
          detected during:
            instantiation of "void flash::compute_attn_1rowblock<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &, int, int, int) [with Kernel_traits=Flash_fwd_kernel_traits<512, 64, 32, 4, false, false, cutlass::bfloat16_t, Flash_kernel_traits<512, 64, 32, 4, cutlass::bfloat16_t>>, Is_dropout=true, Is_causal=true, Is_even_N=true, Is_even_K=true, Return_softmax=true, Params=Flash_fwd_params]" 
(588): here
            instantiation of "void flash::compute_attn<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &) [with Kernel_traits=Flash_fwd_kernel_traits<512, 64, 32, 4, false, false, cutlass::bfloat16_t, Flash_kernel_traits<512, 64, 32, 4, cutlass::bfloat16_t>>, Is_dropout=true, Is_causal=true, Is_even_N=true, Is_even_K=true, Return_softmax=true, Params=Flash_fwd_params]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(15): here
            instantiation of "void flash_fwd_kernel<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax>(Flash_fwd_params) [with Kernel_traits=Flash_fwd_kernel_traits<512, 64, 32, 4, false, false, cutlass::bfloat16_t, Flash_kernel_traits<512, 64, 32, 4, cutlass::bfloat16_t>>, Is_dropout=true, Is_causal=true, Is_even_N=true, Is_even_K=true, Return_softmax=true]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(34): here
            instantiation of "void run_flash_fwd<Kernel_traits,Is_dropout,Is_causal>(Flash_fwd_params &, cudaStream_t) [with Kernel_traits=Flash_fwd_kernel_traits<512, 64, 32, 4, false, false, cutlass::bfloat16_t, Flash_kernel_traits<512, 64, 32, 4, cutlass::bfloat16_t>>, Is_dropout=true, Is_causal=true]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(264): here
            instantiation of "void run_mha_fwd_hdim512<T>(Flash_fwd_params &, cudaStream_t) [with T=cutlass::bfloat16_t]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_hdim512_bf16_sm80.cu(8): here

/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_kernel.h(275): warning: variable "tQrQ" was declared but never referenced
          detected during:
            instantiation of "void flash::compute_attn_1rowblock<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &, int, int, int) [with Kernel_traits=Flash_fwd_kernel_traits<512, 64, 32, 4, false, false, cutlass::bfloat16_t, Flash_kernel_traits<512, 64, 32, 4, cutlass::bfloat16_t>>, Is_dropout=true, Is_causal=true, Is_even_N=true, Is_even_K=true, Return_softmax=true, Params=Flash_fwd_params]" 
(588): here
            instantiation of "void flash::compute_attn<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &) [with Kernel_traits=Flash_fwd_kernel_traits<512, 64, 32, 4, false, false, cutlass::bfloat16_t, Flash_kernel_traits<512, 64, 32, 4, cutlass::bfloat16_t>>, Is_dropout=true, Is_causal=true, Is_even_N=true, Is_even_K=true, Return_softmax=true, Params=Flash_fwd_params]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(15): here
            instantiation of "void flash_fwd_kernel<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax>(Flash_fwd_params) [with Kernel_traits=Flash_fwd_kernel_traits<512, 64, 32, 4, false, false, cutlass::bfloat16_t, Flash_kernel_traits<512, 64, 32, 4, cutlass::bfloat16_t>>, Is_dropout=true, Is_causal=true, Is_even_N=true, Is_even_K=true, Return_softmax=true]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(34): here
            instantiation of "void run_flash_fwd<Kernel_traits,Is_dropout,Is_causal>(Flash_fwd_params &, cudaStream_t) [with Kernel_traits=Flash_fwd_kernel_traits<512, 64, 32, 4, false, false, cutlass::bfloat16_t, Flash_kernel_traits<512, 64, 32, 4, cutlass::bfloat16_t>>, Is_dropout=true, Is_causal=true]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(264): here
            instantiation of "void run_mha_fwd_hdim512<T>(Flash_fwd_params &, cudaStream_t) [with T=cutlass::bfloat16_t]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_hdim512_bf16_sm80.cu(8): here

/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_kernel.h(140): warning: variable "MMA_M" was declared but never referenced
          detected during:
            instantiation of "void flash::compute_attn_1rowblock<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &, int, int, int) [with Kernel_traits=Flash_fwd_kernel_traits<512, 64, 32, 4, false, false, cutlass::bfloat16_t, Flash_kernel_traits<512, 64, 32, 4, cutlass::bfloat16_t>>, Is_dropout=true, Is_causal=true, Is_even_N=true, Is_even_K=true, Return_softmax=false, Params=Flash_fwd_params]" 
(588): here
            instantiation of "void flash::compute_attn<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &) [with Kernel_traits=Flash_fwd_kernel_traits<512, 64, 32, 4, false, false, cutlass::bfloat16_t, Flash_kernel_traits<512, 64, 32, 4, cutlass::bfloat16_t>>, Is_dropout=true, Is_causal=true, Is_even_N=true, Is_even_K=true, Return_softmax=false, Params=Flash_fwd_params]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(15): here
            instantiation of "void flash_fwd_kernel<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax>(Flash_fwd_params) [with Kernel_traits=Flash_fwd_kernel_traits<512, 64, 32, 4, false, false, cutlass::bfloat16_t, Flash_kernel_traits<512, 64, 32, 4, cutlass::bfloat16_t>>, Is_dropout=true, Is_causal=true, Is_even_N=true, Is_even_K=true, Return_softmax=false]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(34): here
            instantiation of "void run_flash_fwd<Kernel_traits,Is_dropout,Is_causal>(Flash_fwd_params &, cudaStream_t) [with Kernel_traits=Flash_fwd_kernel_traits<512, 64, 32, 4, false, false, cutlass::bfloat16_t, Flash_kernel_traits<512, 64, 32, 4, cutlass::bfloat16_t>>, Is_dropout=true, Is_causal=true]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(264): here
            instantiation of "void run_mha_fwd_hdim512<T>(Flash_fwd_params &, cudaStream_t) [with T=cutlass::bfloat16_t]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_hdim512_bf16_sm80.cu(8): here

/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_kernel.h(275): warning: variable "tQrQ" was declared but never referenced
          detected during:
            instantiation of "void flash::compute_attn_1rowblock<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &, int, int, int) [with Kernel_traits=Flash_fwd_kernel_traits<512, 64, 32, 4, false, false, cutlass::bfloat16_t, Flash_kernel_traits<512, 64, 32, 4, cutlass::bfloat16_t>>, Is_dropout=true, Is_causal=true, Is_even_N=true, Is_even_K=true, Return_softmax=false, Params=Flash_fwd_params]" 
(588): here
            instantiation of "void flash::compute_attn<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &) [with Kernel_traits=Flash_fwd_kernel_traits<512, 64, 32, 4, false, false, cutlass::bfloat16_t, Flash_kernel_traits<512, 64, 32, 4, cutlass::bfloat16_t>>, Is_dropout=true, Is_causal=true, Is_even_N=true, Is_even_K=true, Return_softmax=false, Params=Flash_fwd_params]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(15): here
            instantiation of "void flash_fwd_kernel<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax>(Flash_fwd_params) [with Kernel_traits=Flash_fwd_kernel_traits<512, 64, 32, 4, false, false, cutlass::bfloat16_t, Flash_kernel_traits<512, 64, 32, 4, cutlass::bfloat16_t>>, Is_dropout=true, Is_causal=true, Is_even_N=true, Is_even_K=true, Return_softmax=false]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(34): here
            instantiation of "void run_flash_fwd<Kernel_traits,Is_dropout,Is_causal>(Flash_fwd_params &, cudaStream_t) [with Kernel_traits=Flash_fwd_kernel_traits<512, 64, 32, 4, false, false, cutlass::bfloat16_t, Flash_kernel_traits<512, 64, 32, 4, cutlass::bfloat16_t>>, Is_dropout=true, Is_causal=true]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(264): here
            instantiation of "void run_mha_fwd_hdim512<T>(Flash_fwd_params &, cudaStream_t) [with T=cutlass::bfloat16_t]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_hdim512_bf16_sm80.cu(8): here

/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_kernel.h(140): warning: variable "MMA_M" was declared but never referenced
          detected during:
            instantiation of "void flash::compute_attn_1rowblock<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &, int, int, int) [with Kernel_traits=Flash_fwd_kernel_traits<512, 64, 32, 4, false, false, cutlass::bfloat16_t, Flash_kernel_traits<512, 64, 32, 4, cutlass::bfloat16_t>>, Is_dropout=true, Is_causal=true, Is_even_N=true, Is_even_K=false, Return_softmax=true, Params=Flash_fwd_params]" 
(588): here
            instantiation of "void flash::compute_attn<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &) [with Kernel_traits=Flash_fwd_kernel_traits<512, 64, 32, 4, false, false, cutlass::bfloat16_t, Flash_kernel_traits<512, 64, 32, 4, cutlass::bfloat16_t>>, Is_dropout=true, Is_causal=true, Is_even_N=true, Is_even_K=false, Return_softmax=true, Params=Flash_fwd_params]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(15): here
            instantiation of "void flash_fwd_kernel<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax>(Flash_fwd_params) [with Kernel_traits=Flash_fwd_kernel_traits<512, 64, 32, 4, false, false, cutlass::bfloat16_t, Flash_kernel_traits<512, 64, 32, 4, cutlass::bfloat16_t>>, Is_dropout=true, Is_causal=true, Is_even_N=true, Is_even_K=false, Return_softmax=true]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(34): here
            instantiation of "void run_flash_fwd<Kernel_traits,Is_dropout,Is_causal>(Flash_fwd_params &, cudaStream_t) [with Kernel_traits=Flash_fwd_kernel_traits<512, 64, 32, 4, false, false, cutlass::bfloat16_t, Flash_kernel_traits<512, 64, 32, 4, cutlass::bfloat16_t>>, Is_dropout=true, Is_causal=true]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(264): here
            instantiation of "void run_mha_fwd_hdim512<T>(Flash_fwd_params &, cudaStream_t) [with T=cutlass::bfloat16_t]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_hdim512_bf16_sm80.cu(8): here

/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_kernel.h(275): warning: variable "tQrQ" was declared but never referenced
          detected during:
            instantiation of "void flash::compute_attn_1rowblock<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &, int, int, int) [with Kernel_traits=Flash_fwd_kernel_traits<512, 64, 32, 4, false, false, cutlass::bfloat16_t, Flash_kernel_traits<512, 64, 32, 4, cutlass::bfloat16_t>>, Is_dropout=true, Is_causal=true, Is_even_N=true, Is_even_K=false, Return_softmax=true, Params=Flash_fwd_params]" 
(588): here
            instantiation of "void flash::compute_attn<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &) [with Kernel_traits=Flash_fwd_kernel_traits<512, 64, 32, 4, false, false, cutlass::bfloat16_t, Flash_kernel_traits<512, 64, 32, 4, cutlass::bfloat16_t>>, Is_dropout=true, Is_causal=true, Is_even_N=true, Is_even_K=false, Return_softmax=true, Params=Flash_fwd_params]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(15): here
            instantiation of "void flash_fwd_kernel<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax>(Flash_fwd_params) [with Kernel_traits=Flash_fwd_kernel_traits<512, 64, 32, 4, false, false, cutlass::bfloat16_t, Flash_kernel_traits<512, 64, 32, 4, cutlass::bfloat16_t>>, Is_dropout=true, Is_causal=true, Is_even_N=true, Is_even_K=false, Return_softmax=true]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(34): here
            instantiation of "void run_flash_fwd<Kernel_traits,Is_dropout,Is_causal>(Flash_fwd_params &, cudaStream_t) [with Kernel_traits=Flash_fwd_kernel_traits<512, 64, 32, 4, false, false, cutlass::bfloat16_t, Flash_kernel_traits<512, 64, 32, 4, cutlass::bfloat16_t>>, Is_dropout=true, Is_causal=true]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(264): here
            instantiation of "void run_mha_fwd_hdim512<T>(Flash_fwd_params &, cudaStream_t) [with T=cutlass::bfloat16_t]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_hdim512_bf16_sm80.cu(8): here

/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_kernel.h(140): warning: variable "MMA_M" was declared but never referenced
          detected during:
            instantiation of "void flash::compute_attn_1rowblock<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &, int, int, int) [with Kernel_traits=Flash_fwd_kernel_traits<512, 64, 32, 4, false, false, cutlass::bfloat16_t, Flash_kernel_traits<512, 64, 32, 4, cutlass::bfloat16_t>>, Is_dropout=true, Is_causal=true, Is_even_N=true, Is_even_K=false, Return_softmax=false, Params=Flash_fwd_params]" 
(588): here
            instantiation of "void flash::compute_attn<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &) [with Kernel_traits=Flash_fwd_kernel_traits<512, 64, 32, 4, false, false, cutlass::bfloat16_t, Flash_kernel_traits<512, 64, 32, 4, cutlass::bfloat16_t>>, Is_dropout=true, Is_causal=true, Is_even_N=true, Is_even_K=false, Return_softmax=false, Params=Flash_fwd_params]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(15): here
            instantiation of "void flash_fwd_kernel<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax>(Flash_fwd_params) [with Kernel_traits=Flash_fwd_kernel_traits<512, 64, 32, 4, false, false, cutlass::bfloat16_t, Flash_kernel_traits<512, 64, 32, 4, cutlass::bfloat16_t>>, Is_dropout=true, Is_causal=true, Is_even_N=true, Is_even_K=false, Return_softmax=false]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(34): here
            instantiation of "void run_flash_fwd<Kernel_traits,Is_dropout,Is_causal>(Flash_fwd_params &, cudaStream_t) [with Kernel_traits=Flash_fwd_kernel_traits<512, 64, 32, 4, false, false, cutlass::bfloat16_t, Flash_kernel_traits<512, 64, 32, 4, cutlass::bfloat16_t>>, Is_dropout=true, Is_causal=true]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(264): here
            instantiation of "void run_mha_fwd_hdim512<T>(Flash_fwd_params &, cudaStream_t) [with T=cutlass::bfloat16_t]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_hdim512_bf16_sm80.cu(8): here

/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_kernel.h(275): warning: variable "tQrQ" was declared but never referenced
          detected during:
            instantiation of "void flash::compute_attn_1rowblock<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &, int, int, int) [with Kernel_traits=Flash_fwd_kernel_traits<512, 64, 32, 4, false, false, cutlass::bfloat16_t, Flash_kernel_traits<512, 64, 32, 4, cutlass::bfloat16_t>>, Is_dropout=true, Is_causal=true, Is_even_N=true, Is_even_K=false, Return_softmax=false, Params=Flash_fwd_params]" 
(588): here
            instantiation of "void flash::compute_attn<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &) [with Kernel_traits=Flash_fwd_kernel_traits<512, 64, 32, 4, false, false, cutlass::bfloat16_t, Flash_kernel_traits<512, 64, 32, 4, cutlass::bfloat16_t>>, Is_dropout=true, Is_causal=true, Is_even_N=true, Is_even_K=false, Return_softmax=false, Params=Flash_fwd_params]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(15): here
            instantiation of "void flash_fwd_kernel<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax>(Flash_fwd_params) [with Kernel_traits=Flash_fwd_kernel_traits<512, 64, 32, 4, false, false, cutlass::bfloat16_t, Flash_kernel_traits<512, 64, 32, 4, cutlass::bfloat16_t>>, Is_dropout=true, Is_causal=true, Is_even_N=true, Is_even_K=false, Return_softmax=false]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(34): here
            instantiation of "void run_flash_fwd<Kernel_traits,Is_dropout,Is_causal>(Flash_fwd_params &, cudaStream_t) [with Kernel_traits=Flash_fwd_kernel_traits<512, 64, 32, 4, false, false, cutlass::bfloat16_t, Flash_kernel_traits<512, 64, 32, 4, cutlass::bfloat16_t>>, Is_dropout=true, Is_causal=true]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(264): here
            instantiation of "void run_mha_fwd_hdim512<T>(Flash_fwd_params &, cudaStream_t) [with T=cutlass::bfloat16_t]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_hdim512_bf16_sm80.cu(8): here

/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_kernel.h(140): warning: variable "MMA_M" was declared but never referenced
          detected during:
            instantiation of "void flash::compute_attn_1rowblock<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &, int, int, int) [with Kernel_traits=Flash_fwd_kernel_traits<512, 64, 32, 4, false, false, cutlass::bfloat16_t, Flash_kernel_traits<512, 64, 32, 4, cutlass::bfloat16_t>>, Is_dropout=true, Is_causal=true, Is_even_N=false, Is_even_K=true, Return_softmax=true, Params=Flash_fwd_params]" 
(588): here
            instantiation of "void flash::compute_attn<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &) [with Kernel_traits=Flash_fwd_kernel_traits<512, 64, 32, 4, false, false, cutlass::bfloat16_t, Flash_kernel_traits<512, 64, 32, 4, cutlass::bfloat16_t>>, Is_dropout=true, Is_causal=true, Is_even_N=false, Is_even_K=true, Return_softmax=true, Params=Flash_fwd_params]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(15): here
            instantiation of "void flash_fwd_kernel<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax>(Flash_fwd_params) [with Kernel_traits=Flash_fwd_kernel_traits<512, 64, 32, 4, false, false, cutlass::bfloat16_t, Flash_kernel_traits<512, 64, 32, 4, cutlass::bfloat16_t>>, Is_dropout=true, Is_causal=true, Is_even_N=false, Is_even_K=true, Return_softmax=true]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(34): here
            instantiation of "void run_flash_fwd<Kernel_traits,Is_dropout,Is_causal>(Flash_fwd_params &, cudaStream_t) [with Kernel_traits=Flash_fwd_kernel_traits<512, 64, 32, 4, false, false, cutlass::bfloat16_t, Flash_kernel_traits<512, 64, 32, 4, cutlass::bfloat16_t>>, Is_dropout=true, Is_causal=true]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(264): here
            instantiation of "void run_mha_fwd_hdim512<T>(Flash_fwd_params &, cudaStream_t) [with T=cutlass::bfloat16_t]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_hdim512_bf16_sm80.cu(8): here

/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_kernel.h(275): warning: variable "tQrQ" was declared but never referenced
          detected during:
            instantiation of "void flash::compute_attn_1rowblock<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &, int, int, int) [with Kernel_traits=Flash_fwd_kernel_traits<512, 64, 32, 4, false, false, cutlass::bfloat16_t, Flash_kernel_traits<512, 64, 32, 4, cutlass::bfloat16_t>>, Is_dropout=true, Is_causal=true, Is_even_N=false, Is_even_K=true, Return_softmax=true, Params=Flash_fwd_params]" 
(588): here
            instantiation of "void flash::compute_attn<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &) [with Kernel_traits=Flash_fwd_kernel_traits<512, 64, 32, 4, false, false, cutlass::bfloat16_t, Flash_kernel_traits<512, 64, 32, 4, cutlass::bfloat16_t>>, Is_dropout=true, Is_causal=true, Is_even_N=false, Is_even_K=true, Return_softmax=true, Params=Flash_fwd_params]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(15): here
            instantiation of "void flash_fwd_kernel<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax>(Flash_fwd_params) [with Kernel_traits=Flash_fwd_kernel_traits<512, 64, 32, 4, false, false, cutlass::bfloat16_t, Flash_kernel_traits<512, 64, 32, 4, cutlass::bfloat16_t>>, Is_dropout=true, Is_causal=true, Is_even_N=false, Is_even_K=true, Return_softmax=true]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(34): here
            instantiation of "void run_flash_fwd<Kernel_traits,Is_dropout,Is_causal>(Flash_fwd_params &, cudaStream_t) [with Kernel_traits=Flash_fwd_kernel_traits<512, 64, 32, 4, false, false, cutlass::bfloat16_t, Flash_kernel_traits<512, 64, 32, 4, cutlass::bfloat16_t>>, Is_dropout=true, Is_causal=true]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(264): here
            instantiation of "void run_mha_fwd_hdim512<T>(Flash_fwd_params &, cudaStream_t) [with T=cutlass::bfloat16_t]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_hdim512_bf16_sm80.cu(8): here

/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_kernel.h(140): warning: variable "MMA_M" was declared but never referenced
          detected during:
            instantiation of "void flash::compute_attn_1rowblock<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &, int, int, int) [with Kernel_traits=Flash_fwd_kernel_traits<512, 64, 32, 4, false, false, cutlass::bfloat16_t, Flash_kernel_traits<512, 64, 32, 4, cutlass::bfloat16_t>>, Is_dropout=true, Is_causal=true, Is_even_N=false, Is_even_K=true, Return_softmax=false, Params=Flash_fwd_params]" 
(588): here
            instantiation of "void flash::compute_attn<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &) [with Kernel_traits=Flash_fwd_kernel_traits<512, 64, 32, 4, false, false, cutlass::bfloat16_t, Flash_kernel_traits<512, 64, 32, 4, cutlass::bfloat16_t>>, Is_dropout=true, Is_causal=true, Is_even_N=false, Is_even_K=true, Return_softmax=false, Params=Flash_fwd_params]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(15): here
            instantiation of "void flash_fwd_kernel<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax>(Flash_fwd_params) [with Kernel_traits=Flash_fwd_kernel_traits<512, 64, 32, 4, false, false, cutlass::bfloat16_t, Flash_kernel_traits<512, 64, 32, 4, cutlass::bfloat16_t>>, Is_dropout=true, Is_causal=true, Is_even_N=false, Is_even_K=true, Return_softmax=false]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(34): here
            instantiation of "void run_flash_fwd<Kernel_traits,Is_dropout,Is_causal>(Flash_fwd_params &, cudaStream_t) [with Kernel_traits=Flash_fwd_kernel_traits<512, 64, 32, 4, false, false, cutlass::bfloat16_t, Flash_kernel_traits<512, 64, 32, 4, cutlass::bfloat16_t>>, Is_dropout=true, Is_causal=true]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(264): here
            instantiation of "void run_mha_fwd_hdim512<T>(Flash_fwd_params &, cudaStream_t) [with T=cutlass::bfloat16_t]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_hdim512_bf16_sm80.cu(8): here

/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_kernel.h(275): warning: variable "tQrQ" was declared but never referenced
          detected during:
            instantiation of "void flash::compute_attn_1rowblock<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &, int, int, int) [with Kernel_traits=Flash_fwd_kernel_traits<512, 64, 32, 4, false, false, cutlass::bfloat16_t, Flash_kernel_traits<512, 64, 32, 4, cutlass::bfloat16_t>>, Is_dropout=true, Is_causal=true, Is_even_N=false, Is_even_K=true, Return_softmax=false, Params=Flash_fwd_params]" 
(588): here
            instantiation of "void flash::compute_attn<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &) [with Kernel_traits=Flash_fwd_kernel_traits<512, 64, 32, 4, false, false, cutlass::bfloat16_t, Flash_kernel_traits<512, 64, 32, 4, cutlass::bfloat16_t>>, Is_dropout=true, Is_causal=true, Is_even_N=false, Is_even_K=true, Return_softmax=false, Params=Flash_fwd_params]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(15): here
            instantiation of "void flash_fwd_kernel<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax>(Flash_fwd_params) [with Kernel_traits=Flash_fwd_kernel_traits<512, 64, 32, 4, false, false, cutlass::bfloat16_t, Flash_kernel_traits<512, 64, 32, 4, cutlass::bfloat16_t>>, Is_dropout=true, Is_causal=true, Is_even_N=false, Is_even_K=true, Return_softmax=false]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(34): here
            instantiation of "void run_flash_fwd<Kernel_traits,Is_dropout,Is_causal>(Flash_fwd_params &, cudaStream_t) [with Kernel_traits=Flash_fwd_kernel_traits<512, 64, 32, 4, false, false, cutlass::bfloat16_t, Flash_kernel_traits<512, 64, 32, 4, cutlass::bfloat16_t>>, Is_dropout=true, Is_causal=true]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(264): here
            instantiation of "void run_mha_fwd_hdim512<T>(Flash_fwd_params &, cudaStream_t) [with T=cutlass::bfloat16_t]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_hdim512_bf16_sm80.cu(8): here

/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_kernel.h(140): warning: variable "MMA_M" was declared but never referenced
          detected during:
            instantiation of "void flash::compute_attn_1rowblock<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &, int, int, int) [with Kernel_traits=Flash_fwd_kernel_traits<512, 64, 32, 4, false, false, cutlass::bfloat16_t, Flash_kernel_traits<512, 64, 32, 4, cutlass::bfloat16_t>>, Is_dropout=true, Is_causal=true, Is_even_N=false, Is_even_K=false, Return_softmax=true, Params=Flash_fwd_params]" 
(588): here
            instantiation of "void flash::compute_attn<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &) [with Kernel_traits=Flash_fwd_kernel_traits<512, 64, 32, 4, false, false, cutlass::bfloat16_t, Flash_kernel_traits<512, 64, 32, 4, cutlass::bfloat16_t>>, Is_dropout=true, Is_causal=true, Is_even_N=false, Is_even_K=false, Return_softmax=true, Params=Flash_fwd_params]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(15): here
            instantiation of "void flash_fwd_kernel<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax>(Flash_fwd_params) [with Kernel_traits=Flash_fwd_kernel_traits<512, 64, 32, 4, false, false, cutlass::bfloat16_t, Flash_kernel_traits<512, 64, 32, 4, cutlass::bfloat16_t>>, Is_dropout=true, Is_causal=true, Is_even_N=false, Is_even_K=false, Return_softmax=true]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(34): here
            instantiation of "void run_flash_fwd<Kernel_traits,Is_dropout,Is_causal>(Flash_fwd_params &, cudaStream_t) [with Kernel_traits=Flash_fwd_kernel_traits<512, 64, 32, 4, false, false, cutlass::bfloat16_t, Flash_kernel_traits<512, 64, 32, 4, cutlass::bfloat16_t>>, Is_dropout=true, Is_causal=true]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(264): here
            instantiation of "void run_mha_fwd_hdim512<T>(Flash_fwd_params &, cudaStream_t) [with T=cutlass::bfloat16_t]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_hdim512_bf16_sm80.cu(8): here

/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_kernel.h(275): warning: variable "tQrQ" was declared but never referenced
          detected during:
            instantiation of "void flash::compute_attn_1rowblock<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &, int, int, int) [with Kernel_traits=Flash_fwd_kernel_traits<512, 64, 32, 4, false, false, cutlass::bfloat16_t, Flash_kernel_traits<512, 64, 32, 4, cutlass::bfloat16_t>>, Is_dropout=true, Is_causal=true, Is_even_N=false, Is_even_K=false, Return_softmax=true, Params=Flash_fwd_params]" 
(588): here
            instantiation of "void flash::compute_attn<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &) [with Kernel_traits=Flash_fwd_kernel_traits<512, 64, 32, 4, false, false, cutlass::bfloat16_t, Flash_kernel_traits<512, 64, 32, 4, cutlass::bfloat16_t>>, Is_dropout=true, Is_causal=true, Is_even_N=false, Is_even_K=false, Return_softmax=true, Params=Flash_fwd_params]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(15): here
            instantiation of "void flash_fwd_kernel<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax>(Flash_fwd_params) [with Kernel_traits=Flash_fwd_kernel_traits<512, 64, 32, 4, false, false, cutlass::bfloat16_t, Flash_kernel_traits<512, 64, 32, 4, cutlass::bfloat16_t>>, Is_dropout=true, Is_causal=true, Is_even_N=false, Is_even_K=false, Return_softmax=true]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(34): here
            instantiation of "void run_flash_fwd<Kernel_traits,Is_dropout,Is_causal>(Flash_fwd_params &, cudaStream_t) [with Kernel_traits=Flash_fwd_kernel_traits<512, 64, 32, 4, false, false, cutlass::bfloat16_t, Flash_kernel_traits<512, 64, 32, 4, cutlass::bfloat16_t>>, Is_dropout=true, Is_causal=true]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(264): here
            instantiation of "void run_mha_fwd_hdim512<T>(Flash_fwd_params &, cudaStream_t) [with T=cutlass::bfloat16_t]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_hdim512_bf16_sm80.cu(8): here

/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_kernel.h(140): warning: variable "MMA_M" was declared but never referenced
          detected during:
            instantiation of "void flash::compute_attn_1rowblock<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &, int, int, int) [with Kernel_traits=Flash_fwd_kernel_traits<512, 64, 32, 4, false, false, cutlass::bfloat16_t, Flash_kernel_traits<512, 64, 32, 4, cutlass::bfloat16_t>>, Is_dropout=true, Is_causal=true, Is_even_N=false, Is_even_K=false, Return_softmax=false, Params=Flash_fwd_params]" 
(588): here
            instantiation of "void flash::compute_attn<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &) [with Kernel_traits=Flash_fwd_kernel_traits<512, 64, 32, 4, false, false, cutlass::bfloat16_t, Flash_kernel_traits<512, 64, 32, 4, cutlass::bfloat16_t>>, Is_dropout=true, Is_causal=true, Is_even_N=false, Is_even_K=false, Return_softmax=false, Params=Flash_fwd_params]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(15): here
            instantiation of "void flash_fwd_kernel<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax>(Flash_fwd_params) [with Kernel_traits=Flash_fwd_kernel_traits<512, 64, 32, 4, false, false, cutlass::bfloat16_t, Flash_kernel_traits<512, 64, 32, 4, cutlass::bfloat16_t>>, Is_dropout=true, Is_causal=true, Is_even_N=false, Is_even_K=false, Return_softmax=false]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(34): here
            instantiation of "void run_flash_fwd<Kernel_traits,Is_dropout,Is_causal>(Flash_fwd_params &, cudaStream_t) [with Kernel_traits=Flash_fwd_kernel_traits<512, 64, 32, 4, false, false, cutlass::bfloat16_t, Flash_kernel_traits<512, 64, 32, 4, cutlass::bfloat16_t>>, Is_dropout=true, Is_causal=true]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(264): here
            instantiation of "void run_mha_fwd_hdim512<T>(Flash_fwd_params &, cudaStream_t) [with T=cutlass::bfloat16_t]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_hdim512_bf16_sm80.cu(8): here

/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_kernel.h(275): warning: variable "tQrQ" was declared but never referenced
          detected during:
            instantiation of "void flash::compute_attn_1rowblock<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &, int, int, int) [with Kernel_traits=Flash_fwd_kernel_traits<512, 64, 32, 4, false, false, cutlass::bfloat16_t, Flash_kernel_traits<512, 64, 32, 4, cutlass::bfloat16_t>>, Is_dropout=true, Is_causal=true, Is_even_N=false, Is_even_K=false, Return_softmax=false, Params=Flash_fwd_params]" 
(588): here
            instantiation of "void flash::compute_attn<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &) [with Kernel_traits=Flash_fwd_kernel_traits<512, 64, 32, 4, false, false, cutlass::bfloat16_t, Flash_kernel_traits<512, 64, 32, 4, cutlass::bfloat16_t>>, Is_dropout=true, Is_causal=true, Is_even_N=false, Is_even_K=false, Return_softmax=false, Params=Flash_fwd_params]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(15): here
            instantiation of "void flash_fwd_kernel<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax>(Flash_fwd_params) [with Kernel_traits=Flash_fwd_kernel_traits<512, 64, 32, 4, false, false, cutlass::bfloat16_t, Flash_kernel_traits<512, 64, 32, 4, cutlass::bfloat16_t>>, Is_dropout=true, Is_causal=true, Is_even_N=false, Is_even_K=false, Return_softmax=false]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(34): here
            instantiation of "void run_flash_fwd<Kernel_traits,Is_dropout,Is_causal>(Flash_fwd_params &, cudaStream_t) [with Kernel_traits=Flash_fwd_kernel_traits<512, 64, 32, 4, false, false, cutlass::bfloat16_t, Flash_kernel_traits<512, 64, 32, 4, cutlass::bfloat16_t>>, Is_dropout=true, Is_causal=true]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(264): here
            instantiation of "void run_mha_fwd_hdim512<T>(Flash_fwd_params &, cudaStream_t) [with T=cutlass::bfloat16_t]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_hdim512_bf16_sm80.cu(8): here

/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_kernel.h(140): warning: variable "MMA_M" was declared but never referenced
          detected during:
            instantiation of "void flash::compute_attn_1rowblock<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &, int, int, int) [with Kernel_traits=Flash_fwd_kernel_traits<512, 32, 32, 2, false, false, cutlass::bfloat16_t, Flash_kernel_traits<512, 32, 32, 2, cutlass::bfloat16_t>>, Is_dropout=true, Is_causal=true, Is_even_N=true, Is_even_K=true, Return_softmax=true, Params=Flash_fwd_params]" 
(588): here
            instantiation of "void flash::compute_attn<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &) [with Kernel_traits=Flash_fwd_kernel_traits<512, 32, 32, 2, false, false, cutlass::bfloat16_t, Flash_kernel_traits<512, 32, 32, 2, cutlass::bfloat16_t>>, Is_dropout=true, Is_causal=true, Is_even_N=true, Is_even_K=true, Return_softmax=true, Params=Flash_fwd_params]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(15): here
            instantiation of "void flash_fwd_kernel<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax>(Flash_fwd_params) [with Kernel_traits=Flash_fwd_kernel_traits<512, 32, 32, 2, false, false, cutlass::bfloat16_t, Flash_kernel_traits<512, 32, 32, 2, cutlass::bfloat16_t>>, Is_dropout=true, Is_causal=true, Is_even_N=true, Is_even_K=true, Return_softmax=true]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(34): here
            instantiation of "void run_flash_fwd<Kernel_traits,Is_dropout,Is_causal>(Flash_fwd_params &, cudaStream_t) [with Kernel_traits=Flash_fwd_kernel_traits<512, 32, 32, 2, false, false, cutlass::bfloat16_t, Flash_kernel_traits<512, 32, 32, 2, cutlass::bfloat16_t>>, Is_dropout=true, Is_causal=true]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(264): here
            instantiation of "void run_mha_fwd_hdim512<T>(Flash_fwd_params &, cudaStream_t) [with T=cutlass::bfloat16_t]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_hdim512_bf16_sm80.cu(8): here

/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_kernel.h(275): warning: variable "tQrQ" was declared but never referenced
          detected during:
            instantiation of "void flash::compute_attn_1rowblock<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &, int, int, int) [with Kernel_traits=Flash_fwd_kernel_traits<512, 32, 32, 2, false, false, cutlass::bfloat16_t, Flash_kernel_traits<512, 32, 32, 2, cutlass::bfloat16_t>>, Is_dropout=true, Is_causal=true, Is_even_N=true, Is_even_K=true, Return_softmax=true, Params=Flash_fwd_params]" 
(588): here
            instantiation of "void flash::compute_attn<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &) [with Kernel_traits=Flash_fwd_kernel_traits<512, 32, 32, 2, false, false, cutlass::bfloat16_t, Flash_kernel_traits<512, 32, 32, 2, cutlass::bfloat16_t>>, Is_dropout=true, Is_causal=true, Is_even_N=true, Is_even_K=true, Return_softmax=true, Params=Flash_fwd_params]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(15): here
            instantiation of "void flash_fwd_kernel<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax>(Flash_fwd_params) [with Kernel_traits=Flash_fwd_kernel_traits<512, 32, 32, 2, false, false, cutlass::bfloat16_t, Flash_kernel_traits<512, 32, 32, 2, cutlass::bfloat16_t>>, Is_dropout=true, Is_causal=true, Is_even_N=true, Is_even_K=true, Return_softmax=true]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(34): here
            instantiation of "void run_flash_fwd<Kernel_traits,Is_dropout,Is_causal>(Flash_fwd_params &, cudaStream_t) [with Kernel_traits=Flash_fwd_kernel_traits<512, 32, 32, 2, false, false, cutlass::bfloat16_t, Flash_kernel_traits<512, 32, 32, 2, cutlass::bfloat16_t>>, Is_dropout=true, Is_causal=true]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(264): here
            instantiation of "void run_mha_fwd_hdim512<T>(Flash_fwd_params &, cudaStream_t) [with T=cutlass::bfloat16_t]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_hdim512_bf16_sm80.cu(8): here

/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_kernel.h(140): warning: variable "MMA_M" was declared but never referenced
          detected during:
            instantiation of "void flash::compute_attn_1rowblock<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &, int, int, int) [with Kernel_traits=Flash_fwd_kernel_traits<512, 32, 32, 2, false, false, cutlass::bfloat16_t, Flash_kernel_traits<512, 32, 32, 2, cutlass::bfloat16_t>>, Is_dropout=true, Is_causal=true, Is_even_N=true, Is_even_K=true, Return_softmax=false, Params=Flash_fwd_params]" 
(588): here
            instantiation of "void flash::compute_attn<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &) [with Kernel_traits=Flash_fwd_kernel_traits<512, 32, 32, 2, false, false, cutlass::bfloat16_t, Flash_kernel_traits<512, 32, 32, 2, cutlass::bfloat16_t>>, Is_dropout=true, Is_causal=true, Is_even_N=true, Is_even_K=true, Return_softmax=false, Params=Flash_fwd_params]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(15): here
            instantiation of "void flash_fwd_kernel<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax>(Flash_fwd_params) [with Kernel_traits=Flash_fwd_kernel_traits<512, 32, 32, 2, false, false, cutlass::bfloat16_t, Flash_kernel_traits<512, 32, 32, 2, cutlass::bfloat16_t>>, Is_dropout=true, Is_causal=true, Is_even_N=true, Is_even_K=true, Return_softmax=false]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(34): here
            instantiation of "void run_flash_fwd<Kernel_traits,Is_dropout,Is_causal>(Flash_fwd_params &, cudaStream_t) [with Kernel_traits=Flash_fwd_kernel_traits<512, 32, 32, 2, false, false, cutlass::bfloat16_t, Flash_kernel_traits<512, 32, 32, 2, cutlass::bfloat16_t>>, Is_dropout=true, Is_causal=true]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(264): here
            instantiation of "void run_mha_fwd_hdim512<T>(Flash_fwd_params &, cudaStream_t) [with T=cutlass::bfloat16_t]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_hdim512_bf16_sm80.cu(8): here

/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_kernel.h(275): warning: variable "tQrQ" was declared but never referenced
          detected during:
            instantiation of "void flash::compute_attn_1rowblock<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &, int, int, int) [with Kernel_traits=Flash_fwd_kernel_traits<512, 32, 32, 2, false, false, cutlass::bfloat16_t, Flash_kernel_traits<512, 32, 32, 2, cutlass::bfloat16_t>>, Is_dropout=true, Is_causal=true, Is_even_N=true, Is_even_K=true, Return_softmax=false, Params=Flash_fwd_params]" 
(588): here
            instantiation of "void flash::compute_attn<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &) [with Kernel_traits=Flash_fwd_kernel_traits<512, 32, 32, 2, false, false, cutlass::bfloat16_t, Flash_kernel_traits<512, 32, 32, 2, cutlass::bfloat16_t>>, Is_dropout=true, Is_causal=true, Is_even_N=true, Is_even_K=true, Return_softmax=false, Params=Flash_fwd_params]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(15): here
            instantiation of "void flash_fwd_kernel<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax>(Flash_fwd_params) [with Kernel_traits=Flash_fwd_kernel_traits<512, 32, 32, 2, false, false, cutlass::bfloat16_t, Flash_kernel_traits<512, 32, 32, 2, cutlass::bfloat16_t>>, Is_dropout=true, Is_causal=true, Is_even_N=true, Is_even_K=true, Return_softmax=false]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(34): here
            instantiation of "void run_flash_fwd<Kernel_traits,Is_dropout,Is_causal>(Flash_fwd_params &, cudaStream_t) [with Kernel_traits=Flash_fwd_kernel_traits<512, 32, 32, 2, false, false, cutlass::bfloat16_t, Flash_kernel_traits<512, 32, 32, 2, cutlass::bfloat16_t>>, Is_dropout=true, Is_causal=true]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(264): here
            instantiation of "void run_mha_fwd_hdim512<T>(Flash_fwd_params &, cudaStream_t) [with T=cutlass::bfloat16_t]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_hdim512_bf16_sm80.cu(8): here

/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_kernel.h(140): warning: variable "MMA_M" was declared but never referenced
          detected during:
            instantiation of "void flash::compute_attn_1rowblock<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &, int, int, int) [with Kernel_traits=Flash_fwd_kernel_traits<512, 32, 32, 2, false, false, cutlass::bfloat16_t, Flash_kernel_traits<512, 32, 32, 2, cutlass::bfloat16_t>>, Is_dropout=true, Is_causal=true, Is_even_N=true, Is_even_K=false, Return_softmax=true, Params=Flash_fwd_params]" 
(588): here
            instantiation of "void flash::compute_attn<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &) [with Kernel_traits=Flash_fwd_kernel_traits<512, 32, 32, 2, false, false, cutlass::bfloat16_t, Flash_kernel_traits<512, 32, 32, 2, cutlass::bfloat16_t>>, Is_dropout=true, Is_causal=true, Is_even_N=true, Is_even_K=false, Return_softmax=true, Params=Flash_fwd_params]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(15): here
            instantiation of "void flash_fwd_kernel<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax>(Flash_fwd_params) [with Kernel_traits=Flash_fwd_kernel_traits<512, 32, 32, 2, false, false, cutlass::bfloat16_t, Flash_kernel_traits<512, 32, 32, 2, cutlass::bfloat16_t>>, Is_dropout=true, Is_causal=true, Is_even_N=true, Is_even_K=false, Return_softmax=true]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(34): here
            instantiation of "void run_flash_fwd<Kernel_traits,Is_dropout,Is_causal>(Flash_fwd_params &, cudaStream_t) [with Kernel_traits=Flash_fwd_kernel_traits<512, 32, 32, 2, false, false, cutlass::bfloat16_t, Flash_kernel_traits<512, 32, 32, 2, cutlass::bfloat16_t>>, Is_dropout=true, Is_causal=true]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(264): here
            instantiation of "void run_mha_fwd_hdim512<T>(Flash_fwd_params &, cudaStream_t) [with T=cutlass::bfloat16_t]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_hdim512_bf16_sm80.cu(8): here

/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_kernel.h(275): warning: variable "tQrQ" was declared but never referenced
          detected during:
            instantiation of "void flash::compute_attn_1rowblock<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &, int, int, int) [with Kernel_traits=Flash_fwd_kernel_traits<512, 32, 32, 2, false, false, cutlass::bfloat16_t, Flash_kernel_traits<512, 32, 32, 2, cutlass::bfloat16_t>>, Is_dropout=true, Is_causal=true, Is_even_N=true, Is_even_K=false, Return_softmax=true, Params=Flash_fwd_params]" 
(588): here
            instantiation of "void flash::compute_attn<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &) [with Kernel_traits=Flash_fwd_kernel_traits<512, 32, 32, 2, false, false, cutlass::bfloat16_t, Flash_kernel_traits<512, 32, 32, 2, cutlass::bfloat16_t>>, Is_dropout=true, Is_causal=true, Is_even_N=true, Is_even_K=false, Return_softmax=true, Params=Flash_fwd_params]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(15): here
            instantiation of "void flash_fwd_kernel<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax>(Flash_fwd_params) [with Kernel_traits=Flash_fwd_kernel_traits<512, 32, 32, 2, false, false, cutlass::bfloat16_t, Flash_kernel_traits<512, 32, 32, 2, cutlass::bfloat16_t>>, Is_dropout=true, Is_causal=true, Is_even_N=true, Is_even_K=false, Return_softmax=true]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(34): here
            instantiation of "void run_flash_fwd<Kernel_traits,Is_dropout,Is_causal>(Flash_fwd_params &, cudaStream_t) [with Kernel_traits=Flash_fwd_kernel_traits<512, 32, 32, 2, false, false, cutlass::bfloat16_t, Flash_kernel_traits<512, 32, 32, 2, cutlass::bfloat16_t>>, Is_dropout=true, Is_causal=true]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(264): here
            instantiation of "void run_mha_fwd_hdim512<T>(Flash_fwd_params &, cudaStream_t) [with T=cutlass::bfloat16_t]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_hdim512_bf16_sm80.cu(8): here

/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_kernel.h(140): warning: variable "MMA_M" was declared but never referenced
          detected during:
            instantiation of "void flash::compute_attn_1rowblock<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &, int, int, int) [with Kernel_traits=Flash_fwd_kernel_traits<512, 32, 32, 2, false, false, cutlass::bfloat16_t, Flash_kernel_traits<512, 32, 32, 2, cutlass::bfloat16_t>>, Is_dropout=true, Is_causal=true, Is_even_N=true, Is_even_K=false, Return_softmax=false, Params=Flash_fwd_params]" 
(588): here
            instantiation of "void flash::compute_attn<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &) [with Kernel_traits=Flash_fwd_kernel_traits<512, 32, 32, 2, false, false, cutlass::bfloat16_t, Flash_kernel_traits<512, 32, 32, 2, cutlass::bfloat16_t>>, Is_dropout=true, Is_causal=true, Is_even_N=true, Is_even_K=false, Return_softmax=false, Params=Flash_fwd_params]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(15): here
            instantiation of "void flash_fwd_kernel<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax>(Flash_fwd_params) [with Kernel_traits=Flash_fwd_kernel_traits<512, 32, 32, 2, false, false, cutlass::bfloat16_t, Flash_kernel_traits<512, 32, 32, 2, cutlass::bfloat16_t>>, Is_dropout=true, Is_causal=true, Is_even_N=true, Is_even_K=false, Return_softmax=false]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(34): here
            instantiation of "void run_flash_fwd<Kernel_traits,Is_dropout,Is_causal>(Flash_fwd_params &, cudaStream_t) [with Kernel_traits=Flash_fwd_kernel_traits<512, 32, 32, 2, false, false, cutlass::bfloat16_t, Flash_kernel_traits<512, 32, 32, 2, cutlass::bfloat16_t>>, Is_dropout=true, Is_causal=true]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(264): here
            instantiation of "void run_mha_fwd_hdim512<T>(Flash_fwd_params &, cudaStream_t) [with T=cutlass::bfloat16_t]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_hdim512_bf16_sm80.cu(8): here

/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_kernel.h(275): warning: variable "tQrQ" was declared but never referenced
          detected during:
            instantiation of "void flash::compute_attn_1rowblock<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &, int, int, int) [with Kernel_traits=Flash_fwd_kernel_traits<512, 32, 32, 2, false, false, cutlass::bfloat16_t, Flash_kernel_traits<512, 32, 32, 2, cutlass::bfloat16_t>>, Is_dropout=true, Is_causal=true, Is_even_N=true, Is_even_K=false, Return_softmax=false, Params=Flash_fwd_params]" 
(588): here
            instantiation of "void flash::compute_attn<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &) [with Kernel_traits=Flash_fwd_kernel_traits<512, 32, 32, 2, false, false, cutlass::bfloat16_t, Flash_kernel_traits<512, 32, 32, 2, cutlass::bfloat16_t>>, Is_dropout=true, Is_causal=true, Is_even_N=true, Is_even_K=false, Return_softmax=false, Params=Flash_fwd_params]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(15): here
            instantiation of "void flash_fwd_kernel<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax>(Flash_fwd_params) [with Kernel_traits=Flash_fwd_kernel_traits<512, 32, 32, 2, false, false, cutlass::bfloat16_t, Flash_kernel_traits<512, 32, 32, 2, cutlass::bfloat16_t>>, Is_dropout=true, Is_causal=true, Is_even_N=true, Is_even_K=false, Return_softmax=false]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(34): here
            instantiation of "void run_flash_fwd<Kernel_traits,Is_dropout,Is_causal>(Flash_fwd_params &, cudaStream_t) [with Kernel_traits=Flash_fwd_kernel_traits<512, 32, 32, 2, false, false, cutlass::bfloat16_t, Flash_kernel_traits<512, 32, 32, 2, cutlass::bfloat16_t>>, Is_dropout=true, Is_causal=true]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(264): here
            instantiation of "void run_mha_fwd_hdim512<T>(Flash_fwd_params &, cudaStream_t) [with T=cutlass::bfloat16_t]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_hdim512_bf16_sm80.cu(8): here

/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_kernel.h(140): warning: variable "MMA_M" was declared but never referenced
          detected during:
            instantiation of "void flash::compute_attn_1rowblock<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &, int, int, int) [with Kernel_traits=Flash_fwd_kernel_traits<512, 32, 32, 2, false, false, cutlass::bfloat16_t, Flash_kernel_traits<512, 32, 32, 2, cutlass::bfloat16_t>>, Is_dropout=true, Is_causal=true, Is_even_N=false, Is_even_K=true, Return_softmax=true, Params=Flash_fwd_params]" 
(588): here
            instantiation of "void flash::compute_attn<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &) [with Kernel_traits=Flash_fwd_kernel_traits<512, 32, 32, 2, false, false, cutlass::bfloat16_t, Flash_kernel_traits<512, 32, 32, 2, cutlass::bfloat16_t>>, Is_dropout=true, Is_causal=true, Is_even_N=false, Is_even_K=true, Return_softmax=true, Params=Flash_fwd_params]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(15): here
            instantiation of "void flash_fwd_kernel<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax>(Flash_fwd_params) [with Kernel_traits=Flash_fwd_kernel_traits<512, 32, 32, 2, false, false, cutlass::bfloat16_t, Flash_kernel_traits<512, 32, 32, 2, cutlass::bfloat16_t>>, Is_dropout=true, Is_causal=true, Is_even_N=false, Is_even_K=true, Return_softmax=true]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(34): here
            instantiation of "void run_flash_fwd<Kernel_traits,Is_dropout,Is_causal>(Flash_fwd_params &, cudaStream_t) [with Kernel_traits=Flash_fwd_kernel_traits<512, 32, 32, 2, false, false, cutlass::bfloat16_t, Flash_kernel_traits<512, 32, 32, 2, cutlass::bfloat16_t>>, Is_dropout=true, Is_causal=true]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(264): here
            instantiation of "void run_mha_fwd_hdim512<T>(Flash_fwd_params &, cudaStream_t) [with T=cutlass::bfloat16_t]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_hdim512_bf16_sm80.cu(8): here

/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_kernel.h(275): warning: variable "tQrQ" was declared but never referenced
          detected during:
            instantiation of "void flash::compute_attn_1rowblock<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &, int, int, int) [with Kernel_traits=Flash_fwd_kernel_traits<512, 32, 32, 2, false, false, cutlass::bfloat16_t, Flash_kernel_traits<512, 32, 32, 2, cutlass::bfloat16_t>>, Is_dropout=true, Is_causal=true, Is_even_N=false, Is_even_K=true, Return_softmax=true, Params=Flash_fwd_params]" 
(588): here
            instantiation of "void flash::compute_attn<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &) [with Kernel_traits=Flash_fwd_kernel_traits<512, 32, 32, 2, false, false, cutlass::bfloat16_t, Flash_kernel_traits<512, 32, 32, 2, cutlass::bfloat16_t>>, Is_dropout=true, Is_causal=true, Is_even_N=false, Is_even_K=true, Return_softmax=true, Params=Flash_fwd_params]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(15): here
            instantiation of "void flash_fwd_kernel<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax>(Flash_fwd_params) [with Kernel_traits=Flash_fwd_kernel_traits<512, 32, 32, 2, false, false, cutlass::bfloat16_t, Flash_kernel_traits<512, 32, 32, 2, cutlass::bfloat16_t>>, Is_dropout=true, Is_causal=true, Is_even_N=false, Is_even_K=true, Return_softmax=true]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(34): here
            instantiation of "void run_flash_fwd<Kernel_traits,Is_dropout,Is_causal>(Flash_fwd_params &, cudaStream_t) [with Kernel_traits=Flash_fwd_kernel_traits<512, 32, 32, 2, false, false, cutlass::bfloat16_t, Flash_kernel_traits<512, 32, 32, 2, cutlass::bfloat16_t>>, Is_dropout=true, Is_causal=true]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(264): here
            instantiation of "void run_mha_fwd_hdim512<T>(Flash_fwd_params &, cudaStream_t) [with T=cutlass::bfloat16_t]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_hdim512_bf16_sm80.cu(8): here

/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_kernel.h(140): warning: variable "MMA_M" was declared but never referenced
          detected during:
            instantiation of "void flash::compute_attn_1rowblock<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &, int, int, int) [with Kernel_traits=Flash_fwd_kernel_traits<512, 32, 32, 2, false, false, cutlass::bfloat16_t, Flash_kernel_traits<512, 32, 32, 2, cutlass::bfloat16_t>>, Is_dropout=true, Is_causal=true, Is_even_N=false, Is_even_K=true, Return_softmax=false, Params=Flash_fwd_params]" 
(588): here
            instantiation of "void flash::compute_attn<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &) [with Kernel_traits=Flash_fwd_kernel_traits<512, 32, 32, 2, false, false, cutlass::bfloat16_t, Flash_kernel_traits<512, 32, 32, 2, cutlass::bfloat16_t>>, Is_dropout=true, Is_causal=true, Is_even_N=false, Is_even_K=true, Return_softmax=false, Params=Flash_fwd_params]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(15): here
            instantiation of "void flash_fwd_kernel<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax>(Flash_fwd_params) [with Kernel_traits=Flash_fwd_kernel_traits<512, 32, 32, 2, false, false, cutlass::bfloat16_t, Flash_kernel_traits<512, 32, 32, 2, cutlass::bfloat16_t>>, Is_dropout=true, Is_causal=true, Is_even_N=false, Is_even_K=true, Return_softmax=false]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(34): here
            instantiation of "void run_flash_fwd<Kernel_traits,Is_dropout,Is_causal>(Flash_fwd_params &, cudaStream_t) [with Kernel_traits=Flash_fwd_kernel_traits<512, 32, 32, 2, false, false, cutlass::bfloat16_t, Flash_kernel_traits<512, 32, 32, 2, cutlass::bfloat16_t>>, Is_dropout=true, Is_causal=true]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(264): here
            instantiation of "void run_mha_fwd_hdim512<T>(Flash_fwd_params &, cudaStream_t) [with T=cutlass::bfloat16_t]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_hdim512_bf16_sm80.cu(8): here

/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_kernel.h(275): warning: variable "tQrQ" was declared but never referenced
          detected during:
            instantiation of "void flash::compute_attn_1rowblock<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &, int, int, int) [with Kernel_traits=Flash_fwd_kernel_traits<512, 32, 32, 2, false, false, cutlass::bfloat16_t, Flash_kernel_traits<512, 32, 32, 2, cutlass::bfloat16_t>>, Is_dropout=true, Is_causal=true, Is_even_N=false, Is_even_K=true, Return_softmax=false, Params=Flash_fwd_params]" 
(588): here
            instantiation of "void flash::compute_attn<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &) [with Kernel_traits=Flash_fwd_kernel_traits<512, 32, 32, 2, false, false, cutlass::bfloat16_t, Flash_kernel_traits<512, 32, 32, 2, cutlass::bfloat16_t>>, Is_dropout=true, Is_causal=true, Is_even_N=false, Is_even_K=true, Return_softmax=false, Params=Flash_fwd_params]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(15): here
            instantiation of "void flash_fwd_kernel<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax>(Flash_fwd_params) [with Kernel_traits=Flash_fwd_kernel_traits<512, 32, 32, 2, false, false, cutlass::bfloat16_t, Flash_kernel_traits<512, 32, 32, 2, cutlass::bfloat16_t>>, Is_dropout=true, Is_causal=true, Is_even_N=false, Is_even_K=true, Return_softmax=false]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(34): here
            instantiation of "void run_flash_fwd<Kernel_traits,Is_dropout,Is_causal>(Flash_fwd_params &, cudaStream_t) [with Kernel_traits=Flash_fwd_kernel_traits<512, 32, 32, 2, false, false, cutlass::bfloat16_t, Flash_kernel_traits<512, 32, 32, 2, cutlass::bfloat16_t>>, Is_dropout=true, Is_causal=true]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(264): here
            instantiation of "void run_mha_fwd_hdim512<T>(Flash_fwd_params &, cudaStream_t) [with T=cutlass::bfloat16_t]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_hdim512_bf16_sm80.cu(8): here

/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_kernel.h(140): warning: variable "MMA_M" was declared but never referenced
          detected during:
            instantiation of "void flash::compute_attn_1rowblock<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &, int, int, int) [with Kernel_traits=Flash_fwd_kernel_traits<512, 32, 32, 2, false, false, cutlass::bfloat16_t, Flash_kernel_traits<512, 32, 32, 2, cutlass::bfloat16_t>>, Is_dropout=true, Is_causal=true, Is_even_N=false, Is_even_K=false, Return_softmax=true, Params=Flash_fwd_params]" 
(588): here
            instantiation of "void flash::compute_attn<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &) [with Kernel_traits=Flash_fwd_kernel_traits<512, 32, 32, 2, false, false, cutlass::bfloat16_t, Flash_kernel_traits<512, 32, 32, 2, cutlass::bfloat16_t>>, Is_dropout=true, Is_causal=true, Is_even_N=false, Is_even_K=false, Return_softmax=true, Params=Flash_fwd_params]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(15): here
            instantiation of "void flash_fwd_kernel<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax>(Flash_fwd_params) [with Kernel_traits=Flash_fwd_kernel_traits<512, 32, 32, 2, false, false, cutlass::bfloat16_t, Flash_kernel_traits<512, 32, 32, 2, cutlass::bfloat16_t>>, Is_dropout=true, Is_causal=true, Is_even_N=false, Is_even_K=false, Return_softmax=true]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(34): here
            instantiation of "void run_flash_fwd<Kernel_traits,Is_dropout,Is_causal>(Flash_fwd_params &, cudaStream_t) [with Kernel_traits=Flash_fwd_kernel_traits<512, 32, 32, 2, false, false, cutlass::bfloat16_t, Flash_kernel_traits<512, 32, 32, 2, cutlass::bfloat16_t>>, Is_dropout=true, Is_causal=true]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(264): here
            instantiation of "void run_mha_fwd_hdim512<T>(Flash_fwd_params &, cudaStream_t) [with T=cutlass::bfloat16_t]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_hdim512_bf16_sm80.cu(8): here

/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_kernel.h(275): warning: variable "tQrQ" was declared but never referenced
          detected during:
            instantiation of "void flash::compute_attn_1rowblock<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &, int, int, int) [with Kernel_traits=Flash_fwd_kernel_traits<512, 32, 32, 2, false, false, cutlass::bfloat16_t, Flash_kernel_traits<512, 32, 32, 2, cutlass::bfloat16_t>>, Is_dropout=true, Is_causal=true, Is_even_N=false, Is_even_K=false, Return_softmax=true, Params=Flash_fwd_params]" 
(588): here
            instantiation of "void flash::compute_attn<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &) [with Kernel_traits=Flash_fwd_kernel_traits<512, 32, 32, 2, false, false, cutlass::bfloat16_t, Flash_kernel_traits<512, 32, 32, 2, cutlass::bfloat16_t>>, Is_dropout=true, Is_causal=true, Is_even_N=false, Is_even_K=false, Return_softmax=true, Params=Flash_fwd_params]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(15): here
            instantiation of "void flash_fwd_kernel<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax>(Flash_fwd_params) [with Kernel_traits=Flash_fwd_kernel_traits<512, 32, 32, 2, false, false, cutlass::bfloat16_t, Flash_kernel_traits<512, 32, 32, 2, cutlass::bfloat16_t>>, Is_dropout=true, Is_causal=true, Is_even_N=false, Is_even_K=false, Return_softmax=true]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(34): here
            instantiation of "void run_flash_fwd<Kernel_traits,Is_dropout,Is_causal>(Flash_fwd_params &, cudaStream_t) [with Kernel_traits=Flash_fwd_kernel_traits<512, 32, 32, 2, false, false, cutlass::bfloat16_t, Flash_kernel_traits<512, 32, 32, 2, cutlass::bfloat16_t>>, Is_dropout=true, Is_causal=true]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(264): here
            instantiation of "void run_mha_fwd_hdim512<T>(Flash_fwd_params &, cudaStream_t) [with T=cutlass::bfloat16_t]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_hdim512_bf16_sm80.cu(8): here

/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_kernel.h(140): warning: variable "MMA_M" was declared but never referenced
          detected during:
            instantiation of "void flash::compute_attn_1rowblock<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &, int, int, int) [with Kernel_traits=Flash_fwd_kernel_traits<512, 32, 32, 2, false, false, cutlass::bfloat16_t, Flash_kernel_traits<512, 32, 32, 2, cutlass::bfloat16_t>>, Is_dropout=true, Is_causal=true, Is_even_N=false, Is_even_K=false, Return_softmax=false, Params=Flash_fwd_params]" 
(588): here
            instantiation of "void flash::compute_attn<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &) [with Kernel_traits=Flash_fwd_kernel_traits<512, 32, 32, 2, false, false, cutlass::bfloat16_t, Flash_kernel_traits<512, 32, 32, 2, cutlass::bfloat16_t>>, Is_dropout=true, Is_causal=true, Is_even_N=false, Is_even_K=false, Return_softmax=false, Params=Flash_fwd_params]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(15): here
            instantiation of "void flash_fwd_kernel<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax>(Flash_fwd_params) [with Kernel_traits=Flash_fwd_kernel_traits<512, 32, 32, 2, false, false, cutlass::bfloat16_t, Flash_kernel_traits<512, 32, 32, 2, cutlass::bfloat16_t>>, Is_dropout=true, Is_causal=true, Is_even_N=false, Is_even_K=false, Return_softmax=false]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(34): here
            instantiation of "void run_flash_fwd<Kernel_traits,Is_dropout,Is_causal>(Flash_fwd_params &, cudaStream_t) [with Kernel_traits=Flash_fwd_kernel_traits<512, 32, 32, 2, false, false, cutlass::bfloat16_t, Flash_kernel_traits<512, 32, 32, 2, cutlass::bfloat16_t>>, Is_dropout=true, Is_causal=true]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(264): here
            instantiation of "void run_mha_fwd_hdim512<T>(Flash_fwd_params &, cudaStream_t) [with T=cutlass::bfloat16_t]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_hdim512_bf16_sm80.cu(8): here

/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_kernel.h(275): warning: variable "tQrQ" was declared but never referenced
          detected during:
            instantiation of "void flash::compute_attn_1rowblock<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &, int, int, int) [with Kernel_traits=Flash_fwd_kernel_traits<512, 32, 32, 2, false, false, cutlass::bfloat16_t, Flash_kernel_traits<512, 32, 32, 2, cutlass::bfloat16_t>>, Is_dropout=true, Is_causal=true, Is_even_N=false, Is_even_K=false, Return_softmax=false, Params=Flash_fwd_params]" 
(588): here
            instantiation of "void flash::compute_attn<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &) [with Kernel_traits=Flash_fwd_kernel_traits<512, 32, 32, 2, false, false, cutlass::bfloat16_t, Flash_kernel_traits<512, 32, 32, 2, cutlass::bfloat16_t>>, Is_dropout=true, Is_causal=true, Is_even_N=false, Is_even_K=false, Return_softmax=false, Params=Flash_fwd_params]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(15): here
            instantiation of "void flash_fwd_kernel<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax>(Flash_fwd_params) [with Kernel_traits=Flash_fwd_kernel_traits<512, 32, 32, 2, false, false, cutlass::bfloat16_t, Flash_kernel_traits<512, 32, 32, 2, cutlass::bfloat16_t>>, Is_dropout=true, Is_causal=true, Is_even_N=false, Is_even_K=false, Return_softmax=false]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(34): here
            instantiation of "void run_flash_fwd<Kernel_traits,Is_dropout,Is_causal>(Flash_fwd_params &, cudaStream_t) [with Kernel_traits=Flash_fwd_kernel_traits<512, 32, 32, 2, false, false, cutlass::bfloat16_t, Flash_kernel_traits<512, 32, 32, 2, cutlass::bfloat16_t>>, Is_dropout=true, Is_causal=true]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(264): here
            instantiation of "void run_mha_fwd_hdim512<T>(Flash_fwd_params &, cudaStream_t) [with T=cutlass::bfloat16_t]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_hdim512_bf16_sm80.cu(8): here

/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_kernel.h(140): warning: variable "MMA_M" was declared but never referenced
          detected during:
            instantiation of "void flash::compute_attn_1rowblock<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &, int, int, int) [with Kernel_traits=Flash_fwd_kernel_traits<512, 64, 32, 4, false, false, cutlass::bfloat16_t, Flash_kernel_traits<512, 64, 32, 4, cutlass::bfloat16_t>>, Is_dropout=true, Is_causal=false, Is_even_N=true, Is_even_K=true, Return_softmax=true, Params=Flash_fwd_params]" 
(588): here
            instantiation of "void flash::compute_attn<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &) [with Kernel_traits=Flash_fwd_kernel_traits<512, 64, 32, 4, false, false, cutlass::bfloat16_t, Flash_kernel_traits<512, 64, 32, 4, cutlass::bfloat16_t>>, Is_dropout=true, Is_causal=false, Is_even_N=true, Is_even_K=true, Return_softmax=true, Params=Flash_fwd_params]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(15): here
            instantiation of "void flash_fwd_kernel<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax>(Flash_fwd_params) [with Kernel_traits=Flash_fwd_kernel_traits<512, 64, 32, 4, false, false, cutlass::bfloat16_t, Flash_kernel_traits<512, 64, 32, 4, cutlass::bfloat16_t>>, Is_dropout=true, Is_causal=false, Is_even_N=true, Is_even_K=true, Return_softmax=true]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(34): here
            instantiation of "void run_flash_fwd<Kernel_traits,Is_dropout,Is_causal>(Flash_fwd_params &, cudaStream_t) [with Kernel_traits=Flash_fwd_kernel_traits<512, 64, 32, 4, false, false, cutlass::bfloat16_t, Flash_kernel_traits<512, 64, 32, 4, cutlass::bfloat16_t>>, Is_dropout=true, Is_causal=false]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(264): here
            instantiation of "void run_mha_fwd_hdim512<T>(Flash_fwd_params &, cudaStream_t) [with T=cutlass::bfloat16_t]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_hdim512_bf16_sm80.cu(8): here

/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_kernel.h(275): warning: variable "tQrQ" was declared but never referenced
          detected during:
            instantiation of "void flash::compute_attn_1rowblock<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &, int, int, int) [with Kernel_traits=Flash_fwd_kernel_traits<512, 64, 32, 4, false, false, cutlass::bfloat16_t, Flash_kernel_traits<512, 64, 32, 4, cutlass::bfloat16_t>>, Is_dropout=true, Is_causal=false, Is_even_N=true, Is_even_K=true, Return_softmax=true, Params=Flash_fwd_params]" 
(588): here
            instantiation of "void flash::compute_attn<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &) [with Kernel_traits=Flash_fwd_kernel_traits<512, 64, 32, 4, false, false, cutlass::bfloat16_t, Flash_kernel_traits<512, 64, 32, 4, cutlass::bfloat16_t>>, Is_dropout=true, Is_causal=false, Is_even_N=true, Is_even_K=true, Return_softmax=true, Params=Flash_fwd_params]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(15): here
            instantiation of "void flash_fwd_kernel<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax>(Flash_fwd_params) [with Kernel_traits=Flash_fwd_kernel_traits<512, 64, 32, 4, false, false, cutlass::bfloat16_t, Flash_kernel_traits<512, 64, 32, 4, cutlass::bfloat16_t>>, Is_dropout=true, Is_causal=false, Is_even_N=true, Is_even_K=true, Return_softmax=true]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(34): here
            instantiation of "void run_flash_fwd<Kernel_traits,Is_dropout,Is_causal>(Flash_fwd_params &, cudaStream_t) [with Kernel_traits=Flash_fwd_kernel_traits<512, 64, 32, 4, false, false, cutlass::bfloat16_t, Flash_kernel_traits<512, 64, 32, 4, cutlass::bfloat16_t>>, Is_dropout=true, Is_causal=false]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(264): here
            instantiation of "void run_mha_fwd_hdim512<T>(Flash_fwd_params &, cudaStream_t) [with T=cutlass::bfloat16_t]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_hdim512_bf16_sm80.cu(8): here

/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_kernel.h(140): warning: variable "MMA_M" was declared but never referenced
          detected during:
            instantiation of "void flash::compute_attn_1rowblock<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &, int, int, int) [with Kernel_traits=Flash_fwd_kernel_traits<512, 64, 32, 4, false, false, cutlass::bfloat16_t, Flash_kernel_traits<512, 64, 32, 4, cutlass::bfloat16_t>>, Is_dropout=true, Is_causal=false, Is_even_N=true, Is_even_K=true, Return_softmax=false, Params=Flash_fwd_params]" 
(588): here
            instantiation of "void flash::compute_attn<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &) [with Kernel_traits=Flash_fwd_kernel_traits<512, 64, 32, 4, false, false, cutlass::bfloat16_t, Flash_kernel_traits<512, 64, 32, 4, cutlass::bfloat16_t>>, Is_dropout=true, Is_causal=false, Is_even_N=true, Is_even_K=true, Return_softmax=false, Params=Flash_fwd_params]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(15): here
            instantiation of "void flash_fwd_kernel<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax>(Flash_fwd_params) [with Kernel_traits=Flash_fwd_kernel_traits<512, 64, 32, 4, false, false, cutlass::bfloat16_t, Flash_kernel_traits<512, 64, 32, 4, cutlass::bfloat16_t>>, Is_dropout=true, Is_causal=false, Is_even_N=true, Is_even_K=true, Return_softmax=false]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(34): here
            instantiation of "void run_flash_fwd<Kernel_traits,Is_dropout,Is_causal>(Flash_fwd_params &, cudaStream_t) [with Kernel_traits=Flash_fwd_kernel_traits<512, 64, 32, 4, false, false, cutlass::bfloat16_t, Flash_kernel_traits<512, 64, 32, 4, cutlass::bfloat16_t>>, Is_dropout=true, Is_causal=false]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(264): here
            instantiation of "void run_mha_fwd_hdim512<T>(Flash_fwd_params &, cudaStream_t) [with T=cutlass::bfloat16_t]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_hdim512_bf16_sm80.cu(8): here

/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_kernel.h(275): warning: variable "tQrQ" was declared but never referenced
          detected during:
            instantiation of "void flash::compute_attn_1rowblock<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &, int, int, int) [with Kernel_traits=Flash_fwd_kernel_traits<512, 64, 32, 4, false, false, cutlass::bfloat16_t, Flash_kernel_traits<512, 64, 32, 4, cutlass::bfloat16_t>>, Is_dropout=true, Is_causal=false, Is_even_N=true, Is_even_K=true, Return_softmax=false, Params=Flash_fwd_params]" 
(588): here
            instantiation of "void flash::compute_attn<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &) [with Kernel_traits=Flash_fwd_kernel_traits<512, 64, 32, 4, false, false, cutlass::bfloat16_t, Flash_kernel_traits<512, 64, 32, 4, cutlass::bfloat16_t>>, Is_dropout=true, Is_causal=false, Is_even_N=true, Is_even_K=true, Return_softmax=false, Params=Flash_fwd_params]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(15): here
            instantiation of "void flash_fwd_kernel<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax>(Flash_fwd_params) [with Kernel_traits=Flash_fwd_kernel_traits<512, 64, 32, 4, false, false, cutlass::bfloat16_t, Flash_kernel_traits<512, 64, 32, 4, cutlass::bfloat16_t>>, Is_dropout=true, Is_causal=false, Is_even_N=true, Is_even_K=true, Return_softmax=false]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(34): here
            instantiation of "void run_flash_fwd<Kernel_traits,Is_dropout,Is_causal>(Flash_fwd_params &, cudaStream_t) [with Kernel_traits=Flash_fwd_kernel_traits<512, 64, 32, 4, false, false, cutlass::bfloat16_t, Flash_kernel_traits<512, 64, 32, 4, cutlass::bfloat16_t>>, Is_dropout=true, Is_causal=false]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(264): here
            instantiation of "void run_mha_fwd_hdim512<T>(Flash_fwd_params &, cudaStream_t) [with T=cutlass::bfloat16_t]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_hdim512_bf16_sm80.cu(8): here

/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_kernel.h(140): warning: variable "MMA_M" was declared but never referenced
          detected during:
            instantiation of "void flash::compute_attn_1rowblock<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &, int, int, int) [with Kernel_traits=Flash_fwd_kernel_traits<512, 64, 32, 4, false, false, cutlass::bfloat16_t, Flash_kernel_traits<512, 64, 32, 4, cutlass::bfloat16_t>>, Is_dropout=true, Is_causal=false, Is_even_N=true, Is_even_K=false, Return_softmax=true, Params=Flash_fwd_params]" 
(588): here
            instantiation of "void flash::compute_attn<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &) [with Kernel_traits=Flash_fwd_kernel_traits<512, 64, 32, 4, false, false, cutlass::bfloat16_t, Flash_kernel_traits<512, 64, 32, 4, cutlass::bfloat16_t>>, Is_dropout=true, Is_causal=false, Is_even_N=true, Is_even_K=false, Return_softmax=true, Params=Flash_fwd_params]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(15): here
            instantiation of "void flash_fwd_kernel<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax>(Flash_fwd_params) [with Kernel_traits=Flash_fwd_kernel_traits<512, 64, 32, 4, false, false, cutlass::bfloat16_t, Flash_kernel_traits<512, 64, 32, 4, cutlass::bfloat16_t>>, Is_dropout=true, Is_causal=false, Is_even_N=true, Is_even_K=false, Return_softmax=true]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(34): here
            instantiation of "void run_flash_fwd<Kernel_traits,Is_dropout,Is_causal>(Flash_fwd_params &, cudaStream_t) [with Kernel_traits=Flash_fwd_kernel_traits<512, 64, 32, 4, false, false, cutlass::bfloat16_t, Flash_kernel_traits<512, 64, 32, 4, cutlass::bfloat16_t>>, Is_dropout=true, Is_causal=false]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(264): here
            instantiation of "void run_mha_fwd_hdim512<T>(Flash_fwd_params &, cudaStream_t) [with T=cutlass::bfloat16_t]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_hdim512_bf16_sm80.cu(8): here

/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_kernel.h(275): warning: variable "tQrQ" was declared but never referenced
          detected during:
            instantiation of "void flash::compute_attn_1rowblock<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &, int, int, int) [with Kernel_traits=Flash_fwd_kernel_traits<512, 64, 32, 4, false, false, cutlass::bfloat16_t, Flash_kernel_traits<512, 64, 32, 4, cutlass::bfloat16_t>>, Is_dropout=true, Is_causal=false, Is_even_N=true, Is_even_K=false, Return_softmax=true, Params=Flash_fwd_params]" 
(588): here
            instantiation of "void flash::compute_attn<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &) [with Kernel_traits=Flash_fwd_kernel_traits<512, 64, 32, 4, false, false, cutlass::bfloat16_t, Flash_kernel_traits<512, 64, 32, 4, cutlass::bfloat16_t>>, Is_dropout=true, Is_causal=false, Is_even_N=true, Is_even_K=false, Return_softmax=true, Params=Flash_fwd_params]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(15): here
            instantiation of "void flash_fwd_kernel<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax>(Flash_fwd_params) [with Kernel_traits=Flash_fwd_kernel_traits<512, 64, 32, 4, false, false, cutlass::bfloat16_t, Flash_kernel_traits<512, 64, 32, 4, cutlass::bfloat16_t>>, Is_dropout=true, Is_causal=false, Is_even_N=true, Is_even_K=false, Return_softmax=true]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(34): here
            instantiation of "void run_flash_fwd<Kernel_traits,Is_dropout,Is_causal>(Flash_fwd_params &, cudaStream_t) [with Kernel_traits=Flash_fwd_kernel_traits<512, 64, 32, 4, false, false, cutlass::bfloat16_t, Flash_kernel_traits<512, 64, 32, 4, cutlass::bfloat16_t>>, Is_dropout=true, Is_causal=false]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(264): here
            instantiation of "void run_mha_fwd_hdim512<T>(Flash_fwd_params &, cudaStream_t) [with T=cutlass::bfloat16_t]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_hdim512_bf16_sm80.cu(8): here

/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_kernel.h(140): warning: variable "MMA_M" was declared but never referenced
          detected during:
            instantiation of "void flash::compute_attn_1rowblock<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &, int, int, int) [with Kernel_traits=Flash_fwd_kernel_traits<512, 64, 32, 4, false, false, cutlass::bfloat16_t, Flash_kernel_traits<512, 64, 32, 4, cutlass::bfloat16_t>>, Is_dropout=true, Is_causal=false, Is_even_N=true, Is_even_K=false, Return_softmax=false, Params=Flash_fwd_params]" 
(588): here
            instantiation of "void flash::compute_attn<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &) [with Kernel_traits=Flash_fwd_kernel_traits<512, 64, 32, 4, false, false, cutlass::bfloat16_t, Flash_kernel_traits<512, 64, 32, 4, cutlass::bfloat16_t>>, Is_dropout=true, Is_causal=false, Is_even_N=true, Is_even_K=false, Return_softmax=false, Params=Flash_fwd_params]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(15): here
            instantiation of "void flash_fwd_kernel<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax>(Flash_fwd_params) [with Kernel_traits=Flash_fwd_kernel_traits<512, 64, 32, 4, false, false, cutlass::bfloat16_t, Flash_kernel_traits<512, 64, 32, 4, cutlass::bfloat16_t>>, Is_dropout=true, Is_causal=false, Is_even_N=true, Is_even_K=false, Return_softmax=false]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(34): here
            instantiation of "void run_flash_fwd<Kernel_traits,Is_dropout,Is_causal>(Flash_fwd_params &, cudaStream_t) [with Kernel_traits=Flash_fwd_kernel_traits<512, 64, 32, 4, false, false, cutlass::bfloat16_t, Flash_kernel_traits<512, 64, 32, 4, cutlass::bfloat16_t>>, Is_dropout=true, Is_causal=false]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(264): here
            instantiation of "void run_mha_fwd_hdim512<T>(Flash_fwd_params &, cudaStream_t) [with T=cutlass::bfloat16_t]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_hdim512_bf16_sm80.cu(8): here

/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_kernel.h(275): warning: variable "tQrQ" was declared but never referenced
          detected during:
            instantiation of "void flash::compute_attn_1rowblock<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &, int, int, int) [with Kernel_traits=Flash_fwd_kernel_traits<512, 64, 32, 4, false, false, cutlass::bfloat16_t, Flash_kernel_traits<512, 64, 32, 4, cutlass::bfloat16_t>>, Is_dropout=true, Is_causal=false, Is_even_N=true, Is_even_K=false, Return_softmax=false, Params=Flash_fwd_params]" 
(588): here
            instantiation of "void flash::compute_attn<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &) [with Kernel_traits=Flash_fwd_kernel_traits<512, 64, 32, 4, false, false, cutlass::bfloat16_t, Flash_kernel_traits<512, 64, 32, 4, cutlass::bfloat16_t>>, Is_dropout=true, Is_causal=false, Is_even_N=true, Is_even_K=false, Return_softmax=false, Params=Flash_fwd_params]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(15): here
            instantiation of "void flash_fwd_kernel<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax>(Flash_fwd_params) [with Kernel_traits=Flash_fwd_kernel_traits<512, 64, 32, 4, false, false, cutlass::bfloat16_t, Flash_kernel_traits<512, 64, 32, 4, cutlass::bfloat16_t>>, Is_dropout=true, Is_causal=false, Is_even_N=true, Is_even_K=false, Return_softmax=false]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(34): here
            instantiation of "void run_flash_fwd<Kernel_traits,Is_dropout,Is_causal>(Flash_fwd_params &, cudaStream_t) [with Kernel_traits=Flash_fwd_kernel_traits<512, 64, 32, 4, false, false, cutlass::bfloat16_t, Flash_kernel_traits<512, 64, 32, 4, cutlass::bfloat16_t>>, Is_dropout=true, Is_causal=false]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(264): here
            instantiation of "void run_mha_fwd_hdim512<T>(Flash_fwd_params &, cudaStream_t) [with T=cutlass::bfloat16_t]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_hdim512_bf16_sm80.cu(8): here

/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_kernel.h(140): warning: variable "MMA_M" was declared but never referenced
          detected during:
            instantiation of "void flash::compute_attn_1rowblock<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &, int, int, int) [with Kernel_traits=Flash_fwd_kernel_traits<512, 64, 32, 4, false, false, cutlass::bfloat16_t, Flash_kernel_traits<512, 64, 32, 4, cutlass::bfloat16_t>>, Is_dropout=true, Is_causal=false, Is_even_N=false, Is_even_K=true, Return_softmax=true, Params=Flash_fwd_params]" 
(588): here
            instantiation of "void flash::compute_attn<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &) [with Kernel_traits=Flash_fwd_kernel_traits<512, 64, 32, 4, false, false, cutlass::bfloat16_t, Flash_kernel_traits<512, 64, 32, 4, cutlass::bfloat16_t>>, Is_dropout=true, Is_causal=false, Is_even_N=false, Is_even_K=true, Return_softmax=true, Params=Flash_fwd_params]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(15): here
            instantiation of "void flash_fwd_kernel<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax>(Flash_fwd_params) [with Kernel_traits=Flash_fwd_kernel_traits<512, 64, 32, 4, false, false, cutlass::bfloat16_t, Flash_kernel_traits<512, 64, 32, 4, cutlass::bfloat16_t>>, Is_dropout=true, Is_causal=false, Is_even_N=false, Is_even_K=true, Return_softmax=true]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(34): here
            instantiation of "void run_flash_fwd<Kernel_traits,Is_dropout,Is_causal>(Flash_fwd_params &, cudaStream_t) [with Kernel_traits=Flash_fwd_kernel_traits<512, 64, 32, 4, false, false, cutlass::bfloat16_t, Flash_kernel_traits<512, 64, 32, 4, cutlass::bfloat16_t>>, Is_dropout=true, Is_causal=false]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(264): here
            instantiation of "void run_mha_fwd_hdim512<T>(Flash_fwd_params &, cudaStream_t) [with T=cutlass::bfloat16_t]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_hdim512_bf16_sm80.cu(8): here

/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_kernel.h(275): warning: variable "tQrQ" was declared but never referenced
          detected during:
            instantiation of "void flash::compute_attn_1rowblock<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &, int, int, int) [with Kernel_traits=Flash_fwd_kernel_traits<512, 64, 32, 4, false, false, cutlass::bfloat16_t, Flash_kernel_traits<512, 64, 32, 4, cutlass::bfloat16_t>>, Is_dropout=true, Is_causal=false, Is_even_N=false, Is_even_K=true, Return_softmax=true, Params=Flash_fwd_params]" 
(588): here
            instantiation of "void flash::compute_attn<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &) [with Kernel_traits=Flash_fwd_kernel_traits<512, 64, 32, 4, false, false, cutlass::bfloat16_t, Flash_kernel_traits<512, 64, 32, 4, cutlass::bfloat16_t>>, Is_dropout=true, Is_causal=false, Is_even_N=false, Is_even_K=true, Return_softmax=true, Params=Flash_fwd_params]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(15): here
            instantiation of "void flash_fwd_kernel<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax>(Flash_fwd_params) [with Kernel_traits=Flash_fwd_kernel_traits<512, 64, 32, 4, false, false, cutlass::bfloat16_t, Flash_kernel_traits<512, 64, 32, 4, cutlass::bfloat16_t>>, Is_dropout=true, Is_causal=false, Is_even_N=false, Is_even_K=true, Return_softmax=true]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(34): here
            instantiation of "void run_flash_fwd<Kernel_traits,Is_dropout,Is_causal>(Flash_fwd_params &, cudaStream_t) [with Kernel_traits=Flash_fwd_kernel_traits<512, 64, 32, 4, false, false, cutlass::bfloat16_t, Flash_kernel_traits<512, 64, 32, 4, cutlass::bfloat16_t>>, Is_dropout=true, Is_causal=false]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(264): here
            instantiation of "void run_mha_fwd_hdim512<T>(Flash_fwd_params &, cudaStream_t) [with T=cutlass::bfloat16_t]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_hdim512_bf16_sm80.cu(8): here

/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_kernel.h(140): warning: variable "MMA_M" was declared but never referenced
          detected during:
            instantiation of "void flash::compute_attn_1rowblock<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &, int, int, int) [with Kernel_traits=Flash_fwd_kernel_traits<512, 64, 32, 4, false, false, cutlass::bfloat16_t, Flash_kernel_traits<512, 64, 32, 4, cutlass::bfloat16_t>>, Is_dropout=true, Is_causal=false, Is_even_N=false, Is_even_K=true, Return_softmax=false, Params=Flash_fwd_params]" 
(588): here
            instantiation of "void flash::compute_attn<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &) [with Kernel_traits=Flash_fwd_kernel_traits<512, 64, 32, 4, false, false, cutlass::bfloat16_t, Flash_kernel_traits<512, 64, 32, 4, cutlass::bfloat16_t>>, Is_dropout=true, Is_causal=false, Is_even_N=false, Is_even_K=true, Return_softmax=false, Params=Flash_fwd_params]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(15): here
            instantiation of "void flash_fwd_kernel<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax>(Flash_fwd_params) [with Kernel_traits=Flash_fwd_kernel_traits<512, 64, 32, 4, false, false, cutlass::bfloat16_t, Flash_kernel_traits<512, 64, 32, 4, cutlass::bfloat16_t>>, Is_dropout=true, Is_causal=false, Is_even_N=false, Is_even_K=true, Return_softmax=false]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(34): here
            instantiation of "void run_flash_fwd<Kernel_traits,Is_dropout,Is_causal>(Flash_fwd_params &, cudaStream_t) [with Kernel_traits=Flash_fwd_kernel_traits<512, 64, 32, 4, false, false, cutlass::bfloat16_t, Flash_kernel_traits<512, 64, 32, 4, cutlass::bfloat16_t>>, Is_dropout=true, Is_causal=false]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(264): here
            instantiation of "void run_mha_fwd_hdim512<T>(Flash_fwd_params &, cudaStream_t) [with T=cutlass::bfloat16_t]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_hdim512_bf16_sm80.cu(8): here

/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_kernel.h(275): warning: variable "tQrQ" was declared but never referenced
          detected during:
            instantiation of "void flash::compute_attn_1rowblock<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &, int, int, int) [with Kernel_traits=Flash_fwd_kernel_traits<512, 64, 32, 4, false, false, cutlass::bfloat16_t, Flash_kernel_traits<512, 64, 32, 4, cutlass::bfloat16_t>>, Is_dropout=true, Is_causal=false, Is_even_N=false, Is_even_K=true, Return_softmax=false, Params=Flash_fwd_params]" 
(588): here
            instantiation of "void flash::compute_attn<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &) [with Kernel_traits=Flash_fwd_kernel_traits<512, 64, 32, 4, false, false, cutlass::bfloat16_t, Flash_kernel_traits<512, 64, 32, 4, cutlass::bfloat16_t>>, Is_dropout=true, Is_causal=false, Is_even_N=false, Is_even_K=true, Return_softmax=false, Params=Flash_fwd_params]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(15): here
            instantiation of "void flash_fwd_kernel<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax>(Flash_fwd_params) [with Kernel_traits=Flash_fwd_kernel_traits<512, 64, 32, 4, false, false, cutlass::bfloat16_t, Flash_kernel_traits<512, 64, 32, 4, cutlass::bfloat16_t>>, Is_dropout=true, Is_causal=false, Is_even_N=false, Is_even_K=true, Return_softmax=false]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(34): here
            instantiation of "void run_flash_fwd<Kernel_traits,Is_dropout,Is_causal>(Flash_fwd_params &, cudaStream_t) [with Kernel_traits=Flash_fwd_kernel_traits<512, 64, 32, 4, false, false, cutlass::bfloat16_t, Flash_kernel_traits<512, 64, 32, 4, cutlass::bfloat16_t>>, Is_dropout=true, Is_causal=false]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(264): here
            instantiation of "void run_mha_fwd_hdim512<T>(Flash_fwd_params &, cudaStream_t) [with T=cutlass::bfloat16_t]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_hdim512_bf16_sm80.cu(8): here

/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_kernel.h(140): warning: variable "MMA_M" was declared but never referenced
          detected during:
            instantiation of "void flash::compute_attn_1rowblock<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &, int, int, int) [with Kernel_traits=Flash_fwd_kernel_traits<512, 64, 32, 4, false, false, cutlass::bfloat16_t, Flash_kernel_traits<512, 64, 32, 4, cutlass::bfloat16_t>>, Is_dropout=true, Is_causal=false, Is_even_N=false, Is_even_K=false, Return_softmax=true, Params=Flash_fwd_params]" 
(588): here
            instantiation of "void flash::compute_attn<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &) [with Kernel_traits=Flash_fwd_kernel_traits<512, 64, 32, 4, false, false, cutlass::bfloat16_t, Flash_kernel_traits<512, 64, 32, 4, cutlass::bfloat16_t>>, Is_dropout=true, Is_causal=false, Is_even_N=false, Is_even_K=false, Return_softmax=true, Params=Flash_fwd_params]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(15): here
            instantiation of "void flash_fwd_kernel<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax>(Flash_fwd_params) [with Kernel_traits=Flash_fwd_kernel_traits<512, 64, 32, 4, false, false, cutlass::bfloat16_t, Flash_kernel_traits<512, 64, 32, 4, cutlass::bfloat16_t>>, Is_dropout=true, Is_causal=false, Is_even_N=false, Is_even_K=false, Return_softmax=true]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(34): here
            instantiation of "void run_flash_fwd<Kernel_traits,Is_dropout,Is_causal>(Flash_fwd_params &, cudaStream_t) [with Kernel_traits=Flash_fwd_kernel_traits<512, 64, 32, 4, false, false, cutlass::bfloat16_t, Flash_kernel_traits<512, 64, 32, 4, cutlass::bfloat16_t>>, Is_dropout=true, Is_causal=false]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(264): here
            instantiation of "void run_mha_fwd_hdim512<T>(Flash_fwd_params &, cudaStream_t) [with T=cutlass::bfloat16_t]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_hdim512_bf16_sm80.cu(8): here

/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_kernel.h(275): warning: variable "tQrQ" was declared but never referenced
          detected during:
            instantiation of "void flash::compute_attn_1rowblock<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &, int, int, int) [with Kernel_traits=Flash_fwd_kernel_traits<512, 64, 32, 4, false, false, cutlass::bfloat16_t, Flash_kernel_traits<512, 64, 32, 4, cutlass::bfloat16_t>>, Is_dropout=true, Is_causal=false, Is_even_N=false, Is_even_K=false, Return_softmax=true, Params=Flash_fwd_params]" 
(588): here
            instantiation of "void flash::compute_attn<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &) [with Kernel_traits=Flash_fwd_kernel_traits<512, 64, 32, 4, false, false, cutlass::bfloat16_t, Flash_kernel_traits<512, 64, 32, 4, cutlass::bfloat16_t>>, Is_dropout=true, Is_causal=false, Is_even_N=false, Is_even_K=false, Return_softmax=true, Params=Flash_fwd_params]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(15): here
            instantiation of "void flash_fwd_kernel<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax>(Flash_fwd_params) [with Kernel_traits=Flash_fwd_kernel_traits<512, 64, 32, 4, false, false, cutlass::bfloat16_t, Flash_kernel_traits<512, 64, 32, 4, cutlass::bfloat16_t>>, Is_dropout=true, Is_causal=false, Is_even_N=false, Is_even_K=false, Return_softmax=true]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(34): here
            instantiation of "void run_flash_fwd<Kernel_traits,Is_dropout,Is_causal>(Flash_fwd_params &, cudaStream_t) [with Kernel_traits=Flash_fwd_kernel_traits<512, 64, 32, 4, false, false, cutlass::bfloat16_t, Flash_kernel_traits<512, 64, 32, 4, cutlass::bfloat16_t>>, Is_dropout=true, Is_causal=false]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(264): here
            instantiation of "void run_mha_fwd_hdim512<T>(Flash_fwd_params &, cudaStream_t) [with T=cutlass::bfloat16_t]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_hdim512_bf16_sm80.cu(8): here

/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_kernel.h(140): warning: variable "MMA_M" was declared but never referenced
          detected during:
            instantiation of "void flash::compute_attn_1rowblock<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &, int, int, int) [with Kernel_traits=Flash_fwd_kernel_traits<512, 64, 32, 4, false, false, cutlass::bfloat16_t, Flash_kernel_traits<512, 64, 32, 4, cutlass::bfloat16_t>>, Is_dropout=true, Is_causal=false, Is_even_N=false, Is_even_K=false, Return_softmax=false, Params=Flash_fwd_params]" 
(588): here
            instantiation of "void flash::compute_attn<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &) [with Kernel_traits=Flash_fwd_kernel_traits<512, 64, 32, 4, false, false, cutlass::bfloat16_t, Flash_kernel_traits<512, 64, 32, 4, cutlass::bfloat16_t>>, Is_dropout=true, Is_causal=false, Is_even_N=false, Is_even_K=false, Return_softmax=false, Params=Flash_fwd_params]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(15): here
            instantiation of "void flash_fwd_kernel<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax>(Flash_fwd_params) [with Kernel_traits=Flash_fwd_kernel_traits<512, 64, 32, 4, false, false, cutlass::bfloat16_t, Flash_kernel_traits<512, 64, 32, 4, cutlass::bfloat16_t>>, Is_dropout=true, Is_causal=false, Is_even_N=false, Is_even_K=false, Return_softmax=false]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(34): here
            instantiation of "void run_flash_fwd<Kernel_traits,Is_dropout,Is_causal>(Flash_fwd_params &, cudaStream_t) [with Kernel_traits=Flash_fwd_kernel_traits<512, 64, 32, 4, false, false, cutlass::bfloat16_t, Flash_kernel_traits<512, 64, 32, 4, cutlass::bfloat16_t>>, Is_dropout=true, Is_causal=false]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(264): here
            instantiation of "void run_mha_fwd_hdim512<T>(Flash_fwd_params &, cudaStream_t) [with T=cutlass::bfloat16_t]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_hdim512_bf16_sm80.cu(8): here

/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_kernel.h(275): warning: variable "tQrQ" was declared but never referenced
          detected during:
            instantiation of "void flash::compute_attn_1rowblock<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &, int, int, int) [with Kernel_traits=Flash_fwd_kernel_traits<512, 64, 32, 4, false, false, cutlass::bfloat16_t, Flash_kernel_traits<512, 64, 32, 4, cutlass::bfloat16_t>>, Is_dropout=true, Is_causal=false, Is_even_N=false, Is_even_K=false, Return_softmax=false, Params=Flash_fwd_params]" 
(588): here
            instantiation of "void flash::compute_attn<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &) [with Kernel_traits=Flash_fwd_kernel_traits<512, 64, 32, 4, false, false, cutlass::bfloat16_t, Flash_kernel_traits<512, 64, 32, 4, cutlass::bfloat16_t>>, Is_dropout=true, Is_causal=false, Is_even_N=false, Is_even_K=false, Return_softmax=false, Params=Flash_fwd_params]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(15): here
            instantiation of "void flash_fwd_kernel<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax>(Flash_fwd_params) [with Kernel_traits=Flash_fwd_kernel_traits<512, 64, 32, 4, false, false, cutlass::bfloat16_t, Flash_kernel_traits<512, 64, 32, 4, cutlass::bfloat16_t>>, Is_dropout=true, Is_causal=false, Is_even_N=false, Is_even_K=false, Return_softmax=false]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(34): here
            instantiation of "void run_flash_fwd<Kernel_traits,Is_dropout,Is_causal>(Flash_fwd_params &, cudaStream_t) [with Kernel_traits=Flash_fwd_kernel_traits<512, 64, 32, 4, false, false, cutlass::bfloat16_t, Flash_kernel_traits<512, 64, 32, 4, cutlass::bfloat16_t>>, Is_dropout=true, Is_causal=false]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(264): here
            instantiation of "void run_mha_fwd_hdim512<T>(Flash_fwd_params &, cudaStream_t) [with T=cutlass::bfloat16_t]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_hdim512_bf16_sm80.cu(8): here

/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_kernel.h(140): warning: variable "MMA_M" was declared but never referenced
          detected during:
            instantiation of "void flash::compute_attn_1rowblock<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &, int, int, int) [with Kernel_traits=Flash_fwd_kernel_traits<512, 32, 32, 2, false, false, cutlass::bfloat16_t, Flash_kernel_traits<512, 32, 32, 2, cutlass::bfloat16_t>>, Is_dropout=true, Is_causal=false, Is_even_N=true, Is_even_K=true, Return_softmax=true, Params=Flash_fwd_params]" 
(588): here
            instantiation of "void flash::compute_attn<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &) [with Kernel_traits=Flash_fwd_kernel_traits<512, 32, 32, 2, false, false, cutlass::bfloat16_t, Flash_kernel_traits<512, 32, 32, 2, cutlass::bfloat16_t>>, Is_dropout=true, Is_causal=false, Is_even_N=true, Is_even_K=true, Return_softmax=true, Params=Flash_fwd_params]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(15): here
            instantiation of "void flash_fwd_kernel<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax>(Flash_fwd_params) [with Kernel_traits=Flash_fwd_kernel_traits<512, 32, 32, 2, false, false, cutlass::bfloat16_t, Flash_kernel_traits<512, 32, 32, 2, cutlass::bfloat16_t>>, Is_dropout=true, Is_causal=false, Is_even_N=true, Is_even_K=true, Return_softmax=true]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(34): here
            instantiation of "void run_flash_fwd<Kernel_traits,Is_dropout,Is_causal>(Flash_fwd_params &, cudaStream_t) [with Kernel_traits=Flash_fwd_kernel_traits<512, 32, 32, 2, false, false, cutlass::bfloat16_t, Flash_kernel_traits<512, 32, 32, 2, cutlass::bfloat16_t>>, Is_dropout=true, Is_causal=false]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(264): here
            instantiation of "void run_mha_fwd_hdim512<T>(Flash_fwd_params &, cudaStream_t) [with T=cutlass::bfloat16_t]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_hdim512_bf16_sm80.cu(8): here

/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_kernel.h(275): warning: variable "tQrQ" was declared but never referenced
          detected during:
            instantiation of "void flash::compute_attn_1rowblock<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &, int, int, int) [with Kernel_traits=Flash_fwd_kernel_traits<512, 32, 32, 2, false, false, cutlass::bfloat16_t, Flash_kernel_traits<512, 32, 32, 2, cutlass::bfloat16_t>>, Is_dropout=true, Is_causal=false, Is_even_N=true, Is_even_K=true, Return_softmax=true, Params=Flash_fwd_params]" 
(588): here
            instantiation of "void flash::compute_attn<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &) [with Kernel_traits=Flash_fwd_kernel_traits<512, 32, 32, 2, false, false, cutlass::bfloat16_t, Flash_kernel_traits<512, 32, 32, 2, cutlass::bfloat16_t>>, Is_dropout=true, Is_causal=false, Is_even_N=true, Is_even_K=true, Return_softmax=true, Params=Flash_fwd_params]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(15): here
            instantiation of "void flash_fwd_kernel<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax>(Flash_fwd_params) [with Kernel_traits=Flash_fwd_kernel_traits<512, 32, 32, 2, false, false, cutlass::bfloat16_t, Flash_kernel_traits<512, 32, 32, 2, cutlass::bfloat16_t>>, Is_dropout=true, Is_causal=false, Is_even_N=true, Is_even_K=true, Return_softmax=true]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(34): here
            instantiation of "void run_flash_fwd<Kernel_traits,Is_dropout,Is_causal>(Flash_fwd_params &, cudaStream_t) [with Kernel_traits=Flash_fwd_kernel_traits<512, 32, 32, 2, false, false, cutlass::bfloat16_t, Flash_kernel_traits<512, 32, 32, 2, cutlass::bfloat16_t>>, Is_dropout=true, Is_causal=false]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(264): here
            instantiation of "void run_mha_fwd_hdim512<T>(Flash_fwd_params &, cudaStream_t) [with T=cutlass::bfloat16_t]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_hdim512_bf16_sm80.cu(8): here

/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_kernel.h(140): warning: variable "MMA_M" was declared but never referenced
          detected during:
            instantiation of "void flash::compute_attn_1rowblock<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &, int, int, int) [with Kernel_traits=Flash_fwd_kernel_traits<512, 32, 32, 2, false, false, cutlass::bfloat16_t, Flash_kernel_traits<512, 32, 32, 2, cutlass::bfloat16_t>>, Is_dropout=true, Is_causal=false, Is_even_N=true, Is_even_K=true, Return_softmax=false, Params=Flash_fwd_params]" 
(588): here
            instantiation of "void flash::compute_attn<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &) [with Kernel_traits=Flash_fwd_kernel_traits<512, 32, 32, 2, false, false, cutlass::bfloat16_t, Flash_kernel_traits<512, 32, 32, 2, cutlass::bfloat16_t>>, Is_dropout=true, Is_causal=false, Is_even_N=true, Is_even_K=true, Return_softmax=false, Params=Flash_fwd_params]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(15): here
            instantiation of "void flash_fwd_kernel<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax>(Flash_fwd_params) [with Kernel_traits=Flash_fwd_kernel_traits<512, 32, 32, 2, false, false, cutlass::bfloat16_t, Flash_kernel_traits<512, 32, 32, 2, cutlass::bfloat16_t>>, Is_dropout=true, Is_causal=false, Is_even_N=true, Is_even_K=true, Return_softmax=false]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(34): here
            instantiation of "void run_flash_fwd<Kernel_traits,Is_dropout,Is_causal>(Flash_fwd_params &, cudaStream_t) [with Kernel_traits=Flash_fwd_kernel_traits<512, 32, 32, 2, false, false, cutlass::bfloat16_t, Flash_kernel_traits<512, 32, 32, 2, cutlass::bfloat16_t>>, Is_dropout=true, Is_causal=false]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(264): here
            instantiation of "void run_mha_fwd_hdim512<T>(Flash_fwd_params &, cudaStream_t) [with T=cutlass::bfloat16_t]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_hdim512_bf16_sm80.cu(8): here

/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_kernel.h(275): warning: variable "tQrQ" was declared but never referenced
          detected during:
            instantiation of "void flash::compute_attn_1rowblock<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &, int, int, int) [with Kernel_traits=Flash_fwd_kernel_traits<512, 32, 32, 2, false, false, cutlass::bfloat16_t, Flash_kernel_traits<512, 32, 32, 2, cutlass::bfloat16_t>>, Is_dropout=true, Is_causal=false, Is_even_N=true, Is_even_K=true, Return_softmax=false, Params=Flash_fwd_params]" 
(588): here
            instantiation of "void flash::compute_attn<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &) [with Kernel_traits=Flash_fwd_kernel_traits<512, 32, 32, 2, false, false, cutlass::bfloat16_t, Flash_kernel_traits<512, 32, 32, 2, cutlass::bfloat16_t>>, Is_dropout=true, Is_causal=false, Is_even_N=true, Is_even_K=true, Return_softmax=false, Params=Flash_fwd_params]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(15): here
            instantiation of "void flash_fwd_kernel<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax>(Flash_fwd_params) [with Kernel_traits=Flash_fwd_kernel_traits<512, 32, 32, 2, false, false, cutlass::bfloat16_t, Flash_kernel_traits<512, 32, 32, 2, cutlass::bfloat16_t>>, Is_dropout=true, Is_causal=false, Is_even_N=true, Is_even_K=true, Return_softmax=false]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(34): here
            instantiation of "void run_flash_fwd<Kernel_traits,Is_dropout,Is_causal>(Flash_fwd_params &, cudaStream_t) [with Kernel_traits=Flash_fwd_kernel_traits<512, 32, 32, 2, false, false, cutlass::bfloat16_t, Flash_kernel_traits<512, 32, 32, 2, cutlass::bfloat16_t>>, Is_dropout=true, Is_causal=false]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(264): here
            instantiation of "void run_mha_fwd_hdim512<T>(Flash_fwd_params &, cudaStream_t) [with T=cutlass::bfloat16_t]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_hdim512_bf16_sm80.cu(8): here

/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_kernel.h(140): warning: variable "MMA_M" was declared but never referenced
          detected during:
            instantiation of "void flash::compute_attn_1rowblock<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &, int, int, int) [with Kernel_traits=Flash_fwd_kernel_traits<512, 32, 32, 2, false, false, cutlass::bfloat16_t, Flash_kernel_traits<512, 32, 32, 2, cutlass::bfloat16_t>>, Is_dropout=true, Is_causal=false, Is_even_N=true, Is_even_K=false, Return_softmax=true, Params=Flash_fwd_params]" 
(588): here
            instantiation of "void flash::compute_attn<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &) [with Kernel_traits=Flash_fwd_kernel_traits<512, 32, 32, 2, false, false, cutlass::bfloat16_t, Flash_kernel_traits<512, 32, 32, 2, cutlass::bfloat16_t>>, Is_dropout=true, Is_causal=false, Is_even_N=true, Is_even_K=false, Return_softmax=true, Params=Flash_fwd_params]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(15): here
            instantiation of "void flash_fwd_kernel<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax>(Flash_fwd_params) [with Kernel_traits=Flash_fwd_kernel_traits<512, 32, 32, 2, false, false, cutlass::bfloat16_t, Flash_kernel_traits<512, 32, 32, 2, cutlass::bfloat16_t>>, Is_dropout=true, Is_causal=false, Is_even_N=true, Is_even_K=false, Return_softmax=true]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(34): here
            instantiation of "void run_flash_fwd<Kernel_traits,Is_dropout,Is_causal>(Flash_fwd_params &, cudaStream_t) [with Kernel_traits=Flash_fwd_kernel_traits<512, 32, 32, 2, false, false, cutlass::bfloat16_t, Flash_kernel_traits<512, 32, 32, 2, cutlass::bfloat16_t>>, Is_dropout=true, Is_causal=false]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(264): here
            instantiation of "void run_mha_fwd_hdim512<T>(Flash_fwd_params &, cudaStream_t) [with T=cutlass::bfloat16_t]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_hdim512_bf16_sm80.cu(8): here

/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_kernel.h(275): warning: variable "tQrQ" was declared but never referenced
          detected during:
            instantiation of "void flash::compute_attn_1rowblock<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &, int, int, int) [with Kernel_traits=Flash_fwd_kernel_traits<512, 32, 32, 2, false, false, cutlass::bfloat16_t, Flash_kernel_traits<512, 32, 32, 2, cutlass::bfloat16_t>>, Is_dropout=true, Is_causal=false, Is_even_N=true, Is_even_K=false, Return_softmax=true, Params=Flash_fwd_params]" 
(588): here
            instantiation of "void flash::compute_attn<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &) [with Kernel_traits=Flash_fwd_kernel_traits<512, 32, 32, 2, false, false, cutlass::bfloat16_t, Flash_kernel_traits<512, 32, 32, 2, cutlass::bfloat16_t>>, Is_dropout=true, Is_causal=false, Is_even_N=true, Is_even_K=false, Return_softmax=true, Params=Flash_fwd_params]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(15): here
            instantiation of "void flash_fwd_kernel<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax>(Flash_fwd_params) [with Kernel_traits=Flash_fwd_kernel_traits<512, 32, 32, 2, false, false, cutlass::bfloat16_t, Flash_kernel_traits<512, 32, 32, 2, cutlass::bfloat16_t>>, Is_dropout=true, Is_causal=false, Is_even_N=true, Is_even_K=false, Return_softmax=true]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(34): here
            instantiation of "void run_flash_fwd<Kernel_traits,Is_dropout,Is_causal>(Flash_fwd_params &, cudaStream_t) [with Kernel_traits=Flash_fwd_kernel_traits<512, 32, 32, 2, false, false, cutlass::bfloat16_t, Flash_kernel_traits<512, 32, 32, 2, cutlass::bfloat16_t>>, Is_dropout=true, Is_causal=false]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(264): here
            instantiation of "void run_mha_fwd_hdim512<T>(Flash_fwd_params &, cudaStream_t) [with T=cutlass::bfloat16_t]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_hdim512_bf16_sm80.cu(8): here

/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_kernel.h(140): warning: variable "MMA_M" was declared but never referenced
          detected during:
            instantiation of "void flash::compute_attn_1rowblock<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &, int, int, int) [with Kernel_traits=Flash_fwd_kernel_traits<512, 32, 32, 2, false, false, cutlass::bfloat16_t, Flash_kernel_traits<512, 32, 32, 2, cutlass::bfloat16_t>>, Is_dropout=true, Is_causal=false, Is_even_N=true, Is_even_K=false, Return_softmax=false, Params=Flash_fwd_params]" 
(588): here
            instantiation of "void flash::compute_attn<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &) [with Kernel_traits=Flash_fwd_kernel_traits<512, 32, 32, 2, false, false, cutlass::bfloat16_t, Flash_kernel_traits<512, 32, 32, 2, cutlass::bfloat16_t>>, Is_dropout=true, Is_causal=false, Is_even_N=true, Is_even_K=false, Return_softmax=false, Params=Flash_fwd_params]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(15): here
            instantiation of "void flash_fwd_kernel<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax>(Flash_fwd_params) [with Kernel_traits=Flash_fwd_kernel_traits<512, 32, 32, 2, false, false, cutlass::bfloat16_t, Flash_kernel_traits<512, 32, 32, 2, cutlass::bfloat16_t>>, Is_dropout=true, Is_causal=false, Is_even_N=true, Is_even_K=false, Return_softmax=false]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(34): here
            instantiation of "void run_flash_fwd<Kernel_traits,Is_dropout,Is_causal>(Flash_fwd_params &, cudaStream_t) [with Kernel_traits=Flash_fwd_kernel_traits<512, 32, 32, 2, false, false, cutlass::bfloat16_t, Flash_kernel_traits<512, 32, 32, 2, cutlass::bfloat16_t>>, Is_dropout=true, Is_causal=false]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(264): here
            instantiation of "void run_mha_fwd_hdim512<T>(Flash_fwd_params &, cudaStream_t) [with T=cutlass::bfloat16_t]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_hdim512_bf16_sm80.cu(8): here

/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_kernel.h(275): warning: variable "tQrQ" was declared but never referenced
          detected during:
            instantiation of "void flash::compute_attn_1rowblock<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &, int, int, int) [with Kernel_traits=Flash_fwd_kernel_traits<512, 32, 32, 2, false, false, cutlass::bfloat16_t, Flash_kernel_traits<512, 32, 32, 2, cutlass::bfloat16_t>>, Is_dropout=true, Is_causal=false, Is_even_N=true, Is_even_K=false, Return_softmax=false, Params=Flash_fwd_params]" 
(588): here
            instantiation of "void flash::compute_attn<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &) [with Kernel_traits=Flash_fwd_kernel_traits<512, 32, 32, 2, false, false, cutlass::bfloat16_t, Flash_kernel_traits<512, 32, 32, 2, cutlass::bfloat16_t>>, Is_dropout=true, Is_causal=false, Is_even_N=true, Is_even_K=false, Return_softmax=false, Params=Flash_fwd_params]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(15): here
            instantiation of "void flash_fwd_kernel<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax>(Flash_fwd_params) [with Kernel_traits=Flash_fwd_kernel_traits<512, 32, 32, 2, false, false, cutlass::bfloat16_t, Flash_kernel_traits<512, 32, 32, 2, cutlass::bfloat16_t>>, Is_dropout=true, Is_causal=false, Is_even_N=true, Is_even_K=false, Return_softmax=false]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(34): here
            instantiation of "void run_flash_fwd<Kernel_traits,Is_dropout,Is_causal>(Flash_fwd_params &, cudaStream_t) [with Kernel_traits=Flash_fwd_kernel_traits<512, 32, 32, 2, false, false, cutlass::bfloat16_t, Flash_kernel_traits<512, 32, 32, 2, cutlass::bfloat16_t>>, Is_dropout=true, Is_causal=false]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(264): here
            instantiation of "void run_mha_fwd_hdim512<T>(Flash_fwd_params &, cudaStream_t) [with T=cutlass::bfloat16_t]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_hdim512_bf16_sm80.cu(8): here

/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_kernel.h(140): warning: variable "MMA_M" was declared but never referenced
          detected during:
            instantiation of "void flash::compute_attn_1rowblock<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &, int, int, int) [with Kernel_traits=Flash_fwd_kernel_traits<512, 32, 32, 2, false, false, cutlass::bfloat16_t, Flash_kernel_traits<512, 32, 32, 2, cutlass::bfloat16_t>>, Is_dropout=true, Is_causal=false, Is_even_N=false, Is_even_K=true, Return_softmax=true, Params=Flash_fwd_params]" 
(588): here
            instantiation of "void flash::compute_attn<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &) [with Kernel_traits=Flash_fwd_kernel_traits<512, 32, 32, 2, false, false, cutlass::bfloat16_t, Flash_kernel_traits<512, 32, 32, 2, cutlass::bfloat16_t>>, Is_dropout=true, Is_causal=false, Is_even_N=false, Is_even_K=true, Return_softmax=true, Params=Flash_fwd_params]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(15): here
            instantiation of "void flash_fwd_kernel<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax>(Flash_fwd_params) [with Kernel_traits=Flash_fwd_kernel_traits<512, 32, 32, 2, false, false, cutlass::bfloat16_t, Flash_kernel_traits<512, 32, 32, 2, cutlass::bfloat16_t>>, Is_dropout=true, Is_causal=false, Is_even_N=false, Is_even_K=true, Return_softmax=true]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(34): here
            instantiation of "void run_flash_fwd<Kernel_traits,Is_dropout,Is_causal>(Flash_fwd_params &, cudaStream_t) [with Kernel_traits=Flash_fwd_kernel_traits<512, 32, 32, 2, false, false, cutlass::bfloat16_t, Flash_kernel_traits<512, 32, 32, 2, cutlass::bfloat16_t>>, Is_dropout=true, Is_causal=false]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(264): here
            instantiation of "void run_mha_fwd_hdim512<T>(Flash_fwd_params &, cudaStream_t) [with T=cutlass::bfloat16_t]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_hdim512_bf16_sm80.cu(8): here

/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_kernel.h(275): warning: variable "tQrQ" was declared but never referenced
          detected during:
            instantiation of "void flash::compute_attn_1rowblock<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &, int, int, int) [with Kernel_traits=Flash_fwd_kernel_traits<512, 32, 32, 2, false, false, cutlass::bfloat16_t, Flash_kernel_traits<512, 32, 32, 2, cutlass::bfloat16_t>>, Is_dropout=true, Is_causal=false, Is_even_N=false, Is_even_K=true, Return_softmax=true, Params=Flash_fwd_params]" 
(588): here
            instantiation of "void flash::compute_attn<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &) [with Kernel_traits=Flash_fwd_kernel_traits<512, 32, 32, 2, false, false, cutlass::bfloat16_t, Flash_kernel_traits<512, 32, 32, 2, cutlass::bfloat16_t>>, Is_dropout=true, Is_causal=false, Is_even_N=false, Is_even_K=true, Return_softmax=true, Params=Flash_fwd_params]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(15): here
            instantiation of "void flash_fwd_kernel<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax>(Flash_fwd_params) [with Kernel_traits=Flash_fwd_kernel_traits<512, 32, 32, 2, false, false, cutlass::bfloat16_t, Flash_kernel_traits<512, 32, 32, 2, cutlass::bfloat16_t>>, Is_dropout=true, Is_causal=false, Is_even_N=false, Is_even_K=true, Return_softmax=true]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(34): here
            instantiation of "void run_flash_fwd<Kernel_traits,Is_dropout,Is_causal>(Flash_fwd_params &, cudaStream_t) [with Kernel_traits=Flash_fwd_kernel_traits<512, 32, 32, 2, false, false, cutlass::bfloat16_t, Flash_kernel_traits<512, 32, 32, 2, cutlass::bfloat16_t>>, Is_dropout=true, Is_causal=false]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(264): here
            instantiation of "void run_mha_fwd_hdim512<T>(Flash_fwd_params &, cudaStream_t) [with T=cutlass::bfloat16_t]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_hdim512_bf16_sm80.cu(8): here

/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_kernel.h(140): warning: variable "MMA_M" was declared but never referenced
          detected during:
            instantiation of "void flash::compute_attn_1rowblock<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &, int, int, int) [with Kernel_traits=Flash_fwd_kernel_traits<512, 32, 32, 2, false, false, cutlass::bfloat16_t, Flash_kernel_traits<512, 32, 32, 2, cutlass::bfloat16_t>>, Is_dropout=true, Is_causal=false, Is_even_N=false, Is_even_K=true, Return_softmax=false, Params=Flash_fwd_params]" 
(588): here
            instantiation of "void flash::compute_attn<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &) [with Kernel_traits=Flash_fwd_kernel_traits<512, 32, 32, 2, false, false, cutlass::bfloat16_t, Flash_kernel_traits<512, 32, 32, 2, cutlass::bfloat16_t>>, Is_dropout=true, Is_causal=false, Is_even_N=false, Is_even_K=true, Return_softmax=false, Params=Flash_fwd_params]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(15): here
            instantiation of "void flash_fwd_kernel<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax>(Flash_fwd_params) [with Kernel_traits=Flash_fwd_kernel_traits<512, 32, 32, 2, false, false, cutlass::bfloat16_t, Flash_kernel_traits<512, 32, 32, 2, cutlass::bfloat16_t>>, Is_dropout=true, Is_causal=false, Is_even_N=false, Is_even_K=true, Return_softmax=false]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(34): here
            instantiation of "void run_flash_fwd<Kernel_traits,Is_dropout,Is_causal>(Flash_fwd_params &, cudaStream_t) [with Kernel_traits=Flash_fwd_kernel_traits<512, 32, 32, 2, false, false, cutlass::bfloat16_t, Flash_kernel_traits<512, 32, 32, 2, cutlass::bfloat16_t>>, Is_dropout=true, Is_causal=false]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(264): here
            instantiation of "void run_mha_fwd_hdim512<T>(Flash_fwd_params &, cudaStream_t) [with T=cutlass::bfloat16_t]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_hdim512_bf16_sm80.cu(8): here

/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_kernel.h(275): warning: variable "tQrQ" was declared but never referenced
          detected during:
            instantiation of "void flash::compute_attn_1rowblock<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &, int, int, int) [with Kernel_traits=Flash_fwd_kernel_traits<512, 32, 32, 2, false, false, cutlass::bfloat16_t, Flash_kernel_traits<512, 32, 32, 2, cutlass::bfloat16_t>>, Is_dropout=true, Is_causal=false, Is_even_N=false, Is_even_K=true, Return_softmax=false, Params=Flash_fwd_params]" 
(588): here
            instantiation of "void flash::compute_attn<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &) [with Kernel_traits=Flash_fwd_kernel_traits<512, 32, 32, 2, false, false, cutlass::bfloat16_t, Flash_kernel_traits<512, 32, 32, 2, cutlass::bfloat16_t>>, Is_dropout=true, Is_causal=false, Is_even_N=false, Is_even_K=true, Return_softmax=false, Params=Flash_fwd_params]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(15): here
            instantiation of "void flash_fwd_kernel<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax>(Flash_fwd_params) [with Kernel_traits=Flash_fwd_kernel_traits<512, 32, 32, 2, false, false, cutlass::bfloat16_t, Flash_kernel_traits<512, 32, 32, 2, cutlass::bfloat16_t>>, Is_dropout=true, Is_causal=false, Is_even_N=false, Is_even_K=true, Return_softmax=false]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(34): here
            instantiation of "void run_flash_fwd<Kernel_traits,Is_dropout,Is_causal>(Flash_fwd_params &, cudaStream_t) [with Kernel_traits=Flash_fwd_kernel_traits<512, 32, 32, 2, false, false, cutlass::bfloat16_t, Flash_kernel_traits<512, 32, 32, 2, cutlass::bfloat16_t>>, Is_dropout=true, Is_causal=false]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(264): here
            instantiation of "void run_mha_fwd_hdim512<T>(Flash_fwd_params &, cudaStream_t) [with T=cutlass::bfloat16_t]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_hdim512_bf16_sm80.cu(8): here

/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_kernel.h(140): warning: variable "MMA_M" was declared but never referenced
          detected during:
            instantiation of "void flash::compute_attn_1rowblock<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &, int, int, int) [with Kernel_traits=Flash_fwd_kernel_traits<512, 32, 32, 2, false, false, cutlass::bfloat16_t, Flash_kernel_traits<512, 32, 32, 2, cutlass::bfloat16_t>>, Is_dropout=true, Is_causal=false, Is_even_N=false, Is_even_K=false, Return_softmax=true, Params=Flash_fwd_params]" 
(588): here
            instantiation of "void flash::compute_attn<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &) [with Kernel_traits=Flash_fwd_kernel_traits<512, 32, 32, 2, false, false, cutlass::bfloat16_t, Flash_kernel_traits<512, 32, 32, 2, cutlass::bfloat16_t>>, Is_dropout=true, Is_causal=false, Is_even_N=false, Is_even_K=false, Return_softmax=true, Params=Flash_fwd_params]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(15): here
            instantiation of "void flash_fwd_kernel<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax>(Flash_fwd_params) [with Kernel_traits=Flash_fwd_kernel_traits<512, 32, 32, 2, false, false, cutlass::bfloat16_t, Flash_kernel_traits<512, 32, 32, 2, cutlass::bfloat16_t>>, Is_dropout=true, Is_causal=false, Is_even_N=false, Is_even_K=false, Return_softmax=true]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(34): here
            instantiation of "void run_flash_fwd<Kernel_traits,Is_dropout,Is_causal>(Flash_fwd_params &, cudaStream_t) [with Kernel_traits=Flash_fwd_kernel_traits<512, 32, 32, 2, false, false, cutlass::bfloat16_t, Flash_kernel_traits<512, 32, 32, 2, cutlass::bfloat16_t>>, Is_dropout=true, Is_causal=false]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(264): here
            instantiation of "void run_mha_fwd_hdim512<T>(Flash_fwd_params &, cudaStream_t) [with T=cutlass::bfloat16_t]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_hdim512_bf16_sm80.cu(8): here

/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_kernel.h(275): warning: variable "tQrQ" was declared but never referenced
          detected during:
            instantiation of "void flash::compute_attn_1rowblock<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &, int, int, int) [with Kernel_traits=Flash_fwd_kernel_traits<512, 32, 32, 2, false, false, cutlass::bfloat16_t, Flash_kernel_traits<512, 32, 32, 2, cutlass::bfloat16_t>>, Is_dropout=true, Is_causal=false, Is_even_N=false, Is_even_K=false, Return_softmax=true, Params=Flash_fwd_params]" 
(588): here
            instantiation of "void flash::compute_attn<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &) [with Kernel_traits=Flash_fwd_kernel_traits<512, 32, 32, 2, false, false, cutlass::bfloat16_t, Flash_kernel_traits<512, 32, 32, 2, cutlass::bfloat16_t>>, Is_dropout=true, Is_causal=false, Is_even_N=false, Is_even_K=false, Return_softmax=true, Params=Flash_fwd_params]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(15): here
            instantiation of "void flash_fwd_kernel<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax>(Flash_fwd_params) [with Kernel_traits=Flash_fwd_kernel_traits<512, 32, 32, 2, false, false, cutlass::bfloat16_t, Flash_kernel_traits<512, 32, 32, 2, cutlass::bfloat16_t>>, Is_dropout=true, Is_causal=false, Is_even_N=false, Is_even_K=false, Return_softmax=true]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(34): here
            instantiation of "void run_flash_fwd<Kernel_traits,Is_dropout,Is_causal>(Flash_fwd_params &, cudaStream_t) [with Kernel_traits=Flash_fwd_kernel_traits<512, 32, 32, 2, false, false, cutlass::bfloat16_t, Flash_kernel_traits<512, 32, 32, 2, cutlass::bfloat16_t>>, Is_dropout=true, Is_causal=false]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(264): here
            instantiation of "void run_mha_fwd_hdim512<T>(Flash_fwd_params &, cudaStream_t) [with T=cutlass::bfloat16_t]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_hdim512_bf16_sm80.cu(8): here

/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_kernel.h(140): warning: variable "MMA_M" was declared but never referenced
          detected during:
            instantiation of "void flash::compute_attn_1rowblock<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &, int, int, int) [with Kernel_traits=Flash_fwd_kernel_traits<512, 32, 32, 2, false, false, cutlass::bfloat16_t, Flash_kernel_traits<512, 32, 32, 2, cutlass::bfloat16_t>>, Is_dropout=true, Is_causal=false, Is_even_N=false, Is_even_K=false, Return_softmax=false, Params=Flash_fwd_params]" 
(588): here
            instantiation of "void flash::compute_attn<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &) [with Kernel_traits=Flash_fwd_kernel_traits<512, 32, 32, 2, false, false, cutlass::bfloat16_t, Flash_kernel_traits<512, 32, 32, 2, cutlass::bfloat16_t>>, Is_dropout=true, Is_causal=false, Is_even_N=false, Is_even_K=false, Return_softmax=false, Params=Flash_fwd_params]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(15): here
            instantiation of "void flash_fwd_kernel<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax>(Flash_fwd_params) [with Kernel_traits=Flash_fwd_kernel_traits<512, 32, 32, 2, false, false, cutlass::bfloat16_t, Flash_kernel_traits<512, 32, 32, 2, cutlass::bfloat16_t>>, Is_dropout=true, Is_causal=false, Is_even_N=false, Is_even_K=false, Return_softmax=false]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(34): here
            instantiation of "void run_flash_fwd<Kernel_traits,Is_dropout,Is_causal>(Flash_fwd_params &, cudaStream_t) [with Kernel_traits=Flash_fwd_kernel_traits<512, 32, 32, 2, false, false, cutlass::bfloat16_t, Flash_kernel_traits<512, 32, 32, 2, cutlass::bfloat16_t>>, Is_dropout=true, Is_causal=false]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(264): here
            instantiation of "void run_mha_fwd_hdim512<T>(Flash_fwd_params &, cudaStream_t) [with T=cutlass::bfloat16_t]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_hdim512_bf16_sm80.cu(8): here

/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_kernel.h(275): warning: variable "tQrQ" was declared but never referenced
          detected during:
            instantiation of "void flash::compute_attn_1rowblock<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &, int, int, int) [with Kernel_traits=Flash_fwd_kernel_traits<512, 32, 32, 2, false, false, cutlass::bfloat16_t, Flash_kernel_traits<512, 32, 32, 2, cutlass::bfloat16_t>>, Is_dropout=true, Is_causal=false, Is_even_N=false, Is_even_K=false, Return_softmax=false, Params=Flash_fwd_params]" 
(588): here
            instantiation of "void flash::compute_attn<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &) [with Kernel_traits=Flash_fwd_kernel_traits<512, 32, 32, 2, false, false, cutlass::bfloat16_t, Flash_kernel_traits<512, 32, 32, 2, cutlass::bfloat16_t>>, Is_dropout=true, Is_causal=false, Is_even_N=false, Is_even_K=false, Return_softmax=false, Params=Flash_fwd_params]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(15): here
            instantiation of "void flash_fwd_kernel<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax>(Flash_fwd_params) [with Kernel_traits=Flash_fwd_kernel_traits<512, 32, 32, 2, false, false, cutlass::bfloat16_t, Flash_kernel_traits<512, 32, 32, 2, cutlass::bfloat16_t>>, Is_dropout=true, Is_causal=false, Is_even_N=false, Is_even_K=false, Return_softmax=false]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(34): here
            instantiation of "void run_flash_fwd<Kernel_traits,Is_dropout,Is_causal>(Flash_fwd_params &, cudaStream_t) [with Kernel_traits=Flash_fwd_kernel_traits<512, 32, 32, 2, false, false, cutlass::bfloat16_t, Flash_kernel_traits<512, 32, 32, 2, cutlass::bfloat16_t>>, Is_dropout=true, Is_causal=false]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(264): here
            instantiation of "void run_mha_fwd_hdim512<T>(Flash_fwd_params &, cudaStream_t) [with T=cutlass::bfloat16_t]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_hdim512_bf16_sm80.cu(8): here

/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_kernel.h(140): warning: variable "MMA_M" was declared but never referenced
          detected during:
            instantiation of "void flash::compute_attn_1rowblock<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &, int, int, int) [with Kernel_traits=Flash_fwd_kernel_traits<512, 64, 32, 4, false, false, cutlass::bfloat16_t, Flash_kernel_traits<512, 64, 32, 4, cutlass::bfloat16_t>>, Is_dropout=false, Is_causal=true, Is_even_N=true, Is_even_K=true, Return_softmax=false, Params=Flash_fwd_params]" 
(588): here
            instantiation of "void flash::compute_attn<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &) [with Kernel_traits=Flash_fwd_kernel_traits<512, 64, 32, 4, false, false, cutlass::bfloat16_t, Flash_kernel_traits<512, 64, 32, 4, cutlass::bfloat16_t>>, Is_dropout=false, Is_causal=true, Is_even_N=true, Is_even_K=true, Return_softmax=false, Params=Flash_fwd_params]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(15): here
            instantiation of "void flash_fwd_kernel<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax>(Flash_fwd_params) [with Kernel_traits=Flash_fwd_kernel_traits<512, 64, 32, 4, false, false, cutlass::bfloat16_t, Flash_kernel_traits<512, 64, 32, 4, cutlass::bfloat16_t>>, Is_dropout=false, Is_causal=true, Is_even_N=true, Is_even_K=true, Return_softmax=false]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(34): here
            instantiation of "void run_flash_fwd<Kernel_traits,Is_dropout,Is_causal>(Flash_fwd_params &, cudaStream_t) [with Kernel_traits=Flash_fwd_kernel_traits<512, 64, 32, 4, false, false, cutlass::bfloat16_t, Flash_kernel_traits<512, 64, 32, 4, cutlass::bfloat16_t>>, Is_dropout=false, Is_causal=true]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(264): here
            instantiation of "void run_mha_fwd_hdim512<T>(Flash_fwd_params &, cudaStream_t) [with T=cutlass::bfloat16_t]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_hdim512_bf16_sm80.cu(8): here

/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_kernel.h(275): warning: variable "tQrQ" was declared but never referenced
          detected during:
            instantiation of "void flash::compute_attn_1rowblock<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &, int, int, int) [with Kernel_traits=Flash_fwd_kernel_traits<512, 64, 32, 4, false, false, cutlass::bfloat16_t, Flash_kernel_traits<512, 64, 32, 4, cutlass::bfloat16_t>>, Is_dropout=false, Is_causal=true, Is_even_N=true, Is_even_K=true, Return_softmax=false, Params=Flash_fwd_params]" 
(588): here
            instantiation of "void flash::compute_attn<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &) [with Kernel_traits=Flash_fwd_kernel_traits<512, 64, 32, 4, false, false, cutlass::bfloat16_t, Flash_kernel_traits<512, 64, 32, 4, cutlass::bfloat16_t>>, Is_dropout=false, Is_causal=true, Is_even_N=true, Is_even_K=true, Return_softmax=false, Params=Flash_fwd_params]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(15): here
            instantiation of "void flash_fwd_kernel<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax>(Flash_fwd_params) [with Kernel_traits=Flash_fwd_kernel_traits<512, 64, 32, 4, false, false, cutlass::bfloat16_t, Flash_kernel_traits<512, 64, 32, 4, cutlass::bfloat16_t>>, Is_dropout=false, Is_causal=true, Is_even_N=true, Is_even_K=true, Return_softmax=false]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(34): here
            instantiation of "void run_flash_fwd<Kernel_traits,Is_dropout,Is_causal>(Flash_fwd_params &, cudaStream_t) [with Kernel_traits=Flash_fwd_kernel_traits<512, 64, 32, 4, false, false, cutlass::bfloat16_t, Flash_kernel_traits<512, 64, 32, 4, cutlass::bfloat16_t>>, Is_dropout=false, Is_causal=true]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(264): here
            instantiation of "void run_mha_fwd_hdim512<T>(Flash_fwd_params &, cudaStream_t) [with T=cutlass::bfloat16_t]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_hdim512_bf16_sm80.cu(8): here

/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_kernel.h(140): warning: variable "MMA_M" was declared but never referenced
          detected during:
            instantiation of "void flash::compute_attn_1rowblock<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &, int, int, int) [with Kernel_traits=Flash_fwd_kernel_traits<512, 64, 32, 4, false, false, cutlass::bfloat16_t, Flash_kernel_traits<512, 64, 32, 4, cutlass::bfloat16_t>>, Is_dropout=false, Is_causal=true, Is_even_N=true, Is_even_K=false, Return_softmax=false, Params=Flash_fwd_params]" 
(588): here
            instantiation of "void flash::compute_attn<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &) [with Kernel_traits=Flash_fwd_kernel_traits<512, 64, 32, 4, false, false, cutlass::bfloat16_t, Flash_kernel_traits<512, 64, 32, 4, cutlass::bfloat16_t>>, Is_dropout=false, Is_causal=true, Is_even_N=true, Is_even_K=false, Return_softmax=false, Params=Flash_fwd_params]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(15): here
            instantiation of "void flash_fwd_kernel<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax>(Flash_fwd_params) [with Kernel_traits=Flash_fwd_kernel_traits<512, 64, 32, 4, false, false, cutlass::bfloat16_t, Flash_kernel_traits<512, 64, 32, 4, cutlass::bfloat16_t>>, Is_dropout=false, Is_causal=true, Is_even_N=true, Is_even_K=false, Return_softmax=false]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(34): here
            instantiation of "void run_flash_fwd<Kernel_traits,Is_dropout,Is_causal>(Flash_fwd_params &, cudaStream_t) [with Kernel_traits=Flash_fwd_kernel_traits<512, 64, 32, 4, false, false, cutlass::bfloat16_t, Flash_kernel_traits<512, 64, 32, 4, cutlass::bfloat16_t>>, Is_dropout=false, Is_causal=true]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(264): here
            instantiation of "void run_mha_fwd_hdim512<T>(Flash_fwd_params &, cudaStream_t) [with T=cutlass::bfloat16_t]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_hdim512_bf16_sm80.cu(8): here

/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_kernel.h(275): warning: variable "tQrQ" was declared but never referenced
          detected during:
            instantiation of "void flash::compute_attn_1rowblock<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &, int, int, int) [with Kernel_traits=Flash_fwd_kernel_traits<512, 64, 32, 4, false, false, cutlass::bfloat16_t, Flash_kernel_traits<512, 64, 32, 4, cutlass::bfloat16_t>>, Is_dropout=false, Is_causal=true, Is_even_N=true, Is_even_K=false, Return_softmax=false, Params=Flash_fwd_params]" 
(588): here
            instantiation of "void flash::compute_attn<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &) [with Kernel_traits=Flash_fwd_kernel_traits<512, 64, 32, 4, false, false, cutlass::bfloat16_t, Flash_kernel_traits<512, 64, 32, 4, cutlass::bfloat16_t>>, Is_dropout=false, Is_causal=true, Is_even_N=true, Is_even_K=false, Return_softmax=false, Params=Flash_fwd_params]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(15): here
            instantiation of "void flash_fwd_kernel<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax>(Flash_fwd_params) [with Kernel_traits=Flash_fwd_kernel_traits<512, 64, 32, 4, false, false, cutlass::bfloat16_t, Flash_kernel_traits<512, 64, 32, 4, cutlass::bfloat16_t>>, Is_dropout=false, Is_causal=true, Is_even_N=true, Is_even_K=false, Return_softmax=false]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(34): here
            instantiation of "void run_flash_fwd<Kernel_traits,Is_dropout,Is_causal>(Flash_fwd_params &, cudaStream_t) [with Kernel_traits=Flash_fwd_kernel_traits<512, 64, 32, 4, false, false, cutlass::bfloat16_t, Flash_kernel_traits<512, 64, 32, 4, cutlass::bfloat16_t>>, Is_dropout=false, Is_causal=true]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(264): here
            instantiation of "void run_mha_fwd_hdim512<T>(Flash_fwd_params &, cudaStream_t) [with T=cutlass::bfloat16_t]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_hdim512_bf16_sm80.cu(8): here

/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_kernel.h(140): warning: variable "MMA_M" was declared but never referenced
          detected during:
            instantiation of "void flash::compute_attn_1rowblock<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &, int, int, int) [with Kernel_traits=Flash_fwd_kernel_traits<512, 64, 32, 4, false, false, cutlass::bfloat16_t, Flash_kernel_traits<512, 64, 32, 4, cutlass::bfloat16_t>>, Is_dropout=false, Is_causal=true, Is_even_N=false, Is_even_K=true, Return_softmax=false, Params=Flash_fwd_params]" 
(588): here
            instantiation of "void flash::compute_attn<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &) [with Kernel_traits=Flash_fwd_kernel_traits<512, 64, 32, 4, false, false, cutlass::bfloat16_t, Flash_kernel_traits<512, 64, 32, 4, cutlass::bfloat16_t>>, Is_dropout=false, Is_causal=true, Is_even_N=false, Is_even_K=true, Return_softmax=false, Params=Flash_fwd_params]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(15): here
            instantiation of "void flash_fwd_kernel<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax>(Flash_fwd_params) [with Kernel_traits=Flash_fwd_kernel_traits<512, 64, 32, 4, false, false, cutlass::bfloat16_t, Flash_kernel_traits<512, 64, 32, 4, cutlass::bfloat16_t>>, Is_dropout=false, Is_causal=true, Is_even_N=false, Is_even_K=true, Return_softmax=false]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(34): here
            instantiation of "void run_flash_fwd<Kernel_traits,Is_dropout,Is_causal>(Flash_fwd_params &, cudaStream_t) [with Kernel_traits=Flash_fwd_kernel_traits<512, 64, 32, 4, false, false, cutlass::bfloat16_t, Flash_kernel_traits<512, 64, 32, 4, cutlass::bfloat16_t>>, Is_dropout=false, Is_causal=true]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(264): here
            instantiation of "void run_mha_fwd_hdim512<T>(Flash_fwd_params &, cudaStream_t) [with T=cutlass::bfloat16_t]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_hdim512_bf16_sm80.cu(8): here

/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_kernel.h(275): warning: variable "tQrQ" was declared but never referenced
          detected during:
            instantiation of "void flash::compute_attn_1rowblock<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &, int, int, int) [with Kernel_traits=Flash_fwd_kernel_traits<512, 64, 32, 4, false, false, cutlass::bfloat16_t, Flash_kernel_traits<512, 64, 32, 4, cutlass::bfloat16_t>>, Is_dropout=false, Is_causal=true, Is_even_N=false, Is_even_K=true, Return_softmax=false, Params=Flash_fwd_params]" 
(588): here
            instantiation of "void flash::compute_attn<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &) [with Kernel_traits=Flash_fwd_kernel_traits<512, 64, 32, 4, false, false, cutlass::bfloat16_t, Flash_kernel_traits<512, 64, 32, 4, cutlass::bfloat16_t>>, Is_dropout=false, Is_causal=true, Is_even_N=false, Is_even_K=true, Return_softmax=false, Params=Flash_fwd_params]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(15): here
            instantiation of "void flash_fwd_kernel<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax>(Flash_fwd_params) [with Kernel_traits=Flash_fwd_kernel_traits<512, 64, 32, 4, false, false, cutlass::bfloat16_t, Flash_kernel_traits<512, 64, 32, 4, cutlass::bfloat16_t>>, Is_dropout=false, Is_causal=true, Is_even_N=false, Is_even_K=true, Return_softmax=false]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(34): here
            instantiation of "void run_flash_fwd<Kernel_traits,Is_dropout,Is_causal>(Flash_fwd_params &, cudaStream_t) [with Kernel_traits=Flash_fwd_kernel_traits<512, 64, 32, 4, false, false, cutlass::bfloat16_t, Flash_kernel_traits<512, 64, 32, 4, cutlass::bfloat16_t>>, Is_dropout=false, Is_causal=true]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(264): here
            instantiation of "void run_mha_fwd_hdim512<T>(Flash_fwd_params &, cudaStream_t) [with T=cutlass::bfloat16_t]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_hdim512_bf16_sm80.cu(8): here

/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_kernel.h(140): warning: variable "MMA_M" was declared but never referenced
          detected during:
            instantiation of "void flash::compute_attn_1rowblock<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &, int, int, int) [with Kernel_traits=Flash_fwd_kernel_traits<512, 64, 32, 4, false, false, cutlass::bfloat16_t, Flash_kernel_traits<512, 64, 32, 4, cutlass::bfloat16_t>>, Is_dropout=false, Is_causal=true, Is_even_N=false, Is_even_K=false, Return_softmax=false, Params=Flash_fwd_params]" 
(588): here
            instantiation of "void flash::compute_attn<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &) [with Kernel_traits=Flash_fwd_kernel_traits<512, 64, 32, 4, false, false, cutlass::bfloat16_t, Flash_kernel_traits<512, 64, 32, 4, cutlass::bfloat16_t>>, Is_dropout=false, Is_causal=true, Is_even_N=false, Is_even_K=false, Return_softmax=false, Params=Flash_fwd_params]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(15): here
            instantiation of "void flash_fwd_kernel<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax>(Flash_fwd_params) [with Kernel_traits=Flash_fwd_kernel_traits<512, 64, 32, 4, false, false, cutlass::bfloat16_t, Flash_kernel_traits<512, 64, 32, 4, cutlass::bfloat16_t>>, Is_dropout=false, Is_causal=true, Is_even_N=false, Is_even_K=false, Return_softmax=false]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(34): here
            instantiation of "void run_flash_fwd<Kernel_traits,Is_dropout,Is_causal>(Flash_fwd_params &, cudaStream_t) [with Kernel_traits=Flash_fwd_kernel_traits<512, 64, 32, 4, false, false, cutlass::bfloat16_t, Flash_kernel_traits<512, 64, 32, 4, cutlass::bfloat16_t>>, Is_dropout=false, Is_causal=true]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(264): here
            instantiation of "void run_mha_fwd_hdim512<T>(Flash_fwd_params &, cudaStream_t) [with T=cutlass::bfloat16_t]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_hdim512_bf16_sm80.cu(8): here

/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_kernel.h(275): warning: variable "tQrQ" was declared but never referenced
          detected during:
            instantiation of "void flash::compute_attn_1rowblock<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &, int, int, int) [with Kernel_traits=Flash_fwd_kernel_traits<512, 64, 32, 4, false, false, cutlass::bfloat16_t, Flash_kernel_traits<512, 64, 32, 4, cutlass::bfloat16_t>>, Is_dropout=false, Is_causal=true, Is_even_N=false, Is_even_K=false, Return_softmax=false, Params=Flash_fwd_params]" 
(588): here
            instantiation of "void flash::compute_attn<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &) [with Kernel_traits=Flash_fwd_kernel_traits<512, 64, 32, 4, false, false, cutlass::bfloat16_t, Flash_kernel_traits<512, 64, 32, 4, cutlass::bfloat16_t>>, Is_dropout=false, Is_causal=true, Is_even_N=false, Is_even_K=false, Return_softmax=false, Params=Flash_fwd_params]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(15): here
            instantiation of "void flash_fwd_kernel<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax>(Flash_fwd_params) [with Kernel_traits=Flash_fwd_kernel_traits<512, 64, 32, 4, false, false, cutlass::bfloat16_t, Flash_kernel_traits<512, 64, 32, 4, cutlass::bfloat16_t>>, Is_dropout=false, Is_causal=true, Is_even_N=false, Is_even_K=false, Return_softmax=false]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(34): here
            instantiation of "void run_flash_fwd<Kernel_traits,Is_dropout,Is_causal>(Flash_fwd_params &, cudaStream_t) [with Kernel_traits=Flash_fwd_kernel_traits<512, 64, 32, 4, false, false, cutlass::bfloat16_t, Flash_kernel_traits<512, 64, 32, 4, cutlass::bfloat16_t>>, Is_dropout=false, Is_causal=true]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(264): here
            instantiation of "void run_mha_fwd_hdim512<T>(Flash_fwd_params &, cudaStream_t) [with T=cutlass::bfloat16_t]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_hdim512_bf16_sm80.cu(8): here

/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_kernel.h(140): warning: variable "MMA_M" was declared but never referenced
          detected during:
            instantiation of "void flash::compute_attn_1rowblock<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &, int, int, int) [with Kernel_traits=Flash_fwd_kernel_traits<512, 32, 32, 2, false, false, cutlass::bfloat16_t, Flash_kernel_traits<512, 32, 32, 2, cutlass::bfloat16_t>>, Is_dropout=false, Is_causal=true, Is_even_N=true, Is_even_K=true, Return_softmax=false, Params=Flash_fwd_params]" 
(588): here
            instantiation of "void flash::compute_attn<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &) [with Kernel_traits=Flash_fwd_kernel_traits<512, 32, 32, 2, false, false, cutlass::bfloat16_t, Flash_kernel_traits<512, 32, 32, 2, cutlass::bfloat16_t>>, Is_dropout=false, Is_causal=true, Is_even_N=true, Is_even_K=true, Return_softmax=false, Params=Flash_fwd_params]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(15): here
            instantiation of "void flash_fwd_kernel<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax>(Flash_fwd_params) [with Kernel_traits=Flash_fwd_kernel_traits<512, 32, 32, 2, false, false, cutlass::bfloat16_t, Flash_kernel_traits<512, 32, 32, 2, cutlass::bfloat16_t>>, Is_dropout=false, Is_causal=true, Is_even_N=true, Is_even_K=true, Return_softmax=false]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(34): here
            instantiation of "void run_flash_fwd<Kernel_traits,Is_dropout,Is_causal>(Flash_fwd_params &, cudaStream_t) [with Kernel_traits=Flash_fwd_kernel_traits<512, 32, 32, 2, false, false, cutlass::bfloat16_t, Flash_kernel_traits<512, 32, 32, 2, cutlass::bfloat16_t>>, Is_dropout=false, Is_causal=true]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(264): here
            instantiation of "void run_mha_fwd_hdim512<T>(Flash_fwd_params &, cudaStream_t) [with T=cutlass::bfloat16_t]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_hdim512_bf16_sm80.cu(8): here

/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_kernel.h(275): warning: variable "tQrQ" was declared but never referenced
          detected during:
            instantiation of "void flash::compute_attn_1rowblock<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &, int, int, int) [with Kernel_traits=Flash_fwd_kernel_traits<512, 32, 32, 2, false, false, cutlass::bfloat16_t, Flash_kernel_traits<512, 32, 32, 2, cutlass::bfloat16_t>>, Is_dropout=false, Is_causal=true, Is_even_N=true, Is_even_K=true, Return_softmax=false, Params=Flash_fwd_params]" 
(588): here
            instantiation of "void flash::compute_attn<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &) [with Kernel_traits=Flash_fwd_kernel_traits<512, 32, 32, 2, false, false, cutlass::bfloat16_t, Flash_kernel_traits<512, 32, 32, 2, cutlass::bfloat16_t>>, Is_dropout=false, Is_causal=true, Is_even_N=true, Is_even_K=true, Return_softmax=false, Params=Flash_fwd_params]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(15): here
            instantiation of "void flash_fwd_kernel<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax>(Flash_fwd_params) [with Kernel_traits=Flash_fwd_kernel_traits<512, 32, 32, 2, false, false, cutlass::bfloat16_t, Flash_kernel_traits<512, 32, 32, 2, cutlass::bfloat16_t>>, Is_dropout=false, Is_causal=true, Is_even_N=true, Is_even_K=true, Return_softmax=false]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(34): here
            instantiation of "void run_flash_fwd<Kernel_traits,Is_dropout,Is_causal>(Flash_fwd_params &, cudaStream_t) [with Kernel_traits=Flash_fwd_kernel_traits<512, 32, 32, 2, false, false, cutlass::bfloat16_t, Flash_kernel_traits<512, 32, 32, 2, cutlass::bfloat16_t>>, Is_dropout=false, Is_causal=true]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(264): here
            instantiation of "void run_mha_fwd_hdim512<T>(Flash_fwd_params &, cudaStream_t) [with T=cutlass::bfloat16_t]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_hdim512_bf16_sm80.cu(8): here

/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_kernel.h(140): warning: variable "MMA_M" was declared but never referenced
          detected during:
            instantiation of "void flash::compute_attn_1rowblock<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &, int, int, int) [with Kernel_traits=Flash_fwd_kernel_traits<512, 32, 32, 2, false, false, cutlass::bfloat16_t, Flash_kernel_traits<512, 32, 32, 2, cutlass::bfloat16_t>>, Is_dropout=false, Is_causal=true, Is_even_N=true, Is_even_K=false, Return_softmax=false, Params=Flash_fwd_params]" 
(588): here
            instantiation of "void flash::compute_attn<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &) [with Kernel_traits=Flash_fwd_kernel_traits<512, 32, 32, 2, false, false, cutlass::bfloat16_t, Flash_kernel_traits<512, 32, 32, 2, cutlass::bfloat16_t>>, Is_dropout=false, Is_causal=true, Is_even_N=true, Is_even_K=false, Return_softmax=false, Params=Flash_fwd_params]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(15): here
            instantiation of "void flash_fwd_kernel<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax>(Flash_fwd_params) [with Kernel_traits=Flash_fwd_kernel_traits<512, 32, 32, 2, false, false, cutlass::bfloat16_t, Flash_kernel_traits<512, 32, 32, 2, cutlass::bfloat16_t>>, Is_dropout=false, Is_causal=true, Is_even_N=true, Is_even_K=false, Return_softmax=false]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(34): here
            instantiation of "void run_flash_fwd<Kernel_traits,Is_dropout,Is_causal>(Flash_fwd_params &, cudaStream_t) [with Kernel_traits=Flash_fwd_kernel_traits<512, 32, 32, 2, false, false, cutlass::bfloat16_t, Flash_kernel_traits<512, 32, 32, 2, cutlass::bfloat16_t>>, Is_dropout=false, Is_causal=true]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(264): here
            instantiation of "void run_mha_fwd_hdim512<T>(Flash_fwd_params &, cudaStream_t) [with T=cutlass::bfloat16_t]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_hdim512_bf16_sm80.cu(8): here

/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_kernel.h(275): warning: variable "tQrQ" was declared but never referenced
          detected during:
            instantiation of "void flash::compute_attn_1rowblock<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &, int, int, int) [with Kernel_traits=Flash_fwd_kernel_traits<512, 32, 32, 2, false, false, cutlass::bfloat16_t, Flash_kernel_traits<512, 32, 32, 2, cutlass::bfloat16_t>>, Is_dropout=false, Is_causal=true, Is_even_N=true, Is_even_K=false, Return_softmax=false, Params=Flash_fwd_params]" 
(588): here
            instantiation of "void flash::compute_attn<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &) [with Kernel_traits=Flash_fwd_kernel_traits<512, 32, 32, 2, false, false, cutlass::bfloat16_t, Flash_kernel_traits<512, 32, 32, 2, cutlass::bfloat16_t>>, Is_dropout=false, Is_causal=true, Is_even_N=true, Is_even_K=false, Return_softmax=false, Params=Flash_fwd_params]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(15): here
            instantiation of "void flash_fwd_kernel<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax>(Flash_fwd_params) [with Kernel_traits=Flash_fwd_kernel_traits<512, 32, 32, 2, false, false, cutlass::bfloat16_t, Flash_kernel_traits<512, 32, 32, 2, cutlass::bfloat16_t>>, Is_dropout=false, Is_causal=true, Is_even_N=true, Is_even_K=false, Return_softmax=false]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(34): here
            instantiation of "void run_flash_fwd<Kernel_traits,Is_dropout,Is_causal>(Flash_fwd_params &, cudaStream_t) [with Kernel_traits=Flash_fwd_kernel_traits<512, 32, 32, 2, false, false, cutlass::bfloat16_t, Flash_kernel_traits<512, 32, 32, 2, cutlass::bfloat16_t>>, Is_dropout=false, Is_causal=true]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(264): here
            instantiation of "void run_mha_fwd_hdim512<T>(Flash_fwd_params &, cudaStream_t) [with T=cutlass::bfloat16_t]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_hdim512_bf16_sm80.cu(8): here

/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_kernel.h(140): warning: variable "MMA_M" was declared but never referenced
          detected during:
            instantiation of "void flash::compute_attn_1rowblock<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &, int, int, int) [with Kernel_traits=Flash_fwd_kernel_traits<512, 32, 32, 2, false, false, cutlass::bfloat16_t, Flash_kernel_traits<512, 32, 32, 2, cutlass::bfloat16_t>>, Is_dropout=false, Is_causal=true, Is_even_N=false, Is_even_K=true, Return_softmax=false, Params=Flash_fwd_params]" 
(588): here
            instantiation of "void flash::compute_attn<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &) [with Kernel_traits=Flash_fwd_kernel_traits<512, 32, 32, 2, false, false, cutlass::bfloat16_t, Flash_kernel_traits<512, 32, 32, 2, cutlass::bfloat16_t>>, Is_dropout=false, Is_causal=true, Is_even_N=false, Is_even_K=true, Return_softmax=false, Params=Flash_fwd_params]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(15): here
            instantiation of "void flash_fwd_kernel<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax>(Flash_fwd_params) [with Kernel_traits=Flash_fwd_kernel_traits<512, 32, 32, 2, false, false, cutlass::bfloat16_t, Flash_kernel_traits<512, 32, 32, 2, cutlass::bfloat16_t>>, Is_dropout=false, Is_causal=true, Is_even_N=false, Is_even_K=true, Return_softmax=false]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(34): here
            instantiation of "void run_flash_fwd<Kernel_traits,Is_dropout,Is_causal>(Flash_fwd_params &, cudaStream_t) [with Kernel_traits=Flash_fwd_kernel_traits<512, 32, 32, 2, false, false, cutlass::bfloat16_t, Flash_kernel_traits<512, 32, 32, 2, cutlass::bfloat16_t>>, Is_dropout=false, Is_causal=true]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(264): here
            instantiation of "void run_mha_fwd_hdim512<T>(Flash_fwd_params &, cudaStream_t) [with T=cutlass::bfloat16_t]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_hdim512_bf16_sm80.cu(8): here

/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_kernel.h(275): warning: variable "tQrQ" was declared but never referenced
          detected during:
            instantiation of "void flash::compute_attn_1rowblock<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &, int, int, int) [with Kernel_traits=Flash_fwd_kernel_traits<512, 32, 32, 2, false, false, cutlass::bfloat16_t, Flash_kernel_traits<512, 32, 32, 2, cutlass::bfloat16_t>>, Is_dropout=false, Is_causal=true, Is_even_N=false, Is_even_K=true, Return_softmax=false, Params=Flash_fwd_params]" 
(588): here
            instantiation of "void flash::compute_attn<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &) [with Kernel_traits=Flash_fwd_kernel_traits<512, 32, 32, 2, false, false, cutlass::bfloat16_t, Flash_kernel_traits<512, 32, 32, 2, cutlass::bfloat16_t>>, Is_dropout=false, Is_causal=true, Is_even_N=false, Is_even_K=true, Return_softmax=false, Params=Flash_fwd_params]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(15): here
            instantiation of "void flash_fwd_kernel<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax>(Flash_fwd_params) [with Kernel_traits=Flash_fwd_kernel_traits<512, 32, 32, 2, false, false, cutlass::bfloat16_t, Flash_kernel_traits<512, 32, 32, 2, cutlass::bfloat16_t>>, Is_dropout=false, Is_causal=true, Is_even_N=false, Is_even_K=true, Return_softmax=false]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(34): here
            instantiation of "void run_flash_fwd<Kernel_traits,Is_dropout,Is_causal>(Flash_fwd_params &, cudaStream_t) [with Kernel_traits=Flash_fwd_kernel_traits<512, 32, 32, 2, false, false, cutlass::bfloat16_t, Flash_kernel_traits<512, 32, 32, 2, cutlass::bfloat16_t>>, Is_dropout=false, Is_causal=true]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(264): here
            instantiation of "void run_mha_fwd_hdim512<T>(Flash_fwd_params &, cudaStream_t) [with T=cutlass::bfloat16_t]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_hdim512_bf16_sm80.cu(8): here

/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_kernel.h(140): warning: variable "MMA_M" was declared but never referenced
          detected during:
            instantiation of "void flash::compute_attn_1rowblock<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &, int, int, int) [with Kernel_traits=Flash_fwd_kernel_traits<512, 32, 32, 2, false, false, cutlass::bfloat16_t, Flash_kernel_traits<512, 32, 32, 2, cutlass::bfloat16_t>>, Is_dropout=false, Is_causal=true, Is_even_N=false, Is_even_K=false, Return_softmax=false, Params=Flash_fwd_params]" 
(588): here
            instantiation of "void flash::compute_attn<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &) [with Kernel_traits=Flash_fwd_kernel_traits<512, 32, 32, 2, false, false, cutlass::bfloat16_t, Flash_kernel_traits<512, 32, 32, 2, cutlass::bfloat16_t>>, Is_dropout=false, Is_causal=true, Is_even_N=false, Is_even_K=false, Return_softmax=false, Params=Flash_fwd_params]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(15): here
            instantiation of "void flash_fwd_kernel<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax>(Flash_fwd_params) [with Kernel_traits=Flash_fwd_kernel_traits<512, 32, 32, 2, false, false, cutlass::bfloat16_t, Flash_kernel_traits<512, 32, 32, 2, cutlass::bfloat16_t>>, Is_dropout=false, Is_causal=true, Is_even_N=false, Is_even_K=false, Return_softmax=false]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(34): here
            instantiation of "void run_flash_fwd<Kernel_traits,Is_dropout,Is_causal>(Flash_fwd_params &, cudaStream_t) [with Kernel_traits=Flash_fwd_kernel_traits<512, 32, 32, 2, false, false, cutlass::bfloat16_t, Flash_kernel_traits<512, 32, 32, 2, cutlass::bfloat16_t>>, Is_dropout=false, Is_causal=true]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(264): here
            instantiation of "void run_mha_fwd_hdim512<T>(Flash_fwd_params &, cudaStream_t) [with T=cutlass::bfloat16_t]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_hdim512_bf16_sm80.cu(8): here

/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_kernel.h(275): warning: variable "tQrQ" was declared but never referenced
          detected during:
            instantiation of "void flash::compute_attn_1rowblock<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &, int, int, int) [with Kernel_traits=Flash_fwd_kernel_traits<512, 32, 32, 2, false, false, cutlass::bfloat16_t, Flash_kernel_traits<512, 32, 32, 2, cutlass::bfloat16_t>>, Is_dropout=false, Is_causal=true, Is_even_N=false, Is_even_K=false, Return_softmax=false, Params=Flash_fwd_params]" 
(588): here
            instantiation of "void flash::compute_attn<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &) [with Kernel_traits=Flash_fwd_kernel_traits<512, 32, 32, 2, false, false, cutlass::bfloat16_t, Flash_kernel_traits<512, 32, 32, 2, cutlass::bfloat16_t>>, Is_dropout=false, Is_causal=true, Is_even_N=false, Is_even_K=false, Return_softmax=false, Params=Flash_fwd_params]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(15): here
            instantiation of "void flash_fwd_kernel<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax>(Flash_fwd_params) [with Kernel_traits=Flash_fwd_kernel_traits<512, 32, 32, 2, false, false, cutlass::bfloat16_t, Flash_kernel_traits<512, 32, 32, 2, cutlass::bfloat16_t>>, Is_dropout=false, Is_causal=true, Is_even_N=false, Is_even_K=false, Return_softmax=false]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(34): here
            instantiation of "void run_flash_fwd<Kernel_traits,Is_dropout,Is_causal>(Flash_fwd_params &, cudaStream_t) [with Kernel_traits=Flash_fwd_kernel_traits<512, 32, 32, 2, false, false, cutlass::bfloat16_t, Flash_kernel_traits<512, 32, 32, 2, cutlass::bfloat16_t>>, Is_dropout=false, Is_causal=true]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(264): here
            instantiation of "void run_mha_fwd_hdim512<T>(Flash_fwd_params &, cudaStream_t) [with T=cutlass::bfloat16_t]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_hdim512_bf16_sm80.cu(8): here

/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_kernel.h(140): warning: variable "MMA_M" was declared but never referenced
          detected during:
            instantiation of "void flash::compute_attn_1rowblock<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &, int, int, int) [with Kernel_traits=Flash_fwd_kernel_traits<512, 64, 32, 4, false, false, cutlass::bfloat16_t, Flash_kernel_traits<512, 64, 32, 4, cutlass::bfloat16_t>>, Is_dropout=false, Is_causal=false, Is_even_N=true, Is_even_K=true, Return_softmax=false, Params=Flash_fwd_params]" 
(588): here
            instantiation of "void flash::compute_attn<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &) [with Kernel_traits=Flash_fwd_kernel_traits<512, 64, 32, 4, false, false, cutlass::bfloat16_t, Flash_kernel_traits<512, 64, 32, 4, cutlass::bfloat16_t>>, Is_dropout=false, Is_causal=false, Is_even_N=true, Is_even_K=true, Return_softmax=false, Params=Flash_fwd_params]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(15): here
            instantiation of "void flash_fwd_kernel<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax>(Flash_fwd_params) [with Kernel_traits=Flash_fwd_kernel_traits<512, 64, 32, 4, false, false, cutlass::bfloat16_t, Flash_kernel_traits<512, 64, 32, 4, cutlass::bfloat16_t>>, Is_dropout=false, Is_causal=false, Is_even_N=true, Is_even_K=true, Return_softmax=false]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(34): here
            instantiation of "void run_flash_fwd<Kernel_traits,Is_dropout,Is_causal>(Flash_fwd_params &, cudaStream_t) [with Kernel_traits=Flash_fwd_kernel_traits<512, 64, 32, 4, false, false, cutlass::bfloat16_t, Flash_kernel_traits<512, 64, 32, 4, cutlass::bfloat16_t>>, Is_dropout=false, Is_causal=false]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(264): here
            instantiation of "void run_mha_fwd_hdim512<T>(Flash_fwd_params &, cudaStream_t) [with T=cutlass::bfloat16_t]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_hdim512_bf16_sm80.cu(8): here

/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_kernel.h(275): warning: variable "tQrQ" was declared but never referenced
          detected during:
            instantiation of "void flash::compute_attn_1rowblock<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &, int, int, int) [with Kernel_traits=Flash_fwd_kernel_traits<512, 64, 32, 4, false, false, cutlass::bfloat16_t, Flash_kernel_traits<512, 64, 32, 4, cutlass::bfloat16_t>>, Is_dropout=false, Is_causal=false, Is_even_N=true, Is_even_K=true, Return_softmax=false, Params=Flash_fwd_params]" 
(588): here
            instantiation of "void flash::compute_attn<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &) [with Kernel_traits=Flash_fwd_kernel_traits<512, 64, 32, 4, false, false, cutlass::bfloat16_t, Flash_kernel_traits<512, 64, 32, 4, cutlass::bfloat16_t>>, Is_dropout=false, Is_causal=false, Is_even_N=true, Is_even_K=true, Return_softmax=false, Params=Flash_fwd_params]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(15): here
            instantiation of "void flash_fwd_kernel<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax>(Flash_fwd_params) [with Kernel_traits=Flash_fwd_kernel_traits<512, 64, 32, 4, false, false, cutlass::bfloat16_t, Flash_kernel_traits<512, 64, 32, 4, cutlass::bfloat16_t>>, Is_dropout=false, Is_causal=false, Is_even_N=true, Is_even_K=true, Return_softmax=false]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(34): here
            instantiation of "void run_flash_fwd<Kernel_traits,Is_dropout,Is_causal>(Flash_fwd_params &, cudaStream_t) [with Kernel_traits=Flash_fwd_kernel_traits<512, 64, 32, 4, false, false, cutlass::bfloat16_t, Flash_kernel_traits<512, 64, 32, 4, cutlass::bfloat16_t>>, Is_dropout=false, Is_causal=false]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(264): here
            instantiation of "void run_mha_fwd_hdim512<T>(Flash_fwd_params &, cudaStream_t) [with T=cutlass::bfloat16_t]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_hdim512_bf16_sm80.cu(8): here

/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_kernel.h(140): warning: variable "MMA_M" was declared but never referenced
          detected during:
            instantiation of "void flash::compute_attn_1rowblock<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &, int, int, int) [with Kernel_traits=Flash_fwd_kernel_traits<512, 64, 32, 4, false, false, cutlass::bfloat16_t, Flash_kernel_traits<512, 64, 32, 4, cutlass::bfloat16_t>>, Is_dropout=false, Is_causal=false, Is_even_N=true, Is_even_K=false, Return_softmax=false, Params=Flash_fwd_params]" 
(588): here
            instantiation of "void flash::compute_attn<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &) [with Kernel_traits=Flash_fwd_kernel_traits<512, 64, 32, 4, false, false, cutlass::bfloat16_t, Flash_kernel_traits<512, 64, 32, 4, cutlass::bfloat16_t>>, Is_dropout=false, Is_causal=false, Is_even_N=true, Is_even_K=false, Return_softmax=false, Params=Flash_fwd_params]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(15): here
            instantiation of "void flash_fwd_kernel<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax>(Flash_fwd_params) [with Kernel_traits=Flash_fwd_kernel_traits<512, 64, 32, 4, false, false, cutlass::bfloat16_t, Flash_kernel_traits<512, 64, 32, 4, cutlass::bfloat16_t>>, Is_dropout=false, Is_causal=false, Is_even_N=true, Is_even_K=false, Return_softmax=false]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(34): here
            instantiation of "void run_flash_fwd<Kernel_traits,Is_dropout,Is_causal>(Flash_fwd_params &, cudaStream_t) [with Kernel_traits=Flash_fwd_kernel_traits<512, 64, 32, 4, false, false, cutlass::bfloat16_t, Flash_kernel_traits<512, 64, 32, 4, cutlass::bfloat16_t>>, Is_dropout=false, Is_causal=false]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(264): here
            instantiation of "void run_mha_fwd_hdim512<T>(Flash_fwd_params &, cudaStream_t) [with T=cutlass::bfloat16_t]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_hdim512_bf16_sm80.cu(8): here

/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_kernel.h(275): warning: variable "tQrQ" was declared but never referenced
          detected during:
            instantiation of "void flash::compute_attn_1rowblock<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &, int, int, int) [with Kernel_traits=Flash_fwd_kernel_traits<512, 64, 32, 4, false, false, cutlass::bfloat16_t, Flash_kernel_traits<512, 64, 32, 4, cutlass::bfloat16_t>>, Is_dropout=false, Is_causal=false, Is_even_N=true, Is_even_K=false, Return_softmax=false, Params=Flash_fwd_params]" 
(588): here
            instantiation of "void flash::compute_attn<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &) [with Kernel_traits=Flash_fwd_kernel_traits<512, 64, 32, 4, false, false, cutlass::bfloat16_t, Flash_kernel_traits<512, 64, 32, 4, cutlass::bfloat16_t>>, Is_dropout=false, Is_causal=false, Is_even_N=true, Is_even_K=false, Return_softmax=false, Params=Flash_fwd_params]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(15): here
            instantiation of "void flash_fwd_kernel<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax>(Flash_fwd_params) [with Kernel_traits=Flash_fwd_kernel_traits<512, 64, 32, 4, false, false, cutlass::bfloat16_t, Flash_kernel_traits<512, 64, 32, 4, cutlass::bfloat16_t>>, Is_dropout=false, Is_causal=false, Is_even_N=true, Is_even_K=false, Return_softmax=false]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(34): here
            instantiation of "void run_flash_fwd<Kernel_traits,Is_dropout,Is_causal>(Flash_fwd_params &, cudaStream_t) [with Kernel_traits=Flash_fwd_kernel_traits<512, 64, 32, 4, false, false, cutlass::bfloat16_t, Flash_kernel_traits<512, 64, 32, 4, cutlass::bfloat16_t>>, Is_dropout=false, Is_causal=false]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(264): here
            instantiation of "void run_mha_fwd_hdim512<T>(Flash_fwd_params &, cudaStream_t) [with T=cutlass::bfloat16_t]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_hdim512_bf16_sm80.cu(8): here

/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_kernel.h(140): warning: variable "MMA_M" was declared but never referenced
          detected during:
            instantiation of "void flash::compute_attn_1rowblock<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &, int, int, int) [with Kernel_traits=Flash_fwd_kernel_traits<512, 64, 32, 4, false, false, cutlass::bfloat16_t, Flash_kernel_traits<512, 64, 32, 4, cutlass::bfloat16_t>>, Is_dropout=false, Is_causal=false, Is_even_N=false, Is_even_K=true, Return_softmax=false, Params=Flash_fwd_params]" 
(588): here
            instantiation of "void flash::compute_attn<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &) [with Kernel_traits=Flash_fwd_kernel_traits<512, 64, 32, 4, false, false, cutlass::bfloat16_t, Flash_kernel_traits<512, 64, 32, 4, cutlass::bfloat16_t>>, Is_dropout=false, Is_causal=false, Is_even_N=false, Is_even_K=true, Return_softmax=false, Params=Flash_fwd_params]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(15): here
            instantiation of "void flash_fwd_kernel<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax>(Flash_fwd_params) [with Kernel_traits=Flash_fwd_kernel_traits<512, 64, 32, 4, false, false, cutlass::bfloat16_t, Flash_kernel_traits<512, 64, 32, 4, cutlass::bfloat16_t>>, Is_dropout=false, Is_causal=false, Is_even_N=false, Is_even_K=true, Return_softmax=false]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(34): here
            instantiation of "void run_flash_fwd<Kernel_traits,Is_dropout,Is_causal>(Flash_fwd_params &, cudaStream_t) [with Kernel_traits=Flash_fwd_kernel_traits<512, 64, 32, 4, false, false, cutlass::bfloat16_t, Flash_kernel_traits<512, 64, 32, 4, cutlass::bfloat16_t>>, Is_dropout=false, Is_causal=false]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(264): here
            instantiation of "void run_mha_fwd_hdim512<T>(Flash_fwd_params &, cudaStream_t) [with T=cutlass::bfloat16_t]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_hdim512_bf16_sm80.cu(8): here

/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_kernel.h(275): warning: variable "tQrQ" was declared but never referenced
          detected during:
            instantiation of "void flash::compute_attn_1rowblock<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &, int, int, int) [with Kernel_traits=Flash_fwd_kernel_traits<512, 64, 32, 4, false, false, cutlass::bfloat16_t, Flash_kernel_traits<512, 64, 32, 4, cutlass::bfloat16_t>>, Is_dropout=false, Is_causal=false, Is_even_N=false, Is_even_K=true, Return_softmax=false, Params=Flash_fwd_params]" 
(588): here
            instantiation of "void flash::compute_attn<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &) [with Kernel_traits=Flash_fwd_kernel_traits<512, 64, 32, 4, false, false, cutlass::bfloat16_t, Flash_kernel_traits<512, 64, 32, 4, cutlass::bfloat16_t>>, Is_dropout=false, Is_causal=false, Is_even_N=false, Is_even_K=true, Return_softmax=false, Params=Flash_fwd_params]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(15): here
            instantiation of "void flash_fwd_kernel<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax>(Flash_fwd_params) [with Kernel_traits=Flash_fwd_kernel_traits<512, 64, 32, 4, false, false, cutlass::bfloat16_t, Flash_kernel_traits<512, 64, 32, 4, cutlass::bfloat16_t>>, Is_dropout=false, Is_causal=false, Is_even_N=false, Is_even_K=true, Return_softmax=false]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(34): here
            instantiation of "void run_flash_fwd<Kernel_traits,Is_dropout,Is_causal>(Flash_fwd_params &, cudaStream_t) [with Kernel_traits=Flash_fwd_kernel_traits<512, 64, 32, 4, false, false, cutlass::bfloat16_t, Flash_kernel_traits<512, 64, 32, 4, cutlass::bfloat16_t>>, Is_dropout=false, Is_causal=false]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(264): here
            instantiation of "void run_mha_fwd_hdim512<T>(Flash_fwd_params &, cudaStream_t) [with T=cutlass::bfloat16_t]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_hdim512_bf16_sm80.cu(8): here

/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_kernel.h(140): warning: variable "MMA_M" was declared but never referenced
          detected during:
            instantiation of "void flash::compute_attn_1rowblock<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &, int, int, int) [with Kernel_traits=Flash_fwd_kernel_traits<512, 64, 32, 4, false, false, cutlass::bfloat16_t, Flash_kernel_traits<512, 64, 32, 4, cutlass::bfloat16_t>>, Is_dropout=false, Is_causal=false, Is_even_N=false, Is_even_K=false, Return_softmax=false, Params=Flash_fwd_params]" 
(588): here
            instantiation of "void flash::compute_attn<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &) [with Kernel_traits=Flash_fwd_kernel_traits<512, 64, 32, 4, false, false, cutlass::bfloat16_t, Flash_kernel_traits<512, 64, 32, 4, cutlass::bfloat16_t>>, Is_dropout=false, Is_causal=false, Is_even_N=false, Is_even_K=false, Return_softmax=false, Params=Flash_fwd_params]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(15): here
            instantiation of "void flash_fwd_kernel<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax>(Flash_fwd_params) [with Kernel_traits=Flash_fwd_kernel_traits<512, 64, 32, 4, false, false, cutlass::bfloat16_t, Flash_kernel_traits<512, 64, 32, 4, cutlass::bfloat16_t>>, Is_dropout=false, Is_causal=false, Is_even_N=false, Is_even_K=false, Return_softmax=false]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(34): here
            instantiation of "void run_flash_fwd<Kernel_traits,Is_dropout,Is_causal>(Flash_fwd_params &, cudaStream_t) [with Kernel_traits=Flash_fwd_kernel_traits<512, 64, 32, 4, false, false, cutlass::bfloat16_t, Flash_kernel_traits<512, 64, 32, 4, cutlass::bfloat16_t>>, Is_dropout=false, Is_causal=false]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(264): here
            instantiation of "void run_mha_fwd_hdim512<T>(Flash_fwd_params &, cudaStream_t) [with T=cutlass::bfloat16_t]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_hdim512_bf16_sm80.cu(8): here

/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_kernel.h(275): warning: variable "tQrQ" was declared but never referenced
          detected during:
            instantiation of "void flash::compute_attn_1rowblock<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &, int, int, int) [with Kernel_traits=Flash_fwd_kernel_traits<512, 64, 32, 4, false, false, cutlass::bfloat16_t, Flash_kernel_traits<512, 64, 32, 4, cutlass::bfloat16_t>>, Is_dropout=false, Is_causal=false, Is_even_N=false, Is_even_K=false, Return_softmax=false, Params=Flash_fwd_params]" 
(588): here
            instantiation of "void flash::compute_attn<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &) [with Kernel_traits=Flash_fwd_kernel_traits<512, 64, 32, 4, false, false, cutlass::bfloat16_t, Flash_kernel_traits<512, 64, 32, 4, cutlass::bfloat16_t>>, Is_dropout=false, Is_causal=false, Is_even_N=false, Is_even_K=false, Return_softmax=false, Params=Flash_fwd_params]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(15): here
            instantiation of "void flash_fwd_kernel<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax>(Flash_fwd_params) [with Kernel_traits=Flash_fwd_kernel_traits<512, 64, 32, 4, false, false, cutlass::bfloat16_t, Flash_kernel_traits<512, 64, 32, 4, cutlass::bfloat16_t>>, Is_dropout=false, Is_causal=false, Is_even_N=false, Is_even_K=false, Return_softmax=false]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(34): here
            instantiation of "void run_flash_fwd<Kernel_traits,Is_dropout,Is_causal>(Flash_fwd_params &, cudaStream_t) [with Kernel_traits=Flash_fwd_kernel_traits<512, 64, 32, 4, false, false, cutlass::bfloat16_t, Flash_kernel_traits<512, 64, 32, 4, cutlass::bfloat16_t>>, Is_dropout=false, Is_causal=false]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(264): here
            instantiation of "void run_mha_fwd_hdim512<T>(Flash_fwd_params &, cudaStream_t) [with T=cutlass::bfloat16_t]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_hdim512_bf16_sm80.cu(8): here

/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_kernel.h(140): warning: variable "MMA_M" was declared but never referenced
          detected during:
            instantiation of "void flash::compute_attn_1rowblock<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &, int, int, int) [with Kernel_traits=Flash_fwd_kernel_traits<512, 32, 32, 2, false, false, cutlass::bfloat16_t, Flash_kernel_traits<512, 32, 32, 2, cutlass::bfloat16_t>>, Is_dropout=false, Is_causal=false, Is_even_N=true, Is_even_K=true, Return_softmax=false, Params=Flash_fwd_params]" 
(588): here
            instantiation of "void flash::compute_attn<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &) [with Kernel_traits=Flash_fwd_kernel_traits<512, 32, 32, 2, false, false, cutlass::bfloat16_t, Flash_kernel_traits<512, 32, 32, 2, cutlass::bfloat16_t>>, Is_dropout=false, Is_causal=false, Is_even_N=true, Is_even_K=true, Return_softmax=false, Params=Flash_fwd_params]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(15): here
            instantiation of "void flash_fwd_kernel<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax>(Flash_fwd_params) [with Kernel_traits=Flash_fwd_kernel_traits<512, 32, 32, 2, false, false, cutlass::bfloat16_t, Flash_kernel_traits<512, 32, 32, 2, cutlass::bfloat16_t>>, Is_dropout=false, Is_causal=false, Is_even_N=true, Is_even_K=true, Return_softmax=false]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(34): here
            instantiation of "void run_flash_fwd<Kernel_traits,Is_dropout,Is_causal>(Flash_fwd_params &, cudaStream_t) [with Kernel_traits=Flash_fwd_kernel_traits<512, 32, 32, 2, false, false, cutlass::bfloat16_t, Flash_kernel_traits<512, 32, 32, 2, cutlass::bfloat16_t>>, Is_dropout=false, Is_causal=false]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(264): here
            instantiation of "void run_mha_fwd_hdim512<T>(Flash_fwd_params &, cudaStream_t) [with T=cutlass::bfloat16_t]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_hdim512_bf16_sm80.cu(8): here

/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_kernel.h(275): warning: variable "tQrQ" was declared but never referenced
          detected during:
            instantiation of "void flash::compute_attn_1rowblock<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &, int, int, int) [with Kernel_traits=Flash_fwd_kernel_traits<512, 32, 32, 2, false, false, cutlass::bfloat16_t, Flash_kernel_traits<512, 32, 32, 2, cutlass::bfloat16_t>>, Is_dropout=false, Is_causal=false, Is_even_N=true, Is_even_K=true, Return_softmax=false, Params=Flash_fwd_params]" 
(588): here
            instantiation of "void flash::compute_attn<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &) [with Kernel_traits=Flash_fwd_kernel_traits<512, 32, 32, 2, false, false, cutlass::bfloat16_t, Flash_kernel_traits<512, 32, 32, 2, cutlass::bfloat16_t>>, Is_dropout=false, Is_causal=false, Is_even_N=true, Is_even_K=true, Return_softmax=false, Params=Flash_fwd_params]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(15): here
            instantiation of "void flash_fwd_kernel<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax>(Flash_fwd_params) [with Kernel_traits=Flash_fwd_kernel_traits<512, 32, 32, 2, false, false, cutlass::bfloat16_t, Flash_kernel_traits<512, 32, 32, 2, cutlass::bfloat16_t>>, Is_dropout=false, Is_causal=false, Is_even_N=true, Is_even_K=true, Return_softmax=false]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(34): here
            instantiation of "void run_flash_fwd<Kernel_traits,Is_dropout,Is_causal>(Flash_fwd_params &, cudaStream_t) [with Kernel_traits=Flash_fwd_kernel_traits<512, 32, 32, 2, false, false, cutlass::bfloat16_t, Flash_kernel_traits<512, 32, 32, 2, cutlass::bfloat16_t>>, Is_dropout=false, Is_causal=false]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(264): here
            instantiation of "void run_mha_fwd_hdim512<T>(Flash_fwd_params &, cudaStream_t) [with T=cutlass::bfloat16_t]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_hdim512_bf16_sm80.cu(8): here

/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_kernel.h(140): warning: variable "MMA_M" was declared but never referenced
          detected during:
            instantiation of "void flash::compute_attn_1rowblock<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &, int, int, int) [with Kernel_traits=Flash_fwd_kernel_traits<512, 32, 32, 2, false, false, cutlass::bfloat16_t, Flash_kernel_traits<512, 32, 32, 2, cutlass::bfloat16_t>>, Is_dropout=false, Is_causal=false, Is_even_N=true, Is_even_K=false, Return_softmax=false, Params=Flash_fwd_params]" 
(588): here
            instantiation of "void flash::compute_attn<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &) [with Kernel_traits=Flash_fwd_kernel_traits<512, 32, 32, 2, false, false, cutlass::bfloat16_t, Flash_kernel_traits<512, 32, 32, 2, cutlass::bfloat16_t>>, Is_dropout=false, Is_causal=false, Is_even_N=true, Is_even_K=false, Return_softmax=false, Params=Flash_fwd_params]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(15): here
            instantiation of "void flash_fwd_kernel<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax>(Flash_fwd_params) [with Kernel_traits=Flash_fwd_kernel_traits<512, 32, 32, 2, false, false, cutlass::bfloat16_t, Flash_kernel_traits<512, 32, 32, 2, cutlass::bfloat16_t>>, Is_dropout=false, Is_causal=false, Is_even_N=true, Is_even_K=false, Return_softmax=false]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(34): here
            instantiation of "void run_flash_fwd<Kernel_traits,Is_dropout,Is_causal>(Flash_fwd_params &, cudaStream_t) [with Kernel_traits=Flash_fwd_kernel_traits<512, 32, 32, 2, false, false, cutlass::bfloat16_t, Flash_kernel_traits<512, 32, 32, 2, cutlass::bfloat16_t>>, Is_dropout=false, Is_causal=false]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(264): here
            instantiation of "void run_mha_fwd_hdim512<T>(Flash_fwd_params &, cudaStream_t) [with T=cutlass::bfloat16_t]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_hdim512_bf16_sm80.cu(8): here

/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_kernel.h(275): warning: variable "tQrQ" was declared but never referenced
          detected during:
            instantiation of "void flash::compute_attn_1rowblock<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &, int, int, int) [with Kernel_traits=Flash_fwd_kernel_traits<512, 32, 32, 2, false, false, cutlass::bfloat16_t, Flash_kernel_traits<512, 32, 32, 2, cutlass::bfloat16_t>>, Is_dropout=false, Is_causal=false, Is_even_N=true, Is_even_K=false, Return_softmax=false, Params=Flash_fwd_params]" 
(588): here
            instantiation of "void flash::compute_attn<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &) [with Kernel_traits=Flash_fwd_kernel_traits<512, 32, 32, 2, false, false, cutlass::bfloat16_t, Flash_kernel_traits<512, 32, 32, 2, cutlass::bfloat16_t>>, Is_dropout=false, Is_causal=false, Is_even_N=true, Is_even_K=false, Return_softmax=false, Params=Flash_fwd_params]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(15): here
            instantiation of "void flash_fwd_kernel<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax>(Flash_fwd_params) [with Kernel_traits=Flash_fwd_kernel_traits<512, 32, 32, 2, false, false, cutlass::bfloat16_t, Flash_kernel_traits<512, 32, 32, 2, cutlass::bfloat16_t>>, Is_dropout=false, Is_causal=false, Is_even_N=true, Is_even_K=false, Return_softmax=false]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(34): here
            instantiation of "void run_flash_fwd<Kernel_traits,Is_dropout,Is_causal>(Flash_fwd_params &, cudaStream_t) [with Kernel_traits=Flash_fwd_kernel_traits<512, 32, 32, 2, false, false, cutlass::bfloat16_t, Flash_kernel_traits<512, 32, 32, 2, cutlass::bfloat16_t>>, Is_dropout=false, Is_causal=false]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(264): here
            instantiation of "void run_mha_fwd_hdim512<T>(Flash_fwd_params &, cudaStream_t) [with T=cutlass::bfloat16_t]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_hdim512_bf16_sm80.cu(8): here

/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_kernel.h(140): warning: variable "MMA_M" was declared but never referenced
          detected during:
            instantiation of "void flash::compute_attn_1rowblock<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &, int, int, int) [with Kernel_traits=Flash_fwd_kernel_traits<512, 32, 32, 2, false, false, cutlass::bfloat16_t, Flash_kernel_traits<512, 32, 32, 2, cutlass::bfloat16_t>>, Is_dropout=false, Is_causal=false, Is_even_N=false, Is_even_K=true, Return_softmax=false, Params=Flash_fwd_params]" 
(588): here
            instantiation of "void flash::compute_attn<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &) [with Kernel_traits=Flash_fwd_kernel_traits<512, 32, 32, 2, false, false, cutlass::bfloat16_t, Flash_kernel_traits<512, 32, 32, 2, cutlass::bfloat16_t>>, Is_dropout=false, Is_causal=false, Is_even_N=false, Is_even_K=true, Return_softmax=false, Params=Flash_fwd_params]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(15): here
            instantiation of "void flash_fwd_kernel<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax>(Flash_fwd_params) [with Kernel_traits=Flash_fwd_kernel_traits<512, 32, 32, 2, false, false, cutlass::bfloat16_t, Flash_kernel_traits<512, 32, 32, 2, cutlass::bfloat16_t>>, Is_dropout=false, Is_causal=false, Is_even_N=false, Is_even_K=true, Return_softmax=false]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(34): here
            instantiation of "void run_flash_fwd<Kernel_traits,Is_dropout,Is_causal>(Flash_fwd_params &, cudaStream_t) [with Kernel_traits=Flash_fwd_kernel_traits<512, 32, 32, 2, false, false, cutlass::bfloat16_t, Flash_kernel_traits<512, 32, 32, 2, cutlass::bfloat16_t>>, Is_dropout=false, Is_causal=false]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(264): here
            instantiation of "void run_mha_fwd_hdim512<T>(Flash_fwd_params &, cudaStream_t) [with T=cutlass::bfloat16_t]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_hdim512_bf16_sm80.cu(8): here

/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_kernel.h(275): warning: variable "tQrQ" was declared but never referenced
          detected during:
            instantiation of "void flash::compute_attn_1rowblock<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &, int, int, int) [with Kernel_traits=Flash_fwd_kernel_traits<512, 32, 32, 2, false, false, cutlass::bfloat16_t, Flash_kernel_traits<512, 32, 32, 2, cutlass::bfloat16_t>>, Is_dropout=false, Is_causal=false, Is_even_N=false, Is_even_K=true, Return_softmax=false, Params=Flash_fwd_params]" 
(588): here
            instantiation of "void flash::compute_attn<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &) [with Kernel_traits=Flash_fwd_kernel_traits<512, 32, 32, 2, false, false, cutlass::bfloat16_t, Flash_kernel_traits<512, 32, 32, 2, cutlass::bfloat16_t>>, Is_dropout=false, Is_causal=false, Is_even_N=false, Is_even_K=true, Return_softmax=false, Params=Flash_fwd_params]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(15): here
            instantiation of "void flash_fwd_kernel<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax>(Flash_fwd_params) [with Kernel_traits=Flash_fwd_kernel_traits<512, 32, 32, 2, false, false, cutlass::bfloat16_t, Flash_kernel_traits<512, 32, 32, 2, cutlass::bfloat16_t>>, Is_dropout=false, Is_causal=false, Is_even_N=false, Is_even_K=true, Return_softmax=false]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(34): here
            instantiation of "void run_flash_fwd<Kernel_traits,Is_dropout,Is_causal>(Flash_fwd_params &, cudaStream_t) [with Kernel_traits=Flash_fwd_kernel_traits<512, 32, 32, 2, false, false, cutlass::bfloat16_t, Flash_kernel_traits<512, 32, 32, 2, cutlass::bfloat16_t>>, Is_dropout=false, Is_causal=false]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(264): here
            instantiation of "void run_mha_fwd_hdim512<T>(Flash_fwd_params &, cudaStream_t) [with T=cutlass::bfloat16_t]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_hdim512_bf16_sm80.cu(8): here

/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_kernel.h(140): warning: variable "MMA_M" was declared but never referenced
          detected during:
            instantiation of "void flash::compute_attn_1rowblock<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &, int, int, int) [with Kernel_traits=Flash_fwd_kernel_traits<512, 32, 32, 2, false, false, cutlass::bfloat16_t, Flash_kernel_traits<512, 32, 32, 2, cutlass::bfloat16_t>>, Is_dropout=false, Is_causal=false, Is_even_N=false, Is_even_K=false, Return_softmax=false, Params=Flash_fwd_params]" 
(588): here
            instantiation of "void flash::compute_attn<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &) [with Kernel_traits=Flash_fwd_kernel_traits<512, 32, 32, 2, false, false, cutlass::bfloat16_t, Flash_kernel_traits<512, 32, 32, 2, cutlass::bfloat16_t>>, Is_dropout=false, Is_causal=false, Is_even_N=false, Is_even_K=false, Return_softmax=false, Params=Flash_fwd_params]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(15): here
            instantiation of "void flash_fwd_kernel<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax>(Flash_fwd_params) [with Kernel_traits=Flash_fwd_kernel_traits<512, 32, 32, 2, false, false, cutlass::bfloat16_t, Flash_kernel_traits<512, 32, 32, 2, cutlass::bfloat16_t>>, Is_dropout=false, Is_causal=false, Is_even_N=false, Is_even_K=false, Return_softmax=false]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(34): here
            instantiation of "void run_flash_fwd<Kernel_traits,Is_dropout,Is_causal>(Flash_fwd_params &, cudaStream_t) [with Kernel_traits=Flash_fwd_kernel_traits<512, 32, 32, 2, false, false, cutlass::bfloat16_t, Flash_kernel_traits<512, 32, 32, 2, cutlass::bfloat16_t>>, Is_dropout=false, Is_causal=false]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(264): here
            instantiation of "void run_mha_fwd_hdim512<T>(Flash_fwd_params &, cudaStream_t) [with T=cutlass::bfloat16_t]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_hdim512_bf16_sm80.cu(8): here

/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_kernel.h(275): warning: variable "tQrQ" was declared but never referenced
          detected during:
            instantiation of "void flash::compute_attn_1rowblock<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &, int, int, int) [with Kernel_traits=Flash_fwd_kernel_traits<512, 32, 32, 2, false, false, cutlass::bfloat16_t, Flash_kernel_traits<512, 32, 32, 2, cutlass::bfloat16_t>>, Is_dropout=false, Is_causal=false, Is_even_N=false, Is_even_K=false, Return_softmax=false, Params=Flash_fwd_params]" 
(588): here
            instantiation of "void flash::compute_attn<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &) [with Kernel_traits=Flash_fwd_kernel_traits<512, 32, 32, 2, false, false, cutlass::bfloat16_t, Flash_kernel_traits<512, 32, 32, 2, cutlass::bfloat16_t>>, Is_dropout=false, Is_causal=false, Is_even_N=false, Is_even_K=false, Return_softmax=false, Params=Flash_fwd_params]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(15): here
            instantiation of "void flash_fwd_kernel<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax>(Flash_fwd_params) [with Kernel_traits=Flash_fwd_kernel_traits<512, 32, 32, 2, false, false, cutlass::bfloat16_t, Flash_kernel_traits<512, 32, 32, 2, cutlass::bfloat16_t>>, Is_dropout=false, Is_causal=false, Is_even_N=false, Is_even_K=false, Return_softmax=false]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(34): here
            instantiation of "void run_flash_fwd<Kernel_traits,Is_dropout,Is_causal>(Flash_fwd_params &, cudaStream_t) [with Kernel_traits=Flash_fwd_kernel_traits<512, 32, 32, 2, false, false, cutlass::bfloat16_t, Flash_kernel_traits<512, 32, 32, 2, cutlass::bfloat16_t>>, Is_dropout=false, Is_causal=false]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(264): here
            instantiation of "void run_mha_fwd_hdim512<T>(Flash_fwd_params &, cudaStream_t) [with T=cutlass::bfloat16_t]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_hdim512_bf16_sm80.cu(8): here

ptxas info    : 2 bytes gmem
ptxas info    : Compiling entry function '_Z16flash_fwd_kernelI23Flash_fwd_kernel_traitsILi512ELi32ELi32ELi2ELb0ELb0EN7cutlass10bfloat16_tE19Flash_kernel_traitsILi512ELi32ELi32ELi2ES2_EELb0ELb0ELb0ELb0ELb0EEv16Flash_fwd_params' for 'sm_80'
ptxas info    : Function properties for _Z16flash_fwd_kernelI23Flash_fwd_kernel_traitsILi512ELi32ELi32ELi2ELb0ELb0EN7cutlass10bfloat16_tE19Flash_kernel_traitsILi512ELi32ELi32ELi2ES2_EELb0ELb0ELb0ELb0ELb0EEv16Flash_fwd_params
    832 bytes stack frame, 1188 bytes spill stores, 1208 bytes spill loads
ptxas info    : Used 255 registers, 584 bytes cmem[0]
ptxas info    : Compiling entry function '_Z16flash_fwd_kernelI23Flash_fwd_kernel_traitsILi512ELi32ELi32ELi2ELb0ELb0EN7cutlass10bfloat16_tE19Flash_kernel_traitsILi512ELi32ELi32ELi2ES2_EELb0ELb0ELb0ELb1ELb0EEv16Flash_fwd_params' for 'sm_80'
ptxas info    : Function properties for _Z16flash_fwd_kernelI23Flash_fwd_kernel_traitsILi512ELi32ELi32ELi2ELb0ELb0EN7cutlass10bfloat16_tE19Flash_kernel_traitsILi512ELi32ELi32ELi2ES2_EELb0ELb0ELb0ELb1ELb0EEv16Flash_fwd_params
    456 bytes stack frame, 792 bytes spill stores, 784 bytes spill loads
ptxas info    : Used 255 registers, 584 bytes cmem[0]
ptxas info    : Compiling entry function '_Z16flash_fwd_kernelI23Flash_fwd_kernel_traitsILi512ELi32ELi32ELi2ELb0ELb0EN7cutlass10bfloat16_tE19Flash_kernel_traitsILi512ELi32ELi32ELi2ES2_EELb0ELb0ELb1ELb0ELb0EEv16Flash_fwd_params' for 'sm_80'
ptxas info    : Function properties for _Z16flash_fwd_kernelI23Flash_fwd_kernel_traitsILi512ELi32ELi32ELi2ELb0ELb0EN7cutlass10bfloat16_tE19Flash_kernel_traitsILi512ELi32ELi32ELi2ES2_EELb0ELb0ELb1ELb0ELb0EEv16Flash_fwd_params
    744 bytes stack frame, 1068 bytes spill stores, 1072 bytes spill loads
ptxas info    : Used 255 registers, 584 bytes cmem[0]
ptxas info    : Compiling entry function '_Z16flash_fwd_kernelI23Flash_fwd_kernel_traitsILi512ELi32ELi32ELi2ELb0ELb0EN7cutlass10bfloat16_tE19Flash_kernel_traitsILi512ELi32ELi32ELi2ES2_EELb0ELb0ELb1ELb1ELb0EEv16Flash_fwd_params' for 'sm_80'
ptxas info    : Function properties for _Z16flash_fwd_kernelI23Flash_fwd_kernel_traitsILi512ELi32ELi32ELi2ELb0ELb0EN7cutlass10bfloat16_tE19Flash_kernel_traitsILi512ELi32ELi32ELi2ES2_EELb0ELb0ELb1ELb1ELb0EEv16Flash_fwd_params
    456 bytes stack frame, 752 bytes spill stores, 772 bytes spill loads
ptxas info    : Used 255 registers, 584 bytes cmem[0]
ptxas info    : Compiling entry function '_Z16flash_fwd_kernelI23Flash_fwd_kernel_traitsILi512ELi64ELi32ELi4ELb0ELb0EN7cutlass10bfloat16_tE19Flash_kernel_traitsILi512ELi64ELi32ELi4ES2_EELb0ELb0ELb0ELb0ELb0EEv16Flash_fwd_params' for 'sm_80'
ptxas info    : Function properties for _Z16flash_fwd_kernelI23Flash_fwd_kernel_traitsILi512ELi64ELi32ELi4ELb0ELb0EN7cutlass10bfloat16_tE19Flash_kernel_traitsILi512ELi64ELi32ELi4ES2_EELb0ELb0ELb0ELb0ELb0EEv16Flash_fwd_params
    736 bytes stack frame, 1072 bytes spill stores, 1084 bytes spill loads
ptxas info    : Used 255 registers, 584 bytes cmem[0]
ptxas info    : Compiling entry function '_Z16flash_fwd_kernelI23Flash_fwd_kernel_traitsILi512ELi64ELi32ELi4ELb0ELb0EN7cutlass10bfloat16_tE19Flash_kernel_traitsILi512ELi64ELi32ELi4ES2_EELb0ELb0ELb0ELb1ELb0EEv16Flash_fwd_params' for 'sm_80'
ptxas info    : Function properties for _Z16flash_fwd_kernelI23Flash_fwd_kernel_traitsILi512ELi64ELi32ELi4ELb0ELb0EN7cutlass10bfloat16_tE19Flash_kernel_traitsILi512ELi64ELi32ELi4ES2_EELb0ELb0ELb0ELb1ELb0EEv16Flash_fwd_params
    464 bytes stack frame, 792 bytes spill stores, 784 bytes spill loads
ptxas info    : Used 255 registers, 584 bytes cmem[0]
ptxas info    : Compiling entry function '_Z16flash_fwd_kernelI23Flash_fwd_kernel_traitsILi512ELi64ELi32ELi4ELb0ELb0EN7cutlass10bfloat16_tE19Flash_kernel_traitsILi512ELi64ELi32ELi4ES2_EELb0ELb0ELb1ELb0ELb0EEv16Flash_fwd_params' for 'sm_80'
ptxas info    : Function properties for _Z16flash_fwd_kernelI23Flash_fwd_kernel_traitsILi512ELi64ELi32ELi4ELb0ELb0EN7cutlass10bfloat16_tE19Flash_kernel_traitsILi512ELi64ELi32ELi4ES2_EELb0ELb0ELb1ELb0ELb0EEv16Flash_fwd_params
    432 bytes stack frame, 744 bytes spill stores, 760 bytes spill loads
ptxas info    : Used 255 registers, 584 bytes cmem[0]
ptxas info    : Compiling entry function '_Z16flash_fwd_kernelI23Flash_fwd_kernel_traitsILi512ELi64ELi32ELi4ELb0ELb0EN7cutlass10bfloat16_tE19Flash_kernel_traitsILi512ELi64ELi32ELi4ES2_EELb0ELb0ELb1ELb1ELb0EEv16Flash_fwd_params' for 'sm_80'
ptxas info    : Function properties for _Z16flash_fwd_kernelI23Flash_fwd_kernel_traitsILi512ELi64ELi32ELi4ELb0ELb0EN7cutlass10bfloat16_tE19Flash_kernel_traitsILi512ELi64ELi32ELi4ES2_EELb0ELb0ELb1ELb1ELb0EEv16Flash_fwd_params
    496 bytes stack frame, 788 bytes spill stores, 792 bytes spill loads
ptxas info    : Used 255 registers, 584 bytes cmem[0]
ptxas info    : Compiling entry function '_Z16flash_fwd_kernelI23Flash_fwd_kernel_traitsILi512ELi32ELi32ELi2ELb0ELb0EN7cutlass10bfloat16_tE19Flash_kernel_traitsILi512ELi32ELi32ELi2ES2_EELb0ELb1ELb0ELb0ELb0EEv16Flash_fwd_params' for 'sm_80'
ptxas info    : Function properties for _Z16flash_fwd_kernelI23Flash_fwd_kernel_traitsILi512ELi32ELi32ELi2ELb0ELb0EN7cutlass10bfloat16_tE19Flash_kernel_traitsILi512ELi32ELi32ELi2ES2_EELb0ELb1ELb0ELb0ELb0EEv16Flash_fwd_params
    832 bytes stack frame, 1192 bytes spill stores, 1212 bytes spill loads
ptxas info    : Used 255 registers, 584 bytes cmem[0]
ptxas info    : Compiling entry function '_Z16flash_fwd_kernelI23Flash_fwd_kernel_traitsILi512ELi32ELi32ELi2ELb0ELb0EN7cutlass10bfloat16_tE19Flash_kernel_traitsILi512ELi32ELi32ELi2ES2_EELb0ELb1ELb0ELb1ELb0EEv16Flash_fwd_params' for 'sm_80'
ptxas info    : Function properties for _Z16flash_fwd_kernelI23Flash_fwd_kernel_traitsILi512ELi32ELi32ELi2ELb0ELb0EN7cutlass10bfloat16_tE19Flash_kernel_traitsILi512ELi32ELi32ELi2ES2_EELb0ELb1ELb0ELb1ELb0EEv16Flash_fwd_params
    464 bytes stack frame, 796 bytes spill stores, 788 bytes spill loads
ptxas info    : Used 255 registers, 584 bytes cmem[0]
ptxas info    : Compiling entry function '_Z16flash_fwd_kernelI23Flash_fwd_kernel_traitsILi512ELi32ELi32ELi2ELb0ELb0EN7cutlass10bfloat16_tE19Flash_kernel_traitsILi512ELi32ELi32ELi2ES2_EELb0ELb1ELb1ELb0ELb0EEv16Flash_fwd_params' for 'sm_80'
ptxas info    : Function properties for _Z16flash_fwd_kernelI23Flash_fwd_kernel_traitsILi512ELi32ELi32ELi2ELb0ELb0EN7cutlass10bfloat16_tE19Flash_kernel_traitsILi512ELi32ELi32ELi2ES2_EELb0ELb1ELb1ELb0ELb0EEv16Flash_fwd_params
    760 bytes stack frame, 1120 bytes spill stores, 1124 bytes spill loads
ptxas info    : Used 255 registers, 584 bytes cmem[0]
ptxas info    : Compiling entry function '_Z16flash_fwd_kernelI23Flash_fwd_kernel_traitsILi512ELi32ELi32ELi2ELb0ELb0EN7cutlass10bfloat16_tE19Flash_kernel_traitsILi512ELi32ELi32ELi2ES2_EELb0ELb1ELb1ELb1ELb0EEv16Flash_fwd_params' for 'sm_80'
ptxas info    : Function properties for _Z16flash_fwd_kernelI23Flash_fwd_kernel_traitsILi512ELi32ELi32ELi2ELb0ELb0EN7cutlass10bfloat16_tE19Flash_kernel_traitsILi512ELi32ELi32ELi2ES2_EELb0ELb1ELb1ELb1ELb0EEv16Flash_fwd_params
    464 bytes stack frame, 756 bytes spill stores, 776 bytes spill loads
ptxas info    : Used 255 registers, 584 bytes cmem[0]
ptxas info    : Compiling entry function '_Z16flash_fwd_kernelI23Flash_fwd_kernel_traitsILi512ELi64ELi32ELi4ELb0ELb0EN7cutlass10bfloat16_tE19Flash_kernel_traitsILi512ELi64ELi32ELi4ES2_EELb0ELb1ELb0ELb0ELb0EEv16Flash_fwd_params' for 'sm_80'
ptxas info    : Function properties for _Z16flash_fwd_kernelI23Flash_fwd_kernel_traitsILi512ELi64ELi32ELi4ELb0ELb0EN7cutlass10bfloat16_tE19Flash_kernel_traitsILi512ELi64ELi32ELi4ES2_EELb0ELb1ELb0ELb0ELb0EEv16Flash_fwd_params
    880 bytes stack frame, 1876 bytes spill stores, 1972 bytes spill loads
ptxas info    : Used 255 registers, 584 bytes cmem[0]
ptxas info    : Compiling entry function '_Z16flash_fwd_kernelI23Flash_fwd_kernel_traitsILi512ELi64ELi32ELi4ELb0ELb0EN7cutlass10bfloat16_tE19Flash_kernel_traitsILi512ELi64ELi32ELi4ES2_EELb0ELb1ELb0ELb1ELb0EEv16Flash_fwd_params' for 'sm_80'
ptxas info    : Function properties for _Z16flash_fwd_kernelI23Flash_fwd_kernel_traitsILi512ELi64ELi32ELi4ELb0ELb0EN7cutlass10bfloat16_tE19Flash_kernel_traitsILi512ELi64ELi32ELi4ES2_EELb0ELb1ELb0ELb1ELb0EEv16Flash_fwd_params
    624 bytes stack frame, 1512 bytes spill stores, 1560 bytes spill loads
ptxas info    : Used 255 registers, 584 bytes cmem[0]
ptxas info    : Compiling entry function '_Z16flash_fwd_kernelI23Flash_fwd_kernel_traitsILi512ELi64ELi32ELi4ELb0ELb0EN7cutlass10bfloat16_tE19Flash_kernel_traitsILi512ELi64ELi32ELi4ES2_EELb0ELb1ELb1ELb0ELb0EEv16Flash_fwd_params' for 'sm_80'
ptxas info    : Function properties for _Z16flash_fwd_kernelI23Flash_fwd_kernel_traitsILi512ELi64ELi32ELi4ELb0ELb0EN7cutlass10bfloat16_tE19Flash_kernel_traitsILi512ELi64ELi32ELi4ES2_EELb0ELb1ELb1ELb0ELb0EEv16Flash_fwd_params
    1000 bytes stack frame, 1868 bytes spill stores, 1932 bytes spill loads
ptxas info    : Used 255 registers, 584 bytes cmem[0]
ptxas info    : Compiling entry function '_Z16flash_fwd_kernelI23Flash_fwd_kernel_traitsILi512ELi64ELi32ELi4ELb0ELb0EN7cutlass10bfloat16_tE19Flash_kernel_traitsILi512ELi64ELi32ELi4ES2_EELb0ELb1ELb1ELb1ELb0EEv16Flash_fwd_params' for 'sm_80'
ptxas info    : Function properties for _Z16flash_fwd_kernelI23Flash_fwd_kernel_traitsILi512ELi64ELi32ELi4ELb0ELb0EN7cutlass10bfloat16_tE19Flash_kernel_traitsILi512ELi64ELi32ELi4ES2_EELb0ELb1ELb1ELb1ELb0EEv16Flash_fwd_params
    664 bytes stack frame, 1556 bytes spill stores, 1616 bytes spill loads
ptxas info    : Used 255 registers, 584 bytes cmem[0]
ptxas info    : Compiling entry function '_Z16flash_fwd_kernelI23Flash_fwd_kernel_traitsILi512ELi32ELi32ELi2ELb0ELb0EN7cutlass10bfloat16_tE19Flash_kernel_traitsILi512ELi32ELi32ELi2ES2_EELb1ELb0ELb0ELb0ELb0EEv16Flash_fwd_params' for 'sm_80'
ptxas info    : Function properties for _Z16flash_fwd_kernelI23Flash_fwd_kernel_traitsILi512ELi32ELi32ELi2ELb0ELb0EN7cutlass10bfloat16_tE19Flash_kernel_traitsILi512ELi32ELi32ELi2ES2_EELb1ELb0ELb0ELb0ELb0EEv16Flash_fwd_params
    736 bytes stack frame, 1088 bytes spill stores, 1116 bytes spill loads
ptxas info    : Used 255 registers, 584 bytes cmem[0]
ptxas info    : Compiling entry function '_Z16flash_fwd_kernelI23Flash_fwd_kernel_traitsILi512ELi32ELi32ELi2ELb0ELb0EN7cutlass10bfloat16_tE19Flash_kernel_traitsILi512ELi32ELi32ELi2ES2_EELb1ELb0ELb0ELb0ELb1EEv16Flash_fwd_params' for 'sm_80'
ptxas info    : Function properties for _Z16flash_fwd_kernelI23Flash_fwd_kernel_traitsILi512ELi32ELi32ELi2ELb0ELb0EN7cutlass10bfloat16_tE19Flash_kernel_traitsILi512ELi32ELi32ELi2ES2_EELb1ELb0ELb0ELb0ELb1EEv16Flash_fwd_params
    728 bytes stack frame, 1086 bytes spill stores, 1100 bytes spill loads
ptxas info    : Used 255 registers, 584 bytes cmem[0]
ptxas info    : Compiling entry function '_Z16flash_fwd_kernelI23Flash_fwd_kernel_traitsILi512ELi32ELi32ELi2ELb0ELb0EN7cutlass10bfloat16_tE19Flash_kernel_traitsILi512ELi32ELi32ELi2ES2_EELb1ELb0ELb0ELb1ELb0EEv16Flash_fwd_params' for 'sm_80'
ptxas info    : Function properties for _Z16flash_fwd_kernelI23Flash_fwd_kernel_traitsILi512ELi32ELi32ELi2ELb0ELb0EN7cutlass10bfloat16_tE19Flash_kernel_traitsILi512ELi32ELi32ELi2ES2_EELb1ELb0ELb0ELb1ELb0EEv16Flash_fwd_params
    496 bytes stack frame, 792 bytes spill stores, 788 bytes spill loads
ptxas info    : Used 255 registers, 584 bytes cmem[0]
ptxas info    : Compiling entry function '_Z16flash_fwd_kernelI23Flash_fwd_kernel_traitsILi512ELi32ELi32ELi2ELb0ELb0EN7cutlass10bfloat16_tE19Flash_kernel_traitsILi512ELi32ELi32ELi2ES2_EELb1ELb0ELb0ELb1ELb1EEv16Flash_fwd_params' for 'sm_80'
ptxas info    : Function properties for _Z16flash_fwd_kernelI23Flash_fwd_kernel_traitsILi512ELi32ELi32ELi2ELb0ELb0EN7cutlass10bfloat16_tE19Flash_kernel_traitsILi512ELi32ELi32ELi2ES2_EELb1ELb0ELb0ELb1ELb1EEv16Flash_fwd_params
    680 bytes stack frame, 1020 bytes spill stores, 994 bytes spill loads
ptxas info    : Used 255 registers, 584 bytes cmem[0]
ptxas info    : Compiling entry function '_Z16flash_fwd_kernelI23Flash_fwd_kernel_traitsILi512ELi32ELi32ELi2ELb0ELb0EN7cutlass10bfloat16_tE19Flash_kernel_traitsILi512ELi32ELi32ELi2ES2_EELb1ELb0ELb1ELb0ELb0EEv16Flash_fwd_params' for 'sm_80'
ptxas info    : Function properties for _Z16flash_fwd_kernelI23Flash_fwd_kernel_traitsILi512ELi32ELi32ELi2ELb0ELb0EN7cutlass10bfloat16_tE19Flash_kernel_traitsILi512ELi32ELi32ELi2ES2_EELb1ELb0ELb1ELb0ELb0EEv16Flash_fwd_params
    704 bytes stack frame, 1004 bytes spill stores, 1008 bytes spill loads
ptxas info    : Used 255 registers, 584 bytes cmem[0]
ptxas info    : Compiling entry function '_Z16flash_fwd_kernelI23Flash_fwd_kernel_traitsILi512ELi32ELi32ELi2ELb0ELb0EN7cutlass10bfloat16_tE19Flash_kernel_traitsILi512ELi32ELi32ELi2ES2_EELb1ELb0ELb1ELb0ELb1EEv16Flash_fwd_params' for 'sm_80'
ptxas info    : Function properties for _Z16flash_fwd_kernelI23Flash_fwd_kernel_traitsILi512ELi32ELi32ELi2ELb0ELb0EN7cutlass10bfloat16_tE19Flash_kernel_traitsILi512ELi32ELi32ELi2ES2_EELb1ELb0ELb1ELb0ELb1EEv16Flash_fwd_params
    728 bytes stack frame, 1042 bytes spill stores, 1032 bytes spill loads
ptxas info    : Used 255 registers, 584 bytes cmem[0]
ptxas info    : Compiling entry function '_Z16flash_fwd_kernelI23Flash_fwd_kernel_traitsILi512ELi32ELi32ELi2ELb0ELb0EN7cutlass10bfloat16_tE19Flash_kernel_traitsILi512ELi32ELi32ELi2ES2_EELb1ELb0ELb1ELb1ELb0EEv16Flash_fwd_params' for 'sm_80'
ptxas info    : Function properties for _Z16flash_fwd_kernelI23Flash_fwd_kernel_traitsILi512ELi32ELi32ELi2ELb0ELb0EN7cutlass10bfloat16_tE19Flash_kernel_traitsILi512ELi32ELi32ELi2ES2_EELb1ELb0ELb1ELb1ELb0EEv16Flash_fwd_params
    448 bytes stack frame, 720 bytes spill stores, 736 bytes spill loads
ptxas info    : Used 255 registers, 584 bytes cmem[0]
ptxas info    : Compiling entry function '_Z16flash_fwd_kernelI23Flash_fwd_kernel_traitsILi512ELi32ELi32ELi2ELb0ELb0EN7cutlass10bfloat16_tE19Flash_kernel_traitsILi512ELi32ELi32ELi2ES2_EELb1ELb0ELb1ELb1ELb1EEv16Flash_fwd_params' for 'sm_80'
ptxas info    : Function properties for _Z16flash_fwd_kernelI23Flash_fwd_kernel_traitsILi512ELi32ELi32ELi2ELb0ELb0EN7cutlass10bfloat16_tE19Flash_kernel_traitsILi512ELi32ELi32ELi2ES2_EELb1ELb0ELb1ELb1ELb1EEv16Flash_fwd_params
    744 bytes stack frame, 1070 bytes spill stores, 1074 bytes spill loads
ptxas info    : Used 255 registers, 584 bytes cmem[0]
ptxas info    : Compiling entry function '_Z16flash_fwd_kernelI23Flash_fwd_kernel_traitsILi512ELi64ELi32ELi4ELb0ELb0EN7cutlass10bfloat16_tE19Flash_kernel_traitsILi512ELi64ELi32ELi4ES2_EELb1ELb0ELb0ELb0ELb0EEv16Flash_fwd_params' for 'sm_80'
ptxas info    : Function properties for _Z16flash_fwd_kernelI23Flash_fwd_kernel_traitsILi512ELi64ELi32ELi4ELb0ELb0EN7cutlass10bfloat16_tE19Flash_kernel_traitsILi512ELi64ELi32ELi4ES2_EELb1ELb0ELb0ELb0ELb0EEv16Flash_fwd_params
    704 bytes stack frame, 1008 bytes spill stores, 1028 bytes spill loads
ptxas info    : Used 255 registers, 584 bytes cmem[0]
ptxas info    : Compiling entry function '_Z16flash_fwd_kernelI23Flash_fwd_kernel_traitsILi512ELi64ELi32ELi4ELb0ELb0EN7cutlass10bfloat16_tE19Flash_kernel_traitsILi512ELi64ELi32ELi4ES2_EELb1ELb0ELb0ELb0ELb1EEv16Flash_fwd_params' for 'sm_80'
ptxas info    : Function properties for _Z16flash_fwd_kernelI23Flash_fwd_kernel_traitsILi512ELi64ELi32ELi4ELb0ELb0EN7cutlass10bfloat16_tE19Flash_kernel_traitsILi512ELi64ELi32ELi4ES2_EELb1ELb0ELb0ELb0ELb1EEv16Flash_fwd_params
    752 bytes stack frame, 1102 bytes spill stores, 1118 bytes spill loads
ptxas info    : Used 255 registers, 584 bytes cmem[0]
ptxas info    : Compiling entry function '_Z16flash_fwd_kernelI23Flash_fwd_kernel_traitsILi512ELi64ELi32ELi4ELb0ELb0EN7cutlass10bfloat16_tE19Flash_kernel_traitsILi512ELi64ELi32ELi4ES2_EELb1ELb0ELb0ELb1ELb0EEv16Flash_fwd_params' for 'sm_80'
ptxas info    : Function properties for _Z16flash_fwd_kernelI23Flash_fwd_kernel_traitsILi512ELi64ELi32ELi4ELb0ELb0EN7cutlass10bfloat16_tE19Flash_kernel_traitsILi512ELi64ELi32ELi4ES2_EELb1ELb0ELb0ELb1ELb0EEv16Flash_fwd_params
    440 bytes stack frame, 748 bytes spill stores, 744 bytes spill loads
ptxas info    : Used 255 registers, 584 bytes cmem[0]
ptxas info    : Compiling entry function '_Z16flash_fwd_kernelI23Flash_fwd_kernel_traitsILi512ELi64ELi32ELi4ELb0ELb0EN7cutlass10bfloat16_tE19Flash_kernel_traitsILi512ELi64ELi32ELi4ES2_EELb1ELb0ELb0ELb1ELb1EEv16Flash_fwd_params' for 'sm_80'
ptxas info    : Function properties for _Z16flash_fwd_kernelI23Flash_fwd_kernel_traitsILi512ELi64ELi32ELi4ELb0ELb0EN7cutlass10bfloat16_tE19Flash_kernel_traitsILi512ELi64ELi32ELi4ES2_EELb1ELb0ELb0ELb1ELb1EEv16Flash_fwd_params
    688 bytes stack frame, 1016 bytes spill stores, 990 bytes spill loads
ptxas info    : Used 255 registers, 584 bytes cmem[0]
ptxas info    : Compiling entry function '_Z16flash_fwd_kernelI23Flash_fwd_kernel_traitsILi512ELi64ELi32ELi4ELb0ELb0EN7cutlass10bfloat16_tE19Flash_kernel_traitsILi512ELi64ELi32ELi4ES2_EELb1ELb0ELb1ELb0ELb0EEv16Flash_fwd_params' for 'sm_80'
ptxas info    : Function properties for _Z16flash_fwd_kernelI23Flash_fwd_kernel_traitsILi512ELi64ELi32ELi4ELb0ELb0EN7cutlass10bfloat16_tE19Flash_kernel_traitsILi512ELi64ELi32ELi4ES2_EELb1ELb0ELb1ELb0ELb0EEv16Flash_fwd_params
    624 bytes stack frame, 928 bytes spill stores, 932 bytes spill loads
ptxas info    : Used 255 registers, 584 bytes cmem[0]
ptxas info    : Compiling entry function '_Z16flash_fwd_kernelI23Flash_fwd_kernel_traitsILi512ELi64ELi32ELi4ELb0ELb0EN7cutlass10bfloat16_tE19Flash_kernel_traitsILi512ELi64ELi32ELi4ES2_EELb1ELb0ELb1ELb0ELb1EEv16Flash_fwd_params' for 'sm_80'
ptxas info    : Function properties for _Z16flash_fwd_kernelI23Flash_fwd_kernel_traitsILi512ELi64ELi32ELi4ELb0ELb0EN7cutlass10bfloat16_tE19Flash_kernel_traitsILi512ELi64ELi32ELi4ES2_EELb1ELb0ELb1ELb0ELb1EEv16Flash_fwd_params
    744 bytes stack frame, 1066 bytes spill stores, 1054 bytes spill loads
ptxas info    : Used 255 registers, 584 bytes cmem[0]
ptxas info    : Compiling entry function '_Z16flash_fwd_kernelI23Flash_fwd_kernel_traitsILi512ELi64ELi32ELi4ELb0ELb0EN7cutlass10bfloat16_tE19Flash_kernel_traitsILi512ELi64ELi32ELi4ES2_EELb1ELb0ELb1ELb1ELb0EEv16Flash_fwd_params' for 'sm_80'
ptxas info    : Function properties for _Z16flash_fwd_kernelI23Flash_fwd_kernel_traitsILi512ELi64ELi32ELi4ELb0ELb0EN7cutlass10bfloat16_tE19Flash_kernel_traitsILi512ELi64ELi32ELi4ES2_EELb1ELb0ELb1ELb1ELb0EEv16Flash_fwd_params
    448 bytes stack frame, 724 bytes spill stores, 732 bytes spill loads
ptxas info    : Used 255 registers, 584 bytes cmem[0]
ptxas info    : Compiling entry function '_Z16flash_fwd_kernelI23Flash_fwd_kernel_traitsILi512ELi64ELi32ELi4ELb0ELb0EN7cutlass10bfloat16_tE19Flash_kernel_traitsILi512ELi64ELi32ELi4ES2_EELb1ELb0ELb1ELb1ELb1EEv16Flash_fwd_params' for 'sm_80'
ptxas info    : Function properties for _Z16flash_fwd_kernelI23Flash_fwd_kernel_traitsILi512ELi64ELi32ELi4ELb0ELb0EN7cutlass10bfloat16_tE19Flash_kernel_traitsILi512ELi64ELi32ELi4ES2_EELb1ELb0ELb1ELb1ELb1EEv16Flash_fwd_params
    752 bytes stack frame, 1128 bytes spill stores, 1114 bytes spill loads
ptxas info    : Used 255 registers, 584 bytes cmem[0]
ptxas info    : Compiling entry function '_Z16flash_fwd_kernelI23Flash_fwd_kernel_traitsILi512ELi32ELi32ELi2ELb0ELb0EN7cutlass10bfloat16_tE19Flash_kernel_traitsILi512ELi32ELi32ELi2ES2_EELb1ELb1ELb0ELb0ELb0EEv16Flash_fwd_params' for 'sm_80'
ptxas info    : Function properties for _Z16flash_fwd_kernelI23Flash_fwd_kernel_traitsILi512ELi32ELi32ELi2ELb0ELb0EN7cutlass10bfloat16_tE19Flash_kernel_traitsILi512ELi32ELi32ELi2ES2_EELb1ELb1ELb0ELb0ELb0EEv16Flash_fwd_params
    736 bytes stack frame, 1092 bytes spill stores, 1120 bytes spill loads
ptxas info    : Used 255 registers, 584 bytes cmem[0]
ptxas info    : Compiling entry function '_Z16flash_fwd_kernelI23Flash_fwd_kernel_traitsILi512ELi32ELi32ELi2ELb0ELb0EN7cutlass10bfloat16_tE19Flash_kernel_traitsILi512ELi32ELi32ELi2ES2_EELb1ELb1ELb0ELb0ELb1EEv16Flash_fwd_params' for 'sm_80'
ptxas info    : Function properties for _Z16flash_fwd_kernelI23Flash_fwd_kernel_traitsILi512ELi32ELi32ELi2ELb0ELb0EN7cutlass10bfloat16_tE19Flash_kernel_traitsILi512ELi32ELi32ELi2ES2_EELb1ELb1ELb0ELb0ELb1EEv16Flash_fwd_params
    744 bytes stack frame, 1122 bytes spill stores, 1124 bytes spill loads
ptxas info    : Used 255 registers, 584 bytes cmem[0]
ptxas info    : Compiling entry function '_Z16flash_fwd_kernelI23Flash_fwd_kernel_traitsILi512ELi32ELi32ELi2ELb0ELb0EN7cutlass10bfloat16_tE19Flash_kernel_traitsILi512ELi32ELi32ELi2ES2_EELb1ELb1ELb0ELb1ELb0EEv16Flash_fwd_params' for 'sm_80'
ptxas info    : Function properties for _Z16flash_fwd_kernelI23Flash_fwd_kernel_traitsILi512ELi32ELi32ELi2ELb0ELb0EN7cutlass10bfloat16_tE19Flash_kernel_traitsILi512ELi32ELi32ELi2ES2_EELb1ELb1ELb0ELb1ELb0EEv16Flash_fwd_params
    504 bytes stack frame, 796 bytes spill stores, 788 bytes spill loads
ptxas info    : Used 255 registers, 584 bytes cmem[0]
ptxas info    : Compiling entry function '_Z16flash_fwd_kernelI23Flash_fwd_kernel_traitsILi512ELi32ELi32ELi2ELb0ELb0EN7cutlass10bfloat16_tE19Flash_kernel_traitsILi512ELi32ELi32ELi2ES2_EELb1ELb1ELb0ELb1ELb1EEv16Flash_fwd_params' for 'sm_80'
ptxas info    : Function properties for _Z16flash_fwd_kernelI23Flash_fwd_kernel_traitsILi512ELi32ELi32ELi2ELb0ELb0EN7cutlass10bfloat16_tE19Flash_kernel_traitsILi512ELi32ELi32ELi2ES2_EELb1ELb1ELb0ELb1ELb1EEv16Flash_fwd_params
    688 bytes stack frame, 1024 bytes spill stores, 998 bytes spill loads
ptxas info    : Used 255 registers, 584 bytes cmem[0]
ptxas info    : Compiling entry function '_Z16flash_fwd_kernelI23Flash_fwd_kernel_traitsILi512ELi32ELi32ELi2ELb0ELb0EN7cutlass10bfloat16_tE19Flash_kernel_traitsILi512ELi32ELi32ELi2ES2_EELb1ELb1ELb1ELb0ELb0EEv16Flash_fwd_params' for 'sm_80'
ptxas info    : Function properties for _Z16flash_fwd_kernelI23Flash_fwd_kernel_traitsILi512ELi32ELi32ELi2ELb0ELb0EN7cutlass10bfloat16_tE19Flash_kernel_traitsILi512ELi32ELi32ELi2ES2_EELb1ELb1ELb1ELb0ELb0EEv16Flash_fwd_params
    712 bytes stack frame, 1060 bytes spill stores, 1072 bytes spill loads
ptxas info    : Used 255 registers, 584 bytes cmem[0]
ptxas info    : Compiling entry function '_Z16flash_fwd_kernelI23Flash_fwd_kernel_traitsILi512ELi32ELi32ELi2ELb0ELb0EN7cutlass10bfloat16_tE19Flash_kernel_traitsILi512ELi32ELi32ELi2ES2_EELb1ELb1ELb1ELb0ELb1EEv16Flash_fwd_params' for 'sm_80'
ptxas info    : Function properties for _Z16flash_fwd_kernelI23Flash_fwd_kernel_traitsILi512ELi32ELi32ELi2ELb0ELb0EN7cutlass10bfloat16_tE19Flash_kernel_traitsILi512ELi32ELi32ELi2ES2_EELb1ELb1ELb1ELb0ELb1EEv16Flash_fwd_params
    736 bytes stack frame, 1106 bytes spill stores, 1104 bytes spill loads
ptxas info    : Used 255 registers, 584 bytes cmem[0]
ptxas info    : Compiling entry function '_Z16flash_fwd_kernelI23Flash_fwd_kernel_traitsILi512ELi32ELi32ELi2ELb0ELb0EN7cutlass10bfloat16_tE19Flash_kernel_traitsILi512ELi32ELi32ELi2ES2_EELb1ELb1ELb1ELb1ELb0EEv16Flash_fwd_params' for 'sm_80'
ptxas info    : Function properties for _Z16flash_fwd_kernelI23Flash_fwd_kernel_traitsILi512ELi32ELi32ELi2ELb0ELb0EN7cutlass10bfloat16_tE19Flash_kernel_traitsILi512ELi32ELi32ELi2ES2_EELb1ELb1ELb1ELb1ELb0EEv16Flash_fwd_params
    456 bytes stack frame, 728 bytes spill stores, 744 bytes spill loads
ptxas info    : Used 255 registers, 584 bytes cmem[0]
ptxas info    : Compiling entry function '_Z16flash_fwd_kernelI23Flash_fwd_kernel_traitsILi512ELi32ELi32ELi2ELb0ELb0EN7cutlass10bfloat16_tE19Flash_kernel_traitsILi512ELi32ELi32ELi2ES2_EELb1ELb1ELb1ELb1ELb1EEv16Flash_fwd_params' for 'sm_80'
ptxas info    : Function properties for _Z16flash_fwd_kernelI23Flash_fwd_kernel_traitsILi512ELi32ELi32ELi2ELb0ELb0EN7cutlass10bfloat16_tE19Flash_kernel_traitsILi512ELi32ELi32ELi2ES2_EELb1ELb1ELb1ELb1ELb1EEv16Flash_fwd_params
    744 bytes stack frame, 1074 bytes spill stores, 1078 bytes spill loads
ptxas info    : Used 255 registers, 584 bytes cmem[0]
ptxas info    : Compiling entry function '_Z16flash_fwd_kernelI23Flash_fwd_kernel_traitsILi512ELi64ELi32ELi4ELb0ELb0EN7cutlass10bfloat16_tE19Flash_kernel_traitsILi512ELi64ELi32ELi4ES2_EELb1ELb1ELb0ELb0ELb0EEv16Flash_fwd_params' for 'sm_80'
ptxas info    : Function properties for _Z16flash_fwd_kernelI23Flash_fwd_kernel_traitsILi512ELi64ELi32ELi4ELb0ELb0EN7cutlass10bfloat16_tE19Flash_kernel_traitsILi512ELi64ELi32ELi4ES2_EELb1ELb1ELb0ELb0ELb0EEv16Flash_fwd_params
    784 bytes stack frame, 1708 bytes spill stores, 1832 bytes spill loads
ptxas info    : Used 255 registers, 584 bytes cmem[0]
ptxas info    : Compiling entry function '_Z16flash_fwd_kernelI23Flash_fwd_kernel_traitsILi512ELi64ELi32ELi4ELb0ELb0EN7cutlass10bfloat16_tE19Flash_kernel_traitsILi512ELi64ELi32ELi4ES2_EELb1ELb1ELb0ELb0ELb1EEv16Flash_fwd_params' for 'sm_80'
ptxas info    : Function properties for _Z16flash_fwd_kernelI23Flash_fwd_kernel_traitsILi512ELi64ELi32ELi4ELb0ELb0EN7cutlass10bfloat16_tE19Flash_kernel_traitsILi512ELi64ELi32ELi4ES2_EELb1ELb1ELb0ELb0ELb1EEv16Flash_fwd_params
    832 bytes stack frame, 1692 bytes spill stores, 1784 bytes spill loads
ptxas info    : Used 255 registers, 584 bytes cmem[0]
ptxas info    : Compiling entry function '_Z16flash_fwd_kernelI23Flash_fwd_kernel_traitsILi512ELi64ELi32ELi4ELb0ELb0EN7cutlass10bfloat16_tE19Flash_kernel_traitsILi512ELi64ELi32ELi4ES2_EELb1ELb1ELb0ELb1ELb0EEv16Flash_fwd_params' for 'sm_80'
ptxas info    : Function properties for _Z16flash_fwd_kernelI23Flash_fwd_kernel_traitsILi512ELi64ELi32ELi4ELb0ELb0EN7cutlass10bfloat16_tE19Flash_kernel_traitsILi512ELi64ELi32ELi4ES2_EELb1ELb1ELb0ELb1ELb0EEv16Flash_fwd_params
    656 bytes stack frame, 1552 bytes spill stores, 1604 bytes spill loads
ptxas info    : Used 255 registers, 584 bytes cmem[0]
ptxas info    : Compiling entry function '_Z16flash_fwd_kernelI23Flash_fwd_kernel_traitsILi512ELi64ELi32ELi4ELb0ELb0EN7cutlass10bfloat16_tE19Flash_kernel_traitsILi512ELi64ELi32ELi4ES2_EELb1ELb1ELb0ELb1ELb1EEv16Flash_fwd_params' for 'sm_80'
ptxas info    : Function properties for _Z16flash_fwd_kernelI23Flash_fwd_kernel_traitsILi512ELi64ELi32ELi4ELb0ELb0EN7cutlass10bfloat16_tE19Flash_kernel_traitsILi512ELi64ELi32ELi4ES2_EELb1ELb1ELb0ELb1ELb1EEv16Flash_fwd_params
    824 bytes stack frame, 1708 bytes spill stores, 1746 bytes spill loads
ptxas info    : Used 255 registers, 584 bytes cmem[0]
ptxas info    : Compiling entry function '_Z16flash_fwd_kernelI23Flash_fwd_kernel_traitsILi512ELi64ELi32ELi4ELb0ELb0EN7cutlass10bfloat16_tE19Flash_kernel_traitsILi512ELi64ELi32ELi4ES2_EELb1ELb1ELb1ELb0ELb0EEv16Flash_fwd_params' for 'sm_80'
ptxas info    : Function properties for _Z16flash_fwd_kernelI23Flash_fwd_kernel_traitsILi512ELi64ELi32ELi4ELb0ELb0EN7cutlass10bfloat16_tE19Flash_kernel_traitsILi512ELi64ELi32ELi4ES2_EELb1ELb1ELb1ELb0ELb0EEv16Flash_fwd_params
    808 bytes stack frame, 1664 bytes spill stores, 1748 bytes spill loads
ptxas info    : Used 255 registers, 584 bytes cmem[0]
ptxas info    : Compiling entry function '_Z16flash_fwd_kernelI23Flash_fwd_kernel_traitsILi512ELi64ELi32ELi4ELb0ELb0EN7cutlass10bfloat16_tE19Flash_kernel_traitsILi512ELi64ELi32ELi4ES2_EELb1ELb1ELb1ELb0ELb1EEv16Flash_fwd_params' for 'sm_80'
ptxas info    : Function properties for _Z16flash_fwd_kernelI23Flash_fwd_kernel_traitsILi512ELi64ELi32ELi4ELb0ELb0EN7cutlass10bfloat16_tE19Flash_kernel_traitsILi512ELi64ELi32ELi4ES2_EELb1ELb1ELb1ELb0ELb1EEv16Flash_fwd_params
    1040 bytes stack frame, 1964 bytes spill stores, 2018 bytes spill loads
ptxas info    : Used 255 registers, 584 bytes cmem[0]
ptxas info    : Compiling entry function '_Z16flash_fwd_kernelI23Flash_fwd_kernel_traitsILi512ELi64ELi32ELi4ELb0ELb0EN7cutlass10bfloat16_tE19Flash_kernel_traitsILi512ELi64ELi32ELi4ES2_EELb1ELb1ELb1ELb1ELb0EEv16Flash_fwd_params' for 'sm_80'
ptxas info    : Function properties for _Z16flash_fwd_kernelI23Flash_fwd_kernel_traitsILi512ELi64ELi32ELi4ELb0ELb0EN7cutlass10bfloat16_tE19Flash_kernel_traitsILi512ELi64ELi32ELi4ES2_EELb1ELb1ELb1ELb1ELb0EEv16Flash_fwd_params
    672 bytes stack frame, 1584 bytes spill stores, 1636 bytes spill loads
ptxas info    : Used 255 registers, 584 bytes cmem[0]
ptxas info    : Compiling entry function '_Z16flash_fwd_kernelI23Flash_fwd_kernel_traitsILi512ELi64ELi32ELi4ELb0ELb0EN7cutlass10bfloat16_tE19Flash_kernel_traitsILi512ELi64ELi32ELi4ES2_EELb1ELb1ELb1ELb1ELb1EEv16Flash_fwd_params' for 'sm_80'
ptxas info    : Function properties for _Z16flash_fwd_kernelI23Flash_fwd_kernel_traitsILi512ELi64ELi32ELi4ELb0ELb0EN7cutlass10bfloat16_tE19Flash_kernel_traitsILi512ELi64ELi32ELi4ES2_EELb1ELb1ELb1ELb1ELb1EEv16Flash_fwd_params
    800 bytes stack frame, 1682 bytes spill stores, 1718 bytes spill loads
ptxas info    : Used 255 registers, 584 bytes cmem[0]
/anaconda/envs/flashattention/lib/python3.9/site-packages/torch/include/c10/core/SymInt.h(84): warning: integer conversion resulted in a change of sign

[13/13] /usr/local/cuda-11.4/bin/nvcc  -I/home/v-feiychen/flash-attention/csrc/flash_attn -I/home/v-feiychen/flash-attention/csrc/flash_attn/src -I/home/v-feiychen/flash-attention/csrc/cutlass/include -I/anaconda/envs/flashattention/lib/python3.9/site-packages/torch/include -I/anaconda/envs/flashattention/lib/python3.9/site-packages/torch/include/torch/csrc/api/include -I/anaconda/envs/flashattention/lib/python3.9/site-packages/torch/include/TH -I/anaconda/envs/flashattention/lib/python3.9/site-packages/torch/include/THC -I/usr/local/cuda-11.4/include -I/anaconda/envs/flashattention/include/python3.9 -c -c /home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_hdim512_fp16_sm80.cu -o /home/v-feiychen/flash-attention/build/temp.linux-x86_64-cpython-39/csrc/flash_attn/src/flash_fwd_hdim512_fp16_sm80.o -D__CUDA_NO_HALF_OPERATORS__ -D__CUDA_NO_HALF_CONVERSIONS__ -D__CUDA_NO_BFLOAT16_CONVERSIONS__ -D__CUDA_NO_HALF2_OPERATORS__ --expt-relaxed-constexpr --compiler-options ''"'"'-fPIC'"'"'' -O3 -std=c++17 -U__CUDA_NO_HALF_OPERATORS__ -U__CUDA_NO_HALF_CONVERSIONS__ -U__CUDA_NO_HALF2_OPERATORS__ -U__CUDA_NO_BFLOAT16_CONVERSIONS__ --expt-relaxed-constexpr --expt-extended-lambda --use_fast_math --ptxas-options=-v -lineinfo -gencode arch=compute_80,code=sm_80 --threads 4 -DTORCH_API_INCLUDE_EXTENSION_H '-DPYBIND11_COMPILER_TYPE="_gcc"' '-DPYBIND11_STDLIB="_libstdcpp"' '-DPYBIND11_BUILD_ABI="_cxxabi1011"' -DTORCH_EXTENSION_NAME=flash_attn_2_cuda -D_GLIBCXX_USE_CXX11_ABI=0
/anaconda/envs/flashattention/lib/python3.9/site-packages/torch/include/c10/core/SymInt.h(84): warning: integer conversion resulted in a change of sign

/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_kernel.h(140): warning: variable "MMA_M" was declared but never referenced
          detected during:
            instantiation of "void flash::compute_attn_1rowblock<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &, int, int, int) [with Kernel_traits=Flash_fwd_kernel_traits<512, 64, 32, 4, false, false, cutlass::half_t, Flash_kernel_traits<512, 64, 32, 4, cutlass::half_t>>, Is_dropout=true, Is_causal=true, Is_even_N=true, Is_even_K=true, Return_softmax=true, Params=Flash_fwd_params]" 
(588): here
            instantiation of "void flash::compute_attn<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &) [with Kernel_traits=Flash_fwd_kernel_traits<512, 64, 32, 4, false, false, cutlass::half_t, Flash_kernel_traits<512, 64, 32, 4, cutlass::half_t>>, Is_dropout=true, Is_causal=true, Is_even_N=true, Is_even_K=true, Return_softmax=true, Params=Flash_fwd_params]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(15): here
            instantiation of "void flash_fwd_kernel<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax>(Flash_fwd_params) [with Kernel_traits=Flash_fwd_kernel_traits<512, 64, 32, 4, false, false, cutlass::half_t, Flash_kernel_traits<512, 64, 32, 4, cutlass::half_t>>, Is_dropout=true, Is_causal=true, Is_even_N=true, Is_even_K=true, Return_softmax=true]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(34): here
            instantiation of "void run_flash_fwd<Kernel_traits,Is_dropout,Is_causal>(Flash_fwd_params &, cudaStream_t) [with Kernel_traits=Flash_fwd_kernel_traits<512, 64, 32, 4, false, false, cutlass::half_t, Flash_kernel_traits<512, 64, 32, 4, cutlass::half_t>>, Is_dropout=true, Is_causal=true]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(264): here
            instantiation of "void run_mha_fwd_hdim512<T>(Flash_fwd_params &, cudaStream_t) [with T=cutlass::half_t]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_hdim512_fp16_sm80.cu(8): here

/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_kernel.h(275): warning: variable "tQrQ" was declared but never referenced
          detected during:
            instantiation of "void flash::compute_attn_1rowblock<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &, int, int, int) [with Kernel_traits=Flash_fwd_kernel_traits<512, 64, 32, 4, false, false, cutlass::half_t, Flash_kernel_traits<512, 64, 32, 4, cutlass::half_t>>, Is_dropout=true, Is_causal=true, Is_even_N=true, Is_even_K=true, Return_softmax=true, Params=Flash_fwd_params]" 
(588): here
            instantiation of "void flash::compute_attn<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &) [with Kernel_traits=Flash_fwd_kernel_traits<512, 64, 32, 4, false, false, cutlass::half_t, Flash_kernel_traits<512, 64, 32, 4, cutlass::half_t>>, Is_dropout=true, Is_causal=true, Is_even_N=true, Is_even_K=true, Return_softmax=true, Params=Flash_fwd_params]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(15): here
            instantiation of "void flash_fwd_kernel<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax>(Flash_fwd_params) [with Kernel_traits=Flash_fwd_kernel_traits<512, 64, 32, 4, false, false, cutlass::half_t, Flash_kernel_traits<512, 64, 32, 4, cutlass::half_t>>, Is_dropout=true, Is_causal=true, Is_even_N=true, Is_even_K=true, Return_softmax=true]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(34): here
            instantiation of "void run_flash_fwd<Kernel_traits,Is_dropout,Is_causal>(Flash_fwd_params &, cudaStream_t) [with Kernel_traits=Flash_fwd_kernel_traits<512, 64, 32, 4, false, false, cutlass::half_t, Flash_kernel_traits<512, 64, 32, 4, cutlass::half_t>>, Is_dropout=true, Is_causal=true]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(264): here
            instantiation of "void run_mha_fwd_hdim512<T>(Flash_fwd_params &, cudaStream_t) [with T=cutlass::half_t]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_hdim512_fp16_sm80.cu(8): here

/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_kernel.h(140): warning: variable "MMA_M" was declared but never referenced
          detected during:
            instantiation of "void flash::compute_attn_1rowblock<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &, int, int, int) [with Kernel_traits=Flash_fwd_kernel_traits<512, 64, 32, 4, false, false, cutlass::half_t, Flash_kernel_traits<512, 64, 32, 4, cutlass::half_t>>, Is_dropout=true, Is_causal=true, Is_even_N=true, Is_even_K=true, Return_softmax=false, Params=Flash_fwd_params]" 
(588): here
            instantiation of "void flash::compute_attn<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &) [with Kernel_traits=Flash_fwd_kernel_traits<512, 64, 32, 4, false, false, cutlass::half_t, Flash_kernel_traits<512, 64, 32, 4, cutlass::half_t>>, Is_dropout=true, Is_causal=true, Is_even_N=true, Is_even_K=true, Return_softmax=false, Params=Flash_fwd_params]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(15): here
            instantiation of "void flash_fwd_kernel<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax>(Flash_fwd_params) [with Kernel_traits=Flash_fwd_kernel_traits<512, 64, 32, 4, false, false, cutlass::half_t, Flash_kernel_traits<512, 64, 32, 4, cutlass::half_t>>, Is_dropout=true, Is_causal=true, Is_even_N=true, Is_even_K=true, Return_softmax=false]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(34): here
            instantiation of "void run_flash_fwd<Kernel_traits,Is_dropout,Is_causal>(Flash_fwd_params &, cudaStream_t) [with Kernel_traits=Flash_fwd_kernel_traits<512, 64, 32, 4, false, false, cutlass::half_t, Flash_kernel_traits<512, 64, 32, 4, cutlass::half_t>>, Is_dropout=true, Is_causal=true]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(264): here
            instantiation of "void run_mha_fwd_hdim512<T>(Flash_fwd_params &, cudaStream_t) [with T=cutlass::half_t]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_hdim512_fp16_sm80.cu(8): here

/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_kernel.h(275): warning: variable "tQrQ" was declared but never referenced
          detected during:
            instantiation of "void flash::compute_attn_1rowblock<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &, int, int, int) [with Kernel_traits=Flash_fwd_kernel_traits<512, 64, 32, 4, false, false, cutlass::half_t, Flash_kernel_traits<512, 64, 32, 4, cutlass::half_t>>, Is_dropout=true, Is_causal=true, Is_even_N=true, Is_even_K=true, Return_softmax=false, Params=Flash_fwd_params]" 
(588): here
            instantiation of "void flash::compute_attn<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &) [with Kernel_traits=Flash_fwd_kernel_traits<512, 64, 32, 4, false, false, cutlass::half_t, Flash_kernel_traits<512, 64, 32, 4, cutlass::half_t>>, Is_dropout=true, Is_causal=true, Is_even_N=true, Is_even_K=true, Return_softmax=false, Params=Flash_fwd_params]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(15): here
            instantiation of "void flash_fwd_kernel<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax>(Flash_fwd_params) [with Kernel_traits=Flash_fwd_kernel_traits<512, 64, 32, 4, false, false, cutlass::half_t, Flash_kernel_traits<512, 64, 32, 4, cutlass::half_t>>, Is_dropout=true, Is_causal=true, Is_even_N=true, Is_even_K=true, Return_softmax=false]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(34): here
            instantiation of "void run_flash_fwd<Kernel_traits,Is_dropout,Is_causal>(Flash_fwd_params &, cudaStream_t) [with Kernel_traits=Flash_fwd_kernel_traits<512, 64, 32, 4, false, false, cutlass::half_t, Flash_kernel_traits<512, 64, 32, 4, cutlass::half_t>>, Is_dropout=true, Is_causal=true]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(264): here
            instantiation of "void run_mha_fwd_hdim512<T>(Flash_fwd_params &, cudaStream_t) [with T=cutlass::half_t]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_hdim512_fp16_sm80.cu(8): here

/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_kernel.h(140): warning: variable "MMA_M" was declared but never referenced
          detected during:
            instantiation of "void flash::compute_attn_1rowblock<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &, int, int, int) [with Kernel_traits=Flash_fwd_kernel_traits<512, 64, 32, 4, false, false, cutlass::half_t, Flash_kernel_traits<512, 64, 32, 4, cutlass::half_t>>, Is_dropout=true, Is_causal=true, Is_even_N=true, Is_even_K=false, Return_softmax=true, Params=Flash_fwd_params]" 
(588): here
            instantiation of "void flash::compute_attn<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &) [with Kernel_traits=Flash_fwd_kernel_traits<512, 64, 32, 4, false, false, cutlass::half_t, Flash_kernel_traits<512, 64, 32, 4, cutlass::half_t>>, Is_dropout=true, Is_causal=true, Is_even_N=true, Is_even_K=false, Return_softmax=true, Params=Flash_fwd_params]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(15): here
            instantiation of "void flash_fwd_kernel<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax>(Flash_fwd_params) [with Kernel_traits=Flash_fwd_kernel_traits<512, 64, 32, 4, false, false, cutlass::half_t, Flash_kernel_traits<512, 64, 32, 4, cutlass::half_t>>, Is_dropout=true, Is_causal=true, Is_even_N=true, Is_even_K=false, Return_softmax=true]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(34): here
            instantiation of "void run_flash_fwd<Kernel_traits,Is_dropout,Is_causal>(Flash_fwd_params &, cudaStream_t) [with Kernel_traits=Flash_fwd_kernel_traits<512, 64, 32, 4, false, false, cutlass::half_t, Flash_kernel_traits<512, 64, 32, 4, cutlass::half_t>>, Is_dropout=true, Is_causal=true]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(264): here
            instantiation of "void run_mha_fwd_hdim512<T>(Flash_fwd_params &, cudaStream_t) [with T=cutlass::half_t]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_hdim512_fp16_sm80.cu(8): here

/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_kernel.h(275): warning: variable "tQrQ" was declared but never referenced
          detected during:
            instantiation of "void flash::compute_attn_1rowblock<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &, int, int, int) [with Kernel_traits=Flash_fwd_kernel_traits<512, 64, 32, 4, false, false, cutlass::half_t, Flash_kernel_traits<512, 64, 32, 4, cutlass::half_t>>, Is_dropout=true, Is_causal=true, Is_even_N=true, Is_even_K=false, Return_softmax=true, Params=Flash_fwd_params]" 
(588): here
            instantiation of "void flash::compute_attn<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &) [with Kernel_traits=Flash_fwd_kernel_traits<512, 64, 32, 4, false, false, cutlass::half_t, Flash_kernel_traits<512, 64, 32, 4, cutlass::half_t>>, Is_dropout=true, Is_causal=true, Is_even_N=true, Is_even_K=false, Return_softmax=true, Params=Flash_fwd_params]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(15): here
            instantiation of "void flash_fwd_kernel<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax>(Flash_fwd_params) [with Kernel_traits=Flash_fwd_kernel_traits<512, 64, 32, 4, false, false, cutlass::half_t, Flash_kernel_traits<512, 64, 32, 4, cutlass::half_t>>, Is_dropout=true, Is_causal=true, Is_even_N=true, Is_even_K=false, Return_softmax=true]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(34): here
            instantiation of "void run_flash_fwd<Kernel_traits,Is_dropout,Is_causal>(Flash_fwd_params &, cudaStream_t) [with Kernel_traits=Flash_fwd_kernel_traits<512, 64, 32, 4, false, false, cutlass::half_t, Flash_kernel_traits<512, 64, 32, 4, cutlass::half_t>>, Is_dropout=true, Is_causal=true]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(264): here
            instantiation of "void run_mha_fwd_hdim512<T>(Flash_fwd_params &, cudaStream_t) [with T=cutlass::half_t]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_hdim512_fp16_sm80.cu(8): here

/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_kernel.h(140): warning: variable "MMA_M" was declared but never referenced
          detected during:
            instantiation of "void flash::compute_attn_1rowblock<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &, int, int, int) [with Kernel_traits=Flash_fwd_kernel_traits<512, 64, 32, 4, false, false, cutlass::half_t, Flash_kernel_traits<512, 64, 32, 4, cutlass::half_t>>, Is_dropout=true, Is_causal=true, Is_even_N=true, Is_even_K=false, Return_softmax=false, Params=Flash_fwd_params]" 
(588): here
            instantiation of "void flash::compute_attn<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &) [with Kernel_traits=Flash_fwd_kernel_traits<512, 64, 32, 4, false, false, cutlass::half_t, Flash_kernel_traits<512, 64, 32, 4, cutlass::half_t>>, Is_dropout=true, Is_causal=true, Is_even_N=true, Is_even_K=false, Return_softmax=false, Params=Flash_fwd_params]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(15): here
            instantiation of "void flash_fwd_kernel<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax>(Flash_fwd_params) [with Kernel_traits=Flash_fwd_kernel_traits<512, 64, 32, 4, false, false, cutlass::half_t, Flash_kernel_traits<512, 64, 32, 4, cutlass::half_t>>, Is_dropout=true, Is_causal=true, Is_even_N=true, Is_even_K=false, Return_softmax=false]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(34): here
            instantiation of "void run_flash_fwd<Kernel_traits,Is_dropout,Is_causal>(Flash_fwd_params &, cudaStream_t) [with Kernel_traits=Flash_fwd_kernel_traits<512, 64, 32, 4, false, false, cutlass::half_t, Flash_kernel_traits<512, 64, 32, 4, cutlass::half_t>>, Is_dropout=true, Is_causal=true]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(264): here
            instantiation of "void run_mha_fwd_hdim512<T>(Flash_fwd_params &, cudaStream_t) [with T=cutlass::half_t]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_hdim512_fp16_sm80.cu(8): here

/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_kernel.h(275): warning: variable "tQrQ" was declared but never referenced
          detected during:
            instantiation of "void flash::compute_attn_1rowblock<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &, int, int, int) [with Kernel_traits=Flash_fwd_kernel_traits<512, 64, 32, 4, false, false, cutlass::half_t, Flash_kernel_traits<512, 64, 32, 4, cutlass::half_t>>, Is_dropout=true, Is_causal=true, Is_even_N=true, Is_even_K=false, Return_softmax=false, Params=Flash_fwd_params]" 
(588): here
            instantiation of "void flash::compute_attn<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &) [with Kernel_traits=Flash_fwd_kernel_traits<512, 64, 32, 4, false, false, cutlass::half_t, Flash_kernel_traits<512, 64, 32, 4, cutlass::half_t>>, Is_dropout=true, Is_causal=true, Is_even_N=true, Is_even_K=false, Return_softmax=false, Params=Flash_fwd_params]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(15): here
            instantiation of "void flash_fwd_kernel<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax>(Flash_fwd_params) [with Kernel_traits=Flash_fwd_kernel_traits<512, 64, 32, 4, false, false, cutlass::half_t, Flash_kernel_traits<512, 64, 32, 4, cutlass::half_t>>, Is_dropout=true, Is_causal=true, Is_even_N=true, Is_even_K=false, Return_softmax=false]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(34): here
            instantiation of "void run_flash_fwd<Kernel_traits,Is_dropout,Is_causal>(Flash_fwd_params &, cudaStream_t) [with Kernel_traits=Flash_fwd_kernel_traits<512, 64, 32, 4, false, false, cutlass::half_t, Flash_kernel_traits<512, 64, 32, 4, cutlass::half_t>>, Is_dropout=true, Is_causal=true]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(264): here
            instantiation of "void run_mha_fwd_hdim512<T>(Flash_fwd_params &, cudaStream_t) [with T=cutlass::half_t]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_hdim512_fp16_sm80.cu(8): here

/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_kernel.h(140): warning: variable "MMA_M" was declared but never referenced
          detected during:
            instantiation of "void flash::compute_attn_1rowblock<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &, int, int, int) [with Kernel_traits=Flash_fwd_kernel_traits<512, 64, 32, 4, false, false, cutlass::half_t, Flash_kernel_traits<512, 64, 32, 4, cutlass::half_t>>, Is_dropout=true, Is_causal=true, Is_even_N=false, Is_even_K=true, Return_softmax=true, Params=Flash_fwd_params]" 
(588): here
            instantiation of "void flash::compute_attn<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &) [with Kernel_traits=Flash_fwd_kernel_traits<512, 64, 32, 4, false, false, cutlass::half_t, Flash_kernel_traits<512, 64, 32, 4, cutlass::half_t>>, Is_dropout=true, Is_causal=true, Is_even_N=false, Is_even_K=true, Return_softmax=true, Params=Flash_fwd_params]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(15): here
            instantiation of "void flash_fwd_kernel<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax>(Flash_fwd_params) [with Kernel_traits=Flash_fwd_kernel_traits<512, 64, 32, 4, false, false, cutlass::half_t, Flash_kernel_traits<512, 64, 32, 4, cutlass::half_t>>, Is_dropout=true, Is_causal=true, Is_even_N=false, Is_even_K=true, Return_softmax=true]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(34): here
            instantiation of "void run_flash_fwd<Kernel_traits,Is_dropout,Is_causal>(Flash_fwd_params &, cudaStream_t) [with Kernel_traits=Flash_fwd_kernel_traits<512, 64, 32, 4, false, false, cutlass::half_t, Flash_kernel_traits<512, 64, 32, 4, cutlass::half_t>>, Is_dropout=true, Is_causal=true]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(264): here
            instantiation of "void run_mha_fwd_hdim512<T>(Flash_fwd_params &, cudaStream_t) [with T=cutlass::half_t]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_hdim512_fp16_sm80.cu(8): here

/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_kernel.h(275): warning: variable "tQrQ" was declared but never referenced
          detected during:
            instantiation of "void flash::compute_attn_1rowblock<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &, int, int, int) [with Kernel_traits=Flash_fwd_kernel_traits<512, 64, 32, 4, false, false, cutlass::half_t, Flash_kernel_traits<512, 64, 32, 4, cutlass::half_t>>, Is_dropout=true, Is_causal=true, Is_even_N=false, Is_even_K=true, Return_softmax=true, Params=Flash_fwd_params]" 
(588): here
            instantiation of "void flash::compute_attn<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &) [with Kernel_traits=Flash_fwd_kernel_traits<512, 64, 32, 4, false, false, cutlass::half_t, Flash_kernel_traits<512, 64, 32, 4, cutlass::half_t>>, Is_dropout=true, Is_causal=true, Is_even_N=false, Is_even_K=true, Return_softmax=true, Params=Flash_fwd_params]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(15): here
            instantiation of "void flash_fwd_kernel<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax>(Flash_fwd_params) [with Kernel_traits=Flash_fwd_kernel_traits<512, 64, 32, 4, false, false, cutlass::half_t, Flash_kernel_traits<512, 64, 32, 4, cutlass::half_t>>, Is_dropout=true, Is_causal=true, Is_even_N=false, Is_even_K=true, Return_softmax=true]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(34): here
            instantiation of "void run_flash_fwd<Kernel_traits,Is_dropout,Is_causal>(Flash_fwd_params &, cudaStream_t) [with Kernel_traits=Flash_fwd_kernel_traits<512, 64, 32, 4, false, false, cutlass::half_t, Flash_kernel_traits<512, 64, 32, 4, cutlass::half_t>>, Is_dropout=true, Is_causal=true]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(264): here
            instantiation of "void run_mha_fwd_hdim512<T>(Flash_fwd_params &, cudaStream_t) [with T=cutlass::half_t]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_hdim512_fp16_sm80.cu(8): here

/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_kernel.h(140): warning: variable "MMA_M" was declared but never referenced
          detected during:
            instantiation of "void flash::compute_attn_1rowblock<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &, int, int, int) [with Kernel_traits=Flash_fwd_kernel_traits<512, 64, 32, 4, false, false, cutlass::half_t, Flash_kernel_traits<512, 64, 32, 4, cutlass::half_t>>, Is_dropout=true, Is_causal=true, Is_even_N=false, Is_even_K=true, Return_softmax=false, Params=Flash_fwd_params]" 
(588): here
            instantiation of "void flash::compute_attn<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &) [with Kernel_traits=Flash_fwd_kernel_traits<512, 64, 32, 4, false, false, cutlass::half_t, Flash_kernel_traits<512, 64, 32, 4, cutlass::half_t>>, Is_dropout=true, Is_causal=true, Is_even_N=false, Is_even_K=true, Return_softmax=false, Params=Flash_fwd_params]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(15): here
            instantiation of "void flash_fwd_kernel<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax>(Flash_fwd_params) [with Kernel_traits=Flash_fwd_kernel_traits<512, 64, 32, 4, false, false, cutlass::half_t, Flash_kernel_traits<512, 64, 32, 4, cutlass::half_t>>, Is_dropout=true, Is_causal=true, Is_even_N=false, Is_even_K=true, Return_softmax=false]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(34): here
            instantiation of "void run_flash_fwd<Kernel_traits,Is_dropout,Is_causal>(Flash_fwd_params &, cudaStream_t) [with Kernel_traits=Flash_fwd_kernel_traits<512, 64, 32, 4, false, false, cutlass::half_t, Flash_kernel_traits<512, 64, 32, 4, cutlass::half_t>>, Is_dropout=true, Is_causal=true]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(264): here
            instantiation of "void run_mha_fwd_hdim512<T>(Flash_fwd_params &, cudaStream_t) [with T=cutlass::half_t]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_hdim512_fp16_sm80.cu(8): here

/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_kernel.h(275): warning: variable "tQrQ" was declared but never referenced
          detected during:
            instantiation of "void flash::compute_attn_1rowblock<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &, int, int, int) [with Kernel_traits=Flash_fwd_kernel_traits<512, 64, 32, 4, false, false, cutlass::half_t, Flash_kernel_traits<512, 64, 32, 4, cutlass::half_t>>, Is_dropout=true, Is_causal=true, Is_even_N=false, Is_even_K=true, Return_softmax=false, Params=Flash_fwd_params]" 
(588): here
            instantiation of "void flash::compute_attn<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &) [with Kernel_traits=Flash_fwd_kernel_traits<512, 64, 32, 4, false, false, cutlass::half_t, Flash_kernel_traits<512, 64, 32, 4, cutlass::half_t>>, Is_dropout=true, Is_causal=true, Is_even_N=false, Is_even_K=true, Return_softmax=false, Params=Flash_fwd_params]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(15): here
            instantiation of "void flash_fwd_kernel<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax>(Flash_fwd_params) [with Kernel_traits=Flash_fwd_kernel_traits<512, 64, 32, 4, false, false, cutlass::half_t, Flash_kernel_traits<512, 64, 32, 4, cutlass::half_t>>, Is_dropout=true, Is_causal=true, Is_even_N=false, Is_even_K=true, Return_softmax=false]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(34): here
            instantiation of "void run_flash_fwd<Kernel_traits,Is_dropout,Is_causal>(Flash_fwd_params &, cudaStream_t) [with Kernel_traits=Flash_fwd_kernel_traits<512, 64, 32, 4, false, false, cutlass::half_t, Flash_kernel_traits<512, 64, 32, 4, cutlass::half_t>>, Is_dropout=true, Is_causal=true]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(264): here
            instantiation of "void run_mha_fwd_hdim512<T>(Flash_fwd_params &, cudaStream_t) [with T=cutlass::half_t]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_hdim512_fp16_sm80.cu(8): here

/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_kernel.h(140): warning: variable "MMA_M" was declared but never referenced
          detected during:
            instantiation of "void flash::compute_attn_1rowblock<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &, int, int, int) [with Kernel_traits=Flash_fwd_kernel_traits<512, 64, 32, 4, false, false, cutlass::half_t, Flash_kernel_traits<512, 64, 32, 4, cutlass::half_t>>, Is_dropout=true, Is_causal=true, Is_even_N=false, Is_even_K=false, Return_softmax=true, Params=Flash_fwd_params]" 
(588): here
            instantiation of "void flash::compute_attn<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &) [with Kernel_traits=Flash_fwd_kernel_traits<512, 64, 32, 4, false, false, cutlass::half_t, Flash_kernel_traits<512, 64, 32, 4, cutlass::half_t>>, Is_dropout=true, Is_causal=true, Is_even_N=false, Is_even_K=false, Return_softmax=true, Params=Flash_fwd_params]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(15): here
            instantiation of "void flash_fwd_kernel<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax>(Flash_fwd_params) [with Kernel_traits=Flash_fwd_kernel_traits<512, 64, 32, 4, false, false, cutlass::half_t, Flash_kernel_traits<512, 64, 32, 4, cutlass::half_t>>, Is_dropout=true, Is_causal=true, Is_even_N=false, Is_even_K=false, Return_softmax=true]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(34): here
            instantiation of "void run_flash_fwd<Kernel_traits,Is_dropout,Is_causal>(Flash_fwd_params &, cudaStream_t) [with Kernel_traits=Flash_fwd_kernel_traits<512, 64, 32, 4, false, false, cutlass::half_t, Flash_kernel_traits<512, 64, 32, 4, cutlass::half_t>>, Is_dropout=true, Is_causal=true]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(264): here
            instantiation of "void run_mha_fwd_hdim512<T>(Flash_fwd_params &, cudaStream_t) [with T=cutlass::half_t]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_hdim512_fp16_sm80.cu(8): here

/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_kernel.h(275): warning: variable "tQrQ" was declared but never referenced
          detected during:
            instantiation of "void flash::compute_attn_1rowblock<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &, int, int, int) [with Kernel_traits=Flash_fwd_kernel_traits<512, 64, 32, 4, false, false, cutlass::half_t, Flash_kernel_traits<512, 64, 32, 4, cutlass::half_t>>, Is_dropout=true, Is_causal=true, Is_even_N=false, Is_even_K=false, Return_softmax=true, Params=Flash_fwd_params]" 
(588): here
            instantiation of "void flash::compute_attn<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &) [with Kernel_traits=Flash_fwd_kernel_traits<512, 64, 32, 4, false, false, cutlass::half_t, Flash_kernel_traits<512, 64, 32, 4, cutlass::half_t>>, Is_dropout=true, Is_causal=true, Is_even_N=false, Is_even_K=false, Return_softmax=true, Params=Flash_fwd_params]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(15): here
            instantiation of "void flash_fwd_kernel<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax>(Flash_fwd_params) [with Kernel_traits=Flash_fwd_kernel_traits<512, 64, 32, 4, false, false, cutlass::half_t, Flash_kernel_traits<512, 64, 32, 4, cutlass::half_t>>, Is_dropout=true, Is_causal=true, Is_even_N=false, Is_even_K=false, Return_softmax=true]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(34): here
            instantiation of "void run_flash_fwd<Kernel_traits,Is_dropout,Is_causal>(Flash_fwd_params &, cudaStream_t) [with Kernel_traits=Flash_fwd_kernel_traits<512, 64, 32, 4, false, false, cutlass::half_t, Flash_kernel_traits<512, 64, 32, 4, cutlass::half_t>>, Is_dropout=true, Is_causal=true]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(264): here
            instantiation of "void run_mha_fwd_hdim512<T>(Flash_fwd_params &, cudaStream_t) [with T=cutlass::half_t]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_hdim512_fp16_sm80.cu(8): here

/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_kernel.h(140): warning: variable "MMA_M" was declared but never referenced
          detected during:
            instantiation of "void flash::compute_attn_1rowblock<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &, int, int, int) [with Kernel_traits=Flash_fwd_kernel_traits<512, 64, 32, 4, false, false, cutlass::half_t, Flash_kernel_traits<512, 64, 32, 4, cutlass::half_t>>, Is_dropout=true, Is_causal=true, Is_even_N=false, Is_even_K=false, Return_softmax=false, Params=Flash_fwd_params]" 
(588): here
            instantiation of "void flash::compute_attn<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &) [with Kernel_traits=Flash_fwd_kernel_traits<512, 64, 32, 4, false, false, cutlass::half_t, Flash_kernel_traits<512, 64, 32, 4, cutlass::half_t>>, Is_dropout=true, Is_causal=true, Is_even_N=false, Is_even_K=false, Return_softmax=false, Params=Flash_fwd_params]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(15): here
            instantiation of "void flash_fwd_kernel<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax>(Flash_fwd_params) [with Kernel_traits=Flash_fwd_kernel_traits<512, 64, 32, 4, false, false, cutlass::half_t, Flash_kernel_traits<512, 64, 32, 4, cutlass::half_t>>, Is_dropout=true, Is_causal=true, Is_even_N=false, Is_even_K=false, Return_softmax=false]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(34): here
            instantiation of "void run_flash_fwd<Kernel_traits,Is_dropout,Is_causal>(Flash_fwd_params &, cudaStream_t) [with Kernel_traits=Flash_fwd_kernel_traits<512, 64, 32, 4, false, false, cutlass::half_t, Flash_kernel_traits<512, 64, 32, 4, cutlass::half_t>>, Is_dropout=true, Is_causal=true]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(264): here
            instantiation of "void run_mha_fwd_hdim512<T>(Flash_fwd_params &, cudaStream_t) [with T=cutlass::half_t]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_hdim512_fp16_sm80.cu(8): here

/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_kernel.h(275): warning: variable "tQrQ" was declared but never referenced
          detected during:
            instantiation of "void flash::compute_attn_1rowblock<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &, int, int, int) [with Kernel_traits=Flash_fwd_kernel_traits<512, 64, 32, 4, false, false, cutlass::half_t, Flash_kernel_traits<512, 64, 32, 4, cutlass::half_t>>, Is_dropout=true, Is_causal=true, Is_even_N=false, Is_even_K=false, Return_softmax=false, Params=Flash_fwd_params]" 
(588): here
            instantiation of "void flash::compute_attn<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &) [with Kernel_traits=Flash_fwd_kernel_traits<512, 64, 32, 4, false, false, cutlass::half_t, Flash_kernel_traits<512, 64, 32, 4, cutlass::half_t>>, Is_dropout=true, Is_causal=true, Is_even_N=false, Is_even_K=false, Return_softmax=false, Params=Flash_fwd_params]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(15): here
            instantiation of "void flash_fwd_kernel<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax>(Flash_fwd_params) [with Kernel_traits=Flash_fwd_kernel_traits<512, 64, 32, 4, false, false, cutlass::half_t, Flash_kernel_traits<512, 64, 32, 4, cutlass::half_t>>, Is_dropout=true, Is_causal=true, Is_even_N=false, Is_even_K=false, Return_softmax=false]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(34): here
            instantiation of "void run_flash_fwd<Kernel_traits,Is_dropout,Is_causal>(Flash_fwd_params &, cudaStream_t) [with Kernel_traits=Flash_fwd_kernel_traits<512, 64, 32, 4, false, false, cutlass::half_t, Flash_kernel_traits<512, 64, 32, 4, cutlass::half_t>>, Is_dropout=true, Is_causal=true]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(264): here
            instantiation of "void run_mha_fwd_hdim512<T>(Flash_fwd_params &, cudaStream_t) [with T=cutlass::half_t]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_hdim512_fp16_sm80.cu(8): here

/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_kernel.h(140): warning: variable "MMA_M" was declared but never referenced
          detected during:
            instantiation of "void flash::compute_attn_1rowblock<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &, int, int, int) [with Kernel_traits=Flash_fwd_kernel_traits<512, 32, 32, 2, false, false, cutlass::half_t, Flash_kernel_traits<512, 32, 32, 2, cutlass::half_t>>, Is_dropout=true, Is_causal=true, Is_even_N=true, Is_even_K=true, Return_softmax=true, Params=Flash_fwd_params]" 
(588): here
            instantiation of "void flash::compute_attn<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &) [with Kernel_traits=Flash_fwd_kernel_traits<512, 32, 32, 2, false, false, cutlass::half_t, Flash_kernel_traits<512, 32, 32, 2, cutlass::half_t>>, Is_dropout=true, Is_causal=true, Is_even_N=true, Is_even_K=true, Return_softmax=true, Params=Flash_fwd_params]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(15): here
            instantiation of "void flash_fwd_kernel<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax>(Flash_fwd_params) [with Kernel_traits=Flash_fwd_kernel_traits<512, 32, 32, 2, false, false, cutlass::half_t, Flash_kernel_traits<512, 32, 32, 2, cutlass::half_t>>, Is_dropout=true, Is_causal=true, Is_even_N=true, Is_even_K=true, Return_softmax=true]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(34): here
            instantiation of "void run_flash_fwd<Kernel_traits,Is_dropout,Is_causal>(Flash_fwd_params &, cudaStream_t) [with Kernel_traits=Flash_fwd_kernel_traits<512, 32, 32, 2, false, false, cutlass::half_t, Flash_kernel_traits<512, 32, 32, 2, cutlass::half_t>>, Is_dropout=true, Is_causal=true]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(264): here
            instantiation of "void run_mha_fwd_hdim512<T>(Flash_fwd_params &, cudaStream_t) [with T=cutlass::half_t]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_hdim512_fp16_sm80.cu(8): here

/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_kernel.h(275): warning: variable "tQrQ" was declared but never referenced
          detected during:
            instantiation of "void flash::compute_attn_1rowblock<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &, int, int, int) [with Kernel_traits=Flash_fwd_kernel_traits<512, 32, 32, 2, false, false, cutlass::half_t, Flash_kernel_traits<512, 32, 32, 2, cutlass::half_t>>, Is_dropout=true, Is_causal=true, Is_even_N=true, Is_even_K=true, Return_softmax=true, Params=Flash_fwd_params]" 
(588): here
            instantiation of "void flash::compute_attn<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &) [with Kernel_traits=Flash_fwd_kernel_traits<512, 32, 32, 2, false, false, cutlass::half_t, Flash_kernel_traits<512, 32, 32, 2, cutlass::half_t>>, Is_dropout=true, Is_causal=true, Is_even_N=true, Is_even_K=true, Return_softmax=true, Params=Flash_fwd_params]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(15): here
            instantiation of "void flash_fwd_kernel<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax>(Flash_fwd_params) [with Kernel_traits=Flash_fwd_kernel_traits<512, 32, 32, 2, false, false, cutlass::half_t, Flash_kernel_traits<512, 32, 32, 2, cutlass::half_t>>, Is_dropout=true, Is_causal=true, Is_even_N=true, Is_even_K=true, Return_softmax=true]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(34): here
            instantiation of "void run_flash_fwd<Kernel_traits,Is_dropout,Is_causal>(Flash_fwd_params &, cudaStream_t) [with Kernel_traits=Flash_fwd_kernel_traits<512, 32, 32, 2, false, false, cutlass::half_t, Flash_kernel_traits<512, 32, 32, 2, cutlass::half_t>>, Is_dropout=true, Is_causal=true]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(264): here
            instantiation of "void run_mha_fwd_hdim512<T>(Flash_fwd_params &, cudaStream_t) [with T=cutlass::half_t]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_hdim512_fp16_sm80.cu(8): here

/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_kernel.h(140): warning: variable "MMA_M" was declared but never referenced
          detected during:
            instantiation of "void flash::compute_attn_1rowblock<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &, int, int, int) [with Kernel_traits=Flash_fwd_kernel_traits<512, 32, 32, 2, false, false, cutlass::half_t, Flash_kernel_traits<512, 32, 32, 2, cutlass::half_t>>, Is_dropout=true, Is_causal=true, Is_even_N=true, Is_even_K=true, Return_softmax=false, Params=Flash_fwd_params]" 
(588): here
            instantiation of "void flash::compute_attn<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &) [with Kernel_traits=Flash_fwd_kernel_traits<512, 32, 32, 2, false, false, cutlass::half_t, Flash_kernel_traits<512, 32, 32, 2, cutlass::half_t>>, Is_dropout=true, Is_causal=true, Is_even_N=true, Is_even_K=true, Return_softmax=false, Params=Flash_fwd_params]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(15): here
            instantiation of "void flash_fwd_kernel<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax>(Flash_fwd_params) [with Kernel_traits=Flash_fwd_kernel_traits<512, 32, 32, 2, false, false, cutlass::half_t, Flash_kernel_traits<512, 32, 32, 2, cutlass::half_t>>, Is_dropout=true, Is_causal=true, Is_even_N=true, Is_even_K=true, Return_softmax=false]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(34): here
            instantiation of "void run_flash_fwd<Kernel_traits,Is_dropout,Is_causal>(Flash_fwd_params &, cudaStream_t) [with Kernel_traits=Flash_fwd_kernel_traits<512, 32, 32, 2, false, false, cutlass::half_t, Flash_kernel_traits<512, 32, 32, 2, cutlass::half_t>>, Is_dropout=true, Is_causal=true]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(264): here
            instantiation of "void run_mha_fwd_hdim512<T>(Flash_fwd_params &, cudaStream_t) [with T=cutlass::half_t]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_hdim512_fp16_sm80.cu(8): here

/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_kernel.h(275): warning: variable "tQrQ" was declared but never referenced
          detected during:
            instantiation of "void flash::compute_attn_1rowblock<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &, int, int, int) [with Kernel_traits=Flash_fwd_kernel_traits<512, 32, 32, 2, false, false, cutlass::half_t, Flash_kernel_traits<512, 32, 32, 2, cutlass::half_t>>, Is_dropout=true, Is_causal=true, Is_even_N=true, Is_even_K=true, Return_softmax=false, Params=Flash_fwd_params]" 
(588): here
            instantiation of "void flash::compute_attn<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &) [with Kernel_traits=Flash_fwd_kernel_traits<512, 32, 32, 2, false, false, cutlass::half_t, Flash_kernel_traits<512, 32, 32, 2, cutlass::half_t>>, Is_dropout=true, Is_causal=true, Is_even_N=true, Is_even_K=true, Return_softmax=false, Params=Flash_fwd_params]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(15): here
            instantiation of "void flash_fwd_kernel<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax>(Flash_fwd_params) [with Kernel_traits=Flash_fwd_kernel_traits<512, 32, 32, 2, false, false, cutlass::half_t, Flash_kernel_traits<512, 32, 32, 2, cutlass::half_t>>, Is_dropout=true, Is_causal=true, Is_even_N=true, Is_even_K=true, Return_softmax=false]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(34): here
            instantiation of "void run_flash_fwd<Kernel_traits,Is_dropout,Is_causal>(Flash_fwd_params &, cudaStream_t) [with Kernel_traits=Flash_fwd_kernel_traits<512, 32, 32, 2, false, false, cutlass::half_t, Flash_kernel_traits<512, 32, 32, 2, cutlass::half_t>>, Is_dropout=true, Is_causal=true]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(264): here
            instantiation of "void run_mha_fwd_hdim512<T>(Flash_fwd_params &, cudaStream_t) [with T=cutlass::half_t]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_hdim512_fp16_sm80.cu(8): here

/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_kernel.h(140): warning: variable "MMA_M" was declared but never referenced
          detected during:
            instantiation of "void flash::compute_attn_1rowblock<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &, int, int, int) [with Kernel_traits=Flash_fwd_kernel_traits<512, 32, 32, 2, false, false, cutlass::half_t, Flash_kernel_traits<512, 32, 32, 2, cutlass::half_t>>, Is_dropout=true, Is_causal=true, Is_even_N=true, Is_even_K=false, Return_softmax=true, Params=Flash_fwd_params]" 
(588): here
            instantiation of "void flash::compute_attn<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &) [with Kernel_traits=Flash_fwd_kernel_traits<512, 32, 32, 2, false, false, cutlass::half_t, Flash_kernel_traits<512, 32, 32, 2, cutlass::half_t>>, Is_dropout=true, Is_causal=true, Is_even_N=true, Is_even_K=false, Return_softmax=true, Params=Flash_fwd_params]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(15): here
            instantiation of "void flash_fwd_kernel<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax>(Flash_fwd_params) [with Kernel_traits=Flash_fwd_kernel_traits<512, 32, 32, 2, false, false, cutlass::half_t, Flash_kernel_traits<512, 32, 32, 2, cutlass::half_t>>, Is_dropout=true, Is_causal=true, Is_even_N=true, Is_even_K=false, Return_softmax=true]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(34): here
            instantiation of "void run_flash_fwd<Kernel_traits,Is_dropout,Is_causal>(Flash_fwd_params &, cudaStream_t) [with Kernel_traits=Flash_fwd_kernel_traits<512, 32, 32, 2, false, false, cutlass::half_t, Flash_kernel_traits<512, 32, 32, 2, cutlass::half_t>>, Is_dropout=true, Is_causal=true]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(264): here
            instantiation of "void run_mha_fwd_hdim512<T>(Flash_fwd_params &, cudaStream_t) [with T=cutlass::half_t]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_hdim512_fp16_sm80.cu(8): here

/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_kernel.h(275): warning: variable "tQrQ" was declared but never referenced
          detected during:
            instantiation of "void flash::compute_attn_1rowblock<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &, int, int, int) [with Kernel_traits=Flash_fwd_kernel_traits<512, 32, 32, 2, false, false, cutlass::half_t, Flash_kernel_traits<512, 32, 32, 2, cutlass::half_t>>, Is_dropout=true, Is_causal=true, Is_even_N=true, Is_even_K=false, Return_softmax=true, Params=Flash_fwd_params]" 
(588): here
            instantiation of "void flash::compute_attn<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &) [with Kernel_traits=Flash_fwd_kernel_traits<512, 32, 32, 2, false, false, cutlass::half_t, Flash_kernel_traits<512, 32, 32, 2, cutlass::half_t>>, Is_dropout=true, Is_causal=true, Is_even_N=true, Is_even_K=false, Return_softmax=true, Params=Flash_fwd_params]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(15): here
            instantiation of "void flash_fwd_kernel<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax>(Flash_fwd_params) [with Kernel_traits=Flash_fwd_kernel_traits<512, 32, 32, 2, false, false, cutlass::half_t, Flash_kernel_traits<512, 32, 32, 2, cutlass::half_t>>, Is_dropout=true, Is_causal=true, Is_even_N=true, Is_even_K=false, Return_softmax=true]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(34): here
            instantiation of "void run_flash_fwd<Kernel_traits,Is_dropout,Is_causal>(Flash_fwd_params &, cudaStream_t) [with Kernel_traits=Flash_fwd_kernel_traits<512, 32, 32, 2, false, false, cutlass::half_t, Flash_kernel_traits<512, 32, 32, 2, cutlass::half_t>>, Is_dropout=true, Is_causal=true]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(264): here
            instantiation of "void run_mha_fwd_hdim512<T>(Flash_fwd_params &, cudaStream_t) [with T=cutlass::half_t]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_hdim512_fp16_sm80.cu(8): here

/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_kernel.h(140): warning: variable "MMA_M" was declared but never referenced
          detected during:
            instantiation of "void flash::compute_attn_1rowblock<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &, int, int, int) [with Kernel_traits=Flash_fwd_kernel_traits<512, 32, 32, 2, false, false, cutlass::half_t, Flash_kernel_traits<512, 32, 32, 2, cutlass::half_t>>, Is_dropout=true, Is_causal=true, Is_even_N=true, Is_even_K=false, Return_softmax=false, Params=Flash_fwd_params]" 
(588): here
            instantiation of "void flash::compute_attn<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &) [with Kernel_traits=Flash_fwd_kernel_traits<512, 32, 32, 2, false, false, cutlass::half_t, Flash_kernel_traits<512, 32, 32, 2, cutlass::half_t>>, Is_dropout=true, Is_causal=true, Is_even_N=true, Is_even_K=false, Return_softmax=false, Params=Flash_fwd_params]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(15): here
            instantiation of "void flash_fwd_kernel<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax>(Flash_fwd_params) [with Kernel_traits=Flash_fwd_kernel_traits<512, 32, 32, 2, false, false, cutlass::half_t, Flash_kernel_traits<512, 32, 32, 2, cutlass::half_t>>, Is_dropout=true, Is_causal=true, Is_even_N=true, Is_even_K=false, Return_softmax=false]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(34): here
            instantiation of "void run_flash_fwd<Kernel_traits,Is_dropout,Is_causal>(Flash_fwd_params &, cudaStream_t) [with Kernel_traits=Flash_fwd_kernel_traits<512, 32, 32, 2, false, false, cutlass::half_t, Flash_kernel_traits<512, 32, 32, 2, cutlass::half_t>>, Is_dropout=true, Is_causal=true]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(264): here
            instantiation of "void run_mha_fwd_hdim512<T>(Flash_fwd_params &, cudaStream_t) [with T=cutlass::half_t]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_hdim512_fp16_sm80.cu(8): here

/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_kernel.h(275): warning: variable "tQrQ" was declared but never referenced
          detected during:
            instantiation of "void flash::compute_attn_1rowblock<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &, int, int, int) [with Kernel_traits=Flash_fwd_kernel_traits<512, 32, 32, 2, false, false, cutlass::half_t, Flash_kernel_traits<512, 32, 32, 2, cutlass::half_t>>, Is_dropout=true, Is_causal=true, Is_even_N=true, Is_even_K=false, Return_softmax=false, Params=Flash_fwd_params]" 
(588): here
            instantiation of "void flash::compute_attn<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &) [with Kernel_traits=Flash_fwd_kernel_traits<512, 32, 32, 2, false, false, cutlass::half_t, Flash_kernel_traits<512, 32, 32, 2, cutlass::half_t>>, Is_dropout=true, Is_causal=true, Is_even_N=true, Is_even_K=false, Return_softmax=false, Params=Flash_fwd_params]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(15): here
            instantiation of "void flash_fwd_kernel<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax>(Flash_fwd_params) [with Kernel_traits=Flash_fwd_kernel_traits<512, 32, 32, 2, false, false, cutlass::half_t, Flash_kernel_traits<512, 32, 32, 2, cutlass::half_t>>, Is_dropout=true, Is_causal=true, Is_even_N=true, Is_even_K=false, Return_softmax=false]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(34): here
            instantiation of "void run_flash_fwd<Kernel_traits,Is_dropout,Is_causal>(Flash_fwd_params &, cudaStream_t) [with Kernel_traits=Flash_fwd_kernel_traits<512, 32, 32, 2, false, false, cutlass::half_t, Flash_kernel_traits<512, 32, 32, 2, cutlass::half_t>>, Is_dropout=true, Is_causal=true]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(264): here
            instantiation of "void run_mha_fwd_hdim512<T>(Flash_fwd_params &, cudaStream_t) [with T=cutlass::half_t]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_hdim512_fp16_sm80.cu(8): here

/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_kernel.h(140): warning: variable "MMA_M" was declared but never referenced
          detected during:
            instantiation of "void flash::compute_attn_1rowblock<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &, int, int, int) [with Kernel_traits=Flash_fwd_kernel_traits<512, 32, 32, 2, false, false, cutlass::half_t, Flash_kernel_traits<512, 32, 32, 2, cutlass::half_t>>, Is_dropout=true, Is_causal=true, Is_even_N=false, Is_even_K=true, Return_softmax=true, Params=Flash_fwd_params]" 
(588): here
            instantiation of "void flash::compute_attn<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &) [with Kernel_traits=Flash_fwd_kernel_traits<512, 32, 32, 2, false, false, cutlass::half_t, Flash_kernel_traits<512, 32, 32, 2, cutlass::half_t>>, Is_dropout=true, Is_causal=true, Is_even_N=false, Is_even_K=true, Return_softmax=true, Params=Flash_fwd_params]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(15): here
            instantiation of "void flash_fwd_kernel<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax>(Flash_fwd_params) [with Kernel_traits=Flash_fwd_kernel_traits<512, 32, 32, 2, false, false, cutlass::half_t, Flash_kernel_traits<512, 32, 32, 2, cutlass::half_t>>, Is_dropout=true, Is_causal=true, Is_even_N=false, Is_even_K=true, Return_softmax=true]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(34): here
            instantiation of "void run_flash_fwd<Kernel_traits,Is_dropout,Is_causal>(Flash_fwd_params &, cudaStream_t) [with Kernel_traits=Flash_fwd_kernel_traits<512, 32, 32, 2, false, false, cutlass::half_t, Flash_kernel_traits<512, 32, 32, 2, cutlass::half_t>>, Is_dropout=true, Is_causal=true]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(264): here
            instantiation of "void run_mha_fwd_hdim512<T>(Flash_fwd_params &, cudaStream_t) [with T=cutlass::half_t]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_hdim512_fp16_sm80.cu(8): here

/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_kernel.h(275): warning: variable "tQrQ" was declared but never referenced
          detected during:
            instantiation of "void flash::compute_attn_1rowblock<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &, int, int, int) [with Kernel_traits=Flash_fwd_kernel_traits<512, 32, 32, 2, false, false, cutlass::half_t, Flash_kernel_traits<512, 32, 32, 2, cutlass::half_t>>, Is_dropout=true, Is_causal=true, Is_even_N=false, Is_even_K=true, Return_softmax=true, Params=Flash_fwd_params]" 
(588): here
            instantiation of "void flash::compute_attn<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &) [with Kernel_traits=Flash_fwd_kernel_traits<512, 32, 32, 2, false, false, cutlass::half_t, Flash_kernel_traits<512, 32, 32, 2, cutlass::half_t>>, Is_dropout=true, Is_causal=true, Is_even_N=false, Is_even_K=true, Return_softmax=true, Params=Flash_fwd_params]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(15): here
            instantiation of "void flash_fwd_kernel<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax>(Flash_fwd_params) [with Kernel_traits=Flash_fwd_kernel_traits<512, 32, 32, 2, false, false, cutlass::half_t, Flash_kernel_traits<512, 32, 32, 2, cutlass::half_t>>, Is_dropout=true, Is_causal=true, Is_even_N=false, Is_even_K=true, Return_softmax=true]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(34): here
            instantiation of "void run_flash_fwd<Kernel_traits,Is_dropout,Is_causal>(Flash_fwd_params &, cudaStream_t) [with Kernel_traits=Flash_fwd_kernel_traits<512, 32, 32, 2, false, false, cutlass::half_t, Flash_kernel_traits<512, 32, 32, 2, cutlass::half_t>>, Is_dropout=true, Is_causal=true]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(264): here
            instantiation of "void run_mha_fwd_hdim512<T>(Flash_fwd_params &, cudaStream_t) [with T=cutlass::half_t]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_hdim512_fp16_sm80.cu(8): here

/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_kernel.h(140): warning: variable "MMA_M" was declared but never referenced
          detected during:
            instantiation of "void flash::compute_attn_1rowblock<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &, int, int, int) [with Kernel_traits=Flash_fwd_kernel_traits<512, 32, 32, 2, false, false, cutlass::half_t, Flash_kernel_traits<512, 32, 32, 2, cutlass::half_t>>, Is_dropout=true, Is_causal=true, Is_even_N=false, Is_even_K=true, Return_softmax=false, Params=Flash_fwd_params]" 
(588): here
            instantiation of "void flash::compute_attn<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &) [with Kernel_traits=Flash_fwd_kernel_traits<512, 32, 32, 2, false, false, cutlass::half_t, Flash_kernel_traits<512, 32, 32, 2, cutlass::half_t>>, Is_dropout=true, Is_causal=true, Is_even_N=false, Is_even_K=true, Return_softmax=false, Params=Flash_fwd_params]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(15): here
            instantiation of "void flash_fwd_kernel<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax>(Flash_fwd_params) [with Kernel_traits=Flash_fwd_kernel_traits<512, 32, 32, 2, false, false, cutlass::half_t, Flash_kernel_traits<512, 32, 32, 2, cutlass::half_t>>, Is_dropout=true, Is_causal=true, Is_even_N=false, Is_even_K=true, Return_softmax=false]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(34): here
            instantiation of "void run_flash_fwd<Kernel_traits,Is_dropout,Is_causal>(Flash_fwd_params &, cudaStream_t) [with Kernel_traits=Flash_fwd_kernel_traits<512, 32, 32, 2, false, false, cutlass::half_t, Flash_kernel_traits<512, 32, 32, 2, cutlass::half_t>>, Is_dropout=true, Is_causal=true]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(264): here
            instantiation of "void run_mha_fwd_hdim512<T>(Flash_fwd_params &, cudaStream_t) [with T=cutlass::half_t]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_hdim512_fp16_sm80.cu(8): here

/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_kernel.h(275): warning: variable "tQrQ" was declared but never referenced
          detected during:
            instantiation of "void flash::compute_attn_1rowblock<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &, int, int, int) [with Kernel_traits=Flash_fwd_kernel_traits<512, 32, 32, 2, false, false, cutlass::half_t, Flash_kernel_traits<512, 32, 32, 2, cutlass::half_t>>, Is_dropout=true, Is_causal=true, Is_even_N=false, Is_even_K=true, Return_softmax=false, Params=Flash_fwd_params]" 
(588): here
            instantiation of "void flash::compute_attn<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &) [with Kernel_traits=Flash_fwd_kernel_traits<512, 32, 32, 2, false, false, cutlass::half_t, Flash_kernel_traits<512, 32, 32, 2, cutlass::half_t>>, Is_dropout=true, Is_causal=true, Is_even_N=false, Is_even_K=true, Return_softmax=false, Params=Flash_fwd_params]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(15): here
            instantiation of "void flash_fwd_kernel<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax>(Flash_fwd_params) [with Kernel_traits=Flash_fwd_kernel_traits<512, 32, 32, 2, false, false, cutlass::half_t, Flash_kernel_traits<512, 32, 32, 2, cutlass::half_t>>, Is_dropout=true, Is_causal=true, Is_even_N=false, Is_even_K=true, Return_softmax=false]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(34): here
            instantiation of "void run_flash_fwd<Kernel_traits,Is_dropout,Is_causal>(Flash_fwd_params &, cudaStream_t) [with Kernel_traits=Flash_fwd_kernel_traits<512, 32, 32, 2, false, false, cutlass::half_t, Flash_kernel_traits<512, 32, 32, 2, cutlass::half_t>>, Is_dropout=true, Is_causal=true]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(264): here
            instantiation of "void run_mha_fwd_hdim512<T>(Flash_fwd_params &, cudaStream_t) [with T=cutlass::half_t]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_hdim512_fp16_sm80.cu(8): here

/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_kernel.h(140): warning: variable "MMA_M" was declared but never referenced
          detected during:
            instantiation of "void flash::compute_attn_1rowblock<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &, int, int, int) [with Kernel_traits=Flash_fwd_kernel_traits<512, 32, 32, 2, false, false, cutlass::half_t, Flash_kernel_traits<512, 32, 32, 2, cutlass::half_t>>, Is_dropout=true, Is_causal=true, Is_even_N=false, Is_even_K=false, Return_softmax=true, Params=Flash_fwd_params]" 
(588): here
            instantiation of "void flash::compute_attn<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &) [with Kernel_traits=Flash_fwd_kernel_traits<512, 32, 32, 2, false, false, cutlass::half_t, Flash_kernel_traits<512, 32, 32, 2, cutlass::half_t>>, Is_dropout=true, Is_causal=true, Is_even_N=false, Is_even_K=false, Return_softmax=true, Params=Flash_fwd_params]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(15): here
            instantiation of "void flash_fwd_kernel<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax>(Flash_fwd_params) [with Kernel_traits=Flash_fwd_kernel_traits<512, 32, 32, 2, false, false, cutlass::half_t, Flash_kernel_traits<512, 32, 32, 2, cutlass::half_t>>, Is_dropout=true, Is_causal=true, Is_even_N=false, Is_even_K=false, Return_softmax=true]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(34): here
            instantiation of "void run_flash_fwd<Kernel_traits,Is_dropout,Is_causal>(Flash_fwd_params &, cudaStream_t) [with Kernel_traits=Flash_fwd_kernel_traits<512, 32, 32, 2, false, false, cutlass::half_t, Flash_kernel_traits<512, 32, 32, 2, cutlass::half_t>>, Is_dropout=true, Is_causal=true]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(264): here
            instantiation of "void run_mha_fwd_hdim512<T>(Flash_fwd_params &, cudaStream_t) [with T=cutlass::half_t]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_hdim512_fp16_sm80.cu(8): here

/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_kernel.h(275): warning: variable "tQrQ" was declared but never referenced
          detected during:
            instantiation of "void flash::compute_attn_1rowblock<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &, int, int, int) [with Kernel_traits=Flash_fwd_kernel_traits<512, 32, 32, 2, false, false, cutlass::half_t, Flash_kernel_traits<512, 32, 32, 2, cutlass::half_t>>, Is_dropout=true, Is_causal=true, Is_even_N=false, Is_even_K=false, Return_softmax=true, Params=Flash_fwd_params]" 
(588): here
            instantiation of "void flash::compute_attn<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &) [with Kernel_traits=Flash_fwd_kernel_traits<512, 32, 32, 2, false, false, cutlass::half_t, Flash_kernel_traits<512, 32, 32, 2, cutlass::half_t>>, Is_dropout=true, Is_causal=true, Is_even_N=false, Is_even_K=false, Return_softmax=true, Params=Flash_fwd_params]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(15): here
            instantiation of "void flash_fwd_kernel<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax>(Flash_fwd_params) [with Kernel_traits=Flash_fwd_kernel_traits<512, 32, 32, 2, false, false, cutlass::half_t, Flash_kernel_traits<512, 32, 32, 2, cutlass::half_t>>, Is_dropout=true, Is_causal=true, Is_even_N=false, Is_even_K=false, Return_softmax=true]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(34): here
            instantiation of "void run_flash_fwd<Kernel_traits,Is_dropout,Is_causal>(Flash_fwd_params &, cudaStream_t) [with Kernel_traits=Flash_fwd_kernel_traits<512, 32, 32, 2, false, false, cutlass::half_t, Flash_kernel_traits<512, 32, 32, 2, cutlass::half_t>>, Is_dropout=true, Is_causal=true]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(264): here
            instantiation of "void run_mha_fwd_hdim512<T>(Flash_fwd_params &, cudaStream_t) [with T=cutlass::half_t]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_hdim512_fp16_sm80.cu(8): here

/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_kernel.h(140): warning: variable "MMA_M" was declared but never referenced
          detected during:
            instantiation of "void flash::compute_attn_1rowblock<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &, int, int, int) [with Kernel_traits=Flash_fwd_kernel_traits<512, 32, 32, 2, false, false, cutlass::half_t, Flash_kernel_traits<512, 32, 32, 2, cutlass::half_t>>, Is_dropout=true, Is_causal=true, Is_even_N=false, Is_even_K=false, Return_softmax=false, Params=Flash_fwd_params]" 
(588): here
            instantiation of "void flash::compute_attn<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &) [with Kernel_traits=Flash_fwd_kernel_traits<512, 32, 32, 2, false, false, cutlass::half_t, Flash_kernel_traits<512, 32, 32, 2, cutlass::half_t>>, Is_dropout=true, Is_causal=true, Is_even_N=false, Is_even_K=false, Return_softmax=false, Params=Flash_fwd_params]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(15): here
            instantiation of "void flash_fwd_kernel<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax>(Flash_fwd_params) [with Kernel_traits=Flash_fwd_kernel_traits<512, 32, 32, 2, false, false, cutlass::half_t, Flash_kernel_traits<512, 32, 32, 2, cutlass::half_t>>, Is_dropout=true, Is_causal=true, Is_even_N=false, Is_even_K=false, Return_softmax=false]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(34): here
            instantiation of "void run_flash_fwd<Kernel_traits,Is_dropout,Is_causal>(Flash_fwd_params &, cudaStream_t) [with Kernel_traits=Flash_fwd_kernel_traits<512, 32, 32, 2, false, false, cutlass::half_t, Flash_kernel_traits<512, 32, 32, 2, cutlass::half_t>>, Is_dropout=true, Is_causal=true]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(264): here
            instantiation of "void run_mha_fwd_hdim512<T>(Flash_fwd_params &, cudaStream_t) [with T=cutlass::half_t]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_hdim512_fp16_sm80.cu(8): here

/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_kernel.h(275): warning: variable "tQrQ" was declared but never referenced
          detected during:
            instantiation of "void flash::compute_attn_1rowblock<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &, int, int, int) [with Kernel_traits=Flash_fwd_kernel_traits<512, 32, 32, 2, false, false, cutlass::half_t, Flash_kernel_traits<512, 32, 32, 2, cutlass::half_t>>, Is_dropout=true, Is_causal=true, Is_even_N=false, Is_even_K=false, Return_softmax=false, Params=Flash_fwd_params]" 
(588): here
            instantiation of "void flash::compute_attn<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &) [with Kernel_traits=Flash_fwd_kernel_traits<512, 32, 32, 2, false, false, cutlass::half_t, Flash_kernel_traits<512, 32, 32, 2, cutlass::half_t>>, Is_dropout=true, Is_causal=true, Is_even_N=false, Is_even_K=false, Return_softmax=false, Params=Flash_fwd_params]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(15): here
            instantiation of "void flash_fwd_kernel<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax>(Flash_fwd_params) [with Kernel_traits=Flash_fwd_kernel_traits<512, 32, 32, 2, false, false, cutlass::half_t, Flash_kernel_traits<512, 32, 32, 2, cutlass::half_t>>, Is_dropout=true, Is_causal=true, Is_even_N=false, Is_even_K=false, Return_softmax=false]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(34): here
            instantiation of "void run_flash_fwd<Kernel_traits,Is_dropout,Is_causal>(Flash_fwd_params &, cudaStream_t) [with Kernel_traits=Flash_fwd_kernel_traits<512, 32, 32, 2, false, false, cutlass::half_t, Flash_kernel_traits<512, 32, 32, 2, cutlass::half_t>>, Is_dropout=true, Is_causal=true]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(264): here
            instantiation of "void run_mha_fwd_hdim512<T>(Flash_fwd_params &, cudaStream_t) [with T=cutlass::half_t]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_hdim512_fp16_sm80.cu(8): here

/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_kernel.h(140): warning: variable "MMA_M" was declared but never referenced
          detected during:
            instantiation of "void flash::compute_attn_1rowblock<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &, int, int, int) [with Kernel_traits=Flash_fwd_kernel_traits<512, 64, 32, 4, false, false, cutlass::half_t, Flash_kernel_traits<512, 64, 32, 4, cutlass::half_t>>, Is_dropout=true, Is_causal=false, Is_even_N=true, Is_even_K=true, Return_softmax=true, Params=Flash_fwd_params]" 
(588): here
            instantiation of "void flash::compute_attn<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &) [with Kernel_traits=Flash_fwd_kernel_traits<512, 64, 32, 4, false, false, cutlass::half_t, Flash_kernel_traits<512, 64, 32, 4, cutlass::half_t>>, Is_dropout=true, Is_causal=false, Is_even_N=true, Is_even_K=true, Return_softmax=true, Params=Flash_fwd_params]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(15): here
            instantiation of "void flash_fwd_kernel<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax>(Flash_fwd_params) [with Kernel_traits=Flash_fwd_kernel_traits<512, 64, 32, 4, false, false, cutlass::half_t, Flash_kernel_traits<512, 64, 32, 4, cutlass::half_t>>, Is_dropout=true, Is_causal=false, Is_even_N=true, Is_even_K=true, Return_softmax=true]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(34): here
            instantiation of "void run_flash_fwd<Kernel_traits,Is_dropout,Is_causal>(Flash_fwd_params &, cudaStream_t) [with Kernel_traits=Flash_fwd_kernel_traits<512, 64, 32, 4, false, false, cutlass::half_t, Flash_kernel_traits<512, 64, 32, 4, cutlass::half_t>>, Is_dropout=true, Is_causal=false]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(264): here
            instantiation of "void run_mha_fwd_hdim512<T>(Flash_fwd_params &, cudaStream_t) [with T=cutlass::half_t]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_hdim512_fp16_sm80.cu(8): here

/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_kernel.h(275): warning: variable "tQrQ" was declared but never referenced
          detected during:
            instantiation of "void flash::compute_attn_1rowblock<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &, int, int, int) [with Kernel_traits=Flash_fwd_kernel_traits<512, 64, 32, 4, false, false, cutlass::half_t, Flash_kernel_traits<512, 64, 32, 4, cutlass::half_t>>, Is_dropout=true, Is_causal=false, Is_even_N=true, Is_even_K=true, Return_softmax=true, Params=Flash_fwd_params]" 
(588): here
            instantiation of "void flash::compute_attn<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &) [with Kernel_traits=Flash_fwd_kernel_traits<512, 64, 32, 4, false, false, cutlass::half_t, Flash_kernel_traits<512, 64, 32, 4, cutlass::half_t>>, Is_dropout=true, Is_causal=false, Is_even_N=true, Is_even_K=true, Return_softmax=true, Params=Flash_fwd_params]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(15): here
            instantiation of "void flash_fwd_kernel<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax>(Flash_fwd_params) [with Kernel_traits=Flash_fwd_kernel_traits<512, 64, 32, 4, false, false, cutlass::half_t, Flash_kernel_traits<512, 64, 32, 4, cutlass::half_t>>, Is_dropout=true, Is_causal=false, Is_even_N=true, Is_even_K=true, Return_softmax=true]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(34): here
            instantiation of "void run_flash_fwd<Kernel_traits,Is_dropout,Is_causal>(Flash_fwd_params &, cudaStream_t) [with Kernel_traits=Flash_fwd_kernel_traits<512, 64, 32, 4, false, false, cutlass::half_t, Flash_kernel_traits<512, 64, 32, 4, cutlass::half_t>>, Is_dropout=true, Is_causal=false]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(264): here
            instantiation of "void run_mha_fwd_hdim512<T>(Flash_fwd_params &, cudaStream_t) [with T=cutlass::half_t]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_hdim512_fp16_sm80.cu(8): here

/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_kernel.h(140): warning: variable "MMA_M" was declared but never referenced
          detected during:
            instantiation of "void flash::compute_attn_1rowblock<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &, int, int, int) [with Kernel_traits=Flash_fwd_kernel_traits<512, 64, 32, 4, false, false, cutlass::half_t, Flash_kernel_traits<512, 64, 32, 4, cutlass::half_t>>, Is_dropout=true, Is_causal=false, Is_even_N=true, Is_even_K=true, Return_softmax=false, Params=Flash_fwd_params]" 
(588): here
            instantiation of "void flash::compute_attn<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &) [with Kernel_traits=Flash_fwd_kernel_traits<512, 64, 32, 4, false, false, cutlass::half_t, Flash_kernel_traits<512, 64, 32, 4, cutlass::half_t>>, Is_dropout=true, Is_causal=false, Is_even_N=true, Is_even_K=true, Return_softmax=false, Params=Flash_fwd_params]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(15): here
            instantiation of "void flash_fwd_kernel<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax>(Flash_fwd_params) [with Kernel_traits=Flash_fwd_kernel_traits<512, 64, 32, 4, false, false, cutlass::half_t, Flash_kernel_traits<512, 64, 32, 4, cutlass::half_t>>, Is_dropout=true, Is_causal=false, Is_even_N=true, Is_even_K=true, Return_softmax=false]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(34): here
            instantiation of "void run_flash_fwd<Kernel_traits,Is_dropout,Is_causal>(Flash_fwd_params &, cudaStream_t) [with Kernel_traits=Flash_fwd_kernel_traits<512, 64, 32, 4, false, false, cutlass::half_t, Flash_kernel_traits<512, 64, 32, 4, cutlass::half_t>>, Is_dropout=true, Is_causal=false]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(264): here
            instantiation of "void run_mha_fwd_hdim512<T>(Flash_fwd_params &, cudaStream_t) [with T=cutlass::half_t]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_hdim512_fp16_sm80.cu(8): here

/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_kernel.h(275): warning: variable "tQrQ" was declared but never referenced
          detected during:
            instantiation of "void flash::compute_attn_1rowblock<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &, int, int, int) [with Kernel_traits=Flash_fwd_kernel_traits<512, 64, 32, 4, false, false, cutlass::half_t, Flash_kernel_traits<512, 64, 32, 4, cutlass::half_t>>, Is_dropout=true, Is_causal=false, Is_even_N=true, Is_even_K=true, Return_softmax=false, Params=Flash_fwd_params]" 
(588): here
            instantiation of "void flash::compute_attn<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &) [with Kernel_traits=Flash_fwd_kernel_traits<512, 64, 32, 4, false, false, cutlass::half_t, Flash_kernel_traits<512, 64, 32, 4, cutlass::half_t>>, Is_dropout=true, Is_causal=false, Is_even_N=true, Is_even_K=true, Return_softmax=false, Params=Flash_fwd_params]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(15): here
            instantiation of "void flash_fwd_kernel<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax>(Flash_fwd_params) [with Kernel_traits=Flash_fwd_kernel_traits<512, 64, 32, 4, false, false, cutlass::half_t, Flash_kernel_traits<512, 64, 32, 4, cutlass::half_t>>, Is_dropout=true, Is_causal=false, Is_even_N=true, Is_even_K=true, Return_softmax=false]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(34): here
            instantiation of "void run_flash_fwd<Kernel_traits,Is_dropout,Is_causal>(Flash_fwd_params &, cudaStream_t) [with Kernel_traits=Flash_fwd_kernel_traits<512, 64, 32, 4, false, false, cutlass::half_t, Flash_kernel_traits<512, 64, 32, 4, cutlass::half_t>>, Is_dropout=true, Is_causal=false]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(264): here
            instantiation of "void run_mha_fwd_hdim512<T>(Flash_fwd_params &, cudaStream_t) [with T=cutlass::half_t]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_hdim512_fp16_sm80.cu(8): here

/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_kernel.h(140): warning: variable "MMA_M" was declared but never referenced
          detected during:
            instantiation of "void flash::compute_attn_1rowblock<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &, int, int, int) [with Kernel_traits=Flash_fwd_kernel_traits<512, 64, 32, 4, false, false, cutlass::half_t, Flash_kernel_traits<512, 64, 32, 4, cutlass::half_t>>, Is_dropout=true, Is_causal=false, Is_even_N=true, Is_even_K=false, Return_softmax=true, Params=Flash_fwd_params]" 
(588): here
            instantiation of "void flash::compute_attn<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &) [with Kernel_traits=Flash_fwd_kernel_traits<512, 64, 32, 4, false, false, cutlass::half_t, Flash_kernel_traits<512, 64, 32, 4, cutlass::half_t>>, Is_dropout=true, Is_causal=false, Is_even_N=true, Is_even_K=false, Return_softmax=true, Params=Flash_fwd_params]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(15): here
            instantiation of "void flash_fwd_kernel<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax>(Flash_fwd_params) [with Kernel_traits=Flash_fwd_kernel_traits<512, 64, 32, 4, false, false, cutlass::half_t, Flash_kernel_traits<512, 64, 32, 4, cutlass::half_t>>, Is_dropout=true, Is_causal=false, Is_even_N=true, Is_even_K=false, Return_softmax=true]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(34): here
            instantiation of "void run_flash_fwd<Kernel_traits,Is_dropout,Is_causal>(Flash_fwd_params &, cudaStream_t) [with Kernel_traits=Flash_fwd_kernel_traits<512, 64, 32, 4, false, false, cutlass::half_t, Flash_kernel_traits<512, 64, 32, 4, cutlass::half_t>>, Is_dropout=true, Is_causal=false]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(264): here
            instantiation of "void run_mha_fwd_hdim512<T>(Flash_fwd_params &, cudaStream_t) [with T=cutlass::half_t]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_hdim512_fp16_sm80.cu(8): here

/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_kernel.h(275): warning: variable "tQrQ" was declared but never referenced
          detected during:
            instantiation of "void flash::compute_attn_1rowblock<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &, int, int, int) [with Kernel_traits=Flash_fwd_kernel_traits<512, 64, 32, 4, false, false, cutlass::half_t, Flash_kernel_traits<512, 64, 32, 4, cutlass::half_t>>, Is_dropout=true, Is_causal=false, Is_even_N=true, Is_even_K=false, Return_softmax=true, Params=Flash_fwd_params]" 
(588): here
            instantiation of "void flash::compute_attn<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &) [with Kernel_traits=Flash_fwd_kernel_traits<512, 64, 32, 4, false, false, cutlass::half_t, Flash_kernel_traits<512, 64, 32, 4, cutlass::half_t>>, Is_dropout=true, Is_causal=false, Is_even_N=true, Is_even_K=false, Return_softmax=true, Params=Flash_fwd_params]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(15): here
            instantiation of "void flash_fwd_kernel<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax>(Flash_fwd_params) [with Kernel_traits=Flash_fwd_kernel_traits<512, 64, 32, 4, false, false, cutlass::half_t, Flash_kernel_traits<512, 64, 32, 4, cutlass::half_t>>, Is_dropout=true, Is_causal=false, Is_even_N=true, Is_even_K=false, Return_softmax=true]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(34): here
            instantiation of "void run_flash_fwd<Kernel_traits,Is_dropout,Is_causal>(Flash_fwd_params &, cudaStream_t) [with Kernel_traits=Flash_fwd_kernel_traits<512, 64, 32, 4, false, false, cutlass::half_t, Flash_kernel_traits<512, 64, 32, 4, cutlass::half_t>>, Is_dropout=true, Is_causal=false]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(264): here
            instantiation of "void run_mha_fwd_hdim512<T>(Flash_fwd_params &, cudaStream_t) [with T=cutlass::half_t]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_hdim512_fp16_sm80.cu(8): here

/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_kernel.h(140): warning: variable "MMA_M" was declared but never referenced
          detected during:
            instantiation of "void flash::compute_attn_1rowblock<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &, int, int, int) [with Kernel_traits=Flash_fwd_kernel_traits<512, 64, 32, 4, false, false, cutlass::half_t, Flash_kernel_traits<512, 64, 32, 4, cutlass::half_t>>, Is_dropout=true, Is_causal=false, Is_even_N=true, Is_even_K=false, Return_softmax=false, Params=Flash_fwd_params]" 
(588): here
            instantiation of "void flash::compute_attn<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &) [with Kernel_traits=Flash_fwd_kernel_traits<512, 64, 32, 4, false, false, cutlass::half_t, Flash_kernel_traits<512, 64, 32, 4, cutlass::half_t>>, Is_dropout=true, Is_causal=false, Is_even_N=true, Is_even_K=false, Return_softmax=false, Params=Flash_fwd_params]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(15): here
            instantiation of "void flash_fwd_kernel<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax>(Flash_fwd_params) [with Kernel_traits=Flash_fwd_kernel_traits<512, 64, 32, 4, false, false, cutlass::half_t, Flash_kernel_traits<512, 64, 32, 4, cutlass::half_t>>, Is_dropout=true, Is_causal=false, Is_even_N=true, Is_even_K=false, Return_softmax=false]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(34): here
            instantiation of "void run_flash_fwd<Kernel_traits,Is_dropout,Is_causal>(Flash_fwd_params &, cudaStream_t) [with Kernel_traits=Flash_fwd_kernel_traits<512, 64, 32, 4, false, false, cutlass::half_t, Flash_kernel_traits<512, 64, 32, 4, cutlass::half_t>>, Is_dropout=true, Is_causal=false]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(264): here
            instantiation of "void run_mha_fwd_hdim512<T>(Flash_fwd_params &, cudaStream_t) [with T=cutlass::half_t]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_hdim512_fp16_sm80.cu(8): here

/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_kernel.h(275): warning: variable "tQrQ" was declared but never referenced
          detected during:
            instantiation of "void flash::compute_attn_1rowblock<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &, int, int, int) [with Kernel_traits=Flash_fwd_kernel_traits<512, 64, 32, 4, false, false, cutlass::half_t, Flash_kernel_traits<512, 64, 32, 4, cutlass::half_t>>, Is_dropout=true, Is_causal=false, Is_even_N=true, Is_even_K=false, Return_softmax=false, Params=Flash_fwd_params]" 
(588): here
            instantiation of "void flash::compute_attn<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &) [with Kernel_traits=Flash_fwd_kernel_traits<512, 64, 32, 4, false, false, cutlass::half_t, Flash_kernel_traits<512, 64, 32, 4, cutlass::half_t>>, Is_dropout=true, Is_causal=false, Is_even_N=true, Is_even_K=false, Return_softmax=false, Params=Flash_fwd_params]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(15): here
            instantiation of "void flash_fwd_kernel<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax>(Flash_fwd_params) [with Kernel_traits=Flash_fwd_kernel_traits<512, 64, 32, 4, false, false, cutlass::half_t, Flash_kernel_traits<512, 64, 32, 4, cutlass::half_t>>, Is_dropout=true, Is_causal=false, Is_even_N=true, Is_even_K=false, Return_softmax=false]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(34): here
            instantiation of "void run_flash_fwd<Kernel_traits,Is_dropout,Is_causal>(Flash_fwd_params &, cudaStream_t) [with Kernel_traits=Flash_fwd_kernel_traits<512, 64, 32, 4, false, false, cutlass::half_t, Flash_kernel_traits<512, 64, 32, 4, cutlass::half_t>>, Is_dropout=true, Is_causal=false]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(264): here
            instantiation of "void run_mha_fwd_hdim512<T>(Flash_fwd_params &, cudaStream_t) [with T=cutlass::half_t]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_hdim512_fp16_sm80.cu(8): here

/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_kernel.h(140): warning: variable "MMA_M" was declared but never referenced
          detected during:
            instantiation of "void flash::compute_attn_1rowblock<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &, int, int, int) [with Kernel_traits=Flash_fwd_kernel_traits<512, 64, 32, 4, false, false, cutlass::half_t, Flash_kernel_traits<512, 64, 32, 4, cutlass::half_t>>, Is_dropout=true, Is_causal=false, Is_even_N=false, Is_even_K=true, Return_softmax=true, Params=Flash_fwd_params]" 
(588): here
            instantiation of "void flash::compute_attn<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &) [with Kernel_traits=Flash_fwd_kernel_traits<512, 64, 32, 4, false, false, cutlass::half_t, Flash_kernel_traits<512, 64, 32, 4, cutlass::half_t>>, Is_dropout=true, Is_causal=false, Is_even_N=false, Is_even_K=true, Return_softmax=true, Params=Flash_fwd_params]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(15): here
            instantiation of "void flash_fwd_kernel<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax>(Flash_fwd_params) [with Kernel_traits=Flash_fwd_kernel_traits<512, 64, 32, 4, false, false, cutlass::half_t, Flash_kernel_traits<512, 64, 32, 4, cutlass::half_t>>, Is_dropout=true, Is_causal=false, Is_even_N=false, Is_even_K=true, Return_softmax=true]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(34): here
            instantiation of "void run_flash_fwd<Kernel_traits,Is_dropout,Is_causal>(Flash_fwd_params &, cudaStream_t) [with Kernel_traits=Flash_fwd_kernel_traits<512, 64, 32, 4, false, false, cutlass::half_t, Flash_kernel_traits<512, 64, 32, 4, cutlass::half_t>>, Is_dropout=true, Is_causal=false]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(264): here
            instantiation of "void run_mha_fwd_hdim512<T>(Flash_fwd_params &, cudaStream_t) [with T=cutlass::half_t]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_hdim512_fp16_sm80.cu(8): here

/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_kernel.h(275): warning: variable "tQrQ" was declared but never referenced
          detected during:
            instantiation of "void flash::compute_attn_1rowblock<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &, int, int, int) [with Kernel_traits=Flash_fwd_kernel_traits<512, 64, 32, 4, false, false, cutlass::half_t, Flash_kernel_traits<512, 64, 32, 4, cutlass::half_t>>, Is_dropout=true, Is_causal=false, Is_even_N=false, Is_even_K=true, Return_softmax=true, Params=Flash_fwd_params]" 
(588): here
            instantiation of "void flash::compute_attn<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &) [with Kernel_traits=Flash_fwd_kernel_traits<512, 64, 32, 4, false, false, cutlass::half_t, Flash_kernel_traits<512, 64, 32, 4, cutlass::half_t>>, Is_dropout=true, Is_causal=false, Is_even_N=false, Is_even_K=true, Return_softmax=true, Params=Flash_fwd_params]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(15): here
            instantiation of "void flash_fwd_kernel<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax>(Flash_fwd_params) [with Kernel_traits=Flash_fwd_kernel_traits<512, 64, 32, 4, false, false, cutlass::half_t, Flash_kernel_traits<512, 64, 32, 4, cutlass::half_t>>, Is_dropout=true, Is_causal=false, Is_even_N=false, Is_even_K=true, Return_softmax=true]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(34): here
            instantiation of "void run_flash_fwd<Kernel_traits,Is_dropout,Is_causal>(Flash_fwd_params &, cudaStream_t) [with Kernel_traits=Flash_fwd_kernel_traits<512, 64, 32, 4, false, false, cutlass::half_t, Flash_kernel_traits<512, 64, 32, 4, cutlass::half_t>>, Is_dropout=true, Is_causal=false]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(264): here
            instantiation of "void run_mha_fwd_hdim512<T>(Flash_fwd_params &, cudaStream_t) [with T=cutlass::half_t]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_hdim512_fp16_sm80.cu(8): here

/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_kernel.h(140): warning: variable "MMA_M" was declared but never referenced
          detected during:
            instantiation of "void flash::compute_attn_1rowblock<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &, int, int, int) [with Kernel_traits=Flash_fwd_kernel_traits<512, 64, 32, 4, false, false, cutlass::half_t, Flash_kernel_traits<512, 64, 32, 4, cutlass::half_t>>, Is_dropout=true, Is_causal=false, Is_even_N=false, Is_even_K=true, Return_softmax=false, Params=Flash_fwd_params]" 
(588): here
            instantiation of "void flash::compute_attn<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &) [with Kernel_traits=Flash_fwd_kernel_traits<512, 64, 32, 4, false, false, cutlass::half_t, Flash_kernel_traits<512, 64, 32, 4, cutlass::half_t>>, Is_dropout=true, Is_causal=false, Is_even_N=false, Is_even_K=true, Return_softmax=false, Params=Flash_fwd_params]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(15): here
            instantiation of "void flash_fwd_kernel<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax>(Flash_fwd_params) [with Kernel_traits=Flash_fwd_kernel_traits<512, 64, 32, 4, false, false, cutlass::half_t, Flash_kernel_traits<512, 64, 32, 4, cutlass::half_t>>, Is_dropout=true, Is_causal=false, Is_even_N=false, Is_even_K=true, Return_softmax=false]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(34): here
            instantiation of "void run_flash_fwd<Kernel_traits,Is_dropout,Is_causal>(Flash_fwd_params &, cudaStream_t) [with Kernel_traits=Flash_fwd_kernel_traits<512, 64, 32, 4, false, false, cutlass::half_t, Flash_kernel_traits<512, 64, 32, 4, cutlass::half_t>>, Is_dropout=true, Is_causal=false]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(264): here
            instantiation of "void run_mha_fwd_hdim512<T>(Flash_fwd_params &, cudaStream_t) [with T=cutlass::half_t]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_hdim512_fp16_sm80.cu(8): here

/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_kernel.h(275): warning: variable "tQrQ" was declared but never referenced
          detected during:
            instantiation of "void flash::compute_attn_1rowblock<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &, int, int, int) [with Kernel_traits=Flash_fwd_kernel_traits<512, 64, 32, 4, false, false, cutlass::half_t, Flash_kernel_traits<512, 64, 32, 4, cutlass::half_t>>, Is_dropout=true, Is_causal=false, Is_even_N=false, Is_even_K=true, Return_softmax=false, Params=Flash_fwd_params]" 
(588): here
            instantiation of "void flash::compute_attn<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &) [with Kernel_traits=Flash_fwd_kernel_traits<512, 64, 32, 4, false, false, cutlass::half_t, Flash_kernel_traits<512, 64, 32, 4, cutlass::half_t>>, Is_dropout=true, Is_causal=false, Is_even_N=false, Is_even_K=true, Return_softmax=false, Params=Flash_fwd_params]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(15): here
            instantiation of "void flash_fwd_kernel<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax>(Flash_fwd_params) [with Kernel_traits=Flash_fwd_kernel_traits<512, 64, 32, 4, false, false, cutlass::half_t, Flash_kernel_traits<512, 64, 32, 4, cutlass::half_t>>, Is_dropout=true, Is_causal=false, Is_even_N=false, Is_even_K=true, Return_softmax=false]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(34): here
            instantiation of "void run_flash_fwd<Kernel_traits,Is_dropout,Is_causal>(Flash_fwd_params &, cudaStream_t) [with Kernel_traits=Flash_fwd_kernel_traits<512, 64, 32, 4, false, false, cutlass::half_t, Flash_kernel_traits<512, 64, 32, 4, cutlass::half_t>>, Is_dropout=true, Is_causal=false]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(264): here
            instantiation of "void run_mha_fwd_hdim512<T>(Flash_fwd_params &, cudaStream_t) [with T=cutlass::half_t]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_hdim512_fp16_sm80.cu(8): here

/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_kernel.h(140): warning: variable "MMA_M" was declared but never referenced
          detected during:
            instantiation of "void flash::compute_attn_1rowblock<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &, int, int, int) [with Kernel_traits=Flash_fwd_kernel_traits<512, 64, 32, 4, false, false, cutlass::half_t, Flash_kernel_traits<512, 64, 32, 4, cutlass::half_t>>, Is_dropout=true, Is_causal=false, Is_even_N=false, Is_even_K=false, Return_softmax=true, Params=Flash_fwd_params]" 
(588): here
            instantiation of "void flash::compute_attn<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &) [with Kernel_traits=Flash_fwd_kernel_traits<512, 64, 32, 4, false, false, cutlass::half_t, Flash_kernel_traits<512, 64, 32, 4, cutlass::half_t>>, Is_dropout=true, Is_causal=false, Is_even_N=false, Is_even_K=false, Return_softmax=true, Params=Flash_fwd_params]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(15): here
            instantiation of "void flash_fwd_kernel<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax>(Flash_fwd_params) [with Kernel_traits=Flash_fwd_kernel_traits<512, 64, 32, 4, false, false, cutlass::half_t, Flash_kernel_traits<512, 64, 32, 4, cutlass::half_t>>, Is_dropout=true, Is_causal=false, Is_even_N=false, Is_even_K=false, Return_softmax=true]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(34): here
            instantiation of "void run_flash_fwd<Kernel_traits,Is_dropout,Is_causal>(Flash_fwd_params &, cudaStream_t) [with Kernel_traits=Flash_fwd_kernel_traits<512, 64, 32, 4, false, false, cutlass::half_t, Flash_kernel_traits<512, 64, 32, 4, cutlass::half_t>>, Is_dropout=true, Is_causal=false]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(264): here
            instantiation of "void run_mha_fwd_hdim512<T>(Flash_fwd_params &, cudaStream_t) [with T=cutlass::half_t]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_hdim512_fp16_sm80.cu(8): here

/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_kernel.h(275): warning: variable "tQrQ" was declared but never referenced
          detected during:
            instantiation of "void flash::compute_attn_1rowblock<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &, int, int, int) [with Kernel_traits=Flash_fwd_kernel_traits<512, 64, 32, 4, false, false, cutlass::half_t, Flash_kernel_traits<512, 64, 32, 4, cutlass::half_t>>, Is_dropout=true, Is_causal=false, Is_even_N=false, Is_even_K=false, Return_softmax=true, Params=Flash_fwd_params]" 
(588): here
            instantiation of "void flash::compute_attn<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &) [with Kernel_traits=Flash_fwd_kernel_traits<512, 64, 32, 4, false, false, cutlass::half_t, Flash_kernel_traits<512, 64, 32, 4, cutlass::half_t>>, Is_dropout=true, Is_causal=false, Is_even_N=false, Is_even_K=false, Return_softmax=true, Params=Flash_fwd_params]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(15): here
            instantiation of "void flash_fwd_kernel<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax>(Flash_fwd_params) [with Kernel_traits=Flash_fwd_kernel_traits<512, 64, 32, 4, false, false, cutlass::half_t, Flash_kernel_traits<512, 64, 32, 4, cutlass::half_t>>, Is_dropout=true, Is_causal=false, Is_even_N=false, Is_even_K=false, Return_softmax=true]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(34): here
            instantiation of "void run_flash_fwd<Kernel_traits,Is_dropout,Is_causal>(Flash_fwd_params &, cudaStream_t) [with Kernel_traits=Flash_fwd_kernel_traits<512, 64, 32, 4, false, false, cutlass::half_t, Flash_kernel_traits<512, 64, 32, 4, cutlass::half_t>>, Is_dropout=true, Is_causal=false]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(264): here
            instantiation of "void run_mha_fwd_hdim512<T>(Flash_fwd_params &, cudaStream_t) [with T=cutlass::half_t]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_hdim512_fp16_sm80.cu(8): here

/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_kernel.h(140): warning: variable "MMA_M" was declared but never referenced
          detected during:
            instantiation of "void flash::compute_attn_1rowblock<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &, int, int, int) [with Kernel_traits=Flash_fwd_kernel_traits<512, 64, 32, 4, false, false, cutlass::half_t, Flash_kernel_traits<512, 64, 32, 4, cutlass::half_t>>, Is_dropout=true, Is_causal=false, Is_even_N=false, Is_even_K=false, Return_softmax=false, Params=Flash_fwd_params]" 
(588): here
            instantiation of "void flash::compute_attn<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &) [with Kernel_traits=Flash_fwd_kernel_traits<512, 64, 32, 4, false, false, cutlass::half_t, Flash_kernel_traits<512, 64, 32, 4, cutlass::half_t>>, Is_dropout=true, Is_causal=false, Is_even_N=false, Is_even_K=false, Return_softmax=false, Params=Flash_fwd_params]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(15): here
            instantiation of "void flash_fwd_kernel<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax>(Flash_fwd_params) [with Kernel_traits=Flash_fwd_kernel_traits<512, 64, 32, 4, false, false, cutlass::half_t, Flash_kernel_traits<512, 64, 32, 4, cutlass::half_t>>, Is_dropout=true, Is_causal=false, Is_even_N=false, Is_even_K=false, Return_softmax=false]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(34): here
            instantiation of "void run_flash_fwd<Kernel_traits,Is_dropout,Is_causal>(Flash_fwd_params &, cudaStream_t) [with Kernel_traits=Flash_fwd_kernel_traits<512, 64, 32, 4, false, false, cutlass::half_t, Flash_kernel_traits<512, 64, 32, 4, cutlass::half_t>>, Is_dropout=true, Is_causal=false]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(264): here
            instantiation of "void run_mha_fwd_hdim512<T>(Flash_fwd_params &, cudaStream_t) [with T=cutlass::half_t]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_hdim512_fp16_sm80.cu(8): here

/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_kernel.h(275): warning: variable "tQrQ" was declared but never referenced
          detected during:
            instantiation of "void flash::compute_attn_1rowblock<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &, int, int, int) [with Kernel_traits=Flash_fwd_kernel_traits<512, 64, 32, 4, false, false, cutlass::half_t, Flash_kernel_traits<512, 64, 32, 4, cutlass::half_t>>, Is_dropout=true, Is_causal=false, Is_even_N=false, Is_even_K=false, Return_softmax=false, Params=Flash_fwd_params]" 
(588): here
            instantiation of "void flash::compute_attn<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &) [with Kernel_traits=Flash_fwd_kernel_traits<512, 64, 32, 4, false, false, cutlass::half_t, Flash_kernel_traits<512, 64, 32, 4, cutlass::half_t>>, Is_dropout=true, Is_causal=false, Is_even_N=false, Is_even_K=false, Return_softmax=false, Params=Flash_fwd_params]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(15): here
            instantiation of "void flash_fwd_kernel<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax>(Flash_fwd_params) [with Kernel_traits=Flash_fwd_kernel_traits<512, 64, 32, 4, false, false, cutlass::half_t, Flash_kernel_traits<512, 64, 32, 4, cutlass::half_t>>, Is_dropout=true, Is_causal=false, Is_even_N=false, Is_even_K=false, Return_softmax=false]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(34): here
            instantiation of "void run_flash_fwd<Kernel_traits,Is_dropout,Is_causal>(Flash_fwd_params &, cudaStream_t) [with Kernel_traits=Flash_fwd_kernel_traits<512, 64, 32, 4, false, false, cutlass::half_t, Flash_kernel_traits<512, 64, 32, 4, cutlass::half_t>>, Is_dropout=true, Is_causal=false]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(264): here
            instantiation of "void run_mha_fwd_hdim512<T>(Flash_fwd_params &, cudaStream_t) [with T=cutlass::half_t]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_hdim512_fp16_sm80.cu(8): here

/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_kernel.h(140): warning: variable "MMA_M" was declared but never referenced
          detected during:
            instantiation of "void flash::compute_attn_1rowblock<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &, int, int, int) [with Kernel_traits=Flash_fwd_kernel_traits<512, 32, 32, 2, false, false, cutlass::half_t, Flash_kernel_traits<512, 32, 32, 2, cutlass::half_t>>, Is_dropout=true, Is_causal=false, Is_even_N=true, Is_even_K=true, Return_softmax=true, Params=Flash_fwd_params]" 
(588): here
            instantiation of "void flash::compute_attn<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &) [with Kernel_traits=Flash_fwd_kernel_traits<512, 32, 32, 2, false, false, cutlass::half_t, Flash_kernel_traits<512, 32, 32, 2, cutlass::half_t>>, Is_dropout=true, Is_causal=false, Is_even_N=true, Is_even_K=true, Return_softmax=true, Params=Flash_fwd_params]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(15): here
            instantiation of "void flash_fwd_kernel<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax>(Flash_fwd_params) [with Kernel_traits=Flash_fwd_kernel_traits<512, 32, 32, 2, false, false, cutlass::half_t, Flash_kernel_traits<512, 32, 32, 2, cutlass::half_t>>, Is_dropout=true, Is_causal=false, Is_even_N=true, Is_even_K=true, Return_softmax=true]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(34): here
            instantiation of "void run_flash_fwd<Kernel_traits,Is_dropout,Is_causal>(Flash_fwd_params &, cudaStream_t) [with Kernel_traits=Flash_fwd_kernel_traits<512, 32, 32, 2, false, false, cutlass::half_t, Flash_kernel_traits<512, 32, 32, 2, cutlass::half_t>>, Is_dropout=true, Is_causal=false]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(264): here
            instantiation of "void run_mha_fwd_hdim512<T>(Flash_fwd_params &, cudaStream_t) [with T=cutlass::half_t]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_hdim512_fp16_sm80.cu(8): here

/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_kernel.h(275): warning: variable "tQrQ" was declared but never referenced
          detected during:
            instantiation of "void flash::compute_attn_1rowblock<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &, int, int, int) [with Kernel_traits=Flash_fwd_kernel_traits<512, 32, 32, 2, false, false, cutlass::half_t, Flash_kernel_traits<512, 32, 32, 2, cutlass::half_t>>, Is_dropout=true, Is_causal=false, Is_even_N=true, Is_even_K=true, Return_softmax=true, Params=Flash_fwd_params]" 
(588): here
            instantiation of "void flash::compute_attn<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &) [with Kernel_traits=Flash_fwd_kernel_traits<512, 32, 32, 2, false, false, cutlass::half_t, Flash_kernel_traits<512, 32, 32, 2, cutlass::half_t>>, Is_dropout=true, Is_causal=false, Is_even_N=true, Is_even_K=true, Return_softmax=true, Params=Flash_fwd_params]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(15): here
            instantiation of "void flash_fwd_kernel<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax>(Flash_fwd_params) [with Kernel_traits=Flash_fwd_kernel_traits<512, 32, 32, 2, false, false, cutlass::half_t, Flash_kernel_traits<512, 32, 32, 2, cutlass::half_t>>, Is_dropout=true, Is_causal=false, Is_even_N=true, Is_even_K=true, Return_softmax=true]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(34): here
            instantiation of "void run_flash_fwd<Kernel_traits,Is_dropout,Is_causal>(Flash_fwd_params &, cudaStream_t) [with Kernel_traits=Flash_fwd_kernel_traits<512, 32, 32, 2, false, false, cutlass::half_t, Flash_kernel_traits<512, 32, 32, 2, cutlass::half_t>>, Is_dropout=true, Is_causal=false]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(264): here
            instantiation of "void run_mha_fwd_hdim512<T>(Flash_fwd_params &, cudaStream_t) [with T=cutlass::half_t]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_hdim512_fp16_sm80.cu(8): here

/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_kernel.h(140): warning: variable "MMA_M" was declared but never referenced
          detected during:
            instantiation of "void flash::compute_attn_1rowblock<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &, int, int, int) [with Kernel_traits=Flash_fwd_kernel_traits<512, 32, 32, 2, false, false, cutlass::half_t, Flash_kernel_traits<512, 32, 32, 2, cutlass::half_t>>, Is_dropout=true, Is_causal=false, Is_even_N=true, Is_even_K=true, Return_softmax=false, Params=Flash_fwd_params]" 
(588): here
            instantiation of "void flash::compute_attn<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &) [with Kernel_traits=Flash_fwd_kernel_traits<512, 32, 32, 2, false, false, cutlass::half_t, Flash_kernel_traits<512, 32, 32, 2, cutlass::half_t>>, Is_dropout=true, Is_causal=false, Is_even_N=true, Is_even_K=true, Return_softmax=false, Params=Flash_fwd_params]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(15): here
            instantiation of "void flash_fwd_kernel<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax>(Flash_fwd_params) [with Kernel_traits=Flash_fwd_kernel_traits<512, 32, 32, 2, false, false, cutlass::half_t, Flash_kernel_traits<512, 32, 32, 2, cutlass::half_t>>, Is_dropout=true, Is_causal=false, Is_even_N=true, Is_even_K=true, Return_softmax=false]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(34): here
            instantiation of "void run_flash_fwd<Kernel_traits,Is_dropout,Is_causal>(Flash_fwd_params &, cudaStream_t) [with Kernel_traits=Flash_fwd_kernel_traits<512, 32, 32, 2, false, false, cutlass::half_t, Flash_kernel_traits<512, 32, 32, 2, cutlass::half_t>>, Is_dropout=true, Is_causal=false]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(264): here
            instantiation of "void run_mha_fwd_hdim512<T>(Flash_fwd_params &, cudaStream_t) [with T=cutlass::half_t]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_hdim512_fp16_sm80.cu(8): here

/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_kernel.h(275): warning: variable "tQrQ" was declared but never referenced
          detected during:
            instantiation of "void flash::compute_attn_1rowblock<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &, int, int, int) [with Kernel_traits=Flash_fwd_kernel_traits<512, 32, 32, 2, false, false, cutlass::half_t, Flash_kernel_traits<512, 32, 32, 2, cutlass::half_t>>, Is_dropout=true, Is_causal=false, Is_even_N=true, Is_even_K=true, Return_softmax=false, Params=Flash_fwd_params]" 
(588): here
            instantiation of "void flash::compute_attn<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &) [with Kernel_traits=Flash_fwd_kernel_traits<512, 32, 32, 2, false, false, cutlass::half_t, Flash_kernel_traits<512, 32, 32, 2, cutlass::half_t>>, Is_dropout=true, Is_causal=false, Is_even_N=true, Is_even_K=true, Return_softmax=false, Params=Flash_fwd_params]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(15): here
            instantiation of "void flash_fwd_kernel<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax>(Flash_fwd_params) [with Kernel_traits=Flash_fwd_kernel_traits<512, 32, 32, 2, false, false, cutlass::half_t, Flash_kernel_traits<512, 32, 32, 2, cutlass::half_t>>, Is_dropout=true, Is_causal=false, Is_even_N=true, Is_even_K=true, Return_softmax=false]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(34): here
            instantiation of "void run_flash_fwd<Kernel_traits,Is_dropout,Is_causal>(Flash_fwd_params &, cudaStream_t) [with Kernel_traits=Flash_fwd_kernel_traits<512, 32, 32, 2, false, false, cutlass::half_t, Flash_kernel_traits<512, 32, 32, 2, cutlass::half_t>>, Is_dropout=true, Is_causal=false]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(264): here
            instantiation of "void run_mha_fwd_hdim512<T>(Flash_fwd_params &, cudaStream_t) [with T=cutlass::half_t]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_hdim512_fp16_sm80.cu(8): here

/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_kernel.h(140): warning: variable "MMA_M" was declared but never referenced
          detected during:
            instantiation of "void flash::compute_attn_1rowblock<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &, int, int, int) [with Kernel_traits=Flash_fwd_kernel_traits<512, 32, 32, 2, false, false, cutlass::half_t, Flash_kernel_traits<512, 32, 32, 2, cutlass::half_t>>, Is_dropout=true, Is_causal=false, Is_even_N=true, Is_even_K=false, Return_softmax=true, Params=Flash_fwd_params]" 
(588): here
            instantiation of "void flash::compute_attn<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &) [with Kernel_traits=Flash_fwd_kernel_traits<512, 32, 32, 2, false, false, cutlass::half_t, Flash_kernel_traits<512, 32, 32, 2, cutlass::half_t>>, Is_dropout=true, Is_causal=false, Is_even_N=true, Is_even_K=false, Return_softmax=true, Params=Flash_fwd_params]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(15): here
            instantiation of "void flash_fwd_kernel<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax>(Flash_fwd_params) [with Kernel_traits=Flash_fwd_kernel_traits<512, 32, 32, 2, false, false, cutlass::half_t, Flash_kernel_traits<512, 32, 32, 2, cutlass::half_t>>, Is_dropout=true, Is_causal=false, Is_even_N=true, Is_even_K=false, Return_softmax=true]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(34): here
            instantiation of "void run_flash_fwd<Kernel_traits,Is_dropout,Is_causal>(Flash_fwd_params &, cudaStream_t) [with Kernel_traits=Flash_fwd_kernel_traits<512, 32, 32, 2, false, false, cutlass::half_t, Flash_kernel_traits<512, 32, 32, 2, cutlass::half_t>>, Is_dropout=true, Is_causal=false]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(264): here
            instantiation of "void run_mha_fwd_hdim512<T>(Flash_fwd_params &, cudaStream_t) [with T=cutlass::half_t]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_hdim512_fp16_sm80.cu(8): here

/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_kernel.h(275): warning: variable "tQrQ" was declared but never referenced
          detected during:
            instantiation of "void flash::compute_attn_1rowblock<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &, int, int, int) [with Kernel_traits=Flash_fwd_kernel_traits<512, 32, 32, 2, false, false, cutlass::half_t, Flash_kernel_traits<512, 32, 32, 2, cutlass::half_t>>, Is_dropout=true, Is_causal=false, Is_even_N=true, Is_even_K=false, Return_softmax=true, Params=Flash_fwd_params]" 
(588): here
            instantiation of "void flash::compute_attn<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &) [with Kernel_traits=Flash_fwd_kernel_traits<512, 32, 32, 2, false, false, cutlass::half_t, Flash_kernel_traits<512, 32, 32, 2, cutlass::half_t>>, Is_dropout=true, Is_causal=false, Is_even_N=true, Is_even_K=false, Return_softmax=true, Params=Flash_fwd_params]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(15): here
            instantiation of "void flash_fwd_kernel<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax>(Flash_fwd_params) [with Kernel_traits=Flash_fwd_kernel_traits<512, 32, 32, 2, false, false, cutlass::half_t, Flash_kernel_traits<512, 32, 32, 2, cutlass::half_t>>, Is_dropout=true, Is_causal=false, Is_even_N=true, Is_even_K=false, Return_softmax=true]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(34): here
            instantiation of "void run_flash_fwd<Kernel_traits,Is_dropout,Is_causal>(Flash_fwd_params &, cudaStream_t) [with Kernel_traits=Flash_fwd_kernel_traits<512, 32, 32, 2, false, false, cutlass::half_t, Flash_kernel_traits<512, 32, 32, 2, cutlass::half_t>>, Is_dropout=true, Is_causal=false]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(264): here
            instantiation of "void run_mha_fwd_hdim512<T>(Flash_fwd_params &, cudaStream_t) [with T=cutlass::half_t]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_hdim512_fp16_sm80.cu(8): here

/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_kernel.h(140): warning: variable "MMA_M" was declared but never referenced
          detected during:
            instantiation of "void flash::compute_attn_1rowblock<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &, int, int, int) [with Kernel_traits=Flash_fwd_kernel_traits<512, 32, 32, 2, false, false, cutlass::half_t, Flash_kernel_traits<512, 32, 32, 2, cutlass::half_t>>, Is_dropout=true, Is_causal=false, Is_even_N=true, Is_even_K=false, Return_softmax=false, Params=Flash_fwd_params]" 
(588): here
            instantiation of "void flash::compute_attn<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &) [with Kernel_traits=Flash_fwd_kernel_traits<512, 32, 32, 2, false, false, cutlass::half_t, Flash_kernel_traits<512, 32, 32, 2, cutlass::half_t>>, Is_dropout=true, Is_causal=false, Is_even_N=true, Is_even_K=false, Return_softmax=false, Params=Flash_fwd_params]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(15): here
            instantiation of "void flash_fwd_kernel<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax>(Flash_fwd_params) [with Kernel_traits=Flash_fwd_kernel_traits<512, 32, 32, 2, false, false, cutlass::half_t, Flash_kernel_traits<512, 32, 32, 2, cutlass::half_t>>, Is_dropout=true, Is_causal=false, Is_even_N=true, Is_even_K=false, Return_softmax=false]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(34): here
            instantiation of "void run_flash_fwd<Kernel_traits,Is_dropout,Is_causal>(Flash_fwd_params &, cudaStream_t) [with Kernel_traits=Flash_fwd_kernel_traits<512, 32, 32, 2, false, false, cutlass::half_t, Flash_kernel_traits<512, 32, 32, 2, cutlass::half_t>>, Is_dropout=true, Is_causal=false]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(264): here
            instantiation of "void run_mha_fwd_hdim512<T>(Flash_fwd_params &, cudaStream_t) [with T=cutlass::half_t]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_hdim512_fp16_sm80.cu(8): here

/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_kernel.h(275): warning: variable "tQrQ" was declared but never referenced
          detected during:
            instantiation of "void flash::compute_attn_1rowblock<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &, int, int, int) [with Kernel_traits=Flash_fwd_kernel_traits<512, 32, 32, 2, false, false, cutlass::half_t, Flash_kernel_traits<512, 32, 32, 2, cutlass::half_t>>, Is_dropout=true, Is_causal=false, Is_even_N=true, Is_even_K=false, Return_softmax=false, Params=Flash_fwd_params]" 
(588): here
            instantiation of "void flash::compute_attn<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &) [with Kernel_traits=Flash_fwd_kernel_traits<512, 32, 32, 2, false, false, cutlass::half_t, Flash_kernel_traits<512, 32, 32, 2, cutlass::half_t>>, Is_dropout=true, Is_causal=false, Is_even_N=true, Is_even_K=false, Return_softmax=false, Params=Flash_fwd_params]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(15): here
            instantiation of "void flash_fwd_kernel<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax>(Flash_fwd_params) [with Kernel_traits=Flash_fwd_kernel_traits<512, 32, 32, 2, false, false, cutlass::half_t, Flash_kernel_traits<512, 32, 32, 2, cutlass::half_t>>, Is_dropout=true, Is_causal=false, Is_even_N=true, Is_even_K=false, Return_softmax=false]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(34): here
            instantiation of "void run_flash_fwd<Kernel_traits,Is_dropout,Is_causal>(Flash_fwd_params &, cudaStream_t) [with Kernel_traits=Flash_fwd_kernel_traits<512, 32, 32, 2, false, false, cutlass::half_t, Flash_kernel_traits<512, 32, 32, 2, cutlass::half_t>>, Is_dropout=true, Is_causal=false]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(264): here
            instantiation of "void run_mha_fwd_hdim512<T>(Flash_fwd_params &, cudaStream_t) [with T=cutlass::half_t]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_hdim512_fp16_sm80.cu(8): here

/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_kernel.h(140): warning: variable "MMA_M" was declared but never referenced
          detected during:
            instantiation of "void flash::compute_attn_1rowblock<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &, int, int, int) [with Kernel_traits=Flash_fwd_kernel_traits<512, 32, 32, 2, false, false, cutlass::half_t, Flash_kernel_traits<512, 32, 32, 2, cutlass::half_t>>, Is_dropout=true, Is_causal=false, Is_even_N=false, Is_even_K=true, Return_softmax=true, Params=Flash_fwd_params]" 
(588): here
            instantiation of "void flash::compute_attn<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &) [with Kernel_traits=Flash_fwd_kernel_traits<512, 32, 32, 2, false, false, cutlass::half_t, Flash_kernel_traits<512, 32, 32, 2, cutlass::half_t>>, Is_dropout=true, Is_causal=false, Is_even_N=false, Is_even_K=true, Return_softmax=true, Params=Flash_fwd_params]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(15): here
            instantiation of "void flash_fwd_kernel<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax>(Flash_fwd_params) [with Kernel_traits=Flash_fwd_kernel_traits<512, 32, 32, 2, false, false, cutlass::half_t, Flash_kernel_traits<512, 32, 32, 2, cutlass::half_t>>, Is_dropout=true, Is_causal=false, Is_even_N=false, Is_even_K=true, Return_softmax=true]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(34): here
            instantiation of "void run_flash_fwd<Kernel_traits,Is_dropout,Is_causal>(Flash_fwd_params &, cudaStream_t) [with Kernel_traits=Flash_fwd_kernel_traits<512, 32, 32, 2, false, false, cutlass::half_t, Flash_kernel_traits<512, 32, 32, 2, cutlass::half_t>>, Is_dropout=true, Is_causal=false]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(264): here
            instantiation of "void run_mha_fwd_hdim512<T>(Flash_fwd_params &, cudaStream_t) [with T=cutlass::half_t]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_hdim512_fp16_sm80.cu(8): here

/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_kernel.h(275): warning: variable "tQrQ" was declared but never referenced
          detected during:
            instantiation of "void flash::compute_attn_1rowblock<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &, int, int, int) [with Kernel_traits=Flash_fwd_kernel_traits<512, 32, 32, 2, false, false, cutlass::half_t, Flash_kernel_traits<512, 32, 32, 2, cutlass::half_t>>, Is_dropout=true, Is_causal=false, Is_even_N=false, Is_even_K=true, Return_softmax=true, Params=Flash_fwd_params]" 
(588): here
            instantiation of "void flash::compute_attn<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &) [with Kernel_traits=Flash_fwd_kernel_traits<512, 32, 32, 2, false, false, cutlass::half_t, Flash_kernel_traits<512, 32, 32, 2, cutlass::half_t>>, Is_dropout=true, Is_causal=false, Is_even_N=false, Is_even_K=true, Return_softmax=true, Params=Flash_fwd_params]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(15): here
            instantiation of "void flash_fwd_kernel<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax>(Flash_fwd_params) [with Kernel_traits=Flash_fwd_kernel_traits<512, 32, 32, 2, false, false, cutlass::half_t, Flash_kernel_traits<512, 32, 32, 2, cutlass::half_t>>, Is_dropout=true, Is_causal=false, Is_even_N=false, Is_even_K=true, Return_softmax=true]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(34): here
            instantiation of "void run_flash_fwd<Kernel_traits,Is_dropout,Is_causal>(Flash_fwd_params &, cudaStream_t) [with Kernel_traits=Flash_fwd_kernel_traits<512, 32, 32, 2, false, false, cutlass::half_t, Flash_kernel_traits<512, 32, 32, 2, cutlass::half_t>>, Is_dropout=true, Is_causal=false]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(264): here
            instantiation of "void run_mha_fwd_hdim512<T>(Flash_fwd_params &, cudaStream_t) [with T=cutlass::half_t]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_hdim512_fp16_sm80.cu(8): here

/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_kernel.h(140): warning: variable "MMA_M" was declared but never referenced
          detected during:
            instantiation of "void flash::compute_attn_1rowblock<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &, int, int, int) [with Kernel_traits=Flash_fwd_kernel_traits<512, 32, 32, 2, false, false, cutlass::half_t, Flash_kernel_traits<512, 32, 32, 2, cutlass::half_t>>, Is_dropout=true, Is_causal=false, Is_even_N=false, Is_even_K=true, Return_softmax=false, Params=Flash_fwd_params]" 
(588): here
            instantiation of "void flash::compute_attn<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &) [with Kernel_traits=Flash_fwd_kernel_traits<512, 32, 32, 2, false, false, cutlass::half_t, Flash_kernel_traits<512, 32, 32, 2, cutlass::half_t>>, Is_dropout=true, Is_causal=false, Is_even_N=false, Is_even_K=true, Return_softmax=false, Params=Flash_fwd_params]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(15): here
            instantiation of "void flash_fwd_kernel<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax>(Flash_fwd_params) [with Kernel_traits=Flash_fwd_kernel_traits<512, 32, 32, 2, false, false, cutlass::half_t, Flash_kernel_traits<512, 32, 32, 2, cutlass::half_t>>, Is_dropout=true, Is_causal=false, Is_even_N=false, Is_even_K=true, Return_softmax=false]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(34): here
            instantiation of "void run_flash_fwd<Kernel_traits,Is_dropout,Is_causal>(Flash_fwd_params &, cudaStream_t) [with Kernel_traits=Flash_fwd_kernel_traits<512, 32, 32, 2, false, false, cutlass::half_t, Flash_kernel_traits<512, 32, 32, 2, cutlass::half_t>>, Is_dropout=true, Is_causal=false]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(264): here
            instantiation of "void run_mha_fwd_hdim512<T>(Flash_fwd_params &, cudaStream_t) [with T=cutlass::half_t]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_hdim512_fp16_sm80.cu(8): here

/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_kernel.h(275): warning: variable "tQrQ" was declared but never referenced
          detected during:
            instantiation of "void flash::compute_attn_1rowblock<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &, int, int, int) [with Kernel_traits=Flash_fwd_kernel_traits<512, 32, 32, 2, false, false, cutlass::half_t, Flash_kernel_traits<512, 32, 32, 2, cutlass::half_t>>, Is_dropout=true, Is_causal=false, Is_even_N=false, Is_even_K=true, Return_softmax=false, Params=Flash_fwd_params]" 
(588): here
            instantiation of "void flash::compute_attn<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &) [with Kernel_traits=Flash_fwd_kernel_traits<512, 32, 32, 2, false, false, cutlass::half_t, Flash_kernel_traits<512, 32, 32, 2, cutlass::half_t>>, Is_dropout=true, Is_causal=false, Is_even_N=false, Is_even_K=true, Return_softmax=false, Params=Flash_fwd_params]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(15): here
            instantiation of "void flash_fwd_kernel<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax>(Flash_fwd_params) [with Kernel_traits=Flash_fwd_kernel_traits<512, 32, 32, 2, false, false, cutlass::half_t, Flash_kernel_traits<512, 32, 32, 2, cutlass::half_t>>, Is_dropout=true, Is_causal=false, Is_even_N=false, Is_even_K=true, Return_softmax=false]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(34): here
            instantiation of "void run_flash_fwd<Kernel_traits,Is_dropout,Is_causal>(Flash_fwd_params &, cudaStream_t) [with Kernel_traits=Flash_fwd_kernel_traits<512, 32, 32, 2, false, false, cutlass::half_t, Flash_kernel_traits<512, 32, 32, 2, cutlass::half_t>>, Is_dropout=true, Is_causal=false]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(264): here
            instantiation of "void run_mha_fwd_hdim512<T>(Flash_fwd_params &, cudaStream_t) [with T=cutlass::half_t]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_hdim512_fp16_sm80.cu(8): here

/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_kernel.h(140): warning: variable "MMA_M" was declared but never referenced
          detected during:
            instantiation of "void flash::compute_attn_1rowblock<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &, int, int, int) [with Kernel_traits=Flash_fwd_kernel_traits<512, 32, 32, 2, false, false, cutlass::half_t, Flash_kernel_traits<512, 32, 32, 2, cutlass::half_t>>, Is_dropout=true, Is_causal=false, Is_even_N=false, Is_even_K=false, Return_softmax=true, Params=Flash_fwd_params]" 
(588): here
            instantiation of "void flash::compute_attn<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &) [with Kernel_traits=Flash_fwd_kernel_traits<512, 32, 32, 2, false, false, cutlass::half_t, Flash_kernel_traits<512, 32, 32, 2, cutlass::half_t>>, Is_dropout=true, Is_causal=false, Is_even_N=false, Is_even_K=false, Return_softmax=true, Params=Flash_fwd_params]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(15): here
            instantiation of "void flash_fwd_kernel<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax>(Flash_fwd_params) [with Kernel_traits=Flash_fwd_kernel_traits<512, 32, 32, 2, false, false, cutlass::half_t, Flash_kernel_traits<512, 32, 32, 2, cutlass::half_t>>, Is_dropout=true, Is_causal=false, Is_even_N=false, Is_even_K=false, Return_softmax=true]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(34): here
            instantiation of "void run_flash_fwd<Kernel_traits,Is_dropout,Is_causal>(Flash_fwd_params &, cudaStream_t) [with Kernel_traits=Flash_fwd_kernel_traits<512, 32, 32, 2, false, false, cutlass::half_t, Flash_kernel_traits<512, 32, 32, 2, cutlass::half_t>>, Is_dropout=true, Is_causal=false]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(264): here
            instantiation of "void run_mha_fwd_hdim512<T>(Flash_fwd_params &, cudaStream_t) [with T=cutlass::half_t]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_hdim512_fp16_sm80.cu(8): here

/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_kernel.h(275): warning: variable "tQrQ" was declared but never referenced
          detected during:
            instantiation of "void flash::compute_attn_1rowblock<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &, int, int, int) [with Kernel_traits=Flash_fwd_kernel_traits<512, 32, 32, 2, false, false, cutlass::half_t, Flash_kernel_traits<512, 32, 32, 2, cutlass::half_t>>, Is_dropout=true, Is_causal=false, Is_even_N=false, Is_even_K=false, Return_softmax=true, Params=Flash_fwd_params]" 
(588): here
            instantiation of "void flash::compute_attn<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &) [with Kernel_traits=Flash_fwd_kernel_traits<512, 32, 32, 2, false, false, cutlass::half_t, Flash_kernel_traits<512, 32, 32, 2, cutlass::half_t>>, Is_dropout=true, Is_causal=false, Is_even_N=false, Is_even_K=false, Return_softmax=true, Params=Flash_fwd_params]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(15): here
            instantiation of "void flash_fwd_kernel<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax>(Flash_fwd_params) [with Kernel_traits=Flash_fwd_kernel_traits<512, 32, 32, 2, false, false, cutlass::half_t, Flash_kernel_traits<512, 32, 32, 2, cutlass::half_t>>, Is_dropout=true, Is_causal=false, Is_even_N=false, Is_even_K=false, Return_softmax=true]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(34): here
            instantiation of "void run_flash_fwd<Kernel_traits,Is_dropout,Is_causal>(Flash_fwd_params &, cudaStream_t) [with Kernel_traits=Flash_fwd_kernel_traits<512, 32, 32, 2, false, false, cutlass::half_t, Flash_kernel_traits<512, 32, 32, 2, cutlass::half_t>>, Is_dropout=true, Is_causal=false]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(264): here
            instantiation of "void run_mha_fwd_hdim512<T>(Flash_fwd_params &, cudaStream_t) [with T=cutlass::half_t]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_hdim512_fp16_sm80.cu(8): here

/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_kernel.h(140): warning: variable "MMA_M" was declared but never referenced
          detected during:
            instantiation of "void flash::compute_attn_1rowblock<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &, int, int, int) [with Kernel_traits=Flash_fwd_kernel_traits<512, 32, 32, 2, false, false, cutlass::half_t, Flash_kernel_traits<512, 32, 32, 2, cutlass::half_t>>, Is_dropout=true, Is_causal=false, Is_even_N=false, Is_even_K=false, Return_softmax=false, Params=Flash_fwd_params]" 
(588): here
            instantiation of "void flash::compute_attn<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &) [with Kernel_traits=Flash_fwd_kernel_traits<512, 32, 32, 2, false, false, cutlass::half_t, Flash_kernel_traits<512, 32, 32, 2, cutlass::half_t>>, Is_dropout=true, Is_causal=false, Is_even_N=false, Is_even_K=false, Return_softmax=false, Params=Flash_fwd_params]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(15): here
            instantiation of "void flash_fwd_kernel<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax>(Flash_fwd_params) [with Kernel_traits=Flash_fwd_kernel_traits<512, 32, 32, 2, false, false, cutlass::half_t, Flash_kernel_traits<512, 32, 32, 2, cutlass::half_t>>, Is_dropout=true, Is_causal=false, Is_even_N=false, Is_even_K=false, Return_softmax=false]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(34): here
            instantiation of "void run_flash_fwd<Kernel_traits,Is_dropout,Is_causal>(Flash_fwd_params &, cudaStream_t) [with Kernel_traits=Flash_fwd_kernel_traits<512, 32, 32, 2, false, false, cutlass::half_t, Flash_kernel_traits<512, 32, 32, 2, cutlass::half_t>>, Is_dropout=true, Is_causal=false]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(264): here
            instantiation of "void run_mha_fwd_hdim512<T>(Flash_fwd_params &, cudaStream_t) [with T=cutlass::half_t]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_hdim512_fp16_sm80.cu(8): here

/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_kernel.h(275): warning: variable "tQrQ" was declared but never referenced
          detected during:
            instantiation of "void flash::compute_attn_1rowblock<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &, int, int, int) [with Kernel_traits=Flash_fwd_kernel_traits<512, 32, 32, 2, false, false, cutlass::half_t, Flash_kernel_traits<512, 32, 32, 2, cutlass::half_t>>, Is_dropout=true, Is_causal=false, Is_even_N=false, Is_even_K=false, Return_softmax=false, Params=Flash_fwd_params]" 
(588): here
            instantiation of "void flash::compute_attn<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &) [with Kernel_traits=Flash_fwd_kernel_traits<512, 32, 32, 2, false, false, cutlass::half_t, Flash_kernel_traits<512, 32, 32, 2, cutlass::half_t>>, Is_dropout=true, Is_causal=false, Is_even_N=false, Is_even_K=false, Return_softmax=false, Params=Flash_fwd_params]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(15): here
            instantiation of "void flash_fwd_kernel<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax>(Flash_fwd_params) [with Kernel_traits=Flash_fwd_kernel_traits<512, 32, 32, 2, false, false, cutlass::half_t, Flash_kernel_traits<512, 32, 32, 2, cutlass::half_t>>, Is_dropout=true, Is_causal=false, Is_even_N=false, Is_even_K=false, Return_softmax=false]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(34): here
            instantiation of "void run_flash_fwd<Kernel_traits,Is_dropout,Is_causal>(Flash_fwd_params &, cudaStream_t) [with Kernel_traits=Flash_fwd_kernel_traits<512, 32, 32, 2, false, false, cutlass::half_t, Flash_kernel_traits<512, 32, 32, 2, cutlass::half_t>>, Is_dropout=true, Is_causal=false]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(264): here
            instantiation of "void run_mha_fwd_hdim512<T>(Flash_fwd_params &, cudaStream_t) [with T=cutlass::half_t]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_hdim512_fp16_sm80.cu(8): here

/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_kernel.h(140): warning: variable "MMA_M" was declared but never referenced
          detected during:
            instantiation of "void flash::compute_attn_1rowblock<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &, int, int, int) [with Kernel_traits=Flash_fwd_kernel_traits<512, 64, 32, 4, false, false, cutlass::half_t, Flash_kernel_traits<512, 64, 32, 4, cutlass::half_t>>, Is_dropout=false, Is_causal=true, Is_even_N=true, Is_even_K=true, Return_softmax=false, Params=Flash_fwd_params]" 
(588): here
            instantiation of "void flash::compute_attn<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &) [with Kernel_traits=Flash_fwd_kernel_traits<512, 64, 32, 4, false, false, cutlass::half_t, Flash_kernel_traits<512, 64, 32, 4, cutlass::half_t>>, Is_dropout=false, Is_causal=true, Is_even_N=true, Is_even_K=true, Return_softmax=false, Params=Flash_fwd_params]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(15): here
            instantiation of "void flash_fwd_kernel<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax>(Flash_fwd_params) [with Kernel_traits=Flash_fwd_kernel_traits<512, 64, 32, 4, false, false, cutlass::half_t, Flash_kernel_traits<512, 64, 32, 4, cutlass::half_t>>, Is_dropout=false, Is_causal=true, Is_even_N=true, Is_even_K=true, Return_softmax=false]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(34): here
            instantiation of "void run_flash_fwd<Kernel_traits,Is_dropout,Is_causal>(Flash_fwd_params &, cudaStream_t) [with Kernel_traits=Flash_fwd_kernel_traits<512, 64, 32, 4, false, false, cutlass::half_t, Flash_kernel_traits<512, 64, 32, 4, cutlass::half_t>>, Is_dropout=false, Is_causal=true]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(264): here
            instantiation of "void run_mha_fwd_hdim512<T>(Flash_fwd_params &, cudaStream_t) [with T=cutlass::half_t]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_hdim512_fp16_sm80.cu(8): here

/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_kernel.h(275): warning: variable "tQrQ" was declared but never referenced
          detected during:
            instantiation of "void flash::compute_attn_1rowblock<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &, int, int, int) [with Kernel_traits=Flash_fwd_kernel_traits<512, 64, 32, 4, false, false, cutlass::half_t, Flash_kernel_traits<512, 64, 32, 4, cutlass::half_t>>, Is_dropout=false, Is_causal=true, Is_even_N=true, Is_even_K=true, Return_softmax=false, Params=Flash_fwd_params]" 
(588): here
            instantiation of "void flash::compute_attn<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &) [with Kernel_traits=Flash_fwd_kernel_traits<512, 64, 32, 4, false, false, cutlass::half_t, Flash_kernel_traits<512, 64, 32, 4, cutlass::half_t>>, Is_dropout=false, Is_causal=true, Is_even_N=true, Is_even_K=true, Return_softmax=false, Params=Flash_fwd_params]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(15): here
            instantiation of "void flash_fwd_kernel<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax>(Flash_fwd_params) [with Kernel_traits=Flash_fwd_kernel_traits<512, 64, 32, 4, false, false, cutlass::half_t, Flash_kernel_traits<512, 64, 32, 4, cutlass::half_t>>, Is_dropout=false, Is_causal=true, Is_even_N=true, Is_even_K=true, Return_softmax=false]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(34): here
            instantiation of "void run_flash_fwd<Kernel_traits,Is_dropout,Is_causal>(Flash_fwd_params &, cudaStream_t) [with Kernel_traits=Flash_fwd_kernel_traits<512, 64, 32, 4, false, false, cutlass::half_t, Flash_kernel_traits<512, 64, 32, 4, cutlass::half_t>>, Is_dropout=false, Is_causal=true]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(264): here
            instantiation of "void run_mha_fwd_hdim512<T>(Flash_fwd_params &, cudaStream_t) [with T=cutlass::half_t]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_hdim512_fp16_sm80.cu(8): here

/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_kernel.h(140): warning: variable "MMA_M" was declared but never referenced
          detected during:
            instantiation of "void flash::compute_attn_1rowblock<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &, int, int, int) [with Kernel_traits=Flash_fwd_kernel_traits<512, 64, 32, 4, false, false, cutlass::half_t, Flash_kernel_traits<512, 64, 32, 4, cutlass::half_t>>, Is_dropout=false, Is_causal=true, Is_even_N=true, Is_even_K=false, Return_softmax=false, Params=Flash_fwd_params]" 
(588): here
            instantiation of "void flash::compute_attn<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &) [with Kernel_traits=Flash_fwd_kernel_traits<512, 64, 32, 4, false, false, cutlass::half_t, Flash_kernel_traits<512, 64, 32, 4, cutlass::half_t>>, Is_dropout=false, Is_causal=true, Is_even_N=true, Is_even_K=false, Return_softmax=false, Params=Flash_fwd_params]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(15): here
            instantiation of "void flash_fwd_kernel<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax>(Flash_fwd_params) [with Kernel_traits=Flash_fwd_kernel_traits<512, 64, 32, 4, false, false, cutlass::half_t, Flash_kernel_traits<512, 64, 32, 4, cutlass::half_t>>, Is_dropout=false, Is_causal=true, Is_even_N=true, Is_even_K=false, Return_softmax=false]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(34): here
            instantiation of "void run_flash_fwd<Kernel_traits,Is_dropout,Is_causal>(Flash_fwd_params &, cudaStream_t) [with Kernel_traits=Flash_fwd_kernel_traits<512, 64, 32, 4, false, false, cutlass::half_t, Flash_kernel_traits<512, 64, 32, 4, cutlass::half_t>>, Is_dropout=false, Is_causal=true]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(264): here
            instantiation of "void run_mha_fwd_hdim512<T>(Flash_fwd_params &, cudaStream_t) [with T=cutlass::half_t]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_hdim512_fp16_sm80.cu(8): here

/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_kernel.h(275): warning: variable "tQrQ" was declared but never referenced
          detected during:
            instantiation of "void flash::compute_attn_1rowblock<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &, int, int, int) [with Kernel_traits=Flash_fwd_kernel_traits<512, 64, 32, 4, false, false, cutlass::half_t, Flash_kernel_traits<512, 64, 32, 4, cutlass::half_t>>, Is_dropout=false, Is_causal=true, Is_even_N=true, Is_even_K=false, Return_softmax=false, Params=Flash_fwd_params]" 
(588): here
            instantiation of "void flash::compute_attn<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &) [with Kernel_traits=Flash_fwd_kernel_traits<512, 64, 32, 4, false, false, cutlass::half_t, Flash_kernel_traits<512, 64, 32, 4, cutlass::half_t>>, Is_dropout=false, Is_causal=true, Is_even_N=true, Is_even_K=false, Return_softmax=false, Params=Flash_fwd_params]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(15): here
            instantiation of "void flash_fwd_kernel<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax>(Flash_fwd_params) [with Kernel_traits=Flash_fwd_kernel_traits<512, 64, 32, 4, false, false, cutlass::half_t, Flash_kernel_traits<512, 64, 32, 4, cutlass::half_t>>, Is_dropout=false, Is_causal=true, Is_even_N=true, Is_even_K=false, Return_softmax=false]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(34): here
            instantiation of "void run_flash_fwd<Kernel_traits,Is_dropout,Is_causal>(Flash_fwd_params &, cudaStream_t) [with Kernel_traits=Flash_fwd_kernel_traits<512, 64, 32, 4, false, false, cutlass::half_t, Flash_kernel_traits<512, 64, 32, 4, cutlass::half_t>>, Is_dropout=false, Is_causal=true]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(264): here
            instantiation of "void run_mha_fwd_hdim512<T>(Flash_fwd_params &, cudaStream_t) [with T=cutlass::half_t]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_hdim512_fp16_sm80.cu(8): here

/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_kernel.h(140): warning: variable "MMA_M" was declared but never referenced
          detected during:
            instantiation of "void flash::compute_attn_1rowblock<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &, int, int, int) [with Kernel_traits=Flash_fwd_kernel_traits<512, 64, 32, 4, false, false, cutlass::half_t, Flash_kernel_traits<512, 64, 32, 4, cutlass::half_t>>, Is_dropout=false, Is_causal=true, Is_even_N=false, Is_even_K=true, Return_softmax=false, Params=Flash_fwd_params]" 
(588): here
            instantiation of "void flash::compute_attn<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &) [with Kernel_traits=Flash_fwd_kernel_traits<512, 64, 32, 4, false, false, cutlass::half_t, Flash_kernel_traits<512, 64, 32, 4, cutlass::half_t>>, Is_dropout=false, Is_causal=true, Is_even_N=false, Is_even_K=true, Return_softmax=false, Params=Flash_fwd_params]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(15): here
            instantiation of "void flash_fwd_kernel<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax>(Flash_fwd_params) [with Kernel_traits=Flash_fwd_kernel_traits<512, 64, 32, 4, false, false, cutlass::half_t, Flash_kernel_traits<512, 64, 32, 4, cutlass::half_t>>, Is_dropout=false, Is_causal=true, Is_even_N=false, Is_even_K=true, Return_softmax=false]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(34): here
            instantiation of "void run_flash_fwd<Kernel_traits,Is_dropout,Is_causal>(Flash_fwd_params &, cudaStream_t) [with Kernel_traits=Flash_fwd_kernel_traits<512, 64, 32, 4, false, false, cutlass::half_t, Flash_kernel_traits<512, 64, 32, 4, cutlass::half_t>>, Is_dropout=false, Is_causal=true]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(264): here
            instantiation of "void run_mha_fwd_hdim512<T>(Flash_fwd_params &, cudaStream_t) [with T=cutlass::half_t]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_hdim512_fp16_sm80.cu(8): here

/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_kernel.h(275): warning: variable "tQrQ" was declared but never referenced
          detected during:
            instantiation of "void flash::compute_attn_1rowblock<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &, int, int, int) [with Kernel_traits=Flash_fwd_kernel_traits<512, 64, 32, 4, false, false, cutlass::half_t, Flash_kernel_traits<512, 64, 32, 4, cutlass::half_t>>, Is_dropout=false, Is_causal=true, Is_even_N=false, Is_even_K=true, Return_softmax=false, Params=Flash_fwd_params]" 
(588): here
            instantiation of "void flash::compute_attn<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &) [with Kernel_traits=Flash_fwd_kernel_traits<512, 64, 32, 4, false, false, cutlass::half_t, Flash_kernel_traits<512, 64, 32, 4, cutlass::half_t>>, Is_dropout=false, Is_causal=true, Is_even_N=false, Is_even_K=true, Return_softmax=false, Params=Flash_fwd_params]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(15): here
            instantiation of "void flash_fwd_kernel<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax>(Flash_fwd_params) [with Kernel_traits=Flash_fwd_kernel_traits<512, 64, 32, 4, false, false, cutlass::half_t, Flash_kernel_traits<512, 64, 32, 4, cutlass::half_t>>, Is_dropout=false, Is_causal=true, Is_even_N=false, Is_even_K=true, Return_softmax=false]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(34): here
            instantiation of "void run_flash_fwd<Kernel_traits,Is_dropout,Is_causal>(Flash_fwd_params &, cudaStream_t) [with Kernel_traits=Flash_fwd_kernel_traits<512, 64, 32, 4, false, false, cutlass::half_t, Flash_kernel_traits<512, 64, 32, 4, cutlass::half_t>>, Is_dropout=false, Is_causal=true]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(264): here
            instantiation of "void run_mha_fwd_hdim512<T>(Flash_fwd_params &, cudaStream_t) [with T=cutlass::half_t]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_hdim512_fp16_sm80.cu(8): here

/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_kernel.h(140): warning: variable "MMA_M" was declared but never referenced
          detected during:
            instantiation of "void flash::compute_attn_1rowblock<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &, int, int, int) [with Kernel_traits=Flash_fwd_kernel_traits<512, 64, 32, 4, false, false, cutlass::half_t, Flash_kernel_traits<512, 64, 32, 4, cutlass::half_t>>, Is_dropout=false, Is_causal=true, Is_even_N=false, Is_even_K=false, Return_softmax=false, Params=Flash_fwd_params]" 
(588): here
            instantiation of "void flash::compute_attn<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &) [with Kernel_traits=Flash_fwd_kernel_traits<512, 64, 32, 4, false, false, cutlass::half_t, Flash_kernel_traits<512, 64, 32, 4, cutlass::half_t>>, Is_dropout=false, Is_causal=true, Is_even_N=false, Is_even_K=false, Return_softmax=false, Params=Flash_fwd_params]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(15): here
            instantiation of "void flash_fwd_kernel<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax>(Flash_fwd_params) [with Kernel_traits=Flash_fwd_kernel_traits<512, 64, 32, 4, false, false, cutlass::half_t, Flash_kernel_traits<512, 64, 32, 4, cutlass::half_t>>, Is_dropout=false, Is_causal=true, Is_even_N=false, Is_even_K=false, Return_softmax=false]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(34): here
            instantiation of "void run_flash_fwd<Kernel_traits,Is_dropout,Is_causal>(Flash_fwd_params &, cudaStream_t) [with Kernel_traits=Flash_fwd_kernel_traits<512, 64, 32, 4, false, false, cutlass::half_t, Flash_kernel_traits<512, 64, 32, 4, cutlass::half_t>>, Is_dropout=false, Is_causal=true]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(264): here
            instantiation of "void run_mha_fwd_hdim512<T>(Flash_fwd_params &, cudaStream_t) [with T=cutlass::half_t]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_hdim512_fp16_sm80.cu(8): here

/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_kernel.h(275): warning: variable "tQrQ" was declared but never referenced
          detected during:
            instantiation of "void flash::compute_attn_1rowblock<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &, int, int, int) [with Kernel_traits=Flash_fwd_kernel_traits<512, 64, 32, 4, false, false, cutlass::half_t, Flash_kernel_traits<512, 64, 32, 4, cutlass::half_t>>, Is_dropout=false, Is_causal=true, Is_even_N=false, Is_even_K=false, Return_softmax=false, Params=Flash_fwd_params]" 
(588): here
            instantiation of "void flash::compute_attn<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &) [with Kernel_traits=Flash_fwd_kernel_traits<512, 64, 32, 4, false, false, cutlass::half_t, Flash_kernel_traits<512, 64, 32, 4, cutlass::half_t>>, Is_dropout=false, Is_causal=true, Is_even_N=false, Is_even_K=false, Return_softmax=false, Params=Flash_fwd_params]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(15): here
            instantiation of "void flash_fwd_kernel<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax>(Flash_fwd_params) [with Kernel_traits=Flash_fwd_kernel_traits<512, 64, 32, 4, false, false, cutlass::half_t, Flash_kernel_traits<512, 64, 32, 4, cutlass::half_t>>, Is_dropout=false, Is_causal=true, Is_even_N=false, Is_even_K=false, Return_softmax=false]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(34): here
            instantiation of "void run_flash_fwd<Kernel_traits,Is_dropout,Is_causal>(Flash_fwd_params &, cudaStream_t) [with Kernel_traits=Flash_fwd_kernel_traits<512, 64, 32, 4, false, false, cutlass::half_t, Flash_kernel_traits<512, 64, 32, 4, cutlass::half_t>>, Is_dropout=false, Is_causal=true]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(264): here
            instantiation of "void run_mha_fwd_hdim512<T>(Flash_fwd_params &, cudaStream_t) [with T=cutlass::half_t]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_hdim512_fp16_sm80.cu(8): here

/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_kernel.h(140): warning: variable "MMA_M" was declared but never referenced
          detected during:
            instantiation of "void flash::compute_attn_1rowblock<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &, int, int, int) [with Kernel_traits=Flash_fwd_kernel_traits<512, 32, 32, 2, false, false, cutlass::half_t, Flash_kernel_traits<512, 32, 32, 2, cutlass::half_t>>, Is_dropout=false, Is_causal=true, Is_even_N=true, Is_even_K=true, Return_softmax=false, Params=Flash_fwd_params]" 
(588): here
            instantiation of "void flash::compute_attn<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &) [with Kernel_traits=Flash_fwd_kernel_traits<512, 32, 32, 2, false, false, cutlass::half_t, Flash_kernel_traits<512, 32, 32, 2, cutlass::half_t>>, Is_dropout=false, Is_causal=true, Is_even_N=true, Is_even_K=true, Return_softmax=false, Params=Flash_fwd_params]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(15): here
            instantiation of "void flash_fwd_kernel<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax>(Flash_fwd_params) [with Kernel_traits=Flash_fwd_kernel_traits<512, 32, 32, 2, false, false, cutlass::half_t, Flash_kernel_traits<512, 32, 32, 2, cutlass::half_t>>, Is_dropout=false, Is_causal=true, Is_even_N=true, Is_even_K=true, Return_softmax=false]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(34): here
            instantiation of "void run_flash_fwd<Kernel_traits,Is_dropout,Is_causal>(Flash_fwd_params &, cudaStream_t) [with Kernel_traits=Flash_fwd_kernel_traits<512, 32, 32, 2, false, false, cutlass::half_t, Flash_kernel_traits<512, 32, 32, 2, cutlass::half_t>>, Is_dropout=false, Is_causal=true]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(264): here
            instantiation of "void run_mha_fwd_hdim512<T>(Flash_fwd_params &, cudaStream_t) [with T=cutlass::half_t]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_hdim512_fp16_sm80.cu(8): here

/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_kernel.h(275): warning: variable "tQrQ" was declared but never referenced
          detected during:
            instantiation of "void flash::compute_attn_1rowblock<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &, int, int, int) [with Kernel_traits=Flash_fwd_kernel_traits<512, 32, 32, 2, false, false, cutlass::half_t, Flash_kernel_traits<512, 32, 32, 2, cutlass::half_t>>, Is_dropout=false, Is_causal=true, Is_even_N=true, Is_even_K=true, Return_softmax=false, Params=Flash_fwd_params]" 
(588): here
            instantiation of "void flash::compute_attn<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &) [with Kernel_traits=Flash_fwd_kernel_traits<512, 32, 32, 2, false, false, cutlass::half_t, Flash_kernel_traits<512, 32, 32, 2, cutlass::half_t>>, Is_dropout=false, Is_causal=true, Is_even_N=true, Is_even_K=true, Return_softmax=false, Params=Flash_fwd_params]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(15): here
            instantiation of "void flash_fwd_kernel<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax>(Flash_fwd_params) [with Kernel_traits=Flash_fwd_kernel_traits<512, 32, 32, 2, false, false, cutlass::half_t, Flash_kernel_traits<512, 32, 32, 2, cutlass::half_t>>, Is_dropout=false, Is_causal=true, Is_even_N=true, Is_even_K=true, Return_softmax=false]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(34): here
            instantiation of "void run_flash_fwd<Kernel_traits,Is_dropout,Is_causal>(Flash_fwd_params &, cudaStream_t) [with Kernel_traits=Flash_fwd_kernel_traits<512, 32, 32, 2, false, false, cutlass::half_t, Flash_kernel_traits<512, 32, 32, 2, cutlass::half_t>>, Is_dropout=false, Is_causal=true]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(264): here
            instantiation of "void run_mha_fwd_hdim512<T>(Flash_fwd_params &, cudaStream_t) [with T=cutlass::half_t]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_hdim512_fp16_sm80.cu(8): here

/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_kernel.h(140): warning: variable "MMA_M" was declared but never referenced
          detected during:
            instantiation of "void flash::compute_attn_1rowblock<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &, int, int, int) [with Kernel_traits=Flash_fwd_kernel_traits<512, 32, 32, 2, false, false, cutlass::half_t, Flash_kernel_traits<512, 32, 32, 2, cutlass::half_t>>, Is_dropout=false, Is_causal=true, Is_even_N=true, Is_even_K=false, Return_softmax=false, Params=Flash_fwd_params]" 
(588): here
            instantiation of "void flash::compute_attn<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &) [with Kernel_traits=Flash_fwd_kernel_traits<512, 32, 32, 2, false, false, cutlass::half_t, Flash_kernel_traits<512, 32, 32, 2, cutlass::half_t>>, Is_dropout=false, Is_causal=true, Is_even_N=true, Is_even_K=false, Return_softmax=false, Params=Flash_fwd_params]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(15): here
            instantiation of "void flash_fwd_kernel<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax>(Flash_fwd_params) [with Kernel_traits=Flash_fwd_kernel_traits<512, 32, 32, 2, false, false, cutlass::half_t, Flash_kernel_traits<512, 32, 32, 2, cutlass::half_t>>, Is_dropout=false, Is_causal=true, Is_even_N=true, Is_even_K=false, Return_softmax=false]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(34): here
            instantiation of "void run_flash_fwd<Kernel_traits,Is_dropout,Is_causal>(Flash_fwd_params &, cudaStream_t) [with Kernel_traits=Flash_fwd_kernel_traits<512, 32, 32, 2, false, false, cutlass::half_t, Flash_kernel_traits<512, 32, 32, 2, cutlass::half_t>>, Is_dropout=false, Is_causal=true]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(264): here
            instantiation of "void run_mha_fwd_hdim512<T>(Flash_fwd_params &, cudaStream_t) [with T=cutlass::half_t]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_hdim512_fp16_sm80.cu(8): here

/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_kernel.h(275): warning: variable "tQrQ" was declared but never referenced
          detected during:
            instantiation of "void flash::compute_attn_1rowblock<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &, int, int, int) [with Kernel_traits=Flash_fwd_kernel_traits<512, 32, 32, 2, false, false, cutlass::half_t, Flash_kernel_traits<512, 32, 32, 2, cutlass::half_t>>, Is_dropout=false, Is_causal=true, Is_even_N=true, Is_even_K=false, Return_softmax=false, Params=Flash_fwd_params]" 
(588): here
            instantiation of "void flash::compute_attn<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &) [with Kernel_traits=Flash_fwd_kernel_traits<512, 32, 32, 2, false, false, cutlass::half_t, Flash_kernel_traits<512, 32, 32, 2, cutlass::half_t>>, Is_dropout=false, Is_causal=true, Is_even_N=true, Is_even_K=false, Return_softmax=false, Params=Flash_fwd_params]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(15): here
            instantiation of "void flash_fwd_kernel<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax>(Flash_fwd_params) [with Kernel_traits=Flash_fwd_kernel_traits<512, 32, 32, 2, false, false, cutlass::half_t, Flash_kernel_traits<512, 32, 32, 2, cutlass::half_t>>, Is_dropout=false, Is_causal=true, Is_even_N=true, Is_even_K=false, Return_softmax=false]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(34): here
            instantiation of "void run_flash_fwd<Kernel_traits,Is_dropout,Is_causal>(Flash_fwd_params &, cudaStream_t) [with Kernel_traits=Flash_fwd_kernel_traits<512, 32, 32, 2, false, false, cutlass::half_t, Flash_kernel_traits<512, 32, 32, 2, cutlass::half_t>>, Is_dropout=false, Is_causal=true]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(264): here
            instantiation of "void run_mha_fwd_hdim512<T>(Flash_fwd_params &, cudaStream_t) [with T=cutlass::half_t]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_hdim512_fp16_sm80.cu(8): here

/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_kernel.h(140): warning: variable "MMA_M" was declared but never referenced
          detected during:
            instantiation of "void flash::compute_attn_1rowblock<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &, int, int, int) [with Kernel_traits=Flash_fwd_kernel_traits<512, 32, 32, 2, false, false, cutlass::half_t, Flash_kernel_traits<512, 32, 32, 2, cutlass::half_t>>, Is_dropout=false, Is_causal=true, Is_even_N=false, Is_even_K=true, Return_softmax=false, Params=Flash_fwd_params]" 
(588): here
            instantiation of "void flash::compute_attn<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &) [with Kernel_traits=Flash_fwd_kernel_traits<512, 32, 32, 2, false, false, cutlass::half_t, Flash_kernel_traits<512, 32, 32, 2, cutlass::half_t>>, Is_dropout=false, Is_causal=true, Is_even_N=false, Is_even_K=true, Return_softmax=false, Params=Flash_fwd_params]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(15): here
            instantiation of "void flash_fwd_kernel<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax>(Flash_fwd_params) [with Kernel_traits=Flash_fwd_kernel_traits<512, 32, 32, 2, false, false, cutlass::half_t, Flash_kernel_traits<512, 32, 32, 2, cutlass::half_t>>, Is_dropout=false, Is_causal=true, Is_even_N=false, Is_even_K=true, Return_softmax=false]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(34): here
            instantiation of "void run_flash_fwd<Kernel_traits,Is_dropout,Is_causal>(Flash_fwd_params &, cudaStream_t) [with Kernel_traits=Flash_fwd_kernel_traits<512, 32, 32, 2, false, false, cutlass::half_t, Flash_kernel_traits<512, 32, 32, 2, cutlass::half_t>>, Is_dropout=false, Is_causal=true]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(264): here
            instantiation of "void run_mha_fwd_hdim512<T>(Flash_fwd_params &, cudaStream_t) [with T=cutlass::half_t]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_hdim512_fp16_sm80.cu(8): here

/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_kernel.h(275): warning: variable "tQrQ" was declared but never referenced
          detected during:
            instantiation of "void flash::compute_attn_1rowblock<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &, int, int, int) [with Kernel_traits=Flash_fwd_kernel_traits<512, 32, 32, 2, false, false, cutlass::half_t, Flash_kernel_traits<512, 32, 32, 2, cutlass::half_t>>, Is_dropout=false, Is_causal=true, Is_even_N=false, Is_even_K=true, Return_softmax=false, Params=Flash_fwd_params]" 
(588): here
            instantiation of "void flash::compute_attn<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &) [with Kernel_traits=Flash_fwd_kernel_traits<512, 32, 32, 2, false, false, cutlass::half_t, Flash_kernel_traits<512, 32, 32, 2, cutlass::half_t>>, Is_dropout=false, Is_causal=true, Is_even_N=false, Is_even_K=true, Return_softmax=false, Params=Flash_fwd_params]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(15): here
            instantiation of "void flash_fwd_kernel<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax>(Flash_fwd_params) [with Kernel_traits=Flash_fwd_kernel_traits<512, 32, 32, 2, false, false, cutlass::half_t, Flash_kernel_traits<512, 32, 32, 2, cutlass::half_t>>, Is_dropout=false, Is_causal=true, Is_even_N=false, Is_even_K=true, Return_softmax=false]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(34): here
            instantiation of "void run_flash_fwd<Kernel_traits,Is_dropout,Is_causal>(Flash_fwd_params &, cudaStream_t) [with Kernel_traits=Flash_fwd_kernel_traits<512, 32, 32, 2, false, false, cutlass::half_t, Flash_kernel_traits<512, 32, 32, 2, cutlass::half_t>>, Is_dropout=false, Is_causal=true]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(264): here
            instantiation of "void run_mha_fwd_hdim512<T>(Flash_fwd_params &, cudaStream_t) [with T=cutlass::half_t]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_hdim512_fp16_sm80.cu(8): here

/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_kernel.h(140): warning: variable "MMA_M" was declared but never referenced
          detected during:
            instantiation of "void flash::compute_attn_1rowblock<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &, int, int, int) [with Kernel_traits=Flash_fwd_kernel_traits<512, 32, 32, 2, false, false, cutlass::half_t, Flash_kernel_traits<512, 32, 32, 2, cutlass::half_t>>, Is_dropout=false, Is_causal=true, Is_even_N=false, Is_even_K=false, Return_softmax=false, Params=Flash_fwd_params]" 
(588): here
            instantiation of "void flash::compute_attn<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &) [with Kernel_traits=Flash_fwd_kernel_traits<512, 32, 32, 2, false, false, cutlass::half_t, Flash_kernel_traits<512, 32, 32, 2, cutlass::half_t>>, Is_dropout=false, Is_causal=true, Is_even_N=false, Is_even_K=false, Return_softmax=false, Params=Flash_fwd_params]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(15): here
            instantiation of "void flash_fwd_kernel<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax>(Flash_fwd_params) [with Kernel_traits=Flash_fwd_kernel_traits<512, 32, 32, 2, false, false, cutlass::half_t, Flash_kernel_traits<512, 32, 32, 2, cutlass::half_t>>, Is_dropout=false, Is_causal=true, Is_even_N=false, Is_even_K=false, Return_softmax=false]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(34): here
            instantiation of "void run_flash_fwd<Kernel_traits,Is_dropout,Is_causal>(Flash_fwd_params &, cudaStream_t) [with Kernel_traits=Flash_fwd_kernel_traits<512, 32, 32, 2, false, false, cutlass::half_t, Flash_kernel_traits<512, 32, 32, 2, cutlass::half_t>>, Is_dropout=false, Is_causal=true]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(264): here
            instantiation of "void run_mha_fwd_hdim512<T>(Flash_fwd_params &, cudaStream_t) [with T=cutlass::half_t]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_hdim512_fp16_sm80.cu(8): here

/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_kernel.h(275): warning: variable "tQrQ" was declared but never referenced
          detected during:
            instantiation of "void flash::compute_attn_1rowblock<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &, int, int, int) [with Kernel_traits=Flash_fwd_kernel_traits<512, 32, 32, 2, false, false, cutlass::half_t, Flash_kernel_traits<512, 32, 32, 2, cutlass::half_t>>, Is_dropout=false, Is_causal=true, Is_even_N=false, Is_even_K=false, Return_softmax=false, Params=Flash_fwd_params]" 
(588): here
            instantiation of "void flash::compute_attn<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &) [with Kernel_traits=Flash_fwd_kernel_traits<512, 32, 32, 2, false, false, cutlass::half_t, Flash_kernel_traits<512, 32, 32, 2, cutlass::half_t>>, Is_dropout=false, Is_causal=true, Is_even_N=false, Is_even_K=false, Return_softmax=false, Params=Flash_fwd_params]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(15): here
            instantiation of "void flash_fwd_kernel<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax>(Flash_fwd_params) [with Kernel_traits=Flash_fwd_kernel_traits<512, 32, 32, 2, false, false, cutlass::half_t, Flash_kernel_traits<512, 32, 32, 2, cutlass::half_t>>, Is_dropout=false, Is_causal=true, Is_even_N=false, Is_even_K=false, Return_softmax=false]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(34): here
            instantiation of "void run_flash_fwd<Kernel_traits,Is_dropout,Is_causal>(Flash_fwd_params &, cudaStream_t) [with Kernel_traits=Flash_fwd_kernel_traits<512, 32, 32, 2, false, false, cutlass::half_t, Flash_kernel_traits<512, 32, 32, 2, cutlass::half_t>>, Is_dropout=false, Is_causal=true]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(264): here
            instantiation of "void run_mha_fwd_hdim512<T>(Flash_fwd_params &, cudaStream_t) [with T=cutlass::half_t]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_hdim512_fp16_sm80.cu(8): here

/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_kernel.h(140): warning: variable "MMA_M" was declared but never referenced
          detected during:
            instantiation of "void flash::compute_attn_1rowblock<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &, int, int, int) [with Kernel_traits=Flash_fwd_kernel_traits<512, 64, 32, 4, false, false, cutlass::half_t, Flash_kernel_traits<512, 64, 32, 4, cutlass::half_t>>, Is_dropout=false, Is_causal=false, Is_even_N=true, Is_even_K=true, Return_softmax=false, Params=Flash_fwd_params]" 
(588): here
            instantiation of "void flash::compute_attn<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &) [with Kernel_traits=Flash_fwd_kernel_traits<512, 64, 32, 4, false, false, cutlass::half_t, Flash_kernel_traits<512, 64, 32, 4, cutlass::half_t>>, Is_dropout=false, Is_causal=false, Is_even_N=true, Is_even_K=true, Return_softmax=false, Params=Flash_fwd_params]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(15): here
            instantiation of "void flash_fwd_kernel<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax>(Flash_fwd_params) [with Kernel_traits=Flash_fwd_kernel_traits<512, 64, 32, 4, false, false, cutlass::half_t, Flash_kernel_traits<512, 64, 32, 4, cutlass::half_t>>, Is_dropout=false, Is_causal=false, Is_even_N=true, Is_even_K=true, Return_softmax=false]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(34): here
            instantiation of "void run_flash_fwd<Kernel_traits,Is_dropout,Is_causal>(Flash_fwd_params &, cudaStream_t) [with Kernel_traits=Flash_fwd_kernel_traits<512, 64, 32, 4, false, false, cutlass::half_t, Flash_kernel_traits<512, 64, 32, 4, cutlass::half_t>>, Is_dropout=false, Is_causal=false]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(264): here
            instantiation of "void run_mha_fwd_hdim512<T>(Flash_fwd_params &, cudaStream_t) [with T=cutlass::half_t]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_hdim512_fp16_sm80.cu(8): here

/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_kernel.h(275): warning: variable "tQrQ" was declared but never referenced
          detected during:
            instantiation of "void flash::compute_attn_1rowblock<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &, int, int, int) [with Kernel_traits=Flash_fwd_kernel_traits<512, 64, 32, 4, false, false, cutlass::half_t, Flash_kernel_traits<512, 64, 32, 4, cutlass::half_t>>, Is_dropout=false, Is_causal=false, Is_even_N=true, Is_even_K=true, Return_softmax=false, Params=Flash_fwd_params]" 
(588): here
            instantiation of "void flash::compute_attn<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &) [with Kernel_traits=Flash_fwd_kernel_traits<512, 64, 32, 4, false, false, cutlass::half_t, Flash_kernel_traits<512, 64, 32, 4, cutlass::half_t>>, Is_dropout=false, Is_causal=false, Is_even_N=true, Is_even_K=true, Return_softmax=false, Params=Flash_fwd_params]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(15): here
            instantiation of "void flash_fwd_kernel<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax>(Flash_fwd_params) [with Kernel_traits=Flash_fwd_kernel_traits<512, 64, 32, 4, false, false, cutlass::half_t, Flash_kernel_traits<512, 64, 32, 4, cutlass::half_t>>, Is_dropout=false, Is_causal=false, Is_even_N=true, Is_even_K=true, Return_softmax=false]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(34): here
            instantiation of "void run_flash_fwd<Kernel_traits,Is_dropout,Is_causal>(Flash_fwd_params &, cudaStream_t) [with Kernel_traits=Flash_fwd_kernel_traits<512, 64, 32, 4, false, false, cutlass::half_t, Flash_kernel_traits<512, 64, 32, 4, cutlass::half_t>>, Is_dropout=false, Is_causal=false]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(264): here
            instantiation of "void run_mha_fwd_hdim512<T>(Flash_fwd_params &, cudaStream_t) [with T=cutlass::half_t]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_hdim512_fp16_sm80.cu(8): here

/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_kernel.h(140): warning: variable "MMA_M" was declared but never referenced
          detected during:
            instantiation of "void flash::compute_attn_1rowblock<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &, int, int, int) [with Kernel_traits=Flash_fwd_kernel_traits<512, 64, 32, 4, false, false, cutlass::half_t, Flash_kernel_traits<512, 64, 32, 4, cutlass::half_t>>, Is_dropout=false, Is_causal=false, Is_even_N=true, Is_even_K=false, Return_softmax=false, Params=Flash_fwd_params]" 
(588): here
            instantiation of "void flash::compute_attn<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &) [with Kernel_traits=Flash_fwd_kernel_traits<512, 64, 32, 4, false, false, cutlass::half_t, Flash_kernel_traits<512, 64, 32, 4, cutlass::half_t>>, Is_dropout=false, Is_causal=false, Is_even_N=true, Is_even_K=false, Return_softmax=false, Params=Flash_fwd_params]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(15): here
            instantiation of "void flash_fwd_kernel<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax>(Flash_fwd_params) [with Kernel_traits=Flash_fwd_kernel_traits<512, 64, 32, 4, false, false, cutlass::half_t, Flash_kernel_traits<512, 64, 32, 4, cutlass::half_t>>, Is_dropout=false, Is_causal=false, Is_even_N=true, Is_even_K=false, Return_softmax=false]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(34): here
            instantiation of "void run_flash_fwd<Kernel_traits,Is_dropout,Is_causal>(Flash_fwd_params &, cudaStream_t) [with Kernel_traits=Flash_fwd_kernel_traits<512, 64, 32, 4, false, false, cutlass::half_t, Flash_kernel_traits<512, 64, 32, 4, cutlass::half_t>>, Is_dropout=false, Is_causal=false]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(264): here
            instantiation of "void run_mha_fwd_hdim512<T>(Flash_fwd_params &, cudaStream_t) [with T=cutlass::half_t]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_hdim512_fp16_sm80.cu(8): here

/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_kernel.h(275): warning: variable "tQrQ" was declared but never referenced
          detected during:
            instantiation of "void flash::compute_attn_1rowblock<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &, int, int, int) [with Kernel_traits=Flash_fwd_kernel_traits<512, 64, 32, 4, false, false, cutlass::half_t, Flash_kernel_traits<512, 64, 32, 4, cutlass::half_t>>, Is_dropout=false, Is_causal=false, Is_even_N=true, Is_even_K=false, Return_softmax=false, Params=Flash_fwd_params]" 
(588): here
            instantiation of "void flash::compute_attn<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &) [with Kernel_traits=Flash_fwd_kernel_traits<512, 64, 32, 4, false, false, cutlass::half_t, Flash_kernel_traits<512, 64, 32, 4, cutlass::half_t>>, Is_dropout=false, Is_causal=false, Is_even_N=true, Is_even_K=false, Return_softmax=false, Params=Flash_fwd_params]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(15): here
            instantiation of "void flash_fwd_kernel<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax>(Flash_fwd_params) [with Kernel_traits=Flash_fwd_kernel_traits<512, 64, 32, 4, false, false, cutlass::half_t, Flash_kernel_traits<512, 64, 32, 4, cutlass::half_t>>, Is_dropout=false, Is_causal=false, Is_even_N=true, Is_even_K=false, Return_softmax=false]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(34): here
            instantiation of "void run_flash_fwd<Kernel_traits,Is_dropout,Is_causal>(Flash_fwd_params &, cudaStream_t) [with Kernel_traits=Flash_fwd_kernel_traits<512, 64, 32, 4, false, false, cutlass::half_t, Flash_kernel_traits<512, 64, 32, 4, cutlass::half_t>>, Is_dropout=false, Is_causal=false]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(264): here
            instantiation of "void run_mha_fwd_hdim512<T>(Flash_fwd_params &, cudaStream_t) [with T=cutlass::half_t]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_hdim512_fp16_sm80.cu(8): here

/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_kernel.h(140): warning: variable "MMA_M" was declared but never referenced
          detected during:
            instantiation of "void flash::compute_attn_1rowblock<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &, int, int, int) [with Kernel_traits=Flash_fwd_kernel_traits<512, 64, 32, 4, false, false, cutlass::half_t, Flash_kernel_traits<512, 64, 32, 4, cutlass::half_t>>, Is_dropout=false, Is_causal=false, Is_even_N=false, Is_even_K=true, Return_softmax=false, Params=Flash_fwd_params]" 
(588): here
            instantiation of "void flash::compute_attn<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &) [with Kernel_traits=Flash_fwd_kernel_traits<512, 64, 32, 4, false, false, cutlass::half_t, Flash_kernel_traits<512, 64, 32, 4, cutlass::half_t>>, Is_dropout=false, Is_causal=false, Is_even_N=false, Is_even_K=true, Return_softmax=false, Params=Flash_fwd_params]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(15): here
            instantiation of "void flash_fwd_kernel<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax>(Flash_fwd_params) [with Kernel_traits=Flash_fwd_kernel_traits<512, 64, 32, 4, false, false, cutlass::half_t, Flash_kernel_traits<512, 64, 32, 4, cutlass::half_t>>, Is_dropout=false, Is_causal=false, Is_even_N=false, Is_even_K=true, Return_softmax=false]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(34): here
            instantiation of "void run_flash_fwd<Kernel_traits,Is_dropout,Is_causal>(Flash_fwd_params &, cudaStream_t) [with Kernel_traits=Flash_fwd_kernel_traits<512, 64, 32, 4, false, false, cutlass::half_t, Flash_kernel_traits<512, 64, 32, 4, cutlass::half_t>>, Is_dropout=false, Is_causal=false]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(264): here
            instantiation of "void run_mha_fwd_hdim512<T>(Flash_fwd_params &, cudaStream_t) [with T=cutlass::half_t]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_hdim512_fp16_sm80.cu(8): here

/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_kernel.h(275): warning: variable "tQrQ" was declared but never referenced
          detected during:
            instantiation of "void flash::compute_attn_1rowblock<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &, int, int, int) [with Kernel_traits=Flash_fwd_kernel_traits<512, 64, 32, 4, false, false, cutlass::half_t, Flash_kernel_traits<512, 64, 32, 4, cutlass::half_t>>, Is_dropout=false, Is_causal=false, Is_even_N=false, Is_even_K=true, Return_softmax=false, Params=Flash_fwd_params]" 
(588): here
            instantiation of "void flash::compute_attn<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &) [with Kernel_traits=Flash_fwd_kernel_traits<512, 64, 32, 4, false, false, cutlass::half_t, Flash_kernel_traits<512, 64, 32, 4, cutlass::half_t>>, Is_dropout=false, Is_causal=false, Is_even_N=false, Is_even_K=true, Return_softmax=false, Params=Flash_fwd_params]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(15): here
            instantiation of "void flash_fwd_kernel<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax>(Flash_fwd_params) [with Kernel_traits=Flash_fwd_kernel_traits<512, 64, 32, 4, false, false, cutlass::half_t, Flash_kernel_traits<512, 64, 32, 4, cutlass::half_t>>, Is_dropout=false, Is_causal=false, Is_even_N=false, Is_even_K=true, Return_softmax=false]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(34): here
            instantiation of "void run_flash_fwd<Kernel_traits,Is_dropout,Is_causal>(Flash_fwd_params &, cudaStream_t) [with Kernel_traits=Flash_fwd_kernel_traits<512, 64, 32, 4, false, false, cutlass::half_t, Flash_kernel_traits<512, 64, 32, 4, cutlass::half_t>>, Is_dropout=false, Is_causal=false]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(264): here
            instantiation of "void run_mha_fwd_hdim512<T>(Flash_fwd_params &, cudaStream_t) [with T=cutlass::half_t]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_hdim512_fp16_sm80.cu(8): here

/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_kernel.h(140): warning: variable "MMA_M" was declared but never referenced
          detected during:
            instantiation of "void flash::compute_attn_1rowblock<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &, int, int, int) [with Kernel_traits=Flash_fwd_kernel_traits<512, 64, 32, 4, false, false, cutlass::half_t, Flash_kernel_traits<512, 64, 32, 4, cutlass::half_t>>, Is_dropout=false, Is_causal=false, Is_even_N=false, Is_even_K=false, Return_softmax=false, Params=Flash_fwd_params]" 
(588): here
            instantiation of "void flash::compute_attn<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &) [with Kernel_traits=Flash_fwd_kernel_traits<512, 64, 32, 4, false, false, cutlass::half_t, Flash_kernel_traits<512, 64, 32, 4, cutlass::half_t>>, Is_dropout=false, Is_causal=false, Is_even_N=false, Is_even_K=false, Return_softmax=false, Params=Flash_fwd_params]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(15): here
            instantiation of "void flash_fwd_kernel<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax>(Flash_fwd_params) [with Kernel_traits=Flash_fwd_kernel_traits<512, 64, 32, 4, false, false, cutlass::half_t, Flash_kernel_traits<512, 64, 32, 4, cutlass::half_t>>, Is_dropout=false, Is_causal=false, Is_even_N=false, Is_even_K=false, Return_softmax=false]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(34): here
            instantiation of "void run_flash_fwd<Kernel_traits,Is_dropout,Is_causal>(Flash_fwd_params &, cudaStream_t) [with Kernel_traits=Flash_fwd_kernel_traits<512, 64, 32, 4, false, false, cutlass::half_t, Flash_kernel_traits<512, 64, 32, 4, cutlass::half_t>>, Is_dropout=false, Is_causal=false]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(264): here
            instantiation of "void run_mha_fwd_hdim512<T>(Flash_fwd_params &, cudaStream_t) [with T=cutlass::half_t]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_hdim512_fp16_sm80.cu(8): here

/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_kernel.h(275): warning: variable "tQrQ" was declared but never referenced
          detected during:
            instantiation of "void flash::compute_attn_1rowblock<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &, int, int, int) [with Kernel_traits=Flash_fwd_kernel_traits<512, 64, 32, 4, false, false, cutlass::half_t, Flash_kernel_traits<512, 64, 32, 4, cutlass::half_t>>, Is_dropout=false, Is_causal=false, Is_even_N=false, Is_even_K=false, Return_softmax=false, Params=Flash_fwd_params]" 
(588): here
            instantiation of "void flash::compute_attn<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &) [with Kernel_traits=Flash_fwd_kernel_traits<512, 64, 32, 4, false, false, cutlass::half_t, Flash_kernel_traits<512, 64, 32, 4, cutlass::half_t>>, Is_dropout=false, Is_causal=false, Is_even_N=false, Is_even_K=false, Return_softmax=false, Params=Flash_fwd_params]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(15): here
            instantiation of "void flash_fwd_kernel<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax>(Flash_fwd_params) [with Kernel_traits=Flash_fwd_kernel_traits<512, 64, 32, 4, false, false, cutlass::half_t, Flash_kernel_traits<512, 64, 32, 4, cutlass::half_t>>, Is_dropout=false, Is_causal=false, Is_even_N=false, Is_even_K=false, Return_softmax=false]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(34): here
            instantiation of "void run_flash_fwd<Kernel_traits,Is_dropout,Is_causal>(Flash_fwd_params &, cudaStream_t) [with Kernel_traits=Flash_fwd_kernel_traits<512, 64, 32, 4, false, false, cutlass::half_t, Flash_kernel_traits<512, 64, 32, 4, cutlass::half_t>>, Is_dropout=false, Is_causal=false]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(264): here
            instantiation of "void run_mha_fwd_hdim512<T>(Flash_fwd_params &, cudaStream_t) [with T=cutlass::half_t]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_hdim512_fp16_sm80.cu(8): here

/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_kernel.h(140): warning: variable "MMA_M" was declared but never referenced
          detected during:
            instantiation of "void flash::compute_attn_1rowblock<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &, int, int, int) [with Kernel_traits=Flash_fwd_kernel_traits<512, 32, 32, 2, false, false, cutlass::half_t, Flash_kernel_traits<512, 32, 32, 2, cutlass::half_t>>, Is_dropout=false, Is_causal=false, Is_even_N=true, Is_even_K=true, Return_softmax=false, Params=Flash_fwd_params]" 
(588): here
            instantiation of "void flash::compute_attn<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &) [with Kernel_traits=Flash_fwd_kernel_traits<512, 32, 32, 2, false, false, cutlass::half_t, Flash_kernel_traits<512, 32, 32, 2, cutlass::half_t>>, Is_dropout=false, Is_causal=false, Is_even_N=true, Is_even_K=true, Return_softmax=false, Params=Flash_fwd_params]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(15): here
            instantiation of "void flash_fwd_kernel<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax>(Flash_fwd_params) [with Kernel_traits=Flash_fwd_kernel_traits<512, 32, 32, 2, false, false, cutlass::half_t, Flash_kernel_traits<512, 32, 32, 2, cutlass::half_t>>, Is_dropout=false, Is_causal=false, Is_even_N=true, Is_even_K=true, Return_softmax=false]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(34): here
            instantiation of "void run_flash_fwd<Kernel_traits,Is_dropout,Is_causal>(Flash_fwd_params &, cudaStream_t) [with Kernel_traits=Flash_fwd_kernel_traits<512, 32, 32, 2, false, false, cutlass::half_t, Flash_kernel_traits<512, 32, 32, 2, cutlass::half_t>>, Is_dropout=false, Is_causal=false]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(264): here
            instantiation of "void run_mha_fwd_hdim512<T>(Flash_fwd_params &, cudaStream_t) [with T=cutlass::half_t]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_hdim512_fp16_sm80.cu(8): here

/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_kernel.h(275): warning: variable "tQrQ" was declared but never referenced
          detected during:
            instantiation of "void flash::compute_attn_1rowblock<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &, int, int, int) [with Kernel_traits=Flash_fwd_kernel_traits<512, 32, 32, 2, false, false, cutlass::half_t, Flash_kernel_traits<512, 32, 32, 2, cutlass::half_t>>, Is_dropout=false, Is_causal=false, Is_even_N=true, Is_even_K=true, Return_softmax=false, Params=Flash_fwd_params]" 
(588): here
            instantiation of "void flash::compute_attn<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &) [with Kernel_traits=Flash_fwd_kernel_traits<512, 32, 32, 2, false, false, cutlass::half_t, Flash_kernel_traits<512, 32, 32, 2, cutlass::half_t>>, Is_dropout=false, Is_causal=false, Is_even_N=true, Is_even_K=true, Return_softmax=false, Params=Flash_fwd_params]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(15): here
            instantiation of "void flash_fwd_kernel<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax>(Flash_fwd_params) [with Kernel_traits=Flash_fwd_kernel_traits<512, 32, 32, 2, false, false, cutlass::half_t, Flash_kernel_traits<512, 32, 32, 2, cutlass::half_t>>, Is_dropout=false, Is_causal=false, Is_even_N=true, Is_even_K=true, Return_softmax=false]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(34): here
            instantiation of "void run_flash_fwd<Kernel_traits,Is_dropout,Is_causal>(Flash_fwd_params &, cudaStream_t) [with Kernel_traits=Flash_fwd_kernel_traits<512, 32, 32, 2, false, false, cutlass::half_t, Flash_kernel_traits<512, 32, 32, 2, cutlass::half_t>>, Is_dropout=false, Is_causal=false]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(264): here
            instantiation of "void run_mha_fwd_hdim512<T>(Flash_fwd_params &, cudaStream_t) [with T=cutlass::half_t]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_hdim512_fp16_sm80.cu(8): here

/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_kernel.h(140): warning: variable "MMA_M" was declared but never referenced
          detected during:
            instantiation of "void flash::compute_attn_1rowblock<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &, int, int, int) [with Kernel_traits=Flash_fwd_kernel_traits<512, 32, 32, 2, false, false, cutlass::half_t, Flash_kernel_traits<512, 32, 32, 2, cutlass::half_t>>, Is_dropout=false, Is_causal=false, Is_even_N=true, Is_even_K=false, Return_softmax=false, Params=Flash_fwd_params]" 
(588): here
            instantiation of "void flash::compute_attn<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &) [with Kernel_traits=Flash_fwd_kernel_traits<512, 32, 32, 2, false, false, cutlass::half_t, Flash_kernel_traits<512, 32, 32, 2, cutlass::half_t>>, Is_dropout=false, Is_causal=false, Is_even_N=true, Is_even_K=false, Return_softmax=false, Params=Flash_fwd_params]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(15): here
            instantiation of "void flash_fwd_kernel<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax>(Flash_fwd_params) [with Kernel_traits=Flash_fwd_kernel_traits<512, 32, 32, 2, false, false, cutlass::half_t, Flash_kernel_traits<512, 32, 32, 2, cutlass::half_t>>, Is_dropout=false, Is_causal=false, Is_even_N=true, Is_even_K=false, Return_softmax=false]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(34): here
            instantiation of "void run_flash_fwd<Kernel_traits,Is_dropout,Is_causal>(Flash_fwd_params &, cudaStream_t) [with Kernel_traits=Flash_fwd_kernel_traits<512, 32, 32, 2, false, false, cutlass::half_t, Flash_kernel_traits<512, 32, 32, 2, cutlass::half_t>>, Is_dropout=false, Is_causal=false]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(264): here
            instantiation of "void run_mha_fwd_hdim512<T>(Flash_fwd_params &, cudaStream_t) [with T=cutlass::half_t]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_hdim512_fp16_sm80.cu(8): here

/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_kernel.h(275): warning: variable "tQrQ" was declared but never referenced
          detected during:
            instantiation of "void flash::compute_attn_1rowblock<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &, int, int, int) [with Kernel_traits=Flash_fwd_kernel_traits<512, 32, 32, 2, false, false, cutlass::half_t, Flash_kernel_traits<512, 32, 32, 2, cutlass::half_t>>, Is_dropout=false, Is_causal=false, Is_even_N=true, Is_even_K=false, Return_softmax=false, Params=Flash_fwd_params]" 
(588): here
            instantiation of "void flash::compute_attn<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &) [with Kernel_traits=Flash_fwd_kernel_traits<512, 32, 32, 2, false, false, cutlass::half_t, Flash_kernel_traits<512, 32, 32, 2, cutlass::half_t>>, Is_dropout=false, Is_causal=false, Is_even_N=true, Is_even_K=false, Return_softmax=false, Params=Flash_fwd_params]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(15): here
            instantiation of "void flash_fwd_kernel<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax>(Flash_fwd_params) [with Kernel_traits=Flash_fwd_kernel_traits<512, 32, 32, 2, false, false, cutlass::half_t, Flash_kernel_traits<512, 32, 32, 2, cutlass::half_t>>, Is_dropout=false, Is_causal=false, Is_even_N=true, Is_even_K=false, Return_softmax=false]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(34): here
            instantiation of "void run_flash_fwd<Kernel_traits,Is_dropout,Is_causal>(Flash_fwd_params &, cudaStream_t) [with Kernel_traits=Flash_fwd_kernel_traits<512, 32, 32, 2, false, false, cutlass::half_t, Flash_kernel_traits<512, 32, 32, 2, cutlass::half_t>>, Is_dropout=false, Is_causal=false]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(264): here
            instantiation of "void run_mha_fwd_hdim512<T>(Flash_fwd_params &, cudaStream_t) [with T=cutlass::half_t]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_hdim512_fp16_sm80.cu(8): here

/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_kernel.h(140): warning: variable "MMA_M" was declared but never referenced
          detected during:
            instantiation of "void flash::compute_attn_1rowblock<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &, int, int, int) [with Kernel_traits=Flash_fwd_kernel_traits<512, 32, 32, 2, false, false, cutlass::half_t, Flash_kernel_traits<512, 32, 32, 2, cutlass::half_t>>, Is_dropout=false, Is_causal=false, Is_even_N=false, Is_even_K=true, Return_softmax=false, Params=Flash_fwd_params]" 
(588): here
            instantiation of "void flash::compute_attn<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &) [with Kernel_traits=Flash_fwd_kernel_traits<512, 32, 32, 2, false, false, cutlass::half_t, Flash_kernel_traits<512, 32, 32, 2, cutlass::half_t>>, Is_dropout=false, Is_causal=false, Is_even_N=false, Is_even_K=true, Return_softmax=false, Params=Flash_fwd_params]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(15): here
            instantiation of "void flash_fwd_kernel<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax>(Flash_fwd_params) [with Kernel_traits=Flash_fwd_kernel_traits<512, 32, 32, 2, false, false, cutlass::half_t, Flash_kernel_traits<512, 32, 32, 2, cutlass::half_t>>, Is_dropout=false, Is_causal=false, Is_even_N=false, Is_even_K=true, Return_softmax=false]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(34): here
            instantiation of "void run_flash_fwd<Kernel_traits,Is_dropout,Is_causal>(Flash_fwd_params &, cudaStream_t) [with Kernel_traits=Flash_fwd_kernel_traits<512, 32, 32, 2, false, false, cutlass::half_t, Flash_kernel_traits<512, 32, 32, 2, cutlass::half_t>>, Is_dropout=false, Is_causal=false]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(264): here
            instantiation of "void run_mha_fwd_hdim512<T>(Flash_fwd_params &, cudaStream_t) [with T=cutlass::half_t]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_hdim512_fp16_sm80.cu(8): here

/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_kernel.h(275): warning: variable "tQrQ" was declared but never referenced
          detected during:
            instantiation of "void flash::compute_attn_1rowblock<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &, int, int, int) [with Kernel_traits=Flash_fwd_kernel_traits<512, 32, 32, 2, false, false, cutlass::half_t, Flash_kernel_traits<512, 32, 32, 2, cutlass::half_t>>, Is_dropout=false, Is_causal=false, Is_even_N=false, Is_even_K=true, Return_softmax=false, Params=Flash_fwd_params]" 
(588): here
            instantiation of "void flash::compute_attn<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &) [with Kernel_traits=Flash_fwd_kernel_traits<512, 32, 32, 2, false, false, cutlass::half_t, Flash_kernel_traits<512, 32, 32, 2, cutlass::half_t>>, Is_dropout=false, Is_causal=false, Is_even_N=false, Is_even_K=true, Return_softmax=false, Params=Flash_fwd_params]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(15): here
            instantiation of "void flash_fwd_kernel<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax>(Flash_fwd_params) [with Kernel_traits=Flash_fwd_kernel_traits<512, 32, 32, 2, false, false, cutlass::half_t, Flash_kernel_traits<512, 32, 32, 2, cutlass::half_t>>, Is_dropout=false, Is_causal=false, Is_even_N=false, Is_even_K=true, Return_softmax=false]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(34): here
            instantiation of "void run_flash_fwd<Kernel_traits,Is_dropout,Is_causal>(Flash_fwd_params &, cudaStream_t) [with Kernel_traits=Flash_fwd_kernel_traits<512, 32, 32, 2, false, false, cutlass::half_t, Flash_kernel_traits<512, 32, 32, 2, cutlass::half_t>>, Is_dropout=false, Is_causal=false]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(264): here
            instantiation of "void run_mha_fwd_hdim512<T>(Flash_fwd_params &, cudaStream_t) [with T=cutlass::half_t]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_hdim512_fp16_sm80.cu(8): here

/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_kernel.h(140): warning: variable "MMA_M" was declared but never referenced
          detected during:
            instantiation of "void flash::compute_attn_1rowblock<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &, int, int, int) [with Kernel_traits=Flash_fwd_kernel_traits<512, 32, 32, 2, false, false, cutlass::half_t, Flash_kernel_traits<512, 32, 32, 2, cutlass::half_t>>, Is_dropout=false, Is_causal=false, Is_even_N=false, Is_even_K=false, Return_softmax=false, Params=Flash_fwd_params]" 
(588): here
            instantiation of "void flash::compute_attn<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &) [with Kernel_traits=Flash_fwd_kernel_traits<512, 32, 32, 2, false, false, cutlass::half_t, Flash_kernel_traits<512, 32, 32, 2, cutlass::half_t>>, Is_dropout=false, Is_causal=false, Is_even_N=false, Is_even_K=false, Return_softmax=false, Params=Flash_fwd_params]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(15): here
            instantiation of "void flash_fwd_kernel<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax>(Flash_fwd_params) [with Kernel_traits=Flash_fwd_kernel_traits<512, 32, 32, 2, false, false, cutlass::half_t, Flash_kernel_traits<512, 32, 32, 2, cutlass::half_t>>, Is_dropout=false, Is_causal=false, Is_even_N=false, Is_even_K=false, Return_softmax=false]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(34): here
            instantiation of "void run_flash_fwd<Kernel_traits,Is_dropout,Is_causal>(Flash_fwd_params &, cudaStream_t) [with Kernel_traits=Flash_fwd_kernel_traits<512, 32, 32, 2, false, false, cutlass::half_t, Flash_kernel_traits<512, 32, 32, 2, cutlass::half_t>>, Is_dropout=false, Is_causal=false]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(264): here
            instantiation of "void run_mha_fwd_hdim512<T>(Flash_fwd_params &, cudaStream_t) [with T=cutlass::half_t]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_hdim512_fp16_sm80.cu(8): here

/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_kernel.h(275): warning: variable "tQrQ" was declared but never referenced
          detected during:
            instantiation of "void flash::compute_attn_1rowblock<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &, int, int, int) [with Kernel_traits=Flash_fwd_kernel_traits<512, 32, 32, 2, false, false, cutlass::half_t, Flash_kernel_traits<512, 32, 32, 2, cutlass::half_t>>, Is_dropout=false, Is_causal=false, Is_even_N=false, Is_even_K=false, Return_softmax=false, Params=Flash_fwd_params]" 
(588): here
            instantiation of "void flash::compute_attn<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax,Params>(const Params &) [with Kernel_traits=Flash_fwd_kernel_traits<512, 32, 32, 2, false, false, cutlass::half_t, Flash_kernel_traits<512, 32, 32, 2, cutlass::half_t>>, Is_dropout=false, Is_causal=false, Is_even_N=false, Is_even_K=false, Return_softmax=false, Params=Flash_fwd_params]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(15): here
            instantiation of "void flash_fwd_kernel<Kernel_traits,Is_dropout,Is_causal,Is_even_N,Is_even_K,Return_softmax>(Flash_fwd_params) [with Kernel_traits=Flash_fwd_kernel_traits<512, 32, 32, 2, false, false, cutlass::half_t, Flash_kernel_traits<512, 32, 32, 2, cutlass::half_t>>, Is_dropout=false, Is_causal=false, Is_even_N=false, Is_even_K=false, Return_softmax=false]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(34): here
            instantiation of "void run_flash_fwd<Kernel_traits,Is_dropout,Is_causal>(Flash_fwd_params &, cudaStream_t) [with Kernel_traits=Flash_fwd_kernel_traits<512, 32, 32, 2, false, false, cutlass::half_t, Flash_kernel_traits<512, 32, 32, 2, cutlass::half_t>>, Is_dropout=false, Is_causal=false]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_launch_template.h(264): here
            instantiation of "void run_mha_fwd_hdim512<T>(Flash_fwd_params &, cudaStream_t) [with T=cutlass::half_t]" 
/home/v-feiychen/flash-attention/csrc/flash_attn/src/flash_fwd_hdim512_fp16_sm80.cu(8): here

ptxas info    : 2 bytes gmem
ptxas info    : Compiling entry function '_Z16flash_fwd_kernelI23Flash_fwd_kernel_traitsILi512ELi32ELi32ELi2ELb0ELb0EN7cutlass6half_tE19Flash_kernel_traitsILi512ELi32ELi32ELi2ES2_EELb0ELb0ELb0ELb0ELb0EEv16Flash_fwd_params' for 'sm_80'
ptxas info    : Function properties for _Z16flash_fwd_kernelI23Flash_fwd_kernel_traitsILi512ELi32ELi32ELi2ELb0ELb0EN7cutlass6half_tE19Flash_kernel_traitsILi512ELi32ELi32ELi2ES2_EELb0ELb0ELb0ELb0ELb0EEv16Flash_fwd_params
    832 bytes stack frame, 1188 bytes spill stores, 1208 bytes spill loads
ptxas info    : Used 255 registers, 584 bytes cmem[0]
ptxas info    : Compiling entry function '_Z16flash_fwd_kernelI23Flash_fwd_kernel_traitsILi512ELi32ELi32ELi2ELb0ELb0EN7cutlass6half_tE19Flash_kernel_traitsILi512ELi32ELi32ELi2ES2_EELb0ELb0ELb0ELb1ELb0EEv16Flash_fwd_params' for 'sm_80'
ptxas info    : Function properties for _Z16flash_fwd_kernelI23Flash_fwd_kernel_traitsILi512ELi32ELi32ELi2ELb0ELb0EN7cutlass6half_tE19Flash_kernel_traitsILi512ELi32ELi32ELi2ES2_EELb0ELb0ELb0ELb1ELb0EEv16Flash_fwd_params
    456 bytes stack frame, 792 bytes spill stores, 784 bytes spill loads
ptxas info    : Used 255 registers, 584 bytes cmem[0]
ptxas info    : Compiling entry function '_Z16flash_fwd_kernelI23Flash_fwd_kernel_traitsILi512ELi32ELi32ELi2ELb0ELb0EN7cutlass6half_tE19Flash_kernel_traitsILi512ELi32ELi32ELi2ES2_EELb0ELb0ELb1ELb0ELb0EEv16Flash_fwd_params' for 'sm_80'
ptxas info    : Function properties for _Z16flash_fwd_kernelI23Flash_fwd_kernel_traitsILi512ELi32ELi32ELi2ELb0ELb0EN7cutlass6half_tE19Flash_kernel_traitsILi512ELi32ELi32ELi2ES2_EELb0ELb0ELb1ELb0ELb0EEv16Flash_fwd_params
    744 bytes stack frame, 1068 bytes spill stores, 1072 bytes spill loads
ptxas info    : Used 255 registers, 584 bytes cmem[0]
ptxas info    : Compiling entry function '_Z16flash_fwd_kernelI23Flash_fwd_kernel_traitsILi512ELi32ELi32ELi2ELb0ELb0EN7cutlass6half_tE19Flash_kernel_traitsILi512ELi32ELi32ELi2ES2_EELb0ELb0ELb1ELb1ELb0EEv16Flash_fwd_params' for 'sm_80'
ptxas info    : Function properties for _Z16flash_fwd_kernelI23Flash_fwd_kernel_traitsILi512ELi32ELi32ELi2ELb0ELb0EN7cutlass6half_tE19Flash_kernel_traitsILi512ELi32ELi32ELi2ES2_EELb0ELb0ELb1ELb1ELb0EEv16Flash_fwd_params
    456 bytes stack frame, 752 bytes spill stores, 772 bytes spill loads
ptxas info    : Used 255 registers, 584 bytes cmem[0]
ptxas info    : Compiling entry function '_Z16flash_fwd_kernelI23Flash_fwd_kernel_traitsILi512ELi64ELi32ELi4ELb0ELb0EN7cutlass6half_tE19Flash_kernel_traitsILi512ELi64ELi32ELi4ES2_EELb0ELb0ELb0ELb0ELb0EEv16Flash_fwd_params' for 'sm_80'
ptxas info    : Function properties for _Z16flash_fwd_kernelI23Flash_fwd_kernel_traitsILi512ELi64ELi32ELi4ELb0ELb0EN7cutlass6half_tE19Flash_kernel_traitsILi512ELi64ELi32ELi4ES2_EELb0ELb0ELb0ELb0ELb0EEv16Flash_fwd_params
    736 bytes stack frame, 1072 bytes spill stores, 1084 bytes spill loads
ptxas info    : Used 255 registers, 584 bytes cmem[0]
ptxas info    : Compiling entry function '_Z16flash_fwd_kernelI23Flash_fwd_kernel_traitsILi512ELi64ELi32ELi4ELb0ELb0EN7cutlass6half_tE19Flash_kernel_traitsILi512ELi64ELi32ELi4ES2_EELb0ELb0ELb0ELb1ELb0EEv16Flash_fwd_params' for 'sm_80'
ptxas info    : Function properties for _Z16flash_fwd_kernelI23Flash_fwd_kernel_traitsILi512ELi64ELi32ELi4ELb0ELb0EN7cutlass6half_tE19Flash_kernel_traitsILi512ELi64ELi32ELi4ES2_EELb0ELb0ELb0ELb1ELb0EEv16Flash_fwd_params
    464 bytes stack frame, 792 bytes spill stores, 784 bytes spill loads
ptxas info    : Used 255 registers, 584 bytes cmem[0]
ptxas info    : Compiling entry function '_Z16flash_fwd_kernelI23Flash_fwd_kernel_traitsILi512ELi64ELi32ELi4ELb0ELb0EN7cutlass6half_tE19Flash_kernel_traitsILi512ELi64ELi32ELi4ES2_EELb0ELb0ELb1ELb0ELb0EEv16Flash_fwd_params' for 'sm_80'
ptxas info    : Function properties for _Z16flash_fwd_kernelI23Flash_fwd_kernel_traitsILi512ELi64ELi32ELi4ELb0ELb0EN7cutlass6half_tE19Flash_kernel_traitsILi512ELi64ELi32ELi4ES2_EELb0ELb0ELb1ELb0ELb0EEv16Flash_fwd_params
    432 bytes stack frame, 744 bytes spill stores, 760 bytes spill loads
ptxas info    : Used 255 registers, 584 bytes cmem[0]
ptxas info    : Compiling entry function '_Z16flash_fwd_kernelI23Flash_fwd_kernel_traitsILi512ELi64ELi32ELi4ELb0ELb0EN7cutlass6half_tE19Flash_kernel_traitsILi512ELi64ELi32ELi4ES2_EELb0ELb0ELb1ELb1ELb0EEv16Flash_fwd_params' for 'sm_80'
ptxas info    : Function properties for _Z16flash_fwd_kernelI23Flash_fwd_kernel_traitsILi512ELi64ELi32ELi4ELb0ELb0EN7cutlass6half_tE19Flash_kernel_traitsILi512ELi64ELi32ELi4ES2_EELb0ELb0ELb1ELb1ELb0EEv16Flash_fwd_params
    496 bytes stack frame, 788 bytes spill stores, 792 bytes spill loads
ptxas info    : Used 255 registers, 584 bytes cmem[0]
ptxas info    : Compiling entry function '_Z16flash_fwd_kernelI23Flash_fwd_kernel_traitsILi512ELi32ELi32ELi2ELb0ELb0EN7cutlass6half_tE19Flash_kernel_traitsILi512ELi32ELi32ELi2ES2_EELb0ELb1ELb0ELb0ELb0EEv16Flash_fwd_params' for 'sm_80'
ptxas info    : Function properties for _Z16flash_fwd_kernelI23Flash_fwd_kernel_traitsILi512ELi32ELi32ELi2ELb0ELb0EN7cutlass6half_tE19Flash_kernel_traitsILi512ELi32ELi32ELi2ES2_EELb0ELb1ELb0ELb0ELb0EEv16Flash_fwd_params
    832 bytes stack frame, 1192 bytes spill stores, 1212 bytes spill loads
ptxas info    : Used 255 registers, 584 bytes cmem[0]
ptxas info    : Compiling entry function '_Z16flash_fwd_kernelI23Flash_fwd_kernel_traitsILi512ELi32ELi32ELi2ELb0ELb0EN7cutlass6half_tE19Flash_kernel_traitsILi512ELi32ELi32ELi2ES2_EELb0ELb1ELb0ELb1ELb0EEv16Flash_fwd_params' for 'sm_80'
ptxas info    : Function properties for _Z16flash_fwd_kernelI23Flash_fwd_kernel_traitsILi512ELi32ELi32ELi2ELb0ELb0EN7cutlass6half_tE19Flash_kernel_traitsILi512ELi32ELi32ELi2ES2_EELb0ELb1ELb0ELb1ELb0EEv16Flash_fwd_params
    464 bytes stack frame, 796 bytes spill stores, 788 bytes spill loads
ptxas info    : Used 255 registers, 584 bytes cmem[0]
ptxas info    : Compiling entry function '_Z16flash_fwd_kernelI23Flash_fwd_kernel_traitsILi512ELi32ELi32ELi2ELb0ELb0EN7cutlass6half_tE19Flash_kernel_traitsILi512ELi32ELi32ELi2ES2_EELb0ELb1ELb1ELb0ELb0EEv16Flash_fwd_params' for 'sm_80'
ptxas info    : Function properties for _Z16flash_fwd_kernelI23Flash_fwd_kernel_traitsILi512ELi32ELi32ELi2ELb0ELb0EN7cutlass6half_tE19Flash_kernel_traitsILi512ELi32ELi32ELi2ES2_EELb0ELb1ELb1ELb0ELb0EEv16Flash_fwd_params
    760 bytes stack frame, 1120 bytes spill stores, 1124 bytes spill loads
ptxas info    : Used 255 registers, 584 bytes cmem[0]
ptxas info    : Compiling entry function '_Z16flash_fwd_kernelI23Flash_fwd_kernel_traitsILi512ELi32ELi32ELi2ELb0ELb0EN7cutlass6half_tE19Flash_kernel_traitsILi512ELi32ELi32ELi2ES2_EELb0ELb1ELb1ELb1ELb0EEv16Flash_fwd_params' for 'sm_80'
ptxas info    : Function properties for _Z16flash_fwd_kernelI23Flash_fwd_kernel_traitsILi512ELi32ELi32ELi2ELb0ELb0EN7cutlass6half_tE19Flash_kernel_traitsILi512ELi32ELi32ELi2ES2_EELb0ELb1ELb1ELb1ELb0EEv16Flash_fwd_params
    464 bytes stack frame, 756 bytes spill stores, 776 bytes spill loads
ptxas info    : Used 255 registers, 584 bytes cmem[0]
ptxas info    : Compiling entry function '_Z16flash_fwd_kernelI23Flash_fwd_kernel_traitsILi512ELi64ELi32ELi4ELb0ELb0EN7cutlass6half_tE19Flash_kernel_traitsILi512ELi64ELi32ELi4ES2_EELb0ELb1ELb0ELb0ELb0EEv16Flash_fwd_params' for 'sm_80'
ptxas info    : Function properties for _Z16flash_fwd_kernelI23Flash_fwd_kernel_traitsILi512ELi64ELi32ELi4ELb0ELb0EN7cutlass6half_tE19Flash_kernel_traitsILi512ELi64ELi32ELi4ES2_EELb0ELb1ELb0ELb0ELb0EEv16Flash_fwd_params
    880 bytes stack frame, 1876 bytes spill stores, 1972 bytes spill loads
ptxas info    : Used 255 registers, 584 bytes cmem[0]
ptxas info    : Compiling entry function '_Z16flash_fwd_kernelI23Flash_fwd_kernel_traitsILi512ELi64ELi32ELi4ELb0ELb0EN7cutlass6half_tE19Flash_kernel_traitsILi512ELi64ELi32ELi4ES2_EELb0ELb1ELb0ELb1ELb0EEv16Flash_fwd_params' for 'sm_80'
ptxas info    : Function properties for _Z16flash_fwd_kernelI23Flash_fwd_kernel_traitsILi512ELi64ELi32ELi4ELb0ELb0EN7cutlass6half_tE19Flash_kernel_traitsILi512ELi64ELi32ELi4ES2_EELb0ELb1ELb0ELb1ELb0EEv16Flash_fwd_params
    624 bytes stack frame, 1512 bytes spill stores, 1560 bytes spill loads
ptxas info    : Used 255 registers, 584 bytes cmem[0]
ptxas info    : Compiling entry function '_Z16flash_fwd_kernelI23Flash_fwd_kernel_traitsILi512ELi64ELi32ELi4ELb0ELb0EN7cutlass6half_tE19Flash_kernel_traitsILi512ELi64ELi32ELi4ES2_EELb0ELb1ELb1ELb0ELb0EEv16Flash_fwd_params' for 'sm_80'
ptxas info    : Function properties for _Z16flash_fwd_kernelI23Flash_fwd_kernel_traitsILi512ELi64ELi32ELi4ELb0ELb0EN7cutlass6half_tE19Flash_kernel_traitsILi512ELi64ELi32ELi4ES2_EELb0ELb1ELb1ELb0ELb0EEv16Flash_fwd_params
    1000 bytes stack frame, 1868 bytes spill stores, 1932 bytes spill loads
ptxas info    : Used 255 registers, 584 bytes cmem[0]
ptxas info    : Compiling entry function '_Z16flash_fwd_kernelI23Flash_fwd_kernel_traitsILi512ELi64ELi32ELi4ELb0ELb0EN7cutlass6half_tE19Flash_kernel_traitsILi512ELi64ELi32ELi4ES2_EELb0ELb1ELb1ELb1ELb0EEv16Flash_fwd_params' for 'sm_80'
ptxas info    : Function properties for _Z16flash_fwd_kernelI23Flash_fwd_kernel_traitsILi512ELi64ELi32ELi4ELb0ELb0EN7cutlass6half_tE19Flash_kernel_traitsILi512ELi64ELi32ELi4ES2_EELb0ELb1ELb1ELb1ELb0EEv16Flash_fwd_params
    664 bytes stack frame, 1556 bytes spill stores, 1616 bytes spill loads
ptxas info    : Used 255 registers, 584 bytes cmem[0]
ptxas info    : Compiling entry function '_Z16flash_fwd_kernelI23Flash_fwd_kernel_traitsILi512ELi32ELi32ELi2ELb0ELb0EN7cutlass6half_tE19Flash_kernel_traitsILi512ELi32ELi32ELi2ES2_EELb1ELb0ELb0ELb0ELb0EEv16Flash_fwd_params' for 'sm_80'
ptxas info    : Function properties for _Z16flash_fwd_kernelI23Flash_fwd_kernel_traitsILi512ELi32ELi32ELi2ELb0ELb0EN7cutlass6half_tE19Flash_kernel_traitsILi512ELi32ELi32ELi2ES2_EELb1ELb0ELb0ELb0ELb0EEv16Flash_fwd_params
    736 bytes stack frame, 1088 bytes spill stores, 1116 bytes spill loads
ptxas info    : Used 255 registers, 584 bytes cmem[0]
ptxas info    : Compiling entry function '_Z16flash_fwd_kernelI23Flash_fwd_kernel_traitsILi512ELi32ELi32ELi2ELb0ELb0EN7cutlass6half_tE19Flash_kernel_traitsILi512ELi32ELi32ELi2ES2_EELb1ELb0ELb0ELb0ELb1EEv16Flash_fwd_params' for 'sm_80'
ptxas info    : Function properties for _Z16flash_fwd_kernelI23Flash_fwd_kernel_traitsILi512ELi32ELi32ELi2ELb0ELb0EN7cutlass6half_tE19Flash_kernel_traitsILi512ELi32ELi32ELi2ES2_EELb1ELb0ELb0ELb0ELb1EEv16Flash_fwd_params
    712 bytes stack frame, 1040 bytes spill stores, 1048 bytes spill loads
ptxas info    : Used 255 registers, 584 bytes cmem[0]
ptxas info    : Compiling entry function '_Z16flash_fwd_kernelI23Flash_fwd_kernel_traitsILi512ELi32ELi32ELi2ELb0ELb0EN7cutlass6half_tE19Flash_kernel_traitsILi512ELi32ELi32ELi2ES2_EELb1ELb0ELb0ELb1ELb0EEv16Flash_fwd_params' for 'sm_80'
ptxas info    : Function properties for _Z16flash_fwd_kernelI23Flash_fwd_kernel_traitsILi512ELi32ELi32ELi2ELb0ELb0EN7cutlass6half_tE19Flash_kernel_traitsILi512ELi32ELi32ELi2ES2_EELb1ELb0ELb0ELb1ELb0EEv16Flash_fwd_params
    496 bytes stack frame, 792 bytes spill stores, 788 bytes spill loads
ptxas info    : Used 255 registers, 584 bytes cmem[0]
ptxas info    : Compiling entry function '_Z16flash_fwd_kernelI23Flash_fwd_kernel_traitsILi512ELi32ELi32ELi2ELb0ELb0EN7cutlass6half_tE19Flash_kernel_traitsILi512ELi32ELi32ELi2ES2_EELb1ELb0ELb0ELb1ELb1EEv16Flash_fwd_params' for 'sm_80'
ptxas info    : Function properties for _Z16flash_fwd_kernelI23Flash_fwd_kernel_traitsILi512ELi32ELi32ELi2ELb0ELb0EN7cutlass6half_tE19Flash_kernel_traitsILi512ELi32ELi32ELi2ES2_EELb1ELb0ELb0ELb1ELb1EEv16Flash_fwd_params
    784 bytes stack frame, 1088 bytes spill stores, 1068 bytes spill loads
ptxas info    : Used 255 registers, 584 bytes cmem[0]
ptxas info    : Compiling entry function '_Z16flash_fwd_kernelI23Flash_fwd_kernel_traitsILi512ELi32ELi32ELi2ELb0ELb0EN7cutlass6half_tE19Flash_kernel_traitsILi512ELi32ELi32ELi2ES2_EELb1ELb0ELb1ELb0ELb0EEv16Flash_fwd_params' for 'sm_80'
ptxas info    : Function properties for _Z16flash_fwd_kernelI23Flash_fwd_kernel_traitsILi512ELi32ELi32ELi2ELb0ELb0EN7cutlass6half_tE19Flash_kernel_traitsILi512ELi32ELi32ELi2ES2_EELb1ELb0ELb1ELb0ELb0EEv16Flash_fwd_params
    704 bytes stack frame, 1004 bytes spill stores, 1008 bytes spill loads
ptxas info    : Used 255 registers, 584 bytes cmem[0]
ptxas info    : Compiling entry function '_Z16flash_fwd_kernelI23Flash_fwd_kernel_traitsILi512ELi32ELi32ELi2ELb0ELb0EN7cutlass6half_tE19Flash_kernel_traitsILi512ELi32ELi32ELi2ES2_EELb1ELb0ELb1ELb0ELb1EEv16Flash_fwd_params' for 'sm_80'
ptxas info    : Function properties for _Z16flash_fwd_kernelI23Flash_fwd_kernel_traitsILi512ELi32ELi32ELi2ELb0ELb0EN7cutlass6half_tE19Flash_kernel_traitsILi512ELi32ELi32ELi2ES2_EELb1ELb0ELb1ELb0ELb1EEv16Flash_fwd_params
    704 bytes stack frame, 992 bytes spill stores, 980 bytes spill loads
ptxas info    : Used 255 registers, 584 bytes cmem[0]
ptxas info    : Compiling entry function '_Z16flash_fwd_kernelI23Flash_fwd_kernel_traitsILi512ELi32ELi32ELi2ELb0ELb0EN7cutlass6half_tE19Flash_kernel_traitsILi512ELi32ELi32ELi2ES2_EELb1ELb0ELb1ELb1ELb0EEv16Flash_fwd_params' for 'sm_80'
ptxas info    : Function properties for _Z16flash_fwd_kernelI23Flash_fwd_kernel_traitsILi512ELi32ELi32ELi2ELb0ELb0EN7cutlass6half_tE19Flash_kernel_traitsILi512ELi32ELi32ELi2ES2_EELb1ELb0ELb1ELb1ELb0EEv16Flash_fwd_params
    448 bytes stack frame, 720 bytes spill stores, 736 bytes spill loads
ptxas info    : Used 255 registers, 584 bytes cmem[0]
ptxas info    : Compiling entry function '_Z16flash_fwd_kernelI23Flash_fwd_kernel_traitsILi512ELi32ELi32ELi2ELb0ELb0EN7cutlass6half_tE19Flash_kernel_traitsILi512ELi32ELi32ELi2ES2_EELb1ELb0ELb1ELb1ELb1EEv16Flash_fwd_params' for 'sm_80'
ptxas info    : Function properties for _Z16flash_fwd_kernelI23Flash_fwd_kernel_traitsILi512ELi32ELi32ELi2ELb0ELb0EN7cutlass6half_tE19Flash_kernel_traitsILi512ELi32ELi32ELi2ES2_EELb1ELb0ELb1ELb1ELb1EEv16Flash_fwd_params
    792 bytes stack frame, 1152 bytes spill stores, 1140 bytes spill loads
ptxas info    : Used 255 registers, 584 bytes cmem[0]
ptxas info    : Compiling entry function '_Z16flash_fwd_kernelI23Flash_fwd_kernel_traitsILi512ELi64ELi32ELi4ELb0ELb0EN7cutlass6half_tE19Flash_kernel_traitsILi512ELi64ELi32ELi4ES2_EELb1ELb0ELb0ELb0ELb0EEv16Flash_fwd_params' for 'sm_80'
ptxas info    : Function properties for _Z16flash_fwd_kernelI23Flash_fwd_kernel_traitsILi512ELi64ELi32ELi4ELb0ELb0EN7cutlass6half_tE19Flash_kernel_traitsILi512ELi64ELi32ELi4ES2_EELb1ELb0ELb0ELb0ELb0EEv16Flash_fwd_params
    704 bytes stack frame, 1008 bytes spill stores, 1028 bytes spill loads
ptxas info    : Used 255 registers, 584 bytes cmem[0]
ptxas info    : Compiling entry function '_Z16flash_fwd_kernelI23Flash_fwd_kernel_traitsILi512ELi64ELi32ELi4ELb0ELb0EN7cutlass6half_tE19Flash_kernel_traitsILi512ELi64ELi32ELi4ES2_EELb1ELb0ELb0ELb0ELb1EEv16Flash_fwd_params' for 'sm_80'
ptxas info    : Function properties for _Z16flash_fwd_kernelI23Flash_fwd_kernel_traitsILi512ELi64ELi32ELi4ELb0ELb0EN7cutlass6half_tE19Flash_kernel_traitsILi512ELi64ELi32ELi4ES2_EELb1ELb0ELb0ELb0ELb1EEv16Flash_fwd_params
    752 bytes stack frame, 1100 bytes spill stores, 1104 bytes spill loads
ptxas info    : Used 255 registers, 584 bytes cmem[0]
ptxas info    : Compiling entry function '_Z16flash_fwd_kernelI23Flash_fwd_kernel_traitsILi512ELi64ELi32ELi4ELb0ELb0EN7cutlass6half_tE19Flash_kernel_traitsILi512ELi64ELi32ELi4ES2_EELb1ELb0ELb0ELb1ELb0EEv16Flash_fwd_params' for 'sm_80'
ptxas info    : Function properties for _Z16flash_fwd_kernelI23Flash_fwd_kernel_traitsILi512ELi64ELi32ELi4ELb0ELb0EN7cutlass6half_tE19Flash_kernel_traitsILi512ELi64ELi32ELi4ES2_EELb1ELb0ELb0ELb1ELb0EEv16Flash_fwd_params
    440 bytes stack frame, 748 bytes spill stores, 744 bytes spill loads
ptxas info    : Used 255 registers, 584 bytes cmem[0]
ptxas info    : Compiling entry function '_Z16flash_fwd_kernelI23Flash_fwd_kernel_traitsILi512ELi64ELi32ELi4ELb0ELb0EN7cutlass6half_tE19Flash_kernel_traitsILi512ELi64ELi32ELi4ES2_EELb1ELb0ELb0ELb1ELb1EEv16Flash_fwd_params' for 'sm_80'
ptxas info    : Function properties for _Z16flash_fwd_kernelI23Flash_fwd_kernel_traitsILi512ELi64ELi32ELi4ELb0ELb0EN7cutlass6half_tE19Flash_kernel_traitsILi512ELi64ELi32ELi4ES2_EELb1ELb0ELb0ELb1ELb1EEv16Flash_fwd_params
    784 bytes stack frame, 1088 bytes spill stores, 1068 bytes spill loads
ptxas info    : Used 255 registers, 584 bytes cmem[0]
ptxas info    : Compiling entry function '_Z16flash_fwd_kernelI23Flash_fwd_kernel_traitsILi512ELi64ELi32ELi4ELb0ELb0EN7cutlass6half_tE19Flash_kernel_traitsILi512ELi64ELi32ELi4ES2_EELb1ELb0ELb1ELb0ELb0EEv16Flash_fwd_params' for 'sm_80'
ptxas info    : Function properties for _Z16flash_fwd_kernelI23Flash_fwd_kernel_traitsILi512ELi64ELi32ELi4ELb0ELb0EN7cutlass6half_tE19Flash_kernel_traitsILi512ELi64ELi32ELi4ES2_EELb1ELb0ELb1ELb0ELb0EEv16Flash_fwd_params
    624 bytes stack frame, 928 bytes spill stores, 932 bytes spill loads
ptxas info    : Used 255 registers, 584 bytes cmem[0]
ptxas info    : Compiling entry function '_Z16flash_fwd_kernelI23Flash_fwd_kernel_traitsILi512ELi64ELi32ELi4ELb0ELb0EN7cutlass6half_tE19Flash_kernel_traitsILi512ELi64ELi32ELi4ES2_EELb1ELb0ELb1ELb0ELb1EEv16Flash_fwd_params' for 'sm_80'
ptxas info    : Function properties for _Z16flash_fwd_kernelI23Flash_fwd_kernel_traitsILi512ELi64ELi32ELi4ELb0ELb0EN7cutlass6half_tE19Flash_kernel_traitsILi512ELi64ELi32ELi4ES2_EELb1ELb0ELb1ELb0ELb1EEv16Flash_fwd_params
    752 bytes stack frame, 1056 bytes spill stores, 1036 bytes spill loads
ptxas info    : Used 255 registers, 584 bytes cmem[0]
ptxas info    : Compiling entry function '_Z16flash_fwd_kernelI23Flash_fwd_kernel_traitsILi512ELi64ELi32ELi4ELb0ELb0EN7cutlass6half_tE19Flash_kernel_traitsILi512ELi64ELi32ELi4ES2_EELb1ELb0ELb1ELb1ELb0EEv16Flash_fwd_params' for 'sm_80'
ptxas info    : Function properties for _Z16flash_fwd_kernelI23Flash_fwd_kernel_traitsILi512ELi64ELi32ELi4ELb0ELb0EN7cutlass6half_tE19Flash_kernel_traitsILi512ELi64ELi32ELi4ES2_EELb1ELb0ELb1ELb1ELb0EEv16Flash_fwd_params
    448 bytes stack frame, 724 bytes spill stores, 732 bytes spill loads
ptxas info    : Used 255 registers, 584 bytes cmem[0]
ptxas info    : Compiling entry function '_Z16flash_fwd_kernelI23Flash_fwd_kernel_traitsILi512ELi64ELi32ELi4ELb0ELb0EN7cutlass6half_tE19Flash_kernel_traitsILi512ELi64ELi32ELi4ES2_EELb1ELb0ELb1ELb1ELb1EEv16Flash_fwd_params' for 'sm_80'
ptxas info    : Function properties for _Z16flash_fwd_kernelI23Flash_fwd_kernel_traitsILi512ELi64ELi32ELi4ELb0ELb0EN7cutlass6half_tE19Flash_kernel_traitsILi512ELi64ELi32ELi4ES2_EELb1ELb0ELb1ELb1ELb1EEv16Flash_fwd_params
    792 bytes stack frame, 1152 bytes spill stores, 1140 bytes spill loads
ptxas info    : Used 255 registers, 584 bytes cmem[0]
ptxas info    : Compiling entry function '_Z16flash_fwd_kernelI23Flash_fwd_kernel_traitsILi512ELi32ELi32ELi2ELb0ELb0EN7cutlass6half_tE19Flash_kernel_traitsILi512ELi32ELi32ELi2ES2_EELb1ELb1ELb0ELb0ELb0EEv16Flash_fwd_params' for 'sm_80'
ptxas info    : Function properties for _Z16flash_fwd_kernelI23Flash_fwd_kernel_traitsILi512ELi32ELi32ELi2ELb0ELb0EN7cutlass6half_tE19Flash_kernel_traitsILi512ELi32ELi32ELi2ES2_EELb1ELb1ELb0ELb0ELb0EEv16Flash_fwd_params
    736 bytes stack frame, 1092 bytes spill stores, 1120 bytes spill loads
ptxas info    : Used 255 registers, 584 bytes cmem[0]
ptxas info    : Compiling entry function '_Z16flash_fwd_kernelI23Flash_fwd_kernel_traitsILi512ELi32ELi32ELi2ELb0ELb0EN7cutlass6half_tE19Flash_kernel_traitsILi512ELi32ELi32ELi2ES2_EELb1ELb1ELb0ELb0ELb1EEv16Flash_fwd_params' for 'sm_80'
ptxas info    : Function properties for _Z16flash_fwd_kernelI23Flash_fwd_kernel_traitsILi512ELi32ELi32ELi2ELb0ELb0EN7cutlass6half_tE19Flash_kernel_traitsILi512ELi32ELi32ELi2ES2_EELb1ELb1ELb0ELb0ELb1EEv16Flash_fwd_params
    720 bytes stack frame, 1052 bytes spill stores, 1060 bytes spill loads
ptxas info    : Used 255 registers, 584 bytes cmem[0]
ptxas info    : Compiling entry function '_Z16flash_fwd_kernelI23Flash_fwd_kernel_traitsILi512ELi32ELi32ELi2ELb0ELb0EN7cutlass6half_tE19Flash_kernel_traitsILi512ELi32ELi32ELi2ES2_EELb1ELb1ELb0ELb1ELb0EEv16Flash_fwd_params' for 'sm_80'
ptxas info    : Function properties for _Z16flash_fwd_kernelI23Flash_fwd_kernel_traitsILi512ELi32ELi32ELi2ELb0ELb0EN7cutlass6half_tE19Flash_kernel_traitsILi512ELi32ELi32ELi2ES2_EELb1ELb1ELb0ELb1ELb0EEv16Flash_fwd_params
    504 bytes stack frame, 796 bytes spill stores, 788 bytes spill loads
ptxas info    : Used 255 registers, 584 bytes cmem[0]
ptxas info    : Compiling entry function '_Z16flash_fwd_kernelI23Flash_fwd_kernel_traitsILi512ELi32ELi32ELi2ELb0ELb0EN7cutlass6half_tE19Flash_kernel_traitsILi512ELi32ELi32ELi2ES2_EELb1ELb1ELb0ELb1ELb1EEv16Flash_fwd_params' for 'sm_80'
ptxas info    : Function properties for _Z16flash_fwd_kernelI23Flash_fwd_kernel_traitsILi512ELi32ELi32ELi2ELb0ELb0EN7cutlass6half_tE19Flash_kernel_traitsILi512ELi32ELi32ELi2ES2_EELb1ELb1ELb0ELb1ELb1EEv16Flash_fwd_params
    792 bytes stack frame, 1092 bytes spill stores, 1072 bytes spill loads
ptxas info    : Used 255 registers, 584 bytes cmem[0]
ptxas info    : Compiling entry function '_Z16flash_fwd_kernelI23Flash_fwd_kernel_traitsILi512ELi32ELi32ELi2ELb0ELb0EN7cutlass6half_tE19Flash_kernel_traitsILi512ELi32ELi32ELi2ES2_EELb1ELb1ELb1ELb0ELb0EEv16Flash_fwd_params' for 'sm_80'
ptxas info    : Function properties for _Z16flash_fwd_kernelI23Flash_fwd_kernel_traitsILi512ELi32ELi32ELi2ELb0ELb0EN7cutlass6half_tE19Flash_kernel_traitsILi512ELi32ELi32ELi2ES2_EELb1ELb1ELb1ELb0ELb0EEv16Flash_fwd_params
    712 bytes stack frame, 1060 bytes spill stores, 1072 bytes spill loads
ptxas info    : Used 255 registers, 584 bytes cmem[0]
ptxas info    : Compiling entry function '_Z16flash_fwd_kernelI23Flash_fwd_kernel_traitsILi512ELi32ELi32ELi2ELb0ELb0EN7cutlass6half_tE19Flash_kernel_traitsILi512ELi32ELi32ELi2ES2_EELb1ELb1ELb1ELb0ELb1EEv16Flash_fwd_params' for 'sm_80'
ptxas info    : Function properties for _Z16flash_fwd_kernelI23Flash_fwd_kernel_traitsILi512ELi32ELi32ELi2ELb0ELb0EN7cutlass6half_tE19Flash_kernel_traitsILi512ELi32ELi32ELi2ES2_EELb1ELb1ELb1ELb0ELb1EEv16Flash_fwd_params
    720 bytes stack frame, 1052 bytes spill stores, 1044 bytes spill loads
ptxas info    : Used 255 registers, 584 bytes cmem[0]
ptxas info    : Compiling entry function '_Z16flash_fwd_kernelI23Flash_fwd_kernel_traitsILi512ELi32ELi32ELi2ELb0ELb0EN7cutlass6half_tE19Flash_kernel_traitsILi512ELi32ELi32ELi2ES2_EELb1ELb1ELb1ELb1ELb0EEv16Flash_fwd_params' for 'sm_80'
ptxas info    : Function properties for _Z16flash_fwd_kernelI23Flash_fwd_kernel_traitsILi512ELi32ELi32ELi2ELb0ELb0EN7cutlass6half_tE19Flash_kernel_traitsILi512ELi32ELi32ELi2ES2_EELb1ELb1ELb1ELb1ELb0EEv16Flash_fwd_params
    472 bytes stack frame, 752 bytes spill stores, 760 bytes spill loads
ptxas info    : Used 255 registers, 584 bytes cmem[0]
ptxas info    : Compiling entry function '_Z16flash_fwd_kernelI23Flash_fwd_kernel_traitsILi512ELi32ELi32ELi2ELb0ELb0EN7cutlass6half_tE19Flash_kernel_traitsILi512ELi32ELi32ELi2ES2_EELb1ELb1ELb1ELb1ELb1EEv16Flash_fwd_params' for 'sm_80'
ptxas info    : Function properties for _Z16flash_fwd_kernelI23Flash_fwd_kernel_traitsILi512ELi32ELi32ELi2ELb0ELb0EN7cutlass6half_tE19Flash_kernel_traitsILi512ELi32ELi32ELi2ES2_EELb1ELb1ELb1ELb1ELb1EEv16Flash_fwd_params
    752 bytes stack frame, 1088 bytes spill stores, 1082 bytes spill loads
ptxas info    : Used 255 registers, 584 bytes cmem[0]
ptxas info    : Compiling entry function '_Z16flash_fwd_kernelI23Flash_fwd_kernel_traitsILi512ELi64ELi32ELi4ELb0ELb0EN7cutlass6half_tE19Flash_kernel_traitsILi512ELi64ELi32ELi4ES2_EELb1ELb1ELb0ELb0ELb0EEv16Flash_fwd_params' for 'sm_80'
ptxas info    : Function properties for _Z16flash_fwd_kernelI23Flash_fwd_kernel_traitsILi512ELi64ELi32ELi4ELb0ELb0EN7cutlass6half_tE19Flash_kernel_traitsILi512ELi64ELi32ELi4ES2_EELb1ELb1ELb0ELb0ELb0EEv16Flash_fwd_params
    784 bytes stack frame, 1708 bytes spill stores, 1832 bytes spill loads
ptxas info    : Used 255 registers, 584 bytes cmem[0]
ptxas info    : Compiling entry function '_Z16flash_fwd_kernelI23Flash_fwd_kernel_traitsILi512ELi64ELi32ELi4ELb0ELb0EN7cutlass6half_tE19Flash_kernel_traitsILi512ELi64ELi32ELi4ES2_EELb1ELb1ELb0ELb0ELb1EEv16Flash_fwd_params' for 'sm_80'
ptxas info    : Function properties for _Z16flash_fwd_kernelI23Flash_fwd_kernel_traitsILi512ELi64ELi32ELi4ELb0ELb0EN7cutlass6half_tE19Flash_kernel_traitsILi512ELi64ELi32ELi4ES2_EELb1ELb1ELb0ELb0ELb1EEv16Flash_fwd_params
    960 bytes stack frame, 1884 bytes spill stores, 1978 bytes spill loads
ptxas info    : Used 255 registers, 584 bytes cmem[0]
ptxas info    : Compiling entry function '_Z16flash_fwd_kernelI23Flash_fwd_kernel_traitsILi512ELi64ELi32ELi4ELb0ELb0EN7cutlass6half_tE19Flash_kernel_traitsILi512ELi64ELi32ELi4ES2_EELb1ELb1ELb0ELb1ELb0EEv16Flash_fwd_params' for 'sm_80'
ptxas info    : Function properties for _Z16flash_fwd_kernelI23Flash_fwd_kernel_traitsILi512ELi64ELi32ELi4ELb0ELb0EN7cutlass6half_tE19Flash_kernel_traitsILi512ELi64ELi32ELi4ES2_EELb1ELb1ELb0ELb1ELb0EEv16Flash_fwd_params
    656 bytes stack frame, 1552 bytes spill stores, 1604 bytes spill loads
ptxas info    : Used 255 registers, 584 bytes cmem[0]
ptxas info    : Compiling entry function '_Z16flash_fwd_kernelI23Flash_fwd_kernel_traitsILi512ELi64ELi32ELi4ELb0ELb0EN7cutlass6half_tE19Flash_kernel_traitsILi512ELi64ELi32ELi4ES2_EELb1ELb1ELb0ELb1ELb1EEv16Flash_fwd_params' for 'sm_80'
ptxas info    : Function properties for _Z16flash_fwd_kernelI23Flash_fwd_kernel_traitsILi512ELi64ELi32ELi4ELb0ELb0EN7cutlass6half_tE19Flash_kernel_traitsILi512ELi64ELi32ELi4ES2_EELb1ELb1ELb0ELb1ELb1EEv16Flash_fwd_params
    912 bytes stack frame, 1886 bytes spill stores, 1938 bytes spill loads
ptxas info    : Used 255 registers, 584 bytes cmem[0]
ptxas info    : Compiling entry function '_Z16flash_fwd_kernelI23Flash_fwd_kernel_traitsILi512ELi64ELi32ELi4ELb0ELb0EN7cutlass6half_tE19Flash_kernel_traitsILi512ELi64ELi32ELi4ES2_EELb1ELb1ELb1ELb0ELb0EEv16Flash_fwd_params' for 'sm_80'
ptxas info    : Function properties for _Z16flash_fwd_kernelI23Flash_fwd_kernel_traitsILi512ELi64ELi32ELi4ELb0ELb0EN7cutlass6half_tE19Flash_kernel_traitsILi512ELi64ELi32ELi4ES2_EELb1ELb1ELb1ELb0ELb0EEv16Flash_fwd_params
    808 bytes stack frame, 1664 bytes spill stores, 1748 bytes spill loads
ptxas info    : Used 255 registers, 584 bytes cmem[0]
ptxas info    : Compiling entry function '_Z16flash_fwd_kernelI23Flash_fwd_kernel_traitsILi512ELi64ELi32ELi4ELb0ELb0EN7cutlass6half_tE19Flash_kernel_traitsILi512ELi64ELi32ELi4ES2_EELb1ELb1ELb1ELb0ELb1EEv16Flash_fwd_params' for 'sm_80'
ptxas info    : Function properties for _Z16flash_fwd_kernelI23Flash_fwd_kernel_traitsILi512ELi64ELi32ELi4ELb0ELb0EN7cutlass6half_tE19Flash_kernel_traitsILi512ELi64ELi32ELi4ES2_EELb1ELb1ELb1ELb0ELb1EEv16Flash_fwd_params
    968 bytes stack frame, 1840 bytes spill stores, 1884 bytes spill loads
ptxas info    : Used 255 registers, 584 bytes cmem[0]
ptxas info    : Compiling entry function '_Z16flash_fwd_kernelI23Flash_fwd_kernel_traitsILi512ELi64ELi32ELi4ELb0ELb0EN7cutlass6half_tE19Flash_kernel_traitsILi512ELi64ELi32ELi4ES2_EELb1ELb1ELb1ELb1ELb0EEv16Flash_fwd_params' for 'sm_80'
ptxas info    : Function properties for _Z16flash_fwd_kernelI23Flash_fwd_kernel_traitsILi512ELi64ELi32ELi4ELb0ELb0EN7cutlass6half_tE19Flash_kernel_traitsILi512ELi64ELi32ELi4ES2_EELb1ELb1ELb1ELb1ELb0EEv16Flash_fwd_params
    672 bytes stack frame, 1584 bytes spill stores, 1636 bytes spill loads
ptxas info    : Used 255 registers, 584 bytes cmem[0]
ptxas info    : Compiling entry function '_Z16flash_fwd_kernelI23Flash_fwd_kernel_traitsILi512ELi64ELi32ELi4ELb0ELb0EN7cutlass6half_tE19Flash_kernel_traitsILi512ELi64ELi32ELi4ES2_EELb1ELb1ELb1ELb1ELb1EEv16Flash_fwd_params' for 'sm_80'
ptxas info    : Function properties for _Z16flash_fwd_kernelI23Flash_fwd_kernel_traitsILi512ELi64ELi32ELi4ELb0ELb0EN7cutlass6half_tE19Flash_kernel_traitsILi512ELi64ELi32ELi4ES2_EELb1ELb1ELb1ELb1ELb1EEv16Flash_fwd_params
    872 bytes stack frame, 1860 bytes spill stores, 1950 bytes spill loads
ptxas info    : Used 255 registers, 584 bytes cmem[0]
/anaconda/envs/flashattention/lib/python3.9/site-packages/torch/include/c10/core/SymInt.h(84): warning: integer conversion resulted in a change of sign

ninja: build stopped: subcommand failed.
